 Recent work has shown that average precision can be ac-curately estimated from a small random sample of judged documents. Unfortunately, such  X  X andom pools X  cannot be used to evaluate retrieval measures in any standard way. In this work, we show that given such estimates of average precision, one can accurately infer the relevances of the re-maining unjudged documents, thus obtaining a fully judged pool that can be used in standard ways for system evalua-tion of all kinds. Using TREC data, we demonstrate that our inferred judged pools are well correlated with assessor judgments, and we further demonstrate that our inferred pools can be used to accurately infer precision recall curves and all commonly used measures of retrieval performance. H3.4 [ Information Storage and Retrieval ]: Systems and Software  X  Performance evaluation (efficiency and effective-ness) Theory, Measurement, Experimentation Relevance Judgments, Incomplete Judgments, Average Pre-cision
We consider the problem of large-scale retrieval evalua-tion. Standard methods of retrieval evaluation can be quite expensive when conducted on a large-scale. Evaluation mea-sures depend on the assumption that the relevance of docu-ments retrieved by a search engine is known, requiring enor-
We gratefully acknowledge the support provided by NSF grant IIS-0534482.
 mous human effort for judging the documents in the docu-ment collection.
 In order to avoid judging the entire document collection, TREC uses a technique called pooling where in the case of depth-k pooling, the union of the top k documents retrieved by each submitted run is formed and the documents in this pool are judged for relevance with respect to the given topic. In the standard TREC setup, depth-100 pools are found to be effectively complete in terms of robust and accurate eval-uation; hence, depth-100 pooling has been shown to be an effective way for evaluating the quality of retrieval systems without judging the complete document collection [8, 11].
Although depth-100 pooling considerably reduces the judg-ment effort needed, the pool still requires a large number of judgments. In the case of TREC, obtaining accurate, robust, and reusable assessments requires that tens of thousands of documents be judged. In TREC 8, for example, the 129 sys-tems submitted were run against 50 queries (topics) and a total of 86,850 documents were judged in order to evaluate systems with respect to all these queries. This number be-comes much larger when real collections are considered (e.g. the world wide web).

Much research has attempted to address the assessment effort required for large-scale retrieval evaluation. Shallower pools [11] and techniques that are likely to contain most of the relevant documents have been studied [6, 1]. However, when the total number of judgments are limited, all of these methods produce biased or unprovable estimates of evalua-tion measures.

Carterette et al. [5] developed a method that aims at ranking systems correctly using limited relevance judgments. However, this method cannot be used to compute a provable estimate of an evaluation measure when incomplete judg-ments are present.

Recently, Aslam et al. [2] and Yilmaz and Aslam [10] pro-posed statistical techniques for estimating the value of an evaluation measure from a judged random sample of doc-uments. Unlike previous methods, these statistical tech-niques determine unbiased estimates of the standard mea-sures themselves. These results show that average precision (and other measures of retrieval performance) can be accu-rately estimated from a carefu lly chosen judged random pool as small as 4% of the typical TREC-style depth pool.
One disadvantage of the aforementioned statistical tech-niques is that they cannot be used to assess the performance of systems in any standard way: in order to estimate the measures, a special procedure requiring access to the infor-mation from the sampling process is needed, so standard tools such as trec_eval or other software implementations which calculate average precision and other performance measures cannot be used. The computation of evaluation measures in a standard way requires complete knowledge of the relevance of documents. Hence, methods that can infer the relevance of documents from a few judged documents are desired.

In this work, we show that estimates of average precision using a few judged documents can be used to accurately infer the relevance of unjudged documents, thus making it possible to infer fully judged pools from a small fraction of judged documents. We further show, through the use of TREC data, that the judgments in these inferred pools cor-relate well with actual assessments and that these inferred pools can be used to compute precision-recall curves and other measures of retrieval pe rformance in standard ways, permitting efficient standard evaluation on a large scale.
The goal of this work is to accurately infer the relevance of unjudged documents given a few relevance judgments. 1 The method for inferring these relevance judgments is based on the following three hypotheses: 1. Given the actual average precision value of a single 2. Given the actual average precision values of multiple 3. Given average precision estimates of multiple systems
In recent work, we have shown that Hypothesis (1) is largely correct; given the value of average precision of a list retrieved in response to a query and the total number of rel-evant documents in the query, one can accurately infer the distribution of relevant and nonrelevant documents in the retrieved list [4].

The main idea behind Hypothesis (2) is as follows: The ac-tual average precision of a single system provides some infor-mation about the relevance of documents retrieved by that system on a particular query. If there are multiple systems, then average precision of each system will provide some in-formation about the relevance of documents retrieved by that system on that query. Furthermore, some documents will be retrieved by multiple systems. This imposes a con-straint on what the relevance of these documents can be as the relevance of a document retrieved by multiple systems should be consistent among all systems. Hence, information from multiple systems could be used to obtain better infer-ences about the relevance of documents than the inferences obtained from a single system.
A preliminary version of this work appeared as a recent poster [3].

Assuming that Hypothesis (2) holds, based on the fact that accurate inferences about the relevance of documents can be obtained using actual average precision values to-gether with total number of relevant documents ( R )values as input, one might then use estimates of average precision computed using a few judgments to infer the relevance of unjudged documents; obtaining complete judgments (qrels) from a few judged documents (Hypothesis (3)).

If Hypothesis (3) holds, then this fact could be used to obtain complete judgments from a small number of incom-plete judgments, enabling efficient retrieval evaluation on a large scale using standard tools.

Every year there are many systems submitted to the an-nual Text REtrieval Conference. These systems are run against some number of queries (typically 50) and for each query, the depth-100 pool of the documents are formed and stored in a qrel file. For each query, the actual average pre-cision of each system is then computed using this depth-100 pool. Due to its setup and the need for extensive judgment effort, aforementioned methods (hypothesis) are applicable and useful to TREC. Hence, we used TREC data to validate our hypothesis.
 In the sections that follow, we validate the correctness of Hypotheses (2) and (3), and we describe the technique that can be used to infer complete judgments given incomplete judgments.

We begin in Section 2.1 by describing our methodology for inferring relevance judgments given average precision values of multiple lists together with the total number of relevant documents in a query as input. In Section 3, we show that the inferences obtained from the proposed method using ac-tual average precision values and actual R are highly accu-rate, thus validating Hypothesis (2).

Note that inferring relevance of documents given the value of actual average precision is not very interesting in practice as the computation of average precision requires the judg-ments at the first place. However, the problem becomes much more interesting if it i s possible to obtain accurate inferences of the relevance of unjudged documents given es-timates of average precision obtained using a few judgments. In Section 4, we show that Hypothesis (3) holds; thus, given estimates of average precision, one can accurately infer the relevance of unjudged documents.
The methodology for inferring relevance assessments from average precision is conceptually simple: given (1) the ranked lists of documents submitted in response to a given topic, (2) the average precisions associated with these lists, and (3) R , the number of documents relevant to the topic, find the binary relevance judgments associated with the underly-ing documents which minimize the  X  X ifference X  between the given average precisions and those incurred by the inferred relevance assessments. This optimization problem can be written as:
In order to ensure that the inferred relevance judgments incur average precision values  X  X lose X  to those given, we min-imize the sum squared error between the actual and inferred average precision values.

The above definition of the problem is a constrained inte-ger optimization problem. Hence, this problem is intractable for the same reason that integer programming is intractable. To alleviate this problem, we relax the condition that the inferred relevance assessments must be binary. We instead allow the inferred relevance assessments to correspond to probabilities of relevance, and we deduce an expected value for average precision from these probabilistic relevance as-sessments. Let p i be the probability of relevance associated with the document at rank i in the list of length Z .Inare-cent work [4], we showed that the expected value of average precision can then be computed as Therefore, we ensure that the inferred relevance judgments incur average precision values  X  X lose X  to those given by min-imizing the sum squared error between the actual and in-ferred expected average precision values. Thus, our opti-mization criterion is to minimize the mean squared error, min i ( E [ AP i ]  X  ap i ) 2 where ap i is the given average pre-cision associated with list i . The problem as formulated above can be solved using any number of constrained opti-mization routines, available, for instance, in MatLab.
The output of the above optimization procedure is the probability of relevance of the documents that are in the complete judgment set (the depth-100 pool in TREC). How-ever, most of the standard evaluation measures such as av-erage precision, R-precision and precision-at-cutoff k use bi-nary judgments, 2 i.e., a document can be either relevant (1) or nonrelevant (0). Hence, in order to make use of the inferred probabilistic judgments in a standard way, these judgments must be converted to binary judgments.

There are three intuitive ways of converting probabilistic relevance assessments to binary. (1) A trivial possibility is to threshold the probabilities at 0 . 5 and assign a relevance score of 1 to documents with probability of relevance at least 0 and assign a relevance score of 0 to the remaining documents. (2) Another possibility is to sort the documents in decreasing order based on their probability of relevance and assign a relevance score of 1 to the top R documents (if we know or assume that there are R relevant documents in total). (3) The last option is based on the idea of randomized rounding
In a recent work [4], together with the definition of general-ized average precision using probability of relevances, we also define the generalized probabilistic versions of R-precision and precision-at-cutoff k . Given a ranked list of N docu-ments with probability of relevances p 1 ,p 2 ,...p N and the total number of relevant documents R in the query, the R-precision and precision-at-cutoff k values can be computed as Hence, one can use the generalized probabilistic versions of these measures to evaluate the retrieval systems using the inferred probabilistic qrels.
 Figure 1: Diagram for inferring relevance judg-ments. The upper right plot corresponds to the in-put estimates of average precisions and R and the upper left plot corresponds to the document con-straints imposed by documents retrieved from mul-tiple lists. Using these inputs, the optimization pro-cedure then can be used to obtain complete rele-vance judgments (qrels) that can be used to accu-rately evaluate systems in a standard way. that is used in linear programming to solve integer programs using probabilities. Based on this method, a document with probability of relevance p is assigned a relevance score of 1 with this probability and a score of 0 with probability 1
Later in this paper, we explore all three methods and show that randomized rounding experimentally gives the best re-sults. Hence, this method will be used to convert proba-bilistic relevance judgments to binary in the remainder of our experiments.

Figure 1 shows how the aforementioned method can be used for inferring complete judgments from incomplete judg-ments. The black box in the figure corresponds to the method described above at an abstract level. The input to the opti-mization method is an estimate of R together with average precision estimates of multiple systems. The optimization method makes use of the fact that any document contained in multiple lists must have the same relevance assessment and outputs complete relevance judgments (inferred qrels) thatcouldbeusedinanystandardway(e.g. trec_eval )to evaluate retrieval systems.
One of the main hypotheses of this paper is that given the actual values of average precision for multiple systems and the total number of relevant documents R in a query, the relevance of documents in the complete pool can accurately be inferred (Hypothesis 2). The goal of this section is to validate this hypothesis using TREC data.

To be consistent with TREC terminology where the set of complete judgments are referre d to as qrels, in the remainder of this paper, we name the inferred relevance judgments as inferred qrels and the relevance judgments available from TREC as actual qrels .

Using data from TREC, we run the method with actual average precision and R values and obtain a probability of relevance for each document. 5and9inTREC8,sortedindecreasingorder.
The runs that are submitted to TREC are naturally split into two groups: those runs which contributed to the original TREC depth 100 pool (actual qrel) and those which did not contribute to the pool. In the case of TREC 8, for example, there are 70 runs that contribute to the pool and 59 runs which did not. We ran our method for inferring qrels using only the runs that contribute to the pool as input. For each topic, we inferred relevance judgments by running the aforementioned method with, as input, the actual average precision values of the runs that contributed to the pool and the actual total number of relevant documents ( R )in the topic.

To verify our hypothesis, we need to evaluate the quality of the inferred qrels to show that without knowing anything about which documents are relevant, and by using the values of actual average precision values and R and the structure of the lists alone, one can accurately infer which documents are relevant.

The quality of the inferred qrels can be evaluated based on at least three criteria: 1. How do the inferred qrels evaluate the input systems, 2. How do the inferred qrels generalize to evaluating un-3. How do the inferred qrels compare with actual qrels?
In order to perform any of these evaluations in a standard way, the inferred qrels need to be binary. However, the inferred qrels obtained from our optimization method are non-binary (probabilistic). Hence, we need to convert the inferred probabilistic relevance assessments to binary.
In the previous section, we mentioned three different meth-ods that could be used to convert probabilities of relevance to binary judgments: (1) thresholding at 0.5, (2) picking the top R documents based on probability of relevance, and (3) randomized rounding. If there is a sharp contrast among the probabilities of relevance of documents in the inferred qrels, i.e., most of the documents have probabilities of relevance either close to 1 or close to 0, all these methods would give similar results. Even though this is the case for some top-ics, for some of the inferred qrels there are many documents with probability of relevance between 0 and 1.

Figure 2 shows the inferred probabilistic qrels obtained for topics 1, 5 and 9 in TREC 8. For each of these queries, we sort the documents in decreasing order based on their prob-ability of relevance and plot the probability of relevances of the top 2 R documents. The first two plots in the figure show that the inferred probability of relevances of documents do not always have a sharp contrast, hence, the method used to convert the probabilities to binary values makes a difference.
In order to assess all three methods for converting prob-abilistic judgments to binary, we focused on the first eval-uation criterion, namely, how do the inferred qrels evaluate the systems compared to actual qrels? We convert the prob-abilistic qrels to binary using all three possible methods, ob-taining three different inferred binary qrels. We then com-pute average precisions of the systems using each inferred binary qrel and compare the inferred average precision val-ues to actual average precisions of systems. queries 1, 2 and 3 in TREC 8.

Figure 3 shows how the mean average precision (average precision averaged over all queries) estimates calculated us-ing the binary relevance judgments obtained using the three proposed methods in the given order compare with the ac-tual MAP (mean average precision) values. The dots in these plots refer to the systems that were used to create the pool (referred to as the training systems) and the plus signs refer to the systems that did not contribute to the pool (re-ferred to as the testing systems). For comparison purposes, the three plots report the root mean squared error (How good are the estimates in terms of value?) and Kendall X  X   X  (How good are the estimates in terms of ranking?). It can be seen that if randomized rounding is used to convert inferred probabilistic qrels to binary, then the MAP values computed through the inferred binary qrels are very close to the actual MAP values. Hence, throughout this paper, we will use the method of randomized rounding to convert the inferred probabilistic qrels to binary.
Once the inferred binary qrels are obtained, we can now evaluate the quality of inferred qrels based on the three eval-uation criteria in order to verify the hypothesis that the in-ferences are highly accurate.
 Criterion 1: How do the inferred qrels evaluate the systems, as compared to actual qrels?
The first evaluation criterion is whether the inferred qrels can evaluate systems in the same way as actual qrels or not. Since many aspects of retrieval performance can be inferred from the precision-recall curves, one way to evaluate this is to compute the precision-recall curves of the systems using the inferred qrels and compare th ese inferred precision-recall curves with actual precision-recall curves. Figure 4 shows the inferred precision-recall curves vs. actual precision-recall curves of a randomly chosen system from TREC 8 (system MITSLStd) on queries 1, 2 and 3. It can be seen that the inferred precision-recall curve s are highly accurate, and they are even identical to actual precision-recall curves for some queries (query 3).

Since the precision-recall curves obtained using the in-ferred qrels are highly accurate, the inferred qrels are ex-pected to evaluate systems similar to the actual qrel. In order to test this, we compute the estimates of standard measures mean R-precision, and mean precision-at-cutoff 10 and 100 using the inferred qrels and compare these estimates to the actual values of these standard measures. Figure 5 Table 1: Precision, recall and F 1 values of the in-ferred binary qrels for TREC 7, 8 and 10. shows that the inferred qrels evaluate the systems in essen-tially the same way as the actual qrels.
 Criterion 2: How do the inferred qrels generalize to evaluating unseen runs?
When a test collection is built, one of the primary goals is to be able to use the test collection to evaluate unseen systems (the systems that didn X  X  contribute to the test col-lection) (reusability). Hence, it is important that our in-ferred qrels generalize to evaluating unseen runs (the sec-ond evaluation criteria). Since we only use the systems that contribute to the pool as input (the training systems), the generalizability of the inferred qrels can be evaluated based on how well the inferred qrels can evaluate the systems that did not contribute to the pool (testing systems). Figure 5 shows that the inferred qrels generalize well to the testing systems, in the sense that the estimates values of the mea-sures for these systems are also very close the actual values of these measures. Hence, the inferred qrels are reusable. Criterion 3: How do the inferred qrels compare with actual qrels?
Note that until now we have shown that the inferred qrels evaluate the systems in the same way as the actual qrel. However, this does not necessarily mean that the relevance judgments in these qrels are  X  X orrect. X  One would like to compare the inferred qrels with the actual qrels to check if the relevant documents in the inferred qrel match the rele-vant documents in the actual qrel (the third evaluation cri-terion). In order to perform this comparison, we treat the relevant documents in the inferred qrel as a set and calcu-late the set precision, recall and F 1 values (averaged over 50 queries) of these sets using the actual qrels for TRECs 7, 8 and 10. Note that precision value computed in this setup corresponds to the question  X  X hat fraction of the documents that are relevant in the inferred qrel are actually relevant in the actual qrel? X  and the recal l value corresponds to the question  X  X hat fraction of the documents that are actually relevant are identified as relevant in the inferred qrels? X .
Table 1 shows the computed precision, recall, and F 1 val-ues for TRECs 7, 8 and 10. It can be seen through the inferred qrels vs. the actual values of these measures for TREC 8. precision, recall and F 1 measures that the relevances of the majority of the documents in the inferred qrels match with their relevances in the actual qrels.

While our relevance inferences are largely correct, there are also clearly differences. One can argue that the actual qrels reported by TREC cannot be considered as the only  X  X orrect X  way of judging the documents. Voorhees [9] shows that different judges may highly disagree on what is relevant and what is not, and hence the qrels formed by different judges may be quite different. This difference between the inferred qrels and the actual qrels may be due to a differ-ent interpretation of relevance compared to the judge that created the actual qrels. In the future, we plan to manually investigate the documents for which the inferred qrels and actual qrels disagree.

Basedonthethreeevaluationcriteria,wehaveshownthat given the values of average precision for multiple lists, to-gether with the total number of relevant documents in a query, one can accurately infer the relevance of documents, validating Hypothesis (2). In the following section, we val-idate Hypothesis (3), which is more interesting and highly useful in practice.
In the previous section, we have shown that using the value of actual average precision and R , one can accurately infer the relevance of the documents using the proposed methodology (Hypothesis (2)). Inferring relevances of docu-ments using the values of actual AP and R is not very useful in practice since in order to obtain these actual values, one needs to have access to the complete relevance judgment set that we wish to infer.

Since Hypothesis (2) is valid, one might reasonably argue expect that using accurate estimates of average precisions and R (obtained by judging a few relevant documents) one could still accurately infer the relevance of documents (even the documents that were not judged to obtain the initial esti-mates); Hypothesis (3). This is particularly important since if one could accurately infer complete judgments by judging a few documents, then large scale evaluation using standard tools would be possible by judging very few documents.
In order to make use of the proposed optimization method to infer complete judgments from a limited number of rele-vance judgments, we first need to obtain estimates of average precision and R values from these limited judgments. Any method that produces accurate estimates of average preci-sion from incomplete judgments could be used to used to estimate these values.

We recently developed a measure, inferred AP, that can be used to estimate average precision when only limited judg-ments are available [10]. We also developed a statistical method for estimating evaluation measures such as average precision, R-precision and precision-at-cutoff k as well as using limited relevance judgments [2]. This latter method uses random sampling to compute the expected value of av-erage precision using limited judgments. Both of these meth-ods are valid options for computing the input average preci-sion estimates; and we will use the latter method to obtain the input estimates of average precision and R . Throughout this paper, we refer to the estimates of AP and R obtained using this statistical method as the sampling estimates .
Figure 6 shows how the MAP sampling estimates com-puted using 29, 71, and 200 judgments on average per query using the above statistical method compare with actual MAP values for TREC 8. These judgments correspond to 1 . 7%, 4 . 1%, and 11 . 5% of the complete judged depth-100 pool. Figure 6 shows that the MAP estimates obtained are highly accurate estimates of the actual MAP value obtained using complete judgments (1737 judgments on average per query).
Having identified the method to compute estimates of AP and R , we now describe in greater detail the method used to infer relevance judgments given access to a limited number of relevance judgments. This method can be explained in four main steps: 1. Obtain input estimates : Sample and judge some doc-2. Optimization : Use the estimates obtained through the 3. From probabilities to binary : Convert the inferred prob-4. Correction : The judgments obtained in the previous (depth 10) judgments for TREC 8. for TREC 8 vs. the actual qrels for queries 1, 2, and 3 in TREC 8.
Note that we include the last step so that the judgment effort that was already used to create the estimates is not wasted. Later in this paper we show that inferred qrels are quite accurate even without this correction and that the effect of this correction is not dramatic.

Using these steps, we now have a method to infer com-plete relevance judgments (qrels) given small number of in-put judgments and the estimates of average precision and R obtained from these input judgments. Now, we can show that these inferred qrels are  X  X lose X  to the actual qrels that are generated using many more relevance judgments, val-idating our hypothesis that average precision estimates of multiple systems (together with an estimate of R )canbe used to infer complete relevance judgments (Hypothesis (3)).
In order to evaluate the quality of the inferred qrels, we use our three criteria defined in the previous section. For evaluation, we use data from TRECs 7, 8 and 10, focusing on results obtained from TREC 8 due to space constraints. For all TRECs, we run the above method using estimates obtained from judgments with d ifferent levels of incomplete-ness. For TREC 8, we mainly focus on the inferred qrels obtained from estimates of AP and R computed with 29 (1 . 7% of complete judgments), 71 (4 . 1% of complete judg-ments), and 200 (11 . 5% of complete judgments) judgments as input. 3 Criterion 1: How do the inferred qrels evaluate the systems, as compared to actual qrels?
Our first criterion in evalua ting the quality of inferred qrels is how well they evaluate systems. Using the same idea from the previous section, we show that (1) precision-recall curves of systems using inferred qrels are very close to actual precision-recall curv es, and (2) inferred qrels and actual qrels evaluate systems similarly.

Figure 7 shows the inferred precision-recall curves of the system MITSLStd for queries 1, 2 and 3 when the inferred qrels are obtained from sampling estimates using 29 (first row), 71 (middle row) and 200 (third row) judgments. It can be seen that with as few as 29 judgments (1 . 7%) per query, the inferred precision-recall curve of the system is close to the actual precision-recall curve of this system. Further-more, with as few as 200 (11 . 5% ) judgments per query, the inferred precision-recall curves are almost exactly the same as the actual precision-recall curves. Also, the precision-recall curves obtained from sampling estimates with 200 judgments are almost as good as the precision-recall curves obtained using actual AP as input (Figure 4).

According to Figure 7, for some queries, the area under the inferred precision-recall curves is more or less than the area under the actual precision-recall curve (e.g., mostly low for inferred precision-recall cur ves from 71 judgments). Since average precision is an approximation to the area under the precision-recall curve, this means that the inferred average precision of the system on these queries is more/less than the actual average precision value.

Furthermore, at first glance, the inferred precision-recall curves obtained using 71 judgments seem worse than the in-ferred precision-recall curves from 29 judgments. This may seem counterintuitive given that the input sampling MAP estimates from 71 judgments are much better than the sam-pling MAP estimates from 29 judgments (Figure 6).
This behavior is due to the variability inherent in the input estimates from the sampling method. Since this method is based on random sampling, the average precision estimates of a system on some queries may be lower/higher than its actual value (variance). However, when these estimates are averaged over many queries, the resulting estimate of mean average precision is an unbiased estimate of mean actual average precision [2] (reduction of variance). Since the opti-mization method finds the best fit to the given average pre-
In the sampling method [2], the sampling estimates of the measures are compared with the estimates obtained using depth k pooling by sampling and judging the same number of documents on average per query as would be judged if depth k pooling was used. Using the same setup, we use the estimates obtained using equivalent judgments needed when depth k pooling is used, for k  X  X  1 , 3 , 5 , 10 , 15 , 20 cision values, the average precision estimates obtained from the inferred qrels are also expected to have some variance.
For example, since the area under the inferred precision-recall curves from 71 judgments is less than the area under the actual precision-recall curves for queries 1, 2 and 3 due to input, it means that this inferred area is higher than the actual area for some other queries, since the inferred mean average precision values are very close to the actual mean average precision values. Note also that as better (lower variance) estimates of average precision are used as input to the optimization procedure, the quality of the inferred qrels would likely increase.

To evaluate how inferred qrels evaluate systems when com-pared to actual qrels, Figure 8 shows how the inferred qrels using sampling estimates obtained from 29, 71 and 200 judg-ments as input evaluate the retrieval systems. It can be seen from the plots that using as few as 29 judgments, the in-ferred MAP, MRP, MPC(10) and MPC(100) values are very close to their actual values, especially for ranking purposes (Kendall X  X   X  ). The inferred values of these measures are even closer to their actual values when 71 judgments are used, and when 200 judgments are used to infer the qrels, the in-ferred qrels evaluate systems almost identically to the actual qrels (third row). Note that the mean average precision val-ues computed using inferred qrels are expected to be close to the actual mean average values, since the optimization procedure finds the best fit to the input sampling average precision estimates, which are quite accurate. Therefore, the fact that inferred mean average precision values are highly correlated with actual mean average values is not very in-teresting or surprising. However, the fact that the inferred MRP, MPC(10) and MPC(100) values are highly accurate proves that the inferred qrels are highly useful and can be used to accurately evaluate any standard measure.
One interesting result is that when the input MAP es-timates obtained through the sampling method (Figure 6) and the MAP estimates obtained through the inferred qrels (leftmost plots in Figure 8) are compared, it can be seen that the estimates obtained through the inferred qrels have less variance and are often better than the input estimates. For example, inferred mean average precision values obtained from 29 judgments (top left plot in Figure 8) are much better than the input sampling estimates from 29 judgments (Fig-ure 6). This improvement can be explained as follows: As it can be seen, the sampling estimates input to the optimiza-tion procedure are noisy due to random sampling. However, the optimization procedure cannot perfectly fit to this ran-dom noise, especially given the constraint that document re-trieved by multiple systems should have the same relevance. Hence, the optimization procedure finds the best possible fit subject to these constraints, resulting in a reduction in the noise associated with the input. Thus, the inferred qrels evaluate systems even better than the input.
 Criterion 2: How do the inferred qrels generalize to evaluating unseen runs?
Our second evaluation criterion is: How well do the in-ferred qrels generalize to evaluating unseen runs? Figure 8 shows that the Kendall X  X   X  and RMS error values for the training and testing systems are not much different, show-ing that the inferred qrels generalize well to unseen systems. Criterion 3: How do the inferred qrels compare with actual qrels? Our final evaluation criterion is whether Table 2: Precision, recall and F 1 values of the cor-rected and uncorrected inferred qrels for TREC 7, 8 and 10 when sampling estimates obtained with var-ious number of judgments as input. the inferred qrels are similar to actual qrels. Following our previous setup, we treat the relevant documents in the in-ferred qrel as a set and calculate the set precision, recall and F 1 values (averaged over 50 queries) of these sets using the actual qrels for TRECs 7, 8 and 10 (Table 2). It can be seen that very high precision values (showing that the rele-vant documents in the inferred qrels are also relevant in the actual qrel), and very high recall values (showing that the inferred qrels contain most of the relevant documents in the actual qrel) can be obtained using as few as approximately 21% (or even less) of the complete relevance judgments as input, concluding that the inferred qrels are quite similar to actual qrels. The table contains two columns labeled correction and no correction . These columns refer to the qrels obtained when the judgments used to obtain the input sampling estimates are used to c orrect the inferred qrel and when these input judgments are not used for correction (the last step of the procedure). The goal of this is to check the effect of these given judgments on the inferred qrels. Is the performance of the method mainly governed by these given judgments, or is the method itself correctly identifying most of the relevant documents? It can be seen that even without using correction, the optimization method accurately iden-tifies many relevant documents.
 This behavior can also be seen in Figure 9. For various TRECs, this figure shows the number of relevant documents in the given input judgments (averaged over all queries) vs. the number of relevant documents in the inferred qrels (with and without correction) as the number of input judgments used in obtaining the sampling estimates varies. It can be to create input sampling estimates changes. seen that especially when the number of input judgments is small, the proposed method finds many more relevant documents than the relevant documents given as the input judgments. Also, as expected, using the input judgments to correct to qrels does not change the inferred qrels much when the number of input judgments is small.

We have shown that based on all three evaluation criteria, the qrels inferred from estimates are highly accurate, validat-ing the claim that the qrels obtained from a small number of judgments using the proposed method can reliably be used to construct test collections with limited judgment budget.
We described a method that can be used to infer com-plete judgments (qrels) given a small number of judged doc-uments. The method uses estimates of average precision of multiple systems together with estimate of R , computed us-ing a small number of relevance judgments, to infer fully judged pools from a small fraction of judged documents.
The proposed method has great potential for efficient large-scale retrieval evaluation. First, we show that the inferred qrels are highly accurate in the sense that (1) they evaluate systems similarly to actual qrels and (2) the relevance of doc-uments in the inferred qrels are very similar to actual qrels. Hence, these inferred qrels obtained from small judgments can reliably be used in place of actual qrels, significantly decreasing the judgment effort needed.

Furthermore, we show that the i nferred qrels contain many relevant documents, many more than the initial judged doc-uments contain. Hence, the proposed method can be used to build qrels with real judgments using many fewer judg-ments than the traditional depth-100 pools, as follows: (1) judge some small number of relevant documents to obtain the sampling estimates, (2) run the optimization procedure to obtain the inferred qrels, and (3) judge the documents that are marked as relevant in the inferred qrels. Since the inferred qrels correctly identi fy most of the relevant docu-ments, by judging only the documents marked as relevant in the inferred qrels (which are much less than the size of the depth-100 pool), most of the relevant documents can be correctly identified. Furthe rmore, this three step process could be repeated by obtaining better estimates of average precision using the judgments in the last step and feeding these estimates back into the first step. In the future, we plan to investigate this process in more detail.
