 A. Z. Broder  X   X  R. Lempel  X  F. Maghoul  X  J. Pedersen
Abstract We present a framework for approximating random-walk based probability dis-tributions over Web pages using graph aggregation. The basic idea is to partition the graph into classes of quasi-equivalent vertices, to project the page-based random walk to be ap-proximated onto those classes, and to compute the stationary probability distribution of the resulting class-based random walk. From this distribution we can quickly reconstruct a dis-tribution on pages. In particular, our framework can approximate the well-known PageRank distribution by setting the classes according to the set of pages on each Web host.
We experimented on a Web-graph containing over 1.4 billion pages and over 6.6 billion links from a crawl of the Web conducted by AltaVista in September 2003. We were able to produce a ranking that has Spearman rank-order correlation of 0.95 with respect to PageRank.
The clock time required by a simplistic implementation of our method was less than half the time required by a highly optimized implementation of PageRank, implying that larger speedup factors are probably possible.

Keywords We b I R  X  Citation and link analysis 1. Introduction page, Web-specific data such as link analysis, anchor-text, and click-through data. Google ( www.google.com ) was the first engine to use link analysis as a primary ranking factor and the now-defunct DirectHit 1 concentrated on click-through data. By now, all major engines handling navigational queries (Broder, 2002).
 independent importance scores that are assigned to all Web pages. The most famous algorithm for producing such scores is PageRank , devised by Brin and Page (1998) while developing the ranking module for the prototype of Google. The basic intuition behind PageRank, de-all links or citations are equal: links from more important pages should count more. We are thus led to a recursive definition of importance, that can be formalized as the solution of a probability distribution of a certain random walk on the Web graph, that is, the graph whose nodes are the Web pages, and whose directed edges are the links between them. We formalize this discussion in Section 2.1.

There is a large PageRank related literature, sometimes showing applications not directly related to ranking. Cho et al. (1998), for example, use PageRank to prioritize crawlers and voting model proposed by Lifantsev (2000) can also be seen as a general and extensible variant of PageRank. Numerical properties of PageRank have been studied in Pandurangan et al. (2002), Ng et al. (2001), Lempel and Moran (2001), Lee (2002), Chien et al. (2002) and topic-sensitive and personalized versions of PageRank were described in Haveliwala, (2002), Haveliwala et al. (2003), Jeh and Widom (2003), Richardson and Domingos (2001). We discuss some of this literature in more detail in Section 2.1.
 links. Computing PageRank on Web graphs of this scale requires considerable computational resources, both in terms of CPU cycles and in terms of random-access memory. Given the importance of PageRank in ranking search results, it is not surprising that there has been considerable interest in schemes that accelerate PageRank type computations (Haveliwala, 1999; Kamvar et al., 2003a,c; Abiteboul et al., 2003).
 To some extent our work follows in this vein. We use a framework for computing basic idea is to partition the graph into classes of quasi-equivalent vertices and to compute the classes used were the sets of pages on a given host. The random walk associated with
PageRank is decomposed into intra-host and inter-host steps. In our algorithm, inter-host not necessarily driven by the underlying graph, but instead are governed by a fixed distri-bution. Thus we obtain only an approximation of PageRank rather than the exact PageRank on the similarities and differences between our approach and that of Kamvar et al. (2003b).
As with any new proposed static ranking, one must examine the appropriateness of the scores to the task at hand X  X chieving high quality Web search. We tackle this evaluation task by statistically comparing the scores produced by our approach with those produced by
PageRank, demonstrating that we are able to construct a good approximation of PageRank. In our experiments, based on a Web-graph containing over 1.4 billion pages and over 6.6 billion links from a crawl of the Web conducted by AltaVista, we are able to produce a ranking that has Spearman rank-order correlation (Snedecor and Cochran, 1989) of 0.95 with respect to PageRank.

In terms of use for ranking of Web search results, we do not know whether the (small) differences between our model and the original PageRank are for better or for worse. Our model is much less sensitive to changes in the internal organization of hosts, which might be an advantage.

The algorithm spends most of its running time finding a stationary distribution on the Web host graph, that is, the graph that has an edge from host h scores. Since the Web X  X  host graph is significantly smaller than the Web X  X  page graph (by factors of 20 and beyond in the number of edges), the algorithm scales well. The clock time required by a simplistic implementation of our method was less than half the time required by a highly optimized implementation of PageRank. We expect that refined implementations will be able to yield speed-up factors that are closer to the ratio between the sizes of the
Web X  X  page and host graphs. Furthermore, optimization techniques for PageRank such as (Haveliwala, 1999; Kamvar et al., 2003c) might be able to speed up our approach as well.
The rest of this paper is organized as follows: Section 2 starts by reviewing the PageRank algorithm, and then moves on to survey previous work on optimizing PageRank X  X  computation process, and discussions of  X  X lavored X  PageRank variants. It recounts alternative static rank approaches that have been proposed, and reports on some studies of the host-graph of the
Web. Section 3 presents the details of our scheme, and compares its complexity with that of the original PageRank algorithm. Section 4 covers our experiments with a specific graph-aggregated PageRank approximation. We concretely define the approximation flavor, provide performance figures and compare the resulting score vector with the PageRank score vector.
Section 5 concludes, and points out directions for future research. 2. Related work 2.1. PageRank and variations
PageRank (Brin and Page, 1998) is an important part of the ranking function of the Google search engine. The PageRank of a page p is the probability of visiting p in a random walk of the entire Web, where the set of states of the random walk is the set of Web pages, and each random step is of one of the following two types: and follow that link to the destination page.
 Teleportation : choose a Web page uniformly at random, and jump to it.
 probability d , or a teleportation step with probability 1
PageRank requires teleportations (jumps to random Web pages) since the Markov chain of pages which forms a strongly connected component in the graph, a majority of Web pages random jumps introduces a (small) probability of transition from any page a to any page b , even in absence of a Web link a  X  b , thus giving rise to an ergodic Markov chain that has a well-defined stationary distribution (Gallager, 1996). Furthermore, the set of PageRank scores obey the following formula (where page p has incoming links from q is the total number of Web pages):
PageRank( p ) =
The above set of equations is easily written in matrix form, with the stationary distribution (the vector of PageRank scores) simply being the principal eigenvector of the corresponding stochastic matrix. The PageRank scores are typically computed by applying the Power method for approximating the principal eigenvector of a matrix (Jennings, 1977). The method involves repeated multiplications of an arbitrary initial vector by the matrix in question, until the iterations converge to a fixed vector. 2.1.1. Numerical properties of PageRank
Pandurangan et al. (2002) have studied the distribution of PageRank scores across several independent Web subgraphs, and noted that it follows a power-law. Furthermore, the exponent of the distribution was found to be 2.1 X  X imilar to the observed power-law exponent of the distribution of in-degrees of Web pages (Barabasi and Albert, 1999; Kleinberg et al., 1999;
Broder et al., 2000). Despite the similarity in distributions, they showed (on two small Web subgraphs) that PageRank is not highly correlated with indegree. On the other hand, Upstill et al. (2003) found log indegree to be highly correlated with the PageRank values reported by Google X  X  toolbar. Thus, the measure of correlation between PageRank and in-degree (or graphs where most nodes of high in-degree have lower PageRanks than most nodes with low-indegree (Lempel and Moran, 2001).
 the L 1 -change to the scores when modifying the outgoing links of a set P of pages. They bounded this change by a linear function of the aggregate PageRanks of all pages in P . Lee (Lee, 2002) argued that an algorithm is stable if the L 1
For that definition, he showed that PageRank is stable for all graphs. Neither of these works examined how the perturbations affect the rankings that are induced by the score vectors.
That aspect was looked upon by Chien et al. (2002), who showed that when a link is added to a graph, (1) the PageRank of the node receiving the link rises, and (2) the same node X  X  rank cannot decrease with respect to its rank prior to the change. However, in Lempel and Moran (2001) is was shown that by changing the destination of a single link in an arbitrarily large graph, the relative rankings of many pairs of nodes (about a quarter of all pairs) may flip. 2.1.2. Towards personalized PageRank
Brin and Page, when presenting PageRank, noted that it is possible to obtain topic-oriented flavors of PageRank by biasing the random jumps of the algorithm to favor topical pages as destinations, rather than jumping uniformly at random over all Web pages (Brin and Page, 1998). This idea was expanded by Haveliwala (2002), where 16 precomputed topic-sensitive flavors of PageRank (corresponding to the 16 top-level categories of the ODP improve rankings of queries in real-time. Each PageRank flavor was computed by distributing its random jumps uniformly across the pages belonging to the corresponding ODP top-level category. Then, at runtime, a linear combination of those flavors was used for ranking, where the combination X  X  coefficients are determined by the similarity of the query (and additional context, if available) to the contents of the pages in each category. Jeh and Widom (2003) discuss personalized PageRank flavors where random jump configurations are more flexible.
They assume that there is a set H of pages (which may contain several thousand pages), such that each personalized flavor may choose any distribution of random jumps over H . See Haveliwala et al. (2003) for additional details on the above approaches.

Richardson and Domingos (2001) proposed computing a PageRank flavor for each term in the lexicon of a search engine X  X  index. For each such term, their formulation biases both the random jump probabilities and the link-following steps towards pages whose relevance score with respect to the term is high. They report gains of about 20% in search quality. 2.1.3. Accelerations of PageRank
As mentioned in the Introduction, Indices of modern search engines contain billions of pages, interconnected by tens of billions of links. Performing PageRank computations on data of this scale requires considerable resources, both in terms of CPU cycles and in terms of random-access memory. Clock-wall times for PageRank computations on large graphs can reach many hours (certainly in single-machine settings). When considering that the topic-induced or perform such computations multiple times, the need for speedy implementations becomes critical. Consequently, several papers have described methods for accelerating PageRank computations.

Haveliwala shows how PageRank computations can be adapted to run on machines with limited RAM (Haveliwala, 1999). The performance gains are due to RAM-aware partitioning of the score vectors and connectivity data that are used during the Power iterations. Certain hardware configurations are shown where speedup factors reach three orders of magnitude.
Chen et al. (2002) addressed RAM limitations by applying techniques of out-of-core graph al-gorithms to PageRank X  X  Power iterations. Their approach resulted in significant performance gains as the ratio between the size of the data and the available RAM grew.
Kamvar et al. (2003b) note the locality of reference present in Web links: most links connect pages of the same host, with many of those connecting pages that are close to each when inter-host links exist, they are usually present between multiple pairs of pages on the two hosts. These observations allow for efficient software representations of the Web graph, reducing I/O and paging costs, and enabling parallelization of the PageRank computations with reduced communication overhead. The authors also report that when starting the Power method X  X  iterations from an initial vector that is based on intra-host PageRank scores, the halved. Overall, a speedup factor of 2 X 3 in PageRank computation is obtained in this work. See Section 3 for more on the differences between Kamvar et al. (2003b) and this paper.
Kamvar et al. (2003c) accelerate PageRank by adapting known algebraic methods for accelerating convergence of general linear sequences, to Power iterations over stochastic matrices. Speedup factors of up to 3 are reported. In Kamvar et al. (2003a), the authors requiring many more power iterations to converge. This leads them to an algorithm that their contributions to scores of other pages) in subsequent iterations. Speedup factors of nearly 30% are achieved. 2.2. Alternatives to PageRank alterations of the random jump behavior of the algorithm that produce different flavors of
PageRank. In either case, the basic random walk model remained the same. This section discusses this model more critically, and surveys some alternatives (or major deviations) suggested in the literature.

While little is known about the exact use of PageRank by Google, it is widely believed used by the engine. The practical implementation of PageRank, for example, might adopt a common practice in many link analysis algorithms, and follow internal links (links connecting pages within a site) with different probabilities than external links (links connecting pages browsing step. In particular, it was noted in Amitay et al. (2003) that PageRank may not be the most appropriate browsing model for many sites. One scenario discussed there is of a to submit a query and continue to browse the search results (essentially performing a random jump), than to follow the link from the engine X  X  home page to its  X  X bout X  page. As defined, level of some Web hierarchy, e.g. Yahoo! ( www.yahoo.com ). Such surfers will either enter the directory structure to one of the category pages. Today X  X  hierarchies contain thousands of categories, and the layout of the sites is such that browsing paths to some categories are quite long. In PageRank, the probability of sustaining a long browsing sequence decreases exponentially, because of the random jumps that are performed probabilistically at each step.
Thus, the contribution of the PageRank of the hierarchy X  X  home page to a category page decreases exponentially in the category X  X  depth. The category pages are essentially punished because of the fact that the directory is well organized.
 Tsoi et al. (2003) study the problem of assigning static scores, close as possible to normal algorithm based on quadratic programming, and then propose some dimensionality reduction schemes on the scale of the data so as to enable practical implementations of the algorithm. In Tomlin (2003), the author suggested a ranking model based on network flows through the
Web graph. PageRank is actually a special case of this more general model, where the flow of probabilities through the network is conserved. He heuristically compared the quality of the rankings produced by PageRank with those produced by two other variants of this model, one of which seemed to produce rankings of equal or higher quality than those produced by PageRank. Abiteboul et al. ( 2003) propose an algorithm that is able to compute static scores in an online, incremental fashion while continually scanning the underlying graph graph need not be stored X  X t is sufficient to only consider the incident (or outgoing) edges of the current node being visited. Furthermore, no constraints are imposed on the scanning order of the graph X  X hey merely require that each node will be visited infinitely often. Also presented is an adaptation of OPIC that is suitable for scoring changing graphs, which is of special interest on the ever-changing Web. 2.3. The host-graph of the web
Our algorithm, described in the next section, makes use of the fact that the number of hosts on the Web, and the number of links between pages of different hosts, are significantly lower than the corresponding numbers of pages and interconnecting links.

Ruhl et al. (2001) studied the host-graph induced by a large snapshot (604 M pages) of the Web from August 2000. The nodes of the hostgraph represented Web hosts, and a directed link between two hosts existed if any page from the source host linked to any page of the destination host. They report that the number of hosts in that snapshot was merely 10.37 million. Furthermore, the 5540 million links between Web pages induced less than 263 million links (below 5% of the original number) in the host-graph. Kamvar et al. (2003b) more than 93% of the links connected same-host pages.

Our experimental setup, presented in Section 4, exhibits similar ratios: we used a Web-graph containing over 1446 million pages with almost 6650 million links. The number of unique hosts in this graph was about 31 million (just over 2% of the number of pages), with 241 million host-to-host edges (3.6% of the number of page-to-page links). 3. Stationary distributions based on graph aggregation
Let T be a random walk on a graph with n nodes. T will denote both the random walk and the stochastic matrix that governs it. Let the n nodes be partitioned into m classes H
From T and the m classes of nodes we develop an alternative random walk, T , whose from a node x  X  H i consists of the following two-stage process:  X 
Move to some node y  X  H i according to a distribution  X  i class, but not on the particular vertex x in the class.  X  From y move on according to T .

Note that in general, this alternative walk is not mathematically equivalent to the original random walk T . Furthermore, for many choices of T , H 1 ,..., random walks may result in very different stationary distributions.

For the purposes of this paper, we concentrate on the case where T denotes PageRank and the partitioning of Web pages is according to their host. Hence, a class H
Web pages by hosts is quite natural, as this partitioning reflects the ownership and themes of the content. Furthermore, intra-host linkage patterns are more regular and predictable than inter-host linkage patterns, due to the prevalent use of templates in Web sites (Bar-
Yossef and Rajagopalan, 2002). Thus, representing intra-host random browsing by some host-specific distribution may enable our alternative random walk to not stray very far from PageRank.

In what follows, we show that the stationary distribution of T can be derived from the principal eigenvector of a m  X  m stochastic matrix (recall that m is the number of classes), stochastic matrix T .
 Throughout this section, the following notations are used:
T = [ t i , j ] = the original page-to-page probability transition matrix. We assume that T is n = the number of pages; thus, T has size n  X  n .

H = { H 1 ,..., H m } is the set of all hosts represented in T . m = | H | = the number of hosts.  X  h = a positive probability distribution on pages with support limited to the pages of host As explained earlier, we replace the original random walk T , by a new random walk T .
Using the notations above, and assuming that T is at page p , a transition (step) consists of two parts: 1. Jump to a page in q  X  h ( p ), chosen at random according to the distribution of the original random walk T when leaving page q .

There are several plausible models for the distributions  X  a.  X  h could be uniform over the pages of host h . b.  X  h could be proportional to deg( q ) for q  X  h and 0 elsewhere, where deg( q ) can be chosen links. c.  X  h could be based on an intra-host PageRank calculation, e.g. in the spirit of Kamvar et al. (2003b).

Let S = [ s i , j ] and  X  S = [  X  s i , j ] be the following n same-host pages. In S , each row of the sub-matrix that corresponds to the pages of host h is a uniform distribution over the pages of h . Thus  X  SS =
Clearly, the random walk T described above is defined by the stochastic matrix ST : the aperiodic n  X  n stochastic matrix T , both ST and ST  X  S are also irreducible and aperiodic, and so both matrices have uniquely defined (positive, normalized) principal eigenvectors. In
Our goal can now be formally expressed X  X e aim to efficiently compute a distribution vector  X  that satisfies  X  ST =  X . (1) in calculating  X  is to compute a probability vector  X  such that
We claim that  X   X  ST satisfies Eq. (1). Indeed,  X 
ST = (  X  ST ) ST = (  X  ST )(  X  SS ) T  X  directly, we have chosen to calculate  X  via  X  , which is also a principal eigenvector of an n  X  n stochastic matrix. In what follows, however, we show that
The matrix ST  X  S satisfies the following equations: h ( p ) = h ( q ) = X   X  r , ST  X  S ( p , r ) = ST  X  S ( q h ( p ) = h ( q ) = X   X  r , ST  X  S ( r , p ) = ST  X  S ( r
Therefore, same-host sources/destinations are indistinguishable and hence the iterations re-quired to find a can be carried out using m  X  m transition matrices and m -dimensional prob-ability vectors, whose columns correspond to the distinct hosts. The transition probability from host h 1 to host h 2 is given by
Let  X   X  denote the m -dimensional stationary distribution vector corresponding to the above transition probabilities. Intra-host symmetry considerations imply that (the n -dimensional)  X  can be derived from  X   X  by simply dividing the probability assigned to each host by the number of hosted pages. Formally,  X  ( p ) =  X   X  h ( p ) / | h ( p ) | .

However, there is no need to explicitly compute  X  , since one can easily compute from  X   X  : (  X  All that is left for obtaining  X  is to multiply  X  S by T .

Note that the algorithm calculates an m -dimensional probability vector  X  probability. However, pages of the same host are weighted differently in in  X  =  X  ST . We have thus managed to calculate a distribution in page-granularity while performing eigenvector calculations in host-granularity.

To summarize, below is the algorithm to derive a PageRank approximation based on host aggregation, given the matrix T and the distributions  X  1 hosts of H : 1. Define an m  X  m stochastic matrix  X  T = [  X  t i , j ] as follows:  X  t satisfying  X   X   X  T =  X   X  . 3. Compute an n -dimensional probability distribution  X  , where for each node p ,  X  4. The stationary distribution of T is the vector  X   X  T .

We now revisit in more detail the BlockRank algorithm proposed by Kamvar et al. (2003b), and highlight several differences between their work and ours. BlockRank is a method that accelerates PageRank computations by selecting an initial distribution vector (empirically) fewer Power iterations are needed until convergence, as compared with Power iterations starting from the uniform distribution. The vector to the vector  X  produced by our algorithm when (1) T , the original random walk, represents
PageRank and (2) each distribution  X  H i is set to be the intra-host PageRank vector of H differences between v and the appropriate  X  flavor concern the definition of teleportations in the random walk over the hosts: in BlockRank, teleportations between hosts are uniform over all hosts, whereas in our approach, it follows from Eq. (2) that teleportations land on each host in proportion to the number of pages on that host.

After computing v , BlockRank performs Power iterations from
PageRank, or equivalently X  X epeatedly multiplies distribution vectors by the stochastic ma-of  X  by T . While it is clear that our approach is speedier than BlockRank, the artifacts of both algorithms are different as we do not end up calculating PageRank.
 3.1. Efficiency How does the time needed by our algorithm compare to the usual computation of PageRank? The standard power-iteration computation of PageRank converges in a few dozen iterations. Each iteration requires one pass over the complete list of links for the entire Web graph.
In contrast our algorithm needs only two passes over the entire set of links: the first when defining  X  T (step 1), and the second when transforming  X  part in our algorithm is linear in the number of links of the host-graph, which typically is much smaller (maybe by a factor of 20), than the number of links in the page-graph.
Note that the reduction in the number of links between the page-graph and the host-graph has implications beyond the simple number-of-operations accounting: modern search engines are required to compute link-based measures on connectivity data of billions of pages. This corresponds to tens of billions of links, an amount of data whose representation exceeds the
RAM capabilities of most single-machine platforms. Moving to smaller graphs such as the host graph may enable the connectivity data to once again fit in the RAM of a single machine.
In such a case our algorithm will have to perform only two (inevitably slow) passes over the full list of links, versus 25 X 40 passes for standard PageRank.

Furthermore, moving to smaller graphs that can be fully held in memory simplifies the development of software that analyzes and manipulates the transition probability matrix to achieve faster convergence. 4. Experiments
The section reports on experiments with a specific flavor of host-aggregated PageRank ap-ing two-step process: 1. Jump uniformly at random to a page q  X  h ( p ). 2. Perform a regular PageRank step from page q .

Note that in the terminology of the previous section, the matrix S that corresponds to the  X  X -model X  is simply  X  S .

Our experiments are based on a Web-graph containing over 1446 million pages with almost 6650 million links, from a crawl of the Web conducted by AltaVista in September 2003.
The graph, which is stored and accessed using AltaVista X  X  Connectivity Server (Bharat et al., 1998), was built on an Alpha server with 4 CPUs (each running at 667 MHz with its own internal floating point processor) and 32 gigabytes of memory. The number of unique hosts in this graph is about 31 million (just over 2% of the number of pages), with 241 million host-to-host edges (3.6% of the number of page-to-page links).

Computing PageRank in this setting required 12.5 h., while computing the U-model scores took 5.8 h. (a speedup factor of about 2.1). It should be noted that the PageRank computations use the robust and optimized infrastructure of the connectivity server, while our modified algorithm was written in an ad-hoc, non-optimized manner. We predict that by optimizing our implementation in the spirit of the connectivity server, speedup factors can the corresponding figure in the host-graph. In particular, it follows from the discussion in
Section 3 that optimized implementations can achieve considerable speedup factors relative to BlockRank (Kamvar et al., 2003b), which itself empirically speeds up PageRank by factors of 2 X 3.

In what follows we provide statistical comparisons between the U-model flavor and PageR-ank. We show that U-model approximates PageRank very closely, especially if one considers the correlation between the ranks induced by these measures.

To assess the relation between PageRank and its approximation, the U-model, we sampled from the available 1446 million pages. However, we did not take a simple random sample since, by virtue of the power-law distribution of PageRank, the bulk of the population are pages with few or no inlinks. Such pages would attain similar, low scores by practically any link-based static score measure. Furthermore, as a consequence of their low scores, these pages will rarely appear as the top ranking results for queries, and so small fluctuations in their static scores will hardly be noticed by search engine users.

Therefore, instead of sampling uniformly, we used PageRank to stratify the population and uniformly sampled from each decile. This produced a sample containing many more representative pages with larger values of PageRank than would have been possible with a similar sized simple random sample. In particular, we sorted pages according to PageRank and then uniformly sampled within each decile according to the schedule in Table 1. The resulting sample contained 1298 pages distributed over the full range of PageRank values. For each page in the sample we had available both the PageRank and U-model values.
In this sample U-model has Pearson correlation 0.81 with PageRank. Given the amply fidence the hypothesis that U-model is statistically independent of PageRank. This agrees with expectation since U-model is designed to approximate PageRank.

To examine the relation more closely, we fit a linear model via least-squares with U-model as the single predictor and PageRank as the response. If U-model is indeed a good approximation to PageRank we expect the linear fit to be very good, with slope close to 1.0, is close to 1.0 as hoped. However several data point are far from the regression line.
See Fig. 2 for a closer look at the discrepancy between PageRank and its U-model approx-imation. Most of standardized residuals from the fit are very small. Indeed, there are more residuals close to zero than expected if the residuals are compared to a standard Gaussian distribution. However, the outlying points (with large residuals) tend to correspond to pages with large PageRank values, suggesting that U-Model could be a less precise approximation of PageRank in the very high end of the scale.

The best assessment of the U-model approximation would be task oriented; does U-model offer as good information as PageRank for relevance ranking of Web pages? However, this requires considerable machinery outside the scope of this paper. A reasonable surrogate is to ask if the rank order of pages given by U-model resembles the order given by PageRank. In particular, if the orderings were identical the two measures would offer the same information for relevance ranking even if their values deviated considerably. In fact, the Spearman rank-order correlation (Snedecor and Cochran, 1989) between U-model and PageRank is 0.95, considerably higher than the Pearson correlation of 0.81. This suggests that U-model can be used as an effective approximation for PageRank in relevance ranking.

In terms of use for ranking of Web search results, we do not know whether the (small) differences between our model and the original PageRank are for better or for worse. One possible advantage of our model is that it is almost insensitive to changes in the internal organization of hosts. A reorganization that modifies the linkage patterns between the pages on a given host might change the relative ranking of these pages, but will have much less all its pages.

Papers citing improvements abound, as well as papers showing very little or no gain from benchmark for Web IR:  X 
A corpus that constitutes a large and representative sample of the Web (including  X  X pti-mized X  pages, spam, and the likes).  X 
A large set of queries that is representative of user needs.  X 
A clear methodology for assessing quality of search results over the scale of data and queries.  X 
A clear baseline with a state-of-the-art text scoring component (including anchor-text), over which retrieval improvements will be sought.

Furthermore, even though it is unlikely that commercial search companies would publish defining the benchmark. In the meantime, we hope that even researchers who are skeptical of
PageRank X  X  contribution to Web IR would appreciate the mathemathical ideas and random walk approximation framework presented in this paper. 5. Conclusions of pages on the Web. The framework approximates the behavior of a given random walk on the Web X  X  graph in a scalable manner, by performing most of its calculations on a more compact representation of the graph X  X  representation that follows from aggregating multiple pages onto a single node. In particular, this compaction significantly reduces the memory requirements of the computations that arise when dealing with Web graphs of large crawls. As defined in Section 3, our method changes the semantics of random browsing on the
Web by decoupling intra-host and inter-host steps. One should note, however, that the method can be defined in terms of any equivalence relation that partitions the set of Web pages. An immediate example is to relax the physical host-based partitioning to one driven by logi-cal sites: sometimes, pages on separate hosts might correspond to the same logical entity (e.g., different academic units of some university); in other cases, pages on the same host might belong to independent entities X  X .g., sites virtually hosted by services such as Geoci-ties (http://www.geocities.com) .

We approximated PageRank with a random walk flavor in which departures from a page consist of a two stage process: first, a random transition to another page of the same host, and then a PageRank-like step from that page. Whereas PageRank requires repeated scans of the Web-graph X  X  links, most of the computations required by our transformation involve scanning the edges of the Web X  X  host-graph. We achieved a speedup factor of 2.1, and predict that more careful implementations can achieve speedups that are closer to the ratio of the sizes of the graph. Furthermore, we showed that the resulting scores indeed approximate
PageRank: we were able to produce a ranking that has Spearman rank-order correlation of 0.95 with respect to PageRank.

It should be noted that the performance gains achieved by this approach can be further improved by many of the schemes surveyed in Section 2.1.3. For example, the ideas presented problem remains computing the principal eigenvector of a large and sparse stochastic matrix.
Future efforts should be devoted to experimenting with different aggregates of the Web X  X  should be evaluated in terms of the speedup factors it achieves, and in terms of the search quality that it induces when used in the ranking core of search engines.
 References
