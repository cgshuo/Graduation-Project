 Spoken Web is a web of VoiceSites that can be accessed by a phone. The content in a VoiceSite is audio. Therefore Spoken Web provides an alternate to the World Wide Web (WWW) in developing regions where low Internet penetra-tion and low literacy are barriers to accessing the conven-tional WWW. Searching of audio content in Spoken Web through an audio query-result interface presents two key challenges: indexing of audio content is not accurate, and the presentation of results in audio is sequential, and ther e-fore cumbersome. In this paper, we apply the concepts of faceted search and browsing to the Spoken Web search prob-lem. We use the concepts of facets to index the meta-data associated with the audio content. We provide a mechanism to rank the facets based on the search results. We develop an interactive query interface that enables easy browsing o f search results through the top ranked facets. To our knowl-edge, this is the first system to use the concepts of facets in audio search, and the first solution that provides an audio search for the rural population.

We present quantitative results to illustrate the accuracy and effectiveness of the faceted search and qualitative resu lts to highlight the usability of the interactive browsing syst em. The experiments have been conducted on more than 4000 audio documents collected from a live Spoken Web VoiceSite and evaluations were carried out with 40 farmers who are the target users of the VoiceSite.
 H.3.3 [ Information Search and Retrieval ]: Information Filtering; H.5.1 [ Multimedia Information Systems ]: Au-dio input/output; H.5.4 [ Hypertext/Hypermedia ]: Nav-igation Algorithms, Design, Experimentation, Human Factors creating and accessing information through the Spoken Web platform. The content ranges from information about peo-ple and events in a village [4] to information about the crop prices in the market [5], to business information for the un-organized workforce in developing regions [20].

Once the Spoken Web enables large amount of content cre-ation and access, the problems of information management need to be solved to ensure users are able to access the in-formation they need. In the WWW history, the initial years of information management was performed by the directo-ries that would create a catalog of web pages [11]. When the number of web pages kept on increasing, the catalog was not a scalable solution for content management. The search engines then [9] changed the Internet information ac -cess paradigm, reducing the information access problem to a simple query interface. The success of the WWW owes a lot to the search engines.

However, effectively searching through VoiceSites on a voice user interface for navigation and browsing poses sig-nificant challenges for Spoken Web. Searching and browsing problems are exacerbated by the audio modality of the con-tent, the requirement to have a speech user interface and the need to deal with content in resource constrained languages with little to no speech recognition systems. Moreover, the query-result interface needs to be accessible over phone an d needs to be in the audio mode. But audio is a sequential interface and so presentation of the search results is a non-trivial translation from the visual Web world.

In this paper we address two key problems: indexing and searching of the audio content; and solutions for the query-formulation and results-presentation in the query-search in-terface. We propose the use of metadata for indexing the audio content and the use of a faceted browser to narrow the search results.
Information in a visual interface can be presented in a par-allel manner. A standard visual search interface is shown in Figure 1. The query is typed in the box and results are displayed on a web page. A simple transformation of this interface to the audio domain will involve the system asking the user to speak a query, and would then play out the search results sequentially. Even if the system returns 10 search r e-sults, each with 5 second of audio, it will take about one minute for the user to listen to all the search results. In a spoken query interface, it is not possible to parse open-end ed user queries due to the low accuracy of speech recognition systems. Secondly, the results cannot be presented in a se-quential manner since this will take a large amount of time for a user to provide inputs about the relevance of search results. The key, therefore, is to build a spoken query inter -face that can enable efficient navigation of large number of search results.

These issues suggest that we need an alternate query in-terface for the voice domain. Faceted search exposes the search results through a small number of facets. A user can choose a particular facet and the results in that facet are then presented. The disadvantage is that the user has to go through another round of interaction, but the advantage is that the user can quickly narrow down to very few search results. Since few search results is the key in audio search in-terface, this is the motivation for us to explore faceted sea rch for the Spoken Web. accuracies are not in the scope of this paper. We also as-sume that the system has access to the search data and no crawling is required.
The work presented in this paper is based on the premise that there is a need to provide a system to search the user-generated audio content, and the interface needs to be a spoken query system accessible through a phone. Given this on-the-field problem, and the technical challenges mention ed above, this paper presents a solution that provides a facete d browsing interface over metadata indexed system. Followin g are the key contributions of the work presented in this paper :
We first present the background of the Spoken Web to ex-plain the content and the interface constraints (Section 2) . Then we describe the metadata indexing of the audio con-tent (Section 3). The faceted browsing interface on audio is then presented, which will also explain the importance of ranking the facets based on the retrieved results (Section 4 ). The experimental setup to evaluate the proposed technique (Section 5) and the results are then presented (Section 6) to substantiate the usefulness of the faceted search techniqu e.
In this section, we briefly introduce the Spoken Web and refer to the papers for a detailed description of the tech-nology. It is important to explain the Spoken Web to (a) provide an understanding of the data that is used for search in the next few sections, and (b) explain the broader appli-cability of the search technique proposed in this paper.
VoiceSites are voice-driven applications that are hosted i n the telecom network [21]. They are addressed by a unique phone number and can be accessed from any phone instru-ment, mobile or land-line through an ordinary phone call to that number. The phone does not require any extra fea-tures or software to be installed on the device. VoiceSites are therefore analogous to websites in the WWW but can be accessed by dialing a phone number and information can be heard rather than being read or seen.
 Creation of content on a Visited is made easy by the VoiGen system [21] to which anyone can call and interact with it through voice. This can enable any illiterate person to create her VoiceSite. Such a system enables easy local audio-content creation. A sample interaction for creating content on the VoiceSite is shown in Figure 2. The text in
In this section, we will provide details about the data that was used for building the metadata index and the process of indexing.
The data for testing the proposed search technique has been obtained from a live system in the western state of In-dia, Gujarat [29]. The VoiceSite acts as a knowledge sharing platform for farmers, where they can post farming related questions, which can be answered by other farmers, or by specific experts. Other content of the VoiceSite are the an-nouncements that are posted by the administrator. The sys-tem is used by over 500 registered farmers, who are spread over 28 villages in the state. The data has been collected over a period of 14 months.

A total of 4436 audio documents were used for the ex-periment. Each document is 5 seconds to 30 second in length. The metadata associated with a document is ex-plained through Table 1.

The above 6 type of features constitute the 6 facets. We derive 21604 facet-value pair from the 4136 audio docu-ments. Since one audio document can be attributed to more than one facet, we have a large number of facet-value pairs. The name of the content creator and the village are related to the phone number from where the person calls. Use of caller identification in the Spoken Web enables us to get the name and village information. The structure of the Voice-Site provides information about the type of the content. The time and month of posting of the content is easily extracted by storing the time-stamp when the content was created. We perform audio analysis of the content to access the qual-ity of the data. We categorize each audio document as noisy, silence, speech or music. This information can be useful for an administrator who may choose to listen to all noisy data in order to remove it from the system.
The metadata is able to capture the most important arti-facts of the audio content, the 4 Ws  X  Where, What, Who and When. The metadata is then used to create an XML document for each audio document. A sample XML docu-ment for a particular audio file is shown in Figure 4.
Each category of this metadata is then used with addi-tional optional flag that determined how the metadata will be indexed. A flag of searchable can support a query that will return documents that include this metadata. Such an indexing mechanism is important for faceted search, where a facet is used to reduce the search space to a particular cate-gory. If the flag is set to stored , the index stores the metadata and the category as a standard stored content and cannot be used to reduce the search space using a faceted mechanism, using this metadata. As shown in Figure 5, all the audio documents in the different VoiceSites are extracted and the metadata associated with each audio document is created. The metadata is then stored in an XML format shown in Figure 4, which contains the metadata and the correspond-ing category. The indexer then access the data from these XML metadata files and creates a faceted index. The crawler is out of scope of this paper and can be considered to be a manual process for the VoiceSites in consideration. When a user defines a query through the voice interface, the faceted index is used to extract the facet-value pairs and presented in an interactive manner to the user to narrow the search results. The latter part of efficiently narrowing the search results through the different facets in the query interface w ill be explained in the next section.
Having created a faceted index of the metadata described in the previous section, we will focus on the spoken query interface that can exploit the information available in the facets for easier browsing of the search results. We will firs t describe how the facets are ranked, given a query. Then we will describe the call flow for the spoken query interface.
Unlike GUI, where several facets can be displayed on a screen, spoken query interfaces are constrained by the amou nt of information that we can play to the user. The real es-tate of an audio information system is very low. We cannot present a large number of facets to a user to choose from. This would involve presenting a long list of facets to the user, which is not a desirable interface in the spoken domain . Therefore we rank each facet based on the information gain that can be achieved on exploration of a facet. Only the top few facets are then presented to the user.

Given a search result set R = { r 1 , r 2 , ..., r N } of N docu-that maximizes the entropy of the distribution of R in the different values that the facet can be assigned. In order to calculate the entropy of a facet, we first calculate the count of the facet values in the search result.
 where, Therefore, We define the entropy of a Facet F i by
In this section, we describe the setting in which the faceted search and browsing system was evaluated. We used the ex-ploratory formative tests [31] to examine the effectiveness of the preliminary design of our system. In the exploratory study, the users are provided with a high level task and then they are free to use the system. An exploratory study usu-ally answers the questions related to usefulness of a featur e and ease of use of the navigation [28].
An ideal search interface is one where the user speaks the query and the system returns the audio file that had the query term. However due to various technology limitations mentioned in this paper, this is not possible. So we con-ducted a user study with the aim to understand the effec-tiveness of the faceted search interface for users who would actually be requiring the content. The hypotheses to be tested in this study are (a) if the interactive faceted voice query interface is acceptable to users who have never inter-acted with a search interface, and (b) if the ranked search results have a perceived value to the users.
The experiment was performed on a VoiceSite that has user generated content for more than 14 months. The con-tent of the VoiceSite are the questions by farmers, answers posted by farmers and some experts, and important an-nouncements posted by the administrator. An average ques-tion or answer was 21 seconds, with a standard deviation of 6.3 seconds. The aim of the experiment was to provide a search interface to the users so that they can get to the con-tent that they wanted, with an easy to use interface, and in less time.
VoiceSites are authored in VoiceXML [15] which is a tag language similar to that of HTML. The audio content that is referred in these VoiceSites are stored separately in the database. We implemented the faceted indexing on the Lucene [26] search engine library. We used the Lucene ex-tension libraries to add metadata to the otherwise textual support provided by Lucene. The spoken query interface has been implemented in VoiceXML-jsps. The entire audio content and the query interface were in the Gujarati lan-guage, which is the native language of the farmers who were using the VoiceSite.
We selected 40 participants (all male farmers), all of whom had used the VoiceSite more than 100 times earlier. The reason to choose the participants with prior experience on the VoiceSite is that they would be familiar with the content being searched. On an average they have gone through 10 years of formal education, but none was computer literate. Each had their own mobile phone.
The faceted search option was introduced to all the par-ticipants by providing an instructional audio prompt on the VoiceSite. Prior to the introduction of faceted search, use rs would listen to all the content in a sequential manner. The system would play the content in a chronological manner, Figure 8: The number of times the different facets were called by the participants. 200 search results were returned by the system for the initial query. On an average, this was reduced to 24 after selection of the first facet-value by the user. The average number of search results was reduced to 9 at the selection of second facet-value. Typically one interaction of choosi ng the facet and the value would take about 10 seconds. So the faceted search system was able to reduce the number of results to a manageable length in a 20 seconds. It is to be noted that a user would have listened only to roughly 2 search results in this 20 seconds incase all the results woul d have been presented to the user in the first instance itself.
We were able to contact 32 users for the qualitative feed-back. The users were asked to respond to the questions on a 5-point likert scale [25]. Table 2 presents the results of the qualitative survey. A score of 1 indicates strong dis-agreement and a score of 5 indicates strong agreement. The results clearly shows that the faceted search interface was easy to use by the participants. People expressed some dis-satisfaction over the types of facets that were used in the experiment, however most were satisfied with the faceted search interface.

Due to the limited amount of metadata associated with the audio content, we could not provide sufficient choices to the users in terms of choosing a facet. However, ethno-graphic study reveal that people in rural areas associate most activities and content in a diachronic fashion. They identify content based on a historic event or based on a cal-endar. Figure 9 shows a typical calendar of a village house-hold, where they mark most items in the calendar. It is for this reason that a large number of callers had used the month facet. After talking to a couple of farmers, we real-ized that it is not expected that a farmer would remember a guage independent indexing to augment content search with the metadata search presented in this paper.

Most systems for audio search use a visual query interface for accessing content indexed from audio. This is typical of a web based search system for searching video content [6], and in call centers where supervisors wish to monitor offline content of the call center agents [27]. In such situations, the query interface is still visual and hence simple. What differentiates our problem is that the query interface and th e content, both are in audio.
Faceted Search has become the de-facto standard for most e-commerce and product-related web sites [24] [35]. It pro-vides an extremely simplistic interface to reduce the searc h results. For this reason, faceted search is also being used in the area of image search [36] and for searching on mo-bile devices that have limited layout to display all search results [19]. Our work in this paper has been significantly influenced by the work on faceted search in the image and mobile world. As far as we could find, this is the first paper to extend the concepts of faceted search and browsing to the audio domain.
With the increasing ease of tagging content, and the avail-ability of a large number of sensors, the amount of metadata associated with the content has been on the rise in the last decade. Swoogle uses metadata to define relationships be-tween documents in the Semantic Web [17]. Metadata has been the backbone for information retrieval in Digital Li-braries [7] [18]. Since it is difficult to process and extract semantics from images, metadata has been the key for image search systems [36]. Not surprisingly, the metadata based content management systems for mobiles have started to ap-pear in the research community. The authors in [32] have used metadata to manage the images on a mobile. Reusabil-ity of mobile multimedia objects is addressed through meta-data in [34]. Surprisingly not much work has been done on improving the audio search using metadata. This is the other differentiating space where our paper borrows ideas from im-age and multimedia metadata research and applies them to the audio domain.
In this paper we have presented a faceted search and browsing for audio content on the Spoken Web. The inter-face is well suited for the content that is created by low lit-erate users in languages that do not have good speech recog-nition systems. The spoken query interface ensures that a user is able to narrow the search results quickly, which is a key in a spoken interface where the search results are pre-sented sequentially. Facet ranking based on the informatio n gain of the search results in the facet values provides anoth er mechanism to focus the search results on the most promis-ing facets. The experiment with users on the field, with real life data shows promising results for the faceted searc h and browsing approach presented in this paper. Based on our literature search, this is the first time faceted search h as been tried on the audio content. Moreover, this is the first system for audio content search, focusing on content create d by the rural population.
 [11] A. Callerya and D. Tracy-Proulxa. Yahoo! cataloging [12] M. Cha, H. Kwak, P. Rodriguez, Yong-Yeol, and A. S. [13] C. Chelba and A. Acero. Position specific posterior [14] T. K. Chia, K. C. Sim, H. Li, and H. T. Ng. A [15] W. Consortium. Voice Extensible Markup Language [16] J. S. da Silva. Future internet research: The EU [17] L. Ding, T. Finin, A. Joshi, R. Pan, R. S. Cost, [18] B. Hughes and A. Kamat. A metadata search engine [19] A. K. Karlson, G. G. Robertson, D. C. Robbins, M. P. [20] A. Kumar, N. Rajput, S. Agarwal, D. Chakraborty, [21] A. Kumar, N. Rajput, D. Chakraborty, S. Agarwal, [22] A. Kumar, N. Rajput, D. Chakraborty, S. Agarwal, [23] J. Ledlie, B. Odero, E. Minkov, I. Kiss, and
