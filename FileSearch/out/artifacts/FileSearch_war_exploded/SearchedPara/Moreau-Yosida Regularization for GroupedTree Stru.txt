 Many machine learning algorithms can be formulated as a penalized optimization problem: group Lasso assumes that the groups are non-overlapping. However, in many applications the fea-tures may form more complex overlapping groups. Zhao et al. [23] extended the group Lasso to an active set algorithm.
 much more difficult to solve than Lasso and group Lasso, due to the complex regularization. was under review, we became aware of a recent work by Jenatton et al. [8] which applied block coordinate ascent in the dual and showed that the algorithm converges in one pass. We begin with the definition of the so-called index tree: Definition 1. For an index tree T of depth d , we let T i = { G i 1 , G i 2 , . . . , G i n do not overlap; and 3) the index set of a child node is a subset of that of its parent node. The grouped tree structure regularization is defined as: k X k is the Euclidean norm, and x G i specify the meaningful interval for the regularization parameter  X  . a given v  X  R p is given by:  X  (  X  ) is a non-expansive operator. More properties on the general Moreau-Yosida regularization 3.1 An Analytical Solution procedure for finding the minimizer in Algorithm 1.
 Algorithm 1 Moreau-Yosida Regularization of the tree structured group Lasso (MY tgLasso ) 1: Set 2: for i = d to 0 do 3: for j = 1 to n i do 4: Compute 5: end for 6: end for date u . At the traversed node G i j , we update u G i Euclidean norm of u G i By using Definition 1, we have O ( p log p ) .
 tree given in Figure 2, with the solution denoted as x  X  . We let w i j = 1 ,  X  i, j ,  X  = traversing the root node, we obtain x  X  = [0 , 0 , 0 , 0 , 1 , 1 , 0 , 0] T . the following theorem: Theorem 1. u 0 returned by Algorithm 1 is the unique solution to (3) .
 Before giving the detailed proof for Theorem 1, we introduce some notations, and present several technical lemmas.
 Define the mapping  X  i j : R p  X  R as We can then express  X  ( x ) defined in (2) as: The subdifferential of f (  X  ) defined in (3) at the point x can be written as: where In addition, based on the structure of the index tree, we have (9) and (10).  X  Lemma 2. For any i = 1 , 2 , . . . , d, j = 1 , 2 , . . . , n i , we have Proof : We can verify (11) using (5), (6) and (8).
 For (12), it follows from (6) and (8) that, it is sufficient to verify that (9). Otherwise, if u By a similar argument, we have Together with (9), we have From (14) and (16), we show (13) holds with  X  i j =  X  i l =1  X  l . This completes the proof.  X  We are now ready to prove our main result: unique minimizer. Our methodology for the proof is to show that which is the sufficient and necessary condition for u 0 to be the minimizer of f (  X  ) . it suffices to show 0  X   X  X  ( u 0 ) G i following relationship: other words,  X  x , we have by using (6) and (8).
 Applying (11) and (12) of Lemma 2 to each node on the aformetioned path, we have Making using of (9), we obtain from (20) the following relationship: Adding (21) for l = 0 , 1 , . . . , d , we have It follows from (4), (7), (19) and (22) that (18) holds.
 Similarly, we have 0  X  f ( u 0 ) G i 3.2 The Proposed Optimization Algorithm the interval for the values of  X  .
 Lemma 3. Let x  X  be an optimal solution to (1) . Then, x  X  satisfies: Proof : x  X  is an optimal solution to (1), if and only if which leads to of (3). We have (23).  X  3.3 The Effective Interval for the Values of  X  entries of x are penalized in  X  ( x ) . Next, we summarize the main results of this subsection. have 0  X  l 0 ( 0 ) +  X  X  X  ( 0 ) , which indicates that x  X  = 0 is the solution to (1). R,  X  x  X   X  X  ( 0 ) , where R is a finite radius constant. Let Define I = S initial guess of the solution. Our empirical study shows that  X  0 = apply bisection for computing  X  max . Algorithm 2 Finding  X  max via Bisection Output:  X  max 1: Set  X  =  X  0 2: if  X   X  (  X  l 0 ( 0 )) = 0 then 4: else 6: end if 7: while  X  2  X   X  1  X   X  do 9: if  X   X  (  X  l 0 ( 0 )) = 0 then 10: Set  X  2 =  X  11: else 12: Set  X  1 =  X  13: end if 14: end while set the regularization parameter  X  = r  X   X  max , where  X  max is computed using Algorithm 2, and Efficiency of the Proposed tgLasso We compare our proposed tgLasso with the recently proposed in each iteration is the matrix inversion, which does not scale well to high-dimensional data. Classification Performance We compare the classification performance of tgLasso with Lasso. On AR, we use 50 subjects for training, and the remaining 50 subjects for testing; and on JAFFE, we and Lasso under the best regularization parameter. We can observe from the figure that tgLasso technical result show the Moreau-Yosida regularization associated with the tree structured group the parameter  X  . Our experimental results on the AR and JAFFE face data sets demonstrate the other applications in computer vision and bioinformatics involving the tree structure. Acknowledgments This work was supported by NSF IIS-0612069, IIS-0812551, CCF-0811790, IIS-0953662, NGA HM1582-08-1-0016, NSFC 60905035, 61035003, and the Office of the Director of National Intelli-gence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), through the US Army.
