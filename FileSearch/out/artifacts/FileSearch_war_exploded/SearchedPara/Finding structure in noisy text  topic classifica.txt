 ORIGINAL PAPER Prem Natarajan  X  Rohit Prasad  X  Krishna Subramanian  X  Shirin Saleem  X  Fred Choi  X  Rich Schwartz Abstract This paper addresses two types of classification of noisy, unstructured text such as newsgroup messages: (1) spotting messages containing topics of interest, and (2) automatic conceptual organization of messages without prior knowledge of topics of interest. In addition to applying our hidden Markov model methodology to spotting topics of interest in newsgroup messages, we present a robust methodology for rejecting messages which are off-topic .We describe a novel approach for automatically organizing a large, unstructured collection of messages. The approach applies an unsupervised topic clustering procedure to gene-rate a hierarchical tree of topics.
 Keywords Topic classification  X  Unsupervised topic discovery  X  Clustering  X  Hidden markov model Abbreviations HMM Hidden Markov Model TFIDF Term frequency inverse document frequency SVM Support vector machines UTD Unsupervised topic discovery ROC Receiver operating characteristics 1 Introduction The  X  X nformation tsunami X  is a new phrase that has been used in recent times to describe the immense volumes of data, including unstructured text, that are generated on an ongoing basis. The sheer volume of such data makes it intrac-table for humans to analyze without some kind of automatic analysts X  assistant . While analyzing large collections of text data represents one end of the spectrum, at the other end, even from the perspective of a single user, the number of messages or documents that are received and sent by the user makes manual organization cumbersome and burdensome. At the same time, automatic organization of documents and data can only be useful if such organization exhibits characteris-tics that, to some extent, follow human intuition.

Humans categorize documents based on the topic or the primary theme of the document. But it is important to note that the  X  X rimary X  theme is typically a function of the imme-diate context within which the user is operating. In addition, different users may associate different  X  X rimary X  or domi-nant themes with the same document. So any organization that is based on the dominant theme of a document is also a deficient approach. A more robust technique is to automati-cally organize a set of documents based on a collection, or set, of topics contained in each document.

Topic based document classification has been applied to various domains including broadcast news articles [ 1 , 2 ] and text messages posted on newsgroups [ 3  X  5 ]. In these approaches the choice of classifiers has ranged from Na X ve X  Bayes [ 4 ] classifiers to Support Vector Machines (SVM) [ 2 , 3 ]. Most of these efforts have focused on text catego-rization with only tens of topics. In [ 1 ], we introduced a hidden Markov model (HMM) based framework for topic classification. We demonstrated that our HMM based engine, OnTopic TM outperformed Na X ve X  X ayes classifiers and term frequency-inverse document frequency (TFIDF) classifiers on a large collection of broadcast news articles consisting of thousands of topic labels. Unlike most approaches [ 2  X  7 ], which assume that a text document is related to a single topic only, in [ 1 ] we postulated that a document is almost always on a set of topics rather than a single topic. Also, every word in a document is not required to be associated with a topic, that is, some words are allowed to be relevant to the general language only.

In this paper, we extend our HMM methodology for topic classification [ 1 ] to two distinct operational scenarios. In the first scenario, the topics of interest to the user are known a priori, such as in a topic spotting or an alerting applica-tion. In [ 1 ] we focused only on closed-set , supervised clas-sification with a pre-defined set of topics, whereas in this work we address the problem of rejecting documents that are not related to the topics of interest, i.e. are  X  X ff-topic X . We present a robust methodology for rejecting such mes-sages that, in addition to modeling the topics of interest, uses a so-called alternate model for topics that are not included in the set of topics of interest. Specifically, we introduce two techniques for estimating topic-specific rejection thre-sholds [ 8 ] X  X  parametric technique that can be viewed as transformation of topic-independent thresholds, and a non-parametric technique based on constrained optimization of false rejections subject to a pre-specified number of false acceptances.

In the second operation scenario, a large corpus of unstruc-tured text documents is provided, and the system is required to organize the documents in a structure that enables effective human analysis. Our algorithm referred to as unsupervised topic clustering (UTC) consists of two steps: (1) automati-cally discovering topics using our unsupervised topic dis-covery (UTD) [ 9 ] system, and (2) clustering the discovered topics in a hierarchical tree structure. The end result of the proposed algorithm is a tree, where each node is a topic clus-ter consisting of one or more topics and points to a set of documents containing at least one of the topics in the topic cluster represented by that node. This approach results in a multi-resolution organization of documents, thereby allo-wing users to analyze the corpus in different ways.
In the context of message classification, noise can mani-fests itself as errors (typos, etc.), novel unknown (during trai-ning) words, or as extraneous text that is irrelevant to the task at hand. Our proposed methodology mitigates the impact of such noise in multiple ways. First, we use probabilistic models that are able to absorb errors or unknown words within a General Language state. Second, common typogra-phical errors in words related to a topic are directly and auto-matically incorporated into the relevant model during the training phase. Third, information-bearing words are typi-cally redundant which makes the multinomial probability computation within our classification algorithm robust to errors and typos. Finally, the General Language state pro-vides a robust generative model for extraneous text that is not related, in a discriminative sense, to any of the topics in the model set. Although it is not discussed in this paper, automatic model adaptation provides a mechanism for our system to incrementally include new words (associated with a topic or otherwise) that were not seen during training.
Given our interest on categorization of unstructured and noisy text, we performed most of our experiments on newsgroup messages posted on the Internet. The presence of informal language, broad subject matter, typographical errors (abbreviated as typos ), abbreviations, etc., makes newsgroups a readily available, rich source of noisy text data. 2 Definition of topics By  X  X opics X , we mean a concept or a theme that can be used to categorize a document. Human annotators often represent a topic by a single term (a phrase or a word) like  X  X nha-lation Anthrax X  or named entities such as  X  X resident Bill Clinton X . Often, the topic label does not appear, in its exact form, inside a document. For example, an article discussing the threat posed by Anthrax may not consider the term  X  X nha-lation Anthrax X  in the exact form. Specifically, our definition for topics has the following components: 1. A descriptive and human comprehensible label.  X  X irds, X  2. A set of terms, known as support words, that supports the 3. A statistical measure of significance of the support term 3 Hidden Markov model based topic classification Our topic classification system, OnTopic TM [ 1 ] assumes that documents are associated with several topics. The model topology is shown in Fig. 1 . Each topic is represented by a 1-state HMM. In addition, there is an HMM for the Gene-ral Language. Each of the topic-specific states has a proba-bility distribution for the words in the language about that topic. In the simplest case, this language model is a unigram distribution P ( W n | T j ) on words. However, the state could also contain a higher order n -gram language model for word sequences for that topic.

According to the model shown in Fig. 1 , when an author decides to write an article, the first task is to select a set of topics he wants to write about. The set of topics, denoted as the  X  X et X  are chosen according to the prior distribution P (
Set ) for topics. The model assumes that the author writes the article one word at a time. Before choosing each word, he first chooses which of the topics that word will be about. This is chosen according to the probability of each topic, given the set of topics in the article, i.e. P ( T j | Set ) . Once the topic is chosen, the author chooses a word from the corresponding topic state according to the distribution of words for the topic. The author chooses the topic for the next word, and so on until the article is completed.

The parameters of the OnTopic model shown in Fig. 1 are estimated using the expectation maximization (EM) algo-rithm from a corpus of documents labeled with associated topics.

Classification of a test document D is performed in two stages. First we consider each topic independently using Eq. ( 1 ) to choose a small set of likely topics, where P is the posterior probability of topic T j given the document D , P ( T the average percentage of words generated by topic state T given it is in the set of topics, and  X  is an exponential weight to counteract for the independence assumption. Note that the function  X  x ) is equal to x when x is positive and 0 when x is negative. log P ( T j | D ) = log P ( T j ) Then we rescore all subsets of the top-N topics using (2): P (
Set | D ) = P ( Set ) In all our experiments reported in this paper we present results from only the first stage of classification, i.e. we have reduced the problem of finding the optimal set of topics for a docu-ment to finding the top-N topic labels for the document. 4 Supervised classification of noisy text In [ 1 ], we had performed closed-set, supervised topic clas-sification experiments on broadcast news articles using the OnTopic engine. In other words, our test set consisted only of documents related to the topics of interest (or the on-topic documents), and the training data used for estimating the topic models was manually labeled.

The training and the test set for experiments reported in [ 1 ] consisted of news articles from the Primary Source Media corpus (PSM). This corpus consists of 45,000 articles from July 1995 through July 1996. Each article was annotated with up to 13 topic labels. In total there are 5,239 unique topic labels with an average of 4.5 topic labels per article. All articles from July 1996 were selected as a test set and the rest of the data was used for training the OnTopic models.
We measured classification performance using the  X  X op-1 accuracy X  metric. The top-1 accuracy is defined as the per-centage of times the top-choice topic was the correct answer. On the held-out test set from July 1996 the top-choice accu-racy was 75.7%, which is significantly better than the accu-racy achieved by TFIDF and Na X ve X  X ayes classifiers [ 1 ].
In this section, we apply OnTopic to supervised classifica-tion of noisy, informal text of newsgroup messages. We report results on two different corpora of newsgroup messages. In both sets of experiments, instead of manually annotating messages with topic labels, all the messages in a newsgroup are automatically annotated with the name of that newsgroup. In this annotation scheme, each message is assumed (an inac-curate assumption) to be on a single topic. Although cost effective, the assumption that the name of the newsgroup is the only valid topic for the message often leads to inaccura-cies in estimating system performance because all non-trivial messages consist of multiple topics, some of which may be related to the dominant theme of another. 4.1 Classification accuracy on the AFE corpus The Automated Front End (AFE) newsgroup corpus [ 5 ]was collected by Washington University by harvesting messages from 12 Google newsgroups. There are 11,503 messages in this corpus. Since our intent as in [ 5 ] was to use the newsgroup name as the topic for text message, the message headers were stripped to exclude newsgroup name from training and test.

The 10,768 messages from talk.origins newsgroup were used as off-topic or  X  X haff X  messages (messages not rela-ted to topics of interest) in the experiments performed by Washington University [ 5 ]. We used these messages for the same purpose in our experiments on rejection of off-topic messages described later in this paper. Therefore, there are only 735 on-topic messages available for use as training and test examples from the 11 newsgroups that constitute our topic set of interest.

First, the entire corpus was randomly partitioned into trai-ning, development, and validation sets in the same proportion as described in [ 5 ]. Next, we trained our topic classifica-tion engine with 241 messages available for training from 11 newsgroups in the AFE corpus. Chaff or off-topic mes-sages from the talk.origins newsgroup of the AFE corpus were excluded from training and test, since in these experi-ments we are interested in evaluating the on-topic classifi-cation accuracy. Finally, we classified the 376 held-out test messages from the 11 newsgroups.

In Table 1 , we summarize the top-choice accuracy obtained on test messages for each individual newsgroup as a function of the amount of training messages. As shown, the overall top-choice accuracy was 91.2%. We also note that messages from newsgroups Misc.consumers.frugal_living and Soc.libraries.talk newsgroups seem to be most difficult to classify, primarily due to the lack of training data.
Although we used the same number of messages as in [ 5 ] for training and test, we did not have access to the same set of messages. Therefore, the training and test sets differ in terms of the content. As a result, we are unable make a direct comparison with the work reported in [ 5 ]. 4.2 Classification accuracy on the 20 NG corpus The 20 Newsgroup (20 NG) corpus [ 4 , 10 ] consists of 18,820 messages from 20 newsgroups. These cover six broad sub-ject matters as indicated by the creators of the corpus in [ 10 ]. The average number of messages per newsgroup (i.e., per topic) is much higher for the 20 NG corpus as compared to the AFE newsgroup data. Also, unlike the AFE newsgroup data, the 20 NG corpus has significant overlap in subject mat-ter in messages from different newsgroups X  X hich makes it a more challenging data set for experimentation. In Fig. 2 , we show a few sample messages from the 20 NG corpus. These samples illustrate that there is a significant presence of  X  X oise X  in newsgroup messages, including typos, informal language, abbreviations, etc.

As was the case in dealing with AFE messages, for our classification experiments we stripped message headers, e-mail IDs, and the signatures to exclude newsgroup infor-mation. Next, we split the entire 20 NG corpus into training, development, and validation sets using the following three partitioning methods. In each of the partitioning, 80% of the entire data was kept for training and the remaining 20% was divided equally among development and validation sets. 1. Thread Partitioning : The entire message thread was assi-2. Chronological Partitioning : Each message in a thread 3. Random Partitioning : 80:20 split between training and Chronological partitioning and thread partitioning are more likely to be representative of an actual operational scenario. We performed a random partitioning experiment in order to compare the classification accuracy with prior work reported in [ 3 , 4 ].

We trained topic models for 20 topics (each newsgroup was treated as a topic) on the available training data for each of the partitioning methods. In Table 2 , we list the results obtained for each of the partitioning methods. As one would expect, the classification accuracy is the best for the random partitioning, followed by chronological partitioning, and then thread partitioning.

The top-choice accuracy of 83.2% with random partitio-ning on the 20 NG corpus is similar to the state-of-the-art per-formance reported in [ 4 ]. However, direct comparison with [ 3 , 4 ] was not possible as we did not have access to the same messages (with exactly the same processing for removing headers, signatures etc.) for training and test.

The top-choice precision on the 20 NG corpus is signifi-cantly lower than the 91.2% top-choice accuracy we obtained on the AFE data set. The lower accuracy is another indicator that the 20 NG corpus is more challenging, and better suited for our research. One specific cause for the more challen-ging nature is the significant overlap in the subject matter for different newsgroups in our topic model set. The authors of the 20 NG corpus [ 10 ] suggest a subject-wise partitio-ning of the corpus into six broad subject areas. We did not use the subject-wise partitioning suggested by the creators of the corpus. However, our implicit assumption that each newsgroup can be treated as a completely different topic is not valid because any non-trivial message contains multiple topics. Ideally, every message in the corpus should have been annotated with ALL relevant topic labels, but such annotation for such a large data set is impractical. Therefore, we revie-wed a few training messages from each newsgroup and then manually organized newsgroups that have similar content into a single topic. Our manual organization resulted in 12 different  X  X opics X , which is still conservative compared to the 6 clusters proposed in [ 10 ]. We have recomputed the top-1 accuracy by using the manual organization of the topic clusters. As shown in Table 2 , the classification accuracy increases by 5% absolute across the board. For the chronolo-gical partitioning condition, we further analyzed the classi-fication errors for the 4 topics with the lowest accuracy. Our analysis revealed that for a majority of the messages that were considered to be misclassified by the scoring script, the top-choice topic label assigned to the messages by the OnTopic engine was indeed relevant and appropriate to the message. If these messages are given credit for being correctly classi-fied, then the classification accuracy rises by 3% absolute to 88%. 5 Rejection of off-topic messages In many runtime scenarios, it is highly likely that an overw-helming fraction of the data will be off-topic. Therefore, it is critical for the topic classification engine to have the ability to reject almost all the off-topic messages accurately, while still retaining a large fraction of the on-topic messages.
In this section, we describe a novel rejection mechanism based on the assumption that the General Language (GL) state in the model shown in Fig. 1 can serve as the alternate model, i.e., a composite model for all topics that are not of interest.

Accepting a message means asserting that the message contains the top-choice topic, while rejecting a message means asserting that the message does not contain the top-choice topic. As shown in Eq. ( 3 ), we accept a message if the ratio of the log posterior probability of the top-choice topic to the log posterior probability for the GL topic is grea-ter than a pre-specified threshold,  X , otherwise, we reject that message.
 LR ( T j ) = Note that we used the ratio of the log-posterior instead of the log likelihood ratio because the ratio in Eq. ( 3 ) resulted in lower false alarm rates than the log-likelihood ratio in our pre-liminary experiments. The threshold  X  in Eq. ( 3 ) controls the trade-off between false acceptances (FA) and false rejections (FR).  X  can either be independent of the top-choice topic or be dependent on the top-choice topic. Given that each topic has a different FA/FR characteristics, topic-specific thresholds are likely to outperform topic-independent thresholds.
In the following, we compare different threshold estima-tion techniques. We begin with a brief description of the cor-pus we created to reliably measure low false alarm rates. 5.1 Corpus for rejection experiments In [ 5 ], messages from the talk.origins newsgroup in the AFE corpus were used as  X  X haff X  or off-topic messages. Although, these messages from the talk.origins newsgroup in the AFE newsgroup corpus covers a broad range of topics, a neces-sary characteristic for off-topic messages, their number does not permit reliable measurement of false alarm rates lower than approximately 20 in 10,000. Therefore, we downloaded approximately 250,000 messages from 10 Yahoo! Groups as our out-of-topic test set for comparing different rejection algorithms. Next, we created a new on-topic and off-topic set for development and test using the AFE, 20 NG, and the large chaff collection from Yahoo! Groups. The distribution of the on-topic and off-topic messages across training, deve-lopment, and validation sets is shown in Table 3 . The on-topic messages are from 14 newsgroups from the 20 NG corpus and the off-topic messages are from the AFE and large chaff corpus. Note that we excluded the messages from 6 of the 20 newsgroups from the 20 NG corpus from our experiment because there was significant subject matter overlap between these newsgroups and the large chaff corpus. 5.2 Baseline performance using topic-independent First, we trained our topic classification engine with the mes-sages from 14 newsgroups. The 9.6 K chaff messages were used to train the General Language state of the model. Next, we classified the on-topic as well as off-topic messages from the development and validation set. Then for each message, we accepted or rejected the top-choice topic if the ratio in Eq. ( 3 ) was higher than a topic-independent threshold  X  ded a priori.

In Fig. 3 , we plot %FA versus %FR for different values of  X  (denoted as  X  X opic-ind X  in the figure) on the validation set. As shown in Table 4 , the topic-independent thresholds result in 31.4% false rejections at an operating point of 1% false acceptances. 5.3 Parametric topic-specific thresholds In the parametric approach for estimating topic-specific thre-sholds, for each topic in our model we compute the empirical distribution (mean and variance) of the log-posterior ratio for all the chaff/off-topic messages in the development set that are mislabeled as that particular topic. At runtime, we first normalize the log-posterior ratio for each message according to the following equation: where  X  X core X  is the log-posterior ratio of the message for the top-choice topic, T i , that is being considered as a valid label for the message,  X  off ( T i ) and  X  off ( T i ) are the empirical mean and variance computed from the log-posterior ratios of the off-topic messages for the topic T i in the development set, and score normalized is the normalized log-posterior ratio. For accepting or rejecting a message we now compare the normalized score for the top-choice topic to a threshold  X 
Rather than viewing the calculation in Eq. ( 4 ) as a nor-malization of the score, it can be viewed as a parametric topic-specific transformation of the threshold. The score normalization effectively results in a set of topic-specific thresholds that have been obtained through parametric trans-formations of a single threshold value.

In Fig. 3 , we plot the %FA vs. %FR for the parametric topic-specific thresholds on the validation set (denoted as  X  X aram-topic-dep X  in the figure). As shown in Table 4 ,at 1% false acceptances, there is a 4% absolute reduction in the false rejections over the topic-independent thresholds. 5.4 Non-parametric topic-specific thresholds In this section we briefly describe a novel non-parametric threshold estimation technique that is more completely des-cribed in [ 8 ].

The problem of estimating topic-specific thresholds  X ( T i is a constrained optimization problem, which can be formu-lated as finding thresholds  X ( T i ) that min In Eq. ( 5 ) x i is the number of off-topic (chaff) messages accepted as topic T i for a particular value of  X ( T i ), the number of messages related to topic T i that were falsely rejected (represented as a function of number of false accept contribution from topic T i ), and k is the total number of false accepts.

We used a differentiable nonlinear optimization [ 11 ] algo-rithm on the development set to estimate topic-specific thre-sholds for different values of k . Since, we have a finite number of data points, the function f i ( x i ) is a step function for each topic T i , which is not differentiable. Therefore, we smooth our estimate for f i ( x i ) using a Gaussian smoothing function.
In Fig. 3 , we plot the %FA vs. %FR for the validation set (denoted as  X  X on-param-topic-dep X ) using topic-specific thresholds estimated from the development set. As shown in Fig. 3 , the performance of the non-parametric topic-specific threshold is significantly better than the topic-independent and the parametric topic-specific scheme. Also at 1% false acceptances, there is a 3.7% absolute reduction in the false rejections over the parametric method for topic-specific thre-sholds.

In another experiment, we excluded the off-topic mes-sages from training of the GL state in our model. Next, we estimated the non-parametric thresholds from the develop-ment set as before. In Fig. 3 , we denote the FA/FR curve for this experiment as  X  X on-param-topic-dep-nochaff X . As one would expect, the FA/FR characteristics are significantly worse when off-topic messages were excluded from training of the GL state. Therefore, training the topic classifier on some amount of off-topic data is critical for maintaining low FA.

In Fig. 4 , we compare the Receiver Operator Characteris-tics (ROC) for the different rejection schemes and in Fig. 5 we compare the ROC curve for excluding off-topic messages from training the GL state. 6 Unsupervised topic clustering Supervised topic classification requires annotating docume-nts with topic labels, which for a large number of topics of interest can be a significant cost. Moreover, when topics of interest are not pre-defined, then supervised classification is not a viable solution for document categorization.

Several approaches for automatically extracting thematic or topic information from unstructured data have been pro-posed in the literature [ 3 , 12 , 13 ]. These approaches are based on clustering documents using measures for inter-document similarity. Such simplified flat clustering approaches not only violate the assumption that a document is often on multiple topics, but also lack a multi-resolution structure. The hie-rarchical document clustering work in [ 14 , 15 ] addressed to some extent the need for multiple levels of granularity for interactive navigation of a large corpus of text. However, these approaches are still based on clustering documents.
In this section, we present a significantly different appr-oach for organizing unstructured text documents. The key difference in our approach from prior work is that we pro-pose to cluster topics (not documents) for organizing docu-ments. Our algorithm is based on our Unsupervised Topic Discovery (UTD) [ 9 ] system, which discovers topics from a collection of documents and provides a set of human unders-tandable topic labels for each document. The result of the topic discovery system is a flat list of detailed topic labels, which can be used for document categorization, summariza-tion, and story segmentation [ 16 ]. The flat list of topic labels from the UTD system is still a single level of granularity for topic based navigation of documents. Since no single level of granularity is appropriate for all users or tasks, we cluster the discovered topics into a hierarchical tree structure. We refer to this approach as unsupervised topic clustering (UTC). 6.1 Overview of the Unsupervised Topic Discovery system The goal of the UTD [ 9 ] system is to extract topic labels or themes from a large collection of documents. Specifically, the topic discovery algorithm in the UTD system has the following high-level steps: 1. Find descriptive phrases in each document including 2. Augment each document with names and phrases iden-3. Compute the TFIDF [ 20 ] score for each term (word or 4. Prune a key term  X  K  X  if it occurs in less than P docu-5. Create topic models using the OnTopic training system. 6. Use the OnTopic topic classifier and the topic models 6.2 Agglomerative clustering for unsupervised topic Our approach for extracting topics from a large corpus of documents and organizing them in a hierarchical structure consists of two steps. First we use our UTD system descri-bed in the previous section to discover topics that are shared among multiple documents. We then cluster the topic models to create a hierarchical arrangement of topic clusters (not documents) because no single level of granularity is likely to be appropriate for all users or tasks. The topic hierarchy is not required to be a single tree for each cluster and different low-level topics may fit under several higher-level topics. As a byproduct of the clustering of topics, mnemonic labels are automatically assigned to each node in the hierarchy of topic clusters, thereby giving the user a flavor of the content of each cluster.

For clustering topics, we used the following agglomerative clustering procedure: 1. Each topic is initially assigned to its own individual clus-2. For every pair of clusters, we compute the distance bet-3. The two clusters that are closest to each other are merged 4. Steps 2 and 3 are repeated iteratively until the distance The result of the above clustering is a hierarchical topic tree, where the leaves of the tree are the individual topics disco-vered from the UTD process and intermediate nodes are a collection of topics. Since the UTD process typically assigns multiple topics to a document, a document can belong to mul-tiple clusters if the topics assigned to the document belong to different clusters. Thus, unsupervised topic clustering (UTC) overcomes the problem of single cluster assignment of tra-ditional document clustering. 6.3 Distance metrics for topic similarity Since the performance of any clustering depends on the cho-ice of the similarity measure, we investigated several distance measures for topic similarity. These measures can be broadly categorized into two the following groups. 6.3.1 Topic co-occurrence based metrics These metrics are based on co-occurrence counts of topics in documents labeled by the topic discovery system. 1. Co-occurrence probability : The co-occurrence probabi-2. Mutual information : We used the topic co-occurrence 6.3.2 Support word based metrics These are metrics based on comparison of output observation probability distribution, P ( W | T ) for two topics. We refer to these metrics as the support word based metrics, since the words that  X  X upport X  a topic are the ones that have a non-zero output observation probability for that topic. 1. Support word overlap : This metric measures the overlap 2. J-divergence : The Kullback X  X eibler (KL) [ 22 ]diver-6.4 Controlling the shape of the hierarchical topic tree During each iteration of the clustering algorithm, if we only allow a single pair of topics or clusters to merge, then the output of the agglomerative clustering algorithm will be a binary tree of topic clusters, with the leaves of the tree being the individual topics themselves. Given that the topic tree is not guaranteed, or indeed even likely, to be balanced, this procedure could result in very deep trees.

From a usability perspective, a hierarchical topic tree that is very deep is, ultimately, difficult to navigate because the user will have to click/traverse multiple levels before finding the topics that reside at lower levels. On the other hand, a high branching factor is also undesirable, as the user will be forced to choose from a large number of alternatives at each node in the tree. Therefore for effective navigation of documents, the UTC algorithm must limit the depth of the hierarchical topic tree to a reasonable value, and, at the same time, inhibit excessive branching at each node.

For reducing the number of levels in the topic tree we explored the following modifications in the agglomerative clustering procedure described earlier.  X  Depth penalty : We introduced a  X  X epth penalty X  to modify  X  Clustering multiple (greater than 2) topics : We modi-6.5 Experimental results and evaluation We developed and fine-tuned the UTC algorithm on a set consisting of 10,830 documents from the PSM corpus. The UTD system discovered 2,816 unique topics discovered from this set. First, we visually inspected the output of the agglo-merative clustering with different topic similarity measures. Next, we defined a metric called the clustering rate that see-med to be correlated with our subjective evaluation. The clustering rate measures, at each iteration of the clustering algorithm, the rate of increase of clusters that contain more than one topic. A higher clustering rate generally implies that single-topics are being merged into a cluster whereas a lower rate implies that clusters with multiple topics are being mer-ged. In Fig. 6 , we plot the trend in the number of clusters containing 2 or more topics (as a function of the number of iterations). We show four trends, one for each of the four different distance metrics discussed above. As can be seen from the figure, the mutual information metric creates the most number of clusters with size  X  2 topics at any given iteration, and reaches its peak the latest indicating that its clusters are most uniform , whereas, the J-divergence seems to be the worst in terms of generating uniform clusters.
We have also explored combining topic co-occurrence and support word based metrics. Subjective evaluation indi-cates that combining the J-divergence and mutual informa-tion metrics results in more uniform clusters than using either metric by itself.

Following the development of the UTC methodology on the PSM corpus, we performed experiments on newsgroup messages from the 20 NG corpus. The UTD system discove-red 3,343 unique topics from 19 K messages. Next, we per-formed agglomerative clustering using a combination of the J-divergence and mutual information metric. Figure 7 illus-trates a sub-tree of topic clusters from the 20 NG corpus.
In the following, we summarize some of the key statistics for the topic tree generated after executing 550 iterations of the agglomerative clustering. 1. Number of levels : The depth of the resulting tree was 6, 2. Branching factor : The average branching factor of the 3. Cluster size : The maximum number of topics in any given
Although the above statistics are useful in evaluating the quality of the topic tree, ultimately a subjective evaluation with real users would be required to measure the effectiveness of the UTC tree. 6.6 Graphical user interface for UTC based browsing In this section, we describe a graphical user interface (GUI) we have developed for analyzing a corpus of unstructured text documents using the UTC topic tree. Figure 8 shows a screenshot of the GUI, which we refer to as the  X  X opic Browser X .

Some of the key features in the Topic Browser are:  X  Hierarchical folder structure : The topic clusters are dis- X  Document view : Documents in each folder are displayed  X  Topic display : The topics associated with a selected docu- X  Topic search : The  X  X earch X  function allows substring mat- X  Graphical view for topic sub-tree : The topic browser  X  Document display : A document can be displayed in the We believe that the Topic Browser GUI will help us perform subjective evaluation of the UTC topic tree for the purpose of navigating large, unstructured text corpora. 7 Summary and future work In this paper we successfully applied the HMM methodo-logy for topic classification to classifying noisy, unstructu-red text such as newsgroup messages. To ensure low false alarm rates for topic spotting, we introduced a robust method for rejecting off-topic messages. Our rejection algorithm, in addition to modeling the topics of interest, uses an alternate model for topics that are not included in the pre-defined set of topics. We also compared different approaches for setting the operating point for the above rejection framework. In particular, the topic-specific thresholds outperformed topic-independent thresholds in terms of ROC characteristics. A comparison of the performance of our classification algo-rithm against other competing algorithms in NIST X  X  TREC evaluation is presented in [ 1 , 9 , 16 ].

For automatic categorization and navigation of unstruc-tured text, we introduced an entirely novel concept of topic clustering that builds upon the topic classifier and its output. Unlike conventional clustering approaches, the UTC algo-rithm clusters topics instead of documents. Coupled with our automatic topic discovery system, UTC can generate a hie-rarchical organization of topics (and documents) without any human supervision or annotation. The topic tree generated by the UTC system allows different users to view the text cor-pus at different resolutions and find the desired information in context. We have also implemented a demonstration pro-totype for topic based navigation of unstructured text docu-ments based on the UTC tree. We plan to use this prototype for evaluating the usefulness of UTC with real users. References
