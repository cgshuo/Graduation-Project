 Clustering is a common procedure of stati stical data analysis, which is exten-sively used in machine learning, data mining and pattern recognition([7], [16], [6], [9]). The goal of clustering is to partition a data set into a number of groups with high intra-group relevance and low inter-group relevance. Among all the clustering techniques, spectral clustering is popular and has many fundamental advantages ([11], [1], [23], [3], [22], [12], [4]). Spectral clustering stems from a strong theoretical foundation ([2], [5]) and its performance often outperforms other traditional approaches. Hence, sp ectral clustering h as been successfully applied in many areas including image segmentations ([19],[13], [15], [28], [27], and document clustering ([8]). Among all the previous works, Ng-Jordan-Weiss X  framework (NJW for short) lays a remarkable foundation in emphasizing the importance of expressing the data appropriately [15]. Firstly, NJW utilizes a weighted graph to represent the raw data . Then it calculates the eigenvectors of this weighted graph X  X  Laplacian matrix and selects the top k biggest eigenvectors w.r.t the eigenvalues for further processi ng. Finally, the selected eigenvectors are normalized and k-means clustering method is utilized to cluster the normalized eigenvectors data. Apparently, how to build the weighted graph and further the quality of the weighted graph are critical for the clustering performance.
In NJW X  X  framework, a global scale parameter is used in establishing the weighted graph from the raw data. But this solution ignores the difference among data points in term of their local shapes. To fix this weakness, Zelnik-Manor and Perona (ZP) [28] specify a local scaling parameter for each data point. In their method, the distance between a point and its K -th neighbor is selected as this point X  X  scale parameter. Though this approach takes into account a point X  X  local shape characteristics to some extent, it may neglect the facts that the resulting neighbor data points may belong to differ ent groups. In seeking a better solution to remedy NJW X  X  weakness, we propose in this paper another approach to spec-ifying the local scaling parameter based on data local connectivity information, with an aim of providing a scale paramete r self-adjusting sp ectral clustering algorithm.

Besides specifying the scaling parameter for each data point, another impor-tant problem in spectral clustering is to s elect appropriate eig envectors for fur-ther processing (e.g., clus tering). Recently, several works have been proposed. Xiang and Gong proposed a probability-based method to select eigenvectors [25]. In their approach, the elements of each eigenvector follow either unimodal or multmodal distributions based on whether the eigenvector is relevant. Un-der this formulation, every eigenvector has an importance score, which is used to select informative eigenv ectors. Another work by Zhao et al. [29] evaluates the importance of an eigenvector by its e ntropy. Different from the existing ap-proaches, our method utilizes both eigenvalues and eigenvectors in choosing most appropriate eigenvectors. Moreover, the proposed method is simple to implement yet very efficient in practice.

The contribution of this work is three-fold. First, we build a unified frame-work to study the impact of the scale parameter on calculating similarity between data objects. This framework can easily accommodate various state of art meth-ods spectral clustering methods for p erformance comparison study. Second, we design a new approach to specifying the scale parameter based on local connec-tivity analysis. Third, we present an eff ective method for eige nvector selection. Compared with previous works, our solution has sound theoretical basis and is efficient from practical perspective. Th e experimental results show the efficacy of our approach in handling the data clustering of different scenarios. This work is inspired by Ng et al. [15] that establishes t he spectral clustering framework (NJW for abbreviation) and Zelnik-Manor and Perona X  X  attempt on improving NJW method [28]. To make this paper self-contained, a concise intro-duction of NJW algorithm is presented in the following.

Foragivendataset S = { s 1 ,s 2 ,...,s n } ( s i  X  R l ,where l  X  R is the total dimensions of the data object attributes space; n isthenumberofdataobjects), the goal is to partition S into k different subsets. Algorithm 1 describes the NJW framework.
 Algorithm 1. Ng-Jordan-Weiss Algorithm As shown in Algorithm 1, there are two data representation transformations. The first one is that the raw data S is represented by the affinity matrix A .The second is to represent the data by top K eigenvectors of the Laplacian matrix, denoted by X . In this paper, we focus on how to i mprove the spectral clustering from each transformation. In Section 2.1, we propose a local connectivity-based method to specify the scale parameter, which produces a high quality affinity matrix proved by the experimental evaluation. In Section 2.2, method taking ad-vantage of both eigenvectors and eigenvalues is proposed to choose eigenvectors. 2.1 Local Connectivity-Based Scaling While representing the raw data with the affinity matrix(which is also named as the weighted graph) an ideal transformation of the data is to allocate the data points in a same group closely in the new representation space, and data points from different groups far away from each other. Therefore, different groups have weak connectivities, which is helpful to se parate the data objects correctly ([2], [5]). Let us give one example to explain the impact of different affinity matrices on representing the data.

Figure 1(a) is the desired clustering result, where there are two groups of data points: a fish shape group and a line shaped group. In Figures 1(b) to 1(d), we use the horizontal axis and the vertical axis to represent the first and second eigenvectors, respectively. By comparin g these three figures, we can easily draw the conclusion that the representation in Figure 1(b) is better for clustering the data, since points in different groups ar e separated clearly. On the other hand, shadow and hollow points are quite mixed each other in Figures 1(c) and 1(d), which shows the difficulty in correct clustering. Actually, these three affinity matrices are generated by th ree different parameter values, which will be intro-duced in more details later. In spectral clustering, all affinities are calculated by Equation 1. In this paper, we focus on how to improve this calculation (Equation 1), and further the quality of the affinity matrix.

Before giving a detailed description of the proposed approach, we firstly intro-duce a concept of Core Set .Considerapoint s i  X  S ,its Core Set , denoted as C ( s i ), includes all the data points that have high affinities to s i .Let  X  be the threshold to determine whether there exists an affinity or not. C ( s i ) is defined as: Let  X  s i be the specified scale parameter for s i , then: where R ( s i )isthe Core Radius of C ( s i ), which is defined by: Therefore, if point s j has the distance of R ( s i )topoint s i , the affinity between s i and s j will be quantified as  X  .Given  X  and  X  s i , R ( s i ) can also be calculated by: In NJW algorithm,  X  is a user-customized global parameter, which means for each data point, its  X  s i is always equal to the assigned  X  .Furthermore,  X  ,usedto define a small similarity value, is also a global parameter. According to Equation 5, the Core Radius values of all points are equal. Unfortunately, this global scaling scheme can easily cause t wo unexpected situations, namely, partial-fitting and over-fitting , which are illustrated in Figures 2(a) and 2(b).

Figure 2(a) shows the scenario of partial-fitting . For a point s i , partial-fitting means that there exists at least one point whose desired class is the same as s i , partial-fitting canbeformulatedas: Figure 2(b) shows the scenario of over-fitting . over-fitting means there exists at least one such point, whose desired class is not the same as s i  X  X , but it is involved in s i  X  X  Core Set . Formally, To fix this weakness, Zelnik-Manor and Perona utilized a point X  X  neighboring information to specify the sca le parameter. In their method,  X  s i is equal to the distance between point s i and its K -th closest neighbor. Therefore according to Equation 5, the Core Radius value will be different for different points. This method improves NJW spect ral clustering to certain extent. However, it can-not resolve the problem completely. Fig ures 2(c) and 2(d) show the results of their method, where K is equal to 4. Apparently, partial-fitting and over-fitting problems do exist.

The essential reason that both NJW and ZP algorithms cannot overcome partial-fitting and over-fitting problems is that their approaches cannot guaran-tee that all the points in C ( s ) have the same desired cluster label as s  X  X . In order to solve these problems, we propose an approach based on local connectivity analysis that the data point can utilize its local information to self adjust the scale parameter. Figures 2(e) and 2(f) show the results from our method for data points s 1 and s 2 . Obviously, all points in C ( s 1 )( C ( s 2 )) share the same expected cluster label of s 1 ( s 2 ) so there is no over-fitting . Our approach avoids the partial-fitting problem for data point s 1 and over-fitting problem for data point s 2 ,which were suffered from other approaches shown in Figures 2(a) through 2(d).
In the following, we introduce the concept of adjacent region for a data point s in a multi-dimension space. It is denoted by  X  ( s ) and formally defined as: where s i denotes the i -th dimension of data point s ,and r is a user specified constant parameter. In this work, we set r isequalto1.Itiseasytocheckthat are adjacent .

Let s i ,s j  X  S betwodatapoints, s i ,s j are said to be connected ,if(1) s i  X  s nected . For simplicity purpose, we define this point sequence as a path , denoted urated in U , denoted as  X  ( p s i ,s j ,U ). With the above notations, we can redefine Core Set of s i as: As shown in Equation 9, we employed two principles in forming the Core Set :no over-fitting and minimize partial-fitting . With the constraint, we can avoid over-fitting absolutely. The objective function tries to maximize the size of the core set, which is equivalent to minimizing partial-fitting problem. Figure 3 illustrate theconceptwithanexample.
There are nine points labeled a,b,...,i in Figure 3. All the points are scattered in a grid space where each grid cell has a length of one unit. When consider the point a , using the principle of no over-fitting , we can expand and find its Core Set according to the formulation in Equation 9. According to the criterion defined in Equation 8 in determining ad jacency relationship, points b , c , d are supposed to be involved in point a  X  X  Core Set . The distance between points a and c is If we further expand the Core Set , the next point being considered should be Therefore, e can not be involved. So point a  X  X  Core Set is { b, c, d } ,andits Core Radius R ( a )isequalto
Consider two points s i and s j , the affinity between them is different when we look it from distinct viewpoints. From point s i , the affinity is exp(  X  1  X  d spective. We unify the affinity between points s i and s j by Equation 10. In addition, for the point itself, its own affinity A ii is defined to be equal to 0.
In our approach, parameter  X  needs to be specified. We will show in the following that this is an easy task. Clustering performance can be evaluated by the difference between the intra-cluster affinity summation and the inter-cluster affinity summation, which is given as: where A is a given data set, k is the total number of clusters, V is the clustering result, and V t indicates the point set of the t th cluster. The ideal performance  X   X  of clustering on A is defined as: We utilize an example to illustrate the relation between  X  and  X   X  in Figure 4. In Figure 4(a), the horizontal axis represents different  X  values, and the vertical axis shows the clustering performance values of  X   X  .AsdepictedinFigure4(a), there is a high  X   X  area, which corresponds to a value range of  X  close to 0. When  X  is equal to 0 . 0052,  X   X  achieves the highest value of 20 . 9393, and the data set will be clustered correctly as shown in Figure 4(b). When  X  value increases,  X   X  value will decrease (clusterin g performance drops). When  X  value is too big, it is difficult to cluster data set correctly. T his scenario is reflected in Figures 4(c) and 4(d), when  X  is equal to 0.045 and 0.1, respectively.

Theoretically,  X  is supposed to be a small value. Recall the process of forming the Core Set , we prefer the data points involved to have the same clustering results as the one being considered. Acco rding to Equation 11, a good clustering result should have big difference between the internal affinity summation and the (a) a function between  X  and  X   X  (b) clustering result when  X  =0 . 0052 (c) clustering result when  X  =0 . 045 (d) clustering result when  X  =0 . 1 external affinity summation. Therefore, a simple way is to let points outside of s  X  X  Core Set have zero affinity to s i . In other words, the points whose distances to s i are equal to the Core Radius should have a small affinity. On the other hand, if let s i have high affinity to all other points,  X   X  will be small, which has been proved by Figure 4(a). Practically,  X  can be regarded as a small value constant, say 0.0001 . 2.2 Eigenvector Selection Recently, eigenvector selection has been deployed to improve the performance of spectral clustering [25] [29]. In this work, we proposed a simple yet effective strategy, which considers both eigenvalu es and eigenvectors, to select eigenvec-tors for clustering. Eigenvalues are used to determine the candidate eigenvectors. Further decisions are based on ana lyzing the selected eigenvectors.

The goal of eigenvector selection is to select informative eigenvectors as the representation of the original data obj ects set. Different from the method given in NJW framework (where the top K biggest eigenvectors are selected), we firstly choose K + t ( t is a constant, and it is equal to 2 in this work) biggest ones as candidates. And then we employ the following criterion for further selection: Where z is an eigenvector in X , z i is the i -th item in z . mean ( z )isthemean value of z , which is equal to 1 n n i =1 z i and var ( z ) is the standard deviation of its elements. We give one example to illustrate the effectiveness of this approach.
In Figure 5, there is a comparison be tween using and not using eigenvector selection. Figure 5(a) is drawn by top 2 e igenvectors without eigenvector selec-tion, and Figure 5(b) is the corresponding clustering result of this approach. Accordingtothesetwofigures,wecanl earn that the top 2 eigenvectors are not always good candidates for clustering. Figure 5(c) is generated by the two eigenvectors selected throug h Equation 13, and Figure 5(d) shows its clustering result. Obviously, these two eigenvectors represent the data in a better way, and the clustering result is desirable.
 To evaluate the performance of our approach, we compare it with both NJW and ZP methods. As there is a user-specified parameter in each algorithm, our comparison will focus on both the correct ness of the results and the convenience of assigning the parameters.

On evaluating the convenience of assigning the parameter, we developed two metrics: accuracy and coverage .Let D = { d 1 ,d 2 ,...,d n } ( n  X  R ) be a data set, p be a parameter and its value will be drawn from v = { v 1 ,v 2 ,...,v m } ( m  X  R ). Assume f ( v i ,d j ) be the function to judge whether an algorithm can cluster d j correctly when p = v i . If the algorithm can produce the desired result, then f ( v i ,d j ) is equal to 1; otherwise, we assign 0 to it. With these notations, accuracy is defined as: As shown in Equation 14, if accu ( p = v i ,D ) is larger then more data points can be clustered correctly given p = v i . In other words, it is easy for us to decide the value of parameter p . An ideal situation is that accu ( p = v i ,D ) is equal to 1, then p can be treated as a constant. Metric accuracy is used to measure the effectiveness of clustering on entire training data set with a specified parameter value.
Metric coverage is introduced to quantify the complexity of determining the parameter to one sample, we define it as: According to Equation 15, if cover ( p, d j ) is larger, it is easier to find a proper p value for d j . Scenario cover ( p, d j ) = 1 means any candidate parameter value can give the expected result.

To examine the effectiveness of the prop osed approach, we evaluated 12 data sets in the experiments, which are shown in Figure 6. The data sets evaluated are roughly in four groups. Figures 6(a) to 6(c) are Letters . Figures 6(d) to 6(f) are Numbers . Figures 6(g) to 6(i) are Dot-Line in different shapes. Figures 6(j) to 6(l) are different combinations of Cycles .

These four classes of figures are very ty pical on analyzing the performance of spectral clustering, and similar data sets have been studied widely ([15], [28], [29]). The challenge in clustering Letters is that some letters are complex, such as  X  X  X  and  X  X  X , and if some letters are embedded into others, such as  X  X AKDD X  and  X  X P X , which makes the problem more complicated. Without proper scaling, most of the example figures cannot be clu stered correctly. F igure 7(a) is K-means X  X  result on the data in Figure 6(b).

Numbers is also a difficult task for clustering in many cases. For example, there are four numbers in Figure 6(e), and these numbers are mixed together. K-means cannot separate them correctly, whose result is shown in Figure 7(b).
The difficulty of clustering Dot-Line data sets is that the segment of the line which is very close to a dot can be easily clustered into the dot group. Figure 7(c) gives one example of this scenario.
For Cycles data sets, more cycles may share the same center, then they are difficult to be clustered without re-pres entation. Figure 7(d) is one example of this case.

We test all three algorithms (NJW, ZP and ours) on all figures given in Figure 6. For each algorithm, we test its parameter within a range. For NJW algorithm,  X  changes from 0.1 to 2.0, and its step of change is set to be 0.1. For k in ZP algorithm, it ranges from 1 to 10, and its step length is 1.  X  in our algorithm is between 0.0001 to 0.02, and its step length is 0.0001. All the results of these algorithm are given in Figure 8. In these three figures, the horizontal axis indicates the data set indexes, where 1 to 16 correspond Figures 6(a) to 7( d), and the vertical axis represent the parameter values of the algorithm. Furthermore, one square indicates a correct case. For example, there is a square at (1, 0.1) in Figure 8(c), which means that when  X  =0 . 1, Figure 6(a) can be c lustered correctly.

The comparison results of coverage and accuracy on these three algorithms are given in Figure 9. As described in Figures 9(a) to 9(c), our algorithm works very well. When  X  is in { 0 . 0001, 0 . 0002, 0 . 0003, 0 . 0004 } , it reaches the ideal scenario -the accuracy isequalto1.Therefore,wecansimplytake  X  as a constant, whose value is 0 . 0001. Furthermore  X   X  X  accuracy is much higher than  X   X  X  and k  X  X , and it also fluctuates much smaller than  X  , k .

On the metric coverage , our parameter  X  also gets high performance, which is described in Figure 9(d). Surprisingly,  X  gets the coverage of 1 in 83.3% data sets (10/12). Due to the page limitation, we won X  X  be able to show more scenarios that our algorithm can work effectively with  X  =0 . 0001.

It should be noticed that though connectivity plays an important role in our algorithm, we still can handle  X  X nexpected X  situations. In Figure 6(j), while the outer cycle is broken,  X   X  X  coverage of this data set is even 1.
 In this paper, we proposed a scale parameter specification approach to improve the NJW algorithm [15] as a scale parameter self-adjusting one. Compared with the specification method given in [28] , our approach is less sensitive to the datasets. We also presented a new method for eigenvector selection, which is easier to apply than current approaches [25], [29]. Experimental results and comparisons demonstrated that our algor ithm can deal with d ifferent scenarios, and can handle more instances which ZP X  X  algorithm cannot cluster well. In the future, we will test and improve the proposed approach through more real applications.
 This work is partially supported by OU-Beaumont Multidisciplinary Research Award. Xingquan Zhu is sponsored by ARC Future Fellowship under Grant No. FT100100971.

