 University of Illinois at Urbana-Champaign
In this article we explore the syntactic and semantic properties of prepositions in the context of the semantic interpretation of nominal phrases and compounds. We investigate the problem Portuguese, and Romanian. The focus on English and Romance languages is well motivated.
Most of the time, English nominal phrases and compounds translate into constructions of the form N PN in Romance languages, where the P (preposition) may vary in ways that correlate with the semantics. Thus, we present empirical observations on the distribution of nominal phrases and compounds and the distribution of their meanings on two different corpora, based semantic relations. A mapping between the two tag sets is also provided. Furthermore, given a training set of English nominal phrases and compounds along with their translations in the five
Romance languages, our algorithm automatically learns classification rules and applies them to unseen test instances for semantic interpretation. Experimental results are compared against two state-of-the-art models reported in the literature. 1 .Introduction
Prepositions are an important and frequently used category in both English and Ro-mance languages .In a corpus study of one million English words, Fang (2000) shows that one in ten words is a preposition .Moreover, about 10% of the 175 most frequent words in a corpus of 20 million Spanish words were found to be prepositions (Almela et al .2005) .Studies on language acquisition (Romaine 1995; Celce-Murcia and Larsen-
Freeman 1999) have shown that the acquisition and understanding of prepositions in languages such as English and Romance is a difficult task for native speakers, and even more difficult for second language learners .For example, together with articles, prepositions represent the primary source of grammatical errors for learners of English as a foreign language (Gocsik 2004). by various scholars in linguistics, psycholinguistics, and computational linguistics, very few studies have been done on the function of prepositions in natural language processing (NLP) applications .The reason is that prepositions are probably the most polysemous category and thus, their linguistic realizations are difficult to predict and their cross-linguistic regularities difficult to identify (Saint-Dizier 2005a). tic interpretation of English nominal phrases and compounds .The problem is simple to define: Given a compositional noun phrase (the meaning of the phrase derives from the meaning of the constituents) constructed out of a pair of nouns, N the head and the other the modifier, determine the semantic relationship between the two nouns .For example, the noun X  X oun compound family estate encodes a POSSESSION relation, while the nominal phrase the faces of the children refers to PA RT -WHOLE .The problem, although simple to state, is difficult for automatic semantic interpretation.
The reason is that the meaning of these constructions is most of the time implicit (it cannot be easily recovered from morphological analysis) .Interpreting nominal phrases and compounds correctly requires various types of information, from world knowledge to lexico-syntactic and discourse information.
 (N N) and investigates the problem based on cross-linguistic evidence from a set of six languages: English, Spanish, Italian, French, Portuguese, and Romanian .The choice of these constructions is empirically motivated .In a study of 6,200 (Europarl (CLUVI 2 ) English token nominal phrase and compound instances randomly chosen from two English X  X omance parallel text collections of different genres, we show that over 80% of their Romance noun phrase translations are encoded by N P N and N N constructions .For instance, beer glass , an English compound of the form N lates into N 2 P N 1 instances in Romance: tarro de cerveza ( X  X lass of beer X ) in Spanish, de cerveja ( X  X lass of beer X ) in Portuguese, and pahar de bere ( X  X lass of beer X ) in Romanian. provide the word-by-word gloss (in  X  X arentheses X ) .Moreover, we use N the two lexical nouns that encode a semantic relation (where N and N 2 is the syntactic head), and Arg 1 , Arg 2 to denote the semantic arguments of the relation encoded by the two nouns .For example, beer glass encodes a PURPOSE relation where Arg 1 ( beer ) is the purpose of Arg 2 ( X  X lass X ; thus  X  X lass (used) for beer X ). ( N 1 N 2 in noun compounds and N 2 P N 1 in nominal phrases) is not always the same as the semantic directionality given by the semantic argument frame of the semantic relation .Otherwise said, N 1 does not always map to Arg 1 relation.
 ships between nouns .For example, English nominal phrases and compounds of the 186 form N 1 N 2 (e.g., wood stove )and N 2 P 1 N 1 (e.g., book on the table ) usually translate in Romance languages as N 2 P 2 N 1 (e.g., four ` abois in French  X   X  X tove at/to wood X , and livre sur la table  X   X  X ook on the table X ) .Romance languages have very few N N compounds and they are of limited semantic categories, such as TYPE (e.g., legge quadro in Italian  X   X  X aw framework X   X  translates as framework law ) .Besides the unproductive
N N and the productive N P N phrases, Romanian also uses another productive con-struction: the genitive-marked noun X  X oun compounds (e.g., frumuset  X ea fetei  X  X eauty-the girl-GEN  X  translated as the beauty of the girl ) .Whereas English N N compounds are right-headed (e.g., framework /Modifier law /Head), Romance compounds are left-headed (e.g., legge /Head quadro /Modifier) .Moreover, the Romance preposition used in the translations of English nominal phrase instances of the type N P N is one that comes closest to having overlapping semantic range as intended in the English instance, but may not be the exact counterpart for the whole semantic range .For example, Committee on Culture translates as Comisi  X  on de la Cultura (Spanish) ( X  X ommittee of the Culture X ),
Commission de la Culture (French) ( X  X ommittee of the Culture X ), Commissione per la Cul-tura (Italian) ( X  X ommittee for the Culture X ), Comiss  X  ao para Cultura (Portuguese) ( X  X om-mittee for Culture X ), and Comitet pentru Cultur  X  a (Romanian) ( X  X ommittee for Culture X ).
Even those Romance prepositions that are spelled  X  X e X  are pronounced differently in different Romance languages.
 guages is also motivated linguistically .The extension of this task to natural languages other than English brings forth both new insights and new challenges .The Romance prepositions used in the translations of English nominal phrases and compounds, may vary in ways that correlate with the semantics .Thus, Romance language prepositions will give us another source of evidence for disambiguating the semantic relations in
English nominal phrases and compounds .We argue that, in languages with multiple syntactic options such as English (N N and N P N) and Romanian (N N, genitive-marked N N, and N P N), the choice between such constructions in context is governed in part by semantic factors .For example, the set of semantic relations that can be encoded by pairs of nouns such as tea  X  cup and sailor  X  suit varies with the syntactic construction used .In English, while the noun X  X oun compounds tea cup and sailor suit encode only PURPOSE , the N P N constructions cup of tea and suit of the sailor encode
SION , respectively .Similarly, in Romanian both tea cup and cup of tea translate only as the N P N instance cea  X  sc  X  adeceai ( X  X up of tea X ), while sailor suit translates as costum de marinar ( X  X uit of sailor X ) and the suit of the sailor as the genitive-marked N N costumul marinarului ( X  X uit-the sailor-GEN  X ) .Thus, we study the distribution of semantic relations across different nominal phrases and compounds in one language and across all six languages, and analyze the resulting similarities and differences .This distribution is evaluated over the two different corpora based on two state-of-the-art classification tag sets: Lauer X  X  set of eight prepositions (Lauer 1995) and our list of 22 semantic relations. A mapping between the two tag sets is also provided.
 tions and other linguistic clues are employed as features in a supervised, knowledge-intensive model .Furthermore, given a training set of English nominal phrases and compounds along with their translations in the five Romance languages, our algo-rithm automatically learns classification rules and applies them to unseen test instances for semantic interpretation .As training and test data we used 3,124 Europarl and 2,023 CLUVI token instances .These instances were annotated with semantic relations and analyzed for inter-annotator agreement .The results are compared against two state-of-the-art approaches: a supervised machine learning model, semantic scattering (Moldovan and Badulescu 2005), and a Web-based unsupervised model (Lapata and
Keller 2005) .Moreover, we show that the Romanian linguistic features contribute more substantially to the overall performance than the features obtained for the other Ro-mance languages .This is explained by the fact that the choice of the linguistic construc-tions (either genitive-marked N N or N P N) in Romanian is highly correlated with their meaning.

In Section 3 we describe the general approach to the interpretation of nominal phrases and compounds and list the syntactic and semantic interpretation categories used along with observations regarding their distribution in the two different cross-linguistic corpora .Sections 4 and 5 present a learning model and experimental results .Section 6 presents linguistic observations on the behavior of English and Romanian N N and
N P N constructions .Finally, in Section 7 we provide an error analysis and in Section 8 we offer some discussion and conclusions. 2 .Previous Work 2.1 Noun Phrase Semantic Interpretation
The semantic interpretation of nominal phrases and compounds in particular and noun phrases (NPs) in general has been a long-term research topic in linguistics, computa-tional linguistics, 3 and artificial intelligence.
 Noun X  X oun compounds in linguistics
Early studies in linguistics (Lees 1963) classified noun X  X oun compounds on purely grammatical criteria using a transformational approach, criteria which failed to account for the large variety of constraints needed to interpret these constructions .Later on, Levi (1978) attempted to give a tight account of noun X  X oun interpretation, distinguishing two types of noun X  X oun compounds: (a) compounds interpreted as involving one of nine predicates ( CAUSE , HAVE , MAKE , USE , BE , IN , FOR , FROM , ABOUT )(e.g., onion tears encodes CAUSE ) and (b) those involving nominalizations, namely, compounds whose heads are nouns derived from a verb, and whose modifiers are interpreted as arguments of the related verb (e.g., a music lover loves music) .Levi X  X  theory was cast in terms of the more general theory of Generative Semantics .In that theory it was assumed that the interpretation of compounds was available because the examples were derived from underlying relative clauses that had the same meanings .Thus, honey bee , expressing the relation MAKE , was taken to be derived from a headed relative abee that makes honey .Levi was committed to the view that a very limited set of predicates constituted all of the relations that could hold between nouns in simple noun X  X oun compounds .This reductionist approach has been criticized in studies of language use by psycholinguists (Gleitman and Gleitman 1970; Downing 1977) who claim that noun X  noun compounds, which are frequent in languages like English, encode in principle an 188 unbounded number of possible relations .One such example is apple juice seat  X  X  X  seat in front of which an apple juice [is] placed X  (Downing 1977, page 818) X  X hich can only be interpreted in the current discourse context.
 agree with Downing (1977) that pragmatics plays an important factor in noun X  X oun interpretation, a large variety of noun X  X oun meanings can be captured with a well-chosen set of semantic relations .Our proposed semantic classification set differs from that of Levi (1978) in the sense that it contains more homogenous categories .Levi X  X  categories, instead, are more heterogeneous, including both prepositions and verbs, some of which are too general (e.g., the prepositions for, in and the verb to have ), and thus, too ambiguous .Moreover, in our approach to automatic semantic interpretation we focus on both N N and N P N constructions and exploit a set of five Romance languages.
 Noun X  X oun compounds in computational linguistics
The automatic interpretation of nominal phrases and compounds is a difficult task for both unsupervised and supervised approaches .Currently, the best-performing noun X  X oun interpretation methods in computational linguistics focus mostly on two or three-word noun X  X oun compounds and rely either on ad hoc, domain-specific, hand-coded semantic taxonomies, or statistical models on large collections of unlabeled data .Recent results have shown that symbolic noun X  X oun compound interpretation systems using machine learning techniques coupled with a large lexical hierarchy perform with very good accuracy, but they are most of the time tailored to a specific domain (Rosario and Hearst 2001; Rosario, Hearst, and Fillmore 2002), or are general purpose (Turney 2006) but rely on semantic similarity metrics on WordNet (Fellbaum 1998) .On the other hand, the majority of corpus statistics approaches to noun X  X oun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer 1995) .The problem is that most noun X  X oun compounds are rare and thus, statistics on such infrequent instances lead in general to unreliable estimates of probabilities .More recently, Lapata and Keller (2005) showed that simple unsupervised models applied to the noun X  X oun compound interpretation task perform significantly better when the n-gram frequencies are obtained from the Web (55.71% accuracy 4 ), rather than from a large standard corpus.
Nakov and Hearst (2005) improve over Lapata and Keller X  X  method through the use of surface features and paraphrases only for the task of noun X  X oun compound bracketing (syntactic parsing of three-word noun compounds) without their interpretation. Other researchers (Pantel and Ravichandran 2004; Pantel and Pennacchiotti 2006;
Pennacchiotti and Pantel 2006) use clustering techniques coupled with syntactic dependency features to identify IS -A relations in large text collections .Kim and Baldwin (2005) propose a general-purpose method that computes the lexical similarity of unseen noun X  X oun compounds with those found in training .More recently Kim and Baldwin (2006) developed an automatic method for interpreting noun X  X oun compounds based constructions involving the constituent nouns and a set of seed verbs denoting the semantic relation (e.g., to own denotes POSSESSION ) .Then all noun X  X oun instances in transitive sentential contexts (i.e., those sentences containing a transitive verb) are mapped onto the selected set of constructions based on lexical similarity over the verbs. lem, current probabilistic models are limited because they do not take full advantage of the structure and the meaning of language.
 interpretation of nominal phrases and compounds .Busa and Johnston (1996), Johnston and Busa (1996), and Calzolari et al .(2002), for example, focus on the differences between English and Italian noun X  X oun compounds .In their work they argue that a computational approach to the cross-linguistic interpretation of these compounds has to rely on a rich lexical representation model, such as those provided by FrameNet frames (Baker, Fillmore, and Lowe 1998) and qualia structure (Pustejovsky 1995) .In the qualia structure representation, for example, the meaning of a lexical concept, such as the modifier in a noun X  X oun compound, is defined in terms of four elements representing concept attributes along with their use and purpose .Thus, qualia structure provides a relational structure that enables the compositional interpretation of the modifier in relation to the head noun .Two implementations of such representations are provided by the SIMPLE Project ontology (Lenci et al .2000) and the OMB ontology (Pustejovsky et al .2006) .The SIMPLE ontology, for example, is developed for 12 European languages and defines entry words that are mapped onto high-level concepts in EuroWordNet (Vossen 1998), a version of WordNet developed for European languages.
 linguistic features generated from corpus evidence coupled with word sense disam-biguation and WordNet concept structure information .The results obtained are com-pared against two state-of-the-art approaches: a supervised machine learning model, semantic scattering (Moldovan and Badulescu 2005), and a Web-based unsupervised model (Lapata and Keller 2005) .In this research we do not consider extra cross-linguistic information, such as semantic classes of Romance nouns (those provided by IS -A re-
SIMPLE ontology .However, such resources can be added at any time to further improve the performance of noun X  X oun interpretation systems. 2.2 Semantics of Prepositions Although prepositions have been studied intensively in linguistics (Herskovits 1987;
Zelinski-Wibbelt 1993; Linstromberg 1997; Tyler and Evans 2003; Evans and Chilton 2009, among others), they have only recently started to receive more attention in the computational linguistics community. 5 Moreover, the findings from these broad stud-ies have not yet been fully integrated into NLP applications .For example, although information retrieval, and even question answering systems, would benefit from the incorporation of prepositions into their NLP techniques, they often discard them as stop words. 190 Prepositions in linguistics
Considerable effort has been allocated to the investigation of spatial prepositions mainly based on a cognitive approach, not only in English (Herskovits 1987; Linstromberg 1997;
Tyler and Evans 2003; Evans and Chilton 2009), but also in many of the Indo-European languages (Casadei 1991; Vandeloise 1993; Cadiot 1997; Melis 2002; Luraghi 2003) .These studies provide a detailed analysis of such prepositions trying to give a methodologi-cal motivated account for the range of their polysemy .These works identify special constraints on various prepositional patterns, such as semantic restrictions on the noun phrases occurring as complements of the preposition .For example, in prepositional phrase constructions such as in NP , the head noun can be a container ( in a cup ), a geometrical area ( in a region ), a geo-political area ( in Paris ), an atmospheric condition ( in the rain ), and so on .These selectional restrictions imposed by the preposition on the noun phrases it combines with are presented in various formats from lists (Herskovits 1987; Linstromberg 1997) to semantic networks of cluster senses (Tyler and Evans 2003).
In this article we also focus on the polysemy of such prepositions, but we identify the se-lectional restrictions automatically based on a specialization procedure on the WordNet as relevance and tolerance .These account for the difference that pragmatic motivations and context dependency make to how expressions are understood .Relevance has to do with communicative goals and choice of means and is evident, for example, in instances such as cat on the mat which is still relevant even when only the paws and not the whole cat are on the mat .Tolerance occurs in situations in which a book, for example, table.
 the man at his desk (cf .Herskovits 1987) implies, besides a man is using the desk, thus an INSTRUMENT relation .Other inferences are more subtle, involving spatial reasoning about the actions that can be performed on the arguments of the preposition .One such instance is infant in a playpen (cf .Tyler and Evans 2003), where the movement of the playpen involves the movement of the infant .In order to identify such inferences the automatic interpretation system has to rely on pragmatic knowledge .In this research we do not deal with such inference issues, rather we identify the meaning of N P N constructions based on the local context of the sentence. Prepositions in computational linguistics
In order to incorporate prepositions into various resources and applications, it is neces-sary to perform first a systematic investigation of their syntax and semantics .Various researchers (Dorr 1993; Litkowski and Hargraves 2005; Saint-Dizier 2005b; Lersundi and Aggire 2006) have already provided inventories of preposition senses in English and other languages .Others have focused on the analysis of verb particles (Baldwin 2006a, 2006b; Villavicencio 2006), the distributional similarity (Baldwin 2005) and the semantics of prepositions (Kordoni 2005) in a multilingual context, and the meaning of prepositions in applications such as prepositional phrase attachment (O X  X ara and Wiebe 2003; Kordoni 2006; Volk 2006).
 tional linguistics relating to contrastive analysis of prepositions (Busa and Johnston (1996); Johnston and Busa (1996); Jensen and Nilsson (2005); Kordoni (2005), inter alia), to our knowledge, there have not been any attempts to provide an investigation of the prepositions X  role in the task of automatic noun phrase interpretation in a large cross-linguistic English X  X omance framework. 3 .Linguistic Considerations of Nominal Phrases and Compounds
The meaning of nominal phrases and compounds can be compositional (e.g., spoon handle  X  PA RT  X  WHOLE , kiss in the morning  X  TEMPORAL ), or idiosyncratic , when the also encode metaphorical names (e.g., ladyfinger ), proper names (e.g., John Doe ), and dvandva compounds 6 in which neither noun is the head (e.g., player X  X oach ). root (non-verbal, e.g., tea cup ) constructions. 7 It is widely held (Levi 1978; Selkirk 1982b) that the modified noun of a synthetic noun X  X oun compound, for example, may be associated with a theta-role of the compound X  X  head noun, which is derived from a verb .For instance, in truck driver , the noun truck satisfies the THEME relation associated with the direct object in the corresponding argument structure of the verb to drive . compounds of the type N N (noun X  X oun compounds which can be either genitive-marked or not genitive-marked) and N P N, and disregard metaphorical names, proper names, and dvandva structures .In the following we present two state-of-the-art se-mantic classification sets used in automatic noun X  X oun interpretation and analyze their distribution in two different corpora. 3.1 Lists of Semantic Classification Relations
Although researchers (Jespersen 1954; Downing 1977) argued that noun X  X oun com-pounds, and noun phrases in general, encode an infinite set of semantic relations, with high frequency in these constructions .However, the number and the level of abstraction of these frequently used semantic categories are not agreed upon .They can vary from a few prepositions (Lauer 1995) to hundreds and even thousands of more specific semantic relations (Finin 1980) .The more abstract the category, the more noun should be assigned .Lauer, for example, classifies the relation between the head and the modifier nouns in a noun X  X oun compound by making use of a set of eight frequently used prepositions: of, for, with, in, on, at, about ,and from .However, according to this classification, the noun X  X oun compound love story , for instance, can be classified both as story of love and story about love .The main problem with these abstract categories is that much of the meaning of individual compounds is lost, and sometimes there is no way to decide whether a form is derived from one category or another .On the other hand, lists of very specific semantic relations are difficult to build as they usually contain a very large number of predicates, such as the list of all possible verbs that can link the noun constituents .Finin, for example, uses semantic categories such as dissolved in to build interpretations of compounds such as salt water and sugar water .
 defined at different levels of abstraction .The first is a core set of 22 semantic relations (SRs), a set which was identified by us from the linguistics literature and from various experiments after many iterations over a period of time (Moldovan and Girju 2003). 192
Moldovan and Girju proved empirically that this set is encoded by noun X  X oun pairs in noun phrases; the set is a subset of their larger list of 35 semantic relations used in a large set of semantics tasks .This list, presented in Table 1 along with examples and semantic argument frames, is general enough to cover a large majority of text semantics while keeping the semantic relations to a manageable number .A semantic argument frame is defined for each semantic relation and indicates the position of each semantic argument in the underlying relation .For example,  X  Arg 2 is part of (whole) Arg ( Arg 2 ) and the whole ( Arg 1 ) entities in this relation .This representation is important because it allows us to distinguish between different arrangements of the arguments for given relation instances .For example, most of the time, in N N compounds Arg precedes Arg 2 , whereas in N P N constructions the position is reversed ( Arg
However, this is not always the case as shown by N N instances such as ham / Arg sandwich / Arg 1 and spoon / Arg 1 handle / Arg 2 , both encoding PA RT  X  WHOLE .More details on subtypes of PA RT  X  WHOLE relations are presented in Section 6.2. A special relation here is KINSHIP , which is encoded only by N P N constructions and whose argument order is irrelevant .Thus, the labeling of the semantic arguments for each relation as
Arg 1 and Arg 2 is just a matter of convention and they were introduced to provide a consistent guide to the annotators to easily test the goodness-of-fit of the relations .The examples in column 4 are presented with their WordNet senses identified in context from the CLUVI and Europarl text collections, where the specific sense is represented as the sense number preceded by a  X # X  sign.
 be applied only to noun X  X oun compounds, because in N P N instances the preposition is explicit .We selected these two state-of-the-art sets as they are of different size and contain semantic classification categories at different levels of abstraction .Lauer X  X  list is more abstract and thus capable of encoding a large number of noun X  X oun compound instances found in a corpus (e.g., many N 1 N 2 instances can be paraphrased as N
N ), whereas our list contains finer grained semantic categories (e.g., only some N instances encode a CAUSE relation).
 corpora, how well they solve the interpretation problem of noun phrases, and the mapping from one list to another. 3.2 Corpus Analysis
For a better understanding of the semantic relations encoded by N N and N P N instances, we analyzed the semantic behavior of these constructions on two large cross-linguistic corpora of examples .Our intention is to answer questions like:
Romance languages and vice versa? (cross-linguistic syntactic mapping) mapping) nominal phrases and compounds? (1990), Giorgi and Longobardi (1991), and Alexiadou, Haegeman, and Stavrou (2007) on the syntax of noun phrases in English and Romance languages by providing cross-linguistic empirical evidence for in-context instances on two different corpora based on the set of 22 semantic tags .Following a configurational approach, Giorgi and
Longobardi, for example, focus only on synthetic nominal phrases, such as the capture of the soldier ( THEME ), where the noun capture is derived through nominalization from the verb to capture .Besides synthetic constructions, we also consider root nominal phrases and compounds, such as family estate ( POSSESSION ).
 The data
In order to perform empirical investigations of the semantics of nominal phrases and compounds, and to train and test a learning model for the interpretation of noun X  X oun 194 instances encoded by these constructions, we collected data from two text collections with different distributions and of different genres, Europarl and CLUVI.
 aligned corpora made public as part of the freely available Europarl corpus .Specif-ically, the Spanish X  X nglish, Italian X  X nglish, French X  X nglish and Portuguese X  X nglish corpora were automatically aligned based on exact matches of English translations.
Then, only those English sentences which appeared verbatim in all four language pairs were considered .The resulting English corpus contained 10,000 sentences which were syntactically parsed using Charniak X  X  parser (Charniak 2000) .From these we extracted 6,200 token instances of N N (49.62%) and N P N (50.38%) constructions.
 parallel corpora of contemporary oral and written languages, a resource that besides
Galician also contains literary text collections in other Romance languages .Because the collection provides translations into only two of the Romance languages considered here, Spanish and Portuguese, we focused only on the English X  X ortuguese and English X  Spanish literary parallel texts from the works of Agatha Christie, James Joyce, and H .G .
Wells, among others .Using the CLUVI search interfaces we created a sentence-aligned parallel corpus of 4,800 unique English X  X ortuguese X  X panish sentences .The English version was syntactically parsed using Charniak X  X  parser (Charniak 2000) after which each N N and N P N instance was manually mapped to the corresponding translations.
The resulting corpus contains 2,310 English token instances with a distribution of 25.97% N N and 74.03% N P N.
 Corpus annotation and inter-annotator agreement
For each corpus, each nominal phrase and compound instance was presented separately to two experienced annotators 9 in a Web interface in context along with the English sentence and its translations .Because the corpora do not cover some of the languages (Romanian in Europarl, and Romanian, Italian, and French in CLUVI), three other native speakers of these languages who were fluent in English provided the translations, which were added to the list .The two computational semantics annotators had to tag each English constituent noun with its corresponding WordNet sense. not found in WordNet the instance was not considered .The annotators were also asked to identify the translation phrases, tag each instance with the corresponding semantic relation, and identify the semantic arguments Arg 1 and Arg frame of the corresponding relation .Whenever the annotators found an example encod-ing a semantic relation or a preposition paraphrase other than those provided, or if they did not know what interpretation to give, they had to tag it as OTHER -SR (e.g., melody of the pearl : here the context of the sentence did not indicate the association between the two nouns; cry of death : the cry announcing death), and OTHER -PP (e.g., box by the wall , searches after knowledge ) respectively.
 important not only as a feature employed in the training models, but also as guidance ing sentences, daisy flower expresses a PA RT  X  WHOLE relation in Example (1) and an 2.1: flower#2 is a  X  X eproductive organ of angiosperm plants especially one having showy or colorful parts, X  whereas flower#1 is  X  X  plant cultivated for its blooms or blossoms X ). (1) Usually, more than one daisy#1 flower#2 grows on top of a single stem. (2) Try them with orange or yellow flowers of red-hot poker, solidago, or other late had to rely on a larger context provided by the sentence and its translations. noun X  X oun compound is not fixed (Girju et al .2005), the annotators were presented with the semantic argument frame for each of the 22 semantic relations and were asked to tag the instances accordingly .For example, in PA RT  X  WHOLE instances such as chair / Arg 1 arm / Arg 2 the part arm follows the whole chair , whereas in spoon / Arg handle / Arg 2 the order is reversed .In the annotation process the translators also used the five corresponding translations as additional information in selecting the semantic relation .For instance, the context provided by the Europarl English sentence in Exam-ple (3) does not give enough information for the disambiguation of the English nominal phrase judgment of the presidency , where the modifier noun presidency can be either
AGENT or THEME in relation to the nominalized noun head judgment .The annotators had to rely on the Romance translations in order to identify the correct meaning in Portuguese), evaluarea Pre  X  sendint  X iei (Ro . X  Romanian) .
 preposition paraphrase (in case of noun X  X oun compounds), but there were also situa-tions in which an example could belong to more than one category in the same context.
For example, Texas city is tagged as PA RT  X  WHOLE , but also as a LOCATION relation using the 22-SR classification set, and as of, from, in based on the 8-PP set (e.g., city of Texas , city from Texas ,and city in Texas ). Overall, 8.2% CLUVI and 4.8% Europarl instances were tagged with more than one semantic relation, and almost half of the noun X  X oun compound instances were tagged with more than one preposition. (3) En.: If you do , the final judgment of the Spanish presidency will be even more 196 format: NP En ;NP Es ;NP It ;NP Fr ;NP Port ;NP Ro ; target .The word target is one of the 23 (22 + OTHER -SR ) semantic relations and one of the eight prepositions considered for noun compound instances, and one of the 23 semantic relations for N P N instances .For example, development cooperation; cooperaci  X  on para el desarrollo; cooperazione allo sviluppo; used measures of inter-annotator agreement for classification tasks: K = where Pr ( A ) is the proportion of times the annotators agree and Pr ( E ) is the probability of agreement by chance .The K coefficient is 1 if there is a total agreement among chance.
 shown in Table 3 .We also computed the number of instances that were tagged with the number of examples classified in that category by at least one of the judges .For the instances that encoded more than one classification category, the agreement was measured on the first relation on which the annotators agreed.
 agreement for the corpus data on the set of 22 relations, with a higher agreement for the preposition paraphrases .However, according to Artstein (2007), kappa values can drop significantly if the frequency distribution of the annotation categories in the text corpus is skewed .This is the case here, as will be shown in the next section .Thus, for a better understanding of the annotation results we also computed the percentage agreement, which is indicated for each classification set in parentheses in Table 3.
Lauer X  X  prepositions were included in the OTHER -PP category .From these, 2 .1% and 2.3%, respectively, could be paraphrased with prepositions other than those considered by Lauer (e.g., bus service : service by bus ), and 5.7% and 3.4%, respectively, could not be paraphrased with prepositions (e.g., daisy flower ).
 pretation categories on the two different cross-linguistic corpora.
 3.3 Distribution of Syntactic Constructions and Semantic Relations A .Cross-linguistic distribution and mapping of nominal phrases and compounds translation of the 6,200 (3,076 N N and 3,124 N P N) Europarl and 2,310 (600 N N and 1,710 N P N) CLUVI English token instances in each of the five target languages considered .The data show that N N and N P N constructions cover over 83% of the translation patterns for both text corpora .However, whereas the distribution of both constructions is balanced in the Europarl corpus (about 45%, with the exception of
Romanian for which N P N constructions are less frequent), in CLUVI the N P N constructions occur in more than 85% of the cases (again, with the exception of Ro-manian where they represent about 56% of the data) .The high percentage obtained for
N P N instances in CLUVI is explained by the fact that Romance languages have very few N N compounds which are of limited semantic types, such as TYPE .Moreover, it is interesting to note here that some of the English instances are translated into both noun X  X oun (N N) and noun X  X djective (N A) compounds in the target languages .For example, love affair translates into either the N A construction enredo amoroso (Spanish), aventure amoureuse (French), relazione amorosa (Italian), rela  X  cao amorosa (Portuguese), and aventur  X  a amoroas X  a (Romanian), or using the more common N de N pattern aventura de amor (Spanish), aventure d X  X mour (French), storia d X  X more (Italian), estoria de amor (Portuguese), and aventur  X  a de dragoste (Romanian) .There are also instances which translate as one word in the target language, shown in Table 4, column 6 .For example, 198 ankle boot is translated into bottine in French and stivaletto in Italian .The rest of the data is encoded by other syntactic paraphrases, as shown in Table 4, column 7 .For example, bomb site is translated into Italian as luogo dove ` e esplosa la bomba ( X  X he place where the bomb has exploded X ) .Moreover, Table 5 shows the distribution of the prepositions present in the N P N translations.
 instances, we selected those which had all the translations encoded only by N N and
N P N constructions .Columns 3 and 4 in Table 4 show the number of N N and N P N translation instances in each Romance language .Out of these, we considered only 3,124 Europarl and 2,023 CLUVI token instances representing the examples encoded by N N and N P N in all languages considered, after inter-annotator agreement.
B .Cross-linguistic distribution of semantic relations and their mapping to nominal phrases and compounds instances .For example, in Europarl most of the N N instances were naming noun X  noun compounds referring to entities such as member states and framework law which were repeated in many sentences .Many of them encoded TYPE relations (e.g., member state , framework law ) which, most of the time, are encoded by N N patterns in the target languages ( stato membro and legge quadro in Italian, respectively) .In the CLUVI corpus, on the other hand, the N N Romance translations represented only 1% of the data .A notable exception here is Romanian (64.68% of Europarl and 32.8% of CLUVI). This is explained by the fact that, in Romanian, many noun phrases are represented as genitive-marked noun compounds ( N 1 N 2 ) .In Romanian the genitive case is realized either as a suffix attached to the modifier noun N 2 or as one of the genitival articles a/al/ale .If the modifier noun N 2 is determined by an indefinite article then the genitive mark is applied to the article, not to the noun, for example ofat  X  a  X  unei fete ( X  X  girl  X  of/to a determined by the definite article (which is enclitic in Romanian), the genitive mark is added at the end of the noun together with the article .For example, fata  X  fetei (the girl  X  girl , for instance, is translated as frumuset  X ea fetei ( X  X eauty-the girl-agirl as frumuset  X ea unei fete ( X  X eauty-the of/to a girl X ).

N N constructions depends on the specificity of the instance .Some noun X  X oun instances preferred is the genitive-marked N N, or they can refer in general to the category of those entities (generic interpretation), 11 thus using N de N .For example, the instance
GEN  X ), whereas a scorpion bite ( AGENT ) translates into scorpion X ).
 tations can be encoded by both N P N and genitive-marked N N constructions as shown by the example above .However, there are situations when the generic and the existential interpretations change the meaning of the noun X  X oun pair .One such example is thesuitofthesailor ( POSSESSION ) translated as costumul marinarului ( X  X uit-the sailor-GEN  X ), and sailor suit ( PURPOSE ) translated as costum de marinar ( X  X uit of sailor X ). 200 existential interpretation .For example, some POSSESSION -encoding instances such as the budget of the University translate as  X  bugetul Universit  X  at  X ii  X  (budget-the University-and not as  X  bugetul de Universitate  X  (budget-the of University) .Other relations such as
GEN ), and (b) the milk glass ( PURPOSE ) translates as  X  and not as  X  paharul laptelui  X  (glass-the milk-GEN ) .Other examples include CAUSE and compounds and is used in the learning model to discriminate among the possible interpretations.

This distribution is represented both in number of tokens (the total number of instances per relation) and types (the unique number of instances per relation) .In Europarl, the most frequently occurring relations are TYPE and THEME that together represent about 50% of the data with an equal distribution .The next most frequent relations are TOPIC , PURPOSE , AGENT ,and PROPERTY with an average coverage of about 8%. Moreover, eight relations of the 22-SR set ( KINSHIP , DEPICTION , CAUSE , INSTRUMENT , 9.61% of the OTHER -SR relation represents the ratio of those instances that did not encode any of the 22 semantic relations .It is interesting to note here the large difference between the number of types versus tokens for the TYPE relation in Europarl .This is accounted for by various N N instances such as member states that repeat across the corpus.
 22 semantic relations .Moreover, in CLUVI 256 instances were tagged with more than (e.g., a couple of cigarettes ), 28.2% PA RT  X  WHOLE / LOCATION (e.g., bottom of the sea ), 10.9%
MEASURE / LOCATION (e.g., cup of chocolate ), 8.2% PURPOSE / LOCATION (e.g., den ), and 5.9% THEME / MAKE -PRODUCE (e.g., makers of songs ) .In Europarl, on the other hand, there were only 97 such cases: 81.4% THEME / MAKE -PRODUCE (e.g., bus manufac-turers ) and 18.6% MEASURE / PA RT  X  WHOLE (e.g., number of states ).
 to the interpretation task is to look at their distribution over the set of semantic relations on two reasonably large text corpora of different genres .Of course, this approach does not provide an analysis that generates an exhaustive generalization over the properties of the language .However, as Tables 6 and 7 show, there are dependencies between the structure of the Romance language translations and the semantic relations encoded by the nominal phrases and compounds, although the most frequently occurring preposi-tions are de and its English equivalent of .Here we use the preposition de to represent a set of translation equivalents in Romance languages (e.g., the Italian counterpart is di ).
These prepositions are semantically underspecified, encoding a large set of semantic relations .The many-to-many mappings of the prepositions to the semantic classes adds to the complexity of the interpretation task .For example, in the Europarl corpus LOCATION is encoded in French by de, sur, devant ,and ` apr` es de , while in English by of , for , on , about and noun compounds, and in Spanish by de , sobre , en materia de . 98.7% use the preposition of and its Romance equivalent de .In the CLUVI corpus, 14.1% of the examples were verbal, from which the preposition of/de has a coverage of 77.66%.

Matushansky, and Ruys 2006) and our own observations, the preposition of/de in both root and synthetic nominal phrases may have a functional or a semantic role, acting as a linking device with no apparent semantic content, or with a meaning of its own.
Thus, for the interpretation of these constructions a system must rely on the meaning of preposition and the meaning of the two constituent nouns in particular, and on context 202 in general .Because the two corpora used in this paper contain both root and synthetic instances, we employed two semantic resources for this task: WordNet noun semantic classes and a collection of verb classes in English that correspond to special types of nominalizations .These resources are defined in Section 4 .2 .Moreover, in Section 6 we present a detailed linguistic analysis of the prepositions of in English and de in
Romance languages, and show how their selection correlates with the meaning of the construction. 4 .Model 4.1 Mathematical Formulation
Given the syntactic constructions considered, the goal is to develop a procedure for the automatic annotation of the semantic relations they encode .The semantic relations derive from various lexical and semantic features of each instance.
 formulated as a learning problem, and thus benefits from the theoretical foundation and experience gained with various learning paradigms .The task is a multi-class clas-sification problem since the output can be one of the semantic relations in the set .We cast this as a supervised learning problem where input/output pairs are available as training data.
 properties that describe the instance, usually not numerical) into feature vectors .Let us define x i as the feature vector of an instance i and let X be the space of all instances; that is, x i  X  X .

X into a semantic space S , f : X  X  S , where S is the set of semantic relations from Table 1, namely, r j  X  S , where r j is a semantic relation.
 l is the number of examples x each accompanied by its semantic relation label r .The problem is to decide which semantic relation to assign to a new, unseen example x
In order to classify a given set of examples (members of X ), one needs some kind of measure of the similarity (or the difference) between any two given members of X . instances along with their translations in the Romance languages, plus a set of extra-linguistic features .The output is a set of learning rules that classify the data based on the set of 22 semantic target categories .The learning procedure is supervised and takes into consideration the cross-linguistic lexico-syntactic information gathered for each instance. 4.2 Feature Space
The set of features allows a supervised machine learning algorithm to induce a function that can be applied to accurately classify unseen instances .Based on the study of the instances and their semantic distribution presented in Section 3, we have identified and experimented with the following features presented subsequently for each language in-volved .Features F1 X  X 5 have been employed by us in our previous research (Moldovan et al .2004; Girju et al .2005; Girju, Badulescu, and Moldovan 2006) .All the other features are novel.
 A .English features
F1 and F2. Semantic class of noun specifies the WordNet sense of the head noun (F1), and the modifier noun (F2) and implicitly points to all its hypernyms .The semantics of the instances of nominal phrases and compounds is heavily influenced by the meaning of the noun constituents .One such example is family#2 car#1 , which encodes a POSSESSION relation .The hypernyms of the head noun car#1 are: 204 vehicle } ... { entity } (cf .WordNet 2 .1) .These features will help generalize over the se-mantic classes of the two nouns in the instance corpus.

F3 and F4. WordNet derivationally related form specifies if the head noun (F3), and the modifier noun (F4) are related to a corresponding verb in WordNet .WordNet contains information about nouns derived from verbs (e.g., statement derived from to state ; cry from to cry ; death from to die ).

F5. Prepositional cues link the two nouns in a nominal phrase .These can be either simple or complex prepositions such as of or according to . In case of N N instances (e.g., member state ), this feature is  X  X  X .

F6 and F7. Type of nominalized noun indicates the specific class of nouns the head (F6) or modifier (F7) belongs to depending on the verb from which it derives .First, we check if the noun is a nominalization or not .For English we used the NomLex-Plus dictionary of nominalizations (Meyers at al .2004) to map nouns to corresponding verbs . example is the destruction of the city , where destruction is a nominalization .F6 and F7 may overlap with features F3 and F4 which are used in case the noun to be checked has no entry in the NomLex-Plus dictionary.
 on the possible set of relations the instance can encode .They take the following values: a) active form nouns, b) unaccusative nouns, c) unergative nouns, and d) inherently passive nouns .We present them in more detail subsequently . a .Active form nouns are derived through nominalization from psych verbs and rep-resent states of emotion, such as love, fear, desire , and so forth .They have an intrinsic active voice predicate X  X rgument structure and, thus, resist passivisation .For example, we can say the desire of Anna ,butnot the desire by Anna .This is also explained by the fact that in English the AGENT or EXPERIENCER relations are mostly expressed by the clitic genitive  X  X  (e.g., Anna X  X  desire ) and less or never by N P N constructions.
Citing Anderson (1983), Giorgi and Longobardi (1991) mention that with such nouns that resist passivisation, the preposition introducing the internal argument, even if genitive case .Moreover, they argue that the meaning of these nouns might pattern differently in different languages .Consider for example the Italian sentences (4) and (5) below and their English equivalents (see Giorgi and Longobardi 1991, pages 121 X  122) .In English the instance Anna X  X  desire identifies the subject of desire (and thus encodes an EXPERIENCER relation), whereas in Italian it can identify either the subject ( EXPERIENCER ) as in Example (4), or the object of desire ( THEME ) as in Example (5), the disambiguation being done at the discourse level .In Example (6) the prenominal construction il suo desiderio encodes only EXPERIENCER . (4) Il desiderio di Anna fu esaudito .( EXPERIENCER ) (5) Il desiderio di Anna lo porter ` a alla rovina .( THEME ) (6) Il suo desiderio fu esaudito .( EXPERIENCER )
However, our observations on the Romanian training instances in Europarl and CLUVI (captured by features F12 and F13 below) indicate that the choice of syntactic construc-tions can help in the disambiguation of instances that include such active nouns .Thus, whereas genitive-marked N N compounds identify only the subject (thus encoding EXPERIENCER ), the N de/pentru N constructions identify only the object (thus encoding THEME ) .Such examples are dorint  X a Anei ( X  X esire-the Anna-
PERIENCER )and dorint  X a de/pentru Ana ( X  X esire-the of/for Anna X   X  the desire for Anna ) ( THEME ).
 are the recipients of love, not its experiencers .In Italian the instance translates as l X  X more per i bambini ( X  X he love for the children X ), whereas in Romanian it translates as dragostea pentru copii ( X  X ove-the for children X ) .These nouns mark their internal argument through of in English and most of the time require prepositions such as for in Romance languages and vice versa. b .Unaccusative nouns are derived from ergative verbs that take only internal ar-example, the transitive verb to disband allows the subject to be deleted as in the following sentences: (7) The lead singer disbanded the group in 1991. (8) The group disbanded.

Thus, the corresponding unaccusative nominalization of to disband , the disbandment of the group , encodes THEME and not AGENT . c .Unergative nouns are derived from intransitive verbs .They can take only AGENT semantic relations .One such case is exemplified in the instance l X  X rrivo della cavalleria in
Italian which translates in English as the arrival of the cavalry and in Romanian as sorirea cavaleriei ( X  X rrival-the cavalry-GEN  X ). d .Inherently passive nouns . These nouns, like the verbs they are derived from, assume an implicit AGENT relation and, being transitive, associate to their internal argument the THEME relation .One such example is the capture of the soldier which translates in
Italian as la cattura del soldato ( X  X he capture of the soldier X ), la capture du soldat in French ( X  X he capture of soldier X ), and la captura de soldado in Spanish and Portuguese ( X  X he capture of soldier X ), where the nominalization capture ( cattura, capture, captura in Italian, French, and Spanish and Portuguese respectively) is derived from the verb to capture .
Here, whereas English and Italian, Spanish, Portuguese, and French use the N of/de N construction (as shown in Examples (9) and (10) for English and Italian), Romanian uses genitive-marked noun compounds .In Romanian, however, nominalizations are formed through suffixation, where a suffix is added to the root of the verb it comes from.
Different suffixes attached to the same verb may lead, however, to more than one nom-inalization, producing different meanings .The verb to capture ( a captura in Romanian), for example, can result through suffixation in two nominalizations: capturare (with the 206 infinitive suffix -are and encoding an implicit AGENT relation) and captur  X  a (through zero derivation and encoding an implicit THEME relation) (Cornilescu 2001) .Thus, the noun phrase capturarea soldatului ( X  X apture-the soldier-GEN  X ) encodes a THEME relation, while captura soldatului ( X  X apture-the soldier-GEN  X ) encodes an AGENT relation .In all the
Romance languages with the exception of Romanian, this construction is ambiguous, unless the AGENT is explicitly stated or inferred as shown in Example (9) for Italian.
The same ambiguity might occur sometimes in English, with the difference that besides the of -genitive, English also uses the s -genitive: the soldier X  X  capture ( AGENT is preferred if the context doesn X  X  mention otherwise), the soldier X  X  capture by the enemy ( THEME ), the capture of the soldier ( THEME is preferred if the context doesn X  X  mention otherwise), capture of the soldier by the enemy ( THEME ). (9) La cattura del soldato (da parte del nemigo) ` e cominciata come un atto terroristico. (10) La sua cattura ` e cominciata come un atto terroristico .( THEME ) previously, the object of inherently passive nouns can move to the subject position as in the soldier X  X  capture by the enemy , whereas it cannot do so for active form nouns (e.g., * Anna X  X  desire by John ) .Similarly, in Italian, although active form nouns allow only the subject reading in prenominal constructions (e.g., il suo desiderio  X   X  X er desire X ), inher-ently passive nouns allow only the object reading (e.g., la sua cattura  X   X  X is capture X ). logical patterns presented in Cornilescu (2001).
 mation on subcategorization frames and thematic roles of the verbs in VerbNet (Kipper,
Dang, and Palmer 2000) .VerbNet is a database which encodes rich lexical information for a large number of English verbs in the form of subcategorization information, selectional restrictions, thematic roles for each argument of the verb, and alternations (the syntactic constructions in which the verb participates).
 B .Romance features
F8, F9, F10, F11, and F12. Prepositional cues that link the two nouns are extracted from each translation of the English instance: F8 (Sp.), F9 (Fr.), F10 (It.), F11 (Port.), and F12 (Ro.). These can be either simple or complex prepositions (e.g., de , in materia de [Sp.]) in all five Romance languages, or the Romanian genitival article a/al/ale .For N N instances, this feature is  X  X  X .

F13. Noun inflection is defined only for Romanian and shows if the modifier noun in N N instances is not inflected or is inflected and modifies the head noun which is or is not a nominalization .This feature is used to help differentiate between instances encoded by genitive-marked N N constructions and noun X  X oun compounds, when the choice of syntactic construction reflects different semantic content .Two such examples are the noun X  X oun compound lege cadru (law framework) ( TYPE ) which translates as framework law and the genitive-marked N N instance frumuset  X ea fetei ( X  X eauty-the girl-
ERTY ) meaning the beauty of the girl .It also covers examples such as capturarea soldatului ( X  X apture-the soldier-GEN  X ), where the modifier soldatului is inflected and the head noun capturarea is a nominalization derived through infinitive suffixation.
 of the soldiers . (11) The instance the capture of the soldiers has the following Romance translations: captura dos soldados; capturarea soldat  X ilor ; THEME .

Its corresponding feature vector is: mod-inflected-inf-nom ; THEME , manian translation capturarea soldat  X ilor ( X  X apture-the soldiers-GEN  X ) is inflected and that the head noun capturarea is an infinitive nominalization. 4.3 Learning Models
Several learning models can be used to provide the discriminating function f .We have experimented with the support vector machines model and compared the results against two state-of-the-art models: semantic scattering, a supervised model described in Moldovan et al .(2004), Girju et al .(2005), and Moldovan and Badulescu (2005), and Lapata and Keller X  X  Web-based unsupervised model (Lapata and Keller 2005).
 7:3 training X  X esting ratio .All the test nouns were tagged with the corresponding sense in context using a state-of-the-art WSD tool (Mihalcea and Faruque 2004) .The default semantic argument frame for each relation was used in the automatic identification of the argument positions.
 A .Support vector machines
Support vector machines (SVMs) are a set of related supervised learning methods used for creating a learning function from a set of labeled training instances .The function category X?), or it can be a general regression function .For classification, SVMs operate by finding a hypersurface in the space of possible inputs .This hypersurface will attempt to split the positive examples from the negative examples .The split will be chosen to have the largest distance from the hypersurface to the nearest of the positive and negative examples .Intuitively, this makes the classification correct for testing data that is similar but not identical to the training data.
 classifier for each pair of classes (a total of C 2 n classifiers), and then we used a voting procedure to establish the class of a new example .For the experiments with semantic relations, the simplest voting scheme has been chosen; each binary classifier has one vote, which is assigned to the class it chooses when it is run .Then the class with the largest number of votes is considered to be the answer .The software used in these experiments is the package LIBSVM ( http://www.csie.ntu.edu.tw/ which implements an SVM model .We tested with the radial-based kernel . 208 the corresponding features, we had to prepare them for the SVM model .The set-up procedure is now described.
 Corpus set-up for the SVM model:
The processing method consists of a set of iterative procedures of specialization of the examples on the WordNet IS -A hierarchy .Thus, after a set of necessary specializa-tion iterations, the method produces specialized examples which through supervised machine learning are transformed into sets of semantic rules for the semantic interpre-tation of nominal phrases and compounds .The specialization procedure is described subsequently.
 at the end of Section 4.2 (Example [11]). Note that for the English instances, each noun constituent was expanded with the corresponding WordNet top semantic class .At this point, the generalized training corpus contains two types of examples: unambiguous and ambiguous .The second situation occurs when the training corpus classifies the same noun X  X oun pair into more than one semantic category .For example, both rela-tionships chocolate#2 cake#3 ( PA RT  X  WHOLE )and chocolate#2 article#1 ( TOPIC ) are mapped into the more general type entity # 1, entity # 1, PA RT  X  WHOLE / TOPIC . specialize these examples to eliminate the ambiguity .By specialization, the semantic class is replaced with the corresponding hyponym for that particular sense, that is, the concept immediately below in the hierarchy .These steps are repeated until there are no more ambiguous examples .For this example, the specialization stops at the first hyponym of entity : physical entity (for cake )and abstract entity (for article ) .For the unambiguous examples in the generalized training corpus (those that are classified with a single semantic relation), constraints are determined using cross-validation on the SVM model.
 B .Semantic scattering
The semantic scattering (SS) model was initially tested on the classification of genitive constructions, but it is also applicable to nominal phrases and compounds (Moldovan et al .2004) .SS is a supervised model which, like the SVM model described previously, reliesonWordNet X  X  IS -A semantic hierarchy to learn a function which separates positive and negative examples .Essentially, it consists of using a training data set to establish a boundary G  X  on WordNet noun hierarchies such that each feature pair of noun X  X oun senses f ij on this boundary maps uniquely into one of a predefined list of semantic relations .The algorithm starts with the most general boundary corresponding to the entity WordNet noun hierarchy and then specializes it based on the training data until a good approximation is reached. 14 Any feature pair above the boundary maps into more than one semantic relation .Due to the specialization property on noun hierarchies, feature pairs below the boundary also map into only one semantic relation .For any new pair of noun X  X oun senses, the model finds the closest boundary pair which maps to one semantic relation.
 for modifier noun and, respectively, head noun .A pair of &lt; modifier, head &gt; nouns maps semantic relation r given the feature pair f ij , P ( r | the number of occurrences of a relation r in the presence of the feature pair f number of occurrences of the feature pair f ij in the corpus .The most probable semantic relation  X  r is ing on the level of abstraction of f ij two cases are possible: relation r for which P ( r | f ij ) = 1 and 0 for all the other semantic relations. relations for which P ( r | f ij ) = 0 .In this case Equation (1) is used to find the most appro-priate  X  r .
 Definition
A boundary G  X  in the WordNet noun hierarchies is a set of synset pairs such that: Boundary Detection Algorithm Step 1. Create an initial boundary.

The initial boundary denoted G 1 is formed from combinations of the entity#1  X  entity#1 noun class pairs .For each training example a corresponding feature f first determined, after which it is replaced with the most general corresponding feature consisting of top WordNet hierarchy concepts .For example, both instances family#2 estate#2 ( POSSESSION )and the sister#1 of the boy#1 ( entity#1  X  entity#1 .At this level, the noun X  X oun feature encodes a number of semantic relations .For each feature, one can determine the most probable relation using Equa-tion (1) .For instance, the feature entity#1  X  entity#1 can be encoded by any of the 23 relations.
 of the ambiguous features .A feature f ij is ambiguous if it corresponds to more than one relation and its most relevant relation has a conditional probability less than 0.9. 210
To eliminate irrelevant specializations, the algorithm specializes only the ambiguous classes that occur in more than 1% of the training examples.
 correspond more than one semantic relation, then replacing these features with their hyponym synsets .Thus one feature breaks into several new specialized features .
For example, the feature entity#1  X  entity#1 generated through generalization for the examples family#2 estate#2 and the sister#1 of the boy#1 is specialized now as kin group#1  X  real property#1 and female sibling#1  X  male person#1 corresponding to the relations that were attached to f ij will be  X  X cattered X  across the new specialized features which form the second boundary .The probability of the semantic relations that are encoded by these specialized features is recalculated again using Equation (1) .The number of relations encoded by each of this boundary X  X  features is less than the one for the features defining the previous boundary .This process continues until each feature has only one semantic relation attached .Each iteration creates a new boundary . Step 2. Test the new boundary.

The new boundary is more specific than the previous boundary and it is closer to the ideal boundary .One does not know how well it behaves on unseen examples, but the goal is to find a boundary that classifies these instances with high accuracy .Thus, the boundary is first tested on only 10% of the annotated examples (different from the 10% of the examples used for testing) .If the accuracy is larger than the previous boundary X  X  accuracy, the algorithm is converging toward the best approximation of the boundary and thus it repeats Step 2 for the new boundary .If the accuracy is lower than the previous boundary X  X  accuracy, the new boundary is too specific and the previous boundary is a better approximation of the ideal boundary.
 C .Lapata and Keller X  X  Web-based unsupervised model
Lauer (1995) was the first to devise and test an unsupervised probabilistic model for noun X  X oun compound interpretation on Grolier X  X  Encyclopedia, an eight million word corpus, based on a set of eight preposition paraphrases .His probabilistic model com-putes the probability of a preposition p given a noun X  X oun pair n the most likely preposition paraphrase p  X  = argmax p noticed, this model requires a very large training corpus to estimate these proba-bilities .More recently, Lapata and Keller (2005) replicated the model using the Web as training corpus and showed that the best performance was obtained with the trigram model f ( n 1 , p , n 2 ) .In their approach, they used as the count for a given trigram the num-ber of pages returned by using the trigram as a query .These co-occurrence frequencies were estimated using inflected queries which are obtained by expanding a noun X  X oun compound into all its morphological forms; then searching for N P N instances, for each of the eight prepositions P in Lauer X  X  list .All queries are performed as exact matches using quotation marks .For example, for the test noun X  X oun compound instance war sto-ries , all possible combinations of definite/indefinite articles and singular/plural noun forms are tried resulting in the queries story about war , a/the story about war , story about and so on .These forms are then submitted as literal queries, and the resulting hits are summed up .The query, and thus the preposition, with the largest number of hits is selected as the correct semantic interpretation category. periments using Google. 15 We formed inflected queries with the patterns they proposed and searched the Web. 5 .Experimental Results
We performed various experiments on both the Europarl and CLUVI testing corpora using seven sets of supervised models .Table 8 shows the results obtained against SS and Lapata and Keller X  X  model on both corpora and the contribution of the features exemplified in seven versions of the SVM model .Supervised models 1 and 2 are defined only for the English features .Here, features F1 and F2 measure the contribution of the
WordNet IS -A lexical hierarchy specialization .However, supervised model 1, which is also the baseline, does not differentiate between unambiguous and ambiguous training examples and thus does not specialize those that are ambiguous .These models show the difference between SS and SVM and the contribution of the other English features, such as preposition and nominalization (F1 X  X 7).
 than for CLUVI .For the supervised models 1 and 2, SS [F1 + F2] gives better re-sults than SVM [F1 + F2] .The inclusion of the other English features (SVM [F1 X  X 7]) adds more than 10% accuracy (with a higher increase in Europarl) for the supervised model 1.
 number of correctly labeled instances over the number of instances in the test set). 5.1 The Contribution of Romance Linguistic Features
Our intuition is that the more information we use from other languages for the interpre-tation of an English instance, the better the results .Thus, we wanted to see the impact of each Romance language on the overall performance .Supervised model 3 shows the results obtained for English and the Romance language that contributed the least to the performance (English and Spanish for the entire English feature subset F1 X  X 8) .Here we computed the performance on all five English X  X omance language combinations and chose the Romance language that provided the best result .Thus, supervised models 3 through 7 add Spanish, French, Portuguese, Italian, and Romanian in this order and show the contribution of each Romance preposition and all English features.
 have a different contribution to the overall performance .Whereas the addition of
Portuguese in CLUVI decreases the performance, in Europarl it increases it, if only by a few points .However, a closer analysis of the data shows that this is mostly due to the distribution of the corpus instances .For example, French, Italian, Spanish, and
Portuguese are consistent in the choice of preposition (e.g., if the preposition de [ of ]is used in French, then the corresponding preposition is used in the other four language translations) .A notable exception here is Romanian which provides two possible con-structions with almost equal distribution: the N P N and the genitive-marked N N .The table shows (in the increase in performance between supervised models 6 and 7) that 212 this choice is not random, but influenced by the meaning of the instances (features F12,
F13) .This observation is also supported by the contribution of each feature to the overall performance .For example, in Europarl, the WordNet verb and nominalization features of the head noun (F3, F6) have a contribution of 5.12%, whereas for the modifier nouns they decrease by about 2.7%. The English preposition (F5) contributes 6.11% (Europarl) and 4.82% (CLUVI) to the overall performance.
 preposition de ( of ), encoding almost all of the 22 semantic relations .The many-to-many mappings of the preposition to the semantic classes adds to the complexity of the interpretation task .A closer look at the Europarl and CLUVI data shows that Lauer X  X  set of eight prepositions represents 88.2% (Europarl) and 91.8% (CLUVI) of the
N P N instances .From these, the most frequent preposition is of with a coverage of 79% (Europarl) and 88% (CLUVI) .Because the polysemy of this preposition is very high, we wanted to analyze its behavior on the set of most representative semantic relations in both corpora .Moreover, we wanted to see what prepositions were used to translate the
English nominal phrase and compound instances in the target Romance languages, and thus to capture the semantic (ir)regularities among these languages in the two corpora and their contribution to the semantic interpretation task.

Romance languages in terms of the prepositions used .This behavior can be classified roughly in four categories exemplified subsequently: Example (12) shows a combination of the preposition of/de and more specific prepositions; Example (13) shows different prepositions than the one corresponding to the English equivalent in the instance; and
Examples (14) and (15) show corresponding translations of the equivalent preposi-tion in English in all Romance languages with variations in Romanian (e.g., de for of , para/pour/par/pentru for for ). (12) Committee on Culture (En.)  X  Comisi  X  on de la Cultura (Sp.)  X  commission de la (13) the supervision of the administration (En.)  X  control sobre la administraci  X  on (14) lack of protection (En.)  X  falta de protecci  X  on (Sp.)  X  manque de protection (Fr.)  X  (15) the cry of a man (En.)  X  el llanto de un hombre (Sp.)  X  un cri d  X  X omme (Fr.)  X  their instances .Most of the time Spanish, French, Italian, and Portuguese make use of specific prepositions such as those in Examples (12) and (13) to encode some semantic relations such as PURPOSE and LOCATION ,butrelyonN de N constructions for almost all the other relations .English and Romanian, however, can choose between N N and
N P N constructions .In the next section we present in more detail an analysis of the semantic correlations between English and Romanian nominal phrases and compounds and their role in the semantic interpretation task. 6 .Linguistic Observations
In this section we present some linguistic observations derived from the analysis of the system X  X  performance on the CLUVI and Europarl corpora .More specifically, we present different types of ambiguity that can occur in the interpretation of nominal phrases and compounds when using more abstract interpretation categories such as
Lauer X  X  eight prepositions .We also show that the choice of syntactic constructions in English and Romanian can help in the identification of the correct position of the semantic arguments in test instances. 6.1 Observations on Lapata and Keller X  X  Unsupervised Model
In this section we show some of the limitations of the unsupervised probabilistic ap-proaches that rely on more abstract interpretation categories, such as Lauer X  X  set of eight prepositions .For this, we used Lapata and Keller X  X  approach, a state-of-the-art knowledge-poor Web-based unsupervised probabilistic model which provided a per-formance of 42.12% on Europarl and 41.10% on CLUVI. We manually checked the first 214 five entries of the pages returned by Google for each most frequent N P N paraphrase for 100 CLUVI and Europarl instances and noticed that about 35% of them were wrong due to syntactic (e.g., part of speech) and/or semantic ambiguities. For example, baby cry generated instances such as  X  X t will make moms crywiththebaby , X  where cry is a verb, not a noun .This shows that many of the NP instances selected by Google as matching the N P N query are incorrect, and thus the number of hits returned for the query is over-estimated .Thus, because we wanted to measure the impact of various types of noun X  noun compound ambiguities on the interpretation performance, we further tested the probabilistic Web-based model on four distinct test sets selected from Europarl, each containing 30 noun X  X oun compounds encoding different types of ambiguity: In Set#1 the noun constituents had only one part of speech and one WordNet sense; in Set#2 the nouns had at least two possible parts of speech and were semantically unambiguous; in
Set#3 the nouns were ambiguous only semantically; and in Set#4 they were ambiguous both syntactically and semantically .Table 9 shows that for Set#1, the model obtained an accuracy of 35.28%, while for more semantically ambiguous compounds it obtained an average accuracy of about 48% (50.63% [Set#3] and 43.25% [Set#4]. This shows that for more syntactically ambiguous instances, the Web-based probabilistic model introduces a significant number of false positives, thus decreasing the accuracy (cf .sets #1 vs .#2 and #3 vs .#4) .
 showed that about 30% of the noun X  X oun compounds in sets #3 and #4 were ambiguous with at least two possible readings .For example, paper bag can be interpreted out-of-context both as bag of paper (bag made of paper X  STUFF  X  OBJECT ,asubtypeof PA RT  X  WHOLE )andas bag for papers (bag used for storing papers X  larly, gingerbread bowl can be correctly paraphrased both as bowl of/with gingerbread ( CONTENT  X  CONTAINER )andas bowl of gingerbread (bowl made of gingerbread X  STUFF  X  pound gingerbread bowl as found on Google: (16) Stir a bowl of gingerbread , (17) The gingerbread will take the shape of the glass bowl .Let it cool for a few and #4 are higher then the ones obtained for the other two sets .The semantic ambiguity also explains why the accuracy obtained for set #2 is higher than that for set #4 .For these sets of examples the syntactic ambiguity affected the accuracy much less than the se-mantic ambiguity (that is, more NPN combinations were possible due to various noun senses) .This shows one more time that a large number of noun X  X oun compounds are covered by more abstract categories, such as prepositions .Moreover, these categories also allow for a large variation as to which category a compound should be assigned. 6.2 Observations on the Symmetry of Semantic Relations: A Study on English and Romanian
Nominal phrases and compounds in English, nominal phrases in the Romance lan-guages considered here, and genitive-marked noun X  X oun compounds in Romanian have an inherent directionality imposed by their fixed syntactic structure .For example, in English noun X  X oun compounds the syntactic head always follows the syntactic modifier, whereas in English and Romance nominal phrases the order is reversed .Two such examples are tea /Modifier cup /Head and glass /Head of wine /Modifier. however, is not fixed and thus it is not always the same as the inherent direc-tionality imposed by the syntactic structure .Two such examples are ham /Modifier/
Arg 2 sandwich /Head/ Arg 1 and spoon /Modifier/ Arg 1 handle /Head/ Arg both instances encode a PA RT  X  WHOLE relation ( Arg 1 tifying the whole and Arg 2 is the semantic argument identifying the part), their se-mantic arguments are not listed in the same order ( Arg
Arg 2 Arg 1 for ham sandwich ) .For a better understanding of this phenomenon, we performed a more thorough analysis of the training instances in both CLUVI and
Europarl .Because the choice of syntactic constructions in context is governed in part by semantic factors, we focused on English and Romanian because they are the only languages from the set considered here with two productive syntactic options: N N and N P N (English) and genitive-marked N N and N P N (Romanian) .Thus, we grouped the English X  X omanian parallel instances per each semantic relation and each syntactic construction and checked if the relation was symmetric or not, according to the following definition.
 Definition
We s a y t h a t a semantic relation is symmetric relative to a particular syntactic construction if there is at least one relation instance whose arguments are in a different order than the order indicated by the relation X  X  default argument frame for that construction.
 the semantic arguments of the instance the building/Arg 1 with parapets/Arg 2areina different order than the one imposed by the relation X  X  default argument frame ( Arg 2
P Arg 1) for nominal phrases (cf .Table 1) . 216 those relations encoded by at least 50 instances in both Europarl and CLUVI .For example, in English the POSSESSION relation is symmetric when encoded by N P N and noun X  X oun compounds .For instance, we can say the girl with three dogs and the resources of the Union ,butalso family estate and land proprietor .The findings are summarized and presented in Table 10 along with examples .Some relations such as IS -A , PURPOSE ,and (indicated by  X  X  X  in the table) .A checkmark symbol indicates if the relation is symmetric ( X   X ) or not ( X  X  X ) for a particular syntactic construction .It is interesting to note that not all the relations are symmetric and this behavior varies from one syntactic construction are symmetric irrespective of the syntactic construction used.
 nominal phrases and compounds because the system has to know which of the nouns of the semantic arguments has been manually identified and marked in the training corpora .However, this information is not provided for unseen test instances .So far, in our experiments with the test data the system used the order indicated by the default argument frames .Another solution is to build argument frames for clusters of prepositions which impose a particular order of the arguments in N P N constructions.
For example, in the N 2 P N 1 phrases the books on the table ( LOCATION )and relaxation during the summer ( TEMPORAL ), the semantic content of the prepositions identifies the position of the physical and temporal location (e.g., that N and TEMPORAL because in both English and Romance languages they rely mostly on prepositions indicating location and time and less on underspecified prepositions such as of or de .However, a closer look at these relations shows that some of the noun X  X oun pairs that encode them are not symmetric and this is true for both English and Romance.
For instance, cut on the chin and house in the city cannot be reversed as chin P cut or city P house .One notable exception here is indicated by examples such as box of/with matches  X  matches in/inside the box and vessels of/with blood  X  blood in vessels (e.g., the book under the folder and the folder on the book ) .However, even here symmetry is not always possible, being influenced by pragmatic factors (Herskovits 1987) (e.g., we can say the vase on the table ,butnot the table under the vase  X  X his has to do with the difference in size of the objects indicated by the head and modifier nouns .Thus, a larger object cannot be said to be placed under a smaller one).
 relations does not focus in particular on the symmetry of an instance noun X  X oun pair that encodes the relation, although it doesn X  X  exclude such a case .We call this lexical symmetry and define it here.
 Definition We s a y t h a t a noun X  X oun pair ( N 1 N 1 N 1  X  N 2 N 2 construction and the semantic relation it encodes in that construction if the order of the nouns in the construction can be changed provided the semantic relation is preserved. 218 parapets/Arg 2 and the parapets/Arg 2 of the building/Arg Here, both the noun X  X oun pair and the semantic relation are symmetric relative to
N P N .However, the situation is different for instances such as the book/Arg the folder/Arg 1 and the folder/Arg 2 on the book/Arg 1 , both encoding LOCATION .Here, the book  X  folder pair is symmetric in N P N constructions (in the first instance the book is the syntactic head and the folder is the modifier, whereas in the second instance the order is reversed) .However, the LOCATION relation they encode is not symmetric (in both instances, the order of the semantic arguments matches the default argument frame for paraphrases of one another .This can be explained by the fact that both the book and the folder can act as a location with respect to the other, and that the prepositions under and on are location antonyms .In comparison, the building with parapets is not a paraphrase of the parapets of the building .Here, the nouns building and parapets cannot act as a whole/part with respect to each other (e.g., the only possible whole here is the noun building , and the only possible part here is the noun parapets ) .This is because parts and wholes have an inherent semantic directionality imposed by the inclusion operation on the set of things representing parts and wholes, respectively.
 in nominal phrases and compounds, but we do not focus in particular on the acquisition of paraphrases in these constructions .Our goal is to build an accurate semantic parser which will automatically annotate instances of nominal phrases and compounds with semantic relations in context .This approach promises to be very useful in applications that require semantic inference, such as textual entailment (Tatu and Moldovan 2005).
However, a thorough analysis of the semantics of nominal phrases and compounds should focus on both semantic relations and paraphrases .We leave this topic for future research.
 we focused on PA RT  X  WHOLE .These relations, and most of the semantic relations con-sidered here, are encoded mostly by N of/de N constructions, genitive-marked N N (Romanian), and noun X  X oun compounds (English) and thus, the task of argument order identification becomes more challenging .For the purpose of this research we decided to take a closer look at the PA RT  X  WHOLE relation in both CLUVI and Europarl where together it accounted for 920 token and 636 type instances .We show subsequently a detailed analysis of the symmetry property on a classification of PA RT  X  WHOLE relations starting with a set of five PA RT  X  WHOLE subtypes identified by Winston, Chaffin, and
Hermann (1987): 18 (1) Component X  X ntegral object ,(2) Member X  X ollection ,(3) Portion X  X ass , (4) Stuff X  X bject ,and(5) Place X  X rea . (1) Component X  X ntegral object objects have a structure with their components being separable and having a functional relation with their wholes .This type of PA RT  X  WHOLE relation can be encoded by N of N and less often by N N constructions .Moreover, here the existential interpretation is preferred over the generic one .Such examples are the leg of the table and the table leg which translate in Romanian as piciorul mesei ( X  X eg-the table-generic interpretation is also possible, but with change of construction and most of the time of semantic relation (e.g., picior de mas  X  a  X   X  X eg of table X  encoding PURPOSE structions .In Romanian, however, it is symmetric only when encoded by N P N .
Moreover, it is interesting to note that Modifier/ Arg pound instances translate as genitive noun X  X oun compounds in Romanian, whereas
Modifier/ Arg 2 Head/ Arg 1 instances translate as N P N, with P different from of .For example, chair / Arg 1 arm / Arg 2 and ham / Arg 2 sandwich / Arg
Head/ Arg 2 Modifier/ Arg 1  X  brat  X ul scaunului ( X  X rm-the chair-GEN  X ) and Head/ Arg Modifier/ Arg 2  X  sandwich cu  X  sunc  X  a ( X  X andwich with ham X ).
 argument orderings are possible, but with a different choice of preposition (with P different from of/de ) .For example, one can say the parapets/Arg but also the building/Arg 1 with parapets/Arg 2 .A closer look at such instances shows that symmetry is possible when the modifier (in this case the part) is not a mandatory part of the whole, but an optional part with special features (e.g., color, shape). For example, the car with the door is less preferred than the car with the red door which differentiates the car from other types of cars. (2) Stuff X  X bject is partly or entirely made .The parts are not similar to the wholes which they compose, cannot be separated from the whole, and have no functional role .The relation can be encoded by both N of N and N N English and Romanian patterns and the choice between existential and generic interpretations correlates with the relation symmetry.
For N N constructions this relation subtype is not symmetric, while for N P N it is symmetric only in English .Such examples are brush/Arg metalul/Arg 2 scaunului/Arg 1 ( X  X etal-the seat-GEN  X  X  the metal of the seat )and scaun de metal ( X  X hair of metal X   X  metal chair ) in Romanian.
 manian .If the position of the arguments is Arg 1 of Arg 2 indicating the part then the instance interpretation is generic .For example, seat of metal translates as scaun de/din metal ( X  X hair of/from metal X ) in Romanian .It is important to note here the possible choice of the preposition from in Romanian, a preposition which is rarely used in English for this type of relation.
 tion of is used and the semantic relation is still STUFF  X  OBJECT , but the instance is more specific having an existential interpretation .For instance, the metal of the seat translates in
Romanian as metalul scaunului ( X  X etal-the seat-GEN  X ) and not as metalul de scaun ( X  X etal-the of seat X ). (3) Portion X  X ass observations on the CLUVI and Europarl data, this type of PA RT  X  WHOLE relation can be further classified into mass, measure, and fraction partitives. Here the parts are separable and similar to each other and to the whole they are part of .An example of a mass partitive is half/Arg 2 of the cake/Arg 1 which translates in Romanian as jum  X  atate/Arg 220 de/din prajitur  X  a/Arg 1 ( X  X alf of/from cake X ) .Note that here the noun cake is indefinite in
Romanian, and thus the instance is generic .An existential interpretation is possible when the noun is modified by a possessive (e.g., half of your cake ).
 because they can express both PA RT  X  WHOLE and MEASURE depending on the context.
They are encoded by N 1 of N 2 constructions, where N cate both existential and generic interpretations .Two such examples are bottles/Arg wine/Arg 2 and cup/Arg 1 of sugar/Arg 2 .In Romanian, the preposition used is either de ( of ), or cu ( with ) .For example, sticle/Arg 1 de/cu vin/Arg cea  X  sc  X  a/Arg 1 de/cu zah  X  ar/Arg 2 ( X  X up of/with sugar X ).
 pie/Arg 1 ( trei p  X  atrimi/Arg 2 de pl  X  acint  X  a/Arg 1 third/Arg 2 of the nation/Arg 1 ( o treime/Arg 2 din populat  X ia/Arg population-the X  X  and not o treime de populat  X ia  X  [ X  X  third of population-the X  X ) .Here again, we notice the choice of the Romanian preposition din and not de when the second noun is definite .The preposition from indicates the idea of separation of the part from the whole, an idea which characterizes PA RT  X  WHOLE relations.

Romanian and they are not symmetric in N P N constructions. (4) Member X  X ollection not play any functional role with respect to their whole .That is, compared with
Component X  X ntegral instances such as the knob of the door , where the knob is a round handle one turns in order to open a door, in an example like bunch of cats ,the cats don X  X  play any functional role to the whole bunch .
 team ), count partitives (e.g., two of these people ), fraction count partitives (e.g., two out of three workers ), and vague measure partitives (e.g., a number/lot/bunch of cats ).
Although the basic Member X  X ollection partitives are symmetric for N N (Romanian only) and N P N (English and Romanian), the other subtypes can be encoded only by
N P N constructions and are not symmetric in English or in Romanian .For example, the children/Arg 2 of the group/Arg 1 and children/Arg 2 group/Arg din grup/Arg 1 ( X  X hildren-the from group X ) and as grup/Arg children X ).
 oameni/Arg 1 ( X  X wo from these people X ) and doi/Arg 2 din trei lucr  X  atori/Arg workers X ), by always using the preposition din ( from ) instead of de ( of ) .On the other hand, vague measure partitives translate as un num  X  ar/Arg cats X ) and not as un num  X  ar din pisici ( X  X  number from cats X ) .Although all these subtypes need to have a plural modifier noun and are not symmetric, count partitives always have an existential interpretation, whereas fraction count and vague measure partitives have a generic meaning. (5) Location X  X rea within them .The parts are similar to their wholes, but they are not separable from them .Thus, this relation overlaps with the LOCATION relation .One such example is the surface/Arg 2 of the water/Arg 1 .Both nouns can be either definite or indefinite and the rela-tion is not symmetric when the part is a relational noun (e.g., surface , end ) .In Romanian, both N de N and genitive-marked N N constructions are possible: suprafat  X a/Arg apei/Arg 1 ( X  X urface-the water-GEN  X ) and suprafat  X   X  a/Arg The relation is symmetric only for N P N in both English and Romanian.
 accompanied by examples.
 tic constructions in English and Romanian X  X ore specifically, the preposition features for English (F5) and Romanian (F12) and the inflection feature for Romanian (F13) X  can be used to train a classifier for the identification of the argument order in nominal phrases and compounds encoding different subtypes of PA RT  X  WHOLE relations .For example, the argument order for Portion X  X ass instances can be easily identified if it is determined that they are encoded by N 2 of / de N 1 in English and Romanian and the head noun N 2 is identified as a fraction in the WordNet IS -A hierarchy, thus representing Arg (the part) .It is interesting to note here that all the other Member X  X ollection subtypes with the exception of the basic one are also encoded only by N of / de N, but here the order is reversed in both English and Romanian ( N 1 of / de N 2 identified as a collection concept in WordNet, represents the whole concept ( Arg into more specific subtypes for argument order identification .Thus, local classifiers can be trained for each subtype on features such as those mentioned herein and tested on unseen instances .However, for training this procedure requires a sufficiently large number of examples for each subtype of the semantic relation considered.
 222
Romanian correlates with the meaning of the instances encoded by such structures .In the next section we present a list of errors and situations that, currently, our system fails to recognize, and suggest possible improvements. 7 .Error Analysis and Suggested Improvements
A major part of the difficulty of interpreting nominal phrases and compounds stems from multiple sources of ambiguity .These factors range from syntactic analysis, to semantic, pragmatic, and contextual information and translation issues .In this section we show various sources of error we found in our experiments and present some possible improvements.
 A .Error analysis
Two basic factors are wrong part-of-speech and word sense disambiguation tags .Thus, if the syntactic tagger and WSD system fail to annotate the nouns with the correct senses, the system can generate wrong semantic classes which will lead to wrong conclusions.
Moreover, there were also instances for which the nouns or the corresponding senses of these nouns were not found in WordNet. There were 42.21% WSD and 6.7% POS tagging errors in Europarl and 54.8% and 7.32% in CLUVI. Additionally, 6.9% (Europarl) and 4.6% (CLUVI) instances had missing senses.
 biguation is not enough for relation detection and when access to a larger discourse context is needed .Various researchers (Sp  X  arck Jones 1983; Lascarides and Copestake 1998; Lapata 2002) have shown that the interpretation of noun X  X oun compounds, for example, may be influenced by discourse and pragmatic knowledge .This context may be identified at the level of local nominal phrases and compounds or sentences or at the document and even collection level .For example, a noun X  X oun compound modified by a relative clause might be disambiguated in the context of another argument of the same verb in the clause, which can limit the number of possible semantic relations .For instance, the interpretation of the instance museum book in the subject position in the following examples is influenced by another argument of the verbs bought in Exam-ple (18), and informed in Example (19): (18) the [ museum book ] TOPIC John bought in the bookshop at the museum (19) the [ museum book ] LOCATION that informed John about the ancient art to their usage in various visual contexts .For example, the instance nails in the box (cf.
Herskovits 1987) indicates two possible arrangements of the nails: either held by the box, or hammered into it .We cannot capture these subtleties with the current procedure even if they are mentioned in the context of the sentence or discourse.
 B .Suggested improvements
In this article we investigated the contribution of English and Romance prepositions to the task of interpreting nominal phrases and compounds, both as features employed approach would be to look into more detail at the functional X  X emantic aspect of these prepositions and to define various tests that would classify them as pure functional components with no semantic content or semantic devices with their own meaning. by N N and N P N patterns .A more general approach would extend the investigation to adjective X  X oun constructions in English and Romance languages as well.

English and Romance nominal phrases and compounds in both directions .Such an analysis might be also useful for machine translation, especially when translating into a language with multiple choices of syntactic constructions .One such example is tarro de cerveza ( X  X lass of beer X ) in Spanish which can be translated as either glass of beer ( MEASURE )or beer glass ( PURPOSE ) in English .The current machine translation language models do not differentiate between such options, choosing the most frequent instance in a large training corpus.
 learning methods, is the need for a large number of training examples .If a certain class of negative or positive examples is not seen in the training data (and therefore it is not captured by the classification rules), the system cannot classify its instances. Thus, the larger and more diverse the training data, the better the classification rules.
Moreover, each cross-linguistic study requires translated data, which is not easy to obtain in electronic form, especially for most of the world X  X  languages .However, more and more parallel corpora in various languages are expected to be forthcoming. 8 .Discussion and Conclusions
In this article we investigated the contribution of English and Romance prepositions to the interpretation of N N and N P N instances and presented a supervised, knowledge-intensive interpretation model.
 in several ways .We investigated the problem based on cross-linguistic evidence from a set of six languages: English, Spanish, Italian, French, Portuguese, and Romanian.
Thus, we presented empirical observations on the distribution of nominal phrases and compounds and the distribution of their meanings on two different corpora, based on two state-of-the-art classification tag sets: Lauer X  X  set of eight prepositions (Lauer 1995) and our list of 22 semantic relations .A mapping between the two tag sets was also provided .A supervised learning model employing various linguistic features was successfully compared against two state-of-the-art models reported in the literature. hope that the corpus investigations presented in this article provide new insight for the machine translation and multilingual question answering communities .The translation of nominal phrase and compound instances from one language to another is highly correlated with the structure of each language, or set of languages .In this article we measured the contribution of a set of five Romance languages to the task of semantic in-terpretation of English nominal phrases and compounds .More specifically, we showed that the Romanian linguistic features contribute more substantially to the overall per-formance than the features obtained for the other Romance languages .The choice of the Romanian linguistic constructions (either N N or N P N) is highly correlated with their meaning .This distinct behavior of Romanian constructions is also explained by the Slavic and Balkanic influences .An interesting future research direction would be to consider other Indo-and non Indo-European languages and measure their contribution to the task of interpreting nominal phrases and compounds in particular, and noun phrases in general. 224 Acknowledgments References 226 228
This article has been cited by:
