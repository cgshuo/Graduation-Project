 through low density regions, while respecting labels in the input space. approach for TSVM. It begins with minimizing an easy con vex object function, and then gradu-ally approximates the objecti ve of TSVM with more complicated functions. The solution of the Figure 1: Computation time of the proposed con vex relaxation approach for TSVM (i.e., CTSVM) examples. The Course data set is used, and the number of labeled examples is 20. TSVM by a semi-definite programming problem, which leads to a relaxation solution to TSVM the high computational cost and can only be applied to small sized data sets. method is to approximate the non-con vex optimization problem of TSVM by its dual problem. The adv antage of doing so is twofold: for TSVM. Section 5 sets out the conclusion. programming relaxation for TSVM.
 Let X = ( x unlabeled ones. We assume that the first l examples within X are labeled by y where y ` ( y 1 ; y 2 ; : : : ; y n ) 2 f X  1 ; +1 g n of TSVM is to estimate y by using both the labeled examples and the unlabeled ones. amounts to solv e the follo wing optimization problem: The prediction function can be formulated as f ( x ) = w &gt; x  X  b .
 Evidently , the abo ve problem is a non-con vex optimization problem due to the product term y first rewrite the abo ve problem into the follo wing form using the Lagrange Theorem: be further formulated as follo ws: where the operator  X  represents the element wise product.
 quadratic term yy &gt; by a linear variable. Based on the result that the set S problem in (2) as follo ws: where M approximation is often referred to as the Semi-Definite Programming (SDP) relaxation. As re-of ing TSVM into a con vex problem. Compared to the SDP relaxation approach, it is adv antageous efficient than the pre vious SDP relaxation. the case of hard mar gin, and then extending it to the case of soft mar gin. 3.1 Hard Mar gin TSVM in (1). The resulting formulism of TSVM becomes as (4) can be rewritten as follo ws: to ensure that no class tak es all the unlabeled examples: between the labeled data and the unlabeled data in their class assignment is small. To simplify the expression, we further define w = ( z ;  X  ) 2 R n +1 and P = ( I Then, the problem in (5) becomes: diction function, i.e., sign( w ) . This is because w Theor em 1. Given a sample X = f x where 1  X  l  X  n , the variable w that optimizes (7) can be calculated by where a = ( y l ; 0 n  X  l ; 0) 2 R n +1 , b = ( 0 l ; 1 n  X  l ; 0) 2 R n +1 , c = ( 1 A = P &gt; K  X  1 P , and  X  is determined by the follo wing semi-definite programming: Pr oof Sk etch . We define the Lagrangian of the minimization problem (7) as follo ws: where  X  max  X  min w F ( w ;  X  ) : At the optimum, the deri vatives of F with respect to the variable w are deri ved as belo w: parameter . Therefore, w is able to be calculated by: Thus, the dual form of the problem becomes: max We import a variable t , so that According to the Schur Complement, we obtain a semi-definite programming cone, from which the optimization problem (9) can be formulated.  X  approximation than the pre vious method.
 expressed as follo ws: It can be further deri ved as follo ws: where I i ement which is 1 , and K ( x P The key dif ference in our solution is that (1) dif ferent weights (i.e.,  X  examples, and (2) the adjustment factor is dif ferent to that in the harmonic function [16]. 3.2 Soft Mar gin TSVM We extend TSVM to the case of soft mar gin by considering the follo wing problem: where  X  unlabeled examples by introducing dif ferent penalty constants for mar gin errors, C examples and C Similarly , we introduce the slack variable z , and then deri ve the follo wing dual problem: which is also a semi-definite programming problem and can be solv ed similarly . 4.1 Data Sets Description denotes the total number of examples.
 number of labeled data points, and n denotes the total number of examples. 4.2 Experimental Pr otocol To evaluate the effecti veness of the proposed CTSVM method, we choose the con ventional SVM methods: the SVM-light algorithm [8], the Gradient Decent TSVM ( r TSVM) algorithm [5], and the Conca ve Con vex Procedure (CCCP) [6]. Since the SDP approximation TSVM [14 ] has very Thus, it is only evaluated on the smaller data sets, i.e.,  X  X BM-s X  and  X  X ourse-s X . labeled data. The mar gin parameter C the small number of labeled examples, for CTSVM and CCCP , the mar gin parameter for unlabeled data, C according to the rele vant literatures. 4.3 Experimental Results proposed CTSVM algorithm performs much better than other TSVM methods over  X  X  inMac-m X  the state-of-the-art methods. ing to the state-of-the-art methods.
 classify the new incoming data.
 No. CUHK4150/07E).
 [1] T. D. Bie and N. Cristianini. Con vex methods for transduction. In S. Thrun, L. Saul, and [2] O. Chapelle, M. Chi, and A. Zien. A continuation method for semi-supervised SVMs. In ICML [4] O. Chapelle, V. Sindhw ani, and S. Keerthi. Branch and bound for semi-supervised support [5] O. Chapelle and A. Zien. Semi-supervised classification by low density separation. In Pro-[7] J.-B. Hiriart-Urruty and C. Lemarechal. Con vex analysis and minimization algorithms II: [12] J. F. Sturm. Using SeDuMi 1.02, a MA TLAB toolbox for optimization over symmetric cones. [14] L. Xu and D. Schuurmans. Unsupervised and semi-supervised multi-class support vector ma-
