 Changyou Chen 1 , 3 Changyou.Chen@nicta.com.au Vinayak Rao 2 vrao@gatsby.ucl.ac.uk Wray Buntine 3 , 1 Wray.Buntine@nicta.com.au YeeWhye Teh 4 y.w.teh@stats.ox.ac.uk In recent years there has been growing interest in extending models for random probability measures (RPMs) like the Dirichlet process (DP) to dependent random probability measures (MacEachern, 1999). A popular class is the hierarchical Dirichlet process (HDP) (Teh et al., 2006), which introduces dependen-cies in an exchangeable set of DPs by having them share the same random base measure. A number of al-ternate approaches exist in the literature, many index-ing DPs by more structured sets, and allowing more refined control of dependency. Examples include (Sre-bro &amp; Roweis, 2005; Griffin &amp; Steel, 2006; Caron et al., 2007; Ahmed &amp; Xing, 2008; MacEachern et al., 2001; Gelfand et al., 2005). Of relevance to this paper is the spatial normalized Gamma process (Rao &amp; Teh, 2009), which exploits the representation of the DP as a normalized Gamma process, and constructs depen-dent DPs from overlapping restrictions of a common Gamma process. (Lin et al., 2010) considered addi-tional operations to introduce dependencies between DPs, viz. subsampling and perturbing atoms of a com-mon Gamma process.
 There has also been a growing body of work extend-ing the DP to more expressive RPMs. A flexible framework is the class of normalized random measures (NRMs) (James et al., 2009), which includes the DP, the normalized inverse Gaussian process and the nor-malized generalized Gamma process.
 In this paper we consider constructions for depen-dent random measures, all of which are marginally distributed as a specific NRM. We propose two ap-proaches which we call mixed normalized random mea-sures (MNRM) and thinned normalized random mea-sures (TNRM), and study these models using tools from the Poisson process partition calculus of (James, 2005). Our framework encompasses and extends work such as (Rao &amp; Teh, 2009), (Griffin et al., 2012), (Chen 2012), and is an alternative to (Williamson et al., 2010). One contribution of this work is a systematic comparison of these related models, and we find that on a number of real world datasets, the MNRM (a novel and simple model) perform best. Additionally, many of the listed works propose approximate poste-rior MCMC samplers; here we develop and compare two exact samplers, a marginal Gibbs sampler and a slice sampler. We find the latter preferable in most cases. We also provide faster approximations to this sampler, as well as bounds on the approximation error. Proofs are provided in the appendices in the supple-mentary material. In this section we provide a concise review of normal-ized random measures (NRMs). Consider a Poisson process N on a product space S = R +  X   X  , with inten-sity  X  ( w, X  ). A completely random measure (CRM) on  X  is defined as a linear functional of N : Here { ( w i , X  i ) } are the atoms of N . The Poisson in-tensity  X  ( w, X  ) is the density of the L  X evy measure of  X   X  (called L  X evy intensity), and is defined so that the total measure Z = R  X   X   X  (d  X  ) = R finite and positive almost surely. A CRM is so called because of its property that the random masses as-signed to disjoint sets are independent, this follows from the properties of the Poisson process (which it-self is a CRM) 1 . When  X  ( w, X  ) = Mw  X  1 e  X  w H (  X  ), for some probability density H , we get a homogeneous Gamma process with concentration parameter M and base distribution with density H . Other examples in-clude the stable process, the inverse Gaussian process and the generalized Gamma process, which has L  X evy intensity  X  ( w, X  ) = Mw  X   X   X  1 e  X  w H (  X  ) and which in-cludes the other examples as subclasses.
 The total mass is finite and nonzero. An NRM is ob-tained by normalizing  X   X  to a random probability mea-sure:  X  (d  X  ) = 1 Z  X   X  (d  X  ). From the Poisson construction, it follows that like the DP, an NRM is a discrete RPM with a countably infinite number of atoms: In many applications, one has observations at a finite collection of indices T = ( t 1 ,  X  X  X  ,t T ), we refer to the indices as  X  X imes X  from now on. The observations at each time t are modeled as i.i.d. draws from a random probability measure  X  t . By allowing dependencies in the measures  X  t , one allows the sharing of statistical information between observations at different times. We use the term dependent normalized random mea-sures (dNRMs) to refer to a dependent set of random measures {  X  t } , each distributed marginally as a NRM. Here we propose two approaches to model dependen-cies between the measures  X  t : mixed normalized ran-dom measures (MNRM) and thinned normalized ran-dom measures (TNRM). We start by defining R := { 1 ,...,R } for some positive integer R . We refer to the elements of R as regions, and define a collection of independent CRMs  X   X  r with L  X evy intensity  X  r ( w, X  ) for each r  X  R . At a high level, for each time t , our approach involves transforming and combining the CRMs  X   X  r (the nature of the transformation differing for MNRM and TNRM) 2 . This forms a new CRM  X   X  t at each t , which is then normalized to give probability measure  X  t . The shared regions make the CRMs {  X   X  t } , and thus the NRMs {  X  t } dependent. In the following, we detail the operations used in the constructions. 3.1. Mixed Normalized Random Measures In our first construction, the CRM at time t is a weighted combination of the independent CRMs  X   X  r Let q rt be a nonnegative weight between region r and time t . We define  X  t simply as follows:  X  t (d  X  ) = where  X   X  t (  X  ) = Z t is the normalizing constant at time t . Note in particular that  X  t is a mixture of the indi-vidual region-specific NRMs  X  r , with mixing weights given by q rt  X   X  r ( X ) /  X   X  t (  X  ). We then have: Proposition 1 Conditioned on the q rt  X  X , each ran-dom probability measure  X  t defined in (3) is marginally distributed as a NRM with L  X evy intensity P This result follows from the facts that 1) a scaled CRM is still a CRM, and 2) a sum of independent CRMs is still a CRM. See Appendix B for a detailed proof using characteristic functionals of the CRMs. In our experiments, we placed independent Gamma priors on the q rt  X  X , and inferred their values from the data. 3.1.1. Comparison with related work The spatial normalized Gamma process (SNGP) of (Rao &amp; Teh, 2009) is a special case of MNRM, with the weights fixed to be binary (i.e. q rt  X  { 0 , 1 } , with the actual value determined a priori ). Our MNRM is thus a generalization of the SNGP, from a normal-ized gamma process to a general NRM, and from fixed and binary q rt  X  X  to arbitrary positive values that will be inferred along with the rest of the model. On the other hand, the SNGP imposes a spatial structure to the q rt  X  X  which may allow better generalization. 3.2. Thinned Normalized Random Measures In our previous construction, a set of weights con-trolled the contribution of a set of CRMs to the NRM at any time, thus forming a  X  X oftening X  of (Rao &amp; Teh, 2009) (where each of the CRMs is either present or absent). Our second construction is a different gen-eralization of (Rao &amp; Teh, 2009); rather than includ-ing or excluding entire CRMs, we control whether or not individual atoms in each of the CRMs are present in the NRM at a given time. More precisely, to each region-time pair ( r,t ) we associate a parameter q rt tak-ing values in [0 , 1]. 3 q rt is the subsampling rate of the atoms in region r for time t , with each atom of re-gion r independently assigned to time t with probabil-ity q rt (otherwise it is thinned ). We call the resulting NRMs thinned normalized random measures (TNRM). Define a countably infinite sequence of Bernoulli vari-ables ( z rt 1 ,z rt 2  X  X  X  ) for each region-time pair: Then, the probability measure at time t is given by  X  t (d  X  ) = Again, we can show the  X  t  X  X  are marginally NRMs: Proposition 2 Conditioned on the set of q rt  X  X , each random probability measure  X  t defined in (4) is marginally distributed as a normalized random mea-sure with L  X evy measure P r q rt  X  r (d w, d  X  ) . The intuition behind this result is that independently thinning the atoms of a CRM maintains the property of complete randomness. Thus,  X   X  t is a CRM, and  X  t , which is obtained by normalizing it is an NRM. For a formal proof, see Appendix B. 3.2.1. Comparision with related work The idea of thinning atoms is similar to (Lin et al., 2010) for DPs and to (Chen et al., 2012) for NGGs, but these were restricted to random probability mea-sures with chain-structured dependence. In addition, posterior samplers developed in these prior works were approximate. The TNRM is also a generalization of a very recent work (Lin &amp; Fisher, 2012). This model is restricted to dependent DPs, and again, the pro-posed sampler has an incorrect equilibrium distribu-tion (more details in Section 4.2 and Appendix E). The TNRM is also related to an unpublished report by (Foti et al., 2012), where they focus on differ-ent thinning constructions of dependent CRMs. Our focus is on NRMs; the normalization provides addi-tional challenges. Their posterior inference is also ap-proximate, being based on truncated representations of the CRMs (which are restricted only to Beta and Gamma CRMs). Finally, the TNRM can be viewed as an alternative to the IBP compound Dirichlet Pro-cess (Williamson et al., 2010). These are finite dimen-sional probability measures constructed by selecting a finite subset of an infinite collection of atoms (via the Indian buffet process (IBP)). Our model allows this to be infinite, allowing it to be used as a convenient build-ing block in deeper hierarchical models. By treating the atoms present at each time as features, we can con-trast the TNRM with the Indian buffet process (Grif-fiths &amp; Ghahramani, 2011): in addition to allowing an infinite number of possible features, TNRM allows the number of active features to display phenomena like power-law behaviour; this is not possible in the IBP (Teh &amp; Gorur, 2009; Broderick et al., 2012). 3.2.2. Interpretation as Mixture of NRMs It is possible to represent the TNRM construction as a mixture of NRMs. Associate the k th atom in a re-gion r  X  R with a binary vector b r ( k ) of length T . b ( k ) = 1 means this atom is inherited by the NRM  X  t of time t (i.e. z trk = 1). Accordingly, we can split each region r into 2 T further subregions, each associ-ated with atoms with a particular configuration of b r . It is easy to see that with subregion b r = b r 1  X  X  X  b r region r is associated a CRM  X  G r b with L  X evy measure Q
Thus, the NRM at any time t can be expressed as a mixture of a number of NRMs G r b (d  X  ) =  X  G r b (d  X  ) / the number of times T . We can also see from this in-terpretation that TNRMs can be seen as fixed-weight (binary) MNRMs but with many more regions (2 T ). The number of components also grows linearly with the number of regions R ; we will see that this flexi-bility improves the performance of our model without too great an increase in complexity. In the following, we consider a specific NRM viz. the normalized generalized Gamma process (NGG) 4 to demonstrate posterior inference, generalization to other NRMs is straightforward. The generalized Gamma process (GGP) is a CRM whose L  X evy measure where 0 &lt;  X  &lt; 1 is known as the index parameter , M  X  R + is the mass parameter , and H (  X  ) is the base probability density. Normalizing this CRM gives a flexible class of NRMs called NGGs, which includes the DP as a special case, and is preferable in applica-tions where one wishes to place less informative priors on the number of clusters, power-law distributions on the cluster sizes etc. Its flexibility comes without a loss of computational tractability: the NGG is a so-called Gibbs-type prior, whose partition probability function (the clustering probability with the RPM integrated out) has a convenient closed form that generalizes the Chinese restaurant process (CRP) (see Appendix A.3). A consequence of this is that marginal samplers are available for both MNGG and TNGG. However, we saw in the previous section that the number of mix-ture components for TNGG grows exponentially with the number of times, and this can make the marginal sampler impractical. Consequently, we also develop slice samplers that instantiate the underlying RPMs. Since the marginal sampler for TNGG is impractical in most cases, and since the slice sampler for MNGG is an easy modification of the more complex slice sampler for TNGG, we move their descriptions to the appendix. In the following, T denotes the set of times with ob-servations, n trk denotes the number of observations from time t associated with the k -th atom in region r . The superscript in n \ tl trk indicates the previous count excluding the l th observation at time t . s tl indexes the atom to which observation l at time t is attached, and g tl indexes the corresponding region. Dots in the subscript denote sums over the corresponding index, for example, n  X  rk = P t n trk . For simplicity, denote N t = n t  X  X  . F ( x |  X  ) is the likelihood function. 4.1. The marginal sampler for MNGG For this sampler, we follow (James, 2005) and intro-duce a set of auxiliary variables u t  X  t  X  T , each con-ditionally distributed as a Gamma distribution with shape parameter N t , and inverse scale parameter Z t : malization constant corresponding to N t independent draws from  X  t (see equation (3)). Thus the variables u t allow us to move the normalization constants Z t from the denominator to the exponent. Now, integrating out the w  X  X  and Z t of each  X  t involves not much more that looking up the characteristic functional of a CRM, and it is not hard to see that p ( u t | others)  X  If we set v t = log( u t ) we get a log-concave function, allowing easy sampling of the u t  X  X  with a slice sam-pler (Neal, 2003). Additionally, conditioned on the u  X  X , the cluster (and simultaneously, region) assign-ment of each observation can be sequentially resam-pled by a generalization of the CRP: p ( s tl = k,g tl = r | others } )  X  (7) H (  X  rk )d  X  rk is the conditional density. Sampling the other variables is easy, see Appendix C.1.1 for details. 4.2. The slice sampler for TNGG Our second sampler is a conditional sampler that in-stantiates (rather than integrates out) the underlying RPM. Recall that the L  X evy measure in region r is  X  (d w, d  X  ), and that q rt is the subsampling rate for atoms from this region for time t . In addition to the atoms to which observations are assigned (call this set W ), we must also consider the remaining infinite atoms of each  X  r . The following result, involving the same auxiliary variables u t as before, tells us that these atoms are distributed as independent L  X evy processes: Proposition 3 Given observations associated with weights W , and auxiliary variables u t for each t  X  T , the remaining weights in region r are independent of W , and are distributed as a CRM with L  X evy measure  X  0 r (d w, d  X  ) = Y Proof See Appendix B, building on (James, 2005). Remark Proposition 3 indicates that conditioned on observations, the remaining weights are distributed as a CRM from a different class than the original. The marginal samplers in (Lin et al., 2010; Lin &amp; Fisher, 2012) implicitly assume these are the same, and are incorrect. We elaborate on this in the appendix. Now, we deal with the infinite atoms associated with  X  . We follow (Griffin &amp; Walker, 2011), and introduce auxiliary slice variables v tl for each observation x If x tl is assigned to atom s tl in region g tl , then v tl defined to be uniformly distributed in [0 ,w g sequently, conditioned on v tl , x tl can only be assigned to atoms with weights greater than w g properties of the NGG, this is a finite set, reducing posterior inference, conditioned on the v  X  X , to that of a finite mixture model. Thus, at each iteration, for each region r , we only have to simulate atoms ( w rk , X  with weights larger than the smallest v tl in that re-gion (in fact, we simulate weights larger than a smaller number L r , as we explain later). As detailed in Ap-pendix C.2.2, sampling the above variables (as well as indicators z rtk ) proceeds as follows:  X  Jointly sample { ( s tl ,g tl )  X  t,l } as:  X  Sample v tl uniformly on (0 ,w g  X  In each region r , we sample two kinds of w rk  X  X ,  X  Sampling z rtk : z rtk  X  X  are Bernoulli variables, they The remaining variables ( M r , u t and q rt ), can be sam-pled exactly using the pseudo-marginal Metropolis-Hastings algorithm (Andrieu &amp; Roberts, 2009), details in Appendix C.2.2. However by setting L r to a small value, we can sample from an accurate approximation and gain significant computational savings. In Ap-pendix C.2, we describe the exact sampler, perform a bound analysis on the approximation, and derive the approximate update rules listed below:  X  M r : the posterior of M r is a Gamma distri- X  u t : the posterior of u t is also Gamma  X  q rt : the posterior of q rt is approximately a Beta We did not find any significant difference in accuracy between this and the true sampler, although the com-putational benefits were significant. In the following, we applied our ideas to modelling text documents organized in time. We focused on six mod-els: MNGG, TNGG, HMNGG, HMNGP, HTNGG and HSNGG. The first two are based on the mixed and thinned constructions respectively, with each docu-ment is assigned to its own  X  X ime X , thus TNGG resem-bles focused topic models (Williamson et al., 2010). On one hand, this disregards statistical information that might be shared across documents from the same true time period, on the other hand, this affords more flexibility, since each document can have its own set of q rt parameters. Letting G be the Dirichlet distri-bution, F the multinomial distribution, and t span all documents in the corpus, the generative process is as follows: where dNGG (  X  0 ,M 0 ,G, { q rt } ) denotes the dependent NGG constructed via MNGG or TNGG with index parameter  X  , mass parameter M 0 , base distribution G and the set of weights/subsampling rates { q rt } . The remaining models specify the organization of doc-uments into time-periods by adding another layer to the hierarchy. In particular, we used our dNGG con-structions to produce an RPM  X  t for each time-period t ; each document in time period t then had a distri-bution over topics drawn from an NGG with base-measure  X  t , replacing (11) by Both HMNGG and HTNGG follow this construction, with the dependent NGGs produced by mixing and thinning respectively. HMNGP is the same as HM-NGG but with the NGG replaced with a Gamma process (GP). HSNGG denotes the spatial normalized generalized Gamma process (Rao &amp; Teh, 2009), a spe-cial case of HMNGG with q rt  X  { 0 , 1 } . We also com-pare our models with the popular hierarchical Dirichlet process (HDP), furthermore, we generalize the HDP to the HNGG (Appendix D), where the construction is the same as HDP but using NGGs instead of DPs. 5.1. Synthetic data In our first experiment, we generated 3000 observa-tions from a hierarchical Pitman-Yor topic model (Du et al., 2010). We set the vocabulary size to 100, and used the following generative process:
G 0  X  X Y (  X  0 ,d 0 ,G ) ,G t  X  X Y (  X  t ,d t ,G 0 ) t = 1 , 2 , 3  X  tj  X  G t , x tj  X  F (  X |  X  tj ) j = 1 ,  X  X  X  , 3000 The base measure G over topic distributions was a 100-dimensional symmetric Dirichlet with parameter 0 . 1, while F (  X |  X  ) is the 100-dimensional discrete distri-bution. The concentration parameters  X  i ,i = 0 ,  X  X  X  , 3 were set to 1 , 3 , 4 and 5 respectively, while all discount parameters d i were set to 0 . 5. Following the genera-tive process described above, we then split the data at each time into 30 documents of 100 words each, and modelled the resulting corpus using the HMNGG and HTNGG described in (12). The Pitman-Yor process (which is not an NRM) exhibits a power-law behavior, and the purpose of this experiment is to demonstrate the flexibility of the NGG over the DP. Accordingly, we compare the performance of HMNGG and HTNGG on this dataset against their dDP equivalences, the HM-NGP and the HTNGP (obtained by replacing the gen-eralized Gamma process with the Gamma process in the constructions). We set the number of regions equal to the number of times, and sampled all the model pa-rameters (placing Gamma(0 . 1 , 0 . 1) priors on all scalars in R + , and Beta(0 . 5 , 0 . 5) priors on all scalars in [0 , 1]). We plot the predictive likelihood on a 20% held-out dataset in Figure 2. We see that both HMNGG and HTNGG outperform their non-power-law variants HMNGP and HTNGP in terms of predictive likeli-hoods. The inferred parameter  X  is around 0 . 2 (a value of 0 recovers the Gamma process). Furthermore, HT-NGG gets higher likelihoods than HMNGG, in this case, this follows from the added flexibility afforded by allowing the thinning of individual atoms. 5.2. Topic Modelling Datasets Next, we considered four real-world docu-ment datasets, viz. ICML, TPAMI, Person and NIPS. The first 2 corpora consisted of abstracts obtained from the ICML and PAMI websites; ICML contained 765 documents from 2007-2011 with a total of about 44K words, and a vocabulary size of about 2K; TPAMI had 1108 documents from 2006-2011, with total of 91K words and vocabulary size of 3K. The Person dataset was extracted from Reuters RCV1 using the query person under Lucene,and contained 8616 documents, 1.55M words and a vocabulary size of 60K. It spanned the period 08/96 to 08/97. The NIPS corpus consisted of proceedings over the years 1987 to 2003 (Globerson et al., 2007). It was not postprocessed, and has 2483 documents, 3.28M words and vocabulary size 14K. Parameter Setting and Evaluation In modelling these datasets, for MNGG and TNGG (where we dis-regard the years associated with each document and assign it to its own time), we set the number of regions to be 20; in the other models these were set equal to the number of years. The Dirichlet base distribution was symmetric with parameter 0.3, and as in the previ-ous section, weak Gamma and Beta priors were placed appropriately on all nonnegative scalars.
 To evaluate the models, we computed perplexity scores Datasets ICML TPAMI Person NIPS Models train test train test train test train test on a held-out test dataset. In all cases, 20% of the original data sets was held-out, following the stan-dard dictionary hold-out method (50% of the held-out documents was used to estimate topic probabili-ties) (Rosen-Zvi et al., 2004). Test perplexity was cal-culated over 10 repeated runs with random initializa-tion, we report mean values and standard deviations. In each run 2000 cycles were used as burn-in, followed by 1000 cycles to collect samples for perplexity calcula-tion. To avoid complications resulting from the differ-ent representations used by the marginal and slice sam-pler, we calculated perplexities after first transforming the representation of the slice sampler to those in the marginal sampler. In other words, given the state of the slice sampler, we determined the induced partition structure, and used this to calculate prediction prob-abilities (calling the same piece of code).
 Quantitative comparison for different models We calculated both training and test perplexities for the models specified above, these are shown in Ta-ble 1. We see that HMNGG and HTNGG perform best, achieving significant lower perplexities than the others. While HTNGG is more flexible than HMNGG, it performs sightly worse when the datasets becomes large; this is more obvious when comparing MNGG and TNGG. Part of the reason for this is the com-plex posterior structure for the thinned models, so that the samplers are often stuck in local optima, re-sulting in much worse perplexities. Interestingly, HM-NGP (without the power-law property) does not per-form much worse than HMNGG, indicating topic dis-tributions in topic models might not follow an obvi-ous power-law behavior. This coincides with the sam-pled value of the index parameter  X  (around 0.01). Thus it is not surprising that HDP is comparable to HNGG: slightly better in small datasets, but a bit worse in large datasets. Moreover, the simple MNGG and TNGG do much worse than HMNGG and HT-NGG, emphasizing the importance of statistical infor-mation shared across documents in the same year. Topic evolutions Figure 3 is a posterior sample, showing the evolution of 12 randomly selected topics on the NIPS dataset for HMNGG and HTNGG. In all cases, we calculated the proportion of words assigned to the topic k in region r at each time t (i.e. n trk n the predictive probabilities for each topic at each time. The latter is defined for MNGG to be proportional to proportional to q rt w rk (see equation 8) by integrating out v tl and z rtk . We see (as we expect) HMNGG gen-erating smoother topic proportions over time (topics in HTNGG can die and then be reborn later because of the thinning mechanism).
 Marginal vs slice sampler We next compare the performance of the marginal and slice samplers for MNGG, HMNGG and HTNGG. The marginal sampler for TNGG could not handle datasets with more than even 2 times. Instead, we had to divide each dataset into two times (the first and the second halves, call the resulting datasets as 2 -time datasets), and treat these as the only covariates available. We emphasize that we do this only for a comparison with our slice sampler, which can handle more complex datasets (the slice sampler was used in the previous sections). Ta-ble 2 shows the average effective sample sizes and run-ning times over 5 repeated runs for the two samplers on the original datasets and the 2-time datasets. On the original datasets, in MNGG, the marginal sam-pler generally obtains larger ESS values than the slice sampler; while it is opposite for HMNGG. Regarding to the running time, the marginal sampler is more effi-cient in small datasets ( i.e. , ICML and TPAMI), while they are comparable in the other datasets. The rea-son for this is that in small datasets, a large amount of the running time in the slice sampler was used in sampling the extra atoms (which is unnecessary in the marginal sampler), while in large datasets, the time for sampling word allocations starts to become signifi-cant. In the 2-time datasets, we observe that the slice sampler obtains larger ESS values than its marginal sampler in both HMNGG and HTNGG, with compa-rable running times. We repeat that for HTNGG, the slice sampler is applicable for any number of times, while the marginal sampler is computationally infeasi-ble even for a moderately large number of times. We proposed two classes of dependent normalized ran-dom measures for the nonparametric modeling of de-pendent probability measures, the mixed normalized random measure and the thinned normalized random measure . Our construction involves weighting and thinning independent CRMs, before combining and normalizing them. We developed two different MCMC algorithms for posterior inference, a marginal and a slice sampler. In our experiments, our models showed significantly superior performance compared to related dependent nonparametric models such as HDP and SNGP, with the simpler MNRM performing better on complex data. We also find the slice sampler gener-ally mixes better than the marginal sampler in both models. Interesting future work includes extending our framework to allow each atom to have its own thinning probability, as well as allowing marginal RPMs in the broader class of Poisson-Kingman processes , which in-cludes the Pitman-Yor process as a special case. More-over, our models can be applied not just for time-series in topic modelling but also to allow sparsity of proba-bilities, for instance (Williamson et al., 2010). Acknowledgments: NICTA is funded by the Aus-Ahmed, A. and Xing, E. P. Dynamic non-parametric mixture models and the recurrent Chinese restau-rant process. In SDM , 2008.
 Andrieu, C. and Roberts, G. O. The pseudo-marginal approach for efficient Monte Carlo computations. Ann. Statist. , 37(2):697 X 725, 2009.
 Broderick, T., Jordan, M. I., and Pitman, J. Beta processes, stick-breaking, and power laws. Bayesian Anal. , 7:439 X 476, 2012.
 Caron, F., Davy, M., and Doucet, A. Generalized
Polya urn for time-varying Dirichlet process mix-tures. In UAI , 2007.
 Chen, C., Ding, N., and Buntine, W. Dependent hier-archical normalized random measures for dynamic topic modeling. In ICML . 2012.
 Du, L., Buntine, W., and Jin, H. A segmented topic model based on the two-parameter Poisson-Dirichlet process. Mach. Learn. , 81:5 X 19, 2010.
 Favaro, S. and Teh, Y. W. MCMC for normalized random measure mixture models. Stat. Sci. , 2012. Foti, N. J., Futoma, J., Rockmore, D., and
Williamson, S. A. A unifying representation for a class of dependent random measures. Techni-cal Report arXiv:1211.4753, Dartmouth College and
CMU, USA, 2012. URL http://arxiv.org/abs/ 1211.4753 .
 Gelfand, A. E., Kottas, A., and MacEachern, S. N. Bayesian nonparametric spatial modeling with
Dirichlet process mixing. J. Amer. Statist. Assoc. , 100(471):1021 X 1035, 2005.
 Globerson, A., Chechik, G., Pereira, F., and Tishby, N.
Euclidean embedding of co-occurrence data. JMLR , 8:2265 X 2295, 2007.
 Griffin, J. E. and Steel, M. F. J. Order-based depen-dent Dirichlet processes. J. Amer. Statist. Assoc. , 101:179 X 194, 2006.
 Griffin, J. E., Kolossiatis, M., and Steel, M. F. J.
Comparing distributions using dependent normal-ized random measure mixtures. J. R. Stat. Soc. Ser. B Stat. Methodol. , 2012.
 Griffin, J.E. and Walker, S.G. Posterior simulation of normalized random measure mixtures. J. Comput. Graph. Stat. , 20(1):241 X 259, 2011.
 Griffiths, T. L. and Ghahramani, Z. The Indian buffet process: An introduction and review. JMLR , 12: 1185 X 1224, 2011.
 James, L. F. Bayesian Poisson process partition cal-culus with an application to Bayesian L  X evy moving averages. Ann. Statist. , 33(4):1771 X 1799, 2005. James, L.F., Lijoi, A., and Pr  X  u nster, I. Posterior anal-ysis for normalized random measures with indepen-dent increments. Scand. J. Stat. , 36:76 X 97, 2009. Lin, D., Grimson, E., and Fisher, J. Construction of dependent Dirichlet processes based on Poisson processes. In NIPS . 2010.
 Lin, D. H. and Fisher, J. Coupling nonparametric mixtures via latent Dirichlet processes. In NIPS . 2012.
 MacEachern, S. Dependent nonparametric processes. In Proc. of the SBSS , 1999.
 MacEachern, S.N., Kottas, A., and Gelfand, A.E. Spa-tial nonparametric Bayesian models. In Proc. of the 2001 Joint Statistical Meetings , 2001.
 Neal, R. M. Slice sampling. Ann. Statist. , 31(3):705 X  767, 2003.
 Rao, V. and Teh, Y. W. Spatial normalized Gamma processes. In NIPS . 2009.
 Rosen-Zvi, M., Griffiths, T., Steyvers, M., and Smyth,
P. The author-topic model for authors and docu-ments. In UAI , 2004.
 Srebro, N. and Roweis, S. Time-varying topic mod-els using dependent Dirichlet processes. Technical report, University of Toronto, 2005.
 Teh, Y.W. and Gorur, D. Indian buffet processes with power-law behavior. In NIPS . 2009.
 Teh, Y.W., Jordan, M.I., Beal, M.J., and Blei, D.M. Hierarchical Dirichlet processes. J. Amer. Statist. Assoc. , 101(476):1566 X 1581, 2006.
 Williamson, S. A., Wang, C., Heller, K. A., and Blei,
D. The IBP compound Dirichlet process and its application to focused topic modeling. In ICML .
