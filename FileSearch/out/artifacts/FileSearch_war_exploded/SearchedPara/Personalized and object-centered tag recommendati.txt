  X  1. Introduction
Several Web 2.0 applications have reached unprecedented popularity mainly due to the strong stimuli and easiness for users to create their own content and share it with others, establishing online communities and social networks. Each page in a Web 2.0 application is composed by a main object , which can be stored in different media types (e.g., text, audio, image, features , such as the user X  X  age, sex and tags frequently assigned by her.

Among all textual features, tags have become one of the main textual features in Web 2.0 applications, as they are often exploited to provide better organization and description of the content. Moreover, recent studies have shown that tags are and content recommendation ( Guy, Zwerdling, Ronen, Carmel, &amp; Uziel, 2010 ).

In this context, tag recommendation services aim at assisting users in the task of assigning tags to a target object by sug-gesting keywords that are related to its content, thus improving the quality of the available information and ultimately the effectiveness of various IR services that rely on tags as main data source. The tag recommendation problem can be tackled gesting tags that are related to the object X  X  content.

The second perspective, centered at both object and user, aims at performing personalized tag recommendation to a target vocabulary biases, and may also have different purposes when choosing tags for a target object (e.g., content organization or content description). Moreover, even users with similar purposes may perceive the object X  X  content differently, particularly
Thus, personalized tag recommendation aims at suggesting tags that not only are related to the object X  X  content but also captures the user interests, profile and background, and thus might help services such as content organization. Moreover, as illustrated and further discussed in Sections 3 and 6.5.3 , personalized tag recommendations may also provide, either in isolation or collectively (i.e., all personalized recommendations provided to all users who tagged an object) better and more complete descriptions of the object X  X  content, compared to object-centered recommendations, which, in turn, help improve services, such as search and content recommendation.

Many existing object-centered strategies exploit tag co-occurrence patterns in previous tag assignments in the collection, expanding an initial tag set I o of the target object o with other tags that frequently co-occur together with the tags in I ods do not assume the existence of such tags in the target object, using, instead, terms extracted from other textual features
By relevant, we mean terms that are good descriptors of the object X  X  content and/or that help discriminate it from others, for boost candidates with more potential. Thus, most existing object-centered methods exploit a subset of the following dimen-features, and (iii) metrics of relevance. However, to our knowledge, they exploit at most two of these three dimensions.
In the context of personalized tag recommendation, most previous work exploits user profile features, specifically the history of tag assignments of all users of the application, known as folksonomy , or of a particular (target) user, known as along with the three aforementioned dimensions, which may also be important for personalized recommendations.
Accordingly, we here address the tag recommendation from both object-centered and personalized perspectives, model-ing it as a multiple candidate tag ranking problem. In other words, we develop functions that estimate the relevance of a candidate tag as a tag recommendation to a given object or to a given object-user pair, thus enabling us to rank the candidate tags according to such estimates, and recommend the most relevant ones as tags to the target object (for object-centered recommendation) or object-user pair (for personalized recommendation).

Unlike previous solutions, we here address the object-centered tag recommendation problem by jointly exploiting all three aforementioned dimensions. 2 In other words, we extend traditional tag co-occurrence based approaches to include ject. Specifically, we propose eight heuristic strategies for object-centered tag recommendation. Our heuristics extend two by exploiting multiple textual features. They are simple, easy to compute, and quite efficient.

In order to address the personalized tag recommendation problem, we first analyze different strategies to extract tag co-occurrence patterns. Some of these strategies have never been proposed in previous work. More specifically, we define the tag sets exploited to compute co-occurrence patterns in two different ways: (1) all tags assigned to an object by different users and (2) all tags assigned to an object by the same user. While the first strategy benefits from a larger amount of tag results, and the best strategy depends on the employed co-occurrence-based method. We then propose two new heuristics that extend our two best object-centered tag recommendation heuristics to include a metric that estimates the relevance of a candidate tag to the target user, named User Frequency (UF). UF is based on the history of tag assignments of the target user.
We note that, for both object-centered and personalized tag recommendation, a number of heuristics can be devised to combine multiple metrics of relevance into a final tag recommendation function. Finding the  X  X  X est X  X  heuristic is not an easy tions that can be built using the suggested metrics. Thus, we also investigate the benefits of applying learning-to-rank (L2R) techniques for tag recommendation. We propose three L2R-based object-centered strategies: one exploits the traditional
RankSVM method ( Joachims, 2006 ), whereas the other two are based on Genetic Programming (GP) ( Banzhaf, Nordin, Keller, erate ranking functions that exploit all given metrics as attributes to accurately estimate the relevance of each given candi-date tag. Our motivation to use L2R methods are threefold: (1) they can effectively exploit many attributes in the generation of ranking functions, (2) they can be easily extended to include more attributes, and (3) there is a strong theoretical back-ground on learning methods, which has been recently extended for ranking problems ( Qin, Liu, &amp; Li, 2010 ). We also extend our three L2R-based strategies to perform personalized recommendation by including the aforementioned UF metric as an attribute.

We evaluate our object-centered and personalized tag recommendation strategies with real datasets collected from four popular Web 2.0 applications, namely, YouTube and YahooVideo, two video sharing sites, LastFM, an online radio station, and Bibsonomy, a bookmark and publication sharing system.
In particular, we evaluate our object-centered tag recommendation strategies, comparing them against three state-of-the-art techniques, namely, Sum  X  , the best function proposed in Sigurbj X rnsson and van Zwol (2008) , LATRE ( Menezes et al., 2010 ), and the winner of the ECML Discovery Challenge 2009 ( Lipczak et al., 2009; Lipczak &amp; Milios, 2011 ), here referred to as Co-occurrence and Text-based Tag Recommender  X  CTTR . Sum with some tag frequency statistics. LATRE , in turn, is a more recent, efficient and effective method that exploits solely tag co-occurrence patterns. Our heuristics are extensions of these two methods. CTTR exploits the contents of textual features associated with the target object along with one metric of tag relevance, but does not consider the tags previously assigned to the target object.

Our results indicate that our object-centered heuristics produce improvements over the original techniques on which they are based of 36% in precision and 40% in recall on average across all datasets and heuristics, with gains on a single dataset reaching as much as 105% and 116%, respectively. Moreover, our best heuristic outperforms the best baseline, with porating a new metric that tries to capture the descriptive power of each candidate tag and by exploiting multiple textual features.

Further improvements over our best heuristics can also be achieved with our L2R-based strategies, with gains in precision and recall of up to 23% and 14% on average. In particular, our results show that Random Forest is the best of the three L2R techniques, with average gains in precision over the best of GP and RankSVM in each dataset of 7% (and reaching up to 10%).
Similar performance gains are also achieved in terms of others metrics, notably Normalized Discounted Cumulative Gain (NDCG) and Mean-Reciprocal Rank (MRR) ( Sigurbj X rnsson &amp; van Zwol, 2008 ).
We also evaluate our personalized tag recommendation strategies, comparing them against the state-of-the-art PITF per-recall, on average. Moreover, like for object-centered strategies, our L2R-based personalized tag recommendation methods also yield further improvements: the best L2R strategy  X  the RF-based method  X  provides gains of 10% on average (and up to 13%) in precision over our best heuristic. Once again, similar improvements in recall, NDCG and MRR were also achieved. In general, for both object-centered and personalized recommendation, we note that the L2R-based strategies provide a flexible framework that can be easily extended to include other attributes (i.e., tag relevance metrics) or to address other aspects of the tag recommendation problem. As a final result, we also quantify the benefits of personalized tag recommendations to produce better descriptions of the target object when compared to object-centered recommendations, by measuring the rel-sonalized and object-centered tag recommenders, both based on the RF technique, we find that the former outperforms the latter, with average gains in precision of 10%.

In sum, we here greatly extend our previous effort ( Bel X m, Martins, Pontes, Almeida, &amp; Gon X alves, 2011 ), by bringing the following main contributions: (1) the proposal of seven personalized tag recommendation solutions (four heuristics and three
L2R-based approaches); (2) the analysis of new tag co-occurrence patterns not exploited in previous work; (3) an extended experimental evaluation, for both object-centered and personalized strategies, including data obtained from Bibsonomy, a standard dataset to evaluate tag recommendation ( Benz et al., 2010; Lipczak &amp; Milios, 2011 ), and new evaluation metrics; (4) the inclusion of new tag relevance metrics used as attributes in our L2R-based strategies, namely, Predictability (Pred) and, for personalized strategies, User Frequency (UF); (5) the application of the Random Forest technique to the tag recom-mendation problem, particularly in the context of personalized recommendation, producing results that are significantly superior to the results of previously evaluated L2R-based techniques; and (6) a quantitative assessment of the benefits of personalized tag recommendation to provide better descriptions of the target object.

The rest of this article is organized as follows. Section 2 discusses related work, whereas Section 3 describes how we model the tag recommendation problem. The main metrics used to estimate the relevance of a candidate tag are defined in Section 4 , while our new tag recommendation techniques are introduced in Section 5 . Section 6 presents our experimental evaluation and discusses the most representative results. Section 7 concludes the article and points out some directions for future work. 2. Related work
In this section, we review related efforts, starting by presenting existing tag recommendation methods in Section 2.1 .We then discuss previous studies of Learning-to-Rank techniques in general and in the tag recommendation domain, specifically, in Section 2.2 . Finally, in Section 2.3 , we briefly review previous characterizations of tagging systems. 2.1. Tag recommendation
Many tag recommendation strategies exploit co-occurrence patterns computed over a history of tag assignments. In par-ticular, some of them exploit tag co-occurrences to expand an initial set of tags I antecedent X is a set of tags, and the consequent y is a candidate tag for recommendation, restricting the rules by a confi-dence threshold. However, the authors do not provide a ranking of the recommended tags. Sigurbj X rnsson and van Zwol (2008) , on the contrary, exploit simple global metrics of tag co-occurrence (e.g., confidence), applying them over all tags in the initial set to produce a final ranking of candidate tags. Some of the considered metrics, related to tag frequency, try to capture the  X  X  X elevance X  X  of each candidate. In comparison, we here consider a much richer set of metrics, including new metrics based on multiple textual features, which, as we shall see, are responsible for our largest improvements.
Due to efficiency issues, most of these strategies usually compute co-occurrences between only two tags (i.e., X contains only one tag), thus possibly missing important co-occurrence relationships. To address this problem, Menezes et al. (2010) propose LATRE  X  Lazy Associative Tag Recommendation, which computes association rules in an on-demand manner, allow-ing an efficient generation of more complex and potentially better rules, and producing superior results in comparison with the best method in Sigurbj X rnsson and van Zwol (2008) .

A few other efforts do not exploit tags previously assigned to the target object, focusing, rather, on other textual features title) of the target object, expanding them by association rules, and sorting the extracted terms by their usage as tags in a training set. Wang et al. (2009) use the traditional TF IDF metric to extract and rank the most important terms from the object X  X  textual content. Lu et al. (2009) as well as Zhang et al. (2009) propagate tags between objects with similar textual
These previous methods address the object-centered tag recommendation problem by exploiting a combination of (i) co-our knowledge, none of them jointly exploits all three dimensions of the problem. In contrast, in Bel X m et al. (2011) ,we investigate the benefits of combining these three dimensions to perform tag recommendation by designing new heuristics and L2R-based methods for object-centered recommendation. Our evaluation, comparing the proposed strategies against various state-of-the-art baselines in three different datasets, showed that our solutions outperform the previous techniques. the problem of personalized tag recommendation, (2) using one more dataset to evaluate our methods (the Bibsonomy dataset) as well as two new metrics (NDCG and MRR), (3) comparing different tag co-occurrence patterns for personalized recommendation, (4) introducing another L2R-based method (Random Forest), (5) extending the set of attributes exploited by the L2R-based methods, and (6) quantifying the benefits of personalized tag recommendations to provide better descrip-tions of the target object, when compared to object-centered recommendations.

In addition to co-occurrence and text based strategies, other approaches have also been exploited for object-centered tag recommendation. For example, Wu et al. (2009) add image content information to rank tags. Lin, Ding, Hu, Wang, and Sun (2012) perform a random search process over the graph of images with similar visual content. Siersdorfer et al. (2009) and
Pedro et al. (2011) create a graph of videos based on content similarity, and make recommendations by propagating Song et al. (2008), Song et al. (2011) exploit clustering in a bipartite graph containing tags, documents and words to recommend tags to a document. The documents are first associated to multiple clusters, and the most representative tags of each cluster are recommended to the target document. Yin et al. (2013) address not only the problem of predicting tags, but also of pre-dicting different kinds of relationships (such as relations between users, comments and items, and social links between users), exploiting a generalized latent factor model and Bayesian inference. The authors find that connecting comments and tags within the same model allows mutual reinforcement and improves prediction accuracy. However, it is not possible to compare this method with our strategies because comments are absent in our datasets, while they are also noisy and a different direction, we address not only relevance, but also other important aspects in tag recommendation systems, namely novelty and diversity, in some other previous work ( Bel X m, Martins, Almeida, &amp; Gon X alves, 2013; Bel X m, Santos, Gon X alves, &amp; Almeida, 2013 ).

Other studies address the problem of personalized tag recommendation , often exploiting the history of tag assignments of
Mei, Chen, and Wang (2009) fall into this category. Some studies focus on exploiting tag co-occurrences in these histories of urbj X rnsson and van Zwol (2008) to include personalization, exploiting tag co-occurrences in different contexts: (1) the ommendations. Feng and Wang (2012) model the folksonomy as a heterogeneous graph containing tags, users and objects as nodes, and employ an optimization strategy, OptRank, to learn the weights of the edges that connect these nodes. Finally,
Yin, Hong, Xue, and Davison (2011) consider the temporal aspect of tagging systems, i.e., the variation of user interests over regarding the use of textual features. More broadly, none of these previously proposed personalized tag recommendation methods jointly exploits all three dimensions of the problem identified and exploited by us, that is, tag co-occurrences, multiple textual features and metrics of tag relevance (including the user-related UF metric for personalization). Moreover, our experimental results indicate that our extension of object-centered recommendation strategies to include personaliza-tion can significantly outperform state-of-the-art personalized tag recommendation methods as well as the original object-centered strategies in providing better descriptions for the objects. 2.2. Learning-to-rank
Learning-to-rank (L2R) for Information Retrieval (IR) is the task of automatically constructing a ranking model using training data, such that the model can sort new objects according to their degrees of relevance, preference, or importance wise. The pointwise approach assumes that each query-document pair in the training data has a numerical score, and thus the L2R problem can be approximated by a regression problem. Pairwise approaches are approximated by binary classifica-optimize a given evaluation measure. The advantages and disadvantages of each approach are analyzed, and the relation-ships between the loss functions used in these approaches and IR evaluation measures are discussed. Moreover, experiments using the datasets of the LETOR benchmark indicate that the listwise approach is the most effective among the three approaches.

The effectiveness of L2R has been studied in various specific domains, particularly document and image retrieval ( Faria et al., 2010; Gomes, Oliveira, de Almeida, &amp; Gon X alves, 2013 ). For example, Gomes et al. (2013) compare thirteen L2R techniques for document ranking using various datasets of the LETOR benchmark. They find that: (i) in most datasets the results of all analyzed methods are statistically tied with each other and with an unsupervised strategy that uses the best of image ranking, Faria et al. (2010) show that all studied L2R methods outperform the unsupervised ranking by attribute values in isolation, and that two L2R methods, one based on association rules and the other based on Genetic Programming (GP), are clear winners.

In a recent work, Niu, Guo, Lan, and Cheng (2012) propose a novel top-k L2R framework, in which pairwise preference judgements with multiple levels of relevance are performed to partially sort search results, generating a top-k ranking of documents, in contrast to the traditional ordering of all documents in the result list. This strategy reduces time complexity of generating the ground-truth from O ( n log n )toO( n log k ), where n is the number of documents in the ranking. The frame-work also includes a new L2R strategy, FocusedRank, which fully captures the characteristics of the top-k ground-truth. One of the best variations of FocusedRank, called FocusedSVM, combines characteristics of RankSVM and SVM approach that directly optimizes mean average precision).

Other recent work on L2R tackles the problems of selecting documents for L2R datasets and the impact of these choices on discuss how to build effective systems for ranking social updates from a unique perspective of LinkedIn  X  the largest profes-sional network in the world. They address this problem as an intersection of L2R, collaborative filtering, and click-through modeling, while leveraging ideas from information retrieval and recommender systems. On a complementary direction, Chen et al. analyze the relationship between ranking measures and loss functions in several L2R methods, such as RankSVM,
RankBoost and ListNet, showing that the loss functions of these methods are upper bounds of the measure-based ranking errors. Thus, the minimization of these loss functions leads to the maximization of the ranking measures. Modeling ranking as a sequence of classification tasks, the authors prove that the essential loss (defined as the weighted sum of the classifi-cation errors of individual tasks in the sequence) is both an upper bound of the measure-based ranking errors and a lower bound of the loss functions in the aforementioned methods. With that in mind, they propose modified loss functions that are tighter bounds of the measure-based ranking errors, obtaining better ranking performances.
 Regarding the application of L2R techniques to the tag recommendation problem, we are aware of only a few prior efforts.
One such effort is our prior investigation ( Bel X m et al., 2011 ), where both RankSVM and Genetic Programming were applied to the tag recommendation problem. Two other related studies are those by Cao et al. (2009) and Wu et al. (2009) , which exploit RankSVM and RankBoost as L2R techniques, respectively. However, all these previous studies focus only on object-centered tag recommendation. Instead, we here address both personalized and object-centered recommendations, and compare three L2R techniques, namely, RankSVM, GP and Random Forest. Moreover, unlike our current effort, Wu et al. (2009) consider only metrics related to tag co-occurrences and image content, and do not exploit textual features, whereas Cao et al. Cao et al. (2009) consider only metrics related to tag frequency, disregarding tag co-occurrence metrics. 2.3. Tagging systems characterization
A related body of previous work focuses on characterizing tagging systems, thus producing useful knowledge for the de-are the main factors that impact tagging decisions, whereas Rader and Wash (2008) show that personal organization has a stronger impact on tagging decisions than social influences. In another direction, Almeida et al. (2010) and Figueiredo et al. (2013) propose several metrics to assess the quality of different textual features commonly associated with objects followed by tags. Similarly, Li et al. (2008) find that user-generated tags are consistent with the web textual content with which they are associated, and that they capture the user X  X  interests. In a more recent work, Zhang, Korayem, You, and
Crandall (2012) study geo-spatial and temporal relationships between tags, but only apply them to cluster and visualize tags, and not to recommend tags. 3. Tag recommendation: problem statement
A Web 2.0 object is a media instance (e.g., a text, an audio, a video, an image) in a given Web 2.0 application. There are main sources of information, comprise the self-contained textual blocks that are associated with an object, usually with a ated with each object.

Let U ; O and T be the sets of users, objects and tags of a Web 2.0 application, respectively. The tag recommendation strat-egies proposed here are based on the following sources of information: (1) the set of tag assignments P # U O T , represented by a set of triples defined as: and (2) for each object o 2 O , a set of textual features (other than tags) F
Let I o be the set of tags previously assigned to the target object o , and I the target user u , that is, Thus, we define two tag recommendation tasks:
Object-Centered Tag Recommendation. Given a set of input tags I target object o , generate a set of candidates C o ( C o \ I
Personalized Tag Recommendation. Given a set of input tags I get object o , generate a set of candidates C o ; u ( C o ; u
We note that, when the recommendation task is centered at the object, the same tags are provided regardless of the target user. The primary goal of this kind of recommendation is improving the quality of the tags in these objects, thus, improving the personalized tag recommendation takes the target user into account: the goal is to suggest relevant tags for the object that match the interests, profile and background of the target user. Personalized tag recommenders might provide different answers to different users (or users with different profiles). One important service that can benefit from personalized tag recommendations is personal content organization. However, we argue that other services that rely on good descriptions of the object X  X  content, such as content recommendation and search, might also benefit from personalized tag recommendations.

One observation that supports our argument is that different users may use very different tags to describe the same ob-ject, depending on their backgrounds and interests, and how they perceive the object X  X  content. Moreover, objects shared on
Web 2.0 applications are often multifaceted, being related to various topics, and different users may relate to such topics sonalized tag recommender is not only useful for the individual user (e.g., for content organization) but also in a collective sense, as jointly the tags recommended to different users may provide a more complete description of the object, which indi-rectly helps search and recommendation services. In other words, a set of personalized tags for the same object produced to thus helping with the semantic gap. Moreover, personalized tag recommendations might provide better descriptions of the object, compared to object-centered recommendations, even if considering a single user. Take, for instance, the user who created and uploaded the object. Personalized tag recommendations tailored to her interests might better capture the object X  X  content.

In order to support our argument, we present three real-world examples extracted from one of our datasets (Last.FM) where personalization produced qualitatively better results than a non-personalized method. Table 1 shows the top-5 tags recommended by our best object-centered recommender (4th column) and by our best personalized tag recommender (5th column) to two artists: Taylor Swift and Nina Simone. For the latter, Table 1 includes the recommendations produced to two different users.

While the object-centered strategy recommended general tags such as  X  X  X een X  X  and  X  X  X ive X  X  for Taylor Swift (example 1), the tags recommended by the personalized strategy to one user represent well the artist X  X  main music genres ( X  X  X op X  X  and  X  X  X oun-try X  X ) and initial decade of success (2000s). These tags might be considered a better description of the artist.
In examples 2 and 3, the object-centered strategy suggested the tag  X  X  X ock X  X  to Nina Simone, probably due to a large num-ber of co-occurrences of this tag with the previously available tags of the object. Again, the personalized recommendations provided more related genres for the singer ( X  X  X azz X  X  and  X  X  X oul X  X , in example 2) among the top recommendations. Moreover, the personalized method recommended tags that might be more important to the target user (and thus to users of similar interests and backgrounds), such as  X  X  X nstrumental X  X  and  X  X  X iano X  X , which are two of the most emphasized tags on the tag cloud of the page of this user on Last.FM. To another user (example 3), the method correctly recommended the nationality of the this user, who appreciates female vocalists in general (according to her user profile). Moreover, note that, if taken collec-tively, the set of tags suggested to both users by our personalized recommender provides a much better and more complete description of the singer than the object-centered recommendations, which would be the same to all users.
These examples illustrate our argument that personalized tag recommendation may provide better descriptions of the tar-where we empirically compare the effectiveness of object-centered and personalized tag recommendation strategies.
Having motivated the tag recommendation tasks addressed here, we now formally define our target scenarios. For both recommendation tasks, we focus on cases in which there are some available tags in the target object (i.e., I want to recommend new (different) tags to it. We note, however, that our methods are also able to recommend relevant tags work.

Many tag recommendation strategies, and in particular the ones proposed here, exploit co-occurrence patterns by mining relations among tags assigned to the same object (or additionally by the same user) in an object collection. The process of learning such patterns is defined as follows.
 For object-centered tag recommendation, we define a training set signed to object d , and F d contains the term sets of the other textual features associated with d . There is also a test set which is a collection of tuples fh I o ; F o ; Y o ig , where both I tags in I o are known (and given as input to the recommender), tags in Y two subsets facilitates an automatic assessment of the recommendations, as performed in Garg and Weber (2008), Menezes et al. (2010), Heymann et al. (2008), Lipczak and Milios (2011), Rendle and Lars (2010), and Guan et al. (2009) and further discussed in Section 6.2 . Similarly, there might also be a validation set mendation functions (see Section 6.2 ). Thus, each object swer ( Y v ).

For personalized tag recommendation, we exploit two different kinds of tag co-occurrences: (1) between tags assigned to the same object by various users (as we do in the object-centered recommendation task) and (2) between tags assigned by the same user to the same object. Thus, there are two variants of the training set for personalized recommendation: (1)
D  X fh I d ; F d ig , where I d contains all tags assigned to object d (by any user) and (2) tags assigned to an object d by each user u d 2 U d , where U
In both cases, F d contains the term sets of the other textual features associated with d , as defined above. The elements of the test object collection O are tuples h I o ; F o ; Y o ; u user, and Y o ; u o (expected answer) is a set of tags assigned by each user u element of validation set V also has its tag set split into input tags ( I
Thus, we define associative and text based tag recommendation methods as algorithms that estimate the relevance of tag candidates relying on the elements described above. We model tag recommendation as a multiple term candidate ranking problem. That is, we develop functions to estimate the relevance of a candidate term as a tag recommendation to a given object (or object-user pair, for personalized tag recommendation), thus enabling us to rank the candidates according to such estimates and recommend the most relevant ones. 4. Relevance metrics for tag recommendation
In this section, we present several metrics that can be used to estimate the relevance of a candidate for tag recommen-dation. They are used as attributes by our tag recommendation methods, particularly the L2R-based strategies. Some metrics, like Sum ; Stability ; TF and Entropy , have been previously applied to recommend tags ( Heymann et al., 2008; Menezes et al., and are here applied to both object-centered and personalized tag recommendation tasks. We note that the use of these metrics in the second task is a novel contribution of this work.
 Each metric falls into one of these categories: 4.1. Tag co-occurrence Co-occurrence based tag recommendation approaches usually exploit association rules, that is, implications of type
X ! y , where the antecedent X is a set of tags and the consequent y is a candidate tag for recommendation. The importance and confidence ( h ), which is the conditional probability that y is assigned as a tag to an element d 2 are also associated with d . As the number of rules mined from the training set bounds to select only the most frequent and/or reliable rules. This selection can improve both effectiveness and efficiency of the recommender.

At recommendation time, we select the rules whose antecedents are included in the previously assigned set of tags I user in the personalized case), given the initial tag set I where R is a set of association rules computed offline over the training set limit for the association rules X  antecedents. Sum was proposed in Sigurbj X rnsson and van Zwol (2008) , which also proposed several other metrics related to tag co-occurrences, including Vote and Vote estimates the relevance of a candidate tag c by the number of association rules whose antecedents are tags in I consequent is the candidate c . That is: Vote  X  is built from Vote as follows: and k x ; k c and k r are tuning parameters. Rank  X  c ; x ; k ing of candidates according to the confidence of the corresponding association rule (whose antecedent is x ). This factor is employed to make confidence values decay smoother. Stab is a metric used to reduce the relative importance of terms that occur either too often or very rarely in the training set, and thus may represent poor recommendations. This metric, defined the association rules. A similar extension of Sum , called Sum reported as the metric that produces the best tag recommendations out of all metrics proposed in that study. Thus, we here apply Sum  X  as a baseline method for object-centered recommendation (see Section 5.1 ).

For personalized tag recommendation, we here distinguish two types of co-occurrence patterns: (1) between all tags et al., 2009; Sigurbj X rnsson &amp; van Zwol, 2008 ) and is here adopted for object-centered recommendation as well and (2) between all tags assigned by the same user to an object, which we propose here. While the first strategy benefits from a larger amount of data, the second strategy may generate less noise. As we will show in Section 6 , these two strategies pro-vide quite different results, and the best strategy depends on the complexity of the exploited association rules, given by parameter  X  . For all metrics of tag co-occurrence, we use a subscript u to indicate that the second type of co-occurrence is used. When there is no such subscript, we refer to the first strategy to generate co-occurrence patterns. For example, by the same user to an object (that is, training set D 0 , defined in Section 3 ). Sum  X  c ; I exploits co-occurrences between all tags assigned to an object by different users. 4.2. Descriptive power
We here exploit four heuristic metrics that try to capture, to some extent, the descriptive power of a candidate c .In Bel X m et al. (2011) , we exploited them for object-centered tag recommendation, while here we also apply them to the personalized tag recommendation task. This is a novel aspect of this work. in the present context) 5 of o that contain c ( Figueiredo et al., 2013 ):
The assumption behind TS  X  c ; o  X  is that the larger the number of features of o containing c , the more related c is to o  X  X  to the famous singer. The maximum TS is given by the number of textual features, other than tags, considered. As we here consider title and description, TS 6 2.
 The Term Frequency of c in object o ; TF  X  c ; o  X  , is: where tf  X  c ; F i o  X  is the number of occurrences of c in feature F bag of words, counting all occurrences of c in it. In contrast, TS considers the structure of an object, composed by textual features, which are well-defined blocks of text, counting the number of blocks containing c .

Although both TS and TF try to capture how accurately a term describes an object X  X  content, neither of them considers that different features may present, in general, different descriptive capacities. For example, the title may describe an object X  X  content more accurately than other textual features ( Figueiredo et al., 2013 ). Thus, we proposed in Bel X m et al. (2011) two other metrics, built on TF and TS , that weight a term based on the average descriptive powers of the textual features in which it appears.

The average descriptive power of a textual feature F i is assessed by the Average Feature Spread (AFS) heuristic ( Figueiredo et al., 2013 ). Let the Feature Instance Spread of a feature F
F . We define AFS  X  F i  X  as the average FIS  X  F i o  X  over all instances of F define weighted TS and TF metrics as: 4.3. Discriminative power
One may argue that recommending more infrequent terms (provided that they are not too rare) may be desirable, since eral services (e.g., classification, searching) often perform IR on multimedia content by using the associated tags as data (in our case, tags). Given the number of elements in the training set j where f tag c is the number of elements (objects for object-centered recommendation, or object-user pairs for personalized recommendation) in D that are tagged with c . Note that c may be extracted from other textual features. The value 1 is added to both numerator and denominator to deal with new terms that do not appear as tags in the training data. We note that this metric may privilege terms from other textual features that do not appear as tags in the training data. Nevertheless, this metric will be combined with the other metrics into a function, using learning-to-rank algorithms. Thus, its relative weight can be adjusted.

Along the same lines, one may consider that terms that are very common, such as  X  X  X ideo X  X  in a YouTube object collection, are too general and broad, whereas very rare terms may be too specific or may represent noise (e.g., misspellings, neologisms and unknown words). In either case, such terms represent poor recommendations as they have very poor discriminative power . Sigurbj X rnsson and van Zwol (2008) propose the Stability ( Stab ) metric, which gives more importance to terms with intermediate frequency values: where k s represents the  X  X  X deal frequency X  X  of a term and must be adjusted to the data collection. We here also use Stab to to terms extracted from all textual features F o associated with target object o . 4.4. Term predictability
Another important aspect for tag recommendation is term predictability. Heymann et al. (2008) measure this character-istic through the term X  X  entropy . The entropy of a candidate c in the tags feature, H
If a term occurs consistently with certain tags, it is more predictable, thus having lower entropy. Terms that occur indiscriminately with many other tags are less predictable, thus having higher entropy. In other words, H the concentration of confidence values of all association rules whose antecedent is c . If a term is absent in the training set, it receives an arbitrarily high entropy, as, in this case, the result is not a real number. Term entropy can be useful particularly for breaking ties, as it is better to recommend more  X  X  X onsistent X  X  or less  X  X  X onfusing X  X  terms. Whereas term entropy was used in Heymann et al. (2008) only to evaluate recommendations, we here apply it as an input to the recom-mendation functions.
 Inspired by the method proposed in Lipczak et al. (2009) , described in Section 5.1 , we here propose a metric called textual feature of the same object. Unlike the metric proposed in Lipczak et al. (2009) , which computes such co-occurrences separately for each textual feature, Pred is computed by aggregating all textual features of the object. In other words, the Predictability of a candidate tag c ; Pred  X  c  X  , is defined as: feature, and f F c is the number of objects in which c is a term associated with any of its textual features (except tags). 4.5. User frequency
In order to estimate the relevance of a candidate for a target user and thus provide personalized recommendations, we an object. In other words, given a candidate c and a target user u ; UF  X  c ; u  X  is defined as: where N c ; u is the number of times that user u tagged an object with c in the training set user u submitted a tag. Thus, the rationale behind UF is: the more frequently a user u assigns a candidate tag c to other objects in the application, the more relevant c is for u . This metric is computed for all tags used by the target user u in the training set. 5. Tag recommendation strategies
In this section, we present the tag recommendation strategies analyzed in this work. First, in Section 5.1 , we describe state-of-the-art methods used here as baselines for object-centered and personalized tag recommendation. The object-centered baseline methods exploit a combination of at most two of the following dimensions: term co-occurrence with pre-assigned tags, multiple textual features and relevance metrics. The personalized baseline method is a matrix factoriza-tion approach that exploits the folksonomy.

We then present our new proposed solutions, which exploit all aforementioned dimensions as well as new relevance metrics. In Section 5.2 , we introduce our new object-centered and personalized heuristics, while we describe the learn-ing-to-rank based strategies in Section 5.3 . 5.1. State-of-the-art baselines
The following sections briefly describe the baseline methods used for evaluating our new object-centered and personal-ized tag recommendation methods. 5.1.1. Object-centered tag recommendation baselines
Our first baseline is Sum  X  , the best function proposed in Sigurbj X rnsson and van Zwol (2008) , which exploits both tag co-occurrences and metrics of tag relevance. Sum  X  extends the Sum metric (Eq. (1) ) similarly to how Vote
Section 4.1 ), that is, by weighting the confidence values by the Stability of the terms in the antecedent and consequent of rules. In other words, given a candidate c for an object o associated with a set of previously assigned tags I as: where k x ; k c and k r are tuning parameters.
 (i.e., offline), possibly including rules that might not be useful when recommending for objects in the test set. LATRE ranks thus exploiting solely co-occurrence patterns. We note that two variations of LATRE , with and without calibration, are pre-sented in Menezes et al. (2010) . We here consider the LATRE without calibration since it has lower complexity and, according most cases.

Our third baseline is called here Co-occurrence and Text based Tag Recommender (CTTR). It exploits terms extracted from adaptation of the winner of the ECML Discovery Challenge 2009 ( Lipczak et al., 2009 ), which, in addition to the two afore-mentioned aspects, also takes the user X  X  tag assignment history into account. We here do not include such user statistics in
CTTR, because they include the time instants when the tag assignments were done by each user, and this information is not available in our datasets. Thus, we here use CTTR as a baseline for object-centered recommendation only. benefits of applying our metrics of relevance to such terms and to exploit co-occurrence of previously assigned tags.
The basic structure of CTTR is depicted in Fig. 1 . As described below, CTTR distinguishes two types of co-occurrences: (1) between tags, in which the antecedents are tags in the objects of the training set and (2) between terms in the title of an related to the extracted terms are combined using corresponding scores. As a final step, the scores obtained from the asso-ciation rules and from the title and description of the target objects are rescored once again, and merged, resulting in the final ranking.

The first step is the extraction of potential candidates from other textual features associated with the target object, namely its title and description. Each term extracted from the title (or description) is scored according to its usage in pre-and as a tag to the total number of objects in which x is associated with the textual feature F
Next, the candidate sets generated by title and description are merged. As observed in Lipczak et al. (2009) , titles tend to provide more precise recommendations than other textual features, which should be reflected in the merging step. Towards that goal, the authors propose to use a leading precision rescorer for weighting the different candidate sources (textual fea-tures). This rescorer sets the average precision at the first position of the rank, a new score for the top candidate, and modifies the scores s top candidate. The new score s 0 i of the i th candidate tag is given by:
After re-scoring, the new scores should be merged in a probabilistic sum. Let S for candidate tag t . The merging function is given by: The terms extracted in the first step are then expanded through association rules. However, unlike ( Sigurbj X rnsson &amp; van
Zwol, 2008 ), CTTR method does not consider any tag that had been previously assigned to the target object. Towards the purpose of generating term candidates by co-occurrences with terms in the target object, Lipczak et al. (2009) and Lipczak and Milios (2011) distinguish two types of co-occurrence relationships: (1) between tags ( the title of an object and its tags ( R TitleToTag ). In other words, while the antecedents of the antecedents of R TitleToTag are terms in the titles of objects in it.

In the online recommendation step, the rule sets related to the extracted terms are combined. Title terms are used as antecedent in the following equation to find title-related tags: terms of the title-description merge are taken as antecedent in: where  X  x ! t  X 2 R TagToTag , and s x  X  merge  X f p title x tance of a term in the textual features of an object.
 At the final step, scores obtained from association rules ( S target object are re-scored and merged (with Eqs. (14) and (15) ), resulting in the final ranking. 5.1.2. Personalized tag recommendation baseline The state-of-the-art personalized tag recommendation method analyzed here is called Pairwise Interactions Tensor
Factorization (PITF) ( Rendle &amp; Lars, 2010 ). It was the winner of the graph-based personalized tag recommendation task in the PKDD Discovery Challenge 2009. PITF exploits the vocabulary of the target user expressed by the tags assigned by her to other objects as a representation of her interests and as the main evidence to support personalization.
Briefly, this approach explicitly models the two-way interactions between users, tags and objects by factorizing each of the three as a tensor product. From the set of tag assignments idea is that, for a given h user u ; object o i pair, one can assume that a tag t a Bayesian Personalized Ranking (BPR) optimization criterion. This learning method is based on stochastic gradient descent with bootstrap sampling. In other words, the pairwise constraints are sampled from the training data. PITF has the following parameters: the dimension of factorization d , the number of interactions s , the learning rate for BPR k , and the number of pair samples drawn for each training tuple s .

In previous work ( Rendle &amp; Lars, 2010 ), PITF was only evaluated in denser datasets, that is, datasets in which unpopular this kind of filtering. As we will see in Section 6.5.2 , our strategies outperform PITF because we exploit several sources of evidence not exploited by it.
 5.2. New heuristics for tag recommendation
Our new heuristics for object-centered tag recommendation extend the Sum posed by a weighted linear combination of the output of Sum previously assigned tags I o . The proposed heuristics have the following general structures:
Parameter a (0 6 a 6 1) is used as a weighting factor. Note that Sum erated from the association rules, whereas DP is computed for terms extracted from other textual features of target object o .
Our new heuristics for personalized tag recommendation extend the Sum include the user related metric UF (Section 4.5 ). We thus propose eight new ranking functions composed by a weighted linear combination of the output of Sum  X  DP (or LATRE + DP ) and the value of UF . Let c be a candidate tag for target pair h u ; o i . The proposed heuristics have the following general structures:
Parameter b (0 6 b 6 1) is used as a weighting factor. Note that Sum dates generated from the association rules and terms extracted from other textual features of target object o , while UF is computed for terms which were assigned as tags by user u in the training set. We note that a candidate tag c generated by co-occurrences or extracted from textual features may not be included in the tag assignment history of the target user (personomy). In this case, we set UF  X  c ; u  X  X  0. Similarly, a candidate tag extracted from the user X  X  personomy may not be in any textual feature of the target object o , presenting value 0 for its descriptive power metrics ( DP ).
Additionally, we propose variants of these two sets of heuristics, defined by the same, Eqs. (20) and (21) , but differing in the training set used. While Sum  X  DP  X  UF and LATRE + DP + UF exploit co-occurrences between tags assigned to objects by different users (training set D defined in Section 3 ), the two variants, referred to here as Sum LATRE u  X  DP  X  UF , exploit co-occurrences between tags assigned to the same object by only one user (training set in Section 3 ). 5.3. Learning-to-rank based strategies
We here investigate the potential benefits of applying L2R techniques to the tag recommendation problem. The basic idea is to use such algorithms to  X  X  X earn X  X  a good ranking function based on a list L
We here consider three different L2R techniques: the traditional RankSVM method (Section 5.3.1 ), the Genetic Programming (GP) framework (Section 5.3.2 ), and the Random Forest algorithm (Section 5.3.3 ). We chose these L2R methods since they represent three different learning paradigms that have been successfully applied to other IR tasks such as classification, search/ranking and image retrieval ( Gomes et al., 2013; Faria et al., 2010; Yeh, Lin, Ke, &amp; Yang, 2007 ).
We start by focusing on how we apply these techniques to the object-centered tag recommendation task, discussing extensions to address personalization in Section 5.3.4 . For object-centered tag recommendation, the list of attributes L exploited by all three L2R methods, shown in Table 2 (2nd column) includes: Sum ; Vote ; Vote ating two attributes for this metric. Moreover, the set of candidate tags C
LATRE and all terms extracted from other textual features. For each candidate c 2 C represented by a vector of attribute values M c 2 R m , where m is the number of considered attributes ( m centered tag recommendation). We also assign a binary label y cating whether c is a relevant recommendation for v ( y c training set D only to extract the association rules and to compute the attribute values, relying on validation set the solutions. 5.3.1. RankSVM based strategy
RankSVM is based on the state-of-the-art Support Vector Machine (SVM) classification method ( Joachims, 2006 ). We use the SVM-rank tool 9 to learn a function f  X  M c  X  X  f  X  W ; M considered attributes (i.e., W 2 R m ). W is learned by a maximum-margin optimization method that tries to find a hyperplane, defined by W , that best separates the  X  X  X losest X  X  candidate tags (represented by their attribute vectors in R employed to produce ranking statements (i.e., relevant tags must precede irrelevant ones), which in turn are used as input to the RankSVM learning process.
 the separating hyperplane.

RankSVM has two main parameters, namely, the type of kernel function, which indicates the structure of the solution how we choose the best values for these parameters.
 RankSVM is still a very competitive performer when considering average results across all collections in the LETOR 3.0
L2R benchmark, 10 besides being readily available for experimentation, unlike other methods that are proprietary. Moreover, we also note that we found RankSVM to be statistically tied (if not superior) to the recently proposed FocusedSVM method ( Niu et al., 2012 ), which combines both RankSVM with a listwise SVM approach that directly optimizes mean average precision, targeting the top-k positions of the ranking. Indeed, we have implemented and evaluated FocusedSVM for object-centered and of relevance (relevant and irrelevant), while the suggested method relies on a larger number of distinct levels of relevance, focusing on the highest positions of the ranking. 11 5.3.2. GP based strategy
We apply GP to find a good ranking function f  X  M c  X  for tagging recommendation purposes. This is done through the evolution of a population in which each individual represents a different ranking function. This evolution is inspired on ing, we provide an overview of the GP framework, and introduce how we model the tag recommendation problem with it. 5.3.2.1. Overview of the GP framework. Genetic Programming (GP) implements a global search mechanism. A GP algorithm evolves a population of tree-represented individuals (i.e., the solutions for the problem at hand), created from a set of ter-minals and functions related to the target problem. In each generation of the evolutionary process, individuals are evaluated mutation. We here implement the tournament selection method, which randomly chooses, with replacement, k individuals from the population and takes the one with highest Fitness value.

While the number of new individuals is smaller than the desired population size n , two individuals are picked through the adopted selection method, and, with probability p c , have their  X  X  X enetic material X  X  exchanged in the crossover operation to generate a new individual. That is, we randomly choose one node of each of the two trees (the two individuals) and exchange the subtrees below them. The role of the crossover operation is to combine good solutions towards the most promising solu-tions in the search space. Moreover, with probability p m by first randomly selecting one of its nodes, and then replacing it (and its corresponding subtree) by a new randomly generated subtree, without exceeding a maximum tree depth d .

The whole process, depicted in Fig. 2 , is repeated until a target Fitness value f target or a maximum number of generations g process, is chosen as the final solution for the problem.

GP is an effective non-linear method that has been successfully employed when there is a large search space and a goal to be optimized, having produced results close to the optimal in many applications ( Banzhaf et al., 1998 ). Indeed, GP has been applied to various Information Retrieval tasks such as classification, search/ranking and image retrieval (e.g., Yeh et al., 2007 ). However, to our knowledge, we were the first to apply it to tag recommendation ( Bel X m et al., 2011 ). Our motivations were twofold. First, in comparison with RankSVM, GP exploits a different learning paradigm, which directly optimizes a target function (e.g., recommendation precision). Second, GP has been demonstrated to be competitive with other learning-to-rank techniques such as RankSVM itself and RankBoost ( Freund, Iyer, Schapire, &amp; Singer, 2003; Yeh et al., 2007 ). shall see, GP outperforms RankSVM in a few scenarios, being statistically tied with it in many others. 5.3.2.2. GP applied to tag recommendation. To apply GP to tag recommendation, we need to define the tree representation of an individual, which, in our case, is a tag ranking function, and a Fitness function. A tree is composed of terminals (leaves) arithm ( ln ) operations as non-terminals. To ensure the closure property, we implement protected division and logarithm, such that these operators return the default value 0 when their inputs are out of their domains. The terminals are composed the given L attr list, which, for a candidate c , is given by vector M sponding function.

The Fitness of an individual in this context represents the quality of the recommendations produced by the corresponding of recommended terms ( p @ k ). 13 Let Y be the set of relevant tags to recommend for object o ( Y  X  Y pair ( Y  X  Y o ; u ). Let C be the sorted set of recommendations produced by the GP-generated function being evaluated, and C top k elements in C . The precision in the top-k positions, p @ k , is defined as:
The min operator guarantees that the denominator does not exceed j Y j , which is important because the number of available relevant tags in the expected answer Y may be less than k .

The Fitness (i.e., quality) of ranking function f  X  M c  X  is then computed as the average p @ k over all recommendations produced by f  X  M c  X  in a sample of objects of size s , extracted from the validation set 5.3.3. Random forest based strategy The Random Forest (RF) algorithm ( Breiman, 2001 ) is an ensemble method that combines a collection of decision trees. age attribute value), and the process repeats in a top-down fashion to form a tree with l terminal nodes, where l is a tuning parameter. Once the decision tree is built, it can assign a real-valued score as output to an unseen (test) candidate tag.
The RF method exploits the bagging ensemble technique, that is, each tree within the forest is built with a different boot-strap sample drawn from the original set of pairs  X  M c ; y our dataset, where, as discussed before, M c 2 R m and y c of candidates.

Besides l , the number T of trees (in each bootstrap sample) to grow, and the number of attributes n to consider when splitting each node are tuning parameters in RF. We note that, although each decision tree may suffer from overfitting, the aggregation of a larger number of low-correlated trees can mitigate this problem. The generalization error of a RF depends on both the correlation between trees in the forest and the strength of each individual tree. The more correlated ing l , each tree becomes more independent (less correlated), but also weaker. Thus, there exists some optimal values of n and l that provide the optimal balance between the correlation and the strength to get the minimum generalization error. We set those parameters using the validation set, as described in Section 6.4 . The implementation of RF is provided by the RankLib learning to rank tool. 14
RF has been shown consistently effective and competitive in several real world benchmarks ( Mohan, Chen, &amp; Weinberger, ization due the fact that single decision trees are built independently from others, thus making RFs inherently parallel. 5.3.4. Extensions of L2R-based strategies for personalization
The L2R-based strategies described in Sections 5.3.1 X 5.3.3 can be easily extended to include new relevance metrics. In particular, in order to extend them to address personalized recommendations, we included the metric UF in the list L each co-occurrence metric presents two variations, depending on whether the training data used is separated per user or not.
Thus, we adopted the best performing version when they are used as heuristics. user to objects in the training set D were included as candidates. That is, the candidate set C in the training set D . For each candidate c 2 C o ; u , we compute the values of all attributes in L associated with o .

Thus, the algorithms for personalized tag recommendation are the same as those described in the previous sections, except for the additional candidates and slightly different set of attributes. We argue that these methods perform well for both personalized and object-centered recommendation because they are flexible and robust strategies to generate relevant methods can provide relevant recommendations to a user even when she does not have a history of tag assignments. In that case, the extraction of candidates from tag co-occurrences and multiple textual features provide more general recommen-dations to the considered object, which may be relevant to any user. ods can provide a higher level of personalization, thanks to the use of the UF metric.

Table 3 lists all analyzed tag recommendation methods, while Table 4 summarizes key characteristics of the different techniques employed in our L2R-based methods. 6. Experimental evaluation
In this section, we first present the datasets used to evaluate the tag recommendation strategies (Section 6.1 ) as well as our evaluation methodology (Section 6.2 ) and metrics (Section 6.3 ). Next, we describe how we parameterized each strategy (Section 6.4 ), and discuss a set of representative results (Section 6.5 ). 6.1. Data collections
We evaluate the tag recommendation methods on four datasets, each containing the title , tags and description associated with real objects from Bibsonomy, LastFM, YouTube and YahooVideo. The Bibsonomy, LastFM and YouTube datasets also methods. The YahooVideo dataset, in contrast, does not identify the user who assigned each tag, and thus is here used only in the evaluation of object-centered methods.

The Bibsonomy dataset is a recent snapshot of the system, obtained on January 1st 2012, comprising 543,872 objects (bibtex records of publications). It is publicly available,
Lipczak &amp; Milios, 2011; Lipczak et al., 2009; Rendle &amp; Lars, 2010 ). The LastFM and YouTube datasets them. Our datasets include the textual features and tag assignments associated with 2,758,992 LastFM artists and with more than 9 million YouTube videos. The YahooVideo dataset was also gathered by snowball sampling, but using the most popular objects as seeds and following links of related videos. 20 objects.

We considered only objects with textual features in English, and used the Porter Stemming algorithm affixes of each word in each collected feature. Stemming was performed to avoid trivial recommendations such as plurals and other simple variations of the same word. We also removed stopwords, as well as terms that are either too frequent (with more than 100,000 occurrences in the dataset) or too rare (with fewer than 30 occurrences), as such terms are hardly good recommendations 22 ( Sigurbj X rnsson &amp; van Zwol, 2008 ). 6.2. Evaluation methodology
We adopted a fully automatic evaluation methodology that has been used by most prior studies on tag recommendation et al., 2010; Rendle, Balby Marinho, Nanopoulos, &amp; Schmidt-Thieme, 2009 ), including personalized tag recommendation assigned by the target user to the target object is used as expected answer.

Following the proposed methodology, for object-centered recommendation, for each object o in the test and validation sets, we randomly select half of its tags to be included in I
Similarly, for personalized tag recommendation, for each object o in the test and validation sets, half of the tags assigned by the target user u to the object o are included in I o and the other half in Y (i.e., I o ; u 0 for u 0  X  u ) are also used as input, being included in I in F o . Each object is thus represented by tuple h I o ; F recommendation.

We note that the tags in the expected answer for an object o are not exploited, in any way, to produce the recommen-dations for o (i.e., they are not used neither for metric computation nor for learning the recommendation function). Thus, from the perspective of the evaluation being performed, these tags are effectively new . This methodology allows us to simulate a scenario where these tags have not been assigned to the object yet and, thus, are potential candidates for new recommendations. Moreover, these tags can be considered relevant as we know that one or multiple users actually used them to annotate the object.

Alternative evaluation methodologies would rely on manual assessment of the tag recommendations by either: (1) real users of the system under study, who created the objects for which tags are recommended and/or have already added some tags to them, or (2) external volunteers. Whereas the former would be desirable, it is extremely hard to perform, particularly when covering different systems and a large number of different methods, as we do here. vious work on tag recommendation relied on evaluations by real users of the target system, perhaps due to the aforementioned by real users.
 volunteers to evaluate the recommendations. However, we argue that this approach is not necessarily better than the automatic one. Indeed, in the case of personalized tag recommendations, this approach may not be adequate at all, as the external evaluations might introduce significant biases and inaccuracies to the evaluation which would be very hard to dations adopted the same automatic evaluation approach used here, i.e., none relied on manual evaluations.
The few prior efforts that relied on manual evaluation focused on object-centered recommendation. In that context, the be non-negligible: if external evaluators are not very familiar with the topic of the object, their evaluations might have a significant impact on the results. In order to minimize this possible impact, a very large number of evaluators might be approach is much cheaper and scalable to large experiments. Moreover, we argue that, in a sense, the automatic evaluation is simulating a manual evaluation in which the evaluator is the object X  X  owner herself, an  X  X  X deal X  X  evaluator.
All in all, we have adopted the automatic strategy, which is a well-established and widely adopted evaluation protocol in the area, in favor of a more extensive quantitative evaluation. This choice allowed us to cover a large number of methods and datasets, enabling us to draw solid conclusions from statistically significant results.

To apply the selected methodology, we sampled 150,000 objects from each dataset (120,000 for YahooVideo). Each sam-ple was divided into five equal-sized portions, which were used in a fivefold cross validation. That is, three portions were treated as training set ( D ), which was used for extracting association rules and computing all metrics. A fourth portion (that is, to compute the Fitness function in the GP evolutionary process as well as learn vector W in RankSVM and the forest of regression trees in RF), and to tune parameters of all recommendation methods (including the RankSVM and RF-based process for the three L2R-based strategies is slightly different from the traditional use: we learn the ranking function and in the same set from which association rules and metrics were extracted (i.e., training set rules could be over-inflated. 25
We argue that our experimental design is fair because: (1) we do not use any privileged information from the test set in dence of low variation and thus learning convergence. Moreover, having a large amount of training data to generate the tag co-occurrence rules can help increasing the coverage of the rules (i.e., more co-occurrences can be potentially found), thus generating more candidates and more precise metric values. This benefits all methods. 6.3. Evaluation metrics
We now present the metrics used to evaluate the quality of the recommendations produced by all considered tag recom-mendation methods. They are also used by the GP framework, whose search process tries to directly maximize the consid-ered evaluation metric, as described in Section 5.3.2 .

Our primary metric is p @ k , defined in Section 5.3.2.2 (Eq. 22 ), with k =5.
Normalized Discounted Cumulative Gain (NDCG) in the first k  X  5 recommendations, of the recommendations. Recall is the fraction of the set of relevant tags for an object that were indeed recommended, whereas NDCG considers the order in which tags are recommended, emphasizing ranking relevant tags higher ( Baeza-Yates &amp; Ribeiro-son &amp; van Zwol, 2008 ). Thus, jointly, these four metrics provide complementary views and a more complete picture of the results.

Specifically, let Y be the set of relevant tags for object o ( Y  X  Y top k elements in C , and C i the i th element in C . Recall in the first k positions of the ranking is defined as: Let DCG @ k be the discounted cumulative gain in the first k recommendations, defined as: discounted cumulative gain in the first k recommendations, NDCG @ k , is defined as: where IdealDCG is the value obtained for DCG @ k when there are only relevant candidates at the top-k (or fewer) positions. Finally, MRR is defined as: where r  X  C ; Y  X  is the first position of the ranking where a relevant candidate appears, that is, r  X  C ; Y  X  X  min
C 2 Y . 6.4. Parameterization Our evaluation starts with a series of experiments with the validation set each method in each dataset. The best choice was defined as the one that maximizes precision (i.e., p @ k for k = 5) in the validation set, although the values remain the same if any of the other considered evaluation metrics are maximized. We summarize the parameterization of the object-centered and personalized tag recommendation strategies in Sections 6.4.1 and 6.4.2 , respectively. 6.4.1. Object-centered tag recommendation methods
We found that, for both Sum  X  and Sum  X  DP (for DP equal to TS ; TF ; wTS and wTF ), the best parameter values are k equal to 1, 5, 10, 20 and 50. Best results for a varied between 0 : 8 and 0 : 99, depending on the dataset. For both LATRE and
LATRE + DP (as well as for the L2R-based strategies), we set  X   X  3, as in Menezes et al. (2010) . Parameters r directly impact the number of association rules generated, thus affecting the processing time of the recommender. We
We now turn to the parameterization of the L2R object-centered tag recommendation methods. For the RankSVM based results more efficiently. Using cross-validation in V , we also varied the cost parameter j between 10 the best choice was j = 100 for all datasets.

For the GP based strategy, we experimented with population sizes n equal to 50 ; 100 ; 200 and 500, selecting n = 200, as we did not observe improvements for larger values. For this population size, the algorithm converges (i.e., the values of Fitness stop improving) before 200 generations, value assigned to g . We fixed k = 2, and set d  X  7 ; p can be infeasible. Thus, we used a sample of s = 500 of those objects, as this was enough to learn functions that are more effective than our heuristics.

We found our RF -based tag recommender to be very insensitive to parameterization. The results obtained in our cross-split of the tree according to the default value originally suggested in Breiman (2001) , i.e., n  X b log
Wiener, 2002 ), we tested other values ranging from 0.25 m to 0.75 m, observing no significant impact of the choice on our results. The only parameter that (slightly) impacts the results is the number of terminal nodes l . We used cross-validation to determine the best value of l in the 10 2  X 10
Table 5 shows the selected parameter values for all object-centered tag recommendation methods analyzed. Note that, for 6.4.2. Personalized tag recommendation methods
For the personalized tag recommendation strategies, the parameters in common with the object-centered methods (both heuristic and L2R-based methods) were set with the same best values discussed in the previous section (shown in Table 5 ).
Moreover, we set the descriptive power metric DP = wTS in the experiments with heuristics Sum
DP + UF , and their variants Sum  X  u DP + UF and LATRE u  X  DP  X  UF , since wTS was the most promising descriptive power metric according to our findings.
 The best values of parameter b , used by heuristics Sum  X  ble 6 . These values allow us to compare the contribution of the UF metric for personalized recommendation purposes. For example, considering Sum  X  u wTS + UF strategy, we found that setting b according to Table 6 leads to improvements in p @ 5 of up to 10% in LastFM and up to 7% in Bibsonomy and YouTube, with respect to results obtained with b  X  1 (that is, the weight assigned to UF equal to 0). The improvements are larger in LastFM probably due to the higher collaborative nature ity in this application when compared to LastFM. Thus, LastFM present a richer tag assignment history, which benefits all personalized recommendation methods. This fact reflects also on the best choices for b , whose values for LastFM and
Bibsonomy are smaller than for YouTube in several cases. Indeed, the importance given to the UF metric in LastFM is slightly higher than in the other two applications, particularly when Sum
The parameters of the PITF baseline were set as following. Similarly to Rendle and Lars (2010) , we set the factorization dimension d  X  64, and the sample size s = 100. We tested two different values for the learning rate k , namely, 0 : 05 and iments, we set this value for s . These parameter values are also shown in Table 6 . 6.5. Representative results
In this section, we present the results of all methods in the test sets, using the best parameter values found in the vali-then present the results of the personalized methods (Section 6.5.2 ). Finally, in Section 6.5.3 , we compare the best object-centered and personalized methods to show the benefits of personalization in tag recommendation. 6.5.1. Object-centered tag recommendation results
We discuss the most relevant results of our 11 object-centered tag recommendation methods (8 heuristics and 3 L2R-based strategies), comparing them against the 3 baselines. Table 7 shows average p @ 5 results for all methods and datasets. Average recall@5, NDCG@5 and MRR results are shown in Tables 8 X 10 , respectively.

All reported results are averages over 5 folds (test sets). For the GP-based and RF-based strategies, which are stochastic, each experiment was repeated 5 times. Thus, results are averages over 25 runs (5 folds, 5 seeds). Tables 7 X 10 also show 95% confidence intervals, indicating that, with that confidence, results do not deviate from the reported means by more than 3%. For each dataset, the tables are broken into 3 blocks: baselines, new heuristics and L2R-based methods. Best results and (and statistical ties) are shown in bold.

We start with a general finding: the improvements obtained with our methods over the baselines are much more modest in the LastFM dataset. This is possibly due to two factors: (1) there tends to be less overlap between the contents of title, description and tags associated with the same object on LastFM ( Figueiredo et al., 2013 ), which leads to a greater concen-and (2) the number of tags per object tends to be smaller in our LastFM and Bibsonomy datasets (e.g., 48% and 73% of our YahooVideo and YouTube objects have fewer than 10 tags, against 94% of Bibsonomy objects and 88% of LastFM objects).
These factors limit the benefits from using TS and wTS and from exploiting co-occurrence patterns among pre-assigned tags in that dataset.

Next, we turn our attention to the relative performance of specific methods, starting with the baselines. Consistently with ( Menezes et al., 2010 ), we find that LATRE outperforms Sum the gains in recall@5, NDCG@5 and MRR are also very impressive, reaching 27%, 22% and 12%, respectively. The only excep-tion is LastFM: the difference between the two methods in this dataset is under 2% for all evaluation metrics. Moreover, CTTR appears as a good alternative to LATRE in the YouTube dataset, with improvements of 53%, 61%, 51% and 48% in average p@5, recall@5, NDCG@5 and MRR, respectively. This occurs because CTTR exploits the terms of other textual features, while Sum and LATRE are purely based on tag co-occurrences. Next, we discuss the results of our new heuristics and L2R-based strategies. 6.5.1.1. New heuristics. We find that our best heuristic in each dataset produces gains over the best baseline of 14% in p@5, considering average results across all datasets. Similarly, the average gains in recall@5, NDCG@5 and MRR are 15%, 11% and
NDCG@5 and 14% in MRR. Thus, introducing a metric of descriptive power can greatly improve recommendation effectiveness.

Comparing each new heuristic with the original method on which it was based ( Sum ristics provide gains, on average, of 36% in p@5, 40% in recall@5, 31% in NDCG@5 and 24% in MRR. These are average results across all datasets. The improvements on any given dataset can be as high as 105% in p @ 5 and 116% in recall@5, such as the case of Sum  X  wTS on the YouTube dataset. As discussed, the gains for LastFM are much more modest (e.g., under 2% in p@5).
In comparison with CTTR , the improvements in p@5, recall@5, NDCG@5 and MRR produced by our new heuristics reach 57%, 61%, 55% and 51%, respectively, and remain quite impressive even if averaged across all datasets. For example, the cor-responding gains produced by LATRE + wTS , which is one of our best performing heuristics (see below), over CTTR , averaged power metrics as well as exploiting pre-assigned tags. Moreover, the strategy adopted by CTTR to combine the different dimensions exploited for tag recommendation (i.e., co-occurrences and terms extracted from textual attributes), which is the same authors later analyzed the potential benefits of introducing a tuning parameter to combine the different dimen-Our new heuristics use the a parameter that can be adjusted to the dataset (i.e., learned in a training set), producing better results. Moreover, as we show in Section 6.5.1.2 , our L2R-based strategies produce further improvements by learning the weights applied to the different dimensions exploited by our methods and by using a larger set of relevance metrics.
Among the new heuristics, the most promising ones are LATRE + wTS and LATRE + wTF , as they yield the best results in most cases. To reach this conclusion, we make two observations. First, for any given descriptive power metric DP (i.e.,
TS ; TF ; wTS or wTF ), LATRE + DP slightly outperforms Sum and 5% in MRR). Thus there is still (modest) benefits when we exploit more complex association rules, but the inclusion of the textual features mitigates the difference in effectiveness among our heuristics. In the few cases where Sum forms LATRE + DP , the gains in p @ 5 are under 3%.

Our second observation is that, comparing all four descriptive power metrics, wTS tends to yield the best results, followed 7% in p@5, 6% in recall@5, 7% in NDCG@5 and 22% in MRR. This is mainly because wTS considers that objects are composed of even if they appear in a single feature. Such terms are often less relevant than those appearing across multiple features. 6.5.1.2. Learning-to-rank based strategies. Turning our attention to the L2R-based strategies, the three analyzed methods pro-vide further improvements over our heuristics. We start by noting that the best L2R-based strategy  X  RF  X  outperforms the best heuristic ( LATRE + wTS ) by up to 23% in p@5, whereas the improvements in recall@5, NDCG@5 and MRR reach up to 22%, 24% and 19%, respectively. The corresponding gains averaged over all datasets are 14%, 13%, 15% and 11%. These improve-ments confirm the benefits of exploiting supervised L2R methods for tag recommendation, allowing an automatic search for a solution that combines a larger number of attributes when compared to unsupervised heuristics such as LATRE + wTS . We now turn our attention to the comparison of the three L2R-based strategies. Unlike existing comparisons of different datasets. The gains in p@5 of the winner method over the best of the remaining L2R techniques considered (i.e., either GP or
RankSVM ), averaged across all four datasets, are 7%, reaching almost 10% in Bibsonomy and LastFM. The corresponding aver-age gains in NDCG@5, recall@5 and MRR are 7%, 7% and 5%, respectively. These results confirm the effectiveness of methods based on an ensemble of decision trees, which are non-linear L2R strategies that have been shown to be effective and com-petitive in other studies ( Friedman, 2000; Mohan et al., 2011 ).

The other two L2R-based strategies  X  GP and RankSVM  X  have been previously exploited in object-centered tag recom-mendation ( Bel X m et al., 2011 ). GP is the most flexible strategy, allowing a wider range of types of recommendation func-tions (any function built from the considered operators and attributes). However, this can also be a disadvantage because the search space is larger when compared to the search space of the other two L2R-based methods, making it more difficult to find the best function. On the other hand, the shape of functions produced by RankSVM is pre-defined by the kernel func-binations of the attributes. Moreover, we note that the results of the GP-based method exhibit a higher variability than those of RankSVM because the functions generated by GP are inherently more diversified due to the stochastic operations explored by the method (e.g., mutation). Although RF is also stochastic, the variability introduced to its generated functions tends to be smaller than the variability produced by GP, since there exists fewer points in the RF algorithm where randomness can be introduced.

Analyzing the importance of each attribute in each  X  X  X earned X  X  ranking function, we verified that the most frequently used 71% of the functions. Sum  X  ; IFF and Pred come next, occurring in 67%, 60% and 57% of the functions, respectively. All other metrics appear in fewer than 50% of the functions. Similarly, we also analyzed the weight vector W learned by RankSVM, tance given to each attribute by the RF generated models, we measured the average depth of each attribute in the regression to this measure is Sum  X  , followed by Sum  X   X   X  3  X  ; Vote other L2R approaches.

Furthermore, we note that our L2R-based strategies provide flexible frameworks that can be easily extended to incorpo-rate new metrics and to exploit other aspects of the tag recommendation problem (e.g., personalization, as we do in this at recommendation time does not incur in significant extra processing time in relation to the best heuristic ( Bel X m et al., 2011 ). One possible source of delay for all of them is the on-demand generation of association rules by LATRE . However, as shown in Menezes et al. (2010) , LATRE  X  X  average processing time is well-suited for real-time recommendation.
In sum, we found that: (1) L2R-based strategies can significantly outperform state-of-the-art unsupervised heuristics and (2) RF is the best L2R strategy out of the three analyzed techniques, providing further gains over previously evaluated
L2R-based strategies. 6.5.2. Personalized tag recommendation results
We now discuss the most relevant results of our new personalized tag recommendation methods (4 heuristics and 3 L2R-based strategies), comparing them against the PITF baseline. Table 11 shows p @ 5 results for all personalized methods and datasets. Recall@5, NDCG@5 and MRR results are shown in Tables 12 X 14 , respectively.

Once again, all reported results are averages over 5 folds (test sets), whereas the results of GP and RF are averages over 25 runs (5 folds, 5 random generator seeds). Tables 11 X 14 show 95% confidence intervals, indicating that, with that confidence, most results deviate from the means by less than 2%. For each dataset, the tables are broken into 3 blocks: baseline, new heuristics and L2R-based methods. Best results and statistical ties (according to a 2-sided t -test the personalized methods for YahooVideo as our dataset of this system does not identify the user who assigned each tag. 6.5.2.1. New heuristics. We start by comparing our new heuristics against the baseline PITF. We found that our best heuristic considering overall results ( LATRE + wTS + UF ) produces gains in p@5 ranging from 48% to 251%, and in recall@5, NDCG@5 and MRR of up to 255%, 295%, and 209% respectively. Average gains across all datasets are 121% (p@5), 122% (recall@5), 157% (NDCG@5), and 120% (MRR). Thus, using a combination of tag co-occurrences, multiple textual features and metrics of relevance, including a metric that captures the tagging history of the user ( UF ), can greatly outperform recommendation methods that are based only on the interrelationships between users, objects and tags, like PITF.

The effectiveness of PITF is particularly poor in YouTube due to the non-collaborative nature of the application, where the user who uploaded the object can assign tags to it. This characteristic makes training data sparser, limiting the benefits of
PITF, which depends on a sufficient amount of postings involving a user u and an object o to recommend relevant tags for the pair h u ; o i . Nevertheless, we note that even in collaborative tagging applications, such as Bibsonomy and LastFM, the gains of our heuristics over PITF are very large. For example, in Bibsonomy, LATRE + wTS + UF outperforms PITF by as much as 65% in p@5 (see Table 11 ).

We note that, like PITF, our methods also exploit the vocabulary of the target user, expressed by the tags assigned by her to other objects, as a representation of her interests and main evidence to support personalization. We argue that it is not unlikely that the same user may assign tags to similar objects as these objects better match the user interests and vocabu-lary. Thus, as our results confirm, it may be interesting to recommend tags that the user had already assigned to other objects.

Next, we compare our four proposed heuristics, focusing first on the two different types of tag co-occurrence patterns exploited by them: (1) between tags assigned to the same object by various users (exploited by Sum
TRE + wTS + UF ) and (2) between tags assigned by the same user to the same object (used by Sum
LATRE u + wTS + UF ). In YouTube, these two kinds of co-occurrence patterns lead to the same results, since only one user can assign tags to an object. In the other two applications, interestingly, the most effective type of co-occurrence pattern depends on the co-occurrence based method exploited by the recommendation strategy ( Sum the recommendation is based on Sum  X  , which exploits relationships between only 2 tags, type (2) is preferred as
Sum  X  u wTS + UF produces results that are, if not statistically tied, much better than those produced by Sum example, the improvements in p@5 reach 30% in the LastFM dataset. This occurs due to the larger amount of noise generated when co-occurrences between all tags in an object are considered. On the other hand, exploiting co-occurrences between tags assigned to the same object by various users benefits LATRE , which exploits more complex association rules (i.e., co-occurrences between more than 2 tags), being more resilient to noise. For example, the improvements in p@5 of LATRE + wT-
S + UF over LATRE u + wTS + UF vary from 2% up to 23%. This illustrates the benefits of exploiting more complex rules, which can deal with a larger amount of noise. The same conclusions hold for the other three evaluation metrics considered. Consistently with the results of the object-centered recommendation methods that they extend, we find that LATRE + wT-S + UF outperforms Sum  X  u wTS + UF in all datasets but YouTube. For example, LATRE + wTS + UF improves the p@5 of Sum  X  u wTS + UF by 3%, on average, in both LastFM and Bibsonomy, while experiencing only a small loss (less than 1%) in You-
Tube. Similarly, the average gains in recall@5, NDCG@5 and MRR produced by LATRE + wTS + UF on LastFM and Bibsonomy are 4%, 4% and 3%, respectively, whereas the losses in YouTube do not exceed 1.1%. Thus, LATRE + wTS + UF is the best heuristic for personalized tag recommendation. 6.5.2.2. Learning-to-rank based strategies. Like observed for object-centered tag recommendation, all three L2R-based meth-ods provide further improvements over the heuristics for personalized tag recommendation, in all datasets, although the RF-based strategy is clearly the best performer. For instance, the improvements in p@5 achieved with the RF-based strategy over
LATRE + wTS + UF are 10%, on average across all datasets. Similarly, average gains in recall@5, NDCG@5 and MRR are 10%, 12% and 9%, respectively. Moreover, the RF-based strategy consistently outperforms the best of the other two L2R-based strate-gies in around 5%, on average, in any of the considered metrics. These results confirm the benefits of exploiting Random For-est as an L2R approach for tag recommendation, and the resilience of our methods when applied to both object-centered and personalized tag recommendation tasks, as discussed in Section 6.5.2.1 .

Analyzing the relative importance of each considered attribute in the X  X  X earned X  X  ranking functions, we found that the most all datasets, these metrics occur in respectively 73% and 63% of the generated functions. The next most frequently used metrics are wTS and IFF , which occur in 42% of the functions. All other metrics appear in fewer than 30% of the functions.
The same conclusions hold if we analyze the weight vector W learned by RankSVM. For the RF-based approach, the most important attribute is Sum  X   X   X  3  X  , followed by UF ; Sum
Overall, an important factor that explains the success of our personalized methods (both heuristics and L2R-based meth-ods) is that, as previously mentioned, they can provide relevant recommendations for a user even if she does not present a history of tag assignments. In that case, the extraction of candidates from tag co-occurrences and multiple textual features provide more general recommendations for the considered object, which can be relevant to any user. If the user is more ac-tive, however, our methods can provide a higher level of personalization, due to the use of the UF metric. In other words, our methods are flexible and robust to deal with both object-centered and personalized tag recommendation tasks. In particular, the RF -based strategy has shown to be the most effective solution for both tag recommendation tasks. 6.5.3. Benefits of personalization in tag recommendation
In Section 3 we argued that personalized tag recommendations might provide better descriptions of the object when compared to object-centered recommendations, thus improving services that rely on those descriptions, such as search and content recommendation. We also showed some examples extracted from our LastFM dataset to qualitatively support our argument. In this section, we provide further evidence of it by quantitatively comparing our best object-centered and personalized methods under similar conditions.

Specifically, we compare the results produced by the RF-based object-centered and personalized tag recommendation methods for each user against the same expected answer. In other words, for each target object-user pair h o ; u i , we use the same input tags I o to feed both methods, and compare their results against the same expected answer Y a fair comparison of both methods, we build these two tag sets such that each one contains half of the tags posted by each user who assigned tags to o (randomly selected). Note that this setup is different from the ones used in Sections 6.5.1 and 6.5.2 . In the former, the tags of the object were randomly split into I recommended tags were compared against the other tags posted by u ( Y compare the tags recommended by the personalized method for a user u with all tags that were not used as input (i.e., all given object o , the object-centered method produces the same results to all users.

Precision, recall, NDCG and MRR of both methods are shown in Table 15 for the Bibsonomy, LastFM, and YouTube data-sets. Note that the personalized strategy produces results that significantly outperform the object-centered method. The average gains in p@5 are 10%, while corresponding gains in recall@5, NDCG@5 and MRR are 15%, 10% and 8%, respectively.
That is, having fixed the expected answer, the personalized recommendations match this expected answer more closely than the object-centered recommendations. These results are in alignment with observations in Rendle et al. (2009) and Rendle and Lars (2010) , which showed that their personalized tag recommenders outperform even the theoretical upper-bound for any non-personalized tag recommender.

These results are evidence that personalized tag recommendations may help improve the quality of the recommenda-tions, providing tags that not only might be more important to the target user, and thus to other users with similar interests and profiles, but also that cover the different facets of the object, allowing a more complete description of the content than object-centered recommendations. 7. Conclusions and future work
In this article, we proposed several new object-centered tag recommendation strategies that jointly exploit term co-occurrence with pre-assigned tags, multiple textual features and metrics of tag relevance. We also proposed personalized strategies, which exploit the aforementioned dimensions and the tag assignment history of the target user. Our strategies include several heuristics and learning-to-rank based methods. We compared our strategies against four state-of-the-art techniques, in different datasets.

We found that LATRE + wTS , our best heuristic for object-centered tag recommendation produces gains in precision in the top-5 recommended tags in 12%, on average across all datasets. In terms of average recall and average NDCG in the top-5 recommendations, the average gains are 13% and 9%, whereas the improvements in average MRR are 6%. Some further improvements over our best heuristic (14% in precision, averaged across the four datasets) can also be achieved with the
L2R-based strategies, particularly with the Random Forest (RF) based method, which produces the best results among the three analyzed L2R-based approaches for tag recommendation. Similarly, we found that LATRE + wTS + UF , our best heuristic for personalized tag recommendation, produces gains in precision, on average, of 121% over the PITF baseline. The improve-ments in recall, NDCG and MRR are 122%, 157% and 120%, respectively. Moreover, once again further improvements can be achieved by the L2R-based methods, particularly the RF-based method, which improves the precision of the best heuristic in 10%, on average. As a final result, we also quantified the benefits of personalized tag recommendation to also improve the description of the object, finding that our best personalized method produces gains in precision over our best object-centered tag recommendation of 10%, on average.

The superiority of our strategies can be credited to the multiple dimensions they exploit. For example, when a user has no tag assignment history in the application, our methods are still able to recommend tags that are related to the textual content of the target object, which may be relevant to any user. In contrast, users with some history of tag assignments can benefit from more personalized recommendations. Our results illustrate the benefits of jointly exploiting the aforemen-tioned dimensions, particularly metrics of descriptive power and user interests. Moreover, the use of learning-to-rank techniques arise as a promising solution, as they yield very competitive results while still being quite flexible, allowing the easy inclusion of new metrics and the extension of the scope of the problem.

Directions for future work include the exploration of new metrics and techniques (e.g., features extracted from the media content and other user related features) as well as extensions of the methods to consider not only relevance but also other aspects such as novelty and diversity. Another venue of interest is a thorough investigation of the biases and benefits of a manual evaluation of the recommendations by volunteers, for both object-centered and personalized tag recommendation. Acknowledgements CNPq/INCT Web Grant Number 573871/2008-6), and by the authors individual grants from CNPq, CAPES and FAPEMIG. We also would like to thank the reviewers for their comments and suggestions, which greatly contributed for this work. References
