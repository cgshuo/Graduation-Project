 We introduce a new theoretical derivation, evaluation meth-ods, and extensive empirical analysis for an automatic query expansion framework in which model estimation is cast as a robust constrained optimization problem. This framework provides a powerful method for modeling and solving com-plex expansion problems, by allowing multiple sources of domain knowledge or evidence to be encoded as simultane-ous optimization constraints. Our robust optimization ap-proach provides a clean theoretical way to model not only expansion benefit, but also expansion risk, by optimizing over uncertainty sets for the data. In addition, we introduce risk-reward curves to visualize expansion algorithm perfor-mance and analyze parameter sensitivity. We show that a robust approach significantly reduces the number and mag-nitude of expansion failures for a strong baseline algorithm, with no loss in average gain. Our approach is implemented as a highly efficient post-processing step that assumes little about the baseline expansion method used as input, making it easy to apply to existing expansion methods. We provide analysis showing that this approach is a natural and effec-tive way to do selective expansion, automatically reducing or avoiding expansion in risky scenarios, and successfully attenuating noise in poor baseline methods.
 Categories and Subject Descriptors: H.3.3 [ Informa-tion Retrieval ]: Retrieval Models General Terms: Algorithms, Experimentation Keywords: Query expansion, convex optimization Despite decades of research on automatic query expansion [12], even state-of-the-art methods suffer from drawbacks that have limited their deployment in real-world scenarios. One example of this is their failure to account for the tradeoff between risk and reward: current techniques are optimized to perform well on average, but are unstable and have high variance across queries [10]. They are ineffective at avoiding risk by operating selectively , reducing or avoiding expansion when it is likely to dramatically decrease performance. In addition, Web search engines must support an increasingly complex decision environment in which potential enhance-ments to a query may be influenced and constrained by mul-tiple factors including personalization [26], implicit and ex-plicit relevance information, and computation budgets. Ex-pansion terms are usually selected individually, when the correct approach should be to optimize over the entire so-lution as a set. This is particularly problematic for Web search, where the set of related words is forced to be very small for speed and reliability reasons.

Existing closed-form term-weighting formulas for query expansion find such scenarios difficult or impossible to han-dle, and cannot easily balance risk and reward. For auto-matic expansion algorithms to be widely used, is important to find techniques that maintain the good average perfor-mance of existing methods, but which are more general and more reliable.

Convex optimization methods [3] are a special class of optimization technique that includes well-known algorithms such as least-squares, but with more general capabilities, so that they can handle a much wider set of problems. They provide an attractive starting point for several reasons. Their ability to integrate different criteria for expansion, such as risk and reward, as optimization constraints and objectives allows us to express an extremely rich set of expansion sce-narios in simple form. Convex programs also have unique, globally optimal solutions and reliable, highly efficient so-lution methods. For example, the quadratic programs we develop below can now be solved in just tens of millisec-onds for a few hundred variables. Finally, many widely-used functions used in IR tasks, such as vector dot products and KL-divergence, are convex functions, making it possible to cast realistic IR problems as convex programs.

We contribute a new general formulation of query expan-sion as a convex robust optimization problem that produces more conservative solutions by optimizing with respect to uncertainty sets defined around the observed data. Because our goal is to mitigate the risk-reward tradeoff of expansion, we introduce risk-reward curves as an evaluation tool. We make a detailed study of algorithm constraints, and demon-strate that ignoring risk leads to less stable expansion algo-rithms. Our approach significantly reduces the number and magnitude of expansion failures of a strong baseline method with no loss in average gain, while successfully attenuating noise when the baseline is poor quality. Our goal in this section is to show how query expansion can be cast as a robust convex optimization problem. Instead of deriving a term weighting formula in closed form, we will define a set of objectives and constraints that good expan-sion models would be expected to satisfy. We then give the resulting convex program to a solver , which either finds an optimal point x  X  satisfying the constraints, or determines that no solution is feasible , i.e. can satisfy all constraints.
We define a query expansion for a query Q of length K as a set of weights x = ( x 1 , . . . , x n ) over terms in a vocabulary V , with x i  X  [0 , 1]. Each x i represents a weight for term w i  X  X  . If used with a rounding threshold, x i can also be thought of as a soft decision to include or exclude the term from the expansion 1 . We assume our observations are the initial query Q , and a set of resulting top-ranked documents for the query D .

For query expansion, the key idea is that we want to bal-ance two criteria: reward and risk . The reward criterion for a feasible expansion x , which we denote R ( x ), reflects the selection of  X  X ood X  individual expansion terms, while the risk criterion, denoted V ( x ) penalizes choices for which there is greater uncertainty, both for individual terms and the entire expansion set. We now develop these in more detail. In general, obtaining an appropriate objective function for query expansion would seem to be very difficult, especially since we want general-purpose methods that can be easily applied to existing algorithms. We want to make few as-sumptions about the nature of the retrieval model.
Our solution is to view query expansion in two phases: the first step obtains initial candidate weights p =  X ( Q, D ) using a baseline expansion method  X , that we treat as a black box. The second step is a constrained optimization phase applied to the initial candidates p to obtain the final expansion solution x  X  . In that sense, our approach can be seen as a general-purpose wrapper method. Note that we do not assume the baseline method is  X  X easonable X : in fact, we show in Section 4.6 that our approach can tolerate even the most ill-behaved baselines.

With this view, choosing a reward objective becomes much easier: we want to bias the solution toward those words with the highest p i values. One simple, effective function is the expected relevance of a solution x : the weighted sum R ( x ) = p x = P k p k x k . Other choices for R ( x ) are certainly possi-ble, although we do not explore them here. For example, if p and x represent probability distributions over terms terms, then we could choose KL-divergence R ( x ) = KL ( p || x ) as an objective since it is convex.

For Indri X  X  language model-based expansion, we have Rel-evance Model estimates p ( w | R ) over the highest-ranking k documents, where the symbols R and N represent relevance and non-relevance respectively. We can also estimate a non-relevance model p ( w | N ) to approximate non-relevant doc-uments using either the collection, or the bottom-ranked k documents from the initial query Q . To set p i , we first compute p ( R | w ) for each word w via Bayes Theorem, Then, using p ( R | Q ) and p ( R |  X  Q ) to denote the probability that any query word or non-query word respectively should
Useful for search engines without term weighting operators. be included in the expansion, the expected value is then We choose p ( R | Q ) = 0 . 75 and p ( R |  X  Q ) = 0 . 5. If reward were our only objective, maximizing the objective R ( x ) would simply correspond to preferring the expansion terms with highest p i values assigned by the baseline expan-sion algorithm, given whatever constraints or other condi-tions, such as sparsity, that were in force. These baseline p i weights, however, are inherently uncertain, and the fi-nal expansion solution may vary considerably for different hypotheses about the  X  X rue X  p i values. To account for this uncertainty, we invoke robust optimization methods. Robust methods model uncertainty in p by defining an uncertainty set U around p , and then applying a minimax approach  X  that is, minimizing the worst-case solution over all possi-ble expansions realized by p varying in its uncertainty set U . The result is a more conservative algorithm that avoids relying on data in directions of higher uncertainty. We treat p as a random variable varying within an uncer-tainty set U distributed with mean p and covariance  X . We define U  X  to be an ellipsoidal region around p We set the covariance matrix entries  X  ij by choosing a term similarity measure  X  ( w i , w j ) and define  X  ij = d ( w exp(  X   X   X  ( w i , w j )), where the constant  X  is a normalization factor that depends on the measure chosen for  X  ( w i , w In this study,  X  ( w i , w j ) was defined using a word similarity measure based on query perturbations described in [6]. In practice, however, we have also used simpler methods for  X  ( w i , w j ), such as the Jaccard similarity 2 over the 2-by-2 contingency table of term frequencies for w i and w j in the top-ranked documents, with only small reductions in effec-tiveness.

The adjustable parameter  X  represents our tolerance of risk, with larger  X  indicating a higher aversion to risk. When  X  = 0, the uncertainty set U 0 is a single point, p , and we have our original reward-only problem. If  X  = I , U  X  is the ellipsoid of largest volume inside box B = { u || u i  X  p The key result we now use is the following theorem from Ben-Tal &amp; Nemirovski [2].

Theorem 1. The robust counterpart of an uncertain lin-ear program with general ellipsoidal uncertainty can be con-verted to a conic quadratic program.
 As an example, applying this theorem to the very simple reward-only linear program
The Jaccard similarity is defined as  X  J ( w i , w j M 11 / ( M 01 + M 10 + M 11 ) where the M jk are the cells of the contigency table containing total counts in the top-ranked documents, and j , k are the binary variables indicating pres-ence or absence of w i and w j respectively in a document. results in the robust quadratic program version when p varies within the uncertainty set U  X  as defined above. We thus have our risk objective V ( x ) =  X  2 x T  X  x . Combin-ing risk and reward gives the bi-objective function U ( x ) =  X  p We can make the solution even more conservative by defining an uncertainty set for  X  itself. We do this using a simple diagonal matrix W , with W ii reflecting uncertainty in the estimate of  X  ii . To set W ii , we introduce the idea of term centrality . Previous work [9][29] has shown that terms are more reliable for expansion if they are related to multiple query terms. We thus define W ii in terms of the vector d all similarities of w i with all query terms. This gives W k d i k 2 2 = P w q  X  Q d 2 ( w i , w q ). It can be shown [15] that the effect of this regularization is a modified covariance matrix  X   X  =  X  +  X   X  1 W , where  X  controls the relative influence of the diagonal W . Our joint reward and risk objective becomes We discuss the settings for  X  and  X  in Sec. 4. We now add domain-specific constraints for query expan-sion: aspect balance , aspect coverage , and query support .
Aspect balance. Intuitively, we want an expansion that does not contain words that are all related to just one part of a query. Instead, there should be a balance among its component aspects. We assume a simplistic model where each query term represents a different aspect of the user X  X  information need. We represent each query term q k as a vector  X  k ( w i ) =  X  ik of its similarities to all words in V . We then form a matrix A with K rows A k =  X  k . One way to express a  X  X alanced X  solution x with respect to the aspect vectors  X  k is to require that the mean of the projection Ax be within a tolerance  X  of the centroid = 1 /K P  X  k , i.e. Ax  X   X  where indicates component-wise comparison. Query support. We also want the initial query terms q k to have high weight in the optimal solution. We express this mathematically with simple box constraints, requiring x i to lie above a threshold threshold i for x i  X  Q as well as below the upper limit u i , which is 1 for all terms. Term-specific values for l i may also be desirable to reflect the rarity or ambiguity of individual query terms. We note that we could also implement query support using KL-divergence KL (  X  Q || x ) in a probabilistic model.

The role of query support constraints differs from inter-polation using  X  in model-based feedback since constraining feedback solutions to have high query support doesn X  X  pre-clude significant expansion term support if there are many strongly related terms. However, as Section 4 shows, our risk framework is effective at finding expansion models that are closer to the ideal in which the  X  X ight X  amount of empha-sis on the initial query terms is determined automatically. Our ultimate goal is to make model interpolation unneces-sary, but we continue using it here to study the potential for further improvements. minimize  X  p T x +  X  subject to Ax  X   X  Aspect balance (9) Figure 1: The basic constrained quadratic program REXP used for query expansion.
 Aspect coverage. This constraint is useful for retrieval oriented toward recall: it controls the absolute number of related words that are acceptable per query term. Similar to aspect balance, we denote the set of distances to neigh-boring words of query term q k by the vector g k =  X  k . The projection g k T x gives us the aspect coverage, or how well the words selected by the solution x  X  X over X  term q k . The more expansion terms near q k that are given higher weights, the larger this value becomes. We want the aspect coverage for each of the vectors g k to exceed a threshold  X  k , resulting in the constraint g k T x  X   X  k , for all query terms q k
Putting together reward and risk objectives with the above constraints we obtain the final quadratic program, which we call REXP and is shown in Figure 1. To show the flexibil-ity of our framework we give two useful extensions: setting budget constraints and finding top-k expansions. Since every expansion term with non-zero weight adds run-time cost for the search engine, we may prefer sparse solu-tions that minimize the number of expansion terms (while still respecting program constraints). More generally, some terms, such as those with very high frequency, may have a higher computation cost than rarer terms due to time re-quired to load inverted lists from disk. All else being equal, we prefer solutions with lower probable computation cost. We handle such scenarios by introducing a weighted  X  1 -norm penalty constraint with weight vector w . Non-zero entries for x i will be discouraged when the corresponding w i value is large. Since x  X  0 this leads to the budget constraint w
T x  X  y . We can either set y to a fixed upper bound, or we can add it as a  X  X oft X  constraint in the updated objective U ( x ) = R ( x ) +  X V ( x ) +  X y where  X  controls the relative importance of sparsity against risk and reward objectives. Setting w = 1 gives a standard  X  1 -norm solution. We can penalize common terms by setting w i  X  f ( t i ), where f ( t ) is some function of term frequency of t in the index. If sparsity is critical, with more computation time we can use reweighted  X  1 -norm minimization [13] to improve the initial  X  solution. In some applications, we may wish to find a set of the best k alternatives, either for presenting alternative expansions to a user, or as a measure of confidence in the optimal expansion: if the sub-optimal expansion scores are far from the optimal we have more confidence in our estimate.

To do this, given an optimal solution x  X  , we gather a set of near-optimal points x  X  by varying a rounding threshold  X  from 0 to 1 and setting entries with x  X  i  X   X  to zero. We then calculate the objective function U ( x  X  ) for each of these alternate expansion solutions and sort by descending U ( x selecting the top k expansions from this list 3 . Our work was initially inspired by the idea that query ex-pansion can be viewed as a type of portfolio allocation prob-lem under uncertainty , in which a set of financial securities must be selected that maximizes the expected return of the portfolio but also reduces risk by diversifying among dif-ferent industry sectors [18]. Typically, there is the option of buying a safe  X  X isk-free X  asset whose return is fixed and known in advance. Applied to IR, the user X  X  query (in the-ory) represents the  X  X isk-free asset X , and we can place bets on expansion terms 4 . IR has different task-specific constraints and estimating a  X  X ate of return X  for the user X  X  query may be problematic. Robust portfolio allocation methods are well-developed in the computational finance community but we have not seen much work in IR fully exploit this connection.
Our optimization constraints and objectives bring together several previous studies on the nature and causes of query drift [21][14]. Our constraint for aspect balance is based on observations from the 2003 RIA Workshop [4]. The empirically-derived Local Context Analysis (LCA) [29] in-cludes a weighting factor preferring expansion terms that co-occur with multiple query terms. This corresponds to the term centrality criterion in our model. Downweight-ing the contribution of correlated terms has been shown to improve results [21], and our correlation matrix  X  plays a similar role. Our query support constraint keeps the ex-pansion model  X  X lose X  to the query model, a condition that has been shown to be effective in other approaches [28][25]. The downside risk of query expansion has been noted for decades [23]: recently this problem has started to get some attention in evaluations [10][20][1] and we focus extensively on it here.

Optimization methods have been used implicitly and ex-plicitly for IR problems. Implicit use has been via the use of machine learning techniques such as Support Vector Ma-chines (SVM) Cao et al. [5] used an SVM to find good individual expansion terms but did not optimize over the expansion terms as a set, instead picking terms using a threshold. Explicitly, unconstrained optimization has been used for query expansion, typically using specialized code for a specific objective. Tao &amp; Zhai [25] used EM with a non-convex, unconstrained likelihood objective to find a lo-cally optimal expansion model, regularized using a prior that preferred models close to the initial query. Related work in smoothing [19] used gradient descent to smooth language models by minimizing a graph-based quadratic objective. To our knowledge, the use of robust optimization for IR-related problems has been limited to text classification [15].
In previous work [7] we introduced an early version of a Markowitz-type optimization framework for more reliable query expansion based on portfolio theory. Our work here greatly extends that initial study by providing the following contributions:
This problem has close connections to finding ambiguity groups in fault detection [30].
Naturally, the actual query may contain typos, misspellings or verbal disagreement. Here we assume that alterations like spelling correction have already been performed. More generally, an extensive development of risk-aware the-oretical models, algorithms, and evaluation methods was given in the author X  X  doctoral dissertation [8]. That work in-troduced the risk framework for query expansion described here and also discussed extensions to other areas of informa-tion retrieval. Recently, we note that Wang [27] applied a Markowitz-type mean-variance objective to balance risk and reward for document ranking. In this section we give an extensive analysis showing that applying REXP to a strong, state-of-the-art baseline expan-sion algorithm (Indri 2.2) not only consistently and substan-tially reduces the worst-case performance (downside risk) of the baseline expansion algorithm across queries, but does so without reducing its strong average performance . In other words, REXP greatly improves the stability of the baseline expansion algorithm without hurting its overall effectiveness. Moreover, we show that REXP is effective when applied to alternate baseline expansion algorithms: most notably, REXP is effective at attenuating noise when a very poor quality baseline is used as input to REXP instead (Sec. 4.6). Furthermore, we analyze the sensitivity of the algorithms X  X  risk-reward profile to changes in the various parameters and show that a single, consistent set of parameter settings for REXP works well for all collections in the study (Sec. 4.5).
Because our approach is a post-process that assumes lit-tle about the baseline expansion method used as input, we emphasize that the key performance question here is not how absolute expansion gain compares across other studies, but how much relative improvement we gain from applying REXP to an already strong baseline expansion algorithm.
We report results using standard retrieval measures, ro-bustness histograms, and risk-reward curves. Our evalua-tion uses six TREC topic sets, totaling 700 unique queries: TREC 1&amp;2 (topics 51 X 200), TREC 7 (topics 351 X 400), TREC 8 (topics 401 X 450), wt10g (topics 451 X 550), robust2004 (top-ics 301-450, 601 X 700), and gov2 (topics 701 X 850). We chose these corpora for their varied content and document proper-ties. Indexing and retrieval were performed using the Indri system in the Lemur toolkit [24][17]. Our queries were de-rived from the title field of the TREC topics and phrases were not used. We wrapped the initial query terms with In-dri X  X  #combine operator, performed Krovetz stemming, and used a stoplist of 419 common English words.

For our baseline expansion method, we used the default expansion method in Indri 2.2, which first selects terms using a log-odds calculation, then assigns final term weights using the Relevance Model [16]: document models were Dirichlet-smoothed with = 1000. We chose this baseline for its consistently strong average performance: for example, in a TREC evaluation using the GOV2 corpus [11], the Indri ex-pansion method gave a 19.8% gain in MAP over unexpanded queries, and achieved an average MAP gain of 14.4% over the six collections in this study. Indri X  X  feedback model is lin -early interpolated with the original query model weighted by a parameter  X  . By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter  X  = 0 . 5 unless otherwise stated.
We set the inputs to REXP as follows. For efficiency, we limited our vocabulary V to the top n = 100 expansion term candidates based on their Relevance Model probability. With these Indri term scores, the entries of the vector p were set using Eq. 1. The matrices  X , A , and g i were also calculated dynamically for each query using the definitions given in Section 2. The entries of  X  and g i are determined by the definition of the distance function d ( w i , w j ), which in turn is defined in terms of the similarity function  X  ( w Here, experiments used the perturbation kernel measure [6], but as discussed earlier a simpler method such as the Jaccard similarity may also be used. The matrix A is a | Q | X  K matrix, with each row being the feature vector  X  ( q i ) for query term q i . There is one vector g i for each query term q  X  Q , as defined in Sec. 2.3. We set  X  = 1 . 0 and  X  = 0 . 75 after experimenting with a subset of queries from the TREC 1&amp;2 and TREC 7&amp;8 collections: in general, a unified set of parameters appears to work well across all collections in this study, and this is discussed further in Section 4.5. Informally, a reward measure reflects the quality or relevance of results for a given query, in a way that is appropriate for the task 5 . Because we are interested in ad-hoc retrieval we use Average Precision (AP) as our default reward measure. We call an expansion failure a case where the reward mea-sure from applying expansion to a query is worse than the reward from the initial query results. Mathematically, we denote R I ( Q ) as the initial reward obtained with the query Q with no expansion, and R F ( Q ) as the final reward ob-tained when a query expansion algorithm is applied to Q .
The key aspects of a risk measure are: 1) that it captures variance or related negative aspect of retrieval performance across queries and 2) this variance/risk is based on the un-derlying  X  X eward X  measure chosen. To evaluate expansion algorithms, we assume the results from the initial query rep-resent our minimal acceptable retrieval performance: we do not want to obtain worse results than the initial query 6 are particularly interested in the downside risk of an algo-rithm, which we define as the reduction in reward due to expansion failures. Then the downside risk F F AIL ( Q ) for query Q is simply F When the reward measure is precision at the top k docu-ments (P@k) we define R-Loss at k as the net loss of relevant documents in the top-k due to failure . When the reward measure R I ( Q ) is average precision we refer to this simply as R-Loss , setting k to the size of the retrieved document
For example, Web search might use the average relevance of the top-ranked document (P1) while legal applications may focus on Recall.
Hypothetically, if we could reliably estimate the quality of the initial results, we could modify this assumption. set ( k = 1000 unless otherwise specified). Just as AP gives a combined picture of precision results averaged over multiple values of k , so the R-Loss measure gives an averaged net loss of relevant documents due to failure 7 . We use R-Loss as our default risk measure in this study. A risk-reward curve is generated by plotting a risk measure on the x axis, and a reward measure on the y axis, so that we can view how they trade off as the amount of expansion, con-trolled by the interpolation parameter  X  , is increased from  X  = 0 (no expansion at the origin) to  X  = 1 (all expansion, no initial query). The x -axis summarizes downside risk with R-Loss, the net loss in relevant documents lost due to ex-pansion failures. To emphasize the difference over using no expansion, the y -axis summarizes the change in reward aver-aged over all queries, that is the percentage MAP gain over using no expansion, so that all curves start at the origin (  X  = 0). We will typically plot in  X  increments of 0 . 1.
Risk-reward curves for both the Indri expansion baseline and the robust REXP algorithm are shown in Figure 2 for all six collections, using MAP as the reward measure with percentage MAP gain on the y -axis. The dashed line is the curve given by the strong baseline Indri expansion algorithm. The solid line is the curve of the resulting robust expansion after REXP is applied to the strong baseline Indri expansion algorithm. We enlarge the point at  X  = 0 . 5 since this is our default setting. As discussed further in Section 4.5, all results use the same unified set of parameters that was found to work well across all collections.
 We say that a tradeoff curve A dominates a second curve B if A is higher and to the left of B . An algorithm with a dominant curve gives the same or better reward for any given level of risk. It is evident that, except for one brief segment at the end of the Robust2004 curve, the REXP tradeoff curve dominates the corresponding baseline curve for every topic set. For  X  = 0 . 5, the robust algorithm loses fewer relevant documents for all collections, while achieving comparable or higher MAP gain compared to the baseline. Also, the optimal MAP gain for REXP is always higher than the corresponding optimal baseline MAP gain.
 For an alternate perspective, the risk-reward curves using Precision at 20 (P20) as reward measure instead of MAP are shown in Figure 3 for the same collections and algorithms. Note that while the baseline algorithm significantly hurts performance (negative P20 gain) for values of  X  close to 1 on five out of six collections, applying REXP results in a far more reliable expansion model that virtually never hurts P20 at any setting of  X   X  only on TREC 7, at the extreme  X  = 1, does it give a very small loss. Overall, applying REXP consistently (and sometimes dramatically) improves the P20 risk-reward tradeoff across all values of  X  for five out of six collections. As a general summary statistic for robustness we employ a very simple measure called the robustness index (RI). For a set of queries Q , the RI measure is defined as: RI ( Q ) = ( n +  X  n  X  ) / | Q | where n + is the number of queries helped (i.e. with positive AP gain after expansion) n  X  is the number of queries hurt, and | Q | the total number of queries.
This weights relevant documents equally, giving more weight to queries with more relevant documents. As with micro/macro-averaging, we could also define a normalized variant of R-Loss to weight all queries equally. for comparison.
 higher and to the left is better.
  X  ), aspect balance (AB:  X  ), covariance (COV:  X  ), query support (QT: l Figure 5: Histograms combining results for all six TREC collections, showing the robust REXP version hurts signifi-cantly fewer queries, seen by the greatly reduced tail on the left half (queries hurt). (Recall that MAP performance of REXP is also as good or better than the strong expansion baseline.) The histograms show counts of queries, binned by percent change in MAP, for the REXP algorithm (dark) and baseline (lighter).

Table 1 compares average precision, R-Loss, and RI statis-tics for the initial, baseline, and REXP feedback methods for specific choices of  X  = 0 . 5 (the standard setting). For all six collections, at  X  = 0 . 5 the average precision and P20 for REXP are statistically equal or superior to the Indri base-line expansion, while REXP also reduces the number of rel-evant documents in the top 20 lost to failures (R-Loss@20) by amounts ranging from 34.5% (TREC 8) to 76.9% (TREC 1&amp; 2). (Note that R-Loss is relative to initial retrieval and thus always zero for the no-expansion case.) The total num-ber of relevant documents is shown in the denominator of the R-Loss fraction. Looking at the simple fraction of net queries helped using the Robustness Index (RI), REXP at  X  = 0 . 5 outperforms the baseline at  X  = 0 . 5 on 5 out of 6 collections, and has equal performance for TREC 8. Robustness histograms provide a more detailed look at how badly queries were hurt and helped by an expansion algo-rithm. Figure 5 gives the combined robustness histogram across the six topic sets for REXP (dark) and the baseline (light). The worst failures  X  cases where a query X  X  average precision was hurt by more than 60%  X  have been virtu-ally eliminated by the REXP algorithm, while the upside gain distribution remains very similar to the baseline gains. The most noticeable differences in gains are a reduction in the highest category (more than 100% AP gain) and an in-crease in the lowest gains (0 to 10%). Both of these are due to the selective expansion mechanism of the REXP al-gorithm: queries that are deemed too risky to expand are not expanded, resulting in a zero AP gain. Because the REXP program uses several parameters, we provide a detailed study of how different choices in this parameter space affect performance and how sensitive the quality of the expansion solution is to changes in the param-eters. We also show that there is a single, consistent choice of parameters that works well for all six collections we tried.
Figure 4 summarizes the sensitivity of tradeoff curves to different constraint parameter values. The most dominant tradeoff curves were obtained using an intermediate mix of risk and reward objectives, with all constraints active. Some Figure 6: The effect on risk-reward tradeoff curves of ap-plying REXP (solid line) to a Rocchio-style expansion algo-rithm (dotted line) instead of the default Relevance model baseline. Tradeoff curves that are higher and to the left are better. Points are plotted in  X  -increments of 0 . 1, starting with  X  = 0 at the origin and increasing to  X  = 1 . 0. Figure 7: Risk-reward tradeoff curves for two representative TREC topic sets, showing the much greater tolerance of the convex REXP algorithm (solid) to noise from a poor baseline expansion algorithm (dashed). The point corresponding to  X  = 0 . 5 for each method is enlarged for visibility. Results for other collections are similar. constraints had a more dramatic effect on the tradeoff curve than others. Query support ( l i ) was a highly influential con-straint: it had a strong effect on MAP gain, but little effect on risk. Conversely, the use of off-diagonal  X  ij covariance entries (  X  ) had a larger effect on risk reduction (and a weaker effect on MAP). Activating both of these together resulted in most of the improvement in the REXP tradeoff curve. Other constraints such as the aspect balance constraint  X  were less critical but acted to further shrink the risk of the tradeoff curve with little reduction in MAP. Increasing the aspect coverage parameter  X  k also acted to increase the conserva-tivism of the solution. The role of  X  is similar to that of the interpolation  X  , controlling the mix between a solution close to the original query and one using all expansion terms. We used a setting for REXP parameter values that is effective on all collections: high query support ( i = 0.95), moderately relaxed aspect balance (  X  = 2 . 0), minimal aspect coverage constraint (  X  k = 0 . 1 for all q k ), medium covariance regular-ization (  X  = 0 . 75) and equal objective weighting (  X  = 1 . 0). To show the generality of REXP X  X  black-box approach and its tolerance to noise, we replaced the Indri baseline algo-rithm with a strong Rocchio-type method [22] based on a vector-space model, and a noisy idf-only version. Rocchio-type. We used a Rocchio-style vector space baseline in which the top k document vectors were given equal weight and used a tf.idf representation. For space reasons the tradeoff curves for two representative collections are shown in Figure 6: others are similar. As it did with the Relevance model baseline, REXP dominates the Roc-chio tf.idf baseline for wt10g. It also reduces R-Loss for trec12, while keeping average MAP gain comparable. High-noise Rocchio. When faced with a poor-quality ex-pansion baseline, a good selective algorithm should usually avoid expansion altogether and revert to the original query: REXP does indeed behave exactly this way. This baseline method is a noisy version of the Rocchio scheme that ignores term frequency ( tf ) and uses only idf in the term weight-ing, which results in a noisy expansion model dominated by rare terms that are poor discriminators for relevance. The results for the same two representative collections, TREC 7 and wt10g, are shown in Figure 7. This idf -only baseline has terrible performance, with MAP loss at  X  = 1 . 0 worse than -80%. However, REXP using this baseline almost completely attenuates the damage by scaling back to very conservative expansion. At  X  = 0 . 5, for TREC 7a, MAP loss is reduced from -11.8% to almost zero (0.88%) with reduction in R-Loss from 1136 to 390. For wt10g, MAP loss is reduced from -35.1% to -6.1% with reduction in R-Loss from 5485 to 1703. The other four standard collections have similar re-sults: REXP MAP loss at  X  = 0 . 5 is between 0% and -5%, versus baseline MAP loss of -20% to -40%. Based on our evaluation and observations, we believe there are three distinct capabilities that any expansion algorithm should have to be both reliable and effective. First, uncer-tainty in the data should be captured and applied to adjust the conservativeness of the solution from query-to-query. In our model this done via the robust problem X  X  uncertainty set U . Second, a  X  Shard  X  S selection process should eliminate im-plausible models completely: if necessary, all hypotheses ex-cept the observed query may be rejected. We implement this property by defining a feasible set using hard constraints, such as query support. Such constraints are very effective in attenuating noise when the baseline expansion model is very poor. With sparsity or budget constraints included, this dynamically chooses the number of top-k final expan-sion terms (including zero terms), rather than forcing us to choose a fixed k in advance. Third, a final process searches for the optimal model based on the objective function over the remaining good (feasible) models, effectively performing a kind of model combination over the space of feasible so-lutions. Current algorithms implement some of these, but beyond the present work, no existing algorithms that we are aware of effectively address all three requirements at once. The result of combining them is a reliable, selective, effective expansion algorithm. This paper contributes fundamental new tools for the devel-opment and evaluation of query expansion algorithms. By applying concepts from computational finance to cast query expansion as a robust constrained convex optimization prob-lem, we can bring the full power of this technology to bear on expansion problems in order to tradeoff risk and reward and handle domain constraints that would be difficult or im-possible using traditional expansion methods. We showed how we can model our uncertainty in important constraints by defining uncertainty sets and minimizing the optimal loss over the uncertainty set. This leads to conservative solutions by using robust optimization versions of the basic program, which turn out to have a simple, efficient analytical form. While most proposed improvements to query expansion only apply to a particular retrieval model, our algorithms treat the retrieval model as a black box which could be imple-mented using vector space models, inference networks, sta-tistical language modeling, or other approaches. Thus, the approach we have described is broadly applicable.
We also introduced risk-reward tradeoff curves, which we believe should be a standard evaluation method for query expansion algorithms. With these curves and other evalua-tion measures, we showed how the downside risk of existing algorithms can be significantly improved, with no loss of average upside gain. Furthermore, we showed that our ro-bust optimization method almost completely attenuates the damage caused by a poor baseline algorithm.

We also described extensions such as budget constraints and k -best expansions that fit easily into this framework. This work opens new research directions to explore fur-ther constraints and objectives, such as biasing expansions with personalization models or implicit and explicit rele-vance feedback. Finally, further gains may be possible with data-driven learning of constraints or objective parameters. We thank Stephen Boyd for valuable discussions on related work and real-time implementation issues; Jamie Callan, William Cohen, Susan Dumais, and John Lafferty for their extensive feedback on many aspects of this research; and Paul Bennett and Jaime Teevan for their helpful comments and editing suggestions. [1] G. Amati, C. Carpineto, and G. Romano. Query [2] A. Ben-Tal and A. Nemirovski. Robust solutions of [3] S. Boyd and L. Vandenberghe. Convex Optimization . [4] C. Buckley. Why current IR engines fail. In [5] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting [6] K. Collins-Thompson. Robust word similarity [7] K. Collins-Thompson. Estimating robust query models [8] K. Collins-Thompson. Robust model estimation [9] K. Collins-Thompson and J. Callan. Query expansion [10] K. Collins-Thompson and J. Callan. Estimation and [11] K. Collins-Thompson, P. Ogilvie, and J. Callan. Initial [12] E. N. Efthimiadis. Annual Review of Information [13] S. P. B. Emmanuel J. Cand`es, Michael B. Wakin. [14] D. Harman and C. Buckley. The NRRC Reliable [15] G. Lanckriet, L. El Ghaoui, C. Bhattacharyya, and [16] V. Lavrenko. A Generative Theory of Relevance . PhD [17] Lemur. Lemur toolkit for language modeling &amp; [18] H. M. Markowitz. Portfolio selection. Journal of [19] Q. Mei, D. Zhang, and C. Zhai. A general [20] D. Metzler and W. B. Croft. Latent concept expansion [21] M. Mitra, A. Singhal, and C. Buckley. Improving [22] J. Rocchio. The SMART Retrieval System , chapter [23] A. Smeaton and C. J. van Rijsbergen. The retrieval [24] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. [25] T. Tao and C. Zhai. Regularized estimation of mixture [26] J. Teevan, S. T. Dumais, and E. Horvitz.
 [27] J. Wang. Mean-variance analysis: A new document [28] M. Winaver, O. Kurland, and C. Domshlak. Towards [29] J. Xu and W. B. Croft. Improving the effectiveness of [30] A. Zymnis, S. Boyd, and D. Gorinevsky. Relaxed
