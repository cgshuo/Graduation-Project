 As online Collaborative Question Answering (CQA) services such as Yahoo! Answers and Baidu Knows are attracting users, questions, and answers at an explosive rate, the truly urgent and important questions are increasingly getting lost in the crowd. That is, questions that require immediate re-sponses are pushed out of the way by the trivial but more recently arriving questions. Unlike other questions in col-laborative question answering (CQA) for which users might be willing to wait until good answers appear, urgent ques-tions are likely to be of interest to the asker only if answered in the next few minutes or hours. For such questions, late responses are either not useful or are simply not applicable. Unfortunately, current collaborative question-answering sys-tems do not distinguish urgent questions from the rest, and could thus be ineffective for urgent information needs. We explore text-and data-mining methods for automatically identifying urgent questions in the CQA setting. Our re-sults indicate that modeling the question context (i.e., the particular forum/category where the question was posted) can increase classification accuracy compared to the text of the question alone.
 Categories and Subject Descriptors: H.3.4 Systems and Software: Question-answering systems.
 General Terms: Experimentation, Human Factors Keywords: Social Media, Community Question Answering
Current Collaborative Question Answering (CQA) services treat all questions equally: all newly arriving questions are posted earlier in the respective category pages, pushing down the earlier ones. Some sorting functionality is often provided, that so far has been limited to question popularity of var-ious forms (such as number of answers or average question rating). However, other dimensions of the question, such as subjectivity or quality or urgency, are not easy to determine and therefore are not available for searching or browsing the available questions. Furthermore, urgent questions can eas-ily become lost in the flood of the newly arriving questions, and may never obtain the needed answers.

We envision a system where urgent questions could be pro-moted to the front page of a CQA to receive timely response. This would attract serious and high quality questions and eventually allow CQA to become a robust and reliable plat-form for information seeking. The following examples illus-question. The intuition is that for urgent questions, users will post answers quicker than for non-urgent ones.
We experimented with a large sample of approximately 261,000 questions drawn from over 100 categories on the Yahoo! Answers site, described in [1]. Since labeling all the questions in the set for urgency would be prohibitive, we de-cided to first identify a smaller set of candidates that may be urgent, for subsequent manual verification. For this, we created a set of regular expression such as /need (an)? an-swer(s)? (by tonight|by tomorrow|soon)/ , to find poten-tially urgent questions.

We then used the Amazon Turk service 2 to get human annotations, i.e. Urgent vs. Not Urgent . We used a majority threshold of at least 4 out of 5 workers, and finally obtained 867 rated questions where the supermajority of the Amazon workers agreed on the question classification (and discarded those where consensus was not reached).
 Table 1: Question dataset, labeled for urgency.

To complement the set of likely urgent questions above, we also included the likely non-urgent questions, by select-ing an additional 867 questions which were closed by the asker after at least 3 days, and labeled them as Not Urgent . The intuition is that these questions were still useful and interesting to the asker more than three days after posting. Table 1 reports the statistics for our labeled dataset. We apply supervised classification methods, namely the SVM and C4.5 decision tree implementations from Weka, SMO and J48, respectively, over the feature sets described above. Table 2 reports classification results over the dataset above with 5-fold cross validation. The results indicate that using the question text information alone results in reason-able classification performance. Adding category features can slightly improve performance of SVM. Interestingly, fea-tures from answers are not helpful. With answer arrival time and answer text included, classification performance slightly drops, which is consistent with our hypothesis that current CQA services are not sufficiently prioritizing urgent ques-tions to influence the answering behavior.

To better understand the utility of the specific features, we report the Information Gain (IG) of top 10 features in Ta-ble 3. Of these, 7 are the category features (indicated by the http://www.mturk.com
