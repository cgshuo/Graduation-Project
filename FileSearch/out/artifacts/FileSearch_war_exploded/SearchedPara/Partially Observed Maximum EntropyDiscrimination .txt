  X  State Key Lab of Intelligent Tech &amp; Sys, Tsinghua National TNList Lab, Dept. Comp Sci &amp; Tech,  X  Tsinghua University, Beijing China. jun-zhu@mails.thu.edu.cn ; dcszb@thu.edu.cn School of Comp. Sci., Carnegie Mellon University, Pittsburgh, PA 15213, epxing@cs.cmu.edu Inferring structured predictions based on high-dimensional, often multi-modal and hybrid covari-ates remains a central problem in data mining (e.g., web-info extraction), machine intelligence (e.g., machine translation), and scientific discovery (e.g., genome annotation). Several recent approaches to this problem are based on learning discriminative graphical models defined on composite fea-tures that explicitly exploit the structured dependencies among input elements and structured in-terpretational outputs. Different learning paradigms have been explored, including the maximum conditional likelihood [7] and max-margin learning [2, 12, 13], with remarkable success. However, the problem of structured input/output learning can be intriguing and significantly more difficult when there exist hidden substructures in the data, which is not uncommon in realistic prob-lems. As is well-known in the probabilistic graphical model literature, hidden variables can facili-tate natural incorporation of structured domain knowledge such as latent semantic concepts or unob-served dependence hierarchies into the model, which can often result in more intuitive representation and more compact parameterization of the model; but learning a partially observed model is often non-trivial because it involves optimizing against a more complex cost function, which is usually not convex and requires additional efforts to impute or marginalize out hidden variables. Most exist-ing work along this line, such as the hidden CRF for object recognition [9] and scene segmentation [14] and the dynamic hierarchical MRF for web data extraction [18], falls in the likelihood-based learning. For the max-margin learning, which is arguably a more desirable discriminative learning paradigm in many application scenarios, learning a Makov network with hidden variables can be extremely difficult and little work has been done except [11], where, in order to obtain a convex pro-gram, the uncertainty in mixture modeling is simplified by a reduction using the MAP component. A major reason for the difficulty of considering latent structures in max-margin models is the lack of a natural probabilistic interpretation of such models, which on the other hand offers the key insight in likelihood-based learning to design algorithms such as EM for learning partially observed mod-els. Recent work on semi-supervised or unsupervised max-margin learning [1, 4, 16] was all short of an explicit probabilistic interpretation of their algorithms of handling latent variables. The recently proposed Maximum Entropy Discrimination Markov Networks (MaxEnDNet) [20, 19] represent a key advance in this direction. MaxEnDNet offers a general framework to combine Bayesian-style learning and max-margin learning in structured prediction. Given a prior distribution of a structured-prediction model, and leveraging a new prediction-rule that is based on a weighted average over an ensemble of prediction models, MaxEnDNet adopts a structured minimum relative entropy prin-ciple to learn a posterior distribution of the prediction model in a subspace defined by a set of ex-pected margin constraints. This elegant combination of probabilistic and maximum margin concepts provides a natural path to incorporate hidden structured variables in learning max-margin Markov networks (M 3 N), which is the focus of this paper.
 It has been shown in [20] that, in the fully observed case, MaxEnDNet subsumes the standard M 3 N [12]. But MaxEnDNet in its full generality offers a number of important advantages while retaining all the merits of the M 3 N. For example, structured prediction under MaxEnDNet is based on an av-eraging model and therefore enjoys a desirable smoothing effect, with a uniform convergence bound on generalization error, as shown in [20]; MaxEnDNet admits a prior that can be designed to intro-duce useful regularization effects, such as a sparsity bias, as explored in the Laplace M 3 N [19, 20]. In this paper, we explore yet another advantage of MaxEnDNet stemmed from the Bayesian-style max-margin learning formalism on incorporating hidden variables. We present the partially ob-served MaxEnDNet (PoMEN), which offers a principled way to incorporate latent structures carry-ing domain knowledge and learn a discriminative model with partially labeled data. The reducibil-ity of MaxEnDNet to M 3 N renders many existing convex optimization algorithms developed for learning M 3 N directly applicable as subroutines for learning our proposed model. We describe an EM-style algorithm for PoMEN based on existing algorithms for M 3 N. As a practical application, we apply the proposed model to a web data extraction task X  X roduct information extraction, where collecting fully labeled training data is very difficult. The results show the promise of max-margin learning as opposed to likelihood-based estimation in the presence of hidden variables. The paper is organized as follows. Section 2 reviews the basic max-margin structured prediction formalism and MaxEnDNet. Section 3 presents the partially observed MaxEnDNet. Section 4 applies the model to real web data extraction, and Section 5 brings this paper to a conclusion. Our goal is to learn a predictive function h : X7 X  X  from a structured input x  X  X  to a structured output y  X  X  , where Y = Y 1  X  X  X  X  X Y l represents a combinatorial space of structured interpretations of multi-facet objects. For example, in part-of-speech (POS) tagging, Y i consists of all the POS tags and each label y = ( y 1 ,  X  X  X  , y l ) is a sequence of POS tags, and each input x is a sentence (word sequence). We assume that the feasible set of labels Y ( x )  X  X  is finite for any x .
 Let F ( x , y ; w ) be a parametric discriminant function. A common choice of F is a linear model, where F is defined by a set of K feature functions f k : X X Y7 X  R and their weights w k : F ( x , y ; w ) = w &gt; f ( x , y ) . A commonly used predictive function is: By using different loss functions, the parameters w can be estimated by maximizing the conditional likelihood [7] or by maximizing the margin [2, 12, 13] on labeled training data. 2.1 Maximum margin Markov networks Under the M 3 N formalism, which we will generalize in this paper, given a set of fully labeled training data D = { ( x i , y i ) } N i =1 , the max-margin learning [12] solves the following optimization problem and achieves an optimum point estimate of the weight vector w : where  X  i represents a slack variable absorbing errors in training data, C is a positive constant, R + denotes non-negative real numbers, and F 0 is the feasible space for w : F 0 = { w : w &gt;  X  f i ( y )  X  between the true label y i and a prediction y , and  X  ` i ( y ) is a loss function with respect to y i . Various loss functions have been proposed for P0. In this paper, we adopt the hamming loss [12]: true and 0 otherwise. The optimization problem P0 is intractable because of the exponential number of constraints in F 0 . Exploring sparse dependencies among individual labels y i in y , as reflected in the specific design of the feature functions (e.g., based on pair-wise labeling potentials), efficient optimization algorithms based on cutting-plane [13] or message-passing [12], and various gradient-based methods [3, 10] have been proposed to obtain approximate solution to P0. As described shortly, these algorithms can be directly employed as subroutines in solving our proposed model. 2.2 Maximum Entropy Discrimination Markov Networks Instead of predicting based on a single rule F (  X  ; w ) as in M 3 N using w , the structured maximum entropy discrimination formalism [19] facilitates a Bayes-style prediction by averaging F (  X  ; w ) over a distribution of rules according to a posterior distribution of the weights, p ( w ) : where p ( w ) is learned by solving an optimization problem referred to as a maximum entropy dis-crimination Markov network (MaxEnDNet, or MEN) [20] that elegantly combines Bayesian-style learning with max-margin learning. In a MaxEnDNet, a prior over w is introduced to regularize its distribution, and the margins resulting from predictor (3) are used to define a feasible distribution subspace. More formally, given a set of fully observed training data D and a prior distribution p ( w ) , MaxEnDNet solves the following problem for an optimal posterior p ( w |D ) or p ( w ) : or regularized KL-divergence, and U (  X  ) is a closed proper convex function over the slack variables  X  . U is also known as an additional  X  X otential X  term in the maximum entropy principle. The feasible distribution subspace F 1 is defined as follows: where  X  F i ( y ; w ) = F ( x i , y i ; w )  X  F ( x i , y ; w ) .
 P1 is a variational optimization problem over p ( w ) in the feasible subspace F 1 . Since both the KL-divergence and the U function in P1 are convex, and the constraints in F 1 are linear, P1 is a convex program. Thus, one can apply the calculus of variations to the Lagrangian to obtain a variational extremum, followed by a dual transformation of P1. As proved in [20], solution to P1 leads to a GLIM for p ( w ) , whose parameters are closely connected to the solution of the M 3 N. Theorem 1 (MaxEnDNet (adapted from [20])) The variational optimization problem P1 under-lying a MaxEnDNet gives rise to the following optimum distribution of Markov network parameters: where Z (  X  ) is a normalization factor and the Lagrangian multipliers  X  i ( y ) (corresponding to constraints in F 1 ) can be obtained by solving the following dual problem of P1 : variance matrix I , where the Lagrangian multipliers  X  i ( y ) can be obtained by solving problem D1 of the form that is isomorphic to the dual of M 3 N. When applying this p ( w ) to Eq. (3), one can obtain a predictor that is identical to that of the M 3 N.
 From the above reduction, it should be clear that M 3 N is a special case of MaxEnDNet. But the MaxEnDNet in its full generality offers a number of important advantages while retaining all the merits of the M 3 N. First , the MaxEnDNet prediction is based on model averaging and therefore enjoys a desirable smoothing effect, with a uniform convergence bound on generalization error, as shown in [20]. Second , MaxEnDNet admits a prior that can be designed to introduce useful regular-ization effects, such as a sparsity bias, as explored in the Laplace M 3 N [19, 20]. Third , as explored in this paper, MaxEnDNet offers a principled way to incorporate hidden generative models underly-ing the structured predictions, but allows the predictive model to be discriminatively trained based on partially labeled data. In the sequel, we introduce partially observed MaxEnDNet (PoMEN), that combines (possibly latent) generative model and discriminative training for structured prediction. Consider, for example, the problem of web data extraction, which is to identify interested informa-tion from web pages. Each sample is a data record or an entire web page which is represented as a set of HTML elements. One striking characteristic of web data extraction is that various types of struc-tural dependencies between HTML elements exist, e.g. the HTML tag tree or the Document Object Model (DOM) structure is itself hierarchical. In [17], fully observed hierarchical CRFs are shown to have great promise and achieve better performance than flat models like linear-chain CRFs [7]. One method to construct a hierarchical model is to first use a parser to construct a so called vision tree [17]. For example, Figure 1(b) is a part of the vision tree of the page in Figure 1(a). Then, based on the vision tree, a hierarchical model can be constructed accordingly to extract the interested at-tributes, e.g. a product X  X  name, image, price, description, etc. In such a hierarchical extraction model, inner nodes are useful to incorporate long distance dependencies, and the variables at one level are refinements of the variables at upper levels. To reflect the refinement relationship, the class labels defined as in [17] are also organized in a hierarchy as in Figure 1(c). Due to concerns over labeling cost and annotation-ambiguity caused by the overlapping of class labels as in Figure 1(c), it is desirable to effectively learn a hierarchical extraction model with partially labeled data. Without loss of generality, assume that the structured labeling of a sample consists of two parts X  X n observed part y and a hidden part z . Both y and z are structured labels, and furthermore the hidden variables are not isolated, but are statistically dependent on each other and on the observed data condition as in CRFs [7]. Following the spirit of a margin-based structured predictor such as M 3 N, we employ only the unnormalized energy function F ( x , y , z ; w ) (which usually consists of linear combinations of feature functions or potentials) as the cost function for structured prediction, and we adopt a prediction rule directly extended from the MaxEnDNet X  X verage over all the possible models defined by different w , and at the same time marginalized over all hidden variables z . That is, Now our problem is learning the optimum p ( w , z ) from data. Let { z } X  ( z 1 , . . . , z N ) denote the ensemble of hidden labels of all the samples. Analogous to the setup for learning the MaxEnDNet, we specify a prior distribution p 0 ( { z } ) over all the hidden structured labels. The feasible space F 2 of p ( w , { z } ) can be defined as follows according to the margin constraints: p ( w , { z } ) on a single sample, which will be used in (6) to compute the structured prediction. Again we learn the optimum p ( w , { z } ) based on a structured minimum relative entropy principle as in MaxEnDNet. Specifically, let p 0 ( w , { z } ) represent a given joint prior over the parameters and the hidden variables, we define the PoMEN problem that gives rise to the optimum p ( w , { z } ) : Analogous to P1, P2 is a variational optimization problem over p ( w , { z } ) in the feasible space F 2 . Again since both the KL and the U function in P2 are convex, and the constraints in F 2 are linear, P2 is a convex program. Thus, we can employ a technique similar to that used to solve MaxEnDNet to solve the PoMEN problem. 3.1 Learning PoMEN For a fully general p ( w , { z } ) where hidden variables in all samples are coupled, solving P2 based on an extension of Theorem 1 would involve very high-dimensional integration and summation that is in practice intractable. In this paper we consider a simpler case where the hidden labels of different samples are iid and independent of the parameter w in both the prior and the posterior distributions, will hold true in a graphical model where w corresponds to only the observed y variables at the bottom of a hierarchical model. For many practical applications such as the hierarchical web-info extraction, such a model is realistic and adequate. For more general models where dependencies are more global, we can use the above factored model as a generalized mean field approximation to the true distribution, but this extension is beyond the scope of this paper, and will be explored later in the full paper. Generalizing Theorem 1, following a coordinate descent principle, now we present an alternating minimization (EM-style) procedure for P2: Step 1 : keep p ( z ) fixed, infer p ( w ) by solving the following problem: generalized version of F 1 with hidden variables. Thus, we can apply the same convex optimization techniques as being used for solving the problem P1. Specifically, assume that the prior distribution p  X  are achieved by solving a dual problem: where P ( C ) = {  X  : P y  X  i ( y ) = C ;  X  i ( y )  X  0 ,  X  i,  X  y } . This dual problem is isomorphic to the dual form of the M 3 N optimization problem, and we can use existing algorithms developed for M 3 N, such as [12, 3] to solve it. Alternatively, we can solve the following primal problem via employing existing subgradient [10] or cutting plane [13] algorithms: version of F 0 . It is easy to show that the solution to this primal problem is the posterior mean of p ( w ) , which will be used to make prediction in the predictive function h 2 . Note that the primal problem is very similar to that of M 3 N, except the expectations in F 0 0 . This is not surprising since it can be shown that M 3 N is a special case of MaxEnDNet. We will discuss how to efficiently compute the expectations E p ( z ) [ X  f i ( y , z )] in Step 2.
 Step 2 : keep p ( w ) fixed, based on the factorization assumption p ( { z } ) = Q i p ( z i ) and p ( { z } ) = Q i p 0 ( z i ) , the distribution p ( z ) for each sample i can be obtained by solving the following problem:  X   X  i ,  X  y } . Similarly, by introducing a set of Lagrangian multipliers  X  ( y ) , we can get: and the dual variables  X  ( y ) can be obtained by solving the following dual problem: where P i ( C ) = { P y  X  ( y ) = C,  X  ( y )  X  0 ,  X  y } . This non-linear constrained optimization problem can be solved with existing solvers, like IPOPT [15]. With a little algebra, we can compute the gradients as follows: dients. We make a gentle assumption that the prior distribution p 0 ( z ) is an exponential distribution of the following form: This assumption is general enough for our purpose, and covers the following commonly used priors: i. Log-linear Prior : defined by a set of feature functions and their weights. For ex-iii. Markov Prior : the prior model have the Markov property w.r.t the model X  X  structure. For exam-With the above assumption, p ( z ) is an exponential family distribution, and the expectations, E to compute marginal probabilities, e.g. p ( z i ) and p ( z i , z j ) in pairwise Markov networks. When the model X  X  tree width is not large, this can be done exactly. For complex models, approximate inference like loopy belief propagation and variational methods can be applied. However, since the number of constraints in (12) is exponential to the size of the observed labels, the optimization problem cannot be efficiently solved. A key observation, as explored in [12], is that we can interpret  X  ( y ) as a prob-ability distribution of y because of the regularity constraints: P y  X  ( y ) = C,  X  ( y )  X  0 ,  X  y . Thus, we can introduce a set of marginal dual variables and transfer the dual problem (12) to an equivalent form with a polynomial number of constraints. The derivatives with respect to each marginal dual parameter is of the same structure as the above gradients. We apply PoMEN to the problem of web data extraction, and compare it with partially observed CRFs (PoHCRF) [9], and fully observed hierarchical CRFs (HCRF) [17] and hierarchical M 3 N (HM 3 N) which has the same hierarchical model structure as the HCRF. 4.1 Data Sets, Evaluation Criteria, and Prior for Latent Variables We concern ourselves with the problem of identifying product items for sale on the web. For each product item, four attributes  X  Name , Image , Price , and Description are extracted in our experiments. The evaluation data consists of product web pages generated from 37 different templates. For each template, there are 5 pages for training and 10 for testing. We evaluate all the methods on two different levels of inputs, record level and page level . For record-level evaluation, we assume that data records are given, and we compare different models on accuracy of extracting attributes in the given records. For page-level evaluation, the inputs are raw web pages and all the models perform Figure 2: (a) The F1 and block instance accuracy of record-level evaluation from 4 models under different both record detection and attribute extraction simultaneously as in [17]. In the 185 training pages, there are 1585 data records in total; in the 370 testing pages, 3391 data records are collected. As for evaluation criteria, we use the standard precision, recall, and their harmonic value F1 for each attribute and the two comprehensive measures, i.e. average F1 and block instance accuracy, as defined in [17]. We adopt an independent prior described earlier for the latent variables, each factor p ( z i ) over a single latent label is assumed to be uniform. 4.2 Record-Level Evaluation In this evaluation, partially observed training data are the data records whose leaf nodes are labeled and inner nodes are hidden. We randomly select m = 5 , 10 , 20 , 30 , 40 , or , 50 percent of the training records as training data, and test on all the testing records. For each m , 10 independent experiments were conducted and the average performance is summarized in Figure 2. From Figure 2(a), it can be seen that the HM 3 N performs slightly better than HCRF trained on fully labeled data. For the two partially observed models, PoMEN performs much better than PoHCRF in both average F1 and block instance accuracy, and with lower variances of the score, especially when the training set is small. As the number of training data increases, PoMEN performs comparably w.r.t. the fully observed HM 3 N. For all the models, higher scores and lower variances are achieved with more training data. Figure 2(b) shows the F1 score on each attribute. Overall, for attributes Image , Price , and Description , although all models generally perform better with more training data, the improvement is small; and the differences between different models are small. This is possibly because the features of these attributes are usually consistent and distinctive, and therefore easier to learn and predict. For the attribute Name , however, a large number of training data are needed to learn a good model because its underlying features have diverse appearance on web pages. 4.3 Page-Level Evaluation Experiments on page-level prediction is conducted similarly as above, and the results are summa-rized in Figure 3. Two different partial labeling strategies are used to generate training data. ST1 : label the leaf nodes and the nodes that represent data records; ST2 : label more information based on ST1, e.g., label also the nodes above the  X  X ata Record X  nodes in the hierarchy as in Figure 1(c). Due to space limitation, we only report average F1 and block instance accuracy.
 For ST1 , PoMEN achieves better scores and lower variances than PoHCRF in both average F1 and block instance accuracy. The HM 3 N performs slightly better than HCRF (both trained on full label-ing), and PoMEN performs comparably with the fully observed HCRF in block instance accuracy. For ST2 , with more supervision information, PoHCRF achieves higher performance that is compa-rable to that of HM 3 N in average F1, but slightly lower than HM 3 N in block instance accuracy. For the latent models, PoHCRF performs slightly better in average F1, and PoMEN does better in block instance accuracy; moreover, the variances of PoMEN are much smaller than those of PoHCRF in both average F1 and block instance accuracy. We can also see that PoMEN does not change much when additional label information is provided in ST2 . Thus, the max-margin principle could provide a better paradigm than the likelihood-based estimation for learning latent hierarchical models. For the second step of learning PoMEN, the IPOPT solver [15] was used to compute the distribution p ( z ) . Interestingly, the performance of PoMEN does not change much during the iteration, and our results were achieved within 3 iterations. It is possible that in hierarchical models, since inner variables usually represent overlapping concepts, the initial distribution are already reasonably good to describe confidence on the labeling due to implicit consistence across the labels. This is unlike the multi-label learning [6] where only one of the multiple labels is true and during the iteration more probability mass should be redistributed on the true label during the EM iterations. We have presented an extension of the standard max-margin learning to address the challenging problem of learning Markov networks with the existence of structured hidden variables. Our ap-proach is a generalization of the maximum entropy discrimination Markov networks (MaxEnDNet), which offer a general framework to combine Bayesian-style and max-margin learning and subsume the standard M 3 N as a special case, to consider structured hidden variables. For the partially ob-served MaxEnDNet, we developed an EM-style algorithm based on existing convex optimization algorithms developed for the standard M 3 N. We applied the proposed model to a real-world web data extraction task and showed that learning latent hierarchical models based on the max-margin principle could be better than the likelihood-based learning with hidden variables.
 Acknowledgments
