 In recent years, personalized web search (PWS) has demonstrated effectiveness in improving the quality of search service on the In-ternet. Unfortunately, the need for collecting private information in PWS has become a major barrier for its wide proliferation. We study privacy protection in PWS engines which capture personali-ties in user profiles. We propose a PWS framework called UPS that can generalize profiles in for each query according to user-specified privacy requirements. Two predictive metrics are proposed to eval-uate the privacy breach risk and the query utility for hierarchical user profile. We develop two simple but effective generalization algorithms for user profiles allowing for query-level customization using our proposed metrics. We also provide an online prediction mechanism based on query utility for deciding whether to person-alize a query in UPS. Extensive experiments demonstrate the effi-ciency and effectiveness of our framework.
 H.3.3 [ Information Search and Retrieval ]: Search process; H.3.m [ Miscellaneous ]; D.2.8 [ Software Engineering ]: Metrics X  com-plexity measures, performance measures Experimentation, Measurement Personalized, Privacy, Metric, Utility, Trade-off
Personalized web search (PWS) is a general category of search techniques which provide search results tailored for individual user needs. As a price, user information has to be collected and analyzed to figure out the user intention behind the issued query. The solu-tions to PWS can generally be categorized into two types, namely click-log-based methods and profile-based ones. Although there are pros and cons for both types [ 5], the profile-based PWS has demonstrated more effectiveness in improving the quality of web search recently, with increasing usa ge of personal and behavior in-formation to profile its users, which is usually gathered implicitly from query history [ 22, 19, 21], browsing history [ 20, 17], click-through data [ 16, 12, 5], bookmarks [ 10], user documents [ 22, 27] and so forth. Unfortunately, this implicitly collected personal data can easily reveal a gamut of the user X  X  private life. Privacy issues rising from the lack of protection for such data, for instance the AOL query logs scandal[ 7], not only cause panic among individual users, but also dampen the data-publisher X  X  enthusiasm in offering personalized service. In fact, privacy concerns have become the major barrier for wide proliferation of PWS services.

To address this privacy threat, researchers of profile-based PWS have to consider two contradicting effects during the search pro-cess. On one hand, they attempt to improve the search quality with the personalization utility of the user profile. On the other hand, they need to hide the privacy existing in the user profile to min-imize the risk of privacy breach. A few previous studies [27 , 9] suggest that people may compromise privacy if the personalization yields them profitability in service quality. In an ideal case, sig-nificant gain in personalization would be easily traded with only a small portion, often referred to as a generalization , of the user pro-file. Thus, user privacy can be protected without compromising the personalized search quality.

Unfortunately, these previous works do not address privacy pro-tection in PWS adequately. The problems with the existing meth-ods are explained in the following observations. (1) The existing works in privacy protection of PWS generalizes each user profile for only once off line, and then use Such profile to personalize all queries from a same user indiscriminatingly. The  X  X ne profile fits all X  strategy certainly has drawbacks given a va-riety of queries. One evidence reported in [5 ] is that profile-based personalization may not even help to improve the search quality for some ad-hoc queries, though exposing user profile to a server could put the user X  X  privacy at risk. Therefore, a query-oriented decision on generalization degree, and even on whether personal-ize the search or not, would help to provide flexible service. To the best of our knowledge, no previous work has supported such feature. (2) The current solutions do not take into account the customiza-tion of privacy requirements, probably making some user profiles to be over-generalized while some others insufficiently generalized. For example, in [ 27], all the sensitive topics are detected using an absolute metric called surprisal based on the information theory, as-suming that the interests with less user document support are more sensitive. However, this assumption may not be correct. If a user has a large number of documents about  X  X ex X , the surprisal of this topic may lead to a conclusion that  X  X ex X  is very general and not sensitive, despite the truth which is opposite. Unfortunately, few prior work can effectively address individual privacy needs during the generalization. (3) Many personalization techniques require iterative user inter-actions when creating personalized search results. For example, some click-based metrics as rank scoring [2]and average rank [12], etc., are used to refine the search results through multiple query iterations. This paradigm is however infeasible as user pro-file generalization is needed for protecting privacy for each query. This inevitably pose a new challenge in terms of efficiency. Thus we need predictive metrics for the search quality and breach risk after personalization, without incurring iterative user interaction.
To solve the above problems, our approach customizes profile generalization on two levels, namely the user-level and the query-level .The user-level customization allows users to specify person-alized privacy requirements. The query-level customization varies the generalization granularity based on the contents of the query. Meanwhile, our approach guarantees the efficiency during the gen-eralization as it has to be applied online.

We perform the online generalization based on two conflicting metrics, namely the query utility and the risk of privacy disclosure , defined for user profiles, which are captured in a hierarchical tax-onomy. The basic idea is to maintain the complete user profile on the client side, and exploit the semantic information contained in the taxonomy to model user-specified privacy requirements. Given a query, a local search wrapper employs a generalization algorithm which attempts to strike a balance between the two metrics, while respecting the privacy requirements. The output is then exposed to the server to personalize the search results. Our main contributions are summarized as following.
The rest of this paper is organized as follows. Section 2 reviews the related work, focusing on PWS and its privacy preservation. Section 3 briefly describes the UPS architecture. Section 4 intro-duces the user profile data and gives the problem statement. The generalization techniques used in UPS are proposed in Section 5. The experimental results and findings are reported in Section 6.Fi-nally, Section 7 concludes the paper.
In this section, we overview the related works. We focus on the literature of profile-based personalization and privacy protection in PWS system.
Previous works on profile-based PWS mainly focus on improv-ing the search utility. The basic idea of these works is to tailor the search results by referring to, often implicitly, a user profile that reveals an individual information goal. In the remainder of this subsection, we review the previous solutions to PWS on two as-pects, namely the representation of profiles, and the measure of the effectiveness of personalization.

Many profile representations are available in the literature to fa-cilitate different personalization strategies. Earlier techniques uti-lize term lists/vectors [ 20]orbagofwords[ 22] to represent their profile. However, most recent works build profiles in hierarchical structures due to their stronger descriptive ability, b etter scalabil-ity, and higher access efficiency. The majority of the hierarchical representations are constructed with existing weighted topic hier-archy/graph, such as ODP 1 [5, 4, 19, 11], Wikipedia 2 [6, 14], etc. Another work in [ 27] builds the hierarchical profile automatically via term-frequency analysis on t he user data. In our proposed UPS framework, we do not focus on the implementation of the user pro-files. Actually, our framework can potentially adopt any hierarchi-cal representation based on a taxonomy of knowledge.

As for the performance measures of PWS in the literature, Nor-malized Discounted Cumulative Gain (nDCG) [ 8] is a common measure of the effectiveness of an information retrieval system. It is based on a human-graded relevance scale of item-positions in the result list, and is therefore known for its high cost in explicit feed-back collection. To reduce the human involvement in performance measuring, researchers also propose other metrics of personalized web search that rely on clicking decisions, including Average Pre-cision [1, 27], Rank Scoring [2], and Average Rank [19, 12]. We use the Average Precision metric, proposed by [ 5], to measure the effectiveness of the personalization in UPS. Meanwhile, our work is distinguished from previous studies as it also proposes two pre-dictive metrics, namely search utility and risk of privacy disclosure , on a profile instance without requesting for user feedback.
Generally there are two classes of privacy protection problems for PWS. One class includes those treat privacy as the identification of an individual, as described in [ 18]. The other includes those consider the sensitivity of the data, particularly the user profiles, exposed to the PWS server.

Typical works in the literature of protecting user identifications (class one) try to solve the privacy problem on different levels, in-cluding the pseudo-identity ,the group identity , no identity ,and no personal information . Solution to the first level is proved to frag-ile[ 7]. The third and fourth levels are impractical due to high cost in communication and cryptography. Therefore, the existing ef-forts focus on the second level. Both [ 26]and[ 28] provide online anonymity on user profiles by generating a group profile of k users. Using this approach, the linkage between the query and a single user is broken. In [ 3], the useless user profile (UUP) protocol is pro-posed to shuffle queries among a group of users who issue them. As a result any entity cannot profile a certain individual. These works all assume the existence of a trustworthy third-party anonymizer, which is not readily available over the Internet at large. The solutions in class two do not require third-party assistance. In these solutions, users only trust themselves and cannot tolerate the exposure of their complete profiles an anonymity server. In [ 9], Krause et al. employ statistical techniques to learn a probabilistic model, and then use this model to generate the near-optimal partial profile. One main limitation in this work is that it builds the user profile as a finite set of attributes, and the probabilistic model is trained through predefined frequent queries. These assumptions are impractical in the context of PWS. Xu et al. proposed a privacy pro-
Open Directory Project (ODP), http://dmoz.org/
Wikipedia, the Free Encyclopedia, http://www.wikipedia.org/ tection solution for PWS based on hierarchical profiles [ 27]. Using a user-specified threshold, a generalized profile is obtained in ef-fect as a rooted subtree of the complete profile. Unfortunately, this work does not address the query utility, which is crucial for the ser-vice quality of PWS. For comparison, our approach takes both the privacy requirement and t he query u tility into account.
A more important property which distinguishes our work from [27] is that we provide personalized privacy protection in PWS. The concept of personalized privacy protection is first introduced by Xiao et al. [ 24] in Privacy-Preserving Data Publishing (PPDP). A person can specify the degree of privacy protection for her/his sensitive values by specifying  X  X uarding nodes X  in the taxonomy of the sensitive attribute. Motivate by this, we allow users to cus-tomize privacy needs in their hierarchical user profiles.
Aside from the above works, a couple of recent studies have raised an interesting question which concerns the privacy protec-tioninPWS.Theworksin[ 5, 23] have found that personaliza-tion may have different effects on different queries. Queries with larger click-entropies , namely distinct queries, are expected to ben-efit more from personalization, while those with smaller values (ambiguous ones) are not. Moreover, the latter may even cause pri-vacy disclosure. Therefore, the need for personalization becomes questionable for such queries. Teevan et al. [ 23] collect a set of fea-tures of the query to classify queries by their click-entropy. While these works are motivative in questioning whether to personalize or not to, they assume the availability of massive user query logs (on the server side) and user feedback. In our UPS framework, we differentiate distinct queries from ambiguous ones based on a client-side solution using the predictive query utility metric.
The UPS framework consists of a non-trusty search engine server and a number of clients. As illustrated in Figure 1, each client user accessing the search service trusts no one but himself/herself. The key facility for privacy protection is an online generalizer imple-mented as a search proxy running on the client machine itself. The proxy maintains both the complete user profile , in a hierarchy of nodes with semantics, and the user-specified (customized) privacy requirements represented as a set of forbidden nodes .

The search process is described as following. (1) When a user issues a query q i on the client, the proxy generalizes the user pro-file according to both the user-specified privacy requirements and the query content . The former is used for user-level customization while the latter is for query-level customization. The output of this run-time step is a generalized user profile G i satisfying the privacy requirements. (2) Subsequently the query and the generalized user profile are sent together to the PWS server for personalized search. (3) The search results are personalized with the profile and deliv-ered back to the query proxy. (4) Finally, the proxy either presents the raw results to the user, or reranks them with the complete user profile.

In the next section, we shall focus on the novel techniques in (1) representing user profile in a hierarchical taxonomy; (2) generaliz-ing the profile according to customized privacy requirements; (3) optimzing the generalization. In this section, we first introduce the structure of user profile in UPS. Then we describe the techniques to generalize a user profile. Finally, we present the attack model and formulate the problem of privacy protection in user profile generalization.
Consistent with many previous works in personalized Web ser-vices, each user profile in UPS adopts a hierarchical tree structure. Each node in the tree is selected from a taxonomy repository. For ease of presentation, Table 1 summarizes all the symbols used in this paper.

We start by introducing two assumptions to formalize our defini-tionofuserprofile:
A SSUMPTION 1. There exists a public accessible taxonomy repos-itory, denoted as R , which represents the entire topic domain (of human knowledge) as a huge topic hierarchy.

Specifically, given any topic t detected with human knowledge, we can find a corresponding node (also referred to as t ) within Moreover, the subtree subtr ( t, R ) is recognized as the taxonomy accompanying topic t .
 A SSUMPTION 2. Every node t within the taxonomy repository R is labeled with a coefficient called succession ratio , denoted by t.succ , which describes how likely a topic can be narrowed down from its parent node. Formally, succession ratio is defined as:
The above assumptions can be used to define a probability model which is crucial for defining the user profile. If both assumptions are satisfied, R can be mapped to a weighted partition strategy on the probability space of the topic domain. Then, every topic inside R can be considered as a random event. The root of R represents the domain itself, and its probability is obviously 100%. For any other topic t , the probability can be calculated by multiplying the succession ratio when traversing the path from the root to t .
Before giving the formal definition of user profile, we also need to introduce the notion of rooted subtree.
 T exists a node set D  X  S ( T 2 ) that, T 1 is generated by removing the subtree set { subtr ( t, T 2 ) | t  X  D } from T 2 .Weuse rsubtr ( T 2 ,D ) to denote that T 1 is a rooted subtree of moving node set D .
 Now we can formalize the user profile as follows:
D EFINITION 2. (U SER P ROFILE / H -T REE ) Given a taxonomy repository R , a user profile H is a rooted subtree generated from the taxonomy repository R . i.e., the predicate  X  H is a rooted sub-tree of R  X  X s T RUE .

As Definition 2 states, user profile H is a hierarchical represen-tation of user X  X  interests, in the form of a subset selected (no matter implicitly or explicitly) from the entire topic domain denoted by the repository R . Hence, H inherits all its node features from including the succession ratios of all nodes in H .

A diagram of a sample H -Tree is illustrated in Figure 2(a) .We can observe that the user is mainly interested in Computer Science and Music , because the major portion of this profile is made up of fragments from taxonomies of CS and Music , as shown in Fig-ure 2(b) and 2(c) respectively. Some other taxonomies also serve in comprising the profile, for example Sports and Sex . Note that the irrelevant parts of the tree are omitted.
Customized privacy requirements are given by specifying a num-ber of so-called forbidden-nodes . This process is also called forbid-ding in our work.

Given a user profile H , a user can easily specify his/her privacy preference by selecting in H a number of sensitive topics (nodes) which are not allowed to appear in the generalized profile. These forbidden-nodes indicate the customized privacy requirements of the user and will not be disclosed to the server.
 D EFINITION 3. (F ORBIDDEN -N ODE S ET / F ( U ) ) Given a user U and his/her profile H , the Forbidden-Node Set, denoted as F ( U ) , is the set of sensitive topics specified by U that satisfies the follow-ing two constraints: 1) F ( U )  X  S ( H ) . 2)  X  f 1 ,f 2  X  F ( U ) ( f 1 = f 2 ), f 1 cannot present in the subtree of f 2 , i.e., subtr ( f 2 , H ) .
 In the sample profile shown in Figure 2(a) ,user U specifies F ( U ) ={ PWS-privacy , Freestyle , Perl , Eagles , Harp , Sex }asthe forbidden-node set, which is highlighted in dark color in parently, all the subtrees rooted at any nodes in F ( U ) must also be removed in the generalized profile. For example, both Bluesharp and Chromatic under Harp should be removed. This is because an adversary can easily infer the forbidden-nodes from them given the knowledge of the public accessible taxonomy repository R .
Not e that there are some special cases for F ( U ) whichdonot require generalization. Specifically, F ( U )=  X  implies that U is willing to release the entire H , while F ( U )= { root ( that U will not disclose any personal information for PWS.
Specifying a forbidden-node set F ( U ) may not be enough for privacy protection, as rooted subtree rsubtr ( H ,F ( U )) may still lead to severe privacy disclosure. In the profile shown in 2(a) ,the user forbids Freestyle to conceal his/her swimming stroke from oth-ers. Thus, the output of a forbidding operation on the profile only releases its parent node Swim . Unfortunately, an adversary may probably infer Freestyle , because in the background knowledge of Swim (Figure 2(d) ), Freestyle is a main branch besides the other three. If all styles are equally distributed, the probability of leakage is 25%. In contrast, suppose that node Swim has numerous sib-ling nodes in the background knowledge of the Sports taxonomy, if we remove node Swim , making node Sports a leaf node of the generalized profile, the output would provide much stronger pro-tection to the user X  X  privacy. The reason is that the probability of the forbidden-node leakage (Freestyle) becomes much smaller now. The process of removing nodes whi ch cause large p robabilities of forbidden-node leakage is called pruning .

As another example, the parent node of Harp, namely Instru-ment, does not need to be pruned because the latter owns a large number of child nodes in the background knowledge. To save space, we do not plot the background knowledge of the taxonomy under node Instrument.

By performing the forbidding and pruning procedures, we can obtain a generalized user profile defined as follows.
 alized profile is a rooted subtree obtained in effect by removing a node set X ( U ) from H , i.e., rsubtr ( H ,X ( U )) ,where X ( U ) sat-isfies 1)  X  f  X  F ( U ) ,  X  x  X  X ( U ) and f  X  S ( subtr ( x, H )) , 2)  X  x 1 ,x 2  X  X ( U ) ( x 1 = x 2 ), x 1 cannot present in the subtree of x 2 , i.e., subtr ( x 2 , H ) . The nodes in X ( U ) define a guarding boundary in H . In our run-ning example, X ( U ) ={ PWS-privacy , Perl , Swim , Band , Harp , Sex }, and is visualized as the red curve in Figure 2(a) .
Based on the above discussions, we can hereby define the at-tack and formulate the optimization problem for generalizing user profile. Our work aims at providing protection against a typical kind of privacy attack, namely eavesdropping . Consider the sce-nario shown in Figure 3. To corrupt Alice X  X  privacy, the eaves-dropper Eve successfully intercepts the communication between Alice and the PWS-server through some measures, such as man-in-the-middle attack, invading the server, etc. Consequently, when-ever Alice issues a query, the entire copy of the query together with a run-time generalized profile G will be captured by Eve. Based on G , Eve will attempt to rebuild the original H and predict the Forbidden-Node Set of Alice, with the background knowledge from the public-accessible taxonomy repository R .

It is worth mentioning that, in our attack model, Eve is regarded as an adversary satisfying the following assumptions.

Knowledge-bounded The background knowledge of the adversary should be limited to the taxonomy repository R . Both the profile H and privacy are defined based on R . This makes the profile-base generalization is incapable of resisting an adversary holding knowledge beyond such scope.

Session-bounded None of previous captured information is avail-able for tracing the same victim in a long duration. In other words, the eavesdropping will be started and ended within a single query session.
 These assumptions seem strong, but are reasonable in practice. This is due to the fact that majority of privacy attacks on the Web are undertaken by some automatic programs for sending targeted (spam) advertisements to a large amount of PWS-users. These programs rarely act as a real person that collects prolific informa-tion of a specific victim for a long time as the latter is much more costly. Subsequently, the privacy disclosure can be simplified to the exposure of user-specified Forbidden-Nodes, and can be mea-sured by the probability the adversary touch any Forbidden-Node (  X  f  X  F ( U ) ) of an individual, provided that the generalized profile (
G ) for a given query q is intercepted. Thus, the privacy protection of personalized web search is to minimize this probability.
Meanwhile, it is always important to minimize the loss of util-ity when generalizing the user profile for privacy protection. For instance, in the running example of Fig 2(a) , if we prune node Sports instead of Swim from ( G ) , the privacy of U will be pro-tected. However, this operation should be avoided because it will introduce considerable ambiguity if U queries for something re-lated to the freestyle stroke (maybe the server will respond with items about improvised hip-hop). We now define the problem of privacy-preserving generalization in UPS as follows. Our defini-tion relies on both the metric of privacy and the one of utility, user profile H with Forbidden-Node Set F ( U ) specified, a query q , metric of privacy risk ( q, G ) , metric of utility util ( q, user specified threshold  X  , the optimal privacy-preserving general-ization is to find an optimal instance of G (denoted as G X  satisfies the following Equation 3.

Where  X  is the user-tolerant upper bound of the pr obability the attacker can touch his/her privacy. Note that metrics risk ( q, util ( q, G ) only depends on the instance of G and the issued query q as they are implemented to predict the privacy risk and personal-ization utility of G over q , without any user feedback. More details of these metrics will be presented in Section 5.2.
In this section, we present our implementation of privacy-preserving generalization in UPS. The generalization is implemented in two phases, namely the offline and online phases. We shall first describe the key techniques in each phase briefly. Second, we discuss the problem of measuring utility and privacy online based on the query and generalized profile. Third, we propose two efficient heuristic algorithms to find the near-optimal trade-off between the utility and the user privacy during generalization. Finally, some extra issues are discussed.
In our solution, we initialize the client proxy (generalizer) by generating the complete user profile and his/her privacy preference during the offline processing. In the run-time, when a query is is-sued, the proxy computes a generalized G -profile on the fly. Specif-ically, our solution consists of the following steps:
Offline-1. Hierarchical profile construction The first step of the offline processing is to build the user profile in a topic hier-archy H that reveals user interests. To construct the profile, we first transform various types of user data into a set of plain-text documents, denoted as D ( U ) . Then we initialize H as a single root node. Given a public accessible taxonomy repository R ODP, Wikipedia, etc), we detect the topic t in R for every docu-ment d  X  D ( U ) , and insert the vector of t into H . At last, the profile of U is created when all the user documents are processed. It should be noted that this step is replaceable and thus it is open to all available solutions if only the generated profile satisfy Defi-nition 2.
 Algorithm 1: InitSensitivityLabels( t )
Input : A profile node t in H 1if C ( t, H ) &gt; 0 then 2 foreach node i  X  X  ( t, H ) do 3 InitSensitivityLabels( i ); Offline-2. Integration of customized privacy requirements This step detects the risk (severity) of U  X  X  privacy corruption for the exposure of every node t in H according to F ( U ) , denoted as t.sen . First, for each node t  X  S ( H ) , we need to prepare for the initial value of its sensitivity based on the following rule. If t F ( U ) ,then t.sen is initialized to 1.0. Otherwise, t.sen is initial-ized to 0.0 as it is safe for sharing it. Second, our framework com-putes and labels the sensitivity of each node in H as Equation 4, following bottom-up fashion. This is achieved by invoking the re-cursive routine InitSensitivityLabels on root ( H ) ,asshownin Algorithm 1.
When the above two steps are finished, we have obtained the pro-file H with privacy preference F ( U ) ready, which sets up the basis for the Online processing. When a query q is issued, the online customized generalization consists of the following two steps:
Online-1. Query-topic mapping. Typically, a user profile H contains a variety of topics. To process query-level customized generalization on it, we need to extract the topic domain detected from the query, and use it as an early-pruning strategy to reduce the search space in the user profile. Following this idea, we em-ploy a mapping strategy denoted by M R ( . ) , which depends on to assign the query q to the related topic domain M R ( q ) ,where each topic can be found as a node in R . Moreover, each topic t  X  X  R ( q ) is associated with a weight t.gain , quantifying the relevancy between q and t . Since the number of related topics of different queries might be different. For fairness these weights are normalized as t.gain = Pr ( t | q ) . Thus, the greater t.gain is, the higher possibility q covers the topic t . As the mapping strategy used here is also replaceable, different solutions can be adopted. The details of query-topic mapping will be presented in Section 5.4 .

Online-2. Cost-based generalization. On the topic domain bounded by q and H in the previous step, the Online Generalizer solves Problem 1 in a cost-based manner. The details of this tech-nique will also be given in Section 5.3.
Before looking into the generalization algorithm, we need to handle a major challenge for designing the algorithm. That is how to construct an appropriate metrics, namely util ( q, G ) and risk ( q, to predict the personalization utility and the privacy risk on issuing a query q with a selected instance of G effectively. We will analyze these two metrics separately at first.

The purpose of the utility metric is to predict the potential gain (in revealing the user X  X  information goal) of the query q on a gen-eralized profile G . Different from the similar problem proposed by Krause. et. al [ 9], we do not have appropriate probabilistic tools to model the user X  X  target intention. However, we can transform this utility prediction into the estimation of the discriminating power of a given query q (together with G ) under the assumption below.
A SSUMPTION 3. When a PWS-strategy is given ,the search qual-ity is only determined by the discriminating power of the query on the exposed profile.

Intuitively, the fewer topics a query covers, the more specific these topics are, and the more discriminating power the query has. Based on this observation, we formalize the utility of a given q and G with the tool Mutual Information , i.e., I G ( Q ; T ) in Equa-tion 5, which estimates how much knowing q reduces the uncer-tainty about the topic domain bounded by G .

Moreover, as our solution focuses on query-level customized generalization, the query set Q contains only one query q and thus Pr ( q )=1 . As a result, Equation 5 can be simplified as
Notice that in Equation 6, S q ( G ) is a special topic set constructed as follows:  X  t  X  X  R ( q ) , (i) if t  X  S ( G ) , t is added to S erwise (ii) walk through the path to t to reach the farthest topic t within G , and then add t to S q ( G ) . In this case, we need to add the quota of t.gain to t .gain . For example, assume that  X  X oot/Sports/Football X  is a related topic of the query issued by the owner of H -profile in Figure 2(a) , the corresponding topic that can be added to S q ( G ) is  X  X oot/Sport X .

The cost model of privacy is recognized by the overall identifi-cation of an individual X  X  sensitive topics. For simplicity, we treat the confidentiality of F ( U ) as a rigid target. That is, the exposure of any single Forbidden-Node x  X  F ( U ) is recognized as a pri-vacy corruption. Then, the overall cost model over G is modeled as the maximum of the probability to reveal any Forbidden-Node with any topic t in G exposed. Using sensitivity labeled on each topic in G during Offline-2 , we can define the privacy cost with the metric risk ( q, G ) in Equation 7. We have described how we can quantify the utility util ( q, for any given instance of generalized profile G , and its associated privacy cost risk ( q, G ) . Using these two predictive metrics, the framework UPS can embody the generalization self-adaptively. In-tuitively, our goal is to find a G , that maximizes util ( q, keeping risk ( q, G ) under the threshold  X  , as defined in Problem 1.
Basically, finding the optimal generalization needs to evaluate metrics util and risk on all possible candidates. Due to the huge space of possible topics determined by the repository R ,themain challenge here is to reduce the computational hardness. Suppose is a tree having depth n and an average fanout of m . As all rooted subtrees of H are candidates of G , the number of candidates grows exponentially with m  X  n , as shown in Equation 8.

O ( m, n )= a 0
It can be proven that finding the optimal generalization candi-date is NP-hard, as it is a reduction from another NP-hard problem proposed in [ 9]. In [9 ], Krause et al. model the user profile as a fi-nite set of personal attributes V = { v 1 ,v 2 , ...v n } utility (of predicting the user X  X  target) and the risk (of identifying the user) over a given set A  X  V . They then propose the problem of selecting an optimal subset A  X  of attributes to reveal (known as Attribute Selection ), which maximally an aggregative metrics con-sidering both the utility and the risk as much as possible. Krause et al. have proven that this is an NP-hard search problem, which can be reduced to our problem.

T HEOREM 1. Finding Optimal  X  -Risk Profile Generalization (Prob-lem 1) is NP-hard.

P ROOF . The reduction is quite straightforward: Given an in-stance of set V in Attribute Selection , we construct a correspond-ing instance of the hierarchal profile H in Profile Generalization by simply considering all the attributes of V as leaves and assign-ing root ( H ) as the parent to them. In this depth-2 hierarchy, for each leaf v , its v.gain and v.sen are specified with definitions in Attribute Selection . Obviously, the output instance of G Generalization is also the solution to Attribute Selection since G is another depth-2 hierarchy having set A  X  as leaves.
 Algorithm 2: GreedyUtility( H , M R ( q ) ,  X  ) Input :profile H ; topic domain M R ( q ) ; threshold  X 
Output : generalized profile G X  satisfying  X  -privacy 1 Calculate clarity ( q ) with Equation 9; 2if clarity ( q ) &lt; X  then 3 Let u max  X  X  X  X  ; 4 Let G X   X   X  ; 5 Let G X  rsubtr ( H ,F ( U )) ; 6 Generate S q ( G ) by imposing M R ( q ) on G ; 7 Rebuild G from S q ( G ) ; 8 while G = root ( R ) do 9if risk ( q, G ) &lt; X  and util ( q, G ) &gt;u max then 10 G X   X  G ; 11 u max  X  util ( q, G ) ; 12 Let x  X  argmax t  X  S q ( G ) util ( q,rsubtr ( G , { t 13 G X  rsubtr ( G , { x } ) ; 14 Update S q ( G ) by replacing x with paren ( x, G ) ; 15 paren ( x, G ) .gain  X  paren ( x, G ) .gain + x.gain ; 16 Normalize t.gain among  X  t  X  S q ( G ) ; 17 return G X  ; 18 return root ( R ) as G X  ;
In view of the computational hardness in finding the optimal gen-eralization, we propose a greedy search algorithm, called GreedyU-tility, to approximate this optimization. Algorithm 2 presents the pseudo-code of the algorithm.
 The loop on line 8-16 is the core component of the algorithm. The basic idea is as follows: The algorithm iteratively chooses a topic t from the candidate set S q ( G ) when a further generalization, namely removing t from G , can obtain the highest gain in the per-sonalization utility performance util ( q, G ) . The iteration does not terminate until G is generalized to the single root. The intermediate instance of G with maximum util and risk under the threshold  X  is recorded as G X  . Obviously, the search space of this algorithm is bounded to the number of nodes in the profile, i.e. O ( m profile with average depth n and fanout m .

The time complexity of Algorithm 2 looks poor as it needs to greedily traverse through the generalization space to find the global optimal G . Fortunately, this cost is significantly offset by the benefit of employing query-topic mapping. when we employ the related topic domain M R ( q ) (output of Query-topic mapping) to rebuild G , the search space of the algorithm is reduced to S q ( G G X  is initialized with the topics in S q ( G ) , a considerable number of irrelevant branches (topics) at upper levels of H are pruned. Thus the initial number of leaves in G becomes very limited. As a result, the necessary number of iterations is reduced significantly after this process.

An additional optimization implemented in Algorithm 2 is the clarity prediction , namely to make online decision of whether to personalize a query. This is based on an observation that a great number of queries are distinct enough to describe the users X  infor-mation needs without ambiguity. For such queries, personalization may be unnecessary, and profile-based personalization will con-tribute little or even harm the quality of service, as demonstrated in [ 5]. We use the proposed predicti ve query utility to detect such queries. Formally, each query is given a clarity measure over the taxonomy repository. In the algorithm, if clarity ( q ) &gt; X  ,where  X  is a system parameter, we skip the generalization and do not send the personalized profile to the server.

Algorithm 2, GreedyUtility, is the direct approximate solution to the Problem 1, which is based on the observation that users usually prefer to constrain the risk of privacy disclosure during the general-ization and try to maximize the query utility. Moreover, GreedyU-tility can resist advers aries without the Session-bounded constant, if the user does not increase the threshold  X  .

We also propose another algorithm, GreedyPerformance, which simply tries to find G X  that maximizes an aggregative metric perf combined with util and risk (instead of only util in GreedyU-tility). GreedyPerformance is only used in experiment to analyze the trade-off between the utility and the privacy. The aggregative metric perf will be presented in Section 6.2;
As mentioned in Section 5.1, step Offline-1 and Online-1 can both be replaced by alternative methods. However, the effective-ness of the UPS framework depends largely on the taxonomy repos-itory being used. We recommend to employ hands-on and well-defined concept categorizations, such as ODP [ 4, 19], Wikipedia [ 6, 14] etc. For other methods not based on such taxonomies [ 27], we can extract the repository by mining the topic distribution among the documents stored in the server. Nevertheless, this simulation is expensive to process.

Meanwhile, our framework assumes the availability of succes-sion ratio information within the repository R , which has been supported by many solutions, e.g., the tree-profile of [ 27], where each node is associated with a set of supporting documents cover-ing its topic. The succession ratio of a topic t (i.e., t.succ ) can be instanced as the proportion of t  X  X  supporting documents in the sup-porting documents of t  X  X  parent. In fact, even if these succession ratios are absent in the repository, we can easily simulate them. One straightforward method is to use the topological structure of the taxonomy itself. For instance, Swim only has four children in the taxonomy of Swim, as shown in Figure 2(d) , and hence Freestyle.succ can be simply calculated as 1 / 4=0 . 25 .Sim-ilarly, succession ratios of Breaststroke , Backstroke and Butterfly can be computed.

As for the mapping strategy used in Online-1, different solutions can be adopted. While some strategies [ 15, 13] assigns weights to all candidates in M R ( q ) , others do not. In the latter situation, we can simply share weights equally among the related topics, i.e., for |M R ( q ) | = n ,  X  t  X  X  R ( q ) , t.gain = Pr ( t | addition, to solutions without the ready-to-use repository, the alter-native of mappings is to use the term-distance between the query and topic label. Nevertheless, such mapping strategy is unstable because it suffers from the common limitation of term-based pro-file, such as irrelevant words , polysomy ,and synonymy etc.
In this section, we present the experimental results of UPS. We conduct three experiments on UPS. In the first experiment, we study the detailed results of the metrics in each iteration of the proposed algorithms. Second, we look at the effectiveness of the proposed Query-Topic Mapping techniques. In the third experiment, we study the effectiveness of Clarity Prediction technique and the search quality of UPS. The UPS framework is implemented on a PC with a Pentium Dual-Core 2.50GHz CPU and 2GB main memory, running Mi-crosoft Windows XP. All the algorithms are implemented in Java.
To evaluate the UPS framework, we use the AOL search query data as our dataset, which is the most recent published data we could find. The AOL search query data contains over 20 million queries and 30 million c licks of 650k users over 3 months (March 1, 2006 to May 31, 2006). The data format of each record is as follows: { AnonID, Query, QueryT ime [ , ItemRank, ClickURL ] }
Where the first 3 fields stand for the user of the identifier AnonID issued the Query at the timestamp QueryTime , and the last 2 op-tional fields appear when the user further click the URL ClickURL at the position ItemRank in the rank list returned by AOL over Query . We extract query logs of 50 distinct users (with #clicks 2000) to build 50 user profiles. For each user, the user pro-file is built with the documents dumped from all ClickURL in his/her log. In the original AOL-data, the ClickURL field is trun-cated to the domain name (e.g. www.une.edu/mwwc/ becomes www.une.edu) except the URL is of the protocol https .Asare-sult, we have to recover the full URLs by means of REPLAY .That is, to re-issue the Query of the AOL-records to some designate search engines and then retrieve the URLs match ClickURL .We processed REPLAY respectively on top-100 results returned by Ya-hoo and ODP, and 40% (#clicks 800) and 25% (#clicks 500) full URLs is recovered. 80% of recovered documents are used to build profile and the rest are used as the test set.

We utilize the ODP Web Directory as the public accessible tax-onomy repository R . Note that the replaceable components of the framework (i.e. Offline-1 and Online-1) are both implemented us-ing it. To build the complete user profile, each clicked document recovered from REPLAY , is mapped to a deep ODP category 3 the method proposed by [ 25] and all these generated category vec-tors are combined together to form the profile H . Additionally, the number of pages mapped to a specific ODP category can indicate the user X  X  personal preference on it. Finally, for each profile, we randomly choose no more than 5 topics (with depth &gt;= 3) as the Forbidden-Nodes.

During the Online process, we issue every query form the test
To focus on the pure English categories, we filtered all categories under " Top/World "and" Top/Adult/World ". set to a local search engine on the dump of ODP hierarchy dexed with Luecene. The query-topic mapping of Online-1 is im-plemented by analyzing the statistics of ODP categories labeled on the returned documents, and the relevance weight of each topic, i.e., t.gain , is represented with the category frequency. After that the optimal generalized profile G X  is used to re-rank the result list of the test query, the search quality can be evaluated on the ranked lists before and after personalization.
In this experiment, we analyze and compare the effect of the gen-eralization on queries with different clarity , and study the trade-off between utility and privacy in the GreedyPerformance algorithm. As mentioned in Section 5.3, GreedyPerformance use an aggrega-tive metric perf which is defined with both util and risk as fol-lows: where  X  can be considered a privacy-to-utility conversion factor. We can find different solutions of G * by varying  X  . In effect, asmall  X  leads to solutions with higher utility and higher cost, whereas a large values of  X  incurs solutions with lower utility and lower privacy cost. The algorithm GreedyPerformance is only used to validate the feasibility to trade-off between the two contradicting metrics of utility and privacy. This idea is borrowed from [ 9].
All queries are clustered by their clarity values into three groups using the 1-dimensional k-means algorithm. These three groups are called Distinct Queries , Medium Queries ,and Ambiguous Queries , in descending order of their respective clarity values. These groups can be specified according to the following empirical rules obtained by splittin g the boundaries between tw o neighboring clusters. As the results of queries within the same group display similar trends in this experiment, we only plot the results of three repre-sentative queries ( X  X ikipedia X  for Distinct Queries,  X  X reestyle X  for Medium Queries, and  X  X rogram X  for Ambiguous Queries) in Fig-ure 4. To facilitate the comparison, all results in this experiment are generated by using a particular user profile, that is, the entire repository R . We simply use the related topics set M R ( q ) to be F ( U ) .

As Figure 4(a) shows, the utilities of all three samples show a diminishing-returns property during generalization, especially the ambiguous one (i.e.  X  X rogram X ). This indicates that the higher-level topics in the profile are more effective in improving the search quality during the personalization, while the lower-level ones are less. This property has also been reported in [ 9, 27].

In addition, we observe the following results in Figure 4(a) .First the distinct queries (i.e.  X  X ikipedia X ) require much fewer iterations than the ambiguous ones. Second, the diminishing-returns proper-ties of the utility of ambiguous queries are m ore apparent, while the utility of distinct queries increases almost linearly in most iter-ations. The reason of such results is that the distinct queries have fewer related topics (usually less than 3 in our experiment) than the ambiguous ones, and these topics are more specific. This signifi-cantly contributes to their clarity .

In Figure 4(b) , the risk first declines rapidly, but the decrease slows down as more specific profile information becomes hidden. Figure 4(c) illustrates the trade-off pattern between the utility and
URL: http://rdf.dmoz.org/rdf/content.rdf.u8.gz (c) Trade-off with different  X 
Figure 4: Results of queries during each iteration in GreedP risk measures. For all the queries, we observe an interesting find-ing that there is an apparent  X  X nee X  on their trade-off curve when  X   X  clarity ( q ) . Before this tuning point, small concessions on pri-vacy can bring great promotion on utility; while after that, any tiny increase of utility will lead to enormous increase in risk. Hence, at this knee, we can achieve near-maximal utility at near-minimum cost. Therefore, we plot perf with  X  = clarity ( q ) in Figure 4(d) for the three samples. The generalization finds out the near-optimal trade-off for all of them within limited iterations. The maximum value of overall performance of distinct queries is larger than the other groups since the y have better utility.
We first verify the effectiveness of query-topic mapping in our generalization algorithms. To facilitate the comparison, we sim-ulate two typical user profiles: one named diverse is obtained by merging the subtrees of the related topics of the three queries groups (namely Distinct, Medium, and Ambiguous); the other named sim-ilar is obtained by randomly choosing three specific topics (low-level nodes) that are close in the taxonomy.
 Table 2: Average Number of Iterations in Generalization
Profile Query Group GreedyP/U Opti-QTM Optimal diverse similar
Table 2 lists the average number of iterations of different algo-rithms, where Opti-QTM and Optimal stand for the exhaustive al-gorithms (with or without Query-Topic Mapping respectively) to find the optimal generalized profile. We can find that the orig-inal search space of Optimal is so huge that it is impractical to process the exhaustion on a complete profile. The introduction of Query-Topic Mapping reduces the search space significantly in Opti-QTM . However, the efficiency of exhaustion is still intolera-ble when S q ( H ) is large. Our greedy algorithms further reduce the search space to the size of the rebuilt profile (with S q ( leaves), therefore ensuring the ef ficiency of online generalization.
In this experiment, we demonstrate the effectiveness of the pro-posed Clarity Prediction technique, and evaluate the real search quality on commercial search engines using our UPS framework. The search results is re-ranked with the generalized profile output by GreedyUtility over 50 target users. The final search quality is evaluated using the Average Precision (denoted as AP ) of the click records of the users. AP is defined as where l i is the i th relevant link identified for a query, and n is the number of relevant links.

For each test query, the framework computes the Borda fusion[ 5] of the UPRank  X  u and the original rank  X  o as the personalized rank-ing, and then evaluate the AP of the search results on both the fu-sion and the original ranking. UPRank is defined as the descending order of link l sorted by the uscore , which is the weighted sum over topics reserved in profile G X  , where the weight rel ( l, t ) is the relevance between the link and the profile topic [ 25]. The uscore is given by
Figure 5 shows the average AP of the ranks before (Original) and after (Fusion) personalization (GreedyUtility with  X  =0 . 1 and clarity prediction feature disabled) on the test queries respectively on Yahoo and ODP. From both results we can observe that improve-ments of the search quality for Medium Queries and Ambiguous Queries are much more significant than that of Distinct Queries. In particular, the personalization on Distinct Queries of Yahoo results reduces the average performance from 73.4% to 63.6%. This is be-cause some irrelevant profile topics (noises) are added. The results demonstrate that profile-based personalization is more suitable for queries with small clarity ( q ) . Therefore, clarity prediction is an effective technique to determine whether personalization can im-prove the search quality of a given query.
Figure 6 shows the results of search quality by varying the  X  threshold. It is observed that the average precision of FusionRank increases rapidly when  X  grows from 0.0 to 0.1. Then, further in-creasing  X  (in effect exposing more specific topics) will only im-prove the search quality marginally. Moreover, the AP of Fu-sionRank based on Yahoo (Figure 6(a) ) has a dramatic drops when  X &gt; =0 . 3 .
 A comparison between the personalization results of ODP and Yahoo reveals that, although the original ODP-Rank ( AP =37 . 3% ) is much worse than the original Yahoo-Rank ( AP =46 . 7% ), per-sonalization on ODP will generate better ranking than that on Ya-hoo. The reason for this maybe is that the document-distribution of ODP over all the available topics is expectedly more consistent with its own taxonomy repository, which has been used to imple-ment our framework.
This paper presented a client-side privacy protection framework called UPS for personalized Web search. UPS could potentially be adopted by any PWS which captures user profiles in a hier-archical taxonomy. The framework allowed users to specify cus-tomized privacy requirements via the hierarchical profiles. In ad-dition, UPS also performed online generalization on user profiles to preserve the personal privacy without compromising the search quality. We proposed two greedy algorithms, namely GreedyU-tility and GreedyPerformance, for the online generalization. The GreedyUtility algorithm could maximize the query utility while maintaining the disclosure probability below a user-specified thresh-old. The GreedyPerformance algorithm could be used to strike a balance between the disclosure risk and the search quality. We also designed a novel method for deciding whether to personalize a query on-the-fly. This method was distinguished from the existing approaches as it completely relied on the client-side utility estima-tion of the query. Our experimental results revealed that UPS could achieve quality search results while preserving user X  X  customized privacy requirements. The results also confirmed the effectiveness and efficiency of our solution.

For future work we will try to resist adversaries with boarder background knowledge, such as richer relationship among topics (e.g., exclusiveness, sequentiality, etc.), or capability to capture a series of k requests from the victim. We will also seek more so-phisticated method to build the user profile, and better metrics to predict the performance (especially the utility) of UPS.
This work is supported in part by the National Science Founda-tion of China (NSFC Grant No. 60803003, 60970124). [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information [2] J. S. Breese, D. Heckerman, and C. M. Kadie. Empirical [3] J. Castell X -Roca, A. Viejo, and J. Herrera-Joancomart X . [4] P.A.Chirita,W.Nejdl,R.Paiu,andC.Kohlsch X tter.Using [5] Z. Dou, R. Song, and J.-R. Wen. A large-scale evaluation and [6] E. Gabrilovich and S. Markovich. Overcoming the brittleness [7] K. Hafner. Researchers Yearn to Use AOL Logs, but They [8] K. J X rvelin and J. Kek X l X inen. Ir evaluation methods for [9] A. Krause and E. Horvitz. A utility-theoretic approach to [10] J. Pitkow, H. Sch X tze, T. Cass, R. Cooley, D. Turnbull, [11] A. Pretschner and S. Gauch. Ontology-based personalized [12] F. Qiu and J. Cho. Automatic identification of user interest [13] G. Qiu, K. Liu, J. Bu, C. Chen, and Z. Kang. Quantify query [14] K. Ramanathan, J. Giraudi, and A. Gupta. Creating [15] P. Sch X nhofen. Identifying document topics using the [16] X. Shen, B. Tan, and C. Zhai. Context-sensitive information [17] X. Shen, B. Tan, and C. Zhai. Implicit user modeling for [18] X. Shen, B. Tan, and C. Zhai. Privacy protection in [19] M. Spertta and S. Gach. Personalizing search based on user [20] K. Sugiyama, K. Hatano, and M. Yoshikawa. Adaptive web [21] B. Tan, X. Shen, and C. Zhai. Mining long-term search [22] J. Teevan, S. T. Dumais, and E. Horvitz. Personalizing search [23] J. Teevan, S. T. Dumais, and D. J. Liebling. To personalize or [24] X. Xiao and Y. Tao. Personalized privacy preservation. In [25] D. Xing, G.-R. Xue, Q. Yang, and Y. Yu. Deep classifier: [26] Y. Xu, K. Wang, G. Yang, and A. W.-C. Fu. Online [27] Y. Xu, K. Wang, B. Zhang, and Z. Chen. Privacy-enhancing [28] Y. Zhu, L. Xiong, and C. Verdery. Anonymizing user profile
