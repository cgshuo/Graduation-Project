 The heat kernel is a type of graph diffusion that, like the much-used personalized PageRank diffusion, is useful in iden-tifying a community nearby a starting seed node. We present the first deterministic, local algorithm to compute this diffu-sion and use that algorithm to study the communities that it produces. Our algorithm is formally a relaxation method for solving a linear system to estimate the matrix exponential in a degree-weighted norm. We prove that this algorithm stays localized in a large graph and has a worst-case constant runtime that depends only on the parameters of the diffusion, not the size of the graph. On large graphs, our experiments indicate that the communities produced by this method have better conductance than those produced by PageRank, al-though they take slightly longer to compute. On a real-world community identification task, the heat kernel communities perform better than those from the PageRank diffusion. G.2.2 [ Discrete mathematics ]: Graph theory X  Graph al-gorithms ; I.5.3 [ Pattern recognition ]: Clustering X  Algo-rithms Algorithms,Theory heat kernel; local clustering
The community detection problem is to identify a set of nodes in a graph that are internally cohesive but also separated from the remainder of the network. One popular way to capture this idea is through the conductance measure of a set. We treat this idea formally in the next section, but informally, the conductance of a set is a ratio of the number of edges leaving the set to the number of edges touched by the set of vertices. If this value is small, then it indicates a set with many internal edges and few edges leaving.
In many surveys and empirical studies [47, 33, 17], the conductance measure surfaces as one of the most reliable measures of a community. Although this measure has been critized for producing cavemen -type communities [24], empiri-cal properties of real-world communities correlate highly with sets produced by algorithms that optimize conductance [1]. Furthermore, state-of-the-art methods for identifying overlap-ping sets of communities use conductance to find real-world communities better than any alternative [52].

Virtually all of the rigorous algorithms that identify sets of small conductance are based on min-cuts [6, 46], eigenvector computations [21, 4], or local graph diffusions [5, 14]. (One notable exception is the graclus method [17] that uses a relationship between kernel k -means and a variant of conduc-tance.) In this paper, we study a new algorithm that uses a heat kernel diffusion [14] to identify small-conductance com-munities in a network. (The heat kernel is discussed formally in Section 3.) Although the properties of this diffusion had been analyzed in theory in Chung X  X  work [14], that work did not provide an efficient algorithm to compute the diffusion. Recently, Chung and Simpson stated a randomized Monte Carlo method to estimate the diffusion [16].

This paper introduces an efficient and deterministic method to estimate this diffusion. We use it to study the properties of the small conductance sets identified by the method as communities. For this use, a deterministic approach is critical as we need to differentiate between subtle properties of the diffusion. Our primary point of comparison is the well-known personalized PageRank diffusion [5], which has been used to establish new insights into the properties of communities in large scale networks [33]. Thus, we wish to understand how the communities produced by the heat kernel compare to those produced by personalized PageRank.

The basic operation of our algorithm is a coordinate re-laxation step. This has been used to efficiently compute personalized PageRank [23, 36, 5] where it is known as the  X  X ush X  operation on a graph; the term  X  X oordinate relaxation X  is the classical name for this operation, which dates back to the Gauss-Seidel method. What distinguishes our approach from prior work is the use of coordinate relaxation on an implicitly constructed linear system that will estimate the heat kernel diffusion, which is formally the exponential of the random walk transition matrix.

We began looking into this problem recently in a workshop paper [27], where we showed that this style of algorithm successfully estimates a related quantity. This paper tack-les a fundamentally new direction and, although similar in techniques, required an entirely new analysis. In particular, we are able to estimate this diffusion in constant time in a degree-weighted norm that depends only on the parameters of the diffusion, and not the size of the graph. A Python implementation of our algorithm to accomplish this task is presented in Figure 2.

In the remainder of the paper, we first review these topics formally (Sections 2 and 3); present our algorithm (Section 4) and discuss related work (Section 5). Then we show how our new approach differs from and improves upon the per-sonalized PageRank diffusion in synthetic and real-world problems.
  X  We propose the first local, deterministic method to accu-rately compute a heat kernel diffusion in a graph. The code is simple and scalable to any graph where out-link access is inexpensive .  X  Our method is always localized, even in massive graphs, because it has a provably constant runtime in a degree-weighted norm.  X  We compare the new heat kernel diffusion method to the venerable PageRank diffusion in synthetic, small, and large networks (up to 2 billion edges) to demonstrate the dif-ferences. On large networks such as Twitter, our method tends to produce smaller and tighter communities. It is also more accurate at detecting ground-truth communities. We make our experimental codes available in the spirit of reproducible research:
We begin by fixing our notation. Let G = ( V,E ) be a simple, undirected graph with n = | V | vertices. Fix an or-dering of the vertices from 1 to n such that we can refer to a vertex by it X  X  numeric ID. For a vertex v  X  V , we denote by d v the degree of node v . Let A be the associated adjacency matrix. Because G is undirected, A is symmetric. Further-more, let D be the diagonal matrix of degrees ( D ii = d i and P = ( D  X  1 A ) T = AD  X  1 be the random walk transition matrix. Finally, we denote by e i the vector (of an appropri-ate length) of zeros having a single 1 in entry i , and by e the vector of all 1s.

Conductance. Given a subset S  X  V , we denote by vol ( S ) the sum of the degrees of all vertices in S , and by  X  ( S ), the boundary of S , the number of edges with one endpoint inside of S and one endpoint outside of S . With this notation, the conductance of a set S is given by Conceptually,  X  ( S ) is the probability that a random walk of length one will land outside of S , given that we start from a node chosen uniformly at random inside S .

Matrix exponential. The heat kernel of a graph involves the matrix exponential, and we wish to briefly mention some facts about this operation. See Higham [22] for a more in-depth treatment. Consider a general matrix G . The exponential function of a matrix is not just the exponential applied element-wise, but rather is given by substituting the matrix into the Taylor series expansion of the exponential function: That said, the exponential of a diagonal matrix is the ex-ponential function applied to the diagonal entries. This phenomenon occurs because powers of a diagonal matrix are simply the diagonal elements raised to the given power. For any matrix G , if G T G = GG T (which is a generalization of the notion of a symmetric matrix, called a normal matrix), then G has an eigenvalue decomposition G = X X X  X  1 and there is a simple, albeit inefficient, means of computing the exponential: exp { G } = X exp {  X  } X  X  1 . Computing the matrix exponential, or even the product of the exponential with a vector, exp { G } z , is still an active area of research [2].
A graph diffusion is any sum of the following form: where P k  X  k = 1 and s is a stochastic vector (that is, it is non-negative and sums to one.) Intuitively, a diffusion captures how a quantity of s i material on node i flows through the graph. The terms  X  k provide a decaying weight that ensures that the diffusion eventually dissipates. In the context of this paper, we are interested in the diffusions of single nodes or neighborhood sets of a single vertex; and so in these cases s = e i or s = P i  X  S e i / | S | for a small set S . We call the origins of the diffusion the seeds .

Given an estimate of a diffusion f from a seed, we can produce a small conductance community from it using a sweep procedure. This involves computing D  X  1 f , sorting the nodes in descending order by their magnitude in this vector, and computing the conductance of each prefix of the sorted list. Due to the properties of conductance, there is an efficient means of computing all of these conductances. We then return the set of smallest conductance as the community around the seeds.

The personalized PageRank diffusion. One of the most well-known instances of this framework is the personal-ized PageRank diffusion. Fix  X   X  (0 , 1). Then p is defined: The properties of this diffusion have been studied exten-sively. In particular, Andersen et al. [5] establish a local Cheeger inequality using a particular algorithm called  X  X ush X  that locally distributes mass. The local Cheeger inequal-ity informally states that, if the seed is nearby a set with small conductance, then the result of the sweep procedure is a set with a related conductance. Moreover, they show that their  X  X ush X  algorithm estimates f with an error  X  in a degree-weighted norm by looking at 1 (1  X   X  )  X  edges.
The heat kernel diffusion. Another instance of the same framework is the heat kernel diffusion [14, 15]. It simply replaces the weights  X  k with t k /k !: While it was known that estimating h gave rise to a similar type of local Cheeger inequality [15]; until Chung and Simp-son X  X  Monte Carlo approach [16], no methods were known to estimate this quantity efficiently. Our new algorithm is a deterministic approach that is more suitable for comparing the properties of the diffusions. It terminates after exploring  X  edges (Theorem 1), where N is a parameter that grows slowly with  X  .

Heat kernels compared to PageRank. These different sets of coefficients simply assign different levels of importance to walks of varying lengths: the heat kernel coefficients cay much more quickly than  X  k , and so the heat kernel more heavily weights shorter walks in the overall sum (depicted in Figure 1). This property, in turn, will have important consequences when we study these methods in large graphs in Section 6. Figure 1: Each curve represents the coefficients of ( AD  X  1 ) k in a sum of walks. The dotted blue lines give  X  , and the red give t k /k ! , for the indicated values of  X  and t .
The overall idea of the local clustering algorithm is to approximate a heat kernel vector of the form so that we can perform a sweep over h . Here we describe a coordinate-relaxation method, which we call hk-relax , for approximating h . This algorithm is rooted in our recent work on computing an accurate column of exp { P } [27]; but is heavily tuned to the objective below. Thus, while the overall strategy is classical  X  just as the PageRank push method is a classic relaxation method  X  the simplifications and efficient implementation are entirely novel. In particular, the new objective in this paper enables us to get a constant runtime bound independent of any property of the graph, which differs markedly from both of our previous methods [27, 26].

Our objective. Recall that the final step of finding a small conductance community involves dividing by the degree of each node. Thus, our goal is to compute x  X  h satisfying the degree weighted bound: By using standard properties of the matrix exponential, we can factor exp { X  t ( I  X  P ) } = e  X  t exp { t P ) } and scale by e so that the above problem is equivalent to computing y characterization is that y must satisfy: for all i . A similar weighted objective was used in the push algorithm for PageRank [5].

Outline of algorithm. To accomplish this, we first ap-proximate exp { t P } with its degree N Taylor polynomial, T
N ( t P ), and then we compute T N ( t P ) s . But we use a large, implicitly formed linear system to avoid explicitly evaluating the Taylor polynomial. Once we have the linear system, we state a relaxation method in the spirit of Gauss-Seidel and the PageRank push algorithm in order to compute an accurate approximation of h .
Determining the exponential of a matrix is a sensitive computation with a rich history [39, 40]. For a general matrix G , an approximation via the Taylor polynomial, can be inaccurate when k G k is large and G has mixed signs, as large powers G k can contain large, oppositely signed num-bers that cancel properly only in exact arithmetic. However, we intend to compute exp { t P } s , where P , t , and s are nonnegative, so the calculation does not rely on any deli-cate cancellations. Furthermore, our approximation need not be highly precise. We therefore use the polynomial For details on choosing N , see Section 4.5. For now, assume that we have chosen N such that This way, if we compute y  X  T N ( t P ) s satisfying then by the triangle inequality we will have our objective.
Using a degree N Taylor polynomial, hk-relax ultimately approximates h by approximating each term in the sum of the polynomial times the vector s : The total error of our computed solution is then a weighted sum of the errors at each individual term, t k k ! P k s . We show in Lemma 1 that these weights are given by the polynomials  X  ( t ), which we define now. For a fixed degree N Taylor polynomial of the exponential, T N = P N k =0 t k k ! , we define These polynomials  X  k ( t ) are closely related to the  X  functions central to exponential integrators in ODEs [37]. Note that  X 
To guarantee the total error satisfies the criterion (3) then, it is enough to show that the error at each Taylor term satisfies an  X  -norm inequality analogous to (3) . This is discussed in more detail in Section A.
To define the basic step of the hk-relax algorithm and to show how the  X  k influence the total error, we rearrange the Taylor polynomial computation into a linear system. Denote by v k the k th term of the vector sum T N ( t P ) s : implies that the terms v k exactly satisfy the linear system Let v = [ v 0 ; v 1 ;  X  X  X  ; v N ]. An approximate solution  X  v to (9) would have block components  X  v k such that P N k =0  X  v T
N ( t P ) s , our desired approximation to e t h . In practice, we update only a single length n solution vector, adding all updates to that vector, instead of maintaining the N + 1 different block vectors  X  v k as part of  X  v ; furthermore, the block matrix and right-hand side are never formed explicitly.
With this block system in place, we can describe the algo-rithm X  X  steps.
Given a random walk transition matrix P , scalar t &gt; 0, and seed vector s as inputs, we solve the linear system from (9) as follows. Denote the initial solution vector by y and the initial nN  X  1 residual by r (0) = e 1  X  s . Denote by r ( i,j ) the entry of r corresponding to node i in residual block j . The idea is to iteratively remove all entries from r that satisfy To organize this process, we begin by placing the nonzero entries of r (0) in a queue, Q ( r ), and place updated entries of r into Q ( r ) only if they satisfy (10).

Then hk-relax proceeds as follows. 1. At each step, pop the top entry of Q ( r ), call it r ( i,j ), 2. Add r ( i,j ) to y i . 3. Add r ( i,j ) t j +1 Pe j to residual block r j +1 . 4. For each entry of r j +1 that was updated, add that entry
Once all entries of r that satisfy (10) have been removed, the resulting solution vector y will satisfy (3) , which we prove in Section A, along with a bound on the work required to achieve this. We present working code for our method in Figure 2 that shows how to optimize this computation using sparse data structures. These make it highly efficient in practice. # G is graph as dictionary -of-sets , # seed is an array of seeds , # t, eps , N, psis are precomputed x = {} # Store x, r as dictionaries r = {} # initialize residual Q = collections.deque() # initialize queue for s in seed: r[(s,0)] = 1./len(seed)
Q.append ((s,0)) while len(Q) &gt; 0: (v,j) = Q.popleft () # v has r[(v,j)] ... rvj = r[(v,j)] # perform the hk-relax step if v not in x: x[v] = 0. x[v] += rvj r[(v,j)] = 0. for u in G[v]: # for neighbors of v Figure 2: Pseudo-code for our algorithm as work-ing python code. The graph is stored as a dic-tionary of sets so that the G[v] statement re-turns the set of neighbors associated with vertex v . The solution is the vector x indexed by ver-tices and the residual vector is indexed by tuples ( v,j ) that are pairs of vertices and steps j . A fully working demo may be downloaded from github
The last detail of the algorithm we need to discuss is how to pick N . In (4) we want to guarantee the accuracy of D  X  1 exp { t P } s  X  D  X  1 T N ( t P ) s . By using D  X  1 exp t P T D  X  1 and D  X  1 T N ( t P ) = T N ( t P T ) D  X  1 a new upperbound on k D  X  1 exp { t P } s  X  D  X  1 T N ( t P ) s k noting Since s is stochastic, we have k D  X  1 s k  X   X  k D  X  1 From [34] we know that the norm k exp t P T  X  T N ( t P T is bounded by So to guarantee (4) , it is enough to choose N that implies ciently simply by iteratively computing terms of the Taylor polynomial for e t until the error is less than the desired error for hk-relax . In practice, this required a choice of N no greater than 2 t log ( 1  X  ), which we think can be made rigorous.
The proof proceeds as follows. First, we relate the error vector of the Taylor approximation E 1 = T N ( t P ) s  X  x , to the error vector from solving the linear system described in Sec-tion 4.4, E 2 = T N ( t P ) s  X  y . Second, we express the error vec-tor E 2 in terms of the residual blocks of the linear system (9) ; this will involve writing E 2 as a sum of residual blocks r weights  X  k ( t P ). Third, we use the previous results to upper-bound k D  X  1 T N ( t P ) s  X  D  X  1 x k  X  with P N k =0  X  and use this to show that k D  X  1 T N ( t P ) s  X  D  X  1 x k is guaranteed by the stopping criterion of hk-relax , (3) . Finally, we prove that performing steps of hk-relax until the stopping criterion is attained requires work bounded by
We wish to highlight a few ideas that have recently emerged in the literature to clarify how our method differs. We discuss these in terms of community detection, the matrix exponential, fast diffusion methods, and relaxation methods.
Community detection and conductance. Conduc-tance often appears in community detection and is known to be one of the most important measures of a community [47]. The personalized PageRank method is one of the most scal-able methods to find sets of small conductance, although recent work opened up a new possibility with localized max-flow algorithms [46]. For the PageRank algorithm we use as a point of comparison, Zhu et al. [54] recently provided an improved bound on the performance of this algorithm when finding sets with high internal conductance. The internal conductance of a set is the minimum conductance of the subgraph induced by the set and we would expect that real-world communities have large internal conductance. Due to the similarity between our algorithm and the personalized PageRank diffusion, we believe that a similar result likely holds here as well.

The matrix exponential in network analysis. Re-cently, the matrix exponential has frequently appeared as a tool in the network analysis literature. It has been used to estimate node centrality [18, 20, 19], for link-prediction [29], in graph kernels [28], and  X  as already mentioned  X  clustering and community detection [14]. Many of these studies involve fast ways to approximate the entire matrix exponential, in-stead of a single column as we study here. For instance, Sui et al. [49] describe a low-parameter decomposition of a network that is useful both for estimating Katz scores [25] and the matrix exponential. Orecchia and Mahoney [44] show that the heat kernel diffusion implicitly approximates a diffusion operator using a particular type of generalized entropy, which provides a principled rationale for its use.
Fast methods for diffusions. Perhaps the most related work is a recent Monte Carlo algorithm by Chung and Simp-son [16] to estimate the heat kernel diffusion via a random walk sampling scheme. This approach involves directly simu-lating a random walk with transition probabilities that mirror the Taylor series expansion of the exponential. In comparison, our approach is entirely deterministic and thus more useful to compare between diffusions, as it eliminates the algorithmic variance endemic to Monte Carlo simulations. A similar idea is used by Borgs et al. [13] to achieve a randomized sublinear time algorithm to estimate the largest PageRank entries, and in fact, Monto Carlo methods frequently feature in PageRank computations due to the relationship between the diffusion and a random walk [7, 13, 8, 9]. Most other deterministic approaches for the matrix exponential involve at least one matrix-vector product [45, 2].

Relaxation methods. The algorithm we use is a coordi-nate relaxation method similar to Gauss-Seidel and Gauss-Southwell. If we applied it to a symmetric positive definite matrix, then it would be a coordinate descent method [35]. It has been proposed for PageRank in a few difference cases [23, 36, 5]. The same type of relaxation method has also been used to estimate the Katz diffusion [12]. We recently used it to estimate a column of the matrix exponential exp { P } e in a strict, 1-norm error and were able to prove a sublin-ear convergence bound by assuming a very slowly growing maximum degree [27] or a power-law degree distribution [26]. This paper, in comparision, treats the scaled exponential exp { X  t ( I  X  P ) } e i in a degree-weighted norm; it also shows a constant runtime independent of any network property.
Here we compare hk-relax with a PageRank-based local clustering algorithm, pprpush [5]. Both algorithms accept as inputs a symmetric graph A and seed set s . The parameters required are t and  X  , for hk-relax , and  X  and  X  for pprpush. Both algorithms compute their respective diffusion ranks starting from the seed set, then perform a sweep-cut on the resulting ranks. The difference in the algorithms lies solely in the diffusion used and the particular parameters. We conducted the timing experiments on a Dual CPU system with the Intel Xeon E5-2670 processor (2.6 GHz, 8 cores) with 16 cores total and 256 GB of RAM. None of the experi-ments needed anywhere near all the memory, nor any of the parallelism. Our implementation uses Matlab X  X  sparse matrix data structure through a C++ mex interface. It uses C++ unordered maps to store sparse vectors and is equivalent to the code in Figure 2.
In this section, we study the behavior of the PageRank and heat kernel diffusions on the symbolic image graph of a chaotic function f [48]. The graphs that result are loosely reminiscent of a social network because they have pockets of structure, like communities, and also chaotic behaviour that results in a small-world like property.

The symbolic image of a function f is a graph where each node represents a region of space, and edges represent the action of the function in that region of space. We consider two-dimensional functions f ( x,y ) in the unit square [0 , 1] so that we can associate each node with a pixel of an image and illustrate the vectors as images. In Figure 3 (left), we illustrate how the graph construction works. In the remaining examples, we let f be a map that results from a chaotic, nonlinear dynamical system [48] (we use the T10 construction with k = 0 . 22 , X  = 0 . 99 and sample 1000 points from each region of space, then symmetrize the result and discard weights). We discretize space in a 512  X  512 grid, which results in a 262 , 144 node graph with 2 M edges. In Figure 3 (right), we also show the PageRank vector with uniform teleportation as an image to  X  X llustrate the structure X  in the function f .

Next, in Figure 4, we compare the vectors and sets iden-tified by both diffusions starting from a single seed node. We chose the parameters  X  = 0 . 85 and t = 3 so the two methods perform the same amount of work. These results are what would be expected from Figure 1, and what many of the remaining experiments show: PageRank diffuses to a larger region of the graph whereas the heat-kernel remains more focused in a sub-region. PageRank, then, finds a large community with about 5,000 nodes whereas the heat ker-nel finds a small community with around 452 nodes with slightly worse conductance. This experiment suggests that, if these results also hold in real-world networks, then because real-world communities are often small [33], the heat ker-nel diffusion should produce more accurate communities in real-world networks. Figure 3: (Left) An illustration of the symbolic im-age of a function f as a graph. Each large, blue node represents a region of space. Thick, blue edges represent how the thin, red values of the function be-have in that region. (Right) The global PageRank vector is then an image that illustrates features of the chaotic map f and shows that it has pockets of structure.
We next compare the runtime and conductance of the al-gorithms on a suite of social networks. For pprpush, we fix  X  = 0 . 99, then compute PageRank for multiple val-best conductance obtained. (This matches the way this method is commonly used in past work.) For hk-relax , we compute the heat kernel rank for four different parameter sets, and output the set of best conductance among them: ( t, X  ) = (10 , 10  X  4 ); (20 , 10  X  3 ); (40 , 5  X  10  X  3 also include in hk-relax an early termination criterion, in the case that the sum of the degrees of the nodes which are relaxed, P d i l , exceeds n 1 . 5 . However, even the smaller input graphs (on which the condition is more likely to be met because of the smaller value of n 1 . 5 ) do not appear to have reached this threshold. Furthermore, the main theorem of this paper implies that the quantity P d i l cannot exceed datasets are modified to be undirected and a single connected component. These datasets were originally presented in the following papers [3, 10, 32, 41, 42, 33, 31, 50, 30, 53, 11, 38].
To compare the runtimes of the two algorithms, we display in Figure 5 for each graph the 25% , 50% , and 75% percentiles of the runtimes from 200 trials performed. For a given graph, each trial consisted of choosing a node of that graph uniformly at random to be a seed, then calling both the PageRank and the heat kernel algorithms. On the larger datasets, which have a much broader spectrum of node degrees and therefore greater variance, we instead performed 1,000 trials. Additionally, we display the 25% , 50% , and 75% percentiles Figure 4: When we compare the heat-kernel and PageRank diffusions on the symbolic image of the Chirikov map (see Figure 3), pprgrow finds a larger set with slightly better conductance, whereas hk-grow finds a tighter set with about the same con-ductance. In real-world networks, these smaller sets are more like real-world communities. of the conductances achieved during the exact same set of trials. The trendlines of these figures omit some of the trials in order to better show the trends, but all of the median results are plotted (as open circles). The figures show that in small graphs, hk-relax is faster than pprpush, but gets larger (worse) conductance. The picture reverses for large graphs where hk-relax is slower but finds smaller (better) conductance sets.

Cluster size and conductance. We highlight the indi-vidual results of the previous experiment on the symmetrized twitter network. Here, we find that hk-relax finds sets of better conductance than pprpush at all sizes of communities in the network. See Figure 6.
We conclude with an evaluation of identifying ground-truth communities in the com-dblp , com-lj , com-amazon , com-orkut , com-youtube , and com-friendster datasets [53, 38]. In this experiment, for each dataset we first located 100 known communities in the dataset of size greater than 10. Given one such community, using every single node as an individual seed, we looked at the sets returned by hk-relax with t = 5 , X  = 10  X  4 and pprpush using the standard proce-dure. We picked the set from the seed that had the highest F 1 measure. (Recall that the F 1 measure is a harmonic mean of precision and recall.) We report the mean of the F 1 measure, conductance, and set size, where the average is taken over all 100 trials in Table 2. These results show that hk-relax produces only slightly inferior conductance
Graph | V | | E | pgp-cc 10,680 24,316 ca-AstroPh-cc 17,903 196,972 marvel-comics-cc 19,365 96,616 as-22july06 22,963 48,436 rand-ff-25000-0.4 25,000 56,071 cond-mat-2003-cc 27,519 116,181 email-Enron-cc 33,696 180,811 cond-mat-2005-fix-cc 36,458 171,735 soc-sign-epinions-cc 119,130 704,267 itdk0304-cc 190,914 607,610 dblp-cc 226,413 716,460 flickr-bidir-cc 513,969 3,190,452 ljournal-2008 5,363,260 50,030,085 twitter-2010 41,652,230 1,202,513,046 friendster 65,608,366 1,806,067,135 com-amazon 334,863 925,872 com-dblp 317,080 1,049,866 com-youtube 1,134,890 2,987,624 com-lj 3,997,962 34,681,189 com-orkut 3,072,441 117,185,083 Table 2: The result of evaluating the heat kernel (hk) vs. PageRank (pr) on finding real-world communi-ties. The heat kernel finds smaller, more accurate, sets with slightly worse conductance. data F 1 -measure conductance set size amazon 0.325 0.140 0.141 0.048 193 15293 dblp 0.257 0.115 0.267 0.173 44 16026 youtube 0.177 0.136 0.337 0.321 1010 6079 lj 0.131 0.107 0.474 0.459 283 738 orkut 0.055 0.044 0.714 0.687 537 1989 friendster 0.078 0.090 0.785 0.802 229 333 scores, but using much smaller sets with substantially better F 1 measures. This suggests that hk-relax better captures the properties of real-world communities than the PageRank diffusion in the sense that the tighter sets produced by the heat kernel are better focused around real-world communities than are the larger sets produced by the PageRank diffusion.
These results suggest that the hk-relax algorithm is a viable companion to the celebrated PageRank push algorithm and may even be a worthy competitor for tasks that require accurate communities of large graphs. Furthermore, we suspect that the hk-relax method will be useful for the myriad other uses of PageRank-style diffusions such as link-prediction [29] or even logic programming [51].

In the future, we plan to explore this method on directed networks as well as better methods for selecting the parame-ters of the diffusion. It is also possible that our new ideas may translate to faster methods for non-conservative diffusions such as the Katz [25] and modularity methods [43], and we plan to explore these diffusions as well. Figure 5: (Top figure) Runtimes of the hk-relax vs. ppr-push, shown with percentile trendlines from a select set of experiments. (Bottom) Conductances of hk-relax vs. ppr-push, shown in the same way.
 This work was supported by NSF CAREER Award CCF-1149756. Figure 6: The top figure shows a scatter plot of con-ductance vs. community size in the twitter graph for the two community detection methods; the bottom figure shows a kernel density estimate of the conduc-tances achieved by each method, which shows that hk-relax is more likely to return a set of lower con-ductance.
