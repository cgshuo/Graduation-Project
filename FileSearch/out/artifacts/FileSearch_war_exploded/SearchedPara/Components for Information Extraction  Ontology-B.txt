 Information Extraction (IE) has existed as a field for sev-eral decades and has produced some impressive systems in the recent past. Despite its success, widespread usage and commercialization remain elusive goals for this field. We identify the lack of effective mechanisms for reuse as one major reason behind this situation. Here, we mean not only the reuse of the same IE technique in different situations but also the reuse of information related to the application of IE techniques (e.g., features used for classification).
We have developed a comprehensive component-based ap-proach for information extraction that promotes reuse to address this situation. We designed this approach starting from our previous work on the use of multiple ontologies in information extraction [24]. The key ideas of our approach are  X  X nformation extractors, X  which are components of an IE system that make extractions with respect to particular components of an ontology and  X  X latforms for IE, X  which are domain and corpus independent implementations of IE tech-niques. A case study has shown that this component-based approach can be successfully applied in practical situations. H.4.0 [ Information Systems Applications ]: General Theory, Experimentation Information Extraction, Ontologies, Software Components
The objective of Information Extraction (IE) is recogniz-ing and extracting certain types of information from natural language text [17]. Here, the decision to leave out irrelevant information is a conscious one and it reduces the difficulty associated with the task at hand. Because information ex-traction deals with natural language sources, it is seen as a subfield of Natural Language Processing (NLP). It has ex-isted as a field for a few decades and has experienced a sig-nificant development since 1990 X  X  partly due to the Message Understanding Conferences (MUC), which provided stan-dard extraction tasks and evaluation criteria.

Recently, Ontology-Based Information Extraction (OBIE) has emerged as a subfield of information extraction. It is based on the use of ontologies to guide the information ex-traction process [25]. An ontology is defined as a formal and explicit specification of a shared conceptualization [20]. Typically, an ontology consists of several components such as classes, properties, individuals and values. OBIE nor-mally takes place by specifying a domain ontology for the domain targeted by an IE system and using an information extraction technique to discover individuals for classes and values for properties. In addition, there are some OBIE sys-tems that construct an ontology for its domain. The details of several OBIE systems such as KIM [16] and Kylin [27] have been published in the recent past.

One of the most important potentials of OBIE is its ability to automatically generate semantic contents for the Seman-tic Web. As envisioned by Berners-Lee et al. [6], the goal of the Semantic Web is to bring meaning to the web, cre-ating an environment where software agents roaming from page to page can readily carry out sophisticated tasks for the users. For this vision to realize, semantic contents that can be processed by such agents should be made available. Information contained in ontologies fall under this category because Semantic Web agents are expected to process them automatically. This has been pointed out by several authors including Cimiano et al. [7].

Most OBIE systems use a single ontology. However, mul-tiple ontologies exist for most domains and there is no rule that prevents an OBIE system from using more than one ontology. In fact, the use of multiple ontologies can be ex-pected to improve the information extraction process: be-cause multiple ontologies provide different perspectives on a domain, a system that uses multiple ontologies has the potential to make more extractions (with respect to the dif-ferent perspectives) and the output of the system can be used to provide more accurate answers to queries related to different perspectives. Based on this idea, we designed the principles for using multiple ontologies in information ex-traction and conducted two case studies [24]. The results from the case studies indicate a significant increase in per-formance metrics when multiple ontologies are used. The insights provided by this work were crucial in the design of the component-based approach for IE described here.
As mentioned earlier, information extraction has improved rapidly as a research field. The details of several advanced IE systems such as TextRunner [5] and KIM [16] have been published in the recent past. However, it can be seen that information extraction still does not enjoy widespread use or commercialization, especially when compared with the field of information retrieval. Information Retrieval (IR) aims to identify documents related to a user X  X  request from a large collection of documents and this field has given rise to many widely used systems including search engines. It can be ar-gued that IE systems should be more useful than IR system because IE systems provide the required information itself instead of a pointer to a document. Yet, the usage of IE systems is very low when compared with IR systems. The costs and complexity associated with setting up an IE system in a new domain or a new text corpora can be seen as one main factor hindering the widespread usage of IE. This results in serious difficulties in applying information extraction techniques in new situations as well as in devel-oping large scale IE systems that work on different domains and different text corpora. It can be seen that there are two problems that give rise to this issue. 1. The requirement of templates : As described by Wilks 2. Bundling domain and corpus specific information with
The first problem mentione d above is targeted by the emerging paradigm of  X  X pen information extraction, X  which aims to discover relations of interest from text instead of be-ing provided in advance [5]. For instance, the TextRunner system [5], which is based on this paradigm, is capable of extracting relations using some grammatical structures and data tuples that fit into these relations in a single pass over the corpus. Open information extraction avoids the second problem by using IE techniques that do not use any domain or corpus specific information.

While open information extraction is a significant step forward, we believe that further advancement is possible by concentrating on the following issues. 1. Caution has to be exercised in the process of  X  X elation 2. IE techniques that use domain and corpus specific in-
In this paper, we present a comprehensive component-based approach for information extraction that addresses the issues mentioned above. As mentioned earlier, this approach is closely related to our work on using multiple ontologies in information extraction [24].

In our previous work we reused components of informa-tion extraction systems related to different ontologies. Such a component is defined as an information extractor ,which extracts individuals for a class or values for a property of an ontology. The general idea is to reuse an information extrac-tor for a class or a property of one ontology when making extractions with respect to some other class or property of another ontology that is related the original entity. Such re-lationships between classes or properties of different ontolo-gies are known as mappings . This technique was shown to improve the results of information extraction in our previous case studies. It was applied on the same corpus using more than one ontology instead of a single ontology. Different IE techniques were used with respect to different ontologies and the information extractor s related to these different IE techniques were reused.

In the component-based approach presented here, we ex-tend this idea to applying information extractors in differ-ent text corpora and different domains .Moreover,wedid not clearly define what constitutes an information extrac-tor in our previous work. Here, we formalize this using the concept of platforms for IE , which are domain and corpus independent implementations of IE techniques. An infor-mation extractor consists of a platform for IE and any do-main and corpus specific information used with the partic-ular class or property, which we call the metadata of the in-formation extractor. The separation between platforms and metadata makes reuse of information extractors structured and straight-forward.

It can be seen that the use of platforms for IE and infor-mation extractors addresses the problem of bundling domain and corpus specific information with IE techniques. Further, they allow the use of IE techniques that use domain and corpus specific information unlike open information extrac-tion, which is restricted to domain and corpus independent IE techniques. Regarding the requirement of templates, it can be seen that  X  X ntology construction X  (extracting classes and properties from text), undertaken by some OBIE sys-tems [13, 14] addresses this issue. In essence, this can be seen as following the paradigm of open information extrac-tion [25]. Moreover, ontologies guarantee that the extracted concepts fit into a conceptual framework unlike the relation discovery process undertaken by TextRunner [5].

In our component-based approach for IE, we identify com-ponents in an IE system based on ontologies. Other types of models such as relational models or UML class diagrams can be used for this purpose. However, we believe that on-tologies are the best option because of the following reasons. 1. Since ontologies are based on logic, they provide for-2. The IE systems that follow this approach can be easily
The rest of the paper is organized as follows. Section 2 discusses some related work and section 3 presents the design of our component-based approach. The details of the case study we have conducted on this approach is presented in section 4. Sections 5 and 6 provide a discussion on our work and future work respectively. Some concluding remarks are provided in section 7.
Details of several IE and OBIE systems have been pub-lished in the recent past and these systems use different IE techniques. One such system is Kylin [27], whose informa-tion extraction technique is based on classification and op-erates in two phases: identifying sentences in which inter-esting information is present and identifying words within sentences that carry the information. Kylin uses the Max-imum Entropy model for the first phase and Conditional Random Fields (CRF) for the second phase. It uses a set of Wikipedia pages as its corpus and attempts to extract information presented by the  X  X nfoboxes, X  which provide a summary of the contents of each page. Kylin can also be considered an OBIE system because it constructs an ontol-ogy based on the structure of the infoboxes in order to aid its information extraction process.

Another information extraction technique used by many information extraction systems is known as  X  X xtraction rules. X  Here, the idea is to specify regular expressions that can be used to extract certain information from text. For example, the expression (belonged|belongs) to &lt;NP&gt; ,where &lt;NP&gt; denotes a noun phrase, might capture the names of organi-zations in a set of news articles. This technique has been used by several IE systems including the ontoX [28] system and the implementations by Embley [9].

The Apache UIMA (Unstructured Information Manage-ment Architecture) project [1] appears to be the most seri-ous attempt so far to develop a component-based approach for information extraction. It targets analysis on all types of unstructured data including text, audio and video. It defines a common structure, known as Common Annotation Struc-ture (CAS), to store the extracted information and provides frameworks in Java and C++ to deploy the developed com-ponents. In terms of analyzing text, UIMA components have been mostly developed for general NLP tasks such as sen-tence splitting, POS tagging and tokenization although some components have been developed for extracting instances of specific classes such as gene names. UIMA components do not separate the domain and corpus specific information from the underlying IE technique, which is a key idea in our component-based approach. Moreover, UIMA assumes that the developed components are interoperable or reusable whereas our approach studies the basis for successful reuse and presents methods to improve reusability. It is also inter-esting to note that UIMA uses UML models (through type systems ) to relate the extracted information to domain mod-els. As mentioned in section 1.3, we believe that ontologies are a better option for this purpose.

In addition, Embley [9], Maedche et al. [14] and Yildiz and Miksch [28] have independently worked on including ex-traction rules in ontologies to come up with what has been termed  X  X xtraction ontologies X  or  X  X oncrete ontologies. X  The general idea here is to include extraction rules related to a class in the ontology itself. As we have pointed out in our previous work [24], the inclusion of these rules, which are known to contain errors, in ontologies appears to violate the requirement that ontologies should be formal. However, this approach can be seen as an attempt to identify components that can be used for information extraction. To a certain extent, our work can be seen as an extension of these works because it is based on a similar insight but attempts to ac-commodate more than one IE technique.

In our component-based approach for IE, we attempt to address the hard problem of information extraction by de-composing it based on ontological concepts. A similar ap-proach has been applied in image retrieval, in an area of study known as ontology-based image retrieval [22] and has produced strong results.
This section presents the design of our component-based approach. We start with its formal representation and then describe its relationship with component-based software en-gineering. Next we describe how it operates in practice and move onto presenting the details of two platforms for IE.
We provide a formal representation of our component-based approach for information extraction using the Z no-tation [19], which is a widely used formal specification lan-guage. We begin by providing a formal specification for a generic OBIE system and then show that this specification can be refined into a specification that uses the components of our component-based approach. This essentially proves that the component-based approach functions correctly.
In order to provide a specification for a generic OBIE sys-tem, we need a formal specification of an ontology. For this, we begin from the definition we have used in our pre-vious work [24], where an ontology is defined as a quintuple consisting of sets of different types of its components, and extend it by defining the nature of these components. This refined definition is shown below.
 Definition 1 Ontology : An ontology O is a quintuple, O =( C , P , I , V , A )where C , P , I , V , and A are the sets of classes, properties, individuals, property values and other axioms (such as constraints) respectively. These sets are defined as follows.
 C = { c | c is a unary predicate } P = { p | p is a binary predicate } I = { c ( i ) | c  X  C  X  iisagroundterm } V = { p ( x , y ) | p  X  P  X  x and y are ground terms  X  (  X  cc  X  C  X  c ( x )) } A = { a | a is an assertion } The important parts of the Z specification of a generic OBIE system is defining a schema for a ontology and show-ing how this schema is changed by the two main operations of an OBIE system, namely ontology construction (identi-fying classes and properties) and ontology population (iden-tifying individuals and values). For the sake of brevity, we only provide these sections of the specification here.
This specification can be generalized into an OBIE system that uses n ontologies instead of a single ontology. In this case, there would be n schemata for the n ontologies and n pairs of operations, one pair for each ontology.
 In the component-based approach, we keep the operation ConstructOK unchanged but refine the PopulateOK opera-tion into a pipeline of three separate operations. These op-erations perform the tasks of preprocessing the documents into a format that can be used by information extractors (using a preprocessor component), deploying information extractor components and combining the results produced by individual information extractors (using an aggregator component). The refinement of the PopulateOK operation into these three operations can be formally represented as follows. We do not provide the formal specifications of these individual operations (which are based on the intuitive de-scription given above) due to lack of space.
 PopulateOK Preprocess &gt;&gt; Extract &gt;&gt; Aggregate
As in the case of specification for an OBIE system, this refinement can be extended into the multiple-ontology case.
To summarize, the components in our approach are pre-processors , information extractors and aggregators .Asde-scribed in section 1.3, an information extractor consists of a platform for an IE technique, which is domain and corpus independent and metadata that is used with the particular class or property represented by the information extractor.
Component-Based Software Engineering (CBSE) studies how component-based approaches can be used in develop-ing software systems. This field has been in existence for more than a decade and has been successful in many do-mains. Since information extraction systems can be viewed as software systems, the field of component-based software engineering can be expected to provide some guidance in developing a component-based approach for IE.

Szyperski [21] provides a simple commonsense justifica-tion for the use of a component-based approach in software engineering in his textbook on this subject by stating  X  X om-ponents are the way to go because all other engineering dis-ciplines introduced components as they became mature and still use them. X  He concedes that from a purely formal view, there is nothing that could be done with components that could not be done without them. The differences are in goal-driven factors such as reusability, time to market, quality and viability. It can be seen that these arguments are more or less valid in the field of information extraction as well: a component-based approach, while not doing anything that cannot be done using existing techniques, has the potential to improve the information extraction process quantitatively and qualitatively.

In CBSE, it is generally agreed that software components have clear interfaces and functionalities. It can be seen that preprocessors, information extractors and aggregators described in section 3.1 satisfy these requirements. Fur-ther, Szyperski states that software components may con-tain  X  X eta-data and resources X  and even concedes that some degenerate components may only consist of such meta-data and resources. Hence, it can be seen that even the meta-data of information extractors can be considered indepen-dent components. This implies that an information extrac-tor is a component that consists of two sub-components, namely a platform and a metadata component.
Figure 1 presents a schematic diagram of an OBIE system consisting of the components described in section 3.1. We focus our attention on the information extractors because we believe that they contain the most valuable information cap-tured by IE systems. But preprocessors and aggregators are also very important in an IE system. The tasks performed by preprocessors depend on the IE techniques (implemented through platforms) that they are meant for and could in-clude tasks such as analyzing or removing HTML/XML tags, identifying section headings and POS tagging. The aggrega-tors combine the results produc ed by information extractors and do some adjustments on them. This includes reference reconciliation [8], which refers to the problem of identifying whether two individuals refer to the same real world entity.
For our component-based approach to work, there should be standard mechanisms to store the different types of infor-mation associated with information extractors and to rep-resent the links between them. We have designed a three-layered architecture, consisting of ontologies, metadata for information extractors and platforms for this purpose. Fig-ure 2 represents this architecture.

The ontologies represent the domain ontologies for which an information extraction system has been developed. Since the Web Ontology Language (OWL) [3] is increasingly being seen as the standard for defining ontologies, we represent ontologies using it. The metadata of information extractors and the details of platforms are stored in XML files and made available through URIs . The links between classes and properties of ontologies and the information extractors that have been developed for them are represented as OWL annotation properties in the ontologies. An XML file for the metadata of an information extractor contains an element that represents the URI of the platform that it uses. The XML file for the platform contains a link to the executable file for the platform.

OWL annotation properties provide a mechanism to in-clude the links between ontological concepts and information extractors in ontologies while excluding these links from rea-soning. These are typically used to include metadata about concepts such as version, creator and comments and are ig-nored by software agents that perform reasoning on the on-tologies. We consider the information extractors developed for classes and properties of ontologies to be a type of such metadata. Moreover, OWL annotation properties can be used with both classes and properties and can be specified as URIs. These properties match nicely with our require-ment to specify the URIs of information extractors for both classes and properties. We have also noticed that OWL an-notation properties have been used by Yildiz and Miksch [28] to include extraction rules in the ontologies. We store URIs of information extractors instead of extraction rules but the general idea is the same.

We selected XML as the format for representing meta-data of information extractors and platforms because it is a lightweight machine processable format that is widely used for information exchange. We considered using ontologies for this purpose but it was quite clear that advanced features of ontologies are not necessary here. We have designed XML schemata for creating XML files for platforms and metadata but do not include them here due to lack of space.
For our component-based approach to operate, mappings between different ontologies have to be identified. This is not a trivial task and there have been several works on dis-covering mappings between ontologies in an automatic or semi-automatic manner [11, 12]. In some cases it is possi-ble to identify mappings manually, but automatic mapping discovery techniques are required to make our component-based approach scalable.
 Once mappings between ontological concepts are discov-ered, reuse of information extractors can be performed in different ways as shown below. 1. Black Box Reuse: This refers to directly reusing an 2. Clear Box Reuse: Here, the metadata of the infor-3. Simple Combination: Here, more than one information 4. Advanced Combination: More advanced techniques,
It is interesting to note that the terms black box reuse and clear box reuse are used to convey similar meanings in the field of component-based software engineering [21].
In this section, we present the details of the domain and corpus independent IE platforms that we have developed.
As mentioned in section 2, this technique converts the problem of extracting information related to some concept into two classification problems: identifying sentences in which information is present and identifying words within sentences that are related to the information. Following Kylin [27], which uses this technique, we tried two different approaches for combining the two classification phases: a pipeline approach, where the word-level classifier only oper-ates on the sentences selected by the sentence-level classifier and a combination approach, where the word-level classi-fier operates on all the sentences but uses the output of the sentence-level classifier as one feature in classification. In the experiments conducted, we got better results using the combination approach and as such we incorporated it into our platform.

Since this IE technique is based on classification, it re-quires a training set in addition to the corpus on which in-formation extraction is to be performed (test set). Instead of requiring an annotated corpus indicating the positions where the particular concept is found, our platform allows providing key files specified for each file in the corpus. Then it internally annotates the text files with the keys provided based on string matching (allowing some prefixes and suf-fixes such as  X   X   X  and  X   X  X   X ). While this reduces the accuracy of the annotations, it also significantly reduces the effort re-quired to create the training corpus. For the test set, it is not necessary to provide keys but the accuracy of the platform can be automatically measured if keys are provided.
We experimented with different classification techniques for the two classification phases and found that Bayesian techniques (specifically Naive Bayes model) used with bag-ging produced best results in sentence-level and Conditional Random Fields (CRF), which is a sequence tagging tech-nique, produced best results at the word-level. Therefore, we incorporated these two techniques into the platform al-though the user has the option of selecting a different clas-sification technique for the sentence-level classification. We used the Mallet system [15] for the CRF technique and the Weka system [26] for other classification techniques.
In sentence-level classification, we used several domain and corpus independent features such as the word count for each POS tag. Similarly, we used domain and corpus inde-pendent features such as POS tags, stop words, the half of the sentence a word belong to (first or second) and capital-ization information at word-level. We adopted some of such features from the Kylin system. In addition, we used spe-cific words, WordNet [10] synsets of some words and some gazetteers 1 as concept-specific information. (Specific exam-ples are provided in section 4.2.) These features represent the metadata of the information extractor and are included in the XML file for the metadata. The platform requires such an XML file with metadata as an input. Currently, this platform runs only in the Windows operating system.
This platform uses the IE technique of extraction rules described in section 2 and considers the extraction rules de-veloped to make extractions for a particular concept to be the metadata for that concept. Unlike the two-phase classi-fication platform, this does not use any domain and corpus independent information. Further, this platform does not require a training set. (But in practice, a training set is re-quired to identify the extraction rules.) As in the case of the platform for two-phase classification, keys for the test set can be provided to evaluate the accuracy although it is not required.

We implemented this platform using the General Architec-ture for Text Engineering (GATE)[2], which a widely used NLP toolkit that can be used to directly deploy extraction rules. Here, the rules have to written in a format known as Java Annotation Patterns Engine (JAPE), which is inter-preted by GATE. As such, these JAPE rules are included in the metadata of the information extractor. In addition, the metadata also include gazetteers, which can be used by the JAPE rules. As in the case of the platform for two-phase classification, the platform requires an XML file with metadata as an input. This platform does not depend on a
Gazetteers provide a list of known entities for a particular category, such as states of the U.S. and are often used in information extraction. particular operating system and we have successfully tested it in Windows and Unix environments.

In developing these platforms, we observed that the imple-mentation is much neater than a regular IE system because we had to consider only one type of extractions instead of a set of different types. However, the platforms have to be ex-ecuted separately for each concept. Parallel processing can be used to improve the efficiency on this regard.
In our previous work on using multiple ontologies in in-formation extraction [24], we conducted a case study on a set of news articles related to terrorist activities using two ontologies that provide different perspectives on the domain of terrorist attacks. The corpus consists of 200 news articles (160 for the training set, 40 for the test set) selected from the corpus of the 4th Message Understanding Conference. These news articles are on terrorist activities in Latin Amer-ican countries in late 1980 X  X  and early 1990 X  X . One ontology has been derived from the structure of the key files provided by the conference (which is called the MUC4 ontology ) while the other is a terrorism ontology defined by the Mindswap group at the University of Maryland (which is called the Mindswap ontology ). We performed information extraction using classification with the MUC 4 ontology and using ex-traction rules with the Mindswap ontology. However, we did not perform full information extraction and stopped at the phase of identifying sentences related to the concepts in concern. Based on the mappings between concepts of different ontologies, we combined the extractions made by different information extractors using the union operation and the results showed that this leads to an improvement in performance measures.

In the case study described here, we first applied our plat-forms for information extract ion in the corpus used by the above case study, which we call the MUC 4 corpus .Weused the two-phase classification platform with the MUC4 ontol-ogy and the extraction rules platform with the Mindswap ontology. We selected a set of classes and properties from each ontology to use in information extraction. In order to apply the two platforms, we identified words, WordNet synsets and gazetteers for each concept used with the two-phase classification platform and extraction rules (in JAPE format) and gazetteers for each concept used with the ex-traction rules platform. Some features and rules of our pre-vious work were reused in this exercise. We also developed techniques for identifying these information, the details of which are presented in section 4.2. We then wrote these features into XML files conforming to the XML schema for metadata, executed the platforms using the XML files and obtained results as well as performance measures.
The next step of our case study was reusing the informa-tion extractors developed for the MUC4 corpus in a different corpus and a different ontology. For this, we compiled a cor-pus of Wikipedia pages on terrorist attacks. We selected 100 Wikipedia pages and randomly split it into a training set of 70 pages and a test set of 30 pages. We also constructed a simple ontology for terrorist attacks based on the fields of infoboxes of the selected Wikipedia pages (which we call Wikipedia ontology ). The keys for the files were also derived from the infoboxes. Then, we identified mappings between the concepts of this Wikipedia ontology and the MUC4 and Mindswap ontologies described above. Based on these map-pings, we reused the XML files containing the metadata of the information extractors together with the platforms to perform information extraction on the Wikipedia corpus.
Figure 3 shows sections of the ontologies used and some mappings between them.
In order to discover the words and WordNet synsets to be used with the two-phase classification platform, we used human knowledge as well as a statistical approach. We se-lected certain words and their WordNet synsets based on our understanding of the concepts in concern. For instance, we selected the words  X  X ill X  and  X  X idnap, X  as well as their WordNet synsets as features for the class  X  X uman Target. X  In addition, we employed the following statistical technique. First we selected the sentences that contain key values from the training files. Then we identified the words that are found in more than a predefined fraction of these sentences (5% was selected as the threshold after some experiments). Next we performed correlation analysis (using the frequency of these words among all sentences) to ensure that the se-lected words have a positive correlation with keys instead of just being frequent words. We used the statistical mea-sure of lift for this purpose and words having a lift of more than 1 were selected (which mean that they have a positive correlation with key sentences). Still, common words such as  X  X he X  and  X  X o X  were often included in the selected set of words and we excluded them from the final set of features. There was some overlap between the words selected based on human knowledge and words mined using the statistical technique but many new words were also discovered by the statistical technique.

Following the generally accepted practice on the use of gazetteers, we selected standard sets of lists for certain con-cepts as gazetteers. For instance, we used a set of Spanish first names provided by a website 2 as a gazetteer. In addi-tion, we used some lists provided by the support material of the MUC 4 conference.

The extraction rules to be used with the Mindswap on-tology were discovered by first separating the sentences con-taining keys from the training files and then manually going through these key sentences to identify extraction patterns. Correlation analysis was used, in a manner similar to its application described above, to ensure that the discovered patterns are useful. In addition, gazetteers were identified in http://www.zelo.com/firstnames/names/spanish.asp the manner described above. Most of these gazetteers were the same ones used with the classification platform.
The Wikipedia pages on terrorist attacks were identified from a list of terrorist incidents provided by Wikipedia. The majority of selected pages were on terrorist activities in the decade of 2000.

It was seen that infobox structure is not uniform among different Wikipedia pages (since authors of pages can add their own attributes for the infoboxes) and as such we only included attributes that are found in at least 20% of the pages in the Wikipedia terrorism ontology. A similar ap-proach has been adopted by Kylin [27]. In addition, it was seen that the infoboxes of most of the pages had to be man-ually refined before being used as gold standards. Often this included removing descriptions such as  X (as reported) X  and removing fields such as  X  X elligerent: Unknown. X  In some situations, we also manually filled missing information.
As mentioned earlier, we need the mappings between on-tologies in order to reuse the information extractors. Here, we need the mappings between the MUC4 and Wikipedia ontologies as well as the mappings between the Mindswap and Wikipedia ontologies. In order to discover these map-pings, we tried to use Anchor Flood [11] and Falcon-AO [12] systems, which are two recently developed mapping discov-ery systems. The precision of the discovered mappings were quite high (close to 80%) but although the recall was also high (close to 70%), we observed that some important map-pings we were planning to use were not discovered by these systems. For instance, both systems failed to discover the mappings between Belligerent class of the Wikipedia ontol-ogy and Perpetrator and Perpetrator Organization classes of the MUC4 ontology shown in figure 3 above. As we discuss in the following sections, our component-based approach can detect incorrect mappings to a certain extent but in the case of missing mappings there is no alternative other than man-ually reviewing the entire ontologies to discover mappings.
In addition to the mappings shown in figure 3, we used the following mappings. Location 25.86 52.94 34.75 15.83 40.00 22.68 18.26 46.32 26.19 Human Target 27.71 36.51 31.51 1.83 5.26 2.72 1.35 5.26 2.15 Physical Target 38.89 67.74 49.41 1.34 5.26 2.14 1.80 5.26 2.68 Perpetrator 26.67 27.27 26.97 19.39 28.36 23.03 22.69 40.30 29.03 Instrument 53.33 66.67 59.26 25.40 40.00 31.07 25.81 40.00 31.38 Instrument Type 60.00 46.15 52.17 25.71 45.00 32.72 25.42 37.50 30.30 City 53.85 90.32 67.47 34.69 17.89 23.61 22.39 31.58 26.20 Country 41.27 66.67 50.98 34.69 17.89 23.61 32.88 25.26 28.57 Location 35.21 56.39 43.35 8.93 5.26 6.62 20.88 20.00 20.43 Military Organization 39.86 79.73 53.15 Victim 34.86 45.86 39.61 0.82 5.26 1.42 0.82 5.26 1.42 Givernment Agent 33.80 55.17 41.92 Terrorist 22.81 52.00 31.71 17.92 28.36 21.96 17.72 41.79 24.89
In reusing the information extractors of the MUC4 cor-pus with the Wikipedia corpus, we used the black-box reuse, clear-box reuse and simple combination techniques described in section 3.3. When reusing clear-box reuse, we identified words and extraction rules that can be used as features using the statistical analysis techniques described above. In addi-tion, we changed the gazetteers used to take the new domain into consideration. For instance, we replaced the gazetteer of Spanish first names with a list of common Indian, Arabic, U.S. and Spanish first names. We did not apply the plat-forms on the Wikipedia corpus while ignoring the MUC4 information extractors, because it was seen that this would be quite similar to clear-box reuse in most cases.
In evaluating the results, we used a scorer that compares extractions made with the gold standard provided in the key files for the test set. It operates based on string matching (while allowing some prefixes and suffixes such as  X   X   X  and  X   X  X   X ) and counting words. The figures calculated using this scorer for precision, recall and F1 are shown in tables 1 and 2. The first column of each table shows the concept of the MUC4 or Mindswap ontology used with the MUC4 corpus. Results are shown separately for the MUC4 corpus and black-box and clear-box reuse in the Wikipedia corpus.
From the results shown in tables 1 and 2, it can be seen that the reuse of information extractors has been gener-ally successful. While the performance measurements have recorded a drop when the inform ation extractors are reused, they have normally recorded a F1 in the range of 25%-30%. Not surprisingly, clear-box reuse has shown better results than black-box reuse (because it adds some features better suited for the Wikipedia corpus).

The exception to this are the results for reuse based on mappings for the Target class of Wikipedia. For this map-ping, the F1 is lower than 5%. In analyzing the reasons for this, we found out that targets specified by Wikipedia in-foboxes are very different from the human targets (victims) and physical targets specified for MUC4 corpus. They con-tained very few person names, effectively invalidating the reuse of information extractors for the Human Target class of MUC4 and the Victim class of Mindswap. On the first glance, it appeared that there was a somewhat stronger re-lationship between the Target class of Wikipedia and the Physical Target class of MUC4 but a closer inspection re-vealed that even this relationship is questionable. Targets of Wikipedia often contained phrases such as  X  X oscow Metro, X  which were quite different from the physical targets identi-fied in the MUC4 corpus. In other words, the mappings identified between the Target class of the Wikipedia ontol-ogy and classes of other ontologies appear to be contradicted by the actual data of the corpora although the mappings make sense intuitively. In other words, even manually iden-tified mappings can not be considered 100% accurate.
For the application of each platform in each ontology, ag-gregate performance measures can be computed as follows. Assume that C = { C 1 , C 2 ,..., C n } denotes the set of con-cepts (classes and properties) for which extractions are made. For each concept C i ,let correct ( C i ) , total ( C i )and all ( C denote the number of correct extractions, total number of extractions and the number of extractions in the gold stan-dard respectively. Then aggregate measures for precision and recall can be calculated as follows. F1 will be the har-monic mean between precision and recall as usual.
Tables 3 and 4 present these results. The Target class has been excluded from the calculations for the Wikipedia corpus. These results further highlight that the reuse of in-formation extractors has been generally successful. It can also be seen that the reusability of information extractors is higher with the classification platform. This can be expected Wikipedia/Black-Box 19.68 35.60 25.35 Wikipedia/Clear-Box 21.54 41.75 28.42 Wikipedia/Black-Box 23.70 17.42 20.08 Wikipedia/Clear-Box 22.76 30.31 26.00
Table 4: Aggregate Results for Extraction Rules because it uses words and WordNet synsets as features in-stead of hand-crafted rules for a particular corpus.
As mentioned earlier, we also attempted to use the sim-ple combination technique described in section 3.3. For instance, the output of the information extractors for the Perpetrator and Perpetrator Organization classes from the MUC4 ontology can be combined to get the results for the Belligerent class of Wikipedia and the output of this combi-nation can be combined again with the output produced by combining the output for Terrorist Organization and Ter-rorist classes of Mindswap. When union was used for com-bination, it was observed that F1 measure drops by around 2% -5%, when compared with the best original informa-tion extractor. The possible reason for this is the increase of errors when the information extractors are reused in a different corpus. (We obtained better results by combining results in this manner for the corpus for which information extractors are developed in our previous work [24]). We also experimented with the intersection operator and it was seen that this results in unpredictable behavior ranging from a slight gain in F1 measure to a drastic drop in F1 (when individual information extractors identify sub-concepts of a broader concept, such as countries and cities falling under the Location class).

In discussing the implementation work, we have concen-trated on the use of platforms and information extractors. Currently, these platforms do not require any preprocessors but we are considering to delegate tasks such as POS tagging to such a preprocessor component. We have developed an aggregator component that can be used with the Wikipedia corpus, where all the extractions are related to a main class (in this case, Terrorist Attack). In literature, this form of information extraction is often known as attribute-based IE [18]. The information extraction using the Mindswap and MUC4 ontologies represent a more complex type of in-formation extraction, where extractions are related to mul-tiple classes (known as relation-based IE [18]). Developing a generic aggregator component for this case a complex prob-lem. Currently, we are developing an aggeragator using the simplifying assumption that all object properties should have a single main class as the domain . This assumption is valid with the MUC4 and Mindswap ontologies as well as in many other ontologies used in information extraction.

All the details of this case study including executable plat-forms and XML files will be made available from our project web site 3 . The executable platforms in particular should be useful tools for researchers. http://aimlab.cs.uoregon.edu/obie/
It can be seen that the performance measures we have obtained are somewhat lower than those of some other sys-tems. For instance, the Kylin system [27] has recorded F1 measures in the range of 40% -45% for similar Wikipedia pages. For the classification platform, the main reason for this appears to be the insufficiency of training data. Kylin has faced the same problem and has employed special mecha-nismstoaddnewtrainingdatatoimproveperformance[27]. For the extraction rules platform, the only way to improve results is to identify better extraction rules. On the other hand, these results mark an improvement over the results of our previous case study [24] (F1 measures of 30% -35% for the MUC4 corpus) because we only performed information extraction at the sentence-level in that case study.
In conducting our case study, we tried to reuse the extrac-tion rules used by the FASTUS system [4], which has par-ticipated in the MUC4 conference and which is one of the first IE systems to use this technique. By communicating with the authors of this system, we learnt that it has been superseded by another system and that even the developers of the system do not have easy access to the extraction rules, which are considered source of the program. While conced-ing that the situation might be different in some other IE systems we believe that this provides an indication of what happens to the useful information captured by IE systems. Such information (extraction rules in this case), get mixed up with source code of the programs and often it is diffi-cult to retrieve them to be used in a new application. A component-based approach with structured mechanisms to store such information would correct this situation.
Even though there has been a surge in interest in informa-tion extraction, experts appear to have very different opin-ions on its future. Some are convinced that it is the way to go while others suspect whether it would be good for anything. Such diametrically opposing views were expressed in a panel discussion in the CIKM 2009 conference 4 . Much of the crit-icisms of information extraction target its inflexibility and apparent brittleness. We believe that this situation shows the need to seriously look at radically different approaches for information extraction in addition to attempting to make incremental improvements in existing techniques.
The following can be identified as the main work to be carried out to make our component-based approach for in-formation extraction fully usable. http://www.comp.polyu.edu.hk/conference/cikm2009/ program/panels.htm
In this paper, we have presented the design of a com-prehensive component-based approach for information ex-traction. As a part of this approach, we have presented two generic platforms for information extraction which have the potential to be applied in different situations. We have also conducted a case study that shows that the presented component-based approach functions as expected.

Due to the current state of affairs in information extrac-tion -showing a lot of potential but still failing to achieve widespread usage or commercialization -we believe that re-search community should seriously consider new approaches for the field in addition to making incremental changes in the existing techniques. We believe that our component-based approach constitutes such an approach and thus deserves the attention of the research community. We would like to thank Dr. Michal Young and Dr. Arthur Farley of the Computer and Information Science Depart-ment of University of Oregon for their help in this work. [1] Apache UIMA. [2] General Architecture for Text Engineering (GATE). [3] OWL Web Ontology Language. [4] D. E. Appelt, J. R. Hobbs, J. Bear, D. J. Israel, and [5] M. Banko, M. J. Cafarella, S. Soderland, O. Etzioni, [6] T. Berners-Lee, J. Hendler, and O. Lassila. The [7] P. Cimiano, S. Handschuh, and S. Staab. Towards the [8] X. Dong, A. Y. Halevy, and J. Madhavan. Reference [9] D. W. Embley. Toward semantic understanding: an [10] C. Fellbaum, editor. WordNet: An Electronic Lexical [11] M. S. Hanif and M. Aono. Alignment results of [12] W. Hu and Y. Qu. Falcon-AO: A practical ontology [13] C. H. Hwang. Incompletely and imprecisely speaking: [14] A. Maedche, G. Neumann, and S. Staab.
 [15] A. K. McCallum. Mallet: A machine learning for [16] B. Popov, A. Kiryakov, D. Ognyanoff, D. Manov, and [17] E. Riloff. Information extraction as a stepping stone [18] S. Russell and P. Norvig. Artificial Intelligence: A [19] J. M. Spivey. The Z notation: a reference manual . [20] R. Studer, V. R. Benjamins, and D. Fensel.
 [21] C. Szyperski. Component Software . Addison-Wesley [22] H. Wang, S. Liu, and L.-T. Chia. Does ontology help [23] Y. Wilks and C. Brewster. Natural language [24] D. C. Wimalasuriya and D. Dou. Using multiple [25] D. C. Wimalasuriya and D. Dou. Ontology-based [26] I. H. Witten and E. Frank. Data Mining: Practical [27] F. Wu, R. Hoffmann, and D. S. Weld. Information [28] B. Yildiz and S. Miksch. ontox -a method for
