 Earmarks are legislative provisions that direct federal funds to specific projects, circumventing the competitive grant-making process of federal agencies. Identifying and cata-loging earmarks is a tedious, time-consuming process carried out by experts from public interest groups. In this paper, we present a machine learning system for automatically ex-tracting earmarks from congressional bills and reports. We first describe a table-parsing algorithm for extracting budget allocations from appropriations tables in congressional bills. We then use machine learning classifiers to identify budget allocations as earmarked objects with an out of sample ROC AUC score of 0.89. Using this system, we construct the first publicly available database of earmarks dating back to 1995. Our machine learning approach adds transparency, accuracy and speed to the congressional appropriations process.
An earmark is a legislative provision that directs federal funds to specific projects, circumventing the competitive grant-making process of federal agencies. It has been dif-ficult to study how earmarking has affected the legislative process due to a lack of comprehensive and open data on earmarks. In fact, earmarking is often considered  X  X he best known, most notorious, and most misunderstood aspect of the congressional budgetary process X  [14].

In the past, earmark datasets were created manually by experts from governmental agencies, such as the Office of Management and Budget (OMB) and public interest groups such as Taxpayers for Common Sense, Washington Watch, and Citizens Against Government Waste. Congress pro-duces thousands of pages of legal text each year, making the process time-consuming and expensive. As a result, past ef-forts were limited to short time spans and a limited set of documents. A further issue, is that each of these groups had The first two authors contributed equally to this study. different definitions, motivations, and processes for identify-ing and cataloging earmarks.

Given the significant changes in American politics in the last 20 years, e.g. increased polarization, changes to cam-paign financing, and a recent ban on earmarks [3, 6], datasets with short-term coverage are inadequate for political scien-tists to draw robust policy conclusions. A consistent, histor-ically complete dataset has the potential to reveal valuable insights on effective governance and answer questions such as:  X  X ow instrumental is earmarking to passing controver-sial legislation? X ;  X  X hat effect does securing earmarks have on campaign financing and reelection? X ; or  X  X oes being a chair of congressional subcommittee affect how funds are appropriated to the legislator X  X  state or district? X 
In this paper, we present a machine learning system for automatically extracting earmarks from congressional bills and reports. This approach allows us to cheaply, reliably, and consistently extract earmarks from historical congres-sional documents. Furthermore, the method is transparent and reproducible, enabling analysts to easily understand, evaluate, and build on our work [7]. Using our system, we construct the first publicly available database of earmarks dating back to 1995.
 The dataset is already being used by Harris School of Public Policy researchers of the University of Chicago to do public policy research and analysis. In addition, it is being promoted by both the Center for Data Science and Public Policy, also of the University of Chicago, as well as the Sun-light Foundation to researchers and practitioners interested in government transparency and public policy. An overview of the entire system is depicted in Figure 1. The ultimate goal is to be able to take a document, extract all the potential budget allocations that occur in the doc-ument, and finally classify which allocations are earmarks. Extracting potential budget allocations involves identifying tables in the document that contain budget allocations and parsing them into rows, where each row is a separate al-location. This process is described in Section 4. Building a classifier for determining which allocations are earmarks requires a labeled set of allocations. We generate a corpus of labeled allocations by matching the extracted allocations to an earmark dataset compiled by the Office of Manage-ment and Budget (OMB) in the late 2000s (see Section 5.1). Our method for labeling earmarks involves a combination of hand-labeling and machine learning and is discussed in Section 5. Given this labeled set of annotations, we train an earmark detection classifier as described in Section 6. Fi-nally, we run our earmark classifier on all congressional bills and reports going back to 1995 and show some preliminary analysis in Section 7.
As many readers may not be familiar with the congres-sional budgetary process and how earmarking occurs, we provide a brief overview.

At the beginning of each fiscal year, the President submits a proposal for the year X  X  budget to Congress. The budget proposes funding levels for the various government entities and broadly outlines spending limits and revenue expecta-tions for at least the next five years. Next, the House and Senate budget committees review the President X  X  proposal and pass a budget resolution.

Mandatory spending and interest payments account for the majority of federal spending [2]. The House and Senate appropriations committees divide what remains in the bud-get resolution to their twelve sub-committees. The twelve sub-committees then write the bills that authorize discre-tionary spending, and each of those bills becomes a law if approved by each chamber and signed by the President. Congress combines those bills into a single omnibus bill and votes on it.
 Earmarks can enter at almost any point of the process. Senators and representatives can insert earmarks into the text of appropriations bills, including supplemental appro-priations and continuing resolutions [15]. they can place ear-marks in the explanatory report attached to the bill. They can also contact bureaucrats directly.

As the budget process has changed over time, so has the placement of earmarks:  X  X uring the 19th century, earmarks were often placed in the law. But after the adoption of the Budget and Accounting Act of 1921, most earmarks were in-cluded in legislative reports X  [8]. Congress officially banned earmarks in 2010, but members have continued to request and receive them [9, 4]. Senators and representatives have increasingly turned to calling and writing federal agencies directly [10].
The first step to identifying earmarks is to extract appro-priations found in congressional bills and reports. We fo-cus on extracting allocations mentioned within tables, where 85% of the earmarks occur (see Sec. 5.4). Future work will identify earmarks in free text to increase the coverage of our dataset.

Several approaches to table parsing have been developed in the field of information retrieval. Pyreddy et al. ex-ploit table layout in text documents and develop a character alignment graph (CAG) that uses heuristic methods to iden-tify tables within documents [13]. They identify sections of tables within documents. Pinto et al. extend the CAG to extract individual cells from tables [11]. Our initial analyses found that many tables shared similar attributes. We thus employ a heuristic based approach described in the following section.
The Government Printing Office (GPO) provides congres-sional bills and reports as plain text files where tables ap-pear as blocks of formatted text. Indentation, white space, and dots and dashes are used to format tables. We first segment a document into paragraphs, where paragraphs are separated by two new line characters or more. Then each paragraph is classified as a table or free text. A paragraph is labeled as a table if the percentage of rows satisfying any of the following conditions exceeds a threshold:
The threshold was set empirically to 0.3. In the exper-iments section (5.6), we show that this heuristic retrieved more than 98% of the tables in congressional reports and bills.

Furthermore, tables in Congressional bills and reports can be categorized into two main types: dotted tables and dashed tables . Dashed tables use lines of all dashes to separate rows and whitespaces to separate columns. Dotted tables do not have special lines separating rows: each line is a row or part of a multi-line row, and columns are separated by dots and whitespace. See Figure 2 for examples. We distinguish these two classes of tables because parsing each type requires different rules and heuristics.
Parsing a one-line table header requires splitting on two or more white spaces. In the case where headers span mul-tiple lines, we introduce an algorithm that clusters words in the header based on their vertical overlap. The simple idea is that two words on two consecutive lines that inter-sect vertically belong to the same header. First, each line is split into cells by two or more consecutive white spaces. Each cell in every row is represented as a four-dimensional tuple ( text, line, begin, end ), where text is the clean text of the cell, line is the line number, begin is the offset within the line at which the text begins, and end is the last index within the line at which the cell ends. The tuples are fed into a clustering algorithm 1 to detect headers. The algorithm returns the list of headers of the table.

Algorithm 1: Header Identification Algorithm input : List of cell tuples Cells output : Table Headers Sorted = Sort ( Cells, key = begin ); Clusters  X  X  X  List ; C  X  X  X  List ; Add Sorted [0] to C ;
Add C to Clusters ; for i = 1; i &lt; length ( Sorted ); i + + do end
Headers  X  X  X  List ; for C  X  Clusters do end return Headers ;
Tables are treated as a collection of columns where each column divides rows into multiple cells. The idea behind detecting column dividers is that column boundaries do not contain any text. Instead, they consist of whitespace or other delimiting characters across all the rows. That is, a column divider is a pair of begin and end positions such that only whitespace is observed within this span for all the lines in the table. Algorithm 2 shows how these tuples are found. In the following section, we discuss how multiline rows are detected and merged. Each line is initially treated as a separate row in the table. Because rows can span multiple lines, we develop heuristics to detect multiline rows and merge the related lines. For dashes tables, identifying multiline rows is trivial because rows are separated by a line of dashes. For dotted tables, things are more involved. The basic concept is that all valid table rows need to have a money allocation in one of their as-sociated lines. This is guaranteed by our table-identification heuristic described earlier. In the case of multiline rows, money allocations can appear either in the first or the last line. Fortunately, the position of the allocation tends to be consistent within a table, which makes it easy to group multiline rows. This heuristic is shown to provide accurate extraction in the experiments section.

Algorithm 2: Column Identification Algorithm input : List of table rows Rows p  X  X  X  get white space positions ( Rows [0]); for row  X  Rows do end indices  X  X  X  Sort ( p ); dividers  X  X  X  list ; if length ( indices ) = 0 then else end return dividers ;
To evaluate the the accuracy of our table-identification methodology, we randomly selected 40 documents and tagged all 384 tables in those documents. On this subset, our table-identification heuristic recalled 98 . 6% of all of true tables, while 89 . 2% of the predicted tables were true tables. For the table identification task, we value recall higher than preci-sion because an earmark missed in this step will never be recovered and rows that are not allocations can be weeded out later. To evaluate the column-identification and row-merging algorithms, we randomly chose 30 tables from those 40 documents. We correctly identified columns and rows in all 30. The output of the table parsing algorithm, described in Sec. 4, is a collection of tables that are neatly parsed into rows and columns. Each non-header row in an allocation table describes an allocation and is a potential earmark. Eventually, we want to take a supervised learning approach to building a model to classify allocations as earmarks. This approach, however, requires labeling a set of allocations as earmarks or just regular appropriations.

In this section, we describe how we use a corpus of ear-marks from the Office of Management and Budget (OMB), described in Sec. 5.1, to generate a labeled set of alloca-tions. In particular, we attempt to match each earmark in the OMB dataset to a corresponding allocation. In order to perform this matching task at scale, we first match a subset by hand (Sec. 5.4) and then train a classifier to pre-dict whether a budget allocation matches an earmark in the OMB dataset (Sec. 5.5 and Sec. 5.6).
In 2007, the OMB ordered all departments and agencies to identify and catalog earmarks that appeared in appropri-ations and authorization bills and reports for 2005. Over the course of three months, those departments and agencies sent the OMB congressional funding data. The OMB used this to compile a list of congressional earmarks [12]. This process was repeated in 2008, 2009, and 2010. The OMB posted all the data in CSV format on its website. 1
For each earmark, the OMB may provide the following: See Table 1 for an example OMB record.
 Documents Congress 111, House Bill 3293
Excerpt ...of which $13,455,000 shall be used for
Short Desc. Dillard, University, New Orleans, LA Full Desc. NA Recipient NA
The first step in matching OMB records to allocations is to map each document cited in the OMB corpus to a congressional document 2 . Unfortunately, there is no unique government document ID, which would make linking trivial. Instead, the OMB references the documents in which an earmark appears in myriad ways, such as specifying a bill number, a report number accompanying a bill, a public law, or a common name for a bill. Table 2 lists typical examples of references. Table 3 describes how references are resolved for OMB data from 2008. The heuristics for the other years are similar.

Earmark ID Citation Reference 340045 H. Rept. 110-434 235530 P.L. 110-161 235531 Joint Explanatory Statement to accom-http://earmarks.omb.gov/earmarks-public/
We use the Government Printing Office (GPO) as our source of congressional texts.
 Citation Reference H.Rept.
 XXX-YYY; S.Rept.
 XXX-YYY H.R. XXX; S.XXX P.L. XXX-YYY
Joint Ex-planatory Statements
After linking OMB document references to documents in our corpus of congressional texts, we used fields from an OMB record to link OMB records with allocations extracted from the corpus. For example, consider the OMB record in Table 1 and a table extracted from the cited document in Figure 3.

If an earmark occurs in a table, the OMB does not actu-ally cite the text of the table row the earmark appears in. Instead, they cite part of a bill, which alludes to the fact that an allocation is specified in a report accompanying the bill. This is the case in the example given in Table 1. Even when the excerpt is taken from the report containing the earmark, it will cite the section of the report that alludes to a table which contains the earmark; for example, Thus, the excerpt is of little use in matching the table rows. It would, however, directly provide labels for earmarks that occur in plain text. As a result, we need to use the recipient and description fields for matching. In the example above, there is a perfect string match between the short descrip-tion and the table cell corresponding to the 11th row of the project column. In general, the short description could be the concatenation of multiple table cells, a single table cell could be the concatenation of the short description and the full description, the description could be a permutation of entities in the table cell text, or the descriptions could con-tain abbreviations and misspellings of entities in the table cell text.

The matching task is even more complicated when recipi-ents and descriptions are not unique within a document. A recipient can receive multiple earmarks within the same doc-ument, and multiple recipients can receive earmarks for the same purpose (e.g.  X  X or equipment and facilities X ). Further-more, the same recipient can receive funding for the same purpose in multiple table rows within the same document. This means there is a one-to-many relationship between an earmark record from the OMB and table rows in a docu-ment.

Our matching evaluation works as follows. The matching algorithm is perfect if it matches a row for the earmarked appropriation in the document to its OMB record.

Consider a table row that represents an earmark. It will be incorrectly labeled negative if it does not get matched to a record from OMB either because the record is missing or because the matching was done incorrectly. It will correctly receive a positive label if it matches with a record from the OMB. Note that even if it was matched with the wrong OMB record, it would still be correctly labeled. So if we do matching for the purpose of labeling only, then for the algorithm to perfectly label the table rows, it must only match every table row that represents an earmark to some earmark.

Let X  X  now consider a row that does not represent an ear-mark. It will correctly receive a negative label if it does not get matched with a record from OMB. It will be incorrectly labeled positive if it matches with a record from OMB, ei-ther because the matching was done incorrectly or because the OMB contained a record that is not really an earmark. For the purposes of this paper, we treat the database from OMB as definitional. We do not exert our own judgment in adding or removing earmarks from their records. We attempt to get the best labels we can by building the best matching algorithm that we can.

As mentioned above, multiple table rows can map to the same OMB record and mapping two OMB records to the same table row does not necessarily imply a labeling error. Because there are no clear constraints on the mapping from OMB records to table rows, we treat the matching problem as a simple classification problem.
As mentioned above, our goal is to build a classifier that, given an OMB record and a table row, predicts whether they match. We generated a training set to train this matching classifier in a semi-manual fashion. We first take an OMB record r and extract the set of documents D r in which r oc-curs. Then for each document d r  X  D r , we compute a simi-larity score SIM ( t d r , r ) between every table row t d The similarity score is the maximum of the Jaccard similar-ities between the bigrams in t r and the bigrams in r  X  X  short description r.sd , full description r.f d and recipient r.rec . For convenience lets define F r = { r.sd, r.f d, r.rec }
Within each document, pairs of the OMB record and the table rows are ranked according to the similarity score SIM . We then looked at the top 20 pairs and hand label them as being a match or not. All other pairs are automatically labeled as not matching. Table 4 gives an example of de-scriptions from an OMB record and the five most similar table rows. Cells within the table are separated by the | symbol and are removed before computing similarities. Short Desc Full Desc 1 Trimble Local School District, Glouster, OH 2 Elementary and Secondary Education (in-3 YMCA of Warren, Warren, OH for an after-4 City of Newark, CA for an after-school pro-5 Memphis City Schools, Memphis, TN for an Table 4: Descriptions from an OMB record along with the top 5 most similar rows defined by SIM .

In this example, the earmark occurs in two tables within the same document, and the two corresponding rows are ranked highest. The other rows are also earmarks for after-school programs, but they are for different districts or or-ganizations. We applied this labeling procedure to 516 ran-domly selected OMB records and found at least one match-ing table row 438 times, thereby giving 85% as an estimated lower bound of earmarks in tables. Because the OMB may cite multiple documents for every record, there were 840 cases in which we tried to match a record to a table row within a specific document, and we found at least one match-ing row 534 times. There are at least seven possible reasons an OMB record is not matched with a table row within a document that the OMB cites. For each error, assume the previous errors where not made: 1. The document is actually a bill that cites a report 2. The document is a public law or resolution; we do not 3. The citation was parsed incorrectly; we are looking in 4. The earmark does not appear in a table but in plain 5. The table was not parsed correctly, so the matching 6. The matching row did not appear in the top 20. 7. The matching row was hand-labeled as not matching.
Although one might like to find every occurrence of every earmark, we are most concerned with finding every earmark an earmark in a correctly parsed table, we would have an incorrectly labeled table row. This can only happen if errors 1-5 are not made and either error 6 or error 7 seven is made. If our matching algorithm is as good as the gold-standard human labels and the statistics above generalize, then we can estimate the upper bound of incorrectly labeled rows. We extracted 530k table rows and the OMB gives 122k oc-currences of earmarks. In the worst case, where errors 1-5 are never made, we would have 8 . 4% of data mislabeled.
In this section, we describe the feature sets we designed for the matching task. Features are computed over pairs of table rows and OMB record pairs. For ease of notation, fix the OMB record r as well as the document d r it appears in. Let T be the set of table rows t in d r . Let F be defined as above as the set containing the short description, full de-scription, and recipient texts from r . Let C t be the set of table cells in table row t .
 Jaccard Similarity Features : Jaccard similarity between the table row and each field of the OMB record: Maximum similarity between the table row and each field of the OMB record: Maximum Jaccard similarity between a field of an OMB record and each cell in the table row for each field of the OMB record: Maximum Jaccard similarity between all pairs of cells in the table row and fields of the OMB record: Relative Performance Features : For any of the similarity features above, one can compare similarity scores for pairs of table rows and a particular OMB record within a document. A simple way to do this is to take the difference in similarity feature scores between a particu-lar pair and the highest scoring pair. Alternatively, one can find the rank of a pair in the list of all pairs of table rows and a specific OMB record, where the order is determined by a similarity feature score. Here is an example of a difference feature: Because the classification task is really a matching task, a particular OMB record will usually have only one matching table row per document. Providing information about how similar a table row is to an OMB record compared to others provides a way of normalizing similarity scores within the context of a particular OMB record and document.
As mentioned, the hand-labeling procedure was applied to 516 OMB records. It resulted in 769 matching pairs of OMB records and table rows and 647,157 non-matching pairs. There can be more matching pairs than OMB records because in some reports the same earmark can occur in two tables, resulting in two matches for a single record.
Out of the matching pairs, the lowest SIM score observed was 0 . 077. To reduce the number of negative examples, we only include pairs with SIM scores greater than 0.05. This reduced the number of negative instances to 32 , 715. One can think of this threshold as a high-recall, low-precision filter. Correspondingly, when we use the model to match all remaining pairs beyond the ones that were hand labeled, we compute SIM and label the pair negative if the value is less than 0.05. If the score is greater than 0.05, we use the model to label the pair. We present the result of our SVM classifier in Table 5. Varying the weight parameter on the sum of the slack variables in the SVM objective function in the range [0.001, 100] did not change performance.
 AUC 0.9966 (0.08) Precision: non-matching pairs 0.9203, (4.19) Precision: matching pairs 0.9955, (0.26) Recall: non-matching pairs 0.9373, (1.52) Recall: matching pairs 0.9961, (0.12) F-Score: non-matching pairs 0.9961, (0.12) F-Score: matching pairs 0.9280, (2.02) Table 5: Average Precision, Recall and F1 scores computed via 5 fold cross-validation.

To evaluate the quality of our features, we train a model using all the features described above as well as training a model on just the Jaccard similarity features, just the rank-ing features, and just the difference features. Figure 4 shows an ROC curve for each set of features. The ranking features are the best individual feature set, but including the dif-ference features and the Jaccard similarity features gives a small but significant increase in the ROC AUC. The results show our algorithm can perform the matching task almost as well as an expert human annotator.
 We applied the matching algorithm to all OMB records. For every OMB record r and for every document d referenced by r , we computed the features described above over pairs of table rows in d and r and record whether the matching algorithm predicts a match for each pair. Table 6 shows the number of OMB records, the number of OMB records that have at least one matching table row, and the percentage of OMB records that have at least one matching table row by year. The results for 2005 are dramatically worse, which we traced back to errors in linking documents in our corpus to documents that the OMB cites for earmarks in 2005.
Enacted Year OMB Records Distinct % Matched
Table 6: Annual matching performance on OMB data.
Given the labels on table rows induced by matching, we build a classifier that takes as input features computed over the table row and predicts earmark characteristics.
We compute four broad categories of features for the ear-mark classification task, which include geographic features, sponsor features, unigrams, and simple string heuristics. Geo Features : presence of a city, presence of a county, and presence of a state Sponsor Features : presence of a senator X  X  last name and presence of a representative X  X  last name Unigrams : indicator variables for all unigrams in the train-ing data except states, cities, counties, and last names of members of Congress.
 Simple Heuristic Features :
To measure the generalization performance of our earmark classifier over time, we would like to train on documents from one year and test on documents from another. This is complicated by the fact that an earmark can occur in a document from the year it was enacted or the previous year. For example, when looking at the documents that the OMB references for earmarks enacted in 2009, we find that 3786 references are from documents dating from 2008 and 8500 references are dating from 2009. When grouping documents by year, we cannot use documents from 2010 since they could contain earmarks enacted in 2011, for which there is no OMB data. Our labeling policy is that an allocation is labeled as an earmark if and only if it matches an OMB record. Hence, we will mislabel all earmarks enacted in 2011, leading to poor training data and a poor classifier. We can however, use documents from 2008 and 2009.

Table 7 shows the results of training an SVM on the fea-tures described above. We are most interested in measuring how a model trained on one year performs on prior years, since most of the OMB data we need to fill in is from be-fore 2008. We report cross-validated metrics for a model tuned and trained on 2009 documents. We also report the metrics for the 2009 model applied to documents from 2008. As a reference point for the generalization performance, we also include cross-validated metrics for a model tuned and trained on 2008 documents.
 Precision 0.74 (0.017) 0.85 (0.014) 0.72 Recall 0.71 (0.042) 0.87 (0.017) 0.48 F-Score 0.72 (0.016) 0.86 (0.007) 0.58 ROC 0.93 (0.001) 0.97 (0.002) 0.91 Table 7: Metrics for documents grouped by document year.
The data suggest that if a document contains OMB ear-marks, they were all enacted in the same year. Hence, we can assign an enacted year to those documents referenced by OMB earmarks. This allows us to group documents by the inferred enacted year and assign negative examples to the enacted year of the document they are in. This approach leaves out documents that are not referenced by any OMB records. Hence we may lose negative examples from omit-ted documents. The advantage is that we can increase our dataset by using documents enacted in 2008, 2009 and 2010. Table 8 shows results analogous to table 7.
 Precision 0.92(0.012) 0.90 (0.005) 0.87 Recall 0.93 (0.042) 0.93 (0.014) 0.84 F-Score 0.92 (0.016) 0.92 (0.005) 0.85 ROC 0.96 (0.004) 0.94 (0.004) 0.89 Table 8: Metrics for document grouped by enacted year.
Although there appears to be little difference in results between the two grouping approaches as measured by the area under the ROC curve, grouping by enacted year gives a much better F1. In both cases, cross-validation overes-timates the generalization error, suggesting that locations, entities, and sponsors indicative of earmarks vary from year to year.

To evaluate the quality of our features, we train a model using all the features described above as well as training a Figure 5: ROC curves for each feature set, trained on doc-uments enacted in 2010 and 2009 and tested on documents enacted in 2008. model on each set individually. Figure 5 shows an ROC curve for each feature set for a model trained on data en-acted in 2009 and 2010 and tested on data enacted in 2008. Unigrams are the most powerful feature set, followed by the set of simple string heuristics.
After retraining our model on all years, we applied our sys-tem, to documents going back to 1995. For each extracted allocation, we include: Earmark Confidence Score : The score is the signed dis-tance of the candidate earmark from the SVM margin. Pos-itive scores reflect allocations predicted to be earmarks. The magnitude of the score corresponds to confidence in the pre-diction.
 Allocation Location : We used OpenCalais, an off-the-shelf named-entity recognizer (NER), to geotag allocations. We obtained state-level locations for at least 85% of the ear-marks and district-level associations for nearly 45% of the earmarks. Future work will include more sophisticated geo-tagging based on the location of entities mentioned in the text.
 Allocation Topic : The original OMB data includes the spending committee associated with each earmark, such as Agriculture, Commerce, Education, Energy and Water, etc. We trained a spending committee classifier on the OMB data using a Softmax Regression. Before training, we collapsed spending committees related to Homeland Security, Military and Veterans Affairs, and Defense into a single category: De-fense and Military Affairs. The average of the out-of-sample precision and recall scores for each class was approximately 85%. Using this classifier, we assigned spending committee labels to each allocation.

Figure 6 compares our generated dataset (DSSG) with the existing databases of earmarks from Citizens Against Gov-ernment Waste (CAGW) and the Congressional Research Figure 6: Comparison of our earmarks database (DSSG) with CAGW and CRS databases. Missing bars imply that the database doesn X  X  contain earmark counts for that par-ticular year or those years couldn X  X  be retrieved easily. Service (CRS). On average, our dataset includes approxi-mately 3,000 more earmarks than CAGW and approximately 2,100 fewer earmarks than CRS. Our dataset also contains five times more earmarks in 2007 than CAGW. CAGW iden-tified only 2,658 earmarks that year because of a  X  X oint reso-lution that excluded pork from every appropriations bill ex-cept Defense and Homeland Security. X  X 5] Our system, how-ever, identified budget items from the appropriation bills in 2007 that very closely resembled earmarked projects from other years.

The 2009 results look like an outlier, but we randomly ex-amined 100 of those identified earmarks, and 95% of them were correct. We interviewed a K Street lobbyist, and he confirmed that these results are consistent with his impres-sion of earmark behavior over the last decade. He said the big spike in 2009 earmarks is what led Republicans to ban earmarks in the House the following year [1]. Rather than finding too many earmarks in 2009, it may be that we found too few in other years.

From the 1990s through 2005, there is an upward trend in the number of earmarks in all three datasets. Then the use of earmarks appears to have declined except in 2009. The up-and-down trends in earmarks suggests a shift in the nature and processes of earmarking projects over time.
Using our dataset, we can conduct more longitudinal anal-yses of congressional processes. One outstanding question that has huge implications for political scientists and pub-lic policy is whether chairing an appropriations committee impacts the number earmarks granted to the chair X  X  state. Figure 7 shows the results of a difference-in-differences anal-ysis for the nine times a chair of a House or Senate Appro-priations committee changed hands between 1995 and 2010. Before a state gained the chair, it could be expected to have the same number of earmarks as other states that lacked the chair. But after a state gained the chair, it could expect about 15 more earmarks than the states that did not gain the chair X  X  6% bump over the baseline. This relationship holds when leaving the 111th congress (which covered 2009 and 2010) out of the analysis. Figure 7: The number of expected earmarks before and af-ter a state gained a chair of Appropriations (blue) and the number of expected earmarks before and after another state gained a chair of Appropriations.
Our focus in this work has been on extracting earmarks from tables. We built the allocation table parser based on documents from 2005 and 2008-2010. If Congress used dif-ferent formats in other years, the parser may fail to extract all allocations, leading to incomplete data. We plan to make a more in-depth survey of table formats used in congressional texts and generalize our table parser if necessary. Another avenue of future research involves identifying earmarks in free text. We believe the direct citations of free-text ear-marks provided by the OMB make this task tractable. Fi-nally, we hope to augment our earmarks dataset with more fields. In the current release, we provide the state and dis-trict the earmark went to if it is explicitly mentioned in the extracted allocation. We would like to provide more de-tailed geo-coding by locating the entities mentioned in the allocation. We also plan to include the dollar amount of the earmark, which involves determining the units the allocation is in.
It has been difficult to study how earmarking affects the legislative process due to the lack of comprehensive and open data on earmarks. This is mainly due to the immense amount of effort required by humans to sift through the thousands of pages of legal text produced by Congress each year. For the fiscal years 2005 and 2008 X 2010 the Office of Management and Budget (OMB) published a comprehensive dataset of earmarks through a massive inter-agency effort. We developed an automated system that learned how the OMB classifies budget allocations as earmarks. As a result, we were able to extend the scope of the OMB effort to ad-ditional fiscal years.

More generally, we have presented an example of how data mining and machine learning can be used to glean structured data from congressional documents. This structured data can be used to make transparent and quantitatively eval-uate the functioning of legislative processes. Much of the difficulty in our work arose from the lack common table for-mats, document identifiers and document structure. These problems would be solved by Congress adopting machine-readable formats.
 Our system is available at the Data Science for Social Good (DSSG) GitHub repository, and our dataset is avail-able at the DSSG website: http://dssg.uchicago.edu/earmarks/. The dataset is already being used by Harris School of Public Policy researchers to do public policy research and analysis. In addition, it is being promoted by both the Center for Data Science and Public Policy as well as the Sunlight Founda-tion to researchers and practitioners interested in govern-ment transparency and public policy. We thank the Eric &amp; Wendy Schmidt Data Science for Social Good Fellowship for generously supporting this work. We also thank Irina Matveeva and Rob Mitchum for useful discussions. [1] Anonymous Republican Lobbyist. Interview.
 [2] D. A. Austin and M. R. Levit. Mandatory spending [3] A. Bonica. Mapping the ideological marketplace. [4] S. Condon. Ron paul, don young, and joseph cao [5] T. Finnigan. All about pork: The abuse of earmarks [6] C. Hare and K. T. Poole. The polarization of [7] G. King. Replication, replication. PS: Political Science [8] R. T. Meyers. Strategic Budgeting . Ann Arbor: [9] B. Montopoli. House republicans adopt earmarks ban [10] R. Nixon. Lawmakers finance pet projects without [11] D. Pinto, A. McCallum, X. Wei, and W. B. Croft. [12] R. Portman. Memorandum for the heads of [13] P. Pyreddy and W. B. Croft. Tintin: A system for [14] B. Sinclair. Unorthodox Lawmaking: New Legislative [15] S. Streeter. Earmarks and limitations in .
