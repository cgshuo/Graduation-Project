 a vector x so that the ` weighted sum of the feature values approximates the target v alue. minimize a weighted sum of the residual norm and a certain reg ularization term, k x k regularization and k x k parameters often has no fundamental connection to an underl ying noise model [2]. squares formulation is equivalent to ` disturbance matrix [11].
 In particular, our main contributions in this paper are as fo llows. vectors. For a vector z , we let z the i th column and the j th row of the observation matrix A , respectively; a hence it is the j th element of r any of its sub-gradients evaluated at z . 2.1 Formulation the following min-max formulation: for given c following ` inequality. Conversely, one can take the worst-case noise t o be  X   X  by from which the result follows after some algebra.
 If we take c 2.2 Arbitrary norm and correlated disturbance It is possible to generalize this result to the case where the ` to the full version ([15]), and simply state the main results here. Theorem 2. Let k X k is equivalent to the regularized regression problem min set: where f vex and tractable optimization problem.
 Theorem 3. Assume that the set Z , { z  X  R m | f relative interior. The robust regression problem is equivalent to the following regularized regression prob lem Example 1. Suppose U 0 = n (  X  k X k s , then the resulting regularized regression problem is computationally efficient ways to use chance constraints to construct uncertainty sets. (e.g., [16]).
 problems.
 Let I  X  X  1 ,  X  X  X  , m } be such that for all i  X  I , x  X  for any A that satisfies k a Proof. Notice that for i  X  I , x  X  residual. We have For i  X  I , k a Therefore, for any fixed x 0 , the following holds: By definition of x  X  , Therefore we have Since this holds for arbitrary x 0 , we establish the theorem.
 property is satisfied (this result is more in line with the tra ditional sparsity results). Corollary 1. Suppose that for all i , c satisfies x  X  Proof. For j 6 X  I , let a = a Now let Consider the robust regression problem min to min  X  x n b  X   X  A  X  x that  X  x  X  Hence for any given  X  x , by changing  X  x increase.
 Since k  X  a  X   X  a Theorem 4 we establish the corollary.
 The next corollary follows easily from Corollary 1.
 Corollary 2. Suppose there exists I  X  { 1 ,  X  X  X  , m } , such that for all i  X  I , k a optimal solution x  X  satisfies x  X  dent. Again we refer to [15] for the proof.
 Theorem 5. Given I  X  X  1 ,  X  X  X  , m } such that there exists a non-zero vector ( w then there exists an optimal solution x  X  such that  X  i  X  I : x  X  Notice that for linearly dependent features, there exists n on-zero ( w 0 , which leads to the following corollary.
 Corollary 3. Given I  X  { 1 ,  X  X  X  , m } , let A optimal solution x  X  such that x  X  Setting I = { 1 ,  X  X  X  , m } , we immediately get the following corollary. cients. to a fundamental tenet of decision-theory.
 Throughout, we use c bounded support that generates i.i.d. samples ( b first n samples by S x ( c n , S n ) , arg min In words, x ( c prove their consistency.
 Theorem 6. Let { c constant H such that k x ( c almost surely. kernel density estimation.
 worst-case expected generalization error . To show this we establish a more general result: Proposition 1. Given a function g : R m +1  X  R and Borel sets Z The following holds samples ( b and hence belongs to  X  P ( n ) =  X  P ( n ) , S of distributions used in the representation from Propositi on 1. Step 3 : Combining the last two steps, and using the fact that R zero almost surely when c (see e.g. Theorem 3.1 of [21]), we prove consistency of robus t regression. We can remove the assumption that k x ( c rather than the result itself is of interest. We postpone the proof to [15]. Theorem 7. Let { c almost surely. convex optimization problems.
 obtain more generally from robustified algorithms.
