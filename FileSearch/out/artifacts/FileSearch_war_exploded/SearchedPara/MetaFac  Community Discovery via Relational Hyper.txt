 This paper aims at discover ing community structure in rich media social networks, through analysis of tim e-varying , multi -relational data. Community structure represents the latent social context of user actions . It has important applications in information tasks such as search and recommendation. Social media has several unique challenges. (a) In social medi a, the context of user actions is constantly changing and co -evolving; hence the social context contains time -evolving multi -dimensional relations . (b) The social context is determined by the available system features and is unique in each social media web site. In this paper we propose MetaFac (MetaGraph Factorization), a framework that extracts community structures from various social contexts and interactions. Our work has three key contributions: (1) metagraph, a novel relational hypergraph representatio n for modeling multi -relational and multi -dimensional social data; (2) an efficient factorization method for community extraction on a given metagraph; (3) an on -line method to handle time -varying relations through incremental metagraph factorization. Exte nsive experiments on real -world social data collected from the Digg social media website suggest that our technique is scalable and is able to extract meaningful communities based on the social media context s. We illustrate the usefulness of our framework thr ough prediction task s. We outperform baseline methods ( including aspect model and tensor analysis ) by an order of magnitude . Categories and Subject Descriptors H.2.8 [ Database Management ]: Database Applications  X  Data mining ; H.3.3 [ Information Storage a nd Retrieval ]: Information Search and Retrieval  X  Information filtering ; I.5 .3 [ Pattern Recognition ]: Clustering; J.4 [ Computer Applications ]: Social and Behavioral Sciences  X  Economics General Terms Algorithms, Experime ntation, Measurement , Theory Keywords Me taFac, metagraph factorization, relational hypergraph, n on -negativ e tensor factorization, community discovery, dynamic social network analysis This paper aims at discover ing community structure in rich media social networks, through analysis of the time -varying multi -relational data from social media websites . Social media websites such as Flickr, Digg and Facebook allow a wide array of actions on media objects  X  e.g. uploading photos, su bmitting and commenting on news stories, bookmarking and tagging, posting documents, creating web -links, as well as actions with respect to other users (e.g. sharing media and links with a friend ). The key to social media information tasks such as media recommendation relies in understanding the context of thes e actions  X  how they relate to other actions, users and media objects . For example, a user might be motivated to search a story after viewing her friend X  X  bookmarks . The problem has two challenges : (1) i n social media, the context of user actions is cons tantly changing and co -evolving, e.g. with respect to other users X  actions, emergent concepts and users X  historic preferences. Hence the social context contains time -evolv ing multi -dimensional relations; (2) t he social context is determined by the availabl e system features that allow interaction s on media objects and among people. Hence the social context is unique in each social media website. For example, Figure 1 shows the main actions available in Digg and Flick r, as well as related media objects. In Digg, users might submit / vote (digg ) / comment on a story, reply to a comment, reply to a reply, etc. Fli ckr users might post, tag and comment on a photo, make friend 
Figure 1 : The social context of user actions vary across social media websites  X  we propose a metagraph representation to model various social context; (a) primary actions and related media objects in Diggs; (b) primary actions and related objects in Flickr; (c) a metagraph representation for Digg; (c) a metagraph for Flick r . (c) (d) (a) (b) contacts , label a photo as a favorite, join a p hoto sharing group (pool), etc. There are some common actions, but mo re site -specific actions cater to the purpose of each site. The discovery of social context needs to deal with the diverse and dynamic nature of actions in social media.
 In this paper we propose MetaGraph Factorization (MetaFac) , a framework that extracts community structures (i.e. the latent social context) from various social interactions. Our work has three key contributions: (1) metagraph , a novel relational hypergraph representation for modeling multi -relational and multi -dimensional social data; (2) an efficient factorization method for community extraction on a given metagraph; (3) an on -line method to handle time -varying relations through incremental metagraph factorization.
 Extens ive experiments on real -world social media data suggest that our technique is scalable and is able to extract mean ingful communities based on social media context. We illustrate the usefulness of our framework thr ough prediction task s  X  to predict users X  f uture interests on voting or commenting on Digg stories . Our prediction significantly outperforms baseline methods (frequency counts, tensor analysis, etc. ), suggesting the utility of leveraging metagraph s to handle time -varying soc ial relational contexts.
 The rest of the paper is organized as follows. Section 2 reviews the related work. Section 3 introduces preliminaries and section 4 formalizes the problem. Section 5 and 6 presents our community extraction method on both static and dynamic multi -relational data. Section 7 presents experiments and section 8 concludes . Community discovery in rich media social networks deals with a constantly changing  X  mishmash  X  of interrelated users and media objects . The problem has three aspects : (1) evolutionary characterization of communities in time -varying social networks , (2) analysis of multi -dimensional data , and (3) relational learning adaptable to different social contexts. To the best of our knowledge, our work is t he first unified attempt to address all three aspects within a single problem . Evolutiona ry community characterization . Social interactions among people have been studied through a unipartite or bipartite graph, in which the community structure can be characterized by clustering method s [14] , and the evolution of community structure is captured in terms of various criteria. Kumar et al. [9] study the evolution of the blogosphere in terms of the change of graph statistics and the burstiness of extracted communities . Sun et al. [14] use the Minimum Description Length principle to extract communities and to detect their changes. Lin et al. [11] use an evolutionary clustering criterion [4] to extract community structures based on both observed networked da ta and historic community structure. All t hese works restrict themselves to pair -wise relations between entities (e.g. user -user or user -paper). In rich online social media, networked data consists of multiple co-evolving dimensions, e.g. users , tags , feed s, comments, etc. Collapsing such multi -way networks into pairwise networks results in the loss of valuable information, and the analysis of temporal correlation among multi -dimensions is difficult . Multi -dimensional mining. In multi -dimensional n etwork analysis , network s have more than two types of entities. Existing techniques include tensor based analysis [5,14] or multi -graph mining . Tensor factorization is a generalized approach for analyzing multi -way interactions among entities. Note that a tensor represents complete interactions among all involved entities, which is a very strong assumption in social media since there might be events involving some but not all dimensions . Multi -graph mining considers joint factorization over two or more matrices. The combination of such matrices is domain -specific , e.g. i n text mining, Zhu et al. [15] propose a joint matrix factorization combining both linkage and document -term matrices to improve the h ypertext classification . In social media, relations depend on the system features, which might vary across websites. Moreover, the s ystem features may change over time in a social media website , which requires flexible relational learning. Relational learning. Relational techniques such as PRMs [6] extend generative models to deal with various combinations of probabilistic dependency among entities. Such techniques can be computationally expensive, and may not scale to the large amount of data typically collected by socia l media websites. There have been relational learning techniques through pairwise relationships among entities, e.g. [3,12] , which involve loss of information when data has higher -order interactions. Our work shares the same advantages as Kemp el at. [8] and Banerjee et al. [2] , which deal with multiple tensors, but their static settings are different from our problem . In sum, social media analysis requires a flexible and scalable framework that exploits relational context defined by the system features of individual social media sites. Such relational context is multi -dimensional, sparse (not all dimensions are involved in an event), specific, and evolving over time. We propose a unified approach to analyze the dynamics of rich media social networks. This section provides notations and minimal background on tensors and some basic operations used in this work . We refer readers to [1] for a more compr ehensive review on tensors . A tensor is a mathematical representation of a multi -way array. The order of a tensor is the number of modes (or ways ). A first -order tensor is a vector, a second -order tensor is a matrix, and a higher -order tensor has t hree or more modes. We use x as a vector, X as a matrix, and  X  as a tensor. The dimensionality of a mode is the number of elements in that mode. We use I q to denote the dimensionality of mode q . E.g. , the tensor 1 2 3 modes with dimensionalities of I 1 , I 2 and I 3 , respectively.  X  indicates all elements of the tensor  X  have nonnegative values, which is usually the case for a data tensor. The ( i 1 , i a third -order tensor is denoted by from 1 to their capital version, e.g. i 1 =1,..., I 1 . Mode -d matricization or unfolding : Matricization is the process of reordering the elements of an M -way array into a matrix. The mode -d matricization of a tensor 1  X  X  X   X  X  X   X  M II  X  is denoted by X mode d results in a matrix with height I d and its width is the product of dimensionalities of all other modes. The inverse operation is denoted as 1 () ()  X  X  X   X   X  X  X   X  M II In general the unfolding operation can be defined on multiple modes. For example, we can define mode -( c , d ) unfolding as unfold c d  X  X  X  . Unfolding a tensor on two modes c and d results in a cube (three -way tensor). Similarly, we can define a vectorization operation x = vec (  X  ), which linearizes the tensor into a vector. Mode -d product : The mode -d matrix product of a tensor results in a tenso r of size I 1  X  ...  X  I d -1  X  J  X  I we have Mode -d accumulation : A mode -d accumulation or summation is defined as () ( , )  X   X  X  X  d I d acc d X1  X  . The operation sums up all entries across all modes except for mode d , whi ch results in a vector of length I d . Accumulating a tensor on mode d can be obtained by unfolding the tensor on mode d into a matrix and then multiplying the ma trix with an all -one vector. Like unfolding operation, accumulation can be defined on multiple m odes, e.g. a mode -( c , d ) accumulation ( , ) 3 ( , ( , )) will result in a matrix of size I c  X  I d . Tensor decomposition or factorization is a form of higher -order principal component analysis. It decomposes a tensor into a core tensor multiplied by a matrix along each mode. Thus, in the three -way case where  X   X  X  X  I  X  J  X  K , we have  X   X   X   X  1 A  X  means each element of the tensor  X  is the product of the corresponding matrix elements multiplied by weight z pqr x z a b c . Here, A  X  X  X  I  X  P , B  X  X  X  J  X  Q and C  X  X  X  K  X  R are called factor matrices or factors and can be thought of as the principal component s of the original tensor along each mode. The tensor  X   X  X  X  P  X  Q  X  R is called the core tensor and its elements show the level of interaction between d ifferent components. A special case of tensor decomposition is referred as CP or PARAFAC decomposition [1] , where the core tensor is superdiagonal and P = Q = R . (A tensor 1  X  X  X   X  X  X   X  M II  X  is diagonal if x only if i 1 =...= i M .) The CP decomposition of a third -order tensor is then simplified as illustrated in Figure 2 . We use [ z ] to denote a superdiagonal tenso r, where the operation [ X ] transforms a vector z to a superdiagonal tensor by setting tensor element z k ... k elements as 0. Thus the CP decomposition of a three -way tensor corresponding superd iagonal core tensor . This section defines the problem of discovering latent community structure that represents the context of user actions in social networks. The problem has three parts : (1) how to represent mul ti-relational social da ta (section 4.1 ), (2) h ow to reveal the latent communities consi stent ly across multiple relations, and (3) how to track the communities over time (section 4.2 ). We introdu ce metagraph , a relational hypergraph for representing multi -relational and multi -dimensional social data. We use a metagraph to configure the relational context specific to the system features  X  this is the key to mak ing our community analysis adaptable t o various social media contexts, e.g. Digg and Flickr ( Figure 1 ). We shall use the Digg example to illustrate three concepts: facet , relation , and relational hypergraph . As shown in Figure 1 (a), Digg allows various actions for news sharing  X  users might submit (indicated by the line labeled  X  X  X ) a news story associated with a particular topic . They might vote (or digg, line  X  X  X ) or comment (li ne  X  X  X ) on the submitted story , reply (line  X  X  X ) to a comment created by other users , or even reply to a reply (not shown in the figure) , etc. To describ e the context of actions , we call a set of objects or entities of the same type a facet , e.g. a user facet is a set of users, a story facet is a set of stories, etc. We call the interactions among facets a relation ; a relation can involve two (i.e. binary relation) or more facets, e.g. the  X  X igg X  relation involves two facets (user , story ), and the  X  X ake -comment X  is a 3 -way relation ( user , story , comment ). A facet can be implicit, depending on whether the facet entities interact with other facets, e.g. the set of digg objects might be omitted due to no interactions with other facets.
 Formally, we denote the q -th facet as v ( q ) and the set o f all facets as V . A set of instantiations of a n M -way relation e on facets v v ,..., v ( M ) is a subset of the Cartesian product v The observations of an M -way relation e ( r ) is represented as an M -way data tensor  X  ( r ) . Now we introduce a multi -relational hypergraph (denoted as metagraph in this paper) to describe the combination of relations and facets in a social media context. A hypergraph is a graph where edges, called hyperedges , connect to any number of vertices. The idea is to use a n M -way hyperedge to represent the interactions of M facets : each facet as a vertex and each relation as a hyperedge on a hypergraph. A metagraph defines a particular structure of interacti ons among facets, not among facet elements. Formally, for a set of facets V ={ v ( q ) } and a set of relations E ={ e ( r ) }, we construct a metagraph G =( V , E ). To reduce notation al complexity, V and E also represent the set of all vertex and edge indices respectivel y. A hyperedge/relation e ( r ) incident to a facet/vertex v ( q ) if v ( q )  X  e ( r ) v ~ e ( r ) or e ( r ) ~ v ( q ) . E.g ., in Figure 1 (c) v We summarize our notations in Table 1 . We formalize the commun ity discovery problem as latent space extraction from multi -relational social data represented by a metagraph . Our goal is to discover latent community structure s that represent the context of user actions in social media networks. We are interested in clu sters of people who interact with each other in a coherent manner. Some of the interaction can be implicit , e.g. two users may comment on the same stories , and the i nteractions can be further enhanced by other interactions. Hence we consider a community as a latent space of consistent interactions or relations among users and objects . By assuming consistent interactions in a community, the int eraction between any two entities (users or media objects) i and j in a community k , written as x ij , can be viewed as a function of the relationship s between community k with entity i , and k with j . If we consider the function to be stochastic, i.e. let p how likely an interaction in the k -th community involves the i -th entity and p k is the probability of an interaction in the k -th community, we can express x ij by x ij  X  X  X  k p k  X  i 3-way interaction among entity i 1 , i 2 and i 3 is x p p p p . A set of such interactions among entities in facet v (1) , v (2) and v (3) can be written by : where 1 2 3  X  X  X   X   X  X  X  I I I  X  is the data tensor representing the observed three -way interactions among f acet v (1) , v (2) and v written as an ( i q , k )-element of U ( q ) for q =1,2,3 . U communities are elements of z , i.e. p k = z k . This is similar to the CP decomp osition of a tensor (section 3.2 ), except that the core tensor [ z ] and the factor matrices { U ( q ) } are constrained to contain nonnegative probability values . Under the nonnegative constraints, the 3-way tensor factorization is e quivalent to the three -way aspect model in a three -dimensional co -occurrence data [13] . The nonnegative tensor decomposition can be viewed as community discovery in a single relation. The interactions in social media networks ar e more com plex  X  usually involving multiple two -or multi -way relations. By using metagraph s, we represent a di verse set of rela tional context s in the same form and define the community discovery problem on a metagraph, with the following two technical issues.
 The first issue is how to extract community structure as coherent interaction latent space s from observed social data defined on a metagraph, which is formally stated as follows.
 Problem (Metagraph Factorization, or MF): given a metagraph G =( V , E ) and a set of observed data tensors {  X  ( r ) find a nonnegative core tensor [ z ] and factors { U corresponding facets V ={ v ( q ) }. (Since E also represents the set of all edge indices, the notations r  X  E and e ( r )  X  Likewise, q  X  V and v ( q )  X  V are exchangeable.) The second issue concerns the dynamic nature of human activities  X  those interactions might be consistent during a short time period but are unlikely to be consistent all the time. The problem , how to extract community str ucture as coherent interaction latent spaces from time evolving data given a metagraph, is defined as follows . Problem (Metagraph Factorization for Time evolving data, or MFT): given a metagraph G =( V , E ) and a sequential set of find a nonnegative core tensor [ z t ] and factors { U corresponding to facets V ={ v ( q ) } for each time t . We will present our method in two steps: (1) present a solution to MF (section 5 ); (2) ext end the solution to solve MFT (section 6 ). This section presents our solution to the metagraph factorization problem (MF) . Our method relies on formulating MF as a n optimization problem (section 5.1 ). We then provide an algorithm to solve the optimization objective (section 5.2 ) and discuss its computational complexity (section 5.3 ). The MF proble m can be stated in terms of optimization. Let us first consider a simple metagraph case. Assume we are given a metagraph G =( V , E ) with three vertices V ={ v (1) , v among these three facets , as shown in Figure 3 . The observed data correspond ing to the hyperedges are two second -order data tensors (i.e. matrices) {  X  ( a ) ,  X  ( b ) } with facets { v v } respectively . The facet v (2) is shared by both tensors . The goal is to extract community structure from data tensors, through finding a nonnegative core tensor [ z ] and factors { U U (2) , U (3) } corresponding to the three facets. T he core tensor and factors need to consistently exp lain the data, i.e. we can approximately express the data by  X  ( a )  X  [ z ]  X   X  U (2) are s hared by th e two approximations, and the length of z is determined by the number of latent spaces (communities) to be extracted. Since both the left -and the right -hand side of the approximation are probability distributions, it is natural to use the KL -divergence (denoted as D (  X  ||  X  )) as a measure of approximation cost. To simul taneously reduce two approximation costs we can define a cost function as: ( || [ ] ) ( || [ ] )  X   X   X   X   X  ab DD z U U z U U  X  X  X  , &lt;2&gt; where D (  X  ||  X  )=  X  i ( a i log a i / b i  X  a i + b between tensor  X  and  X  and a = vec (  X  ), b = vec (  X  ). The solution to eq. &lt;2&gt; will be an MF solution for the metagraph each D (  X  ||  X  ) correspond to a hyperedge, each tensor product correspond s to how facets are incident to an hyperedge and the summation correspond s to all hyperedges on the graph. We can generalize eq. &lt;2&gt; to any metagraph G , as follows . Figure 3 : An example of the metagraph factorization (MF). 
Given observed data tensors {  X  ( a ) ,  X  ( b ) } and a metagraph G that describe s the interaction among facets { v (1) find a consistent community structure expressed by core tensor [ z ] and facet factors { U (1) , U (2) , U (3) Given a metagraph G =( V , E ), the objective is to factorize all data tensors such that all tensors can be approximated by a common nonnegative core tensor [ z ] and a shared set of nonnegative factors { U ( q ) }, i.e. to minimize th e following cost function: ( ) min ( || [ ] ) . , , 1 
J G D s t q q k where K is the number of communities, and D (  X  ||  X  ) is the KL -divergence as described above . The constraint that each column of { U ( q ) } must sum to one is added due to the modeling assumption that the probability of an occurrence of a relation on an entity is independent of other entities in a community . Eq. &lt;3&gt; can be easily extended to incorporate weights on relations. We provide a solution to the objective function defined in eq. &lt;3&gt;. not convex in all variables. By employing the concavity of the log function (in the KL -divergence), we derive a local minima solution to eq. &lt;3&gt;. The solution can be found by the following updating algorithm: z  X  , &lt;4&gt; 
U  X  , &lt;5&gt; where z is a length K vector, L =| E | denotes the total number of hyperedges incident to v ( q ) , and  X   X  After updates, each column of U ( q ) are normalized to sum to one. Because of this normalization step, we can omit dividing by L algorithm proposed by Lee et al. [10] for solving the single nonnegative matrix factorization problem. In metagraph factorization, the update for core tensor [ z ] depends on all hyperedges on the m etagraph, and the update for each facet factor U ( q ) depends on the hyperedges incident to the facet. T he proof for the convergence of our algorithm is omitted due to space limit. the high dimensionalities of tensors . We now discuss an efficient implementation of the update rules. In e q.&lt;4&gt;  X  &lt;6&gt;, element of a 1  X   X   X   X  tensor, where  X  denotes the dimensionalities short. Because  X  ( r ) is expensive to compute and operate, we want to reduce computation that involves  X  ( r ) . By observing the shared part for updating t he core tensor and all facet factors in efficient computation: Instead of computing  X  ( r ) explicitly, we compute an intermediate tensor  X  ( r ) of the same dimensionalities as  X  ( r ) .  X  ( r ) will save the repeating part of multiplication of  X  can be rewritten as follows. First, for each e ( r )  X  where  X  denotes the element -wise division, and  X  denotes the Khatri -Rao product . The Khatri -Rao product of two matrices A and B , denoted by A  X  B , is defined by  X   X   X   X   X   X 
A B a b a b a b , where a k and b k are the k th column vectors of A and B respectively, and where a  X  b is the Kronecker product of a and b . The second step is to update z and { U ( q ) } by: z  X  &lt;9&gt; where M r +1 is the last mode of  X  ( r ) . The multiplication of  X  &lt;8&gt; by utilizing the Khatri -Rao product. To obtain z and { U we only need to accumulate  X  ( r ) on the corresponding mod es. { U ( q ) } obtained from eq. &lt;10 &gt; will be equivalent to those from results as eq. &lt;4&gt;  X  &lt;6&gt;. The algorithm shares the same form of the expectation -maximization algorithm, where eq. &lt;7&gt; and &lt;8&gt; correspond to the E -step and eq. &lt;9&gt; and &lt;10 &gt; correspond to the M-step. Note that t he information contained in each data tensor with respect to a hyperedge is aggregated through the E -step and is shared by the core tensor and all facet factors in the M -step, thus the extracted communit ies will be coherent latent spaces. Table 2 summarizes t he whole process to solve an MF problem . We refer the solution core tensor and facet matrices as a community model , from which we infer the probabilistic (soft) membership of entities in each facet. As described in section 4.2 , each ( i , k )-element of a facet matrix U is p ( i | k ) (i.e. p an interaction in the community k involves entity i ), and each ele ment z k = p ( k ) (i.e. p k , the probability of an interaction in community k ). Thus we compute the conditional probability p ( k | i ) to indicate the soft membership of entity i with respect to community k by p ( i | k ) p ( k )/ p ( i ), where p ( i )=  X  proba bility of an interaction involving entity i . We now discuss the time complexity for the updates. The most time -consuming step in the algorithm is to compute  X  of the sparseness of the data tensor  X  ( r ) and compute only the non -zero elements (total number of tuples) in  X  ( r ) the largest number of non -zero elements of the involved data tensors. This step has time complexity O( nKML ), where K is the number of clusters, M is the maximal number of incident facets of a relation, and L is the total number of input relations. Usually, K , M , L are much smaller than n . If we consider K , M and L are bounded by some constants, the time complexity per iteration is linear in O( n ), the number of non -zero elements in all data tensors. This section presents our solution to the problem of metagraph factorizati on with time evolving data (MFT). In the MFT problem, the relational data is constantly changing as evolving tensor sequences. We propose an online version of MF to handle dynamic data. Since historic information is contained in the community model extracted based on previously observed data, the new community structure to be extracted should be consistent with previous communit y model and new observations , which is similar to the evolutionary clustering discussed in [11] . To achieve this, w e extend the objective in eq. &lt;3&gt; as follows . A community model for a particular time t is defined uniquely by the factors { U t ( q ) } and core tensor [ z t ]. (To avoid notation clutter, we omit the time indices for t .) For each time t , the objective is to factorize the observed data into the nonnegative factors { U core tensor [ z ] which are close to the prior community model , [ z and { U t -1 ( q ) }. We introd uce a cost l prior community structure deviates from the previous structure in terms of the KL -divergence . The new objective is defined as follows: ( ) min (1 ) ( || [ ] ) . , , 1 
J G D l l D D s t q q k where  X  is a real positive number between 0 and 1 to specify how much the prior community model contribute s to the new community structure . l prior is a regularizer used to find similar pair s of core tensors and pairs of facet factors for consecutive time s. The new community structure will be a solution incrementally update d base d on a prior community model . Based on a derivation similar to the discussion in section 5 , we provide a solution to eq. &lt;11 &gt; as follows : zz  X  , &lt;12 &gt; 
UU  X  , &lt;13 &gt; where column of U ( q ) and the vector z are normalized to sum to one. Because of this normalization step, we have dropped the scali ng constant for updating z and U ( q ) . It can be shown that the parameters in the previous model ( z { U t -1 ( q ) }) act as Dirichlet prior distribution to inform the solution search (ref. [11] ), thus the solution is consistent with previous comm unity structure. The update rules can be rewritten as the  X   X   X   X   X  r where M r +1 is the last mode of  X  ( r ) . The whole process of finding solutions to the MFT problem is summarized in Table 3 . For time evolving social data, changes mig ht happen in interactions among entities, or even in interactions among facets (e.g. due to the evolution of system features ) which lead to change s in metagraph. One advantage of our MFT algorithm is it only requires new observed data defined on any given metagraph, so it is straightforward to incorporate th e changes of a metagraph . This section reports our experimental study on a real -world social media dataset collected from Digg . We first describe the data set (section 7.1 ) and present the extracted communities (section 7.2 ). We evaluate our technique through prediction tasks (section 7.3 ). Finally , w e evaluate the scalability of our factorization method on synthetic datasets (section 7.4 ). We have collected data from a large set of user actions from Digg. Digg is a popular social news aggregator that allows users to submit , vote (i.e. digg) and comment on news stori es. It also allows users to create social networks by designating other users as friends and track ing friends X  activities. The dataset used in our experiments include stories , users and their actions (submit, digg, comment and reply ) with respect to the st ories, as well as the explicit friendship (contact) relation among these users. To analyze users X  topical interests, w e also retrieve the topics of the stories and extract keywords from the stories X  titles.
 From this dataset, we select 5 facets ( user, story, comment, keyword and top ic) and build 6 relations among them. The relations are summarized in Table 4 , which correspond to the metagraph shown in Figure 1 (c) . Except for the contact relation, all relations have timestamps . We assume the contact relation is static and consider the other relation s as dy namic. For dynamic relations, we extract tuples with timestamps ranging from August 1 to August 27, 2008. To study the data evolution, we segment the duration into 9 time slots ( i.e. every three days), and construct a sequence of data tensors for each dyna mic relation. In the following we shall use t  X  [1,9] to denote a time slot index. The total number of tuples in each tensor sequence per relation is listed in Table 4 . Our dataset and code are available online (http://www.public.as u.edu/~ylin56/ kdd09sup.html ). We present a qualitative analysis of the communities extracted by our method , which demonstrates an advantage of probabilistic interpretation given by our method. We first show all communities extracted for a particular time and then examine the community evolution within two of these communities.
 To illustrate what kinds of stories are  X  X ugg X  by what kind of communities, we track the l atent communities based on the digging activities which involve relation R1 and R4 . Figur e 6 (a) and (e) shows the corresponding metagraph and the number of tuples in the two relations. In our factorization algorithm, we assume that the number of communities, K , is given beforehand. Here we show communi ties extracted given K =2, 4 and 12.
 Based on relation R1 and R4, four facets are involved: user, story, keyword and topic. We present the keyword and topic facets because they are more informative to the readers than other facets . Figure 4 shows the most likely keywords and topics in each community . We present the results of t =3 (August 6 -9, 2008). We project those keyword and topic (shown within brackets) terms onto a 2D plane . The location of the i -th keyword or topic term indi cates its relative proximity to other terms and is computed based on its soft membership p ( k | i ). (The position is determined by standard multidimensional scaling with the soft membership as input. ) The size of the i -th term indicates how likely the term appears in a story and is determined based on the probability p ( i ). Each term is colored based on its most likely community, i.e. by choosing k with maximal p ( k | i ). In the figure we can see the communities based on users X  digging activities have coherent top ical preference, as the terms with the same colors are located closely. The 2 -, 4 -and 12 -community result s show the communities at different resolution. The 2 -community result distinguishes political interests from the Olympics news ( Figure 4 (a)) . The 4 -community shows four topical interests in communities : C1: gaming industry news, C2: US election news, C3 : world news , and C4: general political news ( Figure 4 (b)) . The two major topics ( X  X lympics X  and  X  X e orgia X ) in C3 are further split in the 12 -community result ( Figure 4 (c)) . Community Evolution. We select the 4 -community result and examine its evolution. Figure 5 (a) shows the probabiliti es of the four communities over time, and Figure 5 (b) shows the keyword dissimilarity across time where the dissimilarity is computed based on the cosine similarity of keyword distribution in each community of cons ecutive timestamps. (We use a cosine similarity measure in order to emphasize the differences at the  X  X ead X  of the distributions .) We observe two critical times in Figure 5 (b) : for community C2 and C3 , the keywords distribution change drastically at t =3 (August 6 -9) and t =8 (August 21 -24). To examine the events occurring during these times, we look at the keyword distributions of the two communities. Table 5 list s the top 10 keywords that are mostly likely to appear in C 3 and C 2, at t =2,3 and t =7,8 respectively . At t =3, the new popped keywords  X  X lympics X  and  X  X eorgia X  reflect users X  attention to two significant world news items : the 2008 Summer Olympics began on August 8 and the 2008 Russia -Georgia conflict started on August 7 . At t =8, the new pop ped keywords  X  X oe X ,  X  X iden X ,  X  X p X  correspond to the time whe n president ial candidate Barack Obama announced that Joe Biden would be his running mate (on August 22). Another critical time is captured by the change in community size. In Figure 5 (a), we see the community C3 keeps growing until t =6, when the Russia -Georgia conflict ended with a ceasefire agreement signed on August 15 and 16.
 Table 5: The keyword distribution of community C2 and C3 during two critical times, t =3 and t =8 . C4 (a) community popularity (b) concept evolution
Figure 5 : The community evolution characterized based on (a) change in size of communities, and (b) change in keyword distribution in each community. The characterization of community evolution based on change in the probability of a cluster and on change in the distribution of entities such as keywords ( Figure 5 and Table 5 ) demonstrates the advantage of our soft clustering method. The presented case study suggests that our method is able to generate meaningful mining results from dynami c multi -relational social networks. We use a prediction task to demonstrat e the utility of our techniques . Based on the Digg scenario, we design t wo prediction tasks  X  to predict users X  future interests on digging (i.e. voting) and commenting on Digg stories. We study three aspects of our method through the prediction task s: (1) How does our community discovery framework help predict users X  future interests? (2) How much historic information do we need? (3) Which relation is releva nt to the prediction ? Prediction setting . There are two tasks: (a) digg prediction  X  what stories a user will digg, and (b) comment prediction  X  what stories a user will comment on. Both tasks are evaluated on data from each time slot. We use stories that have digging or commenting events in time slot t s  X  [2,9] as testing set s and the available relational data (ref. Table 4 ) in time slot t set s. The prediction results are compared with the actual diggs and comments oc curring in slot t s . This is a constrained setting because there might be more digging or commenting activities occu rring after t s . In our prediction experiments we only consider diggs and comments in each single slot t s as ground truth. Evaluation metrics . We use two metrics adopted in Information Retrieval : (1)  X  P@10  X  (the precision of the top 10 results): For each user we compute the precision based on the top 10 stories retrieved for the user. The overall P@10 for the set of users is computed by taking the mean of P@10 per user, per time slot . (2)  X  NDCG  X  (Normalized Discount Cumulative Gain [7] ): One advantage of the measure i s its sensitivity to the prediction order. correct and 0 otherwise.
 Our p rediction method . We generate predictions base d on the community structure extracted by our method, denoted by MF and MFT. The MF algorithm output s community st ructure from relational data of each time slot t s -1. The MFT algorithm use s the same data as MF, with the aid of a community model extracted for time t s -2 as an informative prior. Hence MFT gives results incrementally. From an extracted community model we obtain the probability of a community k , p ( k ), and the probability of a user u , and p ( j | k ). To predict if a user u will digg or comment on a story r , we first use a folding -in technique ( ref . e.g. [13] ) to compute p ( r | k ), the probability of a story given each community k , based on the story topic and keywords. T hen a prediction is made based on the condition probability p ( r | u )  X  p ( u , r )  X  X  X  Baseline methods. Three baseline methods are used : (1) Frequency based heuristics (FREQ)  X  predicting stories based on the frequencies of story topic and keywords at t s tensor analysis (PARAFAC )  X  predicting stories by using the CP /PARAFAC tensor decomposi tion [1] for data in slot t stories to be predicted are first projected on the latent spaces , and the prediction is made based on the dot product of the user and story projected vectors . (3) Multi -way aspect model (MWA)  X  predicting stories by using the multi -way aspect model [13] , a special case of our model (ref. section 4.2 ). The ability to handle relational context s is the key to our comparison. We choose specific relations t o illustrate the utility of leveraging a specific context by a metagraph  X  relation R1 and R4 for dig g prediction and R1 and R5 for comment prediction (ref. Figur e 6 (a) and (b) ), and we shal l evaluate the effect of other relations later in this section. Since PARAFAC and MWA o nly deal with a single high dimensional relation, we construct two 4 -way tensors per time that contains digg actions and comment actions with respect to stories. The two tensors, denoted by TD and TC are shown in Figur e 6 (c) and (d). Figur e 6 (e) shows the number of non -zero entries (tuples) of these data tensors over time. The number of tuples in an R5 tensor corresponds to the number of stories per time.
 Except for the FREQ method, all methods are tested with number of clusters or latent spaces K =4... 20. For MFT, we use  X  =0.2. Table 6 : The average prediction performance for digg and comment prediction, evaluated by P@10 and NDCG metrics . Results and Discussi on. The overall prediction performance is obtained by taking the average of prediction performance on data over different K values . The results (mean and standard deviation ) are given in Table 6 . There are several ob servations. First, our method significantly outperform s all baseline method s. In digg prediction, our MF method outperform s the baseline s by 43% to 5X on the average. In comment predic tion, the MF method outp erforms the baseline s by 73% to 10X . Second, the MFT performs the best . It slightly out performs MF in digg prediction and improves MF by 15% in comment prediction . Next we show how our prediction can be further improved by (a) incorporating a historic mode l and (b) leveraging other relations through a metagraph.
 Effect of historic information. We vary the weight of the prior model in MFT and report the average P@10 over  X  values ( Figure 7 (a) ). The results suggest that incorporating historic information as prior knowledge work s better than no prior (  X  =0). Note f or comment prediction, the performance increase s until  X   X  0.8. This suggests that the comment activities are more consistent with the historic community structure than the digg activities . 
Figur e 6 : Relations used by different methods for digg and comment prediction: (a) R1 and R4 used in our method for digg prediction; (b) R1 and R5 used in our method for comment prediction; (c) TD tensors used in PARAFAC and 
MWA for dig g prediction; (d) TC tensors used in PARAFAC and MWA for comment prediction; (e) number of tuples in each relation over time. Effect of various relational context. For comment prediction, w e evaluate the prediction performance ove r different relational contexts. Figure 7 (b) shows the average prediction result s. The label R* indicates which r elations are used in the training set, e.g. R125 denotes relation R1, R2 and R5. We observe that different combination s of the relations affect the prediction performance. For example, incorporating the contact relation R2 with R1 and R5 significantly help s predict users X  comment activities . We use synthetic datasets to illustrate the scalability of our algorithms . We study how the computational time of our algorithm increase s with four variables, including different types of data gr owth  X  (a) non -zero elements in a data tensor , (b) number of tensor modes (dimensions) , (c) number of relations (tensors) on a given metagraph, as well as (d) the algorithm parameter, i.e. number of clusters. In the simulation w e randomly generate tensors by varying one of the above variables (e.g. the number of non -zero elements ) and fixing all remaining variables . Figure 8 shows the simulation results, indicating that the running time per ite ration scales linearly with the data size , the number of tensor modes, the total number of relations and the number of clusters. Note that the slope for increasing tensor modes is steeper than increasing relations. Empirically the non -zero elements in a higher mode tensor are u sually much more than lower mode tensors (as in Figur e 6 (e)). Therefore by leveraging a metagraph we can efficiently combine multiple low -dimensional relations instead of constructing a high -dimensional tensor. The experimental re sults on the synthetic datasets correspond to our analysis in section 5.3 and suggest that our algorithm can efficient ly deal with large sparse multi -relational data. We propose d the MetaFac framework to extract com munity structures from various social contexts and interactions. There were three key ideas: (1) metagraph, a relational hypergraph for representing multi -relational social data; (2) MF algorithm, an efficient non -negative multi -tensor factorization method for community extraction on a given metagraph; (3) MFT, an on -line factorization method to handle time -varying relations . We conduct ed ext ensive experiments on real -world data collected from Digg. Our case study demonstrate d that meaning ful mining results can be generated by our method. We evaluate d our method by predicting digg / comment action s on stories. We generate d the predictions based on the extracted community models and compare results with baselines. Our method outperformed baseline measures up to an order of magnitude . Our method can be further improved by (a) incorporating a historic model and (b) leveraging other relations through a metagraph. As part of our future work, we plan to investigate the following open issues: (1) the resolution (inc luding number of communities) of community structures; (2) kernel based representation of facet relationships . [1] B. B ADER and T. K OLDA (2006). Algorithm 862: MATLAB [2] A. B ANERJEE , S. B ASU and S. M ERUGU (2007). Multi -way [3] R. B EKKERMAN , R. E L -Y ANIV and A. M C C ALLUM (2005). [4] D. C HAKRABARTI , R. K UMAR and A. T OMKINS (2006). [5] Y. C HI , S. Z HU , Y. G ONG and Y. Z HANG (2008). [6] N. F RIEDMAN , L. G ETOOR , D. K OLLER and A. P FEFFER [7] K. J  X RVELIN and J. K EK X L X INEN (2000). IR evaluation [9] R. K UMAR , J. N OVAK and A. T OMKINS (2006). Structure and [10] D. L EE and H. S EUNG (2001). Algorithms for non -negative [11] Y. L IN , Y. C HI , S. Z HU , H. S UNDARAM and B. T [12] B. L ONG , Z. Z HANG and P. Y U (2007). A probabilistic [13] A. P OPESCUL , L. H. U NGAR , D. M. P ENNOCK and S.
 [14] J. S UN , C. F ALOUTSOS , S. P APADIMITRIOU and P. Y [15] S. Z HU , K. Y U , Y. C HI and Y. G ONG (2007). Combining 
Figure 7 : Effect of (a) prior community model (historic information) and (b) differen t input relations.
Figure 8 : Running time per iteration (sec.) for different types of data growth (let n denote the value on the x -axis of each plot): (a) number of non -zero elements (one 3 -way tensor with n non -zero elements), (b) number of tensor modes (one n -way tensor), (c) number of relations ( n 3 -way tensors) in a metagraph, and (d) for different algorithm parameter, the number of clusters ( K ).
