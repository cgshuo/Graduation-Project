 KARTIK TALAMADUPULA, J. BENTON, and SUBBARAO KAMBHAMPATI
Arizona State University and PAUL SCHERMERHORN and MATTHIAS SCHEUTZ Indiana University 1. INTRODUCTION
As the fields of robotics and Human-Robot Interaction (HRI) have advanced, de-mand has escalated for applications that require humans and robots to  X  X eam X  and work together to solve complex problems. While some of these scenarios may be handled through  X  X eleoperation X , an increasing number require team-ing between humans and autonomous robots. A compelling application of this type involves the urban search and rescue scenario, where a human is in re-mote contact with the robot and provides high-level instructions and goals.
Clearly, robots operating in such teaming scenarios require the ability to plan (and revise) a course of action in response to human instructions. Our focus in this article is on understanding the challenges faced by the planner that guides a robot in such teaming scenarios.

Although there has been extensive work in the past on understanding the challenges of human-planner interactions (refer to mixed-initiative planning) and planner-robot interaction (refer to planning and execution), these efforts do not provide a complete solution in human-robot teaming scenarios (see Section 6).

Several parts of the state-of-the-art planning technology that go beyond typ-ical classical planning are both required and easily adapted to human-robot teaming scenarios. In particular, the planner should allow for actions with du-rations to handle goals with deadlines, and partial satisfaction of goals should be possible to allow the planner to  X  X kip X  seemingly unreachable goals (e.g., if the goal of exiting a building cannot be currently satisfied, that should not prevent the robot from reporting on injured humans). For partial satisfaction planning, we model soft goals (i.e., goals that may remain unachieved) with a reward and give a cost to each action and the planner seeks to find a plan with maximum net benefit (i.e., summed goal reward -summed action cost). Along with these, an important part of any online system is execution monitoring and replanning to allow the planner to receive and react to new information from either the environment (e.g., the discovery of a new area) or from a human com-mander (e.g., a change in goal deadline). To accept information from a human commander, the robotic architecture parses and processes natural language (i.e., speech) into goals or new facts. If the architecture cannot handle a goal or fact by following a simple script located in its library, it calls the planner to find a method of achieving the goal.

Human-robot teaming tasks present an additional critical challenge not han-dled by current planning technology: open worlds . Most teaming tasks involve open worlds and require the ability to handle both counterfactual knowledge and conditional goals. For example, a human commander might instruct the robot to report on any injured humans that it encounters in a search-and-rescue scenario. In such a scenario, the world is open in that neither the human nor the robot know where injured humans are.

While the state-of-the-art planners are very efficient, they focus mostly on closed worlds. Specifically, they expect full knowledge of the initial state, and expect up-front specification of the goals. Adapting them to handle open worlds presents many thorny challenges. Three tempting yet ultimately flawed approaches for making closed world planners handle open worlds involve ei-ther blindly assuming that the world is indeed closed, deliberately  X  X losing X  the world by acquiring all the missing knowledge before planning, or account-ing for all contingencies during planning by developing conditional plans. The alternative (assuming a closed world) will not only necessitate frequent re-planning during execution, but can also lead to highly suboptimal plans in the presence of conditional goal rewards. Acquiring full knowledge up front would involve the robot doing a sensing sweep to learn everything about its world before commencing the planning. This is clearly infeasible in open worlds where we do not even know how many objects may be there and thus do not know when to stop sensing. After all, a robot cannot be simply com-manded to  X  X ense everything, X  but rather has to be directed to perform spe-cific sensing tasks. Accounting for missing knowledge would involve making conditional plans to handle every type of contingency, and letting the robot follow the branches of the plan that are consistent with the outcomes of its sensing. Such full contingency planning is already known to be impractical in propositional worlds with bounded indeterminacy (refer to Meuleau and Smith [2003]); it is so in open worlds where the number of objects (and their types) are unknown.

What is needed instead is both a framework for specifying conditional knowl-edge and rewards, and an approach for using it to direct the robot in such a way as to intelligently trade sensing costs and goal rewards. Accordingly, we propose an approach for representing and handling a class of conditional goals called Open World Quantified Goals (OWQGs). OWQGs provide a compact way of specifying conditional reward opportunities over an  X  X pen X  set of objects. Us-ing OWQGs, we can specify (for instance) that for a robot to report an injured human, it must have found an injured human and that finding an injured human involves sensing. We shall see how OWQGs foreground the trade-off between sensing cost and goal reward. We will discuss the issues involved in optimally selecting the conditional rewards to pursue, and describe the approx-imate  X  X ptimistic X  method we used in the current work.

The rest of this article is devoted to describing the details of our planner and its open world extensions. We also discuss how the planner is integrated into the robotic architecture to support human-robot teaming in USAR, and present an empirical evaluation establishing the effectiveness of our approach. The article is organized as follows. We start by describing some details of our motivating
USAR scenario. In Section 2, we present the automated planner that is used as a part of this system, including a description of the update syntax that enables new information in the world to be relayed to the planner. In Section 3, we discuss the challenges of planning in an open world, culminating in the definition of a new construct for conveying open world information and a description of its implementation in our system. Section 4 describes the architecture used to control the robotic agent and the integration of the planner and this architecture, including a description of how sensing and updates to the world state are handled. In Section 5, we present the results of an empirical evaluation conducted on the robot. Section 6 provides an overview of related work, and Section 7 presents our conclusions.
 1.1 Overview of USAR Scenario We consider the problem of a human-robot team engaged in an Urban Search
And Rescue (USAR) scenario inside a building. The robot is placed at the be-ginning of a long hallway; a sample layout X  X  map is presented in Figure 1. The human team member has intimate knowledge of the building X  X  layout, but is removed from the scene and can only interact with the robot via on-board wire-less audio communication. 1 The hallway in which the robot is located has doors leading off from either side into rooms, a fact known to the robot. However, unknown to the robot (and the human team member) is the possibility that these rooms may contain injured humans (victims). The robot is initially given a hard goal of reaching the end of the hallway by a given deadline based on wall-clock time. As the robot executes a plan to achieve that goal, the team is given the (additional) information regarding victims being in rooms. Also spec-ified with this information is the quantified soft goal of reporting the location of victims.

In this example, the planner must reason about the cost-benefit trade-off (net benefit) of attempting to find a victim, since it is a soft goal and can be ignored if it is not worth the pursuit; it must then direct the robot to sense for the information that it needs in order to determine the presence of a victim in a particular location. The dynamic nature of the domain coupled with the par-tial observability of the world precludes complete, a priori specification of the domain, and forces the robot and its planner to handle incomplete and evolving domain models [Kambhampati 2007]. This fact, coupled with the inability of human experts to completely specify information relevant to the given problem and goals up front, makes it quite likely that information needed to achieve some goals may become available at some later stage during the planning process. 2. BASE PLANNER
The planner that we use, called SapaReplan , is an extension of the metric-temporal planner Sapa [Do and Kambhampati 2002] that handles partial sat-isfaction planning [Benton et al. 2009] and replanning [Cushing et al. 2008].
Specifically, the planning problem is defined in terms of the initial state and the set of goals that need to be satisfied. Actions have known (real-valued) costs.
Each goal can have a reward and a penalty  X  [0 ,  X  ]. The reward is accrued when the goal is satisfied in the final state, while the penalty is incurred for not satisfying it. The costs, rewards, and penalties are all assumed to be in the same units. The net benefit of a solution plan is defined as the sum of rewards of the goals it achieves, minus the sum of penalties of the goals it fails to achieve, and minus the sum of costs of the actions used in the plan. The use of reward/penalty model allows our planner to model both opportunities and commitments/constraints in a uniform fashion. A goal with zero penalty is a pure opportunity, while one with zero reward is a pure commitment. A  X  X ard" goal has finite reward but infinite penalty (and thus must be achieved by any plan).

The planner consists of three coupled, but distinct parts:  X  X earch. SapaReplan performs a weighted A*, forward search using net benefit as the optimization criterion.  X  X euristic. The heuristic used to guide the planner X  X  search is based on well-known relaxed planning graph heuristics where, during search, relaxed so-lutions are found in polynomial time per state. Sapa uses a temporal relaxed planning graph that accounts for the durations of actions when calculat-ing costs and finding relaxed solutions. In the partial satisfaction planning extensions, the heuristic also performs online goal selection. In essence, it solves for all goals (hard and soft) in the relaxed problem and gives a cost for reaching each of them (  X  for unreachable goals). If the cost of reaching a soft goal is greater than its reward, it removes that goal from the heuristic calculation. If the cost of reaching a hard goal is infinity, it marks a state as a dead end. Finally, the difference between the total reward and total cost of the remaining goals is calculated and used as the heuristic value.  X  X onitoring / Replanning. The extensions for replanning require the use of an execution monitor, which takes updates from the human-robot team ar-chitecture (in this case). Upon receiving an update, the planner updates its knowledge of the  X  X urrent state X  and replans. Replanning itself is posed as a new partial satisfaction planning problem, where the initial and goal states capture the status and commitments of the current plan [Cushing et al. 2008].

To see how our planning system copes with open environment scenarios, it is important to understand the details of its execution monitoring component.
This is arguably the most important part of the planning system for the problem at hand, as its focus is on handling unexpected events and gathering new information for the planner. It serves as an interface between the human-robot team architecture (discussed in Section 4.1) and the planning engine.
Problem updates . New sensory information, goals, or facts given by a hu-man commander can be sent to the planner at any time, either during plan-ning or after a plan has been output. Regardless of the originating source, the monitor listens for updates from a single source from the architecture and correspondingly modifies the planner X  X  representation of the problem. Updates can include new objects, timed events (i.e., an addition or deletion of a fact at a particular time, or a change in a numeric value such as action cost), the addition or modification (on the deadline or reward) of a goal, and a time point to plan from. An example update is given next.
All goals are on propositions from the set of boolean fluents in the problem, and there can only be one goal on any given proposition. In the default setting, goals are hard, lack deadlines, and have zero reward. 2 All fields in an update specification, with the exception of  X :now X  (representing the time we expect to begin executing the plan), may be repeated as many times as required, or left out altogether. The intent of allowing such a flexible representation for updates is to provide for accumulation of changes to the world in one place. In the particular example provided, a new object  X  X ed3 X  of type  X  X one X  is declared. In addition, three new events are defined, one of them with a temporal annotation that describes the time at which that event became true. A new hard goal that carries 500 units of reward is also specified, and the update concludes with the specification of the current time.

As discussed by Cushing et al. [2008], allowing for updates to the planning problem provides the ability to look at unexpected events in the open world as new information rather than faults to be corrected. In our setup, problem updates cause the monitor process to restart the planner (if it is running) after updating its internal problem representation. 3. PLANNING IN THE OPEN WORLD
As previously discussed, there exists an obvious problem with using a planner that assumes a closed world within an open world environment. Because the world is open, the robot (as well as the human) do not have full knowledge of all the objects in the world. In the USAR scenario, neither the human nor the robot know where the injured humans might be. Furthermore, it is also possible that the human-robot team does not have a full  X  X ap X  of the building in which the rescue is taking place.

One immediate ramification of the open world is that the goals are often conditioned on particular facts whose truth value may be unknown at the initial state. For example, the most critical goal in the USAR scenario, namely reporting the locations of injured humans, is conditioned on finding injured humans in the first place.

To see this, consider our Urban Search And Rescue (USAR) scenario, where we have a set of objects that imply certain facts. For instance, when moving through the hallway we can say that sensing a door implies the existence of a room. Subsequently, doors imply the potential for goal achievement (i.e., oppor-tunities for reward). Specifically, the robot X  X  task is to find injured people in a building. While the number of injured individuals remains unknown, the com-mander becomes aware that people are likely within rooms (and subsequently passes this information on to the robot). This goal is over an open world, in that new objects and facts may be brought to light through either external sources like the mission commander or through action execution.

To be effective in such scenarios, the planner should be opportunistic, gen-erating plans that enable goal achievement as against finding the most direct path to the currently known goals (e.g., entering rooms to look for injured indi-viduals). This planning is interleaved with plan execution. Unfortunately, we have several other constraints that may preclude the achievement of goals. The robot must meet a hard deadline and may run out of exploration time; it may also be unable to fully explore the building due to parts of it being inaccessible.
Additionally, sensing to resolve the truth of world facts and the existence of ob-jects may often be costly and time consuming. This means that certain aspects of the world may remain open (and therefore unknown) by design. 3.1 Conditional Goals
To formally model the USAR robot X  X  goal of looking for and reporting injured people, it is useful to consider the fact that this goal is certainly not one of simple achievement, since the robot does not need to (and should not) report victims unless they are actually present in the rooms. The uncertainty in this scenario (and other similar real-world problems) stems from the inherently conditional presence of objects (and the truth of facts about them) in the world. Such goals can be looked at as conditional goals .

For exposition purposes, we shall start with a discussion of challenges in-volved in handling propositional conditional goals. A propositional conditional goal P G is interpreted as  X  G needs to be satisfied if P is true initially X . Formally, we have the following.

Conditional goal . Given ground predicates A and B ,a (hard) conditional goal A B is defined as the requirement that if A is true in the initial state
I , then any solution plan  X  must make B true in the final state resulting from the application of  X  to I .

From the previous definition of conditional goals, it holds that the set of goals that a plan  X  needs to fulfil in order to be considered a solution to the problem is variable, and that the composition of such a set depends on the state of the antecedents of the conditional goals initially (at I ). It also follows that a plan  X  will not be considered a solution unless it fulfils each and every one of the conditional goals g c  X  G c .

The conditional goal as defined before poses a  X  X ard X  constraint: if the an-tecedent holds, then every solution plan must achieve the goal. It is useful to relax this requirement.

Soft conditional goal .A soft conditional goal A B [ u ][ p ] is defined as the provision that if A is true in the initial state I , then the achievement of B in the final state G  X  G (the set of all goals) will accrue a reward of u units, while the failure to achieve B will incur a penalty of p units.

In general, it is useful consider a spectrum of planning methods (as shown in Figure 2) to deal with conditional goals, all of which are contingent on the the observability of the initial state I  X   X  .If I is fully observable, the planner knows the values of the antecedents of all the conditional goals g this information, a problem with conditional goals may be compiled into a standard classical planning problem (in case only hard conditional goals are present and a Partial Satisfaction Planning (PSP) problem otherwise).
However, if I is partially observable, the planner is faced with a more complex problem. If all the conditional goals are hard (and hence must be achieved for plan success), the planner has no option but to direct the robot to sense for all the facts that occur in the antecedents of the goals in G compilation approach mentioned previously.

If the conditional goals in the scenario are all soft instead, is confronted with an interesting problem: it must not only sense in order to establish which of the antecedents are true in the initial state, but must also select a subset of these goals whose achievement will optimize the net benefit achieved given the costs and rewards of achieving the original goals and the costs of sensing for the antecedents (the standard PSP problem).

The most general way of dealing with conditional goals in such a case would be to accept knowledge on the antecedents in the form of distributions ,andto use a probabilistic planner to compute the set of goals with the best expected net benefit. As an illustration, suppose the planner decided to sense the conditional plan synthesis, to decide whether this sensing cost will be offset by the increased net benefit, the planner has to compute the expected net benefit achievable. In order to do this, it needs to have (or assume) some prior knowledge on how the truth values of the antecedents P : P i of the conditional goals are jointly distributed. Let this distribution be ( P ). Further, let conditional goals that are triggered by a specific valuation of the antecedents.
For each such valuation P , the optimal net benefit achievable by the planner is
B ( G optimal set of conditional goals to be sensed  X  G c is computed as
Focusing sensing this way, while optimal, can be impractical both because of the need for distributional information, and because of the computational cost of computing optimal net benefit plans for each potential goal set. We may be forced to make reasonable assumptions on the distribution of the antecedents of these conditional goals, or resort to regret minimization approaches that do not require distributional information.
 Conditional Goals in the Open World
While propositional conditional goals give us an understanding of the trade-offs between sensing costs and goal rewards, they are not expressive enough for open world scenarios such as USAR, where the conditional rewards are often quantified over an open set of objects. Accordingly, we will consider quantified conditional goals  X  x P ( x ) G ( x ). Because the quantification is over an open set, it cannot be simply expanded into a set of propositional conditional goals. For example, we cannot convert a quantified conditional goal of type injured ( x ) reportLocation ( x ) into a finite set of ground conditional goals since we do not know a priori how many persons are there, let alone which of them are injured. This makes direct application of the decision-theoretic goal selection approach described earlier infeasible. Indeed, even the naive approaches of planning for all contingencies or closing the world by sensing for everything up front are infeasible to realize, as the robot does not know how many objects may be there, and thus does not quite know when to stop sensing. Instead, we have to resort to a more incremental expansion of the quantified goal that interleaves planning and execution, and deliberately closes sensing operations in particular parts of the world. In the following sections, we describe both our representation for quantified conditional goals, called OWQG, and our current method for handling them during planning. 3.2 Open World Quantified Goals
Open World Quantified Goals (OWQG) [Talamadupula et al. 2010] combine information about objects that may be discovered during execution with partial satisfaction aspects of the problem. Using an OWQG, the domain expert can furnish details about what new objects may be encountered through sensing and include goals that relate directly to the sensed objects. An Open World Quantified Goal (OWQG) is a tuple Q = F , S , P , C , G where F and typed variables that are part of the planning problem. F belongs to the object type that Q is quantified over, and S belongs to the object type about which information is to be sensed. P is a predicate which ensures sensing closure for every pair f , s such that f is of type F and s is of type belong to the set of objects in the problem, O  X  ; for this reason, we term a closure condition . C = i c i is a conjunctive first-order formula where each c is a statement about the openness of the world with respect to the variable S . c will hold for new objects of the type  X  X uman X  that are sensed. Finally quantified goal on S .

Newly discovered objects may enable the achievement of goals, granting the opportunity to pursue reward. For example, detecting a victim in a room will allow the robot to report the location of the victim (where reporting gives reward). Given that reward in our case is for each reported injured person, there exists a quantified goal that must be allowed partial satisfaction. In other words, the universal base [Golden and Weld 1996], or total grounding of the quantified goal on the real world, may remain unsatisfied while its component terms may be satisfied. To handle this, we rely on the partial satisfaction capability of the base planner (Section 2).

As an example, we present an illustration from our scenario: the robot is directed to  X  X eport the location of all victims X . This goal can be classified as open world, since it references objects that do not exist yet in the planner X  X  object database O ; and it is quantified, since the robot X  X  objective is to report all victims that it can find. In our syntax, this information is encoded as follows.
In the preceding example, line 2 denotes F , the typed variable that the goal is quantified over; line 3 contains the typed variable S , the object to be sensed.
Line 4 is the unground predicate P known as the closure condition (defined earlier). Lines 5 and 6 together describe the formula C that will hold for all objects of type S that are sensed. The quantified goal over 7, and line 8 indicates that it is a soft goal and has an associated reward of 100 units. Of the components that make up an open world quantified goal required 4 and F and S must be nonempty, while the others may be empty. If
G is empty, that is, there is no new goal to work on, the OWQG simply as additional knowledge that might help in reasoning about other goals. 3.3 Handling OWQGs in the Planning System
To handle open world quantified goals, the planner grounds the problem into the closed world using a process similar to Skolemization. More specifically, we generate runtime objects from the sensed variable S that explicitly represent the potential existence of an object to be sensed. These objects are marked as system-generated runtime objects. Given an OWQG Q = F , S , P , C , G can look at S as a Skolem function of F , and runtime objects as Skolem entities that substitute for the function. Runtime objects are then added to the problem and ground into the closure condition P , the conjunctive formula open world quantified goal G . Runtime objects substitute for the existence of S dependent upon the variable F . The facts generated by following this process over C are included in the set of facts in the problem through the problem update process. The goals generated by G are similarly added. This process is repeated for every new object that F may instantiate.

We treat P as an optimistic closure condition , meaning a particular state of the world is considered closed once the ground closure condition is true. On ev-ery update the ground closure conditions are checked and if true the facts in the corresponding ground values from C and G are removed from the problem. By planning over this representation, we provide a plan that is executable given the planning system X  X  current representation of the world until new informa-tion can be discovered (via a sensing action returning the closure condition).
The idea is that the system is interleaving planning and execution in a manner that moves the robot towards rewarding goals by generating an optimistic view of the true state of the world.

As an example, consider the scenario at hand and its open world quantified goal. Given two known zones, zone1 and zone2 , the process would generate a run-100) would be generated and added to the problem (where the exclamation mark (!) indicates a runtime object). A closure condition zone1) would also be created. Similarly, a runtime object and goal (report human!2 injured zone2) added to the problem, and the clo-sure condition (looked for human!2 zone2) would be created. When the planning system receives an update including (looked for human!1 zone1) point. Similar actions are taken when (looked for human!2 zone2)
The planner must only output a plan up to (and including) an action that will make the closure condition true. Therefore once the condition becomes true, the truth values of the facts in C are known. 4. OVERALL SYSTEM AND INTEGRATION
The SapaReplan planner is integrated into the robotic architecture as a newly created client server that interacts directly with a goal manager , as detailed in Schermerhorn et al. [2009] (see Figure 4). This new server does not manage action execution, as the existing goal manager already has that capability. The planner is viewed by the goal manager, in effect, as an external library that augments its internally maintained store of procedural knowledge. When a new goal is presented, the goal manager determines whether there is a procedure already known to achieve it; if so, then that procedure is executed, otherwise the goal is sent to the planning component, which returns a script representation of a plan to achieve the goal, if one is found. In the following, we describe these parts and the integration of the system in detail. 4.1 DIARC Control Architecture
The architecture used to control the robotic agent in the aforesaid scenario (shown in Figure 3) is a subset of the Distributed, Integrated, Affect, Reflection, and Cognition architecture (DIARC) 5 [Scheutz et al. 2007]. DIARC is designed with human-robot interaction in mind, using multiple sensor modalities (e.g., cameras for visual processing, microphones for speech recognition and sound localization, laser range finders for object detection and identification) to rec-ognize and respond appropriately to user requests. DIARC is implemented in the Agent Development Environment (ADE) 6 [Scheutz 2006], a framework that allows developers to create modular components and deploy them on multiple hosts. Each functional component is implemented as a server . A list of all ac-tive ADE servers, along with their functionalities, is maintained in an ADE registry . The registry helps in resource location, security policy enforcement, and fault tolerance and error recovery. When an ADE server requires function-ality that is implemented by another component, it requests a reference to that component from the registry, which verifies that it has permission to access the component and provides the information needed for the two components to communicate directly.
The ADE goal manager is a goal-based action selection and management system that allows multiple goals to be pursued concurrently, so long as no resource conflicts arise. When the actions being executed for one goal present a hazard to the achievement of another goal, the goal manager resolves the conflict in favor of the goal with the higher priority, as determined by the net benefit (reward minus cost) of achieving the goals and the time urgency of each (based on the time remaining within which to complete the goals).

The goal manager maintains a  X  X ibrary X  of procedural knowledge in the form of (1) action scripts which specify the steps required to achieve a goal, and (2) action primitives which typically interface with other ADE servers that provide functionality to the architecture (e.g., a motion server could provide an interface to the robot X  X  wheel motors, allowing other ADE servers to drive the robot). Scripts are constructed of calls to other scripts or action primitives. Aside from this predefined procedural knowledge, however, the goal manager has no problem-solving functionality built in. Therefore, if there is no script available that achieves a specified goal, or actions are missing in a complex script, then the action interpreter fails. The addition of the planning system thus provides
DIARC with the problem-solving capabilities of a standard planner in order to synthesize action sequences to achieve goals for which no prior procedural knowledge exists. 4.2 Integrating the Planner into DIARC
The integration uses a new interface to the planner to facilitate updates from the goal manager. The modified version of the planner is encapsulated as a new DIARC component that provides access to this interface to other ADE servers (although in practice, the goal manager is the only client of the planning server). The interface specifies how the goal manager can send state updates to the planner, and how the planner, in turn, can send updated or new plans to the goal manager. State updates are sent whenever relevant data of the requested type is received via sensors. In the USAR scenario that we use, for example, information about doors and boxes (which stand in for humans in our experi-mental runs; see Section 5) would be considered relevant. In this manner, the goal manager filters the information that is sent back in the form of problem updates, to avoid overwhelming the planning system. These updates can then trigger a replanning process, which returns a plan in the form of action scripts that the goal manager can adopt and execute in the same way as its prede-fined scripts. Moreover, the new plan can be added to the goal manager X  X  local knowledge base so that future requests can be serviced locally without having to invoke the planner. This plan reuse is applicable only when the relevant parts of the world remain unchanged, where relevance is determined by exam-ining the preconditions of the actions in the plan. If there is a change in these facts due to updates to the world, ADE initiates replanning via SapaReplan.
The SapaReplan planner server starts the SapaReplan problem update mon-itor, specifies the planning domain, and (when applicable) the sensory update types that are of interest to the planner are sent to the goal manager (via the  X  X ttend X  mechanism, described in Section 4.3 shortly), and the planner server enters its main execution loop. In this loop, it retrieves new plans from the planner (to be forwarded to the goal manager) and sends new percepts and goal status updates (received from the goal manager) to the planner. If a per-cept triggers replanning, the previously executing plan (and script) is discarded and a new plan takes its place.
 A closely related issue that crops up when integrating a planner such as
SapaReplan into a robotic architecture is that actions (and consequently plans) take time to execute on a robot and carry temporal annotations denoting the time it takes to execute them (as outlined in Section 2). Since we are executing in an open world, it is entirely possible that an action takes more time to execute than was planned. We circumvent this problem by assigning conservative time estimates to each action available to the robotic agent (and consequently the planner). If there is slack time during the execution, we simply bring forward the execution of the actions that are next in the plan. Though this approach would fail for certain types of concurrency exhibited by actions, the USAR scenario that we seek to solve does not contain any actions that need to be executed concurrently. 7 In case an action takes longer time to execute than even the conservative estimate assigned to it (due to a failure of some nature), the planner is called into play in order to provide a new plan. 4.3 Sensing and Updates
The planner X  X  ability to exploit opportunities requires, of course, that it be informed of changes in the environment that signal when an opportunity arises.
One major issue for any robotic system operating in the real world is how to determine which small fraction of the features of the environment are of greatest salience to its goals. Resource limitations preclude a  X  X atch out for anything X  approach, necessitating some guidance with regard to how sensory processing resources should be allocated. For example, in a search and rescue scenario where victims are likely to be located in rooms, the appearance of a doorway would be of high relevance to the system X  X  goals.

A special  X  X ttend X  primitive has been defined in the goal manager to allow servers (such as the planner server) to specify which percepts are of interest.
This will focus attention on those types of percepts by causing the instantiation in the goal manager of monitoring processes that communicate with other ADE servers (e.g., the vision server to detect targets of interest that are visually per-ceivable, the laser range finder server to detect doorways, which are detected in the range finder profile, etc.). In the case of the SapaReplan planner server, the percept types of interest are those that could prompt opportunistic replanning (e.g., detection of a new doorway might trigger a new plan to explore the room).
A variety of percept types are available from various ADE servers; a subset of those most relevant to the present study are: (2) (landmark ?name type ?t heading ?dir distance ?dist) provides in-(3) (box ?name color ?value heading ?dir distance ?dist) provides in-(4) (doorway ?name heading ?dir distance ?dist) provides information
When the monitoring process of the goal manager detects a percept in its attend list, it constructs a plan update and sends it to the planner via the planner server X  X  update method. Updates from the goal manager can trigger the planner to replan to take advantage of detected opportunities. Plans are generated in the form of ADE action scripts which are directly executable by an action interpreter in the goal manager. Some examples of ADE scripts (both simple and complex) available to the planner for creating plans are: (1) (look-for ?t) scans the room for percepts of type ?t while turning 360 (2) (move-to ?location) moves to the location specified (e.g., as indicated by (3) (turn-to ?location) turns to face the location specified (e.g., (4) (move-through ?doorway) moves through the specified doorway (5) (report ?object ?c1 ...) reports the given characteristics The new plan/script is passed to the goal manager, which oversees its execution.
When a plan completes, its postconditions are sent to the planner server as goal status updates. If a newly encountered percept triggers replanning, the previously executing plan is discarded and the new plan takes its place. Hence, the SapaReplan planner server can provide problem-solving capabilities to architectures constructed in the ADE infrastructure.

Example . The following example illustrates the interaction between the goal manager, the planner server, and the planner. In this case, the robot is travers-ing a hallway from hall-start to hall-end when it encounters a doorway (having previously added doorways to the attend list). The goal manager sends to the planner server a state update indicating that a new doorway ( has been detected. The planner server generates an update to the planner that includes the new door, but also updates the planner X  X  representation of the envi-ronment; to begin with the planner knows only of the two locations and hall-end and the path between them ( hall-start  X  hall-end a hard goal of going to the end of the hallway. When the new doorway is detected, a new room ( room1 ) is created and a new location outside-room1 and linked into the path ( hall-start  X  outside-room1  X  hall-end ilarly, the path between the hallway and the newly detected room is added ( room1  X  outside-room1 ). This allows the planner to generate paths into and out of the room if it determines that it is worth investigating the room (see the following for details). This update to the environment is sent to the planner by the planner server, and if the update causes a change in the currently executing plan, the resultant script is sent to the goal manager for execution. 5. EMPIRICAL EVALUATION
The integration of the robotic architecture with the planner, along with all of its attendant extensions, was evaluated via experimental runs in the USAR task scenario introduced earlier. The task at hand is the following: the robot is required to deliver essential supplies (which it is carrying) to the end of a long hallway; this is a hard goal. The hallway has doorways leading off into rooms on either side, a fact that is unknown to the robot initially. When the robot encounters a doorway, it must weigh (via the planner) the action costs and goal deadline (on the hard delivery goal) in deciding whether to pursue a search through the doorway.

In the specific runs described here, green boxes act as stand-ins for victims, whereas blue boxes denote healthy people (whose locations need not be re-ported). The experimental setup consisted of three rooms, which we represent as R 1 , R 2 and R 3 . The room R 1 contained a green box (GB), representing a victim; R 2 contained a blue box (BB), representing a healthy person; and R not contain a box. 8 The respective doorways leading into the three rooms R through R 3 are encountered in order as the robot traverses from the beginning of the hallway to its end.

The aim of these experimental runs was to demonstrate the importance of each of the planning components that make up this integrated system, and to showcase the tight integration that was achieved in order to control the robot in this scenario. To achieve these goals, we conducted a set of experiments where we varied four parameters, each of which could take on one of two values, thus giving us 16 different experimental conditions through the scenario. The factors that we varied were: (1) Hard Goal Deadline . The hard goal deadline was fixed at 100 time units, (2) Cost . Presence or absence of action costs to demonstrate the inhibiting (3) Reward . Presence or absence of a reward for reporting injured people in (4) Goal Satisfaction . Label the goal of reporting injured people as either soft
In the tables provided, a + symbol stands for the presence of a certain feature, while a -denotes its absence. For example, run number 5 from Table I denotes an instance where the deadline on the hard goal (going to the end of the hallway) was 100 time units, action costs were absent, the open world goal of reporting people carried reward, and this goal was classified as soft.
 The experimental runs detailed in this section were obtained on a Pioneer
P3-AT robot (see Figure 5) as it navigated the USAR scenario with the initial hard goal of getting to the end of the hallway, while trying to accrue the maxi-mum net benefit possible from the additional soft goal of reporting the location of injured people. A video of the robot performing these tasks can be viewed via the following link:
The robot starts at the beginning of the hallway, and initially has a plan for getting to the end in fulfilment of the original hard goal. An update is sent to the planner whenever a doorway is discovered, and the planner subsequently replans to determine whether to enter that doorway. In the first set of runs, with a deadline of 100 units on being at the end of the hallway, the robot has time to enter only the first room, R 1 (before it must rush to the end of the hallway in order to make the deadline on the hard goal).

Even with this restriction, some interesting plans are generated. The plan-ner directs the robot to enter R 1 in all the runs except 3 and 7. This can be attributed to the fact that there is no reward on reporting injured people in those cases, and the reporting goal is soft; hence the planner does not consider it worthwhile to enter the room and simply ignores the goal on reporting. The alert reader may ask why it is not the case that entering R 4 and 8 as well, since there is no reward on reporting injured people in those cases either; however, it must be noted that this goal is hard in cases 4 and 8, and hence the planner must plan to achieve it (even though there may be no injured person in that room, or reward to offset the action cost). This example illustrates the complex interaction between the various facets of this scenario (deadlines, costs, rewards, and goal satisfaction), and shows how the absence of even one of these factors may result in the robot being unable to plan for opportunities that arise during execution, in this case, detecting and reporting injured people.

When the deadline on reaching the end of the hallway is extended to 200 units, the robot is afforded enough time to enter all the rooms. In such a scenario, it is expected that the robot would enter all the rooms to check for victims, and this is indeed what transpires, except in runs 11 and 15. In those runs, the robot skips all rooms for precisely the same reasons outlined before (for runs 3 and 7): the lack of reward for reporting the goal, combined with the softness of that goal. Indeed, runs 3 and 7 are respectively identical to runs 11 and 15 save the longer deadline on the hard goal.

Another interesting observation is that in all the cases where the robot does enter R 2 , it refuses to report the blue box (BB), since there is no reward attached to reporting blue boxes (nonvictims). Since the deadline is far enough away for runs 9 through 16, the planner never fails to generate a plan to enter rooms in order to look for injured people, avoiding the situation encountered in runs 2, 4, 6, and 8 where there is no feasible plan that fulfils all hard goals since the robot has run out of time (denoted  X  in Table I).

In terms of computational performance, the planning time taken by the planning system was typically less than one second (on the order of a hun-dred milliseconds). Our empirical experience thus suggests that the planning process always ends in a specific, predictable time frame in this scenario (an important property when actions have temporal durations and goals have dead-lines). Additionally, in order to test the scale-up of the system, we evaluated it on a problem instance with ten doors (and consequently more runtime objects) and found that there was no significant impact on the performance.

These runs thus confirm the importance of the three main components of the planning system that directs the robot in this USAR scenario X  X ithout replan-ning, the system would not be able to take new information about doorways and rooms connected to them into account; without support for soft goals, the planner may fail to return a plan given an overconstrained problem; and with-out an open world representation, the planner would be unable to reason about new objects (doorways, rooms, injured persons) that result in the fulfilment of new goals. 6. RELATED WORK
In this article, we focused on planning support for robots where human-robot teams work in tandem to solve complex problems in open world scenarios.
Although there has not been any prior work that directly addresses planning for human-robot teaming in open worlds, there does exist a rich body of work that is related to various aspects of our overall problem. As shown in Figure 6, this related work can be classified into three parts: human-robot interaction, human-planner interaction, and planner-robot interaction. Specifically:  X  X lanning and execution monitoring deals with the interactions between a fully autonomous robot and a planner;  X  X uman-Robot Interaction (HRI) works toward smooth interactions between a human user and a robot;  X  X ixed initiative planning relates to interactions between humans who are receiving plans and the automated planners that generate them.

Since our focus is on how a planner fits into human-robot teams, we are most interested in work that relates to planning and execution monitoring and mixed initiative planning. There has been significant work in planning and execution monitoring, often in the context of replanning and contingent planning. Contin-gent planners (refer to Albore et al. [2009] and Meuleau and Smith [2003]) can be viewed as solving for the problem of execution monitoring by assuming full sensing knowledge is available at execution time, so no replanning would ever be necessary. However, as Gat [1992] has pointed out, in designing a planner whose ultimate goal is finding plans for execution, it is difficult (and sometimes impossible) to model for all contingencies, and often it is better to design an execution monitoring system that is capable of recognizing failures (i.e., cog-nizant failures [Firby 1989]). That is, we can relax the problem for the planner by removing uncertainty in the world. Agre and Chapman [1990] also discuss these issues in relationship to planning and execution monitoring and viewing  X  X lans as advice. X 
A number of systems (refer to [Lemai and Ingrand 2003], [Knight et al. 2001], [Myers 1998]) have worked by performing execution monitoring and subsequent plan repair or replanning upon the discovery of an inconsistent ex-ecution state. For instance, the CASPER planner [Knight et al. 2001] performs plan repair upon failure. While the IxTeT-eXeC [Lemai and Ingrand 2003] sys-tem attempts a similar repair strategy, it replans only if no repair can be found. It handles the arrival of new goals through replanning.
 The work that is most closely related to our system, however, seems to be
Bagchi et al. X  X  [1996] system for controlling service robots. The emphasis of their work is on the robotic agent X  X  capability to not only plan and act autonomously, butalsotodosoinan interactive way such that the user X  X  comfort and safety are kept in mind. In order to achieve this, the robot is equipped to comprehend the user X  X  (changing) goals and advice at different levels of detail. In turn, the planner can refine and modify these goals dynamically and react to unexpected changes in the environment. This system thus includes the human user in the loop via interaction with the robot and a probabilistic planner.

However, one critical area where the action selection mechanism employed by Bagchi et al. will fail is when sensing and/or sensing actions are expensive.
Sensing is critical to real-world applications, since most scenarios involve par-tial knowledge of the world state and the system needs a mechanism to update itself of changes and new information in the world. Our system handles this problem by closing the loop shown in Figure 6: the planner interacts with the robot using the ADE architecture and the SapaReplan execution monitor, while simultaneously providing the human user a way of interacting with it via the specification of new information and goals through OWQGs.

Handling an open environment using a closed world planner has been con-sidered before, notably in the work of Etzioni et al. [1997] via the specification of Local Closed-World (LCW) statements. However, there exists at least one major difference between their work and this attempt. We note that the repre-sentation used in that work, of closing a world that is open otherwise via the
LCW statements, is complementary to our representation. Since our interest in providing support for open world quantified goals is to relax our planner X  X  as-sumption of a world closed with respect to object creation, we are opening parts of a completely closed-world with the aid of OWQGs. This approach provides a method of specifying conditional goals , where goal existence hinges upon the truth value of facts.
 Semantics of goals involving sensing have received attention in Scherl and
Levesque [1993] and Golden and Weld [1996]. The latter work is particularly relevant as they consider representations that leads to tractable planning, and propose three annotations initially , hands-off and satisfy to specify goals involving sensing. A conditional goal A B will translate, in their no-tation, to initially ( A )  X  satisfy ( B ). Conditional goals require sensing the antecedent X  X  truth in the initial state to decide whether to pursue the reward offered by the consequent. In this sense, they are inherently temporal (a point noted also by Golden and Weld [1996]). There has been significant work on  X  X emporal goals X  [Bacchus and Kabanza 1996; Baral et al. 2001], and  X  X rajec-tory constraints X  [Gerevini et al. 2009].

An important difference, however, is that earlier work focused on problems with completely known initial states (where, as we saw in Section 3.1, these richer goals can be compiled down to goals of achievement). Our interest is on handing conditional goals with incomplete information about initial states (as is the norm in open world scenarios we deal with). Here, conditional goals present interesting trade-offs between goal rewards and sensing costs.
On the  X  X lanners interacting with humans X  side, there have been some plan-ning systems that work toward accepting input from users. In particular, work by Myers [1996] has dealt specifically with advisable planning (i.e., allowing a human to specify partial plans, recommendations of goals and actions, or methods to evaluate plan quality; all in natural language). The Continuous
Planning and Execution framework, also developed by Myers [1998], con-tained such a framework allowing natural language advice. This system pro-vided for plan execution monitoring and initiated plan repairs when necessary (though appears to have never handled fully open world scenarios). Another system that relies on high-level advice from a human is TRAINS-95 [Fergu-son et al. 1996]. This system engages the human in a dialog, explicitly elic-iting advice from the user and asking for the best way to complete tasks at the high level, while the planner engages in planning using more primitive actions. 7. CONCLUSION
In this article, we focused on the challenges of adapting planning technology to applications involving human-robot teaming. Our motivating problem is an
Urban Search And Rescue (USAR) scenario where a human is in remote contact with an autonomous robot and provides high-level instructions to it. We noted that several aspects of state-of-the-art planning technology, such as temporal planning and partial satisfaction planning, can be imported out-of-the-box; in particular we use SapaReplan as the base planner. We then showed that the teaming problem also presents a critical challenge, namely the need to handle open worlds. Given that existing planners operate under closed world assump-tions, we had to focus on effective ways of enabling them to handle open world requirements. Of particular interest in the USAR scenario is the fact that the most important goals (reporting on wounded people) are  X  X onditional X , in that the planner and the robot do not know where the injured people are. To cap-ture this, we investigated the general notion of conditional goals, and showed how they foreground the trade-offs between goal reward and sensing cost. We then developed an approach to handle a specific form of conditional goals called open world quantified goals. We discussed the details of integrating the plan-ner and robot, and presented an empirical evaluation of the effectiveness of our solution.

A fruitful line of extension for this work is to handle open world quanti-fied goals more generally (refer to Section 3.1) without the optimistic sensing assumption. We believe that sampling-based planning techniques such as hind-sight optimization [Yoon et al. 2008] and anticipatory planning [Hubbe et al. 2008] would be useful in better balancing sensing costs with expected reward.
We are also considering methods of performing domain analysis to determine what objects should be attended to by the DIARC architecture before plan execution begins.

Finally, although we focused only on conditional goals, open worlds also present challenges involving counterfactual knowledge. For example, whereas a statement such as  X  X  door implies a room X  can be evaluated and used at  X  X lan time X  in classical planning scenarios, in open worlds, where the full map may not be known, the existence of a door can only be sensed during execution. Any use of this knowledge during planning will make the plan  X  X peculative X  in that its robustness is subject to the outcomes of sensing during execution. It would thus be worthwhile to develop both representations and planning methods for handling such counterfactual domain knowledge. OWQGs already provide rudimentary representation support.

Indeed, as we mention at the end of Section 3.2,  X  If G is empty, that is, there is no new goal to work on, the OWQG Q can be seen simply as additional knowl-edge that might help in reasoning about other goals.  X  Such goalless OWQGs can be seen as sensing-based domain axioms, in that they allow the planner to deduce more world facts based on sensing results. The presence of such knowledge at planning time allows the planner to make counterfactual plans whose success is predicated on specific sensing results. A naive planner might consider such counterfactual plans to be on par with normal ones, and pick the least expensive. The problem with this approach is that the least expensive plan may be  X  X ishful X  in that it is predicated on sensing results that are un-likely. Generating robust plans in such scenarios is a challenge that we hope to address in future work.

We thank W. Cushing for helpful discussions and the development of SapaRe-plan , and the TIST reviewers for valuable suggestions on the organization of the article.

