 this proposal (e.g., [5].) data. Two important examples come to mind: regardless of the assumed rate for the statistical estimati on error. 2.1 Setup y expected risk that is, training examples ( x finding the function f
F = arg min f  X  X  E ( f ) We can then decompose the excess error as error E estimation error E the number of examples [7, 8]. 2.2 Optimization Error Finding f Since the empirical risk E iterative optimization algorithm long before its converge nce. Let us assume that our minimization algorithm returns an app roximate solution  X  f where  X   X  0 is a predefined tolerance. An additional term E in the decomposition of the excess error E = E E (  X  f 2.3 The Approximation X  X stimation X  X ptimization Tradeoff  X  the three variables F , n , and  X  increase.
 constraint n &lt; n tradeoff with sufficient generality, this section makes sev eral simplifications: several optimization algorithms. 3.1 Convergence of the Estimation and Optimization Errors The optimization error E  X  involves the empirical quantity E its expected counterpart E (  X  f error E convergence concepts pioneered by Vapnik and Chervonenkis (e.g., [2].) letter c do not necessarily imply that the constants have identical v alues. 3.1.1 Simple Uniform Convergence Bounds results then state that immediately provides a bound on the estimation error: sophisticated bounds are required. 3.1.2 Faster Rates in the Realizable Case convergence bounds state that case , that is when  X  ( f E Viewing this as a second degree polynomial inequality in var iable q E (  X  f rate of the combined estimation and optimization error: 3.1.3 Fast Rate Bounds These bounds have the general form This result holds when one can establish the following varia nce condition: a variance condition. bound of the form following result: See also [15, 4] for more bounds taking into account the optim ization accuracy. 3.2 Gradient Optimization Algorithms correspond to the functions f  X  functions w 7 X   X  ( f Convexity ensures that the empirical const function C ( w ) = E ance matrix G , both measured at the empirical optimum w them, we assume that there are constants  X  with probability greater than 1  X   X  : The condition number  X  =  X  The condition  X  to iteratively update their current estimate w ( t ) of the parameter vector. Legend : n number of examples; d parameter dimension;  X  ,  X  see equation (10).
Algorithm Cost of one Iterations Time to reach Time to reach
GD O ( nd ) O  X  log 1 2GD O d 2 + nd O log log 1
SGD O ( d )  X  X  2 2SGD O d 2  X  probability of their complement is smaller than  X  for any  X  &gt; 0 . The fourth column bounds the time necessary to reduce the exc ess error E below c ( E is the constant from (6). This is computed by observing that c hoosing  X   X  ` d the best excess error achieved by each algorithm within the limited time T our assumptions.
 In contrast, since the optimization error E of their estimation procedure. algorithms.
 non-trivial impact in the large-scale case.
 Acknowledgments Part of this work was funded by NSF grant CCR-0325463.
