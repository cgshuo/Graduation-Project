 Abstract The role of the Web for text corpus construction is becoming increas-ingly significant. However, the contribution of the Web is largely confined to building a general virtual corpus or low quality specialised corpora. In this paper, we introduce a new technique called SPARTAN for constructing specialised cor-pora from the Web by systematically analysing website contents. Our evaluations show that the corpora constructed using our technique are independent of the search engines employed. In particular, SPARTAN-derived corpora outperform all corpora based on existing techniques for the task of term recognition.
 Keywords Corpus construction Specialised corpus Web-derived corpus Virtual corpus Website ranking Boilerplate removal Term recognition 1 Introduction Broadly, a text corpus is considered as any collection containing more than one text of a certain language. A general corpus is balanced with regard to the various types of information covered by the language of choice (McEnery et al. 2005 ). In contrast, the content of a specialised corpus , also known as domain corpus , is biased towards a certain sub-language. For example, the British National Corpus (BNC) is a general corpus designed to represent modern British English. On the other hand, the specialised corpus GENIA contains texts solely in the molecular biology domain. Several connotations associated with text corpora including size, representativeness, balance and sampling have been the main topics of ongoing debate within the field of corpus linguistics . In reality, great manual effort is required to construct and maintain text corpora to satisfy these connotations. Although these curated corpora do play a significant role, several related inadequacies such as the inability to incorporate frequent changes, the rarity of traditional corpora for certain domains, and the limited corpus size have hampered the development of corpus-driven applications in knowledge discovery and information extraction.
 The increasingly accessible, diverse and inexpensive information on the World Wide Web (the Web) has attracted the attention of researchers who are in search of alternatives to manual construction of corpora. Despite issues such as the poor reproducibility of results, noise, duplicates and sampling, many researchers (Cavaglia and Kilgarriff 2001 ; Kilgarriff and Grefenstette 2003 ; Baroni and Bernardini 2006 ; Sharoff 2006 ; Fletcher 2007 ) agreed that the vastness and the diversity of the Web remains the most promising solution to the increasing need for very large corpora. Current work on using the Web for language processing can be broadly grouped into (1) the Web as a distributed source of data accessible via search engines, also known as virtual corpus (Halliday et al. 2004 ), and (2) the Web as a source of data for constructing locally-accessible corpora known as Web-derived corpora . The contents of a virtual corpus are distributed over heterogeneous servers, and accessed using URLs and search engines. It is not difficult to see that these two types of corpora are not mutually exclusive, and that a Web-derived corpus can be straightforwardly constructed using the URLs from the corresponding virtual corpus. The choice between these two types of corpora then becomes a question of trade-off between effort and control. On the one hand, applications that require stable counts and complete access to texts for processing and analysis can opt for Web-derived corpora. On the other hand, in applications where large corpus size supersedes any other concerns, a virtual corpus alone suffices.

The current state-of-the-art techniques focus mainly on the construction of Web-derived corpora, ranging from the simple query-and-download approach using search engines (Baroni and Bernardini 2004 ), to the more ambitious custom Web crawlers for very large collections (Liu and Curran 2006 ; Renouf et al. 2007 ). BootCat (Baroni and Bernardini 2004 ) is a widely-used toolkit to construct specialised Web-derived corpora . This technique simply downloads webpages returned by search engines without further analysis. Sharoff ( 2006 ) extended the use of BootCat to construct a large general Web-derived corpus using 500 seed terms. This technique requires a large number of seed terms (in the order of hundreds) to produce very large Web-derived corpora, and the composition of the corpora may vary depending on the search engines used. Instead of relying on search engines and seed terms, Liu and Curran ( 2006 ) constructed a very large general Web-derived corpus by crawling the Web using seed URLs. In this approach, the lack of control and the absence of further analysis cause topic drift as the crawler traverses further away from the seeds. It is obvious that the widely-adopted techniques in this area lack the systematic analysis of website contents during corpus construction. Some of these techniques simply rely on search engines to dictate which webpages are suitable for the domain while others allow their Web crawlers to run astray without systematic controls.
 We propose a technique, called SP ecialised Corpor A const R uction based on web T exts AN alysis (SPARTAN) to automatically analyse the content of websites to discover domain-specific texts to construct very large specialised corpora. The first part of our technique analyses the domain representativeness of websites for discovering specialised virtual corpora. The second part selectively localises the distributed contents of websites in the virtual corpora to create specialised Web-derived corpora. In this technique, virtual corpora are regarded as intermediate outputs towards the creation of Web-derived corpora. This technique can also be employed to construct BNC-style balanced corpora through stratified random sampling from a balanced mixture of domain-categorised Web texts. Our experiments show that unlike BootCat-derived corpora which may vary across different search engines, our technique is independent of the search engine employed. Instead of indiscriminately using the results returned by search engines, our systematic analysis allows the most suitable websites and their content to surface and to contribute to the specialised corpora. This systematic analysis significantly improves the quality of our specialised corpora compared to BootCat-based corpora, and the naive S eed-RE stricted Q uerying (SREQ) of the Web. This is verified using the term recognition task. The main contributions of this paper are:  X  a technique to construct very large, high quality specialised text corpora using  X  the use of systematic content analysis to re-rank websites based on their domain  X  processes for extending user-provided seed terms and localising domain-
This paper is structured as follows. In Sect. 2 , we summarise current work on corpus construction. In Sect. 3 , we outline our specialised corpora construction technique. In Sect. 4 , we evaluate the specialised corpora constructed using our technique in the context of term recognition. We end this paper with an outlook to future work in Sect. 5 . 2 Related research The process of constructing corpora using data from the Web generally comprises webpage sourcing , and relevant text identification , which is discussed in Sect. 2.1 and 2.2 , respectively. In Sect. 2.3 , we outline several studies demonstrating the significance of search engine page counts in natural language applications to offset concerns over their inconsistencies.
 2.1 Webpage sourcing Currently, there are two main approaches for sourcing webpages to construct Web-derived corpora, namely, using seed terms for querying search engines (Baroni and Bernardini 2004 ; Fletcher 2007 ), and using seed URLs for guiding custom crawlers (Liu and Curran 2006 ; Resnik and Smith 2003 ).

The first approach is popular amongst current corpus construction practices due to a toolkit known as BootCat (Baroni et al. 2006 ). BootCat requires several seed terms as input, and formulates queries as conjunctions of randomly selected seeds for submission to Google or Yahoo! search engines. The method then gathers the webpages listed in Google X  X  search result to create a specialised corpus. There are several shortcomings related to the construction of large corpora using this technique:  X  First, different search engines employ different ranking algorithms and criteria to  X  Second, the aim of creating very large Web-derived corpora using this technique  X  Third, to overcome issues related to inadequate seed terms for creating very In a similar approach, Fletcher ( 2007 ) used the most frequent words in BNC and Microsoft X  X  Live Search to construct a very large BNC-like corpus from the Web. Fletcher also discussed the reasons behind his choice of Live Search, which include generous query allowance, higher quality search results, and a better response to changes on the Web.

Several concerns related to the use of search engines including unknown algorithms to sort search results (Kilgarriff 2007 ) and restrictions on the amount of data that can be obtained (Baroni and Ueyama 2006 ) have become targets of critics in the recent years. This indirectly encouraged the wider use of custom crawlers based on seed URLs for gathering webpages to construct corpora. Some of the current work based on custom crawlers includes a general corpus of 10 billion words downloaded from the Web based on seed URLs from dmoz.org by Liu and Curran ( 2006 ). Similarly, Renouf et al. ( 2007 ) developed a Web crawler to find a large subset of random texts from the Web using seed URLs from both human experts and dmoz.org as part of the WebCorp 2 project. Ravichandran et al. ( 2005 ) demonstrated the use of randomised algorithms to generate noun similarity lists from very large corpora. The authors used URLs from dmoz.org as seed links to guide their crawlers to download 70 million webpages. After boilerplate and duplicates removal, their corpus is reduced to approximately 31 million documents. Rather than sampling URLs from online directories, Baroni and Ueyama ( 2006 ) used search engines to obtain webpage URLs for seeding their custom crawlers. The authors used combinations of frequent Italian words for querying Google, and retrieved a maximum of 10 pages per query. A resulting 5,231 URLs were used to seed breadth-first crawling to obtain a final 4 million-document Italian corpus. The approach of custom crawling is not without its shortcomings. This approach is typically based on the assumption that webpages of one domain tend to link to others in the same domain. It is obvious that the reliance on this assumption alone without explicit control will result in topic drift, which is not a desirable outcome when constructing specialised corpora. Moreover, most authors do not provide explicit statements to address important issues such as selection policy (e.g. when to stop the crawl, where to crawl next), and politeness policy (e.g. respecting the robot exclusion standard, how to handle disgruntled webmasters due to the extra bandwidth). This trend of using custom crawlers, exemplified by the words of Baroni and Ueyama ( 2006 ) X  X  ... the only viable long term approach to constructing Web corpora is for linguists to perform their own crawls ...  X  X , calls for careful planning and justification. Issues such as cost-benefit analysis, hardware and software requirements, and sustainability in the long run have to be considered. Moreover, poorly-implemented crawlers are a nuisance on the Web, consuming bandwidth and clogging networks at the expense of other netizens (Thelwall and Stuart 2006 ).

In fact, the concerns about unknown ranking and other restrictions by search engines, often placed on the center stage by critics, expose the inadequacies of certain existing techniques for constructing Web-derived corpora. These so-called  X  X hortcomings X  of search engines are merely mismatches in expectations. Linguists expect white box algorithms and unrestricted data access from search engine companies, something that is almost impossible to obtain. Obviously, these issues do pose certain challenges in our quest for very large corpora, but should we totally avoid search engines and disregard their integral role on the Web? If so, would we risk missing the forest just for these few trees? The quick alternative, which is infesting the Web with more crawlers, poses even greater challenges. Rather than reinventing the wheel, we should think of how existing corpus construction techniques can be improved using existing large search engine repositories out there. 2.2 Relevant text identification The process of identifying relevant texts, which usually comprises webpage filtering and content extraction , is an important step after the sourcing of webpages. A filtering phase is fundamental in identifying relevant texts since not all webpages returned by search engines or custom Web crawlers are suitable for specialised corpora. This phase, however, is often absent from most of the existing techniques such as BootCat. The commonly used techniques include some kind of richness or density measures with thresholds. For instance, Kida et al. ( 2007 ) constructed domain corpora by collecting the top 100 webpages returned by search engines for each seed term. As a way of refining the corpora, webpages containing only a small number of user-provided seed terms are excluded. Agbago and Barriere ( 2005 ) proposed a knowledge-richness estimator that takes into account semantic relations to support the construction of Web-derived corpora. Webpages containing both the seed terms and the desired relations are considered as better candidates to be included in the corpus. The candidate documents are ranked and manually filtered based on several term and relation richness measures.

In addition to webpage filtering, content extraction (i.e. boilerplate removal) is necessary to remove HTML tags and boilerplate (e.g. texts used in navigation bars, headers, disclaimers). HTMLCleaner by Girardi ( 2007 ) is a boilerplate remover based on the heuristics that content-rich sections of webpages have longer sentences, lower number of links, and more function words compared to the boilerplate. Evert ( 2008 ) developed a boilerplate stripper called NCLEANER based on two character-level n-gram models. A text segment is considered as a boilerplate and discarded if the  X  X irty X  model (based on texts to be cleaned) achieves a higher probability compared to the  X  X lean X  model (based on training data). 2.3 Variability of search engine counts Much work has been done in an attempt to discredit the use of search engines by demonstrating the arbitrariness of page counts. The fact remains that page counts are merely estimations (Liberman 2005 ). We are not here to argue otherwise. However, for natural language applications that deal mainly with relative frequencies, ratios and ranking, page count variations have been shown to be insignificant. Nakov and Hearst ( 2005 ) conducted a study on using page counts to estimate n-gram frequencies for noun compound bracketing. They showed that the variability of page counts over time and across search engines do not significantly affect the results of their task. Lapata and Keller ( 2005 ) examined the use of page counts for several NLP tasks such as spelling correction, compound bracketing, adjective ordering and prepositional phrase attachment. The authors concluded that for the majority of the conducted tasks, simple and unsupervised techniques perform better when n-gram frequencies are obtained from the Web. This is in line with the study by Turney ( 2001 ) which showed that a simple algorithm relying on page counts outperforms a complex method trained on a smaller corpus for synonym detection. Keller et al. ( 2002 ) used search engines to estimate frequencies for predicate-argument bigrams. They demonstrated the high correlations between search engines page counts and frequencies obtained from balanced, carefully edited corpora such as the BNC. Similarly, experiments by Blair et al. ( 2002 ) showed that search engine page counts were reliable over a period of 6 months, and highly consistent with those reported by several manually-curated corpora including the Brown Corpus (Francis and Kucera 1979 ).

In short, we can safely conclude that page counts from search engines are far from accurate and stable (Liberman 2005 ). Moreover, due to the inherent differences in their relevance ranking and index sizes, the page counts provided by the different search engines are not comparable. However, adequate studies have been conducted to show that n-gram frequency estimations obtained from search engines indeed work well for a certain class of applications. As such, we should stop focusing on the primitive issue of unstable page count and instead, find ways to make good use of what is available. The key question now is not whether search engine counts are stable or otherwise, but rather, how they can be used. 3 Analysis of website contents for corpus construction It is apparent from our discussion in Sect. 2 that the current techniques for constructing corpora from the Web using search engines can be greatly improved. In this section, we address the question of how corpus construction can benefit from the current large search engine indices despite several inherent mismatches in expectations. Due to the restrictions imposed by search engines, we only have access to a limited number of webpage URLs (Kilgarriff 2007 ). As such, the common BootCat technique of downloading  X  X ff-the-shelf X  webpages by search engines to construct corpora is not the best approach since (1) the number of webpages provided is inadequate, and (2) not all contents are appropriate for a domain corpus (Baroni and Ueyama 2006 ). Moreover, (3) the authoritativeness of webpages has to be taken into consideration in order to eliminate low-quality contents from questionable sources.
 Putting into consideration these problems, we have developed a PRO babilistic S ite s E lector (PROSE) to re-rank and filter websites returned by search engines for the purpose of constructing virtual corpora. We will discuss in detail this analysis mechanism in Sect. 3.1 and 3.2 . In addition, Sect. 3.3 outlines the S eed T erm E xpansion P rocess (STEP) , the S elective LO calisation P rocess (SLOP) , and the HE u R istic-based C leaning U ti L ity for w E b text S (HERCULES) designed to construct Web-derived corpora from virtual corpora to address the need to access local texts by certain natural language applications. An overview of the proposed technique is shown in Fig. 1 . A summary of the three phases in SPARTAN is as follows: Input  X  A set of seed terms, W  X f w 1 ; w 2 ; ... ; w n g .
 Phase 1: Website Preparation  X  Gather the top 1,000 webpages returned by search engines containing the seed  X  Generalise the webpages to obtain a set of website URLs, J .
 Phase 2: Website Filtering  X  Obtain estimates of the inlinks, number of webpages in the website, and the  X  Analyze the domain representativeness of the websites in J using PROSE.  X  Select websites with good domain representativeness to form a new set J 0 . These Phase 3: Website Content Localisation  X  Obtain a set of expanded seed terms, W X , using Wikipedia through the STEP  X  Selectively download contents from websites in J 0 based on the expanded seed  X  Extract relevant contents from the downloaded webpages using HERCULES.
 Output  X  A specialised virtual corpus consisting of website URLs with high domain  X  A specialised Web-derived corpus consisting of domain-relevant contents 3.1 Website preparation During this initial preparation phase, a set of candidate websites to represent the domain of interest, D , is generated. Methods such as random walk and random IP address generation have been suggested to obtain random samples of webpages (Henzinger and Lawrence 2004 ; O X  X eill et al. 2001 ). Such random sampling methods may work well for constructing general or topic-diverse corpora from the Web if conducted under careful scrutiny. For our specialised corpora, we employ purposive sampling instead to seek items (i.e. websites) belonging to a specific, predefined group (i.e. domain D ). Since there is no direct way of deciding if a website belongs to domain D , a set of seed terms W  X f w 1 ; w 2 ; ... ; w n g is employed as the determining factor. Next, we submit queries to the search engines for webpages containing the conjunction of the seed terms W . The set of webpage URLs, which contains the purposive samples that we require, is returned as a result. At this moment, only webpages in the form of HTML files or plain text files are accepted. Since most search engines only serve the first 1,000 documents, the size of our sample is no larger than 1,000. We then process the webpage URLs to obtain the corresponding domain names of the websites. In other words, only the segment of the URL beginning from the scheme (e.g. http:// ) until the authority segment of the hierarchical part is considered for further processing, which we term as the site URL. For example, in the URL http://www.web.csse.uwa.edu.au/research/areas/ , only the segment http://www.web.csse.uwa.edu.au/ is applicable. This collection of distinct websites (i.e. collection of site URLs), represented using the notation J will be subjected to re-ranking and filtering in the next phase.

We selected websites as the basic unit for analysis instead of the typical webpages for two main reasons. Firstly, websites are typically collections of related webpages belonging to the same theme. 3 This allows us to construct a much larger corpus using the same number of units. For instance, assume that a search engine returns 1,000 distinct webpages belonging to 300 distinct websites. In this example, we can construct a corpus comprising of at most 1,000 documents using a webpage as a unit. However, using a website as a unit, we would be able to derive a much larger 90,000-document corpus, assuming an average of 300 webpages per website. Secondly, the fine granularity and volatility of individual webpages makes analysis and maintenance of the corpus difficult. Considering that 0.25 X 0.5% webpages dissappear every week (Fetterly et al. 2003 ), virtual corpora based on webpage URLs are extremely unstable and require constant monitoring as pointed out by Kilgarriff ( 2001 ) to replace offline sources. Virtual corpora based on websites as units are far less volatile. This is especially true if the virtual corpora are composed of highly authoritative websites. 3.2 Website filtering In this section, we describe our probabilistic website selector called PROSE for measuring and determining the domain representativeness of candidate websites in J . The domain representativeness of a website is determined using PROSE based on the following criteria introduced by Wong et al. ( 2008a ):  X  The extent to which the vocabulary covered by a website is inclined towards  X  The extent to which the vocabulary of a website is specific to domain D ; and  X  The authoritativeness of a website with respect to domain D .

The websites from J which satisfy these criteria are considered as sites with good domain representativeness, denoted as set J 0 . The selected sites in J 0 form our virtual corpus. For the next three subsections, we will discuss in detail the notations involved, the means to quantify the three criteria for measuring domain represen-tativeness, and the ways to automatically determine the selection thresholds. 3.2.1 Notations Each site u i 2 J has three pieces of important information, namely, an authority rank, r i , the number of webpages containing the conjunction of the seed terms in W , n wi , and the total number of webpages, n X i : The authority rank, r i is obtained by ranking the candidate sites in J according to their number of inlinks. More inlinks indicate higher ranks which in turn is represented by smaller numerical values of r i (e.g. r i = 1 has a higher rank than r i = 2). The inlinks to a website can be obtained using the  X  X  link : X  X  operator in certain search engines (e.g. Google, Yahoo). As for the second (i.e. n wi ) and the third (i.e. n X i ) piece of information, additional queries using the operator  X  X  site : X  X  need to be performed. The total number of webpages in site u The number of webpages in site u i containing W can be obtained using the query  X  X  w site : u i  X  X , where w is the conjunction of the seeds in W with the AND operator. Figure 2 shows the distribution of webpages within the sites in J . Each rectangle divided into the collection of webpages containing seed terms W , and the collection of webpages not containing W . The size of the collection of webpages for site u i that contain W is n wi . Using the total number of webpages for the i -th site, n X i ; we estimate the number of webpages in the same site not containing W as n wi  X  n X i n wi : With the page counts n wi and n X i ; we can obtain the total page count for webpages not containing W in J as where N is the total number of webpages in J , and n w is the total number of webpages in J which contains W (i.e. the area within the circle in Fig. 2 ). 3.2.2 Probabilistic site selector A site X  X  domain representativeness is assessed based on three criteria, namely, vocabulary coverage, vocabulary specificity and authoritativeness. Assuming independence, the odds in favour of a site X  X  ability to represent a domain, defined as the Odds of Domain Representativeness ( OD ), is measured as a product of the odds for realising each individual criterion: where OC is the Odds of Vocabulary Coverage , OS is the Odds of Vocabulary Specificity , and OA is the Odds of Authoritativeness . OC quantifies the extent to which site u is able to cover the vocabulary of the domain represented by W , while OS captures the chances of the vocabulary of website u being specific to the domain represented by W . On the other hand, OA measures the chances of u being an authoritative website with respect to the domain represented by W . Next, we define the probabilities that make up these three odds.  X  Odds of Vocabulary Coverage : Intuitively, the more webpages from site u i that where | Z \ Y | = n wi is the number of webpages from the site u i containing W . We compute OC as:  X  Odds of Vocabulary Specificity : This odds acts as an offset for sites which have a where | V \ Z | = | V | = n wi . We compute OS as:  X  Odds of Authoritativeness : We first define a distribution for computing the where | J | is the number of websites under consideration, and H | J | is the | J |-th gen-eralised harmonic number computed as: We then compute OA as: 3.2.3 Selection thresholds In order to select websites with good domain representativeness, a threshold for OD is derived automatically as a combination of the individual thresholds related to OC , OS and OA : Depending on the desired output, these individual thresholds can be determined using either one of the three options associated with each probability mass function. All sites u i 2 J with their odds OD ( u i ) exceeding OD T will be considered as suitable candidates for representing the domain. These selected sites, denoted as the set J 0 , constitute our virtual corpus.

We now go through the details of deriving the thresholds for the individual odds.  X  Firstly, the threshold for OC is defined as: s C can either by P C ; P C max or , P C min . The mean of the distribution is given by: while the highest and lowest probabilities are defined as: where max P C  X  n wi  X  returns the maximum probability of the function P C  X  n wi  X  where n wi ranges over the page counts of all websites u i in J .  X  Secondly, the threshold for OS is given by: Note that P S 6  X  1 = j J j since the sum of P S  X  u i  X  for all u i 2 J is not equal to 1.  X  Thirdly, the threshold for OA is defined as: where s A can either be P A ; P A max or P A min . The expected value of the random variable X for the Zipfian distribution is defined as: and since s = 1 in our distribution of authority rank, the expected value of the variable r , can be obtained through: Using r , we have P A as: The highest and lowest probabilities are given by: where max P A  X  r i  X  returns the maximum probability of the function P A  X  r i  X  where r i ranges over the authority ranks of all websites u i in J . 3.3 Website content localisation This content localisation phase is designed to construct Web-derived corpora using the virtual corpora created in the previous phase. The three main processes in this phase are seed term expansion (STEP), selective content downloading (SLOP), and content extraction (HERCULES).

STEP uses the categorical organisation of Wikipedia topics to discover related terms to complement the user-provided seed terms. Under each Wikipedia category, there is typically a listing of subordinate topics. For instance, there is category called  X  X  Category:Blood_cells  X  X  which corresponds to the  X  X  blood cell  X  X  seed term. STEP begins by finding the category page  X  X  Category:w  X  X  on Wikipedia which corresponds to each w 2 W (line 3 in Algorithm 1 ). Under the category page  X  X  Cate-gory:Blood_cells  X  X  is a listing of the various types of blood cells such as leukocytes , red blood cell , reticulocytes , etc. STEP relies on regular expressions to scrap the category page to obtain these related terms (line 4 in Algorithm 1 ). The related topics that not all topics listed under a Wikipedia category adhere strictly to the hypernym-hyponym relation. Nevertheless, the terms obtained through such means are highly related to the encompassing category since they are determined by human contributors. These related terms can be relatively large in numbers. As such, we employed the Normalised Web Distance 4 (NWD) (Wong et al. 2007 ) for selecting the m most related ones (line 6 and 8 in Algorithm 1 ). Algorithm 1 summarises STEP. The existing set of seed terms W  X f w 1 ; w 2 ; ... ; w n g is expanded to become W X  X f W 1  X f w 1 ; ... g ; W 2  X f w 2 ; ... g ; ... ; W n  X f w n ; ... gg through this process.
SLOP then uses the expanded seed terms W X to selectively download the contents from the websites in J 0 . Firstly, all possible pairs of seed terms are obtained for every combination of sets W i and W j from W X : Using the seed term pairs in C , SLOP localises the webpages for all websites in J 0 . For every site u 2 J 0 , all pairs ( x , y )in C are used to construct queries in the form of q =  X  X  x  X  X   X  X  y  X  X  site:u . These queries are then submitted to search engines to obtain the URLs of webpages that contain the seed terms from each site. This move ensures that only relevant pages from a website are downloaded. This prevents the localising only HTML and plain text pages are considered. Using these URLs, SLOP downloads the corresponding webpages to a local repository.

The final step of content localisation makes use of HERCULES to extract contents from the downloaded webpages. HERCULES is based on the following sequence of heuristics: (1) all relevant texts are located within the &lt; body &gt; tag. (2) the contribution of invisible elements and formatting tags for determining the (3) the segmentation of relevant texts, typically paragraphs, are defined by (4) length of sentences in relevant texts are typically higher. (5) the concentration of function words in relevant texts is higher (Girardi 2007 ). (6) the concentration of certain non-alphanumeric characters such as  X  X  X  X  X  ,  X  X - X  X ,  X  X . X  X  (7) other common observations such as the capitalisation of the first character of
HERCULES begins the process by detecting the presence of the &lt; body &gt; and &lt; present, the complete HTML source code is used. Next, HERCULES removes all invisible elements (e.g. comments, javascript codes) and all tags without contents discarded. Structural tags are then used to break the remaining texts in the page into segments. The length of each segment relative to all other segments is determined. In addition, the ratio of function words and certain non-alphanumeric characters (i.e.  X  X  X  X  X ,  X  X - X  X ,  X  X . X  X ,  X  X , X  X ) to the number of words in each segment is measured. The ratios related to non-alphanumeric characters are particularly useful for further removing boilerplate such as Disclaimer | Contact Us | ... , or the reference section of academic papers where the concentration of such characters is higher than normal. Using these indicators, HERCULES removes segments which do not satisfy the heuristics 4 X 7. The remaining segments are aggregated and returned as contents. 4 Evaluations and discussions In this section, we discuss the results of three experiments conducted to assess the different aspects of our technique. 4.1 The impact of search engine variations on virtual corpus construction We conducted a three-part experiment to study the impact of the choice of search engines on the resulting virtual corpus. In this experiment, we examine the extent of correlation between the websites ranked by the different search engines. Then, we study whether or not the websites re-ranked using PROSE achieve higher levels of correlations. A high correlation between the websites re-ranked by PROSE will suggest that the composition of the virtual corpora will remain relatively stable regardless of the choice of search engines.

We performed a scaled-down version of the virtual corpus construction procedure outlined in Sects. 3.1 and 3.2 . For this experiment, we employed three major search engines, namely, Yahoo, Google and Live Search (by Microsoft), and their APIs for constructing virtual corpora. We chose the seed terms  X  X  transcription factor  X  X  a n d  X  X  blood cell  X  X  to represent the domain of molecular biology D 1 , while the reliability engineering domain D 2 is represented using the seed terms  X  X  risk management  X  X  a n d  X  X  process safety  X  X . For each domain D 1 and D 2 , we gathered the first 1,000 webpage URLs from the three search engines. We then processed the URLs to obtain the corresponding websites X  addresses. The set of websites obtained for domain D 1 using Google, Yahoo and Live Search is denoted as J 1 G ; J 1 Y and J 1 M , respectively. The same notations apply for domain D 2 . Next, these websites were assigned with ranks based on their corresponding webpages X  order of relevance determined by the respective search engines. We refer to these ranks as native ranks . If a site has multiple webpages included in the search results, the highest rank shall prevail. This ranking information is kept for use in the later part of this experiment. Table 1 summarises the number of websites obtained from each search engines for each domain.

In the first part of this experiment, we sorted the 103 common websites for D 1 , native ranks (i.e. the ranks generated by the search engines). We then determined their Spearman X  X  rank correlation coefficients. The native columns in Table 2 show the correlations between websites sorted by different pairs of search engines. The correlation between websites based on native rank is moderate, ranging between 0.45 to 0.54. This extent of correlation does not come as a surprise. In fact, this result supports our implicit knowledge that different search engines rank the same webpages differently. Assuming the same query, the same webpage will inevitably be assigned distinct ranks due to the inherent differences in the index size and the algorithm itself. For this reason, the ranks generated by search engines (i.e. native ranks) do not necessarily reflect the domain representativeness of the webpages. In the second part of the experiment, we re-rank the websites in J {1,2} C using PROSE. For simplicity, we only employed the coverage and specificity criteria to determine the domain representativeness of websites, in the form of odds of domain representativeness (OD). The information required by PROSE, namely, the number of webpages containing W , n wi , and the total number of webpages, n X i are obtained from the respective search engines. In other words, the OD of each website is estimated three times, each using different n wi and n X i obtained from the three different search engines. The three variants of estimation are later translated into ranks for re-ordering the websites. Due to the varying nature of page counts across different search engines as discussed in Sect. 2.3 , many would expect that re-ranking the websites using metrics based on such information would yield an even worse correlation. On the contrary, the significant increases in correlation between websites after re-ranking by PROSE as shown in the PROSE columns in Table 2 demonstrated otherwise.

We discuss the reasons behind this interesting finding. As we have mentioned before, search engine indices vary greatly. For instance, based on page counts by Google, we have a 15,900/23,800,000 = 0.000668 5 probability of encountering a webpage from the site http://www.pubmedcentral.nih.gov that contains the bi-gram  X  X  blood cell  X  X . However, Yahoo provides us with a higher estimate at 0.001440. This is not because Yahoo is more accurate than Google or vice versa, they are just different. We have discussed this in detail in Sect. 2.3 This re-affirms that estimations using different search engines are by themselves not comparable. Consider the next example n-gram  X  X  gasoline  X  X . Google and Yahoo provides the estimates 0.000046 and 0.000093 for the same site, respectively. Again, they are very different from one another. While these estimations are inconsistent (i.e. Google and Yahoo offer different page counts for the same n-grams), the conclusion is the same, namely, one has better chances of encountering a page in http://www.pubmedcentral.nih.gov that contains  X  X  blood cell  X  X  . In other words, estimations based on search engine counts have significance only in relation to something else (i.e. relativity). This is exactly how PROSE works. PROSE determines a site X  X  OD based entirely on its contents. OD is computed by PROSE using search engine counts. Even though the analysis of the same site using different search engines eventually produces different OD, the object of the study, namely, the content of the site, remains constant. In this sense, the only variable in the analysis by PROSE is the search engine count. Since the ODs generated by PROSE are used to compare the websites in J {1,2} C (i.e. ranking), the numerical differences introduced through vari-able page counts by the different search engines become insignificant. Ultimately, the same site analysed by PROSE using unstable page counts by different search engines can still achieve the same rank.

In the third part of this experiment, we examine the general  X  X uality X  of the websites ranked by PROSE using information provided by the different search engines. As discussed in Sect. 3.2 , PROSE measures the odds in favour of the websites X  authority, vocabulary coverage and specificity. Websites with low OD can be considered as poor representatives of the domain. The ranking of sites by PROSE using information from Google consistently resulted in the most number of websites with OD less than -6, as shown in Table 3 . About 70.13% in domain D 1 and 34.95% in domain D 2 by Google are considered as poor representatives. On the other hand, the sites ranked using information by Yahoo and Live Search have relatively higher OD. To explain this trend, let us consider the seed terms { X  X  transcription factor  X  X  ,  X  X  blood cell  X  X  X . According to Google, there are 23,800,000 webpages in http://www.pubmedcentral.nih.gov and out of that number, 1,180 contain both seed terms. As for Yahoo, it indexes far less 9,051,487 webpages from the same site but offers approximately the same page count 1,060 for the seed terms. This trend is consistent when we examined the page count for the non-related n-gram  X  X  vehicle  X  X  from the same site. Google and Yahoo report the approximately same page counts of 24,900 and 20,100, respectively. There are few possibilities. Firstly, the remaining 23,800,000 -9,051,487 = 14,748,513 indexed by Google really do not contain the n-grams, or secondly, Google overestimated the overall figure of 23,800,000. The second possibility becomes more evident as we look at the page count by other search engines. 6 Live Search reports a total page count of 61,400 for the same site with 1,460 webpages containing the seed terms { X  X  tran-scription factor  X  X  ,  X  X  blood cell  X  X  X . Ask.com, with a much larger site index at 15,600,000 has 914 pages with the seed terms. The index sizes of all these other search engines are much smaller than that of Google X  X , and yet, they provided us with approximately the same number of pages containing the seed terms. Due to the excessively high figures by Google, the significance of domain-relevant n-grams are greatly undermined when we take the relative frequency of n-grams using Google X  X  page counts. The seed terms (i.e.  X  X  transcription factor  X  X  ,  X  X  blood cell  X  X ) achieved a much lower probability at 1,180/23,800,000 = 0.000049 when assessed using Google X  X  page count as compared to the probability by Yahoo 1,060/9,051,487 = 0.000117. This explains the devaluation of domain-relevant seed terms when assessed by PROSE using information from Google, which leads to the falling of the OD of websites.

In short, Live Search and Yahoo revealed to be comparatively better search engines for the task of measuring OD by PROSE. However, the index size of Live Search is undesirably small, a problem agreed upon by other researchers such as occasionally turned off by Microsoft, and it sometimes offers illogical estimates. While this problem is present in all search engines, it is particularly evident in Live Search when site search is used. For instance, there are about 61,400 pages from http://www.pubmedcentral.nih.gov indexed by Live Search. However, Live Search reports that there are 159,000 pages in that site which contains the n-gram  X  X  tran-scription factor  X  X . For this reason, we preferred the balance between the index size and the more reasonable page counts offered by Yahoo. In addition, this study also partially demonstrates a very high chance of variation in the composition of corpora constructed using techniques such as BootCat that indiscriminately accepts webpage ranking by existing commercial search engines. 4.2 The evaluation of HERCULES We conducted a simple evaluation of our content extraction utility HERCULES using the Cleaneval development set, 7 in line with other boilerplate removal tools such as Girardi ( 2007 ). Due to some implementation difficulties as experienced by Evert ( 2007 ), the scoring program provided by Cleaneval was not used for this evaluation. Instead, we employed a text comparison module 8 written in Perl. The module, based on vector-space model, is used to compare the contents of the texts cleaned by HERCULES with the gold standard provided by Cleaneval. The module uses a rudimentary stop list to filter out common words and then the cosine similarity measure is employed to compute text similarity. The texts cleaned by HERCULES achieved a 0.8919 similarity with the gold standard, and has a standard deviation of 0.0832. This similarity reflects the  X  X  text only  X  X  score, or in other words, all markups of paragraphs, headers and lists were ignored. The relatively small standard deviation shows that HERCULES is able to consistently extract contents that meet the standard of human curators. We would like to emphasize that this score may not be comparable to the scores achieved by other cleaning tools under different circumstances (e.g. testing set, scoring software).

We have made available an online demo of HERCULES. 9 An API for HERCULES has also been released. The request URL is http://www.ontology.csse.uwa.edu.au/ research/algorithm_hercules_api.pl , and the two request parameters are key and url . The developer key can be obtained for free by contacting the authors. The results (i.e. textual content extracted from a webpage) are returned in plain text. A sample request URL is as follows: http://www.ontology.csse.u wa.edu.au/research/algorit hm_hercules_api.pl?key= DEMO&amp;url=http://news.bbc.co.uk/2 /hi/science/nature/8538060.stm . 4.3 The performance of term recognition using SPARTAN-based corpora In this section, we evaluated the quality of the corpora constructed using SPARTAN in the context of term recognition for the domain of molecular biology. We compared the performance of term recognition using several specialised corpora, namely:  X  SPARTAN-based corpora  X  the manually-crafted GENIA corpus (Kim et al. 2003 )  X  BootCat-derived corpora  X  seed-restricted querying of the Web (SREQ), as a virtual corpus
We employed the gold standard reference provided along with the GENIA corpus for evaluating term recognition. We used the same set of seed terms W = { X  X  human  X  X  , evaluation. The reason behind our choice of seed terms is simple: these are the same seed terms used for the construction of GENIA, which is our gold standard. 4.3.1 BootCat-derived corpora We downloaded and employed the BootCat toolkit 10 with the new support for Yahoo API to construct two BootCat-derived corpora. For reasons discussed in Sect. 2.1 , BootCat will not be able to construct a very large corpus using only the three seeds W= {  X  X  X uman X  X , X  X  X lood cell X  X , X  X  X ranscription factor X  X  }. To demonstrate the effect of the number of seeds on corpus construction using BootCat, the first corpus was built using only the three seeds in W , while the second corpus was based on 30 seeds. The authors Baroni and Bernardini ( 2004 ) suggested a bootstrapping phase to increase the number of seed terms by extracting more unigram seeds from the initial corpus (in our case constructed using W ) and then  X  X  comparing the frequency of occurrence of each word in this set with its frequency of occurrence in a reference corpus  X  X  (Baroni and Bernardini 2004 ). As discussed before, increasing the number of seeds using low-performance term extraction techniques will result in topic drift and low quality corpora. Instead, the additional 27 seeds were the top-ranked terms 11 extracted from our benchmark, the GENIA corpus. This way, we can be certain that the additional seeds are of the highest relevance to our domain of interest, which is molecular biology.

In regard to the first corpus, the three terms in W are used with the BootCat technique to construct a Web-derived corpus referred to as BootCat-3 comprising N = 2,493 documents with F = 6,652,809 unigrams. The corpus is constructed in the following manner. First, the default settings of three terms per tuple and ten randomly selected tuples for querying were not applied due to the small number of seeds. As such, we generated all possible combinations of all possible lengths in this experiment, except for the 1-tuple  X  X  human  X  X  due to its potential of causing topic drift. In other words, we have two 1-tuple, three 2-tuple and one 3-tuple for use. While this move may appear redundant since all webpages which contain the 3-tuple will also have the 2-tuples, we can never be sure that the same webpages will be provided as results by the search engines given their black-box nature. Second, we altered a default setting in the script by BootCat collect_urls_from_yahoo.pl to allow the first 100 results for each query to be collected. Third, using the six tuples, we obtained 3,431 distinct webpage URLs for downloading. Fourth, we employed the script by BootCat retrieve_and_clean_pages_from_url_list.pl to download and clean the webpages, producing the final corpus. The reduced number of documents downloaded as compared to the webpage URLs available is due to reasons such as invalid links and duplicate contents.
In regard to the second corpus, the ten times higher number of seeds (i.e. 30 seeds in total) resulted in 4,060 3-tuples. Using the script collect_urls_from_yahoo.pl and restricting the results returned for each query to only the first 10 hits, 12,983 distinct webpage URLs were obtained from the 4,060 3-tuples. We then employed the script retrieve_and_clean_pages_from_url_list.pl to download and clean the webpages, producing a final corpus called BootCat-30 containing N = 8,823 documents with F = 18,985,430 unigrams. By comparing BootCat-3 and BootCat-30, the ten times increase in the number of seeds contributed to a 2.85 times larger corpus (in terms of unigram count). From our records, the process of producing the final BootCat-3 and BootCat-30 corpora, including the gathering of URLs and the downloading of webpages, are approximately 12 and 36 hours respectively. The laptop computer used to construct the corpora is an Intel Core Duo T2300 1.66 GHz with 504 MB of RAM and a download speed varying between 90 -111 KB/s 12 . 4.3.2 SPARTAN-based corpora and SREQ We first constructed a virtual corpus using SPARTAN and the seed terms W . Yahoo is selected as our search engine of choice for this experiment for reasons outlined in Sect. 4.1 We employed the API 13 provided by Yahoo. All requests to Yahoo are sent to this server process http://www.search.yahooapis.comWebSearchService/ V1/webSearch?/. We format our query strings as appid=APIKEY&amp;query= SEEDTERMS&amp;results=100 . Additional options such as start=START are applied to enable SPARTAN to obtain results beyond the first 100 webpages. This service by Yahoo is limited to 5,000 queries per IP address per day. However, the implemen-tation of this rule is actually quite lenient. In the first phase of SPARTAN, we obtained 176 distinct websites from the first 1,000 webpages returned by Yahoo using the conjunction of the three seed terms. For the second phase of SPARTAN, we selected the average values as described in Sect. 3.2 for all three thresholds, namely, s ; s S and s A to derive our selection cut-off point OD T . The selection process using PROSE provided us with a reduced 43 sites. The virtual corpus thus contains about N = 84,963,524 documents (i.e. webpages) distributed over 43 websites. In this evaluation, we would refer to this virtual corpus as SPARTAN-V, where the letter V stands for virtual . We have made available an online query tool for SPARTAN-V. 14 We then extended the virtual corpus during the third phase of SPARTAN to construct a Web-derived corpus. We selected three most related topics for each seed term in W category page on Wikipedia and hence, cannot be expanded. The set of expanded seed SLOP decided on 80,633 webpage URLs for downloading. A total of 76,876 pages were actually downloaded while the remaining 3,743 could not be reached for reasons such as connection error. Finally, HERCULES is used to extract contents from the downloaded pages for constructing the Web-derived corpus. About 15% of the webpages were discarded by HERCULES due to the absence of proper contents. The final Web-derived corpus, denoted as SPARTAN-L (the letter L refers to local )is composed of N = 64,578 documents with F = 118,790,478 unigrams. We have made available an online query tool for SPARTAN-L. 15 It is worth pointing out that using SPARTAN and only 3 seeds, we can easily construct a corpus that is at least 20 times the size of a BootCat-derived corpus. From our records, the process of con-structing the final SPARTAN-V corpus on the same machine as BootCat is 18 h with 7 more hours required for localisation to produce SPARTAN-L.

Many researchers have found good use of page counts for a wide range of NLP applications using search engines as gateways to the Web (i.e. general virtual corpus). In order to justify the need for content analysis during the construction of virtual corpora by SPARTAN, we included the use of guided search engine queries as a form of specialised virtual corpus during term recognition. We refer to this virtual corpus as SREQ, the seed-restricted querying of the Web. Quite simply, we append the conjunction of the seed terms W for every query made to the search engines. In a sense, we can consider SREQ as the portion of the Web which contains the seed terms W . For instance, the normal approach for obtaining the general page count (i.e. the number of pages on the Web) for  X  X  TNF beta  X  X  is by submitting the n-gram as a query to any search engine. Using Yahoo, the general virtual corpus has 56,400 documents containing  X  X  TNF beta  X  X . In SREQ, the conjunction of the seeds in W is appended to  X  X  TNF beta  X  X , resulting in the query q= X  X  X NF beta X  X   X  X  X ranscription factor X  X   X  X  X lood cell X  X   X  X  X uman  X  X . Using this query, Yahoo provides us with 218 webpages, while the conjunction of the seed terms alone results in the page count N = 149,000. We can consider the latter as the size of SREQ (i.e. total number of documents in SREQ), while the former as the number of documents in SREQ which contains the term  X  X  TNF beta  X  X  . 4.3.3 GENIA corpus and the preparations for term recognition In this section, we evaluate the performance of term recognition using the different corpora discussed in Sects. 4.3.1 and 4.3.2 . Terms are content-bearing words which are unambiguous, highly specific and relevant to a certain domain of interest. Most existing term recognition techniques identify terms from among the candidates through some scoring and ranking mechanisms. The performance of term recognition is heavily dependent on the quality and the coverage of the text corpora. Therefore, we find it appropriate to use this task to judge the adequacy and applicability of both SPARTAN-V and SPARTAN-L in real-world applications. The term candidates and gold standard employed in this evaluation comes with the GENIA corpus (Kim et al. 2003 ). The term candidates were extracted from the GENIA corpus based on the readily-available part-of-speech and semantic mark-up. A gold standard, denoted as the set G , was constructed by extracting the terms which have semantic descriptors enclosed by cons tags. For practicality reasons, we randomly selected 1,300 term candidates for evaluation, denoted as T . We manually inspected the list of candidates and compared them against the gold standard. Out of the 1,300 candidates, 121 are non-terms (i.e. misses) while the remaining 1,179 are domain-relevant terms (i.e. hits).

Instead of relying on some complex measures, we used a simple, unsupervised technique based solely on the cross-domain distributional behaviour of words for term recognition. Our intention is to observe the extent of contribution of the quality of corpora towards term recognition without being obscured by the complexity of state-of-the-art techniques. We employed relative frequencies to determine whether a word (i.e. term candidate) is a domain-relevant term or otherwise. The idea is simple: if a word is encountered more often in a specialised corpus than the contrastive corpus, then the word is considered as relevant to the domain represented by the former. As such, this technique places even more emphasis on the coverage and adequacy of the corpora to achieve a good performance term recognition. For the contrastive corpus, we prepared a collection comprising of texts from a broad sweeping range of domains other than our domain of interest, which is molecular biology. Table 4 summarises the composition of the contrastive corpus.
The term recognition procedure is performed as follows. Firstly, we took note of the total number of unigrams F in each local corpus (i.e. BootCat-3, BootCat-30, GENIA, SPARTAN-L, contrastive corpus). For the two virtual corpora, namely, SPARTAN-V and SREQ, the total page count (i.e. total number of documents) N is used instead. Secondly, the word frequency f t for each candidate t 2 T is obtained from each local corpus. We use page counts (i.e. document frequencies), n t as substitutes for the virtual corpora. Thirdly, the relative frequency, p t for each t 2 T local). Fourthly, we evaluated the performance of term recognition using these relative frequencies. Please take note that when comparing local corpora (i.e. BootCat-3, BootCat-30, GENIA, SPARTAN-L) with the contrastive corpus, the p t based on word frequency is used. The p t based on document frequency is used for comparing virtual corpora (i.e. SPARTAN-V, SREQ) with the contrastive corpus. If the p t by a specialised corpus (i.e. BootCat-3, BootCat-30, GENIA, SPARTAN-L, SPARTAN-V, SREQ), denoted as d t , is larger than or equal to the p t by the contrastive corpus, c t , then the candidate t is classified as a term. The candidate t is classified as a non-term if d t \ c t . An assessment function described in Algorithm 2 is employed to grade the decisions achieved using the various specialised corpora. 4.3.4 Term recognition results Contingency tables are constructed using the number of false positives and negatives, and true positives and negatives obtained from Algorithm 2 . Table 5 summarises the errors introduced during the classification process for term recognition using several different specialised corpora. We then computed the precision, accuracy, F 1 and F .5 score using the values in the contingency tables. Table 6 summarises the performance metrics for term recognition using the different corpora.

First, in the context of local corpora, Table 6 shows that SPARTAN-L achieved a better performance compared to both BootCat-3 and BootCat-30. SPARTAN-L is 2.31% and 0.7% more precise compared to BootCat-3 and BootCat-30, respectively. BootCat-3, however, gave the worst recall at 64.55% amongst all other corpora included in the evaluation. The poor recall by BootCat-3 is due to its high false negative rate where true terms were not classified as terms by BootCat-3 due to its low-quality composition (e.g. poor coverage, specificity). In other words, many domain-relevant terms in the vocabulary of molecular biology were not covered by BootCat-3. This is most likely caused by the small size of the corpus that resulted from inadequate seed terms used during the construction process. In order to improve the precision of term recognition by 1.61%, a BootCat-derived corpus based on ten times the number of seeds (i.e. BootCat-30) is necessary. By exponentially extrapolating the data we have a BootCat-derived corpus based on about 3,000 seed terms may be necessary to achieve a term recognition precision of 99.21%. 16 If this conjecture turns out to be true, it would mean that the efforts required to decide on the 3,000 seeds and to send 4.5 billion search engine queries (using the corresponding 3-tuples) may actually exceed the computational cost of building the virtual SPARTAN-V. Second, in the context of virtual corpora, term recognition using the SPARTAN-V achieved the best performance across all metrics with a 99.56% precision, even outperforming the local version SPARTAN-L. An interesting point here is that the other virtual corpus, SREQ achieved a good result with precision and recall close to 90% despite the relative ease of setting up the apparatus required for guided search engine querying. For this reason, we regard SREQ as the baseline for comparing the use of specialised virtual corpus in term recognition. In our opinion, a 9% improvement in precision justifies the additional systematic analysis of website content performed by SPARTAN for creating a virtual corpus. From our experience, the analysis of 200 websites (i.e. the construction of SPARTAN-V) generally requires on average, ceteris paribus, 1 X 1.5 hours of processing time using Yahoo API on a standard 1 GHz computer with a 256 Mbps Internet connection. The ad-hoc use of search engines for accessing the general virtual corpus may work for many NLP tasks. However, the relatively poor performance by SREQ here justifies the need for more systematic techniques such as SPARTAN when the Web is used as a specialised corpus for tasks such as term recognition.

Third, comparing between virtual and local corpora, only SPARTAN-V scored a recall above 90% at 96.44%. Upon localising, the recall of SPARTAN-L dropped to 89.40%. This further confirms that term recognition requires large corpora with high vocabulary coverage, and that the SPARTAN technique has the ability to systematically construct virtual corpora with the required coverage. It is interesting to note that a large 118 million token local corpus (i.e. SPARTAN-L) matches the recall of a 149,000 document virtual corpus (i.e. SREQ). However, due to the heterogenous nature of the Web and the inadequacy of simple seed term restriction, SREQ scored 6% less than SPARTAN-L in precision. This concurred with our earlier conclusion that ad-hoc querying, as in SREQ, is not the optimal way of using the Web as specialised virtual corpora. Even the considerably smaller BootCat-3 achieved about 3.9% higher precision compared to SREQ. This shows that size and coverage (there is 46 times more documents in SREQ than in BootCat-3) contributes only to recall, which explains SREQ X  X  25.36% better recall than BootCat-3. Due to SREQ X  X  lack of vocabulary specificity, it fared the least precision at 90.44%.

Overall, certain tasks indeed benefit from larger corpora, obviously when meticulously constructed. More specifically, tasks which do not require local access to the texts in the corpora such as term recognition may well benefit from the considerably larger and distributed nature of virtual corpora. This is evident when the SPARTAN-based corpus fared 3 X 7% less across all metrics upon localising (i.e. SPARTAN-L). Furthermore, the very close F 1 score achieved by the worst performing virtual corpus (i.e. baseline SREQ) with the best performing local corpus SPARTAN-L shows that virtual corpus may indeed be more suitable for the task of term recognition. We speculate that several reasons are at play, including the ever-evolving vocabulary on the Web, and the sheer size of the vocabulary that even Web-derived corpora cannot match.

In short, in the context of term recognition, the two most important factors which determine the adequacy of the constructed corpora are coverage and specificity. On the one hand, larger corpora, even when conceived in an ad-hoc manner, can potentially lead to higher coverage, which in turn contributes significantly to recall. On the other hand, the extra efforts spent on systematic analysis lead to more specific vocabulary, which in turn contributes to precision. Most existing techniques lack focus on either one or both factors, leading to poorly constructed and inadequate virtual corpora and Web-derived corpora. For instance, BootCat has difficulty in practically constructing very large corpora, while ad-hoc techniques such as SREQ lack systematic analysis which results in poor specificity. From our evaluation, only SPARTAN-V achieved a balance F 1 score exceeding 95%. In other words, virtual corpora constructed using SPARTAN are both adequately large with high coverage and have specific enough vocabulary to achieve highly desirable term recognition performance. We can construct much larger specialised corpora using SPARTAN by adjusting certain thresholds. We can adjust s C , s S and s A to allow for more websites to be included into the virtual corpora. We can also permit more related terms to be included as extended seed terms during STEP. This will allow more webpages to be downloaded to create even larger Web-derived corpora. This is possible since the maximum pages derivable from the 43 websites are 84,963,524 as shown in Table 4 . During the localisation phase, only 64,578 webpages which is a mere 0.07% of the total, were actually downloaded. In other words, the SPARTAN technique is highly customisable to create both small and very large virtual and Web-derived corpora using only several thresholds.
 5 Conclusions The sheer volume of textual data available on the Web, the ubiquitous coverage of topics, and the growth of content have become the catalysts in promoting a wider acceptance of the Web for corpus construction in various applications of knowledge discovery and information extraction. Despite the extensive use of the Web as a general virtual corpus, very few studies have focused on the systematic analysis of website contents for constructing specialised corpora from the Web. Existing techniques such as BootCat simply pass the responsibility of deciding on suitable webpages to the search engines. Other techniques use Web crawlers to download webpages without systematic controls for specialised corpus construction. In the face of these inadequacies, we introduced a novel technique called SPARTAN which places emphasis on the analysis of the domain representativeness of websites for constructing virtual corpora. This technique also provides the means to extend the virtual corpora in a systematic way to construct specialised Web-derived corpora with high vocabulary coverage and specificity.

Overall, we have shown that SPARTAN is independent of the search engines used during corpus construction. SPARTAN performed the re-ranking of websites provided by search engines based on their domain representativeness to allow those with the highest vocabulary coverage, specificity and authority to surface. The systematic analysis performed by SPARTAN is adequately justified when the performance of term recognition using SPARTAN-based corpora achieved the best precision and recall in comparison to all other corpora based on existing techniques. Moreover, our evaluation showed that only virtual corpora constructed using SPARTAN are both adequately large with a high coverage and have specific enough vocabulary to achieve a balanced term recognition performance (i.e. highest F 1 score). Most other existing techniques lack focus on either one or both of these factors. There are two conclusions that can be drawn from our experiments, namely, (1) high performance term recognition can be achieved using automatically constructed corpora that only requires a very small number of seed terms as input, and (2) larger corpora such as SPARTAN-V, when constructed with consideration for both vocabulary coverage and specificity, can deliver the prerequisites required for producing consistent and high-quality output during term recognition.

Several additional work has been planned to further assess SPARTAN. In the near future, we hope to study the effect of corpus construction using different seed terms W . It would be interesting to examine how the content of SPARTAN-based corpora evolves over time and its effect on term recognition. We are planning to perform a test of significance to statistically confirm the performance of SPARTAN against other corpus construction techniques. We are also studying the possibility of extending the use of virtual corpora to other applications which require contrastive analysis. We will also be looking into the comparisons of SPARTAN against future techniques for constructing specialised corpora that incorporate webpage filtering. For the proposed comparison to take place now, we would have to combine an existing corpus construction technique such as BootCat with a webpage filter, which as a whole we refer to hypothetically as BootCat ? Filter. To the best of our knowledge, there is currently no other published work on systems resembling BootCat ? Filter except for SPARTAN. For this reason, the proposed SPARTAN and existing webpage filtering techniques were not compared in this paper. References
