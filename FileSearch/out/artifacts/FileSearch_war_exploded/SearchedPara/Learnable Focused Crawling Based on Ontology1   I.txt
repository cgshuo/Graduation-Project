 Due to the tremendous size of information on the Web, it is increasingly difficult to search for useful information. For this reason, it is important to develop document discovery mechanisms based on intelligent techniques such as focused crawling [4,6,7]. In a classical sense, cra wling is one of the basic techniques for building data storages. Focused crawlin g goes a step further than the classical approach. It was proposed to selectivel y seek out pages relevant to a predefined set of topics called crawling topics.

One of the main issues of focused crawling is how to effectively traverse off-topic areas and maintain a high harvest rate during the crawling process. To address this issue, an effective strategy is to apply background knowledge of crawling topics to focused crawling. Since an ontology 1 is defined as a well-organized knowledge scheme that represents high-level background knowledge with concepts and relations, ontology-based focused crawling approaches have come into research [6,8,11]. Such approaches use not only keywords, but also an ontology to scale the relevance between web pages and crawling topics.
In the ontology-based focused crawling approaches, because concept weights are heuristically predefined before being applied to calculate the relevance scores of web pages, it is difficult to acquire the optimal concept weights to maintain a stable harvest rate during the crawling process. To address this issue, a learnable focused crawling approach based on ontology is proposed in this paper. Since the ANN is particularly useful for solving problems that cannot be expressed as a series of steps, such as recognizing patterns, classifying into groups, series prediction and data mining, it is suitable to be applied to focused crawling. In our approach, an ANN (Artificial Neural Network) is constructed by using a domain-specific ontology and applied to the classification of web pages.
The outline of this paper is as follows: Section 2 reviews the related work to provide an overview of focused crawling. Section 3 describes the framework of our learnable focused crawling approach and elaborates the mechanism of relevance computation. Section 4 provides an empirical evaluation that makes a comparison with the breadth-first search crawling approach, the simple keyword-based crawling approach, and the focused crawling approach using only the domain-specific ontology. Section 5 discusses the conclusion and future work. Focused crawlers aim to search only the subset of the web related to a specific category. There are lots of methods that h ave been proposed for focused crawling. A generic architecture of foc used crawling is proposed by S. Chakrabarti [4]. This architecture includes a classifier that e valuates the relevance of a web page with respect to the focused topics and a distiller that identifies web nodes with high access points to many relevant pages within links. M. Diligenti [5] proposes a focused crawling algorithm that constructs a model named context graph for the contexts in which topically relevant pages occur on the web. This context graph model can capture typical link hierarchies in which valuable pages occur, and model content on documents that frequently occur with relevant pages. In [7], focused crawling using a context graph is improved by constructing a relevancy context graph, which can estimate the dis tance and the relevancy degree between the retrieved document and the given top ic.J.Rennie[10]proposesamachine learning oriented approach for focused crawling: the Q-learning algorithm is used to guide the crawler to pass through the off-topic document to highly relevant documents. A domain search engine, Me dicoPort [3], uses a topical crawler, Lokman crawler, to build and update a collection of documents relevant to the health and medical field.

However, most of the above approaches do not consider applying background knowledge such as ontology to focused crawling. Since an ontology is a descrip-tion of the concepts and relationships that can well represent the background knowledge, a good way to improve the harvest rate of focused crawling is to use ontology for crawling. Several focused crawling approaches based on ontology have been proposed [6,8,11].

The most prevalent focused crawling approach based on ontology is called ontology-focused crawling approach [6]. Figure 1 depicts the ontology-focused crawling framework. In this approach, the crawler exploits the web X  X  hyperlink structure to retrieve new pages by traversing links from previously retrieved ones. As pages are fetched, their outward links can be added to a list of unvisited pages, which is refer to as the crawl frontier. To judge one page is relevant to the specific topic or not, a domain-specific ontology is used to compute the relevance score. After preprocessing, entities (words occurring in the ontology) are extracted from the visited pages and counted. In this way, frequencies are calculated for the corresponding entities in visited web pages. With the entity distance being defined as the links between an entity in the ontology and the crawling topic, concept weights are calculated by a heuris tically predefined dis count factor raised to the power of the entity distance. The relevance score is the summation of concept weights multiplied by the freque ncies of corresponding entities in the visited web pages. With the relevance score, a candidate list of web pages in order of increasing priority is maintained in the priority queue. Based on the priority queue, the crawler can crawl th e relevant web pages and store them into the topic specific web pages repository.

Ehrig X  X  approach, the ontology-focused crawling approach, uses the ontology as background knowledge and applies the weights of concepts in the ontology to compute the relevance score. Since the concept weights are manually prede-fined, the relevance score can not optimally reflect relevance of concepts with the crawling topics during the whole crawling process. Compared to these ap-proaches discussed above, our method not only provides a way to apply the ontology as background knowledge for focused crawling, but also use an ANN, which quantifies the concept weights based on a set of training examples, to classify the visited web pages. The idea of combining ontology and the ANN for focused crawling has not been researched in detail until now.
 In this section, we propose the framework of our learnable focused crawling approach. Based on the framework, we describe each step in the crawling process in detail. Then, the mechanism of relevance computation is elaborated by an example. 3.1 Framework of Learnable Focused Crawling The proposed learnable focused crawling approach consists of three stages ac-cording to their distinct functions. Figure 2 depicts these three stages: data preparation stage, training stage, and crawling stage. The data preparation stage is responsible for preparing training examples that are used for the ANN con-struction. In the training stage, an ANN is trained by the training examples. In the crawling stage, web pages are visited and judged by the ANN as to whether they will be downloaded or not. Finally, the downloaded web pages will be stored in a topic specific web pages repository.
 Data Preparation Stage. In the first stage, the data preparation stage, we construct a set of training examples by crawling web pages. The weblech 1 crawler is employed to crawl web pages. The weble ch crawler is a breadth-first crawler that supports many features required to download web pages and emulate stan-dard web-browser behavior as much as possible. Given a seed URL, the weblech crawler will crawl all the web pages near to this URL without bias. After crawl-ing a collection of web pages, we classi fy the web pages relevant to the topic manually. As a result, a set of training examples is composed of positive training examples and negative training examples.
 Training Stage. All the training examples constructed in the first stage are fed to the second stage, the training stage, for training an ANN. In the train-ing stage, a set of concepts is selected from a given domain-specific ontology to represent the background knowledge of crawling topics. Those concepts are selected because they are considered to have close relationships with crawling topics. Those selected concepts are ca lled relevant concepts of crawling topics. For each web page in the training examples, we can get a list of entities (words occurring in the set of relevant concepts) by preprocessing it.

A preprocessing module includes functions of parsing web pages from html to text, making POS (Part-Of-Speech) tagging for web pages, stop words removal, and word stemming. HTML Parser 2 is employed to process the html by removing all the html tags. QTag is used to make POS tagging 3 . After preprocessing of the visited web pages, the entities (words o ccurring in the set of relevant concepts) are extracted and counted . We get the term frequencies of relevant concepts in the visited web pages.

After calculating the term frequenci es of relevant concepts, we use them to train an ANN. The ANN is initialized as a feed forward neural network with three layers (Fig. 3). The first layer is composed of a linear layer with transfer function y =  X x . The number of input nodes is decided by the number of relevant concepts. The hidden layer is composed of a sigmoid layer with transfer function y = 1 1+ e  X  x . There are four nodes in the hidden layer. Considering the ANN will be used for binary classification, the output layer is also composed of a sigmoid layer. The ANN outputs are the relevance scores of corresponding web pages. We applied the well-known Backp ropagation algorit hm to train the ANN. The training process will not stop until the root mean squared error (RMSE) is smaller than 0.01. A more detailed description of the Backpropagation algorithm can be found in [9].
 Crawling Stage. The third stage, the crawling stage, is concerned with the actual crawling and the use of the constructed ANN for classification. First, we retrieve the robots.txt information for the host either from the metadata store or directly from the host. Then we determine if the crawler is allowed to crawl the page or not. If the URL passes the check, we begin to crawl web pages and judge them relevant to the crawling topics or not. Given a domain-specific ontology, a set of concepts, which are called relevan t concepts, is selected to represent the background knowledge of crawling topics. When the crawler visits web pages, the term frequencies of relevant concepts are c alculated after preprocessing. Then the term frequencies of relevant concepts are acquired as inputs for the ANN. The ANN will determine the relativity of the visited web pages to the crawling topics by computing the relevance scores. The mechanism of relevance computation will be described in detail in the next section. If the web pages are classified as relevant to the crawling topics, we download them and save them into the topic specific web pages repository. 3.2 Mechanism of Relevance Computation The most important component within the framework is the relevance compu-tation component in the crawling stage. First, a domain-specific ontology was given as a background knowledge base in w hich the relevant concepts are selected to calculate the relevance of the web pages.
 Definition 1. (Domain-Specific Ontology) Ontology O:= { C, P, I, is-a, inst, prop } ,withasetofclassesC,asetofpropertiesP,asetofinstancesI,aclass function is-a: C  X  C(is-a( C 1 )= C 2 ) means that C 1 is a sub-class of C 2 ), a class instantiation function inst: C  X  2 I ( inst(C) = I means I is an instance of C ), a relation function prop: P  X  C  X  C ( prop(P) = ( C 1 , C 2 ) means property P has domain C 1 and range C 2 ).
 Given the domain-specific ontology, those relevant concepts in the ontology will be selected to calculate the relevance s core according to the distance between those concepts and the crawling topics.
 Definition 2. (Distance between Concepts) Distance between concepts d ( t, c i ) = k,wherek  X  N being the number of links between t and c i in ontology O; t being the crawling topic contained in ontology O; c i being concepts in ontology O, i  X  { 1,...,n } , n being the amount of concepts of ontology O.
 The distance of concepts reflects the rela tivity between concepts in ontology and the crawling topic. If d ( t, c i ) is too large, the concept c i will have low relativity with the crawling topic t . Moreover, the number of concepts that need to select from the ontology will increase exponent ially when the distanc e between concepts is becoming large. Because of that, the time complexity of crawling will become higher. To maintain a high crawling efficiency, we need to judge a suitable d ( t, c i ) for selecting a set of relevant concepts.

According to the distance between concepts, given a crawling topic t ,aset of relevant concepts c i will be selected from the domain-specific ontology. When visiting web pages, the preprocessing module parses the web pages from html to text, performs POS (Part-Of-Speech) tagging, removes stop words, and pro-cesses word stemming. After getting a list of entities (words occurring in the relevant concepts) from the visited web pages, we calculate the term frequencies of relevant concepts based on their occurrence in the visited web pages. The term frequencies are input into the ANN. The ANN will calculate the relevance score of each visited web page and decide whether or not to download the web page.

Figure 4 shows an example of the relevance computation process. The rele-vance is calculated through several steps, starting with preprocessing such as word stemming and word counting etc., followed by ANN classification and fin-ished with a relevance judgment.

The crawling topic in the example is  X  X ell X . The ontology used here is UMLS (Unified Medical Language System) ontology (Fig. 5) [2]. The UMLS integrates over 2 million names for some 900 000 concepts from more than 60 families of biomedical vocabularies, as well as 12 million relations among these concepts. The concepts relevant to the crawling topics are measured in terms of the dis-tance of the concepts, i.e., d ( t, c i ) &lt; = 3. Based on the distance of concepts, we select 38 concepts in the UMLS ontol ogy that are relevant to the crawling topic X  X  X ell X  and apply them to construct the ANN with training examples.
Note that some of the class concepts in the UMLS ontology are composite words such as  X  X ene or Genome X . With respect to these concepts, we decompose them into single concepts. For exampl e, we decompose the composite concept  X  X ene or Genome X  to concept  X  X ene X  and concept  X  X enome X . When the crawler visits web pages, the term frequencies of  X  X ene X  and  X  X enome X  are calculated.
After preprocessing the visited web pages, entities contained in relevant con-cepts such as  X  X ell X ,  X  X issue X , and so on , are searched for in the visited web pages. For each entity found, the corresponding relevant concept term frequency is counted. In this example, the concept  X  X ene X  has term frequency 6, the con-cept  X  X issue X  has term frequency 5, th e concept  X  X ell X  has term frequency 13, the concept  X  X natomical X  has term frequency 1 and so on. The concept  X  X om-ponent X  has term frequency 0, which means there is no such entity in the visited web page. All these calculated term frequencies are input into the ANN. The ANN calculate the relevance score of the visited web pages. Since the result is a  X  X ositive X  relevance judgement with the relevance score being 1, the crawler decides to download the page. We compare our approach with the standard breadth-first search crawling ap-proach, the focused crawling approach with simple keyword spotting, and Ehrig X  X  ontology-focused crawling approach [6].
Our experiment focuses on two crawling topics:  X  X ell X  and  X  X ammal X . Be-cause the UMLS ontology [2] (Fig. 5) contains abundant medical information relevant to topic  X  X ell X  and  X  X ammal X , we use it as the background knowledge for focused crawling. For evaluating our approach, we refer to the biology defini-tion of  X  X ell X  4 and  X  X ammal X  5 in Wikipedia to judge the web pages whether or not relevant to the crawling topics.

The rates at which relevant pages are eff ectively filtered ar e the most crucial evaluation criteria for focused crawling. [1,4] provides a well-established harvest rate metric for our evaluation. The harvest rate represents the fractio n of web pages crawled that satisfy the crawling target r among the crawled pages p . If the harvest ratio is high, it means the focused crawler can crawl the rel evant web pages effectively; otherwise, it means the focused crawler spends a lot of time eliminating irrelevant pages, and it may be better to use another crawler instead. Hence, a high harvest rate is a sign of a good crawling run.

With respect to the crawling topic  X  X ell X , the URL  X  X ttp://www.biology.arizona.edu/ X  is used to crawl the web pages in the data preparation stage. 100 training examples are filtered to train an ANN with 64 positive example and 36 negative examples.

In the training stage,, with the distance d ( t, c i ) &lt; = 3, 38 relevant concepts c i are selected from the UMLS ontology, such as  X  X issue X ,  X  X ene X , and so on. In the crawling stage, we start crawling with the seed URL :  X  X ttp://web.jjay.cuny.edu/acarpi/NSC/13-cells.htm X . Also, the other four crawling approached are applied to crawl web pages with the same seed URL. Af-ter crawling 1150 web pages, we get the harvest rates of corresponding crawling approaches shown in figure 6.

With respect to the crawling topic  X  X ammal X , the URL  X  X ttp://www.enchantedlearning.com/ X  is used to crawl the web pages in the data preparation stage. 120 training examples are filtered to train an ANN with 73 positive examples and 47 negative examples.

In the training stage, with distance d ( t, c i ) &lt; = 4, 56 relevant concepts c i ,are selected from the UMLS ontology such as  X  X ertebrate X ,  X  X hale X , and so on. We apply the five crawling approaches to start crawling with the seed URL:  X  X ttp://www.ucmp.berkeley.edu/mammal/mammal.html X . After crawling 980 web pages, we get the harvest rates shown in figure 7.
 Table 1 shows the average harvest rate of topic  X  X ell X  and topic  X  X ammal X . When the crawling topic is  X  X ell X , our method X  X  average harvest rate is 41 . 6% higher than the ontology-focused crawling, 87 . 1% higher than the keyword-based crawling, and 514 . 1% higher than the breadth-first crawling.
For the crawling topic  X  X ammal X , our method X  X  average harvest rate is 19 . 2% higher than the ontology-focused crawling, 45 . 6% higher than the keyword-based crawling, and 152 . 6% higher than the breadth-first crawling.

In the crawling process, harvest rate of the breadth-first crawling was lowest because it does not consider the crawlin g topics. The keyword-based crawling also performs low because it only considers the crawling topic during the crawling process.

The ontology-focused crawling and our approach have better harvest rates because they relate the crawling topics to the background knowledge base to filter irrelevant web pages. Note that all the crawling processes begin with high harvest rates, but decrease as more page s are crawled. The main reason is that the seed URL has high relevance with the crawling topics. When more pages are crawled, the ratio of irrelevant web pages get higher.

Compared to the ontology-focused crawling, our approach has better har-vest rates. It is because the ontology-focused crawling calculates the relevance score based on heuristically predefined concept weights. These predefined con-cept weights are determined by humans sub jectively. It is difficult to optimally scale the contributions of those relevant concepts in real-time. In our approach, concept weights are determined objectiv ely to describe the contribution of each relevant concept. Therefore, the harve st rate of our approach is higher during the entire crawling process. Based on the experiment results, we believe that our approach will play a promising role for focused crawling. In order to maintain a high harvest rate during the crawling process, we propose an ontology-based learnable focused crawling approach. The framework of our approach includes three stages: the data preparation stage, the training stage, and the crawling stage. Based on a domain-specific ontology, an ANN is trained by filtered training examples. The ANN is used to calculate the relevance score of visited web pages. The empirical evaluation shows that our approach outper-formed the ontology-focused crawling, br eadth-first crawling, and keyword-based crawling. Based on the experiment results, we believe that our approach improves the efficiency of the focused crawling approach, which can be employed to build a comprehensive data coll ection for a given domain.

In the future, the crawler will be applied in a larger application scenario including document and metadata disco very. In particular, our efforts will be focused on building an effective focused crawler with high crawling precision for the medical domain.
 This work was supported by the IT R&amp;D program of MIC/IITA [2005-S-083-02, Development of Semantic Service Agen t Technology for the Next Generation Web].

