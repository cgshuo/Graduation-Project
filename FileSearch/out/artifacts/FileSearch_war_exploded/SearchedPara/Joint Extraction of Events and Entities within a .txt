 Events are things that happen or occur; they in-volve entities (people, objects, etc.) who perform or are affected by the events and spatio-temporal as-pects of the world. Understanding events and their descriptions in text is necessary for any generally-applicable machine reading systems. It is also essen-tial in facilitating practical applications such as news summarization, information retrieval, and knowl-edge base construction.

The interpretation of event descriptions is highly contextually dependent. To make correct predic-tions, a model needs to account for mentions of events and entities together with the discourse con-text. Consider, for example, the following excerpt from a news report: The excerpt describes two U.S. attacks on Baghdad. The two event anchors (triggers) are boldfaced and the mentions of entities and spatio-temporal infor-mation are italicized. The first event anchor  X  X erial bombardment X  along with its surrounding entity mentions  X   X  X .S. X ,  X  X omahawk cruise missiles X , and  X  X aghdad X , describe an attack from the U.S. on Baghdad with Tomahawk cruise missiles being the weapon. The second sentence on its own contains little event-related information, but together with the context of the previous sentence, it indicates another U.S. attack on Baghdad.

State-of-the-art event extraction systems have dif-ficulties inferring such information due to two main reasons. First, they extract events and entities in separate stages: entities such as people, organiza-tion, and locations are first extracted by a named entity tagger, and then these extracted entities are used as inputs for extracting events and their argu-ments (Li et al., 2013). This often causes errors to propagate. In the above example, if the entity tagger mistakenly identifies  X  X aghdad X  as a person, then the event extractor will fail to extract  X  X aghdad X  as the place where the attack happened. In fact, previ-ous work (Li et al., 2013) observes that using previ-ously extracted entities in event extraction results in a substantial decrease in performance compared to using gold-standard entity information.

Second, most existing work extracts events in-dependently from each individual sentence, ignor-ing the rest of the document (Li et al., 2013; Judea and Strube, 2015; Nguyen and Grishman, 2015). Very few attempts have been made to incorporate document context for event extraction. Ji and Gr-ishman (2008) model the information flow in two stages: the first stage trains classifiers for event triggers and arguments within each sentence; the second stage applies heuristic rules to adjust the classifiers X  outputs to satisfy document-wide (or document-cluster-wide) consistency. Liao and Gr-ishman (2010) further improved the rule-based in-ference by training additional classifiers for event triggers and arguments using document-level infor-mation. Both approaches only propagate the highly confident predictions from the first stage to the sec-ond stage. To the best of our knowledge, there is no unified model that jointly extracts events from sen-tences across the whole document.

In this paper, we propose a novel approach that simultaneously extracts events and entities within a ing problem into three tractable subproblems: (1) learning the dependencies between a single event and all of its potential arguments, (2) learning the co-occurrence relations between events across the doc-ument, and (3) learning for entity extraction. Then we combine the learned models for these subprob-lems into a joint optimization framework that simul-taneously extracts events, semantic roles, and enti-ties in a document. In summary, our main contribu-tions are: 1. We propose a structured model for learning 2. We introduce a joint inference framework that 3. We conduct extensive experiments on the Au-We adopt the ACE definition for entities ((LDC), 2005a) and events ((LDC), 2005b):  X  Entity mention : An entity is an object or set  X  Event trigger : the word or phrase that clearly  X  Event argument : event arguments are entities
We are interested in extracting entity mentions, event triggers, and event arguments. We consider ACE entity types PER, ORG, GPE, LOC, FAC, VEH, WEA and ACE V ALUE and T IME expres-of which has its own set of semantic roles for the potential arguments. There are 35 such roles in to-tal, but we collapse 8 of them that are time-related (e.g., T IME -H OLDS , T IME -A T -E ND ) into one, be-cause most of these roles have very few training ex-amples. Figure 2 shows an example of ACE anno-tations for events and entities in a sentence. Note that not every entity mention in the sentence is in-volved in events and a single entity mention can be associated with multiple events. In this section, we describe our approach for joint extraction of events and entities within a document context. We first decompose the learning problem into three tractable subproblems: learning within-event structures, learning event-event relations, and learning for entity extraction. We will describe the probabilistic models for learning these subproblems. Then we present a joint inference framework that in-tegrates these learned models into a single model to jointly extract events and entities across a document. 3.1 Learning Within-event Structures As described in Section 2, a mention of an event con-sists of an event trigger and a set of event arguments. Each event argument is also an entity mention with an entity type. In the following, we develop a proba-bilistic model to learn such dependency structure for each individual event mention.

Given a document x , we first generate a set of event trigger candidates T and a set of entity can-didates N . 3 For each trigger candidate i  X  T , we associate it with a discrete variable t i that takes val-ues from the 33 ACE event types and a N ONE class indicating other events or no events. Denote the set of entity candidates that are potential arguments for trigger candidate i as N i . 4 For each j  X  N i , we as-sociate it with a discrete variable r ij which models the event-argument relation between trigger candi-date i and entity candidate j . It takes values from 28 semantic roles and a N ONE class indicating invalid roles. Each argument candidate j is also associated with an entity type variable a j , which takes values from 9 entity types and a N ONE class indicating in-valid entity types.

We define the joint distribution of variables t i , r the observations, which can be factorized according to the factor graph shown in Figure 2: p
X
X where  X  1 ,...,  X  5 are vectors of parameters that need to be estimated, and f 1 ,...,f 5 are different forms of feature functions which we will describe later.
Note that not all configurations of the variables are valid in our model. Based on the definitions in Section 2, each event type takes arguments with certain semantic roles. For example, the arguments of the event M ARRY can only play the roles of P
ERSON , T IME , and P LACE . In addition, a N ONE event type should not take any arguments. Similarly, each semantic role should be filled with entities with compatible types. For example, the P ERSON role type can only be filled with an entity of type PER. However, a N ONE role type can be filled with an entity of any type. To account for these compati-bility constraints, we enforce the probabilities of all invalid configurations to be zero.

Features . f 1 , f 2 , and f 4 are unary feature func-tions that depend on trigger variable t i , argument variable r ij , and entity variable a j respectively. We construct a set of features for each feature function (see Table 1). Many of these features overlap with those used in previous work (Li et al., 2013; Li et al., 2014), except for the word embedding fea-tures for triggers and the features for entities which are derived from multiple entity resources. f 3 and f 5 are pairwise feature functions that depend on trigger-argument pair ( t i ,r ij ) and argument-entity pair ( r ij ,a j ) respectively. We consider simple in-dicator functions 1 t,r and 1 r,a as features ( 1 y ( x ) equals 1 when x = y and 0 otherwise).

Training. For model training, we find the opti-mal parameters  X  using the maximum-likelihood es-timates with an L2 regularization: We use L-BFGS to optimize the training objective. To calculate the gradient, we use the sum-product algorithm to compute the exact marginals for the unary cliques t i , r ij , a j and the pairwise cliques ( t ,r ij ) , ( r ij ,a j ) . Typically the training complex-ity for graphical models with unary and pairwise cliques is quadratic in the size of the label set. How-ever, the complexity of our model is much lower than that since we only need to compute the joint distributions over valid variable configurations. De-note the number of event subtypes as T , the num-ber of event argument roles as N , the average num-ber of argument roles for each event subtype as k 1 , the average number of entity types for each event argument as k 2 , and the average number of argu-ment candidates for each trigger candidate as M . The complexity of computing the joint distribution is O ( M  X  ( k 1 T + k 2 N )) , and k 1 and k 2 are expected to be small in practice ( k 1 = 6 ,k 2 = 3 in ACE). 3.2 Learning Event-Event Relations So far we have described a model for learning struc-tures for a single event. However, the inference of the event types for individual events may depend on other events that are mentioned in the document. For example, an A TTACK event is more likely to occur with I NJURE and D IE events than with life events like M ARRY and B ORN . In order to capture this in-tuition, we develop a pairwise model of event-event relations in a document.

Our training data consists of all pairs of trigger candidates that co-occur in the same sentence or are connected by a coreferent subject/object if they are formation between these trigger pairs since they are more likely to be related.

Formally, given a trigger candidate pair ( i,i 0 ) , we estimate the probabilities for their event types ( t p where  X  is a vector of parameters and g is a feature function that depends on the trigger candidate pair and their context. We consider both trigger-specific features and relational features. For trigger-specific features, we use the same trigger features listed in Table 1. For relational features, we consider for each pair of trigger candidates: (1) whether they are con-nected by a conjunction dependency relation (based on dependency parsing); (2) whether they share a subject or an object (based on dependency pars-ing and coreference resolution); (3) whether they have the same head word lemma; (4) whether they share a semantic frame based on FrameNet. During training, we use L-BFGS to compute the maximum-likelihood estimates of  X  . 3.3 Entity Extraction For entity extraction, we trained a standard linear-chain Conditional Random Field (CRF) (Lafferty et al., 2001) using the BIO scheme (i.e., identifying the B eginning, the I nside and the O utside of the text segments). We use features that are similar to those from previous work (Ratinov and Roth, 2009): (1) current words and part-of-speech tags; (2) con-text words in a window of size 2; (3) word type such as all-capitalized, is-capitalized, and all-digits; (4) Gazetteer-based entity type if the current word matches an entry in the gazetteers collected from Wikipedia (Ratinov and Roth, 2009). In addition, we consider pre-trained word embeddings (Mikolov et al., 2013) as dense features for each word in order to improve the generalizability of the model. 3.4 Joint Inference Our end goal is to extract coherent event mentions and entity mentions across a document. To achieve this, we propose a joint inference approach that al-lows information flow among the three local models and finds globally-optimal assignments of all vari-ables, including the trigger variables t , the argument role variables r , and the entity variables a . Specifi-cally, we define the following objective: max The first term is the sum of confidence scores for in-dividual event mentions based on the parameter es-timates from the within-event model. E ( t i , r i  X  , a  X  can be further decomposed into three parts.

E ( t i , r i  X  , a  X  ) = log p + The second term is the sum of confidence scores for event relations based on the parameter estimates from the pairwise event model, where R ( t i ,t i 0 ) = log p of confidence scores for entity mentions, where D ( a j ) = log p  X  ( a j | j,x ) and p  X  ( a j | j,x ) is the marginal probability derived from the linear-chain CRF described in Section 3.3. The optimization is subjected to agreement constraints that enforce the overlapping variables among the three components to agree on their values.

The joint inference problem can be formulated as an integer linear program (ILP). To solve it ef-ficiently, we find solutions for the relaxation of the problem using a dual decomposition algorithm to be orders of magnitude faster than a general pur-pose ILP solver in practice (Das et al., 2012). It is also particularly suitable for our problem since it in-volves decompositions that have many overlapping exact solutions for all the test documents in our ex-periments and the runtime for labeling each docu-ment is only three seconds in average in a 64-bit ma-chine with two 2GHz CPUs and 8GB of RAM. It contains text documents from a variety of sources such as newswire reports, weblogs, and discussion forums. We use the same data split as in Li et al. (2013). Table 2 shows the data statistics.
We adopt the evaluation metrics for events as de-fined in Li et al. (2013). An event trigger is cor-rectly identified if its offsets match those of a gold-standard trigger; and it is correctly classified if its event subtype (33 in total) also match the subtype of the gold-standard trigger. An event argument is correctly identified if its offsets and event subtype match those of any of the reference argument men-tions in the document; and it is correctly classified if its semantic role (28 in total) is also correct. For en-tities, a predicted mention is correctly extracted if its head offsets and entity type (9 in total) match those of the reference entity mention.

Note that our approach requires entity mention candidates and event trigger candidates as input. In-stead of enumerating all possible text spans, we generate high-quality entity mentions from the k -best predictions of our CRF entity extractor (in Sec-ger extraction using the same features except for the gazetteers, and generate trigger candidates based on the k-best predictions. We set k = 50 for enti-ties and k = 10 for event triggers based on perfor-mance on the development set. They cover 92.3% of the gold-standard entity mentions and 96.3% of the gold-standard event triggers in the test set. 4.1 Results Event Extraction . We compare the proposed models W ITHIN E VENT (in Section 3.1) and J OIN -
E VENT E NTITY (in Section 3.4) with two strong baselines. One is J OINT B EAM (Li et al., 2013), a state-of-the-art event extractor that uses a structured perceptron with beam search for sentence-level joint extraction of event triggers and arguments. The other is S TAGED M AX E NT , a typical two-stage ap-proach that detects event triggers first and then event arguments. We use the same event trigger candidates and entity mention candidates as input to all the comparing models except for J OINT B EAM , because J
OINT B EAM only extracts event mentions and as-sumes entity mentions are given. We consider a re-alistic experimental setting where no gold-standard annotations are available for entities during testing. To obtain results from J OINT B EAM , we ran the ac-mentions output by our CRF-based entity extractor. F1 score for event triggers and event arguments. We can see that our W ITHIN E VENT model, which ex-plicitly models the trigger-argument dependencies and argument-role-entity-type dependencies, out-performs the MaxEnt pipeline, especially in event argument extraction. This shows that modeling the trigger-argument dependencies is effective in reduc-ing error propagation.

Comparing to the state-of-the-art event extrac-tor J OINT B EAM , the improvements introduced by W
ITHIN E VENT are substantial in both event triggers and event arguments. We believe there are two main reasons: (1) W ITHIN E VENT considers all possible joint trigger/argument label assignments, whereas J
OINT B EAM considers only a subset of the possi-ble assignments based on a heuristic beam search. More specifically, when predicting labels for token i , JointBeam considers only the K-best ( K = 4 in their paper) partial trigger/argument label con-figurations for the previous i  X  1 tokens. As the length of the sentence increases, a large amount of information will be thrown away. (2) W ITH -
E VENT models argument-role-entity-type depen-dencies, whereas J OINT B EAM assumes the entity types are given. This can cause error propagation.
J OINT E VENT E NTITY provides the best perfor-mance among all the models on all evaluation cate-gories. It boosts both precision and recall compared tages of J OINT E VENT E NTITY in allowing informa-tion propagation across event mentions and entity mentions and making more context-aware and se-mantically coherent predictions.
 We also compare the results of J OINT E VENT E N -TITY with the best known results on the ACE event extraction task in Table 4. C ROSS -DOC (Ji and Gr-ishman, 2008) performs cross-document inference of events using document clustering information, and CNN (Nguyen and Grishman, 2015) is a convo-lutional neural network for extracting event triggers at the sentence level. We see that J OINT E VENT E N -TITY outperforms both models and achieves new state-of-the-art results for event trigger and argu-ment extraction in an end-to-end evaluation setting.
Entity Extraction . In addition to extracting event mentions, J OINT E VENT E NTITY also extracts entity mentions. We compare its output with the output of a strong entity extraction baseline CRFE NTITY (de-scribed in Section 3.3). Table 5 shows the (micro-)average precision, recall, and F1 score. We see that J
OINT E VENT E NTITY introduces a significant im-provement in recall and F1. Table 6 further shows the F1 score for four major entity types PER, GPE, ORG, and TIME in ACE. The promising improve-ments indicate that joint modeling of events and en-tities allows for more accurate predictions about not only events but also entities. 4.2 Error Analysis Table 7 divides the errors made by J OINT E VEN -
E NTITY based on different subtasks and the clas-sification error types in each task. For event trig-gers, the majority of the errors relates to missing triggers and only 3.7% involves misclassified event types (e.g., a D EMONSTRATION event is mistaken for a T RANSPORT event). Among the missing trig-gers, we examine the cases where the event types are correctly identified in a sentence but with in-correct triggers and find that there are only 5% of such cases. For event arguments, the majority of the errors relates to missing arguments and only 4.1% is about misclassified argument roles. Among the missing event arguments, 10% of them has correctly identified entity types.

In general, the errors for event extraction are com-monly due to three reasons: (1) Lexical sparsity. For example, in the sentence  X  X t least three mem-bers of a family ... were hacked to death ... X , our model fails to detect that  X  X acked X  triggers an A T -TACK event, because it has never seen  X  X acked X  with this sense during training. Using WordNet and pre-trained word vectors may alleviate the sparsity issue. It is also important to disambiguate word senses in context. (2) Shallow understanding of context, es-pecially long-range context. For example, given the sentence  X  She is being held on 50,000 dollars bail on a charge of first-degree reckless homicide ... X , the model detects that  X  X omicide X  triggers an event, but fails to detect that  X  X he X  refers to the A GENT who committed the homicide. This is mainly due to the complex long-distance dependency between the trigger and the argument. (3) Use of complex language such as metaphor, idioms, and sarcasm. Addressing these phenomena is in general difficult since it requires richer background knowledge and more sophisticated inference.

For entity extraction, we find that integrating event information into entity extraction successfully improves recall and F1. However, since the ACE dataset is restricted to a limited set of events, a large portion of the sentences does not contain any event triggers and event arguments that are of interest. For these sentences, there is little or no benefit of joint modeling. We also find that some entity misclassifi-cation errors can be avoided if entity coreference in-formation is available. We plan to investigate coref-erence resolution as an additional component to our joint model in future work. Event extraction has been mainly studied using the ACE data (Doddington et al., 2004) and biomedi-cal data for the BioNLP shared tasks (Kim et al., 2009). To reduce task complexity, early work em-ploys a pipeline of classifiers that extracts event trig-gers first, and then determines their arguments (Ahn, 2006; Bj  X  orne et al., 2009). Recently, Convolutional Neural Networks have been used to improve the pipeline classifiers (Nguyen and Grishman, 2015; Chen et al., 2015). As pipeline approaches suf-fer from error propagation, researchers have pro-posed methods for joint extraction of event trig-gers and arguments, using either structured percep-tron (Li et al., 2013), Markov Logic (Poon and Vanderwende, 2010), or dependency parsing algo-rithms (McClosky et al., 2011). However, exist-ing joint models largely rely on heuristic search to aggressively shrink the search space. One ex-ception is work in Riedel and McCallum (2011), which uses dual decomposition to solve joint infer-ence with runtime guarantees. Our work is similar to Riedel and McCallum (2011). However, there are two main differences: first, our model extracts both event mentions and entity mentions; second, it performs joint inference across sentence boundaries. Although our approach is evaluated on ACE, it can be easily adapted to BioNLP data by using appropri-ate features for events triggers, argument roles, and entities. We consider this as future work.

There has been work on improving event extrac-tion by exploiting document-level context. Berant et al. (2014) exploits event-event relations, e.g., causal-ity, inhibition, which frequently occur in biological texts. For general texts most work focuses on ex-ploiting temporal event relations (Chambers and Ju-rafsky, 2008; Do et al., 2012; McClosky and Man-ning, 2012). For the ACE domain, there is work on utilizing event type co-occurrence patterns to prop-agation event classification decisions (Ji and Grish-man, 2008; Liao and Grishman, 2010). Our model is similar to their work. It models the co-occurrence relations between event types (e.g., a D IE event tends to co-occur with A TTACK events and T RANS -PORT events). It can be extended to handle other types of event relations (e.g., causal and temporal) by designing appropriate features. Chambers and Jurafsky (2009; 2011) learn narrative schemas by linking event verbs that have coreferring syntactic arguments. Our model also adopts this intuition to relate event triggers across sentences. In addition, each event argument is grounded by its entity type (e.g., an entity mention of type PER can only fill roles that can be played by a person). In this paper, we introduce a new approach for auto-matic extraction of events and entities across a docu-ment. We first decompose the learning problem into three tractable subproblems: learning within-event structures, learning event-event relations, and learn-ing for entity extraction. We then integrate these learned models into a single model that performs joint inference of all event triggers, semantic roles for events, and entities across the whole document. Experimental results demonstrate that our approach outperforms the state-of-the-art event extractors by a large margin and substantially improves a strong en-tity extraction baseline. For future work, we plan to integrate entity and event coreference as additional components into the joint inference framework. We are also interested in investigating the integration of more sophisticated event-event relation models of causality and temporal ordering.
 This work was supported in part by NSF grant IIS-1250956, and in part by the DARPA DEFT program under contract FA87501320005. We would like to thank members of the CMU NELL group for helpful comments. We also thank the anonymous reviewers for insightful suggestions.

