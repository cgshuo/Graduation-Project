 Every year, millions of new students enter higher educational programs. Publicly available rankings of academic programs play a key role in prospective students X  decisions regarding which universities to apply to and enroll in. While surveys indicate that majority of freshmen enter college to get good jobs after graduation, established methodologies for ranking universities rely on indirect indicators of career outcomes such as reputational assessments of the universities among academic peers, acceptance and graduation rates, learning environment, and availability of research funding. In addi-tion, many of these methodologies rely on arbitrary choices of weighting factors for the different ranking indicators, and suffer from lack of analyses of statistical stability. In this paper, we addresses these challenges holistically by devel-oping a novel methodology for ranking and recommending universities for different professions on the basis of career outcomes of professionals who graduated from those schools. Our methodology incorporates a number of techniques for achieving statistical stability, and represents a step towards personalized educational recommendations based on inter-ests and ambitions of individuals. We have applied this methodology on LinkedIn X  X  Economic Graph data of over 400 million professional from around the world. The re-sulting university rankings have been made available to the public and demonstrate that there are valuable insights to be gleaned from professional career data on LinkedIn. Educational Recommendations; University Rankings; Com-pany Rankings; Statistics  X  Work done while at LinkedIn.

Millions of high school students (3 million in the US alone in 2015) apply for higher education every year. For each aspiring college student, the application process starts with selecting schools to apply to based on the student X  X  career interests and academic performance. A recent survey [16] conducted by Higher Education Research Institute on hun-dreds of thousands of entering freshman found that 88% of freshman attend college to get a good job while 81% state the desire to be very well off financially as one of their per-sonal goals. Thus, the ability to recommend schools on the basis of careers and eventually in a personalized manner has potential to provide tremendous value.

On LinkedIn.com, millions of professionals across the world enter rich information about their careers. We propose to leverage this valuable data and convert it into actionable information for LinkedIn X  X  youngest users and drive change through actionable insights at higher education institutions. In this paper, we for the first time present in full detail our novel approach to ranking and recommending universities given a choice of a profession, on the basis of career out-comes of professionals who graduated from those schools.
The notion of ranking universities in itself is not a new concept. Ranking agencies such as US News &amp; World Re-port, Times Higher Education and QS produce university lists each year -overall and by major. These rankings assess schools on the basis of indicators such as percentages of ac-cepted students who go on to enroll, graduation rates, aver-age SAT scores in addition to somewhat nebulous indicators like reputational assessments by peers at other universities. However, we believe that a more objective way to evaluate a degree program with respect to career outcomes is to mea-sure performance of its graduates in industry. We achieve this by first developing an approach for identifying most de-sirable companies for different professions. We then present a methodology for ranking universities based on the rates at which their graduates are able to obtain jobs at these desirable companies in a given profession. Such data-driven rankings are a complex data product which requires care-ful consideration for a number of statistical aspects includ-ing representation bias and statistical robustness of results. In following sections, we present our methodology and ap-proaches used for correcting potential representation biases and ensuring statistical robustness of the computed rankings with respect to potential noise in the underlying data.
Commonly adopted approaches [5, 6, 2, 1] to university rankings typically rely on a mixture of factors such as a university X  X  reputation amongst peer academics (e.g., deans, provosts and faculty), learning environment (e.g., class size), research quality, volume and funding, and student accep-tance and graduation rates. While some ranking method-ologies focus on a single factor such as quality of research output [19] or reputation among academic peers [4, 3], other methodologies combine multiple factors into a single rank-ing statistic [5, 6, 2, 1, 12, 18]. In the latter case, the rank-ing statistic is often based on a weighted combination of scores on each individual factor. For instance, US News 2016 undergraduate rankings [5] place a weight of 22.5% on Undergraduate academic reputation (as judged by surveyed academic peers and high school counselors), and 12.5% on Student selectivity, which includes application acceptance rate as a sub-factor. Another popular ranking of universi-ties produced by Times Higher Education (THE) [2] follows a similar approach to US News.

Both US News and THE methodologies are examples of approaches in which individual factor weights are assigned arbitrarily at the ranking agency X  X  discretion -a point of criticism which has also been raised by other researchers [19, 14]. Furthermore, no assessment by the ranking agen-cies is provided on (a) statistical confidence of the rankings (e.g., due to limited sample sizes, reporting errors, etc.), and (b) the degree of sensitivity of the rankings with respect to changes in the weights assigned to the individual factors The use of ad-hoc factor weightings coupled with a lack of analyses of statistical stability of the rankings and of their sensitivity to changes in the factor weightings comes at the cost of interpretability of the rankings and makes their re-liability (in some sense, trustworthiness) unclear. At the same time, research suggests that university rankings wield significant influence over prospective students X  educational choices with the strength of the influence increasing with increasing rank of a university [13]. Researchers have also begun to question the strength of the link between quality of education and the quantitative factors used in common university rankings (see [11] for a methodological critique of the major quality assessments in U.S. higher education).
In order to make university rankings robust to potential noise in the underlying data, in the current work, we inte-grate statistical stability mechanisms directly into the rank-ings methodology by making use of confidence intervals and resampling techniques as discussed in subsequent sections. Moreover, instead of attempting to combine multiple differ-ent ranking dimensions into one ranking statistic, we focus exclusively on a single ranking criterion based on career out-comes of university graduates, as will be explained in the following sections.

It is worth noting that factors such as academic reputation are prone to manipulation and feedback effects, with the ef-fects getting stronger with increasing influence of the rank-ings publisher on the general audience. Interestingly and
Unfortunately, the underlying data is not publicly avail-able, which precludes such sensitivity analyses from being conducted by independent researchers. perhaps not surprisingly, broad publicity and the potential for attracting prospective students are cited by ranking and data collection agencies as the key reasons for universities to provide the necessary data 2 . To see the potential for a feedback loop, consider that a better ranking of a univer-sity by a prominent ranking agency may lead to increases in the number of applications to that university in subse-quent years. This in turn could further increase the uni-versity X  X  reputational assessment and acceptance indicators (e.g., lower admission rates). As a result, this university X  X  competitiveness in the rankings could increase over time. In fact, [9] found evidence of strong effects on university acceptance indicators due to ranking by US News. There is also evidence suggesting that the sheer act of publish-ing a ranking can influence peer assessments of reputability of a university in subsequent surveys [8, 10]. For example, [10] found anchoring effects in peer assessments of university reputation due to the first publication of Times Higher Ed-ucation Supplement world university rankings. In addition, another study [8] found evidence of significant influence of US News rankings on future peer assessments, independent of changes in organizational quality and performance . We therefore refrain from using reputational assessment factors in our ranking methodology.

A major limitation faced by the above ranking methodolo-gies is due to the lack of comprehensive data on career out-comes of university graduates. In contrast, LinkedIn Eco-nomic Graph 3 data comprised of over 400 million profes-sional profiles is a unique dataset that provides a view of educational and career paths of individuals across the world at an unprecedented scale and granularity. Not only does this data allow us to rank universities with respect to the overall career outcomes of their graduates, the data also pro-vides the necessary information for ranking universities with respect to career outcomes in specific professions such as Software Developers, Designers, Finance Professionals, etc. This is in stark contrast to rankings focused on academic areas of study. For example, US News engineering rankings, which are based solely on reputational peer assessments [4], may not be as relevant to specific professional outcomes since according to [7] only an estimated 27% of US college gradu-ates end up working in roles related to their undergraduate majors.

There are certainly multiple facets to high quality edu-cation, with some researchers proposing multidimensional rankings for higher education [18]. Learning environment, teaching and research excellence, as well as financial returns on investment in education are important factors to con-sider when evaluating educational opportunities. Compared to university rankings based on the more traditional criteria discussed earlier in this section, the methodology presented in this work brings to light a complementary tool for inform-ing educational decisions on the basis of career outcomes of university graduates. The resulting LinkedIn university rankings based on career outcomes are available to the gen-eral public 4 . Next, we present the underlying methodology. Note that we use the terms  X  X rofession X  and  X  X areer X  inter-changeably.
See a link to a PDF at the bottom of the page at http://www.commondataset.org/ https://www.linkedin.com/company/linkedin-economic-graph https://www.linkedin.com/edu/
Our methodology for ranking universities based on career outcomes consists of two main components shown in Figure 1. First, the most desirable companies for a given profession are identified by CompanyRanker based on employee transi-tion dynamics reflected in LinkedIn member data. Universi-ties are then ranked by SchoolRanker based on how success-ful their graduates are at landing jobs at the most desirable companies in the given profession. There is a number of methodological challenges such as representation bias and robustness to potential noise in the data that are addressed in order to produce statistically sound results. We discuss these in the following subsections where we present Compa-nyRanker and SchoolRanker in detail.

We define companies desirable for a profession as those which are the best at attracting and retaining talent in that profession. We let the career choices of hundreds of millions of LinkedIn members tell us how desirable it is to work at a company.

To illustrate this, imagine there are two companies, A and B . If more finance professionals are choosing to leave company A to work at company B , the data indicates that getting a finance job at B is more desirable. This is based on the hypothesis that when a professional moves from one company to another, she gives the company she moves to a strong vote of confidence.

Extending the hypothesis further, we posit that attracting employees from other desirable companies makes a company more desirable. Similarly, the ability of a company to retain its employees is a strong indicator of that employer X  X  attrac-tiveness. So, hypothetically, if A and B are both attracting external employees at similar rates, but A has much higher churn than B , A would be deemed to be less desirable than B .

The above model can be conveniently represented using a graph structure. Given a profession and a set of LinkedIn members with their career histories, we define a Talent Flow Graph (TFG) to be a directed graph whose nodes correspond to companies and edges are weighted by the numbers of em-ployment transitions between companies. We restrict the set of eligible employment transitions to those taken by mem-bers in the profession of interest. For example, when ranking companies by desirability for software engineers (SWEs), we limit the relevant population of members to those who held a SWE role. The corresponding TFG is then formed on Figure 2: An illustration of a Talent Flow Graph for a given profession. Nodes correspond to companies. Edges are weighted based on numbers of employ-ees transitioning between companies. Variability in edge weights is depicted with different line widths. Weights on self-loops are normalized by the median tenure in the profession. Self-loops typically have higher weights than other edges leaving a node ex-cept in cases of significant lay-offs or poor retention. the basis of companies these members have worked at while in SWE roles. Companies with very few professionals con-tributing to their edges are removed from the graph in order to reduce noise. In order to make company desirability es-timates reflective of contemporary labor market conditions, only transitions during the past five years are considered.
We capture employee retention dynamics by introducing one self-loop edge for each company in a TFG. Weights on self-loop edges are normalized for median retention within the profession. This normalization allows us to effectively control for variations in tenure length across different pro-fessions. For a given profession P , the self-loop weight for a company A in a TFG is defined as P x  X  R where R P ( A ) is the set of professionals with tenure at com-pany A longer than median tenure for profession P , and t ( x,A ) equals professional x  X  X  tenure at A divided by the median tenure for P .
 A schematic illustration of a TFG is shown in Figure 2. Once a TFG is constructed, company desirability scores are determined by applying PageRank [15] on this graph. Note that besides modeling employee transition dynamics more richly, factoring in retention helps mitigate company-size reinforcement caused by the lack of inbound-edge normal-ization in PageRank.

To see this, consider a hypothetical profession with one big company A , two medium-sized companies B and C , and three small companies D , E and F . Figure 3 shows the corresponding TFG in the form of an employee transition probability matrix and in the absence of self-loops. The transition probability matrix is obtained by representing the TFG by a weighted adjacency matrix with ( i,j )-th element encoding the talent flow from company i to company j as discussed above, and with each row normalized to sum to one. For illustrative purposes, the individual probabilities are generated following the observation that bigger com-panies, besides having more open positions available, have higher brand awareness and generally attract more job appli-cations from aspirants. (Some small companies are acquired by large ones as well.) In row 2 of the transition probability matrix for example, we see that A has higher than three times the rate of attracting talent when compared to small companies ( D , E and F ) -0 . 4 as opposed to 0 . 13 -and about two times the rate when compared to medium-sized companies ( B and C ) -0 . 4 as opposed to  X 0 . 2.
The computed page rank scores show that company A scores highest with 0.297 while the medium-sized and small companies score 0.165 and 0.124 respectively.
 Figure 3: Transition probability matrix represent-ing a Talent Flow Graph without retention (self-loop edges), for a hypothetical profession. Sizes and col-ors of the circles encode company sizes from large ( A ) to medium ( B,C ), and small ( D,E,F ). Bottom row shows the corresponding PageRank score for each company from A (left) through F (right).

Now, what if the large company A was experiencing a high employee churn of, say, 20% a year as compared to 5% for the other companies? This would indicate that A may not be such a desirable company to work at and we would like our methodology to reflect this. After representing retention in the form of self-loops, the transition probability matrix and the resulting PageRank scores are shown in Figure 4. With retention factored in, company A now comes in last w.r.t. desirability with a score of 0.13 while the medium-sized companies score the highest with 0.184 due to stronger employee retention. The score for small companies is 0.167.
Note that since we construct different TFGs for different professions, given two companies A and B and professions P 1 and P 2 , it is possible for company A to score better than B for desirability for profession P 1 and score worse than B for profession P 2 .

In order to account for varying labor market dynamics and professional preferences across countries and educational de-gree levels (e.g., Bachelors, Masters, etc.), we construct a separate TFG for each (profession, country, degree level) combination. For example, the list of desirable companies for software engineers who studied in the UK will likely differ from the one for those who attended academic programs in the US. Also, the top companies for graduates of Bachelor X  X  programs might differ from those for MBA graduates. Figure 4: Transition probability matrix with re-tention factored in for the hypothetical profession example. Bottom row shows the corresponding PageRank score for each company from A (left) through F (right).
The foundational assumption underlying our approach to university rankings is that obtaining employment at some of the most desirable companies for a given profession indi-cates a favorable career outcome for a university graduate. We therefore derive a university success score based on the proportion of graduates who obtained employment in a given profession at some of the most desirable companies for that profession. However, not all graduates may be interested in pursuing the same profession. Hence, when computing a success score for a university with respect to a profession, we only consider the subpopulation of graduates who went on to obtain a position in that profession. For example, for the profession of software engineering, we consider every gradu-ate who went on to hold a software engineering role at any company post-graduation.

Note that since some university graduates may not be members of LinkedIn professional network, there may be a representation bias in the data. We correct for potential representation bias using the method of post-stratification weighting [17]. In particular, we apply externally reported university graduation data, such as published by the U.S. Department of Education 5 , to adjust the sufficient statis-tics for university success scores by stratifying on gender, graduation year, degree level and university.

Let X denote the domain of a categorical variable encod-ing all possible combinations of values of the attributes used for post-stratification weighting. For a given university, let p ( x ) denote the proportion of graduates who have the par-ticular combination of attributes represented by x  X  X  , ac-cording to data on LinkedIn. Let q ( x ) denote an externally reported proportion of graduates from the same university and with the same combination of attributes x . Then, the total bias-corrected number of graduates from this univer-sity and relevant to the profession is https://nces.ed.gov/ipeds/datacenter/ where m ( x ) denotes the number of the university X  X  graduates with attributes x on LinkedIn who entered the profession at any of the relevant companies, and w ( x ) = q ( x ) /p ( x ) is the corresponding post-stratification weight. Similarly, the bias-corrected number of graduates who obtained employment at any of the top companies is where n ( x )  X  m ( x ) ,  X  x  X  X , denotes the size of the subset of the university X  X  graduates with attributes x on LinkedIn who obtained employment at some of the top (i.e., most desirable) companies for the profession.

Finally, we define the university success score to be the lower end of the 95% Binomial confidence interval around the bias-corrected ratio of graduates who obtained employment at top companies out of all graduates who entered the profession from the university. The lower end of the confidence interval is used in order to stabilize the ranking statistic w.r.t. limited sample sizes and ground it in sustained trends in career outcomes of university graduates. Note that given the scale of LinkedIn X  X  professional member base of over 400 million members and because we consider all graduates from the last eight years when ranking universities, sample sizes for most universities ranked are large enough for the lower ends of the confidence intervals to be close to the point estimates in Eqn. 2.
The lists of top (i.e., most desirable) companies are based on the career-wise desirability scores provided by the Com-panyRanker. University success score computation by virtue of Eqn. 1, however, requires another parameter to be speci-fied -the number of companies to be considered most desir-able out of all companies ranked by CompanyRanker for the profession. We discuss the selection of the number of top companies later in this section. For now, let us assume that this number has been given, and consider the following. The representation bias alluded to earlier may also affect com-pany desirability estimation. Unfortunately, at the present time there is no comprehensive publicly available company data that would allow to correct for possible representation bias during company desirability estimation. Reporting er-rors in LinkedIn member data may also affect the results. In order to make our methodology robust to potential in-accuracies in member data and company desirability esti-mates, we (a) only use data from member profiles that have successfully passed stringent spam detection checks, and (b) develop a Monte Carlo resampling technique discussed next.
Given a ranking of companies by desirability and the num-ber K of most desirable companies to be used for comput-ing university success scores, a large number (thousands) of perturbed sets of most desirable companies are generated by repeatedly substituting a randomly chosen subset (e.g., 5-10%) of companies from the original set of K most desir-able ones with the same number of companies selected from outside that set (i.e., from those companies which didn X  X  make it into the top K by desirability). Companies from outside the most desirable set are sampled with probabili-ties proportional to their estimated desirability, so the more desirable companies have a higher chance of being placed into a perturbed set.

For each perturbed set of most desirable companies, uni-versity success scores are computed and the universities are ranked based on those scores. Thus, for each university, this procedure results in a distribution over ranks the university attained across perturbed sets of most desirable companies. The 95th percentile rank from this distribution is then taken as the ranking statistic for the university. If two or more universities attain the same 95th percentile rank, the 75th percentile rank is used to resolve the tie. Universities that tie on both 95th and 75th percentile ranks are declared tied and are assigned the same final rank. As a result, univer-sities with larger proportions of graduates obtaining jobs in the more desirable companies for a given profession, rank higher. At the same time, rankings of universities are made robust to potential noise in company desirability estimates. What remains to discuss is the determination of the number K of most desirable companies to use for ranking universi-ties.
 Selection of the number of desirable companies: As we vary the number K of the most desirable companies, the resulting university rankings may change. The degree of sensitivity of university rankings to the choice of K is a function of the distributions of university graduates across companies for a given profession. Figure 5 shows an exam-ple of the cumulative distributions of graduates from five universities over companies ranked by desirability. Figure 5: Cumulative distributions of numbers of graduates from five universities across companies ordered by desirability. Large steps in the curves indicate cases where a school feeds strongly into a single company. For example, school plotted in red would rank 4th among presented schools if top 10 companies are chosen and would rank 1st if top 25 companies are chosen.

The jumps in the distributions in Figure 5 indicate that there are cases where a school  X  X eeds X  strongly to a single company. Hence, the selection of K is important in de-termining the final university rankings. We would like to choose a K around which the changes in university ranks due to  X  X ingle-company effects X  are minimized and the rank-ings are more stable. This is achieved by a grid search over a set of values of K. For each K, we quantify and measure the average pairwise agreement between all pairs of school rankings produced as a result of Monte Carlo resampling of company rankings described above. The agreement between a pair of school rankings is defined as a weighted average of sizes of set intersections between the top N schools in the two rankings and normalized by N, for N=3, 5, 10 and 25. The value of K corresponding to the highest average pair-wise agreement is then chosen for defining the set of most desirable companies.
 Figure 6: Average pairwise agreement between uni-versity rankings across perturbed sets of most desir-able companies, as a function of the number of most desirable companies used for determining university success scores.

Figure 6 shows an example of the average pairwise agree-ment as a function of the number of most desirable compa-nies. In this example, average pairwise agreement is max-imized at fifty most desirable companies. In other words, moderate perturbations to the set of most desirable com-panies affect the resulting university rankings the least at K=50 compared to other values of K. Therefore, in this ex-ample, K=50 most desirable companies would subsequently be used for ranking universities.
Below, we present some of the results obtained by ap-plying our methodology. We discuss company desirability and university rankings for two professions in the US: the Investment Bankers and Software Developers at Startups. For illustrative purposes, Table 1 shows an alphabetically ordered list of the 20 most desirable companies generated by CompanyRanker for Investment Bankers.

It is worth noting that these companies are indeed well-recognized leaders in the investment banking industry. Ad-ditionally, we present an alphabetically ordered list of top startups for Software Developers, as of September 2014, in Table 2. Only non-public companies with less than 5,000 employees and less than 10 years in age are considered for this category. Similar to the list of most desirable com-panies for the Investment Bankers category, the list of top startups is comprised of highly regarded private companies. Note that the above lists of most desirable companies have been identified automatically by our methodology based on the transitions of employees across companies. This further highlights the great potential of LinkedIn X  X  Economic Graph that can be realized through applications of data analytic approaches.
 Table 1: Top 20 companies for Investment Bankers (in alphabetical order).
 Table 2: Top 20 startups for Software Developers (in alphabetical order)
University rankings obtained by the SchoolRanker for In-vestment Bankers and Software Developers at Startups are shown in Tables 3 and 4, resp. As was mentioned in the Methodology section, universities are ranked on the basis of their 95th percentile ranks across perturbed company sets. Ties on the 95th percentile ranks are resolved using the 75th percentile ranks. This can be seen in Table 3, where Columbia University and Duke University both have equal 95th and 75th percentile ranks and are thus considered a tie, both ranking fourth for Investment Bankers. In contrast, Table 4 shows Rochester Institute of Technology (RIT) and University of Illinois at Urbana-Champaign (UIUC) achiev-ing the same 95th percentile rank, but different 75th per-centile ranks. As a result, RIT X  X  final ranking comes out one step above UIUC X  X  for Software Developers at Startups.
The top universities for Investment Bankers have larger numbers of graduates joining some of the top investment in-stitutions such as Goldman Sachs, JPMorgan Chase &amp; Co and Morgan Stanley. Similarly, for Software Developers at Startups, larger numbers of graduates are helping build some of the top startups such as Airbnb, Dropbox, Uber and oth-ers. The complete set of university rankings together with insights about each university and career paths of their grad-uates can be found via LinkedIn University Rankings, Uni-versity Finder, and Field of Study Explorer products, which are all part of LinkedIn X  X  suite of educational decision mak-ing products 6 .

LinkedIn university rankings developed using the method-ology presented here based on career outcomes of profession-als around the world were first made publicly available in October of 2014 for undergraduate programs with a subse-quent expansion into graduate (Masters) programs in March of 2015. The published rankings cover nine professions and three countries -United States, United Kingdom and Canada. The results were viewed by millions of unique users in the first week alone and received broad media coverage includ-ing The New York Times, The Wall Street Journal, Inside HigherEd and numerous other publishers.
We have presented a novel methodology for ranking the world X  X  educational institutions on the basis of career out-https://www.linkedin.com/edu/ comes of their graduates. By leveraging LinkedIn X  X  Eco-nomic Graph data on career paths of hundreds of millions of professionals around the world, we are able to rank universi-ties with respect to individual professions -something that the leading ranking agencies such as US News and Times Higher Education are unable to achieve at a comparable scale and granularity. In contrast, these ranking agencies often fall back on reputational assessments instead of di-rectly measuring career outcomes. Our methodology ad-dresses some of the key limitations of the existing approaches such as their heavy reliance on reputational assessments and scarcity of statistical robustness mechanisms, which make the resulting rankings difficult to interpret.

We believe that the methodology we have developed re-flects more accurately the impact that universities have on industries and their graduates X  careers. Using global ca-reer data as a ground-truth for assessing career outcomes makes the resulting university rankings more interpretable and, eventually, more actionable for schools and students alike. Schools can use these rankings as a tool to fine-tune their educational and career development programs while prospective students can choose universities based on pro-fessions that excite them. In addition, current students can use the tool in combination with the Alumni Explorer 7 on LinkedIn to connect with alumni and inform the students X  career choices and job search.

Even though our methodology improves on the university ranking methodologies used in the industry, the methodol-ogy could be enhanced further by incorporating more gran-ular data on job transitions. For example, when consider-ing the flow of employees between companies when ranking companies by desirability, it is sometimes important to know https://www.linkedin.com/edu/alumni the circumstances around which the employee leaves an em-ployer. While highly desirable companies have highly com-petitive hiring processes, in some cases such companies also have stringent employee reevaluation processes. Factoring in information on job application and acceptance rates and reasons for employees X  departures would increase the pre-cision of company desirability estimates. Such data would also help further guide selection of subpopulations of gradu-ates interested in a profession when ranking universities. In addition, comprehensive salary data would be a great way to enrich our definition of career outcomes. Finally, additional data from industry experts may help enrich the definition of career outcomes and increase precision of the rankings when coupled with appropriate data analytic methods such as semi-supervised versions of PageRank.

There are multiple facets to high quality education. Learn-ing environment, teaching and research excellence, as well as financial returns on investment in education are important factors to consider when evaluating educational opportuni-ties. The methodology presented in this work brings to light a complementary tool for informing educational decisions on the basis of career outcomes of university graduates. [1] QS World University Rankings: Methodology. QS, [2] World University Rankings 2015-2016 methodology. [3] Best Undergraduate Business Programs Methodology. [4] Best Undergraduate Engineering Programs [5] How U.S. News Calculated the 2016 Best Colleges [6] Methodology: 2016 Best Business Schools Rankings. [7] J. R. Abel and R. Deitz. Agglomeration and job [8] M. N. Bastedo and N. A. Bowman. The U.S. News [9] N. A. Bowman and M. N. Bastedo. Getting on the [10] N. A. Bowman and M. N. Bastedo. Anchoring effects [11] R. Brooks. Measuring university quality. The Review [12] C. Claassen. Measuring university quality.
 [13] A. Griffith and K. Rask. The influence of the U.S. [14] S. Lee. Reputation without rigor. [15] L. Page, S. Brin, R. Motwani, and T. Winograd. The [16] J. H. Pryor, K. Eagan, L. Palucki Blake, S. Hurtado, [17] R. Valliant and J. A. Dever. Estimating propensity [18] F. A. van Vught and F. Ziegele, editors.
 [19] L. Waltman, C. Calero-Medina, J. Kosten, E. C.
