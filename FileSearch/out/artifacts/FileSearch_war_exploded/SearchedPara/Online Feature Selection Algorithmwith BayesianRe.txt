 High-throughput technologies now routinely produce large datasets characterized by unprecedented numbers of features. This s eriously undermines the performance of many data analysis algorithms in terms of the ir speed and accuracy. Accordingly, across various scientific disciplines, there has been a surge in demand for efficient feature se-lection methods for high-dimensional data. Not only can its proper design enhance clas-sification performance and reduce system com plexity, but it can also provide significant insights into the nature of the problems under investigation in many applications.
Feature selection for high-dimensional data is considered one of the current challenges in statistical machine learning [1]. Existing algorithms are traditionally cat-egorized as wrapper or filter methods, with respect to the criterion used to search for relevant features [2]. One major issue with wrapper methods is their high computational complexity. Many heuristic algorithms (e.g., forward and backward selection [3]) have been proposed to alleviate this issue. However, due to the heuristic nature, none of them can provide any guarantee of optimality. In the presence of many thousands of features, a hybrid approach is usually adopted, wherein the number of features is first reduced by using a filter method, and then a wrapper method is used on the reduced feature set. Nevertheless, it still may take several hours to perform the search, depending on the classifier used in a wrapper method. Embedded methods [4] have recently received an increasing interest. In contrast to wrapper methods, embedded methods incorporate feature selection directly into the learning process of a classifier. A feature weighting strategy is usually adopted that uses real-valued numbers, instead of binary ones, to indicate the relevance of features in a learning process. This strategy has several ad-vantages. For example, there is no need to pr e-specify the number of relevant features. Also, standard optimization techniques can be used to avoid combinatorial search. Con-sequently, embedded methods are usually computationally more tractable than wrapper methods. Yet, computa tional complexity is still a major issue when the number of fea-tures becomes excessively large.

We recently developed a new feature selec tion algorithm, referred to as LOFE (LO-cal learning based FEature selection), that addresses several major issues with existing methods [5]. The key idea is to decompose an arbitrary nonlinear problem into a set of locally linear ones through local learning, and then estimate the relevance of features globally in a large margin framework with 1 regularization. The algorithm is computa-tionally very efficient. It allows one to process many thousands of features within a few minutes on a personal computer, yet mainta ins a very high accuracy that is nearly in-sensitive to a growing number of irrelevant features. Theoretical analysis suggests that the algorithm have a logarithmical sample complexity with respect to data dimension-ality [5]. That is, the number of samples needed to maintain the same level of learning accuracy grows only logarithmically with respect to data dimensionality.

In this paper, we extend LOFE for online learning, where data arrives sequentially and the estimate of feature relevance is im proved with new data without total recalcu-lation. While it has an increasing demand in many real-time systems [6,7], the issue of online learning for feature selection is rarely addressed in the literature. We develop a new online learning algorithm by using stochastic approximation. The algorithm does not make any assumption on data distributions, and thus is applicable for general prob-lems. One major challenge in designing online learning algorithms is the estimation of model parameters, specifically the regular ization parameter in our case. Unlike batch learning where the parameter can be estimated through cross-validation by analyzing entire training data, online learning has to a utomatically tune the parameter on-the-fly with the increasing of training data. We address this issue within the Bayesian learning paradigm, and provide an analytic solution for automatic estimation of the regulariza-tion parameter via variational methods. Numerical experiments based on a variety of benchmark data sets are presented that demons trate the effectiveness of the newly pro-posed algorithm. This section presents a review of the LOFE algorithm. Let D = { ( x n ,y n ) } N n =1 denote a training dataset, where x n  X  R M is the n -th data sample and y n  X  X  0 , 1 } is its corresponding class label. We are interested in the problems where M N .Westart by defining the margin. Given a distance function, we find two nearest neighbors of each sample x n , one from the same class (called nearest hit or NH), and the other from the different class (called nearest miss or NM) [8]. Following the work of [4], the margin function. For the purpose of this paper, we use the block distance to define a sample X  X  margin and nearest neighbors, while other standard definitions may also be used. An intuitive interpretation of this margin is a measure as to how much x n can  X  X ove X  in the feature space before being misclassified. By the large margin theory [9], a classifier that minimizes a margin-based error function usually generalizes well on unseen test data. One natural idea then is to scale each f eature, thus obtaining a weighted feature space, parameterized by a nonnegative vector w , so that a margin-based error function in the induced feature space is minimized. The margin of x n , computed with respect to w ,isgivenby By defining z n = | x n  X  NM ( x n ) | X  X  x n  X  NH ( x n ) | ,where | X | is an element-wise ab-solute operator,  X  n ( w ) can be simplified as  X  n ( w )= w T z n , which is a linear function of w and has the same form as the sample margin defined in SVM using a kernel func-tion. An important difference, however, is that by construction the magnitude of each element of w in the above margin definition reflects the relevance of the corresponding feature in a learning process. This is not the case in SVM except when a linear kernel is used, which however can capture only linear discriminant information. Note that the margin thus defined requires only information about the neighborhood of x n , while no assumption is made about the underlying data distribution. This implies that by local learning we can transform an arbitrary nonlinear problem into a set of locally linear problems.

The local linearization of a nonlinear problem enables us to estimate the feature weights by using a linear model that has been extensively studied in the literature. The main problem with the above margin definition, however, is that the nearest neighbors of a given sample are unknown before learning. In the presence of thousands of irrel-evant features, the nearest neighbors defined in the original space can be completely different from those in the induced space. To account for the uncertainty in defining local information, we develop a probab ilistic model where the nearest neighbors of a given sample are treated as latent variables . Following the principles of the expectation-maximization algorithm [10], we estimate the margin through taking the expectation of  X  ( w ) by averaging out the latent variables:  X   X  where M n = { i :1  X  i  X  N, y i = y n } , H n = { i :1  X  i  X  N, y i = y n ,i = n } , nearest miss or hit of x n , respectively. These probab ilities are estimated through the standard kernel density estimation method: where k (  X  ) is a kernel function. Specifically, we use exponential kernel k ( d )= exp(  X  d/ X  ) where kernel width  X  determines the resolution at which the data is locally analyzed.

After the margins are defined, the problem of learning feature weights can be directly solved within a margin framework. For computational convenience, we perform the es-timation in the logistic regression formulation. In applications with a huge amount of features (e.g., molecular classification [11 ]), we expect that most of features are irrele-vant. To encourage the sparseness, one commonly used strategy is to add 1 penalty of w to an objective function [12,13], which leads to the following optimization problem: where  X  is a parameter that contro ls the penalty strength and consequently the sparse-ness of the solution.
 Since  X  z n implicitly depends on w through the p robabilities P ( x i = NH ( x n ) | w ) and P ( x i = NM ( x n ) | w ) , we use a fixed-point iteration method to solve for w . In each it-eration,  X  z n is first computed by using the previous estimate of w , which is then updated by solving the optimization problem (4). The iterations are carried out until conver-gence. It is interesting to note that though local learning is a highly nonlinear process, in each iteration we deal with a linear model.

For fixed  X  z n , (4) is a constrained convex optimization problem. Due to the nonneg-ative constraint on w , it cannot be solved directly by using a gradient descent method. To overcome this difficulty, we reformulate the problem slightly as: thus obtaining an unconstrained optimization problem. Here v m is the m -th element of v . It is easy to show that at the optimum solution we have w m = v 2 m , 1  X  m  X  M .The solution of v can be readily found through gradient descent with a simple update rule: where  X  is Hadamard operator, and  X  is the learning rate determined by a line search.
Note that the objective function (5) is no longer a convex function, and thus a gradient descent method may find a local minimizer or a saddle point. It can be shown that if all elements of a initial point are non-zero, the sol ution obtained when the gradient vanishes is a global minimizer [5]. Moreover, by using the Banach fixed point theorem [14], it can be proved that the algorithm converges to a unique solution for any nonnegative initial feature weights, under a loose c ondition that a kernel width is sufficiently large [5]. It is interesting to note that even if the initial f eature weights were wr ongly selected and the algorithm started computing erroneous near est neighbors for each sample, the theorem assures that the algorithm will eventually converge to the same solution obtained when one had perfect prior knowledge.

The computational complexity of LOFE is O ( N 2 M ) , which is linear with respect to feature dimensionality. In contrast, some popular greedy search methods (e.g., forward search) require of the order of O ( M 2 ) moves in feature space. LOFE is based on batch learning  X  that is, feature weights are updated after seeing all of the training data. In case the amount of training data is enormous, or training data arrives sequentially, online learning is computationally much more attractive than batch learning.

We propose a new online learning algorithm by using stochastic approximation tech-niques [15]. The key idea is to estimate the gradient of the objective function of indi-vidual samples, and then perform one gradient-descent step to obtain a solution that reduces the objective function. The theory of stochastic gradient assures that by using a carefully selected step size, the algorithm c onverges to a fixed-point solution identical to that obtained by using batch learning.

To distinguish sequentially arrived samples from those used in batch learning, we use k , instead of n , to index samples in the sequel. At the k -th sampling, we approximate the gradient of the objective function of (5) as where  X  ( x ) is the sigmoid function, defined as  X  ( x )=1 / (1+exp(  X  x )) . The stochastic gradient method gives the following updating rule for v : where  X  k =  X /k ,and  X  is a fixed step size. In each updating step, the vector  X  z k is calculated from the k -th training samples based solely on the current value of v k . Although the above online-learning formulation is straightforward, the estimation of the regularization parameter is not trivial. Un like batch learning where the parameter can be estimated through cross validation by analyzing the full data set, online learning has to determine the parameter on-the-fly with the increasing of training data. This issue is rarely addressed in the literature. 4.1 Bayesian Estimation of Regularization Parameters It has been recently suggested that parameter estimation can be performed by applying full Bayesian treatment to a parametric model, which is also called evidence approxima-tion in the literature [16,17]. The basic idea is to connect a hyper-parameter to the prior distribution of a model, and then select a parameter that makes the model most consistent to the observed data. We below give a brief review of evidence approximation.
Bayesian learning treats a penalized loss function of the form L = L D +  X  L W as the log-likelihood of the following posterior distribution where D is the training data, and Z is some normalization constant. Thus, the empirical loss L D is mapped to the likelihood function p ( D| w ) , and the regularization term L W to the prior distribution p ( w |  X  ) of w .

For the 1 regularization, p ( w |  X  )  X  exp(  X   X  w 1 ) . The prior distribution p ( w |  X  ) is usually chosen to be the exponential distribution or Laplace distribution, depending on the range of w . Since in our case w  X  0 , we use the isotropic exponential distribution for w ,givenby In evidence approximation, p ( D|  X  )= p ( D| w ) p ( w |  X  ) d w is called the evidence func-tion. By assuming a prior distribution p (  X  ) , evidence approximation calculates the pos-terior distribution as and picks the most probable value  X  MAP that maximizes the a posteriori distribution. It has been suggested that one can assume  X  to be sharply peaked near  X  MAP [18,19], and maximize p ( D|  X  ) to obtain  X  MAP . In this paper we adopt this simplification.
Obtaining a closed-form expression of a evidence function that allows for direct maximization is difficult. Approximation methods are usually adopted to simplify the optimization. Two commonly used approaches to approximating a probability distribu-tion function are Laplace approximation [18] and variational methods [16,20]. It has been shown in [20] that variational methods usually produce more accurate results than Laplace approximation. Moreover, Laplace approximation is only applicable at local optimal points of a function, while variational methods can be used to approximate a function at arbitrary points. For the purpose of this paper, we use variational methods. 4.2 Variational Methods Variational methods seek a local approximation of a convex or concave function via its first order Taylor expansion. For example, for a concave function f ( x ) , its Taylor equality at x =  X  . By varying y with  X  , f ( x )=min  X  y ( x |  X  ) . Denoting  X  = f (  X  ) ,we have f ( x )=min  X  (  X x + H (  X  )) .
 For the sigmoid function, ln  X  ( x ) is a concave function. Hence, ln  X  ( x )=min  X  (  X x +
By using variational methods, we represent an objective function with a simple lin-ear approximation, with equality at designated point  X  = x . Also, variational methods transform a univariate function f ( x ) to a bivariate one y ( x |  X  ) , thus introducing extra degrees of freedom to the original problem we aim to solve. Denote F ( y ( x ) , X  ) as a functional of y ( x ) with parameter  X  . The problem of optimizing F ( y ( x ) , X  ) with re-spect to  X  is transformed into a problem of optimizing an approximated functional F ( y ( x |  X  ) , X , X  ) with respect to  X  and  X  individually. The later one is often mathemati-cally more tractable. 4.3 Parameter Estimation We apply variational methods to the objective function (4) to obtain its evidence func-tion, and then estimate the regulariation parameter by maximizing the so-obtained evi-dence function. Although our goal is to derive a parameter estimation method for online learning, we first work on the batch learning case, and then extend the result to the on-line learning case.

We first rewrite Eq. (4) into the standard form of logistic regression by multiplying it by sample size N , which yields: when discussing batch learning, unless otherwise specified. We also define  X  n =  X  z n if y n =1 ,and  X  n =  X   X  z n if y n =0 . The likelihood function p ( D| w ) can then be expressed by p ( D| w )= By using the variational approximation of the sigmoid function and Eqs. (10) and (12), the evidence function is given by p ( D|  X  )= Note that the minimization is inside the integrate operation, which makes the optimiza-tion mathematically intractable. Following the principles of variational methods [20], also explained in Sec. 4.2, we treat the parameters  X  =[  X  1 ,  X  X  X  , X  N ] to be indepen-dent of w so that we can move the minimization out of the integration. Integrating out Eq. (13) yields p ( D|  X  )= min Denote  X   X  as the variational parameters that minimize the likelihood function (14). By using Eqs. (10), (12) and (14), the posterior distribution of w can then be written as p ( w |D )= which is a non-isotropic exponential distribution. Denote  X  w as the mean of w ,the m -th element of which is We then seek the optimal estimates of the variational parameters  X  of p ( D|  X  ) .Taking the derivative of the logarithm function of p ( D|  X  ) with respect to  X  n and forcing it to zero produce Hence, for a fixed  X  , the optimal estimates of  X  are given by It is easy to prove that Eq. (18) has an unique solution and can be solved either by an iterative or Newton X  X  methods.

After obtaining an approximation to the evidence function p ( D|  X  ) , we are able to estimate the hyper-parameter  X  by maximizing the evidence function. The logarithm of Eq. (14) takes the form ln p ( D|  X  )=  X  Since  X  w , given by Eq. (16), is a function of  X  , we denote it by  X  w (  X  ) in the sequel. Taking the derivative of Eq. (19) with respect to  X  and forcing it to zero, we obtain the following iterated solution of  X  ,givenby
With Eqs. (18) and (20), we are now able to determine the optimal choice of regular-ization parameter  X  by using an iterative method.

We now proceed to extend the above derivation for online learning. Using k , instead of N , to denote the sample size and applying  X   X  = k X  to Eq. (20) yield Hence, the k -th estimate of  X  can be calculated as where  X  w k ( m ) can be computed in an online manner
Note that Eq. (14) makes sense only if  X  w ( m ) &gt; 0 . If this condition is violated, it implies that the currently estimated  X  does not provide sufficient penalty to force the feature weights to zero, and the so-estimated feature weights do not follow the expo-iteration. This section presents several numerical expe riments to demonstrate the effectiveness of the newly proposed algorithm. We first perform a simulation study on the well-known Fermat X  X  spiral data. It is a binary classi fication problem. Each class has 230 samples distributed in a two-dimensional space, forming a spiral shape. In addition to the first two relevant features, each sample is cont aminated by a varying number of irrelevant features randomly sampled from the standard normal distribution. The spiral problem, though simple, when contaminated by thousands of irrelevant features, poses a serious challenge for existing algorithms. Fig. 1 presents the feature weights learned by our algorithm and three competing algorithms, including SIMBA [4], RELIEF-F [8], and I-RELIEF [21]. We observe that our algorithm performs remarkably well over a wide range of feature-dimensionality values, yielding always the largest weights for the first two relevant features, while all other weights are less than 10  X  4 . The solution is nearly insensitive to a growing number of irrelevant features. In contrast, the three competing algorithms perform substantially worse than ours.
 In the second experiment, we apply our algorithm to eight UCI benchmark data sets. The data information is summarized in Tabl e 1. For each data set, the set of original features is augmented by 5000 artificially generated irrelevant features randomly sam-pled from the standard normal distribution. It should be noted that some features in the original feature sets may be irrelevant or weakly relevant, and hence may receive zero weights in our algorithm. Unlike the spiral problem, however, the relevance informa-tion of the original features is unknown. To verify that our algorithm indeed identify all relevant features, we set a high standard by comparing the classi fication performance of SVM (with the RBF kernel) in two cases: ( 1) when only the original features are used (i.e., without 5000 useless features), and (2) when the features selected by our al-gorithm are used. It is well known that SVM is very robust against noise, and that the presence of a few irrelevant features in the original feature sets should not significantly affect its performance. Hence, the classifi cation performance of SVM in the first case should be very close to that of SVM performed on the optimal feature subsets that are unknown to us apriori . Essentially, we are comparing our algorithm with the optimal feature selection algorithm. If SVM perform s similarly in both cases, we may conclude that our algorithm achieves close-to-optimum solutions.

The structural parameters of SVM are estimated through ten-fold cross validation using training data. For both online and batch learning algorithms, the kernel width  X  used in Eqs. (2) and (3) is set to 2 . Though not considered in this paper, the kernel width can also be treated as a hyper-parameter and estimated similarly using the proposed algorithm. For the batch learning algor ithm, the regularization parameter  X  is set to 1 . For the online learning algorithm, the regular ization parameter is estimated by using the Bayesian parameter estimation algorithm proposed in Sec. 4. To reduce statistical variations, both batch and online learning algorithms are run 10 times for each dataset. In each run, a dataset is randomly partitioned int o training and test sets. After a feature weight vector is learned, only the features with weights larger than 10  X  4 are used for classification. The averaged classification errors and standard deviations of SVM are reported in Table 2. The false discovery rate (FDR), defined as the ratio between the number of artificially added, irrelevant features identified by our algorithms as useful ones and the total number of irrelevant features (i.e., 5000 ), is reported in Table 2. For reference, the classification performance of SVM using all features (i.e., the original features plus the useless ones) is also reported. From these experimental results, we observe the followings: (1) SVM using all features performs poorly, while SVM using the features identified by our batch learning algorithm performs similarly or even slightly better (e.g., breast-cancer and splice ) than SVM using the original features. This suggests that our batch learning algorithm can achieve a close-to-optimum solution in the presence of a huge number of irrelevant features. This result is consistent with that reported in Fig. 1. (2) Online learning performs slightly worse than batch learning, but with a much lower computational complexity. For most data sets, it only takes batch learning a few minutes to process more than 5000 features. Note however that the CPU times of batch learning on splice and flare-solar are much larger than other data sets. This is due to the fact that the computational complexity of batch learning is quadratic in the number of samples. We should emphasize that though batch learning performs slightly better than online learning, they are used in different scenarios. (3) In addition to successfully identifying relevant features, both batch and online learning algorithms perform remarkably well in removing irrelevant ones. From Ta-ble 2, we observe that for both algorithms, the false discovery rates are very low. For example, for splice , there are only less than 4 out of 5000 irrelevant features that are identified by our algorithms as useful ones.

In the third experiment, we compare the performance of the online learning algorithm using different but fixed regularization par ameters, and the parameter tuned by Bayesian estimation on the twonorm and waveform data sets. The results are shown in Fig. 2(a-b). For both data sets, the classification e rrors are heavily influenced by the choice of  X  . We also observe that with the tuned regul arization parameter the performance of the algorithm is very close to the optimal one that can be achieved by using a fixed parameter. This result clearly demonstrates the effectiveness of our proposed parameter estimation method.
We also conduct an experiment to study the convergence behavior of the proposed parameter estimation algorithm by applying the online learning algorithm to waveform with different initial values of  X  . The learning paths of the parameter are depicted in Fig. 2(c). We observe that the regularization parameter converges regardless of its initial points. This paper addressed the issue of finding sparse solutions for large-scale feature selec-tion problems, and developed a computationally efficient method that is applicable for both online and batch learning applications. The batch learning version exhibits a near optimal performance with affordable computational complexity, and the online learn-ing one provides a means to balance betw een speed and accuracy. We also proposed a Bayesian regularization method for onlin e learning that performs very well with the specified feature selection algorithm.

