 Many real-world applications require data from multiple databases to be inte-grated and combined to improve data analysis and mining. Record linkage (also known as entity resolution or data matching) is the process of identifying match-ing records that refer to the same entity from multiple databases [ 3 ]. identifiers (QIDs) [ 16 ], such as first name, last name, address details, etc. to link records across databases. However, due to privacy and confidentiality concerns organizations generally do not want to share any sensitive information regarding their entities with other data sources. Finding records in multiple databases that relate to the same entity or having approximately the same values for a set of QIDs without revealing any private or sensitive information is the research area known as  X  X rivacy-preserving record linkage X  (PPRL) [ 8 , 16 ].
 The naive pair-wise comparison of multiple data sources is of exponential complexity in terms of the number of parties. This makes record linkage appli-cations not scalable to large databases and increasing number of participating parties. Applying indexing is a possible solution aimed at improving scalabil-ity [ 4 ]. Indexing reduces the large number of potential comparisons by removing as many record sets as possible that correspond to non-matches, such that expen-sive similarity comparisons are only required on a smaller number of candidate record sets. Indexing for PPRL needs to be conducted such that no sensitive information that can be used to infer individual records and their attribute values is revealed to any party involved in the process, or to an external adver-sary [ 18 ].
 We propose a novel indexing mechanism for multi-party PPRL that can provide better scalability, blocking quality, and privacy compared to previous approaches. Our approach efficiently creates blocks across multiple parties with-out revealing any private information. Specific contributions of our paper are (1) a two-step blocking protocol which consists of (2) a multi-bit splitting app-roach for Bloom filters and (3) a hierarchical canopy clustering algorithm; and (4) an empirical evaluation using large datasets, and a comparison with other multi-party approaches in terms of efficiency, effectiveness and privacy. Can we do efficient and effective indexing for record linkage? This is a problem that has been considered for several decades. According to a survey by Chris-ten [ 4 ], a variety of indexing approaches have been developed. Some of these have been adapted for PPRL [ 16 ], including standard blocking [ 1 ], mapping ever, performing scalable record linkage that provides high linkage quality while preserving privacy is an open research question that needs further investigation. Bloom filters are commonly used for encoding of records in PPRL due to their capability of computing similarities. Lai et al. [ 11 ] proposed a multi-party approach that uses Bloom filters to securely transfer data between multiple parties for pri-vate set intersection. This approach was recently adapted by Vatsalan et al. [ 17 ] for multi-party PPRL, however their approach does not address blocking. Schnell [ 15 ] introduced a new blocking method for record linkage, based on the concept of a multi-bit tree data structure [ 9 ] to hold a set of Bloom filters. This approach was further extended by Ranbaduge et al. [ 13 ] as a blocking mechanism for multi-party PPRL. Their experimental results showed that the proposed approach is scalable with the dataset size and the number of parties, and provides better linkage quality and privacy than a phonetic based indexing approach. However, the blocks generated using this approach might miss some true matches due to the recursive splitting of Bloom filter sets.
 are similar to each other, while dissimilar records are in different clusters. Several clustering approaches have been adapted for private blocking [ 8 , 10 , 19 ], however neither of these techniques considers blocking of more than two databases. datasets. It can generate candidate record sets by efficiently calculating similar-ities between blocking key values. Records are inserted into one or more over-lapping clusters based on their similarity to the cluster centroid. Each cluster then forms a block from which candidate sets are generated. However, the use of canopy clustering for indexing in PPRL has so far not been studied. We now introduce the building blocks required for our clustering based PPRL indexing approach, and then study the indexing protocol in detail. 3.1 Building Blocks Bloom Filters : are bit vectors proposed by Bloom [ 2 ]. In a Bloom filter initially all the bits are set to 0. To map an element of a set into the domain between 0 and m  X  1 of a Bloom filter of length m , k independent hash functions h are used. Furthermore, to store n elements of the set S = Bloom filter, each element s i  X  S is encoded using the k hash functions and all bits having index positions h j ( s i )for1  X  j  X  k are set to 1.
 Q-grams : are character sub-strings of length q in a string [ 3 ]. For example, the string  X  X ETER X  can be padded with character  X   X  and the resulting q -gram set (for q =2)is { P,PE,ET,TE,ER,R } . In our approach we encode q -gram sets of the quasi-identifiers (QIDs) into Bloom filters. First, the selected QIDs of a given record are converted into a q -gram set. Then each q -gram set is encoded into a Bloom filter by using k hash functions.
 Secure Summation Protocol : is a method used in secure multi-party computa-tion [ 5 ], and has been used in several PPRL approaches [ 13 , 17 ]. The basic idea is to compute a summation of private inputs of P parties without revealing indi-vidual values to any other parties, and at the end of the computation no party knows anything except its own input and the final sum [ 5 ]. 3.2 Basic Multi-bit Indexing Protocol We now describe our indexing approach for multi-party PPRL. The construction of the index of an individual party consists of two main phases: 1. Multi-bit Bloom filter splitting : This phase can be further extended into three (a) Generate Bloom filters for the records in the dataset . (b) Perform secure summation to find the best splitting bit position . (c) Split the set of Bloom filters into mini-blocks . 2. Merge mini-blocks using clustering .
 Generating large blocks of different sizes makes the comparison step more problematic and requires more computational time. With our approach, a user has control over the block sizes by merging blocks until the size of blocks reaches an acceptable lower limit suitable for comparison. The aim of our protocol is to divide (split) the set of records in the datasets into mini-blocks (phase 1) and merge these mini-blocks based on their similarity (phase 2). Merging of mini-blocks by clustering reduces the overall running time requirement compared to using a clustering technique for blocking the datasets [ 10 , 19 ]. These merged blocks can then be compared using private comparison and classification tech-niques to determine the matching record sets in different databases [ 17 ]. Each party follows these phases to construct the blocks from their own dataset. 3.3 Multi-bit Bloom Filter Splitting Before performing the clustering algorithm, the set of records needs to be encoded into Bloom filters, and split into sets of smaller blocks (which we call mini-blocks ). All P parties need to agree upon a bit array length m (length of the Bloom filter); the length (in characters) of grams q ,the k hash functions, and a set of QID attributes that are used to block the records. The parameters s and s max specify the lower and upper bound of the acceptable size of a mini-block , respectively. The overall splitting approach for each party P 1  X  i  X  P , is outlined in Algorithm 1.
 Bloom filter. Once all the parties have generated their sets of Bloom filters they are added into a queue Q as a single block. At each iteration the first block of Bloom filters b that is available at the front of Q is processed. In line 5, each party calculates a vector of length m that contains the ratios between the number of 0 X  X  and 1 X  X  for each bit position in the Bloom filters, using f where f ij is the ratio value of bit position j of party P in position j ,and l is the number of Bloom filters processed in a given block. secure summation protocol to compute common bit positions suitable for split-ting (line 6). The globally summed ratio vector R g is used to find the d splitting bit positions.
 selected into the set I j of splitting bit positions as I The d max bit positions in I j with the lowest ratio values (which we call match-bits ) are selected as the best splitting bit positions. Fig. 1 illustrates an example of selecting best splitting bits.
 binations BC l (line 7). An example of bit positions { j x the set of combinations {{ j x ,j y ,j z } , { j x ,j y } The value of d max needs to be kept small as this generation grows exponentially with d max . For each combination, the Bloom filters in the current block are pro-cessed to analyze the sizes of resulting mini-blocks with different bit patterns. For a bit combination { j x ,j y } , for example, the set of bit patterns is Once the current block is processed for all possible bit combinations, we find the common best bit combination BC g in line 8 in Algorithm 1. The current block is split into mini-blocks according to this globally accepted bit combination. After splitting, if any of the mini-blocks contains less than s these mini-blocks will not be included into the final mini-block set C (line 10). Instead they are merged back into a single block which is added to C . If all of the resulting mini-blocks contain a number of records greater than s each mini-block is checked against the value of s max . All the mini-blocks that contain records greater than s max are added to Q for future splitting (line 12 in Algorithm 1). If the number of records is less than s max mini-blocks are added to C . The parameters s min and s max control the number of iterations that occur in the multi-bit splitting algorithm. Our clustering of mini-blocks is based on a canopy technique [ 6 , 12 ]. The gen-erated mini-blocks are merged into larger clusters by inserting records into one or more overlapping clusters based on their similarity to their nearest cluster centroid. This allows us to perform computationally efficient indexing. We use a normalized Hamming distance based similarity calculation for computing the similarity of clusters: sim H ( x , y )=1  X  | i | m | x i sim ( x , y ) is the normalized Hamming distance similarity between the two bit vectors x and y , sim H ( x , y ) = 1 if and only if x = y Each mini-block generated by the multi-bit splitting algorithm is initially considered as a separate cluster, which we refer to as a mini-cluster .ABloom filter a c is selected as the centroid for each mini-cluster which has the highest similarity to all other Bloom filters in the cluster. For this selection process we use a maximum average Hamming distance based similarity calculation which is shown in ( 1 ). a c = argmax a We use threshold-based canopy clustering for merging similar mini-clusters . Before starting the clustering algorithm, all parties need to agree upon the tight similarity threshold s t , loose similarity threshold s l size ms max which controls the size of merged clusters (blocks) and indirectly controls the number of iterations of the merging process. For merging of mini-clusters we suggest two canopy based clustering algorithms, which are:  X  Standard canopy clustering (SCC) : Mini-clusters are merged until the result-ing cluster size increases to ms max . This algorithm merges a set of similar mini-clusters greedily in a given iteration.  X  Hierarchical canopy clustering (HCC) : The merging of mini-clusters is based on an agglomerative clustering approach until ms max is met. In a given iteration, the two most similar mini-clusters are merged.
 4.1 Merge Mini-Clusters with Standard Canopy Clustering The suggested SCC approach for merging of mini-clusters is shown in Algorithm 2. In line 2, each party iterates over the set of mini-clusters C . At each iteration the mini-cluster at the front of C is processed as the initial cluster (line 3). As discussed above, the centroid is computed for the initial cluster (line 4). set until the size of the cluster reaches ms max as shown in lines 5 to 15. At the merging step, the computed similarity value s (line 9) is checked against s s . As per lines 10 to 12, if s  X  s t , then the mini-clusters will be merged. The merged mini-clusters are removed from the set C if they are within s initial cluster is deleted from the set C (line 15). Once the size of the resulting merged cluster c xy reaches ms max , the cluster is added to the set of merged clusters O (line 16). Therefore at each iteration a set of mini-clusters which are similar to the initial cluster are merged until the overall cluster size reaches ms max . Each cluster generated by this approach will become a block to be used in the comparison step in the PPRL pipeline. 4.2 Merge Mini-Clusters with Hierarchical Canopy Clustering In the SCC algorithm described in Sect. 4.1 , depending on the sizes of the mini-clusters that are merged, the final cluster size can grow beyond ms will result in more record set comparisons. We propose a novel threshold-based hierarchical canopy clustering (HCC) approach which guarantees that clusters are only merged up-to the size limit ms max as shown in Algorithm 3. To merge clusters, each party iterates over its set of mini-clusters C (line 2). At each iteration one mini-cluster is selected and the centroid is computed as discussed in Sect. 4 (lines 3 and 4). A similarity value s is computed between the initial cluster and other mini-clusters in C (line 7). The computed value s is checked against s t and s l for merging (lines 8 to 13). Similar to Algorithm 2, mini-clusters are merged if the value s satisfies the threshold values. After each iteration, the size of the resulting merged cluster is checked against ms max . If the size of the merged cluster is less than ms max back into C as a new mini-cluster c (line 17). This enables c to be merged further with other close mini-clusters . Therefore, at each iteration the two most similar mini-clusters are merged into one. Once the size of a merged cluster reaches ms max , it is added to the final set of merged clusters O . We now analyze our protocol in terms of complexity, privacy, and quality. Complexity: By assuming there are N records in a dataset with each having an average of n q-grams, we analyze the computational and communication complexities in terms of a single party.
 In the first phase of our protocol all records are encoded using k hash func-tions. The Bloom filter generation for a single party is of O ( k In the multi-bit splitting step, the parameters s min , s to control the size of mini-blocks generated. Suitable values for the parameters s and s max need to be set as the size of the mini-blocks decides the number of iterations that occur in the splitting phase. At each iteration, a set of Bloom phase can be calculated as log 2 ( N/s max ) /d max . Therefore, the splitting of N Bloom filters into a set of mini-blocks is of O ( N  X  log In the second phase of our protocol, merging mini-clusters requires the pro-cessing of | C | merged clusters. The computation of the centroid for all mini-clusters is of O ( s 2 max  X | C | ) complexity. Merging mini-clusters using the SCC approach requires a total computation of O ( ms max /s max iteration ms max /s max clusters are merged. At each iteration in the HCC app-roach the two most similar mini-clusters are merged which requires a total of O ( | C | 2 ) computations.
 The parties only need to communicate with each other to perform the secure summation protocol in the phases of multi-bit splitting and the merging of mini-clusters , with each message of length m and | C | , respectively. By assuming each party directly connects to all other parties, the P parties require P messages to be sent in each iteration. Therefore the entire protocol has a communication complexity of O ( m  X  P  X  log 2 ( N/s max ) /d max + m  X | Quality: We analyze the quality of our protocol in terms of effectiveness and efficiency [ 19 ]. The SCC approach merges mini-clusters greedily which can gen-erate clusters with sizes larger than ms max . This results in the SCC approach to have higher effectiveness and lower efficiency compared to the HCC approach. Both the SCC and HCC approaches retrieve more similar records compared to a previous approach [ 13 ], as the similarity between mini-clusters is used to merge the clusters up-to ms max .
 of candidate record sets generated for each party is ms max eter ms max limits the size of the clusters generated by one of the clustering algorithms, which indirectly determines the number of merged clusters gener-ated by the protocol. In the worst case scenario a merged cluster can be of size 2( ms max  X  1) if the two mini-clusters merged are each of size ms suitable value for ms max therefore needs to be set by considering factors such as the dataset size and the number of parties, such that both high effectiveness and high efficiency are achieved while guaranteeing sufficient privacy as well. Privacy: We assume that each party follows the honest-but-curious adversary model [ 18 ]. All parties participate in a secure summation protocol for exchanging of ratio values of Bloom filters with other parties. During these summations, each party computes the sum of its ratio values but neither of the parties is capable of deducing anything about the other parties X  private inputs [ 5 ]. identification not possible. The parameter ms max is used to guarantee that every resulting cluster contains at least ms max records. This ensures all clusters that are generated have the same minimum number of records, which guarantees k -anonymous mappings ( k = ms max ) privacy [ 8 , 19 ]. The merging of mini-blocks makes the protocol more secure and harder for dictionary and frequency attacks [ 18 ]. A higher value for ms max provides stronger privacy guarantees but requires more computations as more candidate record sets will be generated. We evaluated our protocol using the North Carolina Voter Registration (NCVR) database 1 . We based our experiments on the datasets used in and provided In each of these sub-sets, 50% of records were matches. Some of these datasets included corrupted records which allowed us to evaluate how our approach works with  X  X irty X  data. The corruption levels were set to 20% and 40%.
 experiments were run on a server with 64-bit Intel Xeon (2 . 4 GHz) CPUs, with 128 GBytes of main memory and Ubuntu 14 . 04. The programs and test datasets are available from the authors. We used four attributes commonly used for record linkage as QIDs: Given name, Surname, Suburb (town) name, and Postcode. We set the Bloom filter parameters as m = 1000 bits, k = 30 hash functions, and q = 2 by following earlier Bloom filter work in PPRL [ 13 , 14 ]. For comparative evalu-ation we used the single-bit tree (SBT) multi-party PPRL blocking approach by Ranbaduge et al. [ 13 ] as to our knowledge there are no other blocking approaches for multi-party PPRL available. We also used a phonetic based blocking app-roach (PHO) as a baseline [ 8 , 17 ] to comparatively evaluate the level of privacy. We named our multi-bit splitting, standard canopy clustering, and hierarchical canopy clustering as MBS, SCC, and HCC, respectively.
 In the PHO approach we used Soundex [ 4 ] as encoding function for all QIDs except Postcode where the first three digits of the value were used as the block-ing key. Based on a set of parameter evaluation experiments we set the MBS parameters d max =3, bs t =0 . 1, s max =50and s min = s max and HCC parameters of s t , s l ,and ms max to 0 . 9, 0 . 8, and 500, respectively, as these values gave us the minimum overlap between clusters.
 We measured the average total runtime for the protocol to evaluate the com-plexity of blocking. The reduction ratio (RR), which is the fraction of record sets that are removed by a blocking technique, and pairs completeness (PC), which is the fraction of true matching record sets that are included in the candidate record sets generated by a blocking technique, were used to evaluate the blocking quality. These are standard measures to assess indexing in record linkage [ 3 ]. Fig. 2 illustrates the scalability of our approach in terms of the average time required with different dataset sizes and number of parties. As expected the MBS-SCC approach requires less runtime than the MBS-HCC approach but both show a linear scalability with the size of the datasets and number of parties. Fig. 3(a) illustrates that RR remains close to 1 for all dataset sizes and for different number of parties. This shows our approach significantly reduces the total number of candidate record sets that need to be compared. Fig. 3(b) to (d) illustrate the PC of our approach with different dataset sizes and corruption levels, indicating that our approach can provide significantly better blocking quality than the earlier proposed SBT approach [ 13 ].
 To evaluate the privacy of our approach we use the measure probability of suspicion ( P s )[ 18 ], which is defined for a value in an encoded dataset as 1 /n where n g is the number of values in a global dataset ( G ) that match with the corresponding value in the encoded dataset D . As shown in Fig. 4(a), the MBS-SCC and MBS-HCC approaches both provide significantly better privacy compared to the PHO approach which has a maximum P s of 1 (under the worst case assumption of the global dataset G being equal to the linkage dataset i.e. G  X  D ). By increasing the parameter ms max stronger privacy can be guaranteed in our approach. Fig. 4(b) shows that the PHO approach [ 8 , 17 ] creates a large number of blocks of size 1 which makes this approach not suitable for PPRL. According to Fig. 4(b), the MBS-HCC approach provides clusters within the acceptable size limit of ms max which results in better block structures compared to the SBT and MBS-SCC approaches. This illustrates that our novel MBS-HCC technique provides better privacy than the other compared approaches while achieving higher results for both RR and PC. We proposed a novel blocking protocol for multi-party PPRL based on multi-bit Bloom filter splitting and canopy clustering. We suggested a novel agglomera-tive hierarchical canopy clustering algorithm which generates canopies (blocks) within a specific size range. We demonstrated the efficiency and effectiveness of our approach on datasets containing up-to one million records. The evaluation results indicated that our approach is scalable with both the size of the datasets and the number of parties. Our approach outperforms a previous multi-party private blocking and a phonetic based indexing approach in terms of blocking quality and privacy. A limitation in our approach is the assumption of the semi-honest adversary model. We plan to extend our protocol to adversary models that are applicable for malicious parties and evaluate the privacy over other attack methods applicable to PPRL [ 18 ]. We will also investigate the parallelization of our approach to improve its performance.

