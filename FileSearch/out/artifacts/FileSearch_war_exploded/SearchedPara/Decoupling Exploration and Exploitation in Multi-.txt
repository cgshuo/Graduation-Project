 Orly Avner orlyka@tx.technion.ac.il Shie Mannor shie@ee.technion.ac.il Department of Electrical Engineering, Technion Ohad Shamir ohadsh@microsoft.com Microsoft Research New England Multi-armed bandits have long been a canonical frame-work for studying online learning under partial infor-mation constraints. In this framework, a learner has to repeatedly obtain rewards by choosing from a fixed set of k actions (arms), and gets to see only the re-ward of the chosen action. The goal of the learner is to minimize regret, namely the difference between her own cumulative reward and the cumulative reward of the best single action in hindsight. We focus here on algorithms suited for adversarial settings, which have reasonable regret even without any stochastic assump-tions on the reward generating process.
 A central theme in multi-armed bandits is the exploration-exploitation tradeoff : The learner must choose highly-rewarding actions most of the time in order to minimize regret, but also needs to do some exploration in order to determine which actions to choose. Ultimately, the tradeoff comes from the as-sumption that the learner is constrained to observe only the reward of the action she picked.
 While being a compelling and widely applicable frame-work, there exist several realistic bandit-like settings, which do not correspond to this fundamental assump-tion. For example, in ultra-wide band (UWB) com-munications, the decision maker, also called the  X  X ec-ondary, X  has to decide in which channel to transmit and in what way. There are typically many possible channels (i.e., frequency bands) and several transmis-sion methods (power, code used, modulation, etc.; see (Oppermann et al., 2004)). In some UWB devices, the secondary can sense a different channel (or channels) than the one it currently uses for transmission. In fact, in some settings, the secondary cannot sense the channel it is currently transmitting in because of in-terference. The UWB environment is extremely noisy since it potentially contains many other sources, called  X  X rimaries. X  Some of these sources are sources whose behavior (which channel they use, for how long, and in which power level) can be very hard to predict as they represent a mobile device using WiMAX, WiFI or some other communication protocol. It is therefore sensible to model the behavior of primaries as an ad-versarial process or a piecewise stationary process. We should mention that UWB networks are highly com-plex, with many issues such as power constraints and multi-agency that have been considered in the multi-armed bandit framework (Liu &amp; Zhao, 2010; Avner &amp; Mannor, 2011; Lai et al., 2008), but the decoupling of sensing and transmission has not been considered to the best of our knowledge. More abstractly, our work relates to any bandit-like setting, where we are free to query the environment for some additional partial information, irrespective of our actual actions. In such settings, the assumption that the learner can only observe the reward of the action she picked is an unnecessary constraint, and one might hope that removing this constraint and constructing suitable al-gorithms would allow better performance. We empha-size that this is far from obvious: In this paper, we will mostly focus on the case where the learner may query just a single action, so in some sense the learner gets the same  X  X mount of information X  per round as the standard bandit setting (i.e., the reward of a single action out of k actions overall). The goal of this paper is to devise algorithms for this setting, and analyze theoretically and empirically whether the hope for im-proved performance is indeed justified. We emphasize that our results and techniques naturally generalize to cases where more than one action can be queried, and cases where the reward of the selected action is always revealed (see Sec. 7).
 Specifically, our contributions are the following:  X  We present a  X  X ecoupled X  multi-armed bandit al- X  We prove that in certain settings (in particular,  X  Our algorithms are based on a certain adap- X  We perform a preliminary experimental study, The proofs of our theorems are provided in the ap-pendix of the full version (Avner et al., 2012). Related Work. The idea of decoupling exploration and exploitation has appeared in a few previous works, but in different settings and contexts. For example, (Yu &amp; Mannor, 2009) discuss a setting where the learner is allowed to query an additional action in a multi-armed bandit setting, but the focus there was on algorithms for stochastic bandits, as opposed to adversarial bandits as we do here. (Agarwal et al., 2010) study a bandit setting with (one or more) queries per round. However, they focus on the problem of bandit convex optimization, which is much more gen-eral than ours, and exploration and exploitation re-mains coupled in their framework. A different line of work ((Even-Dar et al., 2006; Audibert et al., 2010; Bubeck et al., 2011)) considers multi-armed bandits in a stochastic setting, where the goal is to identify the best action by performing pure exploration. While this work also conceptually  X  X ecouples X  exploration and exploitation, the goal and setting are quite different than ours. We use [ k ] as shorthand for { 1 ,...,k } . Bold-face let-ters represent vectors, and 1 A represents the indicator function for an event A . We use the standard big-Oh notation O (  X  ) to hide constants, and  X  O (  X  ) to hide constants and logarithmic factors. For a distribution vector p on the k -simplex, we use the notation straightforward to show that for a distribution vector, this quantity is always in [1 ,k ]. In particular, it is k for the uniform distribution, and gets smaller the more non-uniform the distribution is, attaining the value of 1 when p is a unit vector.
 Our setting is a variant of the standard adversarial multi-armed bandit framework, focusing (for simplic-ity) on an oblivious adversary and a fixed horizon. In this setting, we have a fixed set of k &gt; 1 actions and a fixed known number of rounds T . Each action i at each round t has an unknown associated reward g ( t )  X  [0 , 1]. At each round, a learner chooses one of the actions i t , and obtains the associated reward g ( t ). The basic goal in this setting is to minimize the regret with respect to the best single action in hind-sight, namely Unless specified otherwise, we make no assumptions on how the rewards g i ( t ) are generated (other than boundedness), and they might even be generated ad-versarially by an agent with full knowledge of our al-gorithm. However, we assume that the rewards are fixed in advance and do not depend on the learner X  X  (possibly random) choices in previous rounds.
 In standard multi-armed bandits, at the end of each round, the learner only gets to know the reward g i t ( t ) of the action i t which was actually picked, but not the reward of other actions. Instead, in this paper we focus on a different setting, where the learner, af-ter choosing an action i t , may query a single action j t and get to see its associated reward g j t ( t ). This setting is a (slight) relaxation of the standard bandit setting, since we can always query j t = i t . However, here it is possible to query an action different than i t . We em-phasize that the regret is still measured with respect to the chosen actions i t , and the querying only has informational value. In order to compare our results with those obtainable in the standard setting, we will use the term standard bandit algorithm to refer to al-gorithms which are not free to query rewards, and are limited to receiving the reward of the chosen action. A typical example is the EXP3.P (Auer et al., 2002), with a  X  O ( probability, or the Implicitly Normalized Forecaster of (Audibert &amp; Bubeck, 2009) with O ( An interesting variant of our setting is when the learner gets to query more than one action, or gets discussed in Sec. 7. In analyzing our  X  X ecoupled X  setting, perhaps the first question one might ask is whether one can always get improved regret performance, compared to the stan-dard bandit setting. Namely, that for any reward as-signment, the attainable regret will always be signifi-cantly smaller. Unfortunately, this is not the case: It can be shown that there exists an adversarial strategy such that the regret of standard bandit algorithms is  X   X (  X  rithm will be 1  X ( always obtain better performance. However, as we will soon show, this can be obtained under certain realistic conditions on the actions X  rewards.
 We now turn to present our first algorithm (Algo-rithm 1 below) and the associated regret analysis. The algorithm is rather similar in structure to stan-dard bandit algorithms, picking actions at random in each round t according to a weighted distribution p ( t ) which is updated multiplicatively. The main differ-ence is in determining how to query the reward. Here, the queried action is picked at random, according to a query distribution q ( t ) which is based on but not iden-tical to p ( t ). More particularly, the queried action j is chosen with probability Roughly speaking, this distribution can be seen as a  X  X eometric average X  between p ( t ) and a uniform dis-tribution over the k actions. See Algorithm 1 for the precise pseudocode.
 Algorithm 1 Decoupled MAB Algorithm
Input: Step size parameter  X   X  [1 ,k ], confidence parameter  X   X  (0 , 1)
Let  X  = 1 /  X  2 (1 +  X  ) 2 k 2  X  j  X  [ k ] let w j (1) = 1. for t = 1 ,...,T do end for Readers familiar with bandit algorithms might notice the existence of the common  X  X xploration component X   X /k in the definition of p j ( t ). In standard bandit al-gorithm, this is used to force the algorithm to explore all arms to some extent. In our setting, exploration is performed via the separate query distribution q j ( t ), and in fact, this  X /k term can be inserted into the q ( t ) definition instead. While this would be more aes-thetically pleasing , it also seems to make our proofs and results more complicated, without substantially improving performance. Therefore, we will stick with this formulation.
 Before discussing the formal theoretical results, we would like to briefly explain the intuition behind this querying distribution. Most bandit algorithms (in-cluding ours) build upon a standard multiplicative up-dates approach, which updates the distribution p ( t ) multiplicatively based on each action X  X  rewards. In the bandit setting, we only get partial information on the rewards, and therefore resort to multiplicative up-dates based on an unbiased estimate of them. The key quantity which controls the regret is the variance of these estimates, in expectation over the action distri-bution p ( t ). In our case, this quantity turns out to bandit algorithms, which may not query at will, are essentially constrained to have q j ( t ) = p j ( t ), leading to an expected variance of k and hence the k in their  X  O ( free to pick the querying distribution q ( t ) as we wish. mized by choosing q ( t ) as in Eq. (1), with the value of k p ( t ) k 1 / 2 . Thus, roughly speaking, instead of depen-dence on k , we get a dependence on 1 T P T t =1 k p ( t ) k as will be seen shortly.
 The theoretical analysis of our algorithm relies on the following technical quantity: For any algorithm pa-rameter choices  X , X  , and for any v  X  [1 ,k ], define where the probability is over the algorithm X  X  random-ness, run with parameters  X , X  , with respect to the (fixed) reward sequence. The formal result we obtain is the following: Theorem 1. Suppose that T is sufficiently large (and thus  X  and  X  sufficiently small) so that (1 +  X  ) 2  X  2 . Then for any v  X  [1 ,k ] , it holds that with probability at least 1  X   X   X  P ( v, X , X  ) that the sequence of rewards g (1) ,...,g i T ( T ) returned by Algorithm 1 satisfies where the  X  O notation hides numerical constants and factors logarithmic in k and  X  .
 At this point, the nature of this result might seem a bit cryptic. We will soon provide more concrete examples, but would like to give a brief general intuition. First of all, if we pick  X  = v = k , then P ( v, X , X  ) = 0 always (as k p ( t ) k 1 / 2  X  k ), and the bound becomes  X  O ( holding with probability 1  X   X  , similar to standard multi-armed bandit guarantees. This shows that our algorithm X  X  regret guarantee is never worse than that of standard bandit algorithms. However, the theorem also implies that under certain conditions, the result-ing bound may be significantly better. For example, if we run the algorithm with  X  = 1 and have v = O (1), then the bound becomes  X  O T . This bound is meaningful only if P ( O (1) , X , 1) is reasonably small. This would happen if the distribu-tion vectors p ( t ) chosen by the algorithm tend to be highly non-uniform, since it leads to a small value for We now turn to provide a concrete scenario, where the bound we obtain is better than those obtained by standard bandit algorithms. Informally, the scenario we discuss assumes that although there are k actions, where k is possibly large, only a small number of them are actually  X  X elevant X  and have a performance close to that of the best action in hindsight. Intuitively, such cases would lead to the distribution vectors p ( t ) to be non-uniform, which is favorable to our analysis. Theorem 2. Suppose that the reward of each action is chosen i.i.d. from a distribution supported on [0 , 1] . Furthermore, suppose that there exist a subset G  X  [ k ] of actions and a parameter  X  &gt; 0 (where | G | ,  X  are considered constants independent of k,T ), such that the expected reward of any action in G is larger than the expected reward of any action in [ k ] \ G by at least  X  . Then if we run our algorithm with it holds with probability at least 1  X   X  that the regret of the algorithm is at most where the  X  O notation hides numerical constants and factors logarithmic in  X ,k .
 The bound we obtain interpolates between the usual  X  O ( gorithm, and a considerably better  X  O ( larger compared with k . We note that a mathemati-cally equivalent form of the bound is Namely, the average per-round regret scales down as ( k/T ) 2 / 3 , until T is sufficiently large and we switch to a (1 /T ) 1 / 2 regime. In contrast, the bound for standard bandit algorithms is always of the form ( k/T ) 1 / 2 , and the rate of regret decay is significantly slower. We emphasize that although the setting discussed above is a stochastic one (where the rewards are cho-sen i.i.d.), our algorithm can cope simultaneously with arbitrary rewards, unlike algorithms designed specifi-cally for stochastic i.i.d. rewards (which do admit bet-ter dependence in T , although not necessarily in k ). Finally, we note in practice, the optimal choice of  X  depends on the (unknown) rewards, and hence can-not be determined by the learner in advance. How-ever, this can be resolved algorithmically by a stan-dard doubling trick (cf. (Cesa-Bianchi &amp; Lugosi, 2006)), without materially affecting the regret guaran-tee. Roughly speaking, we can guess an upper bound v on 1 T P T t =1 k p ( t ) k 1 / 2 and pick  X  = v , and if the cu-mulative sum P k p ( t ) k 1 / 2 eventually exceeds Tv at some round, then we double v and  X  and restart the algorithm. So far, we have seen how the bounds obtained for our approach are better than the ones known for standard bandit algorithms. However, this doesn X  X  imply that our approach would indeed yield better performance in practice: it might be possible, for instance, that for the setting described in Thm. 2, one can provide a tighter analysis of standard bandit algorithms, and recover a similar result. In this section, we show that there are cases where decoupling provably helps, and our approach can provide performance provably better than any standard bandit algorithm, for information-theoretic reasons. We note that the idea of decoupling has been shown to be helpful in cases reminiscent of the one we will be discussing (Yu &amp; Mannor, 2009), but here we study it in the more general and challenging adversarial setting.
 Instead of the plain-vanilla multi-armed bandit set-ting, we will discuss here a slightly more general set-ting, where our goal is not to achieve regret with re-spect to the best single action, but rather to the best sequence of S &gt; 1 actions. More specifically, we wish to obtain a regret bound of the form This setting is well-known in the online learning litera-ture, and has been considered for instance in (Herbster &amp; Warmuth, 1998) for full-information online learning (under the name of  X  X racking the best expert X ) and in (Auer et al., 2002) for the bandit setting (under the name of  X  X egret against arbitrary strategies X ). This setting is particularly suitable when the best ac-tion changes with time. Intuitively, our decoupling approach helps here, since we can exploit much more aggressively while still performing reasonable explo-ration, which is important for detecting such changes. The algorithm we use follows the lead of (Auer et al., 2002) and is presented as Algorithm 2. The only dif-ference compared to Algorithm 1 is that the w j ( t + 1) parameters are computed differently. This change fa-cilitates more aggressive exploration.
 Algorithm 2 Decoupled MAB Algorithm For Switch-ing
Input: Step size parameter  X   X  [1 ,k ], confidence parameter  X   X  (0 , 1), number of switches S
Let  X  = p S/ X T ,  X  = 1 /T ,  X  = 2  X  p 6 log(3 k/ X  ) and  X  =  X  2 (1 +  X  ) 2 k 2  X  j  X  [ k ] let w j (1) = 1. for t = 1 ,...,T do end for The following theorem, which is proven along similar lines to Thm. 1, shows that in this setting as well, we get the same kind of dependence on the distribution vectors p ( t ) as in the standard bandit setting. Theorem 3. Suppose that T is sufficiently large (and thus  X  and  X  sufficiently small) so that (1 +  X  ) 2  X  2 . Then for any v  X  [1 ,k ] , it holds that with probabil-ity at least 1  X   X   X  P ( v, X , X  ) that the sequence of re-wards g i 1 (1) ,...,g i T ( T ) returned by algorithm 2 sat-isfies the following, simultaneously over all segmenta-tions of { 1 ,...,T } to S epochs and a choice of action i to each epoch: The  X  O notation hides numerical constants and factors logarithmic in k and  X  .
 In particular, we can also get a parallel version of Thm. 2, which shows that when there are only a small number of  X  X ood X  actions (compared to k ), the leading term has decaying dependence on k , unlike standard bandit algorithms where the dependence on k is always  X  k .
 Theorem 4. Suppose that the reward of each action is chosen i.i.d. from a distribution supported on [0 , 1] . Furthermore, suppose that at each epoch s , there exists a subset G s  X  [ k ] of actions and a parameter  X  &gt; 0 (where | G s | ,  X  are considered constants independent of k,T ), such that the expected reward of any action in G s is larger than the expected reward of any action in [ k ] \ G s by at least  X  . Then if we run Algorithm 2 with it holds with probability at least 1  X   X  that the regret of the algorithm is at most where the  X  O notation hides numerical constants and factors logarithmic in  X  and k .
 Now, we are ready to present the main negative re-sult of this section, which shows that in the setting of Thm. 2, any standard bandit algorithm cannot have a regret better than  X ( worse. For simplicity, we will focus on the case where S = 2: namely, that we measure regret with respect to a single action from round 1 till some t 0 , and then from t 0 + 1 till T . Moreover, we consider a simple case where | G 1 | = | G 2 | = 1 and  X  = 1 / 5, so there is just a single action at a time which is significantly better than all the other actions in expectation.
 Theorem 5. Suppose that T  X  Ck for some suffi-ciently large universal constant C . Then in the setting of Thm. 2, there exists a randomized reward assign-ment (with | G 1 | = | G 2 | = 1 and  X  = 1 / 5 ), such that for any standard bandit algorithm, its expected regret (over the rewards assignment and the algorithm X  X  ran-domness) is at least 0 . 007 p ( k  X  1) T .
 The constant 0 . 007 is rather arbitrary and is not the tightest possible.
 We note that a related  X ( obtained in (Garivier &amp; Moulines, 2011). However, their result does not apply to the case S = 2 and more importantly, does not quantify a dependence on k . It is interesting to note that unlike the standard lower bound proof for standard bandits (Auer et al., 2002), we obtain here an  X ( fixed and doesn X  X  decay with T . The theoretical results above demonstrated the effi-cacy of our approach, compared to standard bandit algorithms. However, the exact form of our querying distribution (querying action i with probability pro-portional to p p j ( t )) might still seem a bit mysterious. For example, maybe one can obtain similar results just by querying actions uniformly at random? Indeed, this is what has been done in some other online learning scenarios where queries were allowed (e.g., (Yu &amp; Man-nor, 2009; Agarwal et al., 2010)). However, we show below that in the adversarial setting, an adaptive and non-uniform querying distribution is indeed necessary to obtain regret bounds better than plicity, we return to our basic setting, where our goal is to compete with just the best single fixed action in hindsight.
 Theorem 6. Consider any online algorithm over k &gt; 2 actions and horizon T , which queries the actions based on a fixed distribution. Then there exists a strat-egy for the adversary conforming to the setting de-scribed in Thm. 2, for which the algorithm X  X  regret is at least c A proof sketch is presented in the appendix of the full version. The intuition of the proof is that if the query-ing distribution is fixed, and there are only a small number of  X  X ood X  actions, then we spend too much time querying irrelevant actions, and this hurts our regret performance. We compare the decoupled approach with common multi-armed bandit algorithms in a simulated adver-sarial setting. Our user chooses between k communica-tion channels, where sensing and transmission can be decoupled. In other words, she may choose a certain channel for transmission while sensing (i.e., querying) a different, seemingly less attractive, channel. We simulate a heavily loaded UWB environment with a single, alternating, channel which is fit for transmis-sion. The rewards of k  X  1 channels are drawn from alternating uniform and truncated Gaussian distribu-tions with random parameters, yielding adversarial re-wards in the range [0 , 6]. The remaining channel yields stochastic rewards drawn from a truncated Gaussian distribution bounded in the same range but with a mean drawn from [3 , 6]. The identity of the better channel and its distribution parameters are re-drawn at exponentially distributed switching times.
 Figure 1 displays the results of a scenario with k = 10 channels, comparing the average reward acquired by the different algorithms over T = 10 , 000 rounds. We implemented Algorithm 1, Exp3 (Auer et al., 2002), Exp3.P (Auer et al., 2002), a simple round robin policy (which just cycles through the arms in a fixed order) and a  X  X reedy X  decoupled form of round robin, which performs uniform queries and picks actions greedily based on the highest empirical average reward. The black arrows indicate rounds in which the identity of the stochastic arm and its distribution parameters were re-drawn. The results are averaged over 50 rep-etitions of a specific realization of rewards. Although we have tested our algorithm X  X  performance on several realizations of switching times and rewards with very good results, we display a single realization of these for the sake of clarity.
 Figure 2 displays the dynamics of channel selection for two of the k = 10 channels. The thick plots represent the number of times a channel was chosen over time, and the thin plots represent the number of times it was queried. The dashed plots represent a channel which was drawn as the better channel during some periods, resulting in a relatively high average reward, while the solid plots represent a channel with a low average reward. The increased flexibility of the decou-pled approach is evident from the graph, as well as the adaptive, nonlinear sampling policy.
 Comments: We implement Algorithm 1 and not Al-gorithm 2 since the number of switches is unknown a-priori. Also, the rewards are in the range [0 , 6] in order to keep all implemented algorithms on a similar scale, without violating the boundedness assumption. In this paper, we analyzed if and how one can benefit in settings where exploration and exploitation can be  X  X ecoupled: X  namely, that one can query for rewards independently of the action actually picked. We devel-oped some algorithms for this setting, and showed that these can indeed lead to improved results, compared to the standard bandit setting, under certain conditions. We also performed some experiments that corroborate our theoretical findings.
 For simplicity, we focused on the case where only a single reward may be queried. If c &gt; 1 queries are allowed, it is not hard to show parallel guarantees to those in this paper, where the dependence on k is re-placed by dependence on k/c . Algorithmically, one simply needs to repeatedly sample from the query dis-tribution c times, instead of a single time. We con-jecture that similar lower bounds can be obtained as well. Interestingly, it seems that being allowed to see the reward of the action actually picked, on top of the queried reward, does not result in significantly improved regret guarantees (other than better con-stants).
 Several open questions remain. First, our results do not apply when the rewards are chosen by an adap-tive adversary (namely, that the rewards are not fixed in advance but may be chosen individually at each round, based on the algorithm X  X  behavior in previous rounds). This is not just for technical reasons, but also because data and algorithm dependent quantities like P ( v, X , X  ) do not make much sense if the rewards are not considered as fixed quantities.
 A second open question concerns the possible correla-tion between sensing and exploration. In some appli-cations it is plausible that the choice of which arm to exploit affects the quality of the sample of the arm that is explored. For instance, in the UWB sensing example discussed in the introduction transmitting and receiv-ing in the same channel is much less preferred than sensing in another channel because of interference in the same frequency band. It would be interesting to model such dependence and take it into account in the learning process.
 Finally, it remains to extend other bandit-related al-gorithms, such as EXP4 (Auer et al., 2002), to our set-ting, and study the advantage of decoupling in other adversarial online learning problems.
 Acknowledgements.
 This research was partially supported by the COR-NET consortium ( http://www.cornet.org.il/ ).
 Agarwal, A., Dekel, O., and Xiao, L. Optimal algo-rithms for online convex optimization with multi-point bandit feedback. In COLT , 2010.
 Audibert, J.-Y. and Bubeck, S. Minimax policies for adversarial and stochastic bandits. In COLT , 2009. Audibert, J.-Y., Bubeck, S., and Munos, R. Best arm identification in multi-armed bandits. In COLT , 2010.
 Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R. The nonstochastic multiarmed bandit problem. SIAM J. Comput. , 32(1):48 X 77, 2002.
 Avner, O. and Mannor, S. Stochastic bandits with pathwise constraints. In 50th IEEE Conference on Decision and Control , 2011.
 Avner, O., Mannor, S., and Shamir, O. Decoupling ex-ploration and exploitation in multi-armed bandits. arXiv:1205.2874v1 [cs.LG], 2012.
 Bubeck, S., Munos, R., and Stoltz, G. Pure explo-ration in finitely-armed and continuous-armed ban-dits. Theor. Comput. Sci. , 412(19):1832 X 1852, 2011. Cesa-Bianchi, N. and Lugosi, G. Prediction, learning, and games . Cambridge University Press, 2006. Even-Dar, E., Mannor, S., and Mansour, Y. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems.
Journal of Machine Learning Research , 7:1079 X  1105, 2006.
 Freedman, D.A. On tail probabilities for martingales. Annals of Probability , 3:100 X 118, 1975.
 Garivier, A. and Moulines, E. On upper-confidence bound policies for switching bandit problems. In ALT , 2011.
 Herbster, M. and Warmuth, M. K. Tracking the best expert. Machine Learning , 32(2):151 X 178, 1998. Lai, L., Jiang, H., and Poor, H. V. Medium access in cognitive radio networks: A competitive multi-armed bandit framework. In Proc. Asilomar Confer-ence on Signals, Systems, and Computers , pp. 98 X  102, 2008.
 Liu, K. and Zhao, Q. Distributed learning in multi-armed bandit with multiple players. IEEE Transac-tions on Signal Processing , 58(11):5667  X 5681, nov. 2010.
 Oppermann, I., Hamalainen, M., and Iinatti, J. UWB Theory and Application . Wiley, 2004.
 Yu, J. Y. and Mannor, S. Piecewise-stationary bandit
