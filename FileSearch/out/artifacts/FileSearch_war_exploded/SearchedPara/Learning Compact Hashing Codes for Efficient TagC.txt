 Tags have been popularly utilized in many applications with image and text data for better managing, organizing and searching for useful information. Tag completion provides missing tag information for a set of existing images or text documents while tag prediction recommends tag information for any new image or text document. Valuable prior research has focused on improving the accuracy of tag completion and prediction, but limited research has been conducted for the efficiency issue in tag completion and prediction, which is a critical problem in many large scale real world applications. This paper proposes a novel efficient Hashing approach for Tag Completion and Prediction (HashTCP). In particular, we construct compact hashing codes for both data examples and tags such that the observed tags are consistent with the constructed hashing codes and the similarities between data examples are also preserved. We then formulate the problem of learning binary hashing codes as a discrete optimization problem. An efficient coordinate descent method is developed as the optimization procedure for the relaxation problem. A novel binarization method based on orthogonal transformation is proposed to obtain the binary codes from the relaxed solution. Experimental results on four datasets demonstrate that the proposed approach can achieve similar or even better accuracy with state-of-the-art methods and can be much more efficient, which is important for large scale applications.
 I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Performance, Experimentation Tag Completion, Hashing, Multi-Label Learning
The purpose of data tagging, assigning tags or keywords to images, documents and video clips, is to benefit people for managing, organizing and searching desired information from various resources. Users can better categorize or search desired data based on the tags associated with them. Due to the rapid growth of the internet, a huge amount of data has been generated and users can only manually tag a very small portion of the data. Therefore, it is a challenging issue to complete missing tags for existing examples and predict tags for query examples on large scale data.

In the problems of tag completion and prediction, we are given a collection of data examples associated with a small number of tags. The goal is to complete missing tags for existing data examples, and to predict tags for unseen (query) examples. A large amount of research have been conducted on improving the accuracy of tag completion and prediction and generated promising results. However, most existing methods only focus on the effectiveness without paying much attention to efficiency. In many real world applications, the data size grows explosively and there are often a large number of possible tags and thus efficiency is a practical and critical issue. It is a practical and important research problem to design efficient methods for large scale tag completion and prediction.

This paper proposes a novel efficient approach for tag completion and prediction by designing compact binary hashing codes for both data examples and tags. In particular, each data example is represented by a C -bit binary code and each tag is also represented using a C -bit binary code. Our key idea of constructing the hashing codes is that a tag should be assigned to a data example if the Hamming distance between their corresponding codes is small, and two similar data examples should also have similar codes . We then formulate the problem of learning binary hashing codes as a discrete optimization problem by simultaneously ensuring the observed tags to be consistent with the constructed hashing codes and preserving the similarities between data examples.
This section reviews the related work in two areas: tag completion and prediction and learning hashing codes.
Many research on tag completion and prediction have been proposed in the past several years. Matrix completion [1, 2] methods can be applied to the tag completion task by recovering the missing entries in the tag matrix. Cabral et al. [1] propose two convex algorithms for matrix completion based on a rank minimization criterion. Cai et al. [2] introduce an efficient singular value thresholding (SVT) algorithm for completing matrix. Tag completion and prediction can also be viewed as a multi-label learning problem that exploits the dependence among labels/tags, where each data example is associated with multiple labels/tags. Numerous work have been proposed on multi-label learning for automatic image/document annotation and classification [3, 9, 10, 6]. Desai et al. [6] propose a discriminative model for multi-label learning. Hariharan et al. [10] introduce a max-margin framework for large scale multi-label classification. A conditional dependency networks is utilized in [9] to structured multi-label learning. In work [3], Chen et al. propose an efficient multi-label classification method using hypergraph regularization.
Besides the multi-label learning methods, several recent machine learning approaches have been proposed to tag completion and prediction, including tag propagation [8], distance metric learning [14] and tag recommendation [7, 18]. Li et al. [14] propose a neighbor voting algorithm for social tagging which accurately and efficiently learns tag relevance by accumulating votes from visual neighbors. A tag propagation (TagProp) method has been proposed in [8] that propagates the label/tag information from the labeled examples to the unlabeled examples via a weighted nearest neighbor graph. More recently, Wu et al. [23] introduce a direct tag matrix completion algorithm by ensuring the completed tag matrix to be consistent with both the observed tags and the visual similarity.

Existing methods generate promising results in complet-ing missing tags and predicting new tags. However, very limited prior research addresses the efficiency problem, while efficiency is a practical and critical issue in many large scale real world applications.
Extensive research on learning hashing codes for fast similarity search [5, 22] have been proposed in recent years. Locality Sensitive Hashing (LSH) [5] utilizes random linear projections to map data examples from a high-dimensional Euclidean space to a low-dimensional one. LSH has been extended to Kernelized Locality Sensitive Hashing (KLSH) [12] by exploiting kernel similarity for better retrieval efficacy. The work in [16] uses stacked Restricted Boltzman Machine (RBM) to generate compact binary hashing codes for fast similarity search of documents. The PCA Hashing (PCAH) [15] method projects each example to the top principal components of the training set, and then binarizes the coefficients by setting a bit to 1 when its value is larger than the median value seen for the training set, and -1 otherwise.

Recently, Spectral Hashing (SH) [22] is proposed to learn compact hashing codes that preserve the similarity between data examples by balancing the hashing codes. Self Taught Hashing (STH) [24] combines an unsupervised learning step with a supervised learning step to learn effective hashing codes. More recently, work [25] investigates a new signature generation algorithm for learning hashing codes in content reuse detection. Most recently, Wang et al. [21] propose a semi-supervised learning framework to achieve binary hashing codes by using tags and topic modeling. One piece of research work related to our approach is [26], which constructs hashing codes for collaborative filtering (CFCode). Collaborative filtering can be viewed as a special kind of tag completion by treating users as data examples and items as tags.

These hashing methods only consider the problem of obtaining binary codes for data examples. However, in tag completion and prediction, hashing codes for both data examples and tags are involved and thus should be consider simultaneously, which makes it difficult to apply these method directly in this research work.
We first introduce the problem of HashTCP. Assume there are total n training examples in the dataset, denoted as: x ,i  X  X  1 , 2 ,...,n } , where x i is the d -dimensional feature of the i -th example. There are total m possible tags denoted as: t j ,j  X  X  1 , 2 ,...,m } . Denote the observed tag matrix as: T  X  { 0 , 1 } n  X  m , where a label T ij = 1 means the j -th tag is assigned to the i -th example, and T ij = 0 means a missing tag or the i -th example is not associated with the j -th tag. The main purpose of HashTCP is to obtain optimal binary hashing codes y i  X  { X  1 , 1 } C  X  1 ,i  X  { 1 , 2 ,...,n } for the training examples and z j  X  { X  1 , 1 } C  X  1 ,j  X  { 1 , 2 ,...,m } for all possible tags, where C is the code length. We also want to learn a hashing function f : R R R d  X  X  X  1 , 1 } C maps each example x i to its hashing code y i (i.e, y i = f ( x
The goal of tag completion and prediction is to complete missing tags for the training examples, and to predict tags for new query examples. Two main ingredients of constructing the compact hashing codes are: (1) A tag t j should be assigned to a data example x i if their corresponding hashing codes z j and y i are similar. (2) Similar data examples x i and x j should also have similar codes y i and y j . A natural way of defining the similarity between two hashing codes is to measure their normalized Hamming distance as follows: where dist Ham is the Hamming distance between two binary hashing codes, which is given by the number of bits that are different between them. It can be seen from Eqn.1 that the smaller the Hamming distance is, the more similar their hashing codes become. Note that the similarity between two hashing codes is a real value in the range of 0 to 1.
The first key problem in designing hashing codes is to ensure the consistency between the observed tags and the constructed hashing codes. Specifically, we propose to minimize the squared loss of the observed tags and the similarity estimated from the hashing codes, which is a commonly used loss function in many machine learning applications.
As discussed before, T ij = 0 can be interpreted in two ways that tag T ij is missing or the i -th data example is not related to the j -th tag, which indicates that T ij = 1 contains more useful information than a tag with value 0. Therefore, an importance matrix I  X  R R R n  X  m is introduced to denote the confidence of how we trust tag information in tag matrix T . We set I ij to a higher value a when T ij = 1 and I ij to a lower value b if T ij = 0. 1 . Then the square loss term becomes:
X
The other key problem in learning hashing codes is similarity preserving, which indicates that similar examples should be mapped to similar hashing codes within a short Hamming distance. To measure the similarity between data examples represented by the binary hashing codes, one natural way is to minimize the weighted average Hamming distance as follow: Here, S is the similarity matrix which is calculated based on the features of the data examples. To meet the similarity preservation criterion, we seek to minimize this quantity, because it incurs a heavy penalty if two similar examples are mapped far away. There are many different ways of defining the similarity matrix S . In SH [22], the authors used the global similarity structure of all data pairs, while in [21] and [24], the local similarity structure, i.e., k -nearest-neighborhood, is used. In this paper, we adopt the local similarity.

The entire objective function of the proposed HashTCP approach consists of two components: the square loss of tag consistency term in Eqn.3 and the similarity preservation term given in Eqn.4 as follows: min where  X  is trade-off parameter between the two terms. The constraints P n i =1 y i = 0 and P m j =1 z j = 0 are the bit balance constraints, which require each bit of the hashing codes to have equal chance to be 1 or -1. The balance constraints are equivalent to maximizing the entropy of each bit of the binary hashing codes to ensure each bit carrying as much information as possible.
Directly minimizing the objective function in Eqn.5 is intractable since it is a constrained discrete optimization problem. Moreover, the balance constraints also make this problem NP-hard to solve [22]. Therefore, we propose to relax the balance constraints into soft penalty terms and then relaxing the space of solution to [  X  1 , 1] C  X  1 . However,
In our experiments, we set the importance parameters a=1 and b=0.01 consistently throughout all experiments. even after the relaxation, the objective function is still non-convex with respect to  X  y and  X  z jointly, which makes it difficult to optimize. Fortunately, this relaxed problem is differentiable with respect to either one of the two sets of parameters when the other one is fixed, and therefore we propose to solve the problem by coordinate descent method similar to [19, 20]. In particular, we alternatively update  X  y and  X  z while fixing the other set of parameters by doing the following two steps until convergence.

Step 1: Fix  X  y , optimize  X  z . By taking the partial derivative of Eqn.5 with respect to z j , we can obtain the gradient below:  X 
Eqn. 5 With this obtained gradient, LBFGS method [27] is applied for solving this optimization problem to obtain optimal  X  z .
Step 2: Fix  X  z , optimize  X  y . By taking the partial derivative of Eqn.5 with respect to y i , we can obtain the gradient as: Similar to step 1, we use LBFGS method with this gradient to solve for the optimal  X  y .
After obtaining the optimal real value solution  X  y and  X  z for the relax problem, we need to binarize them to obtain binary codes y and z . In this work, we propose a novel binarization method that improves the quantization error through an orthogonal transformation by making use of the structure of the relaxed solution as follows: The above optimization problem can be solved by minimiz-ing Eqn.8 with respect to y , z and Q alternatively.
Fix Q , update y and z . The close form solution can be expressed as:
Fix y and z , update Q . The objective function becomes: In this case, the objective function is essentially a variant of classic Orthogonal Procrustes problem [17], which can be solved efficiently by singular value decomposition (we refer to [17] for details).
In order to deal with the out-of-example problem in tag prediction, we propose to learn linear hashing function to map the data examples into binary hashing codes as: length of hashing code is fix to 32 for all hashing methods. where H is a C  X  d parameter matrix representing the hashing function. Then the optimal hashing function can be obtained by: here  X  is a weight parameter for the regularization term to avoid overfitting.
We conduct our experiments on four datasets, including two image datasets and two text collections as follows:
Flickr 1 m [11] is collected from Flicker images containing 1 million image examples associated with more than 7 k unique tags. A subset of 250 k image examples with the most common 2 k tags is used in our experiment by filtering out those images with less than 10 tags. 512-dimensional GIST descriptors are used as image features. We randomly choose 240 k image examples as training set and 10 k for query testing.

NUS -WIDE [4] is created by NUS lab containing 270 k images associated with 5 k possible tags. We use a subset of 110 k image examples with 2 k tags in our experiments. 500-dimensional visual features are extracted using a bag-of-visual-word model with local SIFT descriptor. We randomly partition this dataset into two parts, 10 k for query testing and around 100 k for training.

ReutersV 1 (Reuters-Volume I) contains over 800 k manu-ally categorized newswire stories [13]. There are in total 126 different tags associated with this dataset. A subset of about 130 k documents of ReutersV 1 is used in our experiment by discarding those documents with less than 3 tags. 120 k text documents are randomly selected as the training data, while the remaining 10 k documents are used as testing queries.
Reuters (Reuters21578) 2 contains 21578 documents, and 135 tags. In our experiments, documents with less than http://daviddlewis.com/resources/textcollections/reuters2 1578/. 3 tags are removed. The remaining 13713 documents are randomly partitioned into training set with 12713 documents and 1000 test queries.
The proposed HashTCP approach is compared with five state-of-the-art methods, including two non-hashing methods TMC [23] and LM3L [10], and three hashing methods CFCode [26], STH [24] and SH [22].

Tag Matrix Completion (TMC) [23] directly completes the tag matrix by exploiting the tag correlation and data examples similarity to ensure the consistency between the observed tags and the estimated tags. Multi-Label Classification (LM3L) [10] proposes a large scale multi-label classification algorithm to overcome the training bias by incorporating correlation prior in a max-margin framework.
Binary Codes for Collaborative Filtering (CFCode) [26] constructs binary codes for collaborative filtering to recommend items to users. Self-Taught Hashing (STH) [24] combines an unsupervised learning step with a supervised learning step to learn effective hashing codes. Spectral Hashing (SH) [22] is proposed to learn compact hashing codes that preserve the similarity between data examples by balancing the hashing codes.
In the first set of experiments, we evaluate the perfor-mance of different algorithms by varying the number of training tags. In particular, for image datasets, we vary the number of training tags for each image from { 2 , 4 , 6 , 8 , 10 } . For document datasets, since the tags associated with each document is relatively small, we vary the number of training tags for each document from { 1 , 2 , 3 } . We then rank the tags based on their relevance scores in the completed tag matrix, and return the top ranked tags. We use the average precision (AP) as the evaluation metric. Tables 1 summarizes the results for different methods on two image datasets. Note that for all hashing methods in this set of experiments, we fix the length of hashing codes to be 32. It is not surprising to see that the performance of all methods improve with the increasing number of training tags. From these comparison HashTCP 352 1.23 126 0.38 63.75 0.24 16.93 0.05 TMC[23] 1537 N/A 528 N/A 234 N/A 56.34 N/A LM3L[10] 489 23.52 154 7.86 72.87 4.91 27.48 1.27 CFCode[26] 337 1.31 108 0.39 58.49 0.24 15.50 0.05 hashing code is fix to 32 for all hashing methods. results, we can also see that HashTCP achieves the same or even better accuracy to the non-hashing methods (i.e., TMC and LM3L) and substantially outperforms all other hashing methods (i.e., CFCode, STH and SH). Similar results have been observed on the document datasets. Our hypothesis is that the compared hashing methods focus on encoding the consistency of the hashing codes to the observed tags without considering the relationship among the data examples, which tend to overfit. On the other hand, the proposed HashTCP constructs binary hashing codes by simultaneously ensuring the learned codes to be consistent with observed tags and preserving the similarity between data examples, which indicates that HashTCP generates more effective codes and completes the missing tags accurately.

In the second set of experiments, we evaluate the efficiency of different methods on four datasets. The training time and tag completion time are reported in Table 2. We also fix the hashing bits to be 32 for all hashing methods. For the TMC method, we only report the training time since the tag completion procedure is embedded in its training procedure. From the reported results, it is clear that tag completion process of hashing methods is 20 to 25 times faster than traditional multi-label learning method LM3L. This is because hashing methods use binary codes to calculate the tag relevance scores, which only involves efficient bit-wise operations XOR, while traditional methods need to deal with real value vectors to compute the tag scores. We also observe that the training time of our method is comparable with other hashing methods and is much faster than TMC since the learning algorithm of TMC is quite involved with multiple terms. We will discuss more in next section that our method can achieve sub-linear time for tag prediction using a fixed small radius Hamming ball.
In the first set of experiments, we evaluate the effective-ness of different algorithms on four datasets. We use the full observed tags as the training tags and predict tags on the test examples. CFCode is not compared here since it cannot be directly used to predict tags for unseen examples. For TMC, we use the same experiment setting in their work for tag prediction. We use average precision (AP) and average recall (AR) as the evaluation metrics. The performance results are given in Fig.1. We can see from this figure that these performance results for tag prediction are consistent with the results in Table 1 for tag completion. From these comparison results, we find that the precision usually declines with the increasing of the number of returned tags, Figure 1: Performance of tag prediction for different methods on four datasets. The length of hashing code is fixed to be 32 for all hashing methods. while the recall usually improves. This is called precision-recall tradeoff which is also observed in [23].

In the second set of experiments, we evaluate the efficiency of two prediction metrics Hamming Ranking and Hash Lookup on four datasets. Hamming Ranking , which we use in all our previous experiments, ranks all the tags according to their Hamming distance from the query example and the top k are returned as the desired tags. Hash Lookup returns all the tags within a small Hamming radius r of the query example. Note that Hash Lookup is not applicable for non-hashing methods. The comparison results of both two metrics are reported in Table 3. We choose top 10 and 5 tags on image and document datasets respectively for Hamming Ranking . Hamming radius 2 is used for Hash Lookup . From this table we can see that the results of Hamming Ranking is consistent with the results in Table 2 for tag completion. For Hash Lookup , it takes even less time than Hamming Ranking in two image datasets which is about 120 to 130 times faster compared with non-hashing methods. This is because Hash Lookup returns tags within a small Hamming radius of the query code, which can be performed in sub-linear time [16]. Therefore, tag prediction using Hash Lookup can be conducted by only going through a small Table 3: Tag prediction time per query (sec) for different methods on four datasets. The length of hashing code is fix to 32 for all hashing methods. fraction of all possible tags which significantly improves the efficiency. Furthermore, we conduct an additional experiment to evaluate the effectiveness of Hash Lookup for different hashing methods. The results shows that the proposed HashTCP substantially outperforms other hashing methods in Hash Lookup setting, which is consistent with our expectation.
This paper proposes a novel efficient approach for tag completion and prediction by designing compact binary hashing codes for both data examples and tags. In particular, each data example is represented by a C -bit binary code and each tag is also represented using a C -bit binary code. We then formulate the problem of learning binary hashing codes as a discrete optimization problem by simultaneously ensuring the observed tags to be consistent with the constructed hashing codes and preserving the similarities between data examples. Extensive experiments on four datasets, including two image datasets and two document collections, demonstrate that the proposed approach can achieve similar or even better accuracy with state-of-the-art methods while using much less time. This work is partially supported by NSF research grants IIS-0746830, CNS-1012208 and IIS-1017837. This work is also partially supported by the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370.
