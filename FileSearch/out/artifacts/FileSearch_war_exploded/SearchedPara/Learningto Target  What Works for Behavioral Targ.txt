 Understanding what interests and delights users is critical to effective behavioral targeting, especially in information-poor contexts. As users interact with content and advertis-ing, their passive behavior can reveal their interests towards advertising. Two issues are critical for building effective targeting methods: what metric to optimize for and how to optimize. More specifically, we first attempt to under-stand what the learning objective should be for behavioral targeting so as to maximize advertiser X  X  performance. While most popular advertising methods optimize for user clicks, as we will show, maximizing clicks does not necessarily imply maximizing purchase activities or transactions, called con-versions , which directly translate to advertiser X  X  revenue. In this work we focus on conversions which makes a more rel-evant metric but also the more challenging one. Second is the issue of how to represent and combine the plethora of user activities such as search queries, page views, ad clicks to perform the targeting. We investigate several sources of user activities as well as methods for inferring conversion likeli-hood given the activities. We also explore the role played by the temporal aspect of user activities for targeting, e.g., how recent activities compare to the old ones. Based on a rigorous offline empirical evaluation over 200 individual ad-vertising campaigns, we arrive at what we believe are best practices for behavioral targeting. We deploy our approach over live user traffic to demonstrate its superiority over ex-isting state-of-the-art targeting methods.
 H.4.m [ Information Systems ]: Miscellaneous Algorithms, Performance, Experimentation Behavioral targeting, user modeling, advertising
Advertisers want to spend the smallest amount of money to get the largest increase in profit, irrespective of the form of advertising they are involved in such as newspapers, mag-azines, television ( offline advertising ), and internet (online advertising ). However, online advertising provides adver-tisers with more immediate feedback (when users click on their ads and visit their web pages) and publishers have greater knowledge of their users (demographic information and past behavior). For example, suppose that you want to sell trucks. You could advertise during baseball games and football games on television, hoping that people watching the game want a truck. If you advertise online, you could put your truck advertisements next to sports stories on an online publisher X  X  site. Also, you could have your adver-tisements appear in  X  X ar and Driver X  or on  X  X ahoo Autos X  (where the content is related to the ad). But the more re-markable power of online advertising is that you could show an advertisement next to a sports page only for people who have recently visited Yahoo Autos. This is behavioral tar-geting .

The key behind behavioral targeting is that the advertisers can show ads only to users within a specific demographic of high value (such as people likely to buy a car) and combine that with a larger number of opportunities (places to show ads) per user. Moreover, as we gather more information about a user, we can provide them with a better experience in every future interaction.

In general, targeting methods match the users within a given context to an appropriate ad. The context of the user consists of the page the user is currently visiting, the time, and the user X  X  historical online behavior. Three types of targeting methods are popular in the advertising industry: property , user segment , and behavioral targeting . Property targeting refers to placing ads on specific web pages where interested users will appear, such as showing online broker-age ads on financial related pages. Although this reaches users who visit these finance pages, it may miss users who paper. * Work done while at Yahoo!. use some other web sites for their financial information. User segment targeting focuses on the gender and age of a user, and is only capable of targeting broad groups. Behavioral targeting involves using historical online information about the user to aid the publisher in showing them relevant ads wherever they appear. Whereas property targeting targets pages, and user segment targeting targets generic groups, behavioral targeting targets individuals.

Clearly, understanding user interests is critical to effective behavioral targeting. As users interact with content and ad-vertising, their passive behavior can reveal their interests to-wards advertising. Exploiting this behavior at a large scale is the goal of behavioral targeting solutions. In this paper we frame the targeting problem as an optimization problem and describe a machine learning based approach for solving it. We tackle with two key issues in doing so. First, it is im-portant to optimize behavioral targeting so that advertisers get most bang for their buck. Most existing work in behav-ioral targeting uses clicks on ads as a proxy of interest [4, 11, 14]. Clicks are used simply because they are available and other information is not available at a large scale. Recently, however, advertisers have been willing to share feedback at the level of individual users, telling publishers which of the users who saw the ad have actually purchased the product [1, 2, 3, 6, 12]. Since conversions are the ultimate goal of advertisers, we test whether models based on conversions or clicks better predict whether a conversion eventually occurs. In particular, we show that maximizing for clicks does not lead to maximizing for conversions, hence providing evidence for the necessity of developing models that are specifically optimized for conversions.

Conversions are so rare that developing models for them presents further challenges, which is the second issue we ad-dress in this paper. Rarity of conversions forces us to parsi-moniously mine the user historical online behavior. Several questions are worth investigating here. For example, how should we represent and combine different user activities such as search queries, page views and ad clicks? Which ac-tivities are more indicative of user X  X  interests, browsed pages or issued queries? Does more activity on a certain topic imply more chances of converting? How does the temporal aspect of user behavior (i.e., timestamps associated with the activities) relate to conversion likelihood?
Our main contributions in this paper are as follows:
Advertisers are now spending increasingly larger fractions of their overall advertising budgets on online advertising. Effective online advertising is based on three key factors: the context in which the ad appears; the audience to whom the ad is targeted; and the creative that specifies the message being delivered via the ad. We focus on the audience for the purposes of this paper.

Next we present a generic framework for understanding audience targeting problems. Users are modeled as streams of typed events . The targeting system is modeled as a func-tion defined over user histories. Within this framework the popular targeting methods such as property, user segments, and behavioral targeting are special cases.
We denote a user u  X  U as a process that emits a sequence of events  X  e 1 ,...,e m  X  , where events e i  X  E are defined as follows: where t is the timestamp , T is the type of the event such as  X  X earch X  or  X  X age visit X , and p is a payload . Examples of events include  X  X ssued a search query for shoes at 5PM X  in which the type is  X  X earch query X , the payload is  X  X hoes X  and the timestamp is 5PM.

Any targeting method can be abstracted as a function such that given a particular ad campaign a , the function outputs a binary decision as to whether or not to target the user. The function is defined as g a : U  X  { 0 , 1 } where for each user u  X  U , whether the user should be targeted or not. As a targeting function, it cannot guarantee that the ad will be shown to the user. We can then abstract an ad server as a process that for a given a set of ads a  X  A , performs two main operations at each serving opportunity. First it determines the set of ads for which the user is said to qualify as follows: where the function Q : U  X A  X  2 A returns the set of ads which the user is allowed to see  X  qualify. Given this set of ads, the ad server then decides which one of these ads should actually be shown to the user, based on some set of business policies which are outside of the scope of this paper. Based on this description of an ad server, we see that the purpose of a targeting system is simply to target users prior to the actual ad serving.
Property targeting is a simple, yet popular targeting mech-anism. The advertiser specifies some set of pages P on which the ad should be shown. Users visiting those page are tar-geted with the corresponding ads. For example, an adver-tiser who sells cars could show ads on websites about cars.
This form of property targeting is completely independent of the user X  X  history or state, instead it exploits exactly one feature  X  the page the user is visiting at this moment. The property targeting function is defined as follows: where p is the current webpage. The effectiveness of this model assumes that there is a causal link between the page and the user X  X  interest.
Marketing research and practice has a long history of ad-vertising based on user profiles or personas [10]. Users are grouped into homogeneous segments and different segments are targeted with appropriate ads. For example, a major beverage company conducts market research for its low-calorie carbonated beverage and determine that the drink would appeal to the following segments: Segments are defined as logical predicates such as age ( u, (15 , 25)) which is true if the age of user u is between 15 and 25 years. Common types of segments are demographic , geographic , and psychographic attributes [8].

In the above example, the advertiser would define the tar-geting function as follows: where S i is the i th segment and g i,j is the set of users who qualify for the j th predicate of segment S i .

Segment targeting is popular since it is easy to understand and implement and moreover provides advertisers transparency and control over the audience selected for targeting their ads. There are two main limitations. a) The advertiser must match the ad to the pre-existing segments. b) The segments may not be expressive enough to truly capture the audience of interest to the advertiser. For example in the  X  X oms X  segment, we are missing the important attribute hasChildren ( u ), which is clearly a defining feature of the persona.
Behavioral targeting provides an approach to learn the targeting function from historical data [4, 14]. In general, behavioral targeting focuses on leveraging the past behavior of a user to predict their future behavior. Although behav-ioral targeting methods vary in terms of the set of features, the objective optimized, they can be generalized within our targeting framework. A behavioral targeting method B can be viewed as a process that learns a targeting function g ( u ) from data about users and ads. Formally this can be de-scribed as function B : { U } m  X  G that maps a sample of m users from the set of all users U onto a specific space of targeting functions G such as linear classifiers trained on feature vectors constructed from user histories.

The predictive model can leverage a very rich set of user features and can be trained to directly optimize for the per-formance of the ad, e.g., CTR, conversion rate etc. More im-portantly, the framework of the behavioral targeting model is general enough to accommodate other information such as the current context  X  the webpage the user is visiting, mem-bership in various user segments, and more recently social network data [11].

Continuing with the targeting example, online behavior could be used for targeting rather than simple segments. The focus of this paper is on behavioral targeting that goes beyond age and gender. By training our algorithm on actual data, we can move beyond simple ad targeting based on stereotypes (showing pickup trucks during baseball games) and towards a personalized experience that can benefit the user and the advertiser alike.
In our setting, we are trying to improve various existing campaigns where the advertisers pay per conversion, called CPA campaigns. Each campaign has already been tuned manually with user segment targeting and property target-ing. However, no behavioral targeting has been done. As noted in previous work [3], our objective with this system is to refine the targeting constraints using past behavior of the user. Through refinement we can improve the number of conversions per ad impression without greatly increasing the number of impressions, which increases the value of our inventory. 1
Note that when making a decision about whether to tar-get an impression based on user behavior, we cannot use any information from the day of the impression or any day afterward to impact the classification of the impression. As shown in Figure 1, we consider user history as a sequence of events relative to some target time ,  X  , at which time the user is being considered for targeting. We decompose the user X  X  sequence of events around the target time  X  as follows: Here E F (  X  ) denotes the events prior to the target time which we call the feature window and E T (  X  ) denotes the events that occur between  X  and  X  +  X  (where  X  is 1 day) which we call the target window .

Hence, when we model behavioral targeting as a machine learning task where each user history forms an example: a user is a positive example if it is credited to a conversion in the target window, a negative example otherwise. 2 Given training users { 1 ,...,m } define T =  X  ( x 1 ,y 1 ) ,.. ( x (
R n  X  { X  1 , +1 } ) m to be the training data, where x i is a feature vector constructed from the events of the user i in
A lthough it is interesting to imagine relaxing these con-straints, that is impossible to analyze offline.
If several impressions are shown to a user and the user eventually converted, no more than one impression from that user is considered a positive example. F igure 1: Targeting model is trained on user histo-ries (rectangles) as they existed prior to the start of the conversion process (open circle) that led to the conversion (solid circle). For evaluation, all users are given the same target time (yesterday) and the ad server may choose to show ads at some point in the future to start the conversion process (open circle). the feature window, y i = +1 if the i th example is positive, and y i =  X  1 otherwise. The test set is defined similarly.
The most significant difference between traditional tar-geting and this work is in exploiting the user profiles for maximizing conversions. Hence, in this paper we fix our learning algorithm to be a support vector machine [13] and focus on the issue of how to leverage user history efficiently. In particular, how to convert user profiles into feature vec-tors (e.g., representing different events as features, comput-ing their weights, incorporating timestamps) and how to construct target labels (e.g., clicks, conversions). A user representation method consists of a function  X  : U  X  R n , where R n is the Euclidean space of dimension n . A target label function is defined as  X  : U  X  { X  1 , +1 } . Given this, we extract an appropriate feature and target label set as ( x,y ) = (  X  ( u ) , X  ( u )). We then select a vector w  X  solving the following optimization: where L ( X  y,y ) = max(1  X   X  yy, 0) and C is a constant that controls the balance between regularization and minimizing the loss on the training set.

In this work, we investigate different user representation operators  X  ( u ) and different target labeling functions  X  ( u ). These are summarized as follows:
In this section we propose and empirically compare differ-ent feature extraction methods for behavioral targeting.
In our experiments we build targeting models for display advertising campaigns. We collected 4 weeks of data for 226 c ampaigns. Each campaign is treated as a separate targeting task. The first three weeks of data is used for training, while the last week is used for testing. Each example impression is preceded by at least 4 weeks of user events. In total, with some sampling of negative examples we have a total of approximately 40M examples in training and 80K of these examples are positive. This is a benchmark set that enables us to do rigorous offline experiments.
In each of the experiments, we compare targeting meth-ods in terms of the area under the ROC curve (AUC). The benchmark baseline consists of a simple application of the support vector machine classifier. Based on some simple ex-periments, we arrived at the following parameters. Because we have a large number of mostly irrelevant features, we choose a strong regularization parameter of C  X  [0 . 001 , 0 . 05]. Next, since we have an highly imbalanced class distribution, we choose a class weighting parameter of 10:1 cost for the positive and negative class, respectively. In order to have a single parameter configuration for all learning tasks, we sam-ple the negative examples such that the class ratio is always satisfied. The empirical evaluation is based on this base-line. Unless otherwise specified, all metrics are measured as conversion-weighted average of AUC across all campaigns in the benchmark set.
When collecting the user X  X  historical online behavior, we consider both active and passive activities. Passive activities include viewing ads and visiting pages in which an action is not specifically required upon seeing the page. Active activ-ities include issuing search queries and clicking ads in which users actually perform an action on the page. Browsing ac-tivity is somewhat active because the user took action to visit the page, but we argue that the activity is less than specifically typing a search query or clicking on an ad. We investigate which source of activity is stronger in predicting the user X  X  propensity to convert on a set of advertisements.
The activities of the users are a sequence of events col-lected from server logs. Events are associated with both a timestamp and metadata. For example, an event could be a visit to a finance web page and the metadata associated with the event is the content of the page, which is logged separately and then joined with the event along with the anonymized identifier of the user and the time. We consider several different events, each with a corresponding feature extraction method. For all feature types, we use a common feature representa-tion (i.e., weighting) operator  X  : E ?  X  R n which we call relative frequency bag of events. The frequency of an event is defined as the number of days in which the user has per-formed the event. We consider events separately by type (e.g., page visits, search queries). For example we concen-trate on page visits and denote by event p the  X  X isit to any e-mail page X  and other page view events by q and r . If the user had visited the pages p , q , and r in a sequence over four days as follows: where each event p i denotes the visit on page p on day i , then the frequency bag of events representation for page visits would be: F p ( e ) = ( p : 2 ,q : 2 ,r : 1) where we use the convention of p : n to mean that the feature p has the value n in the feature vector. Here, the page p was visited 3 times but on just 2 distinct dates. The relative frequency repre-sentation normalizes within each feature type such that the final feature vector is defined as follows: where n is the number of feature types such as page views, ad views, ad clicks, etc.
As users interact with a website they consume content either through direct action such as searching for informa-tion or casual browsing such as checking e-mail or reading news headlines. However, the particular stream of content consumed gives us some insight into the user X  interest.
Rather than considering each individual URL, we group different URLs together and count visits on any of them. We considered 3 grouping methods: site index , semantic cate-gories , and server location . The site index is based on the navigational pattern within the specific website. For exam-ple, if a user visits a page showing the profile of a company in the stock market, then the site index would be something like X  X tock quotes X  X hich is a sub-index of the main X  X inance X  page. This reduces the set of URLs from millions into ap-proximately 30,000. The benefit is a reduced space, but the main disadvantage is that we lose the granular information such as which stock quote the user is viewing. The semantic categories come from a manually annotated categorization system and map the pages into a taxonomy of roughly 1,000 nodes that describe the information on the page. In our example of a stock quote page, we consider the categories along the path in the taxonomy. If the category is  X  X inance / Stocks / Quotes X , then we add the features  X  X inance X ,  X  X i-nance / Stocks X , and  X  X inance / Stocks / Quotes X  to the user X  X  profile. With the rollup, the model should consider the hierarchical placement of the page even when the cover-age is low at the leaf nodes. Finally, we reduce the space of pages even further to consider the international location of the user when he or she saw the page. For example, if the user is viewing a finance page in the US, then the feature would be  X  X age in US X , however if user were to see the same page in Europe, then the page would be  X  X age in EU X . This encoding of pages is at a high level of aggregation but can still tell us something about the user X  X anguage, location, etc.
Table 1 compares the improvement in average area under the ROC curve for the benchmark campaigns when consid-ering the different browsing patterns. In the figure the user location information makes the baseline and has the least Table 1: Relative difference in performance relative to user location when adding more browsing activity. Table 2: Relative difference with respect to baseline when adding more ad activities. information , but we note that it is significantly better than random targeting despite being very coarse. The site index grouping performs very well as a single source of page visits, slightly better than the semantic categories. We think that the semantic categories can be too coarse as a 30:1 decrease in the number of features. Overall, however, adding all page browsing features is best, which indicates that the different aggregations bring some useful signal.
When users visit pages, they typically interact with graph-ical ads either by clicking or simply viewing ads. Although ad clicks are clearly useful features in predicting ad activi-ties, the ad views may also be useful. To see this, we con-sider the following example, an advertiser targets users who previously visited their home page and the ad is shown to these users anywhere they go (this is a common targeting method). If the only way that the users have seen this ad is by going to the advertiser X  X  site in the past, we can infer that if the user saw the ad, he or she must have visited the advertiser X  X  site regardless of whether we have the browsing history.

Like the browsing activity features, we consider multiple groups of ad features. Ads are placed on pages in pre-specified positions such as on the left or right sides and in different sizes such as 300  X  250-pixel rectangles or tall verti-cal bars. Of course each ad is a different image and may be a static image, video, or flash. All of these features influence the click rate on the ad as well as the degree to which the user notices the ad. In order to capture all the factors that influence ad interest, we denote an ad by a triplet of: cre-ative, position id, and targeting parameters. Each of these elements is denoted by a unique identifier. Like the browsing history, we consider both the clicks and views for ad triplets as well as clicks and views on semantic categories of ads.
Table 2 shows the relative improvement of adding catego-rized ads to raw ads. We see that there is not much difference in performance for ad triplets and categorized ad activities when treated separately. These results indicate that catego-rized ads do provide some signal in addition to just the raw ad activities.
Presumably the most direct user behavior is a search query because users have to manually type in the query. The po-Table 3: Relative difference with respect to base when adding more query features. Table 4: Relative difference in feature types. For query activity (*), we note that the testing set is a subset of the larger set. tential disadvantage to queries is that they are typically very short and are very ambiguous, containing misspellings, syn-onyms, different spacing etc . We consider three types of behavior for queries: simply issuing a query, clicking on a link, and clicking on a search ad on the results page for the query. We group all these activities together and eval-uate whether they are predictive of conversions. We group queries together into semantic categories using a machine learned query categorizer. We consider both categorized and raw queries. For the raw queries, we split on white space to create unigrams.

Table 3 shows the relative improvement from adding cat-egorized query features to raw queries. There is a perfor-mance decrease from considering only the categorized queries versus raw queries. This is interesting when we consider that there are only about 1,000 categories but millions of unique queries issued by the users. This suggests that the catego-rization of the queries may be noisy or incomplete. However, it seems that when available the categorized queries help in conjunction with the raw queries.
We see that within each feature type, categorized features alone do not perform as well as the raw features. However, the best strategy seems to be to combine raw and catego-rized features. We now consider adding all the feature types together. Table 4 shows that, relative to user location, con-sidering each feature type alone does very well. In addition, adding all feature types does very well. Of all the features we considered, ads and browsing activity appear to be the most informative.

In Table 4 we see that queries show the largest improve-ment in performance; however, we argue that this is mis-leading. To better understand the results, we consider the coverage of the different feature types. We define coverage as the proportion of users that have the feature defined over the set of all users. From our dataset we see that all users have some browsing activity, and nearly all users have some ad activity. However, a smaller proportion of users have search activity. And the fewest users have some ad click activity. This indicates that the most intuitively informa-tive features: searches and ad clicks have the lowest cover-age. Hence, although they are very informative and might have good performance when available, their overall impact is lower. Based on the coverage analysis, we see that even if browsing activity is less informative when users have ad click or query activities, when we consider the union of all activities, those which occur most frequently are preferred X  especially in regularized learning algorithms.

Table 5 shows the relevant features from 3 example cam-paign models. These features support our hypothesis that the baseline approach of adding all features is better over-all. In both the university model and the airline, we see that searches are relevant. In addition, these searches are very relevant to the subject of the ad such as air travel or airports. However in the home telecom model, we see that ads are the most relevant features. To see why ads are most relevant for home telecom versus the other campaigns is that for the telecom service there are no good search terms that indicate interest. Instead, the main customers are people who own homes or rent apartments. Viewing health-related ads and checking news and mail are activities common to homeowners  X  the events that describe the persona of the target audience.
In Section 4.3, we use L 2 normalized frequency across each feature group (e.g., page visits and search queries) to define a baseline feature representation. In the following section, we discuss an alternative representation based on statistical measures of how predictive individual features are of con-version events.
We tried using a feature representation based on log-likelihood ratios (LLRs) of raw features occurring in conversion events. This approach is based on the campaign-dependent feature representation described in [7]. In this representation, ad campaign i extracts feature vectors of the form,  X  i ` e j  X  `  X  tures are derived from the corresponding baseline feature vector of Section 4.3:  X  ` e j  X   X  `  X  j 1 ,..., X  j N  X  feature  X  j n exists in example e j  X  that is, if it has a non-zero count  X  then feature  X  j i,n is set equal to the log-likelihood ratio of  X  n occurring in a conversion event for ad campaign i . We use c ( a i ) to represent a conversion event on ad campaign i (conversely, c ( a i ) represents a non-conversion event). Fea-ture  X  j i,n can now be expressed mathematically as follows:  X  i,n  X  1 (  X  We also define feature  X  j i, 0 as It can be shown that if the baseline features are conditionally independent of one-another given c ( a i ), then the following is true [7]: where  X  + ` e j  X  is defined as Table 6: Gain in AUC for LLR-based feature repre-sentation relative to baseline.
 Since summing over  X   X  j i, 0 ,..., X  j i,N  X  yields the logit of Pr ` c ( a i ) |  X  + ` e j  X   X  , it follows that if the  X  are conditionally independent of one-another given c ( a i then  X  i ` e j  X  is the globally optimal representation of  X  (note, however, that this optimality depends on one X  X  ability to exactly compute the LLRs in (1) and (2); this is often not possible in practice). One can further show that the following linear decision function is equivalent to a Naive Bayes classifier trained on  X  + ` e j  X  if the weight vector w in the following expression is composed of all ones (i.e., w = (1 ,..., 1) T ) : Here,  X  y i ` e j  X  represents the hypothesized class (1 or -1) of example e j in ad campaign i . In our experiments, we es-timated the probabilities in (1) and (2) empirically from the training data. We used the following count threshold when estimating probabilities: For all  X  n that appear in fewer than 5 conversion examples in ad campaign i , we set  X  i,n  X  0 for all j .

Table 6 shows the AUC results for the LLR-based fea-ture representation relative to the baseline. We divided our campaigns into three categories, large, medium and small. The large group contains the top one-third of the campaigns with the most number of conversions, while the small group contains the bottom one-third. These results show that the LLRs tend to outperform the baseline representation on large campaigns but not on small campaigns. One ex-planation for the reduced performance on small campaigns is that the probability estimates in Equation 1 tend to be noisy when only a small number of events are available. We believe that it may be possible to improve the performance of the LLR-based representation on all campaigns by using smoothed probability estimates when computing the LLRs. We leave this as future work.
As mentioned before, behavioral targeting can be opti-mized using a variety of metrics, e.g., number of ad impres-sions (CPI), clicks obtained (CPC), number of conversions F igure 2: Improvement in prediction accuracy by us-ing conversions for training instead of clicks. Testing is done using conversions in both cases. made (CPA), revenue earned. While CPA (revenue) is the end goal, clicks (CPC) are often used for evaluation since they are easy to instrument and measure. In this section we study the relation between clicks and conversions.
We start by performing the following experiment: use clicks to predict conversions and see how it compares to using conversions. In other words, in one case we train the model using clicks in the training set while in the other case the model is trained using conversion examples. Of course, the testset is labeled by conversions in both cases. As before we use support vector machines for training the models.
Clearly, using conversions for both training and testing is naturally superior. But if this hypothesis is true that clicks are  X  X ell aligned X  with conversions, then using clicks for training should also perform comparable.

In Figure 2 we show the improvement in prediction accu-racy achieved by using conversions for training, in compari-son to using clicks for training, on 10 large campaigns. The y-axis is the difference between the performance of using conversions and using clicks (in terms of the area under the ROC curve for a campaign). Except on campaign index 4 and 5, on other campaigns using clicks performs significantly worse than using conversions. This implies that targeting users based on clicks does not necessarily mean maximizing for conversions.
From the previous experiment we found that clicks and conversions are related but cannot be substituted for each other. In other words, for predicting conversions it is better F igure 3: Improvement in prediction using clicks in conjunction with conversions. to train the model using conversions. However, in a practical setting we often lack positive examples to train conversion models for two reasons: (a) clicks are known to be rare and conversions are even rarer, (b) due to business constraints, examples from different campaigns are not allowed to be mixed together. As a result, while we can afford to learn conversion models on large campaigns, for the medium and small campaigns we face great difficulty in doing so. Also, when the model is learned using a small number of positive examples, it often does not perform well on the testset.
To deal with this challenge, we tried a hybrid approach whereby we use clicks, which are much more abundant com-pare to conversions, in conjunction with conversions to train models. In particular, we treat those examples as positives which contained any of the two, a conversion(s) or a click(s). It makes sense because clicks hint of positive intent of the user. Hence, instead of grouping them with negative exam-ples, perhaps we can generalize from them to identify other potentially interested users.

Next we compare the performance of the following two approaches: (a) label positive examples for training using conversions only and (b) using both clicks and conversions. The results are shown in Figure 3. We divided our cam-paigns as in Section 4.4 into three categories, large, medium and small. We treat the performance of the conversions-only approach on the small group as the baseline and show other performance numbers with respect to this baseline.
We notice that the performance of conversions-only ap-proach worsens as we go from the large to the small group. This is expected because as the positive examples get rare, it becomes difficult to train models and perform well. We note that in the medium and small campaigns, by using clicks we can improve the performance significantly. This vali-dates how clicks examples show positive intent of the user and when conversions are scarce, these examples can help in generalizing the model.

When there is abundant positive conversion data, like the campaigns in the large group, click examples do not help. In fact, they make the model slightly worse, which is in line with our observation from the previous section (Figure 2).
In this section we focus on understanding the temporal aspect of user history. In particular, we investigate how far we should track back the user history before it stops pay-ing off in terms of prediction performance. Also, we explore whether recent activities are more indicative of online pur-chases than the older ones.
In this experiment we study the effect of history length on prediction performance. More specifically, when history length is set to l days, we take into account the user history from [  X   X  l, X  ] period to make the prediction where  X  is the target time. We treat the one day history length ( l = 1) as the baseline and show other performance numbers with respect to this setting. The results are shown in Figure 4. We note that using the recent two weeks of user history performs significantly well. However, to our surprise, we note that we get a substantial improvement by extending the history from 2 weeks to 4 weeks.

These results show that a short recent history may not be enough for behavioral targeting. A reason behind this is that, broadly speaking, users make two kinds of conver-sions/online purchases on the web: (a) transactions which are quick and do not take much thought such as purchasing a movie ticket ( short-term conversions ) and (b) transac-tions which span a longer period of time such as car purchase or online education enrollment, and can often take several days/weeks for the user to make up her mind before making the purchase/conversion ( long-term conversions ). While short-term conversions can be made using a short history (of 1 or 2 weeks), in order to be able to predict long-term conversions, a longer history of user X  X  activities is essential.
Longer history can also help with short-term conversions because more data allows better inferencing and refinement of user interests, e.g., within movie enthusiasts distinguish between those interested in horror versus romantic movies. As a result, longer history leads to better targeting of users, in general. However, beyond a certain history length the improvement obtained (in terms of revenue dollars) by ex-tending history might surpass the cost of maintaining the systems storing the data. Hence, in a practical setting one might have to truncate the history beyond a certain length.
From the previous section we concluded that longer user history leads to better performance. In this experiment we investigate whether by giving more weight to the recent ac-tivities in comparison to the older ones, we can further im-prove the performance.

As described before (Section 4.3), we set the weight of each feature in user profile based on the number of days on which the feature is present. Hence, we do not not distinguish between the features which occurred in recent days and those which had occurred earlier. For this experiment, to give more weights to recent features we set the weight of feature f to: where  X  is the target time (when the prediction is being made),  X  is the decay factor and ( f,u ) is the sequence of days on which feature f is present in the history of user u . When  X  = 1 the above weighting approach reduces to uniform weighting. By setting  X  to a larger constant, we can give more weight to recent user activities in comparison to the older ones. (The weights are then normalized within each feature type, as before.)
In Figure 5 we show the effect of decay constant  X  on the prediction performance. (We treat  X  = 2 as the baseline and report other performance numbers with respect to it.) For the sake of completeness, we also show  X  &lt; 1 which favors older features compare to the recent ones. It is clear that the best performance is achieved by uniform weighting (i.e.,  X  = 1). This is somewhat counter-intuitive since we expect recent activities to be a better indicator of user interests.
Given our experiments from Section 6.1, a possible expla-nation for this result can be that decay effectively shortens the user history. For example, when  X  = 2 a feature that is 10 days old gets a weight of smaller than 10  X  3 , hence the history length is practically reduced to 10 days (or smaller). As a result, long-term conversions (explained earlier) be-come more difficult to predict. In other words, there is a trade-off between recency and history length. While the former might help in predicting short-term conversions, the latter is needed for long-term conversions.

To further validate this finding, we tried another approach for taking recency into account. For this experiment we di-vided the user history into multiple buckets based on time periods, e.g., activities from the last day are put into one bucket, activities from 2-7 days in the next bucket and so on. We labeled the features with the buckets they occurred in and let the SVM learn weights for different (feature, bucket) Table 7: Gain in conversion rates and eCPA values over existing CTR-based targeting methods in live experiments. pairs. Unlike the previous experiment where we were con-fined to exponential decay, this method allows us to auto-matically learn non-uniform weighting of features to account for recency.

However, similar to the decay experiment, we found that the results from this bucket experiment were worse than the uniform weighting approach.
Driven by the good performance of our behavioral target-ing approach in offline experiments, we tested the system on live traffic on a few ad campaigns on a major US adver-tising network. We trained our models using the practices described in the previous sections. We compared our models to those trained by an existing behavioral targeting system (as in [4]): (a) which targets users whose activities/interest in the advertiser X  X  category is above a certain threshold, (b) the model optimizes for click-through rates.

For training/scoring the models we generated user profiles spanning 8 weeks of user history. We scored the models on a daily basis and the experiment was run live for three months. For each of the campaigns, each of the existing and new models received at least a total number of 1M impressions on a monthly basis.

Table 7 shows the overall performance of our models as compared to the original CTR-optimized behavioral target-ing models. In terms of the conversion rate, i.e., the per-centage of viewers that convert, we note that our models achieved significant improvements compared to the existing models (while keeping the same coverage). This demon-strates the ability of our models to target users with much higher tendency to convert than existing CTR-optimized models. Another important metric that we report is the Effective Cost Per Action (eCPA), used to measure the ef-fectiveness of the inventory purchased by the advertiser. Ef-fectively, the eCPA tells the advertiser what they would have to pay for each conversion. Our results show a considerable decrease in eCPA that our models achieved compared to the other models (we could not report eCPA values for campaign 3 and 4 for business reasons). This is considered as a ma-jor gain from the point of view of the advertisers, as this means that we are able to reach the audience desired for the campaign while decreasing the amount of impressions (i.e., retries) needed. Both these results demonstrate the effec-tiveness of our models in large-scale behavioral targeting.
In this work, we use behavioral targeting to improve the conversion rate. Targeting users who will convert is a diffi-cult problem: often, the problem is divided into predicting clicks, and predicting the probability that the click will con-vert [6, 1, 12]. The advantage of this division relates to business logic: the publisher (such as Yahoo! or Google) has data about how likely users are to follow various paths towards clicks on advertisements on their site. On the other hand, advertisers have more information about the paths of users on their website. Therefore, there is a certain cleanli-ness with regards to data ownership.
 On the other hand, paying for conversions has two effects. First of all, there is the maximum level of quality control of the traffic. Problems such as click fraud [5, 9, 15] do not arise. Second, when creating a conversion model, one is aware of the abundance of users who did not click. This plethora of negative data can really help: intuitively, know-ing that someone was unlikely to click makes it quite possible that they are unlikely to convert.

In this paper we focused on building such conversion mod-els. We compared our approach with existing behavioral targeting methods (such as [4, 14]) which optimize for click-through rates and showed how optimizing directly for con-versions can lead to improved performance. Compared to previous work on conversion optimization [1, 2, 3, 6, 12], our work makes several new contributions: we look into under-standing the effect of different user activities on prediction, give insights about the temporal aspect of user behavior (re-cency vs. long-term trends) and explore different variants (user representation and target label) through large offline and online experiments.
Determining which users are most likely to respond to a given advertisement is the primary goal of targeting. By us-ing the user X  X  historical online behavior, behavioral targeting can greatly improve in terms of performance and reach rela-tive to hand-tuned segments. In training models for behav-ioral targeting, we are concerned with both what to predict and how to leverage user profiles. Our empirical analysis of over 200 advertising campaigns has yielded some useful best practices for targeting.

First, we compare whether training on clicks or conver-sions is best. Our results indicate that clicks often suffice but they are not always good proxies for conversions. By predicting directly for conversions, we can now improve the impact of targeting methods on ad campaigns. Most existing behavioral targeting systems are limited to clicks because it is the only data readily available. But, recently, advertisers have been willing to share individual responses to ads [3], facilitating such conversion-optimized models.

Second, of critical importance to behavioral targeting is the online behavior of the users. Although the behavior we can observe is only passive, we find there is considerable signal in the profile. We first consider the types of events: browsing, ads, or query activity. The results indicate that although query activity is quite useful, browsing activity is more valuable overall. Then given a set of selected user events, we consider a feature representation (LLRs) that en-codes the correlation between feature and conversion. We found that using LLRs improves performance on large cam-paigns but not on small campaigns. Next we turn to the amount of user history to use for prediction. The results indicate that more data is better, but there is a point of di-minishing returns of roughly 30 days which is primarily due to the changing nature of user interests and web sites. We next find that rather than explicitly incorporating tempo-rality into the features, simply aggregating events performs well.

Lastly, based on extensive offline and online experiments, we validate our findings and have arrived at several best practices for behavioral targeting. [1] N. Archak, V. S. Mirrokni, and S. Muthukrishnan. [2] A. Bagherjeiran, A. O. Hatch, and A. Ratnaparkhi. [3] A. Bagherjeiran, A. O. Hatch, A. Ratnaparkhi, and [4] Y. Chen, D. Pavlov, and J. Canny. Large-scale [5] I. Click Forensics. Click fraud index. [6] Google, Inc. Google analytics. [7] A. Hatch, A. Bagherjeiran, and A. Ratnaparkhi. [8] I. Nielsen Company. Nielsen Claritas PRIZM. [9] Y. Peng, L. Zhang, M. Chang, and Y. Guan. An [10] B. J. Pine. Mass customizing products and services. [11] F. Provost, B. Dalessandro, R. Hook, X. Zhang, and [12] B. Rey and A. Kannan. Conversion rate based bid [13] X.-R. Wang, K.-W. Chang, C.-J. Hsieh, R.-E. Fan, [14] J. Yan, N. Liu, G. Wang, W. Zhang, Y. Jiang, and [15] L. Zhang and Y. Guan. Detecting click fraud in
