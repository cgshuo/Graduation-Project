 Information retrieval is the process that provides users with the most relevant documents from an existing collection [ 1 ]. The semantics of queries submitted by users to search engine, even though specified as questions, are inherently ambigu-ous. For example, given the query:  X  X hat is big bang? X , it may be about the band named  X  X ig bang X , and also may be about  X  X ig bang theory X , even about the TV play named  X  X ig bang X . Various retrieval models [ 2  X  4 ] have been proposed. They return a relevant list of query results. However, due to the ambiguity of query semantics, it is impossible to design a fixed ranking scheme which could always perfectly measure the relevance of query results pertaining to users X  inten-tions. To compensate for the inaccuracy of ranking functions, document snippet has become almost standard components for search engines to augment query results. A snippet was used to reflect the relevance between a document and a query [ 5 , 6 ], which we name it as  X  X raditional snippets X . If the snippets are high-quality, users could implicitly deduce relevance of the search results. And it could help the search engine better align the user X  X  perceived relevance of doc-uments with the true relevance. Therefore, it is critical for any search engine to produce high quality snippets to avoid biasing the perceived relevance of doc-uments [ 7 ]. If the snippet for a highly relevant document is poorly generated, the user may perceive the document as non-relevant and does not click on the document. What X  X  more, snippet quality is also an important criteria for evalu-ating search engines [ 8 ]. Traditionally, a snippet was used to reflect the relevance between a document and a query [ 5 , 6 ]. For traditional snippets, the basic and most important ability is relevance judgement. The ability of snippets is called relevance .
 avoid clicking on the original document and the browsing efficientness would be higher. The property of snippets is called satisfaction , and we name this kind of snippet as answer-contained snippet . The main goal of satisfaction is to improve the user-friendliness and effectiveness, similar to the goal of the One Click Access Task of NTCIR 1 , aiming to satisfy information need with the first system output that X  X  displayed, without clicking any more. However, it X  X  not clear whether answer-contained snippets are better than traditional snippets. To the best of our knowledge, there is no comparative and quantitative study between two kinds of snippets. In this paper, we will investigate the ability of answer-contained snippets, comparing with traditional snippets. We will evaluate their relevance and satisfaction ability on a large-scale archive, by browsing speed , reference to the full text of the documents and feedback from users .  X  We compare answer-contained snippets and traditional snippets.  X  We design a simple answer-contained snippet generating algorithm that can help users judge relevance quickly and satisfy information need fast.  X  By our experimental studies, we verify the utility of answer-contained snip-pets, compared with traditional snippets. The results show that it X  X  promising research direction to contain answer in a snippet in future. Early works generated static and query-independent snippets, consisting of the first several sentences of the returned document. Such an approach is efficient but often ineffective [ 11 , 12 ]. Selecting sentences for inclusion in the snippet based on the degree to which they match the keywords has become the state-of-the-art of query-biased snippet generation for text documents [ 5 , 13 , 14 ]. Generally, there are two kinds of sentence selection methods in query-based snippet generation. One is heuristic method [ 5 , 14  X  17 ], the importance of sentences are expressed by heuristic rules. For example, if a sentence is the first sentence of paragraph, it may be more important than others. The other is machine learning method [ 6 , 18 ]. Metrics or features are commonly used in those methods. The features include whether the sentence is a heading or the first line of the document, the number of keywords and distinct keywords that appear in the sentence and the number of title words appearing in the sentence, etc. However, snippet generation techniques designed for plain text documents are unable to process structural or semi-structural data well, which can X  X  leverage the structural information of data and therefore do not perform well.
 Recently, structural snippets have been explored for XML documents [ 7 ]. Huang et al. [ 7 ] propose a set of requirements for snippets and design a novel algorithm to efficiently generate informative yet small snippets. Ellkvist et al. [ 19 ] have explored how to generate snippet for workflow data by considering fine-grained structural information. However, to the best of our knowledge, there is still no snippet generation methods for cQA data, which still adopted query-biased snippet generation generally and do not perform well, as observed in the user studies. In cQA, everybody could ask and answer questions on any topic, and people seeking information are connected to the ones knowing answers. As answers are usually explicitly provided by human, they can be helpful in answering real world questions [ 9 ]. Although it is difficult to achieve satisfaction for general search, it is relatively easier for cQA search, since a question in a cQA document has corresponding answers. The sentences in these answers can be utilized to satisfy user information needs. Meanwhile, the goal of this paper is to verify the utility of answer-contained snippets, not to extract answers from documents. Thus in this paper, we will choose the cQA archive as our experimental dataset, focusing on our research goal. 3.1 DataSet To construct a comprehensive dataset for our experiments, we crawled nearly all the question and answer pairs (QA pairs) of two top categories of the most popular(Computers &amp; Internet, Health) with fourty-three sub-categories from 2005.11 to 2008.11 in Yahoo!Answer 2 , which produces an archive of 6,345,786 QA pairs. A cQA document includes question title, question body, best answer, other answers and metadata . 3.2 Answer-Contained Snippets To design a reasonable answer-contained snippet method for cQA search, we have to consider two questions: (1) how to rank answers? (2) what if the size of one answer is large?  X  The Answer Importance: We consider answer quality from 3 angles. Firstly, people could take Best Answer chosed by asker or by cQA system in a cQA document as the highest quality answer. Secondly, using vote number to reflect the answers X  popularity, the answer with the largest vote number could be thought as the highest quality answer among answers. At last, the answer submitted by the highest authority user could be taken as the highest quality answer.
In our implementation, all these factors are considered to measure the qual-ity of answers. Simply, we combined all these 3 factors together using linear combination as follows: where  X ,  X ,  X  denote the weight of each factor;  X  +  X  +  X  = 1. The answer with highest score computed by Formula ( 4 ) would be took as the highest quality answer. In all our experiments,  X  was set to 0.6,  X  to 0.3 and  X  to 0.1.  X  Size Consideration: We first obtained the distributions of word number in each part(question body, best answer and answers) of cQA documents in our dataset. We found the distributions follow power laws.

The power law relations show that the size of most of component is small, only a small number of component have large size. Here, component denotes one of question body, answers and Best Answer. The results are quantitatively display in Table 1 . Thus we can choose a threshold T to filter the answers, if the number of words in an answer is more than T , we simply use the heading
T words to represent the answers; if not, we take this answer as the highest quality answer.
 Thus, we have designed a answer-contained snippet framework for cQA search consisting of three parts, i.e. title part, question body part and high quality answer part. In summary, the proposed algorithm first parses the cQA document to obtain all parts, including question subject, question body and answers. Then the algorithm ranks all answers to get the highest quality answer by Formula ( 4 ). If the word number of the highest quality answer is more than threshold T , the algorithm only trunks the heading T words as the highest quality answer. Thirdly, the algorithm deletes all redundant and less substantial information in question body, and obtain a clean question body. Finally, the algorithm returns the cleaned question body and the highest quality answer as the cQA snippet of the cQA document. 3.3 Baseline Snippet Algorithm The state-of-the-art snippet generation method proposed by Metzler et al. [ 18 ] was chosen as our baseline algorithms, which use gradient boosted decision tree (GBDTs) learning approach for the snippet generation task. The features adopted are exact match of query , overlap proportion , overlap-syn proportion , sentence language model , sentence length and sentence location . 10 queries sam-pled from questions in our Dataset and their corresponding top 20 retrieved documents were used as training data. Human evaluator was asked to summa-rize all the 200 pages by extracting sentences according to the corresponding queries. The number of sentences for a summary is recommended to be five. However, if there are more or less appropriate sentences, they could be selected in spite of the recommendation. For GBDTs, we use the GBM package for R 3.4 Retrieval Model In order to perform retrieval, we use a ranking function similar to the one proposed by Xue et al. [ 4 ], which builds upon previous work on translation-based retrieval models and tries to overcome some of their flaws, formulated as following: ( q, a ) 2 , ..., ( q, a ) L } .  X  is the smoothing parameter for C .And P is the maximum likelihood estimator while | C | is the length of C . P ( w probability of translating a question term t to the query term w. We control the relative importance by  X  , beta and  X  .  X  +  X  +  X  =1.
 Jelinek-Mercer smoothing for Eq. 6 instead of Dirichlet Smoothing, as it has been done by Delphine et al. [ 10 ]. The other is that we take use of the statistical word translation model trained by Delphine et al. [ 10 ], which perform better than the one of original model by Xue et al. In all our experiments,  X  was set to 0.5,  X  to 0.3 and  X  to 0.5. 3.5 Evaluation Criteria We have two experimental procedures: experimental procedure for relevance judgement and experimental procedure for information satisfaction . Because of the similar task, the criteria proposed by Tombros et al. [ 5 ] would be adopted in this section. Such criteria for the experiment adopted were: (a) The recall, precision and F 1 of the relevance judgements. (b) The speed of judgements performing. (c) The need of the evaluators to seek assistance from the full text of the (d) The subjective opinion of the users about the assistance provided by the Recall, Precision and F 1 . They are often used to evaluate the effectiveness of the relevance judgements, and are calculated as follows: where N cr denotes number of relevant documents correctly identified by a evalu-ator for a query; N tir denotes the total number of relevant documents, within the examined ones, for that query; N tr total number of indicated relevant documents for a query.
 from the questions in our dataset; meanwhile, top 30 retrieved documents of each query were manually judged to be relevant or not, as our groudtruth. 4.1 Experiments of Relevance In this section, we examine users X  performance in the process of judging relevance between documents retrieved and specific queries (i.e. questions). It includes two tasks: to judge the relevance of the documents in a ranked list, with either baseline or answer-contained snippets. To achieve this, two groups consisting of 10 evaluators each were invited. Evaluators were randomly assigned to a group, and each group was assigned to one task only [ 5 ]. For each query, evaluators were presented with the query and a retrieved document list with snippets, and told that the list was the returned retrieval results of a particular query. The only actions evaluators could perform were to move through the list or to click the full text of the cQA document. Thus, their goal was to identify, in 2 min, as many relevant documents as possible.
 The results obtained through the experimental procedure are presented and analysed as following section.
 Recall, Precision and F 1. As we can see from Table 2 5 . The precision, recall and F 1 values for the group of evaluators using the proposed snippet are all considerably larger than that of the group using the baseline snippet: the per-formance difference is 20.25 %, 6.3 % and 11.36 %. We conclude that evaluators using proposed snippet in a retrieved cQA document list, performed their rel-evance judgements significantly better than those using the traditional state-of-the-art snippet. So it shows that proposed snippet algorithm allows users to identify more relevant cQA documents, and identify them more accurately. Speed. The speed result has been shown in Fig. 1 . We examined the average numbers of documents using baseline snippet and proposed snippet.
 The figure shows that evaluators using the proposed snippet returned on average 15 documents per query, while the other examined on average 12.32 documents. It amounts to a 21.75 % increase in the average number of documents examined. Therefore, there is a definite tendency for users presented with the proposed snippet to perform relevance judgements quicker than users presented with a baseline snippet. Reference to the Full Text of the Documents. The data collected on the users X  reference to the full text of documents showed that evaluators using the baseline snippet had to refer to 2.54 full texts per query, whereas evaluators from the other experimental group had to refer to 0.4 on average. If we normalise these values to the average number of documents that each experimental group examined for each query, we obtain the results shown in the Fig. 2 . The full text of 20.62 % of the documents for each query had to be refered by each evaluator using the baseline snippets, while 2.67 % using the answer-contained snippets. companied for each retrieved document. We can find that the proposed method performs better. Users need less clues to establish the relevance of documents, and especially they need clues about the context by which the question-type query are generated. What X  X  more, Our results shows the proposed snippet pro-vided the evaluators with enough evidence to support their relevance judgements. Opinions of Users. As a form of confirmation of the results obtained in the previous categories, the subjective opinions of the users, gathered from the ques-tionnaire they were asked to till in after their session, rated the utility of the proposed snippet higher than that of the baseline snippet. This result is depicted in Fig. 3 where the scale ranges from 1 (least helpful) to 5 (most helpful). The data indicates that evaluators using baseline snippet rated on average the utility of the accompanying information at 3.25, while evaluators assigned in the other task indicated a rating of 3.65. It means that users require more clues about the relevance of the retrieved documents and the answer-contained snippets have focused on capturing that requirement.
 4.2 Experiments of Satisfaction Here, we would examine two tasks: to get the words that could satisfy informa-tion need from a ranked list, with either baseline or answer-contained snippets. To achieve this, two groups consisting of 10 evaluators each were invited. Evalua-tors were randomly assigned to a group, and each group was assigned to one task only. For each query, evaluators were presented with the query and a retrieved document list with snippets, and told that this list was the result of a retrieval based on a particular query. The only actions evaluators could perform were to move through the list or to click the full text of the cQA document. Thus, their goal was to obtain information that satisfy the query, and stop until the information has been obtained.
 The results obtained through the experimental procedure are presented in Fig. 4 .
 Speed. The Fig. 4 (a) shows that evaluators using the proposed snippet exam-ined on average 3.68 documents per query to satisfy the information need, while evaluators using the baseline snippet examined on average 4.76 documents to satisfy the information need. Although this difference is small, it amounts to a 22.69 % decrease in the average number of documents examined. Thus we could conclude that there is a definite tendency for users presented with the answer-contained snippets to satisfy information need quicker than users presented with the baseline snippets.
 Reference to the Full Text of the Documents. The evaluators using the baseline snippet had to refer to 4.34 full texts per query to satisfy the information need, whereas evaluators from the other experimental group had to refer to 0.4 on average. If we normalise these values to the average number of documents that each experimental group examined for each query, we obtain the results shown in the Fig. 4 (b). The full text of 91.18 % of the documents for each query had to be refered by each evaluator using the baseline snippets, while 10.87 % using the answer-contained snippets. This difference can be clearly attributed to the snippet information that was companied for each retrieved document. The result verifies the initial assumption that proposed snippet method could perform better for helping user satisfy information need. If user X  X  information cann X  X  be satisfied from snippet, users refer to the full text of the documents. Our results shows the proposed snippet provided the evaluators with enough evidence to satisfy information need.
 Opinions of Users. In Fig. 4 (c), the scale ranges from 1 (least helpful) to 5 (most helpful). The data shown in this figure indicates that evaluators using baseline snippet rated on average the utility of the accompanying information at 2.65, while evaluators assigned in the other task indicated a rating of 4.15. snippet expressed their dissatisfaction regarding the information they were pre-sented with. More specifically, they emphasised on the fact that they had to refer to the full text for almost every document they were examining to satisfy information need. Hence, the outcome of the post-experimental discussions is yet another indication in favour of the assumption made, that users require the words that could satisfy information need contained in snippet. The answer-contained snippets that include high quality answer have focused on capturing that requirement. To the best of our knowledge, this is the first work that addresses the prob-lem of generating answer-contained snippets for cQA search; Meanwhile, our quantitative study shows that the answer-contained snippet method significantly outperforms the state-of-the-art traditional methods in terms of relevance judge-ments and information satisfaction evaluations, which shows that it X  X  promising research direction to contain answer in a snippet in future.

