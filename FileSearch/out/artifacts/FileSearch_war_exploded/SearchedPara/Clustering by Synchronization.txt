 Synchronization is a powerful basic concept in nature regu-lating a large variety of complex processes ranging from the metabolism in the cell to social behavior in groups of individ-uals. Therefore, synchronization phenomena have been ex-tensively studied and models robustly capturing the dynam-ical synchronization process have been proposed, e.g. the Extensive Kuramoto Model. Inspired by the powerful con-cept of synchronization, we propose Sync , a novel approach to clustering. The basic idea is to view each data object as a phase oscillator and simulate the interaction behavior of the objects over time. As time evolves, similar objects naturally synchronize together and form distinct clusters. Inherited from synchronization, Sync has several desirable properties: The clusters revealed by dynamic synchronization truly re-flect the intrinsic structure of the data set, Sync does not rely on any distribution assumption and allows detecting clusters of arbitrary number, shape and size. Moreover, the concept of synchronization allows natural outlier handling, since outliers do not synchronize with cluster objects. For fully automatic clustering, we propose to combine Sync with the Minimum Description Length principle. Extensive ex-periments on synthetic and real world data demonstrate the effectiveness and efficiency of our approach.
 H.2.8 [ Database applications ]: Data mining Algorithms, Design, Reliability Synchronization, Clustering, Kuramoto Model
Clustering is essential for knowledge discovery in a large variety of applications. During the last decades, cluster-ing has therefore attracted a huge volume of attention, with multiple books, e.g. [18], surveys, e.g. [23] and research pa-pers, e.g. [3, 4, 11, 31] to mention a few. Many approaches have been proposed to address the clustering problem from different points of view, and each cluster notion comes with specific advantages and drawbacks. As an example, consider the wide-spread Expectation Maximization (EM) algorithm [11]. The result of EM, a mixture model of multivariate Gaussians, is very useful for interpretation in many applica-tions. However, EM only yields good results if the data at least approximately follows the model assumption, i.e. con-sists of Gaussian clusters. Moreover, the number of clusters needs to be specified as an input parameter and the result of EM is very sensitive w.r.t. outliers.

In this paper, we consider clustering from a novel dif-ferent point of view: synchronization . Synchronization is the phenomenon that a group of events spontaneously come into co-occurrence with a common rhythm, despite of the differences between individual rhythms of the events. Syn-chronous behavior has attracted a large volume of interest in physics, biology, ecology, sociology, communication and other fields of science and technology. It is known that syn-chrony is rooted in human life from the metabolic processes in our cells to the highest cognitive tasks we perform as a group of individuals [5]. We will demonstrate that clustering by synchronization has many attractive benefits, but let us first illustrate the basic idea.
Synchronization phenomena are prevalent in every day life, as an example consider opinion formation. In the begin-ning, each person has its own view about a certain problem. After interaction by conversation or discussion, some peo-ple with similar background, such as education or hobby, will easily group together and form a common opinion. As time evolves, in the phase of local synchronization , groups with diverse opinions will be formed. After further conver-sation with different groups, all people may finally achieve a uniform opinion ( global synchronization ).

Inspired by these synchronization phenomena, we exploit the synchronization dynamics to obtain a natural clustering of a given data set. The key idea is to regard each data ob-ject as a phase oscillator. Each object interacts dynamically Figure 1: Clustering by synchronization. (a) The initial state of the objects: the back arrows indi-cate the mutual interaction and red arrows indicate the direction of movement for some sample objects. (b) The comparison of objects states before and af-ter one time step: black points represent the initial state, red points indicate the new state. (c) The final state of objects in local synchronization: syn-chronized clusters and outliers. with similar objects. We will elaborate a formal description of the dynamical object interaction patterns by a reformu-lated Kuramoto model in Section 3.2 and now provide the intuition of clustering by synchronization. The process of the dynamical clustering involves the following stages: First, starting from initial conditions, each object runs indepen-dently at its own phase. As time evolves, those objects with highest similarity will synchronize first. Then, in a sequen-tial process, more and more objects synchronize together and clusters are produced driven by the intrinsic structure of the data set. Finally, the whole population is split into several stable distinct synchronized clusters. Outliers are effectively detected due to their different dynamic behavior hardly synchronizing with any of the cluster objects.
Figure 1 displays 3 snapshots of the simulated dynamical movement. For simplicity, 2-dimensional data are displayed. Consider the 2 objects ( P 1, P 2, P 3), where P 1 and P 2 are cluster objects and P 3 is an outlier. For border object P 1, its neighborhood contains less objects and all neighbors are located towards the inside of the cluster. Therefore, the bor-der object is driven towards the inside of the cluster through the non-linear dynamical interaction with its neighbors (Fig-ure 1(a)). The larger the distance between an object and its neighbors, the stronger is the interaction imposed, which implies that neighbors with larger distances have a stronger impact on the object. For the center objects, e.g. P 2, the neighborhood contains more objects, however, all these neighbors locate around the object, causing a relatively slow movement of P 2 in comparison to the border objects, such as P 1. The movement of objects depends on the structure of their overall neighborhood, which implies that the objects will move towards the main direction of their neighborhood. Through the non-linear interaction, all objects with similar attributes finally synchronize together. In contrast, outlier objects remain isolated from all other objects, e.g. P 3. Fig-ure 1(b) displays the old and the new positions of the objects for comparison. The objects with high similarity gradually synchronize together through mutual coupling. The initial points (black color) are replaced by the red points after one time stamp. Figure 1(c) depicts the final states of the ob-jects. The data set is split into clusters and outliers, where cluster objects are synchronized together and have the same phase while outlier objects remain isolated over time.
The major benefits of our algorithm Sync for Clustering by Sync hronization, can be summarized as follows: 1. Clusters detected by SynC truly reflect the intrinsic 2. Sync is a powerful technique for clustering and outlier 3. Sync is robust against parameter settings and fully au-The remainder of this paper is organized as follows: In the following section, we briefly survey related work. Section 3 presents our algorithm in detail. Section 4 contains an extensive experimental evaluation and Section 5 concludes the paper.
During the past several decades, many algorithms were proposed for clustering, such as EM [11], CURE [16], CLIQUE [3], BIRCH [31], CLARANS [25], DBSCAN [13], etc. Due to space limitations, we can only provide a very brief survey on some important major research directions. In addition, we briefly introduce related work on synchro-nization phenomena.

PDF-based Clustering. The key idea of these methods is to detect clusters by using a model of probability den-sity functions (PDFs) to describe the data structure. The most fundamental techniques in this line are K-means [22] and EM [11]. These methods are suitable to detect spheri-cally Gaussian clusters and the number of clusters K needs to be specified by the user. The algorithm X-means [26] ex-tends K-means by a technique for automatically detecting K founded on information theory. The algorithm G-means [17] additionally provides to detect non-spherical Gaussian clusters. Another information-theoretic method, RIC [8], has been designed as a postprocessing step to improve an initial clustering of an arbitrary conventional clustering al-gorithm. The cluster model comprises a rotation matrix determined by PCA and a PDF assigned to each coordinate selected from a set of predefined PDFs. Clusters with sim-ilar characteristics are finally merged. However, the result strongly depends on the quality of the initial clustering and the cluster model is limited to linear attribute correlations and a predefined set of PDFs. Recently, OCI [9] is proposed to detect clusters using Independent Components. This al-gorithm uses the Exponential Power Distribution as cluster model and applies the Independent Component Analysis for determining the main directions inside a cluster as well as for finding split planes in a top-down clustering approach. All methods in this category tend to fail if the data distribution does not correspond to the cluster model. An alternative largely avoiding restricting assumptions on the data distri-bution is the idea of Parzen Windows. Local information on the object density is collected by histograms over local windows in the feature space or by local kernel functions, e.g. Gaussians. Thereby, at a global level, arbitrary data distributions can be captured. The algorithm MeanShift [10] combines this idea with clustering: During the run of the algorithm, the windows move towards the direction of the highest object density until convergence. However, the window size needs to be suitably selected and our experi-ments demonstrate that this algorithm tends to degrade in performance in the presence of outliers and noise points.
Density-Based and Spectral Clustering. In density-based clustering, clusters are regarded as regions of high ob-ject density in the data space which are separated by regions of low object density (noise). The algorithm DBSCAN [13] formalizes this idea using two parameters : the neighbor-hood of a given radius  X  has to contain at least a minimum number of objects ( MinPts ). Arbitrarily shaped clusters can be easily detected by DBSCAN, however the choice of a suitable settings of  X  and MinPts is often difficult, especially since the parameters are correlated. Conceptually closely re-lated, spectral clustering refers to a class of techniques which rely on the Eigenstructure of a similarity matrix to partition objects into disjoint clusters. These algorithms, e.g. [24, 7] detect arbitrarily shaped clusters by considering the clus-tering problem from a graph-theoretic perspective. A clus-tering is obtained by removing the weakest edges between highly connected subgraphs, which is formally defined by the normalized cut or similar objective functions. Similar to K-means, the problem of these approaches is to choose a suitable number of clusters and they are sensitive w.r.t. outliers. The approach [7] partially overcomes these prob-lems by proposing a new cost function for spectral clustering based on the error between a given graph partitioning and a clustering result. However, this approach requires sam-ple data with a known partitioning which is not available in most cases. The approach of Affinity Propagation [14] is closely related to spectral and density-based clustering. Each data point is regarded as a node in a network. The algorithm performs clustering by letting the data points ex-change messages along the edges of the networks. By mes-sage passing, certain data points emerge as so-called exem-plars (cluster representatives) and the other data points es-tablish their cluster membership. As input parameter for each data point a real value must be specified characteriz-ing the probability that this particular point is selected as an exemplar. The number of clusters is controlled by this parameter but also by the structure of the network.
Kuramoto Model and Synchronization. Synchro-nization phenomena in large populations of interacting el-ements are the subject of intense research efforts in physi-cal, biological, chemical, and social systems. The extensive Kuramoto model [20, 21, 1] is one of the most successful approaches to explore synchronization phenomena. Seliger et al. [27] discuss mechanisms of learning and plasticity in networks of phase oscillators through a generalized Ku-ramoto model. Arenas et al. [6] apply the Kuramoto model for network analysis, and study the relationship between topological scales and dynamic time scales in complex net-works. This analysis provides a useful connection between synchronization dynamics, network topology and spectral graph analysis. From bioinformatics, Kim et al.[19] propose a strategy to find groups of genes by analyzing the cell cycle specific gene expression with a modified Kuramoto model. Aeyels et al. [2] introduce a mathematical model for the dy-namics of chaos system. They characterize the data struc-ture by a set of inequalities in the parameters of the model and apply it to a system of interconnected water basins.
In summary, previous approaches mainly focus on the syn-chronization phenomena of a dynamic system from a global perspective. Moreover, to our best knowledge, the synchro-nization principle and the Kuramoto model have not been in-vestigated in the data mining community. Inspired by ideas from dynamical system analysis, we propose a novel cluster-ing technique based on local synchronization.
Synchronization phenomena are prevalent in nature. For example, the effect of synchrony has been described in exper-iments of people conversation, song or rhythm, or of groups of children interacting to an unconscious beat. In all cases the purpose of the common wave length or rhythm is to strengthen the group bond. The lack of such synchrony can index unconscious tension, when goals cannot be iden-tified nor worked towards because the members are  X  X ut of sync X . In this section, we introduce Sync , to explore clus-ters by synchronization. We start with an introduction of the Kuramoto Model and then develop a variant suitable for clustering. In Section 3.3 we discuss the algorithm Sync in detail.
One of the most successful attempts to understand collec-tive synchronization phenomena was due to Kuramoto [20], [21], who analyzed a model of phase oscillators which run at arbitrary intrinsic frequencies but are coupled through the sine of their phase differences. The Kuramoto model (KM) consists of a population of N coupled phase oscillators where the phase of the i -th unit, denoted by  X  i , evolves in time ac-cording to the following dynamics: where  X  i stands for its natural frequency and S describes the coupling strength between units. There are two major tendencies in this model: the coupling strength S whose effect tends to synchronize the oscillators versus the vari-ance of these natural frequencies,  X  i , the source of disorder which drives them to stay away from each other. When the coupling strength S = 0 , each oscillator tries to run inde-pendently at its own frequency. However, if the coupling is strong enough, all oscillators will freeze into synchrony, which called global synchronization. To characterize the level of synchronization between oscillators, a global order parameter is defined as: where 0  X  r ( t )  X  1, measures the coherence of the oscillator population, and  X  ( t ) is the average phase.

The Kuramoto model well describes the global synchro-nization behavior of all coupled phase oscillators, which im-plies that all the oscillators will be in phase finally through mutual coupling. This situation rarely occurs in real-life sys-tems. Phase locking or partial synchronization is observed more frequently. This is the case a local ensemble of oscilla-tors are synchronized together, where the whole oscillators are split into several clusters of mutually synchronized oscil-lators.
To apply KM for clustering, it is necessary to extensively reformulate Eq. (1). We need to formally introduce the con-cept of local synchronization of phase oscillators to reflect the intrinsic data structure. Table 1 gives a list of symbols used in the following. To introduce local synchronization, we need to define the notion of  X  -neighborhood.

Definition 1 (  X  -neighborhood of an object x ): The  X  -neighborhood of object x , which denoted by Nb  X  ( x ), is de-fined as: where dist ( y, x ) is a metric distance function. Definition 2 (Extensive Kuramoto model for Clustering): Let x  X  R d be an object in the data set D and x i be the i -th dimension of the data object x respectively. We regard each object x as a phase oscillator, according to Eq.(1), with an  X  -neighborhood interaction. The dynamics of each dimension x of the object x is governed by: Let dt =  X  t , then: x i ( t +1) = x i ( t )+ X  t X  i + It is essential that all objects have a common frequency  X  since different individual frequencies would disturb or even prevent cluster formation. However, the particular choice of omega has no effect on the clustering result. The term  X  t  X   X  i can thus be safely ignored.  X  t  X  S is a constant and simply set to 1. Finally the dynamics of each dimension x of the object x over time is provided by: x i ( t + 1) = x i ( t ) + The object x at time step t = 0 : x (0) = ( x 1 (0) , ..., x represents the initial phase of the object (the original loca-tion of object x ). The x i ( t + 1) describes the renewal phase value of i -th dimension of object x at the t = (0 , . . . , T ) time evolution.

To characterize the level of synchronization between os-cillators during the synchronization process, an order pa-rameter needs be defined. The order parameter of Eq.(2) is suitable for global synchrony. However, it is not effective to identify local dynamic effects. In particular it does not give information about the route to the synchronization in terms of local clusters which is so important to identify compact clusters of synchronized objects. For this reason, instead of considering a global observable, we define a cluster order pa-rameter r c , measuring the coherence of the local oscillator population.

Definition 3 (Cluster Order Parameter): The cluster or-der parameter r c characterizing the degree of local synchro-nization is provided by: The value of r c increases as more neighbors synchronize to-gether with time evolution. The dynamical clustering will terminate when r c ( t )  X  1 , which indicates the local phase coherence, also called local perfect synchronization . At this moment, all local objects (within in a cluster) have the same phase (location).
In this section, we present the SynC algorithm based on our reformulated extensive Kuramoto model.
The basic idea of Sync is to regard each object as a non-identical phase oscillator which changes its phase (location) dynamically with time evolution according to Eq. (6). The process of the dynamical clustering involves the following steps: 1. At initial time ( t = 0 ), without any interaction, all 2. As time evolves, each object interacts with its  X  -neigh-3. Finally, the cluster order parameter (Def. 3), which
During the dynamical process toward synchronization, all objects change their locations through the interaction with similar objects according to Eq. (6). This means that also the neighborhood of each object changes dynamically with time evolution. The traces of all objects are in line with the main direction of the local data structure. Finally, all objects in each cluster synchronize at a certain common phase. Figure 2 (a)-(e) shows the detailed process of dynamical clustering of 2-dimensional points from t = 0 to t = 4 . t = 0 indicates the original data set at the initial time. From that moment on, all objects with similar attributes start to synchronize together through the dynamical interaction and finally, all objects in the data set synchronize at 4 different phases after 4 time steps. The level of local synchronization with time evolution is characterized by the cluster order parameter, which indicated in Figure 2 (e). When r c  X  1, all objects in a cluster synchronize together (coherence or phase locking).
Depending on their dynamical behavior with time evolu-tion, all objects can be divided into two types: synchronized objects (objects in several synchronized clusters) and non-synchronized objects (outliers). Most objects synchronize with other objects and form synchronized clusters. Objects which remain isolated all the time are regarded as outliers. In order to illustrate the process of handling outliers, we use two simple one-dimensional data sets: Without outliers D 1 = { 0 . 1, 0.2, 0.3, 0.4, 0.5, 0.8, 1.0, 1.1, 1.3, 1 . 5 } , and with outliers D 2 = { -0.5, 0.1, 0.2, 0.3, 0.4, 0.5, 0.8, 1.0, 1.1, 1.3, 2.0 } . As displayed in Figure 3, in D 1 , all objects synchronize to different clusters and achieve the phase lock. However, in D 2 , owing to the different movement dynamics and direc-tions, outlier objects can not synchronize with other objects Figure 3: Object dynamics plot displaying the dif-ferent dynamics of cluster and outlier objects. (a): data set without outliers, (b): data set with outliers. and keep their original location during the whole time evo-lution. Moreover, the plots of dynamic object movement are a concise visualization for the intrinsic data structure including cluster points and outliers.
To explore clusters based on synchronization, an appro-priate initial size of the neighborhood for mutual interaction needs to be determined. Although our experiments demon-strated that the clustering result is quite robust against the initial neighborhood size  X  , we propose to combine Sync with ideas from Minimum Description Length [15] for fully automatic clustering. More specifically, the MDL principle is applied to compress a set of candidate clustering models M l , where in our case different models correspond to the clustering results with various  X  .

To generate a suitable set of models, we apply the fol-lowing heuristic: To guarantee a stable interaction of each object, we initiate  X  with the average value of the k -nearest neighbor distance determined from a sample from the data set for a small k . We increase  X  stepwise until all objects syn-chronize in a cluster. A reasonable step size is determined by the difference between average ( k + 1) -nearest neighbor distance and average k -nearest neighbor distance. We rec-ommend to choose k sufficiently small and k = 3 nearest neighbors were applied in all experiments. From all candi-date models, we select the model resulting in the minimum description length. Pseudocode the Sync algorithm is pro-vided in Figure 4. In the following we explain in detail how to compress a candidate model.

Given a data set D and a clustering model M , the funda-mental idea of MDL is to exploit the regularities in the data described by M for effective compression of the data. To avoid overly complex models, the overall description length includes not only the cost for coding the data exploiting the model, denoted by L ( D| M ), but also the cost L ( M ) for coding the model M itself.

Assuming there are K clusters, the coding cost or descrip-tion length for the data L ( D | M ) is provided by: where the pdf ( x ) means the probability of object x in each cluster. To allow for arbitrarily shaped clusters, we propose to estimate pdf ( x ) by non-parametric kernel density estima-tion, a widely used technique with numerous applications in machine learning, pattern recognition and computer vi-sion [30].

For coding the cluster model L ( M ), we need to specify the cluster assignment as well as the model parameters, which is given as: where the first term represents the coding cost for the cluster assignment and the second term is dedicated for coding the parameters. Here p i is the number of bandwidths for kernel density estimation, which equals the dimensionality of the data set.
 Finally, the total coding cost for a clustering model M is: The objective is to find the best cluster model minimizing the total coding cost.
 Kernel Density Estimation. The multivariate kernel density estimation is defined as follows: where h = ( h 1 ,  X  X  X  , h d ) T is the bandwidth and the term K (  X  ) is a d  X  dimensional kernel function that is non-negative and integrates to one. As a common choice we use the Gaussian kernel with mean 0 and variance 1. More specifically: where K ( x ) = (2  X  )  X  1 / 2 exp(  X  x 2 / 2). The bandwidth h is selected using an established heuristic which is proven to work well in various applications, even in the case of outliers and bi-modal distributions [28]: h i = 0 . 9  X  N  X  1 / ( d +4) min(  X  j , IQR i / 1 . 34) , where  X  variance and IQR i is the interquartile range of dimension i .
Finally, the probability of each object x is given as: Unlike parametric statistics, kernel density estimation al-lows to robustly estimate the probability for any unknown distribution. Consider for instance the cluster in Figure 5. Kernel density estimation provides an exact and concise rep-resentation of the data. In contrast, the Gaussian model is inappropriate for this cluster.
For each dynamical clustering by synchronization, the run-time complexity with respect to the number of data ob-jects is O ( T  X  N 2 ), where N is the number of objects and T is the time evolution. In most cases, T is small with 5  X  T  X  20. If there exists an efficient index, the com-plexity reduces to O ( T  X  N log N ). With clustering model selection based on MDL principle, the final complexity of Sync is O ( L  X  T  X  N log N ) and L is the number of different clustering models. Figure 6: Clustering driven by the intrinsic data structure.
To extensively study the performance of Sync , we com-pare its performance to representatives of various cluster-ing paradigms on synthetic and real-world data: We se-lected the iterative partitioning algorithm K-Means [22], the density-based algorithm DBSCAN [13], spectral clustering (SC) [24], the algorithm MeanShift [10] and the affinity prop-agation algorithm (AP) [14]. For the comparison methods we tried several parameter settings and present the best result. Moreover, we also compare to several state-of-the-art parameter-free clustering algorithms: To X-Means [26], which extends K-Means by estimation of the number of clus-ters based on MDL and to RIC [8] and OCI [9] which have been recently proposed for parameter-free and outlier-robust clustering. For X-Means, RIC and OCI, no parameter set-tings are required. In all experiments, we used Sync in com-bination with MDL as introduced in Section 3.3.3 which also does not require any input parameters.

We implemented spectral clustering and Sync in Java and obtained source code of RIC and OCI from the authors. The source code of the MeanShift algorithm is available at: http://www.mathworks.com/matlabcentral/fileexchange/ 10161-mean-shift-clustering and that of AP at: http://www.psi.toronto.edu/affinitypropagation/ . K-Means, DBSCAN and X-Means are implemented in WEKA available at http://www.cs.waikato.ac.nz/ml/weka . All experiments have been performed on a workstation with 2.4 GHz CPU and 2.0 GB RAM. Comparing the result of dif-ferent clustering algorithms with respect to effectiveness is a non-trivial problem, especially if different algorithms pro-duce results with different numbers of clusters. To provide an objective comparison of effectiveness, we report EC, an information-theoretic cluster validity measure proposed in [12]. Intuitively, EC corresponds to the number of bits re-quired to encode the class labels when the cluster labels are known, the smaller EC the better is the clustering. Recently, 3 more robust information-theoretic measures for cluster quality have been proposed in [29]: Adjusted Mutual In-formation (AMI), Adjusted Variation of Information (AVI), and Normalized Variation of Information (NVI). For these measures, higher values represent better clusterings.
We start the evaluation with two-dimensional synthetic data to facilitate presentation and demonstrate the benefits of Sync mentioned in Section 1.2.
We first evaluate the performance of Sync to detect natu-ral clusters in difficult settings, starting with clusters of ar-bitrary shape and data distribution. The data set displayed in Figure 6(a) consists of 6 clusters: one Gaussian cluster, one correlation clusters and 3 other arbitrarily shaped clus-ters. Sync successfully detects all types of clusters without requiring any input parameters. Moreover, Sync allows de-tecting clusters of very different object density, as Figure 6(b) demonstrates.
The performance of Sync does not degrade in the pres-ence of outliers or noise points. Regardless if the data set contains only single outliers, such as the example in Section 3.3.2, or more noise, such as the data set displayed in Figure 7(a), the clustering result remains stable. In addition, the object dynamics plot provides interesting information on the different kinds of objects: border cluster points, local as well as global outliers.
To assess the impact of the dimensionality of the data space on Sync , we created a 5-dimensional synthetic data set (1,000 objects, 5 clusters) and successively increased the di-mensionality by adding noise dimensions with uniform data distribution. Table 2 presents the results. The performance of Sync does not degrade up to an dimensionality of 8 as demonstrated by a constant EC of 0.09. Also on the 10-dimensional data set, which corresponds to a fraction of one third noise dimensions, 5 clusters are detected but one in-stance is regarded as a noise point resulting to a slight in-crease of EG to 0.092. When the dimensionality increases to 12, more and more objects are viewed as noise but Sync still obtains the correct clusters (EC 0.44). Sync detects 4 clusters with dimensionality of 15. Two clusters are merged together and some objects are regarded as noise (EC 0.8). In summary, the Sync performs very well on high-dimensional data with a fraction of approximately up to 50% noise di-mensions.
 Table 2: Performance of Sync on High-dimensional data. agation (AP), MeanShift (MS), X-Means, RIC and OCI. Figure 9: The result of DBSCAN with different pa-rameters on the synthetic data.
For comparison, we created a 2-d data set consisting of 8 arbitrarily shaped clusters plus noise points, cf. Figure 8. For K-Means and spectral clustering, best results have been achieved for K = 9 clusters. K-Means can not successfully detect the clusters in this data set which are not limited to Gaussian or spherical data distribution, cf. Figure 8(a). Spectral clustering, which in principle has the potential to detect arbitrarily shaped clusters also performs poorly on the data set cf. Figure 8(b). The reason lies in the fact that the performance of spectral clustering is severely affected by outliers or noise objects. Parameterized as recommended in [14], affinity propagation yields 341 clusters. However, the implementation also allows manually selecting the number of clusters. For comparison with the other algorithms, the result with K = 9 clusters is displayed in Figure 8(c). As for spectral clustering, the performance of AF degrades in the presence of noise and outliers. For MeanShift cluster-ing, best results have been achieved setting the window size to 6.3. In Figure 8(d) the areas of estimated major object density are marked by black circles. Similar to SC and AF, the density estimation in Meanshift is disturbed by outliers. For DBSCAN, we tried a wide range of different settings for the parameters MinPts and  X  . However, to find two suitable parameters is not a trivial task as they are corre-lated. Fixing MinPts = 6 (the default value in Weka) and varying  X  yielded the best clustering results, displayed in Figure 9(a-b). Since the data includes clusters of various object density, DBSCAN can not identify all these clusters at the same time. In comparison with these established clus-tering algorithms, Sync automatically and correctly detects all clusters and noise points driven by the synchronization principle, cf. Figure 8(e).

In the experiments displayed in Figure 8(f-h), we further compared the performance of Sync to various parameter-free clustering algorithms. As evident from Figure 8(f), X-Means fails to detect the clusters since they are not Gaussian and the data set contains noise points. The cluster model of RIC is limited to linear attribute correlations and a prede-fined set of PDFs, which does not fit to our example data set. Therefore, RIC does not perform very well on this ex-ample, cf. Figure 8(g). Also the performance of OCI is not satisfying since again, the assumption of the data distribu-tion, in this case Exponential Power Distribution, does not fit to our example data set, cf. Figure 8(h).
In this section, we evaluate the performance of Sync on real-world data publicly available at the UCI machine learn-ing repository ( http://archive.ics.uci.edu/ml ). Due to space limitation and difficult parametrization, we limit the comparison to the parameter-free clustering algorithms.
The Wisconsin data set deriving from a study on breast cancer consists of 683 instances which are labeled to the classes malignant (M: 239 instances) and benign (B: 444 instances) (16 instances with missing values have been re-moved from the original data set). Each instance is de-scribed by 9 numerical attributes.

Sync detects two clusters successfully. The first cluster with 433 objects represents the class benign, the second mass index, diabetes pedigree function and age, respectively. cluster with 250 objects the class malignant. In total, 23 in-stances have been wrongly clustered which results in an EC of 0.154. X-Means detects 3 clusters (C1:197(M:23,B:174), C2:261(B:261), C3:225(M:216 B:9)). 32 instances have been wrongly clustered with EC of 0.183. With RIC, five clusters are obtained with EC of 0.182. On the data set, also the OCI algorithm obtains a good result comprising 13 clusters with good class purity, as reported in [9]. This clustering result has an equally well EC(0.154) as the result of Sync . However, the model complexity of Sync corresponds with 2 clusters to the expert rating that there are 2 classes in the data set.
 In total, we notice that Sync achieves all we wanted: (1) Sync automatically finds the correct number of clusters; (2) Sync detects natural clusters in the data (with high EC value); (3) Sync discovers almost all objects of each clus-ter with high recall (96.2% and 97.5%); (4) all instances in each cluster match with corresponding type (with highest precision of 98.6% and 93.2%).
The Pima Indian diabetes database is a collection of med-ical diagnostic reports of 768 samples from a population liv-ing near Phoenix, Arizona, USA. Each sample consists of eight significant risk factors which were chosen for forecast-ing the onset of diabetes, including e.g. the number of preg-nancies, the diastolic blood pressure, the diabetes pedigree function, age, etc. These samples are labeled to 2 classes (Positive:268; Negative:500), namely whether the patient is tested positive for diabetes or not according to World Health Organization criteria. Sync detects 6 clusters on the data set. Figure 10 summarizes the cluster contents. Each bin represents the scaled average of each attribute within the cluster. The first cluster consists of 477 samples and mainly hosts the healthy subjects (365 samples). This group is char-acterized with the relative low value of these eight risk fac-tors, especially for number of pregnancies and the body mass index which is characteristic for young people. Sync assigns 118 out of 216 samples of the patient group to cluster 2, where are mainly middle-aged people who have a high num-ber of pregnancies, and a high plasma glucose concentration. Cluster 3 and Cluster 6 are mainly hosting subjects with di-abetes, which are characterized by high diabetes pedigree function, skin fold thickness and age. Subjects in cluster 4 are characterized by a high plasma glucose concentration a 2 hours in an oral glucose tolerance test and missing mea-surements for blood pressure, skin fold thickness and 2-hour serum insulin. Moreover, Cluster 5 hosts people who have no measurement of blood pressure, skin fold thickness, 2-jour serum insulin and body mass index. In total, the clustering results in an EC of 0.625. X-Means merges all instances in Table 3: Performance of Sync on high-dimensional data. one big cluster. RIC and OCI detect 3 and 4 clusters, respec-tively and corresponding EC values are 0.661 and 0.635. In summary, Sync detects meaningful clusters and yields best result with the lowest EC value. For a detailed comparison of the clustering results, Tables 3-6 provide a summary of the quality criteria introduced in [29]. As with EC, SynC clearly outperforms the comparison methods on most data sets.
In this paper, we introduced Sync , a novel clustering algo-rithm based on synchronization. The basic idea is to regard each data object as a phase oscillator and simulate the dy-namical object interaction over time. To characterize the object interaction, we proposed an adapted variant of the Kuramoto model suitable for clustering. Our extensive ex-periments demonstrated that the Sync algorithm based on synchronization shows several desirable properties: In ongoing and future work, we focus on exploiting the pow-erful concept of synchronization for hierarchical clustering. In addition, we investigate on data visualization techniques based on the simulated object movement. As a long term goal we want to closely integrate simulation into the data mining process to design robust algorithms. [1] J. A. Acebron, L. L. Bonilla, C. J. P. Vicente, [2] D. Aeyels, and F. D. Smet. A mathematical model for [3] R. Agrawal, J. Gehrke, D. Gunopulos, and [4] M. Ankerst, M. M. Breunig, H.-P. Kriegel, and [5] A. Arenas, A. Diaz-Guilera, J. Kurths, Y. Moreno and [6] A. Arenas, A. Diaz-Guilera, and C. J. Perez-Vicente. [7] F. Bach and M. Jordan. Learning spectral clustering. [8] C. B  X  ohm, C. Faloutsos, J.-Y. Pan, and C. Plant. [9] C. B  X  ohm, C. Faloutsos, and C. Plant. Outlier-robust [10] D. Comaniciu, and P. Meer. Mean shift: A robust [11] A. P. Dempster, N. M. Laird, and D. B. Rubin. [12] B. Dom. An information-theoretic external [13] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A [14] B. J. Frey, D. Dueck. Clustering by passing messages [15] P. Gr  X  unwald. A tutorial introduction to the minimum [16] S. Guha, R. Rastogi and K. Shim. CURE: An Efficient [17] G. Hamerly and C. Elkan. Learning the k in k-means. [18] A. K. Jain and R. C. Dubes. Algorithms for [19] C. S. Kim, C. S. Bae, and H. J. Tcha. A phase [20] Y. Kuramoto. Self-entrainment of a population of [21] Y. Kuramoto. Chemical oscillations, waves, and [22] J. B. MacQueen. Some methods for classification and [23] F. Murtagh. A Survey of Recent Advances in [24] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral [25] R. T. Ng and J. Han. Efficient and effective clustering [26] D. Pelleg and A. Moore. X-means: Extending k-means [27] P. Seliger, S. C. Young, and L. S. Tsimring. Plasticity [28] B. Silverman. Density Estimation for Statistics and [29] N. X. Vinh, J. Epps, and J. Bailey. Information [30] M. P. Wand and M. C. Jones. Kernel Smoothing. [31] T. Zhang, R. Ramakrishnan, and M. Livny. An
