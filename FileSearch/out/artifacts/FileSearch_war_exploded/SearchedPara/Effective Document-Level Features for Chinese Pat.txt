 It is well known that Chinese text does not come with natural word delimiters, and the first step for many Chinese language processing tasks is word segmentation, the automatic determination of word boundaries in Chinese text. Tremendous progress was made in this area in the last decade or so due to the availability of large-scale human segmented corpora coupled with better statistical modeling techniques. On the data side, there exist a few large-scale human annotated corpora based on established word segmentation standards, and these include the Chinese TreeBank (Xue et al., 2005), the Sinica Balanced Corpus (Chen et al., 1996), the PKU Peoples X  Daily Corpus (Duan et al., 2003), and the LIVAC balanced corpus (T X  X ou et al., 1997). Another driver for the improvemen-t in Chinese word segmentation accuracy comes from the evolution of statistical modeling tech-niques. Dictionaries used to play a central role in early heuristics-based word segmentation tech-niques (Chen and Liu, 1996; Sproat et al., 1996). Modern word segmentation systems have moved away from dictionary-based approaches in favor of character tagging approaches. This allows the word segmentation problem to be modeled as a sequence labeling problem, and lends itself to dis-criminative sequence modeling techniques (Xue, 2003; Peng et al., 2004). With these better model-ing techniques, state-of-the-art systems routinely report accuracy in the high 90%, and a few recen-t systems report accuracies of over 98% in F 1 s-core (Sun, 2011; Zeng et al., 2013b).

Chinese word segmentation is not a solved problem however and significant challenges re-main. Advanced word segmentation systems per-form very well in domains such as newswire where everyday language is used and there is a large amount of human annotated training data. There is often a rapid degradation in performance when systems trained on one domain (let us call it the source domain) are used to segment data in a different domain (let us call it the target domain). This problem is especially severe when the target domain is distant from the source domain. This is the problem we are facing when we perform word segmentation on Chinese patent data. The word segmentation accuracy on Chinese patents is very poor if the word segmentation model is trained on the Chinese TreeBank data, which consists of data sources from a variety of genres but no patents. To address this issue, we annotated a corpus of 142 patents which contain about 440K words ac-cording to the Chinese TreeBank standards. We trained a character-tagging based CRF model for word segmentation, and based on the writing style of patents, we propose a group of document-level features as well as a novel character part-of-speech feature (C_POS). Our results show these new fea-tures are effective and we are able to achieve an accuracy of 96.3% ( F 1 score) on the development set and 95% ( F 1 score) on the test set. We adopt the character-based sequence labeling approach, first proposed in (Xue, 2003), as our modeling technique for its simplicity and effec-tiveness. This approach treats each sentence as a sequence of characters and assigns to each charac-ter a label that indicates its position in the word. In this paper, we use the BMES tag set to indicate the character positions. The tag set has four labels that represent for possible positions a character can oc-cupy within a word: B for beginning, M for mid-dle, E for ending, and S for a single character as a word. After each character in a sentence is tagged with a BMES label, a sequence of words can be derived from this labeled character sequence.
We train a Conditional Random Field (CRF) (Lafferty et al., 2001) model for this sequence labeling. When extracting features to train a CRF model from a sequence of n characters C each character C i from a fixed window. We start with a set of core features extracted from the anno-tated corpus that have been shown to be effective in previous works and propose some new features for patent word segmentation. We describe each group of features in detail below. 2.1 Character features (CF) When predicting the position of a character with-in a word, features based on its surrounding char-acters and their types have shown to be the most effective features for this task (Xue, 2003). There are some variations of these features depending on the window size in terms of the number of char-acters to examine, and here we adopt the feature templates used in (Ng and Low, 2004).
 Character N-gram features The N-gram fea-tures are various combinations of the surrounding characters of the candidate character C i . The 10 features we used are listed below: Character type N-gram features We classify the characters in Chinese text into 4 types: Chi-nese characters or hanzi , English letters, numbers and others. T i is the character type of C i . The character type has been used in the previous work-s in various forms (Ng and Low, 2004; Jiang et al., 2009), and the 4 features we use are as follows:
Starting with this baseline, we extract some new features to improve Chinese patent word segmen-tation accuracy. 2.2 POS of single-character words (C_POS) Chinese words are composed of Chinese hanzi, and an overwhelming majority of these Chinese characters can be single-character words them-selves in some context. In fact, most of the multi-character words are compounds that are 2-4 char-acters in length. The formation of these compound words is not random and abide by word formation rules that are similar to the formation of phras-es (Xue, 2000; Packard, 2000). In fact, the Chi-nese TreeBank word segmentation guidelines (X-ia, 2000) specify how words are segmented based on the part-of-speech (POS) of their componen-t characters. We hypothesize that the POS tags of the single-character words would be useful in-formation to help predict how they form the com-pound words, and these POS tags are more fine-grained information than the character type infor-mation described in the previous section, but are more robust and more generalizable than the char-acters themselves.

Since we do not have POS-tagged patent da-ta, we extract this information from the Chinese TreeBank (CTB) 7.0, a 1.2-million-word out-of-domain dataset. We extract the POS tags for al-l the single-character words in the CTB. Some of the single-character words will have more than one POS tag. In this case, we select the POS tag with the highest frequency as the C_POS tag for this character. The result of this extraction process is a list of single-character Chinese words, each of which is assigned a single POS tag.
 When extracting features for the target character C , if C i is in this list, the POS tag of C i is used as a feature for this target character. 2.3 Document-level features A patent is a property right for an invention grant-ed by the government to the inventor, and many of the patents have a high concentration of scientif-ic and technical terms. From a machine learning perspective, these terms are hard to detect and seg-ment because they are often "new words" that are not seen in everyday language. These technical Algorithm 1 Longest n-gram sequence extraction. Input: Output: 1: For each sentence s i in P i do : 2: Count the frequency of each n-gram sequence; 3: Delete the sequence if its frequency &lt; 2; 4: Delete sequence i if it is contained in a longer 5: All the remaining sequences form a longest n-6: return Longest n-gram sequences list. terminologies also tend to be very sparse, either because they are related to the latest invention that has not made into everyday language, or because our limited patent dataset cannot possibly cover all possible technical topics. However, these techni-cal terms are also topical and they tend to have high relative frequency within a patent document even though they are sparse in the entire patent da-ta set. We attempt to exploit this distribution prop-erty with some document-level features which are extracted based on each patent document.
 Longest n-gram features (LNG) We propose a longest n-gram (LNG) feature as a document-level feature. Each patent document is treated as an in-dependent unit and the candidate longest n-gram sequence lists for each patent are obtained as de-scribed in Algorithm 1.

For a given patent, the LNG feature value for the target character C i  X  X  LNG is set to 'S' if the bigram ( C i , C i +1 ) are the first two characters of an n-gram sequence in this patent X  X  longest n-gram sequence list. If ( C i  X  1 , C i ) are the last two characters of an n-gram sequence in this patent X  X  longest n-gram sequence list, the target character C i  X  X  LNG is set to 'F'. It is set to 'O' otherwise. If C i can be labeled as both 'S' and 'F' at the same time, label 'T' will be given as the final label. For example, if '  X  ' is the target character C i in patent A and the sequence '  X   X  Z 6  X  ' is in patent A X  X  longest n-gram se-quence list. If the character next to '  X  ' is '  X  ', the value of the LNG feature is set to 'S'. If the next character is not '  X  ', the value of the LNG feature is set to 'O'.
 Algorithm 2 Pseudo KL divergence.
 Input: Output: 1: For each sentence s i in P i do : 2: Count the frequency of each trigram; 3: Delete the trigram if its frequency &lt; 2; 4: For C i in trigram C i C i +1 C i +2 do : 5: return PKL ( C i ,C i +1 ) and PKL ( C i ,C i +2 ) Pseudo Kullback-Leibler divergence (PKL) The second document-level feature we propose is the Pseudo Kullback-Leibler divergence fea-ture which is calculated following the form of the Kullback-Leibler divergence. The relative position information is very important for Chi-nese word segmentation as a sequence labeling task. Characters XY may constitute a meaningful word, but characters Y X may not be. Therefore, if we want to determine whether character X and character Y can form a word, the relative position of these two characters should be considered. We adopt a pseudo KL divergence with the relative po-sition information as a measure of the association strength between two adjacent characters X and Y . The pseudo KL divergence is an asymmetric measure. The PKL value between character X and character Y is described in Algorithm 2.
The PKL values are real numbers and are s-parse. A common solution to sparsity reduction is binning. We rank the PKL values between t-wo adjacent characters in each patent from low to high, and then divide all values into five bins. Each bin is assigned a unique ID and all PKL values in the same bin are replaced by this ID. This ID is then used as the PKL feature value for the target character C i . Pointwise Mutual information (PMI) Point-wise Mutual information has been widely used in previous work on Chinese word segmentation (Sun and Xu, 2011; Zhang et al., 2013b) and it is a measure of the mutual dependence of two strings and reflects the tendency of two strings appearing in one word. In previous work, PMI statistics are gathered on the entire data set, and here we gather PMI statistics for each patent in an attempt to cap-ture character strings with high PMI in a particu-lar patent. The procedure for calculating PMI is the same as that for computing pseudo KL diver-gence, but the functions (1) and (2) are replaced with the following functions:
For the target character C i , we obtain the values for PMI ( C i ,C i +1 ) and PMI ( C i ,C i +2 ) . In each patent document, we rank these values from high to low and divided them into five bins. Then the PMI feature values are represented by the bin IDs. 3.1 Data preparation We annotated 142 Chinese patents following the CTB word segmentation guidelines (Xia, 2000). Since the original guidelines are mainly designed to cover non-technical everyday language, many scientific and technical terms found in patents are not covered in the guidelines. We had to extend the CTB word segmentation guidelines to han-dle these new words. Deciding on how to seg-ment these scientific and technical terms is a big challenge since these patents cover many differ-ent technical fields and without proper technical background, even a native speaker has difficulty in segmenting them properly. For difficult scien-tific and technical terms, we consult BaiduBaike tific and technical terminology dictionary during our annotation. There are still many words that do not appear in BaiduBaiKe, and these include chemical names and formulas. These chemical names and formulas (e.g., /  X  X   X  X  X   X  Z /1-bromo-3-chloropropane 0 ) are usually very Table 1: Training, development and test data on Patent data long, and unlike everyday words, they often have numbers and punctuation marks in them. We de-cided not to try segmenting the internal structures of such chemical terms and treat them as single words, because without a technical background in chemistry, it is very hard to segment their internal structures consistently.

The annotated patent dataset covers many topics and they include chemistry, mechanics, medicine, etc. If we consider the words in our annotated dataset but not in CTB 7.0 data as new words (or out-of-vocabulary, OOV), the new words account for 18.3% of the patent corpus by token and 68.1% by type. This shows that there is a large number of words in the patent corpus that are not in the ev-eryday language vocabulary. Table 1 presents the data split used in our experiments. 3.2 Main results We use CRF++ (Kudo, 2013) to train our sequence labeling model. Precision , recall , F 1 score and R
OOV are used to evaluate our word segmentation methods, where R OOV for our purposes means the recall of new words which do not appear in CTB 7.0 but in patent data.

Table 2 shows the segmentation results on the development and test sets with different feature templates and different training sets. The CTB training set includes the entire CTB 7.0, which has 1.2 million words. The model with the CF fea-ture template is considered to be the baseline sys-tem. We conducted 4 groups of experiments based on the different datasets: (1) patent training set + patent development set; (2) patent training set + patent test set; (3) CTB training set + patent de-velopment set; (4) CTB training set + patent test set.

The results in Table 2 show that the model-s trained on the patent data outperform the mod-els trained on the CTB data by a big margin on both the development and test set, even if the CTB training set is much bigger. That proves the im-portance of having a training set in the same do-main. The results also show that adding the new features we proposed leads to consistent improve-ment across all experimental conditions, and that the LNG features are the most effective and bring about the largest improvement in accuracy. Most of the previous work on Chinese word seg-mentation focused on newswire, and one wide-ly adopted technique is character-based represen-tation combined with sequential learning models (Xue, 2003; Low et al., 2005; Zhao et al., 2006; Sun and Xu, 2011; Zeng et al., 2013b; Zhang et al., 2013b; Wang and Kan, 2013). More re-cently, word-based models using perceptron learn-ing techniques (Zhang and Clark, 2007) also pro-duce very competitive results. There are also some recent successful attempts to combine character-based and word-based techniques (Sun, 2010; Zeng et al., 2013a).

As Chinese word segmentation has reached a very high accuracy in the newswire domain, the attention of the field has started to shift to other domains where there are few annotated resources and the problem is more challenging, such as work on the word segmentation of literature data (Li-u and Zhang, 2012) and informal language gen-res (Wang and Kan, 2013; Zhang et al., 2013a). Patents are distinctly different from the above gen-res as they contain scientific and technical terms that require some special training to understand. There has been very little work in this area, and the only work that is devoted to Chinese word segmentation is (Guo et al., 2012), which reports work on Chinese patent word segmentation with a fairly small test set without any annotated train-ing data in the target domain. They reported an accuracy of 86.42% ( F 1 score), but the results are incomparable with ours as their evaluation data is not available to us. We differ from their work in that we manually segmented a significant amount of data, and trained a model with document-level features designed to capture the characteristics of patent data. In this paper, we presented an accurate character-based word segmentation model for Chinese patents. Our contributions are two-fold. Our first contribution is that we have annotated a signifi-cant amount of Chinese patent data and we plan to release this data once the copyright issues have been cleared. Our second contribution is that we designed document-level features to capture the distributional characteristics of the scientific and technical terms in patents. Experimental results showed that the document-level features we pro-posed are effective for patent word segmentation. This paper is supported by the Intelligence Ad-vanced Research Projects Activity (IARPA) vi-a contract NO. D11PC20154. All views ex-pressed in this paper are those of the authors and do not necessarily represent the view of IARPA, DoI/NBC, or the U.S. Government.
