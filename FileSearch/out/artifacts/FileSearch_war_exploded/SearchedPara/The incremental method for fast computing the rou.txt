 1. Introduction real-valued in practical databases, Pawlak's rough set theory has dif approximation of a fuzzy set in a crisp approximation space [4 approximation operators are constructed. Dubois and Prade [1] were the been discussed in detail in Ref. [33], which developed a uni computing time.
 propositions. Then the boundary set of a fuzzy set is rede 2. Preliminaries propositions.

Pawlak's rough set model [2] is reviewed fi rstly. Let U be a approximations of X with respect to ( U , R ), denoted by apr
De fi nition 1. Let U be a fi nite non-empty universe, and R be an equivalence relation de by apr
The boundary of A is de fi ned as: BN R A  X  X  x  X  X  = sup
De fi nition 2. Let U be a fi nite non-empty universeover. A fuzzy set A on U is de a number  X   X  [0,1], an  X  -cut, or  X  -level set, of a fuzzy set is de follows:  X  A ( x )=sup{  X  | x  X  A  X  }.

Theorem 1. Let apr exists a pair of fuzzy sets apr
They are de fi ned by the following membership functions: 3. Updating approximations incrementally based on boundary sets incrementally updating the lower and upper approximations is proposed and proved. incrementally update the approximations of a fuzzy set A . However, according to De
De fi nition 3. Let U be a fi nite non-empty universe, R be an equivalence relation de is de fi ned as: BN the upper boundary of A is de fi ned as: BN  X  R A  X  X  =  X  an  X  upper boundary of A .

Proof. (1) If P p Q , then apr
Proof. (1) According to Theorem 2 , when P p Q , we have apr upper approximations of a fuzzy set A .

Proposition 1. Let P and Q be two attribute sets and P  X  Q= apr
P  X  Q A  X  X  x  X  X  = Proof. Partition U into two subsets, denoted as Y and U  X  in Y , update their lower approximations by apr (1) Consider conditions satis fi ed by the elements in Y  X  (2) For  X  x  X  apr
On the other hand, one can obtain apr apr
Thus  X  apr Proof. Partition U into two subsets, denoted as Z and U  X  in Z , update their lower approximations by the original lower approximations. (1) Consider conditions satis fi ed by the elements in Z  X  (2) For  X  x  X  apr
On the other hand, according to Theorem 2 , one can get apr apr
Proposition 3. Let P and Q be two attribute sets and P  X  Q= apr
P  X  Q A  X  X  x  X  X  = (1) Consider conditions satis fi ed by the elements in Y  X  (2) According to Theorem 2 , one obtains apr  X  P A  X   X  X  t
Proof. Similar to Proposition 2 , U is partitioned into two subsets, denoted as Z approximations. (1) Consider conditions satis fi ed by the elements in Z  X  (2) For  X  x  X  apr  X  P  X  Q A  X   X  X  and x  X  Z  X  ',  X   X   X  (0,1], one obtains x fuzzy decision attribute f that includes two linguistic terms A and B . From Table 1 , the partitions generated by single attributes are: (1) Firstly, for the given attribute set P , one obtains (2) According to Proposition 1 , all lower approximations can be updated by apr (1) Firstly, for the given attribute set P , one obtains (2) According to Proposition 2 , compute the lower approximations:
From Proposition 3 , the computing includes two steps: (1)compute Y (1) Firstly, for the given attribute set P , one obtains (2) According to Proposition 3 ,
From Proposition 4 , the computing includes two steps: (1) compute Z (1) Firstly, for the given attribute set P , one obtains (2) According to Proposition 4 , compute the upper approximations: 4. Updating approximations incrementally based on cut sets approximation of a fuzzy set A and the approximation of cut set A
A . In order to incrementally update the approximations of A ,we how to update the lower and upper approximations of A .

Proposition 1  X  . Let P and Q be two attribute sets and P  X  Q= apr apr
Proof. When Q is added to P :if x  X  apr obtain apr
Thus apr
On the other hand, apr
According to Theorem 1 , one can obtain apr apr
Proof. According to Theorem 2 and 3 , it is clear that apr
BN
P  X  Q A  X   X  X  = A  X  = apr  X  P A  X   X  X   X  BN  X  P A  X   X  X  , thus one obtains apr
According to Theorem 1 , one can obtain apr
Proposition 3  X  . Let P and Q be two attribute sets and P  X  Q= as: apr  X  P  X  Q A  X  X  x  X  X  = sup  X  j x  X  apr  X  P  X  Q A  X  X  apr
Proof. Firstly, we prove apr  X  P  X  Q A  X   X  X  p apr  X  P A
BN x  X 
BN
Furthermore, one can obtain  X   X  BN  X  X   X  BN  X  Q A  X   X  X g ,thatis x  X  Y  X   X  . Therefore, one obtains x
Y  X   X  A  X  = apr  X  P A  X   X  X   X  apr  X  Q A  X   X  X   X  Y  X   X  .

On the other hand, let x  X  apr  X  P A  X   X  X   X  apr  X  Q A  X  namely, x  X  Y  X   X  , which contradicts the assumption that x apr
Therefore, one can obtain apr  X  P  X  Q A  X   X  X  = apr  X  P A sup  X  j x  X  apr  X  P  X  Q A  X  X  no sup  X  j x  X  apr  X  P  X  Q A  X  X  no
Proof. It is obvious that apr  X  P A  X   X  X  p apr  X  P  X  Q A x  X  apr
On the other hand, one can obtain apr  X  P  X  Q A  X   X  X  = A x  X  apr  X  b  X 
P  X  Q , one obtains BN  X  P  X  Q A  X   X  X  p  X 
Because  X  the assumption that x  X  apr  X  P  X  Q A  X   X  X  .So x  X  Z  X   X  deduce apr  X  P  X  Q A  X  X  x  X  X  =sup  X  j x  X   X  apr  X  P  X  Q We still use Table 1 to illustrate the previously discussed results.
From Proposition 1  X  , the computing process includes two steps: (1) compute apr sup f  X  j x  X  apr (1) Firstly, for the given attribute set P , one obtains
Based on the previously discussed results, one obtains Y 0.1 {5,7}  X  A 0.2 ,so Y 0.2 =  X  . Analogously, Y 0.3 =  X  , Y 0.7
According to Proposition 1  X  , one obtains (2) Therefore, one can obtain
From Proposition 2  X  , the computing process includes two steps: (1) compute apr sup f  X  j x  X  apr (1) Firstly, for the given attribute set P , one obtains (2) Therefore, one can obtain From Proposition 3  X  , the computing process includes two steps: (1) compute apr sup f  X  j x  X  apr P  X  Q A  X   X  X g . (1) Firstly, for the given attribute set P , one obtains (2) Therefore, one can obtain From Proposition 4  X  , the computing process includes two steps: (1) compute apr sup f  X  j x  X  apr P  X  Q A  X   X  X g . (1) Firstly, for the given attribute set P , one obtains (2) Therefore, one can obtain 5. Two incremental algorithms for updating approximations 5.1. Algorithm UARFB (Updating approximations incrementally based on boundary sets)
Input: 2. apr 3. Adding attributes or removing attributes provided by users.
Output: Updated lower and upper approximations. 1. Let  X  ={  X  A ( x )|  X  x  X  U }, repetitious data in  X  only hold one, then 2. Let R = P , apr 3. Let i =1. 4. Compute  X  apr 5. If an attribute set Q is added to the original attribute set R , let Y 6. Let T  X  i = BN 7. For x  X  T  X  8. T  X  9. If i b t , then i = i +1, go to 4; otherwise, compute Y =  X  10. For x  X  Y , compute apr 11. R = R  X  Q . If we continue to add attributes, go to 3; otherwise, go to 12. 13. Let Z  X  14. For x  X  S  X  15. S  X  16. For x  X  S  X  17. S  X  18. If j b t , then j = j +1, go to 13; otherwise, compute Z =  X  19. For x  X  Z , compute apr 20. R = R  X  T . If we continue to delete attributes, go to 12; otherwise, go to 21. 21. Output the updated lower and upper approximations.
 update their approximations by using the original information. Steps 1 the approximations when adding some attributes, and steps 12 fi 5.2. Algorithm UARFC (Updating approximations incrementally based on cut sets)
Input: 2. apr 3. Adding attributes or removing attributes provided by users.
Output: Updated lower and upper approximations. 1. Let  X  ={  X  A ( x i )| i =1,2, ... , n }, repetitious data in i =1,2, ... , t . 2. Let R = P , apr 3. Let i =1. 4. Compute apr 5. If an attribute set Q is added to the original attribute set R , let Y 6. Let T  X  i = BN 7. For x  X  T  X  8. T  X  9. For x  X  T  X  10. T  X  11. If i b t , then i = i +1, go to 4; otherwise, compute apr 12. R = R  X  Q . If we continue to add attributes, go to 3; otherwise, go to 13. 13. If attributes T are deleted from R , T p R , let j =1; otherwise, go to 23. 14. Let Z  X  15. For x  X  S  X  16. S  X  17. For x  X  S  X  18. S  X  19. If j b t , then j = j +1, go to 15; otherwise, go to 20. 20. Compute apr 21. R = R  X  T . If we continue to delete attributes, go to 13; otherwise, go to 22. 22. Output the updated lower and upper approximations.
 of the lower and upper approximations of cut sets A  X  . One to update the approximations by inserting the new attributes information of current approximations.
 effect of previously discussed algorithms can be expected to be greater. 6. Experimental evaluation and herbicide-injury. In our experiments, the 19 classes are denoted as A the universe.
 attributes.
 sclerotia, fruit-pods, fruit spots, seed, mold-growth, seed-discolor and seed-size. respectively. The experimental results are shown in Table 2 , where R attributes, respectively. j BN objects in the upper boundary set of class A i ( i =1,2, ... to the number of objects in the corresponding boundary sets. 6.2. Performance evaluation of updating rough fuzzy approximations incrementally systems with fuzzy decision.
 compare the running time of computing the approximations.
We randomly select two attribute sets P and Q ( P  X  Q =  X  following experiments.
 UARFB and UARFC is listed in Table 3 , where
P
Q
P
Q 2 {fruit-spots, seed, mold-growth, seed-discolor, seed-size, shriveling, roots}.
P
Q
P  X  Q i , i =1,2,3 means that the attribute set Q i is added to P DCRF, UARFB and UARFC is listed in Table 5 , where
P
Q 4 {leaf-shread, leaf-malf, leaf-mild, stem, lodging, stem-cankers}.
P
Q 5 {fruit-spots, seed, mold-growth, seed-discolor, seed-size, shriveling, roots}.
P
Q
P  X  Q i , i =4,5,6 means that the attribute set Q i is deleted from P
Remark. Tables 3  X  6 show that UARFB and UARFC may not be so signi involved, the two algorithms outperform DCRF obviously. 6.3. Learning classi fi cation rules from data depicted as follows: 6.3.1. Rough-fuzzy QUICKREDUCT algorithm Input: C , the set of condition attributes; D , the set of decision attributes.
Output: R , a reduct. 1. R  X  {} 2. do 3. T  X  R 4. for each x  X  C  X  R 6. T  X  R  X  { x } 7. R  X  T 8. until  X  R ( D )=  X  C ( D ) 9. return R
Where,  X  C D  X  X  = calculated. Fuzzy subsethood is de fi ned as follows: cover the entire problem domain, a rule is produced that classi produce reasonable classi fi cations. This requires another threshold value, reasonable or not.
 fi and the time of extracting rules, is effectively reduced. 7. Conclusions be considered. On the one hand, we have a family of  X  -cut sets ( F relation R  X  is an equivalence relation. For a fi xed pair of numbers (  X  apr approximation space ( U , R  X  ), namely ( U , R  X  )  X  ,  X   X  generalized fuzzy rough set model, both  X  and  X  are not fi
References
