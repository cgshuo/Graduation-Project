 Many problems in signal processing, machine learning, and communications can be cast as a linear regression problem where an unknown signal x  X  R N is related to its observations y  X  R M via In (1), the observation matrix  X   X  R M  X  N is a non-adaptive measurement matrix with random entries in compressive sensing (CS), an over-complete dictionary of features in sparse Bayesian learning (SBL), or a code matrix in communications [1, 2]. The vector n  X  R M usually accounts for physical noise with partially or fully known distribution, or it models bounded perturbations in the measurement matrix or the signal.
 Because of its theoretical and practical interest, we focus on the instances of (1) where there are more unknowns than equations, i.e., M &lt; N . Hence, determining x from y in (1) is ill-posed:  X  v  X  kernel (  X  ) , x + v defines a solution space that produces the same observations y . Prior information is therefore necessary to distinguish the true x among the infinitely many possible solutions. For instance, CS and SBL frameworks assume that the signal x belongs to the set of sparse signals. By sparse, we mean that at most K out of the N signal coefficients are nonzero where K N . CS and SLB algorithms then regularize the solution space by signal priors that promote sparseness and they have been extremely successful in practice in a number of applications even if M N [1 X 3]. Unfortunately, prior information by itself is not sufficient to recover x from noisy y . Two more key ingredients are required: ( i ) the observation matrix  X  must stably embed (or encode) the set of signals x into the space of y , and ( ii ) a tractable decoding algorithm must exist to map y back to x . By stable embedding, we mean that  X  is bi-Lipschitz where the encoding x  X   X  x is one to one and the inverse mapping  X  = {  X  (  X  x )  X  x } is smooth. The bi-Lipschitz property of  X  is crucial to ensure the stability in decoding x by controlling the amount by which perturbations of the observations are amplified [1, 4]. Tractable decoding is important for practical reasons as we have limited time and resources, and it can clearly restrict the class of usable signal priors. In this paper, we describe compressible prior distributions whose independent and identically dis-tributed (iid) realizations result in compressible signals. A signal is compressible when sorted mag-nitudes of its coefficients exhibit a power-law decay. For certain decay rates, compressible signals live close to the sparse signals, i.e., they can be well-approximated by sparse signals. It is well-known that the set of K -sparse signals has stable and tractable encoder-decoder pairs (  X  ,  X ) for M as small as O ( K log ( N/K )) [1, 5]. Hence, an N -dimensional compressible signal with the proper decay rate inherits the encoder-decoder pairs of its K -sparse approximation for a given approxima-tion error, and can be stably embedded into dimensions logarithmic in N .
 Compressible priors analytically summarize the set of compressible signals and shed new light on underdetermined linear regression problems by building upon the literature on sparse signal recov-ery. Our main results are summarized as follows: 1) By using order statistics, we show that the compressibility of the iid realizations of generalized These distributions are natural members of compressible priors: they truly support logarithmic di-mensionality reduction and have important parameter learning guarantees from finite sample sizes. We demonstrate that probabilistic models for the wavelet coefficients of natural images must also be a natural member of compressible priors. 2) We point out a common misconception about the generalized Gaussian distribution (GGD): GGD generates signals that lose their compressibility as N grows. For instance, special cases of the GGD distribution, e.g., Laplacian distribution, are commonly used as sparsity promoting priors in CS and SBL problems where M is assumed to grow logarithmically with N [1 X 3, 6]. We show that signals generated from Laplacian distribution can only be stably embedded into lower dimensions that grow proportional to N . Hence, we identify an inconsistency between the decoding algorithms motivated by the GGD distribution and their sparse solutions. 3) We use compressible priors as a scaffold to build new decoding algorithms based on Bayesian inference arguments. The objective of these algorithms is to approximate the signal realization from a compressible prior as opposed to pragmatically producing sparse solutions. Some of these new algorithms are variants of the popular iterative re-weighting schemes [3, 6 X 8]. We show how the tun-ing of these algorithms explicitly depends on the compressible prior parameters, and how to learn the parameters of the signal X  X  compressible prior on the fly while recovering the signal. The paper is organized as follows. Section 2 provides the necessary background on sparse signal recovery. Section 3 mathematically describes the compressible signals and ties them with the order statistics of distributions to introduce compressible priors. Section 4 defines compressible priors, identifies common misconceptions about the GGD distribution, and examines natural images as in-stances of compressible priors. Section 5 derives new decoding algorithms for underdetermined linear regression problems. Section 6 describes an algorithm for learning the parameters of com-pressible priors. Section 7 provides simulations results and is followed by our conclusions. Any signal x  X  R N can be represented in terms of N coefficients  X  N  X  1 in a basis  X  N  X  N via x =  X   X  . Signal x has a sparse representation if only K N entries of  X  are nonzero. To account for sparse signals in an appropriate basis, (1) should be modified as y =  X  x + n =  X  X   X  + n . isometry property (RIP), it can be shown that  X  X  defines a bi-Lipschitz embedding of  X  K into R
M [1, 4, 5]. Moreover, RIP implies the recovery of K -sparse signals to within a given error bound, and the best attainable lower bounds for M are related to the Gelfand width of  X  K , which is log-arithmic in the signal dimension, i.e., M = O ( K log ( N/K )) [5]. Without loss of generality, we restrict our attention in the sequel to canonically sparse signals and assume that  X  = I (the N  X  N identity matrix) so that x =  X  .
 With the sparsity prior and RIP assumptions, inverse maps can be obtained by solving the following convex problems: basis pursuit (BP) and basis pursuit denoising (BPDN), respectively; and,  X  3 is a scalarization of BPDN [1, 9]. They also have the following deterministic worst-case guarantee when  X  has RIP: where C 1 , 2 are constants, x K is the best K -term approximation, i.e., x K = arg min k x 0 k x k r for r  X  1 , and k x k 0 is a pseudo-norm that counts the number of nonzeros of x [1, 4, 5]. Note that the error guarantee (3) is adaptive to each given signal x because of the definition of x K . Moreover, the guarantee does not assume that the signal is sparse. We define a signal x as p -compressible if it lives close to the shell of the weak-` p ball of radius decreasing order of magnitude as Then, when x  X  sw` p ( R ) , the i -th ordered entry  X  x ( i ) in (4) obeys where . means  X  X ess than or approximately equal to. X  We deliberately substitute . for  X  in the p -compressibility definition of [1] to reduce the ambiguity of multiple feasible R and p values. In Section 6, we describe a geometric approach to learn R and p so that R  X  i  X  1 /p  X   X  x ( i ) . Signals in sw` p ( R ) can be well-approximated by sparse signals as the best K -term approximation error decays rapidly to zero as Given M , a good rule of thumb is to set K = M/ [ C log( N/M )] ( C  X  4 or 5 ) and use (6) to predict the approximation error for the decoders  X  i in Section 2. Since the decoding guarantees are bounded by the best K -term approximation error in ` 1 (i.e., r = 1 ; cf. (3)), we will restrict our attention to x  X  sw` p where p &lt; 1 . Including p = 1 adds a logarithmic error factor to the approximation errors, which is not severe; however, it is not considered in this paper to avoid a messy discussion. Suppose now the individual entries x i of the signal x are random variables (RV) drawn iid with in (4) are also RV X  X  and are known as the order statistics (OS) of yet another pdf  X  f (  X  x ) , which can RV X  X  x i (hence,  X  x i ) are iid, the RV X  X   X  x ( i ) are statistically dependent.
 The concept of OS enables us to create a link between signals summarized by pdf X  X  and their com-pressibility, which is a deterministic property after the signals are realized. The key to establish- X   X  F ? ( u ) as the magnitude quantile function (MQF) of f ( x ) .
 A well-known quantile approximation to the expected OS of a pdf is given by [10]: where E [  X  ] is the expected value. Moreover, we have the following moment matching approximation instance, these deviations for i &gt; K can be used to bound the statistical variations of the best K -term approximation error. In practice, the deviations are relatively small for compressible priors. In Sections 4 X 6, we will use the quantile approximation in (7) as our basis to motivate the set of compressible priors, derive recovery algorithms for x , and learn the parameters of compressible priors during recovery. A compressible prior f ( x ;  X  ) in ` r is a pdf with parameters  X  whose MQF satisfies Table 4 lists example pdf X  X , parameterized by  X  = ( q, X  ) 0 , and the sw` p ( R ) parameters of their N -sample iid realizations. In this paper, we fix r = 1 (cf. Section 3); hence, the example pdf X  X  are compressible priors whenever p &lt; 1 . In (9), we make it explicit that the sw` p ( R ) parameters can depend on the parameters  X  of the specific compressible prior as well as the signal dimension N . The dependence of the parameter p on N is of particular interest since it has important implications in signal recovery as well as parameter learning from finite sample sizes, as discussed below. We define natural p -compressible priors as the set N p of compressible priors such that p = p (  X  ) &lt; 1 is independent of N ,  X  f ( x ;  X  )  X  N p . It is possible to prove that we can capture most of the ` 1 -energy in an N -sample iid realization from a natural p -compressible prior by using a constant K , i.e., k x  X  x K k 1  X  k x k 1 for any desired 0 &lt; 1 by choosing K = d ( p/ ) p 1  X  p e . Hence, N -sample iid signal realizations from the compressible priors in N p can be truly embedded into dimensions M that grow logarithmically with N with tractable decoding guarantees due to (3). N p members include the generalized Pareto (GPD), Fr  X  echet (FD), and log-logistic distributions (LLD). It then only comes as a surprise that generalized Gaussian distribution (GGD) is not a natural p -compressible prior since its iid realizations lose their compressibility as N grows (cf. Table 4). While it is common practice to use a GGD prior with q  X  1 for sparse signal recovery, we have no recov-ery guarantees for signals generated from GGD when M grows logarithmically with N in (1). 1 In fact, to be p -compressible, the shape parameter of a GGD prior should satisfy q = N e W  X  1 (  X  p/N ) , where W  X  1 (  X  ) is the Lambert W -function with the alternate branch. As a result, the learned GGD parameters from dimensionality-reduced data will in general depend on the dimension and may not generalize to other dimensions. Along with GGD, Table 4 shows how Weibull, gamma, and log-normal distributions are dimension-restricted in their membership to the set of compressible priors. Wavelet coefficients of natural images provide a stylized example to demonstrate why we should care about the dimensional independence of the parameter p . 2 As a brief background, we first note that research in natural image modeling to date has had two distinct approaches, with one focus-ing on deterministic explanations and the other pursuing probabilistic models [12]. Deterministic approaches operate under the assumption that the natural images belong to Besov spaces, having a bounded number of derivatives between edges. Unsurprisingly, wavelet thresholding is proven near-optimal for representing and denoising Besov space images. As the simplest example, the magnitude probabilistic approaches, on the other hand, exploit the power-law decay of the power spectra of im-ages and fit various pdf X  X , such as GGD and the Gaussian scale mixtures, to the histograms of wavelet coefficients while trying to simultaneously capture the dependencies observed in the marginal and joint distributions of natural image wavelet coefficients. Probabilistic approaches are quite important in image compression because optimal compressors quantize the wavelet coefficients according to the estimated distributions, dictating the image compression limits via Shannon X  X  coding theorem. We conjecture that probabilistic models that summarize the wavelet coefficients of natural images belong to the set of natural (non-iid) p -compressible priors. We base our claim on two observations: 1) Due to the multiscale nature of the wavelet transform, the decay profile of the magnitude sorted wavelet coefficients are scale-invariant, i.e., preserved at different resolutions, where lower resolu-tions inherit the highest resolution. Hence, probabilistic models that explain the wavelet transform of any signals should exhibit this decay profile inheritance property. 2) The magnitude sorted wavelet coefficients of natural images exhibit a constant decay rate, as expected of Besov space images. Section 7.2 demonstrates the ideas using natural images from the Berkeley natural images database. Convex problems to recover sparse or compressible signals in (2) are usually motivated by Bayesian inference. In a similar fashion, we formalize two new decoding algorithms below by assuming prior distributions on the signal x and the noise n , and then asking inference questions given y in (1). 5.1 Fixed point continuation for a non-iid compressible prior The multivariate Lomax distribution (MLD) provides an elementary example of a non-iid compress-Moreover, given n -realizations x 1: n of MLD ( n  X  N ), the joint marginal distribution of x n +1: N is it can be proved that MLD is compressible with p = 1 [14]. For now, we will only demonstrate this property via simulations in Section 7.1. With the MLD prior on x , we focus on only two op-timization problems below, one based on BP and the other based on maximum a posteriori (MAP) estimation. Other convex formulations, such as BPDN (  X  2 in (2)) and LASSO [15], trivially follow. 1) BP Decoder: When there is no noise, the observations are given by y =  X  x , which has infinitely many solutions, as discussed in Section 1. In this case, we can exploit the MLD likelihood function to regularize the solution space. For instance, when we ask for the solution that maximizes the MLD likelihood given y , it is easy to see that we obtain the BP decoder formulation, i.e.,  X  1 ( y ) in (2). 2) MAP Decoder: Suppose that the noise coefficients ( n i  X  X  in (1)) are iid Gaussian with zero mean and variance  X  2 , n i  X  N ( n ; 0 , X  2 ) . Although many inference questions are possible, here we seek the mode of the posterior distribution to obtain a point estimate, also known as the MAP estimate. be derived using the Bayes rule as Unfortunately, we stumble upon a non-convex problem in (10) during our quest for the MAP es-timate. We circumvent the non-convexity in (10) using a majorization-minimization idea where we iteratively obtain a tractable upperbound on the log-term in (10) using the following inequality:  X  u,v  X  (0 ,  X  ) , log u  X  log v + u/v  X  1 . After some straightforward calculus, we obtain the iterative decoder below, indexed by k , where The decoding approach in (11) can be viewed as a continuation (or a homotopy) algorithm where a fixed point is obtained at each iteration, similar to [16]. This decoding scheme has provable, linear convergence guarantees when k 5.2 Iterative ` s -decoding for iid scale mixtures of GGD We consider a generalization of GPD and the Student X  X  t distribution, which we will denote as the generalized Gaussian gamma scale mixture distribution (SMD, in short), whose pdf is given by SMD ( x ; q, X ,s )  X  (1 + | x | s / X  s )  X  ( q +1) /s . The additional parameter s of SMD modulates its OS near the origin. It can be proved that SMD is p -compressible with p = q [14]. SMD, for instance, arises through the following interaction of the gamma distribution and GGD: x = a  X  1 /s b , a  X  Gamma ( a ; q/s, X   X  s ) , and b  X  GGD ( b ; s, 1) . Given a , the distribution of x is a scaled GGD: distribution of x . SMD arise in multiple contexts, such as the SLB framework that exploit Student X  X  t (i.e., s = 2 ) for learning problems [2], and the Laplacian and Gaussian scale mixtures (i.e., s = 1 and 2 , respectively) that model natural images [17, 18].
 Due to lack of space, we only focus on noiseless observations in (1). We assume that x is an N -sample iid realization from SMD ( x ; q, X ,s ) with known parameters ( q, X ,s ) 0 and choose a solu-tion The majorization-minimization trick in Section 5.1 also circumvents the non-convexity in (12): The decoding scheme in (13) is well-known as the iterative re-weighted ` s algorithms [7, 19 X 21]. While deriving decoding algorithms in Section 5, we assumed that the signal coefficients x i are generated from a compressible prior f ( x ;  X  ) and that  X  is known. We now relax the latter assumption and discuss how to simultaneously estimate x and learn the parameters  X  .
 When we visualize the joint estimation of x and  X  from y in (1) as a graphical model, we imme-diately realize that x creates a Markov blanket for  X  . Hence, to determine  X  , we have to estimate the signal coefficients. When  X  has the stable embedding property, we know that the decoding al-gorithms can obtain x with approximation guarantees, such as (3). Then, given x , we can choose an estimator for  X  via standard Bayesian inference arguments. Unfortunately, this argument leads to one important road block: estimation of the signal x without knowing the prior parameters  X  . on x and  X  while optimizing the Bayesian objective. Unfortunately, there is one important and unrecognized bug in this argument: the estimated signal values are in general not iid, hence we would be minimizing the wrong Bayesian objective to determine  X  . To see this, we first note that the recovered signals approximation of the signal x K and some other coefficients that explain the small tail energy. We then recall from Section (3) that the coefficients of x K are statistically dependent. Hence, at least partially, the significant coefficients of issue is to treat the recovered signals as if they are drawn iid from a censored GPD. However, the optimization becomes complicated and the approach does not provide any additional guarantees. As an alternative, we propose to exploit geometry and use the consensus among the coefficients in fitting the sw` p ( R ) parameters via the auxiliary signal estimates To do this, we employ Fischler and Bolles X  probabilistic random sampling consensus (RANSAC) algorithm [22] to fit a line, whose y -intercept is log R ( N,  X  ) and whose slope is 1 /p ( N,  X  ) : where C  X  4 , 5 as discussed in Section. 3. RANSAC provides excellent results with high probability even if the data contains significant outliers. Because of its probabilistic nature, it is computationally efficient. The RANSAC algorithm requires a threshold to gate the observations and count how much a proposed solution is supported by the observations [22]. We determine this threshold by bounding the tail probability that the OS of a compressible prior will be out of bounds. For the pseudo-code and further details of the RANSAC algorithm, cf. [22]. 7.1 Order Statistics To demonstrate the sw` p ( R ) decay profile of p -compressible priors, we generated iid realizations of GGD with q = 1 (LD) and GPD with q = 1 , and (non-iid) realizations of MLD with q = 1 of varying signal dimensions N = 10 j , where j = 2 , 3 , 4 , 5 . We sorted the magnitudes of the signal coefficients, normalized them by their corresponding value of R . We then plotted the results on a log-log scale in Fig. 1. At http://dsp.rice.edu/randcs , we provide a MATLAB routine ( randcs.m ) so that it is easy to repeat the same experiment for the rest of the distributions in Table 4. Figure 1(a) illustrates that the iid LD slope is much greater than  X  1 and moreover logarithmically grows with N . In contrast, Fig. 1(b) shows that iid GPD with q = 1 exhibits the constant slope of  X  1 that is independent of N . MLD with q = 1 also delivers such a slope (Fig. 1(c)). The latter two distributions thus produce compressible signal realizations, while the Laplacian does not. 7.2 Natural Images We investigate the images from the Berkeley natural images database in the context of p -compressible priors. We randomly sample 100 image patches of varying sizes N = 2 j  X  2 j ( j = 3 ,..., 8 ), take their wavelet transforms (scaling filter: daub2), and plot the average of their magnitude ordered wavelet coefficients in Figs. 2(a) and (b) (solid lines). Figure 2(c) also illustrates the OS of the pixel gradients, which are of particular interest in many applications.
 Along with the wavelet coefficients, Fig. 2(a) superposes the expected OS of GPD with q = 1 . 67 Although wavelet coefficients of natural images do not follow an iid distribution, they exhibit a constant decay rate, which can be well-approximated by an iid GPD distribution. This apparent constant decay rate is well-explained by the decay profile inheritance of the wavelet transform across different resolutions and supports the Besov space assumption used in the deterministic approaches. The GPD rate of q = 1 . 67 implies a disappointing O ( K  X  0 . 1 ) approximation rate in the ` 2 -norm vs. the theoretical O ( K  X  0 . 5 ) rate [23]. Moreover, we lose all the guarantees in the ` 1 -norm. In contrast, Fig. 2(b) demonstrates the GGD histogram fits to the wavelet coefficients, where the GGD exponent q  X  [0 . 5 , 1] depends on the particular dimension and decreases as N increases. The histogram matching is common practice in the existing probabilistic approaches (e.g., [18]) to de-termine pdf X  X  that explain the statistics of natural images. Typically, least square error metrics or Kullback-Liebler (KL) divergence measures are used. Although the GGD fit via histogram matching in Fig. 2(b) deceptively appears to fit a small number of coefficients, we emphasize the log-log scale of the plots and mention that there is a significant number of coefficients in the narrow space where the GGD distribution is a good fit. Unfortunately, these approaches approximate the wavelet coeffi-cients of natural images that have almost no approximation power of the overall image. Moreover, the learned GGD distribution is dimension dependent, assigns lower probability to the large coeffi-cients that explain the image well, and predicts a mismatched OS of natural images (cf.Fig. 2(b)). Figure 2(c) compares the magnitude ordered pixel gradients of the images (solid lines) with the expected OS of GGD (dashed line). From the figure, it appears that the natural image pixel gradients lose their compressibility as the image dimensions grow, similar to the GGD, Weibull, gamma, and log-normal distributions. In the figure, the GGD parameters are given ( q, X  ) = (0 . 95 , 25) . 7.3 Iterative ` 1 Decoding We repeat the compressible signal recovery experiment in Section 3.2 of [7] to demonstrate the performance of our iterative ` s decoder with s = 1 in (13). We first randomly sample a signal x  X  R N ( N = 256 ) where the signal coefficients are iid from the GPD distribution with q = 0 . 4 and  X  = ( N + 1)  X  1 /q so that the E [  X  x (1) ]  X  1 . We set M = 128 and draw a random M  X  N matrix with iid Gaussian entries to obtain y =  X  x . We then decode signals via (13) where maximum iterations is set to 5, with the knowledge of the signal parameters and with learning. During the learning phase, we use log(2) as the threshold for the RANSAC algorithm. We set the maximum iteration count of RANSAC to 500 .
 The results of a Monte Carlo run with 100 independent realizations are illustrated in Fig. 3. In Figs. 3(a) and (b), the plots summarize the average improvement over the standard decoder  X  1 ( y ) via the histograms of k x  X  (0 . 7062 , 0 . 1380) when we know the parameters of the GPD (a) and (0 . 7101 , 0 . 1364) when we learn the parameters of the GPD via RANSAC (b). The learned sw` p exponent is summarized by the his-togram in Fig. 3(c), which has mean and standard deviation (0 . 3757 , 0 . 0539) . Hence, we conclude that the our alternative learning approach via the RANSAC algorithm is competitive with knowing the actual prior parameters that generated the signal. Moreover, the computational time of learning is insignificant compared to time required by the state-of-the art linear SPGL algorithm [24]. Compressible priors create a connection between probabilistic and deterministic models for signal compressibility. The bridge between these seemingly two different modeling frameworks turns out to be the concept of order statistics. We demonstrated that when the p -parameter of a compressible prior is independent of the ambient dimension N , it is possible to have truly logarithmic embedding of its iid signal realizations. Moreover, the learned parameters of such compressible priors are di-mension agnostic. In contrast, we showed that when the p -parameter depends on N , we have many restrictions in signal embedding and recovery as well as in parameter learning. We illustrated that wavelet coefficients of natural images can be well approximated by the generalized Pareto prior, which in turn predicts a disappointing approximation rate for image coding with the n  X  aive sparse model and for CS image recovery from measurements that grow only logarithmically with the im-age dimension. We motivated many of the existing sparse signal recovery algorithm as instances of a corresponding compressible prior and discussed parameter learning for these priors from dimen-sionality reduced data. We hope that the iid compressibility view taken in this paper will pave the way for a better understanding of probabilistic non-iid and structured compressibility models. [1] E. J. Cand ` es. Compressive sampling. In Proc. International Congress of Mathematicians , [2] M.E. Tipping. Sparse bayesian learning and the relevance vector machine. The Journal of [3] D. P. Wipf and B. D. Rao. Sparse Bayesian learning for basis selection. IEEE Transactions on [4] T. Blumensath and M.E. Davies. Sampling theorems for signals from the union of linear [5] A. Cohen, W. Dahmen, and R. DeVore. Compressed sensing and best k-term approximation. [6] I. F. Gorodnitsky, J. S. George, and B. D. Rao. Neuromagnetic source imaging with FO-[7] E. J. Cand ` es, M. B. Wakin, and S. P. Boyd. Enhancing sparsity by reweighted ` 1 minimization. [8] D. P. Wipf and S. Nagarajan. Iterative reweighted ` 1 and ` 2 methods for finding sparse solu-[9] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM [10] H.A. David and H.N. Nagaraja. Order Statistics . Wiley-Interscience, 2004. [11] S. Mallat. A Wavelet Tour of Signal Processing . Academic Press, 1999. [12] H. Choi and R. G. Baraniuk. Wavelet statistical models and Besov spaces. Lecture Notes in [13] T. K. Nayak. Multivariate Lomax distribution: properties and usefulness in reliability theory. [14] V. Cevher. Compressible priors. IEEE Trans. on Information Theory, in preparation, 2010. [15] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical [16] E. T. Hale, W. Yin, and Y. Zhang. Fixed-point continuation for ` 1 -minimization: Methodology [17] P. J. Garrigues. Sparse Coding Models of Natural Images: Algorithms for Efficient Inference [18] M. J. Wainwright and E. P. Simoncelli. Scale mixtures of Gaussians and the statistics of natural [19] D. Wipf and S. Nagarajan. A new view of automatic relevance determination. In NIPS , vol-[20] I. Daubechies, R. DeVore, M. Fornasier, and S. Gunturk. Iteratively re-weighted least squares [21] R. Chartrand and W. Yin. Iteratively reweighted algorithms for compressive sensing. In [22] M.A. Fischler and R.C. Bolles. Random sample consensus: a paradigm for model fitting [23] E. J. Candes and D. L. Donoho. Curvelets and curvilinear integrals. Journal of Approximation [24] E. van den Berg and M. P. Friedlander. Probing the Pareto frontier for basis pursuit solutions.
