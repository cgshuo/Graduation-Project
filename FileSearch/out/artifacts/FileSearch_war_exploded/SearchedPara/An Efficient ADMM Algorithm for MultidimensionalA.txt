 Total variation (TV) regularization has important applica-tions in signal processing including image denoising, image deblurring, and image reconstruction. A significant chal-lenge in the practical use of TV regularization lies in the non-differentiable convex optimization, which is difficult to solve especially for large-scale problems. In this paper, we pro-pose an efficient alternating augmented Lagrangian method (ADMM) to solve total variation regularization problem-s. The proposed algorithm is applicable for tensors, thus it can solve multidimensional total variation regularization problems. One appealing feature of the proposed algorithm is that it does not need to solve a linear system of equa-tions, which is often the most expensive part in previous ADMM-based methods. In addition, each step of the pro-posed algorithm involves a set of independent and smaller problems, which can be solved in parallel. Thus, the pro-posed algorithm scales to large size problems. Furthermore, the global convergence of the proposed algorithm is guaran-teed, and the time complexity of the proposed algorithm is O ( dN/ )ona d -mode tensor with N entries for achieving an -optimal solution. Extensive experimental results demon-strate the superior performance of the proposed algorithm in comparison with current state-of-the-art methods. H.2.8 [ Database Management ]: Database Applications X  Data Mining Algorithms Multidimensional total variation, ADMM, parallel comput-ing, large scale
The presence of noise in signals is unavoidable. To re-cover original signals, many noise reduction techniques have been developed to reduce or remove the noise. Noisy sig-nals usually have high total variation (TV). Several total variation regularization approaches have been developed to exploit the special properties of noisy signals and they have been widely used in noise reduction in signal processing. The total variation model was first introduced by Rudin, Osher and Fatemi in [19] as a regularization approach to remove noise and handle proper edges in images. More recently, the total variation models have been applied successfully for image reconstruction, e.g. Magnetic Resonance (MR) image reconstruction [14, 17]. The wide range of applications in-cluding image restoration, image denoising and deblurring [1, 2, 14, 15, 22, 24], underscore its success in signal/image processing. The discrete penalized version of the TV-based image denoising model solves an unconstrained convex min-imization problem of the following form: where  X  F is the Frobenius norm defined as X F = known image to be recovered, and  X  TV is the discrete TV norm defined below. The nonnegative regularization pa-rameter  X  provides a tradeoff between the noise sensitivity and closeness to the observed image. There are two popu-lar choices for the discrete TV norm: 2 -based isotropic TV defined by and the 1 -based anisotropic TV defined by where  X  denotes the forward finite difference operators on the vertical and horizonal directions, i.e.,  X  x i,j =(  X   X 
Despite the simple form of the TV norm, it is a chal-lenge to solve TV-based regularization problems efficiently. One of the key difficulties in the TV-based image denoising problem is the nonsmoothness of the TV norm. Continued research efforts have been made to build fast and scalable numerical methods in the last few years. Existing methods aim to balance the tradeoff between the convergence rate and the simplicity of each iterative step. For example, com-puting the exact optimal solution at each iteration leads to a better convergence rate [20]. However, this usually requires heavy computations, for instance, a large linear system of equations. Simple methods with less computation efforts at each iteration are more suitable for large-scale problems, but usually they have a slow convergence rate. To this end, we propose a fast but simple ADMM algorithm to solve TV-based problems. The key idea of the proposed method is to decompose the large problem into a set of smaller and inde-pendent problems, which can be solved efficiently and exact-ly. Moreover, these small problems are decoupled, thus they can be solved in parallel. Therefore, the proposed method scales to large-size problems.

Although the TV problems have been extensively stud-ied for matrices (e.g. two-dimensional images), there is not much work on tensors, a higher-dimensional extension of matrices. Tensor data is common in real world applications, for instance, functional magnetic resonance imaging (fMRI) is a 3-mode tensor and a color video is a 4-mode tensor. An-other contribution of this paper is that the proposed ADMM algorithm is designed to solve TV problems for tensors, e.g., multidimensional TV problems. The 2D TV problem can be solved efficiently by a special case of the proposed algorith-m (for matrices). Our experiments show that the proposed method is more efficient than state-of-the-art approaches for solving 2D TV problems. We further demonstrate the ef-ficiency of the proposed method for multidimensional TV problems in image reconstruction, video denoising and im-age deblurring.
Due to the nonsmoothness of the TV norm, solving large-scale TV problems efficiently continues to be a challenging issue despite its simple form. In the past, considerable ef-forts have been devoted to develop an efficient and scalable algorithm for TV problems. The 1D total variation, also known as the fused signal approximator, has been widely used in signal noise reduction. Liu et al. [16] propose an ef-ficient method to solve the fused signal approximator using a warm start technique. It has been shown to be very efficient in practice, though the convergence rate has not been estab-lished. Barabero and Sra [1] introduce a fast Newton-type method for 1D total variation regularization, and solve the 2D total variation problem using the Dyktra X  X  method [7]. Wahlberg et al. [23] propose an ADMM method to solve the 1D total variation problem. A linear system of equation-s has to be solved at each iteration. Recently, a very fast direct, noniterative, algorithm for 1D total variation prob-lem has been proposed in [8]. A dual-based approach to solve the 2D total variation problems is introduced in [6]. Beck and Teboulle [2] propose a fast gradient-based method by combining the dual-based approach with the acceleration technique in Fast Iterative Shrinkage Thresholding Algorith-Figure 1: Fibers of a 3-mode tensor: mode-1 fiber-x m (FISTA) [3]. One potential drawback of the dual-based approaches is that it may not scale well. Goldstein and Osh-er introduce the split Bregman method to solve the 2D total variation problem, which is an application of split Bregman method solving 1 based problems. The total variation has also been widely used in Magnetic Resonance (MR) image reconstruction [14, 17]. Ma et al.[17] introduce an operator-splitting algorithm (TVCMRI) to solve the MR image recon-struction problem. By combining the composite splitting al-gorithm [7] and the acceleration technique in FISTA, Huang et al. [14] propose an efficient MR image reconstruction al-gorithm called FCSA. We show that our proposed method is much more efficient than these methods for solving 2D TV problems.
We use upper case letters for matrices, e.g. X ,lowercase letters for the entries, e.g. x i,j , and bold lower case letters for vectors, e.g. x . The inner product in the matrix space is defined as X,Y = i,j x i,j y i,j .A d -mode tensor (or For example, 1-mode tensor is a vector, and 2-mode tensor at logue of matrix rows and columns (see Figure 1 for an il-lustration). The Frobenius norm of a tensor is defined as X F =( y sent the index set excluding j i , i.e., { j 1 ,...,j i  X  1 / { j i } . In addition, we use a nonnegative superscript num-ber to denote the iteration index, e.g., X k denotes the value of
X at the k -th iteration.
We present the multidimensional total variation regular-ization problems and the proposed ADMM method in Sec-tion 2. One of the key steps in the proposed algorithm in-volves the solution of a 1D TV problem; we show how to estimate the active regularization parameter range for 1D TV problem in Section 3. We report empirical results in Section 4, and conclude this paper in Section 5.
We first introduce the multidimensional total variation regularization problems in Section 2.1. In Section 2.2, we present the details of the proposed algorithm. The glob-al convergence is established in Section 2.3. Section 2.4 presents the time complexity of the proposed algorithm.
Denote F i ( X ) as the fused operator along the i -th mode of X taking the form of In the case of matrix, F i ( X ) only involves the rows or column-sof X . For example, F 1 ( X )= n j =1 m  X  1 i =1 | x i,j m  X  n . It is clear that the 1 -based anisotropic TV norm for matrices can be rewritten as 2 i =1 F i ( X ). The tensor is the generalization of the matrix concept. We generalize the TV norm for the matrix case to higher-order tensors by the fol-lowing tensor TV norm: Based on the definition above, the TV-based denoising prob-lem for the matrix case can be generalized to tensors by solving the following optimization problem: estimated, d i =1 F i ( X ) is the tensor TV norm, and  X  is a nonnegative regularization parameter. The tensor TV regu-larization encourages X to be smooth along all dimensions.
We propose to solve the multidimensional TV problem (MTV) using ADMM [4]. ADMM decomposes a large glob-al problem into a series of smaller local subproblems, and coordinates the local solutions to compute the globally op-timal solution. ADMM attempts to combine the benefits of augmented Lagrangian methods and dual decomposition for constrained optimization problems [4]. The problem solved by ADMM takes the following form: where x, z are unknown variables to be estimated.
ADMM reformulates the problem using a variant of the augmented Lagrangian method as follows: L ( x , z , X  )= f ( x )+ g ( z )+  X  T ( A x + B z  X  c )+  X  with  X  being the augmented Lagrangian multiplier, and  X  being the nonnegative penalty parameter (or dual update length). ADMM solves the original constrained problem by iteratively minimizing L  X  ( x , z , X  )over x , z , and updating  X  according to the following update rule:
Consider the unconstrained optimization problem in (2), which can be reformulated as the following constrained op-timization problem: where Z i , 1  X  i  X  d are slack variables. The optimization problem in (4) can be solved by ADMM. The augmented Lagrangian of (4) is given by
L ( X , Z i , U i )= 1
Applying ADMM, we carry out the following steps at each iteration: Step 1 Update X k +1 with Z k i and U k i fixed: The optimal solution is given by Step 2 Compute Z k +1 i ,i =1 ,  X  X  X  ,d with X k +1 ,and U k 1 ,  X  X  X  ,d fixed: where {Z i } denotes the set {Z 1 ,..., Z d } . This problem is decomposable, i.e., we can solve Z k +1 i , 1  X  i  X  d separately, which can be equivalently written as with T i =  X  1  X  U k i + X k +1 . The problem in (9) is decompos-as a mode-i fiber to be estimated, which is a vector of I length. For simplicity, we use v to represent the vector z set of independent and much smaller problems: where t is the corresponding mode-i fiber of T i . (10) is the formulation of 1D total variation regularization problem, which can be solved exactly and very efficiently [8, 16].
The problem of computing Z k +1 i , 1  X  i  X  d in (8) is there-fore decomposed into a set of much smaller problems of com-puting fibers. Each fiber problem is independent, enabling that the whole set of problems can be computed in parallel. Step 3 Update U k +1 i ,i =1 ,...,d :
A summary of the proposed method is shown in Algorith-m1below.

Algorithm 1: The proposed ADMM algorithm for multi-dimensional total variation Input : Y , X , X  Output : X
Initialization: Z 0 i = X 0  X  X  , U 0 i  X  0; do
Until Convergence ; return X ;
The algorithm stops when the primal and dual residuals [4] satisfy a certain stopping criterion. The stopping crite-rion can be specified by two thresholds: absolute tolerance abs and relative tolerance rel (see Boyd et al. [4] for more details). The penalty parameter  X  affects the primal and d-ual residuals, hence affects the termination of the algorithm. A large  X  tends to produce small primal residuals, but in-creases the dual residuals [4]. A fixed  X  (say 10) is commonly used. But there are some schemes of varying the penalty pa-rameter to achieve better convergence. We refer interested readers to Boyd et al. [4] for more details.

Remark 1. We can add the 1 regularization in the for-mulation of multidimensional TV problems for a sparse so-lution. The subproblem with 1 regularization is called the fused signal approximator. The optimal solution can be ob-tained by first solving 1D total variation problem, then ap-plying soft-thresholding [11, 16].
The convergence of ADMM to solve the standard form (3) has been extensively studied [4, 10, 13]. We establish the convergence of Algorithm 1 by transforming the MTV prob-lem in (4) into a standard form (3), and show that the trans-formed optimization problem satisfies the condition needed to establish the convergence.

Denote x as the vectorization of X , i.e., x = vec ( X )  X  ] ThentheMTVproblemin(4)canberewrittenas where A =[ I,...,I ] T  X  d i I i  X  i I i , and I is the identity matrix of size i I i  X  i I i . The first and second steps of Algorithm 1 are exactly the steps of updating x and z in the standard form. Since f,g are proper, closed, and convex, and A is of column full rank, the convergence of Algorithm 1 directly follows from the results in [4, 10, 13]. Moreover, an O (1 /k ) convergence rate of Algorithm 1 can be established following the conclusion in [13]. The first step of Algorithm 1 involves computations of i ,i =1 ,...,d . Computing j = i I j mode-i fibers of I i length by the 1D total variation algorithm. The complexity of solving the 1D total varia-observe that the empirical complexity is O ( I i )inourexper-iments (see Figure 2). Thus, the complexity of the first step is O ( d j I j ). The time complexity of the second and third steps are O ( j I j ). Hence, the complexity of each iteration is O ( d j I j ). The number of iterations in Algorithm 1 to obtain an -optimal solution is O (1 / ) [13]. Thus, the total complexity of Algorithm 1 is O ( d j I j / ) for achieving an -optimal solution. Figure 2: Computational time (seconds) of three ef-ficient 1D total variation algorithms: Liu et al. [16], Condat [8], and Wahlberg et al. [23]. Left: dimen-sion varies from 10 3 to 10 6 with  X  =1 .Right:  X  varies from 0.15 to 1.45 with dimension 10 4 . The data is sampled from standard normal distribution.
The most time-consuming part of the proposed ADMM algorithm is the first step, which involves the computation of ing X k +1 i , 1  X  i  X  d into a set of small 1D total variation problems. Thus, the computation of the proposed method highly depends on that of 1D total variation. In this section, we show how to estimate the active regularization range for 1D total variation, which only relies on the regularization parameter and the observed vector, to directly compute the optimal solution. More specifically, we compute  X  min and  X  max based on the observed vector; if  X /  X  (  X  min , X  max ), the optimal solution can be computed in a closed form, thus significantly improving the efficiency.

Consider the formulation of 1D total variation, i.e., which can be rewritten as in which y , x  X  n . G  X  ( n  X  1)  X  n encodes the structure of the 1D TV norm. We have
Before we derive the dual formualtion of problem in (13) [5, 9], we first introduce some useful definitions and lemmas.
Definition 1. (Coercivity). [9] A function  X  : n  X   X  is said to be coercive over a set S X  n if for every sequence { x k } X  X  For S = n ,  X  is simply called coercive.
 Denote the objective function in problem (13) as: It is easy to see that f ( x )iscoercive. Foreach  X   X  ,we define the  X  sublevel set of f ( x )as S  X  = { x : f ( x ) Then we have the following lemma.

Lemma 1. For any  X   X  , the sublevel set S  X  = { x : f ( x )  X   X  } is bounded.

Proof. We prove the lemma by contradiction. Suppose there exists an  X  such that S  X  is unbounded. Then we can find a sequence { x k } X  X   X  such that lim k  X  X  X  x k =  X  . Be-cause f ( x ) is coercive, we can conclude that lim k  X  X  X  f ( x +  X  . However, since { x k } X  X   X  ,weknow f ( x k )  X   X  for all k , which leads to a contradiction. Therefore, the proof is complete.
 We derive the dual formulation of problem (13) via the Sion X  X  Minimax Theorem [9, 21]. Let B = { x : y  X   X G T s , s  X  1 } and x =argmax x  X  X  f ( x ). Because B is compact, x must exist. Denote  X  = f ( x )and S = S  X  . inf By Lemma 1, we know that S is compact. Moreover, the function is convex and concave with respect to x and s respectively. Thus, by the Sion X  X  Minimax Theorem [21], we have We can see that x  X  ( s )= y  X   X G T s =argmin and thus Therefore the primal problem (16) is transformed to its dual problem: which is equivalent to
Recall the definition of G in (14), it follows that G T has full column rank. Denote e =(1 ,  X  X  X  , 1) T  X  n ,andthe subspace spanned by the rows of G and e as V G T and V e .
Let P = I  X  ee T e,e and P  X  denote the projection operator into V G T and V e respectively. Therefore the equation must have a unique solution for each y .

Let P y = y , then it follows that and clearly s n  X  1 =  X  n  X  1 j =1 y j = y n since e, y =0. Denote
From the above analysis, it is easy to see that when  X   X   X  max ,thereisan s According to (18), we have x  X  =( P + P  X  ) y  X   X G T s  X  = P  X  y = e, y
The maximal value for  X  has been studied in [16]. Howev-er, a linear system has to be solved. From (22), it is easy to see that the maximal value can be obtained by a close form solution. Thus, our approach is more efficient.
We rewrite the dual problem (19) as:
Denote g ( s )= 1 2 y  X   X G T s 2 . The gradient of g ( s )can be found as: g ( s )=  X   X G ( y  X   X G T s ).
Let B  X  denote the unit  X  norm ball. We know that s  X  is the unique optimal solution to the problem (24) if and only if where N B  X  ( s  X  ) is the normal cone at s  X  with respect to
Let I + ( s )= { i : s i =1 } , I  X  ( s )= { i : s i =  X  1 be found as: Therefore the optimality condition can be expressed as: if and only if Because  X &gt; 0,  X G ( y  X   X G T s )  X  N B  X  ( s  X  )isequivalentto According to (18), we have and By (25) and (27), we have the following observations: B1. If x  X  i +1 &gt;x  X  i , s  X  i =1; B2. If x  X  i +1 = x  X  i , s  X  i  X  [  X  1 , 1]; B3. If x  X  i +1 &lt;x  X  i , s  X  i =  X  1.

Notice that, from (26), we can estimate a range for every x , which is not necessarily the tightest one. In fact, we have
Define It follows that when  X &lt; X  min , the solution to (26) is fixed and can be found as: Then x  X  i can be computed accordingly by (26).
In this section, we evaluate the efficiency of the proposed algorithm on synthetic and real-word data, and show several applications of the proposed algorithm.
We examine the efficiency of the proposed algorithm using synthetic datasets on 2D and 3D cases. For the 2D case, the competitors include For the 3D case, only the Dykstra X  X  method and the pro-posed method (MTV) are compared, since the other algo-rithms are designed specifically for the 2D case. The experiments are performed on a PC with quad-core Intel 2.67GHz CPU and 9GB memory. The code of MTV is written in C. Since the proposed method and the Dykstra X  X  method can be implemented in parallel, we also compare their parallel versions implemented with OpenMP.
Figure 3: Left: clean image; right: noisy image; We generate synthetic images Y  X  N  X  N of different N . The value of each pixel is 1 or 0. A Gaussian noise = N (0 , 0 . 2 2 ) is added to each image as  X  Y = Y + .Asynthetic example image is shown in Figure 3. The comparisons are based on the computation time. For a given  X  ,wefirstrun MTV until a certain precision level specified by abs and rel is reached, and then run the others until they achieve an objective function value smaller than or equal to that of MTV. Different precision levels of the solutions are evaluated such that a fair comparison can be made. In addition, we set the maximal iteration number of all methods to be 2000 in order to avoid slow convergence. The penalty parameters  X  for MTV and ADAL are fixed to 10. We vary the size of image ( N  X  N )from50  X  50 to 2000  X  2000 with  X  =0 . 35, and vary the regularization parameter  X  from 0.15 to 1 with a step size of 0.05 with a fixed N = 500. For each setting, we perform 20 trials and report the average computational time (seconds). The results are shown in Figure 4.
From Figure 4, we observe that the proposed method is much more efficient than its competitors. The non-parallel version of MTV is about 70 times faster than the dual method, and 8 times fasters than ADAL when N is 2000 and abs = rel =1 e  X  3. Although the subproblems of MTV and Dyk-stra are the same, Dykstra is about 12 times slower than MTV, demonstrating that MTV has faster convergence than Dykstra. Utilizing parallel computing, the parallel version of tag7.web.rice.edu/Split_Bregman.html iew3.technion.ac.il/~becka/papers/tv_fista.zip Figure 4: Comparison of SplitBregman [12], ADAL [18], Dual Method [2], Dykstra [7], and our proposed MTV algorithm in terms of computation-al time (in seconds and in the logarithmic scale). Dykstra-P and MTV-P are the parallel version of Dykstra and MTV. Different precision levels are used for comparison. The size of image is N  X  N . Left column:  X  =0 . 35 with N varying from 50 to 2000; right column: N = 500 with  X  varying from 0.15 to 0.95.
 MTV and Dykstra are about 3.5 times more efficient than their non-parallel version in a quad-core PC. We also ob-serve that the Split Bregman method, dual method, and ADAL need more iterations to achieve a similar precision to that of MTV when the regularization parameter  X  increases, i.e., the portion of the nonsmooth part increases. However, MTV and Dykstra are more stable when  X  varies. The rea-son is that we directly compute the exact optimal solution of the proximal operator of the fused regularization in the subproblems of MTV and Drystra, unlike ADAL and the Split Bregman method which perform soft-thresholding.
The synthetic 3D images are generated in a similar man-ner to the 2D case. Gaussian noise = N (0 , 0 . 2 2 ) is added to each pixel. We set the size of 3D images to N  X  N  X  50, and vary N from 50 to 500 with a step size of 25. The regu-larization parameter  X  is set to 0 . 35. We apply the Dykstra X  X  method and MTV on the noisy 3D images. In this exper-iment, we compare the computational time of Dykstra and MTV in a similar setting to the 2D case. Figure 5 shows the comparison between the Dykstra X  X  method and MTV. From Figure 5, we can see that MTV is much more efficient than Dykstra, demonstrating the efficiency of MTV. MTV is about 20 times faster than Dykstra when N = 500 and
We conduct experiments to evaluate the scalability of the proposed method. The experiments are performed on a Lin-Figure 5: Comparison of Dykstra and the proposed MTV in terms of computational time (in seconds and in the logarithmic scale) in the 3D case. Dif-ferent precision levels are used for comparison. The size of 3D images is N  X  N  X  50 ,and N varies from 50 to 500 with  X  =0 . 35 . Figure 6: Scalability of the proposed method. The size of image is N  X  N ,and  X  =0 . 35 .Left:thecom-putational time of MTV and MTV-P with 12 pro-cessors and N varying from 2500 to 11000 ;right:the speedup of MTV-P with respect to the number of processors varying from 1 to 16. ux server with 4 quad-core Intel Xeon 2.93GHz CPUs and 65GB memory. We vary the size of images ( N  X  N )from 2500  X  2500 to 11000  X  11000 with 12 processors, and the number of processors from 1 to 16 with a fixed image size. The regularization parameter  X  is set to be 0.35. For each setting, the average computational time of 10 trials is report-ed to demonstrate the efficiency/speedup of MTV-P (Fig-ure 6). As shown in Figure 6, the computational time of MTV-P is less than 100 seconds when N = 11000, demon-strating the superiority of the proposed method. We also observe that the speedup increases almost linearly with the number of processors used. The speedup is less than the number of processors used because of the parallel overhead.
Due to the excellent depiction of soft tissue changes, Mag-netic Resonance Imaging (MRI) has been widely used in medical diagnosis. Based on the compressive sensing theo-ry, it is possible to reconstruct perfect signals from a limited number of samples by taking advantage of the sparse nature of the signals in a transform domain. In the case of MRI, an accurate reconstruction of MR images from undersampled K-space data is possible, reducing the cost of scanning. The Figure 7: MRI reconstruction. Columns: orignal (left), FCSA-Dual and FCSA-MTV(middle), and the difference image between original image and re-constructed image (right); (a) Cardiac: SNR of two methods are 17.615; (b) Brain: SNR are 20.376; (c) Chest: SNR are 16.082; (d) Artery: the SNR are 23.769; formulation of image reconstruction is given by  X  X =argmin where b is the undersampled measurements of K -space data, R is partial Fourier transformation and W is wavelet trans-form. We try to reconstruct the image X  X  m  X  n from the undersampled measurements b . A fast algorithm, FCSA, is introduced by Huang et al. [14]. One of the key steps in FCSA is the proximal operator of the 2D TV norm, which is a special case of MTV. In [14], the dual method proposed in [2] is used to solve the proximal operator. We follow the same framework as FCSA, but apply the proposed MTV to solve the proximal operator to achieve a speedup gain.
We compare two approaches: FCSA with the dual method (FSCA-Dual)[14] and FCSA with MTV (FSCA-MTV). We apply these two methods on four 2D MR images 3 : cardiac, brain, chest, and artery. We follow the same sampling strat-egy as in [14]. The sample ratio is set to about 25%. A Gaus-ranger.uta.edu/~huang/R_CSMRI.htm Table 1: Comparison of the dual method and MTV in FCSA in terms of average computational time of 50 iterations (seconds).
 Methods Cardiac Brain Chest Artery Dual 0.6762 0.5855 0.5813 0.7588 MTV 0.0066 0.0061 0.0056 0.0078
Speedup 102.45 95.98 103.80 97.28 sian noise = N (0 , 0 . 01 2 ) is added to the observed measure-ments b . For a fair comparison, we first run FCSA-MTV and keep track of the objective function values of MTV in each iteration, then run FCSA-Dual. In each outer iteration, the dual method stops when its objective function value is equal to or smaller than the corresponding tracked objective func-tion value of MTV. Both FCSA-Dual and FCSA-MTV run 50 iterations. Only the computational time of the proximal operator by dual method and MTV, is recorded. The pre-cision parameters of MTV are set to abs = rel =1 e  X  3, and the dual update step length  X  is set to 10. Since the objective function of both methods are identical, and the precision of each iteration are about the same, the solutions of both methods are expected to be the same.
 The reconstruction results of the MR images are shown in Figure 7. Table 1 shows the average time of dual method and MTV for 50 iterations. Since each iteration of FCSA-MTV and FCSA-Dual are the same, FCSA-MTV and FCSA-Dual have the same SNR. But we can observe from Table 1 that MTV is more efficient than dual method(about 100 times speedup), thus FCSA-MTV is more efficient than FCSA-Dual.
The proposed method can be used to deblur images. The formulation of TV-based image deblurring model is given by where Y  X  m  X  n is the observed blurred and noisy image, B : m  X  n  X  m  X  n is a linear transformation encoding the blurring operator, and X  X  m  X  n is the image to be restored. A popular approach to solve the convex optimiza-tion problem in (32) is FISTA [2, 3]. One of the key steps is the proximal operator of TV regularization. Similar to the previous experiment, we use MTV instead of the dual method [2] to solve the proximal operator of TV regular-ization to achieve a speedup gain. The  X  X ena X  image of size 512  X  512 is used in this experiment. The image is rescaled to [0,1], and then blurred by an average filter of size 9 Furthermore, a Gaussian noise, N (0 , 0 . 001 2 ), is added to the blurred image. The parameter setting of MTV is the same as the previous experiment. The regularization parameter  X  is set to 0.001. The results are shown in Figure 8. The aver-age computation time of the dual method for 100 iterations is 1.066 seconds, while that of MTV is 0.037 seconds. The proposed MTV method achieves about 29 times speedup.
A video is a 3-mode tensor. The proposed method in the 3D case can be used to denoise video. We expect that pixel values should be smooth along all 3 modes. In this experi-ment, we use a time series of 2D MR images of heart beats Figure 8: Image deblurring: original image(left), blurred and noisy image (middle), and deblurred im-age (right). The SNR of the blurred image is 11.01, and the SNR of the deblurred image is 17.23.
 Figure 9: Sample frames of video denoising: original frames (top), and denoised frames (bottom) (best viewed on a screen). downloaded from the website of the Cardiac Atlas 4 .The 2D MR images are in the format of avi, which includes 32 frames. We applied the proposed method and the Dystra X  X  method to denoise all the MR images as a 3-mode tensor of size 257  X  209  X  32. The computational time of MTV is 4.482 seconds, and the computational time of the Dykstra X  X  method is 43.751 seconds. The speedup is about 10 times. Some sample result frames are shown in Figure 9. This ex-periment demonstrates the effectiveness of total variation regularization in video denoising.
In this paper, we propose an efficient optimization of the multidimensional total variation regularization problems. We employ an efficient ADMM algorithm to solve the formula-tion. The key idea of our algorithm is to decompose the orig-inal problem into a set of independent and small problems, which can be solved exactly and efficiently. Furthermore, the set of independent problems can be solved in parallel. Thus, the proposed method can handle large-scale problems efficiently. We also establish the global convergence of the proposed algorithm. The experimental results demonstrate the efficiency of the proposed algorithm. The proposed al-gorithm opens the possibility of utilizing the power of GPU computing to further improve the efficiency of the proposed algorithm. We will explore the GPU computing in the future work. Moreover, we plan to apply the proposed algorithm to other real-world applications, such as MBB (mobile broad band) data and 3G network data, both are big data prob-lems. atlas.scmr.org/download.html
