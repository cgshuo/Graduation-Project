 Farshid Keynia n 1. Introduction
With the introduction of restructuring into the electric power industry, the price of electricity has become the focus of all activities in the power market ( Shahidehpour et al., 2002 ). An accurate day ahead price forecasting in the spot market helps the power suppliers to adjust their bidding strategies to achieve the maximum benefit and on the other hand, consumers can derive a plan to maximize their utilities using the electricity purchased from the pool, or use self production capability to protect themselves against price spikes ( De Sanctis and Mari, 2007 ). However, forecasting is hampered by the non-linear and stochas-tic nature of electricity price time series ( Alvarez-Ramirez et al., 2009 ). The predictability of the price time series has been discussed in ( Bigdeli and Afshar, 2009 ). The electrical energy cannot be considerably stored and the power system stability requires constant balance between generation and load. On short time scales, most users of electricity are unaware of its price. Transmission bottlenecks usually limit electricity transportation from one region to another. These facts enforce the extreme price volatility or even price spikes of the electricity market, e.g., the price spikes of the PJM (Pennsylva nia X  X ew Jersey X  X aryland) elec-tricity market in 1999 ( Amjady, 2006 ; Amjady and Hemmati, 2006 ).
As another example, Californian market experienced huge price spikes during the summer of 2000 that led to the closure of the market until new rules were developed.

Importance of electricity price forecasting and its complexity motivates many research works in this area, especially in the recent years. Stationary time series models such as Auto-Regres-sive (AR) ( Fosso et al., 1999 ), dynamic regression and transfer function ( Nogales et al., 2002 ; Zareipour et al., 2006 ),
Auto-Regressive Integrated Moving Average (ARIMA) ( Contreras et al., 2003 ) and nonstationary time series models like General-ized Auto-Regressive Conditional Heteroskedastic (GARCH) ( Garcia et al., 2005 ) have been proposed for this purpose. How-ever most of time series models are linear predictors, while electricity price is generally a nonlinear function of its input features. So, the behavior of the price signal may not be com-pletely captured by these time series techniques ( Amjady and
Hemmati, 2006 ). To solve this problem, some other research works proposed neural networks (NN) and Fuzzy Neural Net-works (FNN) for electricity price forecast ( Amjady, 2006 ; Zhang et al., 2003 ; Guo and Luh, 2004 ; Gonzalez et al., 2005 ; Zhang and
Luh, 2005 ; Rodriguez and Anders, 2004 ; Hong and Lee, 2005 ). NNs and FNNs have the capability of modeling the nonlinear input/ output mapping functions. However, electricity price is a complex mapping function of many input variables, whereas the appro-priate selection of input features is a key factor for the success of
FNN or NN based price forecast methods. Besides, efficiency of these methods is usually dependent on the correct tuning of their adjustable parameters, e.g., number of hidden nodes of the NNs.
Some other price forecast methods have been also proposed in the recent years. In ( Li et al., 2007 ), combination of fuzzy inference system (FIS) and least-squares estimation (LSE) has been pro-posed for prediction of locational marginal price (LMP). When transmission constraints exist in power system, LMP instead of market clearing price should be considered. The marginal cost of each bus is the LMP ( Shahidehpour et al., 2002 ). In ( Conejo et al., 2005 ), electricity price series is decomposed by wavelet transform with the aim of finding less volatile components and then each sub series is separately forecasted by the ARIMA technique.
Combination of Similar Day (SD) and NN techniques is proposed for price prediction in ( Mandal et al., 2007 ). In ( Garcia-Martos et al., 2007 ), mixed model have been proposed for the price forecast, where weekends and weekdays are modeled by separate time series techniques. The use of dynamic harmonic regression for forecasting the hourly price time series of electricity markets has been proposed in ( Pedregal and Trapero, 2007 ). In ( Conejo et al., 2005 ), a comparative overview of different price forecast techniques including ARIMA, dynamic regression, transfer func-tion, NN and wavelet transform is presented where transfer function is identified as a promising technique. In ( Nogales and
Conejo, 2006 ), a detailed analysis of transfer function technique is carried out and different transfer function models are presented.
In ( Ciwei Gao et al., 2008 ; Ciwei Gao et al., 2007 ), support vector machine (SVM) is proposed for electricity price forecasting.
In spite of all performed research in the area of electricity price forecasting, more accurate and robust price forecast methods are still required. In this paper, a new prediction strategy is proposed for day ahead price forecasting of electricity markets. Contribu-tion of the paper can be summarized as follows: 1) Presentation of a new composite neural network (CNN) com-posed of three multi-layer perceptron (MLP) neural networks.
Besides, a new data flow is constructed among the building blocks of the CNN to enhance its forecast accuracy. 2) Consideration of both measured and predicted inputs for price forecast. The predicted inputs are provided by a few auxiliary predictors. 3) Presentation of a new two stage feature selection algorithm that filters out irrelevant and redundant candidate inputs, respectively. The algorithm is based on the Mutual Informa-tion (MI) criterion to evaluate common information content between two random variables. 4) The two stage feature selection technique includes two adjus-table parameters and CNN has one degree of freedom. These three adjustable parameters are fine-tuned by a kind of cross-validation technique.

The remaining parts of the paper are organized as follows. In the second section, the proposed two stage feature selection algorithm is described. In the third section, the proposed CNN is introduced. Obtained numerical results are presented and dis-cussed in section four. Section five concludes the paper. 2. The proposed two stage feature selection algorithm
The set of candidate inputs for electricity price forecast is usually a large set including lagged values of price, load, available generation, etc. ( Amjady and Hemmati, 2006 ; Zhang et al., 2003 ;
Guo and Luh, 2004 ). For instance, if we only focus on the price and load (as the most important price driver ( Shahidehpour et al., 2002 )), considering the short-run trend, daily and weekly peri-odicity characteristics of the electricity price signal, we reach a candidate set of inputs including at least lagged values of price up to 200 h ago ( P t , P t 1 , y , P t 199 ) and lagged values of load up to 200 hours ago ( L t , L t 1 , y , L t 199 ), totally 400 candidates ( Amjady and Keynia, 2009 ). This set of candidate inputs is too large to be directly applied to any forecaster. Besides, it includes irrelevant and redundant candidate inputs, which can be misleading for the forecast engine. So, it is necessary to refine the large set of candidates such that a subset of the most effective inputs are selected to be applied to the forecast engine. We presented an efficient feature selection technique in our previous work ( Amjady and Keynia, 2009 ), which is based on the mutual information (MI) criterion. The technique evaluates mutual information of each candidate input with the target feature and rank the candidates according to their information value for the forecast process. In this technique, an approximation of MI criterion based on the binomial distribution is used, which can be computed by a reasonable amount of historical data and low computation burden.

The MI based feature selection method of ( Amjady and Keynia, 2009 ) can effectively evaluate relevance of target feature (here, price of the next hour) to each candidate input and select relevant inputs. However, it does not consider redundancy of candidate features. In feature selection, it has been recognized that the combinations of individually good features do not necessarily lead to good classification performance. In other words,  X  X  X he m best features are not the best m features X  X  ( Peng et al., 2005 ).
Indeed, the relevant features of a forecast process may have redundant information. A redundant set of candidate inputs not only increases the computation burden of the training phase, but also complicates construction of input/output mapping function for the forecaster leading to degradation of its accuracy.
Especially, considering limited historical data for price forecast, redundant features should be filtered out. Some researchers have studied different means to reduce the redundancy among features ( Peng et al., 2005 ). In this paper, we propose an effective method to remove redundant input variables. Suppose that S 1  X  { x x } is a set of candidate inputs including redundant features.
Mutual information (MI) between each pair of features of S
MI  X  x , x j  X  , 1 o i , j o n  X  1  X  is computed. Higher value of MI ( x i ,x j ) means more common information between the candidate inputs x i and x j and so these candidates have a higher level of redundancy. So, for each feature x
S 1 , its maximum redundancy in the set S 1 is computed:
Max
We can rank the candidate inputs of S 1 according to the redundancy measure of (2) such that a higher value of the measure results in a more redundant feature or equivalently a less informative feature. By combining the feature selection technique of ( Amjady and Keynia, 2009 ) and the proposed redundancy filter, we reach the following two stage feature selection algorithm: 1) Consider the original set of candidate inputs named S I each feature x i A S I , compute its mutual information with the the price of the next hour. MI ( x i ,y ) measures mutual informa-tion between the candidate input x i and target feature y .
Higher value of MI ( x i ,y ) means the candidate input x relevant feature for prediction of the target variable y .To compute values of MI ( .,. ) function, the approximation of ( Amjady and Keynia, 2009 ) based on the binomial distribution is used. The candidate inputs of S I are sorted based on their mutual information with the target variable such that a higher
MI ( x i ,y ) value results in the higher rank. Then, the candidate inputs with MI ( x i ,y ) value greater than a relevancy threshold
TH 1 are retained as the relevant features and the other candidate inputs are filtered out. This is the first stage of the proposed two-stage feature selection algorithm and can be considered as an irrelevancy filter. 2) Suppose that S 1 C S I is a subset of candidate inputs selected by the irrelevancy filter by filtering out the irrelevant features. For each feature x k A S 1 , compute the redundancy measure of (2). If for a feature, the measure value becomes greater than a redundancy threshold TH 2, it is considered as a redundant candidate input and so between this candidate and its partner, one feature should be filtered out. For instance if for x
Max then between x k and its partner x m , one variable should be eliminated. For this purpose, the relevancy factors of these with less relevancy factor (less relevant feature or less effective feature for the forecast process) is filtered out. It is possible that more than two features be redundant such that only one of them is eligible to be retained. So, the redundancy filtering process is repeated for all features of S 1 , until no redundancy measure of (2) becomes greater than TH 2. The redundancy filter is the second stage of the proposed two-stage feature selection algorithm. The subset of features S 2 C S 1 that pass the redundancy filter are finally selected candidate inputs by the proposed two stage feature selection algorithm. The candidate features of S 2 considered as the inputs of the forecast engine.

The thresholds TH 1 and TH 2 are the degrees of freedom of the irrelevancy and redundancy filtering stages of the two stage feature selection technique, respectively. With higher values of TH 1, the pass band of the irrelevancy filter becomes narrower such that less features with higher relevancy factors MI ( x,y ) can pass it. In other words, selection of TH 1 is a trade-off between number of selected features in S 1 and their effectiveness for the forecast process. The redundancy filter has an inverse situation. By decreasing TH 2, the pass band of this filter becomes narrower such that more independent features with less redundant infor-mation can only pass it. So, selection of TH 2 is a trade-off between number of selected features in S 2 and their redundancy. In the next section, it will be shown that the proposed forecast engine also has a degree of freedom, which is number of hidden nodes of its neural networks ( N H ). All degrees of freedom of the proposed price forecast strategy including TH 1 and TH 2 of the two stage feature selection algorithm and N H of the forecast engine are fine-tuned by a kind of cross-validation technique, which will be introduced in the next section.

Finally, training period of the proposed price forecast strategy (feature selection  X  forecast engine) should be selected. A long training period includes more historical data and so the binomial probability distributions of the MI ( .,. ) function can be constructed more accurately ( Amjady and Keynia, 2009 ). On the other hand, electricity price is a time variant mapping function of its input features ( Amjady, 2006 ; Amjady and Hemmati, 2006 ; Gonzalez et al., 2005 ). So, far historical data introduced by a long training period may include irrelevant data for the feature selection of the price forecast. The irrelevant data can also be misleading for the forecast engine to construct the input/output mapping function of the forecast process. In ( Garcia-Martos et al., 2007 ), the effect of different training periods on the price forecast accuracy has been studied. In this paper, previous 50 days is used as training period, also recommended in ( Amjady, 2006 ; Conejo et al., 2005 ), which results in 50 24  X  1200 training samples. 3. The proposed composite neural network
As previously described, electricity price is a nonlinear, time variant and multi-variable function. It is very hard for a single NN to capture correct input/output mapping function of such a signal in all time periods. However, our experience shows that different learning algorithms can usually cover deficiencies of each other provided that a correct data flow is constructed among them.
Some researchers proposed parallel and cascaded structures for this purpose ( Zhang et al., 2003 ; Guo and Luh, 2004 ; Gonzalez et al., 2005 ; Amjady and Keynia, 2009 ). However, these structures share input data among their building blocks. So, obtained knowledge of a block is not really transferred to the other blocks.
To solve this problem, a new composite neural network (CNN) with the architecture shown in Fig. 1 is proposed in this paper.
The CNN is composed of a few NNs, each one owning a suitable learning algorithm for forecasting tasks. All NNs of the CNN have the same multi-layer perceptron (MLP) structure. MLP can approximate any continuous multivariate function to a desired degree of accuracy, provided that there are a sufficient number of hidden neurons ( Zhang et al., 2003 ; Guo and Luh, 2004 ). Besides, according to Kolmogorov X  X  theorem, the MLP can solve a problem using one hidden layer, provided that it has a proper number of neurons ( Tsekouras et al., 2006 ). So, one hidden layer has been considered in the MLP structure of all NNs of the CNN. Besides, all NNs have the same number of input, hidden and output neurons.
Number of selected candidate inputs by the proposed two-stage feature selection technique indicates number of input nodes in the NNs of the CNN. Besides, all NNs have one output node allocated to the forecast of next hour price. The number of hidden neurons of the NNs of the CNN ( N H ) is a degree of freedom of the forecast engine, which is referred to in the previous section.
The same N H is considered for all NNs of the CNN to decrease the number of its adjustable parameters.
 As seen, each NN transfers two kinds of results to the next NN.
The first set of results includes obtained values for the adjustable parameters, i.e., weights and bias values. In other words, each NN transfers its obtained knowledge to the next one and so it can begin its learning process from the point that the previous NN terminated (instead of beginning from a random point). Only the first NN should begin with an initial set of random values for the adjustable parameters. Since all NNs have the same MLP struc-ture, these weights and bias values can be directly used by the next NN and then it can increase the obtained knowledge of the previous one. By suitable selection of training algorithms of the NNs, the CNN can learn much more than a single NN, i.e., a better tuning for the adjustable parameters can be obtained. For the price prediction, we considered three NNs in the CNN, owning LM (Levenberg-Marquardt), BFGS (Broyden, Fletcher, Goldfarb,
Shannon), and BR (Bayesian Regularization) learning algorithms, respectively. These are some of the most efficient NN training mechanisms for the prediction tasks and their mathematical details can be found in ( Amjady, 2002 ; Fausett, 1994 ; Rodriguez and Anders, 2004 ). LM is a fast learning algorithm. At the beginning of training phase, usually MLP can rapidly learn about the problem and its training error decreases fast. So the LM algorithm is selected for the first NN. The BFGS is the most successful quasi-Newton method for the NN training ( Amjady, 2002 ). Especially, this learning algorithm can perform a better search of the solution space provided that it starts from a suitable initial point. Solution space for a NN can be considered as a vector space, where its dimensions are the adjustable parameters of the
NN. So, after the LM algorithm is saturated, the second NN is trained by the BFGS training mechanism to find a fitter solution for the adjustable parameters of the NN. Behavior of the BR learning algorithm is relatively similar to the BFGS. This algorithm is considered for the last NN for final tuning of the adjustable parameters and getting the utmost training efficiency. In Fig. 1 , structure and learning algorithm of each NN of the CNN are indicated in the parentheses as  X  X  X LP  X  LM X  X ,  X  X  X LP  X  BFGS X  X  and  X  X  X LP  X  BR X  X , respectively.

We examined many other configurations for the CNN includ-ing the mentioned learning algorithms or other training mechan-isms. However, performance of the other configurations is generally less than the proposed one. Besides, we increased number of NNs of the CNN. Even we added the fuzzy neural network (FNN) ( Amjady, 2006 ; Amjady, 2006 ) and SVM ( Fan and Chen, 2006 ) at the beginning or at the end of the CNN (the added
FNN or SVM only participates in transferring of the second kind of results, i.e., the prediction for the next hour shown in Fig. 1 , since the adjustable parameters of the FNN and SVM cannot be used for the NNs and vice versa). However, no considerable improvement is obtained and in some cases the performance of the CNN degrades. It seems that the learning capability of the CNN approximately saturates with the three NNs.

Classical choice for the error function of the NN is its training error. However, price is a time variant signal and its functional relationships rapidly vary with time. While it seems that the NN learns well the training data, it may encounter large prediction errorsinthepredictionphase.Thus,tohaveacorrectmeasureofthe price prediction capability of the forecast method, its error for the unseen part of the signal should be evaluated. For this purpose, a kind of cross-validation technique has been developed in the CNN, in which data of the closest day to the forecast day, i.e. its previous day, is removed from the training set. So, the training set of each NN of the CNN really includes 50 1  X  49 days (49 24  X  1176 training samples)andthedaybeforetheforecastdayor24unseensamples are retained as the validation set. Prediction error of each NN for this day (the validation set) can be a measure of its error for the forecast day, evaluating the generalization capability of the NN, and so it is a better choice for the error function.

In general, an important aspect of NN training with an iterative learning algorithm is its termination mechanism. Premature termination can result in incomplete learning of the NN. On the other hand, a large number of training iterations may lead to the over fitting problem, especially for the time variant price signal. Here, an efficient termination mechanism is proposed for the NNs of the CNN. Usually, the test error of an NN (for the unseen samples) will normally decrease during the initial itera-tions of the training phase, as does the training set error.
However, when the network begins to overfit the training samples, the error of the test set will typically begin to rise ( Amjady, 2002 ). So, in each NN of the CNN, when the error function increases for a specified number of iterations, the training is stopped, and the weights and biases at the minimum of the error function are returned. Thus, the training phase of each
NN is automatically terminated when the maximum general-ization capability is obtained.

In most of the previous price prediction works, only the measured values (related to the past performance) have been used as the input features ( Amjady, 2006 ; Nogales et al., 2002 ;
Garcia et al., 2005 ; Rodriguez and Anders, 2004 ; Conejo et al., 2005 ). However, we found that the predicted values, obtained from the other forecast techniques, can be useful information for price forecast. As seen from Fig. 1 , the second kind of results transferred from a NN of the CNN to the next one is its price ARIMA Batch NN NN1 (MLP+LM) NN2 (MLP+BFGS) NN3 (MLP+BR) forecast. Each NN uses the price prediction of the next hour obtained by the previous NN, i.e. ^ P  X  t  X  1  X  , as an input feature and provides a better prediction for it (the superscript ^ indicates the predicted value). For the first NN of the CNN, initial forecast for the price of the next hour is provided by the ARIMA time series, a well-known method for the price forecast ( Amjady and Hemmati, 2006 ; Nogales et al., 2002 ; Contreras et al., 2003 ; Conejo et al., 2005 ). Besides, load forecast of the next hour, i.e. ^ L provided for the CNN by a batch NN, owning MLP structure and LM learning algorithm. We considered a set of lagged prices, date set of input variables and performed the two stage feature selection analysis on this set. Retained candidate inputs after using this feature selection algorithm (the subset of features S ) are selected as the input features of the CNN. Surprisingly, we saw methods, the two stage feature selection analysis always selected these variables in our examined cases. Numerical results in this regard will be presented in the next section. Similarly, the obtained forecast of each NN of the CNN is used by the next one, since price forecast of each block is usually better than the previous one and so can be a better choice for the input feature of the next block. It is noted that in ( Zhang et al., 2003 ) load forecast has been considered as an input feature in a cascaded structure of NNs. However, we extended this idea for feature selection by consideration of both load and price forecasts. Besides, in ( Zhang et al., 2003 ) measured value of L ( t  X  1 ) has been used for the training phase. However, we consider ^ L  X  t  X  1  X  for the training of the CNN (all three NNs). In this way, the CNN can see the load forecast error in the learning phase and so its NNs can be better adapted to the prediction phase when L ( t  X  1 ) is not available. A description about this matter can be found in ( Amjady, 2006 ). Finally, our important idea in transferring the obtained knowledge of a NN (its adjustable parameters) to the next one is not seen in ( Zhang et al., 2003 ).

The proposed price forecast strategy has three adjustable parameters including TH 1 and TH 2 of the two stage feature selection algorithm (described in the previous section) and N of the forecast engine. Usually the adjustable parameters of load or price forecast methods are selected based on the experience or heuristics. Here, the adjustable parameters of the proposed price forecast strategy are fine-tuned by the cross-validation technique. For this purpose, the forecast strategy with different values for the adjustable parameters is executed and its validation error (error of the validation set or the day before the forecast day) is monitored. The set of values of the adjustable parameters leading to the minimum validation error are selected. Sample results of the cross-validation technique are shown in Figs. 2 and 3 . Mean Absolute Percentage Error (MAPE) for the 24 validation samples is calculated and considered as the validation error, which is the vertical axis of Figs. 2 and 3 .In Fig. 2 , the adjustable parameters of the two stage feature selection technique are fixed ( TH 1  X  0.19 and TH 2  X  0.44) and the degree of freedom of the forecast engine, i.e. N H , is varied. In Fig. 3 , N H is kept constant ( N curve of the validation error with respect to variation in TH 1 and TH 2 is plotted. Figs. 2 and 3 are obtained for the PJM electricity market with 49 days training period from September 28, 2006 to November 15, 2006 and the day of November 16, 2006 as the validation set. The minimum validation error in Figs. 2 and 3 is equal to 4.98%.

It is noted that the feature selection analysis is a part of the data preparation block, shown in Fig. 1 , also performing other adaptation tasks for input data including normalization and shuffling. In order to avoid saturation phenomena and eliminate effect of different ranges of variables (price and load), the input and output variables are normalized ( Tsekouras et al., 2006 ).
Various normalization procedures with different output ranges have been examined for this purpose ( Amjady, 2002 ). Based on the prediction results, the best procedure was linear normal-ization with the output range of [0,1]: x  X  X  x r min  X  =  X  r max r min  X  X  4  X  where x n is the normalized form of variable x and r max and r its lower and upper values in the training set, respectively. Price and load variables are separately normalized, due to their differ-ent ranges. After normalization, training samples are separately shuffled 1000 times for each NN of the CNN. This was done to enhance the randomness of the data and eliminate the sequential bias effect. We also examined different softening procedures, e.g. square root, natural logarithm, and numerical integration in the data preparation part, but no considerable improvement was observed. This is due to the fact that price is itself a nonlinear signal and using a nonlinear transformation for it further com-plicates its input/output mapping function, such that deriving this mapping function for an NN becomes so hard.

It is noted that the output of ARIMA and batch NN are also given to the data preparation part, like other input features, to perform the normalization and feature selection analysis.
However, these connections were not shown in Fig. 1 to simplify the figure. According to Fig. 1 , the data preparation part prepares input data of the ARIMA and CNN.

As previously described, the variables of short-run trend of features (subset S 2 ). In the day ahead price forecast, measured value of these features are not known for farther hours (e.g., for price forecast of 3 h ahead, 4 h ahead, etc.) and so their predicted values should be used. Thus, forecast of an hour is used to predict the next hour and this cycle repeats, resulting in the propagation of error in the forecast horizon. As a consequence, farther hours usually contain more forecast error, referred to as the lead time effect ( Amjady, 2001 ). If we can use more accurate forecasts for initial hours, error propagation decreases and better predictions for all later hours can be obtained. Thus, each NN of the CNN (and also ARIMA), instead of forecasting the whole 24 h ahead, only predicts the next hour and transfers it to the following block until the final prediction of the CNN is obtained by the NN3 and inserted to the database (shown in Fig. 1 ). This final forecast, owning the least prediction error in our system, is used in the ARIMA and NNs of the CNN to predict its next hour.
This cycle is repeated until the price of the whole 24 h ahead is forecasted.

Finally, it is noted that the execution of data preparation part and training of ARIMA, batch NN and CNN, even with the consideration of the fine-tuning of parameters, requ ire low computation burden and so the whole method can be separately executed for each day. As previously described, electricity price is a time variant signal and its functional relationships (its dependencies on the inputs) vary with time. Separate execution of the feature selection analysis permits us to adaptively select the most useful input variables for each day.
Besides, separate training of the f orecasters, like CNN, for each day allows them to be trained with the most recent data.

The training phase of the CNN and its data preparation can be summarized as follows: 1) Select the training period (e.g., 50 days ago, recommended in ( Amjady, 2006 ; Conejo et al., 2005 )) and remove the day before the forecast day from this period for cross-validation.
This day should be unseen for both the data preparation part and forecasters (the batch NN, ARIMA, and CNN) to really simulate the situation of the forecast day. 2) Constitute the set of candidate inputs including lagged prices and loads (e.g., up to 200 h ago) plus ^ P  X  t  X  1  X  from ARIMA and ^
L 3) Data preparation part performs the normalization and two stage feature selection analysis on the candidate inputs to select the most useful input features. 4) Data preparation part constructs training samples including the selected input features and one output feature dedicated to the price of the next hour. 5) Training samples are shuffled and three NNs of the CNN are trained by LM, BFGS, and BR training mechanisms, respec-tively. At the end of the training phase of each NN, its adjustable parameters and price forecast are transferred to the next one. 6) The number of hidden nodes of the NNs of the CNN, i.e. N thresholds of the two stage feature selection technique, including TH 1 and TH 2, are adjusted by the cross-validation technique so that the combination resulting in the least value of the error function (prediction error for the day before the forecast day) is obtained.

After training, the CNN can predict hourly price values of the forecast day by the same price and load features selected by the two stage feature selection technique. Predicted prices are nor-malized values and so they should be returned to the actual range, i.e. x n -x , by the inverse of (4).

Note: The new contributions of this paper with respect to our previous work in ( Amjady and Keynia, 2009 ) can be summarized as follows: 1) The forecast engine of ( Amjady and Keynia, 2009 )isacascaded neuro-evolutionary algorithm, which shares input data among its building blocks. However, a new forecast engine, i.e. compo-site neural network, is presented in this paper in which each NN transfers its obtained knowledge (stored in its weights and bias values) to the next NN and so the next block can begin its training phase from the point that the previous NN terminated. 2) In ( Amjady and Keynia, 2009 ), only lagged features (e.g., lagged values of load and price) are considered in the candidate set of inputs. However, in this paper, the candidate inputs consist of both lagged features and forecast features provided by two auxiliary predictors. Besides, another data flow is also con-structed among the building blocks of the CNN, in which price forecast of a NN is transferred to the next one. In this way, the price forecast accuracy of the CNN becomes better step by step. 3) The feature selection technique of ( Amjady and Keynia, 2009 )only has the irrelevancy filter, while the two stage feature selection algorithm of this paper includes both the irrelevancy and redun-dancy filters. By removing redundant candidates, a more efficient set of inputs for the forecast engine can be obtained. 4. Numerical results
The proposed method is examined on the day ahead electricity markets of mainland Spain and PJM, commonly used as test case in many price forecasting papers ( Amjady, 2006 ; Nogales et al., 2002 ; Contreras et al., 2003 ; Garcia et al., 2005 ; Gonzalez et al., 2005 ; Conejo et al., 2005 ; Hong and Hsiao, 2002 ). The Spanish electricity market is a real world case study with considerable complexity. More details about this market can be found in ( Nogales et al., 2002 ; Gonzalez et al., 2005 ; Conejo et al., 2005 ;
OMEL ). In the USA, the PJM competitive market plays an impor-tant role in the U.S. electric system. Forecasting the LMPs of the
PJM market is becoming more and more important for the market participants who bid into the spot price market. Data of the
Spanish and PJM electricity markets have been obtained from ( OMEL ); ( Web Site ), respectively.

Sample results for the cross-validation have been represented in the previous section and similar results have been obtained for the other time periods. So, to avoid repetition, only results of the feature selection and forecast engine are presented in this section.
Sample results of the two stage feature selection algorithm for the PJM electricity market are shown in Fig. 4 and Tables 1 and 2 .
The same training period and validation set of Figs. 2 and 3 are also considered in this experiment. The vertical axis of Fig. 4 indicates MI coefficient, i.e. mutual information with the target feature or next hour price, denoted by MI(x i ,y ) in the Section 2 of the paper. Here, the candidate input variables of the CNN include price forecast for the next hour ^ P  X  t  X  1  X  (ARIMA), measured prices ^
L  X  t  X  1  X  (batch NN), and measured loads for the past 200 h ( L
L ( t 199 ) ), totally 402 candidates, shown respectively on the hor-izontal axis of Fig. 4 . These 402 candidate inputs constitute the original set of candidates S I . Fig. 4 shows relevance of the target feature to each candidate input measured by the MI analysis.
From this figure, high MI coefficients of the forecast features ( ^
P  X  t  X  1  X  and ^ L  X  t  X  1  X  or the candidate inputs 1 and 202 on the horizontal axis of Fig. 4 ) as well as the effect of short-run trend (high MI coefficient of a few previous hours prices like P
P ) and different periodicities (daily and weekly) can be seen. The threshold TH 1  X  0.19 is shown by a dashed line on the figure. This threshold has been obtained by the cross-validation techni-que shown in Fig. 3 of the previous section. The results of Fig. 4 are shown with more details in Table 1 wherein the selected candidate inputs plus their rank and MI coefficient are repre-sented. The results of Table 1 are obtained from the irrelevancy filter or the first stage of the proposed two stage feature selection algorithm. In other words, the selected candidate inputs in Table 1 have MI coefficient or MI(x i ,y ) greater than TH 1  X  0.19.
These selected candidates have more than 0.19 mutual informa-tion with the target feature. As seen from this Table, ^ P ^
L  X  t  X  1  X  get the second and sixth scores among the 402 candidate inputs, respectively, which shows the value of the proposed forecast features to enhance the information content of the candidate set of inputs. The 51 selected candidates in Table 1 comprise the set S 1 S I . The set of S 1 is considered for the redundancy filter or the second stage of the proposed two stage feature selection technique. For the training period and validation set of this experiment, the threshold of the redundancy filter is determined as TH 2  X  0.44 by the cross-validation technique as shown in Fig. 3 of the previous section. This filter removes the redundant features among the 51 candidate inputs of S 1 and selects 37 candidates, shown in Table 2 , constituting the set of S
In other words, the candidate inputs of S 1 that their redundancy measure (represented in (2)) is greater than TH 2  X  0.44 are indicated as the redundant candidates in this experiment. Among the redundant candidates, less relevant ones are removed by the redundancy filter as described in Section 2 . For instance, both
P t 23 and P t 22 belong to set S 1 with 3rd and 8th scores in Table 1 , respectively. However, these are two close lagged price features with mutual information: MI  X  P t 22 , P t 23  X  X  0 : 4926 4 TH 2  X  0 : 44
Hence, P t 22 and P t 23 are considered as the redundant candidate inputs and so one of them should be filtered out. As seen from Table 1 , mutual information of P t 22 with the target feature is 0.3385, while the MI coefficient of P t 23 is equal to 0.4292. Therefore, between these two redundant candidate inputs, the less relevant one, i.e. P t 22 , is filtered out by the redundancy filter. As seen, only P t 23 is appeared in Table 2 .
The 37 candidate inputs of S 2 , shown in Table 2 , are the finally selected candidates by the proposed feature selection technique for the respective forecast day. These candidates are considered as the inputs of the CNN forecast engine. It is seen that ^ P ^
L  X  t  X  1  X  are also among the selected candidates of the redundancy filter in Table 2 . So, the proposed forecast features not only have high relevance with the target feature (next hour price) but also have low redundancy with the other candidate inputs and so pass the both irrelevancy and redundancy filters. This matter shows
Similar results are obtained from the proposed two stage feature selection algorithm for the other test periods. In the following, the efficiency of the suggested forecast strategy including the proposed feature selection algorithm and CNN forecast engine is evaluated.

The next experiment of this paper has been performed by means of real data of the PJM electricity market in year 2006.
Obtained results for four weeks corresponding to the four seasons of year 2006 are presented in Tables 3 and 4 (the last row indicates average value of the four weeks). In this manner representative results for the whole year are provided. Similar procedures have been adopted in the previous works ( Amjady, 2006 ; Nogales et al., 2002 ; Contreras et al., 2003 ; Conejo et al., 2005 ). Here, the considered weeks are February 15 X 21, May 15 X  X ay 21, August 15 X  X ugust 21 and November 15 X 21 (months 2, 5, 8, and 11), indicated in the first column of Tables 3 and 4 .In the remaining columns, obtained results from ARIMA, NN with
LM, NN with BFGS, NN with BR and CNN in two cases are shown, respectively. In the first case, CNN with only the irrelevancy filter (the first stage of the proposed two stage feature selection algorithm) is considered, which is indicated by  X  X  X I  X  CNN X  X  in
Tables 3, 4 and the next tables of the paper. In the second case, the proposed two stage feature selection algorithm is used for the
CNN. This case is denoted by  X  X  X I-MI  X  CNN X  X  in all remaining tables of the paper. The ARIMA, NN with LM, NN with BFGS and
NN with BR have been individually examined without transfer-ring the adjustable parameters and price forecast, while the CNN considers their combination according to the described data flow.
The ARIMA and NNs have the single stage feature selection (only the irrelevancy filter). So, excluding the price and load forecasts, the other input features of the MI  X  CNN and these methods are the same. Error values shown in Table 3 are in terms of Weekly Mean Error (WME): WME  X  1 168 where P iACT and P iFOR are the actual and forecasted price of hour i , respectively. As seen from Table 3 ,MI  X  CNN has much lower
WME values than all other methods, which are common methods for price forecast. Transferring the obtained knowledge between the constituting blocks of the CNN, increases its learning capability. Individual use of NNs encounters high prediction errors due to their limited learning capability. ARIMA has even poorer results due to its linear modeling property. Moreover, using the useful forecast features owning high mutual informa-tion with the target signal enhances the capability of the CNN for the price prediction. In this examination, WME of the batch NN to forecast hourly load was 2.5%, 2.85%, 3.18%, and 2.7%, for the considered four weeks, respectively. Using more accurate load forecasts, better predictions for price can be obtained, however focus of this paper is not on the load forecast. Furthermore, from Table 3 , it can be seen that average WME as well as WME of all test weeks for the MI X  X I  X  CNN are lower than the MI  X  CNN. This matter shows the superiority of the proposed two stage feature selection algorithm compared with the single stage feature selection.
In addition to the mean error, stability of results is another important factor for the comparison of forecast methods. Differ-ent statistical measures such as peak, variance, and standard deviation of error have been proposed as the index of uncertainty ( Amjady, 2006 ; Nogales et al., 2002 ; Contreras et al., 2003 ; Conejo et al., 2005 ; Li et al., 2007 ; Amjady, 2001 ). In Table 4 , weekly peak error (WPE) of the examined methods for the test weeks are shown:
WPE  X  Max
As seen, the MI  X  CNN has considerably lower WPE values than the ARIMA, NN with LM, NN with BFGS and NN with BR. Also, the
MI X  X I  X  CNN has lower WPE values than the MI  X  CNN. Obtained results from the MI X  X I  X  CNN for the fall X  X  test week (with the highest WME and WPE) are shown in Fig. 5 . It can be seen that even for the worst test case, the MI X  X I  X  CNN has acceptable accuracy. It is noted that transferring of the adjustable parameters between NNs of the CNN and transferring of the price forecast of each NN to the next one cause the high price forecast accuracy of the CNN compared with the other forecast engines (such as a single ARIMA or a single NN).

The third examination of this paper has been performed by means of real data of the Spanish electricity market in year 2002.
Similarly, obtained results for four weeks corresponding to the four seasons of year 2002 are presented in Tables 5 and 6 .In ( Amjady, 2006 ; Conejo et al., 2005 ; Amjady and Keynia, 2009 ), the fourth week of February, May, August, and November are selected for winter, spring, summer, and fall seasons, respectively. For the sake of a fair comparison between the technique of the paper and the proposed methods of ( Amjady, 2006 ; Conejo et al., 2005 ;
Amjady and Keynia, 2009 ), the same test weeks have been also considered in this examination. In Tables 5 and 6 , the results of
ARIMA and ARIMA plus wavelet transform have been quoted from ( Conejo et al., 2005 ). The wavelet transform was used to decompose the price signal into less volatile components. The fourth column represents the obtained results from the FNN (quoted from ( Amjady, 2006 )), in which instead of decomposing the price values, its solution space is softly divided into subspaces and each functional relationship of the price is implemented in one subspace. In the fifth column, the results of MI based single stage feature selection (including only the irrelevancy filter) with cascaded neuro-evolutionary algorithm (CNEA) as the forecast engine are represented. These results are quoted from our pre-vious work ( Amjady and Keynia, 2009 ). Error values shown in Table 5 are in terms of e week , defined according to ( Conejo et al., 2005 ; Amjady and Keynia, 2009 ): e P
In the denominator of e week , weekly average P AVE ACT is consid-ered instead of P iACT to avoid adverse effect of prices close to zero ( Conejo et al., 2005 ). As seen from Table 5 ,theMI X  X I  X  CNN has better forecast accuracy than all oth er considered methods in all test periods except the summer test week in which the MI  X  CNEA has slightly lower forecast error. Also, average e week of MI X  X I  X  CNN is considerably lower than all other methods of Table 5 .
Although, the wavelet transform and especially decomposition of the solution space can enhance forecast accuracy compared with a single ARIMA, however, there is still the problem of limited learning capability of a single ARIMA or FNN. Besides, some approximation is inherent in each of these decompositions.
The CNN method can perform a more thoroughly search of the solution space and obtains a better understanding of the future behavior of the price signal. The CNN forecast engine is also better than the CNEA. Table 5 shows that the average forecast error of
MI  X  CNN (5.13%) is lower than that of MI  X  CNEA (5.32%), while the both methods have the same feature selection technique.
To have a viewpoint on the stability of predictions, weekly error variance V week , as an index of uncertainty, for the examined methods is presented in Table 6 .

V Definition of V week is also in accordance with ( Amjady, 2006 ;
Conejo et al., 2005 ; Amjady and Keynia, 2009 ). As seen, the CNN method with both the MI and M X  X I has less V week values than the other methods of Table 6 in all time periods indicating less uncertainty in the CNN predictions.

In Table 7 , the proposed method is compared with na X   X ve, transfer function (TF), dynamic regression (DR), neural network (NN) and wavelet transform (WL) proposed in ( Conejo et al., 2005 ). The same test period of ( Conejo et al., 2005 ), i.e. the last week of all months of year 2002 has been considered here. The results of the other methods of Table 7 have been directly quoted from ( Conejo et al., 2005 ). As seen from Table 6 , the proposed CNN with the both feature selection techniques (especially with
MI X  X I) has lower forecast error ( e week ) than all other methods in the all 12 test weeks. In Table 8 , the CNN is compared with four transfer function models (M1, M2, M3 and M4) proposed in ( Nogales and Conejo, 2006 ). The mean absolute percentage error (MAPE), the mean of the median percentage error (MAPE2), the maximum mean percentage error (EMax) and the root of the mean squared error (RMSE) are used as error measures in Table 8 that their mathematical details can be found in ( Nogales and Conejo, 2006 ). Table 8 shows that the CNN with both the MI and
MI X  X I has lower forecast error than the four transfer function models in terms of all error measures. Tables 7 and 8 further reveal the price forecast capability of the proposed CNN. Also, in the both Tables, the MI X  X I  X  CNN has lower forecast errors than the MI  X  CNN.

Although correlation analysis can be also used for the feature selection, however it is a linear feature selection technique while
MI is a nonlinear criterion. It is noted that electricity price has a nonlinear and multivariate input/output mapping function and so
MI can better evaluate dependencies of the price signal on its input features. For a better illustration of this matter, we replaced the feature selection part of the proposed strategy with correla-tion analysis and examined it on the same test weeks of Table 3 .
The forecast engine of the proposed price forecast strategy (CNN) is kept unchanged. This comparison is presented in Table 9 .As seen from this Table, using autocorrelation function (ACF) instead of MI degrades forecast accuracy of the proposed strategy in the all test weeks, which indicates superiority of the MI based feature selection compared with the correlation analysis. It is noted that the average price forecast error of MI  X  CNN (4.63%) with respect tothe average error of ACF  X  CNN (5.29%) has been improved by 12.5%, which is computed as follows: (Average error of ACF  X  CNN  X  average error of MI  X  CNN)/ average error of ACF  X  CNN  X  (5.29 4.63)/5.29  X  12.5%. Similarly, it can be shown that the average error of MI-
MI  X  CNN with respect to the average error of MI  X  CNN has been improved by (4.63 4.43)/4.63  X  4.3%.

In Tables 10 and 11 , the MI  X  CNN and MI X  X I  X  CNN are compared with the ARIMA models presented in ( Contreras et al., 2003 ) for the Spanish and Californian electricity markets, respec-tively. The ARIMA models with and without explanatory variables have been proposed in ( Contreras et al., 2003 ) and their results are presented in Tables 10 and 11 .In Table 10 , WME for the last week of the first ten months and third week of November of year 2000 in the Spanish electricity market is presented. Test weeks of
Table 11 include the week of April 3rd to 9th, the week of August 21st to 27th, and the week of November 13th to 19th, 2000. It is noted that the results of the ARIMA models in Tables 10 and 11 have been directly quoted from ( Contreras et al., 2003 ). As seen from these Tables, the forecast error of the both MI  X  CNN and MI X  X I  X  CNN in the all test weeks in the both Spanish and Californian electricity markets is lower than the forecast error of
ARIMA without and with explanatory variables. Also, like the previous tables, the forecast errors of the MI-MI  X  CNN are lower than those of the MI  X  CNN. In Table 12 , the MI  X  CNN and MI X  X I  X  CNN are compared with Dynamic Regression (DR) and
Transfer Function (TF) techniques proposed in ( Nogales et al., 2002 ). According to this reference, two test weeks from the
Spanish electricity market (August 21 to 27 and November 13 to 19) and one test week from the Californian market (April 3 to 9) have been considered in this comparison. The results of DR and
TF techniques in Table 12 have been directly quoted from ( Nogales et al., 2002 ). As seen from this table, the both proposed methods and especially MI X  X I  X  CNN outperform DR and TF methods. Tables 10, 11 and 12 better illustrate the price forecast accuracy of the proposed method.

Total set-up time of the MI  X  CNN and MI X  X I  X  CNN including the execution of data preparation part, training of CNN with consideration of the cross-validation (fine-tuning of parameters), and training of ARIMA and batch NN is about 35 and 45 minutes, respectively on a Pentium P4 2.8 GHz personal computer with 512 MB RAM memory. Although these set-up times are more than that of the single ARIMA, NN, and FNN, they are completely reasonable within a day ahead decision making framework. 5. Conclusion Price forecast plays a major role in today X  X  electricity markets.
Companies that trade in electricity markets make extensive use of price forecast techniques either to bid or hedge against volatility.
However, price prediction has its own complexities. At the same time, utility companies usually have limited and uncertain information for price prediction. Thus, companies that trade in the electricity markets require efficient and robust price forecast methods. In this paper, a new price forecast strategy is proposed, which is composed of a new two stage feature selection algorithm and CNN as the forecast engine. Feature selection is a key issue for the success of any price forecast strategy. The proposed feature selection algorithm has irrelevancy and redundancy filters as the first and second stages, respectively. The both filters are based on the MI criterion, which can better evaluate nonlinear dependen-cies of the price signal compared with the previous feature selection methods such as correlation analysis. Additionally, it is shown that the proposed two stage feature selection algorithm is more efficient than single stage feature selection evaluating only relevancy of the candidate inputs. Moreover, a single forecast method usually cannot provide an accurate forecast of price in real electricity markets. However, different NNs are so combined in the framework of the CNN that cover deficiencies of each other. For this purpose, the new idea of transferring the weight values of an NN to another NN is proposed. In this way, each NN transfers its obtained knowledge to the next one and so it can begin its learning process from the point that the previous NN terminated instead of beginning from a random point. Therefore, the CNN can learn much more than a single NN, i.e., a better tuning for the adjustable parameters can be obtained. Additionally, each fore-caster of the CNN transfers its predicted value to the next one. In this manner, each forecaster has an initial guess of the actual price that should be predicted and so it can produce a better forecast value for it. Detailed examinations of the proposed price forecast strategy on the PJM, Spanish and Californian electricity markets confirm the developed ideas.

Composite forecast methods can present more learning cap-ability, which is suitable for predicting complex signals (e.g., electricity price time series). However, care must be taken in the design of these composite methods, as their performance is highly dependent on their building blocks and data flow among them and the selected input features. Optimization of these aspects for the electricity price prediction is still a challenging task, demanding more research. Besides, price forecast in the other market structures such as real time markets can be a matter of the future research.
 References
