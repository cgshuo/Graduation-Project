 Recently, a number of semi-stream join algorithms have been published. The typical system setup for these consists of one fast stream input that has to be joined with a disk-based re-lation R . These semi-stream join approaches typically per-form the join with a limited main memory partition assigned to them, which is generally not large enough to hold the whole relation R . We propose a caching approach that can be used as a front-stage for different semi-stream join algo-rithms, resulting in significant performance gains for com-mon applications. We analyze our approach in the context of a seminal semi-stream join, MESHJOIN (Mesh Join), and provide a cost model for the resulting semi-stream join al-gorithm, which we call CMESHJOIN (Cached Mesh Join). The algorithm takes advantage of skewed distributions; this article presents results for Zipfian distributions of the type that appears in many applications.
 H.2.4 [ Information Systems ]: Database Management X  Systems X  X uery processing Join Operator, Performance Semi-stream join, Performance optimization
Stream-based joins are important operations in modern system architectures, where just-in-time delivery of data is expected. We consider a particular class of stream-based join, a semi-stream join that joins a single stream with a slowly changing table. Such a join can be applied, for ex-ample, in real-time data warehousing [13, 9]. In this appli-cation, the slowly changing table is typically a master data table and the stream contains incoming real-time sales data. The stream-based join is used to enrich the stream data with master data. A common type of join in this scenario is an equijoin, e.g. on a foreign key in the stream data.
In this work we consider one-to-many equijoins only, as they appear between foreign keys and the referenced pri-mary key in another table. This is a very important class of joins that occurs naturally in data warehousing [9], on-line auction systems [2] and supply-chain management [20]. That is, we do not consider joins on categorical attributes, such as gender, in master data.

With the availability of large main memory and power-ful cloud computing platforms, considerable computing re-sources can be utilized when executing stream-based joins. However, there are several scenarios where approaches that can function with limited main memory are of interest. First, the master data may simply be too large for the resources allocated for a stream join, so that a scalable algorithm is necessary. Secondly, an organization may decide to reduce the carbon footprint of the IT infrastructure: main memory as well as cloud-computing approaches can be power-hungry. Thirdly, low-resource consumption approaches may be nec-essary when mobile and embedded devices are involved. For example, stream joins such as the one discussed here could be used in sensor networks. As a consequence, semi-stream join algorithms that can function with limited main memory are interesting building blocks for a resource-aware setup.
In this paper we present a novel caching approach that works as a front-stage for existing semi-stream join algo-rithms. It is different from other cache-based approaches [4, 5] in that it uses a tuple-level rather than a page-level cache. The front-stage significantly improves join performance for data with Zipfian distributions of the foreign keys, which can be found in a wide range of applications [1]. To demonstrate the front-stage, we combine it with MESHJOIN (Mesh Join), which is a seminal algorithm in the field of semi-stream joins [15, 16]. The resulting approach  X  called CMESHJOIN (Cached Mesh Join)  X  separates the concerns of optimizing for a particular distribution of stream data and performing a join under general assumptions. We provide a cost model and experimental results to clarify the properties and bene-fits of the approach.
 Our main findings can be summarized as follows: Tuple-level cache: The caching approach in CMESHJOIN has the granularity of tuples. Every tuple in the cache is frequent in the stream data, so memory is utilized well. Higher service rate: For skewed data, as found in many application scenarios [1] such as data warehousing, CMESH-JOIN outperforms MESHJOIN. The advantage increases as the skew becomes more pronounced.
 Cost model: We present a cost model for CMESHJOIN and perform a sensitivity analysis with respect to various parameters, validating the cost model in the process. In certain applications it can be natural to use Solid State Drives (SSDs) for storing the master data in order to gain performance advantages. However, access to the master data is sequential in CMESHJOIN, and SSDs have only a moderately faster sequential access time compared to en-terprise strength HDD equipment [7]. Hence we did not consider SDD technology in our discussion. Similarly, we do not consider the effect of processor cache or other main memory hardware caches, as main memory access time in CMESHJOIN is insignificant compared to disk access time. Section 2 summarizes related work. Section 3 describes CMESHJOIN and its cost model. Section 4 describes an experimental evaluation. Section 5 concludes the paper.
The Symmetric Hash Join (SHJ) algorithm [19, 18] ex-tends the original hash join algorithm in a pipeline fashion. The Double Pipelined Hash Join [11], XJoin [17] and Early Hash Join (EHJ) [12] are further extensions of SHJ for the pipeline execution of join. All these algorithms take both inputs in the form of streams while our focus is on joins between stream data and non-stream data.

A number of tools have been developed for stream ware-housing that can process stream data with archive data [3, 10, 8, 9]. However, these tools do not provide optimal solu-tions for the non-uniform characteristics of stream data.
MESHJOIN (Mesh Join) [15, 16] has been designed specif-ically for joining a continuous stream with a disk-based re-lation, like the scenario in active data warehouses. The MESHJOIN algorithm is a hash join, where the stream serves as the build input and the disk-based relation serves as the probe input. A characteristic of MESHJOIN is that it per-forms a staggered execution of the hash table build in order to load in stream tuples more steadily. The algorithm makes no assumptions about data distribution and the organization of the master data. The MESHJOIN authors report that the algorithm performs worse with skewed data.

R-MESHJOIN (reduced Mesh Join) [14] clarifies the de-pendencies among the components of MESHJOIN. As a result, it improves the performance slightly. However, R-MESHJOIN again does not consider the non-uniform char-acteristic of stream data.

The partition-based join algorithm described in [5] im-proves MESHJOIN performance. It uses a two-level hash table for attempting to join stream tuples as soon as they ar-rive, and uses a partition-based waiting area for other stream tuples. However, the time that a tuple is waiting for execu-tion is not bounded. We are interested in a join approach where there is a time guarantee for when a stream tuple will be joined. Moreover, [5] uses page-level cache, so the cache memory is not fully exploited if some tuples on a cached page are infrequent in stream data.

Semi-Streaming Index Join (SSIJ) [4] was developed re-cently to join stream data with disk-based data. Again SSIJ also uses a page-level cache. The published work does not include a mathematical cost model. Consequently, the cri-teria for choosing optimal parameters for SSIJ are unclear.
Some other approaches [6, 9] have considered the problem of joining stream data with disk-based data, but to the best of our knowledge they did not propose an algorithm for it.
We propose a generic cache component that can be used as a front-stage for an arbitrary semi-stream join algorithm. It exploits skewed distributions in the stream foreign keys in order to improve the service rate. We demonstrate this concept by adding the front-stage to MESHJOIN, result-ing in a new algorithm called CMESHJOIN. This section gives a high-level description of CMESHJOIN; a detailed walkthrough can be found in Section 3.1. Both the front-stage and MESHJOIN are hash joins, so the CMESHJOIN algorithm can be seen overall as possessing two complemen-tary hash join phases, somewhat similar to Symmetric Hash Join [19, 18]. One phase, the MESHJOIN, uses R as the probe input, with the largest part of R typically being stored in tertiary memory. The other join phase, the front-stage, uses the stream as the probe input and deals only with a small part of R . For each incoming stream tuple, CMESH-JOIN first uses the front-stage to find a match for frequent requests quickly, and if no match is found, the stream tuple is forwarded to the MESHJOIN phase.
 The execution architecture of CMESHJOIN is shown in Figure 1. Relation R and stream S are the external input sources of the join. The key components of CMESHJOIN with respect to memory size are two hash tables: one stor-ing stream tuples, denoted by H S , and the other storing tuples from the disk-based relation, denoted by H R . H R the cache that contains the most frequently accessed part of R . The other main components of CMESHJOIN are a disk buffer, a queue, a frequency recorder, and a stream buffer. The disk buffer is used to load parts of R into memory using equallly sized partitions. The queue stores pointers to the stream tuples in H S , keeping track of their order and en-abling the deletion of fully processed tuples. The frequency recorder records the access frequency of each tuple stored in H R . The stream buffer is only a small buffer for holding part of the stream for a while, if necessary. For reference, we have preserved the original architecture of MESHJOIN in the MESHJOIN phase, although alternative architectures are possible (e.g. by using an order-preserving hash table data structure instead of the queue).
 CMESHJOIN alternates between the front-stage and the MESHJOIN phases. The hash table H S is used to store only that part of the update stream that does not match tuples in H R . A front-stage phase ends if H S is completely filled or if the stream buffer is empty. Then the MESHJOIN phase becomes active. In each iteration of the MESHJOIN phase, the algorithm loads a set of tuples of R into memory to amor-tize the costly disk access. After loading the disk pages into the disk buffer, the algorithm probes each tuple of the disk buffer in the hash table H S . If the required tuple is found Figure 1: Data structures and architecture of CMESHJOIN in H S , the algorithm generates that tuple as an output. Af-ter each iteration the algorithm removes the oldest chunk of stream tuples from H S . This chunk is found at the top of the queue; its tuples were joined with the whole of R and have thus been completely processed at this point. Later we call them expired stream tuples. As the algorithm reads R sequentially, no index on R is required. After one probe step, a sufficient number of stream tuples are deleted from H S , so the algorithm switches back to the front-stage phase. One phase of front-stage with a subsequent phase of MESH-JOIN constitutes one outer iteration of CMESHJOIN. The front-stage phase is used to boost the performance of the al-gorithm by quickly matching the most frequent master data. An important question is how frequently a master data tu-ple must be used in order to get into this phase, so that the memory sacrificed for this phase really delivers a per-formance advantage. In Section 3.2 we give a precise and comprehensive analysis that shows that a remarkably small amount of memory assigned to the front-stage phase can de-liver a substantial performance gain. In order to corroborate the theoretical model, we present experimental performance measurements in Section 4. For determining very frequent tuples in R and loading them into H R , a frequency detection process is required, which is described in Section 3.1.
The execution steps for CMESHJOIN are shown in Algo-rithm 1. The outer loop of the algorithm is an endless loop, which is common in stream processing algorithms (line 1). Thebodyoftheouterloophastwomainparts: thefront-stage phase and the MESHJOIN phase. Due to the endless loop, these two phases alternate. The MESHJOIN algorithm has two parameters, the disk-buffer size b and the number w of stream tuples processed in each iteration, which are computed once the memory allocation for MESHJOIN and thesizeof R are fixed, as described in [15, 16].
Lines 2 to 11 specify the front-stage phase. In this phase the algorithm reads stream tuples from the stream buffer (line 4). The algorithm probes each stream tuple t in the disk-build hash table H R , using an inner loop (line 3). In the case of a match, the algorithm generates the join output without storing t in H S (line6). Inthecasewhere t does not match, the algorithm loads t into H S , while also enqueuing its pointer in the queue Q (line 8). The front-stage phase stops once w stream tuples have been loaded into H S . Algorithm 1 CMESHJOIN Input: A disk based relation R and a stream of updates S Output: RS Parameters: w (tuples of S )and b (tuples of R ). Method: 1: while (true) do 2: u  X  0 (the number of unmatched tuples) 3: while ( u&lt;w ) do 4: READ stream tuple t from the stream buffer 5: if t  X  H R then 6: OUTPUT t 7: else 8: ADD stream tuple t into H S and also place a 9: u  X  u +1 10: end if 11: end while 12: READ b number of tuples of R into the disk buffer 13: for each tuple r in b do 14: if r  X  H S then 15: OUTPUT r 16: f  X  number of matching tuples found in H S 17: if ( f  X  thresholdV alue ) then 18: SWITCH the tuple r into hash table H R 19: end if 20: end if 21: end for 22: DELETE the oldest w tuples from H S along with 23: end while
Lines 12 to 22 specify the MESHJOIN phase. At the start of this phase, the algorithm reads b tuples from R and loads them into the disk buffer (line 12). In an inner loop, the algorithm looks up all tuples from the disk buffer in hash table H S . In the case of a match, the algorithm generates that tuple as an output (line 15). Since H S is a multi-hash-map, there can be more than one match; the number of matches is f (line 16).
 Lines 17 and 18 are concerned with frequency detection. In line 17 the algorithm tests whether the matching fre-quency f of the current tuple is larger than a pre-set thresh-old. If it is, then this tuple is entered into H R . If there are no empty slots in H R , the algorithm overwrites an existing least-frequent tuple in H R using the frequency recorder. Fi-nally, the algorithm removes the expired stream tuples (i.e. the ones that have been joined with the whole of R ) from H
S , along with their pointers from the queue (line 22). If the cache is not full, it means the threshold is too high; in this case, the threshold can be lowered automatically. Sim-ilarly, the threshold can be raised if tuples are evicted from the cache too frequently. This makes the front-stage phase flexible and able to adapt online to changes in the stream behavior. Necessarily, it will take some time to adapt to changes, similar to the warmup phase. However, this is usually deemed acceptable for a stream-based join that is supposed to run for a long time. We now show how a cost model can be used to tune CMESHJOIN theoretically. The notations we use in our cost model are given in Table 1. We use the assumption Table 1: Notation for CMESHJOIN cost model Parameter name Symbol Total allocated memory (bytes) M Service rate (processed tuples/sec)  X  Size of stream tuple (bytes) v S Size of disk tuple (bytes) v R Disk buffer size (tuples) b Size of H R (tuples) h R Size of H S (tuples) h S
Disk relation size (tuples) R t that the stream of updates S has a Zipfian distribution with an exponent of 1. In this Zipfian distribution, the frequency of the second element is half that of the first element. Simi-larly, the frequency of the third element is 1 3 that of the first element, and this decreasing pattern continues in the tail of the distribution [1]. In this case, the matching probability for stream S in the front-stage phase can be determined us-ing Equation 1. The denominator is a normalization term to ensure all probabilities sum up to 1. We consider the derivative of Equation 1: MESHJOIN is used in the second phase of CMESHJOIN (the MESHJOIN phase). The major portion of the total memory is assigned to the MESHJOIN phase. The memory for each component relevant to the front-stage phase can be calculated as follows: Memory for H R (bytes)= h R  X  v R Memory for frequency recorder (bytes)= 8 h R Therefore the memory consumption for MESHJOIN can be estimated as shown in the enumerator of the following equa-tion. MESHJOIN X  X  performance is roughly proportional to the memory available and inversely proportional to the size of R , therefore the service rate produced in this phase can be calculated using the following equation. Since we are considering Zipfian distributions with finite skew, there will always be tuples that need to be processed through the MESHJOIN phase and hence p N ( h R ) &lt; 1. The service rate of CMESHJOIN is a function of h R and can be written as Now we take the derivative of the above equation using the chain rule. Service rate (tuples/sec) Figure 2: Tuning of the cache module: empirical measurements vs. estimates based on the cost model
By using Equations 2, 3, and 4 in Equation 5, we get: To ensure that the root of the first derivative corresponds to a maximum, we consider the second derivative: From here we can determine the value of h R at which the value of  X  cm reaches a maximum. Once the optimal memory size for the front-stage component is determined, the rest of the memory is assigned to the MESHJOIN components using the tuning approach presented in [15, 16].
Hardware and software specifications: We performed our experiments on a Pentium-i5 with 8GB main mem-ory and 500GB HDD. All experiments were implemented in Java, using the org.apache.MultiHashMap as a hash table data structure with support for multiple values per key.
Measurement strategy: The performance or service rate of the join is measured by calculating the number of tuples processed in a unit second. In each experiment, both algorithms first completed their warmup phase before start-ing the actual measurements. The calculation of the confi-dence intervals is based on 1000 to 4000 measurements for one setting.

Data specifications: We analyzed the service rate of the algorithms using synthetic, TPC-H, and real-life datasets. The relation R was stored on disk using a MySQL database. To measure the I/O cost more accurately, we set the fetch size for ResultSet equal to the disk buffer size. The syn-thetic stream dataset was based on a Zipfian distribution. The synthetic master data was unsorted, did not have an in-dex, and comprised 100 million tuples (  X  11.18GB). We used disk tuple, stream tuple and queue pointer sizes similar to the original MESHJOIN (120, 20 and 4 bytes respectively). For the TPC-H dataset we used a scale factor of 100 with the table Customer as master data and the table Order as stream data. The real-life dataset 1 , which was also used to evaluate the original MESHJOIN, contains cloud information stored
Available at: http://cdiac.ornl.gov/ftp/ndp026b/ in a summarized weather report format. Weather data from different months were joined (20 million tuples master data, 6 million tuples stream data, tuple size 128 bytes), using the common attribute longitude ( LON ).

Cache size: The cost model predicts that the cache size h
R influences the service rate of CMESHJOIN. In order to obtain the optimal cache size, we performed experiments and compared the results with estimates obtained from the cost model. The results are shown in Figure 2 for a fixed total memory of 0.11GB and a size of 100 million tuples (11.18GB) for R . As can be seen there is a close match be-tween estimates and measurements. To get optimal results, the memory allocations for the stream-probing phase (cache module) and the disk-probing phase (MESHJOIN) have to be balanced. In our experiments, we adjusted the cache size automatically based on our cost model. Figure 3(a) shows the effect of the cache size for various memory budgets.
Analysis by varying size of memory: We compared the service rate of MESHJOIN and CMESHJOIN while vary-ing the memory size from 1% to 10% of R ,withthesizeof R being 100 million tuples. For each memory setting we repeated the experiment, considering different skew values in the streaming data. In the case of MESHJOIN, we only considered a skew value of 0 since MESHJOIN performs best with uniform data (as indicated later in Figure 3(d)). The results are shown in Figure 3(b). From the figure it can be noted that due to the front-stage phase CMESHJOIN per-formed up to 7 times faster than MESHJOIN with the 10% memory setting. In a limited memory environment (1% of R ), CMESHJOIN still performed up to 5 times better than MESHJOIN, which makes it an adaptive solution suitable for memory-constrained applications.

Analysis by varying size of R : We measured the ser-vice rate for CMESHJOIN, MESHJOIN and INLJ (index nested loop join) at different sizes of R , with a fixed mem-ory size (  X  1.12GB) and a skew value of 1. Since the size of R is the only parameter that affects INLJ X  X  service rate, we did not include it in the analysis of memory and skew. From Figure 3(c) we see that CMESHJOIN performed up to 4.5 times better than MESHJOIN when the size of R was 20 million tuples. This improvement increased to 6.5 times when the size of R was 100 million tuples. One im-portant fact that can be noted here is that for CMESH-JOIN, due to the front-stage phase, the service rate does not decrease inversely when increasing the size of R ,asin MESHJOIN. CMESHJOIN substantially outperforms INLJ because of INLJ X  X  high I/O cost due to random probes to the master data. Also, INLJ processes one stream tuple in each iteration, so does not amortize the expensive I/O cost on fast streaming data.

Analysis by varying skew value: We compared the service rate of both algorithms while varying the skew of the streaming data. To vary the skew, we varied the Zipfian ex-ponent from 0 to 1. At 0 the input stream S is uniform, while at 1 the stream has a strong skew. The size of R was fixed at 100 million tuples (  X  11.18GB) and the available memory was set to 10% of R (  X  1.12GB). The results presented in Figure 3(d) show that CMESHJOIN performs significantly better than MESHJOIN, even for only moderately skewed data. This improvement became more pronounced for in-creasing skew values. At a skew of 1, CMESHJOIN performs approximately 7 times better than MESHJOIN. As MESH-JOIN does not exploit skew in its algorithm, its service rate actually decreased slightly for more skewed data, which is consistent with the original MESHJOIN findings. We do not present data for skew values larger than 1, which would im-ply short tails. However, we predict that for such short tails the trend continues. From Figures 3(b) and (d) we can see that CMESHJOIN only performs worse than MESHJOIN when the stream data is completely uniform (exponent of 0). However, this difference is a constant factor.
TPC-H and real-life datasets: In these experiments we measured the service rate produced by both algorithms at different memory settings. From Figure 3(e) it can be noted that CMESHJOIN performed about 4 times better than MESHJOIN for TPC-H data, which is significant es-pecially for a smaller memory size, 1% of R . Similarly, it is obvious from Figure 3(f) that CMESHJOIN outperforms MESHJOIN for the real-life data under all memory settings.
In this paper we have discussed a new semi-stream join called CMESHJOIN. This algorithm is based on MESH-JOIN, and extends it with a cache front-stage in order to exploit skewed distributions. We have provided a theoreti-cal cost model as well as experimental results that show that this approach yields substantial speed-ups for Zipfian distri-butions in the data. This type of skewed, non-uniformly distributed data is frequently found in real-world applica-tions.

Our findings suggest that the front-stage in CMESHJOIN is able to exploit skewed distributions in the data in very general circumstances and provide the join output for a large portion of the stream, thus reducing the overall load for the subsequent stages. We expect that the front-stage is generic and can be used with any semi-stream join operator to en-hance its service rate, because it has little overhead and de-livers substantial speed-up under very general assumptions. In the future, we will consider many-to-many equijoins and certain classes of non-equijoins.
 Source URL: We have provided open-source implementa-tions of CMESHJOIN that can be used for further analysis. https://www.cs.auckland.ac.nz/research/groups/serg/cmj/ . [1] C. Anderson. The Long Tail: Why the Future of [2] A. Arasu, S. Babu, and J. Widom. An abstract [3] M. H. Bateni, L. Golab, M. T. Hajiaghayi, and [4] M. Bornea, A. Deligiannakis, Y. Kotidis, and [5] A. Chakraborty and A. Singh. A partition-based [6] S. Chandrasekaran, O. Cooper, A. Deshpande, M. J. [7] F. Chen, D. A. Koufaty, and X. Zhang. Understanding [8] L. Golab and T. Johnson. Consistency in a stream [9] L. Golab, T. Johnson, J. S. Seidel, and [10] L. Golab, T. Johnson, and V. Shkapenyuk. Scheduling [11] Z. G. Ives, D. Florescu, M. Friedman, A. Levy, and [12] R. Lawrence. Early H ash J oin: A configurable [13] M. A. Naeem, G. Dobbie, and G. Weber. An [14] M. A. Naeem, G. Dobbie, G. Weber, and S. Alam. [15] N. Polyzotis, S. Skiadopoulos, P. Vassiliadis, [16] N. Polyzotis, S. Skiadopoulos, P. Vassiliadis, [17] T. Urhan and M. J. Franklin. XJ oin: A [18] A. N. Wilschut and P. M. G. Apers. Pipelining in [19] A.N.WilschutandP.M.G.Apers.Dataflowquery [20] E. Wu, Y. Diao, and S. Rizvi. High-performance
