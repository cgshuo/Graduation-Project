
Department of Automation, Shanghai Jiao Tong University, Shanghai, China Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai, China 1. Introduction
Support vector machines (SVMs), pioneered by Vapnik [16], have gained increasing attention as effi-imization, an SVM attempts to find a surface that maximally separates points belonging to two classes. For the nonlinear case, the points can be mapped into a higher, often much higher, dimensional space feature space. Mercer kernels, which satisfy the Mercer condition, play an important role in the SVM. In 2000, Mangasarian proposed generalized support vector machines (GSVMs) [11]. The author stated a mathematical program and derived some special cases to show that a nonlinear separating surface can and Mangasarian proposed smooth support vector machines (SSVMs) [9]. The idea of the SSVM is to convert the traditional quadratic program of the standard SVM to an unconstrained optimization prob-which can be solved using a Newton-Armijo algorithm. Since the nonlinear SSVM was obtained using the GSVM, it is capable of generating nonlinear surfaces using arbitrary kernels.

In recent years, methods of incorporating support vector learning into fuzzy rule-based systems have been proposed by researchers [2,3,6,7,10]. Chen [2] investigated the connection between SVMs and fuzzy classifiers (PDFCs). Based on Chen X  X  approach, Papadimitriou [13] proposed a support vector fuzzy inference system and an interpretable rule system on top of it. In Chen X  X  algorithm, membership functions for the same input variable are generated from location transformation of a reference func-tion [2,4]. A fuzzy classifier is built from the training samples based on an SVM, where the kernel is perimental results demonstrate that the PDFCs have good generalization ability. However, the positive fuzzy classifiers to GSVMs or SSVMs.

In this paper we propose a smooth support vector learning algorithm for fuzzy rule-based classifica-tion systems. Compared with the PDFC, our algorithm extends the connection between fuzzy rule-based classification systems and kernel machines to a more general case. Under the same assumption on mem-(SBFCs). The advantages of using SSVM approach to build SBFCs can be summarized to: 1. The positive definiteness requirement on reference functions is relaxed. Any arbitrary reference 2. The reduced kernel technique can be introduced to simplify the decision function of the SBFC and 3. The important mathematical properties of the SSVM such as strong convexity and infinitely differ-The following briefly describes our notation. All vectors will be column vectors unless transposed to a row vector by a prime superscript . x, y will denote the inner product of two vectors x and y in R n .The p -norm of x will be denoted by x column vector of ones of arbitrary dimension will be denoted by e . The base of the natural logarithm y as an m  X  n matrix A , and the class labels will be rearranged as an m  X  m diagonal matrix D where D The rest of this paper is organized as follows. Section 2 is an overview of SVMs, GSVMs and SSVMs. Section 3 gives a review of previous work. In Section 4 we present our approach. Experimental results and comparisons are given in Section 5. Section 6 is the conclusion.
 2. Smooth support vector machines
In this section we start with the basic concept of the standard SVM. Then we introduce the GSVM and explain how it generates a nonlinear separating surface using a completely arbitrary kernel. Based on the idea of the GSVM, we present the SSVM. 2.1. Support vector machines and y i  X  X  X  1 , 1 } is the class label, an SVM tries to find an optimal hyperplane { x  X  R n : w,x + b =0 ,w  X  R n ,b  X  R } that separates the two classes of training samples with the largest margin between them. The decision function F ( x )=sgn( w,x + b ) is found by solving the following convex quadratic program: the margin 2 w where  X  i are the Lagrange multipliers. A training sample with nonzero  X  i is called a support vector function becomes:
The decision surface is a hyperplane in R n . Nonlinear decision surfaces can be generated by mapping K : R n  X  R n  X  R is named a Mercer kernel if it satisfies the Mercer condition, i.e., R computes the inner product in the space  X  maps to, which is called the feature space. By introducing the Mercer kernel, we can directly define the inner product in the feature space F as K ( x i ,x j )=
 X ( x i ) ,  X ( x j ) . Substituting K ( x i ,x j ) for  X ( x i ) ,  X ( x j ) produces a new program: The solution of Eq. (5) gives a decision function:
The decision surface defined by Eq. (6) is a hyperplane in the high-dimensional feature space while learning machines. 2.2. Generalized support vector machines In GSVMs [11], a general kernel is defined as follows:
Consider a given training set of m training samples in R n represented by an m  X  n matrix A . D is an m  X  m diagonal matrix with +1 or  X  1 along its diagonal specifying the class label of each sample. A nonlinear separating surface can be implicitly defined by some general kernel K : A sample is classified according to the following decision function:
The kernel function K ( x ,A )  X  R m implicitly defines a nonlinear mapping from R n to some other can be represented as a hyperplane in R k and is linear in its parameters u  X  R m and b  X  R .The following mathematical program generates such a surface for a general kernel as follows:
Here f is some convex function on R m , typically some norm or seminorm, that suppresses the sepa-of u can be interpreted as minimizing the number of support vectors, which surrogate maximizing the distance between the separating planes in the higher dimensional space R k . The mathematical program Eq. (9) has a solution whenever f is a piecewise-linear or quadratic function bounded below on R m .
A quadratic program is obtained by setting f to be some convex quadratic function. One particular choice is f ( u )= 1 2 u 2 2 , which leads to a dual problem as follows: inite with no assumptions on K ( A, A ) . Thus Eq. (10) is always solvable for any arbitrary kernel K ( A, A ) . 2.3. Smooth support vector machines
Unlike the standard SVM in Eq. (1), the SSVM [9] minimizes the square of 2-norm of the error  X  with margin in the ( n +1) -dimensional space of ( w,b )  X  R n +1 ,thatis, 2 ( w,b ) problem. The modified object function leads to the following optimization problem: and convert the standard SVM Eq. (1) into the following unconstrained optimization problem:
This is a strongly convex minimization problem with no constraints. It has a unique solution. However, the object function is not twice differentiable, which precludes the use of a fast Newton method. A SSVM is then obtained by replacing the plus function with a very accurate smooth approximation: differentiable. It can be solved using a Newton-Armijo algorithm. The solution of problem Eq. (11) is obtained by solving problem Eq. (13) with  X  approaching infinity.
The problem Eq. (11) can be extended to the nonlinear case using a GSVM as follows:
If we let v = Du ,then v 2 2 = u 2 2 . Thus, the previous problem is equivalent to Repeating the same arguments as previously described, in going from Eqs (11) to (13), obtains the SSVM with a nonlinear kernel as follows: leads to the decision function of the nonlinear SSVM:
The coefficient v i is determined by solving Eq. (16). Training samples with corresponding nonzero coefficients are called support vectors. 3. Review of previous work
The general form of a fuzzy rule-based system with constant THEN part which consists of l fuzzy rules is: junction, addition for fuzzy rule aggregation and COA defuzzification, a fuzzy model can be defined as follows: where x =[ x 1 ,...,x n ]  X  R n . According to [2], the following fuzzy rule is added to ensure the denominator l j =1 n k =1 u k j ( x k ) &gt; 0 for all x  X  R n .
 where the membership functions u k 0 ( x k )  X  1 for k =1 , 2 ,...,n and any x k  X  R . Thus, the model becomes: The above fuzzy rule-based system induces a binary fuzzy classifier with decision function:
According to [2], a binary fuzzy classifier is named a standard binary fuzzy classifier (SBFC) if the membership functions for the same input variable are generated from location transformation of u j : R mation as follows: [ z j ,...,z of an SBFC can be written as { b 1 ,...,b l } as w = Mercer kernel if and only if it has nonnegative Fourier transform. The authors of [2] proved that a function f is a positive definite function [5] if and only if its Fourier transform is nonnegative. An obvious conclusion can be drawn that a translation invariant kernel K defined by definite functions.

An SBFC is called a positive definite fuzzy classifier (PDFC) if the reference functions are positive definite functions. Chen [2] illustrated the connection between PDFCs and SVMs and proposed a support vector learning algorithm for PDFCs. A PDFC is built from the training samples based on an SVM. The number of fuzzy rules equals the number of support vectors and hence is irrelevant to the dimension of by a support vector.

The principle of structural risk minimization establishes the theoretical foundation for good general-ization of PDFCs. However, the positive definiteness requirement on reference functions is somewhat restrictive. Now we present a smooth support vector learning approach to relax it.
 4. Smooth support vector learning for SBFCs Many commonly used reference functions are not positive definite functions. Kernels defined by Eq. (24) are not, in general, Mercer kernels. However, we can extend the PDFC to a general case using an SSVM. Since the SSVM puts no restrictions on the kernel, any arbitrary reference function can be used. Moreover, the definition of reference functions can be relaxed.
 1) u (0) = 1 ,2) u is non-increasing on [0 ,  X  ) .
 functions such as the asymmetric triangle function which is not positive definite. A list of reference functions and their Fourier transform is given in Table 1. Please note that the Asymmetric triangle, Trapezoid and Quadratic reference functions are not positive definite functions. 4.1. How to construct an SBFC
An SBFC with n inputs and l fuzzy rules is parameterized by n reference functions u k ( k =1 , 2 ,..., we assume that the reference functions are predetermined, then the question is how to find a set of R n  X { X  1 , 1 } so that the SBFC has good generalization.

A translation invariant kernel in the form of Eq. (24) can be constructed from reference functions u enables an SBFC, in the form of Eq. (22), to work on a higher dimensional space than the input space without explicitly knowing the f unctional form of the mapping.
 The following theorem illustrates the relationship between SBFCs and translation invariant kernels: Theorem 1. Given n reference functions u k ( k =1 , 2 ,...,n ) defined by Definition 2, we define a { functions that separates the samples.
 A  X  R m  X  n and D  X  R m  X  m and n reference functions u k ( k =1 , 2 ,...,n ) ,akernel K ( A, A ) can be constructed where K is defined by Eq. (24). Because problem Eq. (16) is always solvable for arbitrary kernels, a separating surface can be generated as follows: where v  X  R m .

We can define an SBFC using u k ( k =1 , 2 ,...,n ) as reference functions. Let z j contain location where b =[ b 1 ,...,b l ]  X  R l .

If we let Z = A and b = v , then the location parameters of the IF part membership functions are given by the training samples, and the THEN part constants are given by the separating surface parameter. Remark: Theorem 1 relates the decision boundary of an SBFC to a separating surface in the input space. Therefore, constructing an SBFC transforms to: given a set of training samples, how to find a separating surface that yields good generalization, and how to extract fuzzy rules from the obtained obtained using the SSVM algorithm is an optimal separating surface. Once we get such a separating surface, fuzzy rules can be easily extracted. 4.2. Reduced kernel technique
A drawback of the SSVM compared with the standard SVM is that sparseness is lost due to the choice without degrading its generalization performance. The key idea of the reduced kernel technique is to 17] of the full kernel is given as: where K ( A,  X  A ) is a reduced kernel. Applying the approximation for a vector v  X  R m we have The solution with the reduced kernel approximates the solution with the full kernel [8].
The SSVM with a reduced kernel K ( A,  X  A ) solves the following approximate unconstrained mini-mization problem: The solution to Eq. (31) leads to the following decision function:
The reduced kernel technique has constructed a compressed model, where training samples with nonzero  X  v i s play a similar role of support vectors.
 4.3. Smooth support vector learning for SBFCs The procedure of smooth support vector learning for SBFCs is described by the following algorithm. Algorithm 1: Smooth support vector learning for SBFC of fuzzy rules is l +1 .
 Steps: 1. Construct a translation invariant kernel from the given reference functions according to Eq. (24). 3. Assign some positive number to the penalty parameter C , and solve problem Eq. (31) to get  X  v and 4. Extract fuzzy rules from the decision function of the SSVM defined by Eq. (32):
The algorithm generates a set of fuzzy rules. Each fuzzy rule is parameterized by a training sample ( x determined by the size of the reduced set. The size of the reduced set can be much smaller than the number of support vectors when using the standard SVM. Therefore, for a given training set, we can obtain an SBFC with much less fuzzy rules than a PDFC while maintaining good generalization, which will be demonstrated in the coming section. 5. Experimental results evaluate the performance of SBFCs in terms of generalization and number of fuzzy rules using different real world data sets. Comparisons with PDFCs are also provided. 5.1. A nonlinearly separable classification example function u 2 ( x 2 ) Eq. (33) for input x 2 .
We set the reduce rate r =20 % and the penalty parameter C =20 to construct an SSVM. An SBFC, which consists of 7 fuzzy rules, is obtained from the solution of the SSVM. The obtained SBFC has the following form (excluding Rule 0):
The location parameters for the IF part membership functions and the THEN part constants of the six fuzzy rules are given in Table 2. The THEN part constant of Rule 0 is given by the bias term of the u belong to the other location family generated by u 2 . The final decision boundary of the SBFC is dis-played in Fig. 4. 5.2. Real world data sets
We tested the performance of the SBFC on different real world data sets. The first data set we used is the best known IRIS data set. The IRIS data set contains 50 samples from each of three classes of iris flowers namely Setosa, Versicolor and Verginica. Each sample is represented by four features Versicolor and Verginica classes, the latter are not linearly separable from each other.
Based on Algorithm 1, we design three SBFCs, each of which separates one class from the other two classes. The generalization performance of the SBFC is evaluated via two-fold cross-validation. The data set is randomly divided into two subsets of equal size. An SBFC is trained two times, each time viewed as an estimation of the generalization performance.

For all input variables, we use Gaussian reference function in Table 1. The reduce rate r is set to be 20%. The average validation rate of SBFCs for different values of C (in Algorithm 1) and d (of the Gaussian reference function) are plotted in Figs 5 X 7. The number of fuzzy rules of all the SBFCs is 15. As shown in Fig. 5, the SBFCs generalize perfectly for all values of C and d when separating the Setosa class from the other two classes since they are linearly separable. Figures 6 and 7 show that larger C value corresponds to a higher validation rate. This complies with the property of the reduced kernel that since the reduced kernel technique has already reduced the model complexity via using a The maximal average validation rate of the three SBFCs and a comparison with PDFCs are given in Table 3.
 as [2]: Pick three SBFCs with the same C and d values, the final predicted class label is decided by summarized in Table 4, where we also cite the results reported by Chen [2]. We can see from Table 4 PDFCs. Moreover, the number of fuzzy rules used by SBFCs is less than that of PDFCs. For a PDFC, set.

We also tested the SBFC on several binary classification problems. The data sets we used are iono-sphere, Wisconsin breast cancer and mushroom.

The ionosphere data set contains 351 samples belonging to 2 classes. Each sample is represented by 34 features and the associated class label.

The Wisconsin breast cancer data set consists of 699 samples belonging to 2 classes. Each sample is a single missing attribute value are removed in our experiment.

The mushroom data set is a relatively large sized problem. It consists of 8124 samples belonging to 2 classes. Each sample is represented by 22 features and the associated class label.
 We test the performance of SBFCs using the seven different reference functions in Table 1, namely Symmetric triangle, Gaussian, Cauchy, Laplace, Asymmetric triangle, Trapezoid and Quadratic. Com-parisons with PDFCs using Symmetric triangle, Gaussian, Cauchy and Laplace reference functions are given.
 10%}. For the mushroom data set, the reduce rate r is set to be 1%. The parameter a for the trapezoid and compute the average validation rate. The maximal average validation rate and the average number provided for comparison purpose: validation rate 95.44% with 133.6 support vectors using ionosphere validation rate 94.72% with 1667 support vectors using mushroom data set.

For the SBFC, all seven reference functions achieve similar validation rates. It is worthwhile noting that the asymmetric triangle reference function gives good results. However, an asymmetric function usually has more parameters than a symmetric function. The parameter learning of asymmetric reference smaller number of fuzzy rules than the PDFC. 6. Conclusion
In this paper, we establish a link between smooth support vector machines and fuzzy rule-based clas-sification systems, and propose a smooth support vector learning approach to construct standard binary tions on reference functions, and the resulting SBFC has better generalization with reduced number of all membership functions associated with an input variable are generated from the same reference func-tion can be relaxed. 2). The parameter learning of asymmetric reference functions is important to the performance of the SBFC. 3). Our work can be extended to fuzzy modeling. 4). Although the SBFC most cases. An interpretable rule system can be designed on top of it.
 Acknowledgements
The authors would like to thank Dr. Yuh-Jye Lee and Dr. Yixin Chen for providing the source code of their algorithms, and the anonymous reviewers for their helpful comments which led to the improve-ments of this paper.
 References
