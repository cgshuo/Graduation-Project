 Betweenness centrality ( BWC ) is a fundamental centrality mea-sure in social network analysis. Given a large-scale network, how can we find the most central nodes? This question is of great importance to many key applications that rely on BWC , including community detection and understanding graph vulnerability. Despite the large amount of work on scalable approximation algorithm design for BWC , estimat-ing BWC on large-scale networks remains a computational challenge.

In this paper, we study the Centrality Maximization prob-lem ( CMP ): given a graph G = ( V , E ) and a positive integer k , find a set S  X   X  V that maximizes BWC subject to the car-dinality constraint | S  X  | X  k . We present an efficient random-ized algorithm that provides a ( 1  X  1/ e  X  e ) -approximation with high probability, where e &gt; 0. Our results improve the current state-of-the-art result [40]. Furthermore, we provide the first theoretical evidence for the validity of a crucial as-sumption in betweenness centrality estimation, namely that in real-world networks O ( | V | 2 ) shortest paths pass through the top-k central nodes, where k is a constant. This also explains why our algorithm runs in near linear time on real-world networks. We also show that our algorithm and anal-ysis can be applied to a wider range of centrality measures, by providing a general analytical framework.

On the experimental side, we perform an extensive ex-perimental analysis of our method on real-world networks, demonstrate its accuracy and scalability, and study differ-ent properties of central nodes. Then, we compare the sam-pling method used by the state-of-the-art algorithm with our method. Furthermore, we perform a study of BWC in time evolving networks, and see how the centrality of the central nodes in the graphs changes over time. Finally, we compare the performance of the stochastic Kronecker model [28] to real data, and observe that it generates a similar growth pat-tern.
 H.2.8 [ Database Management ]: Database Applications X  Data mining ; G.2.2 [ Discrete Mathematics ]: Graph Theory X  Graph algorithms Social Network, Centrality, Sampling, Optimization
Betweenness centrality ( BWC ) is a fundamental measure in network analysis, measuring the effectiveness of a ver-tex in connecting pairs of vertices via shortest paths [16]. Numerous graph mining applications rely on betweenness centrality, such as detecting communities in social and bio-logical networks [20] and understanding the capabilities of an adversary with respect to attacking a network X  X  connec-tivity [23]. The betweenness centrality of a node u is defined as where  X  s , t is the number of s -t shortest paths, and  X  s , t the number of s -t shortest paths that have u as their inter-nal node. However, in many applications, e.g. [20, 23], we are interested in centrality of sets of nodes. For this reason, the notion of BWC has been extended to sets of nodes [22, 40]. For a set of nodes S  X  V , we define the betweenness centrality of S as where  X  s , t ( S ) is the number of s -t shortest paths that have an internal node in S . Note that we cannot obtain B ( S ) from the values { B ( v ) , v  X  S } . In this work, we study the Centrality Maximization problem ( CMP ) defined formally as follows:
D efinition 1 ( CMP ). Given a network G = ( V , E ) and a positive integer k, find a subset S  X   X  V such that We also denote the maximum centrality of a set of k nodes by OPT i.e., OPT k = max
It is known that CMP is APX-complete [15]. The best de-terministic algorithms for CMP rely on the fact that BWC is monotone-submodular and provide a ( 1  X  1/ e ) -approximation [15, 12]. However, the running time of these algorithms is at least quadratic in the input size, and do not scale well to large-scale networks.

Finding the most central nodes in a network is a compu-tationally challenging problem that we are able to handle accurately and efficiently. In this paper we focus on scal-ability of CMP , and graph mining applications. Our main contributions are summarized as follows.
 Efficient algorithm. We provide a randomized approxima-tion algorithm, HEDGE , based on sampling shortest paths, for accurately estimating the BWC and solving CMP . Our algo-rithm is simple, scales gracefully as the size of the graph grows, and improves the previous result [40], by (i) pro-viding a ( 1  X  1/ e  X  e ) -approximation, and (ii) smaller sized samples. Specifically, in Yoshida X  X  algorithm [40], a sample contains all the nodes on  X  X ny X  shortest path between a pair, whereas in our algorithm, each sample is just a set of nodes from a single shortest path between the pair.
 The OPT k =  X  ( n 2 ) assumption. Prior work on BWC esti-mation strongly relies on the assumption that OPT k =  X  ( n for a constant integer k [40]. As we show in Appendix, this assumption is not true in general. Only empirical evidence so far supports this strong assumption.

We show that two broad families of networks satisfy this assumption: bounded treewidth networks and a popular family of stochastic networks that provably generate scale-free, small-world graphs with high probability. Note that the classical Barab X si-Albert scale-free random tree model [5, 30] belongs to the former category. Our results imply that the OPT k =  X  ( n 2 ) assumption holds even for k = 1, for these families of networks. To our knowledge, this is the first theoretical evidence for the validity of this crucial assumption on real-world networks.
 General analytical framework. To analyze our algorithm, HEDGE , we provide a general analytical framework based on Chernoff bound and submodular optimization, and show that it can be applied to any other centrality measure if it (i) is monotone-submodular, and (ii) admits a hyper-edge sam-pler (defined in Sect. 3). Two examples of such centralities are the coverage [40] and the  X  -path centralities [2]. Experimental evaluation. We provide an experimental eval-uation of our algorithm that shows that it scales gracefully as the graph size increases and that it provides accurate es-timates. We also provide a comparison between the method in [40] and our sampling method.
 Applications. Our scalable algorithm enables us to study some interesting characteristics of the central nodes. In par-ticular, if S is a set of nodes with high BWC , we focus to answer the following questions. (1) How does the centrality of the most central set of nodes (2) Influence maximization has received a lot of attention (3) We study four strategies for attacking a network us-Centrality measures. There exists a wide variety of cen-trality measures: degree centrality, Pagerank [33], HITS [26], Salsa [27], closeness centrality [6], harmonic centrality [7], betweenness centrality [16], random walk betweenness cen-trality [32], coverage centrality [40],  X  -path centrality [2], Katz centrality [24], rumor centrality [37] are some of the important centrality measures. Boldi and Vigna proposed an axiomatic study of centrality measures [7]. In general, choosing a good centrality measure is application depen-dent [19]. In the following we discuss in further detail the centrality measure of our focus, the betweenness centrality. Betweenness centrality ( BWC ) is a fundamental measure in network analysis. The betweenness centrality index is attributed to Freeman [16]. BWC has been used in a wide variety of graph mining applications. For instance, Girvan and Newman use BWC to find communities in social and biological networks [20]. In a similar spirit, Iyer et al . use BWC to attack the connectivity of networks by iteratively removing the most central vertices [23].

The fastest known exact algorithm for computing BWC exactly requires O ( mn ) time in unweighted, and O ( nm + n log m ) for weighted graphs [10, 14, 36]. There exist ran-domized algorithms [4, 9, 34] which provide either additive error or multiplicative error guarantees with high probabil-ity.

For CMP , the state-of-the-art algorithm [40] (and the only scalable proposed algorithm based on sampling) provides a mixed error guarantee, combining additive and multiplica-tive error. Specifically, this algorithm provides a solution whose centrality is at least ( 1  X  1 e ) OPT k  X  e n 2 , by sampling O ( log n / e 2 ) hyper-edges , where each hyper-edge is a set of all nodes on any shortest path between two random nodes with some assigned weights.

As we mentioned before, CMP is APX-complete, and the best algorithm (i.e. classic greedy algorithm for maximizing the monotone-submodular functions) using exact computa-tions of BWC provides ( 1  X  1/ e ) -approximation [15]. We call this greedy algorithm by EXHAUST algorithm, and works as follows: It starts with an empty set S . Then, at any round, it selects a node u that maximizes the adaptive betweenness centrality ( A-BWC ) of u according to S defined as where  X  st ( u | S ) is the number of s -t shortest paths that do not pass through any node in S and have u as an internal node. The algorithm adds u to S and stops when | S | = k .
Note that the A-BWC is intimately connected to the BWC through the following well-known formula [40]: In this section we provide our algorithm, HEDGE (Hyper-EDge GrEedy), and a general framework for its analysis. We start by defining a hyper-edge sampler , that will be used in HEDGE .

D efinition 2 (H yper -edge sampler ). We say that an al-gorithm A is a hyper-edge sampler for a function C : 2 V  X  it outputs a randomly generated subset of nodes h  X  V such that where  X  is a normalizing factor, and independent of the set S. We call each h (sampled by A ) a random hyper-edge, or in short, a hyper-edge . In this case, we say C admits a hyper-edge sampler.
Our proposed algorithm HEDGE assumes the existence of a hyper-edge sampler and uses it in a black-box manner. Namely, HEDGE is oblivious to the specific mechanics of the hyper-edge sampler. The following lemma provides a sim-ple hyper-edge sampler for BWC .
 L emma 1. The BWC admits a hyper-edge sampler.
 P roof . Let A be an algorithm that selects two nodes s , t  X  V uniformly at random, selects a s -t shortest path P uni-formly at random (this can be done in linear time O ( m + n ) using bread-first-search from s , counting the number of shortest paths from s and backward pointers; e.g. see [34]), and finally outputs the internal nodes of P (i.e., the nodes of P except s and t ).

Now, suppose h is an output of A . Since the probability of choosing each pair is 1 n ( n  X  1 ) , and for a given pair s , t the Also note that in this case, the normalizing factor is n ( n  X  1 ) =  X  ( n 2 ) .

For a subset of nodes S  X  V , and a set H of independently generated hyper-edges, denote The pseudocode of our proposed algorithm HEDGE is given in Algorithm 1. First, it samples q hyper-edges using the hyper-edge sampler A and then it runs a natural greedy procedure on H .
In this section we provide our general analytical frame-work for HEDGE , which works with any hyper-edge sam-pler. To start, define B H ( S ) =  X  |H| deg H ( S ) as the centrality ( BWC ) of a set S according to the sample H of hyper-edges, and for a graph G let where n is the number of nodes in G , and ` is a positive integer. We have the following lemma:
L emma 2. Let H be a sample of independent hyper-edges such that |H|  X  q ( G , e ) . Then, for all S  X  V where | S |  X  k we have Pr ( | B H ( S )  X  B ( S ) | X  e  X  OPT k ) &lt; n  X  ` . Algorithm 1: HEDGE
P roof . Suppose S  X  V and | S | X  k , and let X i be a binary random variable that indicates whether the i -th hyper-edge in H intersects with S . Notice that deg H ( S ) =  X  |H| by the linearity of expectation E ( deg H ( S ) ) = |H| X  E
B ( S ) . Using the independence assumption and the Cher-noff bound, we obtain: Now, by letting  X  = e OPT k B ( S ) and substituting the lower bound for q ( G , e ) we obtain and by taking a union bound over all possible subsets S  X  V of size k we obtain | B H ( S )  X  B ( S ) | &lt; e  X  OPT k ity at least 1  X  1/ n ` , for all such subsets S .

Now, the following theorem shows that if the number of samples, i.e. |H| , is at least q ( G , e /2 ) , then HEDGE provides a ( 1  X  1/ e  X  e ) -approximate solution.

T heorem 1. If H is a sample of at least q ( G , e /2 ) hyper-edges for some e &gt; 0 , and S is the output of HEDGE , we have B ( S )  X  ( 1  X  1/ e  X  e ) OPT k , with high probability.
 P roof . Note that B is (i) monotone since if S 1  X  S 2 then B ( S 1 )  X  B ( S 2 ) , and (ii) submodular since if S u  X  V \ S 2 then B ( S 2  X  { u } )  X  B ( S 2 )  X  B ( S 1  X  { u } )  X  B ( S
Similarly, B H is monotone and submodular. Therefore, using the greedy algorithm (second part of HEDGE ) we have (see [31]) where Notice that OPT k = B ( S  X  ) . Since |H| X  q ( G , e /2 ) , by Lemma 2 with probability 1  X  1 n ` we have where we used the fact B ( S  X  ) = OPT k , and the proof is com-plete.

The total running time of HEDGE depends on the running time of the hyper-edge sampler and the greedy procedure. Specifically, the total running time is O ( t he  X |H| + ( n log ( n ) + |H| )) , where t he is the expected required amount of time for the sampler to output a single hyper-edge. The first term corresponds to the total required time for sampling, and the second term corresponds to an almost linear time implemen-tation of greedy procedure as in [8].

R emark 1. Note that if OPT k =  X  ( n 2 ) , the sample complex-ity in Theorem 1 becomes O k log ( n ) retical study on this assumption in Sect. 4.

Finally, we provide a lower bound on the sample complex-ity of HEDGE , in order to output a ( 1  X  1/ e  X  e ) -approximate solution. This lower bound is still valid even if OPT  X  ( n 2 ) .
 T heorem 2. In order to output a set S of size k such that B ( S )  X  ( 1  X  1/ e  X  e ) OPT k w.h.p., the sample size in both HEDGE and [40] X  X  algorithm needs to be at least O n P roof . Define a graph A = ( V A , E A ) where and two nodes ( i , j ) and ( i 0 , j 0 ) are connected if i = i j = j 0 1 . Note that the distance between every pair of nodes is at most 2, and there are at most 2 shortest paths between a pair of nodes in A . Let G be a graph of size n which has ( 1  X  e ) n isolated nodes and A as its largest connected components. We have the following lemma:
L emma 3. If k = e n, then OPT k = 1 ) =  X  ( e 2 n 2 ) =  X  ( n 2 ) , since e is a constant.
P roof of the lemma . Obviously, all the isolated nodes in G have zero BWC . So, the optimal set, S  X  , is A (which is already of size k = e n ). Now, if for two nodes s , t in G , there is a shortest path with an internal node in A , we have s , t  X  V A such that s = ( i , j ) and t = ( i 0 , j 0 ) where i 6 = i j 6 = j 0 . In this case, there are exactly two s -t shortest paths with exactly 1 internal nodes. Finally, the number of such pairs is exactly
Now note that in both HEDGE and the algorithm of [40], we first choose a pair of nodes in s , t in G , and if s and t are not in the same connected component, the returned hyper-edge is an empty set. Therefore, in order to have a non-empty hyper-edge both nodes should be chosen from V which occurs with probability e 2 . Thus, sampling o ( n /
Without loss of generality we can assume e integers, as the arguments still hold after rounding them to the closest integers. hyper-edge results in reaching to at most o ( n ) = o ( | A | ) = o ( k ) nodes, and so, the algorithm will not be able to tell the difference between the isolated nodes and many (arbitrarily large) number of nodes in A as they were not detected by any hyper-edge.
 R emark 2. Theorem 2 implies that the number of samples M = O ( log ( n ) / e 2 ) as claimed in [40] is not sufficient.
Suppose C : 2 V  X  R  X  0 is a centrality measure that is also defined on subset of nodes. Clearly, if C is monotone-submodular and admits a hyper-edge sampler, the algo-rithm HEDGE can be applied to and all the results in this sec-tion hold for C . Here, we give a couple of examples of such centrality measures, and it is easy to verify their monotonic-ity and submodularity.
 Coverage centrality. The coverage centrality [40] for a set S  X  V is defined as C ( S ) =  X  ( s , t )  X  V 2 P s , t ( S ) , where P is 1 if S has an internal node on any s -t shortest path, and 0 otherwise. The coverage centrality admits a hyper-edge sampler A as follows: uniformly at random pick two nodes s and t . By running a breadth-first-search from s , we output every node that is on at least one shortest path from s to t . Note that for every subset of nodes Pr h  X  X  ( h  X  S 6 =  X  -Path centrality Alahakoon et al. introduced the  X  -path centrality of a node [2].Their notion naturally generalizes to any subset of nodes as C ( S ) =  X  s  X  V P s  X  ( S ) , where P the probability that a random simple path of length  X  starting at s will pass a node in S : a random simple path starting at node s is generated by running a random walk that al-ways chooses an unvisited neighbor uniformly at random, and stops after  X  of edges being traversed or if there is no unvisited neighbor. Note that  X  -path centrality is a general-ization of degree centrality by letting  X  = 1 and considering sets of single nodes.

Obviously,  X  -path centrality admits a hyper-edge sampler based on its definition: Let A be an algorithm that picks a node uniformly at random, and generates a random simple path of length at most  X  , and outputs the generated simple path as a hyper-edge. Therefore, for any subset S we have Pr h  X  X  ( h  X  S 6 =  X  ) = 1 n  X  s  X  V P s  X  ( S ) = 1 n C ( S ) .
Recall that all additive approximation guarantees for BWC as well as all existing approximation guarantees for A-BWC involve an error term which grows as  X  ( n 2 ) . In this Section we provide strong theoretical evidence in favor of the fol-lowing question:  X  X hy does prior work which relies heavily on the strong assumption that OPT k =  X  ( n 2 ) perform well on real-world networks? X  We quote Yoshida [40]: This addi-tive error should not be critical in most applications, as numerous real-world graphs have vertices of centrality  X  ( n 2 ) .
We show that the OPT k =  X  ( n 2 ) assumption holds for two important classes of graphs: graphs of bounded treewidth networks, and for certain stochastic graph models that gen-erate scale-free and small-world networks, known as random Apollonian networks [17].
We start by defining the treewidth of a graph:
D efinition 3 (T reewidth ). For an undirected graph G = ( V , E ) , a tree decomposition is a tree T with nodes V where each V i is (assigned to) a subset of V such that (i) for every u  X  V there exists at least an i where u  X  V i , (ii) if V V j both contains a node u, then u belongs to every V unique shortest path from V i to V j in T, and (iii) for every edge ( u , v )  X  E there exists a V i such that u , v  X  V i . The width of the tree decomposition T is defined as max 1  X  i  X  r | V i treewidth of the graph G is the minimum possible width of any tree decomposition of G.
 Now, we have the following theorem.

T heorem 3. Let G = ( V , E ) be an undirected, connected graph of bounded treewidth. Then OPT k =  X  ( n 2 ) .
P roof . Suppose w is the treewidth of G , which is a con-stant (bounded). It is known that any graph of treewidth w has a balanced vertex separator 2 S  X  V of size at most w + 1 [35]. This implies that O ( n 2 ) shortest paths pass through S . Since | S | = w + 1 =  X  ( 1 ) , there exists at least one vertex u  X  S such that B ( u ) =  X  ( n 2 ) . Hence, OPT 1 =  X  ( n since OPT 1  X  OPT k we have OPT k =  X  ( n 2 ) .

It is worth emphasizing that the classical Barab X si-Albert random tree model [5, 30] belongs to this category. For a recent study of the treewidth parameter on real-world net-works, see [1].
We show that OPT k =  X  ( n 2 ) for random Apollonian net-works. Our proof for the latter model relies on a technique developed by Frieze and Tsourakakis [17] and carries over for random unordered increasing k -trees [18]. A random Apollonian network (RAN) is a network that is generated iter-atively. The RAN generator takes as input the desired num-ber of nodes n  X  3 and runs as follows:  X  Let G 3 be the triangle graph, whose nodes are { 1, 2, 3 } ,  X  for t  X  4 to n :
Figure 1(a) shows an instance of a RAN for n = 100. The triangle is originally embedded on the plane as an equi-lateral triangle. Also, when a new node t chooses its face ( i , j , k ) it is embedded in the barycenter of the corresponding and ( k , t ) . It has been shown that the diameter of a RAN is O ( log ( n )) with high probability [17, 13].

At any step, we refer to set of candidate faces as the set of active faces. Note that there is a bijection between the active faces of a RAN and the leaves of random ternary trees as illustrated in Figure 1(b), and noticed first by [17].
We shall make use of the following formulae for the num-ber of nodes ( v t ), edges ( e t ) and faces ( f t ; excluding the outer face) after t steps in a RAN G t :
Means a set of nodes  X  such that V \  X  = A  X  B , where A and B are disjoint and both have size  X  ( n ) . Figure 1: (a) An instance of a random Apollonian net-work for n = 100. (b) Bijection between RANs and random ternary trees.
 Note that f t =  X  ( v t ) =  X  ( t ) .

T heorem 4. Let G be a RAN of size n. Then OPT k =  X  ( n 2
P roof . Note that removing a node from the random ternary tree T = ( V T , E T ) (as in Fig. 1(b)) corresponds to removing three nodes from G , corresponding to a face F that existed during the RAN generation process. Clearly, the set of these three nodes is a vertex separator that separates the nodes in-side and outside of F . Therefore, all nodes in the tree except for the root r correspond to a vertex separator in G . Observe that the leaves in T correspond to the set of active faces, and thus, T has f n = 2 n  X  5 leaves after n steps.

We claim that there exists an edge ( F , F 0 )  X  E T (recall that the nodes of T are active faces during the generating pro-cess of G ) such that the removal of e from E T results in two subtrees with  X  ( n ) leaves. We define g : V T  X  Z to be the function that returns for a node v  X  V T the number of leaves of the subtree rooted at v . Hence, g ( r ) = 2 n  X  5, g ( u ) = 1 for any leaf u . To find such an edge consider the follow-ing algorithm. We start from the root r of T descending to the bottom according to the following rule which creates a sequence of nodes u 0 = r , u 1 , u 2 , . . .: we set u child with most leaves among the tree subtrees rooted at the three children of u i . We stop when we first find a node u such that g ( u i )  X  cn and g ( u i + 1 ) &lt; cn for some constant c . Clearly, g ( u i + 1 )  X  cn /3 =  X  ( n ) , by pigeonhole principle. So, let F = u i and F 0 = u i + 1 .

Now suppose F 0 = { x , y , z } , and consider removing x , y , and z from G . Clearly, F 0 6 = r as F 0 is a child of F . Also, due to the construction of a RAN, after removing x , y , z , there are exactly two connected components, G 1 and G 2 . Also, since cn /3  X  g ( F 0 ) &lt; cn , the number of nodes in each of G G 2 is  X  ( n ) .

Finally, observe that at least one of the three nodes x , y , z must have betweenness centrality score  X  ( n 2 ) , as the size of the separator is 3 and there exist  X  ( n 2 ) paths that pass through it (connecting the nodes in G 1 and in G 2 ). Therefore, OPT 1  X  max { B ( x ) , B ( y ) , B ( z ) } =  X  ( n 2 ) , and since OPT OPT k we have OPT k =  X  ( n 2 ) .

We believe that this type of coupling can be used to prove similar results for other stochastic graph models.
In this section we present our experimental results. We first start by comparing HEDGE (our sampling based algo-rithm) with EXHAUST (the exhaustive algorithm defined in Sect. 2) and show that the centrality of HEDGE  X  X  output is close to the centrality of EXHAUST  X  X  output, with great speed-up. This part is done for 3 small graphs as EXHAUST cannot scale to larger graphs.
We then compare our sampling method with the method presented in [40]. We show that, although our method stores less per each hyper-edge, it does not loose its accu-racy.

Equipped with our scalable algorithm, HEDGE , we will be able to focus on some of the interesting characteristics of the central nodes: (i) How does their centrality change over time in evolving graphs? (ii) How influential are they? and (iii) How does the size of the largest connected component change after removing them?
In our experiments, we assume the graphs are simple (no self-loop or parallel edge) but the edges can be directed. We used publicly available datasets in our experiments 3 . HEDGE is implemented in C++.
Table 1 shows the results of EXHAUST and HEDGE on three graphs for which we were able to run EXHAUST . The fact that EXHAUST is able to run only on networks of this scale indi-cates already the value of HEDGE  X  X  scalability. As we can see, HEDGE results in significant speedups and negligible loss of accuracy.

In Table 1 the centrality of the output sets and the speed up gained by HEDGE is given, and as shown, HEDGE gives a great speedup with almost the same quality (i.e., the central-ity of the output) of EXHAUST . The centrality of the outputs are scaled by 1 n ( n  X  1 ) , where n is the number of nodes in each graph. Motivated by the result in Sect. 4 we run HEDGE using k log ( n ) / e 2 hyper-edges for e = 0.1, and for each case, ten times (averages are reported). For sake of comparison, these experiments were executed using a single machine with In-tel Xeon cpu at 2.83GHz and with 36GB ram .
 We compare our method against Yoshida X  X  algorithm ( Y-ALG ) [40] on four undirected graphs (as Y-ALG runs on undi-rected graphs). We use Yoshida X  X  implementation which he kindly provided to us. Note that Yoshida X  X  algorithm ap-plies a different sampling method than ours: it is based on sampling random s -t pairs of nodes and assigning weights to every node that is on any s -t shortest path, whereas in our method we only pick one randomly chosen s -t shortest path with no weight on the nodes.
 spectively, where n is the number of nodes in the graph, and we set e = 0.1. We also run a variation of our algo-rithm, HEDGE = , which is essentially HEDGE but with 2 log ( 2 n 3 ) http://snap.stanford.edu and http://konect.uni-koblenz. de/networks/dblp_coauthor samples. This allows a more fair comparison between the methods.

Table 2 shows the estimated centrality of the output sets, and the number of samples each algorithm uses. Surpris-ingly, Y-ALG does not outperform HEDGE = , despite the fact that it maintains extra information. Finally, our proposed algorithm HEDGE is consistently better than the other two al-gorithms.

For the next three experiments, we consider 3 more larger graphs that HEDGE can handle due to its scalability: email-Enron, loc-Brightkite, and soc-Epinion1, with 36692-183831, 58228-214078, and 75879-508837 number of nodes-edges , re-spectively. These experiments are based on orders defined over the set of nodes as follows: we generate 100 log ( n ) / hyper-edges, where n is the number of nodes, and e = 0.25. Then we order the nodes based on the order HEDGE picks the nodes.

For sake of comparison, we ran HEDGE using the coverage and  X  -path (for  X  = 2) centralities, since both of them ad-mit hyper-edge sampler as we showed in Sect. 3.2. Also, we considered a fourth centrality that we call triangle cen-trality, where the centrality of a set of nodes S equals to the number of triangles that intersect with S . For the triangle centrality, we run EXHAUST as computing this centrality is easy and scalable to large graphs 4 . All these experiments are run ten times, and we report the average values. Time evolving networks. Leskovec et al. studied empir-ically properties of time-evolving real-world networks [29]. In this section we investigate how BWC of the most central nodes changes as a function of time.

We study two temporal datasets, the DBLP 5 and Au-tonomous Systems (AS) datasets. We also generate stochastic Kronecker graphs on 2 i vertices for i  X  { 8, . . . , 20 } , using 0.5 0.2 as the core-periphery seed matrix. We assume that the i -th time snapshot for Kronecker graphs corresponds to 2 vertices, for i = 8, . . . , 20. Note that in these evolving sets, the number of nodes also increases along with new edges. Also, note that the main difference between DBLP and Autonomous Systems is that for DBLP edges and nodes only can be added, where in Autonomous Systems nodes and edges can be increased and decreased.
EXHAUST for the triangle centrality, at every iteration simply chooses a node that is incident with more number of new triangles.
Timestamps are in Unix time and can be negative.
The results are plotted in logarithmic scale (Fig. 2), and as shown, we observe that the centrality of the highly cen-tral set of nodes increases. Also, we observe that the model of stochastic Kronecker graphs behaves similar to the real-world evolving networks with respect to these parameters. Figure 2: Largest betweenness centrality score and number of Influence maximization. We consider the Independent Cas-cade model [25], where each edge has the probability 0.01 of being active. For computing and maximizing the influ-ence , we consider the algorithm of [8] using 10 of samples (called hyper-edge but defined differently). We compute the influence of output of HEDGE with output of [8]. As shown in Table 3, and as we observe, the central nodes also have high influence, which shows a great correlation be-tween being highly central and highly influential. It is worth outlining that our main point is to show that our proposed algorithm can be used to scale heuristic uses of BWC . Table 3: Comparing the influence of influential nodes (IM) and cen-Graph attacks. It is a well-known fact that scale-free net-works are robust to random failures but vulnerable to tar-geted attacks [3]. Some of the most efficient strategies for at-tacking graph connectivity is based on removing iteratively the most central vertex in the graph [21]. Such sequential attacks are central in studying the robustness of the Internet and biological networks such as protein-protein interaction networks [23].

We remove the nodes one-by-one (according to the order induced by these centralities and picked by HEDGE ) and mea-sure the size of the largest connected component. The results are plotted in Fig. 3. Our observation is that all the sizes of largest connected components decline significantly (almost linearly in size of S ), which is compatible with our intuition of centralities. We also find that the  X  -path and triangle cen-tralities can be more effective at destroying the connectivity.
In this work, we provide HEDGE , a scalable algorithm for the (betweenness) Centrality Maximization problem, with theoretical guarantees. We also provide a general analyt-ical framework for our analysis which can be applied to any monotone-submodular centrality measure that admits a hyper-edge sampler. We perform an experimental anal-ysis of our method on real-world networks which shows that our algorithm scales gracefully as the size of the graph grows while providing accurate estimations. Finally, we study some interesting properties of the most central nodes.
A question worth investigating is whether removing nodes in the reserve order, namely by starting from the least cen-tral ones, produces a structure-revealing permutation, as it happens with peeling in the context of dense subgraph dis-covery [11, 38].
This work was supported in part by NSF grant IIS-1247581 and NIH grant R01-CA180776. [1] A. B. Adcock, B. D. Sullivan, and M. W. Mahoney. Tree Figure 3: The size of the largest connected component, as we re-[2] T. Alahakoon et al. K-path centrality: A new centrality [3] R. Albert, H. Jeong, and A.-L. Barab X si. Error and [4] D. A. Bader, S. Kintali, K. Madduri, and M. Mihail. [5] A.-L. Barab X si and R. Albert. Emergence of scaling in [6] A. Bavelas. A mathematical model for group [7] P. Boldi and S. Vigna. Axioms for centrality. Internet [8] C. Borgs et al. Maximizing social influence in nearly [9] C. Brandes and C. Pich. Centrality estimation in large [10] U. Brandes. A faster algorithm for betweenness [11] M. Charikar. Greedy approximation algorithms for [12] S. Dolev, Y. Elovici, R. Puzis, and P. Zilberman. [13] E. Ebrahimzadeh, L. Farczadi, P. Gao, A. Mehrabian, [14] D. Erdos, V. Ishakian, A. Bestavros, and E. Terzi. A [15] M. Fink and J. Spoerhase. Maximum betweenness [16] L. C. Freeman. A set of measures of centrality based [17] A. Frieze and C. E. Tsourakakis. Some properties of [18] Y. Gao. The degree distribution of random k-trees. [19] R. Ghosh, S. Teng, K. Lerman, and X. Yan. The [20] M. Girvan and M. E. Newman. Community structure [21] P. Holme, B. J. Kim, C. N. Yoon, and S. K. Han. Attack [22] V. Ishakian, D. Erd X s, E. Terzi, and A. Bestavros. A [23] S. Iyer, T. Killingback, B. Sundaram, and Z. Wang. [24] L. Katz. A new status index derived from sociometric [25] D. Kempe, J. Kleinberg, and  X . Tardos. Maximizing the [26] J. M. Kleinberg. Authoritative sources in a hyperlinked [27] R. Lempel and S. Moran. Salsa: the stochastic [28] J. Leskovec, D. Chakrabarti, J. Kleinberg, and [29] J. Leskovec, J. Kleinberg, and C. Faloutsos. Graph [30] T. F. M X ri. The maximum degree of the barab X si X  X lbert [31] G. L. Nemhauser and L. A. Wolsey. Best algorithms for [32] M. E. Newman. A measure of betweenness centrality [33] L. Page, S. Brin, R. Motwani, and T. Winograd. The [34] M. Riondato and E. M. Kornaropoulos. Fast [35] N. Robertson and P. D. Seymour. Graph minors. ii. [36] A. E. Sar X y X ce, E. Saule, K. Kaya, and  X . V.  X ataly X rek. [37] D. Shah and T. Zaman. Rumor centrality: a universal [38] C. Tsourakakis. The k-clique densest subgraph [39] D. B. West. Introduction to graph theory , volume 2. [40] Y. Yoshida. Almost linear-time algorithms for adaptive
Here, we present two examples in which the assumption of OPT k =  X  (  X  ) does not hold. However, it still remains as an interesting open problem to see what properties of the graphs result in this assumption.
 Complete Graph Obviously the centrality of each set of nodes, both for Betweenness and Coverage centralities, is zero. Thus OPT k = o ( n 2 ) 6 =  X  ( n 2 ) . Note that in Betweenness and Coverage centrality we had  X  = n ( n  X  1 ) =  X  ( n 2 ) . Hypercube The hypercube Q r (with n = 2 r nodes) is a vertex-transitive graph , i.e., for each pair of nodes there is an automorphism that maps one onto the other [39]. So, the centrality of the nodes are the same.

First consider the coverage centrality , and lets count how many pairs of nodes have a shortest path that pass the node ( 0, . . . , 0 ) . Note that a , b  X  { 0, 1 } r have a shortest path that passes the origin if and only if  X  i  X  1, . . . , r : a i count the number of such pairs, we have to first choose a subset I  X  { 1, . . . , r } of the bits that are non-zero either in a or b , in ( r | I | ) ways, and partition the bits of I between a and b (in 2 | I | ways). Therefore, the number of ( a , b )  X  V that their shortest path passes the node ( 0, . . . , 0 ) is So, the maximum coverage centrality of a node is at most n log ( 3 ) (since we counted the endpoints as well, but should not have). Now by submodularity of the coverage centrality we have OPT k  X  kn log ( 3 ) = O ( n 1 + log ( 3 ) ) = o ( n
Finally, since the betweenness centrality is no more than the coverage centrality, we have the similar result for be-tweenness centrality as well.
