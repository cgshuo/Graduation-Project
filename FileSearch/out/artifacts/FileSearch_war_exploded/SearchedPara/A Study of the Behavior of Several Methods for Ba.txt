 There are several aspects that might influence the perfor-mance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbal-ance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evalu-ation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the pres-ence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in gen-eral, over-sampling methods provide more accurate results than under-sampling methods considering the area under the ROC curve (AUC). This result seems to contradict re-sults previously published in the literature. Two of our pro-posed methods, Smote + Tomek and Smote + ENN, pre-sented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees in-duced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods. Most learning systems usually assume that training sets used for learning are balanced. However, this is not always the case in real world data where one class might be represented by a large number of examples, while the other is represented by only a few. This is known as the class imbalance problem and is often reported as an obstacle to the induction of good classifiers by Machine Learning (ML) algorithms. Generally, the problem of imbalanced data sets occurs when one class represents a circumscribed concept, while the other class represents the counterpart of that concept, so that examples from the counterpart class heavily outnumber examples from the positive class. This sort of data is found, for example, in medical record databases regarding a rare disease, were there is a large number of patients who do not have that disease; continuous fault-monitoring tasks where non-faulty examples heavily outnumber faulty examples, and others. In recent years, there have been several attempts at dealing with the class imbalance problem in the field of Data Mining and Knowledge Discovery in Databases, to which ML is a substantial contributor. Related papers have been published in the ML literature aiming to overcome this problem. The ML community seems to agree on the hypothesis that the imbalance between classes is the major obstacle in induc-ing classifiers in imbalanced domains. However, it has also been observed that in some domains, for instance the Sick data set [3], standard ML algorithms are capable of induc-ing good classifiers, even using highly imbalanced training sets. This shows that class imbalance is not the only prob-lem responsible for the decrease in performance of learning algorithms.
 In [18] we developed a systematic study aiming to ques-tion whether class imbalances hinder classifier induction or whether these deficiencies might be explained in other ways. Our study was developed on a series of artificial data sets in order to fully control all the variables we wanted to analyze. The results of our experiments, using a discrimination-based inductive scheme, suggested that the problem is not solely caused by class imbalance, but is also related to the degree of data overlapping among the classes.
 The results obtained in this previous work motivated the proposition of two new methods to deal with the problem of learning in the presence of class imbalance. These methods ally a known over-sampling method, namely Smote [5], with two data cleaning methods: Tomek links [22] and Wilson X  X  Edited Nearest Neighbor Rule [24]. The main motivation behind these methods is not only to balance the training data, but also to remove noisy examples lying on the wrong side of the decision border. The removal of noisy examples might aid in finding better-defined class clusters, therefore, allowing the creation of simpler models with better general-ization capabilities.
 In addition, in this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. We concluded that over-sampling methods are able to aid in the induction of classifiers that are more accurate than those induced from under-sampled data sets. This result seems to contradict results previously published in the literature. Two of our proposed methods performed well in practice, in particular for data sets with a small number of positive examples. It is worth noting that Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods.
 The remainder of the paper is organized as follows: Section 2 discusses why learning from imbalanced data sets might be a difficult task. Section 3 describes the drawbacks of using accuracy (or error rate) to measure the performance of clas-sifiers and discusses alternative metrics. Section 4 presents the methods employed in the experimental evaluation, in-cluding the three methods proposed by the authors. Sec-tion 5 discusses the methodology used in the experiments, as well as the results achieved. Finally, Section 6 presents the conclusions and outlines future research. Learning from imbalanced data sets is often reported as being a difficult task. In order to better understand this problem, imagine the situation illustrated in Figure 1. In Fig. 1( a ) there is a large imbalance between the majority class (-) and the minority class (+), and the data set presents some degree of class overlapping. A much more comfortable situation for learning is represented in Fig. 1( b ), where the classes are balanced with well-defined clusters.
 In a situation similar to the one illustrated in Fig. 1( a ), spare cases from the minority class may confuse a classifier like k-Nearest Neighbor (k-NN) . For instance, 1-NN may in-correctly classify many cases from the minority class because the nearest neighbors of these cases are examples belonging to the majority class. In a situation where the imbalance is very high, the probability of the nearest neighbor of a mi-nority class case is a case of the majority class is likely to be high, and the minority class error rate will tend to have high values, which is unacceptable.
 Figure 1: Many negative cases against some spare positive cases ( a ) balanced data set with well-defined clusters ( b ). Decision trees also experience a similar problem. In the pres-ence of class overlapping, decision trees may need to create many tests to distinguish the minority class cases from ma-jority class cases. Pruning the decision tree might not nec-essarily alleviate the problem. This is due to the fact that pruning removes some branches considered too specialized, labelling new leaf nodes with the dominant class on this node. Thus, there is a high probability that the majority class will also be the dominant class of those leaf nodes. The most straightforward way to evaluate the performance of classifiers is based on the confusion matrix analysis. Ta-ble 1 illustrates a confusion matrix for a two class problem having positive and negative class values. From such a matrix it is possible to extract a number of widely used metrics for measuring the performance of learning systems, However, when the prior class probabilities are very differ-ent, the use of such measures might lead to misleading con-clusions. Error rate and accuracy are particularly suspicious performance measures when studying the effect of class dis-tribution on learning since they are strongly biased to favor the majority class. For instance, it is straightforward to create a classifier having an accuracy of 99% (or an error rate of 1%) in a domain where the majority class proportion corresponds to 99% of the examples, by simply forecasting every new example as belonging to the majority class. Another fact against the use of accuracy (or error rate) is that these metrics consider different classification errors to be equally important. However, highly imbalanced problems generally have highly non-uniform error costs that favor the minority class, which is often the class of primary interest. For instance, a sick patient diagnosed as healthy might be a fatal error while a healthy patient diagnosed as sick is considered a much less serious error since this mistake can be corrected in future exams.
 Finally, another point that should be considered when study-ing the effect of class distribution on learning systems is that the class distribution may change. Consider the confusion matrix shown in Table 1. Note that the class distribution (the proportion of positive to negative examples) is the re-lationship between the first and second lines. Any perfor-mance metric that uses values from both lines will be inher-ently sensitive to class skews. Metrics such as accuracy and error rate use values from both lines of the confusion matrix. As class distribution changes, these measures will change as well, even if the fundamental classifier performance does not. All things considered, it would be more interesting if we use a performance metric that disassociates the errors (or hits) that occurred in each class. From Table 1 it is possible to derive four performance metrics that directly measure the classification performance on positive and negative classes independently: False negative rate F N rate = F N T P + F N is the percentage False positive rate F P rate = F P F P + T N is the percentage True negative rate T N rate = T N F P + T N is the percentage True positive rate T P rate = T P T P + F N is the percentage These four performance measures have the advantage of be-ing independent of class costs and prior probabilities. The aim of a classifier is to minimize the false positive and neg-ative rates or, similarly, to maximize the true negative and positive rates. Unfortunately, for most real world applica-tions there is a tradeoff between F N rate and F P rate , and similarly between T N rate and T P rate . ROC (Receiver Op-erating Characteristic) graphs [19] can be used to analyze the relationship between F N rate and F P rate (or T N rate T P rate ) for a classifier.
 Some classifiers, such as the Na  X  X ve Bayes classifier or some Neural Networks, yield a score that represents the degree to which an example is a member of a class. Such ranking can be used to produce several classifiers, by varying the thresh-old of an example pertaining to a class. Each threshold value produces a different point in the ROC space. These points are linked by tracing straight lines through two con-secutive points to produce a ROC curve. For decision trees, we could use the class distributions at each leaf as a score or, as proposed in [9], by ordering the leaves by their positive class accuracy and producing several trees by re-labelling the leaves, one at a time, from all forecasting negative class to all forecasting positive class in the positive accuracy order. A ROC graph characterizes the performance of a binary classification model across all possible trade-offs between the classifier sensitivity ( T P rate ) and false alarm ( F P graphs are consistent for a given problem, even if the distri-bution of positive and negative examples is highly skewed. A ROC analysis also allows the performance of multiple classification functions to be visualized and compared si-multaneously. The area under the ROC curve (AUC) rep-resents the expected performance as a single scalar. The AUC has a known statistical meaning: it is equivalent to the Wilconxon test of ranks, and is equivalent to several other statistical measures for evaluating classification and ranking models [10]. In this work, we use the method pro-posed in [9] with Laplace correction for measuring the leaf accuracy to produce ROC curves. We also use the AUC as the main method for assessing our experiments. This section describes the notation used as well as our im-plementation of the k -NN algorithm, since this algorithm plays an important role in the behavior of the methods con-sidered. Finally, an explanation of each balancing method is given. In supervised learning, the inducer is fed with a data set E = { E 1 , E 2 , . . . E N } , in which each example E i an associated label. This label defines the class the example belongs to. Each example E i  X  E is a tuple E i = ( ~x i , y in which ~x i is a vector of feature (or attribute) values of the example E i , and y i is its class value. The objective of a supervised learning algorithm is to induce a general mapping of vectors ~x to values y . Thus, the learning system aims to construct a model y = f ( ~x ), of an unknown function f , also known as concept function , that enables one to predict the y values for previously unseen examples. In practice, learning systems are able to induce a function h that approximates f , i.e. , h ( ~x )  X  f ( ~x ). In this case, h is called the hypothesis of the concept function f .
 In Table 2 a data set with N examples and M attributes is presented. Columns ( A 1 , . . . A M ) represent the attributes and lines ( E 1 , . . . E N ) represent the examples. For instance, line i in Table 2 refers to the i th example and the entry x refers to the value of the j th attribute, A j , of example i . For classification problems, the class-attribute Y is a qualitative attribute that may assume a set of N Cl discrete values C = { C 1 , C 2 , . . . C N Cl } .
 As stated earlier, in this work we consider two-class prob-lems where C 1 = + represents the circumscribed concept class and C 2 =  X  represents the counterpart of that con-cept. Furthermore, the examples from the negative class outnumber the examples from the positive class. Several research papers use the Euclidean distance as a dis-tance metric for the k -NN algorithm. However, this distance function might not be appropriate when the domain presents qualitative attributes. For those domains, the distance for qualitative attributes is usually calculated using the over-lap function, in which the value 0 (if two examples have the same value for a given attribute) or the value 1 (if these val-ues differ) are assigned. Our implementation of the k -NN algorithm uses the Heterogeneous Value Difference Metric (HVDM) distance function [25]. This distance function uses the Euclidean distance for quantitative attributes and the VDM distance [21] for qualitative attributes. The VDM metric provides a more appropriate distance function for qualitative attributes if compared with the overlap metric, since the VDM metric considers the classification similarity for each possible value of a qualitative attribute to calculate the distances between these values.
 Another refinement to the basic k -NN algorithm is to weigh the contribution of each of the k neighbors according to their distance to the query example E q , giving greater weight to closer neighbors. The vote of each neighbor is weighed ac-cording to the inverse square of its distance from E q [17]. of E q , according to the distance function d the final classifi-cation is given by Equation 1.
 h ( E q ) = arg max and  X  ( a, b ) = 1 if a = b otherwise  X  ( a, b ) = 0. As the balancing methods make severe use of distance com-putations, we implemented an indexing structure namely M-tree [6] to speed up the execution of k -NN queries. M-tree only considers relative distances of examples rather than their absolute positions in a multi-dimensional space, to or-ganize and partition the metric space . In a metric space, example proximity is only defined by a distance function that satisfies the positivity, symmetry and triangle inequal-ity postulates. In this work, we evaluate ten different methods of under and over-sampling to balance the class distribution on training data. Two of these methods, random over-sampling and ran-dom under-sampling, are non-heuristic methods that were initially included in this evaluation as baseline methods. The evaluated methods are described next.
 Random over-sampling is a non-heuristic method that Random under-sampling is also a non-heuristic method Several authors agree that random over-sampling can in-crease the likelihood of occurring overfitting, since it makes exact copies of the minority class examples. In this way, a symbolic classifier, for instance, might construct rules that are apparently accurate, but actually cover one replicated example. On the other hand, the major drawback of ran-dom under-sampling is that this method can discard poten-tially useful data that could be important for the induction process. The remainder balancing methods use heuristics in order to overcome the limitations of the non-heuristic meth-ods.
 Tomek links Tomek links [22] can be defined as follows: Condensed Nearest Neighbor Rule Hart X  X  Condensed One-sided selection One-sided selection (OSS) [14] is an CNN + Tomek links This is one of the methods pro-Neighborhood Cleaning Rule Neighborhood Cleaning Smote Synthetic Minority Over-sampling Technique (Smo-Smote + Tomek links Although over-sampling minority Figure 2: Balancing a data set: original data set ( a ); over-sampled data set ( b ); Tomek links identification ( c ); and borderline and noise examples removal ( d ). Smote + ENN The motivation behind this method is sim-The main objective of our research is to compare several bal-ancing methods published in the literature, as well as the three proposed methods, in order to verify whether those methods can effectively deal in practice with the problem of class imbalance. To make this comparison, we have selected thirteen data sets from UCI [3] which have different degrees of imbalance. Table 3 summarizes the data employed in this study. For each data set, it shows the number of examples (#Examples), number of attributes (#Attributes), number of quantitative and qualitative attributes, class attribute dis-tribution and the majority class error. For data sets having more than two classes, we chose the class with fewer ex-amples as the positive class, and collapsed the remainder as the negative class. As the Letter and Splice data sets have a similar number of examples in the minority classes, we created two data sets with each of them: Letter-a and Letter-vowel, Splice-ie and Splice-ei.
 In our experiments, we used release 8 of the C4.5 symbolic learning algorithm to induce decision trees [20]. Firstly, we ran C4.5 over the original (imbalanced) data sets and calculated the AUC for each data set using 10-fold cross-validation. The results obtained in this initial experiment are shown in a graph in Figure 3.
 Figure 3: Proportion of negative/positive examples versus AUC. Figure 3 plots the proportion of negative/positive examples versus the mean AUC values for the original data sets. If class imbalances can systematically hinder the performance of imbalanced data sets, then it would be expected that AUC decreases for highly imbalanced data sets. However, in spite of a large degree of imbalance the data sets Letter-a and Nursery obtained almost 100% AUC.
 The results obtained in the UCI data sets seem to be com-patible with previous work of the authors [18] conducted on a series of experiments with artificial domains, in which we varied the degree of overlapping between the classes. It was concluded that class imbalance, by itself, does not seem to be a problem, but when allied to highly overlapped classes, it can significantly decrease the number of minority class ex-amples correctly classified. Domains with non-overlapping classes do not seem to be problematic for learning no mat-ter the degree of imbalance. Moreover, in [12] Japkowicz performed several experiments on artificial data sets and concluded that class imbalances do not seem to systemat-ically cause performance degradation. She concludes that the imbalance problem is a relative problem depending on both the complexity of the concept 1 and the overall size of the training set.
 The relationship between training set size and improper clas-sification performance for imbalanced data sets seems to be that on small imbalanced data sets the minority class is poorly represented by an excessively reduced number of ex-amples, that might not be sufficient for learning, especially when a large degree of class overlapping exists and the class is further divided into subclusters. For larger data sets, the effect of these complicating factors seems to be reduced, as the minority class is better represented by a larger number of examples. This trend is confirmed by the graph shown in Figure 4 which shows how the AUC is affected by the number of positive training examples in the data sets. In a second stage, the over and under-sampling methods de-scribed in Section 4 were applied to the original data sets.
Where the  X  X oncept complexity X  corresponds to the num-ber of subclusters into which the classes are subdivided. Figure 4: Number of positive training examples versus AUC. Smote, Random over-sampling, Random under-sampling and CNN methods have internal parameters that allow the user to set up the resulting class distribution obtained after the application of these methods. We decided to add/remove examples until a balanced distribution was reached. This decision is motivated by the results presented in [23], in which it is shown that when AUC is used as performance measure, the best class distribution for learning tends to be near the balanced class distribution.
 The results obtained in our experiments are summarized in Tables 4 and 5. Table 4 shows the performance results for the original, as well as for the over-sampled data sets. Table 5 shows the results obtained for the under-sampled data sets. The performance results are reported in terms of AUC. The numbers between brackets are the corresponding standard deviations. As stated earlier, these results were obtained with 10-fold cross-validation. AUCs were measured over decision trees pruned with the default C4.5 pruning parameter setting (25% confidence level) and over unpruned decision trees.
 Although some research papers state that pruning might be helpful with imbalanced data sets in some circumstances [4], other papers indicate that when target misclassification costs or class distributions are unknown, then pruning should be avoided [26; 2]. One reason to avoid pruning is that most pruning schemes, including the one used by C4.5, attempt to minimize the overall error rate. These pruning schemes can be detrimental to the minority class, since reducing the error rate in the majority class, which stands for most of the examples, would result in a greater impact over the overall error rate. On the other hand, it still seems to be an open-ended question if pruning can lead to a performance im-provement for decision trees grown over artificially balanced data sets. One argument against pruning is that if pruning is allowed to execute under these conditions, the learning system would prune based on false assumption, i.e. , that the test set distribution matches the training set distribu-tion [23].
 Figure 5 shows a comparison of the effect of pruning de-cision trees on the original and balanced data sets. Line x = y represents when both pruned and unpruned decision trees obtain the same AUC. Plots above this line represent that unpruned decision trees obtained better results, and plots under this line the opposite. Figure 5 clearly shows that pruning rarely leads to an improvement in AUC for the original and balanced data sets.
 Figure 5: AUC of pruned versus unpruned decision trees for the original and balanced data sets. In Tables 4 and 5 the results in bold indicate the best AUCs obtained for each data set considering pruned and unpruned decision trees independently. Note that all best results were obtained by the over-sampling methods. In order to facili-tate the analysis of the results, Tables 6 and 7 present these results as a ranking of methods for pruned and unpruned decision trees respectively. The over-sampling methods are highlighted with a light gray color, and the results obtained with the original data sets with a dark gray color. Note that, in general, over-sampling methods are better ranked than the under-sampling methods. Hsu X  X  Multiple Comparison with the Best (MCB) test was performed in order to ver-ify if significant differences exist, with 95% confidence level, among the best ranked method and the remaining methods. The results are also summarized in Tables 6 and 7, where methods marked with an asterisk obtained statistically in-ferior results when compared to the top ranked method. Conversely, over-sampling methods in general and Random over-sampling in particular are well-ranked among the re-mainder methods. This result seems to diverge with several papers previously published in the literature. Drummond and Holte [8] report that when using C4.5 X  X  default settings, over-sampling is surprisingly ineffective, often producing lit-tle or no change in performance in response to modifications of misclassification costs and class distribution. Moreover, they note that over-sampling prunes less and therefore gen-eralizes less than under-sampling, and that a modification of the C4.5 X  X  parameter settings to increase the influence of pruning and other overfitting avoidance factors can re-establish the performance of over-sampling. In our exper-iments, Random over-sampling did not produce overfitted decision trees even when these trees were left unpruned, as it can be confirmed by the higher AUC values obtained by this method for unpruned trees. In addition, under-sampling methods did not perform as well as over-sampling meth-ods, even when heuristics to remove cases were considered in under-sampling.
 Moreover, Domingos [7] reports that concerning concept learning problems, C4.5 Rules produces lower cost classi-fiers using under-sampling than over-sampling. Ling and Li [16] compare over and under-sampling for boosted C4.5 and report that under-sampling produces better lift index, although extreme over-sampling performs almost as well. On the other hand, Japkowicz and Stephen [13] compare several methods of over and under-sampling on a series of artificial data sets and conclude that over-sampling is more effective than under-sampling at reducing error rate. In our opinion, the good results obtained by over-sampling are not completely unexpected. As stated before, it seems that the loss of performance is directly related to the lack of minority class examples in conjunction with other compli-cating factors. Over-sampling is the class of methods that most directly attack the problem of the lack of minority class examples.
 It is worth mentioning that two of our proposed methods, Smote + Tomek and Smote + ENN are generally ranked among the best for data sets with a small number of positive examples. Considering only data sets with less than 100 pos-itive examples (in our experiments there are 6 of them: Flag, Glass, Post-operative, New-thyroid, E.Coli and Haberman), at least one of the proposed methods provided meaningful results for all 6 data sets for pruned trees  X  Table 6, and for 5 of the 6 data sets for unpruned trees  X  Table 7. This seems to indicate that these methods could be appropriate in domains having such conditions.
 Since over-sampling methods, as well as unpruned decision trees obtained very good performance results, further anal-ysis will focus on these results. In addition to classifier per-formance results, we also attempted to measure the syntac-tic complexity of the induced models. Syntactic complexity is given by two main parameters: the mean number of in-duced rules (branches) and the mean number of conditions per rule. Tables 8 and 9 respectively show the mean number of induced rules and the mean number of condition per rule for the over-sampling methods and the original data sets with unpruned decision trees. The best results are shown in bold, and the best results obtained by an over-sampling method, not considering the results obtained in the original data sets, are highlighted with a light gray color. Figure 6 shows the results in Table 8 in graphical form, where it can be observed that over-sampled data sets usu-ally lead to an increase in the number of induced rules if compared to the ones induced with the original data sets. Comparing the mean number of rules obtained with the over-sampled data sets, Random over-sampling and Smote + ENN are the methods that provide a smaller increase in the mean number of rules. It was expected that the appli-cation of over-sampling would result in an increase in the mean number of rules, since over-sampling increases the to-tal number of training examples, which usually generates larger decision trees. It can also be considered unexpected that Random over-sampling is competitive with Smote + Tomek and Smote + ENN in the number of induced rules, once Tomek and ENN were applied as data cleaning meth-ods with the objective of eliminating noise examples and thus simplifying the induced decision trees.
 Figure 6: Mean number of induced rules for original and balanced data sets and unpruned decision trees. The results presented in Table 9 are shown in a graph in Figure 7 allowing a clearer comparison for the mean number of conditions per rule for the over-sampled data sets. The Smote + ENN method provided very good results. In fact, it was the best ranked in 10 data sets. Furthermore, this method was even able to obtain smaller values than those achieved by decision trees induced from the original data sets in 6 data sets. Moreover, considering only the over-sampled data sets, this method was the best ranked for another 4 data sets.
 Figure 7: Mean number of conditions per rule for original and balanced data sets and unpruned decision trees. In this work we analyze the behavior of several over and under-sampling methods to deal with the problem of learn-ing from imbalanced data sets. Our results show that the over-sampling methods in general, and Smote + Tomek and Smote + ENN (two of the methods proposed in this work) in particular for data sets with few positive (minority) exam-ples, provided very good results in practice. Moreover, Ran-dom over-sampling, frequently considered an unprosperous method provided competitive results with the more complex methods. As a general recommendation, Smote + Tomek or Smote + ENN might be applied to data sets with a small number of positive instances, a condition that is likely to lead to classification performance problems for imbalanced data sets. For data sets with larger number of positive ex-amples, the Random over-sampling method which is compu-tationally less expensive than other methods would produce meaningful results.
 It should be noted that allocating half of the training exam-ples to the minority class does not always provide optimal results [23]. We plan to address this issue in future research. Furthermore, some under-sampling methods, such as Tomek links and NCL, that do not originally allow the user to spec-ify the resulting class distribution, must be improved to in-clude this feature. Another natural extension to this work is to analyze the ROC curves obtained from the classifiers. This might provide us with a more in depth understanding of the behavior of balancing and cleaning methods. Acknowledgements. We wish to thank the anonymous reviewers and Dorival Le  X ao Pinto J  X unior for their helpful comments. This research was partially supported by the Brazilian Research Councils CAPES and FAPESP. [1] Batista, G. E. A. P. A., Bazan, A. L., and [2] Bauer, E., and Kohavi, R. An Empirical Comparison [3] Blake, C., and Merz, C. UCI Repository of Machine [4] Chawla, N. V. C4.5 and Imbalanced Data Sets: In-[5] Chawla, N. V., Bowyer, K. W., Hall, L. O., [6] Ciaccia, P., Patella, M., and Zezula, P. M-tree: [7] Domingos, P. MetaCost: A General Method for Mak-[8] Drummond, C., and Holte, R. C. C4.5, Class Imbal-[9] Ferri, C., Flach, P., and Hern  X  andez-Orallo, J. [10] Hand, D. J. Construction and Assessment of Classifi-[11] Hart, P. E. The Condensed Nearest Neighbor Rule. [12] Japkowicz, N. Class Imbalances: Are We Focusing [13] Japkowicz, N., and Stephen, S. The Class Imbal-[14] Kubat, M., and Matwin, S. Addressing the Course [15] Laurikkala, J. Improving Identification of Difficult [16] Ling, C. X., and Li, C. Data Mining for Direct Min-[17] Mitchell, T. M. Machine Learning . McGraw-Hill, [18] Prati, R. C., Batista, G. E. A. P. A., and [19] Provost, F. J., and Fawcett, T. Analysis and Vi-[20] Quinlan, J. R. C4.5 Programs for Machine Learning . [21] Stanfill, C., and Waltz, D. Instance-based Learning [22] Tomek, I. Two Modifications of CNN. IEEE Trans-[23] Weiss, G. M., and Provost, F. Learning When [24] Wilson, D. L. Asymptotic Properties of Nearest [25] Wilson, D. R., and Martinez, T. R. Reduction [26] Zadrozny, B., and Elkan, C. Learning and Mak-
