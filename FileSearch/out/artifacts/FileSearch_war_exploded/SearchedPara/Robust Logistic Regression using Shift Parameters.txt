 Almost any large dataset has annotation errors, especially those complex, nuanced datasets com-monly used in natural language processing. Low-quality annotations have become even more com-mon in recent years with the rise of Amazon Me-chanical Turk, as well as methods like distant su-pervision and co-training that involve automati-cally generating training data.

Although small amounts of noise may not be detrimental, in some applications the level can be high: upon manually inspecting a relation ex-traction corpus commonly used in distant super-vision, Riedel et al. (2010) report a 31% false positive rate. In cases like these, annotation er-rors have frequently been observed to hurt perfor-mance. Dingare et al. (2005), for example, con-duct error analysis on a system to extract relations from biomedical text, and observe that over half of the system X  X  errors could be attributed to incon-sistencies in how the data was annotated. Simi-larly, in a case study on co-training for natural lan-guage tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic la-belling prevents these systems from performing comparably to their fully-supervised counterparts.
In this work we argue that incorrect exam-ples should be explicitly modelled during train-ing, and present a simple extension of logistic re-gression that incorporates the possibility of mis-labelling directly into the objective. Following a technique from robust statistics, our model intro-duces sparse  X  X hift parameters X  to allow datapoints to slide along the sigmoid, changing class if ap-propriate. It has a convex objective, is well-suited to high-dimensional data, and can be efficiently trained with minimal changes to the logistic re-gression pipeline.

In experiments on a large, noisy NER dataset, we find that this method can provide an improve-ment over standard logistic regression when anno-tation errors are present. The model also provides a means to identify which examples were misla-belled: through experiments on biological data, we demonstrate how our method can be used to accurately identify annotation errors. This robust extension of logistic regression shows particular promise for NLP applications: it helps account for incorrect labels, while remaining efficient on large, high-dimensional datasets. Much of the previous work on dealing with anno-tation errors centers around filtering the data be-fore training. Brodley and Friedl (1999) introduce what is perhaps the simplest form of supervised filtering: they train various classifiers, then record their predictions on a different part of the train set and eliminate contentious examples. Sculley and Cormack (2008) apply this approach to spam fil-tering with noisy user feedback.

One obvious issue with these methods is that the noise-detecting classifiers are themselves trained on noisy labels. Unsupervised filtering tries to avoid this problem by clustering training instances based solely on their features, then using the clus-ters to detect labelling anomalies (Rebbapragada et al., 2009). Recently, Intxaurrondo et al. (2013) applied this approach to distantly-supervised rela-tion extraction, using heuristics such as the num-ber of mentions per tuple to eliminate suspicious examples.

Unsupervised filtering, however, relies on the perhaps unwarranted assumption that examples with the same label lie close together in feature space. Moreover filtering techniques in general may not be well-justified: if a training example does not fit closely with the current model, it is not necessarily mislabelled. It may represent an important exception that would improve the over-all fit, or appear unusual simply because we have made poor modelling assumptions.

Perhaps the most promising approaches are those that directly model annotation errors, han-dling mislabelled examples as they train. This way, there is an active trade-off between fitting the model and identifying suspected errors. Bootkra-jang and Kaban (2012) present an extension of logistic regression that models annotation errors through flipping probabilities. While intuitive, this approach has shortcomings of its own: the objec-tive function is nonconvex and the authors note that local optima are an issue, and the model can be difficult to fit when there are many more fea-tures than training examples.

There is a growing body of literature on learn-ing from several annotators, each of whom may be inaccurate (Bachrach et al., 2012; Raykar et al., 2009). It is important to note that we are consid-ering a separate, and perhaps more general, prob-lem: we have only one source of noisy labels, and the errors need not come from the human annota-tors, but could be introduced through contamina-tion or automatic labelling.

The field of  X  X obust statistics X  seeks to develop estimators that are not unduly affected by devi-ations from the model assumptions (Huber and Ronchetti, 2009). Since mislabelled points are one type of outlier, this goal is naturally related to our interest in dealing with noisy data, and it seems many of the existing techniques would be relevant. A common strategy is to use a modi-fied loss function that gives less influence to points far from the boundary, and several models along Figure 1: Fit resulting from a standard vs. robust model, where data is generated from the dashed sigmoid and negative labels flipped with probabil-ity 0.2. these lines have been proposed (Ding and Vish-wanathan., 2010; Masnadi-Shirazi et al., 2010). Unfortunately these approaches require optimiz-ing nonstandard, often nonconvex objectives, and fail to give insight into which datapoints are mis-labelled.

In a recent advance, She and Owen (2011) demonstrate that introducing a regularized  X  X hift parameter X  per datapoint can help increase the ro-bustness of linear regression. Candes et al. (2009) propose a similar approach for principal compo-nent analysis, while Wright and Ma (2009) ex-plore its effectiveness in sparse signal recovery. In this work we adapt the technique to logistic re-gression. To the best of our knowledge, we are the first to experiment with adding  X  X hift param-eters X  to logistic regression and demonstrate that the model is especially well-suited to the type of high-dimensional, noisy datasets commonly used in NLP. Recall that in binary logistic regression, the prob-ability of an example x i being positive is modeled as For simplicity, we assume the intercept term has been folded into the weight vector  X  , so  X   X  R m +1 where m is the number of features.

Following She and Owen (2011), we propose the following robust extension: for each datapoint i = 1 ,...,n , we introduce a real-valued shift pa-rameter  X  i so that the sigmoid becomes Since we believe that most examples are correctly labelled, we L 1 -regularize the shift parameters to encourage sparsity. Letting y i  X  { 0 , 1 } be the la-bel for datapoint i and fixing  X   X  0 , our objective is now given by l (  X , X  ) =
These parameters  X  i let certain datapoints shift along the sigmoid, perhaps switching from one class to the other. If a datapoint i is correctly an-notated, then we would expect its corresponding  X  i to be zero. If it actually belongs to the posi-tive class but is labelled negative, then  X  i might be positive, and analogously for the other direction.
One way to interpret the model is that it al-lows the log-odds of select datapoints to be shifted. Compared to models based on label-flipping, where there is a global set of flipping probabilities, our method has the advantage of tar-geting each example individually.

It is worth noting that there is no difficulty in regularizing the  X  parameters as well. For exam-ple, if we choose to use an L 1 penalty then our objective becomes
Finally, it may seem concerning that we have introduced a new parameter for each datapoint. But in many applications the number of features already exceeds n , so with proper regularization, this increase is actually quite reasonable. 3.1 Training Notice that adding these shift parameters is equiv-alent to introducing n features, where the i th new feature is 1 for datapoint i and 0 otherwise. With this observation, we can simply modify the fea-ture matrix and parameter vector and train the lo-gistic model as usual. Specifically, we let  X  0 = (  X  0 ,..., X  m , X  1 ,..., X  n ) and X 0 = [ X | I n ] so that the objective (1) simplifies to l (  X  0 ) = + (1  X  y i ) log 1  X  g (  X  0 T x 0 i ) Upon writing the objective in this way, we imme-diately see that it is convex, just as standard L 1 -penalized logistic regression is convex. 3.2 Testing To obtain our final logistic model, we keep only the  X  parameters. Predictions are then made as usual: 3.3 Selecting Regularization Parameters The parameter  X  from equation (1) would nor-mally be chosen through cross-validation, but our set-up is unusual in that the training set may con-tain errors, and even if we have a designated devel-opment set it is unlikely to be error-free. We found in simulations that the errors largely do not inter-fere in selecting  X  , so in the experiments below we cross-validate as normal.

Notice that  X  has a direct effect on the number of nonzero shifts  X  and hence the suspected num-ber of errors in the training set. So if we have in-formation about the noise level, we can directly incorporate it into the selection procedure. For ex-ample, we may believe the training set has no more than 15% noise, and so would restrict the choice of  X  during cross-validation to only those values where 15% or fewer of the estimated shift param-eters are nonzero.

We now consider situations in which the  X  pa-rameters are regularized as well. Assume, for ex-ample, that we use L 1 -regularization as in equa-tion (2), so that we now need to optimize over both  X  and  X  . We perform the following simple proce-dure: 1. Cross-validate using standard logistic regres-2. Fix this value for  X  , and cross-validate using Alon et al. (1999) T2 T30 T33 T36 T37 N8 N12 N34 N36 Furey et al. (2000)  X   X   X   X   X   X  Kadota et al. (2003)  X   X   X   X   X  T6, N2 Malossini et al. (2006)  X   X   X   X   X   X   X  T8, N2, N28, N29 Bootkrajang et al. (2012)  X   X   X   X   X   X   X  Robust LR  X   X   X   X   X   X   X  these results. We conduct two sets of experiments to assess the effectiveness of the approach, in terms of both identifying mislabelled examples and producing accurate predictions. 4.1 Contaminated Data Our first experiment is centered around a biologi-cal dataset with suspected labelling errors. Called the colon cancer dataset, it contains the expres-sion levels of 2000 genes from 40 tumor and 22 normal tissues (Alon et al., 1999). There is evi-dence in the literature that certain tissue samples may have been cross-contaminated. In particular, 5 tumor and 4 normal samples should have their labels flipped.

In this experiment, we examine the model X  X  ability to identify mislabelled training examples. Because there are many more features than data-points and it is likely that not all genes are relevant, we choose to place an L 1 penalty on  X  .

Using glmnet , an R package for training reg-ularized models (Friedman et al., 2009), we se-lect  X  and  X  using cross-validation. Looking at the resulting values for  X  , we find that only 7 of the shift parameters are nonzero and that each one corresponds to a suspicious datapoint. As further confirmation, the signs of the gammas correctly match the direction of the mislabelling. Compared to previous attempts to automatically detect errors in this dataset, our approach identifies at least as many suspicious examples but with no false posi-tives. A detailed comparison is given in Table 1. Although Bootkrajang and Kaban (2012) are quite accurate, it is worth noting that due to its noncon-vexity, their model needed to be trained 20 times to achieve these results. 4.2 Manually Annotated Data We now consider the problem of named entity recognition (NER) to evaluate how our model per-forms in a large-scale prediction task. In tradi-tional NER, the goal is to determine whether each word is a person, organization, location, or not a named entity ( X  X ther X ). Since our model is binary, we concentrate on the task of deciding whether a word is a person or not. (This task does not triv-ially reduce to finding the capitalized words, as the model must distinguish between people and other named entities like organizations).

For training, we use a large, noisy NER dataset collected by Jenny Finkel. The data was created by taking various Wikipedia articles and giving them to five Amazon Mechanical Turkers to anno-tate. Few to no quality controls were put in place, so that certain annotators produced very noisy la-bels. To construct the train set we chose a Turker who was about average in how much he disagreed with the majority vote, and used only his annota-tions. Negative examples are subsampled to bring the class ratio to a reasonable level, for a total of 200,000 negative and 24,002 positive examples. We find that in 0.4% of examples, the majority agreed they were negative but the chosen annota-tor marked them positive, and 7.5% were labelled positive by the majority but negative by the an-notator. Note that we still include examples for which there was no majority consensus, so these noise estimates are quite conservative.

We evaluate on the English development test set from the CoNLL shared task (Tjong Kim Sang and Meulder, 2003). This data consists of news arti-cles from the Reuters corpus, hand-annotated by researchers at the University of Antwerp.
 We extract a set of features using Stanford X  X  NER pipeline (Finkel et al., 2005). This set was Table 2: Performance of standard vs. robust logis-tic regression in the Wikipedia NER experiment. The flipping model refers to the approach from Bootkrajang and Kaban (2012). chosen for simplicity and is not highly engineered  X  it largely consists of lexical features such as the current word, the previous and next words in the sentence, as well as character n-grams and vari-ous word shape features. With a total of 393,633 features in the train set, we choose to use L 2 -regularization, so that our penalty now becomes This choice is natural as L 2 is the most common form of regularization in NLP, and we wish to ver-ify that our approach works for penalties besides L The robust model is fit using Orthant-Wise Limited-Memory Quasi Newton (OWL-QN), a technique for optimizing an L 1 -penalized objec-tive (Andrew and Gao, 2007). We tune both models through 5-fold cross-validation to obtain  X  2 = 1 . 0 and  X  = 0 . 1 . Note that from the way we cross-validate (first tuning  X  using standard lo-gistic regression, fixing this choice, then tuning  X  ) our procedure may give an unfair advantage to the baseline.

We also compare against the algorithm pro-posed in Bootkrajang and Kaban (2012), an exten-sion of logistic regression mentioned in the section on prior work. This approach assumes that each example X  X  true label is flipped with a certain prob-ability before being observed, and fits the resulting latent-variable model using EM.
 The results of these experiments are shown in Table 2 as well as Figure 2. Robust logistic re-gression offers a noticeable improvement over the baseline, and this improvement holds at essentially all levels of precision and recall. Interestingly, be-cause of the large dimension, the flipping model consistently learns that no labels have been flipped and thus does not show a substantial difference with standard logistic regression. Figure 2: Precision-recall curve obtained from training on noisy Wikipedia data and testing on CoNLL. The flipping model refers to the approach from Bootkrajang and Kaban (2012). A natural direction for future work is to extend the model to a multi-class setting. One option is to introduce a  X  for every class except the negative one, so that there are n ( c  X  1) shift parameters in all. We could then apply a group lasso, with each group consisting of the  X  for a particular datapoint (Meier et al., 2008). This way all of a datapoint X  X  shift parameters drop out together, which corre-sponds to the example being correctly labelled.
CRFs and other sequence models could also benefit from the addition of shift parameters. Since the extra variables can be neatly folded into the linear term, convexity is preserved and the model could essentially be trained as usual. Stanford University gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA8750-13-2-0040. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not nec-essarily reflect the view of the DARPA, AFRL, or the US government. We are especially grateful to Rob Tibshirani and Stefan Wager for their invalu-able advice and encouragement.
