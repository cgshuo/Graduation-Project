 Finance, medicine, and risk management form the basis for many machine learning ap-plications. A compelling aspect of these applications is that they present several chal-lenges to the machine learning community. The common thread among these challenges persists to be class imbalance and cost-sens itive application, which has been a focus of significant recent work [2, 3]. However, the common assumption behind most of the related works is that the testing data carries the same class distribution as the training data. This assumption becomes limiting for the classifiers learned on the imbalanced datasets, as the learning usually follows a prior sampling stage to mitigate the effect of observed imbalance. This is, effectively, guided by the premise of improving the predic-tion on the minority class as measured by some evaluation function. Thus, it becomes important to understand the interaction between sampling methods, classifier learning, and evaluation functions when the class distributions change.
 To illustrate, a disease may occur naturally in 15% of a North American population. However, an epidemic condition may drastically increase the rate of infection to 45%, instigating differences in P ( disease ) between the training and testing datasets. Thus, the class distribution between negative and positive classes changes significantly. Scalar evaluations of a classifier learned on the original population will not offer a reasonable expectation for performance during the epidemic. A separate, but related problem oc-curs when the model trained from a segment of North American population is then applied to a European population where the distribution of measured features can po-tentially differ significantly, even if the disease base-rate remains at the original 15%. This issue becomes critical as the learned classifiers are optimized on the sampling dis-tributions spelled out during training to increase performance on minority or positive class, as measured by some evaluation function. If sampling is the strong governing factor for the performance on imbalanced datasets, can we guide the sampling to have more effective generalization? Contributions: We present a comprehensive empiri cal study investigating the effects of changing distributions on a combination of sampling methods and classifier learn-ing. In addition, we also study the robustness of certain evaluation measures. We con-sider two popular sampling methods for countering class imbalance: undersampling and SMOTE [2, 4]. To determine the optimal levels of sampling (under and/or SMOTE), we use a bruteforce wrapper method with cross-validation that optimizes on different eval-uation measures like Negative Cross Entropy ( NCE ), Brier Score ( Brier ), and Area Under the ROC Curve ( AU ROC ) on the original training distribution. The former fo-cuses on quality of probability estimates and the latter focuses on rank-ordering. The guiding question here is  X  what is more effective  X  improved quality of estimates or improved rank-ordering if the eventual testing distribution changes? We use the wrap-per to empirically discover the potentially best sampling amounts for the given classi-fier and evaluation measure. This allows us to draw observations on the suitability of popular sampling methods, in conjunction with the evaluation measures, on evolving testing distributions. We restrict our st udy to PETs [1] given their popularity in the lit-erature. This also allows for a more focused analysis. Essentially, we used unpruned C4.5 decision trees [5] and considered both leaf frequency based probability estimates and Laplace smoothed estimates. We also pre sent an analysis of the interaction between measures used for parameter discovery and evaluation. Is a single evaluation measure more universal than the others, especially in changing distributions? Resampling is a prevalent, highly parameter izable treatment of the class imbalance problem with a large search space. Typically resampling improves positive class ac-curacy and rank-order [6, 7, 8, 2]. To our knowledge, there is no empirical literature detailing the effects of sampling on the quality of probability estimates; however, it is established that sampling improves rank-order. This study examines two sampling meth-ods: random undersampling and SMOTE [9]. While seemingly primitive, randomly removing majority class examples has b een shown to improve performance in class imbalance problems. Some training information is lost, but this is counterbalanced by the improvement in minority class accuracy and rank-order. SMOTE is an advanced oversampling method which generates synthetic examples at random intervals between known positive examples. [2] provides the most comprehensive survey and comparison of current sampling methods.

We search a large sampling space via wrapper [10] using a heuristic to limit the search space. This strategy first removes  X  X xcess X  negative examples by undersampling from 100% to 10% in 10% steps and then synthetically adds from 0% to 1000% more positive examples in 50% increments usin g SMOTE. Each phase ceases when the wrap-per X  X  objective function no longer improves a fter three successive samplings. We use Brier , NCE ,and AU ROC [11, 12] as objective functions to guide the wrapper and final evaluation metrics. Figure 1 shows the Wrapper and Evaluation framework. We consider performance on different samplings of the testing set to explore the range of potential distributions by exploring samplings for which P dataset has 2000 examples from class 0 and 1000 examples of class 1 in the testing set. To evaluate on P (+) = 0 . 5 , 1000 class 0 examples are randomly removed from the evaluation set. We experimented on eight di fferent datasets, summarized in Table 1.
We explore visualizations of the trends in NCE and AU ROC as P (+) is var-ied. Each plot contains several different classifiers: the baseline PET [1]; sampling guided by Brier (called Frequency Brier Wrapper for frequency based estimates and Laplace Brier Wrapper for Laplace based estimates); sampling guided by NCE ;and finally sampling guided by AU ROC (the latter two using similar naming convention as Brier ). In Figures 2 to 9, NCE and AU ROC are depicted as a function of increasing class distribution, ranging from fully negative on the left to fully positive on the right. A vertical line indicates the location of the original class distribution. Brier trends are omitted as they mirror those of NCE .
Figures 2 through 9 show the experimental NCE and AU ROC trends as the class distribution varies. Despite the variety of datasets evaluated, some compelling general trends emerge. Throughout, we note that wrappe rs guided by losses generally improve NCE at and below the natural distribution of P (+) as compared to AU ROC wrap-pers This implies that loss does well in optimizing NCE when the testing distribution resembles the training conditions. It is notable that in some cases, such as Figures 2, 3, 5, 6, 7, 8, &amp; 9, that the baseline classifier actually produces better NCE scores than at least the frequency AU ROC wrapper, if not both AU ROC wrappers. The frequency AU ROC wrapper selected extreme levels of sampling. The reduction in NCE at low P (+) indicates that using loss measures with in the wrapper lowers the loss estimates for the negative class examples. That is, while the loss from the positive class may actually increase, the lower overall losses are driven by better calibrated estimates on the predominantly occurring majority class. On the other hand, classifiers learned from the AU ROC guided wrappers do not result in as well-calibrated estimates. AU ROC favors the positive class rank-order, while Brier and NCE tend to treat both classes equally, which in turn selects extreme sampling levels. Thus, if NCE optimization is desired and the positive class is anticipated to occur as rarely or more rarely than in the training data, sampling shoul d be selected according to either Brier or NCE .
However, the environment producing the data may be quite dynamic, creating a shift in the class ratio and causing the minority class to become much more prevalent. In a complete paradigm shift, the former minority class might become larger than the for-mer majority class, such as in an epidemic. I nvariably, there is a cross-over point in each dataset after which one of the AU ROC wrappers optimizes NCE values. This is logical, as AU ROC measures the quality of rank-order in terms of the positive class  X  extra emphasis is placed on correctly classi fying positive examples and is reflected by the higher selected sampling levels. As the positive examples eventually form the ma-jority of the evaluation set, classifiers pr oducing on average higher quality positive class probability estimates will produce the best NCE . Therefore, if a practitioner anticipates an epidemic-like influx of positive examples sampling methods guided by AU ROC are favored.

Improvement to AU ROC under varied testing distributions is not as uniform. We observe that at least one loss function wrapper generally produces better AU ROC val-ues in Figures 2, 3, &amp; 4, but that an AU ROC wrapper is optimal in Figures 6, 7, &amp; 9. It is difficult to declare a champion in Figures 5 &amp; 8. It is of note that datasets with naturally larger positive classes tend to benefit (in terms of AU ROC ) from a loss wrap-per, while those with naturally smaller positive classes benefit more from the AU ROC wrapper. As seen before, AU ROC guides a wrapper to higher sampling levels than Brier or NCE . In the cases of relatively few positive examples (such as Forest Cover, Oil, and Mammography), a heavy emphasis dur ing training on these few examples pro-duces better AU ROC values. For the datasets with a larger set of positive examples (as in Adult, E-State, and Pendigits) from which to naturally draw, this over-emphasis does not produce as favorable a result. Therefore, in cases where there are very few positive examples, a practitioner should op timize samplin g according to AU ROC .Otherwise, Brier or NCE optimization is sufficient.
The difference of characteristics between the trends in NCE and AU ROC is note-worthy. The NCE trends appear stable and linear. B y calculating the loss on each class at the base distribution, it appears that one is able to project the NCE at any class distri-bution using a weighted average. AU ROC trends are much more violent, likely owing to the highly perturbable nature of the measure. Adding or removing a few examples can heavily impact the produced ranking. As a measure, AU ROC is characteristically less predictable than a loss function.

We also note that sampling mitigates the need of application of Laplace smoothing at the leaves. We can see that the baseline classifier benefits from smoothing, as also noted by other works. However, by treating the dataset for class imbalance first, we are able to counter the bias and variance in estimates arising from small leaf-sizes. The wrapper essentially searches for the ideal training distribution by undersampling and/or injecting synthetic minority class instances that lead to a reduction in loss or improvement in ranking.

Throughout Figures 2 to 9, we also note that Brier and NCE loss wrappers tend to perform similarly across measures and dat asets. This is not surprising as the shape of Brier and NCE values are similar. We observe that the optimal sampling levels found by Brier and NCE are similar, certainly more similar than to those samplings of AU ROC . In general, NCE maintains a slight performance edge. If in the interests of time a practitioner may only experiment using one loss measure, then this study recommends using NCE , although the results found here may not apply to all domains and performance metrics.
 The main focus of our paper was to empirically explore and evaluate the interaction be-tween techniques for countering class imbalance, PETs, and corresponding evaluation measures under circumstances where training and testing samples differ. In light of the questions posited in the Introduction, we make the following key observations.  X  We demonstrated that it is possible to identify potentially optimal quantities of  X  There is a strong inter-play between undersampling and SMOTE. The wrapper de- X  It is much more difficult to anticipate the effects of a class distribution shift on  X  While Laplace smoothing has a profound effect in improving both the quality of
