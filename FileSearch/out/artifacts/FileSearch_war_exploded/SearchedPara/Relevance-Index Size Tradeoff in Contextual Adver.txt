 In Contextual advertising, textual ads relevant to the con-tent in a webpage are embedded in the page. Content key-words are extracted offline by crawling webpages and then stored in an index for fast serving. Given a page, ad se-lection involves index lookup, computing similarity between the keywords of the page and those of candidate ads and returning the top-k scoring ads. In this approach, there is a tradeoff between relevance and index size where better rel-evance can be achieved if there are no limits on the index size. However, the assumption of unlimited index size is not practical due to the large number of pages on the Web and stringent requirements on the serving latency. Secondly, page visits on the web follows power-law distribution where a significant proportion of the pages are visited infrequently, also called the tail pages. Indexing tail pages is not efficient given that these pages are accessed very infrequently.
We propose a novel mechanism to mitigate these prob-lems in the same framework. The basic idea is to index the same keyword vector for a set of similar pages. The scheme involves learning a website specific hierarchy from (page, URL) pairs of the website. Next, keywords are populated on the nodes via bottom-up traversal over the hierarchy. We evaluate our approach on a human labeled dataset where our approach has higher nDCG compared to a recent approach even though the index size of our approach is 7 times less than index size of the recent approach.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing Algorithms, Performance Contextual advertising, Relevance, Index size, Keyword ag-gregation
Contextual advertising is one form of web advertising, where textual ads relevant to the content of a webpage are embedded in the page. When an user visits a webpage, its publisher requests ads from an ad-network such as Google, Microsoft and Yahoo to be placed on the page. Due to stringent requirements on latency and communication cost, it is not feasible for the publisher to send the entire page to the ad network. Neither is it feasible for the ad network to crawl the entire page, analyze its contents and then select the most relevant ads within milliseconds. The prevalent so-lution is to crawl the page and extract relevant keywords and categories from its contents offline . The keywords and cate-gories, extracted from various sections of the page including title, meta keywords, URL, referrer URL and anchor text [13, 14, 6, 2], comprise the  X  X ontext X  of the page. An ad too, is characterized by a set of keywords, partly supplied by the advertiser and partly extracted from the title and the description of the ad. For fast serving, these page and ad keyword vectors are stored in an index. When a page ar-rives for ad selection, index lookup is performed to retrieve its keywords and the most relevant ads are returned after ranking them based on similarity between the keywords of the page and those of candidate ads [6]. One such popular similarity function is cosine similarity. However, these ap-proaches face a tradeoff between relevance and index size as they utilize index for serving ads.

Relevance which is defined as the quality of ads served for a given page is a function of whether the page is present in the index. Availability of all the pages in the index can achieve best relevance. However, for practical systems, storing large number of pages and hence unlimited index is not feasible. Contextual advertising systems resort to cache replacement policies such as Least Recently Used and Least Frequently Used to manage the index efficiently. Sec-ondly, it is well known that the webpage visit frequency follows a power-law distribution [3, 4]. A significant major-ity of tail pages, which are visited very infrequently, are not crawled and indexed for on-line ad-serving because offline processing and index storage costs become prohibitive. Fur-ther, pages with dynamic content, user-generated content or pages which require authentication cannot be crawled and indexed. Typically, contextual advertising systems resort to using keywords derived just from the URL tokens as tail pages are not indexed due to cache policies. These index constraints form the motivation for this paper where we de-vise an approach which reduces the index size considerably with no impact on the relevance.

The key idea in our approach is to index the same keyword vector for a set of similar pages, while restricting the scope of similar pages to be from the same website. Our approach is composed of two stages: first stage involves construction of a hierarchy over the URL tokens with the tokens as fea-tures and page clusters as class labels. The second stage involves populating keywords on the nodes of the learned hierarchy through bottom-up traversal over the tree. Upon completion of the two-stage pipeline, a URL based hierar-chical index is produced with nodes in the hierarchy pop-ulated with context-enriched keywords. Given a URL, this hierarchical index can be efficiently traversed in a top-down fashion to match the right node in the hierarchy. Keywords associated with the matched node are then used to compute the page-ad similarity. As this hierarchical index supports partial URL matches, an unindexed page, can be partially matched to an appropriate node in the hierarchy. Our con-tributions are three-fold as follows: (1) A methodology to solve index constraints in contextual advertising. (2) Technique for inducing website hierarchy using URL tokens constrained on page content and propagating key-words across the hierarchy. (3) Empirical evaluation on real-world data set from com-putational advertising system and significant reduction in index size with no loss in relevance compared to state-of-the-art approach [6].
 Organization. Followed by related work in Section 2, we describe the two stages of the proposed learning algorithm in Section 3. Section 4 explains how on-line ad serving is done using the hierarchical index constructed through offline training. Experimental evaluation is presented in Section 5. We conclude in Section 6.
In prior research, index constraints have not been consid-ered in isolation. Specifically, the focus in [13, 14, 6, 10] is to relate ads to pages through efficient keyword extrac-tion and enrichment. The paper by Ribeiro-Neto et al. [13] was one of the first to explore ad matching strategies using keywords from different sections of a page and an ad. They proposed an impedance coupling strategy to expand the key-words of a webpage with keywords from similar documents and ad landing pages. Yih et al. [14] describe a machine learned approach to keyword extraction from a page and compare it to the IR approach. They proposed a logistic regression model, trained on hand-labeled keywords from webpages and found that query frequency from search logs is an important feature for predicting ad-relevance. Broder et al. [6] proposed a semantic matching approach to over-come the problem of context mismatch between keywords on the page and the ads. Semantic phase classifies the page and the ads into a predetermined, static taxonomy of topics and uses their proximity in the taxonomy to determine rel-evance. They complement semantic match with traditional keyword matching, that is, syntactic match, where the final score is a convex combination of two sub-scores with relative weights.

Anagnostopoulos et al. [2] give a solution for dynamically created pages which cannot be indexed offline. Their ap-proach involves using text summarization techniques to cre-ate a page summary in real-time from a few hundred bytes of page content without losing page context. Further, they employ classification techniques to classify the page sum-mary in to a taxonomy of ad categories to determine the most relevant ads. Pandey et al. [11] propose similarity caching to handle index constraints for contextual advertis-ing. They study two objectives that dictate the efficiency-accuracy tradeoff and provide caching policies for these ob-jectives. They also propose a simple generative model that captures two fundamental characteristics of page requests.
Web-scale tasks are commonly performed using page con-tent and other link information. Recently, URL-based ap-proaches have become popular due to their advantages in terms of speed and resource usage. Koppula et al. [9] and Bar-Yossef et al. [5] have proposed techniques for webpage de-duplication using URL strings.
In this section, we provide basic definitions and present a general formulation of proposed offline training. We then describe the two components of offline algorithm, website hierarchy induction and keyword aggregation.

URL consists of three major components: hostname, di-rectory path and query args. We represent URL as set of (key, value) pairs where key for query args is inherited from query and for directory path, key is composed of dummy string and an index as defined in [9].
 Definition 1 (URL). URL u is defined as a function u : K  X  V where K is the set of all keys and V is the set of all values in URL set. Key for directory path is represented as k where i is the index of key from start of URL.

An example URL www.rocawear.com/nshop/product. php?groupName=wjeans&amp;dept=women is represented as k 1 = www.rocawear.com , k 2 = nshop , k 3 = product.php , groupN ame = wjeans , dept = women .

Webpage is composed of vector of keywords ane categories and is defined as follows.
 Definition 2 (Webpage). Webpage p is defined as vector of keywords and categories { kw } X  KW , where kw is asso-ciated with term frequency tf , document frequency df and section of webpage where keyword is present. KW is the set of all keywords.
 URL-hierarchy is defined as follows.
 Definition 3 (URL-hierarchy). URL-hierarchy = ( N, E ) is a n -ary tree where N is the set of nodes and E is set of edges whose labels are made of ( K, V ). Leaf nodes L  X  N consist of set of URLs and pages and internal nodes subsume URLs and pages of their children.

Given training set of URLs U and webpages P , our ap-proach constructs hierarchy on URL space U constrained by page space P . From definition 1, each URL is tokenized and stored as set of (key, value) pairs. Webpage is also tokenized and represented as vector of keywords as defined in Defini-tion 3. The problem of constructing hierarchy on U con-strained by P can be formulated as constructing a Decision Tree [12] with K as attribute label, V as attribute values and P as class labels. Our offline algorithm has two compo-nents: (1) Website hierarchy construction and (2) Keyword aggregation over hierarchy.

We employ decision tree for inducing website hierarchies on keys and values of URLs. Decision tree recursively par-titions the data space by selecting the best attribute k  X  K that reduces the impurity at each node. This process of partitioning or splitting can be continued until each leaf node corresponds to the lowest impurity, however, decision tree constructed in this fashion will typically over fit the data [12]. Conversely, if splitting is stopped too early, per-formance may suffer as training error is not sufficiently low. Below, we provide detailed explanation of splitting criteria and stopping criteria.

Information Gain (IG) is most commonly used for decision trees to measure the quality of split achieved by attributes. IG for key k and class distribution C is defined as follows.
H ( C/k ) =  X  X where H denotes entropy.

Information gain prefers attributes with large number of distinct values as these keys can predict class labels better than keys with less distinct values. While this might learn training set too well, generalization may not be achieved to the best possible extent. As one of the motivations of the proposed approach is to address index constraints, general-ization error has to be sufficiently low. Hence, Gain Ratio (GR) is considered as splitting criteria as gain ratio com-pensates for the number of values by normalizing by the information encoded in the split itself.

Lemma 1. Algorithm InduceURLHierarchy can be imple-mented to run in time O ( | U | X  X  K | 2 ) .
 Proof. For each call to InduceU RLHierarchy with URL set U n and key set K , k sel which maximizes gain ratio can be found in O ( | U n | X  X  K | ). InduceU RLHierarchy will be called recursively with new URL sets created through split of k sel . Maximum number of such URL sets will be | V k | , where V k is value-set for key k sel . Next level in hierarchy can be that a key will be considered only once in any root to leaf path, total complexity of URL-Hierarchy is O ( | U | X  X  K 2
As number of distinct keys in a given website is not high, computational complexity of website specific hierarchy in-duction is feasible.

After URL-hierarchy construction, keyword aggregation is performed to associate a keyword vector with each node in the hierarchy. The keyword vector associated with node is used to serve relevant ads for URLs matching the node.
Keyword aggregation is motivated by belief networks, where each keyword kw  X  KW is assigned a binary random vari-able, which takes a value 1 if the keyword is observed as relevant for the URL or the Node. For URL u j in training set U , relevance of the keyword kw i is defined as follows. where  X  is the normalization constant and r ij is relevance score of keyword i in URL u j and is a function of ( tf, df ).
Aggregation over hierarchy is used to determine the prob-ability of keywords being relevant to a node. All the URLs associated with the node inherit node X  X  probability.
Probability that the keyword kw i is relevant to the node n j is calculated using all children of n j as defined below and is referred as P BU in the rest of section.

Given, the URL set at node n j is union of all URLs of its children, Equation 3 ca n be rewritten as P
For the leaf node n l , P BU for keyword kw i is calculated using all URLs associated with n l .

Lemma 2. Algorithm KeywordAggregation can be imple-mented to run in O ( N ) time using O ( h ) space, where N and h are number of nodes and depth of the URL-hierarchy.
Proof. KeywordAggregation performs depth-first traver-sal of the URL-hierarchy having N nodes in O ( N ) runtime using space O ( h ).
Our serving is composed of URL hierarchy and keyword cache. Given an URL, lookup is performed on the URL hi-erarchy that returns a matched path. Keyword cache which is implemented using Memcached [1] is then used for retriev-ing keyword vector for the matched path. In Section 5.2, we present index statistics and serving performance for millions of pages.

Serving complexity for a given URL u is O ( | u | ) where | u | denotes the number of keys and values in u .
In this section, we evaluate relevance of our approach through human evaluation and compare with a recent ap-proach [6]. Next, we present index statistics and serving performance.

We used nDCG for quantifying effectiveness of our ap-proach and [6] based on graded relevance judgments [8]. Given a page, nDCG of an ad at a particular rank position R is defined as: where rel i is the graded relevance score of an ad at rank i . The mean of nDCG values across all pages is a typical measure of relevance provided by an ad-matching technique. Relevance scores in our experiments are on a scale of 0 to 2 where 2 means Relevant , 1 means SomewhatRelevant and 0 means Irrelevant . Table 1: Human evaluation: nDCG for various R values We sampled 220 million pages from 95,104 websites from Yahoo! X  X  contextual advertising system and considered all webpage requests from these websites for a duration of 90 days for offline training. Evaluation dataset consists of 400 pages. Evaluation is performed on the ads generated by using keywords and categories from proposed and [6]. Every page-ad pair is graded manually and assigned a relevance score.

Table 1 shows nDCG for [6] and our approach. As seen, the proposed approach outperforms [6] for all values of R and overall nDCG increase at R = 3 is 2 . 5%.

To test statistical significance, we conducted a bootstrap procedure [7]. We selected B (we used B = 100) indepen-dent bootstrap samples, each consisting of n (we used n = 1000) data values drawn with replacement from 1000 queries for computing empirical distributions of relative differences between the nDCGs. t -values presented in Table 1 shows that nDCG improvements of proposed over Semantic are statistically significant. Given df = 99 and most of the t -values  X  2 . 57, nDCG improvements of proposed approach are statistically significant with confidence  X  98%. Results from human evaluation show that we outperform Semantic in terms of relevance.
Table 2 shows the index size of Semantic and the proposed approach. As seen, [6] requires 85 GB for the index while the proposed approach requires only 11 . 4 GB. Index size of the proposed approach includes both the size of the hierarchy that is used for lookup of a given URL and Memcached size that is used for storing keyword vectors corresponding to nodes of the hierarchy. Our approach requires 7 . 5 times less index space compared to Semantic which is a significant reduction in index size. This is under the assumption that Semantic stores all the pages in the index.

For serving 16 million pages on a single machine, it took 19 minutes at an average of 14,000 queries per second (QPS). This shows that our serving is practical for contextual ad-vertising.

Finally, evaluation shows that the proposed approach achieves significant compression of the index with gain in relevance compared to [6].
In this paper, we focused on the tradeoff between rele-vance and index size in the conventional approach to ad selection in contextual advertising. The proposed methodol-ogy uses a learned hierarchy over similar pages to determine the augmentation of keywords over the hierarchy. Human evaluation shows that ads placed based on matching with keywords from the hierarchy have higher relevance to a re-cent approach with significant reduction in index size. We thank Rahul Sood for useful discussions and implementa-tion of the system. We also thank the anonymous reviewers for their comments. [1] Memcached: distributed memory object caching [2] A. Anagnostopoulos, A. Z. Broder, E. Gabrilovich, [3] Z. Bar-Yossef and M. Gurevich. Mining search engine [4] Z. Bar-Yossef and M. Gurevich. Estimating the [5] Z. Bar-Yossef, I. Keidar, and U. Schonfeld. Do not [6] A. Broder, M. Fontoura, V. Josifovski, and L. Riedel. [7] B. Efron and R. Tibshirani. An Introduction to the [8] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [9] H. S. Koppula, K. P. Leela, A. Agarwal, K. P. [10] V. Murdock, M. Ciaramita, and V. Plachouras. A [11] S. Pandey, A. Broder, F. Chierichetti, V. Josifovski, [12] J. R. Quinlan. Induction of decision trees. Mach. [13] B. Ribeiro-Neto, M. Cristo, P. B. Golgher, and [14] W.-t. Yih, J. Goodman, and V. R. Carvalho. Finding
