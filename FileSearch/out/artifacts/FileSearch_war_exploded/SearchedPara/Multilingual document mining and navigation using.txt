 1. Introduction
Nowadays the users of the World Wide Web (or for simplicity, the Web) try to access the huge amount of documents on the Web (or the Web pages) by searching with a search engine or browsing through hyperlinks existed within Web pages.
For users who have no specific goal, browsing Web pages is often the preferred choice. However, many users have difficulty points. These sites often provide, in company with search facility, some sorts of navigating structure which organize Web pages into hierarchies, which are called Web directories henceforth. Users can then achieve a thematic navigation through such hierarchies. However, these hierarchies were generally constructed by human experts manually and were often lack of tion and categorization schemes which make them incapable of information exchange.
 Another problem of existing Web directories comes from the monolingual nature in the construction process. Most of
Web directories categorized only Web pages written in a specific language, such as English. Different Web directories have to be constructed for different native Web pages. Such monolingual interface may limit the spread of users who are unfa-miliar with the used language. For example, a native Chinese may not intend to use a Web directory which provides only
English categorization labels and Web pages. Thus, it will be convenient for users to have a Web directory providing mul-tilingual category labels and categorizing multilingual Web pages.

There are two necessary steps in constructing a multilingual Web directory. The first step is to organize Web pages into hierarchies for easy browsing. Although other structures are possible for Web page navigation, hierarchies were most adopted since they have intrinsic categorization structures that higher-level categories represent superset of lower-level ones. Most users found this convenient since they could achieve their goals by exploiting the structures in a coarse-to-fine manner from the most general theme that meets their goals. Most popular portal sites constructed such hierarchies by hu-man experts. Although these hierarchies have the advantages of precise and consistent, manual construction approach suf-fers from the enormous amount of time and labor to initiate and maintain those hierarchies and prevents it being applied on large datasets. Thus automatic approach should be more feasible for large datasets such as the Web.
 The second step in constructing multilingual Web directories is to obtain the associations between different languages.
One popular approach is to apply some machine translation schemes to translate terms in one language to another. Unfor-some kind of measurements to measure the semantic relatedness between them. Such semantic measurements are generally not able to be explicitly defined, even with human intervention. Thus we need a kind of automated process to discover the relationships between different languages. Such process is often called multilingual text mining (MLTM).
To construct Web directories, human intervention is unavoidable in present time. We need human effort in tasks such as selecting topics and revealing their relationships. Such need is acceptable only when the volume of Web pages is consider-ably small. However, the volume of Web pages under consideration is generally large enough to prevent manual construc-the directories. The degree of automation in such construction process may differ for different constructors with different needs. One may only need a friendly interface to automate the authoring process, and another one may try to automatically identify every component from the ground up. We recognize the importance of a Web directory not only as a navigation tool but also a desirable scheme for knowledge acquisition and representation. According to such recognition, we try to develop a scheme based on a proposed text mining approach to automatically construct Web directories. Our approach is opposite to the navigation task performed by an existing Web directory to obtain goal pages. We extract knowledge from a corpus of Web pages to construct a Web directory.

In this work, we will develop an automatic scheme to arrange multilingual Web pages into Web directories based on a nizing maps ( Kohonen, 1997 ) on a corpus of monolingual Web pages to identify their topics, discover the relations among
The second is the construction of a Web directory for these monolingual Web pages. A second text mining process is then applied on two monolingual directories to discover the associations between categories in different directories. The pro-posed method will provide users novel multilingual Web directories for easy browsing of Web pages in different languages.
Fig. 1 depicts an overview of architecture of the proposed method. When a set of monolingual Web pages is input, we will first cluster them using self-organizing map algorithm. Two feature maps are obtained to reveal the relationships among terms and Web pages, respectively. A hierarchy generation process is then applied on these maps to construct a Web direc-tory for these monolingual Web pages. After constructing two monolingual directories, a hierarchy alignment process is ap-
Web directory is constructed according to such relationships. Our method provides a uniform framework that not only iden-but also explores the relations among these subjects. Besides, incorporating multilingual terms and pages into single direc-tory should provide us a much comprehensive way to navigate and organize Web pages in different languages.
The following text is organized as follows. In Section 2 we will mention some works related to this work. Section 3 de-scribes the Web directory generation process based on self-organizing maps. After generating Web directories, we then show how to find the associations between two monolingual Web directories and obtain a multilingual Web directory in Section 4.1 . The experimental result will be described in Section 5 . Finally, we give some conclusions and discussions in the last section. 2. Related work
In this section, we will review works in areas related to this work, which are Web directory generation, hierarchy align-ment, and multilingual information retrieval/text mining. 2.1. Literature on Web directory generation
Web directory generation belongs to the general task of hierarchy generation. Hierarchical structure is popular when we intend to organize a set of data for the ease of comprehension and access. Most of the related works on hierarchies focus on using them for accessing data. An example is the Cat-a-Cone system developed by Hearst and Karadi (1997) . This usage re-plies on some existing hierarchies which are often constructed manually by domain experts. There is increasing need in con-structing hierarchies automatically recently due to the increasing volume of data that prohibits manual handling. Methods for automatically constructing various types of hierarchies were proposed, such as bounding volume hierarchies in computer concept hierarchies in knowledge discovery researches. We will discuss some works regarding to concept hierarchy gener-ation here. In knowledge discovery research, we often like to organize data, especially textual data, into hierarchies since they are well perceived by humans. In one of the early work by Han and Fu (1994) , they developed a technique for refining existing concept hierarchies and generating new concept hierarchies. However, their method can only applied to numerical attributes. McCallum and Nigam (1999) used a bootstrapping process to generate new terms from a set of human-provided keywords. This approach was then applied in portal site construction ( McCallum, Nigam, Rennie, &amp; Seymore, 2000 ). Human probabilities of the meta-topic groups, which are groups of topics. This allows the individual models for each topic on the second level to focus on finer discrimination within the group. They used a supervised neural network to learn the hierarchy where topic classes were provided and already assigned. A different probabilistic approach by Hofmann (1999) used an unsupervised learning architecture called Cluster-Abstraction Model to organize groups of documents in a hierarchy. Liu and Yang (2007) used the link information within a Web page to build a topic hierarchy of a Website. Their method relies on the precise analysis and specification of link features which are implausible for normal texts. They improves their work edges by various methods. They then adapted the graph search algorithms to generate the Web directories. A close approach to our work was proposed by Chuang and Chien (2005) , which uses the search results of search engines for feature extraction and applies a modified hierarchical agglomerative clustering algorithm to organize short text segments into shallow hierar-low, multi-way hierarchies instead of binary ones. However, our method does not rely on the support of search engines for feature extraction. Besides, our approach can simultaneously identify themes and organize documents merely using the con-tents of the textual documents.

Another approach for Web directory generation resembles the generation of topic maps since they both need to identify topics and discover relationships among topics. Rath (1999) discussed a framework for automatic generation of topic maps in the template. They used a generator to interpret the generation rules and extract necessary information that fulfills the template to create the topic map. However, both the rules and the template are to be constructed explicitly and probably manually. Moore (2000) discussed topic map authoring and how software may support it. He argued that the automatic gen-map comes through the involvement of people in the process. This argument is true if the knowledge that contained in the topic maps can only be obtained by human efforts. Fully automatic generation process is possible only when such knowledge may be discovered from the underlying set of information resources through an automated process, which is generally known as knowledge discovery from texts or text mining. B X hm, Heyer, Quasthoff, and Wolff (2002) proposed a text mining infrastructure called Concept Composer using linguistic and statistical analysis techniques based on word collocations, and applied to the generation of topic maps. However, their method is not able to generate the relationships among topics and need human intervention. 2.2. Literature on hierarchy alignment
Alignment of concept hierarchies resembles the task of ontology mapping or alignment since a concept hierarchy is sim-ilar to an ontology in both structure and function. Many works on ontology alignment or mapping have been proposed ( Choi, classification, the ontology mapping discovery schemes can be based on common reference ontology, lexical similarity, structural similarity, user input, external resources such as Wordnet X  X  annotation, or prior matching. Our method, however, cannot be easily classified in such classification. We may consider our approach a combined scheme of using lexical and structural similarities. Sornlertlamvanich, Kruengkrai, Tongchim, Srichaivattana, and Isahara (2005) proposed an approach to align concepts between two hierarchical ontologies based on term distributions. A close work of our method is the HICAL method is similar to ours in respect of the use of data instances within concepts in the hierarchies. However, the alignment algorithm is significantly different.

Research on automatic hierarchy alignment on multilingual domain is still in its infancy. One of the approach is to devel-op translated version of some existing monolingual hierarchy such as WordNet. For example, one early work is the Multi-in Princeton WordNet. These synsets were then provided to the lexicographers to actually build the Italian synsets. Another early work using translation-based approach was proposed by Daud X , Padr X , and Rigau (1999) . They used relaxation labeling method to select the corresponding English synset for a Spanish one. Levow, Dorr, and Lin (2000) also adopted translation-based approach in creating multilingual hierarchy based on existing monolingual ontologies, namely the Chinese HowNet and English Levin-based English verb classification. Another multilingual hierarchy alignment approach is based on classi-fication of the concepts. Adar, Skinner, and Weld (2009) used a self-supervised classification process to classify and align
Wikipedia infoboxes in four different languages. Total 26 features were extracted and fed in the classifier. Their approach needs no dictionaries to align multilingual infoboxes. However, extensive feature selection and extraction should be performed. 2.3. Literature on multilingual information retrieval/text mining
The aim of multilingual information retrieval (MLIR) is to provide users a way to search documents written in a different language from the query. Query translation thus plays a central role in MLIR research. Three different strategies for query translation were used ( Oard &amp; Dorr, 1996 ), namely dictionary-based, thesaurus-based, and corpus-based methods. We will briefly describe the corpus-based methods which resemble to ours. Corpus-based approach uses knowledge acquisition techniques to discover cross-language relationships and applies them to MLIR. We may divide this approach into three cat-egories, namely word alignment, sentence alignment, and document alignment, based on the granularity of alignment. Word alignment can generate the finest bilingual corpora, which the relationships between words in different languages have been clearly defined automatically or manually. Brown (1996) used this kind of technique to construct a translation table for query translation. Chen and Chen (1994) stated that the precision of alignment will affect the quality of query translation.
Good alignment methods should be developed to ensure the quality of corpus-based MLIR. For sentence alignment, an exam-ple is the work of Davis and Dunning (1996) . Both word alignment and sentence alignment suffer from the fact that such alignments are not easy to achieve. On the other hand, document alignment is much easier. There are two types of corpora that could be used in document alignment, namely parallel corpora and comparable corpora. The former contains documents in multiple translations. The latter contains categories of multi-language documents that have the same topics. Comparable corpora are common since the multi-language copies of a topic are easier to obtain. An example is the work by Sheridan and then used to generate target query. Talvensaari, Juhola, Laurikkala, and J X rvelin (2007) proposed corpus-based translation method that might be used to update the references and gain some comparative insight.

Many multilingual text mining techniques, especially for machine learning approaches, are based on comparable or par-allel corpora. Chau and Yeh (2004) generated fuzzy membership scores between Chinese and English terms and clustered them to perform MLIR. Lee and Yang (2003) also used self-organizing map (SOM) ( Kohonen, 2001 ) to train a set of Chi-nese-English parallel corpora and generate two feature maps. The relationships between bilingual documents and terms are then discovered. Rauber, Merkl, and Dittenbach (2002) applied growing hierarchical self-organizing map (GHSOM) to cluster multilingual corpora which contain Russian, English, German, and French documents. However, they translated all documents into one language first before training. Thus they actually performed monolingual text mining. Since our work adopts GHSOM for benchmarking, we will briefly review the model. GHSOM is a neural network model modified from basic
SOM. The major advantage of GHSOM is its hierarchical structure of expandable maps. A map could expand its size during training to achieve better result. Any neuron in the map could even expand to a lower level map when necessary. The expan-sion could proceed to lower layers.
 We briefly summarize the GHSOM training algorithm in the following.

Step 1 (Initialization step) layer 0 contains a single neuron. The synaptic weight of this neuron, w
Step 2 (Map growing step) construct a small SOM M , e.g. containing 2 3 neurons, below layer 0. This is layer 1. Train this
Step 3 (Hierarchy expansion step) all neurons in the maps of a layer are examined to determine if they need further expan-
Step 4 Repeat Step 3 until no neuron in all maps of a layer needs expansion. 3. Web directory generation In this section we will first describe the method to automatically generate monolingual Web directories. To obtain the
Web directories, we first perform a clustering process on a corpus of Web pages. We then apply an automatic generation follow by the clustering process by SOM learning algorithm. Two labeling processes are then applied to the trained result to obtain two feature maps which characterize the relationships among Web pages and keywords, respectively. The Web directory generation process is then applied to these two maps to develop the Web directory. 3.1. Web page preprocessing and encoding
A document should be converted to proper form before SOM training. Here the proper form is a vector that catches essen-tial (semantic) meaning of the document. In this work we adopt bilingual parallel corpora that contain Chinese and English documents. The encoding of English documents into vectors has been well addressed ( Salton, 1989 ). Processing steps such as word segmentation, stopword elimination, stemming, and keyword selection are often used in extracting representative keywords from a document. In this work, we first used a common segmentation program to segment possible keywords.
A part-of-speech tagger is also applied to these keywords so that only nouns are selected. These selected keywords may con-tain stopwords that have trivial meanings. These stopwords will be removed to reduce the number of keywords. Stemming process will be also applied to convert each keyword to its stem (root). This will further reduce the number of keywords.
After these processing steps, we will obtain a set of keywords that should be representative to this document. All keywords of all documents are collected to build a vocabulary for English keywords. This vocabulary is denoted as V encoded into a binary vector according to those keywords that occurred in it. When a keyword occurs in this document, the corresponding element of the vector will have value 1; otherwise, the element will have value 0. With this scheme, a doc-ument E j will be encoded into a binary vector E j . The size (number of elements) of E j V j . We use binary vector scheme to encode the documents and ignore any kind of term weighting schemes. We decide to use the binary vector scheme due to the following reasons. First, we intend to cluster documents according to the co-occur-rence of the words, which is irrelevant to the weights of the individual words. Second, our experiments showed no advantage the binary scheme is adequate for our need.

The processing of Chinese documents differs significantly from their English counterparts in several aspects. First, a Chi-nese sentence contains a list of consecutive Chinese letters. A Chinese letter, however, carries little meaning individually without referring to its context. Several letters are often combined to give specific meaning and are basic constituents of a sentence in Chinese. Here we will mention such combined letters as a word for consistency with English words. The seg-mentation of Chinese words is more difficult than English words because we have to separate these consecutive letters into a set of words. There are several segmentation schemes for Chinese words. We adopt the segmentation program developed by processing is the need for stemming. Chinese words require no stemming in general. Stopword elimination could be applied we omit this process since we select only nouns as keywords. The reason for selecting nouns as keywords is that they will be used as category themes later in Web directory generation process. We believe that only nouns can provide meaningful guid-ance for users in navigating the directory. A problem of omitting the stopword elimination is that some nouns such as  X  X ome-page X  will occur in lots of documents. This will not affect the clustering process since (1) the distance calculation during clustering is contributed by many keywords that will not be affected much by several common keywords and (2) the dis-tance between documents will not be affected suppose they all contains these common keywords. As in English case, the selected keywords are collected to build Chinese vocabulary V the same manner as English. The size of C j is just the size of the vocabulary V 3.2. Feature map generation
In this sub-section we will briefly review a method to organize documents into clusters by their co-occurrence similarities
We intend to organize these documents into a set of clusters such that similar documents will fall into the same cluster or nearby clusters. The unsupervised learning algorithm of SOM networks ( Kohonen, 2001 ) meets our needs. The SOM algo-rithm organizes a set of high-dimensional vectors into a two-dimensional map of neurons according to the similarities ing (or learning) process. That is, the similarity between vectors in the original space is preserved in the mapped space.
Applying the SOM algorithm to the document vectors, we actually perform a clustering process on the documents. Here a neuron in the map can be treated as a cluster. Similar documents will fall into the same or neighboring neurons (clusters).
Besides, the similarity of two clusters can be measured by the geometrical distance between their corresponding neurons. To decide the cluster to which a document or a word belongs, we apply two labeling processes to the documents and the words, respectively. After the document labeling process, each document will associate with a neuron in the map. We record such associations and obtain the document cluster map (DCM). In the same manner, we label each word to some neuron in the map and obtain the keyword cluster map (KCM). We then use these two maps to generate the hierarchies.
We define some denotations and describe the training process here. Let x vector of the i th document in the corpus, where N is the number of indexed terms and M is the number of the documents.
We use these vectors as the training inputs to the SOM network. The network consists of a regular grid of neurons which each has N synapses. Let w j  X f w j n j 1 6 n 6 N g ; 1 6 where J is the number of neurons in the network. We train the network by the SOM algorithm: Step 1 Randomly select a training vector x i from the corpus.

Step 2 Find the neuron j with synaptic weight vector w j which is the closest to x
Step 3 For each neuron l in the neighborhood of neuron j , update its synaptic weights by certain times. The training gain and neighborhood size both decrease when t increases.
When the document clustering process is accomplished, we then perform a labeling process on the trained network to establish the association between each document and one of the neurons. The labeling process is described as follows. Each DCM. In the DCM, each neuron is labeled by a list of documents which are considered similar and are in the same cluster.
We construct the KCM by labeling each neuron in the trained network with certain keywords. Such labeling is achieved by examining the neurons X  synaptic weight vectors, and is based on the following observation. Since we adopt binary rep-resentation for the document feature vectors, ideally the trained map should consist of synaptic weight vectors with com-ponent values near either 0 or 1. Since a value of 1 in a document vector indicates the presence of a corresponding word in that document, a component with value near 1 in a synaptic weight vector also shows that such neuron has recognized the neuron may be labeled by several keywords which often co-occur in a set of documents, making a neuron a keyword cluster.
The KCM autonomously clusters keywords according to their similarity of co-occurrence. Keywords that tend to occur simultaneously in the same document will be mapped to neighboring neurons in the map. For example, the translated Chi-nese words for  X  X eural X  and  X  X etwork X  often occur simultaneously in a document. They will map to the same neuron, or neigh-boring neurons, in the map because their corresponding components in the encoded document vector are both set to 1. Thus a neuron will try to learn these two keywords simultaneously. On the contrary, words that do not co-occur in the same doc-ument may map to much distant neurons in the map. Thus we can reveal the relationship between two keywords according to their corresponding neurons in the KCM. 3.3. Web directory generation
To generate a hierarchy, we should first cluster documents by the SOM using the method described in Section 3.2 to ob-tain the DCM and the KCM. As we mentioned before, a neuron in the DCM represents a cluster of documents so we will use the terms  X  X luster X  and  X  X euron X  interchangeably in the following text. Documents which are labeled to the same neuron, or neighboring neurons, usually contain keywords that often co-occur in these documents. By virtue of the SOM algorithm, the ilar document clusters will correspond to neighboring neurons in the DCM. Thus we may generate a cluster of similar clus-the parent node is the constructed super cluster and the child nodes are the clusters that compose the super cluster. The
The overall hierarchy can then be established iteratively until a stop criterion is satisfied. 3.3.1. Super cluster construction
Here we use a top-down approach to generate hierarchies based on the construction of super clusters. To form a super cluster, we first define the distance between two clusters: where i and j are the neuron indices of the two clusters and G
For a square formation of neurons, G i  X  X  i mod dinates G i and G j . We also define the dissimilarity between two clusters: similarity , is defined by: where doc  X  i  X  is the number of documents associated with neuron i , i.e. cluster i , in the DCM and B in the neighborhood of neuron i . The function F : R  X  ! R as arguments. 3.3.2. Determining dominating clusters
The super clusters are developed from a set of dominated clusters in the map. A dominated cluster is a cluster which has locally maximal aggregated cluster similarity. We may select all dominated clusters in the map by the following algorithm: Step 2 Eliminate its neighboring clusters so that they will not be considered as dominated clusters in succeeded steps.
Step 3 If
The algorithm finds dominated clusters from all clusters under consideration and creates a level of the hierarchy. The the hierarchy.

A dominated cluster is the centroid of a super cluster, which contains several child clusters. We will use the neuron index of a neuron as the index of the cluster associated with it. For consistency, the index of a dominated cluster is used as the belongs to super cluster k if for all l super clusters: D X  i ; k  X  &lt; D X  i ; l  X  . 3.3.3. Constructing hierarchy
The above process creates a two-level hierarchy. In the following we will show how to obtain the overall hierarchy. In the first application of the super cluster generation process (denoted by STAGE-1), we obtain a set of super clusters. We aggre-dren of the super clusters obtained in STAGE-1, we may apply the super cluster generation process on each super cluster (STAGE-2). Notice that in STAGE-2 we only consider clusters which belong to the same super cluster. A set of child nodes will be obtained and be used as the third level of the hierarchy. The overall hierarchy can then be revealed by recursively applying the same process to each new-found super cluster (STAGE-n ). We decrease the size of neighborhood in selecting dominated clusters when the super cluster generation process proceeds. This will produce a reasonable number of levels for the hierarchies, as we will discuss later. 3.3.4. Parameter setting and discussions
The neighborhood B i in calculating supporting cluster similarity of a cluster i may be arbitrarily selected. Two common selections are circular neighborhood and square neighborhood. In our experiments, the shapes of the neighborhood are not crucial. It is the size of the neighborhood, denoted by N ferent selections of dominated clusters. Small neighborhood may not capture the necessary support from similar clusters. On the other hand, without proper weighting, a large N c 1 will incorporate the support from distant clusters which may not be similar to the cluster under consideration. Besides, large neighborhoods have the disadvantage of costing much computation time.

Another usage of neighborhood is to eliminate similar clusters in the super cluster generation process. In each stage of the hoods will eliminate many clusters and result in less dominated clusters. Oppositely, a small neighborhood produces a large number of dominated clusters. We must decrease the neighborhood size when the process proceeds because the number of neurons under consideration is also decreased.
 hierarchies with too many nodes in each level if the neighborhood size is considerably small. An extreme case happens when selected as dominated clusters and we will obtain a two-level hierarchy with J level-2 nodes. Determining an adequate neighborhood size as well as a proper number of dominated clusters is crucial for obtaining an acceptable result. The third chy. Notice that setting large depth may cause no effect because the neighborhood size and the number of dominated clus-ters may already satisfy the stop criterion. An ad-hoc heuristic rule used in our experiments is to determine the maximum depth d if it satisfies the following rule: where K is the dimension of the neighborhood and is defined as a ratio to the map X  X  dimension. For example, if the map con-tains an 8 8 grid of neurons, K  X  4 means that the dimension of the neighborhood is fourth of the map X  X  dimension, which is two in this case. The depth d which satisfies Eq. (9) is then 2.

Notice that there may exist some  X  X pare X  clusters which are not used after the hierarchy generation process. These clusters are not significant enough to be a dominated cluster of a super cluster in any stage of the process. Although we can extend the depths in the hierarchy generation process to enclose all clusters into the hierarchy, sometimes we may decide not to do so because we want a higher document-cluster ratio. That is, a cluster should contain a significant amount of documents. For hierarchy which contains many nodes without too much information. To avoid producing such over-sized hierarchy, we may adopt a different approach. When the hierarchy has been created and there still exists some spare clusters, we simply assign into the hierarchy. The merging process is necessary to achieve a reasonable document-cluster ratio.

In the generated hierarchy, a node represents an individual neuron which is associated with a cluster of documents. We the same neuron in the KCM. In this regard, a node in the hierarchy is labeled by a document cluster as well as a keyword hierarchies, which will be discussed in the next section. 3.4. Evaluation of the quality of generated hierarchies
We introduce a measure to evaluate the quality of the constructed bilingual hierarchies. Let L two different languages. Let H 1 and H 2 be the hierarchies constructed using L and E i be its counterpart in L 2 . We also denote C k and E tively. We then calculate the mean inter-document path length between each pair of documents in C where function dist  X  C i ; C j  X  returns the shortest path length between E and jC k j denotes the number of documents associated with node k . An example is depicted in Fig. 3 . In the figure, C documents C 1 ; C 3 , and C 5 . Therefore, P k is the average of dist  X  C between E 1 and E 3 ; E 1 and E 5 , and E 3 and E 5 , respectively. In this example, P gual hierarchies can then be measured by the average of all P since it shows that the counterparts of related documents are also related.
 could be varied for different trainings. To ensure that the two hierarchies have comparable result, we may generate a hier-archy H 1 using L 1 first and repeatedly generate hierarchies using L using L 2 is then measured. We stop SOM training for L 2 as soon as P could yield P k that is lower than this threshold, the one with the lowest P spend additional time in generating acceptable hierarchies, we should apply it to ensure that both hierarchies are compa-rable and the discovered associations are accurate. 4. Multilingual Web directory generation
Monolingual Web directories such as those generated by the method in Section 3.3 should provide users a convenient way to find the Web pages they like. However, most users should rely on some translation engines to obtain Web pages in the same topic written in non-native languages of them. It will be convenient for users to have a multilingual Web direc-foreign-made digital camera, he may want to collect all possible Web pages written in various languages besides his native goal. On the other hand, when a multilingual Web directory was available, users can navigate through the directory and reach his goal pages, no matter written in what languages, using his familiar language. Such navigation process may also broaden his search, especially for uncommon language users. For native English users, multilingual Web directories may not be so attractive since most of the Web pages were written in English. However, non-English users will find multilingual
Web directories convenient. Users who intensively access multilingual information sources such as reporters and techni-section we will demonstrate a method to align two monolingual hierarchies generated using the method described in Sec-
V and E 1 denote the set of vertices (nodes) and edges, respectively, of H edges, respectively, of H 2 . We will first introduce an alignment method to map nodes between H
M : V 1 ! V 2 will be derived. According to such mapping, we can construct a multilingual Web directory to access Web pages of both languages. We will discuss the alignment and Web directory construction processes in the following sub-sections. 4.1. Alignment of monolingual Web directories
To map a category C k 2 V 1 to some category E l 2 V 2 , a simple solution is to translate keywords associated with C thesaurus in the whole process. What we should rely on are the trained hierarchies. We consider C they have similar themes. Meanwhile, the theme of a category could be determined by the documents labeled to it. Thus we could associate two categories according to their corresponding document clusters. To define such associations, we use par-allel corpora to train the SOM in this work. The advantage of using parallel corpora is that the correspondence between a document and its counterpart in another language is known a priori. We should then use such correspondences to associate categories of different languages.

To define the association between two categories in different directories, two types of similarity measurements were de-fined here, namely semantic similarity and structural similarity. The first measurement measures the semantic similarity between two categories. The other measures the structural similarity between categories in different hierarchies. First we should define the semantic similarity between two categories in different hierarchies. Here we define two categories being semantically similar if they have similar themes, which are discovered by method described in Section 3.2 . The problem is that these themes are discovered from monolingual documents and the relationships between those themes in different hierarchies cannot be found directly. A typical solution is to translate themes in one language to another. In this work, we would not adopt such approach and would develop a more  X  X elf-discovery X  approach. We argue that two categories have similar themes if they contain similar documents. Since we use parallel corpora to develop the hierarchies, the associations between documents in different languages are intrinsically defined. We should use such associations to define the similarity between categories in different hierarchies. 4.1.1. Calculating semantic similarity
To obtain the semantic similarity between C k in H 1 with E similarity between documents in each language. For each document C counterpart E i labeled. The similarity between C k and E between C k and E l , denoted by S s  X C k ; E l  X  , as follow:
In Eq. (11) , E i is the counterpart document of C i and G  X  E labeled in the DCM for English documents. kk is the norm of a vector. We add 1 to the nominator to avoid the case of divid-ing by zero. The equation is in inverse form so that a large value of S neurons to which every E j 2E l labeled are all close to E documents in C k have their counterparts all in E l ; S s
Fig. 4 depicts the calculation of S s . 4.1.2. Incorporating structural similarity
Two related clusters may have insignificant semantic similarity because related documents in these clusters are labeled to neighboring neurons instead of the same neuron. Besides, there may exist some spurious documents in these related clusters. An example is shown in Fig. 5 . To compensate the effect of such misplacement, we should allow topological relat-edness being incorporated into similarity calculation. The second similarity measures the topological relatedness, or struc-tural similarity, between C k and E l . Here we define the topological relatedness as the amount of related documents in neighboring clusters on the hierarchies. For a cluster C k neighboring clusters. Let N C k be the set of neighboring clusters of C in H 2 , denoted by S t  X C k ; E l  X  , is defined as follow: where and
The function L returns the level of a cluster in the hierarchy. We use the function Q  X C tions from parent and child clusters. When we prefer the case that the two clusters in comparison have their parents also related to the case that their children are related, we should set q culating the structural similarity. Typically we will set q zero values when E i , which is the counterpart of C i , and E but their parent/child relationship is different from that of C 4.1.3. Overall similarity The overall similarity between C k and E l is a weighted sum of their semantic similarity and structural similarity: where b is the weighting coefficient to scale the contributions from the two similarities. Note that S 1 in average after normalization.

Actually, we may measure the structural similarity from the DCM directly using the locations of neurons. However, this developed hierarchies rather than DCMs directly.
 Cluster C k is associated with cluster E l if the following condition holds:
In this section we demonstrate method to find the associated English cluster of a Chinese cluster. We could also reverse two sets of associations for each training. However, we believe choosing one of them will be enough since the experiments exhibited similar result, as shown in Section 5 . 4.2. Multilingual Web directory generation
A multilingual Web directory can be easily constructed after the association between each Chinese cluster and some Eng-lish cluster is found. The monolingual Web directories can be merged into a multilingual one using the discovered associa-tions. A simple merging scheme is to merge each C k with its associated E
All documents labeled to either C k or E l are all labeled to the new cluster C 0 Chinese and English documents, respectively. Fig. 6 demonstrates the merging process of a Chinese cluster and its associated
English cluster. We can also construct the Web directory based on the English hierarchy and obtain a multilingual one with structure the same as the English hierarchy. 5. Experimental result
We constructed the bilingual parallel corpora by collecting parallel documents from Sinorama corpus. tains segments of bilingual articles of Sinorama magazine. parallel documents in our experiments to demonstrate the scalability of our method. Each document is a segment of an arti-that we majorly used for explanation purpose. The other set, denoted as Corpus-2, contains 10672 parallel documents. This corpus resembles to a medium-size corpus that may fit the scale of a real application. Each Chinese document had been seg-mented into a set of keywords though the segmentation program developed by the CKIP team of Academia Sinica. gram is also a part-of-speech tagger. We selected only nouns and discarded stopwords. As a result, we have vocabularies of size 3436 and 12941 for Corpus-1 and Corpus-2, respectively. For English documents, common segmentation program and part-of-speech tagger are used to convert them into keywords. Stopwords were also removed. Furthermore, Porter X  X  stem-ming algorithm ( Porter, 1980 ) was used to obtain stems of English keywords. Finally, we obtained two English vocabularies of size 3711 and 13723 for Corpus-1 and Corpus-2, respectively. These vocabularies were then used to convert each docu-ment into a vector, as described in Section 3.1 . 5.1. SOM training
To train Corpus-1, we constructed a self-organizing map which contains 100 neurons in 10 10 grid format. The number of neurons was determined experimentally such that a better clustering can be achieved. Each neuron in the map contains 3436 and 3711 synapses for training Chinese documents and English documents, respectively. The initial training gain was set to 0.4 and the maximal training time was set to 200 for both trainings. These settings were also determined experimen-tally. We tried different gain values ranged from 0.1 to 1.0 and various training time setting ranged from 50 to 500. We simply adopted the setting which achieved the most satisfying result. After training we labeled the map by documents and keywords, respectively, by the methods described in Section 3.2 and obtained the DCMs and the KCMs for both lan-guages. The above process was also applied to Corpus-2 and obtained the DCMs and the KCMs for Corpus-2. The resulted
DCMs and the KCMs for Corpus-1 are depicted in Figs. 8 and 9 , respectively. Due to space limit, we only show selected neu-rons of the maps. In Fig. 8 the number in each grid in the DCM indicates the number of documents associated with the cor-responding neuron. We then show the documents associated with these neurons by listing their filenames. We use a convention of the filenames that should effectively depict the similarity of the documents. For example, a document with filename  X  20000103024 _ C.txt  X  indicates that it is the Chinese version of the 24th segment of the article appeared at the the documents with filenames started with  X  20000103  X , e.g.  X  themes of the documents labeled to the corresponding neurons in Fig. 8 . Table 1 shows the parameter setting and statistics of the training. 5.2. Hierarchy generation
After the clustering process, we applied the hierarchy generation process to the DCMs to obtain the hierarchies. In our experiments we limited the number of dominated clusters to 10. Both neighborhood sizes N the maps X  dimensions for the two test corpora. We limited the depths of hierarchies to 3 and 4 for Corpus-1 and Corpus-2, respectively. In Fig. 10 we show part of the Chinese hierarchy developed from Corpus-1. Each leaf node in a hierarchy rep-resents a cluster in the corresponding DCM. The parent node of some child nodes in level n of a hierarchy represents a super has four children which are the four dominated clusters obtained in STAGE-2. These child nodes comprise the third level of the hierarchy. Likewise, the child nodes of the third-level nodes are the found dominated clusters after STAGE-3. The key-words in the KCM are used to label every node in the hierarchies. We merge all keywords that belong to those clusters that the generated hierarchy comprises similar clusters which have related keywords. We omitted the rest of the hierarchies as generated hierarchies.

The quality of the generated hierarchies is evaluated using the P the likeliness of related documents being clustered together in different hierarchy. That is, when most of the related docu-ments in one language were also related (being clustered together) in another language, the value of P We conducted experiments on hierarchy generation process, preceded by SOM training, for 100 times using Corpus-1 and Corpus-2, respectively. The average P k values were then calculated for both corpora. The result is summarized in Table 3 .
For comparison, we applied the GHSOM model on the same set of corpora. We chose the GHSOM because it is also based on the SOM and could generate hierarchies as our method. The same P used our method on multilingual corpora which one of the monolingual corpus is direct translation of another using Google translation service. 4 We constructed two corpora which one of them contains English documents translated from Chinese ones and the other contains Chinese documents translated from English ones. We can see that our method improves the
P values for about 10% in average comparing to GHSOM. There are not much difference in P the difference in the term selection processes between two languages. Also note that in the direct translation scheme we translated the documents before they were segmented. Another possibility is to segment the documents first and translate the resulting terms directly. This approach will produce exact duplicates without considering the barriers between lan-guages, thus was not considered in our experiments.

The parameters in applying our method were set as follows. In the super cluster generation process, we used a square neighborhood in calculating aggregated cluster similarities. The dimension of the neighborhood used in generating Fig. 10 plicity, we let N c 1 denote the dimension of this neighborhood. Another important parameter is the neighborhood in elimi-nating neighboring clusters of a dominated cluster. We denote the dimension of this neighborhood by N of dominated clusters N D that we can found in each stage. We allowed tenth of the number of neurons as dominated clusters in our experiments. 5.3. Hierarchy alignment and Web directory generation
After generating hierarchies for two languages, we applied the hierarchy alignment algorithm described in Section 4.1 on these hierarchies. The quality of the alignment is measured by the amount of parallel documents in each pair of associated clusters. For example, let Chinese cluster C k be associated with English cluster E and E l contains documents E 1 ; E 2 ; E 3 , and E 4 . Here C allel documents in this pair of clusters is then 2, namely C over all associated clusters is summed and divided by the total number of parallel document pairs in the training set. We third of the parallel documents fall in different categories. We also conducted another experiment to see the difference be-that our method differs not much in the roles during mapping hierarchies.

The Web directory generation process described in Section 4.2 were then applied on the generated hierarchies. Fig. 11 depicts part of a generated multilingual Web directory which complies with the Chinese hierarchy in Fig. 10 . This directory was evaluated by a set of human subjects who participated in an information retrieval course in two semesters. When be asked if they preferred the multilingual directory to monolingual directory such as Yahoo!, 68 out of 90 subjects, i.e. 76%, gave positive answers. Moreover, 72% (65/90) of subjects thought the multilingual directory is feasible in arrangement of category themes and documents. The subjects were asked to give comments freely. Among 28 subjects who gave comments, relations between categories are not so relevant. These comments suggest the possible weakness of our method and indicate the direction for improving our method. 6. Conclusions In this work, we proposed an automatic method to generate and align multilingual hierarchies to construct a multilingual
Web directory. Our method applies the self-organizing map model to cluster bilingual documents and creates two feature maps for each language. A hierarchy generation process is then applied on these maps to create two monolingual hierarchies. ilarity and structural similarity, between nodes were defined in this work. These similarities are determined by the docu-ments associated with the nodes. We conducted experiments on two sets of corpora and obtained promising result.
The major advantages of our method are the development of multilingual hierarchy alignment method. Our approach is fully automated and requires no human intervention. The quality of the generated hierarchies and alignment result are both Besides, our method can be applied to any sets of symbols which document associations can be specifically defined.
A limitation of our method comes from the need of parallel corpora. We need the knowledge of the correspondences be-tween documents of different language to discover the associations. It is uncommon that such parallel corpora containing
Web pages are available. Two approaches could be applied to tackle this deficiency. The first approach is to use comparable corpora as long as the correspondence between documents could be defined. When both parallel and comparable corpora are unavailable, we may also apply another correspondence discovery process to find such correspondence prior to our method.
The second approach is to use some standard parallel corpora, such as the Sinorama corpora used in this work, to generate the monolingual hierarchies and obtain the associations between these hierarchies. A multilingual hierarchy could also be generated using our method. This hierarchy is then used to  X  X ootstrap X  the generation of the multilingual Web directory.
A set of Web pages could then be labeled to these monolingual hierarchies, i.e. labeled to the multilingual hierarchy simul-taneously, to obtain the multilingual Web directory. The labeling method is the same as described in Section 3.2 . The boot-strapping process is depicted in Fig. 12 .
 References
