 In this pro ject, w e dev elop ed a tec hnique for extracting useful information from databases that con tain b oth xed-format and free-text elds. The presen t state of the art in data mining is a sc hism b et w een tec hniques that handle only xed-format data (pattern recognition, classi cation algo-rithms from mac hine learning), and tec hniques designed for free-form text (information retriev al). Adv anced kno wledge disco v ery tec hnologies ha v e b een dev elop ed in b oth researc h areas, but systems that can categorize or cluster records con-taining b oth kinds of data are still lac king. Sp eci cally ,w e examined database records from a Honeyw ell service cen ter to extract information ab out the exp ected cost of di eren t kinds of service requests. Our goal w as to test the h yp othe-sis that incorp orating information from free-text elds w ould pro vide a b etter categorization of these records; in this case, better predictions of the cost of the service call. In our w ork, w eha v ein tegrated feature extraction and clustering tec hniques from information retriev al with classi cation al-gorithms from mac hine learning in order to categorize the h ybrid elds. Our preliminary results suggested that in-corp orating free-form text could poten tially induce better classi cation mo dels.
 H.2.8 [ Database Managemen t ]: Database Applications| Data mining T extual data mining is a rapidly gro wing application of kno wl-edge disco v ery in databases (KDD) due to the ev er-increasing v olume of structured and unstructured do cumen ts pro duced b y large organizations. In this pro ject, w e dev elop ed an automatic tec hnique for extracting useful information from databases that con tain b oth xed-format and free-text elds. A tpresen t, most of the tec hniques dev elop ed in the area of data mining and information retriev al are designed to han-dle only xed-format data or free-form text, but not a com-bination of b oth. There are man y application domains in whic h b oth xed-format and free-form data are a v ailable. F or example, an e-commerce W eb site ma y collect pro les of their customers as xed-format data as w ell as users' re-views ab out their pro ducts in free-form text. Incorp orat-ing b oth t yp es of information in to the data mining task en-hances our understanding of the domain, but also increases the complexit y of the problem. In this pro ject, w e examined database records con taining b oth t yp es of data from a Hon-eyw ell tec hnical assistance cen ter, whic hpro vides telephone supp ort for industrial con trol pro ducts suc h as distributed con trol systems, automatic v alv es, sensors, etc. W e w ere in terested in gaining insigh ts ab out the nature of problems handled, and the exp ected cost of di eren t kinds of service requests.
 Service cen ter directors w ould lik e to determine what t yp es of cases are the most exp ensiv e to service (where cost is mea-sured in terms of call frequency and call duration). A direc-tor can easily nd out what it costs to pro vide supp ort for eac h pro duct, b y summing o v er all cases with the same co de in the pro duct eld. But kno wing whic h are the most exp en-siv e pro ducts to supp ort is not enough, the director has to kno w what mak es them exp ensiv eto c ho ose an appropriate correctiv e action. Are these cases due to hardw are failures, p o orly-written do cumen tation, installation problems or con-guration problems? These questions cannot b e answ ered with a simple database query , b ecause the information is buried in the engineers' notes. Our goal is to dev elop a clas-si cation mo del for service cen ter case records con taining b oth free-text and xed-format data elds. This mo del will b e useful for impro ving customer services as w ell as iden ti-fying a ws in existing pro ducts. F or example, [4] dev elop ed a system to automatically categorize in-coming email mes-sages at a call cen ter in order to impro v e customer care. Ho w ev er, they used only free-form email messages to clas-sify the calls in to one of the 47 di eren t categories. The accuracies of their system v aried b et w een 22% up to 56%, dep ending on the classi cation algorithms and linguistic pre-pro cessing metho ds used.
 In our w ork, w eha v ecom bined tec hniques from information retriev al and mac hine learning in to a no v el metho d of cate-gorizing h ybrid xed-format and free-text elds. Our initial results suggest that incorp orating free-text information can p oten tially induce b etter classi cation mo dels.
Permission to make digital or hard copies of part or all of this work or permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 In this section, w e presen tan o v erview of our service cen-ter textual data mining system (Figure 1). It is an o -line kno wledge disco v ery system, consisting of 3 main comp o-nen ts : data extraction, prepro cessing and data mining mo d-ules. The functionalit yof eac h mo dule is describ ed b elo w. A t the Honeyw ell Service Cen ter, all requests for customer supp ort are recorded in a call trac king database. The infor-mation stored in this database con tains b oth xed-format and v ariable-format (free-text) elds. The xed-format elds con tain attributes suc h as the case n um b er for eac hservice request, t yp e of problem encoun tered, the n um b er of engi-neers who handled the case, pro duct ID, customer informa-tion and the amoun tof time spen t to resolv e the problem. These elds ha v e strict formatting restrictions in terms of attribute t yp e, range and precision of the attribute v alues. F or example, the total time eld is measured up to one-ten th of an hour.
 The free-text eld con tains a case description written b y the engineers who handled the case. Because this eld is unstructured, it ma y con tain b oth relev an t and irrelev an t information for categorizing the particular case. This is fur-ther complicated b y the di eren tst yles and languages used for writing the description. F or example, some of the de-scription ma ycon tain Spanish text, follo w ed b y an English translation of the text.
 The data extraction mo dule extracts only relev an t features from the call-trac king database. These features w ere ini-tially iden ti ed with the help of domain exp erts. Our data extraction mo dule is implemen ted with SQL scripts to ex-tract the imp ortan t elds and store them in at les for further pro cessing. T o classify the case records, eac h record has to b e con v erted to a v ector in a feature space. F or the xed-format elds suc h as customer and pro duct, the feature space is de ned b y their limited p ossible v alues. F or free-text elds, statistics based on the frequencies of certain w ords and phrases form the feature space. The classi cation algorithm then uses the feature v ector corresp onding to the case record to ev aluate some desired attribute of the case.
 In our text mining system, the xed-format elds require minimal prepro cessing. F or example, w e translate the al-phan umeric v alues of some of the xed-format elds in to suitable n umeric ids, and consolidate the di eren tv ariations of NULL v alues. F or free-text elds, construction of the fea-ture v ector in v olv es t w o steps : prepro cessing and dimension reduction. Prepro cessing of the free-text elds requires more robust tec hniques to handle problems suc h as sp elling errors, abbreviations, m ultilingual text, etc. Figure 2 illustrates the steps needed to transform the ra w free-text data in toafea-ture v ector represen tation.
 The rst step of prepro cessing is text con v ersion, whic hcon-v erts a p ortion of the free-text description in to a form more suitable for subsequen t prepro cessing steps. This includes remo ving irrelev an tsen tences within the free-text eld (suc h as the signature or timestamp of email messages); trans-forming the di eren t morphological v arian ts of a w ord in to the same lexical unit; handling common sp elling mistak es, run-ons and w ord splits; substituting common expressions with k eyw ords (suc h as replacing 128.08.10.10 with \IP ad-dress"), etc. This step w as implemen ted with a set of con-v ersion rules sp ecifying the patterns to lo ok for within the free-text eld and the lexical units with whic h they should b e replaced.
 T ok enization breaks the free-text sen tences in to smaller units called tok ens. Our system uses white-space c haracters as de-limiters for a tok en. The disadv an tage of this metho d is that m ulti-w ord phrases (\sh ut do wn", \IP address", etc.), are brok en up in to separate tok ens. W e reduce the sev erit yof this problem b yiden tifying the most common w ord phrases and replacing them with a single lexical unit during the text con v ersion step.
 The term separation step partitions the tok ens in to v e distinct categories : k eyw ords, prop er names, alphab etical strings, sym bolic terms, and lo w-frequency terms. A list of k eyw ords con taining imp ortan t pro duct and parameter names is used to distinguish bet w een k eyw ords and non-k eyw ords app earing in the free-text eld. Irrelev an tproper names, sym b olic tok ens (consisting of digits and other non-alphab etical sym b ols), and lo w-frequency tok ens are remo v ed during this step. A list of prop er names of p eople and places is used to lter out irrelev an t names from the list of to-k ens generated b y the tok enization step. The lo w-frequency terms are remo v ed b y applying a user-sp eci ed threshold on the tok en list. Only k eyw ords and alphab etical strings (tok ens con taining only alphab etical letters) are used to con-struct the feature v ectors.
 The alphab etical strings then undergo further prepro cess-ing steps, whic h include con v ersion to lo w er case, stop w ord remo v al, sp elling c hec k, and stemming with P orter's su X x remo v al algorithm [10]. Ho w ev er, stemming ma y pro duce con icts bet w een the stemmed w ords and k eyw ords. F or example, the w ord b asic al ly will be stemmed to BASIC ,a pro duct name, while the stemmed w ord for dies ma y con-ict with the pro duct DI . As a result, w eha v etoiden tify p oten tial con icts and circum v en t the problem b yman ually undoing the stemmed w ords or adding new rules to the text con v ersion step. The ltered w ords are com bined with the list of k eyw ords found during the term separation step to form the set of features represen ting the free-text eld. The last step of the free-text prepro cessing stage in v olv es the construction of a feature v ector for ev ery case description extracted b y the data extraction mo dule. F or eac h term T of the description D i ,w e calculate its w eigh t w ij according to the metric where N is the total n um b er of case records, term frequency tf ij is the n um b er of o ccurrences of T j in D i , and do cumen t frequency d f j is the n um b er of case descriptions that con-tain T j . T erms T j of su X cien tly high w eigh t are retained as k eyw ords for description D i . This is a widely used measure in information retriev al [14, 13]. F rom the terms selected in the prepro cessing step, w ede-riv e the free-text feature space w e need to classify the case records. Using eac h w ord T j as a feature w ould lead to an unmanageable n um ber of dimensions. Instead, w e em-pirically reduce the dimensionalit y of the feature space b y grouping together related terms. This strategy w orks be-cause there are strong correlations b et w een the original term features. By exploiting this redundancy , the dimension of the feature space can be reduced to mere h undreds. W e used t w odi eren ttec hniques for dimension reduction: sin-gular v alue decomp osition (SVD) [6] of the original matrix of feature v ectors and term clustering using a h yp ergraph partitioning soft w are, called hMETIS [8, 7]. SVD is a w ell-kno wn metho d for dimensionalit y reduction b ecause it is capable of extracting most of the salien t features of the text do cumen ts and remo ving the noise presen t in the data. Clus-tering using hMETIS partitioning tec hnique has b een sho wn to pro duce higher qualit y clusters compared to traditional hierarc hical agglomerativ e clustering tec hniques and Auto-class [3, 9]. In [3, 9, 5], h yp ergraphs w ere constructed using frequen t itemsets generated b y the Apriori algorithm [2, 1]. Eac h v ertex w ould represen ta term while eac hh yp eredge w ould corresp ond to a frequen t itemset. Ho w ev er, it is not clear what is the b est w a y to assign an appropriate w eigh tto the h yp eredge. F or instance, [3, 9] ha v e suggested using the a v erage con dence of asso ciation rules generated from the frequen t itemset while [5] prop osed using an inter est factor. In con trast, our approac hw as sligh tly easier. F rom the orig-inal matrix of free-text features, w e computed the similarit y bet w een eac hpairof w ords using an L 2 distance measure W e sparsi ed the matrix b y c ho osing the k-largest v alues for eac hro w and setting the rest of the column en tries to zero. W e then assigned eac h non-zero en try in the sparsi-ed matrix as the w eigh t for the corresp onding edge in our graph. Finally ,w e use hMETIS to partition the graph in to highly-connected clusters of w ords.
 Both SVD and graph partitioning tec hniques pro v ed to b e e ectiv e. The results of the dimension reduction phase (the hmetis-disco v ered or SVD-deriv ed clusters) are themselv es in teresting, since they c haracterize the cases in a no v el man-ner. F or example, cluster f57 (T able 1) con tains terms suc h as badp v , latc h, partfail, softfail, switc ho v er, sw ap, unet; these terms p oin t to a sp ecial con trol net w ork pro duct family that features automatic switc hing to redundan t units when a failure has b een detected. 1 This approac hw as suggested in [14]. Other similarit y mea-sures suc h as cosine measure can also b e adopted. As men tioned earlier, one of the main goals of our text min-ing system is to build a classi cation mo del using histor-ical case records. The classi er supplies information that can b e used to impro v e future customer supp ort or existing pro ducts. Our text mining system uses the free-form text description and/or the xed-format header information to predict whether a new case record w ould b e resolv ed rapidly (within one hour) or w ould tak e a longer time. These t w o classes of cases are lab eled LESS and MORE , resp ectiv ely . W eha v e exp erimen ted with t w o inductiv e learning metho ds for building our classi er : W e construct decision trees with the C4.5 algorithm [11]. The algorithm b egins b y considering the en tire feature space as ha ving the same class lab el. It then recursiv ely partitions this space in a greedy manner to minimize impurit ydue to mixed class lab els within a particular region in the feature space. C4.5 uses the information gain ratio as the criteria for splitting. The output of this algorithm is a decision tree in whic heac hin termediate no de denotes the attribute c hosen to partition the feature space, and eac h leaf no de is one of the p ossible class lab els.
 W eusedRoC 2 as our naiv e-Ba y es classi er for this pro ject [12]. Unlik e decision tree induction algorithms, a naiv e Ba y es classi er constructs a classi cation mo del b y estimating the conditional probabilit y of eac h class lab el giv en a new in-stance of the feature v ector. The basic assumption in a naiv eBa y es classi er is that the classes are m utually exclu-siv e and exhaustiv e, while the features are indep enden tof eac h other. RoC handles only discrete v ariables; con tin uous v ariables are discretized. There are t w ow a ys to discretize con tin uous v ariables. The range b et w een the minim um and maxim um v alues can b e partitioned in to equally spaced in-terv als, or in to in terv als ha ving the same n um b er of cases. W e c hose C4.5 and naiv e Ba y es as our classi cation algo-rithms b ecause they are commonly used in text categoriza-tion and their results are easy to understand. Other clas-si cation sc hemes that can b e p oten tially useful include k-nearest neigh b ors, Supp ort V ector Mac hines, etc. Compar-a v ailable at h ttp://kmi.op en.ac.uk/pro jects/bkd/ isons with these classi cation metho ds will b e for future re-searc h. Our dataset con tains 20,816 cases collected o v er a duration of one y ear. Ab out 75% of the cases (in this sample) w ere resolv ed within one hour. W e built the classi cation mo del on a subset of the a v ailable data and ev aluated the di er-en t com binations of classi ers and dimension reduction tec h-niques.
 The b est com bination w as pro vided b y the naiv eBa y esian classi er with SVD (T able 2). W e used SVD to decomp ose the original matrix of features, X ,in to t w o matrices of sin-gular v ectors, U and V , and a diagonal matrix of singular v alues, D : By selecting k of the largest singular v alues in the diagonal matrix (and setting the rest of the en tries to zero), w ecan obtain a new matrix ~ X whic h b est appro ximates X in a least square sense. ~ X is obtained b y deleting the ro ws and columns in U and V corresp onding to small diagonal v alues in D . In our exp erimen ts, w e had used k = 100. Notice that with xed-format data, the classi er is biased to w ards the LESS class lab els. Ho w ev er, when free-form text is incorp orated, a less-biased mo del is obtained.
 With C4.5 classi er, the b est result is obtained using hMETIS clustering. In this approac h, eac h cluster b ecomes a new, comp osite feature of the dimensionally-reduced feature space. The C4.5 classi er with hMETIS consisten tly obtained a hit rate near 53% with a false alarm rate less than 10% | imp er-fect but w ell ab o v ec hance and the results pro duced b y xed-format data. Figure 3 illustrates the hit rate (of MORE ) and false alarm (T yp e I I error) for all the classi cation mo d-els built using C4.5 (with hMETIS clustering). These mo d-els w ere generated after p erforming a 10-fold cross-v alidation on the dataset. Because this is an observ ational dataset, w e ha v e no analytical means to assess the maxim um attain-able prediction accuracy . In short, our exp erimen tal results rev ealed that there is some impro v emen t in the predictiv e accuracy of classi cation mo dels when free-form text is in-corp orated.
 T able 3 sho ws an example of a decision tree built with C4.5 using both header and cluster data. The learned cluster features (100 of them) are all assigned arbitrary names, e.g. f12, f92. The initial splits are frequen tly on the header elds \call status" and \pro duct" (the pro duct family). Cluster f80, highligh ted in T able 3, is a go o d free-text indi-Figure 3: Hit rate v ersus false alarm rate for C4.5 clas-cator of longer ( MORE ) cases. Examination of f80 w ords sho ws it has a large prop ortion of computer net w ork terms, suc h as \lo opbac k", \h ub", \nic", and \gatew a y". Clus-ter f86, another trouble indicator, includes some in teresting terms: \o v erload", \fear", \sleep", \attac k", \nerv ou", \ex-plicit". A precise in terpretation of these clusters is a job for domain exp erts. Our results indicate ho w the initial assem-bly can b e signi can tly automated. Our initial results sho w that incorp orating free-text informa-tion in service databases can p oten tially impro v e the classi -cation mo del. W e are w orking with the service organization to further dev elop these results. These results come in t w o areas: Early con v ersations with our domain exp ert indicate that this clustering pro vides useful information ab out classes of service call. This information could b e used either to im-pro v e handling of service calls, or to impro v e pro ducts (and their do cumen tation) to a v oid the need for supp ort. This researc h could help service cen ters to iden tify case cat-egories that are exp ensiv e to service. A t the presen t time, service cen ters cannot automatically analyze whic ht yp es of cases are consuming the most engineering resources. Case categorization and cost summaries suc h as the ones w eha v e illustrated could enable the service cen tertolocate itsprob-lem areas, and to quan tify the resources exp ended in those areas. This information can b e used to solv e the underlying problems that are generating service requests: p o or do cu-men tation on a particular pro duct, fault yhardw are design, etc.
 The metho dology dev elop ed here could b e applied to other service cen ters. Some e ort w ould b e required to adapt to a new domain (di eren t database format, new acron yms), but the metho dology w ould remain unc hanged.
 This pro ject w ould not ha v e b een p ossible without the ad-vice of our principal domain exp ert, Keith Curtis, Honey-w ell Hi-Sp ec Solutions. W ew ould also lik e to thank George Karypis (hMETIS), Marco Ramoni (RoC) and Ross Quin-lan (C4.5) for making a v ailable the soft w are used in our exp erimen ts. [1] R. Agra w al, T. Imielinski, and A. Sw ami. Database [2] R. Agra w al, T. Imielinski, and A. Sw ami. Mining [3] D. Boley , M. Gini, R. Gross, E. Han, K. Hastings, [4] S. Busemann, S. Sc hmeier, and R. Arens. Message [5] C. Clifton and R. Co oley .T op cat: Data mining for [6] S. Deerw ester, S. Dumais, G. F urnas, T. Landauer, [7] G. Karypis and V. Kumar. Multilev el k -w a y [8] G. Karypis, A. R., V. Kumar, and S. Shekhar.
 [9] J. Mo ore, E. Han, D. Boley , M. Gini, R. Gross, [10] M. P orter. An algorithm for su X x stripping. Pr o gr am , [11] J. Quinlan. C4.5: Pr o gr ams for Machine L e arning . [12] M. Ramoni and P . Sebastiani. Ba y esian metho ds for [13] G. Salton and B. C. T erm-w eigh ting approac hes in [14] G. Salton and M. McGill. Intr o duction to Mo dern Decision tree:
