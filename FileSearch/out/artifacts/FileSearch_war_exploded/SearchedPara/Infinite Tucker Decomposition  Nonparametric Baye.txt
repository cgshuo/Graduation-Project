 Zenglin Xu, Feng Yan { XU 218, YAN 12 } @ PURDUE . EDU Many real-world datasets with multiple aspects can be described by tensors (i.e., multiway arrays). For exam-ple, patient drug responses can be represented by a ten-sor with four modes ( person, medicine, biomarker, time ) and user customer ratings by a tensor with three modes ( user, item, time ). Given tensor-valued data, we want to model complex interactions embedded in data (e.g., drug interactions) and predict missing elements (e.g., unknown drug responses). Traditional multiway factor models X  such as the Tucker decomposition (Tucker, 1966) and CANDECOMP/PARAFAC (CP) (Harshman, 1970) X  X ave been applied for various applications ( e.g. , computer vi-sion (Shashua &amp; Hazan, 2005) and chemometrics (Acar et al., 2011) etc). These models, however, face serious chal-lenges for modeling complex multiway interactions. First, interactions between entities in each mode may be coupled together and highly nonlinear. The classical multi-linear models cannot capture these intricate relationships. Sec-ond, the data are often noisy, but the classical models are not designed to deal with noisy observations. Third, the data may contain many missing values. We need to first impute the missing values before we can apply the clas-sical multiway factor models. Forth, the data may not be restricted to real values: they can be binary as in dynamic network data or have ordinal values for user-movie-ratings. But the classical models simply treat them as continuous data  X  this treatment would lead to degenerated predictive performance.
 To address these challenges we propose a nonparametric Bayesian multiway analysis model, InfTucker . Based on latent Gaussian processes or t processes, it conducts the Tucker decomposition in an infinite dimensional feature space. It generalizes the elegant work of Chu &amp; Ghahra-mani (2009) by capturing nonlinear interactions between different tensor modes. Grounded in a probabilistic frame-work, it naturally handles noisy observations and missing data. Furthermore, it handles various data types X  X inary or continuous X  X y simply using suitable data likelihoods. Although InfTucker offers an elegant solution to multiway analysis, learning the model from data is computationally challenging. To overcome this challenge, we develop an efficient variational Bayesian approach that explores tensor structures to significantly reduce the computational cost. This efficient inference technique also enables the usage of nonlinear covariance functions for latent Gaussian and t processes on datasets with reasonably large sizes. Our experimental results on chemometrics and social net-work datasets demonstrate that the InfTucker achieves significantly higher prediction accuracy than state-of-the-art tensor decomposition approaches X  X ncluding High Or-der Singular Value Decomposition (HOSVD) (Lathauwer et al., 2000), Weighted CP (Acar et al., 2011) and nonneg-ative tensor decomposition (Shashua &amp; Hazan, 2005). the k -th mode has n k dimensions. We use y i to denote the i = ( i 1 ,...,i K ) entry of Y .
 In order to define tensor decomposition, two linear al-gebra operations on matrix are generalized to tensor X  vectorization and tensor-matrix multiplication (Kolda &amp; Bader, 2009). We define the vectorization operation, de-noted by vec( Y ) , to stack the tensor entries into a n = Q k =1 n k by 1 vector. The entry i = ( i 1 ,...,i K ) of Y is mapped to the entry at position j = i K + P K  X  1 i =1 The mode-k tensor-matrix multiplication of a tensor W  X  R which is of size r 1  X  ...  X  r k  X  1  X  s  X  r k +1  X  ...  X  r It takes the products W  X  X  elements in the k -th dimension by the associated elements in the rows of U and sums the products. The corresponding entry-wise definition is There are two families of tensor decomposition, the Tucker family and the CP family. The Tucker family extends bi-linear factorization models on matrix to handle tensor. For-mally, Tucker decomposition defines a multi-linear form on tensor.
 Definition 1 (Tucker decomposition) The Tucker decom-position for a K -mode tensor M X  R n 1  X  ...  X  n K is M = W  X  1 U (1)  X  2 ...  X  K U ( K ) = [[ W ; U (1) ,..., U R The last identity in (2) is introduced by Kolda &amp; Bader (2009) to compactly represent Tucker decomposition. We can relate Tucker decomposition with matrix-vector multi-plication using the vec operation on tensors.
 Proposition 2 The Tucker decomposition (2) can be repre-sented in a vectorized form where  X  is the Kronecker product.
 The alternating least square (ALS) method has been used to solve Tucker decomposition (Kolda &amp; Bader, 2009). A graphical illustration of the Tucker decomposition on a 3-mode tensor is shown in Figure 1.
 The CP family is a restricted form of the Tucker fam-ily. The entry-wise definition of CP for a tensor M is m ing Tucker decomposition to an infinite feature space using tensor-variate Gaussian or t processes in this paper. In this section we present the infinite Tucker decomposi-tion based on latent Gaussian processes and t processes. The following discussion is primarily for latent Gaussian processes. The model derivation for latent t processes is similar to that of latent Gaussian processes.
 We extend classical Tucker decomposition in three aspects: i) flexible noise models for both continuous and binary ob-servations; ii) an infinite core tensor to model complex in-teractions; and iii) latent Gaussian process or t prior pro-cess. More specifically, we assume the observed tensor Y is sampled from a latent real-valued tensor M via a proba-bilistic noise model p ( Y|M ) = Q i p ( y i | m i ) . 3.1 Tensor-variate Gaussian processes We start by conducting Tucker decomposition for M with a core tensor W of uniform size, r 1 = ... = r K = r . We assign independent standard normal distribution on W , which is equivalent to where I is the identity matrix. In light of the vectorized representation of Tucker decomposition (3), it is easy to see that vec( M ) follows a normal distribution by marginaliz-ing out vec( W ) . The covariance matrix of vec( M ) has a Kronecker product structure. Because the covariance of any two elements m i and m j in the tensor M is we can interpret  X  ( k ) as the covariance on the k -th mode. To model nonlinear relationships, we replace each row u k of the latent feature matrix U ( k ) by a nonlinear feature mapping  X  ( u k i ) . We obtain an equivalent nonlinear co-variance matrix  X  ( k ) from nonlinear covariance function core tensor W after the feature mapping has the size of the mapped feature vector  X  ( u ( k ) i ) on mode k , which could be infinite. A rigorous exposition on the relation between core tensors and the feature vectors is given in Appendix B. Note that by marginalizing out the core W in our model means, we effectively use all possible values of the latent core W for our model estimation. Using all possible values of W  X  instead of a particular single value of W  X  can greatly reduce the chance of overfitting and boost predic-tive performance (as demonstrated in our experiments). We can also rewrite the normal probability density function (p.d.f.) in (5) using the vectorization relation (3) as where n = Q k n k and kXk = p P i x 2 i . The above equa-tion (7) essentially defines a tensor-variate normal distribu-tion TN ( 0 ,  X  (1) ,...,  X  ( K ) ) that naturally generalizes the definition of matrix-variate normal distributions (Gupta &amp; Nagar, 2000).
 Combining nonlinear covariance functions and tensor-variate normal distributions, We can define tensor-variate Gaussian processes on M .
 Definition 3 (Tensor-variate Gaussian process) A tensor-variate Gaussian process is a collection of random joint probability over { ( u (1) i variate normal distribution density function. Specifically, given U ( k ) , the zero mean tensor-variate Gaussian process on M is denoted by The finite probability density function is p ( M| U (1) ,..., U ( K ) ) = TN ( M ; 0 ,  X  (1) ,...,  X  where  X  ( k ) = k ( U k , U k ) is the covariance matrix. Intuitively, a tensor-variate Gaussian process is equivalent to defining infinite Tucker decomposition with infinite fea-ture mapping  X  ( u ) and an infinite core tensor W  X  whose elements are independent standard normal random vari-ables Definition 3 shows that probabilistic infinite Tucker decom-position of M can be realized by modeling M as a draw from a tensor-variate Gaussian process. Our definition of tensor-variate Gaussian processes generalizes matrix-variate Gaussian processes defined in (Yu et al., 2007) and (Yan et al., 2011).
 Finally, to encourage sparsity in estimated u ( k ) i  X  X or easy model interpretation X  X e use a Laplace prior u ( k ) i L (  X  )  X  exp(  X   X  k u ( k ) i k 1 ) . 3.2 Tensor-variate t processes Because of the strong relation between t -distributions and normal distributions X  t distributions can be regarded as mixtures of Gaussian distributions weighted by Gamma distributions, we can easily define tensor-variate t pro-cesses as an alternative way to define infinite Tucker de-composition: Definition 4 (Tensor-variate t Processes) M follows a tensor-variate t process TTP (  X , 0 ,k (  X  ,  X  )) with a degree of freedom  X  &gt; 2 , if M follows tensor t distribution with the following density TT ( M ;  X , 0 , {  X  ( k ) } K k =1 ) = where  X ( x ) is the Gamma function. 3.3 Noise models We use a noise model p ( Y|M ) to link the infinite Tucker decomposition and the tensor observation Y .
 Probit model: In this case, each entry of the observa-tion is binary; that is, y i  X  { 0 , 1 } . A probit function observation. Note that  X (  X  ) is the standard normal cumu-lative distribution function. The probit model can be ex-tended to handle multi-class and ordinal data as well. Gaussian model: We use a Gaussian likelihood p ( y i | m i ) = N ( y i | m i , X  2 ) to model the real-valued obser-vation y i .
 Missing values: We allow missing values in the observa-tion. Let O denote the indices of the observed entries in Y . Then we have p ( Y O |M O ) as the likelihood. Given the observed tensor Y , we aim to obtain the MAP estimate of component matrices U ( k ) by maximizing the marginal likelihood p ( Y|{ U ( k ) } K k =1 ) p ( { U ( k ) } K grating out M in the above equation is intractable for the probit noise. Therefore, we develop a variational expec-tation maximization (EM) algorithm. The inference for Gaussian noise is exact, but it can be derived in the vari-ational EM framework as a special case. In the following paragraphs, we first present the inference and prediction algorithms for both of the noise models, and then describe an efficient algebraic approach to significantly reduce the computation complexity. Due to the space limitation, we only describe the inference algorithm for tensor-variate t processes. Tensor-variate Gaussian process inference can be derived similarly. 4.1 Inference Probit noise: We follow the data augmentation scheme by Albert &amp; Chib (1993) to decompose the probit model into tor function, we have p ( z i | m i ) = N ( z i | m i , 1) It is well known that a t distribution can be factorized into a normal distribution convolved with a Gamma distribution, such that The joint probability likelihood with data augmentation is where U = U (1) ,..., U ( K ) , p ( M|  X , U ) and p (  X  ) are the tensor-variate normal distribution and the Gamma distribu-tion in (12). p ( U ) is the Laplace prior.
 Our variational EM algorithm consists of a variational E-step and a gradient-based M-step. In the E-step, we approx-imate the posterior distribution p ( Z , M , X  |Y , U ) by a fully factorized distribution q ( Z , M , X  ) = q ( Z ) q ( M ) q (  X  ) . Variational inference minimizes the Kullback-Leibler (KL) divergence between the approximate posterior and the true posterior. The variational approach optimizes one approximate dis-tribution, e.g. , q ( Z ) , in (13) at a time, while having all the other approximate distributions fixed. We loop over q ( Z ) , q ( M ) and q (  X  ) to iteratively optimize the KL divergence until convergence.
 Given q ( M ) and q (  X  ) , the q ( z i ) is a truncated normal dis-tribution Given q ( Z ) and q (  X  ) , it is more convenient to write the optimized approximate distribution for M in its vectorized form. Let be the covariance matrix of the tensor TP prior, we have The optimized q (  X  ) is also a Gamma distribution: q (  X  ) = Gam (  X  ;  X  1 , X  2 ) , E q [  X  ] = Based on the variational approximate distribution obtained in the E-step, we maximize the expected log likelihood over U in the M-step. After eliminating constant terms, we need to solve the fol-lowing optimization problem with regularization on U ( k ) min where  X  = E q [  X  ] . In the above equation (21),  X  k ( U ( k ) , U ( k ) ) is considered as a function of U ( k )  X  are the statistics computed in the E-step, and they have fixed values. The gradient of f ( U ) w.r.t. to a scalar u With an ` 1 penalty on f ( U ) , we choose a projected scaled subgradient L-BFGS algorithm for optimization X  X ue to its excellent performance (Schmidt, 2010).
 Gaussian noise: The inference for the regression case fol-lows the same format as the probit noise case. E q [ Z ] is replaced by Y O , and we do not need to update q ( Z ) . The variational EM algorithm is only applied to the observed entries of M O and the covariance [ X  p ] O , O . 4.2 Prediction Probit noise: Given a missing value index i = ( i ,...,i K ) , the predictive distribution is p ( y i |Y )  X  The above integral is intractable, so we replace  X  integral q (  X  ) d X  by the mode of its approximate posterior distribu-tion  X   X  = (  X  1  X  1) / X  2 , thus the predictive distribution is approximated by
Z = where Gaussian noise: The predictive distribution for the regres-sion case can be evaluate by a similar integral, which gives 4.3 Efficient Algorithm If n = Q K k =1 n k is the number of all elements in Y , a na  X   X ve implementation of the above algorithm requires pro-hibitive O ( n 3 ) time complexity and O ( n 2 ) space complex-ity for each EM iteration. The key computation bottlenecks are the computations involving  X  , which are tr(  X   X  1 p  X  ) in (21), tr  X  ( k )  X  in (22) and  X  vec( E q [ Z ]) in (18). To avoid this high complexity, we can make use of the Kro-necker product structure and generalize the strategy used by Yan et al. (2011) on matrix-variate Gaussian processes. We assume E q [  X  ] = 1 to simplify the computation. It is easy to adapt our computation strategies to E q [  X  ] 6 = 1 . Let  X  tion (SVD) of the covariance matrix  X  ( k ) , V ( k ) is an or-thogonal matrix and  X  ( k ) is a diagonal matrix.  X  can be represented as  X  =  X  p ( I +  X  p )  X  1 = V X V &gt; , where It is obvious that V is an orthogonal matrix and  X  is a diagonal matrix. The above relation implies that we can actually compute the singular value decomposition of  X  = V X V &gt; from covariance matrices  X  ( k ) .
 Computing tr(  X   X  1 p  X  ) and tr  X  ( k )  X  : being a computed statistics in the E-step, diag(  X  ) denotes the diagonal elements of  X  , and D is a tensor of size n 1  X  ...  X  n K , such that vec( D ) = diag(  X  ) . In order to efficiently compute tr(  X   X  1 p  X  ) appearing in equation (21), we use the following relations Both time and space complexities of the last formula (26) is O ( n ) . The technique of (26) can be used to efficient calculate the last term tr  X  ( k )  X  in the gradient (22) as well. Furthermore, if we use the incremental EM algo-rithm, which only takes one gradient descent step for each M step, equation (26) can be simplified such that d k is the inverse of the eigenvalues of  X  ( k ) . In practice, this incremental EM algorithm performs as good as EM with full optimization.
 Computing  X  vec( E q [ Z ]) : For any tensor A of the same size as D ,  X  vec( A ) means multiplying the j -th element of vec( A ) by the  X  jj , which is the j -th element of vec( D ) . So we have  X  vec( A ) = vec( D A ) , where denotes the entry-wise product. In light of this relation and (3), we can efficiently compute (18) by simplified as:  X  vec( E q [ Z ]) = vec([[ G D ; V (1) ,..., V ( K ) ]]) . (27) Equation (27) greatly reduces the time and space complex-ities of (18). The time complexity of each mode-k multipli-cation is O ( n k n ) , and SVD of the covariance matrix on the k -th mode costs O ( n 3 k ) . So the total time complexity in-cluding other computation steps is O ( P K k =1 n 3 k + n we assume the length of each mode are the same, the time complexity of our efficient algorithm is O ( n 1+ 1 K ) which is a huge reduction from O ( n 3 ) of the na  X   X ve algorithm. The space complexity is reduced to O ( n + P K k =1 n 2 k ) and O ( n ) for tensors with uniform size for each mode, because we only need to store the covariance matrix for each mode rather than the n 2  X  n 2 covariance matrix  X  p . We can further reduce the complexities by approximating the co-variance matrices via truncated SVD. The InfTucker model extends Probabilistic PCA (PPCA) (Tipping &amp; Bishop, 1999) and Gaussian process latent variable models (GPLVMs) (Lawrence, 2006). Using a Dirac covariance and a linear covariance for the two modes of a matrix, our model reduces to the PPCA model TN (0 , I , UU &gt; ) ; similarly, using a Dirac covariance and a nonlinear covariance K , our model reduces to the GPLVM model TN (0 , K , I ) . While PPCA and GPLVM model in-teractions of one mode of a matrix and ignore the joint in-teractions of two modes, InfTucker does. Our model is also related to previous matrix-variate GPs (Yu et al., 2007). The main difference lies in the fact they used linear covari-ance functions to reduce the computational complexities and dealt with matrix-variate data for online recommen-dation and link prediction. Hoff (2011) proposed a hier-archical Bayesian extension to CANDECOMP/PARAFAC that captures the interactions of component matrices. Por-teous et al. (2008) generalized hierarchical Dirichlet pro-cesses to handle tensor data. Unlike these approaches, ours can handle non-Gaussian noise and uses nonlinear covari-ance functions to model complex interactions. Both Hoff (2011) and Porteous et al. (2008) used Gibbs samplers for inference X  X equiring high computational cost and making their approach infeasible for tensors with moderate and large sizes.
 The most closely related work is the probabilistic Tucker decomposition (pTucker) model (Chu &amp; Ghahramani, 2009); actually the GP-based InfTucker with Gaussian noise function reduces to pTucker as a special case when using a linear covariance function. Our TP-based InfTucker further differs from pTucker by marginalizing out a scal-ing hyperparameter of the covariance function and han-dles non-Gaussian noise functions. A kernelized version of pTucker was also discussed by the authors without con-ducting any experiment on it. The kernelized pTucker ap-proach captures nonlinear relationships between observa-tions in the same way as kernel PCA. This is fundamen-tally different from our approach, which captures nonlinear relationships between the latent factors. In addition, the kernelized pTucker approach needs to explicitly calculate the Kronecker product of the latent factors, so it is difficult for pTucker to scale up to large datasets. In contrast, our in-ference method does not conduct any expensive Kronecker product X  X e have exploited properties of Kronecker prod-ucts to greatly simplify the computations.
 To handle missing data, enhance model interpretability, and avoid overfitting, several extensions (e.g., using nonnega-tivity constraints) to tensor decomposition have been pro-posed, including nonnegative tensor decomposition (NTD) (Shashua &amp; Hazan, 2005) and Weighted tensor decomposi-tion (WTD) (Acar et al., 2011). Unlike ours, these models either solve the core tensors explicitly, or do not handle nonlinear multiway interactions. We use InfTucker gp and InfTucker tp to denote the two new infinite Tucker decomposition models based on tensor-variate Gaussian and t processes, respectively. To evaluate them, we conducted two sets of experiments, one on con-tinuous tensor data and the other on binary tensor data, to evaluate the prediction accuracy on hold-out data. The hy-perparameters are determined using cross-validation in our MAP inference framework. For both experiments, we com-pared InfTucker with the following tensor decomposition methods: CANDECOMP/PARAFAC (CP), Tucker decom-position (TD), Nonnegative CP (NCP), High Order SVD (HOSVD), Weighted CP (WCP) and Probabilistic Tucker Decomposition (PTD). We implemented PTD as described in the paper by Chu &amp; Ghahramani (2009) and applied to a small continuous tensor data ( bread as described in the 6.1). To handle larger and binary datasets, we used probit models and the efficient computation techniques described in Section 4.3 for PTD. For the other methods, we used the implementation of the tensor data analysis toolbox 2 devel-oped by T. G. Kolda. 6.1 Experiment on continuous tensor data Experimental setting. We used three continuous chemo-metrics datasets 3 , amino , bread , and flow injection . The dimensions of the tensors are 5  X  51  X  201, 10  X  11  X  8, 12  X  100  X  89, respectively.
 All the above tensor data were normalized such that each element of the tensor has zero mean and unit variance (based on the vectorized representations). For each ten-sor, we randomly split it via 5-fold cross validation: each time four folds are used for training and one fold for test-ing. This procedure was repeated 10 times, each time with a different partition for the 5-fold cross validation. In InfTucker tp , the degree of freedom  X  in the tensor-variate t process is fixed to 10 . We chose the Gaussian/exponential t = 1 , 2 and  X  is selected from [0 . 01 : 0 . 05 : 1] by 5-fold cross validation. The regularization parameter  X  for InfTucker gp and InfTucker tp is chosen from { 1 , 10 , 100 } . Results. We compared the the prediction accuracy val-ues of all the approaches on hold-out elements of the ten-sor data. 80% of the data are held out for training and the remaining 20% are used for testing in each run. For each comparison, we used the same number of latent fac-tors, denoted as r , for all the approaches. We varied r from 3 to 5 and computed the averaged mean square er-rors (MSEs) and the standard errors of the MSEs. Based on cross-validation, we set r = 3 . The MSEs on the three datasets are summarized in Table 1. Based on the predic-tion accuracies, PTD and WCP tie on the third best, while HOSVD is the worst ( perhaps due to the strong nonnega-tivity constraint on the latent factors). Clearly, InfTucker achieved higher prediction accuracies than all the previous approaches on all the datasets, and InfTucker tp further out-performed InfTucker gp for most cases. 6.2 Experiment on binary tensor data Experimental setting. We extracted three binary so-cial network datasets, Enron , Digg1 , and Digg2 , for our experimental evaluation. Enron is a relational dataset de-scribing the three-way relationship: sender-receiver-time. This dataset, extracted from the Enron email dataset 4 , has the dimensionality of 203  X  203  X  200 with 0.01% non-zero elements. The Digg1 and Digg2 datasets were all ex-tracted from a social news website digg.com 5 . Digg1 describes a three-way interaction: news-keyword-topic, and Digg2 describes a four-way interaction: user-news-keyword-topic. Digg1 has the dimensionality of 581  X  124  X  48 with 0.024% non-zero elements, and Digg2 has the dimensionality of 22  X  109  X  330  X  30 with only 0.002% non-zero elements. Apparently these datasets are very sparse.
 Results. We chose r from the range { 3,5,8,10,15,20 } based on cross-validation. Since the data are binary, we evaluated all these approaches by area-under-curve (AUC) values by averaged over 50 runs. 80% of the data are used for training and 20% are used for testing in each run. For InfTucker , the values to compute the AUCs are the pre-dicted probability obtained from equation (24). For other methods, the values to compute the AUCs are the recon-structed values. The larger the averaged AUC value an approach achieves, the better it is. We reported the aver-aged AUC values for all algorithms in Figure 2. Again, the proposed InfTucker gp and InfTucker tp approaches signif-icantly outperform all the others. Note that the nonproba-bilistic approaches X  X uch as CP and TD X -suffer severely from the least square minimization; given the sparse and binary training data, the least-square-minimization leads to too many predictions with zero values, a result of both over-fitting and mis-model fitting. This experimental compari-son fully demonstrates the advantages of InfTucker (stem-ming from the right noise models and the nonparametric Bayesian treatment). To conduct multiway data analysis, we have proposed a new nonparametric Bayesian tensor decomposition frame-work, InfTucker , where the observed tensor is modeled as a sample from stochastic processes on tensors. In particular, we have employed tensor-variate Gaussian and t processes. This new framework can model nonlinear interactions be-tween multi-aspects of the tensor data, handle missing data and noise, and quantify prediction confidence (based on predictive posterior distributions). We have also presented an efficient variational method to estimate InfTucker from data. Experimental results on chemometrics and social net-work datasets demonstrate that the superior predictive per-formance of InfTucker over the alternative tensor decom-position approaches.
 Acknowledgment This work was supported by NSF IIS-0916443, NSF CAREER award IIS-1054903, and the Cen-ter for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370.
