 1. Introduction
There are different types of summary depending on their purpose or function: indicative, informative, aggregative or critical. These summaries can be generated without taking into account the particular user for whom they are intended or, on the contrary, they can be adapted to the peculiarities  X  previous knowledge, areas of interest, or information needs  X  of the reader or group of readers that they are generated for.
In this work, personalized summarization is understood as a process of summarization that preserves the specific information that is relevant for a given user profile, rather than information that truly summarizes the content of the news item. The potential of summary personalization is high, because a summary that would be useless to decide the relevance of a document if summarized in a generic manner, may be useful if the right sentences are selected that match the user interest. In this paper we defend the use of a personalized summa-rization facility to maximize the density of relevance of selections sent by a personalized information system to a given user.

If automatic summarization is to be used as part of a process of intelligent information access, it is crucial to have some means of measuring how much information is lost during the summarization process, and how that information loss may affect the ability of the user to judge the relevance of a given document with respect to his particular information needs. To study this problem, the effect of summarization on the information content of a document needs to be studied. The aim is to determine if the use of summaries saves time without significant loss of effectivity in the relevance decisions.

It is reasonable to think that, especially for high levels of understanding, a summary that does not take into account the needs of the user may be too general to be useful. In particular, in the context of information  X  such as information filtering or personalized information systems  X  user adapted summarization would be a better way of presenting contents than simply using titles or the first few lines of a document.
In this paper we present an automatic personalization summarization process that has been applied in the frame of reference of a personalization system for a digital newspaper. Sections 2 and 3 describe the techniques for the generation of personalized summaries and the evaluation of them, respectively. Section 4 presents the personalized summarization method. Section 5 describes the personalization system used as framework of ref-erence. Section 6 presents the evaluation methodology used. The results of the different experiments are given in Section 7 . Section 8 outlines the main conclusions. 2. Techniques for the generation of personalized summaries
Sentence extraction techniques for building generic summaries are described first, followed by the specific techniques applied to the generation of personalized summaries. 2.1. Sentence extraction
Faced with the variety in type and domains of available documents, sentence extraction techniques are attractive for the generation of personalized summaries. The techniques are based on a phase of analysis dur-ing which the segments of the text (extraction units, usually sentences or paragraphs) that contain the most relevant information are extracted. The degree of relevance of each one of them can be obtained by means of a linear combination of the weights that result from a set of different features ( Edmundson, 1969 ). These features can be positional (if they take into account the position of the segment within the document), linguis-occurrence of certain words).

The summary results from concatenating these selected segments in the order in which they appear in the original document. The length of the summary may vary according to the desired percentage of compression.
From the experiments presented in ( Morris, Kasper, &amp; Adams, 1992 ) one can assume that a percentage of com-pression of between 20% and 30% of the original may be informative enough to replace the original document.
The most frequently used methods for sentence extraction are ( Hahn &amp; Mani, 2000 ): position, indicative expressions, thematic words, proper nouns, text typography, and title. In particular, the title method consist on select those sentences that contain the non-empty words (those which are not on a list of stop words) that appear in the title of the document ( Edmundson, 1969; Teufel &amp; Moens, 1997 ).

In most systems the weights associated to each method, within the linear combination that produces the final value associated to each segment, are adjusted manually. This decision is justified by the fact that the contribution of each method depends on the genre of the texts that are being summarized. However, a clas-sifier may be used to determine the optimal values for these parameters by means of machine learning tech-generated by human experts and their associated texts to determine which the best weights for the linear com-bination are. The problems with this approach are the need for a corpus of summaries and that the resulting weights depend on the genre of the documents in the corpus.

The main disadvantage of sentence extraction techniques is the possible problem of lack of coherence in the resulting summary. One way to reduce this problem is to use a paragraph instead of a sentence as extraction lems of legibility. If the lack of coherence is due to unsolvable anaphoric references, it may be eliminated by: not including in the summary sentences with such references ( Brandow, Mitze, &amp; Rau, 1995 ), by including additional sentences immediately preceding the anaphoric reference ( Nanba &amp; Okumura, 2000 ), or by includ-ing the necessary sentences to solve the anaphora even if they are not immediately preceding the problematic sentence ( Paice, 1990 ). The main disadvantage of adding extra segments is that, given the restrictions on size, they may push out of the summary sentences with relatively higher weight, thereby degrading the quality of the summary. Another way of solving the lack of coherence is to detect expressions that connect separate of a sentence and the previous sentence is not part of the summary.

On the positive side, the sentence extraction method has some justification: approximately 80% of the sen-tences included in manually built summaries appear with little or no modification in the original text ( Kupiec et al., 1995 ). On the other hand, there were no significant differences on reading comprehension texts over manual and automatic summaries, when evaluators where asked to select one out of five possible answers to questions about the content of the text ( Morris et al., 1992 ). 2.2. Personalized summaries
Personalized summaries are not a new idea. The early works on summarization ( Luhn, 1958 ) already men-tion the possibility of personalizing summaries by adapting them to particular areas of interest. To this end, sentences that contain words related to the reader X  X  interests are given more importance. On the other hand, experiments on summary evaluation by users ( Paice &amp; Jones, 1993 ) showed that users tend to select the parts of the text that are more closely related to their interests.

To build a personalized or user-adapted summary a representation of the interests of the corresponding user is required. This representation may vary from a set of keywords ( Man  X  a et al., 1999 ) to a complex user Gerva  X  s, &amp; Garc X   X  a, 2005 ).

The method presented above for occurrence of words from the title in the sentences of the body of the text may be generalized to cover the occurrence of words that may be especially important for a given user. For instance, those words that appear in the user query presented to an information retrieval system, or those words present in a user model. In this way, personalized summaries may be generated adapted to the needs of the user expressed as a query or a user model.

Certain approaches ( Tombros &amp; Sanderson, 1998 ) generate user adapted summaries by combining posi-tion, title, and words-in-the-user-query methods. In ( Carbonell et al., 1997 ) the method for accounting for words in the user query is used, though the introduction of redundant sentences is avoided. In ( Sanderson, 1998 ) the passage ( Callan, 1994 ) most relevant to the query, previously expanded with the words that occur most frequently in the context in which the words of the query appear in the first documents retrieved, is selected. An expansion of the query using WordNet is carried out in ( Man  X  a et al., 1999 ). Another type of frequent words in the sentences that are more relevant to the query. 3. Techniques for the evaluation of summary generation
From the beginning of research on automatic summary generation to the present day, several proposals have arisen (individual and collective) to try to find a consensus on the various aspects related to evaluation, for instance, the SUMMAC ( Mani et al., 2002 ) or DUC 2 conferences. However, no agreement has been reached yet over which is the best evaluation method, due to the complexity of the task.

Among the reasons for this lack of agreement, the following may be mentioned ( Mani, 2001 ): difficulty for agreeing on when an automatically generated summary is good (comparing with an ideal summary generated by a person is not enough), the need to resort to real humans to judge the summaries (this makes the process difficult to carry out and difficult to repeat, additionally there are problems with the degree of agreement between different human judges), the need for metrics that take into account aspects that go beyond the read-ability of the summary, such as the relevance of the obtained information, how well the summary meets the user needs, or how well it satisfies its intended use.

The different approaches to summary evaluation are presented below. They can be classified, according to direct analysis of the summary by means of some metric for establishing its quality. The latter are based on judgments in terms of how useful the summaries are when a specific task is carried out over the summaries instead of over the original documents. 3.1. Direct or intrinsic evaluation
This type of evaluation can take into account ( Baldwin et al., 2000 ) both quality criteria (such as grammat-ical correctness, or text cohesion), and criteria about information coverage. For some of these aspects the sum-mary is compared with the original document, as in the case of checking for the appearance in the summary of the main ingredients of the original text. For others, such as grammatical correctness or cohesion, the sum-mary is considered in isolation.

Human judges are required for this type of evaluation, to build an initial summary to be used for compar-ison or to judge the degree of adequacy and correction of the automatically generated summaries. This ments must be applied to large collections of text, such as the ones used for the DUC conferences. Addition-ally, human judgments may disagree. Large enough disagreements may invalidate the experiments. With respect to the readability of the summaries, the following linguistic criteria were applied at the
DUC X 05 conference ( Dang, 2005 ): grammatical errors, redundancy errors, reference errors, focus, structure and coherence. Each criterion was graded on a scale of 5 points  X  very poor, poor, acceptable, good, very good.

If a good evaluation is obtained according to the criteria above, a certain readability of the summary can be guaranteed. However, this does not take into account what use the summary is intended for. Neither does it guarantee that the summary is good. Other factors must be used to determine the relevance of the information that it includes or its usefulness for a given task.

The most commonly used method for measuring the relevance of the content of an automatically generated summary is the comparison with an  X  X deal X  summary built manually. However, the lack of agreement between human judges seems to indicate that this  X  X deal X  summary does not always exist. For this reason, it is possible that a summary contain relevant information but it is not considered good because it does not resemble the  X  X deal X  summary.

The metrics used for information retrieval, precision, recall and F frequently to evaluate the relation between automatic summaries and the ideal summary. The relevant param-eter is the number of sentences that appear both in the automatic summary and the ideal summary, divided by the number of sentences in the automatic summary (recall) or the number of sentences in the ideal summary (precision).

A first solution, to the direct comparison of sentences, is to leave to the criteria of the judges which sen-tences of the automatic summary express part of the information of the reference summary. These same judges also determine the percentage of information of the ideal summary that is covered by the automatic summary or measures ( Donaway, Drummey, &amp; Mather, 2000 ) of sentence ordering and content similarity based on the cosine measure of the vector space model ( Salton &amp; McGill, 1983 ).

The SEE 3 (Summarization Evaluation Environment) tool, used from the DUC04 conference ( Over &amp; Yen, 2004 ), facilitates the direct evaluation of automatic summaries. This tool compares a model summary with a peer summary, producing judgments of peer linguistic quality (7 questions), coverage on each model unit by the peer (recall) and relevance of peer-only material. The starting point is a manual summary (model sum-mary) divided into model units (MUs). This summary is compared with the automatically generated summa-ries (peer summaries) divided into peer units (PUs). Manual evaluation consists of determining what is the percentage of similar content between each MU and each set of PUs that is somehow related with the corre-sponding MU, and providing answers to the 7 questions about linguistic quality.

Another package used for direct evaluation is ROUGE 4 (Recall-Oriented Understudy for Gisting Evalua-of a summary by comparing it with one or several ideal summaries. The metrics are based on the number of over-lapping units, using measurements such as N-gram co-occurrence (ROUGE-N), longest common subsequence (ROUGE-L), weighted longest common subsequence (ROUGE-W) or skip-bigram recall metric (ROUGE-S). 3.2. Indirect or extrinsic evaluation
This type of evaluation tries to determine quantitatively how appropriate or useful a given summary is with sibilities to perform this kind of evaluation: to use human judges to determine the relevance of the summaries for the task, or to measure the effectivity of the summaries automatically using them as sources for the task.
Indirect evaluation in information access tasks has been applied in many cases. Most of them try to mea-sure the repercussion of using summaries instead of full texts in decision taking about the relevance of docu-ments. The usual hypothesis is that there is a reduction in the time required with no significant loss of effectivity.

The usefulness of summaries of ad hoc retrieval and categorization task was studied in ( Mani &amp; Bloedorn, 1998; Mani et al., 2002 ). For ad hoc retrieval, the evaluation was carried out over indicative summaries ori-ented to the query over newspaper articles. Twenty queries were used, and for each one of them a subset of 50 documents was selected out of the first 200 retrieved by a standard retrieval system. The selection was carried out so that the number of relevant documents for each query was between 25% and 75%, and no document appears in more than one subset. A corpus of 1000 documents resulted. Twenty one human judges took part in the evaluation, which consisted in determining the relevance of a document given a query. The documents pro-vided to the evaluators were: original documents, summaries of a fixed length (10% of the number of charac-ters in the source text), summaries of variable length and initial segments of the source text (10% of the sentences). This last item served as control element, since the documents of the experiment all had a lot of relevant information at the beginning of the document. The evaluation showed similar effectivity (no signifi-cant differences), in terms of F 1 , when the full text and when the variable length summaries are used (average compression between 15% and 30%). However, more than 40% of time is saved when the summaries are used.
For text categorization, the evaluation centred on generic summaries, for there was no query with which to personalize the summary. Two groups of 5 categories belonging to mutually exclusive domains where used.
For each category 100 documents were selected. The evaluation was carried out by 24 judges and it consisted of choosing one of five predetermined categories  X  or  X  X  X one of them X  X   X  for which the content of a given doc-ument was deemed relevant. The documents were of the same type as those used for the experiment on ad hoc retrieval. Similar effectivity was obtained with the two types of summary and the full text. Nevertheless, only the fixed length summaries reduced the time employed (by 40%).

In Tombros and Sanderson (1998) the utility of the initial segment of the original documents is compared with that of query oriented summaries. To this end, an information retrieval system is used in which the user is shown, next to the title of the retrieved document, either the first sentences or the automatically generated summaries. Fifty TREC queries and 50 documents per query are used. Measured parameters include preci-sion, recall, the time needed for the user to decide, the number of times that the user accesses the full text, and the subjective opinion of the users on the quality of the aid provided (either first lines or generated sum-mary). The results show that user oriented summaries significantly improve user efficiency in the ad hoc retrie-val task with respect to the use of the first lines.

In ( Dorr, Monz, President, Schwartz, &amp; Zajic, 2005 ) it is shown that two types of human summaries, Head-line Surrogate (non-extractive  X  X  X ye-catcher X  X ) and Human Surrogate (extractive headline), can be useful for a relevance assessment task in that they help a user achieve a high agreement in relevance judgments with respect to those obtained with the full texts. On the other hand, a 65% reduction in judgment time is observed between full texts and summaries. In this work a new method for measuring agreement, Relevance-Prediction, is intro-duced. This method takes a subject X  X  full-text judgment as the standard against which the same subject X  X  summary judgment is measured.

In this case, the evaluation method consists of comparing the effectivity of the retrieval when the queries are presented over summarized versions of the documents, instead of over the documents in the original collec-tion. The advantage of this approach is that it does not need judges, avoiding the problems of agreement between judges, and making the evaluation fully automatic.

In Brandow et al. (1995) the effectivity of the automated summaries is compared with that of the initial seg-ment of documents. A corpus of newspaper articles and 12 queries are used. Better effectivity is obtained for the initial segments. Based on clustering techniques, ( Nomoto &amp; Matsumoto, 2001 ), uses a similar approach with a keyword-based automatic summary generator as control element. The corpus contains 5000 news items from a Japanese newspaper and 50 queries. Relevance judgments can be: total relevance, somewhat relevant, and irrelevant. Using the first two types of judgments, the system produces more effectivity than the control In no case does the effectivity reach that achieved using the full text.

In Man  X  a et al. (1999) similar experiments are carried out using 5000 documents chosen at random out of the Wall Street Journal corpus (news items), and 50 randomly chosen queries with at least one relevant doc-ument in the chosen set. The experiments compare in terms of precision-recall curves the effectivity of searches with the full text, with generic summaries, with summaries adapted to the query, with summaries adapted by expanding them with WordNet synonyms, and with the initial segment. Three different compression rates have been used: 10%, 15% and 30%. Results show that query adapted summaries improve the average precision between 30% and 40%, compared with the first sentences of the documents. They also improve upon generic summaries in a similar proportion. Finally, the effectivity of adapted summaries is statistically comparable with that obtained using the original texts. On the other hand, the experiments show that the expansion of the query using WordNet does not improve the effectivity of the summaries.

In conclusion, the indirect evaluation, without human judges, allowed an evaluation less expensive and eas-ier to extend to big text corpora and to different summarization techniques. The evaluation can be performed in a more exhaustive and automatic way. 4. Automatic personalized summarization
The use of summaries allows users to save a significant amount of time by allowing them to correctly iden-tify whether a relevant document is really interesting for them without having to read the full text. If the sum-
The summary of each document is extracted automatically from the full text of the document using tech-niques for the selection and extraction of sentences, where a sentence is considered the sequence of words between two periods that indicate the end of sentence.

Techniques for selection and extraction of sentences are very attractive because they are independent of the domain and the language. Additionally, these techniques can take into account information stored in a user model, allowing personalization of the summaries, that is, to generate summaries adapted to a user profile.
The different techniques to be used, as well as their possible combinations, are described below. Three dif-ferent methods for phrase or sentence selection are used to build the summaries. The first two are used two build generic summaries, and the third one is used to generate personalized summaries. Possible combinations of these methods to build summaries with different properties are indicated.

The three methods have a common goal: to assign to each sentence in the text a value that indicates its rele-vance as a candidate for the summary. The final selection will consist of a percentage, usually 20%, of sen-tences that have higher relevance. These sentences are concatenated respecting their original order of appearance in the full text, to avoid inconsistencies.

Each method is described, and the various parameters that can be used to customize the final summaries are described in each case. 4.1. Generic summaries
First we describe the two methods that are independent of the user to which the summary is addressed. The first one takes into account the position of the sentence in the document, and the second one takes into account the significant words that appear in the sentence. 4.1.1. Position method
In many domains (i.e journalism), the first few sentences of a text may constitute a good summary of the 1969 ). In our work, the specific values chosen for these 5 sentences are shown in Table 1 . Sentences from the sixth on are assigned value 0. These values generate the weights A These values are independent of the user u under consideration.

A ranking of the sentences of a document with respect to the position in which they occur in the document is obtained from this method. In the case of this particular version of the method, this ranking corresponds to the order of the sentences in the document. 4.1.2. Thematic word method
Each text has a set of significant or  X  X  X hematic X  X  words that are representative of its content. This method extracts the 8 non-empty most significant words (non-empty words are those that do not appear in the stop list) for each text and it calculates how many occurrences of those words there are in each sentence of the text. Moens, 1997 ).

To obtain the 8 most significant words of each document, documents are indexed to provide the weight of each word in each document using the tf  X  idf method ( Salton &amp; McGill, 1983 ).

To obtain the assigned value B sd for each sentence s in document d using the thematic word method, the number of significant words appearing in a sentence is divided by the total number of non-empty words appearing in the sentence. The aim of this calculation is to assign a higher weight to sentences with a higher density of significant words ( Teufel &amp; Moens, 1997 ). These values are also independent of the user.
This method provides a ranking of the sentences in a document according to the number of thematic words that the sentence contains.
 4.1.3. Combination of generic methods
To obtain the value G sd associated with each sentence s in a document d using combinations of the methods described above, formula (1) is applied:
The parameters u and c allow adjustment of the different methods, depending on whether a position method ( u ) or a significant word method ( c ) is preferred. To ensure significance, the relevance obtained from each method must be normalized.

From formula (1) , a ranking of the sentences in each document is obtained with respect to the generic meth-ods, which are independent of the user. 4.2. Personalized summaries
In the next section the personalization method, which uses information dependent on the user for whom the summary is intended, are described. 4.2.1. Personalization method
The aim of this method is to select those sentences of a document that are most relevant to a given user model. The values P sdu associated to each sentence s in a document d for a user u are obtained by calculating the similarity between the user model for user u and each one of the sentences in the document.
This method provides a ranking of the sentences in each document according to the personalization method.

The complexity of a user model that is being used allows various possibilities when customizing this method. Depending on the part of the user model which is selected to participate in calculating the similarity, different types of personalization may be obtained.

This procedure is applicable to any personalization system capable of assigning to sentences in a document a relevance weight that corresponds to the estimated importance of that sentence with respect to the user X  X  needs. 4.3. Combination of generic and personalized methods
To obtain the value Z sdu associated with each sentence s in a document d for a user u, using a combination of all the summarization methods, formula (2) must be applied:
Parameters v and l allow the adjustment between the different methods, depending on whether they are gen-the summaries: if l is 0, resulting summaries will be generic, and for values of l larger than 0, summaries will have a certain degree of personalization. For this combination to be significant, the relevance obtained with respect to each method must be normalized.

From formula (2) a ranking of sentences is obtained for each document with respect to the combination of all proposed methods. 5. The personalization system
The automatic summarization process will be applied in the frame of reference of a personalization system for a digital newspaper. Each day the system sends to the users a selection of the news items more relevant with respect to his user model, defined in the moment they register in. Moreover, the use can interact with the sys-tem providing feedback information about the news items he receives ( D X   X  az &amp; Gerva  X  s, 2005 ).
In the next sections are described the user model, the multi-tier content selection and the automatic person-alized summarization processes used in the system. 5.1. User model tion, an automatic categorization algorithm and a set of keywords (long-term model), and a relevance feed-back tier (short-term model). The long-term model reflects the information needs of the user that remains stable across the time. On the other hand, the short-term model reflects the changes on these needs through the feedback of the user.

In the long term model, the first tier of selection corresponds to a specific given classification system related with the application domain, that is, the sections of the digital newspaper. The user can assign a weight to each section: national, sport, etc. This information is stored as a matrix where rows correspond to sections and col- X  to characterize his preferences. These keywords are stored, for each user u, as a term weight vector ( k
Yahoo! Spain. This information is stored as a matrix where rows correspond to categories and columns corre-descriptions of the first and second level of Yahoo! Spain categories entries ( Labrou &amp; Finin, 2000 ).
In the fourth tier, short-term interests are represented by means of feedback terms obtained from feedback time, so their weight must be progressively decreased. 5.2. Multi-tier content selection
Documents are downloaded from the web of a daily Spanish newspaper as HTML documents. For each document, title, section, URL and text are extracted, and a term weight vector representation for a document d( d d ) is obtained by application of a stop list, a stemmer, and the tf  X  idf formula for computing actual weights ( Salton &amp; McGill, 1983 ).

Each document is assigned the weight associated with the corresponding section associated to it in the par-ticular user model, which represents the similarity between a document d, belonging to a section s, and a user model u  X  s s du  X  .

The similarities between a document d and a category g ( s user model u  X  s k du  X  , and between a document d and a short-term user model u  X  s t cosine formula for similarity within the Vector Space Model ( Salton &amp; McGill, 1983 ):
The similarity between a document d and the categories of a user model is computed using the next formula:
The results are integrated using a particular combination of reference frameworks. The similarity between a document d and a user model u is computed as: where Greek letters d , e , / , and c represent the importance assigned to each of the reference frameworks, sec-tions, categories, keywords, and feedback terms, respectively. To ensure significance, the relevance obtained from each reference framework must be normalized.
 From formula (5) a ranking of documents is obtained with respect to the complete user model. 5.3. Automatic personalized summarization
In this system the parts of the user model used are the keywords from the long-term model and the feedback terms from the short-term model. From each reference framework a value is obtained by calculating the sim-ilarity between the part of the user model being used and each one of the sentences in the document. The val-ues from both frameworks are combined to obtain, from formula (8) , the final value P Section 4.2.1 . These processes are described in the next sections.

The final value assigned to each sentence in each document is obtained from formula (2) where generic and personalized methods are combined. From this formula the final ranking of sentences for each document is obtained. The sentences selected to build the summary consist of the 20% of sentences that have higher rele-vance. These sentences are concatenated respecting their original order of appearance in the full text, to avoid inconsistencies. 5.3.1. Personalization using keywords from long-term model
The value P k sdu associated with each sentence s, in document d, for a user u, using keywords k of the long term model is obtained by calculating the similarity between the keyword vector and the vector for each sen-tence using formula (6) : where s sd is the term weight vector for sentence s in document d, k the keywords of the long term model of user u and sim is the similarity measure of the Vector Space Model.
From formula (6) a ranking of sentences is obtained for each document with respect to the personalization method using the keywords of the long term model. 5.3.2. Personalization using the short term model
In a similar way personalized summaries can be obtained using the short term model of user u. In this case, the similarity is calculated between sentence s and the relevance feedback terms f back terms of the short term model is calculated as the similarity between the relevance feedback term weight vector and the term weight vector for each of the sentences, as shown in formula (7) : where s sd is the term weight vector for sentence s in document d, f the relevance feedback terms of the short-term model of user u and sim is the similarity measure of the Vector Space Model.

From formula (3) a ranking of sentences is obtained for each document with respect to the personalization method using the short term model. 5.3.3. Personalization using combinations of short and long term models
The values obtained for each part of the user model can be combined to obtain a personalized summary that uses the complete user model. In this case, the value P d, for a user u, is obtained with formula (8) : where Greek letters g and j show the importance assigned to each of the reference systems ( g , for key words in the long term model, j , for the feedback terms of the short term model). To ensure significance, the relevance obtained from each reference framework must be normalized.

From formula (8) , a ranking of the sentences in each document is obtained with respect to the complete user model. 6. Evaluation methodology
To measure the effectivity of personalized summaries a series of methods is needed for measuring to what extent the summary allows the user to obtain the information relevant to his interests without having to read full text.

We have used an indirect/extrinsic evaluation based on the differences resulting between parallel processes of selection, using the user models, over a set of parallel collections: one with the original documents and the others built with differently summarized versions of the documents.

We have also carried out a user centred direct/intrinsic evaluation based on a set of questions answered by each one of the users concerning the summaries that they have received. 6.1. Evaluation collection
An evaluation collection is composed of a set of documents with a similar structure (usually restricted to particular domains, such as journalism or finance) a set of tasks to be carried out over the documents, and a manually by human experts). For instance, in information retrieval the tasks to be carried out are queries pre-sented over the documents in the collection, and the results are relevance judgments associated with each query with respect all the documents. Typical examples are the collections associated to the conferences TREC or DUC.

Evaluation collections for personalization such as the one describe in this paper present a major difficulty when compared with evaluation collections for other tasks: they require different relevance judgments for each select the most relevant documents for each user on each day, and each user has different information needs (as featured in his user model) and these information needs may vary over time as the user becomes aware of new information. This relevance judgments could either be generated artificially by a human expert by cross checking each user model with the set of documents for a given day (very much in the way the system is expected to do), or they can be established each day for the given documents by the real user who created the user model. This second option is more realistic, since real users determine the relevance of the given doc-uments with respect to their interests at the moment of receiving them, therefore using their current informa-tion needs. In existing evaluation collections for text classification this is not done, because judgments are generic for all possible users and they are generated by a human expert that does not know what the particular information needs may be for different users involved in carrying out different tasks at different times.
In our case the relevant judgments between each document and each user model are assigned by the proper users during the normal running of the system during several days. Then, the evaluation collection is com-posed of the set of news item from a digital newspaper corresponding to several days, and the set of binary (relevant/not relevant) judgments cross-indexed with the news items assigned manually by each user with respect to his proper user model.

On the other hand, there is no collection of ideal summaries with which one could compare automatically generated personalized summaries. This is because it is very difficult to build an ideal personalized summary of each document for each possible user. 6.2. Evaluating summary generation
What needs to be evaluated is the loss of significant information that a user suffers when he is presented with a summary instead of the corresponding full document. Additionally, it will be considered the explicit opinions expressed by the users in their replies to some questions about the quality of the summaries. 6.2.1. Indirect evaluation
The technique is based on the assumption that if a document summarization process is good, then the resulting summary must retain enough of the original information to guarantee a correct judgment concerning the similarity between the summarized item and a given user model. This type of quantitative evaluation is typical of summary extraction systems which operate within more complex systems that carry out other tasks, such as information retrieval or information filtering ( Man  X  a et al., 1999 ).

For each user. To access the evaluation corpus, using the algorithm that is to be evaluated. The selection user model corresponding to that user. The hypothesis is that, if the summarization process employed pre-serves the information in the document that is relevant to the user model, the results of the selection process should be comparable to those obtained for the same user with the full text of the documents, which are taken as upper bound reference values. Any deviation from these reference values indicates information loss due to  X  X  X eaks X  X  in the generation of summaries, which have produced a variation in the ranking obtained for the col-lection of summaries with respect to the ranking obtained for the collection of full text documents. By apply-ing a similar process for each algorithm for summary generation, this experiment should show an explicit, though indirect, measure of the adequacy of the personalized summary generation. 6.2.2. User-centred direct evaluation
To perform a direct evaluation about the readability of the summaries, each user might have been asked to evaluate each summary he received. However, the fact that each user receives approximately one hundred summaries per day, during several days, convinced us that we should not consider this possibility. Instead each user was only asked for his opinion at the end of system use.

The questions presented to the user concerned the following aspects: quality of the summary, coherence, clarity and structure, redundancy errors, adaptation to the user model, adaptation to information needs of the user, reflection of the document content and representation of the main components of the document.
Users were also asked about the part of the news item that they had used to decide about the relevance: title, relevance, section, personalized summary or full news item. Of these parts, the first four appeared in the message that was sent to each user. To access the complete news item an additional click over a hyperlink was necessary.

On the other hand, the great quantity of news items processed, together with the associated personalized summaries, made it inadvisable to use a direct evaluation method based on the comparison with an ideal summary. 6.3. Hypothesis The questions that need to be answered are:
For the obtention of personalized summaries using only the personalization methods, is it better to use a static long term model or a dynamic short term model or a combination of both?
How much is lost, in terms of information received by the user, by offering a summary of a document in place of the full document? What type of summary is better in this sense? These questions give rise to the following hypothesis:
H1. Summaries obtained using only the personalization method are better if a combination of the long and
H2. Summaries obtained using only the personalization method are better in terms of information selected
H3. Summaries obtained using only the personalization method are better in terms of information selected
H4. Summaries obtained using only the personalization method are worse than the full document in terms of 6.4. Design of experiments
To evaluate the generation of summaries the system is tested with the various combinations of parameters associated with the process. This involves the different combinations of methods for generating summaries. In this way results will be obtained associated with the different ways of generating summaries. 6.4.1. Design of experiment 1: generating personalized summaries
To test the first hypothesis (H1) one must evaluate for each user all the different types of summary that can be generated using only the personalization method ( P sdu the use of long and short term models for the generation of personalized summaries using only the personal-ization method. One must take into account that this will involve using the key words of the long term model and the relevance feedback terms of the short term model. The different possibilities correspond to the follow-ing assignment of values to the parameters of formula (8) : Ps(L): personalized summary using only the keywords of the long term model ( g =1, j = 0).

Ps(S): personalized summary using only the relevance feedback terms of the short term model ( g =0, j = 1).

Ps(LS): personalized summary using a combination of the keywords of the long term model and the rele-vance feedback terms of the short term model ( g =1, j = 1).

Several collections have been generated for each user, each one consisting of a set of summaries of the ori-ginal documents obtained by means of the application of each one of the indicated methods for generating personalized summaries. In this way, there will be a collection for each user of personalized summaries built using the short term model, another one built with the long term model, and another one built using a com-bination of both.

As the collection of summaries is different for each user, the selection process must be carried out separately for each particular user, so there will be as many selection processes as the number of users involved. 6.4.2. Design of experiment 2: combining of methods for summary generation
To test the hypotheses (H2, H3 and H4) one must evaluate for each user all the different types of summary that can be generated using the different combinations of method ( Z be of different types, depending on the specific methods used to generate them. This corresponds to the fol-lowing assignment of values to the parameters of formulas (2) and (8) :
Fs (baseline summary): 20% first sentences of the full text. This summary will act as baseline for the rest of the experiments. In truth, it would correspond to the position method if the values were extended incremen-tally to all the sentences of a document.

Gs (generic summary): summary obtained using the methods for generating generic summaries ( / =1, c =1, l = 0).

GPs (generic-personalized summary): summary obtained using both types of methods ( / =1, c =1, l =1, g =1, j = 1).

Ps (personalized summary): summary obtained using only the personalization method (combining long and short term models). ( / =0, c =0, l =1, g =1, j = 1). This corresponds to Ps(LS) of the previous experi-ment 1.

Several collections have been generated for each user, each one consisting of a set of summaries of the ori-ginal documents obtained by means of the application of each one of the indicated methods for generating summaries. The collections of baseline summaries and generic summaries are the same for all users, since they do not depend on any user model  X  they are not personalized. This allows simultaneous evaluation of all users.
However, there will be a different collection per user for personalized and generic-personalized summaries, which do depend on user models. In this way, each user will have to be individually evaluated with respect to his collection of personalized summaries and his collection of generic-personalized summaries. 6.5. Evaluation metrics
The results to be obtained are a ranking of documents (complete texts and summaries) for each user, obtained from the application of the selection process by means of formula (5) to those documents.
These results must be compared with the binary relevance judgments associated with the full texts. This comparison between a ranking of documents and binary relevance judgments (relevant/not relevant) suggests the use of normalized recall and precision metrics. This is justified because rankings of documents rather than groups of documents are compared: one does not simply observe whether the first X documents are relevant or not, but rather their relative order in the ranking.

Normalized precision and recall ( Rocchio, 1971 ) measure the effectivity of a ranking in terms of the area on a recall or precision versus levels of ranking graph encompassed by the best possible solution (relevant doc-uments in the first positions) and the solution generated by the system under evaluation. In those cases where the same relevance value appears at consecutive positions in the ranking, the average position of all the match-ing values is taken as value for all of them ( Salton &amp; McGill, 1983 ). This adjustment solves the problem of attributing a random relative ordering to the positions that have the same relevance value associated.
For each of the different configurations the values of normalized recall and precision will be obtained, for lished configurations. 6.6. Statistical significance
For two techniques A and B, one must show that technique A achieves better results than technique B (A &gt; B) with respect to a parameter V . To this end, the number of times that technique A gives better results than B is represented as V +, and the number of times that technique B gives better results than A is repre-sented as V , and the number of times that both techniques give the same results as draws. This will be applied both to normalized recall ( V = R ) and normalized precision ( V = P ).
 there is no assumption about the underlying distribution, and that, given the different normalization processes that are applied at different levels, it must be used the relative values rather than the absolute magnitudes to establish statistical significance ( Salton &amp; McGill, 1983 ). 7. Results
Two experiments have been carried out over two different personalization systems. The first one had a smal-ler evaluation collection (11 users, 5 days) and it did not use the categories of the user model. The second one had a fuller evaluation collection (104 users, 14 days) and it used the full user model. 7.1. First personalization system
For summary generation, the keywords of the long term and the relevance feedback terms of the short term model were used. On the other hand, the selection of the different types of summaries was done using formula Gerva  X  s, 2005 ).
 The hypotheses presented for experiments 1 and 2 were evaluated using the indirect evaluation presented in
Section 6.2 . No direct evaluation was carried out over this system. 7.1.1. Evaluation collection
Experiments were evaluated over data collected for 11 users, mainly computer science lecturers, and the news items corresponding to five days  X  period 6th X 10th May 2002  X  of the digital edition of the ABC Spanish newspaper. The number of news item per day was: 128, 104, 87, 98 and 102. It was used the news items from the next sections: national, international, sports, economy, society, culture, people and opinion. The total number of news items was 519. 7.1.2. Results of experiment 1: generating personalized summaries
There was a collection for each user of personalized summaries generated using the short term model (Ps(S)), a different collection for each user generated using the long term model (Ps(L)) and a third different collection for each user generated using a combination of long term and short term models (Ps(LS)). The multi-tier selection process was applied to each one of these collections, using the corresponding user profile as source for user interests. In each case, values of normalized recall and precision were computed.
These experiments were repeated for all users during the 5 days of evaluation. The results for the three types of personalized summaries were compared only from the second day on, for the fact that on the first day there was no short-term model based on user feedback. In total, it was generated 12903 different personalized sum-maries (11 users, 391 news item, 3 types of summaries per user and per news item).

The results shown in Table 2 indicate that the combination of long and short term models for the gener-ation of personalized summaries provides significantly better results than the use of each model separately, in terms of normalized precision (2.5% against long term only, 1.0% against short term only). As an additional result, it is observed that the short term model on its own is better than the long term model in terms of nor-malized precision (1.4%), though not significantly so. In terms of normalized recall, results are similar: signi-ficant improvement of the long term-short term combination over both short and long on their own, and non-significant improvement of short term only over long term. This confirms hypothesis H1.
The use of both methods adjusts the summaries better to the preferences of the user, as shown by higher values of precision and recall. The slightly better results for the short term could be due to the fact that the terms introduced by the user in his long term model are in general too specific, whereas those obtained through user feedback are terms that appear in the daily news.

From here on, mentions of personalized summaries (Ps) refer to the personalization obtained by means of a combination of the long and short-term models (Ps(LS)). 7.1.3. Results of experiment 2: combining of methods for summary generation
The evaluation collections for the first sentences summaries (Fs) and generic summaries (Gs) are identical for all the users, because they do not depend of the user model, that is, they are not personalized. This allowed evaluating all the users together. However, there are a different collection per user of personalized summaries (Ps) and generic-personalized summaries (GPs).

The multi-tier selection process is applied to each one of these collections, using the corresponding user pro-file as source for user interests. In each case, values of normalized recall and precision have been computed in personalized summaries (24 different summaries per news item, 519 news items).

The analysis of the results shown in Table 3 indicates that personalized summaries give significantly better results with respect to normalized precision of the selected information than generic summaries and generic-personalized summaries. In both cases the improvement is statistically significant (3.7% against generic and 3.3% against generic-personalized). This confirms hypothesis H3. Generic-personalized summaries are better than generic summaries, and generic summaries are better than summaries based on first sentences, but in neither case is the difference statistically significant.

It can also be seen that personalized summaries are worse (1.7% on normalized precision) than complete news items (N) under the same view point. This confirms hypothesis H4. Personalized summaries are better than summaries based on the first sentences of the news item (Fs), with a statistically significant improvement (4.7% in normalized precision). This confirms hypothesis H2. The improvements on recall are similar but with small percentages. 7.2. Second personalization system
As in the first system, the keywords of the long term and the relevance feedback terms of the short term model were used for summary generation. On the other hand, the selection of the different types of summaries was done using formula (5) with all the reference systems: sections, categories, keywords and feedback terms ( D X   X  az &amp; Gerva  X  s, 2004a; D X   X  az et al., 2005 ).
 The hypothesis presented for experiments 1 and 2 were evaluated using the indirect evaluation presented in
Section 6.2 . The user-centred direct evaluation showed the opinions of the users about the summaries received. 7.2.1. Evaluation collection
In this case, experiments were evaluated over data collected for 106 users, most of them lecturers and stu-dents of different studies, and the news items corresponding to three weeks  X  the 14 working days of the period 1st X 19th Dec 2003  X  of the digital edition of the ABC Spanish newspaper. The news items came from the same sections of the first experiment except opinion. The average of news item per day was 78.5. It is also important to indicate that the number of user per day was decreasing in that way that the average number of user per day was only 28.6. 7.2.2. Results of experiment 1: generating personalized summaries
The multi-tier selection process was applied to each one of the collections of personalized summaries and values of normalized recall and precision were computed. The results were compared only from the second day on, for the fact that on the first day there was no short-term model based on user feedback. In total, it was generated 77,598 different personalized summaries (28.6 users on average per day, 1004 news items, 3 types of summaries per user and per news item).

The results shown in Table 4 show that the combination of long and short term models for the generation of personalized summaries provides significantly better results than the use of each model separately, in terms of normalized precision (1.6% against long term only, 2.8% against short term only). As an additional result, it is observed that the short term model on its own is better than the long term model in terms of normalized precision (1.2%), though not significantly so. In terms of normalized recall, results are similar: significant improvement of the long term-short term combination over both short and long on their own, and non-significant improvement of short term only over long term. This confirms hypothesis H1. 7.2.3. Results of experiment 2: combining of methods for summary generation
In each case, normalized recall and precision values were calculated. The experiments have been repeated over the 14 evaluation days. In total, it has been generated approximately 54950 different personalized sum-maries (58 different summaries per news item, 1099 news items).

Personalized summaries offer better results ( Table 5 ) with respect to normalized precision than generic-per-sonalized summaries, though the difference is not significant (1.5%). With respect to baseline summaries and generic summaries the difference is significant (2.0% and 2.8%, respectively). Generic-personalized summaries are better than baseline summaries (0.5%), and baseline summaries are better then generic summaries (0.7%), but the differences involved are not statistically significant. Personalized summaries are worse than full news items under the same criteria (1.7%). With respect to the normalized recall the results are similar.
This suggests that the personalization method generates the summaries better adapted to the user, followed by a combination of all possible methods. Baseline summaries using the first lines of each news item are better than those generated by a combination of the position and keyword methods. For newspaper articles, the generic method does not improve on simply taking the opening lines.

These results confirm the hypothesis H2 and, although the differences are not significant, the hypothesis H3 and H4. 7.2.4. User-centred direct evaluation
The user-centred direct evaluation was based on a questionnaire that users completed after using the sys-tem. In most questions there were 5 options to indicate the degree of satisfaction: very high, high, medium, low and very low. There were 38 users that completed the final evaluation.

Users indicated that the summaries were of high or very high quality in 83.3% of the cases, with 5.6% of very low. Concerning the coherence and clarity of the summaries, the results were as follows: 81.1% valued them as high or very high, and 5.4% as low or very low. With respect to the ability of the system to avoid redundancies, evaluation was high or very high for 69.4% of the users, against 2.8% of low evaluation. At the same time, adaptation of the summary to the user profile was considered high by 59.5% of the users, and low or very low by 8.1%.

The degree of adaptation of the summaries to the information needs was high or very high in 70.3% of the cases, and low or very low in 10.8%. Regarding the extent to which the summaries reflect the content of the original documents, for 81.1% of the users this extent was high or very high, and it was low or very low for 5.4%.

Finally, 89.5% of the users consider that the main ingredients of the news item are represented in the sum-mary. The other 10.5% indicated that at times the summaries were too brief to include them.
Most users consider that the summaries are of high quality, coherent, and clear, and that they reflect the content and the main ingredients of the corresponding document. Most of them also consider, though to a lesser degree, that the summaries contain no redundancies and that they are well adapted to user profile and user needs. This positive evaluation indicates that the method of sentence selection for the construction of summaries is a valid approach for personalized summarization in the face of possible problems of clarity, coherence and redundancy.

On the other hand, users indicate that they sometimes used the summaries to establish the relevance of a news item. This was said to be often so by 48.6% of the users, sometimes by 29.7% and few by 21.6%.
Against these data, 89.2% of the users relied on the heading often, and 10.8% only did in some cases. The section heading was used sometime by 45.9%, often by 29.7%, few by 13.5% and none by 10.8%. The stated relevance was used sometimes by 35.1% of the users, few by 24.3%, none by 21.6% and often by 18.9%.
Finally, the full news item was used few times by 51.4% of the users, some times by 29.7% and none by 18.9%. In conclusion, the summary becomes an important element for defining the relevance of a news item. 7.3. Comparison of the two personalized systems
It can be observed ( Table 6 ) that the results are better with the second personalization system. This is mainly because the use of the categories as classification system for the long term model and the effect of the short-term model during more days. In whatever case, the results for the second system confirm the results obtained in the first system in the sense that the personalized summaries are the best type of summary.
On the other hand, the results in recall were less clear, with improvements in the first experiment, but no significant improvements in the others. These results can be due to the behavior of the normalized recall met-precision is sensitive to the classification of the first correct item. Under the circumstances, our algorithms improve the precision because they raise the more relevant documents to the top of the ranking, but they also drop the less relevant documents to the bottom. This can be due to the user X  X  relevance judgments, which
These last documents might have been detected by the relevance feedback mechanism and this information used to correct the trend. However, given the peculiarities of the dissemination process, the user can only pro-vide feedback on the documents at the top of ranking  X  as specified by his upper bound on the number of news item that he is to receive per day. This implies that documents that, if spotted on time, might have risen in the ranking thanks to the feedback process, not only fail to rise in the ranking but fall to the bottom.
However, we are more interested in precision than in recall because we send to the user only the documents at the top of the ranking within the upper bound expressed by the user, and the number of relevant documents specify explicitly how many news items he wants to receive each day makes it necessary for us to consider how cision and recall: by computing values over the complete ranking rather than just the fragment of it above the (arbitrary) cut-off point, the obtained results remain relevant whatever cut-off value for the upper bound is used in subsequent runs of the system. The impossibility of establishing a fixed cut-off value under this set up also makes it difficult to use other metrics where recall and precision are compounded, such as the F mea-sure, because they rely on establishing such an explicit cut-off value. 8. Conclusions
A summarization process has been presented characterized by the inclusion of a user model to generate per-sonalized summaries adapted to the user. The user model contains different reference systems to represent the information needs of the user: sections, categories, keywords and feedback terms. The aim is to provide a sum-mary oriented to the user that helps him to correctly identify whether a document is really interesting for him without having to read the full text.

The indirect evaluation method used to measure the quality of the summaries has allowed determining that the personalized summaries are the better option in terms of normalized precision and recall. Full news item offer only a slight improvement against personalized summaries, which seems to indicate that the loss of infor-mation for the user is very small with this type of summary. Generic summaries perform very closely to sum-maries obtained by taking the first few lines of the news item. This seems to indicate that the position method is overpowering the thematic word method, which may be corrected by refining the choice of weights.
Although a first-sentences approach may provide good results for indicative summarization, it does not do so well in terms of personalized summarization, where it is crucial to retain in the summary those specific frag-ments of the text that relate to the user profile.

As it has been shown, automatic summarization has the choice of applying generic techniques designed to capture the way in which the more relevant information is distributed over a typical document of the domain under consideration, or to apply personalization techniques that concentrate on specific contents for which the user is known to have an interest (either because he has explicitly stated it in a long-term model or because it has been dynamically captured in a short-term model). The newspaper domain provides a good example where the structuring of information gives very useful cues as to their relevance (news items are usually built as inverted pyramids in terms of relevance: the most relevant information at the top, with relevance decreasing as they are read). This leads to quite simple summarization techniques providing good results in terms of indic-ative summarization  X  allowing the user to get an idea of what a document is about  X  though such a method would be severely domain-dependent, and might not work as well for different domains.

The methods proposed here ensure the selection of the relevant information in terms of information needs, operating efficiently for personalized summarization providing the user an extract of the specific contents of a document that are related to his interests, with no domain-dependent assumptions.

On the other hand, the user centred direct evaluation further sanctions the concept that offering users sum-maries of the news items helps to decrease information overload on the users. The fact the summaries are said to be employed by users much more often than the full original text or the stated relevance to determine how relevant a news item is to them justifies the use of automatic summaries in a personalization system. This eval-uation has also shown that the possible problems of sentence extraction as a summary construction method do not affect performance in the present context of application.
 We can conclude that user adapted summaries are a useful tool to assist users in a personalization system.
Notwithstanding, the information in these summaries can not replace the full text document from an infor-mation retrieval point of view.
 References
