 Young Jun Ko youngjun.ko@epfl.ch Matthias Seeger matthias.seeger@epfl.ch Leaps in performance have been realized for low-level computer vision problems, such as denoising, inpaint-ing, deconvolution (debluring), image coding, under-sampled reconstruction or acquisition optimization, by adopting super-Gaussian ( X  X parse X ) image priors. While most such methods employ simple factorial pri-ors on single coefficients or groups, further substantial gains can be obtained by modelling higher-order de-pendencies via structured non-factorial prior distribu-tions (Portilla et al., 2003; Wipf &amp; Nagarajan, 2008; Cevher et al., 2010). For example, representing the de-pendencies among multi-scale wavelet coefficients by a (latent) tree structure can boost accuracy for image compression and reconstruction (Crouse et al., 1998; Papandreou et al., 2008; He et al., 2010). However, previous approaches employing such non-factorial pri-ors either run much slower than standard factorial methodology, or sacrifice performance by adopting suboptimal MAP estimation or naive mean field fac-torization assumptions, ignoring posterior covariance or uncertainty altogether in problems which are highly underdetermined.
 In this paper, we derive a large scale approxi-mate Bayesian inference algorithm for generalized lin-ear models with non-factorial (latent tree-structured) scale mixture priors. Our contributions are as follows:  X  An image model with scale mixture prior on  X  A large scale double loop algorithm for Bayesian  X  An extension to incorporate non-log-concave po- X  Automatic Bayesian learning of a substantial  X  An extensive evaluation on many inpainting and The structure of the paper is as follows. We describe and motivate our image model in Section 2, and de-velop our large scale Bayesian inference and learning algorithm in Section 3. We present experimental re-sults on image denoising and inpainting in Section 4, and close with conclusions. 1.1. Related Work A range of prior work has employed latent tree-structured priors to represent dependencies between wavelet coefficients. Crouse et al. (1998) use Gaussian mixture potentials as well as a hidden Markov tree on the discrete mixture indicators, estimating latent sig-nal and mixture parameters by expectation maximiza-tion. Portilla et al. (2003) employ continuous scale mixtures, based on a latent Gaussian tree mapped through coordinate-wise nonlinearities. They estimate parameters by nonlinear optimization. Papandreou et al. (2008) employ a hidden Markov tree and Gaus-sian mixture potentials on an overcomplete wavelet representation, estimating signal and parameters by Viterbi training. None of these employ Bayesian in-ference over the image or non-Gaussian potentials. He et al. (2010) use spike-and-slab prior potentials with a hidden Markov tree over the indicators. They per-form standard naive mean field variational inference, employing a posterior distribution which does not rep-resent any dependencies between coefficients. These assumptions lead to simple update equations, which they iterate in parallel. Their algorithm does not seem to reduce to standard scalable optimization primitives. Moreover, their method does not extend beyond spike-and-slab to other sparsity potentials.
 Our framework can be seen as extension of the scal-able double loop algorithm for variational inference in super-Gaussian models proposed in (Seeger &amp; Nick-isch, 2011). However, they did not consider hy-brid models (featuring discrete and continuous vari-ables), latent tree non-factorial prior distributions, or Bayesian learning of hyperparameters, as we do here. In problems such as image denoising, inpainting or deconvolution (debluring), we seek to reconstruct a latent image u  X  R n (an n y  X  n x bitmap, where n = n y n x ) from noisy linear measurements y  X  R m : or P ( y | u ) = N ( y | Xu , X  2 I ), where  X  2 &gt; 0 is the noise variance. For example, X = I for denoising, X = I J,  X  for inpainting ( J the index of observed pixel positions), or Xu = k  X  u for deconvolution ( k the blur kernel). With m  X  n (more unknowns than mea-surements), noise and/or blur, only additional statis-tical information in form of a image prior distribution P ( u ) renders image reconstruction a well-posed prob-lem. Given P ( u ), we can infer the image from the posterior distribution P ( u | y ) = For example, image statistics tend to be leptokur-tic (super-Gaussian, or  X  X parse X ), and even simple factorial image priors P ( u ) respecting these proper-ties can lead to dramatic improvements over classical methodology (least squares, Wiener filtering). Beyond marginals, image statistics exhibit complex dependen-cies, and capturing these in non-factorial priors can lead to further leaps in performance (Portilla et al., 2003). However, with probabilistic inference becom-ing much more difficult and expensive, such extended models enjoy little popularity so far compared to sim-pler factorial alternatives. In contrast, the method-ology 1 developed in this paper scales up in the same way as MAP estimation and variational inference for factorial priors.
 Consider an orthonormal discrete wavelet transform B , and denote corresponding wavelet coefficients by s = Bu . A factorial image prior has the form P ( u )  X  Q j =1 t j ( s j ), where common super-Gaussian potentials include Laplacian or Student X  X  t potentials t j ( s j )  X   X  In the sequel, we assume that R t j ( s j ) ds j = 1. Each coefficient s j belongs to a scale level of analysis l ( j )  X  { 1 ,...,L } , l = 1 the coarsest, l = L the finest scale (assume that n y , n x are multiples of 2 L ). Even though the s j are approximately uncorrelated for natural im-ages, it is well known that there are substantial causal dependencies between coefficients in neighbouring lev-els (Portilla et al., 2003). These are typically mod-elled by a quad-tree 2 T , linking a coefficient at level l &lt; L to four children at level l + 1. For example, en-ergy localized at a fine scale coefficient percolates its way up the tree through coarser scales. In order to capture this signature in a non-factorial prior, we use binary variables  X  j  X  { 0 , 1 } , one for each s j , as well as mixture potentials t j ( s j ;  X  j ) = t 0 j ( s j ) 1  X   X  where t 0 j ( s j ) enforces s j  X  0 more strongly than t ( s j ). Each coefficient s j can be in low (  X  j = 0) or high (  X  j = 1) state, with | s j | penalized accordingly. Moreover, we use a prior P (  X  ) of directed graphical quad-tree structure T , which encourages the inheri-tance of high/low states from a parent node to its parent node of j in the quad-tree. All nodes of a level share a common conditional probability table, so that P (  X  ) is parameterized by 2 L hyperparameters  X  r,l  X  P (  X  j = 1 |  X   X  ( j ) = r ) for l ( j ) = l . The implied prior on the image
P ( u ) = X is a non-factorial scale mixture model, which faithfully represents the causal inheritance between wavelet co-efficient sizes from coarse to fine levels. The normaliza-tion constant of P ( u ) is one, since B is an orthonormal transform.
 In Figure 1, we illustrate the effect a non-factorial prior (4) on results for an inpainting problem (75% pixels removed). The latent tree prior employs Student X  X  t potentials t 1 j ( s j ) for the high, Gaussian potentials t ( s j ) = N (0 , X   X  1 0 l ( j ) ) for the low state, where hyper-parameters  X  1 l ,  X  0 l (two at each level) are learned auto-matically (results in third,  X  j marginals in fourth row). We also show results for a factorial prior with Student X  X  t potentials for comparison (second row), whose hyper-parameters  X  l are learned in the same way. The non-factorial prior leads to a more faithful reconstruction of wavelet coefficient at coarser scales, which motivates superior inpainting results in Section 4.
 The outcome of our procedure is an approximate pos-terior distribution Q ( u | y ) Q (  X  | y ). The covariance of Q ( u | y ) can be used for decision making, e.g. Bayesian experimental design (Seeger &amp; Nickisch, 2011), and our results indicate that it helps the hyperparame-ter learning. Moreover, we predict the (approximate) posterior mean E[ u | y ], not the posterior mode (MAP estimation). Our experiments demonstrate that the variational posterior mean leads to superior results in strongly ill-posed problems. In this section, we derive a scalable algorithm for com-puting variational approximations to the posterior (1) for a non-factorial scale mixture prior (4). There are obviously strong dependencies between components of u or s , and in contrast to previous work (He et al., 2010), we do not require any factorization assumptions between them. The high level idea behind our ap-proach is iterative decoupling . We combine a standard variational bound to decouple u and  X  (allowing us to tackle inference over the latter by belief propagation) with the double loop framework of (Seeger &amp; Nickisch, 2011), which decouples mean and covariance compu-tations over u . The latter provides a computational reduction to convex penalized least squares optimiza-tion and Gaussian sampling, which is crucial for scala-bility. As is shown below, minor simplifications result in inference or maximum a posteriori (MAP) estima-tion algorithms for non-factorial or factorial priors, all based on the same underlying code.
 For the purpose of image priors, we restrict ourselves to even, super-Gaussian potentials t rj ( s j ) (Palmer et al., 2006), which can be represented as Intuitively, t ( s ) can be tightly lower bounded by (un-normalized) Gaussians of any variance  X  . Here, h (  X  ) is convex if and only if  X  2 log t ( s ) is convex (Seeger &amp; Nickisch, 2011). Both Laplacian (2) and Student X  X  t potentials (3) are super-Gaussian, while  X  2 log t ( s ) is convex for Laplacian, but not for Student X  X  t poten-tials. The criterion we will minimize is an upper bound on the negative log marginal likelihood  X  2 log P ( y ) from (1). First, we introduce  X  from (5), consisting of  X  of the integral:  X  2 log P ( y ) =  X  2 log  X  min + X where s = Bu . Second, we apply the variational mean field bound to the remaining log partition function: using the factorization assumption Q ( u ,  X  | y ) = Q ( u | y ) Q (  X  | y ). In the sequel, we denote E by  X  X  X  . The bound maximizer is Q ( u | y ) = Z Gaussian partition function. Plugging this in, we ob-tain the bound min using that h (  X  ;  X  ) is linear in  X  . The relative entropy term is defined as D[ Q (  X  | y ) k P (  X  )] =  X  log Q (  X  | y )  X  log P (  X  )  X  . Finally, and crucially for scalability, we use the variational transformation from (Seeger &amp; Nick-isch, 2011):  X  2 log Z Q is equal to where s  X  = Bu  X  , and z 0 . If A (  X  ) :=  X   X  2 X T X + B
T (diag  X  ) B denotes the inverse covariance matrix of Q ( u | y ), then  X  7 X  log | A (  X  ) | is concave, and the  X  2 log Z Q representation is based on the correspond-ing Fenchel duality. Since the dependence of A on  X  is through  X  , neither z nor g  X  ( z ) depends on  X  . Plug-ging this in and pulling min u  X  , z outside, we obtain our final upper bound on  X  2 log P ( y ): to be minimized over Q (  X  | y ), z , u  X  ,  X  . Notice that  X   X  (  X  )  X  =  X  (  X   X   X  ) by linearity.
 We adapt the scalable convergent double loop algo-rithm from (Seeger &amp; Nickisch, 2011). During the in-ner loop, we fix z and minimize  X  over Q (  X  | y ) and ( u  X  ,  X  ). In between, once per outer loop iteration, we update z to obtain a tangential fit to log | A (  X   X   X  ) | : z  X  Var Q [ s | y ], where Q ( u | y ) is based on  X   X   X  for the current  X  and  X   X   X  . Computing these Gaussian vari-ances is the most computationally intensive part of a variational inference method. We approximate z us-ing the Perturb&amp;MAP technique from (Papandreou &amp; Yuille, 2010), at the cost of solving a small number of linear systems A (  X   X   X  ) x k = r k by preconditioned conjugate gradients.
 In the inner loop, we update Q (  X  | y ) and ( u  X  ,  X  ) alter-natingly. For the latter, we can eliminate  X  by revers-ing the super-Gaussian representation of  X  log t rj ( s  X  j plugging in p j := ( z j + ( s  X  j ) 2 ) 1 / 2 instead of s ping terms independent of u  X  , we need to solve Here, we used that log t j ( s j ;  X  j ) is linear in  X  j standard-form penalized least squares problem, which can be solved by any of a large number of recent al-gorithms developed for MAP estimation. In our ex-periments, we employ a nonlinear conjugate gradients algorithm from the glm-ie toolbox (see Section 4). Importantly, this inner loop problem is convex iff the  X  log t rj ( s j ) are convex, thus iff MAP estimation is convex for the same model (the precise relationship to MAP is detailed shortly). It is not convex if Student X  X  t potentials (3) are used, and we will use additional bounding in order to retain inner loop convexity (Sec-tion 3.3). For the update of Q (  X  | y ), note that where  X  . = X  denotes equality up to an additive con-stant. We can read off the minimizer Q (  X  | y )  X  P (  X  ) Q j t j ( p j ;  X  j ). This is a distribution of the same quad-tree structure T as P (  X  ), differing from the lat-ter in the single node potentials only. We can compute both  X   X   X  and D[ Q (  X  | y ) k P (  X  )] in O ( n ), using Pearl X  X  belief propagation algorithm.
 This completes the description of our large scale in-ference algorithm. At convergence, u  X  constitutes our posterior mean prediction E Q [ u | y ]. Apart from simple direct primitives, it reduces entirely to solving a mod-erate number of linear systems A (  X  ) x = r for differ-ent (  X  , r ), which can be done very efficiently by state-of-the-art preconditioned conjugate gradients solvers. Our double loop structure implies that the most ex-pensive updates of z have to be done least frequently. 3.1. MAP Estimation. Factorial Priors We derived an algorithm for variational Bayesian in-ference with a structured non-factorial prior (4). Im-portantly, we can obtain scalable algorithms for MAP estimation or inference with factorial or non-factorial priors by making minor simplifying modification, oth-erwise using exactly the same underlying code . First, when using a factorial prior of the form P ( u ) = Q eliminate Q (  X  | y ) altogether. The inner loop now con-sists of a single penalized least squares problem (6). Second, MAP estimation is obtained by simply set-ting z = 0 , skipping variances computations and run-ning a single outer loop iteration. MAP estimation for a non-factorial prior proceeds in an expectation-maximization fashion, alternating between penalized least squares (PLS) and belief propagation (Crouse et al., 1998). 3.2. Learning Prior Hyperparameters In order to obtain best performance, it is necessary to endow an image prior P ( u ) (whether factorial or not) with a substantial number of hyperparameters, which have to be adjusted to the problem at hand. For ex-ample, wavelet coefficients s j exhibit higher variance at coarser than at finer scales l ( j ), and corresponding prior potentials t j ( s j ) should take this into account, via  X  j in (2) or (3), or  X  j in Gaussians N (0 , X   X  1 In our experiments with non-factorial priors, we use L = 8 scale levels, giving rise to 16 hyperparame-ters (one for each level l and low/high state r ): too many to be reasonably set by non-Bayesian methods like cross-validation. In this section, we show how to learn hyperparameters in an automatic Bayesian way. Our method is folded into the variational inference process, which lets us optimize hyperparameters for each dataset at hand. We stress that Bayesian learning operates on the raw data (noisy, incomplete, blurred): clean underlying images are not required.
 Bayesian learning works by maximizing the log marginal likelihood log P ( y ) w.r.t. hyperparameters. The obvious variational approximation is to maxi-mize the lower bound (or, equivalently, minimize  X  ) instead. Indeed, we can treat the hyperparameters (say,  X  ) as just another set of parameters to mini-mize  X  over, thereby folding the learning into the infer-ence approximation. Importantly, we can update  X  as part of our inner loop optimization, for fixed z , with-out compromising overall convergence (to a stationary point). Recall that z comes from the Fenchel duality log | A (  X   X   X  ) | = min z z T  X   X   X  X  X  g  X  ( z ). Since A depends on all other parameters 3 only through  X   X   X  , there is no direct dependence between z and  X  . Suppose that all a hyperparameter  X  rl . Denote q rj := Q (  X  j = r | y ). We write  X  j : l  X  short for  X  j : l ( j ) = l  X . The relevant part of  X  is We need to minimize  X  w.r.t.  X  rl , which can often be done analytically. For Laplacian potentials (2):  X  . = 2 X  X  . = X For Student X  X  t potentials, we update hyperparameters  X  rl only once per outer loop iteration, as detailed in Section 3.3.
 Recall the parameterization of the tree prior P (  X  ) in terms of  X  r,l from Section 2. The relevant criterion part is  X  . =  X  log P (  X  )  X  , which implies the updates where q krj := Q (  X  j = k, X   X  ( j ) = r | y ), k,r = 0 , 1, are double node marginals. 3.3. Student X  X  T Potentials When applied to models featuring Student X  X  t poten-tials (3), the algorithm just detailed requires non-convex PLS problems (6) to be solved in the inner loop. Commonly used first order solvers can fail dra-matically on non-convex problems. Since we adopt a double loop strategy anyway, it is simpler and far more robust to use additional bounding in order to obtain a convex inner loop problem. This idea has previously been described in (Seeger &amp; Nickisch, 2011) and ap-plied to Student X  X  t potentials, but our applications here, as well as our hyperparameter learning method, are novel.
 Recall the representation (5) of t ( s ). For a Student X  X  t potential, h (  X  ) is not convex, but can be written as sum of a convex term h  X  (  X  ) and a concave term h  X  (  X  ) (Seeger &amp; Nickisch, 2011, Appendix A.6). Moreover, the concave part can be represented by Fenchel dual-ity: h  X  (  X  ) = min e&gt; 0 e X   X  g  X   X  ( e ). Overall, we end up with a further parameter vector e = [ e rj ], which is updated alongside z . Define t  X  ( s ; e ) as A minimum over jointly convex functions in ( s, X  ), this is a convex function in s . We end up with a modified convex inner loop PLS problem of the form (6), where  X  2 log t rj ( p j ) is replaced by  X  2 log t  X  ; rj ( p j easily computed by a single case distinction. We have to replace log t rj ( p j ) by log t  X  ; rj ( p j ; e update of Q (  X  | y ).
 The  X  rl hyperparameters for Student X  X  t potentials are updated once per outer loop, alongside the e rj . We use the same procedure as in Section 3.2, but applied to  X  2 log t rj ( p j ), not its convexification. The relevant criterion part is  X  . = X This is a smooth convex function in log  X  rl , which is easily minimized by a one-dimensional Newton solver. Once all Student X  X  t hyperparameters have been up-dated, we refit the corresponding e rj and continue with another inner loop. We present experiments on a range of denoising and inpainting problems, comparing variational inference and MAP estimation for different models. Our results are averaged over 77 frequently used images (greyscale, 256  X  256), a dataset 4 from (Seeger &amp; Nickisch, 2008). Our implementation is based on the glm-ie toolbox ( www.mloss.org/software/view/269/ ). We com-pare 8 methods: MAP estimation (MAP) vs. varia-tional inference (VB), factorial prior (fact) vs. latent tree scale mixture prior (tree), and Laplacian (Lap) vs. Student X  X  t potentials (T). The Lap-tree model uses ent hyperparameters  X  0 l ,  X  1 l , a pair for each level. The T-tree model employs Gaussian N ( s j | 0 , X   X  1 0 l ) for the low, Student X  X  t potentials (3) for the high state, with a pair of hyperparameters (  X  0 l , X  1 l ) at each level. We use 2 L hyperparameters in the tree , L (namely, {  X  l } ) in the fact setups. The Student X  X  t shape parameter  X  is fixed to 2.1. For each run, we initialize hyperparam-eters  X  as in (Crouse et al., 1998), by maximizing the prior probability of the raw 5 data y (for the tree cases, this involves a few steps of expectation maximization), then optimize them by minimizing  X  . Hyperparame-ters were updated once per outer loop iteration. VB runs use up to 15 outer loop (OL) iterations. Per-turb&amp;MAP estimation of z , required for inpainting only, is run with 30 samples `a 70 conjugate gradients (CG) iterations. We did 3 belief propagation and PLS calls per OL iteration for tree setups, PLS ran up to 150 iterations of nonlinear CG. Each iteration of CG requires two matrix-vector multiplications with B and X . These choices have not been optimized for maxi-mum efficiency. 4.1. Denoising We add Gaussian random noise of variance  X  2 = 0 . 01 to each image (with pixel values u i  X  [0 , 1]). All methods use the correct value of  X  2 in their likeli-hood. Notice that in this case, Gaussian variances z can be computed exactly at no cost. Namely, A (  X  ) =  X   X  2 I + B T  X  B , where B T B = I , so that Therefore, Perturb&amp;MAP, the dominating cost for VB in general, is not required. Results are shown in Ta-ble 1. For this application, differences between MAP and VB reconstruction are not significant. On the other hand, the non-factorial prior improves PSNR somewhat. Hyperparameter learning improves VB performance substantially, especially when Student X  X  t potentials are used. In contrast, it does not help (and can even hurt) MAP performance.
 4.2. Inpainting We remove 75% of pixels at random, using the same mask J  X  X  1 ,...,n } for all images. The design matrix is X = I J,  X  , the noise variance was fixed to  X  2 10  X  5 . Results are shown in Table 2. As PSNR does not always correlate well with visual quality, we show a range of images in Figure 2, Figure 3, Figure 4. VB posterior mean predictions are clearly superior to MAP reconstruction, and VB with non-factorial latent tree prior performs best. While VB with a factorial Laplace prior (Lap-fact) shows similar PSNR values to VB-Lap-tree, the visual appearance of results with the latter is clearly superior (further results, provided in the supplemental material, support these findings). The additional runtime compared to MAP estimation, mainly due to the estimation of variances z , pays off for these problems. We presented a double loop algorithm for variational Bayesian inference in linear models with non-factorial scale mixture priors, based on a latent discrete tree distribution. Our method can operate at the same scales as MAP estimation, yet its (approximate) pos-terior mean prediction strongly and consistently out-performs the posterior mode across a range of inpaint-ing problems. Both the selective smoothing by predic-tive variances and the coupling of wavelet coefficient across scales by way of the latent tree contribute to the removal of artefacts which plague results of MAP esti-mation, and of inference with factorial priors. Free hy-perparameters are learned automatically by marginal likelihood maximization folded into the variational op-timization.
 In future work, we will try to adapt our large scale inference methodology to more complex hierarchical models, featuring non-Gaussian continuous latent vari-ables (Portilla et al., 2003).
 Support through a DFG Sachbeihilfe SE 2008/1-1 (AOBJ 578593) and an ERC Starting Grant (277815  X  SCALABIM) are gratefully acknowledged.
 Cevher, V., Indyk, P., Carin, L., and Baraniuk, R.
Sparse signal acquisition and recovery with graphi-cal models. IEEE Sig. Proc. Mag. , 26, 2010.
 Crouse, M., Nowak, R., and Baraniuk, R. Wavelet-based statistical signal processing using hidden
Markov models. IEEE Trans. Sig. Proc. , 46:886 X  902, 1998.
 He, L., Chen, H., and Carin, L. Tree-structured com-pressive sensing with variational Bayesian analysis. IEEE Sig. Proc. Letters , 17(3):233 X 236, 2010.
 Palmer, J., Wipf, D., Kreutz-Delgado, K., and Rao, B.
Variational EM algorithms for non-Gaussian latent variable models. In NIPS 18 , pp. 1059 X 1066, 2006. Papandreou, G. and Yuille, A. Gaussian sampling by local perturbations. In NIPS 23 , pp. 1858 X 1866, 2010.
 Papandreou, G., Maragos, P., and Kokaram, A. Im-age inpainting with a wavelet domain hidden markov tree model. In ICASSP , pp. 773 X 776, 2008.
 Portilla, J., Strela, V., Wainwright, M., and Simon-celli, E. Image denoising using Gaussian scale mix-tures in the wavelet domain. IEEE Trans. Im-age Proc. , 12:1338 X 1351, 2003.
 Seeger, M. and Nickisch, H. Compressed sensing and
Bayesian experimental design. In ICML 25 , pp. 912 X  919, 2008.
 Seeger, M. and Nickisch, H. Large scale Bayesian infer-ence and experimental design for sparse linear mod-els. SIAM J. Imag. Sciences , 4(1):166 X 199, 2011. Wipf, D. and Nagarajan, S. A new view of automatic relevance determination. In NIPS 20 , pp. 1625 X 
