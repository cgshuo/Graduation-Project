 With the growing community value of personalized services, recommendation techniques have been playing an significant role in online applications [ 15 ]. To provide personalized services, users X  preference from their past feedback of items and implicit. Explicit feedback (e.g. the graded ratings 1 X 5 in Netflix) expresses the users X  true preference, which has been well studied in many literatures. How-ever, users may be compelled to convey their rating values in some scenarios. Moreover, users just express their behaviors implicitly in many more situations, such as browsing or not browsing, clicking or not clicking in Web sites. The mean-ing of unobserved items are ambiguous because users may not like these items or may be unaware of these items. Therefore, the scenario of recommendation with implicit feedback is more challenging.
 Previous works based on implicit feedback include point-wise regression and supposes that users don X  X  like all unobserved items and optimizes the absolute rating scores, while pair-wise ranking algorithm assumes that users X  preference of observed items is stronger than unobserved items and then directly convert the prediction to rank. The latter algorithm actually relaxes assumption and usually obtains better performance in empirical studies. However, a user typically observes limited number of items and doesn X  X  interact with thousands of items. Therefore, the data sparsity is a big problem for pair-wise ranking algorithm. With the increasing availability of auxiliary information about items (e.g., movie plots and item description), referred to as side information, it is wise to explore the possibility of using such information to improve the performance of pair-wise ranking algorithm through alleviating data sparsity [ 4 ].
 Collaborative topic ranking (CTRank) is a recently proposed hybrid method, which seamlessly combines latent dirchlet allocation and pair-wise ranking model. Although this model learn the feature of side information associated with items, LDA is often not effective enough to learn the latent representation especially when side information is sparse [ 13 ]. Alternatively, deep learning, as a set of representation-learning methods, models multiple levels of representation of raw input by composing simple but non-linear modules that each transform the rep-resentation at one level into a representation at a higher, slightly more abstract level [ 3 ]. It has been proved that deep learning methods are expert in automati-cally mining and representing intricate structures in high-dimensional data. In this paper, we propose a hybrid pair-wise recommendation approach with implicit feedback, named collaborative deep ranking (CDR), which integrates abstract representation of side information about items into Bayesian frame-work of pair-wise ranking model. Specifically, Stacked Denoising Autoencoders (SDAE) is exploited to extract the feature representation of item content. Coop-erating with this, pair-wise ranking component can tackle sparsity problem to some extent and improve the recommendation accuracy. Note that, although CDR employs SDAE for feature representation, CDR as a generic framework also can collaborate with other deep learning methods, such as convolutional neural networks and recurrent neural networks.
 The main contribution is summarized as follows: 1. We propose a hierarchical Bayesian framework, named as CDR, which com-bines deep feature presentation of item content and user implicit preference for sparsity reduction. 2. We conduct experiments on a real-world dataset to evaluate the effective-ness of CDR. Experimental result shows CDR outperforms three state-of-art methods in terms of recall metric under different sparsity level.
 The remainder of this paper is organized as follows. Section 2 gives an overview of the related work. Section 3 demonstrates details of our proposed model. Section 4 shows the experimental results and Sect. 5 concludes the paper. In many practical recommendation scenarios, users rarely express their explicit behaviors, while implicit ones are more common (e.g. clicking and browsing history). This class of collaborative filtering with only positive examples is also called One-Class Collaborative Filtering (OCCF) [ 5 ]. There are mainly two types of existing approaches for OCCF: point-wise and pair-wise [ 14 ].
 Hu et al. [ 1 ] estimate user-item pair preference whether the user would like or dislike the item and then assign a confidence level for this. After defining pref-erence and confidence level, they join them into traditional probabilistic matrix factorization. However, the performance of CF-based models degrades signifi-cantly when facing data sparsity problem. Many models explore side information about items and users to alleviate this problem. Wang et al. [ 11 ] propose col-laborative topic regression (CTR) for recommending scientific articles. In CTR, item content based on probabilistic topic model incorporates into traditional collaborative filtering. Based on this work, Purushotham [ 7 ] further study the influence of users X  social network and propose CTR with SMF model. Recently, Wang et al. [ 13 ] employs deep learning model to automatically learn effective representation of content of items. From their experiments, we can observe that deep learning models are more appealing than traditional topic model in feature representation.
 for recommendation, pair-wise algorithm generates a preference ranking of items for each user. Rendle et al. [ 8 , 9 , 15 ] assume that user prefers observed items than unobserved items and propose a generic Bayesian Personalized Ranking (BPR) framework optimization criterion. They also demonstrate matrix factor-ization and adaptive kNN learned by BPR are superior to the same model with respect to other criteria under AUC evaluation. Pan et al. [ 6 ] relax individual and independence assumption in BPR by adding the interaction of users and propose group Bayesian personalized ranking (GBPR). For sparsity reduction, several models extend pair-wise ranking techniques via taking extra side infor-mation into consideration. Grimberghe et al. [ 2 ] combine social graph and BPR matrix factorization for social network data. Yao et al. [ 14 ] propose a hierarchi-cal Bayesian framework, which integrates latent dirchelet allocation into BPR matrix factorization. However, the topic model is not effective enough when side information is sparse. Therefore, in this paper, we integrate deep representa-tion learning of content of items into pair-wise ranking model and propose a generalized hierarchical Bayesian model, called CDR. In this section, we present details of our proposed algorithm, CDR, which inte-grates pair-wise ranking models and side information about the items. Notation and Problem Definition. Let U denote the set of users and denote the set of items. The size of U and I are n and m , respectively. This paper focuses on implicit feedback recommendation scenarios, which means the implicit interaction matrix R  X  X  X I is available. Specifically, the elements r = 1 in matrix R denotes user i prefers item j , while r ij i is not interesting in item j or might not observe item j yet. For a given user i , pair-wise algorithms [ 9 ] suppose that user i prefers item j over item k if and only if j  X  X  + and k  X  X \I + , where I + = { j : r ij =1 } Except the observed binary matrix R , side information about items could be collected in many scenarios (e.g. item profile in Amazon). Given an m matrix X c presents the side information about all items, the j -th row denotes the bag-of-words vector of item j based on vocabulary of size S (i.e. X Let u i , v j denote the latent factor with low dimension K of user i and item j , respectively. Our objective is to learn the latent factor U =( u ( v ) j =1 from implicit interaction and item information matrix for recommending an personalized ranking list for users.
 Stacked Denoising Autoencoders. Generally, a good representation of side information about items can improve performance of Recommender System. Denosing Autoencoders (DAE) [ 10 ] learns an compressed representation from corrupted input to recover the clean input through a feedforward neural network. SDAE stacks DAE to form a deep network by feeding the output code of DAE found on the layer below as input to the current layer and the highest level output representation is used as item feature. An SDAE network is to minimize the regularized optimization problem as below: where W l and b l is the weight matrix and bias vector of layer l , L is the number of layers, and  X  w is the regularization hyperparameter.
 Supposing that the corrupted input X 0 and the clean input X variables, SDAE can be generalized as a probabilistic model [ 12 ]. The generative process is as follows: 1. For each layer l of the SDAE network, (a) For each column n of the weight matrix W l ,draw W l, (b) Draw the bias vector b l  X  X  (0 , X   X  1 w I K l ) (c) For each row j of X l ,draw X l,j  X   X  X  (  X  ( X l  X  1 ,j 2. For each item j , draw a clean input, where I K l is a K-dimensional identity matrix of layer l ,  X  parameters and  X  ( . ) is the sigmoid function Through maximizing a posteriori estimation, the model will degenerate to be the original SDAE if  X  ity (i.e. X l  X  1 ,j  X  =  X  ( X l  X  1 ,j  X   X  W l + b l )). After this process, X present the latent feature representation of side information about all items. Collabrative Deep Ranking. CDR exploits pair-wise preferences and content-based items feature together for collaborative filtering. Figure 1 shows the graphic model of CDR. Obviously, there are two generative processes in our model. First, the original SDAE process (in the red dashed rectangle) extracts feature representation from side information about items and then integrates them into latent factor of items in pair-wise ranking model. Second, the pair-wise ranking model captures special relationship  X  ijk = r the preference of user i on item j and k . Unlike the point-wise approach directly predicting the value r ij , pair-wise approach instead classifies the difference of r 1. For each layer l of the SDAE network, 2. For each item j , 3. For each user i , than item k . For simplicity, we set c ijk = 1 in the experiments. Note that the linkage between SDAE and pair-wise ranking model is the middle layer X In the extreme case, if  X  j = 0, the latent factor of items completely generates from content information, which will ignore the information contained in user preference matrix.
 estimator can be utilized. Through Bayesian inference, we have
P ( U, V, X l ,W + |  X , X 0 ,X c , X  u , X  v , X  s , X  w , X  n
P ( U |  X  u ) P ( V |  X  v ,X L Because we place Gaussian priors on user, item and wight matrix, corre-sponding conditional probability is Similarly, we have below corresponding conditional probability based on assump-tion of Gaussian distribution: Similar to the generalized SDAE,  X  s goes to infinity and maximization of posterior probability is equivalent to maximizing the joint log-likelihood of U , V , X , X c , W + ,and  X  given  X  u ,  X  v ,  X  n ,
L =  X  In the generative process, we assume that the preference  X  Distribution. However, similar loss function can be obtained with different assumption on  X  ijk (e.g. Bernoulli distribution in [ 14 ]). Note that the model pair-wise ranking and side information about items. The big difference is that CTRank extracts topic proportions from content of items to conduct the learn-ing of latent factors for ranking, while CDR exploits deep network to mine effec-tive feature representation of items. Note that prior distribution of LDA-based models is difficult to define. What X  X  worse, topic proportions can not effectively represent the latent feature of items when side information is very sparse. As showed in the experiments, CDR gets better performance.
 ThefirstterminEq. 5 extracts user preference from implicit matrix construct pair-wise ranking loss, while the fourth term integrates content of items. Therefore, if two item j and k have similar side information (i.e. similar X ), the distance between v j and v k will be reduced. As we have mentioned, X goes to positive infinity, the disappeared reconstruction error will lead to invalid feature representation X L On the other hand, when  X  v / X  n approaches to zeros, CDL will decouple into two models and the learned V is not influenced by side information about items. Both extreme cases demonstrate bad performance in the experiments. Parameter Learning. Similar to [ 1 , 13 , 14 ], we optimize this function using coordinate ascent by alternatively optimizing latent factors u matrix &amp; bias vector W + . Given a current estimate of W and v k based on the following stochastic Newton-Raphson rules: where  X  is learning rate and E ijk =  X  ijk  X  ( u T i v j  X  ing, bootstrap sampling is applied to sample observed item j and unobserved item k of user i [ 9 ].
 back-propagation learning algorithm. The gradient of L with respect to W b is as follows: Prediction. After learning the optimal parameters U , V , W from its expectation: and then a ranked list of items is generated for each user based on these predic-tion values.
 Complexity Analysis. According to updating rules, the complexity of com-puting U is approximately O ( nrK ) where r is the average number of items a user interacts. The complexity of computing the output of encoder is controlled by the computation of first layer. Therefore, the complexity of updating V is O ( nrK + sK 1 ), where K 1 is the dimension of first layer. The complexity of updating all the wights and bias is O ( msK 1 ). Hence, the total complexity is O (2 nrK + sK 1 + msK 1 ). In this section, we compare performance of our approach with some state-of-art algorithms. All experiments are conducted on a server with 2 Intel E5-2620 CPUs and 1 GTX Titan GPU.
 Datasets. To effectively illustrate the performance of CDR, we use the same dataset in [ 11 , 13 , 14 ]. The dataset is collected from CiteULike service for managing and discovering articles for users. In this dataset, if a user the article, rating as  X 1 X  otherwise  X 0 X . The preliminary statistics shows that the dataset contains 5,551 users and 16,980 articles with 204,986 user-item preference pairs. Note that the sparsity is 99.78 % and each user has at least 10 articles in their preference library. To obtain the side information about articles, the title and abstract of articles are exploited. After removing stop words, we extract 8000 distinct words through sorting their TF-IDF values. As a result, the size of X c is 16980 Evaluation. Similar to [ 7 , 11 ], we employ the metric recall to quantize the per-formance of recommendation, since the metric precision is not suited to implicit feedback datasets. Because the meaning of zero entry in the user-item matrix is ambiguous, which represent either user don X  X  like item or is unaware of item. Instead, the positive rating (e.g. r ij = 1) only hints the user i likes the item dataset, we sort them and recommend top M items for each user. The recall@ M is defined as follows: Average recall from all users points out the performance of method. Baselines and Experiments Setting. In order to evaluate effectiveness of our approach, CDR, we compare it with three state-of-art hybrid recommendation algorithm for implicit feedback as follows:  X  CTR. Collaborative Topic Regression is a point-wise algorithm, which com-bines probability matrix factorization and latent dirchelet allocation [ 11 ].  X  CTRank. Collaborative Topic Ranking, a pair-wise algorithm, which inte-grates side information of items into Bayesian personalized ranking [ 14 ]. With different assumption of preference, Two algorithms, CTRank-log and
CTRank-squared, have been proposed. We choose CTRank-squared as our compared approach, because it has higher performance than CTRank-log.  X  CDL. Collaborative deep learning is a point-wise hierarchical Bayesian model, which first tightly couples deep representation feature of content information and collaborative filtering [ 13 ].  X  CDR. Collaborative Deep Ranking is our proposed model described in Sect. 3 . train set and take all the rest as test set in the experiments. In particular, we vary train set sparsity is 0.006 %, 0.03 %, 0.06 % (i.e. P =1 , 5 , 10). Each approach is performed 5 times with different random seeds for each sparsity and the average performance is reported. The grid search is applied to find optimal hyperparameters for each approach. For CTR,  X  u =0 . 1,  X  b =0 . 01, K = 50, and  X  = 1 can reach good performance (note that  X  is the dirchelet prior). For CTRank, we find  X  u =0 . 025, positive  X   X  v =0 . 025, K = 200, and c = 1 can achieve best results. For CDL, we set the same parameters of a =1, b =0 . 01, K = 50,  X  w =0 . 0001, and a 2-layer SDAE architecture  X 8000-200-50-200-8000 X  for different P . Otherwise,  X  and  X  n = 1000 when P = 1 and 5, while  X  u =0 . 1,  X  v =1,and  X  P = 10.
 grating filters (i.e. masking noise) with a noise level of 30 % to obtain the cor-rupted input X 0 from the clean input X c . Meanwhile, dropout rate is set to 0.1 for achieving adaptive regularization when the number of layers is more than 2. The number of hidden units K l is set to 1000 ( l = L 2 ), while the number of mid-dle layer is 200. That is, the dimension of feature representation and latent factor u , v j is 200. Note that K 0 and K L are equal to the size of vocabulary. After grid searching, we find that the hyperparameters  X  u =0 . 01,  X   X  w =0 . 0001 can achieve good performance when P =5and P = 10, while we set larger hyperparameters (i.e.  X  u =1,  X  v = 10,  X  n = 1000, and  X  to prevent overfitting when P =1.
 Comparison. Figure 2 provides comparison results of CDR, CDL, CTRank and CTR under different sparsity. A 3-layers CDR  X 8000-1000-200-1000-8000 X  is used. It can be observed that our proposed approach outperforms other three methods at all sparsity levels. As a whole, baseline CDL performs better than CTR and CTRank. That is, deep learning approach (e.g. SDAE) can admire better feature quality of side information about items than topic model (e.g. LDA). The reason may be that deep learning approach captures distributed features, while the features (i.e., topics) in LDA is independent. Otherwise, CTRank outperforms CTR in most case (both models use LDA model) and CDR outperforms CDL (both model employ deep learning architecture). Therefore, pair-wise algorithm with directly optimizing ranking has advantage over point-wise method which optimizes rating. Concretely, when recommending 300 articles CDR relatively improves 36.58 %, 66.96 %, 9.22 % than CTR, CTRank, CDL respectively at P = 1 while the value is 13.44 %, 11.34 %, 5.25 % at P = 10. Thus, when data is sparse, the relative improvement is more significant. That is, CDR can alleviate data sparsity to some extent.
 Impact of # layers. Table 1 presents the recall@300 under different P with various #layers. As we can observe, when the number of layers is 1 and 2, the recall is quite low at P = 1, while the performance significantly enhance with the number of layers growing. That is, the performance of recommendation depends on the quality of feature representation of side information about items when the data is extremely sparsity. As reducing the sparsity degree, the effective of pair-wise ranking component begins to present and CDR starts to overfit when # layers =4and P = 5. We also can find that the recall value is similar when P = 10 in different number of layers. That means with increasing of train set size, pair-wise ranking model can guide the further learning of features in CDR model.
 Impact of  X  v / X  n . Figure 3 shows the impact of  X  v / X  n changing  X  v and fixing other hyperparameters. We can observe that as increasing or reducing the  X  v from the optimal  X  v , the performance degrades gradually. This result is consistent to the explanation in Sect. 3 . When  X  the side information about items dominates the learning process of V and the performance purely depend on X L generates by the pair-wise ranking component. The experimental result indicates that appropriately combining pair-wise ranking and content of items can achieve better performance than in above two extreme case.
 Latent Factor Interpretability. To demonstrate the validity of CDR deeply, Table 2 show one example users of top 3 topic of his all articles and the top 10 recommended articles under the setting P = 10. From the top 2 topics, we can speculate the user focus on tag recommendation research, while the user also study web search based on the third topic. CDR successfully captures all three topics and reach 80 % recall when recommending top 10 articles. It is worth mentioning, the rank of recommended articles of the first topic is higher than the third topic. In this paper, we propose a hybrid recommendation approach (CDR) with implicit feedback. Specifically, CDR employs SDAE to extract deep feature rep-resentation from side information and then integrates into pair-wise ranking model for alleviating sparsity reduction. Our study presents that CDR outper-forms other three state-of-art algorithms at all sparsity level. In the future, we plan to use other deep learning methods to replace SDAE for boosting further performance in our hierarchical Bayesian framework. For example, the convo-lutional neural network which considers the context and order of words may improve the performance. Beyond that, we also consider how to incorporate other side information into our framework, such as users social network and items relationship.
