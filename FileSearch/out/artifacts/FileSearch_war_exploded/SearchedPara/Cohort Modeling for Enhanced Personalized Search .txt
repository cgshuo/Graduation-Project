 Web search engines utilize behavioral signals to develop search ex-periences tailored to individual users. To be effective, such person-alization relies on access to sufficient information about each user X  X  interests and intentions. For new users or new queries, profile in-formation may be sparse or non-existent. To handle these cases, and perhaps also improve personalization for those with profiles, search engines can employ signals from users who are similar along one or more dimensions, i.e., those in the same cohort . In this paper we describe a characterization and evaluation of the use of such cohort modeling to enhance search personalization. We experiment with three pre-defined cohorts X  X opic, location, and top-level domain preference X  X ndependently and in combination, and also evaluate methods to learn cohorts dynamically. We show via extensive ex-perimentation with large-scale logs from a commercial search en-gine that leveraging cohort behavior can yield significant relevance gains when combined with a production search engine ranking al-gorithm that uses similar classes of personalization signal but at the individual searcher level. Additional experiments show that our gains can be extended when we dynamically learn cohorts and tar-get easily-identifiable classes of ambiguous or unseen queries. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search process , selection process , clustering . Cohort modeling; Personalization; Web search. Personalization of search results has been investigated in detail in domains such as Web search and beyond [24][27][31]. The ability to tailor search results to a particular individual enables a wealth of opportunity to better satisfy their particular information needs. Per-sonalization models are typically learned from observed short-and long-term search behavior (such as queries and result clicks), which is either used directly [32] or is converted into a different represen-tation (e.g., a set of topical categories) to build more general models and improve personalization coverage [5][24]. Despite the value of personalization, one drawback is that it requires sufficient user in-formation to perform effectively; users must be willing to share their search history and the search engine must attain sufficient in-formation on user interests to build accurate profiles. Even short-term personalization depends on the long-term behavior for the first query in the session when no other activity has been observed [5]. It is known that users frequently submit the same query to find the information they have searched previously. Teevan et al. [29] found that approximately 33% of query instances in their case study were an exact repeat of a query submitted by the same user at a previous time. For such refinding queries, the results clicked by individual users in their search history provide a strong signal to identify the correct result for each user when that query is repeated [32]. The remaining 67% queries are new, and it can be challenging to im-prove their search quality via personalization given limited history. One way in which these issues can be addressed is by finding co-horts of searchers who share one of more attributes with the current searcher. Attributes that could be used to form cohorts include lo-cation, topical interest, and domain preferences, all easily accessi-ble to search engines via users X  long-term search histories. Given a user, we can leverage the search behavior of other members of their cohort(s) to enhance personalization by providing signals if suffi-cient information is unavailable or as an additional signal to build richer personalization models if they already exist. Cohorts have been used effectively in applications such as collaborative filtering (CF) [12], where groups of similar users (based on factors such as liking the same item [19]) can yield relevant recommendations. Co-horts have also shown some limited utility in retrieval settings. Groupization has shown promise in laboratory settings [30], task models to find those engaged in similar tasks have yielded strong results [37], and there have even been attempts to use CF more di-rectly in search result ranking [25]. However, there has been no de-tailed study of applying cohort models to enhance Web-scale search personalization. We address that shortcoming in this paper. We propose the construction and application of user cohorts to en-hance Web search personalization. Our initial method creates pre-defined cohorts on three types: topic, location, and top-level do-main preference (e.g., .gov, .edu). Rather than limiting ourselves to these pre-defined sets, we also propose clustering methods capable of learning the cohorts and dynamically assigning users to one or more clusters. We demonstrate through extensive experimentation with search engine log data that our cohort modeling methods can yield significant relevance improvements over a production ranker that already included personalization targeting the current searcher. We show that these gains are even larger when we target particular queries (e.g., those with high ambiguity) and particular users (e.g., those with no query-relevant history). We make the following contributions with this paper:  X  Describe a method to generate pre-defined cohorts using infor- X  Demonstrate that modeling user interests within these cohorts  X  Demonstrate that there are particular sets of easily-identifiable  X  Propose methods to dynamically learn cohorts rather than using The remainder of the paper is structured as follows. In Section 2 we present related work in areas such as personalization, collaborative filtering, and cohort identification and use. Section 3 describes the pre-defined cohorts and the modeling process, as well as various definitions of important attributes such as click-through rate and smoothing, as well as how we can apply clustering methods to learn cohorts dynamically rather than using a pre-defined set. Section 4 describes the datasets. Section 5 describes our methods and exper-imental results, and Section 6 reports the results of learned cohorts. Section 7 discusses our findings, their implications, and concludes. There are three relevant areas of related work: (1) personalization of search engines based on short-and long-term searcher interests, (2) collaborative filtering, and (3) mining the search behavior of other users to complement and enhance search personalization. Large-scale behavioral data from search engines has been mined extensively to improve result relevance in the aggregate across all users [1][15]. Search preferences are personal and research on per-sonalizing retrieval [22][31] has shown that implicitly collected in-formation such as browser history, query history, and desktop in-formation, can be used to improve the relevance of search results for a particular individual. Short-term behavior from within the cur-rent search session has been used for tasks such as result ranking [39] or predicting future search interests [34][35]. Teevan et al. [31] showed that their personalization algorithm improved as more data was available about the current user. Long-term behavior has been used for personalizing search by constructing longitudinal models of user interests [24], including using the previous queries associ-ated with the pursuit of similar information needs [27]. Models can use different sources, ranging from specific query-URL pairs which have high precision but low coverage [32] to more general methods that use topical representations of user search interests [24]. When there is insufficient data about the current user, the search behavior of other related users may be beneficial in modeling user interests and intentions. Teevan et al. [30] explored the similarity of query selection, desktop information, and explicit relevance judgments across a small group of work colleagues grouped along two dimensions: (1) the longevity of their personal relationship, and (2) how explicitly the group was formed. They found that some groupings provide insight into what members considered relevant to queries related to the group focus, but that it can be challenging to identify valuable groups implicitly. White et al. [37] address this issue by implicitly modeling the search task of the user, finding others who have attempted a similar task, and using their on-task behavior to enhance relevance. Although they used cohorts (loca-tion, topic expertise, and search engine entry point) as part of their ranking experiments, they observed limited gain in their experi-mental setting and how they chose to model and integrate cohorts. Collaborative filtering (CF) [12] can also be used to find people with similar interests and leverage their activities and preferences to help the current user. The lack of sufficient personal information (sometimes referred to as the  X  X old start X  problem) has been studied in research on CF and on recommender systems [20]. This research has shown that a number of sources can be used to generate recom-mendations from others in a given community, including agree-ment in item ratings [19] and social network memberships [16]. There are three predefined cohorts that we focus on in our study: topical interests, domain preferences, and geographic location. We now describe relevant related work in each area, beginning with topical interest, some of which leverages CF to find similar users. Topic information can be used directly to improve search engine ranking [4]. Sugiyama et al. [25] addressed sparseness in user term-weight profiles by applying CF techniques to attain term weights based on those of users with similar profiles. Similar approaches have used click-through data to personalize result rankings and backed-off to the clicks of others [2][26]. Almeida and Almeida [2] used Bayesian algorithms to cluster users of an online bookstore into communities based on links clicked within the site and found that the popularity of links within different communities could be used to customize result rankings. Lee [17] proposed a system that uses data mining to uncover patterns in users X  queries and browsing to generate recommendations for users with similar queries. These techniques perform matching with other users based on individual queries or URLs, severely limiting coverage. Freyne and Smyth [10] addressed this concern by connecting different communities based on the degree to which their queries and result clicks overlap. Alternative methods have been proposed that are query independ-ent. Smyth [23] suggested that click-through data from users in the same  X  X earch community X  (e.g., a group of people who use a spe-cial-interest Web portal or work together) could enhance search. He provided evidence for the existence of search communities by showing that a group of co-workers had a higher query similarity threshold than general Web users. Ieong et al. [14] showed that searchers exhibited domain preferences, where they favored partic-ular sources when selecting results. White et al. [38] found that do-main experts preferred different top-level domains than novices, with more focus on educational (.edu) and governmental (.gov) sites, whereas novices preferred commercial (.com) sites. Turning to location, Mei and Church [18] found that geographic location might serve as a reasonable proxy for community, since they observed that grouping users based on the IP address similarity could improve relevance. Cheng and Cant X -Paz [10] developed models for personalized click prediction in online advertising that leveraged demographic and location features to improve prediction accuracy. Bennett et al. [3] showed the effectiveness of location-based personalization, whereby models of searcher interests for particular locations can be learned and used in concert with the searcher X  X  current location to improve relevance. White and Buscher [36] automatically identified users with local expertise (knowledge of a specific city or town) from search log data, and showed that the interests of these local users was both different and that there were differences in the quality of the entities they visited (restaurants reserved in this case), with locals selecting higher-rated venues. Weber and Castillo [33] estimated searcher demographics by joining search location with census data and demonstrated vari-ations in search behavior for different demographic groups. Our research extends previous work in the following ways. First we devise pre-defined searcher cohorts focused on attributes readily available at scale to Web search engines: specifically topic, loca-tion, and top-level domain preference. Second, we experiment with applying cohort models both in isolation and in combination to en-hance personalization across all queries. Third, we analyze the per-formance of our methods in a number of additional search scenarios (e.g., ambiguous or unseen queries), and demonstrate strong rele-vance gains. Finally, we propose methods to learn cohorts via clus-tering, removing the need to use pre-defined sets. We show that employing this method allows us to further enhance personalization effectiveness over using our pre-defined cohort modeling methods. We now describe the construction of our cohort models, beginning with the nature of the data, but also including features computed. Upon submission of a query to a search engine, a list of search re-sults is retrieved and ranked for the user. The user examines the list and decides the next action on the results: click or not click. Search engine logs capture much of this interaction. In our study, we use logs sourced from the popular Microsoft Bing search engine. An entry in the search log comprises a tuple , where is a user selected from the universe of users , is a query in the universe set of queries , and is from the universe of documents (search results) , is a binary value that 1 is a click and 0 other-wise, and is the timestamp. Such tuples can be created for each of the top-ranked search results returned by the search engine. The click-through rate (CTR) of a query-document pair is the ratio of the number of clicks on the document to the number of impressions in which that result is shown for the search query. CTR is commonly used to measure the probability of a click given a query-document pair, i.e., . Given this, plus the sim-plicity and general applicability of CTR, it seems appropriate to fo-cus on applying it for this first study of cohort modeling. In our approach we focus on a subset of result clicks suggesting that searchers are satisfied with the particular search results that they selected. We refer to these in this paper as satisfied (SAT) clicks. Definition 1 (SAT Click): As defined in [12], SAT clicks have an associated dwell time of 30 or more seconds between search engine actions, or it is the last action in a search session (presumed SAT). Using SAT clicks rather than all clicks can provide a more accurate CTR signal since accidental or misinformed clicks are excluded. Therefore, rather than simply counting the number of clicks divided by the number of impressions, we can compute CTR as: Users may search for different information under the same query. This can also be reflected in a CTR tailored to each searcher. We use an individual X  X  click-through rate to estimate the degree of satisfaction of the user with a document given a query: Armed with this important definition, we can now proceed to define the features that we use in our cohort models. As mentioned earlier, we represent users by contextual features cor-responding to domain preference, location, and topical interests. Top-Level Domain: The domain name of a URL represents its net-working context, the administrative autonomy, and authority. A re-cent study showed that searchers exhibit a preference for particular domains irrespective of relevance [14]. The number of unique do-main names across the broad range of information needs in our da-taset is intractable. We therefore used the top-level domain (TLD). TLD includes generic domain extensions such as .com, .net, .org; sponsored extensions such as .mil, .asia, .edu; and country codes such as .us, .uk, .fr. Related work on domain expertise in search revealed that there were differences in the TLDs selected depend-ing on user domain expertise level (experts preferred .edu and .gov, whereas novices preferred .com) [38]. The TLD may therefore of-fer some insight into the subject matter expertise of the searcher, which can be useful in performing richer personalization. We limit our study on search logs collected in the United States geographic locale, but we observe many clicks on URLs with other country code domain extensions. This information could be used to estimate the native language of users or simply countries with which they have an interest. There are many TLDs, and because many are fairly new or visited infrequently (at least from search results), we do not observe many clicks related to them. We include all general and sponsored TLDs (23 in total), and also select 11 country code TLDs which are registered in the first and second year of availability, since we observe the number of Web pages and clicks of a TLD is related to its time in existence. We then randomly sample 3% search logs during a two-month period and examine the number of SAT clicks on selected TLDs. TLDs with &lt; 1000 SAT clicks, e.g., .arpa, .post, .tel, are excluded. We retained the remaining 31 popular TLDs and use  X  X ther X  for all other cases. This set of TLDs is used to construct our cohorts. Location: The location of a user may also reveal their search inter-ests and intentions [3]. We estimated the location of the user at query time using reverse IP geocoding. Since a user may not be confined to a particular city, but will generally remain within a state, we compute location preference for each user at the state level. There are 51 U.S. state features. When we failed to identify the location of a user, we categorize their location as  X  X ther X . Topic: We utilize the Open Directory Project (ODP, dmoz.org), a human-generated hierarchical taxonomy of Websites, as our topical ontology. This has been used extensively in previous work on per-sonalization to model search interests at a level beyond queries and documents [5][24]. Topics are assigned to URLs using the content-based classifier described and evaluated in [4]. The user X  X  degree of interest in a topic is then inferred from the number of clicks of URL results under that topic in their click history. ODP contains 15 top-level categories such as  X  X rts X ,  X  X ports X , etc. To manage the size of the feature space, we focus on top-level categories only. Given features explained above, we define a cohort as a group of users sharing a contextual feature. The total number of selected fea-tures is 99. In other words, we define 99 cohorts of users based on shared contextual features. A user can be a member of multiple co-horts. We model cohort membership to indicate how likely a user is to be a member. It is also used to measure how strongly to weight the user contribution to the cohort when aggregating cohort clicks. We denote as the -th cohort of a particular type, as cohorts of top-level domains, as cohorts of locations, and of ODP categories (topic). Since the following calculations for each of the three cohort types are the same, we ignore the superscript in the cohort notation for simplicity. Definition 2 (Cohort Membership): The cohort membership vec-tor for user is defined as a -tuple ( ) = [ ( , 1), ( , 2), ..., ( , )], in which is the number of cohorts, and repre-sents the degree of membership for the user in -th cohort (say,  X  X alifornia X ). ( ) is normalized such that The cohort membership is drawn from a multinomial distribution of SAT clicks, and calculated as follows: Example 1: Suppose that there are only three cohorts: California, Washington, and Oregon. If we observe three SAT clicks when the user is in California and one SAT click in Washington, the cohort membership across the three states would be [0.57, 0.29, 0.14]. Definition 3 (Cohort CTR): Given a cohort type (e.g., Topic), the cohort CTR for a query and URL document is a -tuple c-ctr ( d , q )=[ ], in which is the number of cohorts, and is the probability that users in -th cohort will click document for the query . It is a weighted aggregation of individual CTR as follows: Cohort CTR is used to measure the cohort preference on the docu-ment given the query . It weights a user X  X  clicks by their cohort membership. Users who exhibit strong preference to the cohort will contribute more to the cohort CTR, e.g., for a California state co-hort, a user residing in that state for a long duration will have a higher influence factor than a user who only visits occasionally. Example 2: Suppose that there are only three cohorts: California, Washington and Oregon, and two users a and b . The cohort mem-bership vector for is ( ) = [0.57, 0.29, 0.14], for is ( ) = [0.1, 0.1, 0.8]. Given the query [osu], considering two search results =  X  X su.ppy.sh X , and =  X  X regonstate.edu X , the number of SAT clicks by user is = [5, 1] for and respectively, and the number of SAT clicks by user b is = [1, 5]. For simplicity, we assume number of impressions on each document for each user is 100. By Equation (4), we can compute the cohort CTR for the result as c-ctr ( , ) = [0.044, 0.039, 0.016], and for as c-ctr ( , ) = [0.0159, 0.02, 0.044]. This demonstrates that the California co-hort prefers the result , and that the Oregon cohort prefers , given the query [osu]. Note that the global CTRs for both results are the same, i.e., ctr ( , ) = ctr ( , ) = (5+1)/(100+100). There are two intuitions behind our model. First, users in a cohort with shared contextual features are likely to be coherent in search intentions and click preferences (an assertion supported by our pre-liminary investigations  X  not reported here for space reasons). Sec-ond, a common approach for handling the problem of insufficient individual historical data is to leverage global CTR. However, global CTR treats clicks from all users equally, and therefore has limited potential to help in personalization. Our approach identifies and separates cohort clicks from global clicks. When estimating an individual X  X  click preference, we can learn more from clicks by cohorts of similar users, who have higher impact on the estimation, and are better aligned with the target user. We show in our later experiments that cohort modeling can outperform global CTR. CTR is one of the most informative metrics to measure search result quality. However, CTR estimates are sometimes noisy when obser-vations are scarce. For example, if we only observe one impression for a pair , and a single SAT click on the document , we will obtain . This is an inaccurate estimate of the true click probability, and is caused by data sparseness. These instances are common in logs, especially for tail queries that occur rarely. To handle this situation, we apply smoothing methods to estimate CTR. We add a pseudo count that counts SAT clicks times during impressions. The smoothed CTR is computed as follows. After smoothing, extreme cases should have lower CTR than URLs with sufficient SAT clicks and impressions, but higher than those with no SAT clicks. Based on this expectation, we sample hundreds of instances and manually validate the output to tune and . Fol-lowing several experiments we set = 0.001, and = 1000. When calculating cohort CTR for a given cohort , we then smooth cohort CTR with smoothed global CTR as follows: We set =10 through similar manual validation. For unobserved or scarcely observed the cohort CTR is aligned with a smoothed global CTR of . For a user, the click probability on a URL document can be esti-mated from the click history of similar people. Given the cohort model, we now derive cohort features, which infer individual click probabilities that are associated with the user X  X  cohort membership. Definition 4 (Cohort Features): Consider a user with cohort membership , and the cohort CTR for a query document pair c-ctr ( d , q ), we derive cohort features as an -tuple: [ ], where is the number of cohorts, and is the click probability in the -th cohort. The probability is computed as follows: Example 3: Following the setting in Example 2, that c-ctr ( , ) = [0.044, 0.039, 0.016], given a new user with = [0.56, 0.22, 0.22], the cohort features for document , query which is [osu] and the user , is = [0.02464, 0.00858, 0.00352]. When a user submits a query , we estimate their click preference on a document depending on their cohort membership and the cohort click probability . The weight of cohort membership controls how much we can infer about this user X  X  click behavior based on a cohort X  X  click behavior. If a user belongs to the California cohort with a weight of 0.9 and the Washington cohort with a weight of 0.1, the estimation of their click probability there-fore relies mainly on the cohort California, and only slightly on the cohort Washington. We create cohort features for each tuple, and let the ranking algorithm decide the ranking of URL can-didates based on these cohort signals. To evaluate the effectiveness of our cohort model for enhancing personalization, we apply it to extend a personalization model on the Microsoft Bing commercial search engine. The existing person-alization approach is built upon the standard search engine that re-trieves the most relevant documents via querying. This is a state-of-the-art personalization method that employs a number of short-and long-term topical, location, and domain preference features, some that are similar to prior work, e.g., [3][5][24][32][35]. These features are derived and used by the engine at the individual level. In addition to these personalized features, the model uses a global CTR feature for each query and document pair. This ranker in pro-duction serves as a strong baseline for our cohort experiments. We evaluate our methods retrospectively using logs from Bing con-taining search behavior and the original (sometimes personalized) result ranking from the engine. We mined over two months of logs from the US English geographic locale, and extracted events com-prising tuples of: query, an ordered list of the top-10 search results returned by the engine, and clicks on those results. The order of the URLs for a query was produced by the baseline ranker which em-ployed personalization for some queries as described above. We re-ranked results using our enhanced model. This methodology allows us to estimate the effectiveness of our cohort modeling approach. Cohort features, which are based on click history, are good indica-tors of document relevance for given queries. However, the volume of URL documents is large, and many documents are not selected or displayed many users. Although we incorporated smoothing techniques to overcome such sparseness, we found in practice that constructing features at a higher level further addresses this chal-lenge. For instance, we can replace URL documents in our cohort models by URL domains and re-rank using the same personaliza-tion approach. Specifically, the symbol is used to represent a URL domain rather than a URL document. A domain is part of a URL, e.g., URL=http://www.cnn.com/politics, domain=cnn.com. As stated above, we use the production ranker from the search en-gine as the baseline for comparison. We then train a new model, with cohort features added. Bing search logs for a two-month pe-riod (March 31 2013 to May 28 2013) are used to construct cohort membership vectors and cohort CTRs. We refer to this time seg-ment as the profiling period . Cohort features are then built for June 4 2013), which is then divided into training , validation and testing periods. The first three days were used for training, the next two days were used for validation, and the last two days were used for testing. The performance is evaluated by re-ranking top results returned from the baseline ranker. This method has been used suc-cessfully in prior studies of search personalization at scale [3][24]. Table 1 presents the statistics on the datasets used, including the number of search queries (impressions), the number of distinct que-ries, the number of distinct URL domains, and the number of users in our dataset. Besides the comparison on all queries, we also clas-sified queries into various segments to facilitate a more detailed analysis of the performance of our cohort modeling methods. Previous studies have shown that although searchers frequently submit repeated queries for refinding purposes, there are also a large fraction of user queries that are new [21][29]. A new query from a particular user means by definition that user has not submit-ted it previously (at least not in an observable period, such as the two months used for profile building). Given their frequency, new queries are a particular subset where search engines could offer sig-nificant benefit, but since there is no user history it is not clear what support they can offer on an individual level. This means that they must resort to global models of all users X  on-query behavior. These are queries where cohort modeling may offer particular assistance. Definition 5 (New Queries): In our experiment, for each user, que-ries that are shown in the testing period but not in the profiling, training and validation periods are defined as new queries. In con-trast, queries that appear in all periods are defined as old queries. In our analysis, we identify new queries for each user and separate them from old queries to evaluate the re-ranking performance of cohort models. To simplify the determination of new queries, we focus on exact match of queries on training and testing periods. The derivation and application of more sophisticated matching methods (e.g., semantically-equivalent queries), is a separate research prob-lem and is reserved for future work. Some preprocessing steps are applied, including converting queries to lowercase, removing sur-plus whitespace, and deleting punctuation while preserving the n-grams for terms joined by punctuation (e.g., asp.net). Our cohort model leverages group click preferences to estimate in-dividual click preferences. For a user who submitted a query, we identified a cohort of other users who are similar. However, if only a small number of users in the group submitted the same query, the prediction of cohort preference for the query will be biased and not representative of the full cohort. As part of our re-ranking experi-ments, we wanted to better understand the impact of query popu-larity on re-ranking performance when cohorts were utilized. Definition 6 (Popular Queries): The popularity of a query is de-termined by the number of distinct users who submitted the query during the profiling period, which is denoted by : 6 . Search has a long tail effect that many tail queries are submitted only one or two times, by a small number of users. Cumulatively, there are a large number of such queries. We divided queries into two datasets: (1) popular : : 6  X  10 , and (2) unpopular : : 6 &lt; 10 . Approximately 30% of distinct queries are popular per our definition. As mentioned earlier, some queries have almost uniform click pref-erence among all users, for example, [facebook] or [amazon] have high CTR on their associated sites. For these cases, individual, co-hort, and global preferences are consistent. Thus the cohort model has limited potential to improve retrieval performance for such que-ries. We measure the diversity of clicks among users for each query by computing the query entropy as follows: where ; , is from Equation (5). We focus on top five URL domains returned as search results or-dered by global CTR. As a result, the maximum entropy value is I 5  X  1.6 , and the minimum is zero. If clicks of all users led to the same destination, the value of the entropy will be zero, indicat-ing the query has the smallest variation in click behavior. A high value of entropy indicates the query has large variations. Click en-tropy has been used in many studies to evaluate the complexity of queries, e.g., [16]. However, by assuming that the same search re-sults are shown to all users who submitted the same query, its im-plementation in those studies only considers the number of clicks and ignores the impression counts. Our data comprises logs of a search engine equipped with personalization. Consequently, URL documents have unequal chance of being shown. Therefore we take advantage of CTR and consider both clicks and impressions. To examine how the performance of our cohort model relates to the level of query entropy, we separate queries into three subsets: low entropy , medium entropy and high entropy . The corresponding en-tropy ranges are [0, 0.2), [0.2, 1.2), and [1.2, 1.6). The motivation is that for queries with small entropy on global CTR, it is less likely that cohort click preference differs from global click preference. For queries with large entropy, global clicks are diverse, thus we expect that cohorts can differentiate clicks, and therefore offer bet-ter personalized search results. Many acronyms are ambiguous and associate with more than one meanings. For example the intent behind [msg] may differ depend-ing on the user location, e.g., users in New York City may be more likely to mean Madison Square Garden, whereas the likely intent elsewhere in the United States could be monosodium glutamate. As such, search engine performance can be improved on acronym que-ries via personalization that considers the location of the searcher Table 1. Data sets used in experiments. All dates from 2013. Data Cohort Profiling Training and Validation Testing 
Date range 03/31 X 05/28 05/29 X 06/02 06/03 X 06/04 #impressions 1,016,333,942 11,615,957 5,352,460 #distinct queries 248,419,356 4,096,337 2,192,327 #distinct domains 25,704,086 3,116,209 2,087,303 #users 23,378,476 1,144,715 739,281 as part of the ranking process [24]. To understand the effect of ac-ronym queries on the performance of our models, we used a set of acronyms defined in previous work [28]. From these data, we se-lected 432,564 acronyms which had a length of 2, 3 and 4 charac-ters. The average number of meanings per acronym was 2.91. We then intersected these with the two days of logs used for testing in our study, resulting in around 11,000 distinct query matches. We now describe our experimental results. As mentioned earlier, our baseline is the current production ranker in the commercial search engine .Our cohort model extends it by integrating cohort features. Comparing the models let us estimate changes in person-alization effectiveness attributable to the cohort modeling. Using the dataset described in the previous section, we train a LambdaMART-based ranking model [39] to re-ranking the top ten search results. LambdaMART is an extension of LambdaRank [8] which is based on boosted decision trees. It has been shown to be one of the best algorithms for learning to rank. Indeed, an ensemble model in which LambdaMART rankers were the key component won Track 1 of the 2010 Yahoo! Learning to Rank Challenge [9]. Our cohort features are insensitive to ranking algorithm, thus any reasonable learning-to-rank algorithm should also observe rele-vance gains as we do. We trained four ranking models using three types of predefined cohort features introduced earlier, specifically: 1. A model with ODP cohorts only (ODP); 2. A model with top-level domain cohorts only (TLD); 3. A model with location cohorts only (Location), and; 4. A model with all three cohorts, concatenated together (ALL). We also construct cohort models dynamically by clustering users based on contextual features. In Section 6, we describe the cohort clustering methods and experiment with varying the number of clusters (cohorts), denoted as in the remainder of this paper. As described earlier, we collected two months of search logs to con-struct user profiles, and the next one week of logs for training, val-idation, and test. Evaluating personalization at scale is challenging; since users can have different intentions for the same query, em-ploying third-party relevance labels may be insufficient. To address this concern, we exploit user clicks to obtain personalized relevance judgments for each query-document pair retrieved for a query. Clicks can be classified into various types by their associated dwell times on the landing page. If the dwell time is too short, the searcher may be dissatisfied with the search result. In this study, we label URLs with a SAT click (defined earlier) positively, and other URLs negatively. This method for generating click-based relevance judg-ments has been used in prior personalization studies [5][24][37]. We measure the quality of re-ranking using mean reciprocal rank (MRR) and mean average precision (MAP). In both cases, the mean is the average across all impressions, including those where the ranking does not change as a result of the treatment. MAP considers cases where there are multiple SAT clicks (better for informational queries); MRR is focusing only on the rank of the first SAT click. MAP is the mean of the average precision scores for each query, where is the number of URLs in the impression, ranging from 4 to 10. ! is an indicator function returning 1 if the URL at rank is relevant, otherwise 0. is the precision at cut-off in the ranked list. MRR targets the rank of the first relevant document in the result list. It is the average of the reciprocal ranks over all queries, where 7 is the rank position of the first URL document that received satisfied click for the query . Due to proprietary concerns, we do not report absolute metric val-ues. Instead, we report relative changes from the cohort model ver-sus the baseline: RM!! = 100  X  M!!  X   X  M!! 87 and RM = 100  X  M  X   X  M 87 . To understand the effect of cohorts in personalized search, we an-swer the following questions in the remainder of the paper: 1. Can our method enhance the baseline generally, for all queries? 2. Can we identify particular classes of queries that benefit from 3. Can we improve relevance further by learning cohorts (rather We now present the results of our analysis on all queries and on each of the query subsets described in the previous section. We now present the findings of our study, grouped by dataset. We begin with the first question: in general, can cohort models im-prove the retrieval performance when used in addition to existing personalization method(s)? This helps us understand the overall im-pact of the cohort modeling on search engine performance. Table 2 reports the MAP/MRR gains of our model versus the baseline (the production ranker) along with the standard error of the mean (SEM). The findings presented in the table show our cohort model significantly outperforms the baseline (with paired t-tests). Results that received SAT clicks by users are promoted by the cohort by the ranking (as can be seen with the low reranked@1 percentage in Table 2). All types of cohorts are informative. In particular, TLD yields the largest gain, perhaps because it captures differences in expertise of interests (e.g., people selecting en.wikipedia.org rather than a commercial domain). Location cohorts may have achieved the lowest gain because firstly, the baseline already covered the in-dividual location preference; secondly we use state to represent lo-cation and it could mask important intra-state movements. A finer grained representation of location may be required, but we also need to consider how best to do that in a scalable manner while ensuring that there are sufficient numbers of users in each cohort. Note that although the changes may appear small, they are averaged over all queries, including many whose performance is unchanged. A trend that we observe in Table 2 that is mirrored in all of our findings is that ALL performs as well or less well than the other models. This model re-ranks slightly more results (Rerank@1 in Table 2), meaning that its application is less focused. Also, the pres-ence of multiple cohorts may make the ALL cohort signal noisier. Table 2. Gains in MAP and MRR over baseline (  X   X  X   X  SEM). Cohort Rerank@1  X  MAP  X  SEM  X  MRR  X  SEM ODP 0.91% 0.0181 X 0.00130 0.0187 X 0.00142 TLD 0.96% 0.0224 X 0.00140 0.0229 X 0.00144 Location 0.90% 0.0111 X 0.00138 0.0113 X 0.00141 
ALL 0.98% 0.0193 X 0.00140 0.0211 X 0.00145 Given the promising gains observed across all queries, we now turn our attention to the various query subsets that were introduced ear-lier in the paper. In the remainder of this section we present results on each of those subsets defined in Section 4. Since the perfor-mance of both of the MRR and MAP metrics is similar, we focus on a single metric (MRR) for the remaining analysis. In our dataset, the average ratio of distinct new queries of all queries is about 70% per user, consistent with previous work [25]. It indi-cates that users submit a large portion of new queries that are not recorded by the search engine previously (at least not in the past two months, which may be all the engine has access to for a user at query time given profile size limitations at scale). Thus personali-zation based solely on an individual history is insufficient. Our cohort model utilizes search and click history of similar users to alleviate the challenge of insufficient data. We split the testing data into two subsets composed of old queries and new queries for each user respectively. Figure 1 shows the performance difference over the baseline for each type of cohorts. In this figure and others in this section, the value of zero denotes the original performance of the baseline. The figure shows that indeed our model works well on new queries that have not been observed previously (at least in the profiling period) from a given user. We observe statistically sig-nificant gains for new queries across all predefined cohorts (all p &lt; 0.001). When queries are repeated, the baseline with individual search history may work well, and adding cohort features had little or even slightly negative effect by introducing noise from other searchers X  activity (as is evidenced by the blue bars and negative MRR changes). It is also interesting to observe that the ODP (topic) cohort performed best for new queries. One possible explanation for this finding is that queries without an exact match that appear in the users X  history are most likely to be informational, and there-fore benefit most from users with similar topical interests. We are also interested in the effect of query popularity on the per-formance of the cohort modeling, in order to understand how sen-sitive our model to the size of cohorts. Figure 2 shows MRR gains on the popular and unpopular query sets, as described earlier. The performance gain on popular set is much larger than that in unpop-ular set. Again all differences are significant given the extent of the gains and large sample sizes ( p &lt; 0.001). The results match our ex-pectation. When a query is searched by many users, we can distin-guish cohort preference accurately. However, if a query is searched by only few people, the estimation is less accurate. We conjectured that since a large entropy implies diverse clicks on URLs, separating and assigning weights on clicks by cohorts can help identify an individual X  X  preference more accurately. Therefore we expect queries with large entropy will obtain large benefit from the cohort model. Figure 3 presents the MRR gain over the baseline for the three query entropy bins: low, medium, and high. The results shown in the figure confirm our intuition regarding where personalization enhancements might help. On all types of cohorts, the query set with low entropy received smallest gain over the baseline. Queries with medium and high entropy obtained larger performance increases (all statistically significant, p &lt; 0.001). This suggests that one strategy to realize strong gains from the cohorts may be to bypass low entropy queries and only apply cohort models on queries with medium or higher entropy. As observed in other analyses in this section, we also observe that the Location cohorts achieved the smallest gain, and even resulted in a loss for low en-tropy queries. As mentioned earlier, one explanation is the use of state-level cohort features, which may be too coarse to capture in-dividual click preferences. More work is required to determine how best to represent and apply location for cohort modeling. As mentioned earlier, acronym queries such as [acl], [atm], etc. are a specific set of ambiguous queries where personalization may help [24]. We examine the effectiveness of our cohort modeling meth-ods on the subset of acronym queries described earlier in Section 4. Table 3 shows the MAP and MRR gains over the baseline for this query set (all significant at p &lt; 0.001). The results clearly demon-strate extremely strong gains in performance for the subset of acro-nym queries for each of the cohort types studied. Although this may only be a relatively small query set (around 11k distinct queries), it is encouraging to see the significant gains in acronym queries. 
Figure 1. Gains in MRR over baseline for each cohort type 
Figure 2. Gains in MRR over baseline for each cohort type 
Figure 3. Gains in MRR over baseline for each cohort type 
MRR Gain
MRR Gain -0.10 -0.05 MRR Gain It is clear from the findings presented in the section so far that there are a broad range of different query classes for which the cohort modeling performs well. However, the performance of the ALL model was generally slightly lower than the other models. There may be a better way to combine the cohorts and in the next section we describe an approach to learn cohorts dynamically. The results in the previous section show that our cohort modeling techniques using pre-defined features can more accurately estimate users X  individual click preferences (as represented via an increased number of SAT clicks) than our competitive baseline method. A challenge of this approach is the tradeoff between the number of cohorts and the predictive power of cohorts on individuals. One can define more granular cohorts, for instance, including second or even lower levels of ODP, and changing locations from state to city or even the ZIP-code level. However, more cohorts result in fewer users in one cohort and less reliable CTR estimation. To overcome this challenge, we propose an alternative that generates cohorts au-tomatically via clustering. The objective is to construct homogene-ous clusters (cohorts) given a large number of features. In this section, we discuss how we learn cohorts automatically us-ing -means clustering. Each user is represented by a vector of con-textual features V 6  X  ! C , which is concatenated from the three sets of pre-defined cohort features on topic, location and top level do-main. The dimension of the feature vector is 99 in our setting. The objective of the method is to assign users into cohorts. Given large data volumes, a map-reduce implementation of -means algorithm is applied to cluster users into k clusters (cohorts). We then define two implementations of customized cohort membership vector. Definition 7 (Learned Membership, Hard): Given learned -co-horts, a particular user X  X  cohort membership vector is defined as a -tuple ' = [( ,1,( ,2,...,( ,] . Membership in the -th cohort depends on whether the user is assigned to the -th cluster. That is, ( , = 1 if the user is in the -th cluster, other-wise ( , = 0. Definition 8 (Learned Membership, Soft): Given learned k-co-horts, a particular user X  X  cohort membership vector is defined as a -tuple ' = [( ,1,( ,2,...,( ,]. The membership to -th cohort is determined by the minimum Euclidean distance be-tween the user and the centroid. Let centroids learned by -means be {\ 1 ,\ 2 ,...,\ ] } . Ideally the Gaussian Mixture Model could achieve the goal with additional computational overhead. In this large-scale study, we leverage the -means results and assign clus-ter membership as follows: where V 6 ,\ " is Euclidean distance between the user vector V and the centroid \ " , and 9 is estimated from the average dis-tance between centroids. This is a simplified implementation of the Gaussian Mixture Model having identity covariance. With the hard membership assignment, each user has only one non-zero cohort membership, which may be preferable on many clusters with large . For users with diverse preferences, it is natural to al-low multiple cluster membership. Therefore soft membership may produce higher performance gain since it is capable of better cap-turing within-user variance in interests and intentions. We compare the performance of re-ranking by clustered cohorts against the model with predefined ALL cohorts. We evaluate MRR change on a selected subset of queries. The subset of queries is new queries with entropy larger or equal to 0.2. This is set on which we observed a large performance gain with predefined cohort model, so it was a competitive dataset on which to assess the learned co-horts. We experiment learned cohorts using select probes of  X  {5, 10, 30, 50, 70 } , with soft assignment of cohort membership, which is expected to be more performant. Figure 4 displays the ex-perimental results at varying value of . Error bars denote standard error of the mean. Note that the baseline ranker (where MRR gain = 0) already contained global CTR as a feature, which is equivalent to =1. We therefore do not report performance at =1 in Figure 4. The figure shows that we can observe the largest MRR gain by the clustered cohorts model when =10. The MRR gains are slightly larger than the model with predefined cohorts in =30, 50, and slightly smaller in =5. The MRR gain decreases sharply when becomes too large, e.g., 70. We did not perform a full sweep of given resource constraints, but the findings are still informative and the gains over predefined at =10 are significant ( p &lt; 0.001). There may be a region 10 &lt; &lt; 30 where we may realize larger gains and we will explore that region in more detail in future work. We also evaluated the model on other subsets of queries and ob-served similar results: the largest gain is obtained in small and largest has smaller gain than the predefined cohorts model. One possible explanation for this is that the user features are sparse and that as increases, the reliability the cohort signal in each cluster degrades. The fact that we can obtain strong performance by reduc-ing the dimensionality of the features from 99 to 10 is promising for large-scale deployment (since it means compact user profiles). It also reveals the opportunity of profiling users with more subtle and sparse features than projecting to a few principal dimensions, as was done in the case of the pre-defined cohorts. As mentioned previously, we can employ either hard or soft clus-tering, depending on whether we want users to reside within a sin-gle cohort only (hard) or appear in multiple cohorts potentially with different weights (soft). The implications of this include the nature of the profile stored by a search engine. In the analysis above, we employed soft clustering. One concern we had regarding hard clus-tering is that it may lead to an inaccurate CTR estimation for users who are far from the cluster centroid. To better understand the im-pact of this decision, we compare the performance of models with cohorts by hard-clustering, soft-clustering and predefined features .

MRR Gain w.r.t. the baseline as in the other experiments presented in the paper thus far. Table 4 presents the findings of this analysis. Table 4 shows that, as expected, cohorts generated using hard mem-bership achieved the smallest performance gain, and are worse than those from predefined cohorts. Soft membership performs signifi-cantly better; other differences are not significant. This suggest that finding weights to assign to each cohort is important for estimating individual preference. Users also have variations inside a cohort, and their preferences cannot simply be generalized by one cohort. Given that we have these different ways to identify cohorts, we were interested in understanding the relationship between existing search engine results and global/cohort preference. To show that our performance improvement on personalization is not simply caused by gathering more features for the ranking algorithm, we conduct analysis on search logs and investigate whether cohorts manifest unique preference, which is directed by users in the co-hort. For a query with many candidate results, global CTR can offer a ranking of URL candidates. We refer to this here as global choice . In each identified cohort, cohort CTR can yield a ranking as well, and we refer to this as cohort choice . We focus on the difference between global choice and cohort choice of the top-ranked result. Definition 9 (DiffTop): DiffTop for query is an -tuple vec-tor = [gg , 1 , gg , 2 ,...,gg , 4 ] . Each value is a binary value to indicate whether a cohort has unique pref-erence. We denote the top ranked URL domain by the cohort choice of # -th cohort as h , we set gg-, " . = 1, otherwise 0. Among selected logs in profiling period, we choose queries with at least two distinct URL domains clicked, and count how many are inconsistent in the cohort choice and global choice. We find that 2% of distinct queries demonstrated unique preference by at least one cohort. The ratio appears small, but considering the query vol-ume is large, and the fact that we focus on clicks on domain level in the top position only, it is still a strong signal of cohort potential. There are cases that the values of cohort CTR for the top and the second top URL domains are very similar, e.g., equally small. This means that the top and the second top URL domains have similar cohort preference. To address such subtle scenarios, we defined a weighted DiffTop measure as follows. Definition 10 (Weighted DiffTop): The weighted DiffTop for query is a -tuple (_ = [(_gg , 1 , (_gg , 2 , ...,(_gg , 4 ]. Each value measures the degree of unique preference by the cohort. We define the decrease delta (  X  ) to meas-ure the difference in the click probability between the top and the second top URL domain as follows: where 1 is the url domain in top position, and 2 the one is the second position. The DiffTop is then weighted by  X  as follows: If  X  equals zero, the top candidate has less cohort dominance, thus DiffTop a weaker signal about unique preference for this cohort. To compare across cohorts, we average weighted DiffTop across queries for each cohort as -" . =  X  (_gg , " Q gg , "  X  . Such weight is then aggregated for a particular cohort type. Taking ODP cohorts for example, &amp; =  X  -" . " / , where is the number of cohorts of type ODP. If the average value is large, it implies that members of the cohort behave differently than non-members. We compare cohorts by ODP, TLD, Location features, also include clustered cohorts with =10. Figure 5 shows the aggregated weighted DiffTop value. At least two insights can be made. The first is that all cohorts have high average DiffTop weights in general. This shows that our selected features are useful in distinguishing cohort choice and global choice. The second is that ODP and Clustered cohorts are more in-formative than TLD and Location, perhaps because they are denser. We have proposed an approach for using cohorts of searchers sim-ilar along one or more dimensions to enhance Web search person-alization. To understand the value of these cohorts we performed an extensive set of experiments with predefined cohorts as well as cohorts dynamically learned from behavioral data, and for different query sets, including acronyms and queries previously unseen from a given user. These are scenarios where we would like to be able to employ personalization but often it does not succeed given insuffi-cient data about the interests of individual users. The results of our experiments have clearly demonstrated the value of cohorts, espe-cially for ambiguous and new queries from users, where our ob-served gains over a production ranker appear to be most significant. In our experiments, we used a competitive baseline a ranking algo-rithm that already had personalization signals based on a number of personal and contextual features for individual searchers. Despite such attention to representing the individual user X  X  interests, the co-hort-based models presented in this paper were still able to enhance the strong personalization baseline and achieve significant gains. This is promising as it suggests that we can learn how to integrate the cohort signals and make decisions about when to use them in combination with individual signals when both are present, or in isolation when only cohort signals are available. That said, further experiments are necessary with other personalization models to as-sess the generalizability of our findings to other settings. The pre-defined cohorts have the disadvantage that they require system designers to select important features manually in advance. Using unsupervised clustering we circumvented this problem and learned cohorts dynamically. We are pleased that using cluster-gen-erated cohorts that outperformed the pre-defined cohorts. However, the success of any clustering method is dependent on the features that are used. In this paper we used a set of features associated with topical preference, location, and top-level domain preference, but 
Table 4. Gains in MAP and MRR over baseline for different clustering methods (hard ( d =10) vs. soft) and vs. pre-defined.
Metric Hard Membership Soft Membership Predefined Cohorts  X  MAP 0.0731  X  0.0158 0.1143  X  0.0170 0.0932  X  0.0172  X  MRR 0.0737  X  0.0165 0.1173  X  0.0177 0.0905  X  0.0180 
Figure 5. Average DiffTop weight for each cohort (  X   X  X   X  SEM). 
Avg DiffTop weight there are other viable alternatives (e.g., demographics, social net-work cliques) and we need to explore their effectiveness in detail. We have shown that that best performance from cohorts learned via -means clustering is attained when we set =10. In a production search engine handling millions of users and billions of queries, the amount of space that can be devoted to each user is minimal. We have shown that for each user we would only have to store a small amount of additional information about their cohorts in each user X  X  profile, e.g., a single membership bit for each of the 10 cohorts. Overall, it is clear that there is significant potential value from mod-eling cohorts in search personalization. Unlike most existing work that learn from each of similar individuals, our approach focuses on learning from the whole group(s). Our modeling has two main com-ponents: cohort construction and cohort behavior modeling. One direction of future work is enhancing each of these components, for example, leveraging other sources of data beyond query-click logs (e.g., browsing signals, social network information) for cohort con-struction, and considering relationships between cohort members (e.g., group dynamics) for cohort behavior modeling. Another di-rection is investigating generalized cohort models (e.g., employing a Bayesian framework with a cohort prior -" . ), and other clus-tering algorithms (e.g., hierarchical clustering). 1. Agichtein, E., Brill, E., and Dumais, S. (2006). Improving 2. Almeida, R. and Almeida, V. (2004). A community-aware 3. Bennett, P.N., Radlinski, F., White, R.W., and Yilmaz, E. 4. Bennett, P., Svore, K., and Dumais, S. (2010). Classification-5. Bennett, P., White, R.W., Chu, W., Dumais, S., Bailey, P., 6. Berger, A.L. and Lafferty, J. (1999). Information retrieval as 7. Bilenko, M. and White, R.W. (2008). Mining the search trails 8. Burges, C.J.C., Ragno, R., and Le, Q.V. (2006). Learning to 9. Chapelle, O., Chang, Y., and Liu, T.-Y. (2010). The Yahoo! 10. Cheng, H. and Cant X -Paz, E. (2010). Personalized click pre-11. Freyne, J. and Smyth, B. (2006). Cooperating search commu-12. Fox, S., Kuldeep, K., Mydland, M., Dumais, S., and White, T. 13. Goldberg, D., Nichols, D., Oki, B.M., and Terry, D. (1992). 14. Ieong, S., Mishra, N., Sadikov, E., and Zhang, L. (2012). Do-15. Joachims, T. (2002). Optimizing search engines using click-16. Kautz, H., Selman, B., and Shah, M. (1997). Referral Web: 17. Lee, Y-J. (2005). VizSearch: A collaborative web searching 18. Mei, Q. and Church, K. (2008). Entropy of search logs: How 19. Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P., and 20. Schein, A.I., Popescul, A., Ungar, L.H., Pennock, D.M., and 21. Shen, S., Hu, B., Chen, W., and Yang, Q. (2012). Personalized 22. Shen, X., Tan, B., and Zhai, C.X. (2005). Implicit user model-23. Smyth, B. (2007). A community-based approach to personal-24. Sontag, D., Collins-Thompson, K., Bennett, P.N., White, 25. Sugiyama, K., Hatano, K., and Yoshikawa, M. (2004). Adap-26. Sun, J.-T., Zeng, H.-J., Liu, H., Lu, Y., and Chen, Z. (2005). 27. Tan, B., Shen, X., and Zhai, C. (2006). Mining long-term 28. Taneva, B., Cheng, T., Chakrabati, K., and He, Y. (2013). 29. Teevan, J., Adar, E., Jones, R., and Potts, M.A.S. (2007). In-30. Teevan, J., Morris, M.R., Bush, S. (2009). Discovering and 31. Teevan, J., Dumais, S.T., and Horvitz, E. (2005). Personaliz-32. Teevan, J., Liebling, D.J., and Geetha, G.R. (2011). Under-33. Weber, I. and Castillo, C. (2010). The demographics of Web 34. White, R.W., Bailey, P., and Chen, L. (2009). Predicting user 35. White, R.W., Bennett, P.N., and Dumais, S.T. (2010). Predict-36. White, R.W. and Buscher, G. (2012). Characterizing local in-37. White, R.W., Chu, W., Hassan, A., He, X., Song, Y., and 38. White, R.W., Dumais, S.T., and Teevan, J. (2009). Character-39. Wu, Q., Burges, C.J.C. Svore, K.M., and Gao, J. (2008). 40. Xiang, B., Jiang, D., Pei, J., Sun, X., Chen, E., Li, H. (2010). 
