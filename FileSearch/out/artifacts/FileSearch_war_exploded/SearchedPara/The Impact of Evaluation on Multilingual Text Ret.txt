 We summarize the impact of the first five years of activity of the Cross-Language Evaluation Forum (CLEF) on multilingual text retrieval system performance and show how the CLEF evaluation campaigns have contributed to advances in the state-of-the-art. H.3.3 [Information Stor age and Retrieval]: Search process; H.3.4 [Systems and Software]: Performance evaluation Measurement, Performance, Experimentation, Languages Multilingual Information Access, System Evaluation. The objective of the Cross-Language Evaluation Forum (CLEF) is to promote research into systems for all kinds of multilingual information access through the creation of reusable test-collections and the organization of annual evaluation campaigns. The aim is to encourage the development of user-friendly multilingual, multimedia systems. In the first years, the core track -also known as the ad-hoc track  X  has consisted of a number of monolingual, bilingual and multilingual document retrieval tasks and has produced the largest available testbed for the study of Cross-Language Text Retrieval. We summarize the features of this testbed and the scientific results of five years of comparative evaluation campaigns which have created this data set. The CLEF strategy has been to help groups to work their way up gradually from mono-to true multilingual text retrieval by providing them with facilities to test and compare search and access techniques over many la nguages, pushing them to investigate the issues involved in processing a growing number of languages with different characteristics. Over the years the language combinations have incr eased and the tasks offered have grown in complexity until, in CLEF 2003, the multilingual track included a task which entailed searching a collection in 8 languages, selected to cover a range of language typologies and linguistic features (see Table 1). The criteria normally adopted to create a test collection, consisting of suitable documents, sample queries and relevance assessments, have been adapted in CLEF to satisfy the particular requirements of the multilingual co ntext. All language dependent tasks such as topic creation and relevance judgments are performed in a distributed setting by native speakers. Table 2 gives some idea of the dimensions of the CLEF adhoc track testbed. Fuller and more precise details can be found in [1]. Great care has been taken to ensure that the test collection is reusable. Each year the reliability of the assessments pool is tested [2]. The statistical significance of differences has been found to be very similar to those of TREC and usually not influential. The results in terms of different approaches and techniques tested have been impressive. Groups submitting results over several years have shown improvements in performance and flexibility in advancing to more complex task s. Much work has been done on fine-tuning for individual language s, while other efforts have concentrated on developing language-independent strategies. As test collections and tasks vary over years it is not easy to document improvements in syst em performance. For bilingual retrieval evaluation, a common method is to compare results against monolingual baselines. We give the following indications: In 1997, at TREC-6, the best CLIR systems had the following results: -EN  X  FR: 49% of best monolingual French IR system -EN  X  DE: 64% of best monolingual German IR system In 2002, at CLEF, where there was no restriction on topic and target language the best systems gave: -EN  X  FR: 83,4% of best monolingual French IR system -EN  X  DE: 85,6% of best monolingual German IR system However, CLEF 2003 enforced the use of  X  X nusual X  language pairs, with the following impressive results: -IT  X  ES: 83% of best monolingual Spanish IR system -DE  X  IT: 87% of best monolingual Italian IR system -FR  X  NL: 82% of best monolingual Dutch IR system With respect to multilingual retrieval (L1  X  Ln), the number of systems participating in this task has grown steadily, peaking to a high of 14 groups in CLEF 2003. Similarly to TREC, over the years, we have seen a convergen ce of techniques and results with very little statistical difference between the best systems. The know-how collectively developed by CLEF participants include language-specific resources (ste mmers, stop-word lists, software to handle compounds in agglu tinative languages, web-mined parallel corpora), algorithms to merge results from different languages (e.g. the 2-step RSV originally introduced at CLEF [3]), or strategies to index agglutinative languages without linguistic knowledge (such as n-gram character indexing). The first lesson is that there is no magic recipe or single best algorithm to build an optimal MLIR system. The best system, as proved by CLEF evaluations, is always the result of careful tuning of every system component, and of combining different algorithms and information sources for every MLIR subtask [5]. These are a few issues for which CLEF evaluations have provided qualitative and quantitative evidence along the years  X  details can be found in the CLEF publications at [4]. Tuning of document and query indexing to individual languages is essential to achieve optim al performance. This includes language-specific stemmers, tf.idf weights and  X  most notably -strategies to handle decompoundi ng for agglutinative languages. Linguistic tools and resources help to reach top performance, as evidenced by the fact that the best monolingual systems tend to be developed by research groups with native knowledge of the language and specific linguistic tools. In the absence of linguistic knowledge, however, n-gram character indexing has proved an efficient and robust way of handling agglutinative languages [6]. The best strategy for query translation is to combine evidence from multiple sources, including bilingual dictionaries, MT systems and translation probabilitie s derived from parallel corpora (also automatically mined Web co rpora). Document translation can be successfully exploited together with query translation, but the performance gain does not seem to compensate for the extra computational effort. Interestingly, in absence of any translation resource (highly probable for certain language pairs), there are strategies to obtain reasonable baseline CLIR (such as unified character n-gram indexing for close language pairs). Remarkably, explicit resolution of translation ambiguity (which seemed the most crucial CL IR problem) is positive but not indispensable to obtain competitive results. Black-box MT systems, raw (unigram) transl ation probabilities, or Pirkola X  X  structured query approach (where alternative translations are treated as synonyms) might suffice, provided that all other system components are well tuned. Preserving ambiguity is, in any case, far better than filtering translation alternatives poorly. Besides indexing and translation, the multilingual retrieval problem requires merging results from different languages . Trivial merging strategies, such as raw-score merging or round-robin techniques, provide reasonable results on the CLEF testbed, due to the balanced amount of relevant information across languages. Strategies specifically tailored to the multilingual problem  X  such as 2-step RSV -provide, however, substantial benefits for the multilingual retrieval problem. The CLEF evaluation campaigns are, probably, the largest and most comprehensive research initia tive for CLIR; but they are far from being complete. One reason is that ad-hoc CLEF tasks evaluate CLIR systems as a whole; there is no direct comparison of alternative solutions for specifi c system components, such as translation strategies (given a fi xed set of translation resources), or resource acquisition techniques (given a fixed translation strategy). This problem has starte d to be addressed in CLEF 2005, with a task devoted specifically to evaluating merging strategies. Furthermore, in order to meet th e needs of applications, over the years, the focus of CLEF has shifted from ad-hoc document retrieval to address other CLIR problems such as interactive issues, multimedia retrieval, c ross-language Q&amp;A and now, in 2005, cross-language Web and Geographic Information retrieval. [1] Peters, C., Braschler, M., Di Nunzio, G., Ferro, N. CLEF 2004: Ad Hoc Track Overview and Results Analysis. LNCS 3491, (2005), forthcoming. [2] Braschler, M. CLEF 2003 Ove rview of Results, LNCS 3237, (2004), 44-63. [3] Mart X nez, F., Ure X a, L. and Mart X n, T. Experiments with merging strategies. LNCS 2785 (2003), 187-196. [4] See CLEF website: http://clef.isti.cnr.it/publications.html [5] Braschler, M., Peters, C. C ross-Language Evaluation Forum: Objectives, Results, Achievements , Information Retrieval, Vol.7 (1-2), (2004) 5-29. [6] McNamee, P. and Mayfield, J. Scalable Multilingual Information Access. LNCS 2785 (2003), 207-218. 
