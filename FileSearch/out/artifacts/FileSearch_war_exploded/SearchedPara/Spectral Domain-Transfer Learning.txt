 Traditional spectral classification has been proved to be ef-fective in dealing with both labeled and unlabeled data when these data are from the same domain. In many real world applications, however, we wish to make use of the labeled data from one domain (called in-domain ) to classify the un-labeled data in a different domain ( out-of-domain ). This problem often happens when obtaining labeled data in one domain is difficult while there are plenty of labeled data from a related but different domain. In general, this is a transfer learning problem where we wish to classify the unlabeled data through the labeled data even though these data are not from the same domain. In this paper, we formulate this domain-transfer learning problem under a novel spec-tral classification framework, where the objective function is introduced to seek consistency between the in-domain super-vision and the out-of-domain intrinsic structure. Through optimization of the cost function, the label information from the in-domain data is effectively transferred to help classify the unlabeled data from the out-of-domain. We conduct ex-tensive experiments to evaluate our method and show that our algorithm achieves significant improvements on classifi-cation performance over many state-of-the-art algorithms. I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation
Spectral learning methods such as normalized cut [28] are increasingly being applied to many learning tasks such as document clustering and image segmentation. Exploiting the information in the eigenvectors of a data similarity ma-trix to find the intrinsic structure, spectral methods have been extended from unsupervised learning to supervised/semi-supervised learning [22, 19], where a unified framework is used for spectral classification (SC). The SC algorithm has been shown to be effective when the data consist of both labeled and unlabeled data.

However, a limitation of these traditional SC methods is that they only focus on the scenario that the labeled and un-labeled data are drawn from the same domain, i.e., with the same bias or feature space. Unfortunately, many scenarios in the real world do not follow this requirement. In contrast to these methods, in this paper, we aim at extending tradi-tional spectral methods to tackle the classification problem when labeled and unlabeled data come from different do-mains. There are several reasons for why it is important to consider this domain-transfer learning problem, which is an instance of transfer learning [27, 30, 7]. First, the labeled information is often scarce in a target domain, while a lot of available labeled data may exist from a different but related domain. In this case, it would be desired to make maximal use of the labeled information, though their domains are dif-ferent. For example, suppose that our task is to categorize some text articles, where the labeled data are Web pages and the unlabeled data are Blog entries. This task is im-portant in practice, since there are much fewer labeled Blog articles than Web pages. These two kinds of articles may share many common terms, but the statistical observations of words may be quite different, as blog articles tend to use informal words. Second, the data distribution in many do-mains changes with time. Thus, classifiers trained during one time period may not be applicable to another time pe-riod again. Take spam email filtering for an example. The topics of spam/ham emails often evolve with time. There-fore the labeled data may fall into one set of topics whereas the unlabeled data other topics. Because traditional SC al-gorithms often fail to generalize across different domains, we must design new ways to deal with the cross-domain classi-fication problem.

This paper focuses on transferring spectral classification models across different domains. Formally speaking, the training data are from a domain D in and the test data are from another domain D out . D in is called in-domain and D out out-of-domain in order to highlight the crossing of the domains where the label set is the same. In addition, it is assumed that in-domain D in and out-of-domain D out are related to make the domain-transfer learning feasible. Our objective is to classify the test data from out-of-domain as accurately as possible using the training data from in-domain D in .

Although several cross-domain classification algorithms have been proposed, e.g., [10, 14], they are all based on local optimization. When the labeled and unlabeled data are not sufficiently large, their optimization function may have a lot of local minima and bring much difficulty for classification. In this paper, a spectral domain-transfer learning method is proposed, where we design a novel cost function from nor-malized cut, so that the in-domain supervision is regularized by out-of-domain structural constraints. By optimizing this cost function, two objectives are simultaneously being fol-lowed. On one hand, we seek an optimal partition of the data that respect the label information, where the labels are considered in the form of must-link constraints [31]; that is, the corresponding data points with respect to each con-straint must be with the same label. On the other hand, the test data are split as separately as possible in terms of the cut size within the test set, which will facilitate the classi-fication process. To sum up, the supervisory knowledge is used to ensure the correctness when searching for the opti-mal cut of all data points. At the same time, the data points in the test set are also separated with small cut sizes. To achieve this aim, a regularization form is introduced to com-bine both considerations, resulting in an effective transfer-ring of the labeled knowledge towards out-of-domain D out
We set out to test our proposed algorithm for domain-transfer learning empirically, where our algorithm is referred as the Cross-Domain Spectral Classifier (abbreviated by CDSC). In our experiments, we set up eleven domain-transfer prob-lems to evaluate our method. Compared against several state-of-the-art algorithms, our method achieves great im-provements on the competent methods.

The rest of this paper is organized as follows. Spectral methods are reviewed in Section 2. Section 3 dives into details of our method. In Section 4, our method is evalu-ated compared with other classifiers. Following related work discussed in Section 5, Section 6 concludes this paper with some future work discussion.
Spectral clustering is aimed at minimizing the inter-cluster similarity and maximizing the intra-cluster connection. Sev-eral criteria were proposed to quantify the objective func-tion, such as Ratio Cut [8], Normalized Cut (NCut) [28], Min-Max Cut (MCut) [15]. Using graph theory terminology, the data are modeled as vertices and the edges are valued using the similarity of the endpoints. We denote V as the universe of all examples and V = A  X  B where { A, B } is a partition of V . The goal is to find a partition that optimizes the cost function as follows: Here, assoc( A, V )= cut( A, B )= similarity between data points i and j . Take normalized cut as an example. The numerator cut( A, B )measureshow loosely the set A and B are connected, while the denomina-tor assoc( A, V ) measures how compact the entire data set is. [28] presents its equivalent objective in matrix represen-tation as where W is the similarity matrix, D = diag ( We )( e is a vector with all coordinates 1) and y is the indicator vector of the partition. Since solving the discrete-valued problem is NP-hard, y is relaxed to be continuous. Minimization of this cost function can be done via Rayleigh quotient [16]. Given a Laplacian ( L = D  X  W ) of a graph, the second smallest eigenvector y 1 meets the optimization constraint [9]. As to the discretization, linear order search [28] and other variant search methods (e.g. linkage differential order [15]) are commonly used to derive the cluster membership. Another approach was proposed in [25] which first normal-izes the eigenvectors and then applies the K -Means cluster-ing method.
For conciseness and clarity, in this paper we mainly focus on binary classification on textual data across different do-mains. Extensions can be easily done for more classes and other domains. Two document sets S in and S out are col-lected from domains D in and D out , respectively. We also denote S = S in  X  S out . In the binary classification set-ting, the label set is { +1 ,  X  1 } , meaning that c ( d i +1 (positive) or  X  1 (negative) where c ( d i )is d i  X  X  true class label. The objective is to find the hypothesis h which satis-fies h ( d i )= c ( d i ) for as many d i  X  S out as possible.
In our approach, the main idea is to regularize two objec-tives, namely, minimizing the cut size on all the data with the least inconsistency of the in-domain data, and at the same time maximizing the separation of the out-of-domain data. Intuitively, the regularization is regarded as the bal-ance between the in-domain supervision and the out-of-domain structure.
Let n = | S | be the size of the whole sample. A similarity matrix W n  X  n is calculated according to a certain similarity measure. Then, the supervisory information is incorporated in the form of must-link constraints by building a constraint matrix U , described in more details in the next subsection. In order to measure the quality of a partition, the cost func-tion for all the data is defined as where D = diag ( We ) is defined as previously mentioned and x is the indicator vector of the partition. In Equation (1), the normalized cut is adopted for the first term and a penalty term  X  || U T x || 2 is used to guarantee a good partition on the training data. The first term represents the associ-ation between two classes. The second term  X  || U T x || 2 constrain the partition of training data since any violation of constraints results in penalty regarding F 1 in Equation (1). The parameter  X  controls the enforcement of constraints. This cost function is similar to that proposed in [19]. In Equation (1), F 1 mainly focuses on the labeled data. However, we wish to classify the out-of-domain test data correctly. Thus, it is important to find the optimal partition for the test data as well. The cost function for the test data alone is defined as where D s = diag ( W s e ), and W s is the similarity matrix for test data only. Note that the dimension of W s is n , similarity entries only within test data are kept, i.e. if node i and j are both in the test data then W s ( ij ) = W ( ij ) ; other entries are set to zero.
Now a regularization parame ter is introduced, incorpo-rating Equation (1) and Equation (2) to get the unified cost function for cross-domain classification: where  X  is a tradeoff parameter for balancing the super-visory information (Equation (1)) and the cut size of the test data (Equation (2)). The first term F 1 ensures a good classification model should maximize the correctness of la-beled data. In the domain-transfer setting, we cannot com-pletely rely on the in-domain data. The second term F 2 can be understood as the domain-transfer fitting constraint, which means a good classification model should also keep the test data with adequately good separation. The trade-off between these competing conditions is captured by the parameter  X  , which interestingly, allows the classification model to be balanced between in-domain D in and out-of-domain D out . In Equation (3), when  X  =0,theoverall cost function degenerates into a spectral cost function over all the data in a semi-supervised manner; when  X  is large enough, the overall objective is biased towards optimizing only the spectral cost function for the test data without any supervisory knowledge.
In Equation (3), a penalty for violations [31] of the super-visory constraints is introduced. In the binary classification setting, assume there are n 1 positive data and n 2 negative data in the training set. The constraint matrix U is con-structed as follows: where each u i is an n -dimentional vector (same row index as W ) with two non-zero entries. Each column u k has an entry of +1 in the i th row,  X  1inthe j th row and the rest are all zero, which represents a pairwise constraint (data i and data j must be with the same label). Therefore U has m = n 1  X  ( n 1  X  1) / 2+ n 2  X  ( n 2  X  1) / 2 columns (constraints).
The detailed construction of the constraint matrix U is presented in Algorithm 1. It is easily seen with the indicator Algorithm 1 FormConstraintMatrix
Input : the size of positive data n 1 , the size of negative data n 2 and n = n 1 + n 2 ; here, without loss of generality, we assume the first n 1 examples are positive, and the next n 2 examples are negative.
 Output : Constraint Matrix U Let colN um =1.

Construct the matrix column by column. for i  X  1to n 1 do end for for i  X  n 1 +1 to n 1 + n 2 do end for return U vector x that when x satisfies all the constraints. Adding this constraint component into the Normalized Cut criterion [28], the cost function becomes Equation (1).
 One problem of the constraint matrix U is that the matrix U (with m rows) is greatly oversized, which makes it hard to compute U = UU T in Equation (3) ( || U T x || 2 = x UU x ). To alleviate this oversize problem, U can be directly built by considering the pairwise property of the constraints. Notice that U ij is the inner product of i th row and j th row of U . Then U ij has four cases: U where n 1 is the size of positive data and n 2 is the size of negative data.
In this section, the optimization of the overall function (Equation (3)) is addressed.

Since Equation (3) is difficult to optimize, we have to seek an approximation. In this work, we use x T ( D s  X  W s ) x of lead the normalized cut on S out . However, in F CDSC ,when F 1 is sufficiently optimized, the partition of in-domain train-ing data will be more or less balanced due to the constraint  X  ||
U T x || 2 , and thus the balancing functionality of the de-nominator x T Dx is reduced on only out-of-domain test data (refer to x T D s x ). Then, we have The similarity matrix is thus modified by amplifying the similarity inside the test data submatrix. In the interpre-tation through random walk [24], this modification can be seen as increasing the transition probability inside the test data.

Replacing y T = x T D 1 / 2 / || x T D 1 / 2 || , Similarly, With Equation (5), Combining Equations (7), (8) and (9), we obtain where T =( D  X  W )+  X UU T +  X  ( D s  X  W s ). Then, F CDSC can be minimized by solving an eigen-system: where d is the eigenvalue. Moreover, Equation (11) can also be rewritten into Similar to other spectral methods, y is relaxed to be a real-valued vector. To this end, our problem has been trans-called Rayleigh Quotient . In [16], we have
Lemma 1 (Rayleigh Quotient). Let A be a real sym-metric matrix. Under the constraint that x is orthogonal to the j  X  1 smallest eigenvectors x 1 ,...,x j  X  1 , the quotient
T x is minimized by the next smallest eigenvector x j and its minimum value is the corresponding eigenvalue d j . Furthermore, we can prove
Lemma 2. T = D  X  1 / 2 TD  X  1 / 2 is symmetric and its eigen-vectors are orthogonal.

Proof. Since D  X  W , D s  X  W s and UU T are all sym-metric, T =( D  X  W )+  X UU T +  X  ( D s  X  W s ) is therefore symmetric. With the diagonal matrix D  X  1 / 2 , T is also sym-metric.

Specifically, let v , w be arbitrarily two different eigenvec-tors of T and d v ,d w be corresponding eigenvalues which are thus different. Since d v = d w , v T w should be equal to 0. This implies that v and w are orthogonal.

By Lemma 1 and Lemma 2, the k smallest orthogonal eigenvectors of T = D  X  1 / 2 TD  X  1 / 2 are used after row nor-malization. Each data point is represented by the corre-sponding row.
 Algorithm 2 Cross-Domain Spectral Classification Input : training data ( n 1 positive instances, n 2 negative instances and n = n 1 + n 2 ) and test data, parameters { k } and a reasonable classifier F .
 Output : class predictions for test data 1: Construct the similarity matrix W n  X  n given both train-2: Let D = diag ( We ), D s = diag ( W s e )and 3: Find the k smallest eigenvectors x 1 , x 2 ,  X  X  X  , x k 4: Normalize X by row into Y where Y ij = 5: Call F with input of the eigenvectors to obtain the clas-
In Algorithm 2, we firstly prepare the data matrix and constraint matrix. Then the cost function is optimized by solving an eigen-system (Equation (11)). Finally, we use a traditional classifier for the final prediction, which is similar to the procedure in [22].Empirically, our algorithm improves several other state-of-the-art classifiers as will be shown in the experiment part (Section 4).

The major computational cost of the above algorithm is for computing the eigenvectors. The eigenvectors can be obtained by Lanczos method, whose computational cost is proportional to the number of nonzero elements of the target matrix. Thus the cost of our algorithm is O ( kN L nnz ( T )), where k denotes the number of eigenvectors desired, N L is the number of Lanczos iteration steps and nnz ( T )isthe number of non-zero entries in T .
Figure 1 plots the rec vs talk data (data details will be presented in Section 4.1) represented by the two smallast eigenvectors using our algorithm CDSC. The data points in the figure are sufficiently separated for classification since the eigenvectors contain the needed structural information. Moreover, the training and test data are similar in terms of Euclidean distance. In this way, the approximate decision boundary can be easily detected (the dashed line) and, as a result, good performance is obtained using our method. omit the composition details of last three data sets here. Figure 1: Projected data of rec vs talk in 2-dimensional eigen-space.
Our method is evaluated extensively on several data sets with the training and test data from different domains. As we will show later, our method outperforms several state-of-the-art classifiers in all the tasks.
The cross-domain data sets are generated in specific strate-gies using 20 Newsgroups 1 , Reuters-21578 2 and SRAA 3 .The basic idea of our design is utilizing the hierarchy of the data sets to distinguish domains. Specifically, the task is defined as top-category classification. Each top category is split into two disjoint parts with different sub-categories, one for training and the other for test. Because the training and test data are in different subcategories, they are across do-mains as a result. To reduce the computational burden, we sampled 500 training and 500 test examples for each task. http://people.csail.mit.edu/jrennie/20Newsgroups/ http://www.daviddlewis.com/resources/testcollections/ http://www.cs.umass.edu/  X mccallum/data/sraa.tar.gz
The 20 Newsgroups is a text collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups nearly evenly. Six different data sets are gen-erated for evaluating cross-domain classification algorithms. For each data set, two top categories 4 are chosen, one as positive and the other as negative. Then, the data are split based on sub-categories. Different sub-categories can be considered as different domains, while the task is defined as top category classification. The splitting strategy ensures the domains of labeled and unlabeled data related, since they are under the same top categories. Besides, the do-mains are also ensured to be different, since they are drawn from different sub-categories. Table 1 shows how the data sets are generated in our experiments.
Reuters-21578 is one of the most famous test collections for evaluation of automatic text categorization techniques. It contains 5 top categories. Among these categories, orgs , people and places are three big ones. For the category places , all the documents about the USA are removed to make the three categories nearly even, because more than a half of the documents in the corpus are in the USA sub-categories. Reuters-21578 corpus also has hierarchical structure. We generated three data sets orgs vs people , orgs vs places and people vs places for cross-domain classification in a similar way as what have been done on the 20 Newsgroups. Since there are too many sub-categories, the detailed description cannot be listed here.
SRAA is a Simulated/Real/Aviation/Auto UseNet data set for document classification. 73,218 UseNet articles are collected from four discussion groups about simulated au-tos ( sim-auto ), simulated aviation ( sim-aviation ), real au-tos ( real-auto ) and real aviation ( real-aviation ). Consider the task that aims to predict labels of instances between real and simulated . The documents in real-auto and sim-
Three top categories, misc , soc and alt are removed, be-cause they are too small. means training on in-domain D in and testing on out-of-domain cross-validation on out-of-domain D out and in-domain D . Note that, the experimental results given by CoCC from each original data set. auto are used as in-domain data, while real-aviation and sim-aviation as out-of-domain data. Then, the data set real vs sim is generated as shown in Table 1. Therefore all the data in the in-domain data set are about auto , while all the data in the out-of-domain set are about aviation .The auto vs aviation data set is generated in the similar way as shown in Table 1.
To verify our data design, the error rates are recorded using the SVM classifier in the scenario of domain-transfer learning ( D in  X  D out ) as well as the single-domain classifi-cation case within the out-of-domain and within the in-domain, respectively. Under the column  X  X VM X  in Table 2, the three groups of classification results are displayed in the sub-columns. The column  X  D in  X  D out  X  X eansthat the classifier is trained on in-domain data and tested on out-of-domain data. The next two columns  X  D out  X  X V X  and  X 
D in  X  X V X  show the best results by the SVM classifier ob-tained during 10-fold cross validation. In these two exper-iments, the training and test data are extracted from the same domain, out-of-domain D out and in-domain D in re-spectively. Note that the error rates under the D in  X  D column is much worse than the ones under D out  X  X V and D in  X  X V. This implies that our data sets are not applicable for traditional classification.
To verify the effectiveness of our classifier, the supervised learner SVM is set as the baseline method. Our method is also compared to several semi-supervised classifiers, includ-ing Transductive SVM (TSVM) [20], Spectral Graph Trans-ducer (SGT) [21] and Spectral Classifier (SC) [22]. Note that [22] is approximately a special case CDSC with  X  =0. We also compare to the co-clustering based classification (CoCC) [10] as the state-of-the-art domain-transfer learning algorithm and one representative selection bias correction (KDE) [29]. CoCC builds connection between in-domain and out-of-domain through feature clustering, and is for-mulated under the co-clustering framework. KDE corrects the domain bias in the in-domain, and then adapts the in-domain classification model to out-of-domain. We use test error rate as the evaluation measure.
On the textual data designed in Section 4.1, we have con-ducted preprocessing procedures including tokenizing text into bag-of-words, converting text into low-case words, stop-word removal and stemming using the Porter stemmer [26]. Each document d i in S is represented by a feature vector using Vector Space Model . Each feature represents a term, whichisweightedbyits tf-idf value. Feature selection is car-ried out by thresholding Document Frequency [34]. In our experiments, Document Frequency threshold is set to 3, and the final result is not sensitive to it. The cosine similarity matrix.

The comparison methods are implemented by SVM light 5 and SGT light 6 . All parameters are set default by the soft-ware. The Spectral Classifier (SC) is implemented according to [22]. CoCC uses the same initialization and parameters in [10]. KDE is implemented according to [29, 35].
By comparing with the traditional supervised classifier, it is observed that the cross-domain data present much diffi-culty in classification, where SVM (training on in-domain D in and testing on out-of-domain D out ) made more than 20% average prediction errors. In Table 2, we observe that the TSVM and SGT always outperformed the supervised classifier SVM. The semi-supervised classifiers worked better since they used the unlabeled data in the classification pro-cess, so that they captured more information in the out-of-domain. However, semi-supervised learning still works un-der the identical-domain assumption, and thus its improve-ment is limited. The situations are similar in SC. CoCC improves a lot over the traditional classification algorithm, since CoCC is a cross-domain classification algorithm, and it effectively transfers knowledge across different domains. KDE shows few improvement against SVM in our experi-ments, although it can effectively correct selection bias be-tween two different domains. In our opinion, KDE fails to improve much in domain-transfer learning because the do-main difference may be affected by the selection bias very few. In general, our algorithm CDSC is a spectral domain-transfer learning method, and achieves the best performance against all the comparison methods. Compared to the state-
Software available at http://svmlight.joachims.org .
Software available at http://sgt.joachims.org . Figure 2: The average error rate curve of  X  when fixing  X  at 15 . Figure 3: The average error rate curve of  X  when fixing  X  at 0 . 025 . of-the-art domain-transfer learning algorithm CoCC, CDSC also shows superiority in this experiments. We believe, it is because the data size in our experiment is not so large, and spectral learning is much more superior in learning with small data than many other learning methods.

However, in some data sets the performance is not sat-isfactory. For example, this can be observed in orgs vs places . This can be attributed to less common knowledge between in-domain and out-of-domain data. Our method requires that the in-domain and out-of-domain should be related, namely that they share some knowledge. If this con-dition cannot be satisfied, the quality of transferred knowl-edge will not be guaranteed. As to the tasks derived from the 20 Newsgroups, the in-domain and out-of-domain data may share a large amount of common knowledge which leads to better performance, despite the fact that other methods failed in most cases. In general, our algorithm can alleviate the classification difficulty better when the in-domain and out-of-domain are not the same albeit related.
There are two parameters in our method:  X  adjusts the enforcement of supervisory constraints;  X  represents the trade-off of transferring knowledge into the target domain. We tested 5 different values of  X  when  X  is fixed.  X  is enumer-ated from 0 . 0125 to 0 . 2 with 5 log-scale values with fixing  X  . We use the average error rate through 11 tasks for evalua-tion. From Figure 2, it can be seen that, empirically the best  X  is between [0 . 0125 , 0 . 05], and we set  X  =0 . 025 in our ex-periments. From Figure 3, the performance of CDSC is not very sensitive to  X  ,andweset  X  =15intheexperiments.
The eigenvectors obtained in the classification process rep-resent the original information approximately in a different Figure 4: The error rates against the number of eigenvectors. Figure 5: The error rate curve on the data set comp vs sci against different sizes of training examples. feature space. In this work, the optimal number is found by enumerating the number of used eigenvectors empiri-cally. Figure 4 illustrates the error rates of several data sets against different numbers of eigenvectors used for clas-sification. From the figure, it can be seen that, generally, the classification on 6 eigenvectors shows the best performance.
We have also investigated the influence by the size of train-ing examples. Take comp vs sci data set for example (Figure 5). We chose a portion of examples in the training data ran-domly ranging from 100 examples to all of the samples (500). We observe that SVM, TSVM and SC often performed, in general, increasingly worse when the number of training ex-amples decreases. In contrast to these baselines, the error rate curve of our algorithm is generally stable. This indi-cates our algorithm CDSC can better deal with the data sparsity problem. More importantly, CDSC tops the perfor-mance over almost all trials.
Spectral methods promise to draw the similar data points nearer by representing the original data in the eigen-space. But how does this projection work on cross-domain data? To answer this question, we illustrate the similarity pattern of the original data, the projected data in Spectral Classi-fier (SC) [22] and the projected data in our method (CDSC). Take the data set rec vs talk for example. The data are in-dexed firstly by category and secondly by training and test, namely positive training, positive test, negative training and negative test in order. Figure 6(a) displays the document-document similarity matrix of the original data valued by the cosine measure, which has a threshold by the mean of this matrix. The latter two patterns are similarly thresh-olded. In Figure 6(b), it is shown that SC fails to draw the In this paper, a novel spectral classification based method CDSC is presented where an objective function is proposed for domain-transfer learning. In the domain-transfer set-ting, the labeled data from the in-domains are available for training and the unlabeled data from out-of-domains are to be classified. Based on the normalized cut cost function, supervisory knowledge is transferred through a constraint matrix, and the regularized objective function (see Equation (10)) finds the consistency between the in-domain supervi-sion and the out-of-domain intrinsic structure. The original data are then represented by a set of eigenvectors, to which a linear classifier is applied to get the final predictions. Sev-eral domain-transfer learning tasks are used to evaluate our learning method, where experimental results justify that our method is effective on handling this cross-domain classifica-tion problem.

There are several directions for future work. The CDSC is given in batch style in this paper. In the future, we would like to extend CDSC to an online cross-domain classifier. It is also important to investigate when negative transfer (do-mains are sufficiently dissimilar) would happen in domain-transfer learning. Qiang Yang would like to thank the support of Hong Kong RGC Grant 621307. Gui-Rong Xue would like to thank Mi-crosoft Research Asia for their support to the MSRA-SJTU joint lab project  X  X ransfer Learning and its application on the Web X . We also thank the anonymous reviewers for their valuable comments. [1] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira. [2] S. Ben-David and R. Schuller. Exploiting task [3] S. Bickel, M. Br  X  uckner, and T. Scheffer.
 [4] S. Bickel and T. Scheffer. Dirichlet-enhanced spam [5] J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and [6] J. Blitzer, R. McDonald, and F. Pereira. Domain [7] R. Caruana. Multitask Learning. Machine Learning , [8] C.-K.ChengandY.-C.A.Wei.Animprovedtwo-way [9] F.R.K.Chung. Spectral Graph Theory .American [10] W. Dai, G.-R. Xue, Q. Yang, and Y. Yu. Co-clustering [11] W. Dai, G.-R. Xue, Q. Yang, and Y. Yu. Transferring [12] W. Dai, Q. Yang, G.-R. Xue, and Y. Yu. Boosting for [13] H. Daum  X  e III. Frustratingly easy domain adaptation. [14] H. Daum  X  e III and D. Marcu. Domain adaptation for [15] C. Ding, X. He, H. Zha, M. Gu, and H. Simon. [16] G. H. Golub and C. F. Van Loan. Matrix [17] J. J. Heckman. Sample selection bias as a specification [18] J. Huang, A. J. Smola, A. Gretton, K. Borgwardt, and [19] X. Ji and W. Xu. Document clustering with prior [20] T. Joachims. Transductive inference for text [21] T. Joachims. Transductive learning via spectral graph [22] S. D. Kamvar, D. Klein, and C. D. Manning. Spectral [23] X. Liao, Y. Xue, and L. Carin. Logistic regression [24] M. Meila and J. Shi. A random walks view of spectral [25] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral [26] M. Porter. An algorithm for suffix stripping program. [27] J. Schmidhuber. On learning how to learn learning [28] J. Shi and J. Malik. Normalized cuts and image [29] H. Shimodaira. Improving predictive inference under [30] S. Thrun and T. Mitchell. Learning one more thing. In [31] K. Wagstaff and C. Cardie. Clustering with [32] P.WuandT.G.Dietterich.ImprovingSVMaccuracy [33] D. Xing, W. Dai, G.-R. Xue, and Y. Yu. Bridged [34] Y. Yang and J. O. Pedersen. A comparative study on [35] B. Zadrozny. Learning and evaluating classifiers under
