 VISHVAS VASUKI, NAGARAJAN NATARAJAN, ZHENGDONG LU, BERKANT SAVAS, and INDERJIT DHILLON , University of Texas at Austin There has been an explosion in the number of online social networks and their active members. The wealth of information in the social networks has driven prolific work on the analysis of the networks, understanding the processes that explain their evolution, modeling the spread of behavior through them, predicting their future states and so on.
The problem. Users of a social network tend to affiliate with communities. Consider the online social networks like Facebook, Orkut, and LiveJournal, where users become members of communities, and affiliation networks are explicitly established. In some social networks, the communities are identified more by the preferences of the mem-bers of the social network than by direct declaration, e.g. the genre of movies that a set of Netflix customers tend to patronize more often. Thus, two networks exist simul-taneously: the friendship network among users, and the affiliation network between users and communities. One of the interesting challenges in social network analysis is the affiliation recommendatio n problem, where the task is to recommend communities to users. The fact that the social and affiliation networks coevolve [Backstrom et al. 2006; Zheleva et al. 2009] suggests that a better solution to the affiliation recommen-dation problem can be obtained if the friendship network is considered along with the affiliation network. This problem setting has applications beyond community recom-mendation. For example, affiliations can be interpreted in general as a user X  X  taste for an item. More interestingly, in biology, the friendship network can correspond to a network among genes, whereas the affiliation network can correspond to a network between genes and traits/diseases, and the affiliation recommendation problem can be viewed as one of identifying genes affecting the expression of a disease. In this article, we consider the affiliation recommendation problem, and in particular, how friendship networks can be used to make affiliation recommendations to the users of a social network.

Contributions. We consider how one can model the interplay between users and com-munities in social and affiliation networks simultaneously. An ideal unifying model would not only explain the current state of the networks, but also help in predicting future relationships among the nodes. Our contributions are threefold. (1) Using a simple way of combining social and affiliation networks, we suggest and (2) Each of the two models suggests methods for affiliation recommendation, albeit (3) We propose a way of evaluating affiliation recommendations, by measuring how
The use of link prediction techniques for the purpose of affiliation recommendation is, to our knowledge, novel. We show that information from the friendship network improves the quality of affiliation recommendation, beyond just using the information from the affiliation network. We also observe that the benefit we derive from the social network in affiliation recommendation is strongly contingent on how the problem is modeled and what algorithms are used.

We would like to note that the two models described in Section 2 have already been introduced in our earlier paper [Vasuki et al. 2010]. Initially, we did not consider scalability aspects of the proposed models. In this article, we explicitly address scal-ability, and present several scalable methods that are computationally more efficient and have performance competitive with the previously published methods. The meth-ods proposed in this article are scalable to very large real-world networks. We per-formed an extensive set of experiments to compare the recommendation performance of the scalable methods against the nonscalable methods. Running times of the algo-rithms are also reported in this article.
 Overview. We now provide a brief overview of the organization of the article. In Section 2, we consider a network formed by merging the friendship and affiliation networks and introduce two models of the behavior of nodes in this network: the graph proximity model and the latent factors model , and explore recommendation al-gorithms that arise from these models. Section 3 introduces two scalable models for large scale group recommendation. In Section 4 we describe our evaluation strategy and present the experimental results of our evaluation of the proposed methods for affiliation recommendation u sing real world networks. We consider how the proposed models and algorithms relate to prior work in Section 5. Finally, in Section 6, we con-clude with a summary of our findings and give a brief discussion on future lines of research. In this section, we first establish the notation used, and then pose the affiliation rec-ommendation problem as a ranking problem. Then we describe a natural way of com-bining the friendship and affiliation networks into a single graph. In the sections that follow, we describe affiliation recommendation approaches based on the graph proxim-ity model and those based on the latent factors model .
 Notation. We use N u to denote the total number of users in the affiliation network and N g to denote the total number of communities in the affiliation network. A denotes the user  X  group adjacency matrix of the affiliation network A . S  X  R N u  X  N u denotes the user  X  user adjacency matrix of the friendship network S . We will use A i , j and [  X  ] i , j to denote the i , j entry of the matrix A or the corresponding matrix expression within the brackets. Other notation will be introduced as needed.

Ranking problem. We are considering the task of recommending affiliations to a given user. This problem can be posed as a problem of ranking various affiliations in the order of the user X  X  interest in joining them. The methods we describe here to solve this problem rely on assigning scores to various affiliations in order to rank them. The task of an affiliation recommendation algorithm can be viewed as one of generating an N Consider the adjacency matrices A and S . For now, we assume S to be symmetric (or equivalently S to be undirected), although a nonsymmetric extension to our model can be easily obtained. Clearly, S corresponds to an undirected graph among users and corresponds to an undirected bipartite graph between users and groups. Despite the heterogeneity of the two types of links, it is rather natural to consider a graph C between all the users and groups, with the combined adjacency matrix where the heterogeneity of two types of links is reduced to a single parameter  X   X  0, which controls the ratio of the weight of friendship to the weight of group membership. Clearly when  X  = 0, user-user friendship ceases to play a role in this joint graph, and it simply degenerates to the bipartite affiliation graph.

As in the case of prediction with a single graph, the prediction on C can be carried out from the following two perspectives. (1) Graph Proximity Model. We assume that the entire graph is known, and that the (2) Latent Factors Model. We model the graph as a matrix, some of whose zero entries We will elaborate on the two perspectives in the following two sections. As described earlier, the affiliation network can be modelled by a graph. The graph proximity model assumes that the probability of there arising a link between two nodes in a graph is based on an estimated proximity between the two nodes. The proximity of two vertices can be calculated as the weighted sum of the number of paths connecting the two, with varying lengths. This is the underlying mechanism of many link predic-tion models in the context of social network analysis. Consider the widely-used Katz measure [Katz 1953; Liben-Nowell and Kleinberg 2003] on the friendship network S . The proximity is given by where the weights of paths decay exponentially with the length and  X  is a parameter to ensure convergence of the series.

A simple extension of Katz to the bipartite graph A is where, in the cooccurrence matrix AA T ,twousers i and j , are considered connected if i and j are members in the same group, i.e. [ AA T ] i , j &gt; 0. In this case, we consider the paths of the following types. The intuition in considering AA T A is if user i shares some community with user j ,itis likely that i will join some other community j belongs to. The higher order terms can be interpreted in a similar way.
 Consider the following Katz measure proximity matrix on the combined graph C The user-community block 1 of Katz ( C ;  X  ) is denoted Katz ( C ;  X  ) 12 and given by Equation (3) is another generalization of the Katz measure on the bipartite graph A by also considering paths involving the frienship graph S ,e.g. and Clearly we can use Katz ( C ;  X  ) 12 as a score matrix.

Computing the Katz measure is expensive as it either involves a summation of a large number (infinite) of terms with matrix powers or the inverse of a potentially large matrix, 2 where the resulting inverse is dense. We consider a truncated Katz measure, e.g. with the combined adjacency matrix C ,wehave where the truncation parameter k is usually quite small. Also for the truncated Katz measure we will be interested in only the user-community block, which we will denote tKatz ( C , X , k ) 12 ;e.g.with k =3wehave Similarly, the truncated version of the extKatz measure in Equation (2) using k =3is given by
For computing the tKatz measures for the graphs C or A , a conservative estimate of the computational cost is O ( N u  X  nnz ), where nnz is the number of nonzeros in ( AA T ) k . We will now describe the latent factors model. We will present the optimization prob-lem it solves, and examine some of its properties. In this model, the zeros in the adja-cency matrix of the affiliation network A may be viewed as being unobserved entries with a huge prior belief in favor of them being actually zero. Every user i and commu-nity j are assumed to have low dimensional representations u i and g j . The affinity of auser i to a community j is assumed to correspond to u T i g j . In other words, users and communities with a high inner product are assumed to be connected to each other, i.e. A , j  X  u T i g j . In matrix form, we express this as factors. Note that rank( U )  X  d , rank( G )  X  d , and d N u , N g .

To get factors that account for both the observed entries in A as well as the interac-tions in S , we consider the following combined network.
 where D is a derived similarity between groups, which is not observed. Note that C = C (  X , 0 ). Let C  X  V V T be the rank-d dominant spectral approximation, where the d columns of V are eigenvectors and is a d  X  d diagonal matrix with the largest magnitude eigenvalues. Partitioning the spectral approximation according to the block structure of C we obtain Clearly, we have A  X  V 1 V T 2 and we can set U T = V 1 and G = V T 2 in the model of Equation (7). The user factors in U and group factors in G , contain information from the friendship network S as the computation of V 1 , V 2 ,and are influenced by the presense of S in C . We also note that the user and group factors are not unique. It is straightforward to show that the factors V 1 , ,and V 2 obtained by the spectral approximation of C solve the following minimization problem. with the constraints rank([ V T 1 V T 2 ]) = rank( )  X  d . We will interpret the low rank approximation of A as a score matrix.

Role of  X  . Intuitively, in Equation (9),  X  controls the contribution of S in deciding the user factors. With larger  X  , the learned factor model describes the friendship network S better, and correspondingly the affiliation network is described less well.
Choice of D. The derived similarity between the communities D in the combined matrix C can be approximated using proximity between communities in the graph corresponding to C . It follows that a potential choice is D = A T A , which is simply the number of users that any two communities share. One may also consider weighing the contribution of D with  X  , which is the weight factor learned for S . Wewillseelater that the experiments suggest that the choice of D is not very important, and that the information from D , when derived from A , is redundant.
 We have proposed two different approaches to group recommendation in the previous section. In this section we will consider the computational feasibility of our approaches when dealing with massive real-world datasets. The latent factors model (Section 2.3) is in fact quite scalable due to the existence of highly efficient algorithms, e.g. ARPACK [Lehoucq et al. 1998] and PROPACK [Larsen 1998], for computing the domi-nant eigenvectors and corresponding eigenvalues. Unfortunately, the graph proximity model (Section 2.2) is computationally formidable for large scale friendship networks S and affiliation networks A . This difficulty lies in the computation of the power of A and S in tKatz ( C ;  X , k ) 12 . For example when k =3, where both AA T and S 2 could get fairly dense and hence the matrix multiplications S A and AA T A become expensive and the results even denser.

Combining the tKatz measure with the latent factor model, we get the score matrix given by where C  X  V V T , V T =[ V T 1 V T 2 ], and the partitioning is as in Equation (1). The cost of matrix multiplication can be reduced if we first take low rank approxima-tions of both S and A .Thatis where S is approximated using the spectral factorization and A is approximated via the truncated singular value decomposition (SVD) [Golub and Van Loan 1996]. However, this simple approximation does not work well, since with very large A and S ,andsmall rank d in the approximations, U A and U S may be almost orthogonal to each other, i.e. entries of U T S U A are very small. Therefore the approximation may have almost all of its entries close to zero. One way to remedy this problem is to extend these low rank approximations so that we have a common subspace that captures the dominant parts of both S and A . We found that the following heuristic works fairly well. We first form [ U S U A ] and compute its QR factorization [Golub and Van Loan 1996], i.e. QR =[ U S U A ]. Then the optimal (in a least squares sense) rank-2 d approximations of S and A in terms of Q are given by where V is an orthogonal matrix that spans the column space of A T Q ,e.g.the Q -part in the QR factorization of A T Q .Wealsointroducedthe2 d  X  2 d matrices D S = Q T SQ and D
A = Q T AV . Using the low rank approximations in Equation (13) the user-community block of the tKatz measure becomes The intuition behind this heuristic is quite simple X  X e force the low rank approxima-tions of A and S in Equation (13) to take the same subspace for users. This constraint certainly lowers the quality of individual approximation task, but better exploits the interaction between S and A when predicting future affiliations.
 An alternative approach related to the latent factor model is to compute a different (than the spectral) low rank approximation of the combined adjacency matrix C .Itis observed that many real-world social netw orks naturally form a number of clusters, where the links within the clusters are much denser than the ones between clusters. We will refer to graphs with clear cluster structure as clusterable .Inthe clustered low rank approximation , [Savas and Dhillon 2011; Song et al. 2010] the cluster structure of the network is utilized in the low rank approximation. We will illustrate the procedure through a 2-cluster example. Assume that the nodes V = { 1 , 2 ,..., | V |} are clustered in two disjoint clusters, V 1  X  V 2 = V and V 1  X  V 2 =  X  . Then by permuting rows and columns of C according to the cluster memberships of the nodes we obtain where P is a permutation matrix that reorders rows of C so that the first | V 1 | rows correspond to nodes from the first cluster, and the remaining | V 2 | rows correspond to nodes from the second cluster. The links between nodes from cluster one will then form the nonzeros of  X  C 11 and the links between nodes from cluster two form the nonzeros of  X  C 22 . Then the nonzeros in Assuming the graph is clusterable, the links/nonzeros of  X  C will be concentrated in the diagonal blocks  X  C 11 and  X  C 22 . The amount of links/nonzeros in the off-diagonal blocks depends on how well the graph clusters, and usually this part is only a small fraction of all links. In a particular example, with five clusters on the YouTube-lcc data set, only about 10% of the links are between vertices from different clusters. After a clustering of the nodes is obtained, using highly efficient multilevel algorithms like Dhillon et al. [2007] and Karypis and Kumar [1998], a low rank approximation is computed for each diagonal block matrix of  X  C . The clusterwise low rank approximations are computed independently and may be obtained as the dominant spectral approximation. For the two-cluster case we have The two cluster-wise approximations are then used to obtain an approximation for the entire  X  C , imation of each  X  C ii is d , we see that Equation (15) yields a rank-2 d approximation of  X  C . An important observation is that the memory usage for the rank-2 d clustered low rank approximation is almost the same as the memory usage for a regular rank-d ap-proximation. Recall that  X  V i are long-skinny, i.e. d N u , thus most of the memory in the low rank approximation is used by the eigenvectors. Although the cluster-wise low rank approximations do not involve off-diagonal blocks, e.g.  X  C 12 , information in these blocks in the approximation of  X  C is included due to the inclusion of  X  D 12 . Experiments show that with the same memory usage, clustered low rank approximation is often much more accurate than the regular low rank approximations. In addition, the total computation time 3 for the clustered low rank approximations is usually lower than the time for computing the truncated SVD for the entire matrix. Interested readers are referred to Savas and Dhillon [2011] and Song et al. [2010].

Combining the clustered low rank approximation with the tKatz measure, we get thefollowingscorematrix: where  X  C  X   X  V  X  D  X  V T is the clustered low rank approximation similar to Equation (15), but with c clusters. Recall that [  X  ] 12 denotes the 1,2 block corresponding to A ,asin Equation (1).

In summary, the clustered low rank approx imation cleverly spends most of its re-sources (parameters) on modeling the links within clusters, and is therefore able to capture more information than regular SVD using the same number of parameters. The model in (15) may then be used both in the graph proximity model to compute various Katz measures and in the latent factors model. We first introduce the datasets on which we conduct our experiments and describe the experimental setup. We then describe our methodology for comparing the performance of various algorithms for the affiliation recommendation task. Finally, we present a series of experiments using methods presented in previous sections, and discuss the results. We use two popular online social networks for our experiments. These are Orkut and Youtube , and both are operated by Google. The users of both social networks explicitly identify themselves as belonging to some communities or groups . Thus, for each of the networks we have an adjacency matrix A , that identifies the memberships of the users in the groups, and an adjacency matrix S , that identifies friendships among users. We use the data gathered by Mislove et al. [2007]. They obtained partial snapshots of Orkut and YouTube networks by crawling the respective Web sites in a specific time period (Refer to Mislove et al. [2007] for specifics). For our experiments, we obtained subgraphs from each of these snapshots, by depth-first search starting at a random node in the social network S and terminating when the number of users in the com-ponent exceeds a threshold. Then, among the affiliations pertaining to the users in the extracted subgraph, we retained only those with at least one user. The networks thus obtained are referred to as Orkut and YouTube datasets in the rest of the sec-tion. In a few of the experiments, we used the largest connected component of the combined network C in Equation (1) formed using each of the Orkut and YouTube net-works, as path-based measures like Katz are decoupled and independent for different components. These components are denoted as Orkut-lcc and YouTube-lcc.

Some statistics for these networks are presented in Table I. Note that in the Orkut-lcc dataset we only retain groups with at least two users. Further, when extracting the largest component from the Orkut dataset no users were removed, as every user is connected to some other user (the social network S has only one component). But since we removed groups with less than two users, there are users that do not belong to any of the remaining groups, and therefore the minimum number of groups per user in the Orkut-lcc data set is zero.
 Though the results presented in this section may not be extrapolated to the entire Orkut and YouTube networks, we have used standard sampling techniques that are known to produce subgraphs that have consistent social network properties [Mislove et al. 2007] to obtain experimental datasets. For every user u ,let E u = ( u , g ) | A u , g =1 denote the affiliations of u ,asobservedin a given affiliation network A . Invariably, in all the experiments, we set aside a subset
All of our recommendation algorithms require learning parameters for some model of the affiliation process, and hence for the purposes of learning the parameters, we process, we compare different model parameters based on the number of correct edges among 25 N u recommendations 4 made using a model. 4.2.1 Evaluation Method. We now describe our methodology for evaluating the perfor-mance of an affiliation recommendation algorithm. We first introduce notions of in-terest, namely recall or sensitivity , specificity , ROC (receiver operating characteristic) and AUC (area under curve). We then describe the way in which we evaluate the per-formance of a recommendation algorithm based on its top 50 recommendations to the average user. We then demonstrate the importance of choosing the right evaluation method for the community recommendation task by showing that using a different, but less appropriate, evaluation strategy yields different results. 4.2.2 Performance Measures. Two commonly used measures of quality of solutions in information retrieval and classification tasks are recall or sensitivity ,and specificity . Sensitivity is the ratio of the number of correctly identified positives to the total num-ommender to exclude uninteresting affiliations from the recommendations it makes. It is defined as the fraction of such negative affiliations correctly excluded from the rec-ommendation. Both performance measures range from zero to one. 4.2.3 ROC X  X eceiver Operating Characteristic. Often, one is interested in evaluating the performance of a recommendation algorithm not for a single value of the number of recommendations n , but for the entire range of n . For a given recommendation al-gorithm and a user, sensitivity is a nondecreasing function of n . The relationship between the increase in sensitivity as n increases, with the decrease in specificity is of interest in comparing the quality of recommendations. For a good recommendation, as n increases, sensitivity increases withou t a big drop in specificity. The ROC curve is a plot of the sensitivity vs [1  X  specificity] for all values of n . It is a common way of comparing the performance of classification algorithms over the entire range of n (or equivalently cutoff scores). The AUC curve (area under the ROC curve), is then used as a way to compare different classification algorithms. The greater the AUC, the better the algorithm X  X  sensitivity vs [1  X  specificity] tradeoff.

Consider a social network Web site, like Orkut or Facebook; or a vendor like Netflix, which sells movies. They would be interested in making, let us say, five pages of rec-ommendations to their users, but not much more than that. Certainly not a hundred. Also, irrespective of whether a user participates in five communities or seventy, the social networking Web site would probably want to make roughly the same number of recommendations per user. So, we choose to evaluate the recommendation algo-rithms we propose based on their top 50 recommendations. We do this by examining the portion of the ROC curve obtained by measuring the sensitivity and specificity the recommendation algorithm achieves for an average user at regular intervals between n =1and n = 50. To do this, for a given n between 1 and 50, we compute the sensitivity and specificity for every user in the network, and take the mean of these values to be the average sensitivity and average specificity for that n . 5 We then plot the average sensitivity vs [1  X  average specificity] curve, as in Figure 2. Note that comparisons made using this method are statistically robust, as the sensitivities and specificities of recommendation algorithms are averaged over, e.g., about 9,100 users in Orkut and about 16,500 users in YouTube. 4.2.4 Global vs Per-User Sensitivity. Let k ( n u ) be the number of good recommendations made by a recommendation algorithm for a user u ,whenitmakes n u recommendations our experiments, we will use n u =50  X  u . Contrast this with finding the global sensitiv-a given recommendation algorithm while making n predictions in total. For a fixed n , global sensitivity is a commonly used measure of performance of link prediction algo-rithms in the context of social network analysis. Note that in this case, while n = u n u , there is no guarantee that for two given users u and v , n u = n v ; the recommendation algorithm, when asked to make n recommendations, may not make any recommenda-tions at all for some users. Therefore, this measure of goodness of a recommendation algorithm is not equivalent to the per-user sensitivity.

Judging identical algorithms on identical data sets, using these alternative eval-uation methods can yield very different rankings of recommendation algorithms, as illustrated by comparing Figure 1 with Figure 2. Hence, the choice of an appropriate method for evaluating affiliation recommendations is an important one. In this section, we report and analyze the p erformance of the var ious recommendation algorithms, based on the graph proximity model and latent factors model discussed in Sections 2 and 3. We compare the performance of the graph proximity methods with the latent factors methods on the average per-user sensitivity and average specificity metrics introduced earlier, for a given number of recommendations { 5 , 10 ,  X  X  X  , 45 , 50 } . We study the performance of the scalable approximation methods proposed in Section 3, and compare these methods among themselves and with the methods pro-posed in Section 2. The list of experiments based on latent factor and graph proximity predictors are presented in Table II. Note that tKatz-A and SVD-A are natural baseline methods for our experiments, as the purpose of our experimental study is to see how the two models effectively use social networks in affiliation recommendation. 4.3.1 Performance of Recommenders Based on the Two Models. Consider the performance of the recommendation algorithms on the Orkut data set in Figure 2(a). SVD-A gives the lowest performance of all the methods. LFM performs better than SVD-A ,which is expected given that it uses information from friendship network S in addition to the information from affiliation network A . For the average user, the graph prox-imity model-based methods significantly outperform latent factors-based methods, as observed in both plots of Figure 2. In particular, we see that tKatz-C performs much better than tKatz-A , which in turn outperforms latent factors-based methods. We see that the information in the friendship network indeed proves highly beneficial in mak-ing affiliation recommendations and graph proximity-based methods exploit this in-formation the most. A summary of the performance of the algorithms on the YouTube data set is shown in Figure 2(b). We observe that the case for YouTube is similar to that of Orkut, in that graph proximity-based algorithms significantly outperform la-tent factors-based algorithms. In particular, tKatz-C is highly successful compared to the other methods.

Another interesting comparison of latent factors methods based on the choice of D in constructing the combined network C given in (8), is presented in Figure 3. We observe that the choice of D does not appear to make any significant difference in the performance of the recommendation algorithm. In the plot we use D 2 = A T A and D though, in case of the Orkut data set, we see that LFM using C ( D ) performs slightly better than LFM on C , it appears as if scaling D = 0 does not affect recommendation quality. In case of YouTube (plots not shown), our experiments indicate that D is not useful at all. The obvious choices for D do not seem to improve the performance compared with D =0.
 The best parameters learned by the various algorithms are presented in Table III. The search grid for  X  is [0 , 3] in steps of 0.1, d is [10 , 100] in steps of 10, log 10  X  is [  X  12 , 0] in steps of 1. For a few methods like tKatz-C , parameter learning is quite time consuming. Note that the best parameter,  X  =10  X  12 , implies that the calculated tKatz measure was effectively using the common neighbors method. In other words, users and communities connected by path lengths 5 or more 6 are not useful in making affiliation recommendations.

We see that the recommendation algorithms perform consistently across the two datasets, and the evaluations are robust as the specificities and sensitivities are aver-aged over 9,100 users in Orkut and 16,500 users in YouTube. 4.3.2 Scalable Approximations to tKatz-C. From Figure 4(b), we observe that combin-ing ideas from the latent factors model with the graph proximity model yields bet-ter performance than simply using the low rank approximations of C .Inparticular, the clustered low rank approximation LMF-c performs better than LMF alone. Also, using the tKatz measure in combination with LMF boosts the performance compared with LMF alone. In addition, using clustering in tKatz-LMF-c further increases the performance compared with tKatz-LMF . The common subspace method tKatz-CS ,per-forms very similarly to tKatz-LMF-c . InFigure4(a)weseethatthemethodsusing clustered low rank approximation, LMF-c and tKatz-LMF-c , actually have the worst performance. Thus the clustering approach could give a considerable performance boost, but this is not always the case. On the other hand, the nonclustered methods for the Orkut-lcc dataset have similar performance in relation to each other, as the nonclustered methods for the YouTube-lcc data set.

In Figure 5, we observe that the recommendation quality of the scalable recom-menders proposed in this article is close to that of tKatz-C . In particular, we note that tKatz-CS consistently performs very well on both datasets. Comparing the performance of tKatz-LFM and tKatz-LFM-c , we also note that the use of clustering is effective in im-proving performance with the YouTube-lcc data set, and this suggests that we can perhaps derive similar benefits, from considering a clustered version of the tKatz-CS algorithm.

Finally, in Figure 6, we study the effects of change in the number of clusters and the number of factors used on the performance of algorithms that use graph clustering for the purpose of scalability. We observe that changing the number of clusters does not make a significant difference in perfo rmance, whereas incre asing the number of factors yields better performance. This is n ot surprising since these algorithms can be viewed as computing approximations of tKatz-C . 4.3.3 Comparison of Run-Times. In Table IV we present timing results for the proposed methods: LMF , LMF-c , tKatz-CS , tKatz-LMF ,and tKatz-LMF-c . All of these methods are based on low rank approximations of either C or A ,and S . The score matrices corre-sponding to these methods are not formed explicitly, but kept in factor form. Forming them is infeasible since the score matrices will become dense. Similarly, forming the score matrices for the graph proximity models, that are not truncated or do not use low rank approximation, is not feasible as they would be dense. Forming the score matrices for truncated Katz measures, e.g. tKatz-C , may in some cases be feasible. This depends on the size of the network, the fraction of nonzeros in its adjacency matrix, and the number of terms k that are included in the summation. In the large-scale setting, even the truncated Katz measures are considered infeasible due to the unman-ageable increase of density in the powers C k ,fore.g. k = 5. For the data sets in our experiments we can actually form explicit score matrices with k =3,andthecompu-tation time of tKatz-C for the Youtube-lcc data set is 21 seconds. In contrast, low-rank approximations of large and sparse matrices are readily available using libraries like ARPACK.
 We report the run-times for rank d  X  { 50 , 100 , 150 , 200 , 250 , 300 } , for each method. In the tKatz-CS method, two low-rank approximations are computed, one for A with rank d / 2 and one for S with rank d / 2. With this setting the combined rank in the low-rank approximation for the common subspace model becomes the same as for the first two methods. In the clustered low-rank approaches, LMF-c and tKatz-LMF-c ,we used c = 5 clusters, and each cluster was computed with a rank d approximation. This way the rank of the approximation in the clustered methods becomes 5 d ,andthe clustered methods have about the same memory usage as the nonclustered low-rank approximations. Note that in Equation (15) we only store  X  V i , i =1 ,  X  X  X  , 5, and not the zero off-diagonal blocks. More detail on this is found in Savas and Dhillon [2011]. It is evident from Table IV that both the common subspace model ( tKatz-CS )andthe clustered low-rank approach ( tKatz-LMF-c ) yield faster computation times compared to regular low-rank approximation methods like LMF .
 Increasing attention to recommendation systems in general can be attributed primar-ily to commercial enterprises like Netflix a nd Amazon, where making good recommen-dations for the customers is important for business. A huge body of literature studies the problem of group recommendation, where the problem is to recommend items or products to a group of users in a friendship network. Affiliation recommendation for users of a friendship network, however, is relatively new and less studied. We now examine the relationship of the latent factors model proposed in this article to a variety of other models proposed recently. Bader and Chew [2008], in the context of information retrieval, tackle the problem of applying LSA to multilingual corpora. In such corpora, one has access to term-similari ty information along with term-document matrices corresponding to various languages. In order to derive low dimensional term and document factors that account for information from both these sources, they form a joint matrix similar to our combined adjacency matrix C and compute its SVD. How-ever, unlike this work, Bader and Chew [2008] does not deal with the item recom-mendation problem, and it does not view this joint matrix as arising out of a pair of networks.

We will now consider two other joint matrix factorization models. One class of mod-els uses probabilistic collaborative filtering to approach the problem, whereas another tackles the problem of combining informat ion from multiple sources from the perspec-tive of joint matrix factorization. 5.1.1 Probabilistic Collaborative Filtering Models. Collaborative filtering is a natural way to approach the affiliation recommendation problem. Typically collaborative filtering is applied to user-item preference problems. This is based on the simple idea that users with similar tastes behave similarly.

This approach has recently been applied to the affiliation recommendation problem by Chen et al. [2009b]. The authors examined the use of Latent Dirichlet Allocation (LDA) in affiliation recommendation. The LDA approach does not use information from the friendship network among users. So, here we briefly examine the relation-ship between the latent factors model we propose and this LDA-based approach, while ignoring the friendship network aspects of our model.

Consider the objective (9) we are trying to minimize. In the proposed model, if we ignore the constraint that the user factors U do not result in too large a deviation from S , we are essentially trying to find a low rank approximation to A in terms of the Frobenius norm. The solution that minimizes that objective is given by the SVD of A . This is the Latent Semantic Analysis approach (LSA), which has long been exploited for similar problems in the area of information retrieval. pLSA, or probabilistic LSA [Hofmann 1999], instead proposes a statistical model for the process generating A and then learns the model parameters that are most likely to have generated the observa-tions in A . These parameters can then be used in finding a low rank approximation to A in terms of the KL divergence. It can thus be viewed as the probabilistic version of LSA. LDA, where Dirichlet priors are added to pLSA X  X  generative model, can be viewed as the Bayesian version of pLSA. Thus, the LDA-based approach to the affiliation rec-ommendation problem may be viewed as trying to find a low rank approximation to A , albeit from a probabilistic, Bayesian perspective, while ignoring information from S .
Combinatorial collaborative filtering [Chen et al. 2008] is another work in the same vein. Unlike the LDA-based approach however, the probabilistic model of user-community relationships used in this work utilizes information not only from A , but also from text descriptions of various communities.

A closely related problem of suggesting groups for images uploaded by users, using the affiliation network between users and groups, was studied in De Choudhury et al. [2009]. In this recommendation framework, groups are represented using latent states derived from features including how the users interact with various groups. Combin-ing information from the affiliation network, with image content and tags, is shown to improve the performance of the recommendation system.

Next, we examine a couple of closely related matrix factorization models. 5.1.2 Linked Matrix Factorization Models. Tang et al. [2009] have proposed Linked Matrix Factorization (LMF) as a way of combining information from multiple graphs on the same set of entities, in order to make more accurate inferences. However, they tackle a different problem, namely, clustering. The link between their network model and ours is established by the objective functions that we optimize. The LMF model tries to si-multaneously find a low-rank approximation for the adjacency matrix of each network, using matrix factorization. Each such matri x factorization has a so urce-specific factor matrix, ( m ) , and a factor matrix, V , that is shared by all the sources. The objective function of LMF is to effectively to minimize the quantity, Comparing this to (9), we see that U , which represents the user factors, is shared by the two sources of information, A and S . However, an important distinction is that we have two graphs that share only one set of entities in common, whereas in LMF, each source of information is a network on exactly the same set of users.

Singh and Gordon [2008] have proposed a model for relational learning called Col-lective Matrix Factorization. They suggest a generalized framework for inferring re-lations, given a set of entities and observed relations among them. This model factors multiple source matrices simultaneously, and uses common factors for approximation whenever the same entity participates in multiple relations. It allows different loss functions for each matrix approximation, and combines the information from multi-ple relations using weights that reflect the relative importance of each relation. This essentially generalizes the idea of Linked Matrix Factorization. The Latent Factors Model proposed in this article uses the parameter  X  to determine the contribution of the friendship network S in generating the user factors.

These papers use optimization techniques based on the alternating least squares approach; we use SVD, which efficiently solves the optimization problem posed by the latent factors model. Researchers have studied the effects of friendship ties on affiliations in other contexts, like the growth and evolution of social networks [Backstrom et al. 2006], and spread of influence through a social network [Chen et al. 2009a; Kempe et al. 2003]. They tend to model the dependence of a user joining a group on the number of friends the user already has in the group. A unified model for the generation of social and affiliation networks is proposed in Zheleva et al. [2009], where the social network is observed to be one of the factors that influences the evolution of the affiliation network. Our idea that the friendship network combined with the affiliation network can be exploited in making affiliation recommendations is inspired by this line of research.
 In this article, we have tackled the affiliation recommendation problem, where the task is to recommend new affiliations to users, given the current state of the friendship and affiliation networks. We show that information from the friendship network can indeed be fruitfully exploited in making affiliation recommendations. This auxiliary source of information was hitherto not used in making community recommendations.

Using a simple way of combining these networks, we suggested two ways of modeling the networks for the purpose of making affiliation recommendations (Sec-tion 2). The first of these approached the problem from the graph proximity viewpoint, whereas the second modelled the interactions of users and groups in the two networks using latent factors derived from optimizing towards a joint matrix factorization ob-jective. We studied the algorithms suggested by these models on real world networks (Section 4). We motivated and proposed a way of evaluating recommenders, by mea-suring how good the top 50 recommendations are, and demonstrated the importance of choosing the right evaluation strategy. Algorithms suggested by the graph proxim-ity model turn out to be the most effective, based on experiments on large real-world data sets. We also introduced scalable versi ons of these algorithms, and demonstrated their performance. These results show that the application of techniques from social network link prediction in affiliation and item recommendation is a promising one. There is the intriguing possibility of using an affiliation network for link prediction in the friendship network. Discovering techniques and models that do this effectively seems to be a challenging research avenue. Our early experiments at doing this indi-cate that this is a much harder problem. The reasons for this are not yet clear, and this question seems fertile for further exploration.

Within the ambit of the affiliation recommendation problem itself, one may research ways of fruitfully using even more sources of information. For example, Chen et al. [2008] use information from textual description of communities along with the affilia-tion network to make affiliation recommendations. It might be useful to consider the social network together with this auxiliary information. Also, predictors based on the latent factors model and the graph proximity model may be suited for different types of users, and creating a metapredictor that combines predictions from both classes of predictors is another attractive research di rection. Finally, fro m the perspective of making scalable recommendations, and considering the relative effectiveness of the common subspace approach to approximate S and A , we may benefit from a clustered version of the truncated Katz measure when it is based on the common subspace ap-proximations of the two networks.

