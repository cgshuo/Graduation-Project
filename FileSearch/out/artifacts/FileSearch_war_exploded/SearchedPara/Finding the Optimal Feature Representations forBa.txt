 With the ever-increasing growth of the on-line information and the permeation of Internet into daily life, web document classification plays an important role in natural language processing (such as E-mail filtering [1], news filtering [2], prediction of user preferences [3]) and information retrieval applications from the linguistic point of view. Because of the variety of languages, applications and domains, machine learning techniques are commonly applied to infer a clas-sification model from instance document s with known class labels. The inferred model can then be used for web document cl assification, i.e. classification based on text content.

Naive Bayes [4][5] has been found particularly attractive for the task of text categorization because it performs surprisingly well in many application areas despite its simplicity. Naive Bayes makes the strong assumption that the predic-tive variables ( X  X eatures X  or  X  X ords X ) are conditionally independent given the class. From the linguistic point of view, a document is made up of words, and the semantics of the document is determined by the meaning of the words and the linguistic structure of the document. The generative model underlying the Naive Bayes can be charact erized with respect to the a mount of information it captures about the words in a document. In information retrieval and text cat-egorization, the most popular probabilistic models are: the multinomial model [7][8] and the binary independence model [6].

The binary independence model specifies that a document is represented by a vector of binary features indicating which words occur and do not occur in the document. It captures the information of which words are used in a document, but not the number of times each words is used, nor the order of the words in the document. This describes a d istribution based on a multi-variate Bernoulli event model. This approach is more appropriate for tasks that have a fixed number of features. The multinomial model sp ecifies that a document is represented by the set of word occurrences from th e document. And we call these words multinomial features for clarity. This approach is more traditional in statistical language modeling for speech recognition, where it would be called a  X  X nigram language model. X 
Before learning, the characteristic of an feature should be analyzed to decide which kind of representation is appropriate. On the other hand, the multino-mial model and the binary independence model can not handle a much more complex situation, that is, both representations are required to appear in the same model. In this paper, we extend the independence assumption to make it more efficient and feasible and then propose General Naive Bayes (GNB) model, which can handle both representations concurrently. The remainder of this paper is organized as follows. Sect. 2 defines an information gain criterion to decide which representation is appropriate for a given feature. Sect. 3 describes the ba-sic idea of General Naive Bayes. Sect. 4 pr esents the corresponding experimental results of compared performance with regarding to the multinomial model and the binary independence model. Sect. 5 wraps up the discussion. In this discussion we use capital letters such as V i ,V j to denote feature names, and lower-case letters such as v i ,v j to denote specific values taken by those features. Let P (  X  ) denote the probability, p (  X  ) refer to the probability density function. Here d i denotes the i th training document and c i is the corresponding category label of d i .Adocument d is normally represented by a vector of n features or words d =( v 1 ,v 2 ,  X  X  X  ,v n ).

Entropy is commonly used to characterize the impurity of an arbitrary col-lection of instances. But Entropy has limitation when dealing with multinomial representation. In the case of binary features, the set of possible values is a nu-merable set. To compute the conditional probability we only need to maintain a counter for each feature value and for each class. In the case of multinomial features, the number of possible occurrences is infinite, thus make it impossible to compute conditional entropy.

We begin by adding two implicit features  X  C and  X  C .Let V i represent one of the predictive features. According to Bayes theorem, if V i is a binary feature, there will be
Since P ( v i ) is the same for all classes, and does not affect the relative values of their probabilities, it can be ignored. When some instances satisfy V i = v i , their class labels are most likely to be:
Correspondingly, if V i is a multinomial feature we will have Then, the corresponding relationship between V i , C ,  X  C and  X  C may be: Accordingly, the conditional entropies of C are: and On the other hand, the entropy of C is The Projective Information Gain PI ( C ; V i ) is defined as extent to which the model constructed by feature V i fits class feature C when V i is treated as binary feature or multin omial feature, respectively. Then we compare them to choose the right representation and that is what PI ( C ; V i ) means. Naive Bayes is one of the most straightforward and widely used method for prob-abilistic induction. This scheme represents each class with a single probabilistic summary and is based on one assumption that predictive features V 1 ,  X  X  X  ,V n are conditionally independent given the category label C , which can be expressed as:
But if multinomial features are required to represent words, the situation is different. Since the independence assumption described above is applicable to binary features only, we should extend it to make it much more effective. For simplicity, we first just consider two features: V 1 (multinomial) and V 2 (binary). Suppose the word frequency values of V 1 have been normalized and discretized, then the independence assumption should be: where [ v 1 ,v 1 + ] is arbitrary interval of the values of feature V 1 . This assump-tion, which is the basis of GNB, supports very efficient algorithms for both classification and learning. By the definition of a derivative, where v 1  X   X , X   X  v 1 + .When  X  0, P ( c | v 1  X  V 1  X  v 1 + ,v 2 )  X  P ( c | v 1 ,v 2 ) and  X , X   X  v 1 , hence
Suppose the first m of n features are multinomial and the remaining features are binary. Similar to the induction process of (9), we will have
Based on Eq. (10), maximum a posterior (MAP) classifier can be constructed by seeking the optimal category which maximizes the posterior P ( c | d ), then the classification rule of GNB is: We compare classification accuracy with and without word frequency informa-tion on two datasets: 20 Newsgroups [9] and Reuters-21578 [10] collection. We produce 20 train/test splits using stratified random sampling with 90% of the data for training and 10% for testing. Experiments are done using 20 trials over these splits and averaging the results . We carried the experiment to compare the General Naive Bayes model, the binary independence model [11] and the multinomial model [12] when different number of unlabelled test data are incor-porated into the training set. Fig. 1 shows the average results on Reuters-21578 corpus, with various vocabulary sizes. T he effectiveness of Bayes classifier was measured by classification accuracy.

All the three models do best at the maximum vocabulary sizes. Multino-mial model achieves 81% accuracy and the binary independence model achieves 68% accuracy. Using GNB improves per formance by more than 3 percentage points. The binary independence model is more sensitive to query length than the multinomial model. The multinomial model treats each occurrence of a word in a document independently of any other occurrence of the same word. It can be argued that the binary statistics used in the binary independence model are more robust than the ones used in the multinomial model. The violations of the multinomial model X  X  independence assumption may be more detrimental than the corresponding violations for the binary independence model. In particular, there is one effect to which the multinomial model is subject which has no coun-terpart in the binary independence model. It is likely that many terms, espe-cially the kind that are likely to be good discriminators, are strongly dependent on their own occurrence. Overall, howeve r, the results show that performance of the GNB model on this task is substantially better than that of the other two Bayes models. We hypothesize that this is primarily due to the independence assumption of the GNB model.

We conducted similar experiment on another dataset: 20-Newsgroups collec-tion. we can also see similar improvement. Richer vocabulary and semantics, which can give more positive information for classification, may be one reason. On the other hand, the binary independence model and multinomial model rep-resent only one aspect of a given word. But GNB can integrate them into one model on text documents, this should be the main reason. In this paper, we presented a novel web mining approach named General Naive Bayes (GNB). By defining Projective Information Gain PI ( C ; V i ), GNB can easily decide which representation is appropriate for a given word, thus overcome the restrictiveness of the binary independence model and multinomial model. Furthermore, the classification rule applied by GNB can directly handle both representations concurrently while not suffering from the user X  X  bias. Our results indicate that GNB constitutes a promising addition to the induction algorithms from the viewpoint of classification performance.

