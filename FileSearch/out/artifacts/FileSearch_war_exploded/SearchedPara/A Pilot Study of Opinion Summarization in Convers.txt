 Both sentiment analysis (opinion recognition) and summarization have been well studied in recent years in the natural language processing (NLP) com-munity. Most of the previous work on sentiment analysis has been conducted on reviews. Summa-rization has been applied to different genres, such as news articles, scientific articles, and speech do-mains including broadcast news, meetings, conver-sations and lectures. However, opinion summariza-tion has not been explored much. This can be use-ful for many domains, especially for processing the increasing amount of conversation recordings (tele-phone conversations, customer service, round-table discussions or interviews in broadcast programs) where we often need to find a person X  X  opinion or attitude, for example,  X  X ow does the speaker think about capital punishment and why? X . This kind of questions can be treated as a topic-oriented opin-ion summarization task. Opinion summarization was run as a pilot task in Text Analysis Conference (TAC) in 2008. The task was to produce summaries of opinions on specified targets from a set of blog documents. In this study, we investigate this prob-lem using spontaneous conversations. The problem is defined as, given a conversation and a topic, a summarization system needs to generate a summary of the speaker X  X  opinion towards the topic.

This task is built upon opinion recognition and topic or query based summarization. However, this problem is challenging in that: (a) Summarization in spontaneous speech is more difficult than well struc-tured text (Mckeown et al., 2005), because speech is always less organized and has recognition errors when using speech recognition output; (b) Senti-ment analysis in dialogues is also much harder be-cause of the genre difference compared to other do-mains like product reviews or news resources, as re-ported in (Raaijmakers et al., 2008); (c) In conversa-tional speech, information density is low and there are often off topic discussions, therefore presenting a need to identify utterances that are relevant to the topic.

In this paper we perform an exploratory study on opinion summarization in conversations. We compare two unsupervised methods that have been widely used in extractive summarization: sentence-ranking and graph-based methods. Our system at-tempts to incorporate more information about topic relevancy and sentiment scores. Furthermore, in the graph-based method, we propose to better in-corporate the dialogue structure information in the graph in order to select salient summary utterances. We have created a corpus of reasonable size in this study. Our experimental results show that both methods achieve better results compared to the base-line.

The rest of this paper is organized as follows. Sec-tion 2 briefly discusses related work. Section 3 de-scribes the corpus and annotation scheme we used. We explain our opinion-oriented conversation sum-marization system in Section 4 and present experi-mental results and analysis in Section 5. Section 6 concludes the paper. Research in document summarization has been well established over the past decades. Many tasks have been defined such as single-document summariza-tion, multi-document summarization, and query-based summarization. Previous studies have used various domains, including news articles, scientific articles, web documents, reviews. Recently there is an increasing research interest in speech sum-marization, such as conversational telephone speech (Zhu and Penn, 2006; Zechner, 2002), broadcast news (Maskey and Hirschberg, 2005; Lin et al., 2009), lectures (Zhang et al., 2007; Furui et al., 2004), meetings (Murray et al., 2005; Xie and Liu, 2010), voice mails (Koumpis and Renals, 2005). In general speech domains seem to be more diffi-cult than well written text for summarization. In previous work, unsupervised methods like Maximal Marginal Relevance (MMR), Latent Semantic Anal-ysis (LSA), and supervised methods that cast the ex-traction problem as a binary classification task have been adopted. Prior research has also explored using speech specific information, including prosodic fea-tures, dialog structure, and speech recognition con-fidence.

In order to provide a summary over opinions, we need to find out which utterances in the conversa-tion contain opinion. Most previous work in senti-ment analysis has focused on reviews (Pang and Lee, 2004; Popescu and Etzioni, 2005; Ng et al., 2006) and news resources (Wiebe and Riloff, 2005). Many kinds of features are explored, such as lexical fea-tures (unigram, bigram and trigram), part-of-speech tags, dependency relations. Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion de-tection. Only a handful studies have used conver-sational speech for opinion recognition (Murray and Carenini, 2009; Raaijmakers et al., 2008), in which some domain-specific features are utilized such as structural features and prosodic features.

Our work is also related to question answering (QA), especially opinion question answering. (Stoy-anov et al., 2005) applies a subjectivity filter based on traditional QA systems to generate opinionated answers. (Balahur et al., 2010) answers some spe-cific opinion questions like  X  X hy do people criti-cize Richard Branson? X  by retrieving candidate sen-tences using traditional QA methods and selecting the ones with the same polarity as the question. Our work is different in that we are not going to an-swer specific opinion questions, instead, we provide a summary on the speaker X  X  opinion towards a given topic.

There exists some work on opinion summariza-tion. For example, (Hu and Liu, 2004; Nishikawa et al., 2010) have explored opinion summarization in review domain, and (Paul et al., 2010) summarizes contrastive viewpoints in opinionated text. How-ever, opinion summarization in spontaneous conver-sation is seldom studied. Though there are many annotated data sets for the research of speech summarization and sentiment analysis, there is no corpus available for opinion summarization on spontaneous speech. Thus for this study, we create a new pilot data set using a sub-set of the Switchboard corpus (Godfrey and Holli-man, 1997). 1 These are conversational telephone speech between two strangers that were assigned a topic to talk about for around 5 minutes. They were told to find the opinions of the other person. There are 70 topics in total. From the Switchboard cor-pus, we selected 88 conversations from 6 topics for this study. Table 1 lists the number of conversations in each topic, their average length (measured in the unit of dialogue acts (DA)) and standard deviation of length.
We recruited 3 annotators that are all undergrad-uate computer science students. From the 88 con-versations, we selected 18 (3 from each topic) and let all three annotators label them in order to study inter-annotator agreement. The rest of the conversa-tions has only one annotation.

The annotators have access to both conversation transcripts and audio files. For each conversation, the annotator writes an abstractive summary of up to 100 words for each speaker about his/her opin-ion or attitude on the given topic. They were told to use the words in the original transcripts if possible. Then the annotator selects up to 15 DAs (no mini-mum limit) in the transcripts for each speaker, from which their abstractive summary is derived. The se-lected DAs are used as the human generated extrac-tive summary. In addition, the annotator is asked to select an overall opinion towards the topic for each speaker among five categories: strongly sup-port, somewhat support, neutral, somewhat against, strongly against. Therefore for each conversation, we have an abstractive summary, an extractive sum-mary, and an overall opinion for each speaker. The following shows an example of such annotation for speaker B in a dialogue about  X  X apital punishment X :
Table 2 shows the compression ratio of the extrac-tive summaries and abstractive summaries as well as their standard deviation. Because in conversations, utterance length varies a lot, we use words as units when calculating the compression ratio.
We measured the inter-annotator agreement among the three annotators for the 18 conversations (each has two speakers, thus 36  X  X ocuments X  in to-tal). Results are shown in Table 3. For the ex-tractive or abstractive summaries, we use ROUGE scores (Lin, 2004), a metric used to evaluate auto-matic summarization performance, to measure the pairwise agreement of summaries from different an-notators. ROUGE F-scores are shown in the table for different matches, unigram (R-1), bigram (R-2), and longest subsequence (R-L). For the overall opin-ion category, since it is a multiclass label (not binary decision), we use Krippendorff X  X   X  coefficient to measure human agreement, and the difference func-tion for interval data:  X  2 ck = ( c  X  k ) 2 (where c,k are the interval values, on a scale of 1 to 5 corresponding to the five categories for the overall opinion).
We notice that the inter-annotator agreement for extractive summaries is comparable to other speech summary annotation (Liu and Liu, 2008). The agreement on abstractive summaries is much lower than extractive summaries, which is as expected. Even for the same opinion or sentence, annotators use different words in the abstractive summaries. The agreement for the overall opinion annotation is similar to other opinion/emotion studies (Wil-son, 2008b), but slightly lower than the level rec-ommended by Krippendorff for reliable data (  X  = 0 . 8 ) (Hayes and Krippendorff, 2007), which shows it is even difficult for humans to determine what opinion a person holds (support or against some-thing). Often human annotators have different inter-pretations about the same sentence, and a speaker X  X  opinion/attitude is sometimes ambiguous. Therefore this also demonstrates that it is more appropriate to provide a summary rather than a simple opinion cat-egory to answer questions about a person X  X  opinion towards something. Automatic summarization can be divided into ex-tractive summarization and abstractive summariza-tion. Extractive summarization selects sentences from the original documents to form a summary; whereas abstractive summarization requires genera-tion of new sentences that represent the most salient content in the original documents like humans do. Often extractive summarization is used as the first step to generate abstractive summary.

As a pilot study for the problem of opinion sum-marization in conversations, we treat this problem as an extractive summarization task. This section describes two approaches we have explored in gen-erating extractive summaries. The first one is a sentence-ranking method, in which we measure the salience of each sentence according to a linear com-bination of scores from several dimensions. The sec-ond one is a graph-based method, which incorpo-rates the dialogue structure in ranking. We choose to investigate these two methods since they have been widely used in text and speech summarization, and perform competitively. In addition, they do not re-quire a large labeled data set for modeling training, as needed in some classification or feature based summarization approaches. 4.1 Sentence Ranking In this method, we use Equation 1 to assign a score to each DA s , and select the most highly ranked ones until the length constriction is satisfied.  X  sim ( s,D ) is the cosine similarity between DA  X  REL ( s,topic ) measures the topic relevance of  X  sentiment ( s ) indicates the probability that ut- X  length ( s ) is the length of the utterance. This 4.2 Graph-based Summarization Graph-based methods have been widely used in doc-ument summarization. In this approach, a document is modeled as an adjacency matrix, where each node represents a sentence, and the weight of the edge be-tween each pair of sentences is their similarity (co-sine similarity is typically used). An iterative pro-cess is used until the scores for the nodes converge. Previous studies (Erkan and Radev, 2004) showed that this method can effectively extract important sentences from documents. The basic framework we use in this study is similar to the query-based graph summarization system in (Zhao et al., 2009). We also consider sentiment and topic relevance infor-mation, and propose to incorporate information ob-tained from dialog structure in this framework. The score for a DA s is based on its content similarity with all other DAs in the dialogue, the connection with other DAs based on the dialogue structure, the topic relevance, and its subjectivity, that is: where C is the set of all DAs in the dialogue; REL ( s,topic ) and sentiment ( s ) are the same as those in the above sentence ranking method; sim ( s,v ) is the cosine similarity between two DAs s and v . In addition to the standard connection be-tween two DAs with an edge weight sim ( s,v ) , we introduce new connections ADJ ( s,v ) to model di-alog structure. It is a directed edge from s to v , de-fined as follows:  X  If s and v are from the same speaker and within  X  If s and v are from the same speaker, and  X  If s and v form a question-answer pair from two  X  If s and v form an agreement or disagreement  X  If there are multiple edges generated from the
Since we are using a directed graph for the sen-tence connections to model dialog structure, the re-sulting adjacency matrix is asymmetric. This is dif-ferent from the widely used graph methods for sum-marization. Also note that in the first sentence rank-ing method or the basic graph methods, summariza-tion is conducted for each speaker separately. Ut-terances from one speaker have no influence on the summary decision for the other speaker. Here in our proposed graph-based method, we introduce con-nections between the two speakers, so that the adja-cency pairs between them can be utilized to extract salient utterances. 5.1 Experimental Setup The 18 conversations annotated by all 3 annotators are used as test set, and the rest of 70 conversa-tions are used as development set to tune the param-eters (determining the best combination weights). In preprocessing we applied word stemming. We per-form extractive summarization using different word compression ratios (ranging from 10% to 25%). We use human annotated dialogue acts (DA) as the ex-traction units. The system-generated summaries are compared to human annotated extractive and ab-stractive summaries. We use ROUGE as the eval-uation metrics for summarization performance.
We compare our methods to two systems. The first one is a baseline system, where we select the longest utterances for each speaker. This has been shown to be a relatively strong baseline for speech summarization (Gillick et al., 2009). The second one is human performance. We treat each annota-tor X  X  extractive summary as a system summary, and compare to the other two annotators X  extractive and abstractive summaries. This can be considered as the upper bound of our system performance. 5.2 Results From the development set, we used the grid search method to obtain the best combination weights for the two summarization methods. In the sentence-ranking method, the best parameters found on the development set are  X  sim = 0 , X  rel = 0 . 3 , X  sent = 0 . 3 , X  len = 0 . 4 . It is surprising to see that the sim-ilarity score is not useful for this task. The possible reason is, in Switchboard conversations, what peo-ple talk about is diverse and in many cases only topic words (except stopwords) appear more than once. In addition, REL score is already able to catch the topic relevancy of the sentence. Thus, the similarity score is redundant here.

In the graph-based method, the best parameters are  X  sim = 0 , X  adj = 0 . 3 , X  rel = 0 . 4 , X  sent = 0 . 3 . The similarity between each pair of utterances is also not useful, which can be explained with similar reasons as in the sentence-ranking method. This is different from graph-based summarization systems for text domains. A similar finding has also been shown in (Garg et al., 2009), where similarity be-tween utterances does not perform well in conversa-tion summarization.

Figure 1 shows the ROUGE-1 F-scores compar-ing to human extractive and abstractive summaries for different compression ratios. Similar patterns are observed for other ROUGE scores such as ROUGE-2 or ROUGE-L, therefore they are not shown here. Both methods improve significantly over the base-line approach. There is relatively less improvement using a higher compression ratio, compared to a lower one. This is reasonable because when the compression ratio is low, the most salient utterances are not necessarily the longest ones, thus using more information sources helps better identify important sentences; but when the compression ratio is higher, longer utterances are more likely to be selected since they contain more content.

There is no significant difference between the two methods. When compared to extractive reference summaries, sentence-ranking is slightly better ex-cept for the compression ratio of 0.1. When com-pared to abstractive reference summaries, the graph-based method is slightly better. The two systems share the same topic relevance score (REL) and sentiment score, but the sentence-ranking method prefers longer DAs and the graph-based method prefers DAs that are emphasized by the ADJ ma-trix, such as the DA in the middle of a cluster of utterances from the same speaker, the answer to a question, etc. 5.3 Analysis To analyze the effect of dialogue structure we in-troduce in the graph-based summarization method, we compare two configurations:  X  adj = 0 (only us-ing REL score and sentiment score in ranking) and  X  adj = 0 . 3 . We generate summaries using these two setups and compare with human selected sentences. Table 4 shows the number of false positive instances (selected by system but not by human) and false neg-ative ones (selected by human but not by system). We use all three annotators X  annotation as reference, and consider an utterance as positive if one annotator selects it. This results in a large number of reference summary DAs (because of low human agreement), and thus the number of false negatives in the system output is very high. As expected, a smaller compres-sion ratio (fewer selected DAs in the system output) yields a higher false negative rate and a lower false positive rate. From the results, we can see that gen-erally adding adjacency matrix information is able to reduce both types of errors except when the com-pression ratio is 0 . 15 .
 The following shows an example, where the third DA is selected by the system with  X  adj = 0 . 3 , but not by  X  adj = 0 . This is partly because the weight of the second DA is enhanced by the the question-answer pair (the first and the second DA), and thus subsequently boosting the score of the third DA.
We also examined the system output and human annotation and found some reasons for the system errors: (a) Topic relevance measure. We use the statis-tics from the Switchboard corpus to measure the rel-evance of each word to a given topic (PMI score), therefore only when people use the same word in different conversations of the topic, the PMI score of this word and the topic is high. However, since the size of the corpus is small, some topics only con-tain a few conversations, and some words only ap-pear in one conversation even though they are topic-relevant. Therefore the current PMI measure cannot properly measure a word X  X  and a sentence X  X  topic relevance. This problem leads to many false neg-ative errors (relevant sentences are not captured by our system). (b) Extraction units. We used DA segments as units for extractive summarization, which can be problematic. In conversational speech, sometimes a DA segment is not a complete sentence because of overlaps and interruptions. We notice that anno-tators tend to select consecutive DAs that constitute a complete sentence, however, since each individual DA is not quite meaningful by itself, they are often not selected by the system. The following segment is extracted from a dialogue about  X  X niversal health insurance X . The two DAs from speaker B are not selected by our system but selected by human anno-tators, causing false negative errors.
 This paper investigates two unsupervised methods in opinion summarization on spontaneous conver-sations by incorporating topic score and sentiment score in existing summarization techniques. In the sentence-ranking method, we linearly combine sev-eral scores in different aspects to select sentences with the highest scores. In the graph-based method, we use an adjacency matrix to model the dialogue structure and utilize it to find salient utterances in conversations. Our experiments show that both methods are able to improve the baseline approach, and we find that the cosine similarity between utter-ances or between an utterance and the whole docu-ment is not as useful as in other document summa-rization tasks.

In future work, we will address some issues iden-tified from our error analysis. First, we will in-vestigate ways to represent a sentence X  X  topic rel-evance. Second, we will evaluate using other ex-traction units, such as applying preprocessing to re-move disfluencies and concatenate incomplete sen-tence segments together. In addition, it would be interesting to test our system on speech recognition output and automatically generated DA boundaries to see how robust it is. The authors thank Julia Hirschberg and Ani Nenkova for useful discussions. This research is supported by NSF awards CNS-1059226 and IIS-0939966.

