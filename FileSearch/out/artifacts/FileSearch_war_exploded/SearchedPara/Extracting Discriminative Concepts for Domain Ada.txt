 One common predictive modeling challenge occurs in text mining problems is that the training data and the oper-ational (testing) data are drawn from different underlying distributions. This poses a great difficulty for many statisti-cal learning methods. However, when the distribution in the source domain and the target domain are not identical but related, there may exist a shared concept space to preserve the relation. Consequently a good feature representation can encode this concept space and minimize the distribu-tion gap. To formalize this intuition, we propose a domain adaptation method that parameterizes this concept space by linear transformation under which we explicitly minimize the distribution difference between the source domain with sufficient labeled data and target domains with only unla-beled data, while at the same time minimizing the empirical loss on the labeled data in the source domain. Another characteristic of our method is its capability for considering multiple classes and their interactions simultaneously. We have conducted extensive experiments on two common text mining problems, namely, information extraction and doc-ument classification to demonstrate the effectiveness of our proposed method.
 H.4 [ Information Systems Applications ]: Miscellaneous; I.5.2 [ Design Methodology ]: Feature evaluation and se-lection  X 
The work described in this paper is substantially sup-ported by grants from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project No: CUHK4128/07) and the Direct Grant of the Fac-ulty of Engineering, CUHK (Project Codes: 2050391 and 2050442). This work is also affiliated with the Microsoft-CUHK Joint Laboratory for Human-centric Computing and Interface Technologies.
 This research is in part supported by Singapore MOE AcRF Tier-1 Research Grant (RG15/08).
 Algorithms Domain Adaptation, Text Mining, Feature Extraction
Traditional statistical learning techniques rely on the ba-sic assumption that the training data and the operational (testing) data are drawn from the same underlying distri-bution. However, in many text mining applications involv-ing high-dimensional feature space, it is difficult to collect sufficient training data for different domains. For example, consider a text information extraction problem whose objec-tive is to automatically extract precise job information such as job title, duty, requirement, etc. from recruitment Web sites in different industries supp orting intelligent analysis of employment information. Usually we may just have few ex-perts who can accurately annotate the information in one specific industry like accounting for preparing the training data. The learned model deployed obviously cannot perform well in other domains (industries) such as logistic or health care due to the distribution of the terms in each domain is different. One strategy to tackle this problem is to adapt the trained model from one domain known as the source domain with sufficient labeled data to another domain known as the target domain where only unlabeled data is available.
It can be observed that domain adaptation is reasonable and practical if the distributions between the source domain and the target domain is related, which is mainly based on the fact that there exists a shared concept space in which the embedded distribution of each domain is close enough. Consequently it is very reasonable to believe that a good feature representation is able to encode this concept space and provide strong adaptive power from the source domain to the target domain. On the other hand, such a changed representation may encode less information leading to an increase of the empirical loss on the labeled data. To cope with this problem, we try to learn the ideal shared concept space with respect to two criteria: the empirical loss in the source domain, and the embedded distribution gap between the source domain and the target domain. Consider again the job information extraction example. For the task of ex-tracting the job requirement information in the domain of accounting, the most representative terms are  X  X ualified X ,  X  X ear X ,  X  X xperience X ,  X  X PA X ,  X  X A X ,  X  X CCA X , etc. Similarly for the domain of health care, the representative terms shift to  X  X ualified X ,  X  X egree X ,  X  X ear X ,  X  X CP X ,  X  X hysiology X ,  X  X xpe-rience X , etc. If we can extract the shared domain indepen-dent features such as  X  X ualified X ,  X  X ear X ,  X  X xperience X  for the specific task, then the learned extractor can be effectively adapted to the domain of health care.

In this paper we propose a domain adaptation method which directly minimizes both the distribution gap between the source domain and the target domain, as well as the empirical loss on the labeled data in the source domain by extracting the low-rank concept subspace. Maximum Mean Discrepancy (MMD) [5] is adopted to measure the embed-ded distribution difference between the source domain with sufficient but finite labeled data and the target domain with sufficient unlabeled data. Then our objective is to minimize the empirical loss and the MMD measurement with respect to the parametric family (linear transformation) which pa-rameterizes the embedded feature subspace. Furthermore, we apply the graph Laplacian [1] to exploit the predictive power for some domain dependent representative features in the target domain based on the co-occurrence with the shared features. This technique can help improve the per-formance especially when the common features are not suf-ficient in the target domain.

Several domain adaptation methods have been proposed to learn a reasonable representation so as to make the distri-butions between the source domain and the target domain closer [3, 12, 13, 11]. However, none of them can automati-cally learn the concept space where the prediction power in the source domain and the adaptive power from the source domain to the target domain are both considered.

Our main contributions can be summarized as follows: (1) We propose a domain adaptation method to extract the low-rank concept space shared by the source domain and the target domain, which can ensure both the predictive power and adaptive power are maximized. (2) We can transfer the predictive power from the extracted common features to the characteristic features in the target domain by the feature graph Laplacian. (3) We theoretically analyze the expected error in the target domain showing that the error bound can be controlled by the expected loss in the source domain, and the embedded distribution gap, so as to prove that what we minimize in the objective function is very reasonable for domain adaptation. (4) Our domain adaptation method is capable of considering multiple classes and their interactions simultaneously. It can be applied to high dimensional text mining applications due to two major properties of text: latent semantic and sparseness. The first property ensures that low-rank concept space can still preserve enough information, and the second property contributes to the computation speed.

We have conducted extensive experiments on two com-mon text mining problems, namely, information extraction and document classification to demonstrate the effectiveness of our proposed method. Experiment results show that our method can get better performance than other existing com-petitive methods.
Domain adaptation is a widely studied area. It addresses a common situation when applying the trained model to a different domain. Many works try to learn a new represen-tation which can bridge the source domain and the target domain. Blitzer et al. [3] proposed a heuristic method to se-lect some domain independent pivot features to learn an em-bedded space where the data coming from both domains can share the same feature structure. Daum  X  e III [4] proposed the Feature Augmentation method to augment features for do-main adaptation. The augmented features are used to con-struct a kernel function for kernel methods. Raina et al. [12] learned the sparse basis from the unlabeled data which may not come from the same domain as the labeled data. Then it represents the labeled data by those learned high-level basis for further classification. Yang et al.[16] proposed Adaptive SVM (A-SVM) to enhance the prediction performance of video concept detection, in which the new SVM classifier is adapted from an existing classifier trained from the auxiliary domain. Several domain adaptation methods [6, 14, 15, 8, 2] suggested to apply the instance weighting technique for domain adaption in various applications. Recently, Pan et al. [11] applied the Maximum Mean Discrepancy (MMD) to learn the embedded space where the distribution between the source domain and the target domain is minimized.
In this paper, we focus on the setting where the opera-tional (testing) samples come from another domain, which is different from the training set. In the sequel, we refer the training set to as the source domain D S = { ( x i ,y where x i  X  R d is the d dimensional input space, and y i the output label. We also assume that the testing samples are available. Denote the testing set as D T = { x i } n 2 x  X  R d is the input. Let P ( x )and Q ( x )(or P and Q for short) be the marginal distributions of the input sets { x and { x i } from the source and target domains, respectively. In general, P and Q can be different. The task of domain adaptation is to predict the labels y i  X  X  corresponding to the inputs x i  X  X  in the target domain. Note that domain adapta-tion is different from Semi-Supervised Learning (SSL). SSL methods employ both labeled and unlabeled data for better classification, in which the labeled and unlabeled data are assumed to be drawn from the same domain. Unlike SSL, the key assumption in domain adaptation is that P = Q , but the class conditional distribution of the source and tar-get domains remains unchanged, i.e. , P ( y | x )= P ( y |
Recall that, in domain adaptation, the fundamental ques-tion is how to evaluate the difference in distribution be-tween two domains given finite observations of { x i { x Leibler (KL) divergence) that can be used to measure their distance. However, many of these estimators are paramet-ric and require an intermediate density estimate. To avoid this non-trivial task, a non-parametric distance estimate be-tween distributions is more desirable. Recently, Gretton et al. [5] introduced the Maximum Mean Discrepancy (MMD) for comparing distributions based on the Reproducing Ker-nel Hilbert Space (RKHS) distance. Let the kernel-induced feature map be  X  : R  X  X  ,where H is the corresponding feature space. The MMD between the source domain D S and the target domain D T is defined as follows: The empirical measure of the MMD in (1) is defined as:
Therefore, the distance between two distributions of two samples is simply the distance between the two mean ele-ments in the RKHS.
Due to the change of distribution from different domains, training with samples from the source domain may degrade the generalization performance in the target domain. To reduce the mismatch between the two different domains, Huang et al. [6] proposed a two-step approach Kernel Mean Matching (KMM). The first step is to diminish the differ-ence of means of samples in RKHS between the two domains by re-weighting the samples  X  ( x i ) in the source domain as  X   X  ( x i ), where  X  i is learned by using the MMD criterion in (2).

Then the second step is to learn a decision classifier f ( x )= w  X  ( x )+ b that separates patterns of opposite classes using the loss function re-weighted by  X  i in the objective function.
However, the simple re-weighting scheme may have a lim-ited improvement in the target domain when the dimen-sionality of the data is high. In particular, some features may cause the data distribution between domains to be dif-ferent, while others may not. Some features may preserve the structure of data for adaptation, while others may not. To address this problem, Pan et al. [11] proposed Maximum Mean Discrepancy Embedding (MMDE) for domain adapta-tion by embedding both the source and target domain data onto a shared low-dimensional latent space. The key idea is to formulate this as a kernel learning problem using the kernel trick K ij = K ( x i ,x j )=  X  ( x i )  X  ( x j ), and to learn the kernel matrix defined on all the data: where K S,S , K T,T and K S,T are the Gram matrices defined on the source domain, target domain, and cross domain data, respectively. By minimizing the distance (measured by MMD) between the source and target domain data. The square of the MMD in (2) can be written as where This leads to a Semi-Definite Programming (SDP) problem.
After that, the embedding of data can be extracted by performing eigen-decomposition on the learned kernel ma-trix K in (3), and can be used for training classifiers.
In previous domain adaptation methods [6, 11], the weights  X   X  X  in KMM or the kernel matrix K of samples in MMDE are learned separately using the MMD criterion in (2) de-fined on the input data only without considering any labels. While the use of labels in linear discriminant analysis usually helps extract more discriminative features, the label infor-mation from the source domains may be also useful to learn kernels or extract features for a better domain adaptation. In addition, there are two main limitations associated with MMDE. First, MMDE is transductive and cannot generalize on unseen patterns. Second, it requires to solve an expen-sive SDP problem, which takes O (( n 1 + n 2 ) 6 . 5 )timetosolve the optimization problem (4). Although polynomial-time solvers are available, current interior-point methods are still too computationally intensive for large-scale SDPs in real applications. Note that only the low dimensional embed-ding of the data is extracted from the learned kernel matrix K inMMDE,andisthenusedforthetrainingofthede-cision classifiers. Therefore, not all components from the learned kernel matrix K are required to train the classifiers for domain adaptation.
Based on the above discussions, instead of using two-step approaches as in [6, 11], we propose a unified domain adap-tation learning framework to find the discriminative feature subspace  X , and to learn decision classifiers f l ( x ) X  X  simulta-neously. In particular, our proposed method minimizes the distribution difference between the samples of the source and target domains after the projection into the subspace  X ( i.e.  X  x i ), as well as the structural risk functional of the n 1 labeled data from the source domain D S .Moreover,we suppose that the learning problem is in multiclass setting, and there are m decision classifiers f l ( x ) X  X . Let us denote the label indicator matrix as Y  X  R n 1  X  m ,and Y il =1ifthe i -th sample belongs to the l -th class, and 0 if it is labeled as others. Similar to other feature extraction methods, we also suppose  X  is orthogonal on rows so that  X  X  T = I .The optimization problem is then formulated as follows: subject to  X  X  T = I . Here, the first term is the empirical risk functional of the decision functions f l  X  X  on the labeled data from the source domain D S ,and (  X  ) is the empirical loss function. The regularizer  X (  X  ) controls the complexity of f l , and the last term measures the distribution difference between the embedding of D S and D T . Two tradeoff pa-rameters  X &gt; 0and  X &gt; 0 are introduced to control the fitness of the decisions functions, and to balance the differ-ence of distribution from the two domains and the structural risk functional for the labeled patterns, respectively. Hence, using (6), the subspace  X  and the decision functions f l  X  X  can be learned at the same time.
To capture the label dependency, we follow [7] to define the m decision functions: where w l  X  R d is the weight vector for the decision function,  X   X  R r  X  d is the matrix of the shared subspace for the m de-cision functions, and v l  X  R r is the weight vector defined in the projected subspace  X , and u l  X  R d is the weight vector defined in the original input space. With the parametric form (7) of the m decision classifiers, the learned subspace  X  can capture the intrinsic structure of label dependency in multiclass problems [7]. The weight vector v l is the discrim-inative direction in the subspace  X  for each class, while the weight vector u l can be used to fit the residue w l  X   X  v each class independently.

Though we learn a linear shared subspace in (7), the lin-ear subspace is usually more efficient and also achieves good generalization performance for high dimensional data such as text documents. Moreover, one can simply replace the input x by the feature mapped input  X  ( x ) in (7) and ap-ply the Representer Theorem for w l , u l and  X , which gives rise to the kernel variant of the proposed framework for the nonlinear generalization performance, which is beyond the scope of this paper. For simplicity, we use the notation x instead of  X  ( x )inthesequel.
For the empirical loss on the labeled data, we employ the square loss function:
Suppose X S =[ x 1 ,  X  X  X  ,x n 1 ]  X  R d  X  n 1 is the data matrix of the source domain, W =[ w 1 ,  X  X  X  ,w m ]  X  R d  X  m , U = [ u thefirsttermin(6)canberewrittenas:
Based on the parametric form (7) of the decision function f , we introduce the following regularizer: which controls the complexity of each classifier indepen-dently. The second term in (6) can be rewritten as:
Recall that the last term in (6) measures the mismatch between the embedding of the source and target domains. Here, we use the MMD criterion in (4) as the nonparamet-ric measure for the mismatch. Suppose X T =[ x 1 ,  X  X  X  ,x trices defined on the target domain and all input data, re-spectively, and assume  X  ( x )= X  x ,andso K = X  X   X  X . Then, the criterion (4) becomes
Combining all the above, we arrive at the following mini-mization problem: which learns both the shared subspace  X , and the parame-ters W and V in decision functions simultaneously.
In this section, we show that the optimization problem (9) can be solved efficiently by alternatively finding the optimal subspace matrix  X , and the matrices V and W of the weight vectors.
First, we show that the optimal V  X  in the optimization problem (9) can be expressed in terms of  X  and W .
Proposition 1. For the fixed W and  X  , the optimal V  X  that solves the optimization problem (9) is
Proof. Setting the derivative of the optimization prob-lem (9) w.r.t. V to zeros, we have:
Since  X  X  = I , this completes the proof.
Second, we show that the optimal W  X  in the optimization problem (9) has a closed-form solution in terms of  X  and V .
Proposition 2. For a fixed  X  and V , the optimal W  X  has a closed-form solution:
Proof. As shown in the optimization problem (9), the last term does not depend on W , so we can simplify the objective function as follows:
Setting the derivatives of (12) w.r.t. W to zeros, we have: This completes the proof.
 Since the matrix inversion (  X I + X S X S )  X  1 can be pre-computed, and the data matrix X S is usually sparse for text documents, this inversion can be computed by performing Singular Value Decomposition (SVD) on the data matrix X S in O ( dn 1 min( d, n 1 )) time. Using (11) and (10), the update of W can be computed in O ( d 2 m )time.
Moreover, we can show that the optimal  X   X  in (9) can be solved efficiently by performing SVD on a matrix in term of W .

Proposition 3. For a fixed W and V , the optimal  X   X  can be obtained by solving the following SVD problem: and the matrix  X   X  has the rank at most min( d, m +1) .
Proof. As shown in the optimization problem (9), the first term does not depend on  X , and using (10), we can rewrite the objective function as follows:
Moreover, using  X  X  = I , the objective is simplified as: so that we can arrive at the optimization problem (13).
Note that D in (5) can be decomposed as D = ee ,where e  X  R n 1 + n 2 is a vector with the first n 1 entries equal 1 /n and the remaining entries equal  X  1 /n 2 ,andso XDX is of rank one. Moreover, the matrix WW has rank at most min( d, m ). Thus, the matrix  X XDX  X   X WW has rank at most min( d, m +1).
 Combining all of the above, the optimization problem (9) can be solved by updating the matrices W , X ,and V iter-atively until convergence. The detailed algorithm of solving the optimization problem (9) is summarized in Algorithm 1. Moreover, based on the Proposition 3, one can perform SVD on the low rank matrix  X XDX  X   X WW to obtain the optimal  X   X  efficiently. Assuming that the input dimension is very high, i.e. d m , the time complexity is O ( d 2 m )only. The update of V takes O ( dm 2 ) time. Therefore, the overall time complexity of Algorithm 1 is only O ( d 2 m ) assuming the inverse of the matrix (  X I + X S X S ) is pre-computed.
After extracting the shared subspace  X , and the weight vectors w l and v l for each class, one can perform prediction using (7). However, the weight vector w l is learned to min-imize the empirical loss of the labeled data in the source domain D S , and may not be the discriminative direction for the testing data in the target domain D T .
 Recall that the subspace  X  is learned to minimize the MMD criterion in (8), and capture the intrinsic structure of data for domain adaptation. Moreover, the weight vector v is the discriminative direction defined on the projected subspace  X , so the prediction on the testing data in the target domain D T can be performed by a decision classi-fier f Tl ( x )= v l  X  x instead of f l ( x )in(7),and X  v the discriminative direction for the l -th class in the target domain.
One major problem in text mining is the sparsity of fea-tures in the high dimensional space. Specifically, some dis-criminative features occur frequently in the target domain D
T but seldom appear or even are absent in the source do-main D S . For example, for the task of extracting sentences corresponding to job requirements from job Web sites, some common terms may be  X  X ualified X ,  X  X ear X ,  X  X xperience X  and so on. However, some characteristic words are dependent of the job nature. For instance,  X  X PA X ,  X  X A X ,  X  X CCA X  are discriminative terms for the domain of accounting whereas  X  X CP X ,  X  X hysiology X  are discriminative terms for the domain of health care. To address this issue, we develop the follow-ing feature propagation strategy.

According to the discussion in Section 3.2, we can ex-tract a common feature set F from both domains for each specific task l by selecting the features with high weight in  X  v l . Based on the co-occurrence information in the target domain, we can compute the similarity between the com-mon features in the set F and the remaining features (non-common features) in another set  X  F . For each non-common feature, we can sum up its similarity with all the common features. Finally we rank all the non-common features by its similarity with the common feature set in descending order. By selecting the top K high similarity non-common terms, and combining with all the existing common features, we can get a set of characteristic features F c  X  X  X   X  F for the target domain.

Based on the assumption that similar features should have similar prediction power in the target domain, we can con-struct a feature si milarity graph G .In G ,eachvertex v represents a feature, and edge weights are given by a sym-metric matrix E  X  R d  X  d ,whoseentries E uv =  X  u , X  v  X  where  X  ,  X  means that the inner product,  X  i represents the vector of normalized occurrence in the target domain. De-fine the degree of vertex v as d v = define the normalized graph Laplacian matrix:
We also define a column vector  X  =[  X  1 ,..., X  d ]  X  R d representing the discriminative weight vector of characteris-tic features. Intuitively, similar features should have similar weights. Therefore, we introduce a manifold regularizer us-ing the feature graph Laplacian matrix in (14) as: which propagates the weight of the common features to other characteristic features via the manifold structure of the fea-ture graph.
Moreover, we also require the discriminative weight vector  X  be close to the discriminative direction learned for each class in the target domain. Thus, we arrive at the following optimization problem: where the first term minimizes the difference between  X  l  X  v l , and the second term enforces that the assignment of the weight of the characteristic features is propagated from the common features. In addition, the optimization problem (15) can be solved according to the following lemma:
Lemma 1. Let v l be the classifier for the class l on the shared feature subspace  X , therefore the corresponding op-timal  X  l has a closed form in term of  X  and v l .
Proof. We first rewrite the objective function as fol-lows:  X   X 
Setting the derivation of (16) with respect to  X  l to zeros, we have: This completes the proof.
 Therefore, the prediction on the testing patterns in the tar-get domain can be performed by:
However, computing the matrix inversion ( I +  X  L l )  X  1 still computational intensive (with complexity O ( d 3 )). Note that when the predefined parameter  X  satisfies 0 &lt; X &lt; 1, we have the following Taylor expansion:
As L l is usually very sparse, especially when  X  is small, one can approximate ( I +  X  L l )  X  1 as I  X   X  L l and the revised discriminative direction is:
Then the decision function on the testing patterns in the target domain becomes:
As a result, the computation of the prediction is much re-duced.

As discussed above,  X  v l is the optimal discriminative direction of the l -th class in (9). From the propagation of the feature graph G , the discriminative information from other characteristic features F c can be used to compute the weight vector  X   X  L l  X  v l to correct the discriminative direction.
In this section, we study the error analysis of our proposed domain adaptation method in the target domain. First, we denote the labeling function in D T as follows: and h ( x ): X X  X  0 , 1 } is the truth labeling function. Let  X  ( x ) be a continuous loss function defined as: The expected loss of g T in D T is defined as:
Note that f S ( x )= u x + v  X  x is the proposed decision function in (7) for the labeled data in the source domain, then we also define the expected loss of f S in D S as: For simplicity, we denote E x  X  D S =E P and E x  X  D T =E Based on the definition of  X  ( x ) in (17), we know that 0  X  ( x )  X  1. With a mild assumption that  X  H is bounded by a finite number C ,where H is a RKHS, we obtain the following theorem: Theorem 1. Suppose x =1 , the expected loss of g T in D T is bounded by Proof.

The second last inequality holds because of the triangle inequality and the last inequality holds due to Moreover, using the Cauchy-Schwarz inequality, we have: Since x =1,sothat
By the virtual of RKHS property, for any function  X  ( x )in this RKHS, it can be expressed as  X  ( x )=  X ,  X  ( x ) H . Then, we can obtain the following bound: Assume  X  H  X  C , similar to (1), we have:
Substitute (20) and (21) into (19). This completes the proof.

Based on the expected error bound in (18), we can con-clude that minimizing the MMD in (8), the empirical loss of labeled data in the source domain D S , and the regularizer u simultaneously as in (9) can also minimize the expected loss in the target domain D T .
We demonstrate the effectiveness of our proposed domain adaptation method by conducting experiments on various data sets covering two common text mining problems: doc-ument classification and information extraction.
We use the 20-Newsgroup corpus to conduct experiments on document classification. This corpus consists of 18,846 newsgroup articles harvested from 20 different Usenet news-groups. It can be observed that the marginal distributions of the articles among different newsgroups are not identical. There exists distribution shift from one newsgroup to any other newsgroups. However, we observe that some news-groups are related. For example, the newsgroups rec.autos and rec.motorcycles are related to car . The newsgroups comp.sys.mac.hardware and comp.sys.ibm.pc.hardware are related to hardware , etc. Table 1 depicts the detailed infor-mation of the data sets, derived from 20-Newsgroup, used in our experiments. There are four class labels, namely, car , bal l game , hardware ,and OS . For each class label, there are two related newsgroups, and we can select the articles in one newsgroup as labeled data in the source domain and the ar-ticles in the other newsgroup as unlabeled data in the target domain. The data sets NG1-2class, NG2-2class, and NG3-2class have only two class labels. For example, the NG1-2class data set has the class labels car and bal l game .The source domain contains 400 random articles selected from the newsgroup rec.auto and rec.basebal l for the class label car and bal l game respectively. There are 800 articles in to-tal for the source domain. The target domain contains 400 random articles selected from the newsgroup rec.motorcycle and rec.hockey for the corresponding class label car and bal l game respectively. There are also 800 articles in the tar-get domain. The datasets NG4-4class and NG5-4class both have 4 class labels, namely, car , bal l game , hardware ,and OS . The composition of articles in each label in the source and target domains is clearly shown in Table 1. Each article is represented by the vector space model and normalized to unit length.

In order to verify the effectiveness of our method, we com-pare with three typical classification methods: SVM, Trans-ductive SVM, and CDSC as presented in [10]. They repre-sent supervised classification, semi-supervised classification, and a recent domain adaptation method respectively. SVM and TSVM [9] are implemented by 1 SVM light and the pa-rameters are all set as default in the package. The parame-ters setting in CDSC is the same as those reported in the pa-per. For those three comparison algorithms, since they can only handle binary classification, we transform the multi-class problems to the 1-VS-rest problem setting for training. For each data set, we repeated all the algorithms 10 times by randomly sampling the articles in each run and calculate the average performance, so as to decrease the sampling bias.
We adopt the recall, precision, and F1-measure as the evaluation metrics. Recall is defined as the number of arti-cles that are correctly classified, divided by the actual num-ber of articles in each class. Precision is defined as the num-ber of articles that are correctly classified, divided by the number of all the articles predicted as the same class. F1-measure is defined as the harmonic mean of recall and pre-cision. Results of all the methods on all data sets depicted in Table 1 are summarized in Table 2 with the best results http://svmlight.joachims.org showninboldfont. Itcanbeobservedthatthesupervised method, namely, SVM, which trains only in the source do-main and tests in the target domain always gets the worst performance among the four algorithms. Semi-supervised learning method TSVM outperforms the supervised learn-ing method SVM by taking advantages of the unlabeled data in the target domain. Because the articles in the source do-main and target domain are related, then the unlabeled data in target domain will supply some distribution information for the training so as to improve the prediction in the tar-get domain. CDSC has been reported for the good perfor-mance in two-class cross-domain adaptation. Those results are verified again in our experiments especially when the two classes in the target domain are well separated such as the data set NG3-2class. However, for multiclass problems especially when the multiple classes in the target domain are not very easy to separate such as the data set NG4-4class and NG5-4class, the performance of CDSC is not as good as that in two-class problems. On the other hand, our domain adaptation method can get comparable results with CDSC for the well separated two-class problems and achieve better performance for all the other data sets.
We conducted a set of experiments in the area of informa-tion extraction. The objective of information extraction is to extract precise text fragments, which are basically chunks of consecutive tokens, for each field of interest from a semi-structured text document. In our experiments, we aim at extracting the job related information from Web pages in some recruitment Web sites. The fields of interest are job title , company , location , salary , post-date , education , expe-rience ,and duty . The online job advertisement documents were collected from different recruitment Web sites in 3 dif-ferent domains (or industries). Table 3 depicts the details of the collected data. The first, second, and third columns refer to the domain label, domain name, and the number of job advertisements collected in the domain respectively. For each online job advertisement collected, we automatically segment the document into a number of text fragments by considering the document object model (DOM) 2 and extract the text contained in the text nodes of the DOM structure. Long paragraphs contained in text nodes are further seg-mented into sentences by an automatic sentence segmen-tator for finer granularity. The fourth column of the table shows the number of text fragments in the domain after seg-mentation. Each text fragment should be labeled as one of the eight job fields mentioned above, or the  X  X ot-a-field X  la-bel. Two human accessors were invited to manually label all the text fragments in the three domains. If there was any disagreement on the judgment between the two accessors, it was resolved by a discussion among them. The manual label information is used as the ground truth in the experiments.
In each domain, we have conducted different sets of exper-iments to demonstrate the performance and compare with existing methods. The first set of experiment is to use the labeled training example in the source domain and the un-labeled data in the target domain to learn the extraction model using our domain adaptation method. The learned
The details of the document object model can be found in http://www.w3.org/DOM . recall, and F1-measure respectively.
 Table 3: The details of the data collected for the information extraction experiments. model is then applied to the testing data in the target do-main and the performance is measured. For example, let D1 and D2 be the source and target domains respectively. We use the labeled training fragments in D1 and the unla-beled fragments in D2 to learn a model. Then the learned model is applied to predict the fields of the text fragments in the testing data. The other sets of experiments are de-signed in a similar manner as the first set. In the second set of experiments, we use transductive support vector machine for model training. As can be seen, in each training, the total number of text fragments in the source domain and target domain is larger than 10,000. Since CDSC needs to compute and store the pairwise similarity for any two frag-ments, it cannot handle this information extraction data set. We cannot compare with it due to its enormous memory re-quirement. Note that each text fragment is represented by the vector space model and normalized to unit length.
We adopt the recall, precision, and F1-measure as the evaluation metrics. Recall is defined as the number of text fragments that are correctly labeled by our framework, di-vided by the actual number of text fragments. Precision is defined as the number of text fragments that are correctly labeled by our framework, divided by the number of pre-dicted text fragments using our framework. F1-measure is defined as the harmonic mean of recall and precision.
In each set of experiments, we have conducted 6 runs us-ing different combination of the source and target domains. Table 4 depicts the performance of the experiments. In each run, we measure the recall, precision, and F1-measure for each field. The figure in each cell of Table 4 is the aver-age performance among the 8 fields of interest in the corre-sponding experiment. For example, our approach achieves an average precision, recall, and F1-measure of 0.814, 0.845, and 0.825 respectively in the target domain when the source and target domains are D1 and D2 respectively. Our ap-proach achieves an average precision, recall, and F1-measure of 0.820, 0.802, and 0.799. It outperforms TSVM which ob-tains a F1-measure of 0.744.

Figure 1 shows the detailed comparison between our meth-od and TSVM. The x -axis denotes the eight job fields and the y -axis denotes the extraction performance measured by F1-measure. In each plot, we show the F1-measure on each job field when training is conducted in one domain and adapting to the other two domains. For example,  X  X SVM-D1-D2 X  and  X  X ur-D1-D2 X  represent the result of TSVM and our method respectively on the data set in which D1 is the source domain and D2 is the target domain. It can be ob-served that our domain adaptation method can get better Table 4: The extraction performance of different sets of experiments. P, R, and F1 refer to the pre-cision, recall, and F1-measure respectively. extraction performance than TSVM in almost all of the fields in each data set.
In this paper, we present a domain adaptation method by extracting the shared concept space between the source do-main with sufficient labeled data and the target domain with only unlabeled data. In our method, we parameterize the shared space by a linear transformation and finding the opti-mal solution by considering the combination of two criteria: the empirical loss on the source domain, and the embedded distribution gap between the source domain and the target domain. Theoretical analysis of the adaptation error bound in the target domain shows that it can be well controlled by the criteria in our objective function. Experimental re-sults on document classification and information extraction demonstrate that our method can outperform other compet-itive methods in the domain adaptation setting.

In the future, we will extend our method to extract dis-criminative concepts in multiple source domain adaptation problems. Exploration of other domain knowledge for ex-tracting more discriminative concepts is also one of major directions to our domain adaptation method.
