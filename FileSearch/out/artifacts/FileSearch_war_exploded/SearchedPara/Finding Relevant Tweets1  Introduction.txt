 Twitter 1 has now become a popular way of sharing breaking news, personal updates and spontaneous ideas and has been observed to function as a social sensor [1]. The home timeline of a twitter user is a stream of updates from users that she has chosen to follow. A list [2] denotes a group of users, and the stream associated with each list compr ises tweets from users in the group. Home timelines and streams could contain tweet s of users outside their purview whose tweets are re-tweeted (i.e., forwarded) by users that are being monitored by the stream. Despite numerous third-party applications, accessing content outside those generated or re-tweeted by users among lists or followees 2 is limited to keyword or hashtag 3 based search apart from other obvious mechanisms like visiting twitter user profiles to see tweets they have authored. All of these require some effort on the part of the user.

When a user authors a tweet, it provides valuable information about the topic that the user is currently interested in. We argue that tweets from the public timeline, i.e., even those outside the home timeline, that pertain to the topic of the authored tweet could provide information that pertain to the topic of interest (as embodied by the exemplary t weet just authored) of the user. In this paper, we address this problem of finding tweets relevant/related to a particular tweet . Retrieving similar/related entities to a query entity is a classical retrieval problem that has been widely addressed in several other domains such as general relational data [3] and program code [4] among others.

To the best of our knowledge, finding related or relevant tweets has not yet been looked into, in past research. Tweets are short messages limited to 140 characters with metadata such as timestamp, author handle and original author in case of re-tweets. The 140 character restriction poses a major challenge to traditional text search engines that are not robust to noise common in tweet data such as deliberate omissions of relevant words and mispellings. Our specific contributions are as follows:  X  We propose various techniques to asse ss the relevance of a tweet wrt a query  X  An extensive empirical study of such techniques on real-world twitter data  X  We then propose a composite approach for tweet relevance assessment that Given a tweet Q , a set of tweets T and k , we would like to identify an ordered set of k tweets from T , T Q =[ t 1 ,t 2 ,...,t k ], such that the tweets in T Q are similar/relevant to Q . T is a set of tweets from the public timeline and could contain tweets outside the home timeline of the author of Q .Inparticular,we would like to develop a scoring function, S ( Q ,t )thatwouldbeusedtoscoreevery tweet t  X  X  wrt Q , the score being directly related to estimated relevance of t to Q . Informally, T Q contains the top-k tweets according to the S ( ., . ) function ordered in a non-increasing order of their scores. In the rest of the paper, we will use D to denote a large corpus of tweets (that are not in T ) that we will use to compute corpus-based stats to denote gen eric twitter behavior such as word idfs to be used in some of the techniques that we describe. We will use traditional IR quality metrics in our evaluation and will elaborate on them in a later section. We will now outline various intuitive scoring functions for tweets; the scoring functions that we develop exploit one of (1) time, (2) author and social network or (3) content of the tweets in question. 3.1 Time-Based Scoring (TS) Taking cue from reverse chronoligical ordering , the standard presentation mode in most feedreaders and twitter website, time-based scoring uses temporal prox-imity as a proxy of relevance. This leads to an intuitive scoring function, S TS in Table 1; since the absolute time-difference is inversely related to relevance, the  X  1 ensures that the scoring is directly related to the relevance. 3.2 Author Social Network Based Scoring In most datasets that are dumps of just tweets, social network connections (e.g., follows , likes type relationships) are unknown. Thus, we outline a graph structure between twitter users based on the tweets using the following notion of edge: sentTweetTo ( ., . ) denotes that the first user addressed (i.e., sent to or mentioned) the second in a tweet. This, unlike the follows relationship, is undirected; we stick with this formulation for tweet relevance assessment since existence of either way of communication is intuitively indicative of being interested in similar topics. Shared Connections (SC). The immediate neighborhood of a user in a social network largely defines her interests, since those are whom she has directly in-teracted with (according to our way of inducing links). The Jaccard similarity 4 between the set of immediate neighbors between the author of the Q and the candidate tweet t i can then be used as the scoring function S SC ; N ( Author ( t )) denotes the set of immediate neighbors of the author of the tweet t according to Eq. 1. Among tweets that have no share d connections with the query, recency based ordering is employed.
 Graph Distance (GD). Jaccard similarity does not differentiate between tweets whose authors do not have any shared connections with the author of Q . However, tweets from users who are just a few hops away may be more rele-vant than those who are further away; a scoring function that assesses relevance of tweets as inversely related to the distance between authors would incorporate such a notion. Such an intuitive scoring function S GD uses GraphDist ( ., . ), the minimum number of hops between authors, in its formulation. 3.3 Content-Based Scoring The 140 character restriction in Twitter leads to deliberate infusion of various kinds of noise such as missing vowels, unnatural abbreviations and omissions of less informative words [5]. Omissions of words as well as usage of different word variants both aggravate the sparsity problem [6], and hence we explore various kinds of noise-robust processing and similarity measures (e.g., ontology-based techniques etc.) that can uncover latent si milarities in estimating relevance of tweets. The dataset that we work with had URLs in only as little as 0.7% of the tweets, and thus, we omit considering URLs and content of their web pages. tf.idf Cosine (TC). tf.idf is a simple and common scoring function that is used for text processing [7]. S TC denotes scoring according to the cosine similarity of tf.idf vectors between tweets; the IDF being computed over D (Ref. Section 2). Query Centric Similarity (QS). Consider the query tweet Q =  X  X lasts in mumbai !!! :O X  and two candidate tweets t 1 =  X  X umbai blasts again, omg X  and t =  X  X umbai blasts kill at least 10 people X  . S TC would score t 1 higher due to it having a larger fraction of query words. However, besides being relevant to Q , t is seen to provide additional information, whereas t 1 is mostly redundant wrt the query tweet. Since the extra information contained in t 2 may be actually useful, we would like to score t 2 at least as much as t 1 5 . A query centric simi-larity measure that does not discount the score for additional information in a candidate tweet would remedy this problem and score both t 1 and t 2 identically. Such a scoring function is given in S QS ,where f ( t i ,w ) denotes the frequency of w in the tweet t i .
 Edit Distance Based Similarity (ED). Twitter X  X  character restriction leads to authors frequently resorting to vowel dropping and usage of uncanny abbre-viations; parliament is often abbreviated to parlmnt or prlmnt whereas atlntc is used to refer to atlantic . Such shortening of words is detrimental to similarity measures that rely on occurences of the same word in two tweets to quantify relevance. Levenstein distance [8] quant ifies the distance between two tokens as the number of character edits required to transform one string to the other [9]. Such similarities could potentially be robust to various kinds of noise; e.g., ( atlntc and atlantic have a low Levenstein distance of 2). We use sim ( w 1 ,w 2 )todenote 1  X  edfrac ( w and w 2 measured as a fraction of the shorter word. We design a scoring function S
ED that aggregates the similarity between each word in the query with its edit distance based similarity with each word in the candidate. The addition of 1 . 0 to the inner sum is a standard practice to ensure that one of the inner sums evaluating to zero does not lead to an overall zero score.
 Word Co-occurences (WC). Edit distance, while being robust to mispellings, is unaware of semantic relatedness between words. For example, Christmas and Yule 6 would have a low score despite the semantic relatedness being evident due to co-occurence in tweet data (an example tweet reads  X  X ule would be the perfect day to take a day off and do my Christmas baking X  ). In exploiting such word co-occurences, we use conditional probabilities first outlined in [10]. The conditional probability of occurence of a word w given a word w is: where D denotes the large corpus of tweets. Such similarities are then aggregated in usual style to yield the scoring function S ED .
 Reply Correlations (RC). Replies to a user X  X  tweets may intuitively be treated as relevant to a tweet. Since reply tweets are not tagged with the tweet being replied to, we heuristically estimate replies to the author within two hours of authoring a tweet as being replies to t he tweet. An example tweet, reply pair thus extracted reads as [ I am actually really excited for Friday what gunna hap-pen to the government , I just hope there is no election ].Givenasetofsuch[ t, r ] pairs, we outline a similar conditional probability formualtion that estimates the probability of a word w occuring in a reply to a tweet containing w : Such word-correlations across question-answer pairs were used in [11] in building translation models for question answer forums. p ( w | w ) values are then aggre-gated to form the S RC scoring function.
 WordNet Similarity (WS). Semantic relatedness between words may be as-sessed using ontologies such as Wordnet 7 . Among the various similarity measures proposed for quantifying pair-wise similarity between words in WordNet [12], we found the Lesk measure [13] to be most effective i n estimating tweet relevance; lesk ( ., . ) estimates the similarity of a pair of words as being proportional to the extent of overlap of their dictionary definitions. S WS outlines a scoring function that uses the lesk ( ., . ) similarity measure between words. Dataset and Experimental Setup: In the absence of twitter datasets with relevance judgements (such as LETOR 8 document datasets), we create our own dataset from a large twitter corpus of 977252 tweets obtained from Fundacion Barcelona Media 9 . Upon choosing 50 random tweets to represent queries, for each query tweet, we collect 200 tweets from the immediate past, but restrict to those that have a common non-stopword with the chosen query tweet. The common non-stopword restrictio n is applied since choosing tweets using only the recency pa-rameter would invariably lead to mostly irrelevant tweets, leaving us with very few non-zero relevance judgements fo r the same labeling effort. We sought the help of human annotators to judge the binary relevance (either as relevant or non-relevant) of each of the 10 k (i.e., 200  X  50) tweets to their respective query tweet. This effort was sizeable and was spread over 2-3 humans, and altogether took roughly 25 hours; this leads to an average of 9 seconds for labeling each tweet. For each query Q ,the T set is formed by the 200 selected tweets 10 . The corpus, after removal of the 10000 potential candidates, and 50 queries, forms D ,adataset of 967202 tweets. The inability to undertake large-scale labeling due to the human effort involved constrains us to work with the set fo 50 queries.
 Evaluation Measures: We use popular Information Retrieval evaluation mea-sures [14] such as Mean Reciprocal Rank (MRR), Mean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG) and Precision (P) to evalu-ate the effectiveness of the techniqu es. The NDCG, MAP and P measures are computed on the top-k results; we choose k =10 consistently, unless otherwise mentioned. We refer the interested reader to [14] for details of these measures, owing to lack of space. Each of these measures are plotted with their 95% confi-dence intervals in the charts we present. Since we work with a small set of queries, we present analysis of statistical significance when appropriate; our evaluation of statistical significance uses randomization tests [15] with p-value at &lt; 0 . 05. 4.1 Time and Author Based Techniques We plot the various metrics for Time and Author based techniques along with an approach that randoml y retrieves tweets from T .Since T comprises of tweets that have at least one common word, Random measures how well such a lexi-cal constraint works in assessing relevance. TS is seen to be 75% better than Random whereas SC and GD provide up to 35% gains over TS. The performance of TS is indicative of the temporal coher ence of topics in tweets, twitter being considered as a mode of rea l-time communication. Table 2 analyzes the correla-tion between temporal closeness and rele vance of tweets where high accuracies of up to 30% are observed when only tweets within 20 seconds are considered. For the social network based techniques, we found it interesting that twitter users who have as many as 2-3 hops in between still exhibit some similarity in the topics of discussion. An analysis of social network distance and precision in Table 3 suggests that very high accuracies of up to 67% may be achieved when only immediate neighbor X  s tweets are considered.
 4.2 Content Based Techniques An evaluation of the various content-based techniques appear in Figure 2. RC is seen to fare the worst at a mere 50% better than Random . This was found to be due to the high conditional probabilities assigned to exclamation words and wishes (e.g., lol, omg, congrats etc), replies to tweets being short and often comprising such words. We found that 30% replies had a question mark. In analyzing WS that fared marginally better than RC, we examined the top word pairs according to lesk metric, and found [ weekend, week ], [ north, south ] etc to be among them. Tweets that talk about north are unlikely to talk about south , despite these being semantically similar words; thus, semantic word similarity measures are found to have some apparent drawbacks in assessing tweet rele-vance. In a similar analysis of ED that was found to be competetive with WS, we find that there were as many as 9 spurious matches (e.g., [ protest, promise ], [ breaking, being ], [ against, again ]and[ chocolate, coconut ]) among the top-20 pairs assessed as maximally similar wrt edit distance based similarity. Word similarity assessments for WC were found to be much more accurate, leading to an improved overall performance. The top-10 pairs according to the p ( . | . )mea-sure (used in WC) as listed in Table 4 are seen to model word similarity fairly accurately, thus explaining the better performance of WC. TC , the traditional document similarity metric, is seen to surprisingly perform slightly better than WC; this indicates that the semantic similarities estimates of WC, ED etc are offset by spurious matches produced by them. QS is seen to be the best per-forming technique, indiciating that additional information embodied in words that do not occur in the query are indeed perceived as relevant by the user. This validates our hypothesis that twitter users are likely to be interested in new information.
 Statistical Significance (p &lt; 0.05): TC X  X  precision was found to be statis-tically significant over many techniques it outperforms, whereas QS provides statistical significant results on all measures over all techniques except for WC and TC. QS is found to be statistically significant over TS on MAP and NDCG , while being significant over WC on all measures except MRR (Ref. Table 8). Though content based techniques are seen to be most effective in assessing tweet relevance, that the different techniques are designed differently could mean that the techniques may have some orthogonality among them. TS is likely to per-form well for extremely time-sensitive topics like an ongoing sporting contest, whereas social network based scoring ma y perform better during times when the common interest in a social network pea ks (e.g., when the c ommon interest is related to politics, activity would peak when there is an election). Thus, a com-posite technique that is able to identify scenarios where specific techniques (e.g., content-based, time-based etc.) are likely to be more effective and rely on them highly for such cases, is likely to perform better than the separate methods. Correlation Analysis: We now analyze whether there is indeed some orthog-onality among the techniques using a simple correlation analysis. For each tech-nique, we create a vector whose i th value denotes the precision (@10) obtained for the i th query. The Pearson co-efficient 11 among such precision vectors for the top-5 techniques are tabulated in Table 5 with high values in boldface. The cor-relation co-efficient ranges between  X  1 . 0 (inverse correlation, i.e., high precision indexes of one technique corresponding to low precision in the other) and 1 . 0 (denoting direct correlation). However, since correlation co-efficients can only uncover linear relations, inferences based on them need to be taken with a pinch of salt. The techniques may be grouped into two groups of correlated techniques, one with TS and SC, and the other comprising the content based techniques. Any pair from across these groups have a low correlation of 0 . 5-0 . 6; though this is higher than 0 indicating independence, that they are far enough from 1 . 0 provides some hope of orthogonality between them that may be exploited. A Weighting Score: For a scoring function, query combination [ S ( . ) , Q ], we first linearly normalize the score of S ( Q ,t i ) to between 0 and 1 by scaling 12 .We use S N ( . ) to denote this normalized version. If this were to represent the true relevance distribution, it would have a low entropy with all the relevant tweets scoring at 1 . 0 and others scoring at 0 . 0. In the absence of relevance judgements, we could just prefer low-entropy distributions of S N ( . ) since entropy is inversely related to the uniformity (randomness) of the scoring. Entropy, however, is un-able to differentiate between distributions in Table 6. Among those, we would intuitively prefer the right distribution since that has more objects scored close to 1 . 0. We use these intuitions to define a weighting for a [ S ( . ) , Q ]: where the top -k denotes the top -k values in the input set, and the average ( . )and entropy ( . ) denote the average and entropy of the distributions respectively. Both these terms are in the range (0 , 1) and the average of the top -k terms (being the average of the highest k terms in a normalized distribution) is likely to be larger than the entropy. This motivates the subtraction-based construction. An Integrated Approach: The weighting score can be used to linearly com-bine multiple scoring functions, {S 1 ,..., S p } , as follows: we use a cut-off of 0.0 thereby not allowing those ( Q , S i ) combinations that have a weighting score evaluating to negative to influence the scoring.
 Evaluation: We combine the top-5 content based techniques WS , ED , WC , TC and QS along with TS and SC in the above construction to form a combined technique S c . Table 7 illustrates that S c beats the the best performing compo-nent technique on each of the measures, albeit by small margins. This establishes the utility of the weighting score formulation in leveraging the strengths of the various techniques in building a technique that outperforms the components, and establishes the combination as the preferred technique for retrieving sim-ilar/relevant tweets. Table 8 shows that S C is statistically significant over TC and QS on MAP and NDCG; it was found to be statistically significant on all metrics against techniques not listed in the table.

Computational Cost: Each of our separate techniques are in O ( |T| l 2 p )where l denotes the number of tokens in a tweet, and p denotes the number of characters in a token (for edit distance calculations that are linear); this is so since token-pair similarities (e.g., lesk ( ., . )and p ( . | . )) may easily be pre-computed (so are IDF values). Normalization (for S N ) and final scoring each take O ( |T| )inseries, leading to an overall complexity of O ( |T| l 2 p ). It may be noted that l is often 15-20 at max since tweets are limited to 140 characters. The problem of finding similar microblog posts (e.g., tweets) is, to the best of our knowledge, has not received much attention yet. We provide a brief overview of literature under two separate heads. Microblog Processing: Retrieval related tasks that have been attempted on mi-croblog data include identifying re-tweet able tweets [16], non-query specific rank-ing of tweets [17] to replace the reverse chro nological ordering, di versity-conscious retrieval of tweets related to a topic (e.g., oil spill ) [18] and mechanisms for gath-ering user input on tweet ranking [19]. Retrieving extrinsic content such as news articles [20] and RSS feed entries [21] in correlation with tweet information has also been of interest.
 Non-content based approaches in Retrieving Entities: Information retrieval and top-k processing [3] have become pervasive. Approaches that focus on user pro-files or temporal information have been of interest in social media process-ing [22][23]. While immediate followees have been found to be useful to rec-ommend to a user to follow [24], [25] assesses familiarity based evidence to be more useful than network proximity. Another aspect of similarity is that pertain-ing to locations [26]; geo-tagging, however, is not yet very popular on twitter. Incorporating temporal information in collaborative filtering has been of utility in retrieving timestamped entities [27]. We analyzed the problem of finding similar/relevant microblog posts, to a query post. Similarity search in microblog posts, to the best of our knowledge, has not received enough attentio n. Microblog posts in the popular microblogging service, Twitter, are short text snippets with associated metadata such as the timestamp and author handle. Towards ret rieving relevant tweets, various intu-itive techniques that separately exploit content and metadata such as timestamp and author social network were developed. An evaluation on real-world data con-firms that content-based tec hniques are most effective and that tweets containing new information while being lexically similar with the query are perceived to be very useful. Based on a correlation analysis of the techniques, we then formu-lated a weighting score that heuristica lly estimates the effectiveness of specific techniques for given queries and used that to build a composite scoring function that assesses relevance using a query-sp ecific linear combination of the different schemes. An empirical evaluation illustrates the statistically significant superi-ority of the composite technique over the component techniques, and establishes it as the technique of preference for assessing tweet relevance.

Many recommendations are accompanies by an interpretable reason e.g., gmail 13 explains why it marked email threads as interesting by including an explanation indicating the reason. We would like to develop techniques to derive interpretable explanations to accompany suggestions of relevant tweets .Further, as and when geo-encoded tweet data are available, we intend to explore the utility of geo-information in assessing tweet relevance.
