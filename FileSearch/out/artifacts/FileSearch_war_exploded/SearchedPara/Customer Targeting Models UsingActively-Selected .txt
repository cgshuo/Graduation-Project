 We consider the problem of predicting the likelihood that a company will purchase a new product from a seller. The statistical models we have developed at IBM for this pur-pose rely on historical transaction data coupled with struc -tured firmographic information like the company revenue, number of employees and so on. In this paper, we extend this methodology to include additional text-based feature s based on analysis of the content on each company X  X  website. Empirical results demonstrate that incorporating such web content can significantly improve customer targeting. Fur-thermore, we present methods to actively select only the web content that is likely to improve our models, while reducing the costs of acquisition and processing.
 I.2.6 [ Artificial Intelligence ]: Learning; I.5.1 [ Pattern Recognition ]: Models Algorithms, Economics, Experimentation, Management Text categorization, web mining, active learning, active f eature-value acquisition
Over the past decade, the Internet has become an increas-ingly dominant channel for companies of all sizes to trans-act business. Just as importantly, companies now use their websites to communicate essentially everything that they want the world to know about them. Companies that sell  X 
Current address: Department of Statistics, Tel Aviv Uni-versity, Israel products go to great lengths to describe these products with the hope of connecting to potential buyers through search engines and sales bots. Companies that sell services simi-larly describe their offerings with the same end objective. I n building increasingly comprehensive websites, literally mil-lions of companies around the world are implicitly commu-nicating insight about their own business requirements and strategies. In comparison with information that can be pur-chased from various third-party data vendors, this insight is likely to be more reliable, because it is generated directly by the company in question.

Indeed, if properly analyzed, this web content can be of enormous value to another class of companies, namely those that sell goods and services to other companies. Like any company that sells to consumers, these business-to-busine ss sellers seek to target their offerings to segments of custome rs (companies in this case) that meet certain firmographic re-quirements like industry (e.g.  X  X inancial Services X ) and c om-pany size (e.g.  X  X nnual revenue between $50M and $500M X ). Many current analytical approaches to this targeting prob-lem are built largely on such firmographic segments. As an example, let us assume that we are selling a risk management application that is designed to integrate other, existing r isk models. We could target all companies in the above firmo-graphic segment. However, it is very likely that companies with a strong relevance to  X  X isk management X  on their web site would be better prospects. Hence, it seems very plausi-ble that the utility of such targeting models could be greatl y enhanced by augmenting the firmographic view with what the company says about itself on its website.

In this paper we present a case study which examines the contribution of website content to improving the per-formance of our existing targeting models, supporting the search for new customers for IBM products. IBM offers its customers a wide range of Information Technology (IT) products, ranging from servers like System P, through cor-porate software offerings like Rational, to high-end servic es in IT and business transformation. We have developed a tar-geting tool called OnTarget to help in sales efforts of these products, which has been successfully deployed in the last several years. This tool offers propensity estimates , indicat-ing the likelihood that the customer will buy the product. These estimates are the results of predictive modeling effor ts on the combined information from IBM-internal databases (containing mainly sales history) and firmographics , that is, information on the companies which are potential IBM cus-tomers, which is purchased from data collection agencies such as Dun &amp; Bradstreet ( http:www.dnb.com ) and Standard and Poor ( http:www.standardandpoor.com ). Several aspects of the methodology and results of OnTarget are discussed in our previous publications [15, 9].

An important distinction in OnTarget is between two types of propensity models: It is clearly expected that the use of website content for im-proving predictive performance will bring much more value in whitespace models than in existing customer models, and so in this paper we concentrate on examining the effect of utilizing web content on the quality of OnTarget whitespace models. We offer two main contributions: In this section, we describe the relevant details of the OnTarget application and the models it builds. As men-tioned above, more details can be found in [9] and references therein.

The goal of the propensity models is to differentiate cus-tomers (or potential customers) by their likelihood of pur-chasing various IBM products. The whitespace prediction models created in OnTarget currently estimate propensity to buy for ten IBM product families (or brands ). There are five software brands: DB2, Lotus, Rational, Tivoli and Web-sphere  X  covering a wide range of corporate software areas from database management (DB2), through software devel-opment (Rational) to application and transaction infrastr uc-ture (Websphere). There are also four hardware brands, rep-resenting families of server products: Series I, Series P, S eries X and Series Z. Finally, OnTarget also predicts propensity to buy for IBM X  X  Storage solutions brand.
 With multiple geographic areas (the Americas, Europe, Asia Pacific), multiple countries within each geographic ar ea, and multiple product brands, a large number of propen-sity models (currently about 160) are built in each quarter. Given a geographic area and a brand Y, our first step is to identify positive examples and negative examples to be used for modeling. In the whitespace modeling problem, we want to try to understand what drives the decision to purchase brand Y by companies that are not currently IBM customer, and to delineate companies by the likelihood of their pur-chase. Assuming a time period t , we formulate our modeling problem as: The time period t is typically the most recent one or two years. Thus, for the whitespace problem, our positive and negative examples are: Companies that do not fall into either one of these categorie s play no role in this modeling problem.
 Next, we define the variables to be used in modeling. These come from the D&amp;B firmographic data, including: We then use these datasets to build prediction models, using mostly logistic regression, and these generate the predict ions presented in OnTarget. We refer to these firmographic-based Whitespace models as Firmographic models in the rest of the paper.

OnTarget models are typically evaluated using cross-valid ated lift curves or ROC curves. These approaches have the advan-tage that they are statistically stable, and that they evalu -ate the performance of the models in ranking the companies correctly according to their real propensity to buy; which reflects well the OnTarget goal of being a sales aid, comple-menting the sales representative X  X  personal knowledge wit h reliable rankings of the potential customer among its peers .
For the experiments in this paper, we chose to focus whites-pace propensity models for two brands Rational and Web-sphere . Following the process outlined above, we obtained data sets for these modeling problems. The period t for definition of positives and negatives was two years between 7/1/05 and 6/30/07. The resulting data set for Rational has a total of 506 companies for which we have an iden-tified website, and of them, 77 are positive examples and the other 429 are negatives. The corresponding numbers for Websphere are 494 (total) and 65 (positives).
As mentioned earlier, a company X  X  website is a rich source of information for potential sellers. So, we begin by ana-lyzing what companies say on the their websites, and the relevance of this content to identifying new customers for our specific brands.

For each company in our data, we crawl the corresponding website up to a depth of 4, and merge the content from all downloaded HTML documents into one. In case the com-bined content of a website exceeds one megabyte, we restrict ourselves to the first megabyte of content, in order to filter possible noise from too many irrelevant pages. We then pre-process the text by removing stop words, stemming the words into inflected forms, and filtering out words that ap-pear in less than three web pages. Each company can now be represented as a document in a fixed vocabulary, which we convert into word vectors using the bag-of-words repre-sentation with TF-IDF term weighting [1].

To examine relevant terms, we rank-ordered the 20,687 unique terms from the Rational data set using  X  2 scores [18]. The 20 (stemmed) terms that are most discriminative in identifying positive examples are listed below: interfac, enabl, deploi, scalabl, integr, deploy, simplifi, autom, multipl, platform, configur, sophist, workflow, leverag, inte roper, enterpris, proposit, softwar, partner, strateg
IBM Rational software X  X ffers industry leading proven soft-ware development tools and processes to enhance ... appli-above are consistent with what one might expect to find on the websites of companies that are heavily involved in software development. Hence, the web content does indeed contain very relevant terms that would be helpful in identi-fying new customers for Rational.

We repeated this exercise for the 19,718 unique terms in the Websphere data set, and the top ranked words are: payment, knowledg, electron, elig, transact, surg, sql, ins ur, desir, ensur, banker, equiti, mastercard, backup, purpos, c heck, scalabl, provid, retriev, benefit
IBM Websphere software  X  X rovides a next-generation so-keywords clearly indicate that the websites of Websphere customers are used to support electronic transactions in-volving payment by credit cards such as Mastercard (we note that Visa appears further down the list). Again, the focus of these Websphere positive examples, as measured by the relevant keywords, is well-aligned with the capabiliti es provided by the Websphere product line.

In order to build our propensity models we collect recent firmographic and web content for companies. This data col-lection is done after the positives (actual buyers) have bee n identified using the historical transactional data. Ideall y, we want to build models using the firmographic and web data that existed prior to the purchase. However, this is not feasible since we are evaluating our modeling techniques on historical data. Hence, there is a chance that this data has changed from its state prior to the purchase that produced a positive example. For example, the word  X  X ebsphere X  can occur in the web content simply because that site is de-ployed using the Websphere Application Server as a result of a prior Websphere purchase. Thus, in order to reduce the risk of being biased by websites that may have changed af-ter adoption of our products, we explicitly remove the terms  X  X ational X  and  X  X ebsphere X  from the raw web content.
Given the web content described above, we cast the task of propensity modeling into one of text categorization, i.e ., given a text document representing a company, classify it as a positive or negative example of a potential customer. We can now use one of many text classification methods avail-able to solve this problem. In particular, we experimented with two popular approaches. The first approach is SVM-light [7], which is an efficient and scalable implementation o f Support Vector Machines for text classification. The second approach is Na  X   X ve Bayes using a multinomial text model[10].
We also ran versions of the classification algorithms mod-ified to deal with the high imbalance between the positive and negative class. SVM-light provides a straightforward mechanism for dealing with class imbalance by specifying a cost factor by which training errors on positive examples ou t-weigh errors on negative examples. For Rational , we set this cost factor to 6.5, and refer to this variant as SVM(c=6.5). In the case of Na  X   X ve Bayes, we re-weighted the instances in the training data so that a positive instance has 6.5 times th e weight of a negative instance (which corresponds to the im-balance in this data set). We refer to this approach as Na  X   X ve Bayes (c=6.5). To understand the effect of this reweighting, let us denote the learning sample for our propensity model by { x i , y i } n i =1 , with x i the vector of p features for the in-stance i and y i  X  X  0 , 1 } the binary response. Then looking at the Na  X   X ve Bayes formula:  X  P ( Y = 1 | x ) = where hats above the quantities mean they are estimated from the data and  X   X  1 ,  X   X  0 are the percentage of positive and negative examples in the sample, respectively. By reweight -ing we are only affecting the  X   X  quantities, and in fact making them equal, leading to a  X  X ikelihood ratio X  type probabilit y estimate:  X  P which basically examines which class makes the observed sequence more likely, compared to the original formulation , which significantly downweights the contribution of the sma ller class in the denominator.
Selecting a subset of features for training can significantl y speed up training text classification models, and quite of-ten can improve classification accuracy. A simple approach to feature selection for text is using Document Frequency, which ranks words for selection based on the number of documents in which the word appears. Past studies have shown that this approach is very effective, often comparable feature selection [18, 5, 13].

We compare the different text classification algorithms de-scribed in the previous section, at different levels of featu re selection. Figure 1 shows performance on Rational of the dif-ferent models in terms of area under the ROC curve (AUC) on data with increasing dimensionality. The SVM models perform quite well and are not affected much by feature selection, except when we drop below 2500 features. This seems to be consistent with the observation by Zheng et al. [20], that regularized linear methods, such as SVMs are not helped by standard feature selection methods. Na  X   X ve Bayes, on the hand seems to be quite sensitive to the se-lection of features. The re-weighted Na  X   X ve Bayes is also af-fected, but to a smaller extent. Performance on Na  X   X ve Bayes, in general, improves with fewer selected features, up until we are left with 2500 features. The computational complexity of the Na  X   X ve Bayes algorithm is linear in the size of the vo-cabulary. So reducing the feature space to 2500 features speeds up the text classification 8 times, and additionally produces better models.

Na  X   X ve Bayes trained on imbalanced data produces predic-tions that are biased in favor of large classes, as noted by Rennie et al. [14] and Frank and Bouckaert [6]. Re-weighting instances, as we do for Na  X   X ve Bayes (c=6.5) does indeed sig-nificantly improve on the equally weighted Na  X   X ve Bayes, and furthermore, outperforms SVMs on this data. For the rest of this paper we use this re-weighted Na  X   X ve Bayes approach for the Web-content models.

For Websphere , using Na  X   X ve Bayes (c=10) produces the best models when applied to the full feature set. Perfor-mance on AUC does not drop much when the dimensional-ity is reduced up until 5000 features. However, to get the best propensity models in the next section, we will use the entire feature set for Websphere . Figure 1: Comparison of text classification methods with feature selection on Rational . In Section 2 we described the generation of structured Firmographic models, and in Section 3 we described the use of unstructured text content for building Web-content mod-els. In this section, we describe different approaches to com -bining the structured and unstructured features in order to build more effective propensity models.

We compare the following approaches of incorporating web content into our structured Firmographic model.
Voting: In this approach, we train separate Firmographic and Web-content models on the same training instances, and then combine the class probability estimates predicted by both models on test instances. We consider two variants of this approach: 1) Vote-Avg, in which probability estimates produced by individual models are simply averaged; and 2) Vote-Prod, in which probability estimates produced by in-dividual models are multiplied and renormalized.
 Nesting: In this approach, we use the output of the Web-content model as an input to the Logistic Regression Firmographic model. Specifically, we add another variable in the Logistic Regression, corresponding to the predicted probability that the candidate company is a potential cus-tomer as given by a Web-content model. In order not to bias our evaluation, we build the Web-content model us-ing the same training set as used by the Logistic Regression model. However, in order to train the Logistic Regression on the additional Web-content score, we need to provide val-ues for this variable on the training data. We could do this by training a Web-content model on the training set and providing the scores on the same training data. However, the Logistic Regression trained on this input could be prone to over-fitting. Hence, a better approach is to use cross-validation on the training set to get unbiased scores from a Web-content model, i.e., the training set is further split into 10 folds, and the instances in each fold are scored by a Web-content model that has been trained on the remaining 9 folds. These unbiased estimates are then used as inputs to the Logistic Regression along with all the other structured features.

Table 1 compares AUCs produced by the individual mod-els and the composite models, averaged over 10 runs of 10-fold cross-validation. As indicated in bold, all method s incorporating web content into Firmographic models per-formed statistically significantly better than the compone nt models, based on paired t-tests ( p &lt; 0 . 05). Clearly, there is much value in the website content  X  both by itself, and more so when combined with existing structured content. Overall , Vote-Avg performs best for Rational , and Vote-Prod is the most effective for Websphere . The ROC curves in Figure 2 and 3 show the relative impact of building these composite model over using only firmographics or web content. Figure 2: Comparing composite propensity models with component models for Rational .

In [9], we compared previous firmographic-based OnTar-get models with a baseline model that simply ranks prospects Table 1: Comparing different propensity models in terms of AUC.
 Figure 3: Comparing composite propensity models with component models for Websphere . by a measure of company size. Cross-validation showed that the OnTarget models out-performed the baseline con-sistently. We also compared the models in a more realistic setting, namely their success in predicting actual sales ba sed on model scores computed in a previous quarter [9]. In this case, we also found that the OnTarget model dominantly out-performed the baseline, suggesting that the performan ce advantage, seen in statistical cross-validation, carried over to the actual deployment of the models. Hence, there is an indi-cation that the improved accuracy of the composite models shown in Table 1 can translate to improved performance in an actual marketing application.
We have demonstrated that using web content of poten-tial customers can significantly improve customer targetin g, compared to using purely firmographic data. However, the firmographic data for over 15 million companies is readily available for modeling from the D&amp;B database; which is not the case for their web content. The advantage of the web content data is that it comes at no direct monetary costs, since it is freely available on the Internet. It does, how-ever, entail some significant processing and modeling costs . Namely, we need to: 1. automatically map company names to home page URLs 2. crawl the identified homepages for raw web content, 3. automatically process raw content into a form useful Having larger amounts of web content may lead to better Web-content models, but it also increases the training time for the text-classification models. For the above reasons, it is beneficial to minimize the number of websites that we need to process.

Randomly selecting a subset of websites to drive the Web-content models may be sub-optimal. A better approach would be to use a model, trained on the firmographic and available (partial) web data, to actively acquire the most useful new web content from which to learn. This problem corresponds to the Instance-Completion setting of the ac-tive feature-value acquisition (AFA) task [11, 19, 12]. In o ur specific instantiation of this problem, we have complete fir-mographic information for each training instance, along wi th the class label. However, we begin with a partial (possibly empty) set of web-content features. Our task is to select the next best instance from the set of incomplete instances ( I ) for which to acquire web-content, and add to the set of complete instances ( C ), so as to maximize the marginal improvement over the current model.

We study this AFA task of actively selecting web con-tent in the iterative framework shown in Algorithm 1. Each iteration estimates the utility of acquiring web content fe a-tures for each instance that currently has only firmographic features. The missing web content of a subset S  X  I of in-complete instances with the highest utility are acquired an d added to T (these examples move from I to C ). A new model is then induced from T , and the process is repeated. Algorithm 1 Framework for Active Selection of Web Con-tent Given:
C -set of (complete) instances with both firmographic and web content
I -set of (incomplete) instances with only firmographic features T -set of training instances, C  X  I
L -learning algorithm m -size of each sample
Different AFA methods correspond to different measures of utility employed to evaluate the informativeness of acqu ir-ing features for an instance. Our baseline policy, Random Sampling, selects acquisitions uniformly at random. In pas t work, Error Sampling has been used as an effective approach to AFA, with the objective of maximizing classification ac-curacy [11]. We describe this approach below, and propose two alternative approaches, which may be better suited for our modeling objective of maximizing AUC.
The first active feature-value acquisition policy we explor e is based on the uncertainty principle that originated in wor k on optimum experimental design [8, 4] and has been exten-sively applied in the active learning literature for classi fica-tion, regression and class probability estimation models [ 3, 2, 16]. The notion of uncertainty has been proposed for the acquisition of class labels in the traditional active learn ing setting, but has not been previously used for feature-value acquisition. For a model trained on incomplete instances, acquiring missing feature values is effective if it enables a learner to capture additional discriminative patterns tha t improve the model X  X  prediction. Acquiring feature values for an example is likely to have an impact, if the model is uncertain of its class membership. In contrast, acquiring feature values of instances for which the current model al-ready embeds strong discriminative patterns is not likely t o impact model accuracy considerably. Uncertainty Sampling , is based on this observation.

The Uncertainty utility measure captures the model X  X  abil-ity to distinguish between instances of different classes. F or a probabilistic model, the absence of discriminative pat-terns in the data results in the model assigning similar like -lihoods for class membership of different classes. Hence, the Uncertainty score is calculated as the absolute differ-ence between the estimated class probabilities of the two most likely classes. Formally, for an instance x , let P y the estimated probability that x belongs to class y as pre-dicted by the model. Then the Uncertainty score is given by P y 1 ( x )  X  P y 2 ( x ), where P y 1 ( x ) and P y 2 highest and second-highest predicted probability estimat es respectively. At each iteration of the feature acquisition al-gorithm, complete feature information is acquired for the m incomplete instances with the lowest scores, i.e. the highe st prediction uncertainties. Note that lower scores correspo nd to higher utilities in Algorithm 1.
Prediction uncertainty implies that the likelihood of cor-rectly classifying an example is similar to that of misclass ify-ing it. Hence uncertainty provides an indication of a model X  X  performance and potential for improvement through feature acquisition. A more direct measure of the model perfor-mance and of the value of acquiring missing features for a particular instance is whether the instance has been mis-classified by the current model. Additional feature values o f misclassified examples may embed predictive patterns and improve the model X  X  classification accuracy. Error Samplin g is motivated by this reasoning, and as such prefers to acquir e feature values for instances that the current model misclas -sifies. At each iteration, it randomly selects m incomplete instances that have been misclassified by the model. If there are fewer than m misclassified instances, then Error Sam-pling selects the remaining instances based on the Uncer-tainty score (defined earlier). Formally, the Error Sampling score for a potential acquisition is set to -1 for misclassifi ed instances; and for correctly classified instances the Uncer-tainty score is used. At each iteration of the feature acquisi-tion algorithm, complete feature information is acquired f or the m incomplete instances with the lowest scores.
The Error Sampling approach described above, prefers the selection of incomplete instances that are misclassified by the current model. However, it does not distinguish be-tween misclassified instances. An alternative would be to prefer misclassified instances that are misclassified with h igh confidence, working under the assumption that correcting a more confident error is more likely to improve the subsequent ranking of instances. This approach can be nicely captured by selecting incomplete instances based on their labeled ma r-gin [17]. Formally, the labeled-margin score of instance x is the predicted probability estimates of the correct class an d P incorrect ( x ) is the maximal probability assigned to any in-correct label. We refer to this approach as Labeled-margin Sampling, where at each iteration of the feature acquisi-tion algorithm, complete feature information is acquired f or the m incomplete instances with the lowest labeled-margin scores.
We ran experiments to compare Random Sampling and the three AFA strategies described above. The performance of each method was averaged over 10 runs of 10-fold cross-validation. In each fold, we generated learning curves in the following fashion. Initially, the learner has access to all incomplete instances, i.e., instances with only firmograph ic features. The learner builds a classifier based on this data. For the active strategies, a sample of instances is then se-lected from the pool of incomplete instances based on the measure of utility using the current classification model. T he missing values (web-content features) for these instances are acquired, making them complete instances. A new classifier is then generated based on this updated training set, and the process is repeated. In the case of Random Sampling, the incomplete instances are selected uniformly at random from the pool. Each system is evaluated on the held-out test set after each iteration of feature acquisition. As in [11], the test data set contains only complete instances, since we wan t to estimate the true generalization performance of the con-structed model given complete data. The resulting learning curves evaluate how well an active feature-value acquisiti on method orders its acquisitions as reflected by model AUC. To maximize the gains of AFA we acquire features for a single instance in each iteration, i.e., sample size m = 1.
For the base learner in Algorithm 1 we use the best method for each data set, i.e., Vote-Avg and Vote-Prod for Ratio-nal and Websphere respectively. The results comparing the different active feature-value acquisition approaches is p re-sented in Figure 4 and Figure 5. The results show that, by actively selecting the most informative websites to lear n from, all three AFA approaches build models with higher AUCs for the same number of websites acquired by Random Sampling. In fact, with active selection of only 100 website s for Rational , we can reach close to the performance of us-ing all web content. For Websphere , we can even exceed the performance of a model that uses all web content, by reaching an AUC of 0.827 with only 100 websites. Clearly, acquiring data from all websites is not only unnecessary, bu t also carefully selecting informative web content and ignor ing potentially noisy data can lead to even better models. In Figure 4 for Websphere , we see that Labeled-margin Sampling and Error Sampling clearly outperform Uncer-tainty Sampling. Uncertainty Sampling uses the current model X  X  predicted class probability estimates to acquire i n-formation for instances it is uncertain about. However, it does not make use of the class labels, which are present for all (including incomplete) instances. Error Sampling uses these class labels to determine which incomplete instances are being misclassified by the current model. By preferen-tially selecting these misclassified instances, Error Samp ling improves on Uncertainty Sampling. Labeled-margin Sam-pling goes a step further, by preferentially selecting mis-classified instances that have been classified with high con-fidence. Acquiring more information, via web content, for these instances helps to correct egregious errors in rankin g faster than Error Sampling. For Rational (Figure 5) the dis-tinction between the different AFA approaches is less clear, as the curves cross over at multiple points. However, it is clear that any of these active sampling methods is a bet-ter approach to selecting websites to process than sampling instances uniformly at random. Figure 4: Different active feature-value acquisition methods compared to Random Sampling for Web-sphere .
In this paper, we address the problem of estimating the propensity of a company to buy a new product. In doing so, we demonstrate how the websites of potential customers provide a rich source of information for determining a com-pany X  X  propensity to buy a specific product or brand. In particular, we show how text classification models built on such web content can significantly boost the performance of existing targeting methods that rely solely on firmographic information. Furthermore, we present methods to minimize the cost of acquiring web content, by actively selecting onl y Figure 5: Different active feature-value acquisition methods compared to Random Sampling for Ratio-nal . the most informative websites to process for modeling. The resulting models, using a small subset of actively-selecte d web content, are not only faster and cheaper to build, but can produce targeting models that are as good as (or bet-ter than) models that use content for all companies in the training data.
We would like to thank Maytal Saar-Tsechansky and Fos-ter Provost for valuable discussions and insight on active feature-value acquisition. We would also like to thank Il-dar Khabibrakhmanov, Cezar Pendus, and Yan Liu for their help in data processing. [1] C. Buckley, G. Salton, and J. Allan. The effect of [2] D. Cohn, L. Atlas, and R. Ladner. Improving [3] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active [4] V. Federov. Theory of optimal experiments . Academic [5] G. Forman. An extensive empirical study of feature [6] E. Frank and R. R. Bouckaert. Naive bayes for text [7] T. Joachims. Text categorization with support vector [8] J. Keifer. Optimal experimental designs. Journal of [9] R. Lawrence, C. Perlich, S. Rosset, J. Arroyo, [10] A. McCallum and K. Nigam. A comparison of event [11] P. Melville, M. Saar-Tsechansky, F. Provost, and [12] P. Melville, M. Saar-Tsechansky, F. Provost, and [13] D. Mladeni  X c and M. Grobelnik. Feature selection for [14] J. Rennie, L. Shih, J. Teevan, and D. Karger. Tackling [15] S. Rosset and R. D. Lawrence. Data enhanced [16] M. Saar-Tsechansky and F. J. Provost. Active [17] R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. [18] Y. Yang and J. O. Pedersen. A comparative study on [19] Z. Zheng and B. Padmanabhan. On active learning for [20] Z. Zheng, X. Wu, and R. Srihari. Feature selection for
