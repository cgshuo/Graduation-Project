 In the last decade, Stream Processing Engines (SPEs) have emerged as a new processing paradigm that can process huge amounts of data while retaining low latency and high-throughputs. Yet, it is often necessary to join streaming data with traditional databases to provide more contextual information for the end-users and applica-tions. The major problem that we confront is to join the fast arriv-ing stream tuples with the static relation tuples that are on a slow database. This is what we call the Stream-Relation Join (SRJ) prob-lem. Currently, SPEs use a naive tuple-by-tuple approach for SRJ processing where the SPE accesses the database for every incom-ing tuple. Some SPEs use cache to avoid accessing the database for every incoming tuple, while others do not because of the stochas-tic nature of streaming data. In this paper, we propose a new SRJ operator to facilitate SRJ processing regardless of the cache perfor-mance using two techniques: batching and out-of-order processing . The proposed operator provides an effective generic solution to the SRJ problem and the cost of incorporating our operator into differ-ent SPEs is minimal. Our experiments use a variety of synthetic and real data sets demonstrating that our operator outperforms the state-of-the-art tuple-by-tuple approach in terms of maximizing the throughput under ordering and memory constraints.
 H.2.4 [ Database Management ]: System X  Query Processing ; H.2.4 [ Database Management ]: System X  Query Optimization Optimization Stream Data Processing; Continuous Query Processing; Stream-Relation Join Processing
Stream processing emerged from the need to collect, process, and disseminate data from networks of sensor and other pervasive devices generating the huge amount of data. More recently, this technology has been applied to automated stock trading, real-time video processing, network traf fi c monitoring and operational busi-ness intelligence (BI). Most of these applications are time-critical, and rely on being able to produce immediate results.

While there have been numerous efforts to reduce the query pro-cessing response time in SPEs (e.g., using parallelism), applica-tions are, at the same time, requiring more and more access to the historical information on local or remote relational databases (e.g., business intelligence [8]). Accessing these databases dramatically slows down the performance of SPEs. There are many real-life sce-narios where SPEs need to access historical information for every tuple to enrich the input/output stream. For example, to provide an end-user with meaningful information about an Electronic Prod-uct Code (EPC), here the Radio-Frequency Identi fi cation (RFID) reader must send a query to the database to provide a description of the object.

In order to meet this important requirement, all the major SPEs provide access to relational databases. Figure 1 depicts a scenario where a typical commercial SPE enriches an output stream before delivering the results to the end-user or the application. However, the immaturity of relational access in current SPEs is one of the ma-jor factors why stream engines are not as established as traditional databases. There has attempts to facilitate Stream-Relation Join (SRJ) processing by building streaming engines on top of relational databases known as stream-relational engines (e.g., MaxStream and Truviso). In our previous work we proposed MaxStream [9, 7] which offers a faster solution b y using a persistent option where the input/output stream is persisted into an input/output event ta-ble and then the join between the input/output event table and the static information source is performed locally. Nonetheless, the solution provided by stream-relational engines is limited because they do not serve existing SPEs that are not built on top of rela-tional databases. Furthermore, MaxStream as a federated stream processing engine in tegrates multiple autonomous and heteroge-neous SPEs with traditional databases must rely on the underlying SPEs to perform the SRJ join where the static information is not lo-cally available. In response to these limitations, this work presents a generic solution for SPEs currently in use.

The problem with the state-of-the-art technique in current SPEs we call tuple-by-tuple is that the SRJ operator accesses the database for every incoming tuple, and where the time to process each database request is more than the input rate, there will be a queue of incoming tuples that might violate the expected QoS (Quality of Service). To overcome the problem of accessing the database for every incoming tuple some SPEs use cache (e.g., Coral8 [4]) while others consider the often stochastic nature of streaming data and choose not do use cache (e.g., StreamBase [3]). Figure 1: The typical architecture of a current SPE for SRJ pro-cessing
Another drawback of the conventional approach is that the SRJ operator performs its computation sequentially: as tuples arrive, they are processed by a single fl ow of control. Therefore, once a query for a miss tuple is sent to the database, the SPE is idle while all the hit tuples queue up.

Motivated by the above observations and drawbacks of existing approaches, we propose a new SR J operator that facilitates SRJ processing regardless of the cache performance. It uses a new pro-cessing paradigm based on the conjunction of: batching and out-of-order processing . In batching, rather than sending the individual requests to the local or remote database, we wait until the optimal size is reached (which is determined by our algorithm) and then send requests as one query to the database. Additionally, we in-troduce parallelism, which uses of idle time for the out-of-order processing of hit tuples. Here, hit tuples are processed immedi-ately while miss tuples continue to be processed. In this paradigm we have to deal with disordering anomalies and ensure disordering does not effect the end result. In the light of these problems, this paper makes the following contributions: 1. We propose a new and ef fi cient SRJ operator that facilitates 2. We provide an extensive study on how to deal with disorder-3. We provide an operator that is general and is extensible with 4. We evaluate the effectiveness of our techniques with a de-
The reminder of this paper is organized as follows: In Section 1.1 we de fi ne the problem more precisely and discuss our assump-tions. In Section 2 we provide a detailed de fi nition of t he proposed operator, including the analytical cost model de fi ning the optimal batch size. We then present our experimental study in Section 3, related work in Section 4, and conclude the paper in Section 5.
The problem we address is the joining of a stream of fast com-ing data inside an SPE with a relation residing on a local or re-mote database that is accessible using a standard interface (e.g., JDBC/ODBC). More formally, let S be the in fi nite stream of in-coming tuples and R be a table inside the database. We consider S C R where C is an equality condition over the common at-tribute of S and R .

Given our approach is complementary to the performance of cache, we do not make any assumptions on the distribution of data on
S and R . In this paper, in order to calculate the amount of disor-der caused by out-of-order processing as well as the optimal batch size, we assume the interval between the two consecutive tuples ( and the cost of processing database requests are constant (
We deal with SRJ processing by de fi ning a new operator that uses out-of-order processing and batching to facilitate SRJ processing in data streaming engines. Our operator, as Figure 2 depicts, proposes a new processing paradigm using a form of intra-operator paral-lelism that splits the incoming data into hit and miss tuples using two different threads (out-of-order and batch threads) to process hit and miss tuples separately. We also use a waiting queue to decouple tuple admission from tuple execution performed by an out-of-order thread and batch thread respectively.

Before describing the details of our operator, we have illustrated the key idea by using a simpli fi ed example. Figure 2 shows a snap-shot of our operator at time t . Here, the input stream carrying tuple id ( tid ) is joined with a relation (DescTable) to enrich input stream with descriptive information (Desc) about that tuple. The out-of-order thread receives t 1 and t 2 , then considering the content of cache at this time, the tuples are added to the waiting queue to be processed by the batch thread later. The batch thread dequeues and t 2 into a batch of size 2 which we assume to be the optimal batch size (See Section 2.2 for more detail on optimal batch size). While a batch thread is waiting for the result of the batch query, the out-of-order thread processes t 3 , t 4 and t 5 .
The combination of multi-threading and the queue, as illustrated in Figure 2, allows both the out-of-order processing of hit tuples and the batch thread processing of the miss tuples concurrently. The out-of-order thread is the focal point for managing the incom-ing stream where tuples are carried by the operator X  X  input stream. This thread replaces the main thread of execution used by the con-ventional SRJ operators.
 Algorithm 1: Out-of-order Processing 1 t p  X  S.next (); 2 while t p = null do 3 TotalInputTuples  X  TotalInputTuples ++; 4 / * Counts the number of tuples has 5 W indowRunner  X  W indowRunner +1; 6 / * WindowRunner is to move along the 7 if Mod(TotalInputTuples, Order constraint )==0 8 W indowRunner  X  0; 10 / * CalMax SD () calculates the maximum 11 if t p is hit and Maximum SD &lt; 12 result  X  ProcessOutOfOrder ( t ); 13 / * ProcessOutOfOrder() processes 14 Result  X  Result + result ; 15 else if T p is miss then 16 W aitingQueue.add ( t p ); 17 else 18 W aitingQueue.add ( t p , cache content); 19 t p  X  S.next ();
Before explaining the details of the out-of-order processing algo-rithm and how we deal with disordering, it is necessary to measure the amount of disorder. We use scrambling distance [5] to measure the distance (number of tuples) that a tuple in the result stream has deviated from its original position due to out-of-order processing. More than one pair of tuples could be affected by out-of-order pro-cessing, therefore, we use the maximum scrambling distance of the pair with the longest shift. For example, as Figure 2 shows, assum-ing I=1 second and to process a batch of size 2 takes 4 seconds, here t2 is deviated 3 tuples from its original place (SD=3) whereas the longest shift is for t1 with 4 tuples (SD=4) and is equal to the batch processing time divided by I. Hence, the maximum scram-bling distance is equal to the time needed to process all the batches in the waiting queue divided by the interval rate between the two consecutive tuples ( I )where n is the number of batches: Note that depending on the optimal batch size and the distribution of hit tuples, batches may have different sizes (see Section 2.2 for more detail on the batching algorithm).
 Algorithm 1 shows the pseudo-code of out-of-order processing. In each iteration, out-of-order threads read a new tuple ( S (Line 1). It processes the hit tuple right away if the amount of disorder ( Maximum SD ) generated as the result of out-of-order processing was within the range of order constraint (line 11), oth-erwise it adds the tuple to the waiting queue (line 16). A hit tuple not processed by the out-of-order thread we call a pinned tuple . For pinned tuples, we add the content of its cache line to the wait-ing queue to ensure we have access to cache content of the tuple even if the tuple is evicted while it is waiting to be processed. Pin-ning the cache is an alternative approach for these tuples given we do not make any assumptions on the cache implementation.
Depending on the consumer or consumers of our operator, or-der constraint can be interpreted differently. For breivty, only one case will be discussed where results of the SRJ operator are fed to the operator with a tumbling windowing construct that has only one open window at each point in time. We will leave other pos-sibilities (e.g., result are fed into operator with sliding window construct-more than one open window) for the extended version of this short paper. Operators with tumbling window construct might receive the outputs of our operator either directly or through other order-agnostic operators (e.g., fi lter) along the chain of operators. In both cases, disordering might cause result anomalies as out of order tuples could end up in different windows than they originally belonged to. However, if disordering among the tuples happens within the boundary of a window, we can argue that disordering is tolerated. In order to make sure the amount of disorder remains within the boundary of the window, we de fi ne the order constraint relative to the slide size of the window, where Slide (  X  of the tumbling window:
As explained in our previous work [10], SPEs use either ap-plication time (e.g., Microsoft StreamInsight [2]) or system time (e.g., StreamBase [3]) to construct their time-based windows. In a system-time-based window , windows are constructed based on the system time when the tuples hit each window-based operator. This means when out-of-order tuples enter the next window-based oper-ator or operators, tuples are ordered based on the time they arrive at the operator or operators. For this case, only by limiting the amount of disorder within the boundary of the downstream window or win-dows, no result anomalies can be guaranteed.

On the other hand, the application-time-based window advances its time based on the incoming tuples, that is, opening and closing the window based on the application timestamps. Therefore dis-order among the results even within the boundary of the window might cause result anomalies. Fortunately, most of the SPEs today have a mechanism to deal with disordering at operator level by al-lowing a speci fi ed amount of tuples or time units for disordering. To do so a buffer space is used to buffer the incoming data and re-ordering any out of order data that arrives within that period before the data is processed. For instance, StreamBase supports TIME-OUT and Microsoft StreamInsight supports slack . Besides restrict-ing the order constraint the downstream operator must set its slack value (tuple or time) to the amount of order constraint, this pre-vents result anomalies in the slide size of downstream windowed-operator or operators.
The batch thread in our operator is responsible for processing miss, hit and pinned tuples from the waiting queue. Miss tuples are processed in the form batches and instead of sending individual queries for every miss tuple to the database, we collect miss tuples before rewriting the query to include all the key values using OR conditions. As shown in Figure 2, rather than sending two individ-ual queries for every tid , we rewrite the query and send one query including t 1 and t 2 . After receiving the results of the batch query, we map the batch query from the database with tuples to produce results and update the cache.

At this point a natural question arises: What is the optimal batch size? We de fi ne the optimal batch size to maximize our gain from batch processing using a tuple-by-tuple approach. for processing miss tuples using a tuple-by-tuple approach and pro-cessing miss tuples in the form of a batch. To de fi ne the optimal batch size, we adjust the size of the batch ( k ) so our gain from batch processing is maximized: Here L tuple  X  by  X  tuple ( i ) is the latency of processing tuples using a tuple-by-tuple approach where i is the position of miss tuples in the batch, and C miss is the cost of processing a miss tuple: L batch ( i ) is the latency of the ith tuple inside the batch made up of two components. The fi rst component is the time a tuple stays in the batch, waiting for the batch to fi ll up and, the second component is the time it takes to process a batch: Replacing C batch with ( k  X  C miss ) /r where r is the ratio of C
Simplifying the above equation, the following formula de fi nes our optimal batch size: Here we assume that C miss and I are constant during the query execution, and the value of I is smaller than C miss .Otherwise,if I is larger than the time it takes to process a single miss tuple, in-creasing the batch size beyond one will increase the average tuple delay. Applying these, the above formula suggests that the opti-mal batch size depends on the value of r .Thevalueof r itself is heavily dependent on the data distribution, the type and existence of an index on the join attribute, the state of the buffer inside the database, and the cost of establishing the connection. In reality can be obtained after the system enters  X  X teady state X  (e.g., cache is warmed up). The formula outlines that if the value of r constant with an increasing value of k , then it is always the small-est batch size that gives us the maximum gain. Alternatively, if the value of r increases when k increases, the bigger the batch size gets, the more gain is achieved from batch processing. In reality the fi rst scenario can occur when the communication cost is negligible (e.g., database location is local) and the join attribute is indexed, meaning regardless of the batch size, the batch processing cost is effectively constant. On the other hand, when the communication cost is high or the joint attribute is not indexed, the gain from batch processing increases by increasing the size of batch.

In our operator, as Algorithm 2 shows, the batch thread reads and processes the miss, hit and pinned tuples from the waiting queue. The batching thread admits as many miss tuples from the waiting Algorithm 2: Batching 1 t p  X  W aitingQueue.poll (); 3 if t p is hit or pinned tuple then 4 if batch = null then 5 result  X  ProcessBatch ( batch ); 6 / * ProcessBatch() processes 7 batch  X  null ; 8 Result  X  Result + result ; 9 result  X  Process ( t p ); 10 / * Process() processes the hit or 11 Result  X  Result + result ; 12 else 13 batch  X  batch + t p ; 14 if Batch Size == k then 15 result  X  ProcessBatch ( batch ); 16 batch  X  null ; 17 Result  X  Result + result ; 18 t p  X  W aitingQueue.poll (); queue until it reaches the optimal size (Line 12 -16) or it closes the batch if it receives a hit or pinned tuple (Line 3 -10). Therefore, depending on the distribution of hit and miss tuples the batches may be different sizes.
The throughput rate is a raw performance metric indicating how many stream tuples can be processed in the time unit. Maximum throughput (supported rate) in this paper refers to the maximum rate that each approach can handle, when the experiment gradually increased input rate and has no memory over fl ow. We completed a prototype implementation of our approach simulating the behav-iorofauser-de fi ned stream-relation join operator. This prototype receives an input stream and produces an enriched output stream by accessing a relation residing on MySQL database. Our pro-totype uses an off-the-shelf Java implementation of a cache that maintains its content using a LRU (Least Recently Used) policy. For the tuple-by-tuple approach, we implemented a module that simulates the general behavior of today X  X  SPEs for SRJ process-ing using MySQL database and similar to approach uses the same Java implementation of a LRU cache. To show how our simula-tor simulates a real commercial SPE, we ran a set of experiments comparing the achieved throughput of our simulator with an open source commercial SPE (Esper [1]). The result which are omitted due to the lack of space shows that the achieved throughput for the commercial SPE to be similar to our simulator.

Data sets. We evaluate the performance of our approach on syn-thetic and real-life data of varying characteristics. A detailed de-scription of the synthetic and real-life data sets are presented below. -Synthetic data. Table 1 summarizes the characteristics of the synthetic data sets that we used in our experiments. We assume that the relation ( R 1 or R 2 ) joins with S on a single integer-typed attribute and that the join attribute values of the relation follows a uniform distribution. For the stream, the join attribute followf a Zipf distribution. We present results for several skew values, vary-ing from 0 (corresponding to a uniform distribution) to high skew values (skewed join values). Then to see the affect of optimal batch size, we used two different relations: R 1 and R 2 where R index on the join attribute.
 Parameter Default Value IndexusedonRelationR1 Primary index Key value distribution: R 1 and R 2 Uniform Key value distribution in S Zip fi an 0-1 Join key domain size, R 1 and R 2 Integer [1, 10 10 6 ] Join key domain size on S Integer [1, 10 10 6 ] Number of stream tuples 20M Number of distinct values on S 2M Number of relation tuples: R 1 and R 2 10M Size of Relation: R 1 and R 2 15GB -Weather data. Our real-life data set is based on weather sensor data that measures cloud cover over different parts of the globe [12]. We use measurements from two different months to create a relation and a stream of update tuples, consisting of 10 million tuples each. -TPC-H. We also experimented using a TPC-H data set that we created using a scale factor of 100. We used the Part relation as the disk resident relation, while the fi rst 10 million rows of the LineItem relation was used as streaming data.
Validation of optimal batch size. Our fi rst set of experiments studied the behavior of optimal batch sizes. We validated our op-timal batch size formula presented in Section 2.2 where the join attribute is indexed and where the join attribute is not indexed. The experiments used the tuple-by-tuple approach as the baseline, from which we applied a synthetic data set with the skew equal to 0.5, unlimited order constraint, cache containing 50% (1000000) dis-tinct values on the stream S , and varied the batch size.
Figure 4a depicts the optimal batch size when the join attribute is indexed ( R 1 ). The result presents the optimal batch size in this case to be 30. Increasing the batch size beyond 30 causes the throughput to decrease because the value of r remains constant for batch sizes bigger than 30. On the other hand, Figure 4b shows that when the join attribute is not indexed ( R 2 ), the bigger the batch size gets, the more gain is maid from batch processing. However, the through-put remains constant after k = 4000 given the consecutive number of miss tuples is not enough for the cache hit ratio. We repeated the same experiments for a lower hit ratio (10%), and the through-put increases for higher batch sizes up to k = 7000 and remains constant after this value.

Sensitivity analysis. We use synthetic data sets and compare our evaluation metric (throughput) to the baseline tuple-by-tuple approach. Note that from this point on, we only use R 1 (indexed table) located on the local machine as the relation for our experi-ments.

Varying skew. In this experiment, we vary the skew of the zip fi an distribution of the join attribute values that appear in the streaming relation from 0 (uniform distribution) up to the high skew value of 1. Cache size, order constraint, and batch size are set to 50% (1000000), unlimited and k =30 (optimal).

Figure 4c shows that the gain from our approach decreases with higher a skew. The reason for the poor performance of our ap-proach is because the effect of batch and out-of-order processing lessens when the cache performs better with higher skew data (less number of miss tuples). Despite this trend, our approach maintains a consistently higher throughput for all skew values.

Varying cache size. We repeat the previous experiment but vary the cache size from 10% to 90% of the number of distinct values on stream S . When the skew is 0 and the cache size is 10% as indicated by Figure 5a, it represents the best case and achieves up to seven times throughput compared to the tuple-by-tuple approach. Similarly, and as expected when the cache size or skewness of the data increases, the gain from our approach decreases because the effect of batching and out-of-order processing is less for the higher cache hit ratio.

To demonstrate the individual effect of batching and out-of-order processing in our operator, we ran another set of experiments where the amount of gain from batching (order constraint=0) and out-of-order processing ( k =2 ) are compared to a tuple-by-tuple ap-proach on uniform data (skew=0) over varying cache sizes. As Figure 5b depicts, the maximum gain from batch processing and out-of-order processing appears at the lowest cache size (10%).
Varying order constraint. We demonstrate the effect of order constraint using a scenario where the output stream from our oper-ator is fed into another query with a windowing construct. The amount of order constraint represents the slide size of the win-dow. The results, as shown in Figure 3 show that the throughput increases when the order constraint decreases, indicated by less tu-ples being processed by the out-of-order thread.

Performance of our approach on real-life data sets. As a fi nal experiment, we conducted an evaluation of our approach on both weather and TPC-H data sets. We varied the cache size from 10% to 90% of the distinct values on the stream. We assumed that there was no order constraint, and again, we compared our approach to the tuple-by-tuple approach using the throughput as our evaluation metric. Figure 5c plots the throughput of our approach and the tuple-by-tuple approach on the weather and TPC-H data as a func-tion of cache size. Similar to our experiments on synthetic data, our approach achieves a higher throughput and outperforms the tuple-by-tuple approach.
The problem of joining a streaming relation with a stored one has been explored in the context of active data warehousing [11, 6]. However, given the nature of stream data processing is intrinsi-cally different than an active data warehouse, the approach is often inadequate or incomplete. For instance, maintaining order among the produced results is not an essential requirement for active data warehousing. But besides the ordering issue, the more recent work [6] assumed the join attribute is indexed ( B + tree ) and that the re-lation tuples themselves are stored at the leaves of the B real life scenarios, and because of security reasons and data own-ership, creating the preferred index on the join attribute as well as reading the index fi le in the SPE might not be allowed.
Attention was also given to the problem related to fact that stream data arrive out of order. Li et al. [13], introduced a new architecture for stream systems and out-of-order processing (OOP) that avoids ordering constraints. We believe that these attempts to deal with the out of order stream data are complementary to our approach making the usability of our approach more feasible.
In this paper we have considered an operation commonly en-countered in the context of stream data processing: the join be-tween a fast arriving stream of data and a disk-based relation ac-cessed via standard ODBC / JDBC under the order constraint. We call this the Stream-Relation Join (SRJ) problem. We have pro-posed a novel join operator that functions under a minimum number of assumptions about the stream, the relation and the performance of the cache. The main design consideration of our operator is that its generic enough so that SPEs can incorporate it without making signi fi cant changes. As a result, the cost of incorporating this new operator into other SPEs is minimal. Our operator uses a form of intra-operator parallelism to allow batching and out-of-order pro-cessing able to facilitate SRJ processing. Finally, we have validated our proposal through an experimental study that demonstrates our operator X  X  fl exibility by varying different parameters for a variety of synthetic and real-life datasets.

Future research includes optimal memory allocation between the cache and the waiting queue for cases where the cache is less use-ful (e.g., stream exhibits temporal locality). Also, extending the proposed approach to deal with higher input rates using a princi-pled load shedding technique that approximates results when the memory is limited.
 We would like to thank Nesime Tatbul and Kyumars Sheykh Es-maili for their valuable comments on the earlier version of this pa-per.
