 Item-based approaches based on SLIM (Sparse LInear Meth-ods) have demonstrated very good performance for top-N recommendation; however they only estimate a single model for all the users. This work is based on the intuition that not all users behave in the same way  X  instead there exist subsets of like-minded users. By using different item-item models for these user subsets, we can capture differences in their prefer-ences and this can lead to improved performance for top-N recommendations. In this work, we extend SLIM by com-bining global and local SLIM models. We present a method that computes the prediction scores as a user-specific combi-nation of the predictions derived by a global and local item-item models. We present an approach in which the global model, the local models, their user-specific combination, and the assignment of users to the local models are jointly opti-mized to improve the top-N recommendation performance. Our experiments show that the proposed method improves upon the standard SLIM model and outperforms competing top-N recommendation approaches.
Top-N recommender systems [3] are everywhere from on-line shopping websites to video portals. They provide users with a ranked list of N items they will likely be interested in, in order to encourage views and purchases.

Several algorithms for the top-N recommendation prob-lem have been developed [18], including approaches that use latent-space models and approaches that rely on neighbor-hoods. The latent space methods [7] factorize the user-item matrix into lower rank user factor and item factor matrices, which represent both the users and the items in a common latent space. The neighborhood-based methods [8] (user-based or item-based) identify similar users or items. The latent-based methods have been shown to be superior for solving the rating prediction problem, whereas the neigbor-hood methods are shown to be better for the top-N rec-ommendation problem [4, 8, 13, 16]. Among them, the item-based methods, which include item k-NN [8] and Sparse LIn-ear Methods (SLIM) [16] have been shown to outperform the user-based schemes for the top-N recommendation task.
However, item-based methods have the drawback of esti-mating only a single model for all users. In many cases, there are differences in users X  behavior, which cannot be captured by a single model. For example, there could be a pair of items that are extremely similar for a specific user subset, while they have low similarity for another user subset. By using a global model, the similarity between these items will tend to be towards some average value; thus, losing the high correlation of the pair for the first user subset.

In this paper we present a top-N recommendation method that extends the SLIM model in order to capture the differ-ences in the preferences between different user subsets. Our method, which we call GLSLIM (Global and Local SLIM), combines global and local SLIM models in a personalized way and automatically identifies the appropriate user sub-sets. This is done by solving a joint optimization problem that estimates the different item-item models (global and local), their user-specific combination, and the assignment of the users to these models. Our experimental evaluation shows that GLSLIM outperforms competing top-N recom-mendation methods, reaching up to 17% improvement in recommendation quality.

The rest of the paper is organized as follows. Section 2 introduces the notation. Section 3 presents the related work. Section 4 presents the proposed model. Section 5 presents the evaluation methodology and the datasets. Sec-tion 6 presents the performance of the method. Finally, Section 7 provides some concluding remarks.
All vectors are represented by bold lower case letters and are column vectors (e.g., p , q ). All matrices are represented by upper case letters (e.g., R , W ). For a given matrix A , its i th row is represented by a T i and its j th column by a predicted value is denoted by having a  X  over it (e.g.,  X  r ).
The number of users is denoted by n and the number of items is denoted by m . Matrix R is used to represent the user-item implicit feedback matrix of size n  X  m . It shows which items the users have purchased/viewed/rated. Symbols u and i are used to denote individual users and items, respectively. If user u provided feedback for item i , the r ui entry of R is 1, otherwise it is 0. We will use the term rating to refer to the non-zero entries of R , even though these entries can represent implicit feedback. We also refer to the items that the user has purchased/viewed/rated as rated items and to the rest as unrated items. The set of items that the user u has rated will be denoted by R u . We will use the symbol to denote the Hadamart product (element-wise multiplication).
There has been extensive work in the area of top-N rec-ommendation. Here we present a few notable works in the area that have advanced the state-of-the-art. Deshpande et. al. [8] developed a nearest neighbor item-based approach, which showed that item-based models lead to better top-N recommendation than user-based. Cremonesi et. al. [7] de-veloped the pureSVD method, which uses a truncated SVD decomposition of matrix R to generate the top-N recommen-dations. Their work demonstrated that treating the missing entries as zero leads to better results than the matrix com-pletion approaches. There is also the point of view of the learning-to-rank formulation [17].
Ning et. al. [16] introduced SLIM, which was the first method to compute the item-item relations using statistical learning and has been shown to be one of the best approaches for top-N recommendation. SLIM estimates a sparse m  X  m aggregation coefficient matrix S . The recommendation score on an urated item i for user u is computed as a sparse aggregation of all the user X  X  past rated items: where r T u is the row-vector of R corresponding to user u and s is the i th column vector of matrix S , that is estimated by solving the following optimization problem: The constants  X  and  X  are regularization parameters. The non-negativity constraint is used so that the vector esti-mated contains positive coefficients. The s ii = 0 constraint makes sure that when computing the weights of an item, that item itself is not used as this would lead to trivial so-lutions.
The idea of estimating multiple local models has been pro-posed in the work by O X  X onnor et. al. [6], who performed rating prediction by clustering the rating matrix item-wise and estimating a separate local model for each cluster with nearest neighbor collaborative filtering.

Xu et al. [19] developed a method that co-clusters users and items and estimates a separate local model on each clus-ter, by applying different collaborative filtering methods; including the item-based neighborhood method. The pre-dicted rating for a user-item pair is the prediction from the subgroup with the largest weight for the user.

Lee et al. [14,15] proposed a method that relies on the idea that the rating matrix is locally low-rank. First, neighbor-hoods are identified surrounding different anchor points of user-item pairs, based on a function that measures distances between pairs of users and items and then a local low-rank model is estimated for every neighborhood. The estimation is done in an iterative way where first the latent factors rep-resenting the anchor points are estimated and then based on the similarities of the observed entries to the anchor points, the latent factors are re-estimated, until convergence. A pre-diction is computed as a convex combination of local models, weighted by the similarity of the corresponding local anchor point to the user X  X tem pair whose rating needs to be pre-dicted.

GLSLIM differs from the earlier work in the following ways: (i) In all of the above-mentioned works, only local models are considered; while GLSLIM also computes a global model and has a personalization factor for each user deter-mining the interplay between the global and the local infor-mation. (ii) GLSLIM updates the assignment of the users to subsets, allowing better local models to be estimated. (iii) Lee et. al. [14, 15] use user and item latent factors, while GLSLIM focuses on item-item models. (iv) In [6] the authors use item clusters, in [19] the authors use co-clusters and in [14, 15] they use user-item anchor points. Instead, GLSLIM uses user subsets.
A global item-item model may not be sufficient to cap-ture the preferences of a set of users, especially when there are user subsets with diverse and sometimes opposing pref-erences. An example of when local item-item models (item-item models capturing similarities in user subsets) will be beneficial and outperform the item-item model capturing the global similarities is shown in Figure 1. It portrays the training matrix R of two different datasets that both con-tain two distinct user subsets. Item i is the target item for which we will try to compute predictions. The predictions are computed by using an item-item cosine similarity-based method, in this motivation example.

In the left dataset, (Figure 1a) there exist some items which have been rated only by the users of one subset, but there is also a set of items which have been rated by users in both subsets. Items c and i will have different similarities when estimated for user-subset A, than when estimated for user-subset B, than for the overall matrix. Specifically, their similarity will be zero for the users of subset B (as item i is not rated by the users of that subset), but it will be non-zero for the users of subset A  X  and we can further assume without loss of generality that in this example it is high. Then, the similarity between i and c will be of average value when computed in the global case. So, estimating the local item-item similarities for the user subsets of this dataset will help capture the diverse preferences of user-subsets A and B, which would otherwise be missed if we only computed them globally.

However, when using item j to make predictions for item i , their similarity will be the same, either globally estimated, either locally for subset A, as they both have been rated only by users of subset A. The same holds for the dataset pictured in Figure 1b, as this dataset consists of user subsets who have no common rated items between them.

Although datasets like the one in Figure 1b cannot benefit from using local item-item similarity models, datasets such as the one pictured in Figure 1a can greatly benefit as they can capture item-item similarities, which could be missed in the case of just having a global model.
 (a) Overlapping rated items between user subsets Figure 1: (a) Local item-item models improve upon global item-item model. (b) Global item-item model and local models yield the same results.
In this work, we present our method GLSLIM, which com-putes top-N recommendations that utilize user X  X ubset spe-cific models and a global model. These models are jointly optimized along with computing the user assignments for them. We use SLIM for estimating the models. Thus, we estimate a global item-item coefficient matrix S and also k local item-item coefficient matrices S p u , where k is the number of user subsets and p u  X  { 1 ,...,k } is the index of the user subset, for which we estimate the local matrix S Every user can belong to one user subset.

The predicted rating of user u , who belongs to subset p u for item i will be estimated by: The meanings of the various terms are as follows: The term s li shows the global item-item similarity between the l th item rated by u and the target item i . The term s p u li the item-item similarity between the l th item rated by u and target item i , corresponding to the local model of the user-subset p u , to which target user u belongs. Finally, the term g u is the personalized weight per user, which controls the interplay between the global and the local part. It lies in the interval [0 , 1], with 0 showing that the recommendation is affected only by the local model and 1 showing that the user u will use only the global model.

In order to perform top-N recommendation for user u , we compute the estimated rating  X  r ui for every unrated item i with Equation 3. Then, we sort these values and we rec-ommend the top-N items with the highest ratings to the user.

The estimation of the item-item coefficient matrices, the user assignments and the personalized weight is done with alternating minimization, which will be further explained in the following subsections.
We first separate the users into subsets with either a clus-tering algorithm (we used CLUTO [1]) or randomly. We initially set g u to be 0 . 5 for all users, in order to have equal contribution of the global and the local part and we estimate the coefficient matrices S and S p u , with p u  X  X  1 ,...,k } . We use two vectors g and g 0 each of size n , where the vector g contains the personalized weight g u for every user u and the vector g 0 contains the complement of the personalized weight (1  X  g u ) for every user u . When assigning the users into k subsets, we split the training matrix R into k training matrices R p u of size n  X  m , with p u  X  X  1 ,...,k } . Every row u of R p u will be the u th row of R , if the user u who corre-sponds to this row belongs in the p u th subset. If the user u does not belong to the p u th subset, then the corresponding mating the local model S p u , only the corresponding R p be used. Following SLIM, the item X  X tem coefficient matrices can be calculated per column, which allows for the different columns (of both the global and the local coefficient matri-ces) to be estimated in parallel. In order to estimate the GLSLIM solves the following optimization problem: minimize subject to s i  X  0 , where r i is the i th column of R .  X  g and  X  l are the l regularization weights corresponding to S and S p u  X  p { 1 ,...,k } respectively. Finally  X  g and  X  l are the l ularization weights controlling the sparsity of S and S  X  p u  X  X  1 ,...,k } , respectively.

By having different regularization parameters for the global and the local sparse coefficient matrices, we allow flexibility in the model. In this way, we can control through regular-ization which of the two components will play a more major part in the recommendation.

The constraint s ii = 0 makes sure that when comput-ing r ui , the element r ui is not used. If this constraint was not enforced, then an item would recommend itself. For the exact same reason, we enforce the constraint s p u ii  X  p u  X  X  1 ,...,k } for the local sparse coefficient matrices too.
The optimization problem of Equation 4 can be solved using coordinate descent and soft thresholding [9]. After estimating the local models (and the global model), GLSLIM fixes them and proceeds with the second part of the optimization: updating the user subsets. While doing that, GLSLIM also determines the personalized weight g u . We will use the term refinement to refer to finding the optimal user assignment to subsets.

Specifically, GLSLIM tries to assign each user u to every possible cluster, while computing the weight g u that the user would have if assigned to that cluster. Then, for every cluster p u and user u , the training error is computed. The cluster for which this error is the smallest is the cluster to which the user is assigned. If there is no difference in the training error, or if there is no cluster for which the training error is smaller, the user u remains at the initial cluster. The training error is computed for both the user X  X  rated and unrated items. Algorithm 1 GLSLIM 1: Assign g u = 0 . 5, to every user u . 2: Compute the initial clustering of users with CLUTO [1]. 3: while number of users who switched clusters &gt; 1% of 4: Estimate S and S p u ,  X  p u  X  { 1 ,...,k } with Equa-5: for all user u do 6: for all cluster p u do 7: Compute g u for cluster p u with Equation 5. 8: Compute the training error. 9: end for 10: Assign user u to the cluster p u that has the smallest 11: end for 12: end while
Name #Users #Items #Transactions Density groceries 63,034 15,846 2,060,719 0.21% ml 69,878 10,677 10,000,054 1.34% jester 57,732 150 1,760,039 20.32% flixster 29,828 10,085 7,356,146 2.45% netflix 274,036 17,770 31,756,784 0.65%
Columns corresponding to #users, #items and #trans-actions show the number of users, number of items and number of transactions, respectively, in each dataset. The column corresponding to density shows the density of each dataset (i.e., density=#transactions/(#users  X  #items)).

In order to compute the personalized weight g u , we mini-mize the squared error of Equation 3 for user u who belongs to subset p u , over all items i . By setting the derivative of the squared error to 0, we get: g
The overview of GLSLIM as well as the stopping criterion are shown in Algorithm 1.
We evaluated the performance of our method on different datasets, whose characteristics are shown in Table 1.
The groceries dataset corresponds to transactions of a local grocery store. Each user corresponds to a customer and the items correspond to the distinct products purchased over a period of one year. The ml dataset corresponds to MovieLens 10M dataset [12], which represents movie rat-ings. The jester dataset [10] corresponds to an online joke recommender system. The flixster dataset is a subset of the original Flixster dataset [2], which consists of movie ratings taken from the corresponding social movie site that allows users to share movie ratings and meet friends. The subset was created by keeping the users who have rated more than thirty items and the items that have been rated by at least twenty-five users. The netflix dataset is a subset of the orig-Algorithm 2 LSLIM 1: Compute the initial clustering of users with CLUTO. 2: while number of users who switched clusters &gt; 1% of 3: Estimate S p u ,  X  p u  X  X  1 ,...,k } with Equation 8. 4: for all user u do 5: for all cluster p u do 6: Compute the training error. 7: end for 8: Assign user u to the cluster p u that has the smallest 9: end for 10: end while Algorithm 3 GLSLIMr0 1: Assign g u = 0 . 5, to every user u . 2: Compute the initial clustering of users with CLUTO. 3: while diff &gt; 0 . 01% do 4: Estimate S and S p u ,  X  p u  X  { 1 ,...,k } with Equa-5:  X  user u compute g u with Equation 5. 6: Compute difference in the objective function ( diff ) 7: end while inal Netflix dataset [5], which contains anonymous movie ratings. The subset was created by keeping the users who have rated between thirty and five hundred items. All the ratings were converted to binary ratings, showing whether a user purchased/rated an item or not.
We employed leave-one-out cross-validation to evaluate the performance of the proposed model. For each user, we randomly selected an item, which we placed in the test set. The rest of the data comprised the training set.

We measure the performance by computing the number of times the single left-out item was in the top-N recommended items for this user and its position in that list. The quality measures used are the hit-rate (HR) and average-reciprocal hit rank (ARHR). HR is defined as and ARHR is defined as where  X #users X  is the total number of users ( n ), p is the position of the item in the list, where p = 1 specifies the top of the list, and  X #hits X  is the number of users whose item in the test set is present in the size-N recommendation list.
As our method contains multiple elements, we want to investigate how each of them impacts the recommendation performance. Thus, beyond GLSLIM, we also investigate the following methods:
The top-N recommendation algorithms that we compare against are: PureSVD [7], BPR-MF [17] and SLIM (de-scribed in Section 3.1.1).

PureSVD [7] is a popular top-N recommendation algo-rithm, which estimates the user X  X tem matrix R by the fac-torization: where U is an n  X  f orthonormal matrix, Q is an m  X  f orthonormal matrix and  X  is an f  X  f diagonal matrix con-taining the first f singular values.
 BPR-MF [17] (Bayesian Personalized Ranking  X  Matrix Factorization) is a well-known top-N recommendation method, which uses the bayesian personalized ranking optimization criterion on matrix factorization. The BPR criterion focuses on finding the correct personalized ranking for all items to maximize the posterior probability.
 For PureSVD, we used the SVDLIBC package 1 , for BPR-MF, we used the LibRec package [11] and for SLIM, we used the SLIM package 2 . https://tedlab.mit.edu/  X dr/SVDLIBC/ www-users.cs.umn.edu/  X xning/slim/html
We performed an extensive search over the parameter space of the various methods, in order to find the set of pa-rameters that gives us the best performance for each method.
We only report the performance corresponding to the pa-rameters that lead to the best results. The l 1 and l 2 reg-ularization parameters were chosen from the set of values: on the values: { 2, 3, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100 and 150 } .

For PureSVD, the number of singular values f tried lie in the interval [10 , 5000]. For BPR-MF, the number of factors we used in order to get the best results lie in the interval [1 , 10000]. The values of the learning rate that we tried are: we tried are: { 0 . 0001 , 0 . 001 , 0 . 01 , 0 . 1 } . In this section we present the results of our experiments. The following questions will be answered: (i) How do the proposed methods compare between them? (ii) How does our method compare against competing top-N recommen-dation methods? (iii) What is the time complexity of our method? The comparison of our proposed approaches (LSLIMr0, LSLIM, GLSLIMr0 and GLSLIM) in terms of HR and ARHR is shown in Table 2. Overall, we can see that the general pattern is that GLSLIM is the best-performing method, fol-lowed by GLSLIMr0 and LSLIM, while LSLIMr0 is the ap-proach with the lowest performance.

By comparing these methods, we can see the relative ben-efits provided by the different components of GLSLIM. The comparisons of LSLIMr0 with GLSLIMr0 and also of LSLIM with GLSLIM show the benefit of adding a global model with a personalized weight g u . The comparisons of LSLIMr0 with LSLIM, and also between GLSLIMr0 and GLSLIM demonstrate the benefit of allowing users to switch subsets. We can see that both these components improve the perfor-mance. However, in all of the datasets but ml , the relative gain of considering a global model beyond the local item-item models and also computing a personalized weight g is higher than the gain of allowing users to switch subsets. When all of the components are combined, as in the case of GLSLIM, we get the best performance, both in terms of HR and ARHR. Figure 2: The effect of the number of clusters on the performance
Dataset Cls  X  l  X  l HR Cls  X  l  X  l HR Cls  X  ml 20 5 1 0.329 15 5 3 0.339 15 7 3 1 5 0.335 10 10 7 1 1 0.345 flixster 3 1 2 0.248 3 0.1 3 0.250 3 1 1 5 5 0.254 3 1 1 1 5 0.255 netflix 10 1 5 0.238 10 1 5 0.241 20 1 1 5 5 0.243 5 1 1 5 5 0.245
Dataset Cls  X  l  X  l ARHR Cls  X  l  X  l ARHR Cls  X  ml 25 7 2 0.163 15 7 3 0.167 15 7 7 1 3 0.166 10 10 7 1 1 0.170 flixster 3 0.1 2 0.121 3 1 3 0.122 3 5 5 1 1 0.125 3 1 1 1 5 0.126 netflix 20 0.1 5 0.113 10 3 10 0.114 20 1 1 5 5 0.115 5 1 1 5 5 0.116 the global l 1 regularization parameter  X  g and the local l
HR/ARHR achieved, per dataset. Figure 2 shows how the number of clusters affects the HR for GLSLIM and its variants in the groceries and ml datasets. The trends are the same for the rest of the datasets and for the metric ARHR. We can see that GLSLIM out-performs the rest of the methods for all clusters. Also, we should note that for all datasets GLSLIM can achieve at least 95% of its best performance for only ten clusters, out-performing its closest competing method. This is the case even for the datasets where the best performance occured at a much bigger number of clusters. Figure 3: Comparing CLUTO initialization with random initialization of user subsets
The results presented up to this point have been obtained by initializing the user subsets with a user clustering algo-rithm (we used CLUTO 3 [1]). In order to show that the good performance of GLSLIM is not dependent on the clus-
We used the clustering program vcluster . The similarity function considered was the cosine similarity. All the other parameters were the default ones. tering algorithm used, we present the performance when the initialization of the user subsets is random.
 In Figure 3, we can see for the ml and flixster datasets, the HR achieved across iterations with the two different ways of initialization, for the same regularization and for ten clus-ters. The same trends hold for ARHR and for different reg-ularizations, clusters and the rest of the datasets.
We can see that the HR of the first iteration with random initialization is lower than the HR of the first iteration when initializing with CLUTO. This is expected, as in the first iteration, only the global and local models are estimated; no personalization nor cluster refinement has been done yet. Thus, the local models estimated from CLUTO are more meaningful than the local models on random user subsets.
As the iterations progress and cluster refinement is done, we see that the HR increases. In the converged state, the final HR achieved is very similar with both initializations. However, when starting from random user subsets, more it-erations are needed until convergence. We can then con-clude that our method is able to estimate the local models and reach convergence, even with random initialization.
In order to see how the local models affect the recommen-dation performance, we look at the l 1 norm of the global algorithm and when the algorithm has converged.

Figure 4 shows these l 1 norms for 5, 50 and 100 clusters, for the groceries and ml datasets. We can see that the l norm of the global model S is small and it remains small for all possible clusters and throughout the iterations of the algorithm. For the local models S p u , their l 1 norm is larger than the l 1 norm of the global model. As the number of clusters increases, the l 1 norm of the local models increases. In addition, the l 1 norm of the local models in the converged state is larger than the l 1 norm of the local models in the beginning. This shows that the effect of local information on the models is major and it becomes greater as the iterations progress and as the number of clusters increases. Figure 4: How the l 1 norm of the global model S the algorithm until convergence.
The results presented throughout the paper show the per-formance of our algorithms for a list of size 10. The recom-mendation list can be of different sizes. In this section, we describe how the performance of our method is affected by using lists of sizes 5, 15 and 20 as well. We choose N to be quite small because users do not look past the very top presented recommendations in a list, anyway.

In Figure 5, we can see the HR of GLSLIM, while using the parameters with the best results as presented in Table 2 for the different sizes of top-N list. We can see that as N increases, the performance of our method increases as well, which is expected, as there is higher probability that the hidden item of our test set will be in the top-N list. The impact of the size of the recommendation list N on ARHR is similar to the one shown in Figure 5.
Table 3 presents the performance of the competing al-gorithms PureSVD, BPR-MF and SLIM versus the perfor-mance of our best method, which is GLSLIM. The above-mentioned table presents the best HR and ARHR achieved, along with the set of parameters for which they were achieved.
We can see that GLSLIM outperforms all competing ap-proaches for all datasets, as seen in Table 3. By comparing Tables 2 with 3, we can also see that LSLIMr0, which is our simplest method, still outperforms the best competing ap-proach, which shows that using multiple item-item models helps top-N recommendation quality.
We will use O ( SLIM i ( R )) to denote the computational cost of estimating the i th column of S . Then, the com-plexity of estimating the i th column of S and S 1 ,...,S is O ( SLIM i ( R )) + O ( SLIM i ( R 1 )) + ... + O ( SLIM where R 1 ,...,R k are non-overlapping submatrices of R . Since in order to estimate the i th column of S , we need to touch ev-ery non-zero in the input matrix R , the complexity O ( SLIM is at least linear in the number of non-zeros (nnz). We can then say that the complexity of estimating the i th column for the submatrices R 1 ,...,R k is less than or equal to the complexity of solving it on the matrix R : O ( SLIM i ( R ))  X  O ( SLIM i ( R 1 ))+ ... + O ( SLIM i ( R k )). As a result, the com-plexity of Equation 4 is the dominant term O ( SLIM i ( R )). Since the regression problem of Equation 4 needs to be solved for all m columns (items), the complexity of esti-mating the global and local models is O ( m  X  SLIM i ( R )). The complexity of updating the cluster assignment for each of the n users, after trying to assign them to each of the k clusters (lines 5  X  11 of Algorithm 1), is O ( nmk ), since both the computation of the training error and g u is O ( m ). Thus, the per iteration cost of GLSLIM is O ( m ( SLIM i ( R ) + nk )) . The number of iterations until GLSLIM converges is typi-cally small, as can be seen in Section 6.1.2. In this paper, we proposed a method to improve upon top-N recommendation item-based schemes, by capturing the differences in the preferences between different user subsets, which cannot be captured by a single model. For this pur-pose, we estimate a separate local item-item model for every user subset, in addition to the global item-item model. The proposed method allows cluster refinement, in the context of users being able to switch the subset they belong to, which leads to updating the local model estimated for this subset, as well as the global model. The method is personalized, as we compute for all users their own personal weight, defining the degree to which their top-N recommendation list will be affected from global or local information. We performed different experiments, which show that our method outper-forms competing top-N recommender methods, indicating the value of using multiple item-item models.
 This work was supported in part by NSF (OCI-1048018, IIS-1247632, IIP-1414153, IIS-1447788), Army Research Office (W911NF-14-1-0316), Intel Software and Services Group, and the Digital Technology Center at the University of Min-nesota. Access to research and computing facilities was pro-vided by the Digital Technology Center and the Minnesota Supercomputing Institute. [1] Cluto clustering toolkit. http: [2] Flixster dataset. http://http: [3] G. Adomavicius and A. Tuzhilin. Toward the next [4] F. Aiolli. A preliminary study on a recommender [5] J. Bennett and S. Lanning. The netflix prize. In [6] M. Connor and J. Herlocker. Clustering items for [7] P. Cremonesi, Y. Koren, and R. Turrin. Performance [8] M. Deshpande and G. Karypis. Item-based top-n [9] J. Friedman, T. Hastie, and R. Tibshirani.
 [10] K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. [11] G. Guo, J. Zhang, Z. Sun, and N. Yorke-Smith. [12] J. L. Herlocker, J. A. Konstan, A. Borchers, and [13] S. Kabbur, X. Ning, and G. Karypis. Fism: Factored [14] J. Lee, S. Bengio, S. Kim, G. Lebanon, and Y. Singer. [15] J. Lee, S. Kim, G. Lebanon, and Y. Singer. Local [16] X. Ning and G. Karypis. Slim: Sparse linear methods [17] S. Rendle, C. Freudenthaler, Z. Gantner, and [18] F. Ricci and B. Shapira. Recommender systems [19] B. Xu, J. Bu, C. Chen, and D. Cai. An exploration of
