 This paper proposes a probabilistic model for definitional question answering (QA) that reflects the characteristics of the definitional question. The intention of the defini-tional question is to request the definition about the ques-tion target. Therefore, an answer for the definitional ques-tion should contain the content relevant to the topic of the target, and have a representation form of the definition style. Modeling the problem of definitional QA from both the topic and definition viewpoints, the proposed probabilistic model converts the task of answering the definitional questions into that of estimating the three language models: topic lan-guage model, definition language model, and general lan-guage model. The proposed model systematically combines several evidences in a probabilistic framework. Experimen-tal results show that a definitional QA system based on the proposed probabilistic model is comparable to state-of-the-art systems.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation Definitional question answering, probabilistic model, lan-guage model
Definitional question answering (QA) is a task of answer-ing definitional questions used for finding out conceptual facts or essential events about the question target. The def-initional questions take the form of  X  X hat is X? X  or  X  X ho is X? X , such as  X  X hat is fractals? X  or  X  X ho is Andrew Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. Carnegie? X , and X is called the question target. There are several characteristics of the definitional QA that are differ-ent from those of the factoid QA handling a question such as What country is Aswan High Dam located in? . Defini-tional questions do not clearly imply an expected answer type but contain only the question target (e.g., fractals and Andrew Carnegie in the above example questions), contrary to the factoid questions involving a narrow answer type (e.g., country name for the above example question). Thus, it is difficult to find which information is useful for the answer to a definitional question. Another difference is that a short passage cannot answer the definitional questions because a definition needs several essential information about the target. Therefore, the answer of definitional questions can consist of several component information called information nuggets. Each answer nugget is naturally represented by a short noun phrase or a verb phrase. Good definitional QA systems have to find out more answer nuggets with shorter length.

Related works have been actively carried out, mainly ori-ented by TREC Question Answering track [17]. Most of the approaches rank answer candidates by combining vari-ous information heuristically without any formal model for definitional QA. The heuristic approaches are widely used for easy implementation. These approaches use external re-sources, such as a dictionary or an encyclopedia, and defi-nition patterns for ranking answer candidates [10, 6, 18, 8]. The definition patterns are also used for extracting shorter phrases than sentences [7, 19, 3]. However, since the heuris-tic approaches cannot explain the role of each information in a conceptual model, it is difficult to analyze why the system performance improves or deteriorates. Therefore, it is not easy to make the systems based on the heuristic approaches perform better.

Prager et al. [10] proposed a definitional QA method which splits a definitional question into several auxiliary fac-toid questions and combines the factoid answers into a def-initional answer. The auxiliary questions are generated by using patterns constructed manually according to the type (e.g., person) of question target. However, it is difficult to set the auxiliary questions in advance, because the essential facts or events can be different between the question targets even if the targets have the same type.

Berger et al. [1] and Soricut &amp; Brill [14] applied transla-tion model to QA, considering the question and answer as a target and a source language, respectively. They collected the bulk of question-answer pairs like FAQ (frequently asked question) list, and trained the probability that each word in the answer is translated to each word in the question. The translation model is a formal model for QA, but it is dif-ficult to use the model for definitional QA. The model re-quires a large amount of training corpus and complex word alignment between the question and the answer. The word translation probability does not seem to be well estimated for definitional QA, because the definition question is very short (i.e., even the only question target) and the answer is very long. Moreover, it is hard to use other information than the translation probability in the translation model.
We propose a formal model for definitional QA, consider-ing the characteristics of the definitional questions. A def-initional question can be interpreted as the request for the definition about the question target. Therefore, the answer for the question should be a definition-style representation form related to the question target. We model the defini-tional QA from the two points of view, topic and definition. The proposed model can naturally incorporate various in-formation during the topic and definition modeling.
We explain our intuition and the proposed model in detail in Section 2 and describe a definitional QA system based on the proposed model in Section 3. Experimental results and discussions are presented in Section 4. Finally, we conclude our work in Section 5.
A definitional question such as  X  X hat is X? X  or  X  X ho is X? X  is a representation for user information need  X  X ind the definition about X X . The definition 1 about X consist of conceptual facts or principal events that are worth being registered in a dictionary or an encyclopedia for explaining X. Provided that D is a set of definitions about all possible targets, the definition about X is a subset of D containing the related to X. Since the topic of a question target is repre-sented by facts or event related to the target, the definition about X can be considered as the definition that represents the topic of X.

We can suppose that the following sentences are answer candidates for a question  X  X hat is NASA? X .
 S1: NASA is the agency responsible for the public space pro-S2: NASA was established in 1958.
 S3: The headquarters of NASA is located in Washington, S4: NASA announced the new annual budget.
 S5: John who works for NASA gave a housewarming party S6: Ji-Sung Park is a famous football player from South Among the candidates, the sentences representing the topic of  X  X ASA X  are { S 1 ,S 2 ,S 3 ,S 4 } , and those representing the definition are { S 1 ,S 2 ,S 3 ,S 6 } . Although the sentence S5
The definition is interpreted in a broad sense according to the TREC 2003 QA track guideline[16]. contains  X  X ASA X , it is about John X  X  housewarming party, and it is of no use as a definition about  X  X ASA X . The sen-tence S4 represents the topic of the target, but it cannot be considered as a definition sentence. On the other hand, the sentence S6 is a definition sentence, but it does not represent the topic of the target. Therefore, it is reasonable to think of { S 1 ,S 2 ,S 3 } as the answer to the question. The example shows that it is necessary for a definitional QA system to see if each answer candidate represents both the topic related to the question target and the definition.

Given that T is a set of answer candidates describing the topic related to the question target and D is a set of answer candidates representing the definition, the answer A to a definitional question about the target is the intersection of T and D .
 We design a probabilistic model for definitional QA from this point of view.
Given a definitional question Q about the target X and a sentence S 2 ,wehavethefollowingevents:
As mentioned above, the answer to the definitional ques-tions consists of the sentences which do not only describe the contents related to the topic of the target X , but also represent the definition. Therefore, definitional QA can be considered as a task of finding the sentences which maximize a joint probability P ( T,D | S )where P ( T,D | S ) is the proba-bility that the sentence S does not only include the contents related to the topic of the target X , but also represent the definition. The probability is rewritten by using the chain rule as follows: where P ( T | S ) is the probability that the sentence S includes the contents related to the topic of the target, and P ( D | T, S is the probability that the sentence S including the contents related to the topic of the target represents the definition.
We assume that we can identify whether a sentence repre-sents the definition or not, regardless of whether the sentence is related to the target topic or not. For example, we can understand that a sentence  X  X opland was born in Brook-lyn. X  represents the definition without regard to the topic that the sentence describes. Even if the target is not known in the sentence (i.e.,  X  X  was born in Brooklyn. X ), the sen-tence can be considered as a definition sentence. Therefore, assuming T and D are conditionally independent given S , we can simplify the probability P ( T,D | S ) as follows:
We use a sentence for convenience, although any text seg-ment can be valid. Equation 3 is expanded into equation 4 by using Bayes X  the-orem.

In order to answer a definitional question, it is necessary to select the sentences that deserve to be the answer by cal-culating the probability P ( T,D | S ). The task of classifying whether each sentence deserves to be the answer or not ac-cording to the probability P ( T,D | S ) can be regarded as the task of ranking the sentences according to the probability. The top ranked sentences are selected for the answer. It is not necessary to calculate the exact probability in order to rank the sentences. The equation can be simplified by order-preserving transformation instead. A scoring function used for ranking the sentences is defined as follows: As P ( T )and P ( D ) in equation 4 do not have an effect on the rank under the condition that a score is assigned to each sen-tence for a given question, the equation can be transformed into equation 5. P ( S | T ) is the probability that the sentence S is generated from the target topic, and P ( S | D )isthe probability that S is generated from a set of definition rep-resentations. P ( S ) is the prior probability of the sentence S .

If the sentence S is a sequence of words w 1 w 2  X  X  X  w n ,the function is rewritten as follows: P ( w 1 ,n | T ) is the probability that the word sequence w 1 ,n generated from the targ et topic, and we call it topic language model . P ( w 1 ,n | D ) is the probability that w 1 ,n is generated from the definition representations, and we call it definition language model . P ( w 1 ,n ) is the prior probability of we call it general language model . A score is assigned to each sentence by combining the probability of the three language models. Different formulations of the model for definitional QA are possible according to the way each probability is estimated.
The probability P ( w 1 ,n ) is rewritten by using the chain rule as follows: Assuming the word occurrences are independent of one an-other, the probability is calculated by the following equa-tion: The prior probability P ( w i )ofaword w i is estimated by MLE (maximum likelihood estimation) based on the whole collection where the answer is searched: where the C ( w i ) is the occurrence count of word w i in the whole collection. The probability of the general language model is also used for the smoothing of probabilities in other language models.
The probability P ( w 1 ,n | T ) of the topic language model is rewritten by using the chain rule and the independence assumption of word occurrence as follows: If we had a set of ideal sentences describing the topic T the question target X , we could model the target topic from the sentences. Alternatively, we model the target topic using the following several evidences:
The topic language model is estimated by linear interpo-lation of the above evidences as follows: P that word w i isgeneratedfromthetoprankeddocuments R , external definitions E , and web pages W , respectively. The  X  ,  X  ,and  X  are interpolation parameters, and they are empirically set satisfying  X  +  X  +  X  =1.

Each probability is estimated by Dirichlet smoothing [5] which is known to be useful in information retrieval based on language model [4].
 of w i in R , E ,and W , respectively. P ( w i ) is the prior prob-ability of w i calculated in the general language model, and  X  is a parameter for Dirichlet smoothing. According to Zhai &amp; Lafferty [4], the value of  X  for a high performance in information retrieval is between 500 and 10,000, and the performance is robust with 2,000 of  X  .
The probability P ( w 1 ,n | D ) of definition language model is rewritten by using the chain rule and independence as-sumption of word occurrence as follows: The definition corpus is necessary for definition modeling, and the definitions can be collected from the online dictio-nary or encyclopedia. We constructed the definition corpus by collecting the definitions for arbitrary definition targets from the online resources, and estimated the probability us-ing the definition corpus.

The word probability distribution can be different depend-ing on the domain of the definition target. For example,  X  X resident X ,  X  X cientist X ,  X  X orn X , and  X  X ied X  will frequently occur in the definition for a person. On the other hand,  X  X s-tablished X ,  X  X ember X ,  X  X eadquarters X , and  X  X ranch X  will do in the definition for an organization. Moreover, there may be some words that reflect the definition itself without regard to the definition domain. Therefore, the definition language model is estimated by the linear interpolation of the definitions for the domain of question target and the definitions for all domains.
 P where P ( w i | D t X ) is the probability that word w i is gener-ated from the definition corpus whose domain for definition target is equal to the domain t X for question target, and P ( w i | D all ) is the probability that w i is generated from the definition corpus for all domains.  X  is a interpolation param-eter whose value is empirically determined. We used three domains in this paper: person, organization, and term. 3
Each probability is estimated by Dirichlet smoothing as follows: where C D t X ( w i ) is the occurrence count of w i in the defi-nition corpus whose domain is t X ,and C D all ( w i )isthatin the whole definition corpus. P ( w i ) is the prior probability of w i calculated in the general language model, and  X  is a parameter for Dirichlet smoothing. Our proposed probabilistic model handles the definitional QA in terms of topic and definition. As the definitional QA is separated into two points for finding the answer, topic
Targets in TREC 2004 were classified into the three do-mains.
 Figure 1: Architecture of definitional question an-swering system relevance and definition representation, the model has an advantage in the aspect of easy estimation of the probabili-ties.

Moreover, the model can be easily extended to other de-scriptiveQA.Ifthequestionis X  X hy  X  X  X  ? X , the QA for the question can be modeled in terms of topic and reason. Given a binary random variable H which is 1 if a sentence represents the reason but 0 otherwise, the answer could be selected from the top ranked sentences according to the joint probability P ( T,H | S ).

The proposed model transforms the definitional QA into the estimation tasks for the three language models: topic language model, definition language model, and general lan-guage model. The model is promising in the sense that the various techniques for the language modeling studied in speech recognition and natural language processing can be applied to it. As shown in equation 5, the general lan-guage model is used for normalizing the probability value. As a consequence, the proposed model prefers the sentences which are more likely to occur i n the target topic and the definition than in the general text. During the probability estimation of each language model, the model is naturally served as a framework for combining various evidences sys-tematically.
In this section, we explain the definitional QA system based on the proposed probabilistic model described in the previous section. An overview of the system is shown in Figure 1.
Given a definitional question, the question target is ex-tracted and its type is identified. Since a definitional ques-tion is simple, it is easy to extract the question target. From a question such as  X  X ho is Andrew Carnegie? X , for exam-ple,  X  X ndrew Carnegie X  is extracted by using simple rules. Moreover, the type of the target is identified by a named entity tagger, BBN Id entifinder [2], and it is used for esti-mating the probabilities of definition language model. We classify the target into three types: person, organization, and term. If a target is not classified into person or or-ganization by the named entity tagger, it is classified into term.
Since calculating the probabilities for all the documents costs too much, relevant documents to the question target are retrieved, and answer candidates are extracted from the retrieved documents. The docu ment retrieval is carried out by using BM25 scoring function of OKAPI [15]. The query consists of each word of the question target.
All sentences in the retrieved passages are usually used as answer candidates. However, a sentence may be too long that it is likely to contain information which is not related to the question target. Thus, we try to extract target-related parts of sentences using syntactic structure of the sentences. If the parts are extracted, they are used as answer candi-dates. Otherwise, the sentences are used as the candidates. We extract noun and verb phrases from the sentences using the syntactic definition patterns [11].
The answer candidates are ranked by using the proposed probabilistic model mentioned in the previous section. In order to keep the probability for each candidate from being too small and calculate it efficiently, we take the logarithm as follows: LDS ( a )=
The answer is selected from the ranked candidates, in or-der. The length of the final answer A is controlled by the score threshold  X  sel as follows: If the target length of the answer is set, the answer is trun-cated to the length.

The redundant candidates are skipped during the answer selection process. The redundancy between two candidates a and a j is calculated by the word overlap as follows: Overlap ( a i | a j )= | a i  X  a j |
Overlap ( a i ,a j )=max( Overlap ( a i | a j ) ,Overlap ( where | a i  X  a j | is the number of common content words in the candidates, and | a i | is the number of content words in the candidate a i . The word overlap between an answer can-didate and the selected answer is calculated as follows: 11.1 Who is the lead singer/musician in Nirvana? 11.2 Who are the band members? 11.3 When was the band formed? 11.4 What is their biggest hit? 11.5 What are their albums? 11.6 What style of music do they play? 11.7 Other Figure 2: Question series for  X  X he band Nirvana X  If the word overlap exceeds the upper threshold (i.e., 0.8), the candidate is considered to be redundant. If the word overlap lies between the upper threshold and the lower thresh-old (i.e., 0.5), the semantic class of the candidate is checked. If there is any selected candidate that shares the synset in WordNet, the candidate is regarded as redundant one.
We have experimented with 50 TREC 2003 topics and 64 TREC 2004 topics, and we found the answer from the AQUAINT corpus used for TREC Question Answering track evaluation. The TREC answer set for the definitional QA task consists of several definition nuggets for each target, and each nugget is a short string.

In TREC 2004, as shown in Figure 2, a topic consists of several factoid/list questions and one definitional question given at the end. The questions are associated with a ques-tion target and assumed to be processed in order[17]. Thus, the gold standard answer for definitional questions about a target does not include the answers for preceding factoid/list questions about the same target. However, the answer for those questions can be considered as the answer for defini-tional questions in order to evaluate definitional QA systems only. Therefore, we expanded the TREC 2004 gold standard answer by adding the answer nuggets for factoid questions of each topic and used the expanded answer to evaluate for TREC 2004 topics. (When we compare our systems with other TREC participant systems, we used the original gold standard answer.) We skimmed through the documents containing the answer string for each factoid question and composed a short phrase manually. For example, a nugget  X  X ormed in 1989 X  is added to the answer for the definitional question about  X  X he band Nirvana X , extracted from the doc-ument containing answer string  X 1989 X  for factoid question  X  X hen was the band formed? X .

The evaluation of systems involves matching up the an-swer nuggets and the system output. Because the man-ual evaluation such as TREC evaluation requires a lot of cost, we evaluated our system using the automatic measure POURPRE [9]. The POURPRE estimates the TREC met-ric, nugget recall, precision, and F-measure, using term co-occurrences between answer nugget and system output. The POURPRE better correlates with the TREC official scores than another automatic measure ROUGE [12] does. There is no rank swap between official scores and POURPRE scores except swaps attributed to error inherent in the evaluation
Definitional questions are called other questions in TREC 2004 Table 1: Performance according to topic modeling: TREC 2003 Table 2: Performance according to topic modeling: TREC 2004 process [9]. F-measure is used for the official measure in TREC evaluation, and  X  is set to three favoring the recall.
For topic modeling, we collected external definitions from various online sites in query time: Acronym Finder, Biog-raphy.com, Columbia Encyclopedia, Wikipedia, FOLDOC, The American Heritage Dictionary of the English Language, Online Medical Dictionary, and Google Glossary. We also used ten web pages retrieved by the Google search engine and five local documents retrieved from the AQUAINT cor-pus for topic modeling. For definition modeling, we con-structed definition corpus from the above sites according to the target type: 14,904 persons, 994 organizations, and 3,639 terms entries. We processed top 200 documents re-trieved in all experiments.
The topic modeling experiments are carried out to find out which information is useful for estimating the topic lan-guage model. Table 1 and Table 2 show the performance of our definitional QA system according to the various pa-rameter settings for TREC 2003 and TREC 2004 questions, respectively. The  X  ,  X  ,  X  are weight of top ranked docu-ments R , external definitions E , and top ranked web pages W , respectively, in equation 7 for the topic modeling.  X  and  X  are set to 0.6 and 2,000, respectively. In order to evalu-ate the ranking performance, the score threshold  X  sel is not applied, and the target length is set to 2,000 byte 5 .
The target length is measured by the number of non-white-space characters. The answer can be shorter than the target length because of the insufficient candidates.
 Table 3: Performance according to definition mod-eling: TREC 2003 Table 4: Performance according to definition mod-eling: TREC 2004
As shown in Table 1, the system using only the exter-nal definition outperforms the other systems. It is probably because there are external definitions for 92% (46/50) ques-tions of TREC 2003 questions. On the other hand, there are external definitions for 86% (55/64) questions of TREC 2004 questions, and the system combining each information, as shown in Table 2, outperforms the others. The experi-mental results for the topic modeling can be summarized as follows:
The definition modeling experiments are conducted to know how much the question target type affects the esti-Table 6: Comparison with top 10 TREC participant systems: TREC 2003 mation of the definition language model. Table 3 and Table 4 show the performance according to the various parameter settings for TREC 2003 and TREC 2004 questions, respec-tively.  X  is the degree to which the target type has an effect on the definition modeling. For a question such as  X  X ho is Andrew Carnegie? X ,  X  is a parameter for the definition corpus of person type. As shown in the tables, the systems reflecting the target type outperform the system which uses the definition corpus as a whole, not considering the target type (  X  = 0). The system considering the target type heav-ily (  X  =1 . 0 for TREC 2003 and  X  =0 . 6 for TREC 2004) performs best.

In order to analyze the reason why the question target type have a such great effect on the system performance, we compared the term probability distribution of each def-inition corpus using Jensen-Shannon divergence (JS diver-gence) [13]. The value of JS divergence lies between zero and two, and closer value to zero means that two input dis-tributions are more similar to each other. Table 5 shows the JS divergences between various distributions where AQ is AQUAINT corpus as a whole, and AQ nyt and AQ apw are NYT and AP part of the AQUAINT corpus, respectively. D all is the definition corpus for all domains, and D person , D org ,and D term are person, organization, and term part of the definition corpus, respectively. The table shows that the term distribution of news text is very different from that of definition text. The divergence (0.1622) between AQ nyt and AQ apw is small, but the divergence (0.4039) between AQ nyt and D all is much larger. On the other hand, the di-vergence between definition corpora is very large according to the definition domain, although they equivalently con-sist of definition text. For example, the divergence between D org and D person is 0.5003, whereas the divergence be-tween D org and AQ apw is 0.4050. The result implies that the term distribution of the definition corpus is very dif-ferent between the target domains. The large difference in the term distribution explains the reason why the system heavily considering the definition type performs so well.
We compared our proposed system with the previous TREC participant systems [16, 17]. For this experiments, the raw Table 7: Comparison with top 10 TREC participant systems: TREC 2004 run files of each system are offered by NIST 6 .Table6and Table 7 show the POURPRE evaluation result of top ten systems and our proposed systems proposed(  X  ,  X  ) of which target length and score threshold is variously set. For ex-ample, proposed(1000,10) is the system whose target length and score threshold is set to 1,000 byte and 10, respectively. Because the responses of the TREC participant systems are generated under the condition that the answers for defini-tional questions do not have to include the answer for other preceding questions, we evaluated the systems with the orig-inal TREC 2004 gold standard answer. Our systems may be slightly underestimated for TREC 2004 questions because ours do not consider other types of questions. The tables show that our system is comparable to the state-of-the-art definitional QA systems.
We proposed a probabilistic model for definitional QA, analyzing the problem into two main components, topic and definition. With the separation of the topic model and the definition model, the definition QA system can estimate each model effectively. The experimental results show that the external definition which has almost no noise is the most valuable information for topic modeling, and that the top ranked documents and web pages complement insufficient coverage of the external definition. The definition corpus is used for estimating the term probability of the definition language model. Because of the large difference in term distribution between the definition domains, it is reasonable to estimate the probability by using the dynamically selected definition corpus based on the question target type.
The proposed model can be easily extended to other de-scriptive QA by replacing the definition model with a new model customized to it. Moreover, as the task of QA is transformed into that of estimating three language models, various techniques related to the language modeling can be applied to the model.

For the future work, we will estimate the probabilities of the language models using more contexts. Furthermore, we will extend our model to other question types and combine the models for each question type into a general model for descriptive QA. http://trec.nist.gov/ [1] Adam Berger, Rich Caruana, David Cohn, Dayne [2] D.M.Bikel,R.L.Schwartz,andR.M.Weischedel.
 [3] S. Blair-Goldensohn, K. R. McKeown, and A. H. [4] Chengxiang Zhai and John Lafferty. A study of [5] David.J.C.MacKayandLindaC.BaumanPeto.A [6] A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu, [7] S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, [8] Horacio Saggion and Robert Gaizauskas. Mining [9] Jimmy Lin and Dina Demner-Fushman. Automatically [10] John Prager, Jennifer Chu-Carroll, Krzysztof Czuba, [11] Kyoung-Soo Han, Young-In Song, Sang-Bum Kim, [12] C.-Y. Lin. ROUGE: A package for automatic [13] J. Lin. Divergence measures based on the Shannon [14] Radu Soricut and Eric Brill. Automatic question [15] S.E. Robertson, S. Walker, and M.M.
 [16] E. M. Voorhees. Overview of the TREC 2003 question [17] E. M. Voorhees. Overview of the TREC 2004 question [18] Wesley Hildebrandt, Boris Katz, and Jimmy Lin. [19] J. Xu, A. Licuanan, and R. Weischedel. TREC 2003
