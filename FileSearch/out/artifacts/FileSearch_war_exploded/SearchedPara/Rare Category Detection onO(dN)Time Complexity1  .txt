 Emerging from anomaly detection, rare category detection (in short as RCD hence-force) [8, 9, 15] is proposed to figure out which rare categories exist in an unlabeled data set with the help of a labeling oracle. Differ ent from imbalanced clustering or classi-fication [3], RCD verifies the existence of a rare category by finding out at least one data example of this category. This work has a wealth of potential applications such as network intrusion detection [13], financial security [2], and scientific experiments [15].
Generally, RCD is carried out by two phases [11], i.e., (1) analyzing characteristics of data examples in a data set and picking out candidate examples with rare category characteristics such as compactness [4, 5, 7, 1 1, 12, 17] and isolation [11, 17], followed by (2) querying the category labels of these candidate examples to a labeling oracle (e.g., a human expert). The first phase involves processing a big amount of data, facing an efficiency challenge which aims to achieve low time complexity; while the second phase involves limited labeling budget, leading to a query challenge which aims to find out at least one data example for each rare category with as less queries as possible.
Most of the existing approaches (e.g., see [4, 12, 15, 17]) focus on query challenge without addressing too much of efficiency challenge with a time complexity not less than O ( dN 2  X  1 d ) . To address both query and efficiency challenges simultaneously, we iFRED algorithm on O ( dN ) time complexity which is linear w.r.t. either d or N .This is done by utilizing Histogram Density Estim ation (HDE) to estimate local data den-sity and identifying candidate data examples of rare categories through the abrupt den-sity changes via wavelet analysis. On the other hand, the existing RCD approaches [11, 15, 17] are often based on the assumptions of isolation and compactness of rare category examples; in contrast, our algorithms do not require rare categories being iso-lated from majority categories, and relax th e compactness assumption to that every rare category may only be compact on partial dimensions. The existing paradigms for RCD can be classified into three groups, namely (1) the mixture model-based [15], (2) the data distribution change-based [4, 5, 7, 8, 11, 12] and (3) the hierarchical clustering-based [17]. A brief review on some representatives of these approaches in terms of time complexity and required prior information about a given data set is shown in Table 1. Note that throughout the paper m stands for the number of all categories, r the number of rare categories, and p i (where 1 i r )the proportion of data examples of rare category R i out of all data examples in a data set.
Mixture model-based algorithms assume that data examples are generated by a mix-ture data model and need to iteratively update the model, the computation cost is usu-ally substantial. For example, Interleave algorithm [15] takes O ( dN 2 ) time to update the covariance for each mixture Gaussian.

Data distribution change-based algorithms select data examples with maximal data distribution changes as candidate examples of rare categories. According to the mea-surements for the data distribution changes, these algorithms can be classified into two sub-groups, namely (1) local density-based, such as SEDER [5], GRADE [7], GRADE-LI [7], and NNDM [4]; and (2) nearest neighborhood-based, such as RADAR [12] and CLOVER [11]. Their time complexities are nearly quadratic or even cubic.

Hierarchical clustering-based algorithms investigate rare category characteristics of clusters on various levels. HMS [17] as a rep resentative uses Mean Shift with increasing bandwidths to create a cluster hierarchy, and adopts Compactness and Isolation criteria to measure rare category characteristics. Its overall time complexity is  X  ( dN 2 ) .
Besides time complexity, the prior information needed on a given data set leads to the existing algorithms falling into two cla sses, namely prior-dependent and prior-free. Adhering to the problem definition by He et al . [4, 5, 7] and Huang et al .[11,12],we formally define the problem of rare category detection as follows.

Given: (1) An unlabeled data set S = { x 1 ,x 2 ,...,x N } containing m categories; (2) a labeling oracle which is able to give category label for any data example. Find: At least one data example for each category.

For the data distribution of majority categories, we have the following assumption which is commonly used in the existing work [4, 5, 7, 11, 12, 15] explicitly or implicitly. Assumption 1. Data distribution of each majority category is locally smooth on each dimension.

Gaussian, Poisson, t -, uniform distribution and many other distributions well satisfy this assumption. Thus this assumption can be satisfied by most applications.
For data distribution of rare categories, the exiting work [4 X 7, 10 X 12, 15] assumes that each rare category forms a compact clus ter in the whole feature space, i.e., data examples from a rare category are similar t o each other on every dimension. We relax this assumption to that every rare category may only be compact on partial dimensions. Assumption 2. Each rare category forms a compact cluster on partial dimensions or on the whole feature space.

This assumption is more realis tic because in many applications, data examples from a rare category are different from those fro m a majority category on partial dimensions. For example, panda subspecies are different from giant panda only in fur color and tooth size. According to t his assumption, data exampl es of each rare category should show cohesiveness and form a compact cluster on at least partial dimensions.
Let D R i be the dimensions such that on each dimension j  X  D R i rare category R i forms a compact cluster. According to the assumptions, we have following observations. Observation 1. In the areas without clusters of rare category examples, data distribu-tion is smooth on each dimension.

According to Assumption 1, dat a distribution of each majo rity category is smooth on each dimension. Due to the additivity of c ontinuous functions, eve n in the overlapped areas of different majority categories, data distribution is smoot h on each dimension. Thus for simplicity, we can assume that there is one majority category R 0 in S . Observation 2. Any abrupt change of local data density on each dimension j  X  D R i indicates the presence of rare category R i .

According to Assumption 2, data examples of R i form a compact cluster on dimen-sion j  X  D R i , thus the local data density of R i on dimension j is significant. This significant data density, combining with overlaps of data examples from majority cate-gory R 0 , brings an abrupt change in local data distribution, which is distinct from the smooth distribution of R 0 . Therefore, abrupt changes of l ocal data density on dimension j  X  D Based on the observations, we present FRED algorithm for RCD by exploring these abrupt local density changes via three steps. (1) On each dimension of a data set, FRED tabulates data examples into bins of appropriate bandwidth, and estimates the local density of each bin by Histogram Density Estimation (HDE) [16]. (2) By conducting wavelet analysis on estimated density function, FRED locates abrupt changes of lo-cal data density and quantitatively evaluates the change rates via our proposed DCR criterion. (3) After summing up each data examples X  weighted DCR scores on all di-mensions, FRED keeps selecting data examples with maximal DCR scores for labeling until at least one data example is discovered for each category. 4.1 Histogram Density Estimation To find abrupt changes of local data density, a crucial step is to estimate local data density. We adopt HDE [16] for this goal due to its accuracy and time efficiency.
HDE firstly tabulates the feature space of a single dimension within interval [ s 1 ,s 2] into w non-overlapped bins B 1 ,B 2 ,...,B w , which have the same bandwidth h ,and uses the number of data examples in each bin to estimate the local data density.
Let  X  k be the number of data examples in the k th bin B k and  X  f ( k ) be the estimated local data density at bin k .Thenwehave
The structure of the histogram is completely determined by two parameters, band-width h and bin origin t 0 . Well established theories (e.g., [16], [18]) show that band-width h has dominant effect and bin origin t 0 is negligible for sufficiently large sample sizes. A very small bandwidth results in a jagged histogram with each distinct obser-vation lying in a separate bin (under-smoothed histogram); and a very large bandwidth results in a histogram with a single bin (over-smoothed histogram) [18]. We propose a criterion on h selection for detecting rare category R i as where C i is the number of data examples of R i and  X  a relaxation factor. This criterion guarantees that the average bin count is approximate to C i , which makes the abrupt density change caused by R i more significant to be detected. 4.2 Wavelet Analysis After estimating local density, we perform wavelet analysis on the estimated density function to find abrupt density changes, which is the key to detecting rare categories.
First, we provide a brief review of main concepts on wavelet analysis. We define normalized, i.e.,  X  ( x ) is zero mean, i.e., A wavelet family can be obtained by translating and scaling the mother wavelet. Mathematically, they are  X  a,b ( x )= 1  X  a  X  x  X  b a for a, b  X  R and a&gt; 0 where a is the scale, which is inversely proportional to the frequency, and b represents the translation, which indicates the point of location where we concern [14].

Given these, we define the wavelet analysis of a quadratic integrable function f ( x ) with real-valued wavelet  X  as
Note that (1) wavelet analysis maps a 1-D signal to a 2-D domain of scale (frequency) variable a and location variable b , which allow for location-frequency analysis. (2) For is called the wavelet coefficient, which represents the resemblance index of f ( x ) on blance [14]. Once an appropriate wavelet is chosen, WT f ( a 0 ,b 0 ) reflects the amplitude of density change at the point of location b 0 . As mentioned above, identifying local density changes is the key to detecting rare categories, thus this amplitude can help us fast locate the location of rare categories. 4.3 Data Distribution Change Rate To quantify local density change rate for bins, we propose a new criterion defined as Definition 1 (Data distribution change rate (DCR)). Given bin density function  X  f , wavelet basis  X  ,scale a , the central point b 0 of bin B , DCR of bin B is defined as
DCR of each bin is calculated by wavelet analysis on  X  f . In practice, either Mexican hat or Reverse biorthogonal 2.2 (in short as Rbio2.2) wavelet can be chosen as wavelet basis  X  because they are similar in shape as cu sps of density function brought by rare categories and have a compact support. Scale a in Eq. (4) is usually set to a positive value smaller than 1 , which is the result of balancing the bandwidth of local region and computing cost.

Given DCR definition for bins, DCR of each data example on dimension j can be calculated by four steps. (1) Calculate the optimal bandwidth h by Eq. (2). (2) Divide the feature space of dimension j into bins and calculate bin density function by Eq. (1). (3) Compute DCR score of each bin by Definition 1, negative DCR scores are set to 0 because negative scores indicate drop of local data density and are of no interests to us here. (4) Perform K -means clustering on each bin with K = v as a parameter. Let x ,x 2 ,...,x K be the central data examples of K clusters in bin B ,then DCR scores of x 1 ,x 2 ,...,x K are set to the DCR score of B , DCR scores of other data examples in bin B are set to zero.
 Algorithm 1. Fast Rare Category Detection Algorithm (FRED) 4.4 FRED Algorithm Algorithm 1 presents FRED algorithm which works as follows. Given proportions of rare categories, data dimension d , and the number of categories m , we first initialize hints set Q and their label set L to empty (line 1). Then for each rare category, (1) we compute the count of data examples C i (lines 3 X 6), which will be used in the h selection step of DCR score calculation. (2) Then we calculate DCR score of  X  x k  X  S on each dimension (lines 8 X 9). (3) For  X  x k  X  S , we sum up its weighed DCR score on each dimension as its final DCR score (line 10). It is recommended that W 1 ,W 2 ,... ,and W k have the same value, whereas users with domain knowledge can modify them. (4) Next, we keep proposing the data example with maximal DCR score to the labeling oracle until a new category is found (lines 12 X 17). Note that DCR scores of selected data examples are set to  X  X  X  (lines 11 &amp; 17) to prevent them from being chosen twice.
The time complexity of FRED consists of two parts, (1) DCR score computation and (2) sampling. (1) In DCR computation on each dimension, the most time consum-ing step is K -means clustering, which takes O ( N ) time complexity. Note that on each dimension the time complexity of HDE is O ( N ) and the time complexity of wavelet analysis is O ( w ) ,where w is the number of bins and w&lt;N . So the overall time com-plexity of DCR score computation is O ( dN ) . (2) Since one data example will never be selected twice according to Algorith m 1, the time complexity of sampling is O ( N ) . Thus the time complexity of FRED is O ( dN ) which is linear w.r.t. either d or N . Algorithm 2. Prior-free Rare Category Detection Algorithm (iFRED) We propose iFRED algorithm as a prior-free version of FRED for scenarios where no prior knowledge about the given data set is available.

The difference between iFRED and FRED is twofold. (1) For bandwidth h selection, iFRED algorithm cannot follow the criterion introduced in Eq. (2) because the number of data examples C i in each rare category is not available. Instead, it uses Cross Val-idation [16] to find the original bandwidth h . Furthermore, if current h is not efficient in finding rare categories, iFRED reduces h by setting h = h  X   X  , 0 &lt; X &lt; 1 .(2) In sampling phase, iFRED does not choose one data example each time for labeling, instead, each time it picks up u ( u  X  N  X  ) data examples to measure the efficiency of current h in detecting rare categories. If at least one of the u data examples belongs to a new category, then current h is efficient; otherwise it sets h = h  X   X  , 0 &lt; X &lt; 1 .
Algorithm 2 presents iFRED algorithm which works as follows. (1) The initializa-tion phase (lines 1 X 4) initializes hints set Q and their label set L to empty, scale is initialized to 1 and bandwidth on each dimension is initialized by Cross Validation. (2) The computation phase (lines 6 X 9) calculates DCR of each data example. (3) The sampling phase (lines 10 X 18) chooses each time u data examples of maximum DCR for labeling. If at least one of the u selected data examples belongs to a new category, we continue the sampling loop; otherwise we break out from the sampling loop, update h j ( 1 j d ) by setting h j = h j and sampling phase until the labeling budget is exhausted or scale is too small (line 5, where  X  is the threshold).

Time complexity of iFRED consists of two parts, (1) DCR score computation and (2) sampling. (1) In DCR computation, since the time complexity of Cross Validation on each dimension is O ( N ) and the other three steps of DCR computation on each dimension takes O ( N ) time complexity as analyzed in Sec. 4.4, the time complexity of DCR score computation is O ( dN ) . (2) Since one data example will never be selected twice according to Algorithm 2, t he time complexity of sampling is O ( N ) . Thus the overall time complexity of iFRED is O ( dN ) which is linear w.r.t. either d or N . ple repeatedly in the region where rare category examples occur with high probability. Without loss of generality, assume that we are searching for rare category R i , 1 i r . Let B R i be the bins where data examples of R i cluster together, D R i the dimensions that on each dimension j  X  D R i rare category R i forms a compact cluster. Claim 1. According to the bandwidth selection criterion of FRED and iFRED, a cusp of bin density function will appear in B R i on each dimension j where j  X  D R i . alent to a cusp of bin count function. We prove from the following three points that a cusp of bin count function will appear in B R i on dimension j  X  D R i . (1) On dimension j  X  D R i , the compact rare category examples of R i will cluster together in the same bin or Q adjacent bins where Q is a small integer. (2) Let  X  1 be the data distribution of majority categories at B R i and  X  2 be the data bins without rare category examples have bin count of  X  1  X   X  . By Assumption 2,  X  2 is significant. Note that  X  is very small because data distribution of majority categories changes slowly according to Assumption 1, thus  X  2 %  X  . (3) Let C i be the number of data examples of R i . For FRED, according to the h for iFRED, the bandwidth h will keep reducing, resulting in smaller and smaller bins where  X  1 will not be too large than  X  2 .

Therefore, bins with rare categories will have significantly higher bin counts than nearby bins without rare categories. Claim 1 is proven.
 Claim 2. According to DCR criterion, bins with cusp will get significantly high DCR scores while bins without cusp will get low DCR scores approximate to 0 . Proof. Here we use Mexican hat wavelet (denoted by  X   X  ( x ) ) as an example wavelet to prove Claim 2 (wavelet Rbio2.2 can also be used in the same way). The shape of  X   X  [  X  ;
Since FRED and iFRED use a positive fixed small scale a to detect local density changes, the integral interval [  X  5 a + b, 5 a + b ] is very narrow, which means that the data distribution change of majority categories is trivial. Fig. 1(b) shows the bin density function on the interval without cusps, and Fig. 1(c) shows the bin density function on For local areas without cusp, the bin density change on this interval is trivial. Thus
For local areas with cusps, Eq. ( 5) has a significant value because dominates the integration and has a significant value as shown in Fig. 1. Combining this conclusion with Eq. (6), we know that bins with cusps of bin density function will get a significantly high coefficients and bins without cusps will get coefficients approximate to zero. Therefore, Claim 2 is proven.
 Claim 3. In FRED and iFRED, representative data examples of rare category R i where 1 i r will get significantly high DCR scores, whereas data examples with locally smooth data density will get low DCR scores approximate to 0 .
 Proof. From Claims 1 &amp; 2, we know that B R i will get significantly high DCR score on each dimension j where j  X  D R i . The significantly high dimensional DCR score of B R i will pass to representative data examples of R i in the K -means clustering steps of FRED and iFRED. Since the DCR score of each data example is the sum of its weighted DCR scores on each dimension, representative data examples of R i will have significantly high DCR scores; whereas according to Claim 2, bins with locally smooth data density will get low DCR scores approximate to 0 ,theselow DCR scores will pass to representative data examples of these bins. Claim 3 is proven.
 According to Assumption 1, the pdf of majority data examples is locally smooth. Combining this conclusion with Claim 3, we know that representative data examples of rare categories have significantly higher probabilities to be selected for labeling. In this section, we conduct experiments to verify the efficiency and effectiveness of FRED and iFRED algorithms from two aspect s, namely (1) time efficiency and scal-ability on data size N and dimension d , and (2) number of queries required for rare category discovery. All algorithms are implemented with MATLAB 7.11 and running on a server computer with Intel Core 4 2.4GHz CPU and 20GB RAM. 7.1 Scalability In this experiment, we compare our me thods with NNDM, SEDER, and CLOVER on a synthetic data set where the pdf of majority categories is Gaussian and the pdf of rare categories is uniform within a small region. The synthetic data set satisfies that (1) the data size N ranges from 10000 to 40000 , (2) rare category R 1 forms a compact cluster in the densest area of the data set and has 395 data examples, (3) rare category R 2 forms a compact cluster in the moderate dense area and has 100 data examples, and (4) rare category R 3 forms a compact cluster in the low dense area and has 157 data examples. (1) We set data dimension to 10 and vary the data size from 10000 to 40000 with incremental 10000 . Fig. 2(a) shows the comparison results which agrees well with the time complexities shown in Table 1; i.e., the SEDER curve raises steeply due to its O ( d 2 N 2 ) time complexity, followed by the curves of NNDM and CLOVER with O ( dN 2  X  1 d ) complexity, and lastly the curves of FRED and iFRED with linear com-plexity w.r.t. N . (2) We set the size of the data set to 10000 and vary the data dimension from 5 to 20 with incremental 5 . Fig. 2(b) shows that (1) iFRED, NNDM and CLOVER consumes much less time than SEDER; (2) time consumption by FRED, iFRED, NNDM and CLOVER grows linearly with data dimension. (3) Our algorithms are much more efficient than other tested algorithms, e.g., on the data set with data size 40000 , the runtime of CLOVER in seconds is 1884 , NNDM 2153 , SEDER 3499 , whereas FRED only needs 5 seconds and iFRED 114 seconds. Experiments on real data sets such as Shuttle and Letter [1] have similar observations. 7.2 Efficiency In RCD, the efficiency of an algorithm is evaluated by the number of queries needed to discover all categories in a data set. Usually these queries involve expensive human ex-perts X  work, thus our goal is to discover each category with minimal number of queries.
This experiment compares our algorithms with other algorithms on six real data sets from the UCI data repository [1], as detailed in Table 2. Specifically, categories with too few ( 10 ) data examples are removed in order to satisfy compactness characteristic of rare categories, and sets Iris, Pen Digits an d Letter are sub-sampled to create skewed data sets. Parameter v is set to either 1 or 2 for both FRED and iFRED.

Fig. 3 illustrates the comparison results on six data sets. From the figure, we have the following observations. (1) On all six data sets, FRED requires the least queries to discover all categories among all tested algorithms and performs significantly better than other algorithms. For example, on Shuttle set (Fig. 3(f)), FRED needs 42 labeling queries, iFRED 197 ,CLOVER 254 , SEDER 575 , and NNDM 884 . (2) On all six data sets, iFRED takes up the second place right after FRED, especially on Vertebral, Pen Digits and Letter sets, its query number is almost as less as that of FRED. (3) A steeper curve means that the algorithm can discover new categories with fewer labeling queries. The curves of FRED and iFRED are much steep er than those of others, meaning that they need much less queries to discover a new category. In this paper, we have proposed FRED and iFRED algorithms for RCD which have achieved O ( dN ) time complexity and required least labeling queries. After using HDE to estimate local data density, they have been ab le to effectively identify candidate ex-amples of rare categories through the abrupt density changes via wavelet analysis. The-oretical analysis has proven the effectiv eness of our algorithms, and comprehensive experiments have further verified the efficiency and effectiveness.

For the next stage of study, a promising direction is to investigate new methods for the estimation of local data density. Another suggestion for future study is to work on sub-space selection which may bring with breakthroughs on this topic since in many scenarios rare categories are distinct on only a few dimensions.
 Acknowledgment. This work is supported by the National Key Technology R&amp;D Program of the Chinese Ministry of Science and Technology under Grant No. 2012BAH94F03.

