 ORIGINAL PAPER Morteza Valizadeh  X  Ehsanollah Kabir Abstract In this paper, we present an adaptive water flow model for the binarization of degraded document images. We regard an image surface as a three-dimensional terrain and pour water on it. The water finds the valleys and fills them. Our algorithm controls the rainfall process, pouring the water, in such a way that the water fills up to half of the valley X  X  depth. After stopping the rainfall, each wet region represents one character or a noisy component. To segment each character, we labeled the wet regions and regarded them as blobs; since some of the blobs are noisy components, we use a multilayer Perceptron to label each blob as either text or non-text. Since our algorithm classifies the blobs instead of pixels, it preserves stroke connectivity. After several exper-iments, the proposed binarization algorithm demonstrated superior performance against six well-known algorithms on three sets of degraded document images. The main superi-ority of our algorithm is on document images with uneven illumination.
 Keywords Adaptive water flow  X  Document binarization  X  Degraded image  X  Blob extraction  X  Multilayer Perceptron 1 Introduction Document image binarization is one the most important stages in character recognition applications. It converts a gray-scale image into binary. Since the quality of binary image affects the performance of character recognition algo-rithms [ 1 ], various binarization algorithms have been devel-oped. However, binarization of document images with poor and variable contrast, shadow, smudge, and variable fore-ground/background intensities is still a challenging problem. The binarization methods reported in the literature are gen-erally either global or local. Global methods, using some criteria based on the gray levels of the image, find a single threshold value. These methods compare the gray level of each pixel with a threshold value and label it as either text or background [ 2  X  6 ]. Global methods have good performance in the case of good separation between the text and back-ground gray levels [ 1 ], but when the histogram of the text overlaps with that of the background, these methods fail to operate properly.

To overcome the disadvantages of global methods, vari-ous local binarization algorithms have been presented. These methods use local information around a pixel to classify it as either text or background. Some local methods use only the gray levels of each pixel and its neighborhoods [ 1 , 7  X  11 ], whereas other methods in addition to gray levels use the structural features of text such as stroke width and double edges to improve the quality of binarization [ 12  X  25 ]. Some well-known local binarization algorithms will be reviewed in Sect. 2 .

Topographic analysis using water flow model introduced by Kim et al. [ 16 ] is a promising idea for document image binarization. They use this fact that the text pixels are darker than their neighboring background pixels. There-fore, the problem of binarization can be reduced to find-ing local intensity minima. They regard an image surface as a three-dimensional terrain that is composed of mountains, corresponding to background, and valleys, corresponding to characters. They pour water onto the terrain, it flows down on the terrain and fills the valleys. Pouring of water, rainfall, stops after a predetermined number of iteration. Then, the regions that have high amount of filled water indicate local minima representing the texts. They applied a global thres-holding algorithm on the filled water to find the regions with high amount of water. These regions are labeled as text and others as background. Since the amount of the filled water represents the local characteristics of the image, their method yields good segmentation results for document images.

As stated in [ 19 ], since Kim X  X  method pours water onto the entire terrain, it is time consuming. Moreover, it has not an automatic criterion for stopping rainfall process. Fur-thermore, it uses a global thresholding to find valleys thus removing the poor-contrast text. In [ 19 ], several modifica-tions have been made on this method. The water is poured on the regions of interest with large gradient magnitudes. The rainfall process stops when forty percent of the regions of interest are flooded with water, and final binary image is obtained by applying two threshold values. Deep and shallow ponds are thresholded by large and small threshold values, respectively. Although these modifications improve the bi-narization results, some problems remain unsolved. Since a global experimental threshold is utilized to extract regions of interest, the texts with weak edges are missed. Moreover, document images having ponds with three or more separable depths cannot be binarized with only two threshold values, and some text ponds are missed. In addition, since the rain-fall process stops when a global stopping criterion is satisfied, some valleys are filled partially leading to broken characters.
In this paper, we propose an adaptive water flow model that efficiently segments each character from the background and other characters. Each character is regarded as a blob. This algorithm, in addition to characters, segments some parts of background as blobs. After segmentation, we extract two fea-tures from each blob and feed them into a trained classifier to label the blobs as either text or non-text. The quality of our binarization algorithm depends on the success in reli-able blob extraction. Therefore, we make extra attention to this process. To have a powerful blob extraction, we modify the water flow model to solve its shortcomings. The modi-fications are as follow: more reliable extraction of region of interest, automatic setting of the rate of rainfall, and stopping the rainfall algorithm locally.

Classifying blobs instead of pixels is a new idea in this work. It helps to preserve the stroke connectivity, hence reducing the number of broken characters in the binary image.

The rest of this paper is organized as follows: Section 2 briefly reviews some related works on local binarization methods. Section 3 describes the proposed binarization algo-rithm. Experimental results and comparison with six well-known binarization algorithms are presented in Sect. 4 , and conclusions are given in Sect. 5 . 2 Related works In general, document image binarization is categorized into global and local algorithms. Local binarization performs bet-ter in extracting text from degraded document images, so we briefly review some popular local binarization algorithms and their drawbacks.

Niblack proposed a dynamic thresholding algorithm, which calculates a separate threshold for each pixel by shift-ing a window across the image [ 11 ]. The threshold T ( x for the center of window is computed using local information. T ( x , y ) = m ( x , y ) + ks ( x , y ) (1) where m ( x , y ) and s ( x , y ) are local mean and standard devia-tion in the local window centered on pixel ( x , y ). The window size and k are the predetermined parameters of this algorithm. The value of k is set into  X  0 . 2. This method can separate text from background in the area around the text, but wherever there is no text inside the local window, some parts of the background are regarded as text and background noise is magnified.

Sauvola solved the problems of Niblack X  X  method assum-ing that the text and background pixels have gray values close to 0 and 255, respectively [ 7 ]. He proposed a threshold cri-terion as follows: T ( x , y ) = m ( x , y ) [ 1  X  k ( 1  X  s ( x , y )/ R )) ] (2) where R is a constant, set into 128 for an image with 256 gray levels, and k is set into 0.5. This procedure gives sat-isfactory binary image in the case of high contrast between text and background. However, the optimal values of R and k are proportional to the contrast of the text. For poor-con-trast images, if the parameters are not set properly, some text regions are missed.

Integrated function algorithm [ 24 ] applies a gradient-like operator, defined as activity A ( x , y ) , on the image. Based on A ( x , y ) and the value of Laplacian operator, the pixels are classified into  X 0 X ,  X + X , and  X   X   X  classes. In a sequence of labels along some straight lines,  X   X  + X  or  X +  X   X  transitions determine the edge pixels. Text pixels are assumed to be  X 0 X  and  X + X  labeled pixels between  X   X  + X  and  X +  X   X  pairs. Since this method does not use the gray level information in the changing background.

Chen et al. [ 14 ] presented a novel binarization algorithm by using the edge and intensity information. This method uses a region-growing algorithm to extract perfect objects. At first, some seeds are located near the edge pixels and edges are connected to surround the seeds. Connected edges are regarded as the barrier to seed filling. Afterward using two low and high threshold values, two binary images are obtained. They used the closed edges to partition the binary image produced by high threshold. The partitioned binary image is filled with the seeds to obtain a primary binary image. The final binary image is obtained by combining the primary and low-threshold binary images. Although this algorithm effectively extracts objects from background, it is not specialized to text. Therefore, smear and smudges are extracted similar to text.

Gatos et al. [ 17 ] incorporated the double edge and gray level information to binarize degraded document images. Their algorithm consists of three main steps. In the first step, they combine several binarization algorithms to obtain a pri-mary binary image. In the second step, the edges are extracted using Canny edge detector and the ones that are far from the texts in primary binary image are removed to reduce the num-ber of spurious edges. In the third step, the image is horizon-tally scanned and the pixels that lie between two successive edges and satisfy the following criteria are regarded as text: (i) the distance between two successive edge pixels is short (ii) the average gray level value of the pixels between two edges is smaller than the average gray level value at the corre-sponding edge pixels. This algorithm is useful for eliminating background pixels falsely labeled as text in primary binary image. However, it cannot restore the faint texts removed in the primary binary image.

Logical-level thresholding technique uses not only the image gray level values, but also the stroke width of the characters to improve the binarization quality [ 18 ]. This algo-rithm is based on the idea of comparing the gray level of each pixel with some local averages in its neighborhoods. These comparisons need a threshold to produce some logical val-ues that are used to generate the binary image. Yang [ 12 ] proposed an adaptive threshold calculation to improve the logical-level technique but this threshold is proportional to a predetermined parameter, so the quality of the final binary image depends on the parameter setting by the user.
Stroke-model-based method uses the two following attri-butes of the text pixel to extract characters [ 23 ]: (i) its gray level is lower than that of its local neighbors and (ii) it belongs to a thin connected component with a width less than the stroke width. Based on these attributes, the gray-scale image is mapped into a double-edge feature image. This mapping increases the separation of text and background pixels, and a global thresholding followed by a reliable post-processing extracts the text. In the double-edge image, the separability of the text and background depends on the contrast of text in the original image. Global thresholding of double-edge image is not suitable for the images with variable foreground/back-ground intensities where the low-contrast texts are missed.
In our previous work [ 25 ], we mapped the image into a 2D feature space in which the text and background pixels were separable and then we partitioned the 2D feature space into small regions. The partitioning was carried out in such a way that almost only the instances from either text or background pixels occupied a region, hence resulting pure regions. After partitioning the feature space, the regions were classified was either text or background using the results of Niblack X  X  bi-narization method. Each pixel was labeled as either text or background according to its corresponding region in the fea-ture space. Although this algorithm efficiently binarizes var-ious types of images, it falsely labels some bleeding-through textsastruetexts.

Lu et al. [ 26 ] presented a binarization algorithm based on background estimation and edge pixels information. This method consists of four stages including document back-ground estimation, edge detection, local thresholding, and post-processing. The background is estimated in two parts. First, a row-scanned surface is generated by fitting a poly-nomial for each row of the document image and then the row-scanned image is smoothed by fitting the polynomials column by column. In the second stage, gradient information of image is calculated and normalized by the local value of background surface. The edge pixels are extracted by apply-ing Otsu X  X  method on the normalized gradient image. In the third stage, a local thresholding based on the number of the edge pixels inside a local window and their average gray level is carried out to extract text pixels. Finally, a post-processing method corrects binarization errors based on the estimated background surface and some document knowledge. This algorithm achieved the first rank in the first document image binarization contest [ 27 ]. This method has several predeter-mined parameters in the post-processing stage, which leads to missing of some texts in images with inhomogeneous back-ground. 3 Proposed binarization algorithm Our binarization algorithm consists of the blob extraction and classification stages. The quality of our binarization algo-rithm depends on the success in blob extraction stage. Weak blob extraction leads to touching, broken, and missing char-acters in the final binary image. Therefore, we apply essen-tial modifications to image binarization based on water flow model to make it a reliable blob extraction algorithm. Our algorithm extracts the blobs in such a way that each blob represents either a character or part of the background. After blob extraction, we use a multilayer Perceptron to label each blob as either text or non-text. 3.1 Adaptive water flow model In the binarization algorithms based on water flow model, drops of water, falling down at image terrain, find the local minima and fill them. The rainfall stops when a global stop-ping criterion is satisfied. Then, the filled water is thresholded to separate text from background [ 16 , 19 ].
The following questions indicate the modifications that can be applied on water flow model.  X  Where does the rain fall?  X  How are the local minima found?  X  How much is the height of a local minimum raised when  X  When does the rainfall process stop? In our adaptive water flow model, we answer the above ques-tion as follows:  X  The water falls down only onto edge pixels.  X  The mask used to find local minimum is modified to  X  The rate of rainfall is automatically determined propor- X  The rainfall selectively stops at each edge pixel accord-Figure 1 illustrates the flowchart of proposed blob extraction method. In our work, the image is iteratively scanned and the drops of water fall down only onto those edge pixels, for them the stopping criterion has not satisfied. Each drop finds a local minimum and fills it. The height of each local min-imum increases by the rate of rainfall, when a drop fills it. The rainfall stops when the stopping criterion reaches for all edge pixels. After stopping the rainfall, we extract the ponds that represent blobs. In the following subsections, we explain the details of this algorithm. 3.1.1 Region of interest extraction We pour the water only onto the edge pixels, named region of interest, ROI. Missing ROI causes the missing of charac-ters. Therefore, it is essential to use a reliable edge detection algorithm to avoid missing weak edges. Canny edge detector efficiently extracts weak edges [ 14 ], so we select it for this application. Although this algorithm extracts many spurious edge pixels, our algorithm is not sensitive to those edges. In  X  X ppendix A  X , we explain the reasons behind the robustness of our algorithm against spurious edges. 3.1.2 Stroke width measurement We use the stroke width, SW, to automatically set the rate of rainfall. It is a useful structural attribute of the text. Appro-priate measurement and usage of this attribute can help us to improve the binarization quality. In [ 12 ], the run length histo-gram was utilized to measure SW, while in [ 28 ], we used the histogram of the distances between two successive edge pix-els in horizontal scan to measure SW. These two algorithms find a global SW for a document and are not appropriate for the images that contain characters of different size. Since the efficiency of our binarization algorithm depends on the accuracy of SW measurement, we propose a simple method to measure SW locally.

We first apply the Niblack X  X  algorithm on document image and then use the resulting binary image to measure SW locally. This binary image is scanned in four directions as shown in Fig. 2 to obtain four Black Run Images, BRIs, as follows:
Scanning the image in direction D i ; the lengths of the lines composed of connected black pixels are measured. The length of each line is assigned to all pixels composing that line. In other words, all connected black pixels in direction D i are labeled by the length of the corresponding black run. In this way, the BRI in direction D i is obtained.

After generating 4 BRIs, for each black pixel, the min-imum value of 4 corresponding pixels in BRIs is taken as primary SW. To make our estimation smooth, the average value of SWs around each pixel is used in our application. We use a 30  X  30 window to find the local averages of SW. 3.1.3 Stopping threshold for rainfall process We use the local contrast of edge pixels as stopping thresh-old for the rainfall process. The contrast is measured by using the intensity of neighboring pixels shown in Fig. 3 . Suppose ( x , y ) is an edge pixel. We define the contrast as follows: C ( x , y ) = max where C is the contrast and I ( p i ) represents the gray level of the original image at location p i .

To u s e C ( x , y ) as stopping threshold, we define an aux-iliary image, S , to control the rainfall process. Before start-ing the rainfall, all pixels of S are set to zero. During the rainfall, we examine S ( x , y ) .Ifitissmallerthan C ( x water is poured onto ( x , y ) and S ( x , y ) is increased by one; otherwise, the rainfall stops at ( x , y ) . 3.1.4 Setting the rate of rainfall The rate of rainfall determines how much the height of a local minimum is raised when a drop of water fills it. It is an important parameter in water flow model. In the previous works [ 16 , 19 ], this parameter is set into a constant value, but in our algorithm, to avoid the broken and touching characters in binarized image, this parameter is set automatically.
Suppose W 1 is the water amount that overflows the pond related to a character and W 2 is the water amount that has filled the related pond when the rainfall stops. We experimen-tally observed that if W 2 = 0 . 5 W 1 , the number of broken and touching characters is minimized. Therefore, we automati-cally set the rate of rainfall to achieve this goal.
Using an example is useful to explain how we can set the rate of rainfall. Suppose the gray levels of text and background are 0 and 255, respectively. The water amount that overflows the pond related to a character and the water amount that flows from edge pixels to the correspond-ing pond are: W W where E ={ ( x , y ) | ( x , y ) is an edge pixel of the character}. Since in this example, C ( x , y ) = 255 , W 2 is simplified to 255  X  ( The number of edge pixels related to that character  X  ( rate of rainfall ) .Ifweset W is obtained as follows: rate of rainfall =  X   X  0 . 5 where Figure 4 a illustrates the number of edge and text pixels in horizontal scanning of a character. Each circle represents a pixel. We observed that for each two edge pixels, there are approximately SW text pixels and we have  X   X  SW / 2(6) Although the stroke width slightly differs in different parts of one character, this is a good approximation for our appli-cation. To further examine this, we measured  X  for characters with different values of SW. The results of this experiment are shown in Fig. 4 b. Comparing the second and third col-umns justifies our approximation.

Substituting ( 6 )into( 5 ) yields rate of rainfall = 0 . 25 SW (7) The rate of rainfall is set proportional to SW. Therefore, it differs in different regions of the image, if there are characters of different size. 3.1.5 Fast algorithm for finding local minima Since water flow is the most time-consuming stage of our bi-narization algorithm, we modify the local minimum search-ing mask, presented in previous works, to increase its speed. Water flow model guides the drops of water toward local min-ima. It iteratively finds the minimum inside a small region and moves the center of the searching mask to the correspond-ing point. This process continues until the center of search-ing mask becomes its minimum. Water flow model spends the most of processing time to find the minimum inside the searching mask. Therefore, we modify the shape of searching mask to increase the speed of this algorithm. The previous works [ 16 , 19 ] use a square searching mask shown in Fig. 5 a. Therefore, ( 2 W + 1 ) 2 comparisons are needed to find the minimum inside the searching mask, where W determines the size of the mask. Most of these comparisons are not nec-essary because there is correlation between the neighboring pixels in a document image. Therefore, to speed up the mini-mum searching algorithm, we modify the searching mask as in Fig. 5 b.

In the proposed mask, the number of comparison decreases to 4 W + 1. However, since this mask finds the minimum in a smaller region, comparison of ( 2 W + 1 ) 2 and 4 W + 1 is not fair. We investigated the effect of mask shape on the speed of water flow model experimentally. We implemented two dif-ferent adaptive water flow models using these masks; applied them on a set of document images and compared the process-ing times. Table 1 shows the results of this experiment.
This experiment showed that using the proposed mask, the computational cost becomes almost 68% of what is required for the square mask.

We use the example shown in Fig. 6 to explain how a drop of water finds the local minimum and fills it. Suppose a drop of water falls down at point, e.g., 1. Mask A is centered at this point and the minimum value, e.g., 2, inside this mask is found. After that, a new mask, B, is centered at point 2. The process is continued until the center of the mask becomes its minimum point, e.g., 4. Then, the water fills this point and its height increases by the rate of the rainfall. 3.1.6 Blob extraction After stopping the rainfall, we separate the water-filled and dry regions. If I ( x , y ) and G ( x , y ) represent the height of terrain before and after rainfall, respectively, this is done as follow: B ( x , y ) = where B is a binary image, in which  X 0 X  and  X 1 X  represent the water-filled and dry regions, respectively. Each black con-nected component in B is a separate pond. Therefore, we apply a labeling process on B to extract ponds representing the blobs in document image. In addition to the blobs related to characters, this algorithm extracts many noisy blobs so that one may worry about merging the adjacent characters caused by a noisy blob formed between them. The competi-tive characteristic of water flow model makes our algorithm robust against this problem. The noisy blobs are not formed in the areas close to the text. The example images in Fig. 7 clarify our claim. In Fig. 7 a, there is an ellipse between char-acters  X  X  X  and  X  X  X , which is eliminated in the blob extraction process. Whereas the weak noisy blobs that are far from the text are formed. We carried out some experiments to find out the extent that our algorithm eliminates the noisy blobs closed to the characters. Considering the stopping criterion of rain-fall and blob extraction processes, a noisy object can merge the adjacent characters when its local contrast to background exceeds approximately 50% of the text contrast. This situa-tion rarely occurs in the real-world document images.
After extraction of blobs, their features are fed to a trained classifier to label them as either text or non-text. 3.2 Blob classification and final binarization We extract two features from each blob and feed them into a classifier to label it as either text or non-text. Reliable feature extraction is one of the most important stages in classifica-tion. In our work, average water amount, AWA, that repre-sents the local contrast of text and average gray level, AGL, are used as features. We define these features for the blob as follows: where S 1 ( X ) ={ ( x , y ) | ( x , y )  X  blob  X  } , while M and M 3 are the normalization parameters calculated from the global attributes of the image. To calculate these param-eters, we first apply the Otsu X  X  algorithm on the filled water image, G ( x , y )  X  I ( x , y ) , to approximately estimate the text and background pixels and then define these parameters as follows. M 1 represents the average of filled water for the text pixels. M 2 and M 3 are the average values of gray levels for the text and background pixels, respectively. Since we use different document images to collect the training data, this normalization makes the text blobs of different images more similar and increases the similarity of background blobs as well, while decreasing the similarity between text and back-ground blobs. Therefore, it helps us to train a reliable classi-fier. Also, in the evaluation stage, this normalization makes the samples of unseen images more similar to the training data, hence achieving better results. To verify the discrim-ination power of the proposed features, the distributions of randomly selected blobs from training images are illustrated in Fig. 8
Distribution of the text and non-text blobs in the proposed feature space shows that we can find a proper decision bound-ary to separate them.
 We used a three-layer Perceptron as classifier (Fig. 9 ). In our work, three low-contrast images, four images with inhomogeneous background, and two historical images from DIBCO2009 dataset [ 29 ] are utilized for training. Our algorithm extracted 8,252 text and 17,346 non-text blobs employed to train the classifier and adjust its weights. In the training phase, text and non-text blobs are labeled by 0 and 1, respectively. To binarize an unseen image, each blob is classified according to the value of the output neuron. If it is smaller than 0.5, we classify it as text otherwise as non-text.
To examine the efficiency of the classifier, we computed the classification error on both the training and test sets. The test set includes the blobs extracted from the images of DIB-CO2009. The result of this experiment is given in Table 2 .
After training the classifier, we use it for the binariza-tion of unseen images. The blobs of image are extracted and classified. The blobs classified as text are set to black, while the other pixels are set to white in the final binary image. The proposed classifier removes almost all blobs related to the non-text and yields a clean binary image. Figure 10 illus-trates ROI extraction, water amount after rainfall stopping, blob extraction as well as blob classification for two degraded document images. 4 Experimental results To test the performance of the proposed algorithm, a com-parative study was carried out. In our experiments, Lu X  X  local method [ 26 ], binarization based on water flow model [ 16 ] and improved water flow model [ 19 ], Sauvola X  X  method [ 7 ], Ni-black X  X  method [ 11 ] and our previous work [ 25 ]wereimple-mented and the results obtained were utilized as benchmarks. For Sauvola X  X  algorithm, as recommended in [ 30 ], we set the parameter k into 0.34 that yields better results.
We considered both visual and quantitative criteria for evaluation. We used three data sets in our experiments. Data-set I is ICDAR2009 dataset that includes 5 handwritten and 5 printed historical document images suffering from degra-dations such as smear, smudge, variable foreground/back-ground intensities, and bleeding through. These degradations are due to aging. Dataset II includes 10 document images we captured under badly illumination conditions, suffering from low-contrast and variable background/text intensities. Data-set III includes 10 document images selected from the Media Team Dataset [ 31 ]. The images of this dataset have degrada-tions such as textured background, shadow through, variable foreground/background intensities, and low-contrast text. 4.1 Visual evaluation We applied different binarization algorithms on a set of degraded document images. The binarization results showed that our algorithm yields satisfactory results over the entire test images, while other methods fail to provide acceptable results for some of the images. 4.1.1 Experiment 1 In this experiment, we applied the binarization algorithms to the low-contrast document images with homogenous back-ground and visually compared the results. Figure 11 shows an example of the images utilized for this experiment and the binarization results by different methods. This experi-ment shows the advantages and disadvantages of different algorithms for the binarization of low-contrast images.
Niblack X  X  method detects a great number of background blobs as text, while Sauvola X  X  method misses the low-contrast characters. Kim X  X  method leads to many touching characters and Oh X  X  method results in many broken characters. Lu X  X  method, our previous method [ 25 ], and the proposed method yield clean binary images. 4.1.2 Experiment 2 In this experiment, we applied the binarization algorithms to the low-contrast document images with non-uniform back-ground/foreground intensities. Example results are shown in Fig. 12 . This experiment showed that the methods presented by Lu, Kim, and Oh miss some of the texts dealing with badly illuminated document images. Niblack X  X  method does not eliminate the background efficiently and Sauvola X  X  method produces many broken characters. Our previous method and proposed method yield satisfactory results for this kind of images. 4.1.3 Experiment 3 In this experiment, we applied the binarization algorithms on historical document images, suffering from bleeding-through degradation. Example results are shown in Fig. 13 . All bina-rization algorithms evaluated in this experiment label some bleeding-through text pixels as true text. Lu X  X  and the pro-posed algorithms eliminate most of those bleeding-through text pixels and yield clean binary images. Classifying the blobs instead of pixels is the main reason for the success of our algorithm. 4.2 OCR-based evaluation OCR-based comparison is one of the most acceptable meth-ods for the quantitative evaluation of binarization algorithms [ 15 ]. To prove the efficiency of the proposed method, we bi-narized 10 document images, which are captured by camera in non-uniform lighting condition, by different algorithms. The binary images are fed into the commercial OCR soft-ware, ABBYY FineReader 9. Using the OCR results, we employed the following measures for evaluation.
 TRC: number of true recognized characters.
 FMC: number of falsely recognized or missed characters.
FRB: number of background blobs falsely recognized as character.

In this experiment, the test images have 7,388 characters in total. The evaluation results are given in Table 3 .
We summarize the results of our OCR-based evaluation as follows:  X  Lu X  X  method eliminates background efficiently. How- X  Niblack X  X  method extracts text efficiently but it intro- X  Sauvola X  X  method eliminates the background properly  X  Kim X  X  method yields satisfactory results for low-con- X  Oh X  X  method performs well in the most of the test cases.  X  Our previous work [ 25 ] yields satisfactory binary images  X  Proposed method uses a reliable algorithm to extract 4.3 Evaluation based on F-measure In this experiment, we used F-measure to compare the per-formance of binarization algorithms. This criterion is utilized in the first document image binarization contest [ 27 ]. It is defined as follows: F-measure = where Recall = TP TP + FN , Precision = TP TP + FP and TP, FP, and FN denote true-positive, false-positive, and false-negative values, respectively. We used all three datasets for this evaluation. The results are given in Table 4 . Our algo-rithm stands in the second rank after the Lu X  X  for Dataset I and yields the best results for Datasets II and III. The algo-rithm is efficient dealing with low-contrast images as well as those with variable text and background intensities, which are common in Datasets II and III. This is the main reason of our algorithm X  X  superiority against others in these datasets. 5 Conclusion In this paper, we presented an adaptive water flow model for the binarization of degraded document images. Our water flow algorithm controls the rainfall process in such a way that segments each character from the background and other characters. When the rainfall process stops in all points of the image, many ponds are established. Each pond indicates a blob. We used a trained classifier to label each blob as either text or non-text. Since our algorithm classifies the blobs instead of pixels, it preserves stroke connectivity. In differ-ent experiments, our algorithm demonstrated superior per-formance against some well-known algorithms on three sets of degraded document images.
 Appendix A We used Canny edge detector to extract the region of interest to pour the water. In this section, we explain the reasons that make our algorithm insensitive against the spurious edges extracted by Canny detector.

We first discuss the problems, which may arise because of spurious edges. 1. The water pouring on the spurious edges may flow to 2. Noisy blobs are formed in the regions not closed to the As mentioned earlier, we stop the rainfall process at each edge pixel according to its contrast and adjust the size of water in such a way that each blob fills until the half of its height. Spurious edge pixels are low contrast compared to the text edges and therefore share less water compared to real text edges. Therefore, the spurious edges cause the algo-rithm to fill the text ponds a little more than the half of their height. However, almost never cause an overflow; hence, the first problem does not happen.

Spurious edges located in the area far from the text form some noisy blobs. Since these blobs absorb the water from low-contrast spurious edges, the water amount gather in them is small and these blobs are labeled as noise in the blob clas-sification stage. Therefore, concerning the second problem, our algorithm is insensitive to spurious edges.
 References
