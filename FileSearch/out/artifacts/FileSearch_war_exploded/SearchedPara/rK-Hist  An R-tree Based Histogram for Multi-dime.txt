 Database query engines typically rely upon query size esti-mators in order to evaluate the potential cost of alternate query plans. In multi-dimensional database systems, such as those typically found in large data warehousing environ-ments, these selectivity estimators often take the form of multi-dimensional histograms. But while single dimensional histograms have proven to be quite accurate, even in the presence of data skew, the multi-dimensional variations have generally been far less reliable. In this paper, we present a new histogram model that is based upon an r-tree space partitioning. The localization of the r-tree boxes is in turn controlled by a Hilbert space filling curve, while a series of efficient area equalization heuristics restructures the initial boxes to provide improved bucket representation. Exper-imental results demonstrate significantly improved estima-tion accuracy relative to state of the art alternatives, as well as superior consistency across a variety of record distribu-tions.
 H.2.4 [ Database Management ]: Systems X  Query Pro-cessing, Relational Databases Algorithms, Design, Performance Data Warehousing, Multi-dimensional Selectivity Estima-tion
As database systems have grown in size, so too has the need to efficiently produce accurate approximations of the underlying data sets. In particular, the estimates of larger data distributions have been utilized in the database con-text to support both approximate query answering and se-lectivity estimation. Very early DBMS platforms provided such functionality through crude statistical estimates that essentially treated the tuple space as a single uniformly dis-tributed partition. However, with the introduction of his-togram partitioning techniques [19], new tools for effective estimation emerged.

The majority of the early research focused on partition-ing in single dimensions. For inherently multi-dimensional spaces, the initial approach involved the simple integration of a series of one-dimensional histograms. However, the un-derlying attribute value independence assumption [4] tends to produce extremely poor estimates of joint data distri-butions. Consequently, several noteworthy attempts have been made to produce true multi-dimensional histograms that more accurately reflect the natural patterns of cluster-ing and skew that occur in such spaces [22, 26, 8].
In general, the focus of multi-dimensional methods is the identification of regions of close to equivalent point density. These hyper-rectangular areas are then divided into a small number of memory resident buckets . Unlike the single di-mensional case, there is no absolute, locality preserving or-der on a multi-dimensional point space, so each histogram model employs heuristic techniques to identify regions of ap-proximate uniformity. Regardless of the technique, however, the uniformity measure is effectively a simple density calcu-lation in which the points within a bucket are assumed to be evenly distributed. We refer to this as the uniform spread assumption [27].

In fact, density can be a relatively poor measure of point distribution, a problem that is exacerbated as dimensional-ity increases. In this paper, we present instead a new metric called k-uniformity (kU) that more accurately measures the quality of tuple distribution. In turn, the kU metric is inte-grated with a partitioning model often employed in multi-dimensional indexing environments. Specifically, we build upon the notion of multi-dimensional r-tree partitioning, in which the distribution of blocks is governed by the Hilbert space filling curve. By  X  X iggy-backing X  on top of existing indexing models, we are able to provide effective selectivity estimation in conjunction with powerful indexing function-ality in multi-dimensional settings.

Experimental results demonstrate that our new r-tree/kU histogram, rK-Hist, produces estimation errors that signifi-cantly improve upon the current state of the art. In fact, in skewed spaces, we see estimation errors as low as 2%-7% in two to four dimensions. Moreover, results are consistently impressive as dimension count increases, suggesting that our new methods have broader applicability than existing tech-niques.

The remainder of the paper is organized as follows. We discuss related work in Section 2, while Section 3 briefly introduces relevant preliminary material. We describe the fundamental r-tree histogram model in Section 4. A new  X  X liding window X  X lgorithm for improved partitioning is then presented in Section 5, with the k-Uniformity metric de-scribed in Section 6. Experimental results are presented in Section 7. Final conclusions are provided in Section 8.
The first non-trivial attempt to approximate data distri-butions for the purposes of selectivity estimation was de-scribed in [19]. Here, a single dimensional point space is divided into a set of buckets, in which the spread of values in each bucket is equivalent. The result is what is now re-ferred to as the equi-width histogram . In [25], the equi-depth (or equi-height) histogram, in which buckets represent equiv-alent frequency summations, was shown to provide signifi-cantly better selectivity estimation than the original equi-width method.

Subsequently, a series of alterative partitioning param-eters and techniques were presented in the literature, each attempting to more accurately reflect the point distributions of arbitrary spaces. In fact, the plethora of methods lead to the design of a histogram taxonomy [27], which described histograms in terms of elements such as the source parame-ter (e.g., spread, frequency, and area) and the partitioning constraint (i.e., how to partition based upon the source pa-rameter). In addition to the equi-sum constraint employed by equi-width and equi-depth histograms, alternative con-straints have included the v-optimal approach that tries to minimize the variance of source parameter values [15], and the maxdiff technique that searches for the largest  X  X aps X  between source values [27].

Later, researchers began to investigate histograms for multi-dimensional spaces. Not surprisingly, the problem is some-what more challenging since it was shown in [23] that opti-mal splitting even in two dimensions is NP-hard. The first attempt to support multi-dimensional selectivity estimation was described in [22]. Here, the hTree model divides the d -dimensional space into a regular grid, using frequency as the source parameter and, as such, essentially represents a direct extension of the equi-depth technique. The mHist his-togram attempts to improve upon the quality of partition-ing by recursively dividing the space on the dimensions that are judged to most benefit from a split (in terms of density) [26]. While both mHist and the hTree produce X  X egular X  X rid patterns, the genHist histogram produces irregular bucket patterns by iteratively identifying high density regions and subsequently permitting bucket overlap [8]. Unfortunately, it has also been shown to be quite expensive to generate. We note as well that multi-table histograms have also been investigated in order to optimize complex join queries. The TUG model, for example, uses a tuple graph to estimate multi-table selectivity [30].

A special class of multi-dimensional histograms includes the dynamically generated models. Here, rather that con-structing the histogram statically from an existing data set, the histogram is generated and updated in real time. stHoles , for example, uses incoming queries to produce nested buck-ets [3], while ISOMER extends the basic stHoles model by adding the notion of entropy minimization [31]. Dynamic generation using an intermediate summary structure has also been pursued in the context of continuous data streams [32]. In general, the dynamic techniques have been shown to work fairly well in low dimensions, though they cannot be expected to outperform the static methods across a broad range of data distributions.

A small number of papers have presented theoretical sup-port for histogram construction models that define upper bounds on estimation errors [17, 7]. However, these limits are available only for the much simpler case of one dimen-sional histograms.

Finally, we add that alternatives to histograms have also been explored in the literature. Examples include wavelets [33], table sampling [11], and discrete cosine transform[20]. In general, however, histograms have remained the most popular target for selectivity estimation and approximate query answering due to their effectiveness and robustness across a wide variety of application domains. A relatively full record of their history can be found in [14].
With respect to multi-dimensional indexing, we note that this has historically been an active area of research [6], though few of the experimental methods have found their way into production systems. The r-tree itself was proposed by Guttman [9], while the concept of pre-packing the r-tree for improved storage and performance was first discussed by Roussopou-los and Leifker [28]. Packing strategies were reviewed by Leutenegger et al. [21] and Kamel and Faloutsos [18]. The former promoted a technique they called Sort Tile Recur-sion (STR), while the latter identified Hilbert ordering as the superior approach.

The concept of representing a multi-dimensional space as a single dimensional, non-differentiable curve began with Peano [24]. Jagadish provides an analysis of four such curves  X  snake scan, gray codes, z-order, and Hilbert  X  and identi-fies the Hilbert curve as providing the best overall clustering properties [16], a result duplicated in [5].
Our new methods build directly upon fundamental r-tree construction techniques, as well as the methods for imple-menting and manipulating space filling curves. Therefore, in this section, we briefly review the salient features and termi-nology relevant to the techniques presented in the remainder of the paper.

Previous multi-dimensional histogram methods have taken what might be described as ad hoc approaches to space par-titioning. However, given that partitioning is a fundamental problem in a number of research domains, there is much to be gained by both an examination and exploitation of pre-vious partitioning work. This point was also made in [14], where the similarity between tree indexing and histogram partitioning was highlighted. Specifically, it was suggested that construction of a B+tree effectively produced single di-mension buckets that could easily form the basis of a simple histogram. Furthermore, the tree-based design suggested the possibility of a hierarchical histogram that could effi-ciently return approximation results. Having said that, the basic technique is hampered by the fact that index parti-tioning is not necessarily conducive to producing uniform bucket distribution and, hence, low error rates [27].
Our new methods in fact build directly upon the notion of Figure 1: (a) r-tree partitioning for a maximum node count of four. (b) A H 2 3 Hilbert curve. index-histogram integration. Of course, given that our focus is multi-dimensional rather than single dimensional spaces, the B-tree and its variations are clearly not appropriate as a starting point. Instead, we draw upon complementary research carried out in multi-dimensional database environ-ments, specifically that associated with enterprise-scale data warehousing (DW). While a number of indexing methods have been utilized in DW domains (e.g., bitmaps, join in-dexes), the r-tree has proven to be one of the more popular and successful. It is particularly appealing in the current context as it represents a true multi-dimensional decompo-sition of the point space.

Briefly, the r-tree is a hierarchical, d -dimensional tree-based index that organizes the query space as a collection of nested, possibly over-lapping hyper-rectangles [10]. The tree is balanced and has a height H =  X  log M n  X  , where M is the maximum branching factor and n is equivalent to the num-ber of points in the data set. A user query  X  may be defined as { r 1 , r 2 , . . . , r d } for ranges ( r min ( i ) , r is answered by comparing the values on { r 1 , r 2 , . . . , r the coordinates of the rectangle  X  that surrounds the points in each data page. Figure 1(a) illustrates a simple r-tree decomposition.

A number of extensions to the basic r-tree were proposed in order to improve the clustering properties of the original tree. The r + -tree [29] alters the basic model by prohibiting box overlap at the same level of the tree, while the r  X  -tree [1] uses an object re-insertion technique to reduce overlap dur-ing node splitting. However, in more static environments such as that found in data warehousing, more effective clus-tering and storage can be obtained by pre-packing the r-tree index. In this regard, the Hilbert order employed in [18] is particularly interesting as it can be used to provide a lin-ear ordering of points in a d -dimensional space, one that can then be mapped to the single dimension of the storage medium.

First proposed in [12], the Hilbert curve is a non-differentiable curve of length s d that traverses all s d points of the d -dimensional grid having side length s , making unit steps and turning only at right angles. The curve can be visualized as an interval I existing within the unit square S . If we decom-pose S into equivalent sub-squares S i , for 1  X  i  X  4, then with a series of reflections and rotations, we may concate-nate the sub-intervals I i to satisfy the requirements of the linear mapping. We say that a d -dimensional cubic space has order k if it has a side length s = 2 k and use the notation H to denote the k -th order approximation of a d -dimensional Figure 2: Packed r-tree leaf level partitioning for an H 2 4 Hilbert curve. Note the order of the bucket sequence. curve, for k  X  1 and d  X  2. Figure 1(b) shows a small 2-d curve.

Given the point order defined by the Hilbert curve, the packing strategy proceeds as follows: 1. Sort the points in terms of their position along the 2. Associate each of the m leaf node pages with an ID 3. Construct the remainder of the index by recursively
The end result is a hierarchy of bounding boxes that par-tition the space at varying levels of granularity. Moreover, the boxes are constructed so as to most effectively maintain point locality. In other words, points close to one another in the multi-dimensional space tend to be partitioned into the same bucket (or sibling buckets). Figure 2 illustrates how the Hilbert packed r-tree can be used to partition a small two dimensional space. Note the inherent clustering properties of the underlying Hilbert curve.
The Hilbert packed r-tree provides us with an interest-ing starting point in terms of its ability to cluster multi-dimensional data buckets. Specifically, following the Hilbert sort of the data and the r-tree partitioning, we are left with a hierarchy of  X  rectangular bounding boxes that each en-close regions of spatially related points. That being said, the r-tree index itself is simply too large to function directly as a histogram, since histograms must remain fully memory resident in order to be effective. As such, the  X  boxes must be integrated in some manner into  X  hyper-buckets . Hyper-bucket integration can in fact be supported in two different ways. Figure 3: Top down histogram partitioning for  X  = 200 . The 800 r-tree buckets at Level 3 are coalesced into a sequence of hyper-buckets.
Because tree-based indexes are hierarchical, there are in fact a series of space partitions at varying degrees of gran-ularity. For example, given n = 15, M = 3, we have height H =  X  log M n  X  = 3. As such, there are three partitions of the d -space, at successively finer levels of detail. In the top down histogram, the idea is to construct  X  hyper-buckets working downwards from the root. At first glance, it would be tempting to simply  X  X lice off the top of the tree X . Doing so, however, would result in a ragged cut of the tree, which is of absolutely no value. Instead, the technique is as follows: 1. Start with a fully constructed r-tree index, consisting 2. Descend from the root, examining each of the i levels, 3. Identify the first level L for which  X   X | L i | 4. Coalesce this level into  X  hyper-buckets, each contain-
As a concrete example, assume that we have enough mem-ory to construct a 200-bucket histogram. Furthermore, as-sume that we also have an index structure with initial bucket counts of L 1 = 20, L 2 = 120, L 3 = 800, and L 4 = 5000. The top down method selects L 3 and coalesces the 800 tree buck-ets into 200 hyper-buckets of 4 nodes each. Figure 3 provides a simple illustration.
In contrast to the top down approach, we can also con-struct the  X  hyper-buckets working strictly from the leaf level of the r-tree. The technique for construction is essen-tially the same as that of the top down method, in terms of the concatenation of r-tree buckets into histogram hyper-buckets. In fact, the boundaries of the hyper-buckets pro-duced by a bottom up histogram are actually quite similar to those of the top down model. Nevertheless, extensive testing has shown that due to its less restrictive initial par-titioning, the bottom up method does produce slightly more uniform hyper-bucket distributions. Moreover, it is simpler to integrate with the inherently bottom-up r-tree construc-tion algorithms. As such, it forms the basis of the rK-Hist methods discussed throughout the remainder of the paper.
It is assumed that rK-Hist would be utilized in a compre-hensive multi-dimensional database environment in which an associated r-tree(s) was also being constructed. This al-lows us to  X  X iggy back X  rK-Hist on top of the core r-tree generation algorithms, thereby obtaining most of the his-togram functionality for little additional cost. While this is the ideal, we note that rK-Hist can also be constructed with-out an associated r-tree, if desired. In either case, the most costly element of the r-tree/rK-Hist process is the sorting of the underlying data set in Hilbert order. It is therefore important to ensure that Hilbert sorting is not prohibitively expensive.

We note the following. Given an O ( n lg n ) comparison based sort, we can assume an average of lg n Hilbert space comparisons for each of the n tuples in the data set R . Since tuple conversions are identical in each case, significant performance improvements can be realized by minimizing the number of redundant Hilbert transformations. Conse-quently, our framework uses a pre-processing step in which the n tuples of R are converted into Hilbert ordinal form (i.e., their numeric position along the curve) prior to sort-ing. A purely integer-based sort is then performed, followed by a linear mapping back into tuple form. This single opti-mization typically reduces costs by an order of magnitude or more, depending on dimension count. In fact, our modified Hilbert sort typically runs within a factor of two of the cost of a simple, multi-dimensional integer sort.
Assuming that the initial r-tree blocks have been coalesced into  X  hyper-buckets that partition the full space, we are now ready to produce an accessible histogram. At this ini-tial stage, we will utilize the standard frequency based equi-depth distribution model for our buckets. That is, within each hyper-bucket B we record its point count B count and volume B volume . Given a user-defined d -dimensional query defined as the intersection of  X  with a subset S of the  X  hyper-rectangles.

Recall, however, that the r-tree blocks, and hence his-togram hyper-rectangles, can overlap in the point space. One might expect selectivity estimation to be considerably more difficult as a result. In fact, this is not the case. When boxes overlap, each defines a distribution that is unique to its own boundaries and to the specific points that it con-tains. In other words, even though portions of the space may be shared, their points are not. Consequently, much like the genHist histogram, the distribution estimates may simply be summed together as though the boxes were non-overlapping. As a concrete example, Figure 4 depicts a pair of over-lapping histogram buckets, B2 and B3. In this case, their contributions to the estimate of  X  can be summed di-rectly. Intuitively, the summation indicates a region of in-creased density.

Calculation of the final estimate therefore proceeds as fol-lows. For a user query  X , we evaluate each histogram bucket B to determine the degree of overlap. Assuming uniform spread inside the bucket, we calculate the estimate on B in terms of the ratio of the overlapped region to the full volume of B . More formally, we may say: where V ol ( X   X  B ) is the volume of the  X  of  X  and B . Figure 4: A user query (dashed line) intersecting a series of over-lapping partitions.

Finally, we note that the r-tree histogram is itself con-structed as a small r-tree. As such, selectivity estimation does not require serial access of the  X  buckets of the his-togram. Instead, the leaf nodes/buckets can be accessed with the standard r-tree cost complexity ( O (  X  log M n  X  )).
We have described the algorithm presented thus far as the  X  X aive X  r-tree histogram. In short, we have exploited the standard r-tree construction algorithm so as to pro-duce a set of  X  buckets approximating the underlying multi-dimensional point space. However, the default ordering of the Hilbert curve may produce less than optimal bucket par-titions. This is primarily because outliers on the curve may occasionally distort the scale of the encapsulating bounding boxes. While we can accept a certain degree of  X  X ox stretch-ing X   X  no space filling curve can perfectly maintain locality in a multi-dimensional space  X  the existence of extreme out-liers may lead to unacceptably inaccurate estimation errors.
As a result, we extend the basic technique with an opti-mization component. Algorithm 1 describes the enhanced r-tree approach. We begin by creating the naive r-tree his-togram. As noted, this will likely contain a small number of hyper-buckets with relatively poor point distribution charac-teristics. To minimize the error generated by such buckets, we actually create an initial set I consisting of  X   X  (  X   X   X  ) buckets, where  X  refers to the under-sampling ratio . We then use a quality measure to identify the ( I  X  1 2  X  ) buck-ets with the worst distributions (the quality measure will be discussed in Section 6). While  X  is a tunable parameter, we note that in practice a  X  ratio of 10% works consistently well. The selected buckets are then placed into a list  X .
At this point, the algorithm scans each hyper-bucket B to determine how its distribution quality might be improved. As noted previously, optimal partitioning is NP-Hard. Con-sequently, we will employ a heuristic method that seeks to re-partition B to produce more uniform point distribu-tion. However, for a hyper-bucket B containing k con-stituent buckets, there are exactly 2 k possible partitions. It is infeasible to calculate each such partitioning for each bucket in  X . Instead, we use a  X  X liding window X  technique to decompose B into two sub-buckets B 1 and B 2 as per the order of the Hilbert curve itself. For example, assume that B contains four constituent blocks B a , B b , B c , B Algorithm 1 Sliding Window Input: A Hilbert ordered data file R , the total bucket count Output: An enhanced multi-dimensional r-tree histogram. 1: Partition into blocks as per the naive r-tree histogram 2: Using the bottom up method, create I =  X   X  (  X   X   X  ) 3: Create a list  X  with the ( I  X  1 2  X  ) hyper-buckets of lowest 4: Within each hyper-block in  X , record the constituent 5: repeat 6: for each hyper-bucket B in the list  X  do 7: Using a  X  X liding window X  technique, examine the 8: Determine the two sub buckets, B 1 and B 2 , with the 9: if B 1 and B 2 represent an improvement over B 10: Replace B in  X  with B 1 and B 2 . 11: end if 12: end for 13: until |  X  | + | I | =  X  OR no buckets in  X  can be further Figure 5: Use of the the  X  X liding window X  method to decompose hyper-blocks along the curve. therefore consider the decompositions { ( B a ) , ( B b , B { ( B a , B b ) , ( B c , B d ) } , { ( B a , B b , B c ) , ( B exactly k  X  1 such splits. Figure 5 illustrates the way in which the sliding window method might be used to eliminate X  X ead space X  X hat has been accumulated during the traversal of the Hilbert curve.

During the recalculation of the quality measure, it is of course necessary to work with the raw distribution of points in the leaf level r-tree buckets. While this information could be retained in memory, the scale of many multi-dimensional data sets would make this infeasible. As such, the hyper-buckets of  X  maintain the list of block IDs constituting the hyper-bucket. When re-calculations are required, these blocks, and only these blocks, need be retrieved. It is there-fore unnecessary to re-scan the full data set in future rounds.
Once the primary hyper-buckets have been partitioned, they may be re-inserted into  X  for consideration during the next partitioning round. We note that while we partition each hyper-bucket into just two sub-buckets during the pre-vious step, the algorithm is free to further partition a sub-Figure 6: Examples of three typical point distribu-tions, each resulting in the same density measure-ment. bucket in subsequent iterations. The current list is then re-processed in a greedy fashion until either we have produced  X  total buckets or no improvement can be produced in the hyper-buckets currently found in  X . Typically, this happens within just two to three iterations.
In the previous section, we presented a sliding window al-gorithm that was meant to improve the uniformity of hyper-buckets. Within the blocks that comprise the hyper-buckets, however, the quality of the distribution must be defined more precisely. Traditionally, distribution quality is asso-ciated with a density metric. In effect, this provides us with an estimate of the number of points per unit square. For example, given a d -dimensional space with dimension cardi-nalities { C 1 , C 2 , . . . C d } , and a bucket B , we have B = therefore calculated as  X  count  X  provide a coarse estimate of the quality of the point distri-bution, it can lead to relatively significant estimation errors in non-uniform spaces. For example, Figure 6 displays three common bucket distributions. Each 2-d box contains ex-actly 10 points and is constructed with axes of equivalent length. Consequently, each has an identical density value and would therefore be deemed to provide selectivity es-timates of equivalent accuracy. However, this assumption is clearly not accurate. While Bucket A represents an es-sentially uniform distribution, Bucket B houses far more  X  X ead space X . Bucket C is even worse. It should be ob-vious that when the uniform spread assumption is applied to these three buckets, estimation error will vary widely.
Alternatively, one can define bucket quality in terms of the uniformity of its bucket distribution. In other words, the objective function should be defined it terms of a met-ric that seeks to minimize the dead space between bucket points. While a number of brute force approaches to such an optimization are certainly possible, the scale of the prob-lem requires a technique that is computationally superior to a naive O ( n 2 ) solution.
 To this end, we propose the k -uniformity (kU) metric. Rather than trying to estimate the relative  X  X loseness X  of bucket points, we take the possibly counter-intuitive ap-proach of assessing the relative  X  X mptiness X  of the space. To do do, we adapt a technique used in the construction of multi-dimensional indexes. Specifically, we will be using a variation of the partitioning strategy for the k-d-tree . Re-call that the k-d-tree [2] is a form of binary search tree in which the  X  X est X  attribute varies at successive levels. To be precise, for a d -dimensional space, we recursively partition on A 1 , A 2 , . . . A d , each time splitting the space on the A median. Figure 7 provides a simple illustration for a 2-d space.

We use this partitioning mechanism as the basis of the calculation of the k-uniformity metric described in Algo-rithm 2. Beginning with the points of the current bucket B , and an initially empty list of the volumes of the bucket X  X  partitions, we recursively partition the bucket space as per the k-d-tree strategy described in Algorithm 3. Essentially, high/low splits eventually reduce the m points of B to a list of m partitions, each containing a single point. If we con-sider the hyper-rectangular boundary of such a partition, its volume may be used as a representation of the dead space encapsulating each point. In other words, the greater the volume, the more isolated the point. It is this partition vol-ume that is added to the volume list L . When the recursion terminates, Algorithm 2 then calculates the standard devi-ation on the box volumes of L . This simple floating point value is the k-uniformity measure for B .
 Algorithm 2 k-uniformity calculation Input: A bucket B , a dimension set A 1 , A 2 , . . . A d Output: A k-uniformity measure. 1: Call the recursive k-partitioning function with argu-2: Calculate the average box volume V avg of the boxes in 3: Return the standard deviation of the boxes in L relative Algorithm 3 Recursive k-partitioning Input: A set of input points  X  current , the current dimension Output: A k-uniformity measure. 1: if The current partition size = 1 then 2: Calculate the volume V olume p of the current parti-3: Append the volume to the volume list L 4: end if 5: for the current dimension A i do 6: Find the median value M A i 7: Set  X  low =  X  M A i 8: Recurse on (  X  low , A i +1 , L ) 9: Set  X  high = &gt; M A i 10: Recurse on  X  high , A i +1 , L ) 11: end for Figure 8: Bucket breakdown by area for three pre-vious buckets.
 Figure 9: Bucket partitioning for the hTree his-togram.

Before describing the formal characteristics of Algorithm 2 and Algorithm 3, it is useful to provide a graphical picture of the k-partitioning technique. Figure 8 displays the space partitioning for the three buckets shown earlier. Bearing in mind that the purpose of a standard deviation calculation is to show the spread of values, the effect of the k-uniformity measure should be clear. Bucket A would produce a low kU measure, indicating desirable uniformity. The kU for Bucket B would be somewhat higher, while Bucket C would the most extreme of the three. As such, it would become an obvious candidate for restructuring.

While the kU metric clearly provides concrete advantages, such functionality cannot come at the price of unacceptable computational cost. Given that the kU measure is calculated on the points of each bucket, processing cost is proportional to the size of the full data set. If we examine Algorithm 3, we see that for an initial box with m points, we create O (lg m ) partitioning levels. For each of these levels, we must identify medians on the m points. For this we employ the random-ized median finding technique described in [13], which runs in O ( m ) time in the average case. As such, the complexity of the partitioning phase is O ( m lg m ), identical to that of an optimized k-d-tree construction algorithm.

Once the partitions have been generated, the kU metric is calculated. Since this is a volume-based calculation, the time complexity for the m partitions is simply O ( d  X  m ). The bound on the full computation is therefore the concate-nation of the two steps, or O ( m lg m + dm ). The dominant component depends of course on the dimension count and the size of the data set, but in most cases, the processing time would be bounded simply as O ( m lg m ). Finally, we note that since the kU for each box is computed indepen-dently, the full cost of kU measurement on the data set is simply the summation of the cost for each bucket. Figure 10: Bucket partitioning for the mHist his-togram.
 Figure 11: Bucket partitioning for the genHist his-togram.

To demonstrate the practical effect of kU based bucket partitioning, we have graphically captured the bucket parti-tioning patterns of hTree, mHist, genHist, and rk-Hist (kU + sliding window) on a two dimensional data set containing one million points. The data was synthetically generated us-ing a zipfian skew of 0.4 and cardinalities of 60000 on each dimension.

Figures 9, 10, 11, and 12 illustrate the final result. As per the design logic of the hTree, its bucket partitions il-lustrate a pronounced grid-like pattern. While effective for densely populated regions of the space, it tends to create boxes containing significant dead space in regions of reduced uniformity. The mHist algorithm, on the other hand, tends to produced extreme striping patterns on the data set which Figure 12: k-U based bucket partitioning for the rK-Hist histogram. clearly do not reflect the inherent clustering of the data set. genHist, in turn, also produces significant dead space, de-spite its ability to generate a more flexible grid.
By contrast, the r-tree/Hilbert cuvre/kU combination pro-duces a remarkably accurate decomposition of the point space. As can be seen in Figure 12, dead space is largely ig-nored. Moreover, the algorithm is able to vary box volumes so as to more accurately reflect clustering properties. The end result is a bucket generation mechanism that is far more likely than the alternatives to produce consistently lower estimation errors for arbitrarily defined multi-dimensional queries.
In this section, we provide experimental results that assess the performance of rK-Hist relative to the competing solu-tions. Specifically, we will compare the quality of rK-Hist to the traditional hTree, mHist, and the more recent genHist. We do not consider the dynamic methods such as stHoles since they serve a different purpose and since they have not been shown to outperform the best static methods across a broad range of test sets.

In terms of the data sets themselves, we use synthetic data as this allows us much greater flexibility in setting the primary test parameters. In this respect, we note that multi-dimensional histograms tend to perform extraordinarily well in uniform spaces (very easy to partition) and massively skewed environments (the data set deteriorates to a small number of singularities). Real data sets commonly used for DW testing (e.g., the ubiquitous weather data sets) actually have very little skew and largely resemble a uniform space. So while they tend to produce very nice results, their use often says relatively little about the performance of the his-tograms in more challenging environments. For this reason, we produce our own data sets using the standard zipfian skew function. (We note that for completeness, we expect to include results for real data sets in the longer version of this paper). Specifically, we generate both a moderately clustered data set D 1 with zipfian skew = 0.4, and a much more densely clustered data set D 2 with zipfian skew = 0.8. Both D 1 and D 2 contain one million tuples and dimensions with cardinalities of 1000 X 50000.
 All tests are conducted on a 3.2 GHz workstation, with 1 GB of main memory and a 160 GB disk drive operating at 7200 RPM. Each test run consists of 1000 multi-dimensional queries randomly generated so as to encapsulate approxi-mately 1% of the data space (query boxes have arbitrary shape). Following the convention described in [3], we calcu-late the average absolute error as error = 1 | W | q actual | for a query batch W . Test results are averaged across a set of five runs. Finally, we note that all rK-Hist testing uses a  X  ratio of 10%, as higher values rarely produce con-sistently superior results.

We begin by evaluating the estimation error for both D 1 and D 2 . For each data set, we provide results for histograms constructed with 300 and 800 buckets. Note that all his-tograms store the same information internally so all use the same amount of memory. In Figure 13 (a) and (b), we see the results for the smaller 300 bucket histograms. For both data sets, a distinct advantage is clear for rk-Hist versus the three competing methods. In fact, for the moderately clustered set, rK-Hist produces approximately half the er-Figure 13: Estimation error for 300 bucket his-tograms for (a) zipf = 0.4 and (b) zipf = 0.8. ror generated by the next best method. Interestingly, the original (and fairly simple) hTree outperforms mHist and is competitive with the recent genHist algorithm. This result is perhaps not surprising given the graphical partitioning re-sults presented in Section 6. We note as well that mHist per-forms particularly poorly on the moderately skewed data, a significant concern since D 1 is likely more indicative of many real world data sets than the more extreme D 2 .

In contrast, Figure 14 provides the results for the larger 800 bucket histograms. As one would expect, estimation er-ror improves noticeably. In fact, for the moderately skewed data, rK-Hist is able to produce errors in the range of just 2%-4% in two to three dimensions. This is extremely low for a multi-dimensional histogram. Moreover, none of the other methods is even close to this range.

As noted, our query workloads are defined so that individ-ual queries encapsulate about 1% of the total point space. Though this is in keeping with most previous research in the area, certain papers (e.g., genHist) have been evaluated against larger ranges. In general, this simplifies the problem and tends to produce lower (perhaps unrealistic) error rates. Nevertheless, in Figure 15(a) we provide a comparison on D (800 buckets) between the effect of using 1% ranges versus 5% ranges in the query batches. As expected, all algorithms improve, with rK-Hist providing single digit error rates up to six dimensions. genHist, while not as impressive, also performs well in this test.

Figure15(b) looks more closely at the effect of increasing dimensions for rK-Hist. In this case, we examine rK-Hist as dimension count grows from 2 to 10, using the 800 bucket histogram and the D 1 data set. There is of course an obvious growth in estimation error as we move into high dimensions, with errors increasing by approximately 30%-50% with each additional dimension. We note, however, that even at 10 dimensions, rK-Hist is competitive with the rates that other techniques produce in 5-6 dimensions. Figure 14: Estimation error for 800 bucket his-tograms for (a) zipf = 0.4 and (b) zipf = 0.8.

In Figure 15(c), we examine the impact of extending the naive r-tree histogram with the sliding window algorithm and the kU-partitioning. Again, we use the D 1 data set with 800 buckets for the comparison. There are two things to note. First, even the naive algorithm performs effectively relative to the numbers produced by the competing algo-rithms. Second, the estimation error for the naive algorithm is between 15% to 50% higher than rK-Hist, depending upon the dimensions count. So while the basic algorithm repre-sents a reasonably good starting point, the improved parti-tioning produced by the sliding window and the kU measure results in a histogram that is vastly superior, particularly in the commonly utilized 2-4 dimension range.

Finally, we compare the computational costs of three of the main algorithms across dimension counts from 2 to 9. Note that we do not include genHist in this test because, even with a logarithmic y-axis, the enormous times for gen-Hist make it difficult to produce a meaningful graph. Fig-ure 15(d) therefore illustrates the results for hTree, mHist, and rK-Hist. Not surprisingly, hTree is the most efficient method with its fairly trivial recursive grid partitioning. That being said, rK-Hist is quite competitive, a result re-flecting the efficiency of the sliding window and kU methods. mHist, by contrast, is almost two orders of magnitude more expensive by 9 dimensions. For comparative purposes, the genHist algorithm takes more than 12 hours to complete at nine dimensions. While it has been suggested that sampling could be used to reduce the cost, this approach would quite possibly lead to increased estimation error.
For many years histograms have been utilized in database environments to produce concise representations of larger data distributions. The resulting estimates are fundamen-tal to both query approximation and selectivity estimation. Figure 15: (a) 1% versus 5% query ranges (b) di-mension counts ranging from 2 to 10 (c) rK-hist versus naive r-tree histogram (d) the relative con-struction cost of the four algorithms (logscale on the y-axis). However, while errors in single dimension environments are quite impressive, multi-dimensional distributions have prov en to be far more challenging. In this paper, we present rK-Hist, an r-tree based histogram that exploits the Hilbert space filling curve to generate an initial space partition-ing. It then uses a sliding window method, coupled with a new uniformity measure, to further improve the quality of the selectivity estimates. Experimental testing against a number of existing methods demonstrates consistent and significantly superior results in terms of estimation qual-ity. Given these results, and the fact that rK-Hist can be integrated so cleanly with with one of the most common multi-dimensional indexing models, we believe the current methods represent an extremely attractive option for selec-tivity estimation and approximate query answering in multi-dimensional environments. [1] N. Beckmann, H. Kriegel, R. Schneider, and B. Seeger. [2] J. Bentley. Multidimensional binary search trees used [3] N. Bruno, S. Chaudhuri, and L. Gravano. STHoles: a [4] S. Christodoulakis. Implications of certain [5] C. Faloutsos and S. Roseman. Fractals for secondary [6] V. Gaede and O. Gunther. Multidimensional access [7] S. Guha, K. Shim, and J. Woo. REHIST: Relative [8] D. Gunopulos, G. Kollios, V. Tsotras, and [9] A. Guttman. R-trees: A dynamic index structure for [10] A. Guttman. R-trees: A dynamic index structure for [11] P. Haas and A. Swami. Sequential sampling [12] D. Hilbert. Ueber die stetige abbildung einer line auf [13] C. Hoare. Algorithym 63 (partition) and algorithm 65 [14] Y. Ioannidis. The history of histograms (abridged). [15] Y. Ioannidis and V. Poosala. Balancing histogram [16] H. Jagadish. Linear clusertering objects with multiple [17] H. Jagadish, N. K. N., S. Muthukrishnan, V. Poosala, [18] I. Kamel and C. Faloutsos. On packing r-trees. CIKM , [19] R. Kooi. The optimization of queries in relational [20] J. Lee, D. Kim, and C. Chung. Multi-dimensional [21] S. Leutenegger, M. Lopez, and J. Eddington. STR: A [22] M. Muthukrishnan and D. Dewitt. Equi-depth [23] S. Muthukrishnan, V. Poosala, and T. Suel.
 [24] G. Peano. Sur une courbe, qui remplit toute une aire [25] G. Piatetsky-Shapiro and C. Connell. Accurate [26] V. Poosala and Y. Ioannidis. Selectivity estimation [27] V. Poosala, Y. Ioannidis, P. Haas, and E. Shekita. [28] N. Roussopoulos and D. Leifker. Direct spatial search [29] T. Sellis, N. Roussopoulos, and C. Faloutsos. The [30] J. Spiegel and N. Polyzotis. Graph-based synopses for [31] U. Srivastava, P. Haas, V. Markl, M. Kutsch, and [32] N. Thaper, S. Guha, P. Indyk, and N. Koudas. [33] J. Vitter, M. Wang, and B. Iyer. Data cube
