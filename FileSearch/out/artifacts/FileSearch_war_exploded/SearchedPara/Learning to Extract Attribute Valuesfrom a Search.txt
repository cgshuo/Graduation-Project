 One of the most important goals of building knowledge bases is to build a big table of entities with attributes and the corresponding attribute values . For example, for a celebrity, has the attributes :  X  X eight X ,  X  X eight X , and  X  X ducation X  and their corr e-sponding attribute values :  X  1.75m X ,  X 65kg X , and  X  X xx university X , et c. Entity extra c-this paper focuses on the more challenging task , attribute value extraction. The pro b-lem setting is: given a list of named entities of a domain (e.g. c elebri ties), and the attribute names (e.g. birthdate, height , etc.), an algorithm is expected to output triples of &lt;entity name, attribute name, attribute va l ue&gt; for all the entities and attributes. In this paper, when we mention  X  va l ue  X  , if not specified, it means  X  attribute value  X  .
An important observation is that many entity attribute values can be found in free text, e.g., there are many sentences like  X  Chris Pine received his bachelor  X  s degree from University of California, Berkeley  X  from which we can extract the  X  X duc a tion X  attribute value. Hence it is feasible to extract attribute values from the web text . Ho w-ever, the challenges are obv i ous: ( A ) . It is a huge computational obstacle to parse all the free text in the entire web; ( B ) . How to make cert ain that a piece of text is indeed description of the target a t-tribute values other than some irrelevant information. 
In our approach, f or a target entity and the attribute name, we take the following steps to get the attribute value: Step1 : we formulate queries according to the entity and the attribute, and submit them to a search engine and harvest the returned sni p-pets, which often contain cand i date attribute values. Step2 : a pattern based detector is applied to locate the candidate attribute values in each sni p pet. Step3 : a statistical classifier is used to predict whether a candidate value in Step2 is a correct one. Step4 : as a correct value often appears in multiple snippets, to exploit such redundant info r-mation, all the i n dividual predictions of Ste p3 are assembled together by voting and then output the final answer of the attribute value. 
In the above steps, the key issue is how to train the classifiers in Step3 and dete r-mine the voting weights in Step4 . Directly l a belling the candidate slots in th e snippets to train the classifier s for Step3 requires tremendous human effort . Instead, we r e-quire few accurate (manually labelled or from some trustful know ledge bases) &lt;ent i-the pieces of texts contai n ing these correct attribute values. Thus they can be used as pseudo labelled data to train the cla s sifier of Step3 .

In the proposed approach, challenge ( A ) is conquered by leveraging a search e n-gine to help us fi nd the ca ndidate pieces of text about the entity attributes from the entire web. The underlying assumption is: if a powerful search engine cannot find the pages containing the co r rect attribute value for an entity, the entity /attribute value must be very rare and w e give up . Also, c onsidering the current commercial search engines have already indexed more than billions of web pages, this as sumption is reasonable. For challenge ( B ) , our approach adopt s a learning approach with very few labelled data. The underlying a ssumption is, for the same attribute, although diffe r ent entities will have differ ent attribute values, the expression ways in free text are sim i-lar. In addition, the a ssembl ing process in Step4 we ll utilized the redundancy info r-mation from the web to depr eciate noise: if many pages agree on an attribute value, it is more likely to be true.

We evaluated our algorithm in celebrity X  X  domain (on both Chinese and English corpus), and remar kable performance is achieved: 7 of the 1 2 attributes X  precision are over 85%. We also compared our algorithm with a state -of -the -art system and find that 1 1 of the 1 2 attributes have promising improvements.

The contributions of this paper are: (1) We proposed an approach to leverage a search engine to retrieve the candidate pi eces of free texts describing a target entity and attributes from the entire web, rather than very limited websites like Wikipedia as in the previous approaches ( [11, 14 ] ) . Experiments show that this can significan t ly improve recall. (2) We proposed a lear ning approach which learns the ways of d e-scribing attribute values of entities in free text. By utilizing the search engine again, the learning approach only requires very few l abelled ground truth triple s of &lt;entity, a t tribute, value&gt; . The rest of the pa per is organized as follows. Section 2 introduc es related work. Section 3 presents the proposed algorithm . The exper i mental setup is presented in Section 4 , and the paper concludes in Section 5 .
The existing attribute value extraction methods can be roughly categorized in to two types : rule -based and machine -learni ng -based algorithms. Rule based algorithms mainly rely on attribute specific rule s . [6 ] is the champion team of the attribute value extraction task in WePS 2009. First, t hey classify t he pages by chec k ing keywords in page title ; then for each type of page, keywords and rules for an attribute is employed to extract attribute values. [7 ] e x tracts numerical values by sending queries such as too m any complex attribute specifi c rules, and they usually have high precision s but low recall s . However, the rules in our method are much simpler; thus, recall is gua r-a n teed. Further, a robust statistical classifier is employed to confirm the extra c tions .
Machine learning based algorithms usually need some training data. [11 ] employs may have several name s (e.g. weight, strung weight ) , t o build a knowledge base , we must disambiguate attribute names , which is not an easy task . [2 ] mainly extract n u-merical values; it is viewed as a decision variable that whether a candidate value should be assigned to an attribute. The final value of t he decision variables are a s-signed by solving a constrained optimization problem and t he ground fact s in training data are constrains of the problem. To further increase the performance, some attri b-ute specific common sense ( e.g. unstrung weight is smaller than strung weight ) is additionally introduced as constrains . In addition, the computational cost is great. While o ur algorithm can extract other values besides n u merical values, and additional common sense constrains are not n eeded . The work in [14 ] is m ost rela ted to ours. They present Kylin , which fill s the empty values in Wikipedia  X  s infobox. Kylin match es the existing values in infobox back to wiki article s and train a sentence cla s-sifier to predi ct whether a se n tence contains certain type of value. T hen e xtracting attribute values from these selected sentence s is viewed as a sequential labelling pro b-lem , in which CRF is employed . T he training data is acquired again by leveraging the existing values in infobox. But they only extract attribute values f rom a single Wi k-ipedia page, and do not take advantage of the information from other sites. In [16 ] , they also noticed that information on a single page is inadequate, so they employ the ontology in [15 ] and then articles in hypernym and hyponym classes ar e used as add i-tional training data. To increase recall, they also use a search engine to get some rel e-vant pages and extract attribute values from these pages . But if models are trained in Wikipedia pages and it run s on general pages, the extraction result s may suffer. B e-sides, for some multi -word values, CRF may cause boundary detection e r rors. While our method extracts the candidate values as a whole, and boundary dete c tion errors will not happen.
 The tasks of Relation Extraction and that of Attribute Val ue Extraction are similar. Some relations such as bornOnDate and graduatedFrom in relation extraction are just the attribute values of a person  X  s birthdate and education attributes , while some relations such as producesProduct and publicationWritesAbout ca nnot be viewed as attribute values of certain entity. Pattern -based relation extraction (e.g. [8 ], [17 ], [3 ] ) usually boo t straps with some seed relations ( facts) , and in every iteration new patterns and facts are extracted and then evaluated by statistical measures such as PMI. Usua l-ly, recalls of these methods are high, but these methods often produce noisy patterns and may dri ft away from target relations. [5 ] proposed a multi -stage bootstrapping ontology construction algorithm . In each iteration, t hey i ntegrated CPL and CSEAL ( [4 ] ), which are all pa t tern -based relation extractor , to fetch candidate facts, and then a statistical classifier is employed to fu r-ther refine the meta -extractions. However, after each iteration of rule learning, bad extraction ru les must be remove d manually .

Our approach ha s a rule based component to detect the candidate attribute values in a piece of free text, get ting high recall but low precision candidates. A nd we also have a successive learning component, a statistical classi fier , to further confirm whether a ca n didate value is indeed a correct one considering the features from it s context. Suppose that in th e target domain , there are N unique named entities and A attri b utes, and our system is expected to output N X A attribute values. For each entity e and an attribute name a , we will get one attribute value v . T he workflow of our approach is described in Figur e 1. The system has four main successive components : Corpus Fetcher , Candidate Value Detector , Attribute Value Classifier and Voter . First, in the component of Corpus Fetcher , we formulate a query by concatenating the entity e and the attribute name a . For example, for an entity e =  X  Micha e l Jackson X , and the attri b-ute a =  X  Birthday X , the formulated query is  X  Micha e l Jackson Birthday X . The query is sen t to a search engine, and we fetch the titles and snippets of the top K (=25 in our experiments) returned r esults. Then, entering into the component Candidate Value Detector , for an attribute, we define some patterns to filter the obviously wrong slots in a snippet, for example, some attribute values like nationality must belong to a finite enumerable list. Suc h a pattern based detector is used to roughly locate candidate values in a snippet . Notice that this detector will have high recall but low precision. Next is the task of the component of Attribute Value Classifier , a binary statistical classifier that is used to predict whether a candidate value is confident. The prediction is based on features extracted from snippet s containing the candidate value and the snippet  X  s corresponding title. Then a ll the non -confident candidate values are discar d-ed . Finally, n ot ice that the previous step may produce the same cand i date value in different snippets, and it is the often case that the true attribute value appears in mu l-tiple snippets. To utilize this redundancy information and make the final decision, we adopt a com ponent of Voter to as semble the predictions in the previous step by vo t-ing. T he voter assigns a weight to each confident value (the refined candidate value in the last step) and accumulates the weights of the same attribute value as it s voting score. The candidate values are ranked by their voting score and the extracted attri b-ute value is the one with the highest score. For instance, we have four confident va l-ues (three unique ones),  X  v1 v2 v1 v3  X  . After weighting, each value gets a weight, e.g.  X  v1:0.98 v2:0.64 v1:0.72 v3:0.99  X  . After accumulation, the voting scores are  X  v1:1.7 v2:0.64 v3:0.99  X  . So, the final extracted attribute value is  X  v1  X  . 
In the following part s of the section, we will introduce the main components of the system in detail . 3.1 Candidate Value Detector For an attribute a , there should be a validity checker to jud ge whether a candidate value is a valid one. For example, for the attribute birthday , a candidate value should be of a valid date format. This paper considers the following two broad cases where enumerable set, e.g., a valid nationality value must belongs to the set of names of all the countries in the world ( there are overall 192 countries ); (2) the range of an attri b-ute value can be described by nontrivial regular expr essions. For example, birthday va l ues have such formats as  X 0000 -00 -00 X  (e.g. 1986 -10 -12) or  X  &lt;Month&gt; 00, 0000  X  , etc. The tested attributes in the p aper are all of the two cases, whose valid formats (validity checkers) are shown in Table 1. Actually, for o ther cases where a value v a-lidity checker is pr o vided, the method of this paper can also apply. For example, f or the attribute of spouse , whose value should be of the type of person , we can define a validity checker based on some NER algorithms .

The valid ity checker for an attribute plays the role of a candidate value detector, which detects the valid values in the snippets as candidates. Notice that in this step, some valid but incorrect values may also be extracted as candidates . For example , when we de tect valid birthdate values , some irrelevant dates such as the report date s and the page  X  s date s may also be extracted. Actually i n this step, we care more about recalls than precisions . I n the next step s (Sec tion 3.2 , and Sec tion 3.3 ) , from different aspe cts, we will further filter the candidate values produced in this step to promise high precision. Sec tion 3.2 will filter the candidate values by a classifier considering the context of a candidate value in a snippet. And Sec tion 3.3 will utilize the fact that a correct value often appears in multiple snippets to design a voting mechanism, so that the correct value agreed by multiple snippets is picked up while many incorrect candidates are filtered. 3.2 Attribute Value Classifier
The candidates Candidate Valu e Detector output may be incorrect for the target e n tity. I t may be the case that o ne snippet may describe several named e ntities (e.g. celebrities), and the candidate may be other entity  X  s value . In addition, a candidate may be some noise in snippets. For example, the candidate  X  birthdate  X  may be just the report date of a piece of news. Thus, we introduce a stati s tical classifier , t he Attribute Value Classifier , which aims at refining these candidates by utilizing the featu res in the snippet containing the candidate and the corresponding title of the snippet. 
We train one binary classifier for each attribute , which tries to predict whether a candidate value is confident . The classifi cation model we used is Maximum Entropy Model, which can pro vide the proba bili ties of prediction s . And in the next section, these probabilities will be used to improve voting.  X  Features
The prediction is based on features extracte d from the snippet containing the ca n-didate value and the corresponding title. We use two types of f eatures: title features descri b ing topics of search results, and snippet features encoding local information of the se fe a tures except (3) and (7) are binary. (1) Whether the title contains the current named entity. This is a strong indication (2) Whether the title contains o ther named entit ies of the same class . For example, (3) Other words w ith their POS tags in the title. For example, the title is  X  Michael (4) In the sentence that the candidate value appears , w he ther the current attribute (5) In the sentence that the candidate value appears, whether the current named entity (6) In the sentence that the candidate value appears, whether the other named entities (7) Other words with their POS tags and distance to the candidate value in the se n-
The intuiti ons under the feature design are: (1) if a search result is describing the current named entity, then it is likely that the candidate value in the snippet is correct; (2) if the current named entity or the current attribute appears in the same sen tence wi th the candidate value, then it is also likely that the value is correct .  X  Generating Training Data
Training the classifier needs labelled data. It is not practical to manually annotate the correct attribute values in each snippet. W e propose a method to re duce the labe l-ling effort. Rather than relying on direct annotations on each snippet, we only require a few correct &lt;entity, attribute, value&gt; triples, which are matched back to the search results (title and snippets) to generate training data for the clas sif i er. An a dvantage of this method is that it is easy to get a few correct &lt;entity, attribute, value&gt; triples, e i-ther by human labelling or from some structured sites such as W i kipedia.

Step 1: A nnotate some &lt; entity, attribute, value&gt; triples (only 15 triples in the e x-periment) . We can also get these triple s from some structured sites (e.g. Wikipedia , Bio27, etc.).

Step 2: Submit queries to a search engine and m atch the se &lt;e ntity, attribute, va l-ue&gt; triple s back to search results. For example, w e are extracting celebrities  X  birthdate, and we have a labelled triple of &lt; X  X ichale Jackson X ,  X  Birthda te  X ,  X  19580829  X  &gt;. We send the query  X  Michael Jackson birthdate  X  to a search engine and get the top K ( K =25 in our experiments ) search results (titles and sni p pets). Then the Candidate Value Detector extracts all the candidate values and converts them to standard form at s ( X  X yyymmdd X ) . If a candidate value equals to the true value (  X  19580829  X  ), then we annotate a positive label to the value in the snippet and get a positive training example; otherwise , we get a negative training example. In this way, each candidate value in a snippet will produce a training example. T he number of training example s generated by the 15 &lt;entity, attribute, value&gt; triple s for each attri b-ute is shown in Table 2. The number of positive training examples is not nece s sarily less than the number of negative training examples. Proportions of positive training examples are in the  X  X os X  columns . 3.3 Voter
After the classification, there may be several candidate values for a n entity  X  s a t-tribute and t he correct value often appears in multiple snippets. Intuitively, the most frequent candidate value is most likely to be the correct value. Th erefore , a simple strategy is to count how many times a c andidate value is classified as confident value by the classi fier. However, this may cause a problem when several candidate va lues get the same highest score . To alleviate the problem, we leverage the classification probabilities provided by the Attribute Value Classifier and use the probabilit y as each vote  X  s weight. E xperiment s show that this strategy can i m prove precision s and recall s by about 1%. 
We use Baidu 1 (Chinese ) and Google (English) to test our algorithm. For our A t-tribute Value C lassifier , we employ a Maximum Entropy m odel impl e mented by Le Zhang [ 18 ] . We employed L -BFGS and the Gaussian prior is 1.0 . 4.1 Evaluation Dataset The experi ments were condu cted in the celebrity domain ( on both Chinese and English corpus ). We collected the 4476 celebrities in  X  qq entertainment 2  X  as our Ch i-nese named entity list . And we crawled celebrities  X  data in  X  qq entertainment  X  ,  X  ifeng entertainment 3  X  ,  X  b aidu baike 4  X  ,  X  hudong baike 5  X  and  X  X ikipedia 6  X  as our Chinese standard evaluation dataset. Similarly , we collected 1600 celebrities in  X  bio27 7  X  as our English named entity list. And we crawled celebrities  X  data in  X  bio27  X  as our En g-lish standard evaluation dataset. Some named entities  X  values cannot be found in all these sites, and their values are empty in our dataset. N umber s of non empty values for each attribute are in Table 3. 4.2 Experimental Results We test ed 14 attribute values, and 8 of them are on Chinese corpora ( alias Cx ) , while 4 of them are on English corpora ( alias Ex ) . In addition , we use 15 &lt;entity, attribute, valu e&gt; triples for each attribute to generate training data an d train the Attribute Value C lassifier . We evaluated their Precisions and Recalls. As some named entities  X  certain attribute may not exist (e.g. M . Jackson is an American, and does not have the attr i b-ute  X   X  X  X  (Ethnic Group)  X  ) , we only evaluate the attribute values in the our d a taset. The results in Table 4 show that 7 of the 1 2 attributes X  precision are over 85% . 4.3 Single Site vs. Multiple S ites
T o increase recall, we leverage a search engine to extract attrib ute value s from the entire web , rather than very limited websites . In this section, we provide evidences for this claim. W e studied the best recall s an algorithm can achieve on two sites , namely, Wikipedia and Baidu -baike (Chinese version Wikipedia ), and c ompared them with that of the proposed algorithm. Specifically, for a target entity and one of its attri b-ute s , if its correct attribute value for the current attribute can be found on the entity  X  s page, it does count for a correct extraction. The compariso n in Figure 2 shows that the recall of the proposed algorithm is better than the best recalls on Wikipedia and Ba i-du -baike. That is to say that no algorithm targeting at these two sites can have a better recall than the proposed algorithm. Therefore, it is necessary to leverage the info r-mation from multiple sites.
 4.4 Comparison to a Previous System I n this section, we compare the performance of different methods on algorithm level (on the same corpora).

It can be difficult to compare our results with other attribute value extraction sy s-tems. Unl ike semantic role labelling, there are some public available datasets (e.g. Pro p Bank and Pen TreeBank) . The datasets used by previous systems are different from ours. One feature of our system is the leverage of multiple sites data, so we ca n-n ot only use W ikipedia as in [ 14 ] ; besides numerical values, we can extract other kind of values, so we cannot use the dataset in [ 2 ] . Finally , we impl e mented Kylin [ 14 ] .
In experiments, we implemented Kylin in [ 14 ] to extract attribute values from sni p-pets. We test ed Kylin with 15 annotated &lt; entity , attr ibute, value&gt; triple s and 100 a n-notated &lt; entity , attribute, value&gt; triple s ( the same amount of annotated triple s with our met h od ) respectively . And the comparisons are shown in Figure 3 and Figure 4 respectiv e ly .

In Figure 3 , ME15+Vote have a better precis ion , recall and F -score on all attributes except the attribute  X   X  X  X  (Ethnic Group)  X  ( CEG ) . Among these attributes, the i m-provements on  X   X  X  X  X  X  X  (birthdate)  X  ( CB ),  X   X  X  X  X  X  X  (education)  X  ( CE ),  X   X  X  X  X  (English name)  X  ( CEN ) and  X  birthdate  X  ( CB ) are obvious. In Figure 4 , results of Kylin with 100 annotated pairs are better than their results with 15 annotated triples on most attributes. However, the results are similar with that of 15 annotated triples: still only the results of  X   X  X  X  (Ethnic Group)  X  ( CEG ) are better than the proposed method. T he proposed method leverages a rule -based detector to locate the candidate values and during classification, the features in title, which r e flects the topic information of the snippet , are used . We believe the se factors above lead to a better pe r formance. 4.5 Impact of the amount of annotated data
We also studied the effects of different amount annotated &lt; e ntity , attribute , value&gt; triple s on F -scores. It is shown in Figure 5 . We can see when the amount of annotated triple s is between 5 and 15, some attributes  X  (e.g.  X  X  X  (weight)) F -scores increase significantly; when the amount is between 15 and 100, F -scores do not increase much; when the amount is between 100 and 300,  X   X  X  X  X  (English name)  X  and weight have about 5% increase. But for 5%  X  s improvement to annotate 20 times of data is not worthy. So we use 15 annotated triple s, and they are suff i cient for most attributes.
In this paper, we proposed an attribute extraction algorithm. The attribute values are extracted from snippets returned by a search engine . We use some strict (mainly rule based) methods to locate the possible values in the snippets. Then a classifier is used to predict whether the candidate value is a confident one. To train the classifier, we only need to annotate very little data, and suf ficient training data will be generated automatically. A correct value may appear in multiple snippets, and we also have a strategy to vote and score the confident values.

