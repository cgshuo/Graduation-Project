 Lawrence Cayton lcayton@cs.ucsd.edu Nearest neighbor (NN) search is a core primitive in machine learning, vision, signal processing, and else-where. Given a database X , a dissimilarity measure d , and a query q , the goal is to find the x  X  X mini-mizing d ( x,q ). Brute-force search is often impractical given the size and dimensionality of modern data sets, so many data structures have been developed to accel-erate NN retrieval.
 Most retrieval data structures are for the ` more generally, metrics. Though many dissimilarity measures are metrics, many are not. For example, the natural notion of dissimilarity between probability distributions is the KL-divergence (relative entropy), which is not a metric. It has been used to compare histograms in a wide variety of applications, includ-ing text analysis, image classification, and content-based image retrieval (Pereira et al., 1993; Puzicha et al., 1999; Rasiwasia et al., 2007). Because the KL-divergence does not satisfy the triangle inequality, very little of the research on NN retrieval structures applies. The KL-divergence belongs to a broad family of dis-similarities called bregman divergences . Other exam-ples include Mahalanobis distance, used e.g. in classi-fication (Weinberger et al., 2006); the Itakura-Saito di-vergence, used in sound processing (Gray et al., 1980); and ` 2 lenge for fast NN retrieval since they need not be sym-metric or satisfy the triangle inequality.
 This paper introduces bregman ball trees (bbtrees), the first NN retrieval data structure for general bregman divergences. The data structure is a relative of the popular metric ball tree (Omohundro, 1989; Uhlmann, 1991; Moore, 2000). Since this data structure is built on the triangle inequality, the extension to bregman divergences is non-trivial.
 A bbtree defines a hierarchical space decomposition based on bregman balls; retrieving a NN with the tree requires computing bounds on the bregman divergence from a query to these balls. We show that this diver-gence can be computed exactly with a simple bisection search that is very efficient. Since only bounds on the divergence are needed, we can often stop the search early using primal and dual function evaluations. In the experiments, we show that the bbtree provides a substantial speedup X  X ften orders of magnitude X  X ver brute-force search. This section provides background on bregman diver-gences and nearest neighbor search. 2.1. Bregman Divergences First we briefly overview bregman divergences. Definition 1 (Bregman, 1967) . Let f be a strictly convex differentiable function. 1 The bregman diver-gence based on f is One can interpret the bregman divergence as the dis-tance between a function and its first-order taylor ex-pansion. In particular, d f ( x,y ) is the difference be-tween f ( x ) and the linear approximation of f ( x ) cen-tered at y ; see figure 1. Since f is convex, d f ( x,y ) is always nonnegative.
 Some standard bregman divergences and their base functions are listed in table 1.
 A bregman divergence is typically used to assess sim-ilarity between two objects, much like a metric. But though metrics and bregman divergences are both used for similarity assessment, they do not share the same fundamental properties. Metrics satisfy three basic properties: non-negativity: d ( x,y )  X  0; symmetry: d ( x,y ) = d ( y,x ); and, perhaps most importantly, the triangle inequality: d ( x,z )  X  d ( x,y ) + d ( y,z ). Breg-man divergences are nonnegative, however they do not satisfy the triangle inequality (in general) and can be asymmetric.
 Bregman divergences do satisfy a variety of geometric properties, a couple of which we will need later. The bregman divergence d f ( x,y ) is convex in x , but not necessarily in y . Define the bregman ball of radius R around  X  as Since d f ( x, X  ) is convex in x , B (  X ,R ) is a convex set. Another interesting property concerns means. For a set of points, the mean under a bregman divergence is well defined and, interestingly, is independent of the choice of divergence: This fact can be used to extend k -means to the family of bregman divergences (Banerjee et al., 2005). 2.2. NN Search Because of the tremendous practical and theoreti-cal importance of nearest neighbor search in machine learning, computational geometry, databases, and else-where, many retrieval schemes have been developed to reduce the computational cost of finding NNs.
 KD-trees (Friedman et al., 1977) are one of the earli-est and most popular data structures for NN retrieval. The data structure and accompanying search algo-rithm provide a blueprint for a huge body of future work (including the present one). The tree defines a hierarchical space partition where each node defines an axis-aligned rectangle. The search algorithm is a sim-ple branch and bound exploration of the tree. Though KD-trees are useful in many applications, their per-formance has been widely observed to degrade badly with the dimensionality of the database.
 Metric ball trees (Omohundro, 1989; Uhlmann, 1991; Yianilos, 1993; Moore, 2000) extend the basic method-ology behind KD-trees to metric spaces by using met-ric balls in place of rectangles. The search algorithm uses the triangle inequality to prune out nodes. They seem to scale with dimensionality better than KD-trees (Moore, 2000), though high-dimensional data remains very challenging. Some high-dimensional datasets are intrinsically low-dimensional; various retrieval schemes have been developed that scale with a notion of intrin-sic dimensionality (Beygelzimer et al., 2006). In many applications, an exact NN is not required; something nearby is good enough. This is especially true in machine learning applications, where there is typically a lot of noise and uncertainty. Thus many researchers have switched to the problem of approxi-mate NN search. This relaxation led to some signifi-cant breakthroughs, perhaps the most important be-ing locality sensitive hashing (Datar et al., 2004). Spill trees (Liu et al., 2004) are another data structure for approximate NN search and have exhibited very strong performance empirically.
 The present paper appears to be the first to describe a general method for efficiently finding bregman NNs; however, some related problems have been examined. (Nielsen et al., 2007) explores the geometric properties of bregman voronoi diagrams. Voronoi diagrams are of course closely related to NN search, but do not lead to an efficient NN data structure beyond dimension 2. (Guha et al., 2007) contains results on sketching breg-man (and other) divergences. Sketching is related to dimensionality reduction, which is the basis for many NN schemes.
 We are aware of only one NN speedup scheme for KL-divergences (Spellman &amp; Vemuri, 2005). The results in this paper are quite limited: experiments were con-ducted on only one dataset and the speedup is less than 3x. Moreover, there appears to be a significant technical flaw in the derivation of their data structure. In particular, they cite the pythagorean theorem as an equality for projection onto an arbitrary convex set, whereas it is actually an inequality . This section describes the bregman ball tree data structure. The data structure and search algorithms follow the same basic program used in KD-trees and metric trees; in place of rectangular cells or metric balls, the fundamental geometric object is a bregman ball.
 A bbtree defines a hierarchical space partition based on bregman balls. The data structure is a binary tree where each node i is associated with a subset of the database X i  X  X . Node i additionally defines a breg-man ball B (  X  i ,R i ) with center  X  i and radius R i such that X i  X  B (  X  i ,R i ). Interior (non-leaf) nodes of tree have two child nodes l and r . The database points belonging to node i are split between child l and r ; each point in X i appears in exactly one of X l or X r . 2 Though X l and X r are disjoint, the balls B (  X  l ,R l and B (  X  r ,R r ) may overlap. The root node of the tree encapsulates the entire database. Each leaf covers a small fraction of the database; the set of all leaves cover the entirety. 3.1. Searching This subsection describes how to retrieve a query X  X  nearest neighbor with a bbtree. Throughout, X = { x is a (fixed) bregman divergence. The point we are searching for is the left NN Finding the right NN (argmin x  X  X d f ( q,x )) is consid-ered in section 5.
 Branch and bound search locates x q in the bbtree. First, the tree is descended; at each node, the search al-gorithm chooses the child for which d f (  X ,q ) is smallest and ignores the sibling node (temporarily). Upon ar-riving at a leaf node i , the algorithm calculates d f ( x,q ) for all x  X  X i . The closest point is the candidate NN; call it x c . Now the algorithm must traverse back up the tree and consider the previously ignored siblings. An ignored sibling j must be explored if The algorithm computes the right side of (1); we come back that in a moment. If (1) holds, then node j and all of its children can be ignored since the NN can-not be found in that subtree. Otherwise, the subtree rooted at j must be explored. This algorithm is easily adjusted to return the k -nearest neighbors.
 The algorithm hinges on the computation of (1) X  X he bregman projection onto a bregman ball . In the ` 2 2 (or arbitrary metric) case, the projection can be computed analytically with the triangle inequality. Since general bregman divergences do not satisfy this inequality, we need a different way to compute X  X r at least bound X  the right side of (1). Computing this projection is the main technical contribution of this paper, so we discuss it separately in section 4. 3.2. Approximate Search As we mentioned in section 2.2, many practical appli-cations do not require an exact NN. This is especially true in machine learning applications, where there is typically a lot of noise and even the representation of points used is heuristic ( e.g. selecting an appropriate kernel for an SVM often involves guesswork). This flexibility is fortunate, since exact NN retrieval meth-ods rarely work well on high-dimensional data. Following (Liu et al., 2004), a simple way to speed up the retrieval time of the bbtree is to simply stop af-ter only a few leaves have been examined. This idea originates from the empirical observation that metric and KD-trees often locate a point very close to the NN quickly, then spend most of the execution time back-tracking. We show empirically that the quality of the NN degrades gracefully as the number of leaves ex-amined decreases. Even when the search procedure is stopped very early, it returns a solution that is among the nearest neighbors.
 3.3. Building The performance of the search algorithm depends on how many nodes can be pruned; the more, the better. Intuitively, the balls of two siblings should be well-separated and compact. If the balls are well-separated, a query is likely to be much closer to one than the other. If the balls are compact, then the distance from a query to a ball will be a good approximation to the distance from a query to the nearest point within the ball. Thus at each level, we X  X  like to divide the points into two well-separated sets, each of which is compact. A natural way to do this is to use k -means, which has already been extended to bregman divergences (Baner-jee et al., 2005).
 The build algorithm proceeds from top down. Start-ing at the top, the algorithm runs k -means to partition the points into two clusters. This process is repeated recursively. The total build time is O ( n log n ). Clus-tering from the bottom-up might yield better results, but the O ( n 2 log n ) build time is impractical for large datasets. Recall that the search procedure needs to determine if the bound holds, where x c is the current candidate NN. We first show that the right side can be computed to accu-racy in only O (log 1 ) steps with a simple bisection search. Since we only actually need upper and lower bounds on the quantity, we then present a procedure that augments the bisection search with primal and dual bounds so that it can stop early.
 The right of (2) is a convex program: The search algorithm will need to solve (P) many times in the course of locating q  X  X  NN, so we need to be able to compute a solution very quickly.
 Before considering the general case, let us pause to examine the ` 2 projection x p analytically: where  X  = What properties of this projection might extend to all of bregman divergences? 1. First, x p lies on the line between q and  X  ; this 2. Second, x p lies on the boundary of B (  X ,R ) X  i.e 3. Finally, since the ` 2 We prove that the first property is a special case of a fact that holds for all bregman divergences. Addi-tionally, the second property generalizes to bregman divergences without change. The final property does not go through, so we will not be able to find a solution to (P) analytically.
 Throughout, we use q 0  X   X  f ( q ),  X  0  X   X  f (  X  ), etc. simplify notation. x p denotes the optimal solution to (P).
 Claim 2. x 0 p lies on the line between q 0 and  X  0 . Proof. The lagrange dual function of (P) is where  X   X  0. Differentiating (3) with respect to x and setting it equal to 0, we get We use the change of variable  X   X   X  to arrive at where  X   X  [0 , 1).
 Thus we see that property 1 of the ` 2 special case of a relationship between the gradients; it follows from claim 2 because  X  f ( x ) = x for the ` 2 divergence.
 Since f is strictly convex, the gradient mapping is one-to-one. Moreover, the inverse mapping is given by the gradient of the convex conjugate , defined as Symbolically: Thus to solve (P), we can look for the optimal x 0 along  X  X  0 + (1  X   X  ) q 0 , and then apply  X  f  X  to recover x p keep notation simple, we define Now onto the second property.
 Claim 3. d f ( x p , X  ) = R  X  X .e. the projection lies on the boundary of B (  X ,R ) .
 The claim follows from complementary slackness ap-plied to (3). Claims 2 and 3 imply that finding the projection of q onto B (  X ,R ) is equivalent to Fortunately, solving this program is simple.
 Claim 4. d f ( x  X  , X  ) is monotonic in  X  .
 This claim follows from the convexity of f  X  . Since d ( x  X  , X  ) is monotonic, we can efficiently search for  X  p satisfying d f ( x  X  We summarize the result in the following theorem. Theorem 5. Suppose k X  2 f  X  k 2 is bounded around x 0 p . Then a point x satisfying can be found in O (log 1 / ) iterations. Each iteration requires one divergence evaluation and one gradient evaluation. 4.1. Stopping Early Recall that the point of all this analysis is to evaluate whether where x c is the current candidate NN. If (7) holds, the node in question must be searched; otherwise it can be pruned. We can evaluate the right side of (7) ex-actly using the bisection method described previously, but an exact solution is not needed. Suppose we have bounds a and A satisfying If d f ( x c ,q ) &lt; a , the node can be pruned; if d f A , the node must be explored. We now describe upper and lower bounds that are computed at each step of the bisection search; the search proceeds until one of the two stopping conditions is met.
 A lower bound is given by weak duality. The lagrange dual function is By weak duality, for any  X   X  [0 , 1), For the upper bound, we use the primal. At any  X  satisfying d f ( x  X  , X  )  X  R , we have Let us now put all of the pieces together. We wish to evaluate whether (7) holds. The algorithm performs bisection search on  X  , attempting to locate the  X  satis-fying d f ( x  X  , X  ) = R . At step i the algorithm evaluates  X  on two functions. First, it checks the lower bound bound given by the dual function L (  X  i ) defined in (8). Otherwise, if x  X  bound. If d f ( x  X  be searched. Otherwise, neither bound holds, so the bisection search continues. See Algorithm 1 for pseu-docode. Since a bregman divergence can be asymmetric, it de-fines two NN problems:  X  (lNN) return argmin x  X  X d f ( x,q ) and  X  (rNN) return argmin x  X  X d f ( q,x ).
 The bbtree data structure finds the left NN . We show that it can also be used to find the right NN. Algorithm 1 CanPrune Input:  X  l , X  r  X  (0 , 1], q,x c , X   X  R D , R  X  R .
Set x  X  =  X  f  X  (  X  X  0 + (1  X   X  ) q 0 ) if L (  X  ) &gt; d f ( x c ,q ) then else if x  X   X  B (  X ,R ) and d f ( x  X  ,q ) &lt; d f ( x c else if d f ( x  X  , X  ) &gt; R then else if d f ( x  X  , X  ) &lt; R then end if Recall that the convex conjugate of f is defined as f  X  ( y )  X  sup x { X  x,y  X  X  X  f ( x ) } . The supremum is realized at a point x satisfying  X  f ( x ) = y ; thus We use this identity to rewrite d f (  X  ,  X  ): This relationship provides a simple prescription for adapting the bbtree to the rNN problem: build a bb-tree for the divergence d f  X  and the database X 0  X  { X  f ( x computed and the bbtree finds x 0  X  X 0 minimizing d  X  ( x 0 ,q 0 ). The point x whose gradient is x 0 is then the rNN to q . We examine the performance benefit of using bbtrees for approximate and exact NN search. All experiments were conducted with a simple C implementation that is available from the author X  X  website.
 The results are for the KL-divergence. We chose to evaluate the bbtree for the KL-divergence because it is used widely in machine learning, text mining, and computer vision; moreover, very little is known about efficient NN retrieval for it. In contrast, there has been a tremendous amount of work for speeding up the ` 2 be handled by standard metric trees and many other methods. Other bregman divergences appear much less often in applications. Still, examining the prac-tical performance of bbtrees for these other bregman divergences is an interesting direction for future work. We ran experiments on several challenging datasets.  X  rcv-D . We used latent dirichlet allocation (LDA)  X  Corel histograms . This dataset contains 60k  X  Semantic space . This dataset is a 371- X  SIFT signatures . This dataset contains 1111-Notice that most of these datasets are fairly high-dimensional.
 We are mostly interested in approximate NN retrieval, since that is likely sufficient for machine learning appli-cations. If the bbtree is stopped early, it is not guar-anteed to return an exact NN, so we need a way to evaluate the quality of the point it returns. One nat-ural evaluation metric is this: How many points from the database are closer to the query than the returned point? Call this value NC for  X  X umber closer X . If NC is small compared to the size of the database, say 10 versus 100k, then it will likely share many properties with the true NN ( e.g. class label). 4 The results are shown in figure 2. These are strong results; it is shown that the bbtree is often orders of magnitude faster than brute-force search without a substantial degradation of quality. More analysis appears in the caption. (no speedup) to 10 5 . The x -axis ranges from 10  X  2 to 10 . All results are averages over queries not in the database. Consider the plot for rcv-128 (center). At x = 10 , the bbtree is returning one of the two nearest speedup over brute force search.
 speedup while receiving NNs in the top 1%.
 Finally, we consider exact NN retrieval. It is well known that finding a (guaranteed) exact NN in mod-erate to high-dimensional databases is very challeng-ing. In particular, metric trees, KD-trees, and relatives typically afford a reasonable speedup in moderate di-mensions, but the speedup diminishes with increasing dimensionality (Moore, 2000; Liu et al., 2004). When used for exact search, the bbtree reflects this basic pat-tern. Table 2 shows the results. The bbtree provides a substantial speedup on the moderate-dimensional databases (up through D = 256), but no speedup on the two databases of highest dimensionality. In this paper, we introduced bregman ball trees and demonstrated their efficacy in NN search. The exper-iments demonstrated that bbtrees can speed up ap-proximate NN retrieval for the KL-divergence by or-ders of magnitude over brute force search. There are many possible directions for future research. On the practical side, which ideas behind the many variants of metric trees might be useful for bbtrees? On the the-oretical side, what is a good notion of intrinsic dimen-sionality for bregman divergences and can a practical data structure be designed around it? Thanks to Serge Belongie, Sanjoy Dasgupta, Charles Elkan, Carolina Galleguillos, Daniel Hsu, Nikhil Rasi-wasia, and Lawrence Saul. Support was provided by the NSF under grants IIS-0347646 and IIS-0713540.
