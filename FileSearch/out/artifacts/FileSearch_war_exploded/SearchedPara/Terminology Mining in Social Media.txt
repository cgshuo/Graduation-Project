 The highly variable and dynamic word usage in social media presents serious challenges for both research and those com-mercial applications that are geared towards blogs or other user-generated non-editorial texts. This paper discusses and exemplifies a terminology mining approach for dealing with the productive character of the textual environment in social media. We explore the challenges of practically acquiring new terminology, and of modeling similarity and related-ness of terms from observing realistic amounts of data. We also discuss semantic evolution and density, and investigate novel measures for characterizing the preconditions for ter-minology mining.
 H.3.1 [ Content Analysis and Indexing ]: Linguistic pro-cessing; I.2.7 [ Natural Language Processing ]: Text anal-ysis; J.5 [ Arts and Humanities ]: Linguistics.
 Algorithms, Experimentation, Performance, Theory.
 Word Space, Distributional Semantics, Random Indexing, Terminology Mining, Social Media. Most of the communication in social media is in textual form. While social media authors adhere to most rules of text production, the low level of editorial oversight, the per-ceived informality of the media, and the comparatively high degree of interactivity create a new communicative situation. There are no previous genres for this new type of communi-cation  X  new text  X  to model itself after: new conventions for expression are created apace and we can expect several new genres to appear eventually, blending features of estab-lished genres from paper-borne text with entirely new types of expression. This change may occur rapidly, in leaps and bounds, but firmly rooted in accepted textual practice as understood by the author  X  meaning that new texts do not necessarily transcend or break norms in every respect. [16, 20, 17, 18].

We claim that pre-compiled lexical resources  X  which work well for thoroughly edited, traditionally produced text  X  cannot be trusted to capture relations from new text. For example, in order to understand what someone means when they write on their blog that  X  X y whip is the shiznit, X  we need to know that  X  X hip X  in this context means  X  X xpensive automobile X  and  X  X he shiznit X  means  X  X ood. X  Consulting a standard dictionary will clearly not be of much help in this case; the only way to extract these relations is to mine the data itself.

We will in these experiments explore the challenges of practically acquiring new terminology, and of modelling sim-ilarity and relatedness of terms from observing realistic am-ounts of data rather than using manually compiled resources. Real-world deployment of terminology mining in social me-dia requires handling the following issues: Scalability: techniques intended to operate on social me-Change: the techniques must be incrementally updateable Noise: the techniques must be robust and not crumble un-In this paper, we suggest using Random Indexing to handle these issues. In the following sections, we give an overview of Random Indexing, and discuss how it can be used for coping with rapidly evolving word usage in social media. We ap-ply the method to a sizeable collection of blogs, and provide a number of examples of how terminology mining in social media differs from using standard balanced corpora. In par-ticular, we explore and investigate the following questions: As argued in the previous section, social media constitute a semantically volatile domain, and if we intend to oper-ate textually in such an environment we need to employ a methodology that can re-align its semantic model according to observed language use. A theoretical perspective that fits particularly well with this requirement is the view professed by structural linguistics that words can be characterized by the contexts in which they occur, and that semantic sim-ilarities between words can be quantified on the basis of distributional information. This idea, most concisely stated in Harris [3], has been enormously influential and has been operationalized by a family of statistical algorithms known as word space models [19, 13], which include well-known al-gorithms like Latent Semantic Analysis (LSA [8]) and Hy-perspace Analogue to Language (HAL [10]).

These models collect distributional statistics in high-dimen-sional vectors called context vectors , which represent the dis-tributional profile for terms (or whatever type of entity we are interested in). The context vectors contain co-occurrence counts that have been collected either by noting co-occurrence events with other words within a context window , as in the HAL-type of models, or by noting co-occurrence events within a document or other type of text region, as in the LSA-type of models. These two main types of models ex-tract different types of similarities between terms. Sahlgren [13] labels them paradigmatic and syntagmatic, since the HAL-type models will group together words that have been used with the same other words, while the LSA-type models will group together words that have been used together. A per-haps more enlightening characterization would be to label these different types of models semantic and associative . We will only discuss the former type of model in this paper, since we are primarily interested in semantic rather than associative similarity; knowing that  X  X hiznit X  means  X  X ood X  is more useful than knowing it is associatively related to  X  X ro. X  1 Furthermore, LSA is not directly applicable to the current problem due to its well-known limitations regarding scalability and efficiency. Semantic (or HAL-type) word spaces produce context vec-tors by noting co-occurrence events within a context window that defines a region of context around each word. In the following example, the context window, indicated by  X  X ], X  spans 2 tokens on each side of the focus word  X  X otion: X  ...the seat [redefines the notion of sustainability] as a... The context vector for  X  X otion X  in this example is 8-dimen-sional, since there are 8 word types, and it has non-zero val-ues in the dimensions representing the words within the con-text window ( X  X edefines, X   X  X he, X   X  X f X  and  X  X ustainability X ): The context window is then moved one step to the right, and the process repeated for the word  X  X f, X  then for  X  X us-tainability, X  and then for  X  X s, X  etc. until the entire data has been processed. At the end of processing, each word type is represented by an accumulated n -dimensional context vec-tor, where n is the size of the vocabulary of the data set, and each element records the number of times the word and the word represented by that dimension has co-occurred within the context window. The resulting matrix of context vec-tors is n  X  n and symmetric. It should be noted that in the original HAL model, co-occurrences are collected in only one direction within the context window, leading to a directional matrix in which rows and columns represent co-occurrence counts in different directions (i.e. with preceding and suc-ceeding words).

If we now search through our n -dimensional vector space for the context vectors that are most similar to each other, we will find words that have been used in similar ways in our data and therefore have a semantic relationship, like synonyms and antonyms. An unsolved problem with these methods is how to distinguish words with the same mean-ing (i.e. synonyms) from words with the opposite meanings (i.e. antonyms). We will see several examples of this issue in the examples and experiments in Sections 3.2 and 4.2.
The most significant difference between various seman-tic word space models is the size and configuration of the context window within which co-occurrence counts are col-lected. The original HAL model uses a window spanning 10 words and collects co-occurrence counts asymmetrically within the window. Other models use small symmetric win-dows spanning two to three words on each side of the focus word [9, 13]. There have also been suggestions to utilize syn-tactic structure for configuring the context window. Pad  X o
This is not to say associative word spaces cannot be of interest for text analysis applications  X  on the contrary, it is often of great interest to known which associations a certain term has (e.g. in buzz monitoring and brand name analysis).
Note that we do not consider the word as co-occurring with itself.
Similarity between context vectors are computed using any vector similarity measure, like Euclidean distance or, more commonly, because it normalizes for vector length, the co-sine of the angles between the vectors: cos( ~x,~y ) = ~x  X  ~y and Lapata [11] use dependency parsed data to produce se-mantic word spaces, but their results on a standardized syn-onym test (73% correct answers) is well below state-of-the-art results using standard context windows (  X  80%). Fur-thermore, syntactic analysis requires non-negligible amounts of preprocessing that would not be feasible in the present application. As shown in the previous section, context vectors produced from semantic word space models are n -dimensional, where n is the size of the vocabulary. This is not a viable approach when dealing with very large, and continuously evolving, vocabularies. An attractive solution to this problem is the Random Indexing framework [5], which allows for incremen-tal accumulation of context vectors in a reduced-dimensional space whose dimensionality never increases.

This is accomplished by letting each word be represented by several randomly chosen dimensions instead of just one. For example, say that we have eight words in our vocabulary, as in the example above. Instead of using an 8-dimensional space where each word is represented by one dimension each, we can use a 4-dimensional space in which each word is rep-resented by two dimensions, by using one negative value and one positive value as in the following example: Such distributed representations are called index vectors , and they are high-dimensional (i.e. on the order of thou-sands) with a small number of +1s and  X  1s randomly dis-tributed according to: where ~r i is the i th element of index vector ~r , d is the pre-determined dimensionality, and is the number of non-zero elements (i.e. +1s and  X  1s) in the random index vectors.
These index vectors can be used to accumulate context vectors by the following simple algorithm: for each word in the data, update its zero-initialized context vector by adding the random index vectors of the words in the context win-dow. For example, the context vector for  X  X otion X  in our example sequence  X ...the seat [redefines the notion of sus-tainability] as a... X  is: which is the vector sum of the index vectors for  X  X edefines, X   X  X he, X   X  X f, X  and  X  X ustainability. X  4 This process is repeated
Note that a context vector in our terminology is a global representation of all the contexts in which a word has oc-curred, and not a representation of a single occurrence. every time we observe  X  X otion X  in our data, adding more information to its context vector. Note that the dimension-ality of the context vector will remain constant, regardless of how much data we add. If we encounter a new word, we simply assign to it a random index vector, and update its zero-initialized context vector. Thus, no re-compilation is necessary when adding new data, making Random Indexing inherently incremental and scalable.

Random Indexing, and related methods like random map-ping [6] and random projections [12], are based on the insight that choosing random directions in a high-dimensional space will approximate orthogonality, which means that by using the random index vectors to accumulate context vectors we will approximate a semantic word space using considerably less dimensions. Thus, if we collect the random indexing-accumulated context vectors for n words using d dimensions in a matrix M n  X  d , it will be an approximation of a stan-dard words-by-words co-occurrence matrix F n  X  n under the following condition: where R n  X  d is a matrix containing the random index vectors. Note that if the random vectors in matrix R are orthogonal, so that R T R = I , then M = F . If the random vectors are nearly orthogonal, then M  X  F in terms of the similarity of their rows (i.e. their context vectors).
 As Sahlgren et al. [14] demonstrate, Random Indexing also allows for incorporating word order information in the con-text vectors. This is achieved by permuting the random index vectors with regard to where in the context window the words occur. For example, in order to utilize word or-der information when accumulating the context vector for  X  X otion X  in our example sentence, we do: ( X   X  2 redefines) + ( X   X  1 the) + ( X  1 of) + ( X  2 sustainability) where  X  is a (random) permutation,  X   X  1 is its inverse, and the exponent n in  X  n signifies that the vector is permuted n times. This operation can be used either to incorporate word order by permuting index vectors by their position in the context window, or to use only the direction of the window (i.e. whether a word precedes or succeeds the focus word) as in the original HAL model, in which case we only use two permutations for preceding and succeeding words, respectively. Sahlgren et al. [14] show that directional con-text vectors outperform both standard unordered context windows and representations that take word order into ac-count in a standardized synonym selection task.

In addition to producing state-of-the-art results in seman-tic tasks, the permutation operation also lets us retrieve the most frequent preceding and succeeding words. This is done by using the inverse permutation on the context vector for the word whose neighbors we want to examine, and then comparing this permuted context vectors to all index vec-tors. The words whose index vectors are most similar to this permuted context vector are the ones that tend to occur in the word X  X  immediate vicinity. Note that this allows us to use a semantic word space as a language model and predict the most likely preceding and succeeding words  X  what we will refer to as directional similarities .
Because of the state-of-the-art results in synonym detec-tion tasks, and their ability to retrieve directional similari-ties, we use a small directional context window spanning two words to the left and two words to the right in the following experiments. Random Indexing is often claimed to be both scalable and efficient. In the remainder of this paper, we will put this claim to the test and apply Random Indexing to a sizeable collection of blog data. We will investigate the usefulness of the approach for terminology mining, and provide exam-ples of both semantically and directionally similar terms to a number of target concepts that might be of interest for various kinds of social media analysis applications. In order to get an idea of how domain specific relations mined from social media are, we will compare these examples to ones produced using a medium-sized balanced corpus.

We will also look at the evolution and density of the se-mantic neighborhoods, and at how these properties compare between balanced corpora and blog data. One important reason to look at semantic evolution and density is to sub-stantiate the assumption that  X  X ore is better X  when it comes to capturing word usage. It also provides us with valuable insight into the nature of semantic word spaces, and their usefulness for terminology mining. The Spinn3r data set [2] is one of the currently largest pub-licly available data sets. It consists of some 44 million blog posts made between August 1st and October 1st, 2008. The data is arranged into tiers based on Spinn3r X  X  in-house blog ranking algorithm; tier 1 is the biggest sub-collection and contains the most  X  X elevant X  (as computed by the ranking algorithm) entries, tier 2 the second most relevant entries, and so forth. We use both tier 1 and tiers 2 X 13 in the fol-lowing experiments.

The collection contains blog posts in many different lan-guages; English being the most common, but Chinese, Ital-ian, and Spanish are also frequent. We did not separate the different languages, since a multilingual environment is the natural habitat of social media, and any system intended to work with such data must be able to cope with multilingual-ity without the recourse to external resources. We will see several examples of where different languages collide in the semantic neighborhoods in the following experiments.
Before applying Random Indexing to the Spinn3r collec-tion, we did a quick-and-dirty cleaning up of the data by removing non-alphabetic characters, downcasing alphabetic characters, and removing the Spinn3r xml tags. A substan-tial amount of noise remains after this na  X   X ve preprocessing, mostly in the form of html code, but also in the form of meta data from the Spinn3r xml format, which could have been removed by parsing the data more carefully. However, we wanted to simulate a noisy real-world environment in which careful preprocessing cannot always be afforded. We will see several symptoms of the noisy data in the examples in Section 4.2. As an example of the kind of result that is typical when performing terminology mining using word spaces, we built a semantic word space from tiers 2 X 13 of the Spinn3r data, Table 1: 40 nearest neighbors in semantic word space to  X  X ood X  and  X  X ad. X  synonyms,  X  indicates errors, ? indicates uncertain cases, and 6 = indicates antonyms. The numbers are cosine similarities. which constitutes some 600,000,000 word tokens. 5 Table 1 shows the 40 nearest neighbors to  X  X ood X  and  X  X ad. X  Neigh-bors that would count as (near) synonyms are indicated with (  X  ), errors are indicated with (  X  ), strange  X  but still in some sense viable  X  neighbors with a question mark ( ? ), and antonyms with ( 6 =). Note that the presence of antonyms in the nearest neighbor lists is to be expected for two reasons: firstly, antonyms tend to occur in similar contexts; secondly, in some cases, semantic role reversal may flip the meaning of a term to resemble its antonym  X  a frequent occurrence for the word  X  X ad. X 
The most notable finding is that there are a number of domain-, genre-, or stylistic-specific terms among the near-est neighbors that would have been difficult, if not impossi-ble, to foresee for a human analyst or by consulting a lex-ical resource. Examples include  X  X ristine, X   X  X mmaculate, X  and  X  X int X  for  X  X ood X  and  X  X ucky, X   X  X hitty, X  and  X  X reepy X  for  X  X ad. X  Another very typical effect when analyzing social
Using 2,000-dimensional vectors and a directional context window consisting of the four nearest surrounding words. media is that misspellings often show up among the near-est neighbors:  X  X refect, X   X  X rat, X   X  X eat, X   X  X xcelent, X   X  X god, X   X  X oos X  and so on.

The words marked with a question mark would most likely be ranked as unrelated if the nearest neighbor list would be evaluated with a standard lexical resource like a thesaurus or a dictionary. However, it is possible that words like  X  X ough, X   X  X oisy, X  X  X trange, X  and  X  X rowded X  actually have very positive loading in the Spinn3r data, and that they therefore quite correctly should be related with  X  X ood. X  Similarly,  X  X og, X   X  X uiet, X  and  X  X omantic, X  are possibly very negative terms in the blog data, and therefore correctly related to  X  X ad. X 
Although not visible in the examples in Table 1, we do en-counter multilingual effects in the nearest neighbor analysis of the Spinn3r data, which is to be expected since we did not attempt language separation. For example, extracting the nearest neighbors for  X  X ove X  we find words like  X  X orge X  (i.e.  X  X orway X  in Norwegian),  X  X tavanger X  and  X  X roms  X  o X  (both Norwegian cities), which can be explained by the fact that  X  X ove X  is a Norwegian male surname. This also accounts for the fact that a number of other person names show up in the nearest neighbor list for  X  X ove X   X  e.g.  X  X shm, X   X  X vevo, X   X  X osanto, X   X  X rew, X   X  X omi, X  and  X  X akako. X  In the remainder of this paper, we use data from tier 1 of the Spinn3r collection, which after our quick-and-dirty cleaning-up consists of some 1,000,000,000 word tokens. We constructed a semantic word space using 2,000-dimensional vectors and a directional context window spanning four sur-rounding words, which took around a day to compute using brute force (i.e. without any optimization such as caching). As comparison, we also include examples computed using the same parameter setting for the British National Corpus (BNC)  X  a balanced English corpus containing approxi-mately 100,000,000 words. Evaluation of automatically extracted lexical resources from non-standard data is notoriously difficult, since there typi-cally does not exist a gold standard available for comparison and benchmarking. This is particularly true for terminology mining in social media, where  X  as we have argued  X  the dynamic nature of language use prohibits the compilation of lexical resources, and makes standard benchmarking pro-cedures inapt. Stretching this line of reasoning, we could say that whatever we find in the data must be the truth for those particular data. However, that would be begging the question; arguably, systematic benchmarking and evalu-ation are crucial to useful natural language engineering and application design.

We believe that evaluation of learned resources must be built on an experimental process based on hypotheses in-formed by some understanding of textual reality, rather than computational expediency, and that results must also be evaluated by the qualities of the representation per se, and not only by their application to some noisy and imprecise task. Using task-based evaluations, while guaranteeing a measure of validity for the experiment, risks swamping the effects of the representation in a context where other factors may induce variation, not obviously visible to the experi-menter.

In this study, we will focus on questions preceding those of general applicability and usefulness. In particular, we are interested in factors that affect the semantic representations, such as the diversity of language use and the evolution of the semantic neighborhoods. Our main motivation for perform-ing this study is that intrinsic evaluation of resources such as word spaces needs to be formalized to model variation across collections and over time. This study is a step to-wards establishing measures for understanding the intrinsic variation of such representations. As discussed in the previous section, our goal here is not to evaluate how well semantic word spaces gathered from blog data replicate a thesaurus or a dictionary (since that is irrelevant from an application-driven perspective). Rather, our ambition in this paper is to investigate and characterize the properties of blog-induced word spaces as compared to standard corpus-built ones. However, since one of the most obvious applications of semantic analysis of social media is buzz monitoring applications, we focus here on a number of hand-chosen target concepts that are relevant for, and often used by, such systems: Buzz monitoring: love, hate, recommend Open source intelligence: attack, bomb, terrorism Epidemiology: flu, infection, symptom Climate: ecology, ecosystem, environment Tables 2 and 3 show the five most semantically and the four most directionally related terms for each target concept. For example, the left column for  X  X ove X  contains its two most commonly preceding terms, whereas the right column con-tains its two most commonly succeeding terms. The column below  X  X ove X  shows its five nearest semantic neighbors.
Nearest neighbor lists are admittedly less clarifying, diffi-cult to interpret, and may even be potentially delusive since authors often have the unfortunate tendency to weed out un-favorable examples. We include these un-edited lists here in order to show the varying quality of the results; some words, like  X  X nfection X  and  X  X omb, X  have very relevant nearest neigh-bors in both tables, while other words, like  X  X cosystem X  and  X  X cology, X  have not.

Also, we want to show the differences between edited cor-pora and noisy data: the nearest neighbors produced from the Spinn3r data are clearly more noisy (see e.g.  X  X errorism X ), and feature numerous spelling variations and surface noise introduced by the simplistic preprocessing (see e.g.  X  X ecom-mend X  and  X  X nfection X ). Furthermore, we include the nearest neighbor lists because we focus on a small number of target concepts, simulating a real-world application scenario where a human analyst consults such lists in order to extract asso-ciations and keep up with vocabulary variation and devel-opment.

The reason we only include four directional neighbors in the examples is that directional analysis using the permu-tation operator in Random Indexing can capture frequently occurring constructions  X  like  X  X ar bomb X   X  but less fre-quent co-occurrences will drown in the noise inherent in the Random Indexing algorithm. This means that looking fur-ther down the directional neighbor lists than, say, the two highest correlated words will result in an increasing rate of and the last column under  X  shows the 2 most likely succeeding words. random neighbors. 6 Another, somewhat more informed ap-proach to extract relevant directional neighbors would be to use a threshold for the cosine similarities. As an example, the average cosine similarity for our targets to their nearest directional neighbor is  X  0 . 49, to the second nearest direc-tional neighbor  X  0 . 26, and to the third nearest directional neighbor  X  0 . 24. This can be contrasted with 0.85, which is the average cosine similarity to the ten nearest seman-tic neighbors, indicating the difference in reliability between these measures.

There are a number of findings of practical import in these lists. First of all, we note that antonyms turn up among the nearest neighbors in semantic spaces built from both types of data  X  see e.g.  X  X ove X  and  X  X ate. X  This artefact of semantic spaces has already been discussed in Section 3.2 above. We also find that certain words have the same nearest neighbors in both types of data  X  e.g.  X  X ate X  has  X  X espise X  and  X  X oathe X  as nearest neighbors in both the BNC and Spinn3r spaces. Furthermore, the most likely preceding words for  X  X ttack X  (i.e.  X  X eart X  and  X  X nder X ) are the same in both spaces, as are a number of strong collocations, like  X  X iv infection, X  X  X ar bomb, X  and  X  X omb attack. X 
This indicates the stability of semantic neighborhoods across domains and data; theoretically, we would expect semantic neighborhoods to remain relatively stable across different styles and genres compared to associations, which are by nature domain specific. Semantic relations, on the other hand, are constraints on vocabulary choice and are there-fore less likely to be subject to individual variation. However  X  which is the point in this paper, and as these examples demonstrate  X  there are also domain specific synonyms that are only used in a certain language sample, and that would not be foreseeable by a human analyst. Such domain spe-
It should be noted that in some cases there are actually viable terms further down the directional lists; e.g.  X  X east X  and  X  X inus X  are the third and fourth nearest left directional neighbors for  X  X nfection, X  and  X  X oadside X  and  X  X uclear X  are the third and fifth left directional neighbors for  X  X omb. X  cific nearest neighbors can be, e.g., the collocation  X  X ird flu X  and  X  X lgeria X  as a nearest neighbor for  X  X omb. X 
The fact that we do find stable semantic neighbors across the different word spaces suggests that we might be able to use the overlap between several domain-specific word spaces as a generic semantic representation. A simple example of this idea is demonstrated in Table 4, which contains those terms that occur in both the BNC and Spinn3r word spaces among the 100 nearest semantic neighbors to four of the tar-gets. Obviously, these semantic neighbors are less impressive when it comes to precision than when it comes to recall, but they do describe the semantic domains of the target words quite well. If we only use the 5 nearest semantic neighbors in both spaces, we get the terms displayed below, showing a much higher semantic precision and again demonstrating the conflation of antonyms and synonyms in word space: hate: detest, adore love: adore, hate, loathe attack: ban environment: theatre Counting the overlap between the 100 nearest semantic neigh-bors in the BNC and Spinn3r word spaces for our targets gives results ranging from 0 and 1 overlap (for  X  X ymptom X  and  X  X cosystem X ) to 21 and 22 terms in common (for  X  X omb X  and  X  X ttack X ). This rather low level of overlap might seem surprising in view of the general character of the chosen tar-get concepts in this study: one might expect the general properties of the English language to provide considerable higher rate of overlap across domains. This is a further in-dication of the necessity of domain-specific terminological mining.

As a last reflection on the examples from the Spinn3r data in Table 3, we can note that there seems to be a qualitative difference between the left and right directional neighbors: the latter contain more noise, particularly in the Spinn3r Table 4: Terms that occur among the 100 nearest semantic neighbors to four targets in both the BNC and Spinn3r word spaces. examples. The fact that the right context presents greater variation than the left context is not by itself surprising, since the left context is governed by phrase-level syntactic constraints, particularly in fixed-word order languages such as English, and especially for nouns. A symptom of this is the higher average cosine similarity to the nearest left direc-tional neighbor (  X  0 . 59) than to the nearest right directional neighbor (  X  0 . 39). This effect is more pronounced for the Spinn3r examples:  X  X llentowns, X   X  X umeputeras, X   X  X rosaico, X  and the right directional neighbors for  X  X ecommend, X   X  X lu, X  and  X  X cosystem X  are all nonsensical. This is an indication of how the lesser level of editorial control in social media affords the authors greater freedom in lexical choice; the overriding constraints from the English language are not relaxed to the same extent as the stylistic and topical ones might be. It may seem natural to assume that, since word spaces are statistical algorithms, adding more data will improve the quality of the word spaces. Since we evade the question of how to assess the quality of semantic resources in this paper, we can rephrase the question in the following terms: how much does the semantic neighborhoods evolve when we add more data? Figure 1: Semantic evolution in word space for the target concepts. The y -axis indicates the number of new terms among the ten nearest neighbors. The x -axis shows percentage of total data used.
 Figure 1 shows how the semantic neighborhoods evolve when adding more data for three different data sets  X  the BNC, the first 100 million words from the Spinn3r data, and the full one billion word Spinn3r tier-1 data. The y -axis shows the number of new terms among the ten nearest neighbors for our 12 target concepts, and the x -axis shows percentage of total data used. As an example, the ten nearest neighbors to  X  X ove X  when having seen 90% of the Spinn3r data is: and when having seen 100% of the data: which differs only by one word (disregarding the order): in the latter list is  X  X oooooooove X  instead of  X  X oved. X 
The tendency is the same in all word spaces; adding more data alters the neighborhoods slightly, but the evolution of the word spaces stagnates quite quickly. The evolution-ary process seems to follow a power-law-related distribu-tion, describing the progressive semantic saturation of the local neighborhood of the focus term, and after we have doubled the amount of data a couple of times we merely see on average one new neighbor in the semantic neighbor-hoods when adding more data. The neighborhood for some words change more than others. For example,  X  X cosystem, X   X  X nfection, X  and  X  X lu X  tend to replace more neighbors when adding data than  X  X ove, X   X  X ate, X  and  X  X omb. X  We suspect this is due to the former words X  relative semantic promiscu-ity  X  words that have a broader usage will also modify its semantic neighborhood more frequently than words with a relatively static usage.

It seems that adding more of the same type of data does not alter the semantic neighborhoods that much after a cer-tain point. Again, this is related to the relative stability of semantic neighborhoods discussed in the previous sec-tion. However, adding more of another type of data would certainly have a discernible effect on the semantic neigh-borhoods; if data discussing swine flu would be added to a word space in which flu is related to bird, we would certainly see an evolution in the semantic neighborhood of  X  X lu. X  The question is whether the evolution of semantic neighborhoods would follow the same type of power-law-like distribution even when continuously adding diverse data? And whether there is any way to determine how semantically homoge-neous a data set is, so as to thereby predict the evolutionary rate of semantic neighborhoods? We leave the former of these questions open for future research, and suggest one direction to approach the latter question in the following section. One way to arrive at an indication of the semantic homo-geneity of a data set is to compute the density of neighbor-hoods in word space, as suggested by [15]. The idea is that very homogeneous data have very dense semantic neighbor-hoods, since words occur in very uniform contexts. Thus, if we extract the n nearest neighbors to a target word, and then the n nearest neighbors to each of the target word X  X  nearest neighbors, we can quantify this neighborhood X  X  density as the density measure d n by simply counting the total num-ber of unique words thus extracted. The maximum number of unique words is n  X  n , indicating an extremely dispersed neighborhood; the minimum number of unique words is n , indicating an extremely homogeneous word usage, where all neighbors form an interconnected set.

Figure 2 shows the density of the semantic neighborhoods of our target concepts for d 10 over ten different sizes of the data sets (i.e. the BNC, the first 100 million words from the Spinn3r data, and the full one billion word Spinn3r tier-1 data). The y -axis shows the average density measure for our 12 target concepts, and the x -axis shows percentage of total data used. As can be seen in the figure, the densities Figure 2: Semantic density of neighborhoods in word space. The y -axis shows d 10 , indicating the number of unique terms among the 10 nearest neigh-bors of the 10 nearest neighbors to each target con-cept, with scores ranging from 10 to 100. The x -axis shows percentage of total data used. of the semantic neighborhoods for our target concepts re-volve around 80, and are comparable in the different spaces. Perhaps not very surprisingly, the density for the balanced BNC data seems to be somewhat higher than in the collec-tion of blog data (around 82 for the BNC and around 77 for the Spinn3r data).
 This speaks of the more focused topical character of the BNC data. Where most of the BNC is constituted of printed text, the Spinn3r data treat large numbers of topics ranging from the whimsical and personal to the factual and informa-tive, none primarily intended for printed publication. Ta-ble 5 gives an indication of this. Two classic measures of vo-cabulary richness are given for the two test collections [21, 4]. Their diverging results show that the Spinn3r data present more singleton term occurrences than the BNC data; they also show that the variability of the vocabulary in general is greater for the BNC. This observation demonstrates the ef-fect of editorial processing on text evident in the BNC data: less misspellings and one-off terms; more variability in the greater pattern of language use.
 Honor  X e X  X  R 4448 4148 3831 Yule X  X  K 53.6 69.8 106.22 Table 5: Two measures of vocabulary richness, com-puted on the two test collections. Honor  X e X  X  R, if large, indicates many hapax legomena, single occur-rences of terms. Yule X  X  K, if large, indicates a vari-able vocabulary. The data for one year of LA Times are given for reference purposes. The form of distributional analysis studied in these experi-ments  X  based on pointwise occurrence and co-occurrence statistics  X  deliver better results for some terms than for others. As we have seen in the examples throughout this paper, some words have very relevant nearest semantic and directional neighbors in word space, while others have more or less random neighbors. Since the differences in compre-hensibility between these terms for the human language user is small, this suggests that there may be further character-istics in term usage available for modelling in a learning framework. Previous studies have extended the study of pointwise statistics of term occurrences to burstiness or re-peat occurrences of terms [7, 1], using Katz X  Poisson mix-ture models which appear to capture more of the topicality of terms than the pointwise frequency estimates are able to. However, the model we utilize here makes it possible to study the co-occurrence neighborhood itself, to examine and com-pare the diversity of the neighborhoods between terms. Our hypothesis is that terms which are distributionally promis-cuous will have a more diffuse global representation in a co-occurrence-based scheme such as ours.

This question merits further systematic study, but given the varying utility of the target terms given in the previous section, we can study their respective qualities within our representation. From a practical standpoint, the question is whether we can determine the distributional suitability of a term apriori, without wasting much computational ef-fort to model its neighborhood? Can this be done from first observable characteristics of the term? Can we somehow de-termine this by merely inspecting the intrinsic properties of the representation? Table 6 gives some candidate measures. We use  X  as a simplified version of the heuristic for param-eter estimation given by Sarkar et al [1]  X  the ratio between Katz X   X  7 , and Katz X   X  8 , estimated from separate newsprint data; we use the d 10 density measure from Section 4.4 [15]; and we use the standard deviation  X  of the context vectors in the Random Indexing word space.

Of these three candidate measures, the ratio between Katz X  two measures gives low scores to ecosystem , terrorism , and ecology , with fairly random neighbors as given in Tables 2 and 3 and high scores to love , attack , and recommend , all three usefully modelled terms. Similarly, the standard devi-ation gives a strong indication of predictive power, with high  X  tending to be more characteristic of better terms, indicat-ing that words with a fairly tightly held context are bet-ter targets for this technology. The density measure yields somewhat more equivocal results. These candidate measure serve here to indicate that the distributional character of terms can be analyzed to show correlation with their useful-ness for distributional semantic modelling; they need con-siderable more refinement for true predictive power. The point of this paper is to critically discuss the task of terminology mining in social media, and in particular to explore the viability of using scalable statistical approaches such as Random Indexing for this task. Our explorations have shown both possibilities and limitations of the proposed approach, and we have identified a number of interesting properties and directions for further study and application.
The estimated probability of occurrence of the term in a document, (essentially the collection frequency of a docu-ment, df ) [7].
The estimated probability of a repeat occurrence given a first observation within the given text [7].
 love 0.23889 41 16390 hate 0.08538 26 3308 recommend 0.14455 38 1059 infection 0.01546 76 230.2 symptom 0.03314 35 97.09 flu 0.01764 81 406.4 attack 0.17663 63 1792 terrorism 0.00906 67 947.0 bomb 0.03041 56 583.1 ecology 0.01636 47 81.70 environment 0.18243 45 2617 ecosystem 0.00659 55 187.4 Table 6: Individual distributional and intrinsic char-acteristics of the target concept terms.

In general, we believe that the formal study of the char-acteristics of high-dimensional representations of linguistic information has been neglected in favor of the study of their applicability to general information access tasks, such as search engine index implementations, or semantic similar-ity extraction. The research field in general still has rather vague notions of how the make-up of the representation al-ters the properties of the semantic model; this study points to a number of representational issues, some of which may be artefacts of the high-dimensional representation.
On the other hand, this study also shows the practicabil-ity and robustness of scalable approaches such as Random Indexing, across collections, and even for new text such as found in user-generated media, which, compared to edited media sources, is characterized by: Several of our observations support these general contentions. It is also clear that the variation given in user-generated media takes the form of very productive exploration of any given synonym space. The multitude of synonyms for the relatively basic notion of  X  X ood X  is a case in point. Where established and experienced writers might work with struc-tural and compositional features of the text, less experienced writers are prone to expending their creative energy on syn-onymy.

In summary, our main findings are: [1] A. D. R. Avik Sarkar, Paul H Garthwaite. A bayesian [2] K. Burton, A. Java, and I. Soboroff. The ICWSM [3] Z. Harris. Mathematical structures of language . [4] A. Honore. Some simple measures of richness of [5] P. Kanerva, J. Kristofersson, and A. Holst. Random [6] S. Kaski. Dimensionality reduction by random [7] S. Katz. Distribution of content words and phrases in [8] T. Landauer and S. Dumais. A solution to plato X  X  [9] J. Levy, J. Bullinaria, and M. Patel. Explorations in [10] K. Lund, C. Burgess, and R. Atchley. Semantic and [11] S. Pad  X o and M. Lapata. Dependency-based [12] C. Papadimitriou, P. Raghavan, H. Tamaki, and [13] M. Sahlgren. The Word-Space Model: Using [14] M. Sahlgren, A. Holst, and P. Kanerva. Permutations [15] M. Sahlgren and J. Karlgren. Counting lumps in word [16] M. Santini. Interpreting genre evolution on the web: [17] M. Santini. Characterizing genres of web pages: Genre [18] M. Santini, A. Mehler, and S. Sharoff. Riding the [19] H. Sch  X  utze. Word space. In Proceedings of the 1993 [20] M. Tavosanis. Linguistic features of italian blogs: [21] G. U. Yule. The statistical study of literary vocabulary .
