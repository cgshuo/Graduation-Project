 Verbal and nominal semantic role labeling (SRL) have been studied independently of each other (Carreras and M ` arquez, 2005; Gerber et al., 2009) as well as jointly (Surdeanu et al., 2008; Haji  X  c et al., 2009). These studies have demonstrated the maturity of SRL within an evaluation setting that restricts the argument search space to the sentence containing the predicate of interest. However, as shown by the following example from the Penn TreeBank (Marcus et al., 1993), this restriction ex-cludes extra-sentential arguments: (1) [ arg 0 The two companies] [ pred produce ] The first sentence in Example 1 includes the Prop-Bank (Kingsbury et al., 2002) analysis of the ver-bal predicate produce , where arg 0 is the agentive producer and arg 1 is the produced entity. The sec-ond sentence contains an instance of the nominal predicate shipping that is not associated with argu-ments in NomBank (Meyers, 2007).

From the sentences in Example 1, the reader can infer that The two companies refers to the agents ( arg 0 ) of the shipping predicate. The reader can also infer that market pulp, containerboard and white paper refers to the shipped entities ( arg 1 of shipping ). 1 These extra-sentential arguments have not been annotated for the shipping predi-cate and cannot be identified by a system that re-stricts the argument search space to the sentence containing the predicate. NomBank also ignores many within-sentence arguments. This is shown in the second sentence of Example 1, where The goods can be interpreted as the arg 1 of shipping . These examples demonstrate the presence of argu-ments that are not included in NomBank and can-not easily be identified by systems trained on the resource. We refer to these arguments as implicit .
This paper presents our study of implicit ar-guments for nominal predicates. We began our study by annotating implicit arguments for a se-lect group of predicates. For these predicates, we found that implicit arguments add 65% to the ex-isting role coverage of NomBank. 2 This increase has implications for tasks (e.g., question answer-ing, information extraction, and summarization) that benefit from semantic analysis. Using our an-notations, we constructed a feature-based model for automatic implicit argument identification that unifies standard verbal and nominal SRL. Our re-sults indicate a 59% relative (15-point absolute) gain in F 1 over an informed baseline. Our analy-ses highlight strengths and weaknesses of the ap-proach, providing insights for future work on this emerging task.
In the following section, we review related re-search, which is historically sparse but recently gaining traction. We present our annotation effort in Section 3, and follow with our implicit argu-ment identification model in Section 4. In Section 5, we describe the evaluation setting and present our experimental results. We analyze these results in Section 6 and conclude in Section 7. Palmer et al. (1986) made one of the earliest at-tempts to automatically recover extra-sentential arguments. Their approach used a fine-grained do-main model to assess the compatibility of candi-date arguments and the slots needing to be filled.
A phenomenon similar to the implicit argu-ment has been studied in the context of Japanese anaphora resolution, where a missing case-marked constituent is viewed as a zero-anaphoric expres-sion whose antecedent is treated as the implicit ar-gument of the predicate of interest. This behavior has been annotated manually by Iida et al. (2007), and researchers have applied standard SRL tech-niques to this corpus, resulting in systems that are able to identify missing case-marked expres-sions in the surrounding discourse (Imamura et al., 2009). Sasano et al. (2004) conducted sim-ilar work with Japanese indirect anaphora. The authors used automatically derived nominal case frames to identify antecedents. However, as noted by Iida et al., grammatical cases do not stand in a one-to-one relationship with semantic roles in Japanese (the same is true for English).

Fillmore and Baker (2001) provided a detailed case study of implicit arguments (termed null in-stantiations in that work), but did not provide con-crete methods to account for them automatically. Previously, we demonstrated the importance of fil-tering out nominal predicates that take no local ar-guments (Gerber et al., 2009); however, this work did not address the identification of implicit ar-guments. Burchardt et al. (2005) suggested ap-proaches to implicit argument identification based on observed coreference patterns; however, the au-thors did not implement and evaluate such meth-ods. We draw insights from all three of these studies. We show that the identification of im-plicit arguments for nominal predicates leads to fuller semantic interpretations when compared to traditional SRL methods. Furthermore, motivated by Burchardt et al., our model uses a quantitative analysis of naturally occurring coreference pat-terns to aid implicit argument identification.
Most recently, Ruppenhofer et al. (2009) con-ducted SemEval Task 10,  X  X inking Events and Their Participants in Discourse X , which evaluated implicit argument identification systems over a common test set. The task organizers annotated implicit arguments across entire passages, result-ing in data that cover many distinct predicates, each associated with a small number of annotated instances. In contrast, our study focused on a se-lect group of nominal predicates, each associated with a large number of annotated instances. 3.1 Data annotation Implicit arguments have not been annotated within the Penn TreeBank, which is the textual and syn-tactic basis for NomBank. Thus, to facilitate our study, we annotated implicit arguments for instances of nominal predicates within the stan-dard training, development, and testing sections of the TreeBank. We limited our attention to nom-inal predicates with unambiguous role sets (i.e., senses) that are derived from verbal role sets. We then ranked this set of predicates using two pieces of information: (1) the average difference between the number of roles expressed in nominal form (in NomBank) versus verbal form (in PropBank) and (2) the frequency of the nominal form in the cor-pus. We assumed that the former gives an indica-tion as to how many implicit roles an instance of the nominal predicate might have. The product of (1) and (2) thus indicates the potential prevalence of implicit arguments for a predicate. To focus our study, we ranked the predicates in NomBank ac-cording to this product and selected the top ten, shown in Table 1.

We annotated implicit arguments document-by-document, selecting all singular and plural nouns derived from the predicates in Table 1. For each missing argument position of each predicate in-stance, we inspected the local discourse for a suit-able implicit argument. We limited our attention to the current sentence as well as all preceding sen-tences in the document, annotating all mentions of an implicit argument within this window.

In the remainder of this paper, we will use iarg n to refer to an implicit argument position n . We will use arg n to refer to an argument provided by PropBank or NomBank. We will use p to mark price 217 42.4 1.7 1.7 55.3 2.2 sale 185 24.3 1.2 2.0 42.0 2.1 investor 160 35.0 1.1 2.0 54.6 1.6 fund 109 8.7 0.4 2.0 21.6 0.9 loss 104 33.2 1.3 2.0 46.9 1.9 plan 102 30.9 1.2 1.8 49.3 2.0 investment 102 15.7 0.5 2.0 33.3 1.0 cost 101 26.2 1.1 2.3 47.5 1.9 bid 88 26.9 0.8 2.2 72.0 2.2 loan 85 22.4 1.1 2.5 41.2 2.1 Overall 1,253 28.0 1.1 2.0 46.2 1.8 predicate instances. Below, we give an example annotation for an instance of the investment predi-cate: (2) [ iarg 0 Participants] will be able to transfer NomBank does not associate this instance of in-vestment with any arguments; however, we were able to identify the investor ( iarg 0 ), the thing in-vested ( iarg 1 ), and two mentions of the thing in-vested in ( iarg 2 ).

Our data set was also independently annotated by an undergraduate linguistics student. For each missing argument position, the student was asked to identify the closest acceptable implicit argu-ment within the current and preceding sentences. The argument position was left unfilled if no ac-ceptable constituent could be found. For a miss-ing argument position, the student X  X  annotation agreed with our own if both identified the same constituent or both left the position unfilled. Anal-ysis indicated an agreement of 67% using Cohen X  X  kappa coefficient (Cohen, 1960). 3.2 Annotation analysis Role coverage for a predicate instance is equal to the number of filled roles divided by the number of roles in the predicate X  X  lexicon entry. Role cov-erage for the marked predicate in Example 2 is 0/3 for NomBank-only arguments and 3/3 when the annotated implicit arguments are also consid-ered. Returning to Table 1, the third column gives role coverage percentages for NomBank-only ar-guments. The sixth column gives role coverage percentages when both NomBank arguments and the annotated implicit arguments are considered. Overall, the addition of implicit arguments created a 65% relative (18-point absolute) gain in role cov-erage across the 1,253 predicate instances that we annotated.

The predicates in Table 1 are typically associ-ated with fewer arguments on average than their corresponding verbal predicates. When consid-ering NomBank-only arguments, this difference (compare columns four and five) varies from zero (for price ) to a factor of five (for fund ). When im-plicit arguments are included in the comparison, these differences are reduced and many nominal predicates express approximately the same num-ber of arguments on average as their verbal coun-terparts (compare the fifth and seventh columns).
In addition to role coverage and average count, we examined the location of implicit arguments. Figure 1 shows that approximately 56% of the im-plicit arguments in our data can be resolved within the sentence containing the predicate. The remain-ing implicit arguments require up to forty-six sen-Figure 1: Location of implicit arguments. For missing argument positions with an implicit filler, the y -axis indicates the likelihood of the filler be-ing found at least once in the previous x sentences. tences for resolution; however, a vast majority of these can be resolved within the previous few sen-tences. Section 6 discusses implications of this skewed distribution. 4.1 Model formulation In our study, we assumed that each sentence in a document had been analyzed for PropBank and NomBank predicate-argument structure. Nom-Bank includes a lexicon listing the possible ar-gument positions for a predicate, allowing us to identify missing argument positions with a simple lookup. Given a nominal predicate instance p with a missing argument position iarg n , the task is to search the surrounding discourse for a constituent c that fills iarg n . Our model conducts this search over all constituents annotated by either PropBank or NomBank with non-adjunct labels.

A candidate constituent c will often form a coreference chain with other constituents in the discourse. Consider the following abridged sen-tences, which are adjacent in their Penn TreeBank document: (3) [Mexico] desperately needs investment. (4) Conservative Japanese investors are put off (5) Japan is the fourth largest investor in NomBank does not associate the labeled instance of investment with any arguments, but it is clear from the surrounding discourse that constituent c (referring to Mexico) is the thing being invested in (the iarg 2 ). When determining whether c is the iarg 2 of investment , one can draw evidence from other mentions in c  X  X  coreference chain. Example 3 states that Mexico needs investment. Example 4 states that Mexico regulates investment. These propositions, which can be derived via traditional SRL analyses, should increase our confidence that c is the iarg 2 of investment in Example 5.

Thus, the unit of classification for a candi-date constituent c is the three-tuple  X  p,iarg n ,c 0  X  , where c 0 is a coreference chain comprising c and its coreferent constituents. 3 We defined a binary classification function Pr (+ | X  p,iarg n ,c 0  X  ) that predicts the probability that the entity referred to by c fills the missing argument position iarg n of predicate instance p . In the remainder of this pa-per, we will refer to c as the primary filler , dif-ferentiating it from other mentions in the corefer-ence chain c 0 . In the following section, we present the feature set used to represent each three-tuple within the classification function. 4.2 Model features Starting with a wide range of features, we per-formed floating forward feature selection (Pudil et al., 1994) over held-out development data com-prising implicit argument annotations from section 24 of the Penn TreeBank. As part of the feature selection process, we conducted a grid search for the best per-class cost within LibLinear X  X  logistic regression solver (Fan et al., 2008). This was done to reduce the negative effects of data imbalance, which is severe even when selecting candidates from the current and previous few sentences. Ta-ble 2 shows the selected features, which are quite different from those used in our previous work to identify traditional semantic arguments (Gerber et al., 2009). 4 Below, we give further explanations for some of the features.

Feature 1 models the semantic role relationship between each mention in c 0 and the missing argu-ment position iarg n . To reduce data sparsity, this feature generalizes predicates and argument posi-tions to their VerbNet (Kipper, 2005) classes and # Feature value description 1* For every f , the VerbNet class/role of p f / arg concatenated with the class/role of p / iarg n . 2* Average pointwise mutual information between  X  p,iarg n 3 Percentage of all f that are definite noun phrases. 4 Minimum absolute sentence distance from any f to p . 5* Minimum pointwise mutual information between  X  p,iarg n 7 Nominal form of p concatenated with iarg n . 9 Number of mentions in c 0 . 10* Head word of p  X  X  right sibling node. 12 Part of speech of the head of p  X  X  parent node. 13 Average absolute sentence distance from any f to p . 15 Number of left siblings of p . 16 Whether p is the head of its parent node. 17 Number of right siblings of p .
 Table 2: Features for determining whether c fills iarg of predicate p . For each mention f (denoting a f iller) in the coreference chain c 0 , we define p f and arg f to be the predicate and argument position of f . semantic roles using SemLink. 5 For explanation purposes, consider again Example 1, where we are trying to fill the iarg 0 of shipping . Let c 0 contain a single mention, The two companies , which is the arg 0 of produce . As described in Table 2, fea-ture 1 is instantiated with a value of create.agent -send.agent , where create and send are the VerbNet classes that contain produce and ship , respectively. In the conversion to LibLinear X  X  instance repre-sentation, this instantiation is converted into a sin-gle binary feature create.agent -send.agent whose value is one. Features 1 and 11 are instantiated once for each mention in c 0 , allowing the model to consider information from multiple mentions of the same entity.

Features 2 and 5 are inspired by the work of Chambers and Jurafsky (2008), who inves-tigated unsupervised learning of narrative event sequences using pointwise mutual information (PMI) between syntactic positions. We used a sim-ilar PMI score, but defined it with respect to se-mantic arguments instead of syntactic dependen-cies. Thus, the values for features 2 and 5 are computed as follows (the notation is explained in the caption for Table 2): pmi (  X  p,iarg n  X  ,  X  p f ,arg f  X  ) = log To compute Equation 6, we first labeled a subset of the Gigaword corpus (Graff, 2003) using the ver-bal SRL system of Punyakanok et al. (2008) and the nominal SRL system of Gerber et al. (2009). We then identified coreferent pairs of arguments using OpenNLP. Suppose the resulting data has N coreferential pairs of argument positions. Also suppose that M of these pairs comprise  X  p,arg n  X  and  X  p f ,arg f  X  . The numerator in Equation 6 is defined as M N . Each term in the denominator is obtained similarly, except that M is computed as the total number of coreference pairs compris-ing an argument position (e.g.,  X  p,arg n  X  ) and any other argument position. Like Chambers and Ju-rafsky, we also used the discounting method sug-gested by Pantel and Ravichandran (2004) for low-frequency observations. The PMI score is some-what noisy due to imperfect output, but it provides information that is useful for classification.
Feature 10 does not depend on c 0 and is specific to each predicate. Consider the following exam-ple: (7) Statistics Canada reported that its [ arg 1 The  X  X  p price] index X  collocation is rarely associ-ated with an arg 0 in NomBank or with an iarg 0 in our annotations (both argument positions denote the seller). Feature 10 accounts for this type of be-havior by encoding the syntactic head of p  X  X  right sibling. The value of feature 10 for Example 7 is price:index . Contrast this with the following: (8) [ iarg 0 The company] is trying to prevent The value of feature 10 for Example 8 is price:drop . This feature captures an important dis-tinction between the two uses of price : the for-mer rarely takes an iarg 0 , whereas the latter often does. Features 12 and 15-17 account for predicate-specific behaviors in a similar manner.

Feature 14 identifies the discourse relation (if any) that holds between the candidate constituent c and the filled predicate p . Consider the following example: (9) [ iarg 0 SFE Technologies] reported a net loss (10) That compared with an operating [ p loss ] of In this case, a comparison discourse relation (sig-naled by the underlined text) holds between the first and sentence sentence. The coherence pro-vided by this relation encourages an inference that identifies the marked iarg 0 (the loser). Through-out our study, we used gold-standard discourse re-lations provided by the Penn Discourse TreeBank (Prasad et al., 2008). We trained the feature-based logistic regression model over 816 annotated predicate instances as-sociated with 650 implicitly filled argument posi-tions (not all predicate instances had implicit ar-guments). During training, a candidate three-tuple  X  p,iarg n ,c 0  X  was given a positive label if the can-didate implicit argument c (the primary filler) was annotated as filling the missing argument position. To factor out errors from standard SRL analyses, the model used gold-standard argument labels pro-vided by PropBank and NomBank. As shown in Figure 1 (Section 3.2), implicit arguments tend to be located in close proximity to the predicate. We found that using all candidate constituents c within the current and previous two sentences worked best on our development data.

We compared our supervised model with the simple baseline heuristic defined below: 6 The normalization allows an existing arg 0 for the verb invested to fill an iarg 0 for the noun in-vestment . We also evaluated an oracle model that made gold-standard predictions for candidates within the two-sentence prediction window.

We evaluated these models using the methodol-ogy proposed by Ruppenhofer et al. (2009). For each missing argument position of a predicate in-stance, the models were required to either (1) iden-tify a single constituent that fills the missing argu-ment position or (2) make no prediction and leave the missing argument position unfilled. We scored predictions using the Dice coefficient, which is de-fined as follows: Predicted is the set of tokens subsumed by the constituent predicted by the model as filling a missing argument position. True is the set of tokens from a single annotated constituent that fills the missing argument position. The model X  X  prediction receives a score equal to the maxi-mum Dice overlap across any one of the annotated fillers. Precision is equal to the summed predic-tion scores divided by the number of argument po-sitions filled by the model. Recall is equal to the summed prediction scores divided by the number of argument positions filled in our annotated data. Predictions not covering the head of a true filler were assigned a score of zero.
P R F 1 p R F 1 sale 64 60 50.0 28.3 36.2 47.2 41.7 44.2 0.118 80.0 88.9 price 121 53 24.0 11.3 15.4 36.0 32.6 34.2 0.008 88.7 94.0 bid 19 26 100.0 19.2 32.3 23.8 19.2 21.3 0.280 57.7 73.2 plan 25 20 83.3 25.0 38.5 78.6 55.0 64.7 0.060 82.7 89.4 cost 25 17 66.7 23.5 34.8 61.1 64.7 62.9 0.024 94.1 97.0 loss 30 12 71.4 41.7 52.6 83.3 83.3 83.3 0.020 100.0 100.0 loan 11 9 50.0 11.1 18.2 42.9 33.3 37.5 0.277 88.9 94.1 investment 21 8 0.0 0.0 0.0 40.0 25.0 30.8 0.182 87.5 93.3 fund 43 6 0.0 0.0 0.0 14.3 16.7 15.4 0.576 50.0 66.7
Our evaluation data comprised 437 predicate in-stances associated with 246 implicitly filled ar-gument positions. Table 3 presents the results. Predicates with the highest number of implicit ar-guments -sale and price -showed F 1 increases of 8 points and 18.8 points, respectively. Over-all, the discriminative model increased F 1 perfor-mance 15.8 points (59.6%) over the baseline.
We measured human performance on this task by running our undergraduate assistant X  X  annota-tions against the evaluation data. Our assistant achieved an overall F 1 score of 58.4% using the same candidate window as the baseline and dis-criminative models. The difference in F 1 between the discriminative and human results had an ex-act p -value of less than 0.001. All significance testing was performed using a two-tailed bootstrap method similar to the one described by Efron and Tibshirani (1993). 6.1 Feature ablation We conducted an ablation study to measure the contribution of specific feature sets. Table 4 presents the ablation configurations and results. For each configuration, we retrained and retested the discriminative model using the features de-scribed. As shown, we observed significant losses when excluding features that relate the seman-tic roles of mentions in c 0 to the semantic role Configuration P R F 1
Remove 1,2,5 -35.3
Use 1,2,5 only -26.3
Remove 14 0.2 Table 4: Feature ablation results. The first column lists the feature configurations. All changes are percentages relative to the full-featured discrimi-native model. p -values for the changes are indi-cated in parentheses. of the missing argument position (first configura-tion). The second configuration tested the effect of using only the SRL-based features. This also re-sulted in significant performance losses, suggest-ing that the other features contribute useful infor-mation. Lastly, we tested the effect of removing discourse relations (feature 14), which are likely to be difficult to extract reliably in a practical set-ting. As shown, this feature did not have a statis-tically significant effect on performance and could be excluded in future applications of the model. 6.2 Unclassified true implicit arguments Of all the errors made by the system, approxi-mately 19% were caused by the system X  X  failure to generate a candidate constituent c that was a cor-rect implicit argument. Without such a candidate, the system stood no chance of identifying a cor-rect implicit argument. Two factors contributed to this type of error, the first being our assumption that implicit arguments are also core (i.e., arg n ) arguments to traditional SRL structures. Approxi-mately 8% of the overall error was due to a failure of this assumption. In many cases, the true im-plicit argument filled a non-core (i.e., adjunct) role within PropBank or NomBank.

More frequently, however, true implicit argu-ments were missed because the candidate window was too narrow. This accounts for 12% of the overall error. Oracle recall (second-to-last col-umn in Table 3) indicates the nominals that suf-fered most from windowing errors. For exam-ple, the sale predicate was associated with the highest number of true implicit arguments, but only 80% of those could be resolved within the two-sentence candidate window. Empirically, we found that extending the candidate window uni-formly for all predicates did not increase perfor-mance on the development data. The oracle re-sults suggest that predicate-specific window set-tings might offer some advantage. 6.3 The investment and fund predicates In Section 4.2, we discussed the price predicate, which frequently occurs in the  X  X  p price] index X  collocation. We observed that this collocation is rarely associated with either an overt arg 0 or an implicit iarg 0 . Similar observations can be made for the investment and fund predicates. Al-though these two predicates are frequent, they are rarely associated with implicit arguments: invest-ment takes only eight implicit arguments across its 21 instances, and fund takes only six implicit ar-guments across its 43 instances. This behavior is due in large part to collocations such as  X  X  p in-vestment] banker X ,  X  X tock [ p fund] X , and  X  X utual [ p fund] X , which use predicate senses that are not eventive. Such collocations also violate our as-sumption that differences between the PropBank and NomBank argument structure for a predicate are indicative of implicit arguments (see Section 3.1 for this assumption).

Despite their lack of implicit arguments, it is important to account for predicates such as in-vestment and fund because incorrect prediction of implicit arguments for them can lower precision. This is precisely what happened for the fund pred-icate, where the model incorrectly identified many implicit arguments for  X  X tock [ p fund] X  and  X  X u-tual [ p fund] X . The left context of fund should help the model avoid this type of error; however, our feature selection process did not identify any over-all gains from including this information. 6.4 Improvements versus the baseline The baseline heuristic covers the simple case where identical predicates share arguments in the same position. Thus, it is interesting to examine cases where the baseline heuristic failed but the discriminative model succeeded. Consider the fol-lowing sentence: (12) Mr. Rogers recommends that [ p investors ] Neither NomBank nor the baseline heuristic asso-ciate the marked predicate in Example 12 with any arguments; however, the feature-based model was able to correctly identify the marked iarg 2 as the entity being invested in. This inference captured a tendency of investors to sell the things they have invested in.

We conclude our discussion with an example of an extra-sentential implicit argument: (13) [ iarg 0 Olivetti] has denied that it violated As shown in Example 13, the system was able to correctly identify Olivetti as the agent in the sell-ing event of the second sentence. This inference involved two key steps. First, the system identified coreferent mentions of Olivetti that participated in exporting and supplying events (not shown). Sec-ond, the system identified a tendency for exporters and suppliers to also be sellers. Using this knowl-edge, the system extracted information that could not be extracted by the baseline heuristic or a tra-ditional SRL system. Current SRL approaches limit the search for ar-guments to the sentence containing the predicate of interest. Many systems take this assumption a step further and restrict the search to the predi-cate X  X  local syntactic environment; however, pred-icates and the sentences that contain them rarely exist in isolation. As shown throughout this paper, they are usually embedded in a coherent and se-mantically rich discourse that must be taken into account. We have presented a preliminary study of implicit arguments for nominal predicates that focused specifically on this problem.

Our contribution is three-fold. First, we have created gold-standard implicit argument annota-tions for a small set of pervasive nominal predi-cates. 7 Our analysis shows that these annotations add 65% to the role coverage of NomBank. Sec-ond, we have demonstrated the feasibility of re-covering implicit arguments for many of the pred-icates, thus establishing a baseline for future work on this emerging task. Third, our study suggests a few ways in which this research can be moved forward. As shown in Section 6, many errors were caused by the absence of true implicit arguments within the set of candidate constituents. More in-telligent windowing strategies in addition to al-ternate candidate sources might offer some im-provement. Although we consistently observed development gains from using automatic coref-erence resolution, this process creates errors that need to be studied more closely. It will also be important to study implicit argument patterns of non-verbal predicates such as the partitive percent . These predicates are among the most frequent in the TreeBank and are likely to require approaches that differ from the ones we pursued.

Finally, any extension of this work is likely to encounter a significant knowledge acquisition bot-tleneck. Implicit argument annotation is difficult because it requires both argument and coreference identification (the data produced by Ruppenhofer et al. (2009) is similar). Thus, it might be produc-tive to focus future work on (1) the extraction of relevant knowledge from existing resources (e.g., our use of coreference patterns from Gigaword) or (2) semi-supervised learning of implicit argument models from a combination of labeled and unla-beled data.
 We would like to thank the anonymous review-ers for their helpful questions and comments. We would also like to thank Malcolm Doering for his annotation effort. This work was supported in part by NSF grants IIS-0347548 and IIS-0840538.
