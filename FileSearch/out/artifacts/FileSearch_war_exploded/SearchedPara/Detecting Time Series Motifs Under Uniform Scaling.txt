 Time series motifs are approximately repeated patterns found within the data. Such motifs have utility for many data min-ing algorithms, including rule-discovery, novelty-detection, summarization and clustering. Since the formalization of the problem and the introduction of efficient linear time al-gorithms, motif discovery has been successfully applied to many domains, including medicine, motion capture, robotics and meteorology.

In this work we show that most previous applications of time series motifs have been severely limited by the defini-tion X  X  brittleness to even slight changes of uniform scaling, the speed at which the patterns develop. We introduce a new algorithm that allows discovery of time series motifs with invariance to uniform scaling, and show that it pro-duces objectively superior results in several important do-mains. Apart from being more general than all other motif discovery algorithms, a further contribution of our work is that it is simpler than previous approaches, in particular we have drastically reduced the number of parameters that need to be specified.
 H.2.8 [ DATABASE MANAGEMENT ]: Database Ap-plications  X  Data mining Algorithms Time Series, Motifs, Random Projection, Uniform Scaling
Time series motifs are approximately repeated patterns found within the data. For many data mining areas the detection of such repeated patterns is of essential impor-tance. A few tasks, among others, that utilize time series Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. motif detection are for example rule-discovery [21], novelty-detection, clustering and summarization [26]. Motif discov-ery has been successfully applied throughout a large range of domains too, such as medicine [2][3], motion-capture [8][22], robotics, video surveillance [13] and meteorology [21]. Here, we show that the existing approaches for motif detection are limited to discovering pattern occurrences of the same length, failing to capture similarities when the occurrences are uniformly scaled along the time axis. To motivate the need for such uniform-scaling invariant motif discovery we will examine a synthetic time series (synthetic data is used here for ease of exposition, real-world examples are studied in the experimental section). Consider the time series in Figure 1.
 Figure 1: A time series of length 600. What is the best motif of length 120 (the length of the gray bar)?
If we are asked to point out the best repeated pattern of length 120, the answer appears trivial: there is an obvious repeated sine wave of approximate length 120 in two loca-tions. However, as we can see in Figure 2, this is not the true motif in this data set.
 Figure 2: Top : An annotated version of Figure 1. Bottom Left : The closest subsequence to A , using Euclidean dis-tance, is in fact subsequence C . Bottom Right : By plotting B on top of A we can see the reason for this unintuitive result.

The reason for this unintuitive result is that A differs from B by a linear scaling of 5%. This means that although shorter subsections of the two subsequences are almost iden-tical, when we attempt to align them, the cumulative error of the out-of-phase sections will tend to dominate. Note that the problem here is exacerbated by the fact that the patterns are complex, meaning that they have many peaks and valleys, insuring that if two otherwise similar patterns have different linear scaling, at least some peaks will have to map to valleys, resulting in a large Euclidean distance. Mo-tif discovery has been shown to be useful in many domains, but practitioners have only used it for relatively simple pat-terns (one or two peaks and valleys).

It is important to dismiss two apparent solutions to this problem before introducing our technique:
In this work we will show that we can efficiently discover motifs under uniform scaling. Furthermore, we will show on diverse datasets that accounting for uniform scaling allows us to discover motifs which are objectively and subjectively correct, and which are missed by the other time series motif discovery algorithms.
The importance of uniform scaling for indexing and match-ing time series has been noted and addressed in several com-munities, including motion-capture [16] and music. How-ever, it has yet to be addressed for the motif problem. The idea that approximately repeated patterns can be informa-tive has permeated the field of bioinformatics for decades [20], and was hinted at in time series data mining literature for as perhaps as long. However, the first formal definition of time series approximately repeated patterns, the time series motif, appeared as recently as 2003 [9]. Since then, there have been dozens of papers that use time series motifs in a variety of applications (hereafter we will use time series motifs and motifs interchangeably).

Garbay and colleagues have used time series motifs in different medical applications, including medical telepres-ence [10] and intensive care monitoring. Minnen et al. use motifs in a series of papers that examine the utility of mon-itoring on-body sensors [22]. Tanaka et al. use motifs to find patterns in sports motion capture data [28]. Murakami et al. use motifs to discover regularities in the behavior of robots [23]. In a series of papers Abe and colleagues [1] have used motifs as an input to rule finding algorithms in medical domains. Androulakis and colleagues use motifs for a vari-ety of bioinformatics problems, for example finding  X  X egimes with similar kinetic characteristics X  in autoignition [2], and selecting maximally informative genes [3]. Hamid et al. use motifs as a primitive in a video surveillance application [13]. Celly and Zordan [8] use motifs to create Animated People Textures (APT) from motion-capture recordings; Applica-tions for APT include video based animations for electronic games and creating background elements and special effects for movies.

Arita et al. [5] use time series motifs in a  X  X eal-time hu-man proxy X  (i.e telepresence) application. The idea is that motif discovery is used offline to capture typical human mo-tions, for example  X  X ointing with right hand X  or  X  X ead nod-ding X . These complex actions can then be represented and efficiently transmitted as a single symbol, rather than a com-plex set of real valued motion trajectories. The authors do understand the need for uniform scaling in their application. This is achieved by a combination of human intervention and a quadratic time dynamic programming technique.

In spite of this wealth of works that use or extend the no-tion of time series motifs, none of them address the uniform scaling problem. Perhaps the most sophisticated extension to the original motif paper is [28], yet even this work ex-plicitly states  X  X e can not extract motifs whose lengths are different from each other though they have the same behav-ior X . This is exactly the problem solved in this work.
We start by briefly revising the uniform scaling distance as introduced in [18]. Suppose that we have a query time series Q = ( q 1 , q 2 , . . . , q m q ) and a candidate matching se-quence C = ( c 1 , c 2 , . . . , c m c ). Without loss of generality, assume that m q  X  m c . A uniformly scaled version of the query Q with scaling factor m q s is the time series Q s = ( q 1 , q s 2 , . . . , q s s ), where q s i = q d i m q s e (see Figure 3). Figure 3: Comparing Q and C directly ( Left ) yields larger Euclidean distance. Right : Stretching Q with 8% produces Q s which resembles very closely the prefix of C .
The uniform scaling distance d u ( Q , C ) is defined as the optimal squared Euclidean distance between some prefix of C with length s  X  m q and the query scaled to the size of that prefix. Or more formally:
To check all s , equation (1) computes ( m c  X  m q + 1) Eu-clidean distances between the scaled query and a prefix of C . As for most practical purposes a very small rescaling is required to identify the best match (see Section 6), d has an overall amortized cost of  X ( cm q ), for some constant c &gt; 1 [18]. With this in mind, one can think of the near-est neighbor search under uniform scaling as a search under Euclidean distance but in a new, denser sample space. In this space every original candidate sequence C is replaced with a neighborhood of c new sequences.
Now, we formally introduce the time series motif finding problem. As searching for motifs in the denser uniform scal-ing space can be hard under the original motif definition, we also provide an equivalent alternative definition that will be utilized in the proposed approach.

Time series motifs are defined as the approximate occur-rences of a subsequence in the time series S = ( s 1 , s 2 at several significantly different positions [19].  X  X pproxi-mate X  here is expressed in terms of a distance function d and a range r , i.e. two subsequences S m i = ( s i , s i +1 , . . . , s if d ( S m i , S m j )  X  r . The starting positions i and j are as-sumed to be significantly different, if there exists i 1 such looking for approximate appearances of a subsequence we are not considering its slightly shifted versions, which often will be within range r from it. Two approximately similar, with respect to r and d , subsequences that start at signifi-cantly different positions are called a non-trivial match .
Using the introduced terminology, the formal definition of a motif is as follows [19]:
Definition 1. Range Motif . Given a time series S, a sub-sequence length m and a range r , the most significant range motif of length m in S is the subsequence S m i , which has the highest count of non-trivial matches S m j , such that d ( S  X  r .

One could also be interested in the subsequence that has the second largest count of similar subsequences. This subse-quence is called second-motif. Other, less significant motifs, can also be defined similarly.

There exists a close analogy between motif detection based on Definition 1 and density estimation methods using neigh-borhood of fixed volume [11]. Indeed, detecting a time series motif is very similar to computing the density around each subsequence, where the examples in the subsequence neigh-borhood are its non-trivial matches. The most significant motif is simply the sequence whose neighborhood has max-imum density (i.e. largest number of neighbors).

One of the problems of the fixed range neighborhood, how-ever, is that specifying the right range is not always intu-itive [11]. Consider for example Figure 4. Even a person Figure 4: The same time series sampled at different rate. Using the same range r , one detects two different motifs. of expertise may not be able to provide the right range to detect the motif in the time series, as different time series may be sampled at different rate and hence require different ranges r . Therefore, probing a number of possible ranges is inevitable. In this process, very small ranges will result in empty neighborhoods, and no motif detection, while large ranges will return as most significant some not so interesting patterns. As pointed out in Section 3, allowing a uniform scaling factor will further increase the number of examples and the probability of having higher density regions, i.e. small neighborhoods populated with many examples. Thus, the task of specifying appropriate ranges for detecting the most significant motif in the uniformly scaled space becomes rather difficult.

Here, we express the concept of significant motifs in terms of nearest neighbors. The k -Nearest-Neighbor ( k NN) method provides an alternative approach to nonparametric density estimation. It is more adaptive to variations in the space density and alleviates the problem of specifying the not so intuitive range parameter. Firstly, we introduce the term non-trivial nearest neighbor as: Definition 2. Non-trivial nearest neighbor . Let S m i and S j be two subsequences of length m in a time series S of length n . We say that S m j , 1  X  j  X  n  X  m +1 is a non-trivial neighbor of S m i , 1  X  i  X  n  X  m +1, if there exists a subsequence S m i 1 in S , such that i &lt; i 1 &lt; j and d ( S d ( S m i , S m j ). The nearest non-trivial neighbor to S non-trivial neighbor S m j with minimal distance d ( S m i
Similarly, one can define the second or another more re-mote non-trivial nearest neighbor. Using the k non-trivial nearest neighbors to a sequence we come with a formaliza-tion which, equivalently to Definition 1, captures the notion of approximately similar patterns in the time series data.
Definition 3. Nearest-Neighbor Motif . The most signifi-cant nearest neighbor motif of length m in a time series S of length n , is the subsequence S m i , 1  X  i  X  n  X  m +1 which has minimal distance to its non-trivial nearest neighbor S 1  X  j  X  n  X  m +1.

The k th most significant motif is now defined as the se-quence with minimal average distance to its k nearest neigh-bors. Definitions 2 and 3 can naturally be extended for motifs under uniform scaling if the length of the matching sequence S m j is allowed to differ from the length of S m if the Euclidean distance d ( S m i , S m j ) is replaced with the uniform scaling distance d u :
Definition 4. Uniform Scaling Motif . The most signifi-cant uniform scaling motif of length m q , in a time series S of length n , is the subsequence S m q i , 1  X  i  X  n  X  m q has minimal uniform scaling distance d u ( S m q i , S m c non-trivial nearest neighbor S m c j , 1  X  j  X  n  X  m c +1.
For the rest of the paper we derive an effective and ef-ficient probabilistic approach to detecting the best motifs under uniform scaling, and also show that such motifs often represent far more  X  X nteresting X  patterns in the data, than the motifs under Euclidean distance.
For a time series S of length n the brute force motif detec-tion algorithm will perform  X ( n 2 ) pairwise distance compu-tations, between subsequences S m q i and S m c j . As discussed in Section 3, if each of these computations is performed with uniform scaling, then they will have a complexity of  X ( cm This means that the total cost of finding the most significant motif under uniform scaling, using a brute force search, will become  X ( cn 2 m q ). Lower bounding techniques, such as the one suggested in [18], can speed up the computation of the uniform scaling distances. For larger values of n , however, the algorithm still remains intractable.

Rather than computing all pairwise distances, the ap-proach proposed here runs a filter linear in n , and removes from consideration a large number of subsequences. In a following refinement step, only the uniform scaling distance between the nonfiltered sequences is computed. The filter-ing step is derived from the random projection algorithm proposed by Buhler and Tompa [6]. Though probabilistic, we demonstrate that the approach has a high motif detec-tion rate. The gain in speed-up over the brute force search, however, is enormous.
The following  X  X hallenge X  problem, introduced by Pevzner and Sze [24], has inspired much endeavor in the bioinformat-ics community since it appeared few years ago: Given is a sample of t = 20 nucleotide strings of length n = 600. An unknown motif M of length l = 15 is used to generate t new motif occurrences. Every occurrence differs from M in exactly d = 4 base pairs (letters). Each one of these motif occurrences is planted at a random location in one of the t nucleotide strings. The goal is to detect the planted ( l, d )-motif.

Detecting the hidden signal turns out to be rather difficult as two occurrences might have as many as eight differences, which disguises the original motif considerably. While deter-ministic solutions of the problem are exponential in the motif length l , and hence impractical, Pevzner and Szi show that approaches, such as sampling or expectation maximization detect local minima while searching for the (15,4)-motif.
In [6], Buhler and Tompa demonstrate an interesting prob-abilistic approach that efficiently identifies the challenge mo-tif as well as other difficult motifs, e.g. the (14,4)-, (16,5)-, and (18,6)-motifs. They study the family of locality-preserving hash functions h ( w ) [14], that project the l -letter words w ( w 1 , w 2 , . . . , w l ) over an alphabet  X , into  X  l -letter words (  X  l &lt; l ). I.e. h ( w ) :  X  l  X   X   X  l , and h ( w ) = b w ( w The basic observation is that if we select  X  l  X  l  X  d positions at random from all l -letter subsequences in the strings, there is high probability that at least &gt; 1 occurrences of the planted motif will hash to the same bucket (string). On the contrary, all other l -letter strings are likely to hash into buckets with less than elements. The algorithm derived by Buhler and Tompa, called PROJECTION, performs I iterations, repeatedly selecting a different hash function h using a random set of projecting dimensions { w i } . After hashing all l -letter subsequences, the threshold is applied to filter out buckets that are unlikely to contain the motif occurrences. Finally, a refinement step based on expecta-tion maximization infers the motif that would maximize the likelihood of observing the unfiltered buckets from each it-eration.
 Using a random set of dimensions for hashing by PRO-JECTION is reasonable, as it guarantees that a set of ran-dom words over the alphabet  X  will be uniformly spread across all possible hash values. Here, of course, an im-plicit assumption is made, that all letters in the alphabet are equiprobable. Another limitation of locality-preserving hashing is that it is effective for a relatively small number of projected dimensions (10  X  20) [14]. Applying it to larger subsequences would practically require all pairwise subse-quence comparisons to be performed.
PROJECTION can be adapted to detect time series range motifs (see Definition 1), provided there is a suitable repre-sentation of the real value data with a finite alphabet. Chiu et al. [9] demonstrate one such adaptation. They first con-vert all extracted time series subsequences to a Symbolic Aggregate approXimation (SAX) form [19] (Figure 5). Figure 5: Symbolic aggregate approximation of a time se-ries as the four letter word  X  X bad X . The size of the alphabet used is 4 ( X  = { a, b, c, d } ). The grey horizontal cut-lines outline equal volume segments under the normal distribu-tion curve.

SAX normalizes every sequence S m to have mean zero and standard deviation one. Then, using a user specified word length l , it computes its piecewise aggregate approximation P AA ( S m ) =  X  S (  X  s 1 ,  X  s 2 , . . . ,  X  s l ), where  X  s Finally it splits the area under the normal curve into |  X  | segments, where |  X  | is a user specified alphabet size. Every value  X  s i in the PAA representation is then substituted with the letter w i , which labels the corresponding normal curve segment. This operation transforms the initial sequence S into the l -letter word w .

Assigning letters to segments of equal volume under the normal curve guarantees the implicit assumption of PRO-JECTION for equiprobable symbols. Therefore, all colli-sions during the hashing process are most likely a result of the similarities between certain sequences.

The time series motif discovery continues by building a sparse collision matrix M n x n (see Figure 6). At each itera-tion of the projection algorithm, when two sequences con-verted to the SAX words w x and w y are subsequently pro-jected to the same value h i ( w x ) = h i ( w y ) = b w x Figure 6: Left : Iteration 1 of the projection algorithm. Di-mensions 1 and 4 are selected as projecting dimensions. The corresponding positions for the identically projected strings in the collision matrix are increased with one. Right : The collision matrix after the second iteration. for cell ( x, y ) in M is incremented by one. At the end of the algorithm, all cells in M with counters bigger than the threshold are returned as possible locations of the most significant motifs. For these, and only these, locations the algorithm computes the actual Euclidean distance between their corresponding sequences. The algorithm, though sub-quadratic in theory, is empirically demonstrated in [9] to be approximately linear with respect to the time series size n both in terms of memory requirements and in terms of the number of brute force Euclidean distance computations.
A significant drawback of the above time series motif find-ing approach is that it requires supervision in selecting a large set of input parameters. Namely, the tuple ( |  X  | , m , l , d ,  X  l , I ), with elements respectively the alphabet size ( |  X  | ), the minimum time series motif length ( m ), the length of its string representation ( l ), the number of letter differences allowed between two motif occurrences ( d ), the number of hashed dimensions (  X  l ), and the number of iterations neces-sary to detect the motif ( I ). This differs from the original PROJECTION algorithm, which assumes that all strings are over the four-letter DNA alphabet, and that the motif has fixed size l and number of differences d (e.g. for the challenge problem l =15 and d =4). Therefore, what needs to be estimated is only the tuple (  X  l , I ) 1 .
Here we show that the time series motif projection algo-rithm of [9] can be extended to capture motifs under the uniform scaling distance d u (see Definition 3). We further demonstrate that an optimal, in terms of effectiveness and completely unsupervised manner, requiring the user to pro-vide only the minimum time series motif length m .
We first make two observations that allow for the efficient adaptation of the uniform scaling distance in the random projection algorithm. The first observation suggests that one can eliminate large scaling factors in the computation of the distance, as they lead to excessive stretches of S m q cannot result in a significant motif. The maximal scaling factor, evaluated on a number of real-world data sets, which
Both algorithms also require the filtering threshold , but the authors show that low values as = 1 or 2 are reasonable and perform satisfactorily. impacts the accuracy is 10%  X  40%. The second observation is that there is no need to check every scaling factor either, as close scaling factors produce similar results. For example, if we have stretched S m q i 5% and identified the motifs, next we might skip stretching it 6% as with high probability the most significant motifs across the data set remain the same.
To compute the maximum scaling factor that is sufficient to detect any possible motifs for a data set, we study the cu-mulative empirical distribution of the scalings on the train-ing data. Suppose that on the training set the probability that the most significant motif requires x % scaling is p The cumulative distribution P ( x  X  k ) = P k x =0 p x now gives us the scaling factor k that is sufficient to detect the best motif under uniform scaling with a high confidence (see Fig-ure 7 left). The results are computed on the brain activity data set discussed in the next section and are consistent with results from the other data sets too.
 Figure 7: Left: Cumulative distribution of the maximal scaling factor that can impact the most significant motif. Large scaling factors ( &gt; 20%) seldomly produce a significant motif. Right: Checking every single scaling may not be nec-essary. For some data sets checking every second (or third) factor still leads to detecting the most significant motif with very high probability.

In the experiments here we impose the constraint P ( x  X  k ) = 1, i.e. we check all scalings that have produced a significant uniform scaling motif for any of the training time series. As seen from Figure 7, the maximum scaling factor k that can impact the most significant motif is relatively small. The heuristic holds across different motif lengths m q too. We further observe that increasing m q leads to a decrease in k . This can intuitively be explained with the fact that larger motif lengths define a higher dimensional, and hence sparser, space where all nearest neighbors start drifting apart. An evaluation of the second observation is presented in Figure 7 right (the statistics are again computed for the brain activity data). The graph shows that if, for example, the scaling factor is increased by 4% at a time, on some data sets (e.g. Dataset C and Dataset D ), one can still de-tect more than 95% of the most significant motifs. The fact, again inferred from the training data, can be used in op-timizing the search for other unseen time series generated from a similar process. Combining the two results allows us to achieve speed-up that makes the uniform scaling dis-tance computation comparable to the computation of the Euclidean distance.

To cope with the requirement of supervision in the algo-rithm X  X  parametrization, we look at some of the properties of the space defined by the projected sequences.

Let us assume that the tuple ( |  X  | , l ) is fixed. Using these alphabet and word sizes, we apply SAX to map the best motif occurrences S m q i and S m c j for a time series t into the equal length words w i and w j respectively. If d t is the Ham-ming distance between these two words, then the maximum Hamming distance on the training set is: d = max t d t . For a particular projection size  X  l  X  l  X  d , with analysis similar to the one in [6], we obtain the lower bounding probability of any two motifs in the training set to be hashed in the same bin:
We would like to perform I iterations of PROJECTION and guarantee that at least one of the functions h i hashes together the motif occurrences for any time series t in the training set. In I independent trials the probability that none of the hashing functions detects the most significant motif, expressed as a function of the tuple  X  = ( |  X  | , l, d, is:
There are several important observations to point out here. Firstly, increasing I minimizes p (  X  ), and hence the chance of omitting a motif. Secondly, the probability p d is monotonically increasing when decreasing the projection size  X  l . Therefore, decreasing  X  l also minimizes p (  X  ). However, the large number of iterations I and the small projection sizes  X  l also increase the number of dissimilar sequences that would hash together. All those false positive pairs will not be filtered by the algorithm, and the actual distance between the sequences will be evaluated.

The two objectives that we try to optimize now are the effectiveness and the efficiency of the system. On one hand, to obtain an effective system, that produces very few false dismissal while searching for the most significant motifs, one needs to minimize p (  X  ). On the other hand, the efficiency of the system depends on how many iterations I are neces-sary to be performed and how many false positives are also returned for the subsequent refinement. As each iteration goes through all n subsequences, the cost of the hashing op-erations is  X ( In ). If E 0 is the number of sequences that are less than or equal to d symbols apart, and E i the number of sequences that are d + i symbols apart, then the term P brute force comparisons to be performed during the refine-ment step. To summarize, we derive the following discrete optimization problem, where the effectiveness objective is expressed as a constraint:
From constraints (5) the number of iterations I can be ex-pressed as a function of ( |  X  | , l ,  X  l ). Namely, I log(1  X  p which yields I ( |  X  | , l,  X  l ) = d log q log(1  X  p set q = 0 . 05, which together with the requirement that the two before mentioned observations should hold with prob-ability higher than 0 . 95, guarantees that the probability of detecting the best motifs for all time series in the training set is at least 0 . 9.

After substituting I in (4), L (  X  ) is minimized by perform-ing a full search in the discrete space defined by |  X  | X  [2 , 10], l  X  [2 , 20] and  X  l  X  [2 , l  X  d ], where the intervals are deter-mined based on the bounds within which the locality pre-serving hashing and the SAX algorithm are known to be effective. Note, that the search is over a relatively small space of discrete tuples, which allows for the training proce-dure to identify the optimal parametrization in reasonable time (within minutes for our experiments), and requires no supervision at all.
We demonstrate the usefulness of the uniform scaling dis-tance for detecting motifs in three real-world data sets -brain activity data, motion capture time series, and time series extracted from the shapes of projectiles 2 . The data sets are selected from diverse domains, and are with different characteristics as periodicity, amount of noise, approximate length of the available motifs, and also variable length of the time series for the example.

An evaluation for the expected number of false dismissals, introduced by the probabilistic scheme, as well as the aver-age speed-up over the brute force approaches is also pre-sented.
Physiological data, such as respiratory recordings, heart-beats or brain waves, often contain scaled motifs that are indicative either of normal activities or of certain precondi-tions. Here we study three data sets of brain wave recordings from epileptic patients [4] (see Figure 8). The time series in Dataset E were recorded during epileptic attacks, while Dataset C and Dataset D contain recordings from seizure-free periods.

Finding the most significant motif in the data can be valu-able for one of the primary tasks in the field, namely pre-dicting the seizure periods. The assumption here is that very similar patterns are likely to have similar continua-tions, which can be used in forecasting the unobserved out-comes [27]. On the other hand, the strong similarity in motif occurrences in seizure data as E , can help in isolating groups of neurons that trigger identical activity during the epileptic attacks.

Each data set contains 100 time series of length 4096 points. We split them into a training and testing set of 50 time series each. Note that the time series were recorded from different patients, and still similar patterns can be ob-served across the patients, which is essential for the learning procedure to be effective. As the low frequency noise can ac-cumulate quickly and disguise the real patterns, we further run a low pass moving average filter of size ten data points to smooth the data. Both, the Euclidean distance and the uniform scaling distance find some subjectively interesting motifs, yet the uniform scaling motifs are often more mean-ingful visually. For example, noisy  X  X lateaus X  are often de-tected by the Euclidean distance (see the graph for Data set D ), whereas the uniform scaling usually identifies bursts or periodic patterns which are more useful for better diagnosis
All data used in the evaluation is available at http://www. cs.ucr.edu/ ~ eamonn/SIGKDD07/UniformScaling.html Figure 8: Brain-wave data sets . Motifs discovered by sim-ple Euclidean distance and with uniform scaling. The sliding window length is set to m q = 174 (recordings for 1 sec.). The detected motif occurrences for C, D and E have lengths m c equal to 184, 185 and 192 respectively. The uniform scaling motifs are often more interpretable and visually meaningful. and treatment.

For the three data sets the maximal scaling that can im-pact the most significant motif has been found to be around 15% (see Section 6). The best tuple ( |  X  | , l ,  X  l , I ) for the random projection algorithm, learned also on the training data, is given in Table 1. For every corresponding tuple, the table also summarizes the average test accuracy of the method when every single scaling factor is checked (column 1); and when every second or third scaling factor is checked (columns 2 and 3). The results are consistent with the ex-pectation computed on the training set (see Figure 7), and show that a speed-up can be obtained in computing the uni-form scaling distance by checking every third (Data sets C and D) or every second (Data set E) scaling factor and still obtain relatively high accuracy rate in the detected motifs. Table 1: Optimal parameters for the three data sets and motif Data set C (4, 7, 6, 45) 0.90 0.90 0.90 Data set D (6, 10, 5, 35) 1.00 0.88 0.86 Data set E (3, 10, 8, 31) 0.92 0.92 0.7
We also compute the average speed improvement for a single scaling factor. Figure 9 left presents the improvement introduced by random projection in terms of the number of performed operations, as compared with the brute force searching algorithm. For all three data sets, the probabilis-tic method performs less than 1% of the operations per-formed by the brute force method. For completeness, we also include the results for a brute force search that uses an early abandon cut-off criterion. The early abandoning is a simple technique that keeps track of the minimal distance found so far, and every time when computing the distance between two new elements, it terminates if it estimates that the current minimum will be exceeded. Note, however, that though the early abandoning can speed up the nearest neigh-bor search, it still has to perform all pairwise comparisons. Figure 9: Brain Activity Data . Left: Average improvement in distance computation of PROJECTION for a single scal-ing over the brute force search, and the brute force search extended with early abandon criterion. Right: Improvement in running time.

The graph shows that PROJECTION performs approx-imately 2% of all operations performed by early abandon. This means that the algorithm, prior to the refinement step, has removed from consideration a vast number of the pair-wise distance comparisons. The results point out the much better pruning capability of the locality preserving hashing, compared to the popular triangular inequality. For com-parison, [7] reports that the triangular inequality prunes between 50%-70% distance computations, which we exceed here notably. This, however, comes at the price of a possi-bility for some false dismissals.
 The speed-up introduced by the method is presented in Figure 9 right. The result does not correlate exactly with the improvement in performed operations, because of the relatively large number of iterations that have to be per-formed by the algorithm to guarantee that constraint (5) is satisfied (i.e. that the most significant motif will not be omitted). Still, the probabilistic approach takes only 2%-3% of the time necessary for the brute force algorithm to complete.
 Figure 10: Brain Activity Data Set C . Left: Average im-provement in performed operations with respect to the time series length. Right: Average improvement in running time.
The number of performed operations and the running time naturally dependent on the size of the time series in the data set too. We illustrate this in Figure 10. The figure shows the results for data set C, where for each time se-ries in the training/testing data sets we have taken only the beginning (the first 500, 1000, etc. points). Retraining of the algorithm to obtain the best tuple ( |  X  | , l ,  X  l , I ) is re-quired, because the length of the time series determines the number of subsequences that will be extracted from it using a sliding window, which impacts the density of the input space. For shorter time series the second additive term in optimization problem (4) will be naturally smaller, which favors selecting smaller word and projection sizes. For ex-ample, when the time series are 500 data points long, the test accuracy of the method remains the same -more than 95% when every single and around 90% when every second scaling step is checked. The worse running time of the algo-rithm for the data set of lengths 500 is due to the additional implementation overhead for supporting the sparse collision matrix. In our realization, the matrix is implemented as a set of hash tables which we index using the projected SAX words as keys. Incrementing the time series length, however, increases the brute force search requirements quadratically while the PROJECTION solution scales linearly, and thus at some point becomes more efficient.
Motion-capture data finds increasing utility in a number of domains, such as animation, computer games or train-ing simulators [8]. Finding significant motifs under uniform scaling in applications from those areas can also be very useful. For example, detecting the motifs in movements, re-gardless of slight time stretches, can allow users to interact better with console games, such as the popular Nintendo Wii. In animation, on the other hand, motion-capture se-quences are often stitched together to form larger animated episodes. To be more realistic and continuous, the stitching process requires the extrapolation of the first episode with several frames before attaching the second episode. Finding similar motifs and looking at their continuation can help in detecting more suitable extrapolation frames.

In this set of experiments, we have a collection of 75 motion-capture sequences with duration of 10sec-30sec. The motions captured are martial arts movements -kicks, blocks, punches and retracting movements. The time series that we extract from the data comprise the z -coordinate of the sen-sor attached to the left arm of the actor (see Figure 11). For ease of the evaluation, all time series are resampled to a length of 1200 data points. We split the data into 40 time series training set, and 35 time series test set. For minimum motif length we use m q = 120 which for most sequences corresponds to 2sec-3sec movements.

Similar actions are never repeated by humans in precisely the same way, and rather tend to differ in how they stretch in time. This explains why the Euclidean distance, though robust in general, will fail in these cases. In Figure 11, for example, we search for the best motif under Euclidean dis-tance (the top frames and the graph underneath them) and also under uniform scaling (bottom frames and graph). In the beginning of the sequence the actor repeats the same blocking movement twice but the second occurrence is  X  7% longer than the first one. This is detected by the uniform scaling distance, while the Euclidean distance in this case detects two quite different actions. Note also that the sub-sequent few frames for the two motif occurrences, detected by the uniform scaling distance, are also quite similar. This demonstrates how very similar motifs can be used in extrap-olating motion-capture episodes when building animations.
The data is quite different from the EEG data studied ear-lier. The time series have no periodicity and no significant noise is present either. The best tuple learned by PROJEC-projection size  X  l equal to the word size l implies that the Figure 11: Motion-capture data . Top: The best motif detected using Euclidean distance and four corresponding frames extracted from each of the two motif occurrences. Bottom: The best motif under uniform scaling. The corre-sponding frames are part of the same action repeated twice. best motifs for all of the training time series have been sym-bolized by SAX to the exactly same letter representation. Naturally in this case only one iteration is required to ob-tain a hit for any of the motifs in the collision table. The accuracy evaluated on the test time series is: 94% (scaling step = 1), 85% (scaling step = 2) and 85% (scaling step = 3). The best motifs were again estimated to require less than 20% scaling. The method again performs less than 10% of the operations performed by early abandon, which is comparable to the results for the EEG series of length 1000.
While the ideas in this paper apply only to real-valued time series, it has long been noted that in many cases it is possible to meaningfully convert data types as diverse as DNA [15], text [12], XML, video and shapes [17] into time series. In such cases we believe that our motif finding algorithm may be of utility in those domains. As a concrete example we consider the problem of finding motifs in parts of two-dimensional shapes. Figure 12 shows an example of a projectile point (arrowhead) converted into time series. Figure 12: A two-dimensional shape, such as an arrow-head, can be converted to a one-dimensional pseudo time series by tracing the boundary of the shape and recording the local angle.
There are many examples of important databases of shapes in various scientific domains, but as hinted at in Figure 12 we will consider arrowheads here as our motivating domain. At the authors institution, the Lithic Technology Laboratory estimates that they have over one million projectile points, and many other institutions have even larger collections. Figure 13 illustrates the surprising diversity of arrowhead shape.
 Figure 13: A random selection of arrowheads hints at the great variability of possible shapes. Note that the two in the bottom right corner have broken tips.

We could consider the problem of finding the pair of ar-rowheads which are most similar, perhaps by hierarchically clustering the shapes and examining the leaf nodes. How-ever, anthropologists are typically more interested in the ar-rangement of local details [25] (barbs, tips, shoulders, tangs etc). In addition, many arrowheads are broken, frustrating attempts at whole matching.

A query suggested to us by an anthropologist is to exam-ine a large collection of arrowheads to find smaller reoccur-ring details. We formalized this by assuming the region of interest occupied one quarter of the boundary.

Note that the one-dimensional representation of shapes we are using does not guarantee rotation invariance, we solve this problem simply by concatenating each signal with one quarter of itself as shown in Figure 14.
 Figure 14: Converting a collection of shapes to a format suitable for motif mining. Note that because we are inter-ested in motifs which occupy about 1/4 of the boundary, we have concatenated 1/4 of each signal to itself (dashed line).
Given this representation there is only one minor modifi-cation to be made to our algorithm. As the sliding window is moving across the long time series which represents the entire collection, it will not extract subsequences which con-tain elements from two different shapes (i.e two different colors in Figure 14).

We performed an experiment on a database of 1,231 di-verse arrowheads which come from all over North America. These images were obtained from the UCR Lithic Technol-ogy Laboratory and from various public domain resources.
Motif discovery without uniform scaling revealed the ob-vious but uninteresting motif of arrowhead tips. However, when we attempted motif discovery allowing uniform scal-ing of up to 40%, several interesting motifs did occur. For brevity we will just consider the best motif, which is shown in Figure 15.
 Figure 15: The best motif discovered under a maxi-mum uniform scaling of 40%. The shorter sequence closely matches the longer one when scaled up by 38%.

Figure 16 shows the motifs in context in the original ar-rowheads.
 Figure 16: The best motif under uniform scaling corre-sponds to the double notches of cornertang arrowheads.
In our follow up investigation we found that cornertang shape has long intrigued anthropologists. These objects are relatively rare and are found almost exclusively in Texas.
It is important to note that the shapes of the two full ar-rowheads shown in Figure 16 are not particularly close when measured under the Euclidean, Warping, Chamfer or Haus-dorff [17] distance measures (The Castroville cornertang is much longer and more pointed). It is only by examining subsections that local similarities are revealed.
We studied the problem of detecting time series motifs, which occur with different stretch along the time axis. Such motifs were demonstrated to have important utility in areas as diverse as medical recording analysis, improving game interactivity and animation, or in categorization of shapes. The work introduced an effective and efficient approach for identifying the existing motifs. The algorithm learns a suit-able parametrization in a fully unsupervised manner, using a training set of time series from the domain under study. Though probabilistic, the scheme was shown to have a very low rate of omitting the true motifs.

Our current efforts are targeted towards applying this methodology as part of other learning tasks as subsequence clustering and classification, novelty detection, and even forecasting. We are also exploring the applicability of the approach to other applications, such as the categorization of songs by only using small representative tunes. We would like to thank Dr. Leslie A. Quintero and Dr. Philip J. Wilke of the UCR Lithic Technology Laboratory, for providing us with the projectile samples used in the cur-rent evaluation. [1] H. Abe and T. Yamaguchi. Implementing an [2] I. Androulakis. New approaches for representing, [3] I. Androulakis, J. Wu, J. Vitolo, and C. Roth. [4] R. Andrzejak, K. Lehnertz, F. Mormann, C. R. P. [5] D. Arita, H. Yoshimatsu, and R. Taniguchi. Frequent [6] J. Buhler and M. Tompa. Finding motifs using [7] W. Burkhard and R. Keller. Some approaches to [8] B. Celly and V. Zordan. Animated people textures. In [9] B. Chiu, E. Keogh, and S. Lonardi. Probabilistic [10] F. Duch X ene, C. Garbay, and V. Rialle. Apprentissage [11] R. Duda, P. Hart, and D. Stork. Pattern [12] P. Fung. A pattern matching method for finding noun [13] R. Hamid, S. Maddi, A. Johnson, A. Bobick, I. Essa, [14] P. Indyk, R. Motwani, P. Raghavan, and S. Vempala. [15] E. Keogh, S. Lonardi, V. Zordan, S. Lee, and M. Jara. [16] E. Keogh, T. Palpanas, V. Zordan, D. Gunopulos, and [17] E. Keogh, L. Wei, X. Xi, S. Lee, and M. Vlachos. [18] E. J. Keogh. Efficiently finding arbitrarily scaled [19] J. Lin, E. Keogh, S. Lonardi, and P. Patel. Finding [20] J. Maizel and R. Lenk. Enhanced graphic matrix [21] A. McGovern, D. Rosendahl, A. Kruger, M. Beaton, [22] D. Minnen, T. Starner, I. Essa, and C. Isbell. [23] K. Murakami, S. Doki, S. Okuma, and Y. Yano. A [24] P. Pevzner and S. Sze. Combinatorial approaches to [25] D. Rogers. Corner tang knives across Texas. Personal [26] S. Rombo and G. Terracina. Discovering [27] T. Sauer. Time series prediction by using delay [28] Y. Tanaka, K. Iwamoto, and K. Uehara. Discovery of
