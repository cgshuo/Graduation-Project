 Probabilistic models provide a powerful and intuitive fram ework for formulating several problems in machine learning and its application areas, such as compu ter vision and computational biology. A critical choice to be made when using a probabilistic model i s its complexity. For example, consider a system that involves n random variables. A probabilistic model that defines a cliqu e of size n has the ability to model any distribution over these random v ariables. However, the task of learning and inference on such a model becomes computationally intra ctable. The other extreme case is to define a tree structured model that allows for efficient learn ing [3] and inference [23]. However, tree distributions have a restrictive form. Hence, they are not s uitable for all applications. A natural way to alleviate the deficiencies of tree distribut ions is to use a mixture of trees [21]. Mixtures of trees can be employed as accurate models for seve ral interesting problems such as pose estimation [11] and recognition [5, 12]. In order to facilit ate their use, we consider the problem of learning them by approximating an observed distribution . Note that the mixture can be learned by minimizing the Kullback-Leibler ( K L ) divergence with respect to the observed distribution usin g an expectation-maximization ( EM ) algorithm [21]. However, there are two main drawbacks of th is approach: (i) minimization of K L divergence mostly tries to explain the dominant mode of the algorithm is prone to local minima, its success depends heav ily on the initialization. An intuitive solution to both these problems is to obtain an initial set of trees that covers as much of the observed distribution as possible. To this end, we pose the learning p roblem as that of obtaining a set of trees that minimize a suitable  X  -divergence [25].
 The  X  -divergence measures are a family of functions over two prob ability distributions that measure the information gain contained in them: that is, given the fir st distribution, how much information is obtained by observing the second distribution. They form a complete family of measures, in that no other function satisfies all the postulates of informatio n gain [25]. When used as an objective function to approximate an observed distribution, the valu e of  X  plays a significant role. For exam-ple, when  X  = 1 we obtain the K L divergence. As the value of  X  keeps increasing, the divergence measure becomes more and more inclusive [8], that is it tries to cover as much of the observed dis-tribution as possible [22]. Hence, a natural choice for our t ask of obtaining a good initial estimate would be to set  X  =  X  .
 We formulate the minimization of  X  -divergence with  X  =  X  within the fractional covering frame-work [24]. However, the standard iterative algorithm for so lving fractional covering is not readily applicable to our problem due to its small stepsize. In order to overcome this deficiency we adapt this approach specifically for the task of learning mixtures of trees. Each iteration of our approach adds one tree to the mixture and only requires solving a conve x optimization problem. In practice, our strategy converges within a small number of iterations t hereby resulting in a small mixture of trees. We demonstrate the effectiveness of our approach by p roviding a comparison with state of the art methods and learning pictorial structures [6] for face r ecognition. The mixture of trees model was introduced by Meila and Jordan [21] who highlighted its appeal by providing simple inference and sampling algorithms. The y also described an EM algorithm that learned a mixture of trees by minimizing the K L divergence. However, the accuracy of the EM that their experiments required a large mixture of trees to e xplain the observed distribution, due to random initialization.
 Several works have attempted to obtain a good set of trees by d evising algorithms for minimizing the K L divergence [8, 13, 19, 26]. In contrast, our method uses  X  =  X  , thereby providing a set of trees that covers the entire observed distribution. It has b een shown that mixture of trees admit a decomposable prior [20]. In other words, one can concisely s pecify a certain prior probability for each of the exponential number of tree structures for a given set of random variables. Kirschner and Smyth [14] have also proposed a method to handle a countably i nfinite mixture of trees. However, the complexity of both learning and inference in these model s restricts their practical use. Researchers have also considered mixtures of trees in the lo g-probability space. Unlike a mixture in the probability space considered in this paper (which conta ins a hidden variable), mixtures of trees in log-probability space still define pairwise Markov netwo rks. Such mixtures of trees have been used to obtain upper bounds on the log partition function [27 ]. However, in this case, the mixture is obtained by considering subgraphs of a given graphical mode l instead of minimizing a divergence measure with respect to the observed data. Finally, we note t hat semi-metric distance functions can be approximated to a mixture of tree metrics using the fracti onal packing framework [24]. This allows us to approximate semi-metric probabilistic models to a simpler mixture of (not necessarily tree) models whose pairwise potentials are defined by tree me trics [15, 17]. Tree Distribution. Consider a set of n random variables V = { v v assignment of values) as a vector x = { x random variables V is a graph whose nodes correspond to the random variables and whose edges E define a tree. Such a model assigns a probability to each label ing that can be written as Here  X  T refers to pairwise potentials whose values depend on two nei ghboring variables at a time. The vector  X 
T is the parameter of the model (which consists of all the poten tials) and Z (  X  T ) is the partition function which ensures that the probability sums to one. The term deg ( a ) denotes the degree of the variable v Mixture of Trees. As the name suggests, a mixture of trees is defined by a set of tr ees along with for all T and P  X  -Divergence. The  X  -divergence between distributions Pr( |  X  1 ) (say the observed distribution) and Pr( |  X  2 ) (the simpler distribution) is given by The  X  -divergence measure is strictly non-negative and is equal t o 0 if and only if  X  1 is a reparame-terization of  X  2 . It is a generalization of K L divergence which corresponds to  X  = 1 , that is As mentioned earlier, we are interested in the case where  X  =  X  , that is The inclusive property of  X  =  X  is evident from the above formula. Since we would like to minimize the maximum ratio of probabilities (i.e. the worst case), we need to ensure that no value of admit very small values of Pr( x |  X  2 ) since it is concerned with the summation shown in equation (4 ) (and not the worst case). To avoid confusion, we shall refer t o the case where  X  = 1 as K L divergence and the  X  =  X  case as  X  -divergence throughout this paper.
 The Learning Problem. Given a set of samples { x  X  P ( x i ) , our task is to learn a mixture of trees  X  M  X  such that dropped). We define T = {  X  T j } to be the set of all t tree distributions that are defined over n variables. It follows that the probability of a labeling for any mixture of trees can be written as for suitable values of  X  distribution. In other words,  X  belongs to the polytope P defined as Our task is to find a sparse vector  X  that minimizes the  X  -divergence with respect to the observed dis-tribution. In order to formally specify the minimization of  X  -divergence as an optimization problem, we define an m  X  t matrix A and an m  X  1 vector b such that We denote the i th row of A as a learning problem can be specified as where  X   X  = min natural way to attack the problem would be to use the fraction al covering framework [24]. We begin by briefly describing fractional covering in the next sectio n. Given an m  X  t matrix A and an m  X  1 vector b &gt; 0 , the fractional covering problem is to determine whether there exists a vector  X   X  P such that A  X   X  b . The only restriction on the polytope P is that A  X   X  0 for all  X   X  P , which is clearly satisfied by our learning problem (since a probability of x If  X   X  &lt; 1 then clearly there does not exist a  X  such that A  X   X  b . However, if  X   X   X  1 , then the fractional covering problem requires us to find an  X  -optimal solution, that is find a  X  such that where  X  &gt; 0 is a user-specified tolerance factor. Using the definitions o f A , b and  X  from the previous section, we observe that in our case  X   X  = 1 . In other words, there exists a solution such that A  X  = b . This can easily be seen by considering a tree with parameter  X  T j such that and setting  X  introducing m trees in the mixture (where m is the number of samples provided). We would like to find an  X  -optimal solution with a smaller number of trees by solving t he LP (10). However, we cannot employ standard interior point algorithms for optim izing problem (10). This is due to the fact that each of its m constraints is defined over an infinite number of unknowns (sp ecifically, the mixture coefficients for each of the infinite number of tree di stributions defined over the n random variables). Fortunately, Plotkin et al. [24] provide an iterative algorithm for solving problem (10 ) that can handle arbitrarily large number of unknowns in ever y constraint.
 The Fractional Covering Algorithm. In order to obtain a solution to problem (10), we solve the following related problem: The objective function  X ( y ) is called the potential function for fractional covering. P lotkin et al. [24] showed that minimizing  X ( y ) solves the original fractional covering problem. The term  X  is a parameter that is inversely proportional to the stepsize  X  of the algorithm. The fractional covering such that the update attempts to decrease the potential func tion. Specifically, the algorithm proposed in [24] suggests using the first order approximation of  X ( y ) , that is where Typically, the above problem is easy to solve (including for our case, as will be seen in the next section). Furthermore, for a sufficiently large value of  X  (  X  log m ) the above update rule decreases  X ( y ) . In more detail, the algorithm of [24] is as follows: Plotkin et al. [24] suggest starting with a tolerance factor of  X  by 2 after every call to the above procedure terminates. This pro cess is continued until a sufficiently accurate (i.e. an  X  -optimal) solution is recovered. Note that during each call to the above procedure the potential function  X ( y ) is both upper and lower bounded, specifically Furthermore, we are guaranteed to decrease the value of  X ( y ) at each iteration. Hence, it follows that the above algorithm will converge. We refer the reader t o [24] for more details. The above algorithm provides an elegant way to solve the gene ral fractional covering problem. However, as will be seen shortly, in our case it leads to undes irable solutions. Nevertheless, we show that appropriate modifications can be made to obtain a sm all and accurate mixture of trees. We begin by identify the deficiencies of the fractional coverin g algorithm for our learning problem. 5.1 Drawbacks of the Algorithm There are two main drawbacks of fractional covering. First, the value of  X  is typically very large, which results in a small stepsize  X  . In our experiments,  X  was of the order of 10 3 , which resulted in slow convergence of the algorithm. Second, the update ste p provides singleton trees, that is trees the update step solves the following problem: Note that the above problem is an LP in  X  . Hence, there must exist an optimal solution on the vertex on the polytope P . In other words, we obtain a single tree distribution  X  T  X  such that The optimal tree distribution for the above problem concent rates the entire mass on the sample x convergence of the algorithm. Furthermore, the learned mix ture only provides a non-zero probability for the samples used during training. Hence, the mixture can not be used for previously unseen samples, thereby rendering it practically useless. Note th at the method of Rosset and Segal [26] also faces a similar problem during their update steps for mi nimizing the K L divergence. In order to overcome this difficulty, they suggest approximating probl em (18) by which can be solved efficiently using the Chow-Liu algorithm [3]. However, our preliminary exper-iments (accuracies not reported) indicate that this approa ch does not work well for minimizing the potential function  X ( y ) . 5.2 Fixing the Drawbacks We adapt the original fractional covering algorithm for our problem in order to overcome the draw-backs mentioned above. The first drawback is handled easily. We start with a small value of  X  and guaranteed to terminate. In our experiments, we initialize d  X  = 1 /w and its value never exceeded equation (16) are sufficiently large (at least exp(  X  (1  X   X  )) ).
 In order to address the second drawback, we note that our aim a t an iteration t of the algorithm is to reduce the potential function  X ( y ) . That is, given the current distribution parameterized by  X  M t we would like to add a new tree  X  T t to the mixture that solves the following problem: Here, T is the set of all tree distributions defined over n random variables. Note that the algorithm of [24] optimizes the first order approximation of the object ive function (21). However, as seen pre-viously, for our problem this results in an undesirable solu tion. Instead, we directly optimize  X ( y ) using an alternative two step strategy. In the first step, we d rop the last constraint from the above problem. In other words, we obtain the values of Pr( x in order linear approximation which provides a singleton dist ribution). In the second step, we project relaxation of the original problem. We solve the convex rela xation using a log-barrier method [1]. Briefly, this implies solving a series of unconstrained opti mization problems until we are within a user-specified tolerance value of  X  from the optimal solution. Specifically, We used = 1 . 5 in all our experiments, which was sufficient to obtain accura te solutions for the convex relaxation. At each iteration, the unconstraine d optimization problem is solved using Newton X  X  method. Recall that Newton X  X  method minimizes a fu nction g ( z ) by updating the current solution as where  X  2 g ( ) denotes the Hessian matrix and  X  g ( ) denotes the gradient vector. Note that the most expensive step in the above approach is the inversion of the H essian matrix. However, it is easy to verify that in our case all the off-diagonal elements of the H essian are equal to each other. By taking advantage of this special form of the Hessian, we compute its inverse in O ( m 2 ) time using Gaussian elimination (i.e. linear in the number of elements of the Hes sian).
 Once the values of Pr( x using the Chow-Liu algorithm [3]. Note that after the projec tion step we are no longer guaranteed to decrease the function  X ( y ) . This would imply that the overall algorithm would not be gua ranteed to converge. In order to overcome this problem, if we are unable to decrease  X ( y ) then we determine the sample x that is the sample best explained by the current mixture. We e nforce Pr( x the above convex relaxation again. Note that the solution to the new convex relaxation (i.e. the one with the newly introduced constraint for sample x previous convex relaxation using the following update: where s = P new convex relaxation. We then project the updated values of Pr( x process of eliminating one sample and projecting to a tree is repeated until we are able to reduce one that corresponds to the update scheme of [24]). In other w ords, we will add a singleton tree. However, in practice our algorithm converges in a small numb er (  X  m ) of iterations and provides an accurate mixture of trees. In fact, in all our experiments we never obtained any singleton trees. We conclude the description of our method by noting that once th e new tree distribution  X  T t is obtained, the value of  X  is easily updated as  X  = arg min We present a comparison of our method with the state of the art algorithms. We also use it to learn pictorial structures for face recognition. Note that our me thod is efficient in practice due to the special form of the Hessian matrix (for the log-barrier meth od) and the Chow-Liu algorithm [3, 21] (for the projection to tree distributions). In all our exper iments, each iteration takes only 5 to 10 minutes (and the number of iterations is equal to the number o f trees in the mixture). Comparison with Previous Work. As mentioned earlier, our approach can be used to obtain a good initialization for the EM algorithm of [21] since it minimizes  X  -divergence (providing comple-mentary information to the K L -divergence used in [21]). This is in contrast to the random i nitial-izations used in the experiments of [21] or the initializati on obtained by [26] (that also attempts to minimize the K L -divergence). We consider the task of using the mixture of tr ees as a classifier, that is given training data that consists of feature vectors x is to correctly classify previously unseen test feature vec tors. Following the protocol of [21], this can be achieved in two ways. For the first type of classifier, we append the feature vector x its class value c probability of x  X  probability. For the second type of classifier, we learn a mix ture of trees for each class value such that it predicts the probability of a feature vector belongi ng to that particular class. Once again, given a new feature vector x we assign it the class c which results in the probability. We tested our approach on the three discrete valued datasets used in [21]. In all our experiments, we initialized the mixture with a single tree obtained from t he Chow-Liu algorithm. We closely followed the experimental setup of [21] to ensure that the co mparisons are fair. Table 1 provides the accuracy of our approach together with the results reported in [21]. For  X  X plice X  the first classifier provides the best results, while  X  X garicus X  and  X  X ursery X  u se the second classifier. Note that our method provides similar accuracies to [21]. More important ly, it uses a smaller mixture of trees to achieve these results. Specifically, the method of [21] uses 12, 30 and 3 trees for the three datasets respectively. In contrast our method uses 3-5 trees for  X  X ga ricus X , 10-15 trees for  X  X ursery X  and 2 trees for Splice (where the number of trees in the mixture was obtained using a validation dataset, see [21] for details). Furthermore, unlike [21, 26], we obta in better accuracies by using a mixture small set of initial trees (with comparable size to our metho d). However, since the trees do not cover the entire observed distribution, their method provides le ss accurate results.
 Face Recognition. We tested our approach on the task of recognizing faces using the publicly available dataset 1 containing the faces of 11 characters in an episode of  X  X uffy the Vampire Slayer X . The total number of faces in the dataset is 24,244. For each fa ce we are provided with the location of 13 facial features (see Fig. 1). Furthermore, for each fac ial feature, we are also provided with a vector that represents the appearance of that facial featu re [5] (using the normalized grayscale values present in a circular region of radius 7 centered at the facial feature). As noted in previous work [5, 18] the task is challenging due to large intra-class variations in expression and lighting conditions.
 Given the appearance vector, the likelihood of each facial f eature belonging to a particular character can be found using logistic regression. However, the relati ve locations of the facial features also offer important cues in distinguishing one character from t he other (e.g. the width of the eyes or the distance between an eye and the nose). Typically, in vision s ystems, this information is not used. In other words, the so-called bag of visual words model is emp loyed. This is due to the somewhat counter-intuitive observation made by several researcher s that models that employ spatial prior on the features, e.g. pictorial structures [6], often provide worse recognition accuracies than those that throw away this information. However, this may be due to the f act that often the structure and parameters of pictorial structures and other related model s are set by hand. In order to test whether a spatial model can help improve reco gnition, we learned a mixture of trees for each of the characters. The random variables of the trees correspond to the facial features and nose. The unary potentials of each random variable is specifi ed using the appearance vectors (i.e. the likelihood obtained by logistic regression). In order t o obtain the pairwise potentials (i.e. the structure and parameters of the mixture of trees), the faces are normalized to remove global scaling of the episode to learn the mixture of trees. The faces found i n the remaining 20% of the episode were used as test data. Splitting the dataset in this manner ( i.e. a non-random split) ensures that we do not have any trivial cases where a face found in frame t is used for training and a (very similar) face found in frame t + 1 is used for testing.
 Fig. 1 shows the structure of the trees learned for 3 characte rs. The structures differ significantly Although the structure of the trees for a particular charact er are similar, they vary considerably in the parameters. This suggests that the distribution is in fa ct multimodal and therefore cannot be represented accurately using a single tree. Although visio n researchers have tried to overcome this learning algorithms. Table 2 shows the accuracy of the mixtu re of trees learned by the method of [26] and our approach. In this experiment, refining the mix ture of trees using the EM algorithm of [21] did not improve the results. This is due to the fact tha t the training and testing data differ significantly (due to non-random splits, unlike the previou s experiments which used random splits of the UCI datasets). In fact, when we split the face dataset randomly, we found that the EM algorithm did help. However, classification problems simulated using random splits of video frames are rare in real-world applications. Since [26] tries to minimize th e K L divergence, it mostly tries to explain the dominant mode of the observed distribution. This is evid ent in the fact that the accuracy of the row). In contrast, the minimization of  X  -divergence provides a diverse set of trees that attempt to explain the entire distribution thereby providing signific antly better results (table 2, second row). We formulated the problem of obtaining a small mixture of tre es by minimizing the  X  -divergence within the fractional covering framework. Our experiments indicate that the suitably modified frac-tional covering algorithm provides accurate models. We bel ieve that our approach offers a natural framework for addressing the problem of minimizing  X  -divergence and could prove useful for other classes of mixture models, for example mixtures of trees in l og-probability space for which there exist several efficient and accurate inference algorithms [ 16, 27]. There also appears to be a connec-tion between fractional covering (proposed in the theory co mmunity) and Discrete AdaBoost [7, 9] (proposed in the machine learning community) that merits fu rther exploration. [1] S. Boyd and L. Vandenberghe. Convex Optimization . Cambridge University Press, 2004. [2] P. Cheeseman and J. Stutz. Bayesian classification (Auto Class): Theory and results. In KDD , [3] C. Chow and C. Liu. Approximating discrete probability d istributions with dependence trees. [4] D. Crandall, P. Felzenszwalb, and D. Huttenlocher. Spat ial priors for parts-based recognition [5] M. Everingham, J. Sivic, and A. Zisserman. Hello! My name is... Buffy -Automatic naming [6] M. Fischler and R. Elschlager. The representation and ma tching of pictorial structures. TC , [7] Y. Freund and R. Schapire. A decision-theoretic general ization of on-line learning and an [9] J. Friedman, T. Hastie, and R. Tibshirani. Additive logi stic regression: A statistical view of [10] N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian net work classifiers. Machine Learning , [11] S. Ioffe and D. Forsyth. Human tracking with mixtures of trees. In ICCV , pages 690 X 695, [12] S. Ioffe and D. Forsyth. Mixtures of trees for object rec ognition. In CVPR , pages 180 X 185, [13] Y. Jing, V. Pavlovic, and J. Rehg. Boosted bayesian netw ork classifiers. Machine Learning , [14] S. Kirschner and P. Smyth. Infinite mixture of trees. In ICML , pages 417 X 423, 2007. [15] J. Kleinberg and E. Tardos. Approximation algorithms f or classification problems with pair-[16] V. Kolmogorov. Convergent tree-reweighted message pa ssing for energy minimization. PAMI , [17] M. P. Kumar and D. Koller. MAP estimation of semi-metric MRFs via hierarchical graph cuts. [18] M. P. Kumar, P. Torr, and A. Zisserman. An invariant larg e margin nearest neighbour classifier. [19] Y. Lin, S. Zhu, D. Lee, and B. Taskar. Learning sparse Mar kov network structure via ensemble-[20] M. Meila and T. Jaakkola. Tractable Bayesian learning o f tree belief networks. In UAI , 2000. [21] M. Meila and M. Jordan. Learning with a mixture of trees. JMLR , 1:1 X 48, 2000. [22] T. Minka. Divergence measures and message passing. Tec hnical report, Microsoft Research, [23] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference . [24] S. Plotkin, D. Shmoys, and E. Tardos. Fast approximatio n algorithms for fractional packing [25] A. Renyi. On measures of information and entropy. In Berkeley Symposium on Mathematics, [26] S. Rosset and E. Segal. Boosting density estimation. In NIPS , 2002. [27] M. Wainwright, T. Jaakkola, and A. Willsky. A new class o f upper bounds on the log partition
