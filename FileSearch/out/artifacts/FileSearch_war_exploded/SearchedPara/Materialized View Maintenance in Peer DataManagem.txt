 The problem of sharing data in peer data management systems (PDMSs) has re-ceived considerable attention in recent y ears. For example, the Piazza PDMS [1] proposes a solution of facilitating ad hoc, decentralized sharing and adminis-tration of data, and defining of semantic relationships between peers. In [2], the authors present a vision for a query and distributed active rule mechanisms which use mapping tables and mapping rules to coordinate data sharing. PeerDB [3] employs an information retrieval-based approach to query reformulation. The Chatty web [4] focuses on gossip protocols for exchanging semantic mapping information.

However, the management of updates in t hose systems has received very little attention. In [5], Gao et al propose a framework called CC-Buddy, for maintain-ing dynamic data coherency in peer-to-peer environment. In [6], Mork et al present a framework for managing updates in large-scale data sharing systems. Based on the above work in PDMSs, this paper proposes a strategy to maintain views in our schema-mapping based PDMS (SPDMS). The main contributions of this paper are as follows.  X  Based on applications, this paper extends the definition of view and proposes  X  If the schema mappings between peers a re changed (insert, delete, update), The paper is organized as follows. Sectio n 2 presents the logical model of our SPDMS. Section 3 discusses view mai ntenance in our SPDMS. We do exten-sive simulation and present the represent ative experimental results in Section 4. Section 5 concludes. Because of a global mediated schema, data integrated systems can only provide limited support for large-scaled data sharing [1]. In a PDMS, each peer is able to create its own schema base d on its query needs. These schemata are interrelated using a network of mappings. Each peer only needs to maintain a small number of mappings to closely related schemata so that, in the event of a change, only a minimal number of mappings need to be updated. So a PDMS can be viewed as a strict generalization of data integration systems.
 Definition 2.1. If a peer is a virtual peer that holds the cs hema mappings of peers in a group or provides a uniform view over a set of peers, the peer is called mapping peer (MP).
 Definition 2.2. We call a mapping peer P and all peers in the same group a local PDMS. The local peer set L o PS ( P ) is made up of peers that lie in the same local PDMS as peer P, that is, L o PS ( P )= { P i | P i and P in the same group } . Definition 2.3. Let peer P be a MP. The association peer set AP S ( P )ismade up of peers, each of which does not belong to L o PS ( P ) but has schema mappings with at least one peer in L o PS ( P ). So AP S ( P )= { P i | P i  X  L o PS ( P )  X  X  X  P j  X  L
PS ( P )  X  P i  X  P j } . P i  X  P j means there are schema mappings between them. In this paper, the schema mappi ngs between peers are symmetric. Definition 2.4. The peer association peer set PAPS ( P ) is made up of peers, which have schema mappings with peer P. So PAPS ( P )= { P i | P i  X  P } .
We illustrate how to construct our SPDMS as shown in Fig. 1. Arrows indicate that there are mappings between the rel ations of the peer schemas. If Stanford and Berkeley, as neighboring universities, come to an agreement to map their schemas, there are three things to do. First, Stanford registers the schema map-pings from it to Berkeley at Mapping Peer ( P 2 ) and Berkeley registers the schema mappings from it to Stanford at P 2 . Second, P 2 puts Stanford and Berkeley into L
PS ( P 2 ). Finally, Stanford puts Berkeley into PAPS ( Stanford )andBerke-ley puts Stanford into PAPS ( Berkeley ) respectively. Then Stanford-Berkeley PDMS is established.
 DB-Projects ( P 1 ) is also a mapping peer that provides a uniform view over UPenn and UW. If a mapping between UW and Stanford is established, there are three things to do. First, UW registers the schema mappings from it to Stanford at P 1 and Stanford registers the schema mappings from it to UW at P . Second, P 2 puts UW into AP S ( P 2 )and P 1 puts Stanford into AP S ( P 1 ) respectively. Finally, UW puts Stanford into PAPS ( UW ) and Stanford puts UW into PAPS ( Stanford ) respectively. 3.1 The Definitions of Materialized Views Let P 1 V m  X  X  X  P 2 denote that a materialized view ( V m ) can be transitive from P 1 to P 2 . We have a transitivity rule of V m s as follows.
 Transitivity rule. If P 1 V m  X  X  X  P 2 holds and P 2 V m  X  X  X  P 3 holds, then P 1 V m  X  X  X  P 3 . Definition 3.1. We call the set of peers that a V m posed over peer P can reformulate over under a set of schema mappings the transitive closure of the V m ;wedenoteitby P + V m . The peers in P + V m form the semantic path of the V m and P is the initiator of the semantic path. The union of a V m and its reformulations over all other peers in P + V m is called the semantics of the V m . Theorem 1. Let peer A and peer B ( A = B ) be two peers in a PDMS. If Proof. Because A V m  X  X  X  B and schema mappings between peers are symmetric, we have B V m  X  X  X  A . For any peer C  X  A + V m ,weget A V m  X  X  X  C from definition 3.1. From transitivity rule, we obtain B V m  X  X  X  C .Sowehave C  X  B + V m .Thusweget A V m  X  B + V m . With the similar method, we can prove B + V m  X  A + V m .Soweobtain A V m = B + V m . Thus the theorem follows.
 Definition 3.2. If a view can only retrieve data from one data source, we call it peer view. If a view can retrieve dat a from more than one data source in its local PDMS, we call it local view. If a view can retrieve data from more than one local PDMS or retrieve data from more than one data source in another local PDMS,wecallitglobalview.

If those views materialize, they ar e called peer materialized view ( P MV ), local materialized view ( L MV ), and global materialized view ( G MV ) respectively. And we call the part of a G MV in a local PDMS its local instance ( G L MV ). 3.2 The Maintenance Strateg y of Definition Consistency A PDMS can be made up of many local PDMSs. For example, the PDMS in Fig. 1 is made up of Stanford-Berkeley PDMS and UPenn-UW-DBProjects PDMS. Because join operations are confined in each local PDMS in our SPDMS, a G
MV is the union of all related G L MV s, that is, G MV = maintenance of a G MV is became the maintenance of all related G L MV s. In our SPDMS, P MV s can accommodate in any peer. H owever, the definitions of G L MV sand L MV s are accommodated in their mapping peer and their data are accommodated in capable peers, which are called propagation peer, of the same local PDMS. And the mapping peer maintains the following relationships:  X  The relationship between the definition of every G L MV and its related data.  X  The relationship between the definition of every G L MV and its related data
In each MP, there is an ECA rule to actively adjust V m s with the changes of the schema mappings between peers. The E CA rule will be triggered as follows.  X  (1) If the schema mappings between p eer A and peer B are registered into  X  (2) If the schema mappings between peer A and peer B are unregistered  X  (3) If the schema mappings between peer A and peer B are updated, the 3.3 The Maintenance Strat egy of Data Consistency In our SPDMS, different version s of relations are denoted by version numbers . We use R t to denote the t -th version of the relation R .Weuse version vectors to specify the versions of a view. The version vector of a view V contains a version number of each base relation on which V depends. And an updategram [6] contains the list of changes (inserts, deletions, and updates) necessary to advance a relation from one version num ber to a latter version number:  X  i,j R contains the changes that must be applied to advance R i to R j . Similarly,  X  to V Definition 3.3. Let V be an SPJ view definition whose FROM clause is R 1 , ..., R n ,let D be a database, and let  X  R 1 be an updategram for R 1 .Theboosterof R 2 w.r.t.  X  R 1 and V is the subset of tuples of R 2 in D that are relevant to some tuple mentioned in  X  R 1 . We denote the booster by  X  V (  X  R 1 ,R 2 ); when V and  X  1 are obvious, we abbreviate  X  ( R 2 ).

In this paper, we focus on the maintenance of G MV and L MV .Wecalla updategram and its corresponding boosters together  X  -relation. If a V m relates to any relation in a peer, we call the p eer viewing peer. If a peer accommodates update data temporarily, we call the peer temp peer. Our maintenance strategy of the G MV is as follows. If any relation is modified and there is a V m related to it in a local PDMS, the viewing peer sends updates to the propagation peer. If a V m can be self-maintainable [7], the viewing peer only sends the updategram to the propagation peer. Otherwise, the viewing peer sends  X  -relation to the propagation peer [6]; If the propagation peer is offline, the viewing peer sends updates to the temp peer. Once the propagation peer becomes online, it accepts data from the temp peer. According to the logical model of our S PDMS, we develop a simulation sys-tem, which is implemented in Java. Based on it, we do extensive simulation ex-periments comparing the mainten ance efficiency of a centralized G MV (Mork X  X  strategy) with that of its co rresponding decentralized G MV (our method). In our experiments, there are 60 simulation nodes, which are classified into 5 local PDMSs. So G MV = 5 i =1 G L MV i . There are no join operations between peers in our experiments. The view has 12 attributions and 2400K tuples. The simu-lation system was tested on a Windows Server 2000 Pentium 4 PC running at 1.7 GHz with 512M of memory.

In the experiments, our strategy adopts even distribution, that is, the data of a G MV is evenly distributed among all related G L MV s. The simulation results are shown in Fig. 2. The legend Gmv denotes the maintenance time of the G MV . And the legend GLmv denotes the maintenance time of each G L MV .Fromthe figure, we see Gmv is much higher than GLmv. This is because each G L MV has only one fifth population as large as that of the G MV and G L MV has only one fifth update operations as many as that of the G MV . When the system maintains each G L MV , it needs fewer I/O operations b ecause the data are almost all in memory. So the maintenance time is very shorter. This paper researches on view maintenance in PDMSs. Based on applications, this paper extends the definition of view and proposes the peer, local and global views. If a peer updates its data, our SPDMS adopts a push-based algorithm to maintain views. If the schema mapping s between peers are changed, our SPDMS adopts an ECA rule for active definition consistency maintenance.

There are two directions we plan to pursue next. First, we will study how to store a L MV or G L MV in different local PDMSs from that of their related data sources. Second, we will study how to adopt ontology in our SPDMS.
 Acknowledgements. This work is supported by National Natural Science Foundation of China under Gra nt No. 60503038, 60473069 and 60496325. The authors wish to thank the anonymous reviewers for their useful comments.
