 Mingyuan Zhou mz1@ee.duke.edu Lingbo Li ll83@duke.edu David Dunson dunson@stat.duke.edu Lawrence Carin lcarin@ee.duke.edu Duke University, Durham NC 27708, USA In numerous scientific studies, the response variable is a count y = 0 , 1 , 2 ,  X  X  X  , which we wish to ex-plain with a set of covariates x = [1 ,x 1 ,  X  X  X  ,x P ] T E [ y | x ] = g  X  1 ( x T  X  ), where  X  = [  X  0 ,  X  X  X  , X  P ] regression coefficients and g is the canonical link func-tion in generalized linear models (GLMs) (McCullagh &amp; Nelder, 1989; Long, 1997; Cameron &amp; Trivedi, 1998; Agresti, 2002; Winkelmann, 2008). Regression models for counts are usually nonlinear and have to take into consideration the specific properties of counts, includ-ing discreteness and nonnegativity, and often charac-terized by overdispersion (variance greater than the mean). In addition, we may wish to impose a sparse prior in the regression coefficients for counts, which is demonstrated to be beneficial for regression analysis of both Gaussian and binary data (Tipping, 2001). Count data are commonly modeled with the Poisson distribution y  X  Pois(  X  ), whose mean and variance are both equal to  X  . Due to heterogeneity (difference between individuals) and contagion (dependence be-tween the occurrence of events), the varance is often much larger than the mean, making the Poisson as-sumption restrictive. By placing a gamma distribution prior with shape r and scale p/ (1  X  p ) on  X  , a negative binomial (NB) distribution y  X  NB( r,p ) can be gener-ated as f Y ( y ) = R  X  0 Pois( y ;  X  )Gamma  X  ; r, p 1  X  p tion, r is the nonnegative dispersion parameter and p is a probability parameter. Therefore, the NB distribu-tion is also known as the gamma-Poisson distribution. It has a variance rp/ (1  X  p ) 2 larger than the mean rp/ (1  X  p ), and thus it is usually favored over the Pois-son distribution for modeling overdispersed counts. The regression analysis of counts is commonly per-formed under the Poisson or NB likelihoods, whose parameters are usually estimated by finding the max-imum of the nonlinear log likelihood (Long, 1997; Cameron &amp; Trivedi, 1998; Agresti, 2002; Winkelmann, 2008). The maximum likelihood estimator (MLE), however, only provides a point estimate and does not allow the incorporation of prior information, such as sparsity in the regression coefficients. In addition, the MLE of the NB dispersion parameter r often lacks ro-bustness and may be severely biased or even fail to converge if the sample size is small, the mean is small or if r is large (Saha &amp; Paul, 2005; Lloyd-Smith, 2007). Compared to the MLE, Bayesian approaches are able to model the uncertainty of estimation and to incor-porate prior information. In regression analysis of counts, however, the lack of simple and efficient algo-rithms for posterior computation has seriously limited routine applications of Bayesian approaches, making Bayesian analysis of counts appear unattractive and thus underdeveloped. For instance, for the NB dis-persion parameter r , the only available closed-form Bayesian solution relies on approximating the ratio of two gamma functions using a polynomial expan-sion (Bradlow et al., 2002); and for the regression co-efficients  X  , Bayesian solutions usually involve com-putationally intensive Metropolis-Hastings algorithms, since the conjugate prior for  X  is not known under the Poisson and NB likelihoods (Chib et al., 1998; Chib &amp; Winkelmann, 2001; Winkelmann, 2008).
 In this paper we propose a lognormal and gamma mixed NB regression model for counts, with default Bayesian analysis presented based on two novel data augmentation approaches. Specifically, we show that the gamma distribution is the conjugate prior to the NB dispersion parameter r , under the compound Pois-son representation, with efficient Gibbs sampling and variational Bayes (VB) inference derived by exploiting conditional conjugacy. Further we show that a log-normal prior can be connected to the logit of the NB probability parameter p , with efficient Gibbs sampling and VB inference developed for the regression coeffi-cients  X  and the lognormal variance parameter  X  2 , by generalizing a Polya-Gamma distribution based data augmentation approach in Polson &amp; Scott (2011). The proposed Bayesian inference can be implemented rou-tinely, while being easily generalizable to more com-plex settings involving multivariate dependence struc-tures. We illustrate the algorithm with real examples on univariate count analysis and count regression, and demonstrate the advantages of the proposed Bayesian approaches over conventional count models. The most basic regression model for counts is the Pois-son regression model (Long, 1997; Cameron &amp; Trivedi, 1998; Winkelmann, 2008), which can be expressed as where x i = [1 ,x i 1 ,  X  X  X  ,x iP ] T is the covariate vector for sample i . The Newton-Raphson method can be used to iteratively find the MLE of  X  (Long, 1997). A seri-ous constraint of the Poisson regression model is that it assumes equal-dispersion, i.e. , E [ y i | x i ] = Var[ y exp( x T i  X  ) . In practice, however, count data are of-ten overdispersed, due to heterogeneity and contagion (Winkelmann, 2008). To model overdispersed counts, the Poisson regression model can be modified as where i is a nonnegative multiplicative random-effect term to model individual heterogeneity (Winkelmann, 2008). Using both the law of total expectation and the law of total variance, it can be shown that Thus Var[ y i | x i ]  X  E [ y i | x i ] and we obtain a regression model for overdispersed counts. We show below that both the gamma and lognormal distributions can be used as the nonnegative prior on i . 2.1. The Negative Binomial Regression Model The NB regression model (Long, 1997; Cameron &amp; Trivedi, 1998; Winkelmann, 2008; Hilbe, 2007) is con-structed by placing a gamma prior on i as where E [ i ] = 1 and Var[ i ] = r  X  1 . Marginalizing out i in (2), we have a NB distribution parame-terized by mean  X  i = exp( x T i  X  ) and inverse disper-sion parameter  X  (the reciprocal of r ) as f Y ( y i ) = The MLEs of  X  and  X  can be found numerically with the Newton-Raphson method (Lawless, 1987). 2.2. The Lognormal-Poisson Regression Model A lognormal-Poisson regression model (Breslow, 1984; Long, 1997; Agresti, 2002; Winkelmann, 2008) can be constructed by placing a lognormal prior on i as where E [ i ] = e  X  2 / 2 and Var[ i ] = e  X  2 e  X  2  X  1 . Us-ing (3) and (4), we have Compared to the NB model, there is no analytical form for the distribution of y i if i is marginalized out and the MLE is less straightforward to calculate, making it less commonly used. However, Winkelmann (2008) suggests to reevaluate the lognormal-Poisson model, since it is appealing in theory and may fit the data better. The inverse Gaussian distribution prior can also be placed on i to construct a heavier-tailed al-ternative to the NB model (Dean et al., 1989), whose density functions are shown to be virtually identical to the lognormal-Poisson model (Winkelmann, 2008). To explicitly model the uncertainty of estimation and incorporate prior information, Bayesian approaches appear attractive. Bayesian analysis of counts, how-ever, is seriously limited by the lack of efficient infer-ence, as the conjugate prior for the regression coeffi-cients  X  is unknown under the Poisson and NB like-lihoods (Winkelmann, 2008), and the conjugate prior for the NB dispersion parameter r is also unknown. To address these issues, we propose a lognormal and gamma mixed NB regression model for counts, termed here the LGNB model, where a lognormal prior ln N (0 , X  2 ) is placed on the multiplicative random ef-fect term i and a gamma prior is placed on r . Denot-the LGNB model is constructed as where  X  =  X   X  2 and a 0 , b 0 , c 0 , d 0 , e 0 , f 0 and g are gamma hyperparameters (they are set as 0.01 in experiments). Since y i  X  NB ( r,p i ) in (11) can be augmented into a gamma-Poisson structure as y i  X  Pois(  X  i ) ,  X  i  X  Gamma r, exp( x T i  X  ) i , the LGNB model can also be considered as a lognormal-gamma-gamma-Poisson regression model. Denoting X = [ x T 1 ,  X  X  X  , x T N ] T , we may equivalently express  X  = [  X  1 ,  X  X  X  , X  N ] T in the above model as If we marginalize out h in (14), we obtain a beta prime distribution prior r  X   X  0 ( a 0 ,b 0 , 1 ,g 0 ). If we marginal-ize out  X  p in (13), we obtain a Student-t prior for  X  p , the sparsity-promoting prior used in Tipping (2001); Bishop &amp; Tipping (2000) for regression analysis of both Gaussian and binary data. Note that  X  is con-nected to p i with a logit link, which is key to deriving efficient Bayesian inference. 3.1. Model Properties and Model Comparison Using the laws of total expectation and total variance and the moments of the NB distribution, we have We define the quasi-dispersion  X  as the coefficient as-sociated with the mean quadratic term in the variance. As shown in (7) and (10),  X  =  X  in the NB model and  X  = e  X  2  X  1 in the lognormal-Poisson model. Ap-parently, they have different distribution assumptions on dispersion, yet there is no clear evidence to favor one over the other in terms of goodness of fit. In the proposed LGNB model, there are two free parame-ters r and  X  2 to adjust both the mean in (16) and dispersion  X  = e  X  2 (1+ r  X  1 )  X  1 , which become the same as those of the NB model when  X  2 = 0, and the same as those of the lognormal-Poisson model when  X  = r  X  1 = 0. Thus the LGNB model has one extra de-gree of freedom to incorporate both kinds of random effects, with their proportion automatically inferred. As discussed in Section 3, the LGNB model has an ad-vantage of having two free parameters to incorporate both kinds of random effects. We show below that it has an additional advantage in that default Bayesian analysis can be performed with two novel data aug-mentation approaches, with closed-form solutions and analytical update equations available for both Gibbs sampling and VB inference. One augmentation ap-proach concerns the inference of the NB dispersion pa-rameter r using the compound Poisson representation, and the other concerns the inference of the regression coefficients  X  using the Polya-Gamma distribution. 4.1. Inferring the Dispersion Parameter Under We first focus on inference of the NB dispersion pa-rameter r and assume we know { p i } i =1 ,N and h , ne-glecting the remaining part of the LGNB model at this moment. We comment here that the novel Bayesian inference developed here can be applied to any other scenarios where the conditional posterior of r is pro-which a hybrid Monte Carlo and a Metropolis-Hastings algorithms had been developed in Williamson et al. (2010) and Zhou et al. (2012), but VB solutions were not yet developed.
 As proved in Quenouille (1949), y  X  NB( r,p ) can also be generated from a compound Poisson distribution as where Log( p ) corresponds to the logarithmic distri-bution (Barndorff-Nielsen et al., 2010) with f U ( k ) = generating function (PGF) is Using the conjugacy between the gamma and Poisson distributions, it is evident that the gamma distribution is the conjugate prior for r under this augmentation. 4.1.1. Gibbs Sampling for r Recalling (18), y i  X  NB( r,p i ) can also be generated from the random sum y i = P L i ` =1 u i` with Exploiting conjugacy between (14) and (20), given L i , we have the conditional posterior of r as where here and below expressions like ( r | X  ) corre-spond to random variable (RV) r , conditioned on all other RVs. The remaining challenge is finding the conditional posterior of L i . Denote w ij = P j ` =1 j = 1 ,  X  X  X  ,y i . Since w ij is the summation of j iid Log( p distributed RVs, using (19), the PGF of w ij is Therefore, we have L i  X  0 if y i = 0 and for 1  X  j  X  y i where f i ( z ) =  X  ln(1  X  p i z ) and F is a lower triangular matrix with F (1 , 1) = 1, F ( m,j ) = 0 if j &gt; m , and if 1  X  j  X  m . Using (22), we have where R r (0 , 0) = 1 and The values of F can be iteratively calculated and each row sums to one, e.g., the 4th and 5th rows of F are Note that to obtain (22), we use the relationship proved in Lemma 1 of the supplementary material that 1 m ! Gibbs sampling for r proceeds by alternately sampling (24) and (21). Note that to ensure numerical stability when r &gt; 1, instead of using (25), we may iteratively calculate R r in the way we calculate F in (23). We show in Figure 1 of the supplementary material the matrices R r for r = . 1, 1, 10 and 100. 4.1.2. Variational Bayes Inference for r Using VB inference (Bishop &amp; Tipping, 2000; Beal, 2003), we approximate the posterior p ( r, L | X ) with Q ( r, L ) = Q r ( r ) Q N i =1 Q L i ( L i ), and we have function, and Equations (29)-(30) constitute the VB inference for the NB dispersion parameter r , with  X  r  X  =  X  a/  X  h . 4.2. Inferring the Regression Coefficients Denote  X  i as a random variable drawn from the Polya-Gamma (PG) distribution (Polson &amp; Scott, 2011) as Thus the likelihood of  X  i in (11) can be expressed as Given the values of {  X  i } i =1 ,N and the prior in (15), the conditional posterior of  X  can be expressed as and given the values of  X  and the prior in (31), the conditional posterior of  X  i can be expressed as 4.3. Gibbs Sampling Inference Exploiting conditional conjugacy and the exponential tilting of the PG distribution in Polson &amp; Scott (2011), we can sample in closed-form all latent parameters of the LGNB model described from (11) to (14) as a PG distributed random variable can be generated from an infinite sum of weighted iid gamma random variables (Devroye, 2009; Polson &amp; Scott, 2011). We provide in the supplementary material a method for accurately truncating the infinite sum. 4.4. Variational Bayes Inference Using VB inference (Bishop &amp; Tipping, 2000; Beal, 2003), we approximate the posterior distribution tr[  X  ] is the trace of  X  ,  X  ln r  X  and  X  L i  X  are calculated analytical forms for  X  i 1 and  X  i 2 in Q  X  i (  X  i ), we can use (36) to calculate  X   X  i  X  as where the mean property of the PG distribution 1 (Pol-son &amp; Scott, 2011) is applied. To calculate  X  ln(1+ e Carlo integration algorithm (Andrieu et al., 2003). 5.1. Univariate Count Data Analysis The inference of the NB dispersion parameter r by it-self plays an important role not only for the NB regres-sion (Lawless, 1987; Winkelmann, 2008) but also for univariate count data analysis (Bliss &amp; Fisher, 1953; Clark &amp; Perry, 1989; Saha &amp; Paul, 2005; Lloyd-Smith, 2007), and it also arises in some recently proposed latent variable models for count matrix factorization (Williamson et al., 2010; Zhou et al., 2012). Thus it is of interest to evaluate the proposed closed-form Gibbs sampling and VB inference for this parameter alone, before introducing the regression analysis part. We consider a real dataset describing counts of red mites on apple leaves, given in Table 1 of Bliss &amp; Fisher (1953). There were in total 172 adult female mites found in 150 randomly selected leaves, with a 0 count on 70 leaves, 1 on 38, 2 on 17, 3 on 10, 4 on 9, 5 on 3, 6 on 2 and 7 on 1. This dataset has a mean of 1.1467 and a variance of 2.2736, clearly overdispersed. We assume the counts are NB distributed and we intend to infer r with a hierarchical model as y where i = 1 ,  X  X  X  ,N and we set a = b =  X  =  X  = 0 . 01. We consider 20,000 Gibbs sampling iterations, with the first 10,000 samples discarded and every fifth sam-ple collected afterwards. As shown in Figure 1, the autocorrelation of Gibbs samples decreases quickly as the lag increases, and the VB lower bound converges quickly even starting from a bad initialization ( r is initialized two times the converged value).
 The estimated posterior mean of r is 1.0812 with Gibbs sampling and 0.9988 with VB. Compared to the method of moments estimator (MME), MLE, and maximum quasi-likelihood estimator (MQLE) (Clark &amp; Perry, 1989), which provides point estimates of 1.1667, 1.0246 and 0.9947 2 , respectively, our algorithm is able to provide a full posterior distribution of r and is convenient to incorporate prior information. The calculating details of the MME, MLE and MQLE, the closed-form Gibbs sampling and VB update equations, and the VB lower bound are all provided in the sup-plementary material, omitted here for brevity. 5.2. Regression Analysis of Counts We test the full LGNB model on two real exam-ples, with comparison to the Poisson, NB, lognormal-Poisson and inverse-Gaussian-Poisson (IG-Poisson) re-gression models. The NASCAR dataset 3 , analyzed in Winner, consists of 151 NASCAR races during the 1975-1979 Seasons. The response variable is the num-ber of lead changes in a race, and the covariates of a race include the number of laps, number of drivers and length of the track (in miles). The MotorIns dataset 4 , analyzed in Dean et al. (1989), consists of Swedish third-party motor insurance claims in 1977. Included in the data are the total number of claims for automo-biles insured in each of the 315 risk groups, defined by a combination of DISTANCE, BONUS, and MAKE factor levels. The number of insured automobile-years for each group is also given. As in Dean et al. (1989), a 19 dimensional covariate vector is constructed for each group to represent levels of the factors. To test goodness-of-fit, we use the Pearson residuals, a met-ric widely used in GLMs (McCullagh &amp; Nelder, 1989), calculated as where  X   X  and  X   X  are the estimated mean and quasi-dispersion, respectively, whose calculations are de-scribed in detail in the supplementary material. The MLEs for the Poisson and NB models are well-known and the update equations can be found in Win-ner; Winkelmann (2008). The MLE results for the IG-Poisson model on the MotorIns data were reported in Dean et al. (1989). For the lognormal-Poisson model, no standard MLE algorithms are available and we choose Metropolis-Hastings (M-H) algorithms for pa-rameter estimation. We also consider a LGNB model under the special setting that r = 1000. As discussed in Section 3.1, this would lead to a model which is approximately the lognormal-Poisson model, yet with closed-form Gibbs sampling inference. We use both VB and Gibbs sampling for the LGNB model. We con-sider 20,000 Gibbs sampling iterations, with the first 10,000 samples discarded and every fifth sample col-lected afterwards. As described in the supplementary material, we sample from the PG distribution with a truncation level of 2000. We initialize r as 100 and other parameters at random. Examining the samples in Gibbs sampling, we find that the autocorrelations of model parameters generally reduce to below 0.2 at the lag of 20, indicating fast mixing.
 Shown in Table 1 are the MLEs or posterior means of key model parameters. Note that  X  0 of the LGNB model differs considerably from that of the Poisson and NB models, which is expected since  X  0 +  X  2 / 2 + ln r in the LGNB model plays about the same role as  X  0 in the Poisson and NB models, as indicated in (16). As shown in Tables 2, in terms of goodness of fit mea-sured by Pearson residuals, the Poisson model per-forms the worst due to its unrealistic equal-dispersion assumption; the NB model, assuming a gamma dis-tributed multiplicative random effect term, signifi-cantly improves the performances compared to the Poisson model; the proposed LGNB model, model-ing extra-Poisson variations with both the gamma and lognormal distributions, clearly outperforms both the Poisson and NB models. Since for the lognormal-Poisson model with the M-H algorithm, we were not able to obtain comparable results even after carefully tuning the proposal distribution, we did not include it here for comparison. However, since the LGNB model reduces to the lognormal-Poisson model as r  X   X  , the results of the LGNB model with r  X  1000 would be able to indicate whether the lognormal distribution alone is appropriate to model the extra-Poisson varia-tions. Despite the popularity of the NB model, which models extra-Poisson variations only with the gamma distribution, the results in Tables 2 suggest the ben-efits of incorporating the lognormal random effects. These observations also support the claim in (Winkel-mann, 2008) that the lognormal-Poisson model should be reevaluated since it is appealing in theory and may fit the data better. Compared to the lognormal-Poisson model, the LGNB model has an additional ad-vantage that its parameters can be estimated with VB inference, which is usually much faster than sampling based methods.
 A clear advantage of the Bayesian inference over the MLE is that a full posterior distribution can be ob-tained, by utilizing the estimated posteriors of  X  2 , r and  X  . For example, shown in Figure 2 are the esti-mated posterior distributions of the quasi-dispersion  X  , represented with histograms. These histograms should be compared to  X  = 0 in the Poisson model, and the NB model X  X  MLEs of  X  = 0 . 1905 and  X  = 0 . 0118, for the NASCAR and MotorIns datasets, respectively. We can also find that VB generally tends to overemphasize the regions around the mode of its estimated poste-rior distribution and consequently places low densities on the tails, whereas Gibbs sampling is able to ex-plore a wider region. This is intuitive since VB relies on the assumption that the posterior distribution can be approximated with the product of independent Q functions, whereas Gibbs sampling only exploits con-ditional independence.
 The estimated posteriors can also assist model inter-pretation. For example, based on  X   X   X  in VB for the NASCAR dataset, we can calculate the correlation matrix for (  X  1 , X  2 , X  3 ) T as which is typically not provided in MLE. Since  X  1 (Laps) and  X  3 (TrkLen) are highly positively corre-lated, we expect the corresponding covariates to be highly negatively correlated. This is confirmed, as the correlation coefficient between the number of laps and the track length is found to be as small as  X  0 . 9006. A lognormal and gamma mixed negative binomial (LGNB) regression model is proposed for regression analysis of overdispersed counts. Efficient closed-form Gibbs sampling and VB inference are both presented, by exploiting the compound Poisson representation and a Polya-Gamma distribution based data augmen-tation approach. Model properties are examined, with comparison to the Poisson, NB and lognormal-Poisson models. As the univariate lognormal-Poisson regres-sion model can be easily generalized to regression analysis of correlated counts, in which the derivatives and Hessian matrixes of parameters are used to con-struct multivariate normal proposals in a Metropolis-Hastings algorithm (Chib et al., 1998; Chib &amp; Winkel-mann, 2001; Ma et al., 2008; Winkelmann, 2008), the proposed LGNB model can be conveniently modified for multivariate count regression, in which we may be able to derive closed-form Gibbs sampling and VB in-ference. As the log Gaussian process can be used to model the intensity of the Poisson process, whose in-ference remains a major challenge (M X ller et al., 1998; Adams et al., 2009; Murray et al., 2010; Rao &amp; Teh, 2011), we may link the log Gaussian process to the logit of the NB probability parameter, leading to a log Gaussian NB process with tractable closed-form Bayesian inference. Furthermore, the NB distribu-tion is shown to be important for the factorization of a term-document count matrix (Williamson et al., 2010; Zhou et al., 2012), and the multinomial logit has been used to model correlated topics in topic model-ing (Blei &amp; Lafferty, 2005; Paisley et al., 2011). Ap-plying the proposed lognormal-gamma-NB framework and the developed closed-form Bayesian inference to these diverse problems is currently under active inves-tigation.
 The research reported here has been supported in part by DARPA under the MSEE program.

