 User-created documents are well known t ypes of user-generated contents, which are produced by end-users. For example, user product reviews in shopping sites or answers in community driven Q&amp;A are t wo common types of user-created doc-uments. This has motivated us to investigate on proposing a quality evaluating model that can be applied to any common types of user-created documents.
Using a supervised classification approach, we first manually labeled experi-mental documents conforming to three levels of document quality, namely good , fair and bad . A classifier trained from annotated corpus then ranked documents according to their prediction scores. I n this work, we concentrate on building a feature combination which does not depend on the type of target documents. Our proposed method empirically worked well, even though documents have been collected from independent sources. Recently, [1] studied a task similar to our work, which is specific to user-created answers. Only non-textual features, such as click-through counts and user rec-ommendation counts , were used for predicting answer X  X  quality. However, it has turned out that the most effective feature is document length (which does not refer to non-textual information), whereas the others are less contributed. This conclusion infers that non-textual information considered previously may not al-ways be stable along time; intuitively, data sparseness may often occur for newly created documents because the y would be seen less by users.
 [3] investigated the task of predicting r eviews X  helpfulness that considered users X  vote as ground-truth evaluation. Firstly, different classes of features are utilized to helpfulness. SVM regression then learned helpfulness function and ranked reviews according to their output scores. In this work, the length of a review , product rating and its unigrams were found to be most useful. However, assessing reviews X  helpfulness based on users X  rating ground-truth is not always reliable due to several voting biases [4].

Showing three biases of [3] X  X  approach, [4] presented a framework for detecting low-quality reviews. Instead of using users X  vote information, the authors manu-ally annotated a set of ground-truth according to manually predefined specifica-tion for reviews X  quality. However, many se lected features are directly extracted from product X  X  attributes such as the number of products, product features, brand names . Such features made this approach domain restricted since they are hardly applicable to other types of user-created documents.

Limitations from prior works have suggested us to employ both textual and non-textual features in the proposed method. To widely exploit this work for almost any types of user-created documents, only general features are chosen regarding intrinsic properties of documents. Our proposed model empirically improved performance in comparison with baseline approach that utilizes only non-textual features. 3.1 Features Categories One of the enhancements in our approac h is the combination of objectively measurements selected upon predefined c lasses. All experimented features are separated into four categories: authority , formality , readability and subjectivity . Features on authority Among four categories, authority is a unique category that relies on non-textual information collected by service providers. Features on authority indicate whether document is written by a trustworthy author or not. Some represen-tative examples of features in this category are as follow:  X  Number of documents previously written by the same writer (NDOC)  X  Number of votes or scores granted by users (NVOT) Features on formality This feature category refers to the writing style of target document. A formal document tends to be accessible to the intended audience. Based on this obser-vation, some of consecutive features are considered:  X  Number of words in the document (NWRD)  X  Number of different words in the document (DWRD)  X  Number of sentences in the document (NSNT)  X  The fourth root of the number of words in the document (RWRD =  X  Average length of sentences in the document (SLEN) Features on readability Typically, a well-organized document imparts much information to reader. With assumption that format of document contributes to its quality prediction; three described features have been chosen for experiments:  X  Lexical density of the document (LXDN = DWRD/NWRD)  X  Number of paragraphs in the document (NPRG)  X  Average length of paragraphs in the document (PLEN) Features on subjectivity Subjectivity refers to opinions of authors in a document. Several following fea-tures have been defined based on simple and easy-measurable criteria:  X  Ratio of positive sentences (RPST)  X  Ratio of negative sentences (RNST)  X  Ratio of subjective sentences regardless of positive or negative (RSST)  X  Ratio of comparative sentences (RCST)
Basically, most of features in formality and readability category are similar to the ones used in Project Essay Grade [6]. Subjectivity category consists of opinion-based features. Using subjective and comparative languages clues [2,8], we refined a set of opinion words and phrases for each testing corpus. Subjectivity features have been extracted by using a simple keyword-based approach. For example, positive sentences are considere d as sentences that contain at least one positive opinion word or phrase. 3.2 Quality Evaluation Model In our proposed model, Maximum Entropy (MaxEnt) is chosen for training a classifier. The main advantage of MaxEnt is that we can easily integrate variety of relevant features since they are expressed in the form of feature functions. For later improving a retrieval system, we intend to build a statistical model of which output scores can be considered as prior information.

The underlying idea of MaxEnt indicates that without external knowledge, one should prefer the most uniform models that also satisfy any given constraints. Once we assume that assessing quality o f documents is a random process that observes documents and assign them a quality label y , MaxEnt motivates to find the model p as close to the empirical probability distribution p X  of random pro-cess as possible. Applying to our classification task, each feature is represented by a feature function f i ( x, y )= x fi where x fi is the value of the i th feature in the document x . MaxEnt then estimate expected value for each feature from training data and take this as constraint of the model distribution.
Firstly, a set of weighting parameters  X  for each feature function are estimated by using Limited-Memory Variable method [5]. The model then computes the conditional probability for predicting the quality of document x by the formula: where p(y | x) is the output score indicates the quality of document x and Z(x) is a normalization factor to ensure y p(y | x) = 1. We specifically use p(y=good | x) as a score output. 4.1 Experimental Corpus We experimented our model on two datas ets of real world user-created doc-uments. The first one consists of 1000 English reviews on Amazon website ( http://amazon.com ). Twenty products in electro nics category were randomly selected for constructin g corpus. For each product, we manually accumulated 50 reviews regardless of order. Other re levant information of reviews such as author X  X  rank, users X  vote, comments are also saved. Two students were asked for hand-tagging each given document as good , fair or bad (Table 1 shows de-scriptions of three-level quality on each dataset).
 The second dataset includes 2589 Korean Q&amp;A samples collected from Naver X  X  Knowledge Search service ( http://kin.naver.com ). Basically, this corpus has already built and experimented in previous research [7]. The dataset is composed of questions, along with one best answer for each question. (Knowledge Search service allows users to select one best answer among all answers corresponding to a question). In this scenario, we used only answers for the experiments. Also, all answers were manually labeled based on three-level quality.
 4.2 Results We ranked the documents in descending order of the score output and used the traditional recall and precision metric to evaluate the results. Conforming to the evaluation metric, we consider good and fair documents as relevant documents while poor documents are treated as non-relevant ones. Aimed to measure the effectiveness of textual features in compar ison with non-textual features, we take the model that utilizes only the features on authority as baseline. The average precision is chosen for measuring the overall performance and the contribution of each feature category.

Table 2 indicates the contribution of e ach feature category based on average precision score. From the table, formality is shown to be the most effective category when incorporated with non-textual features. Features on readability make no contribution, and even slightly decreased precision on the answer corpus. Subjectivity features conduced a remarkable imp rovement on review corpus that can be justified because of the imbalance size between two sentiment word sets.
Fig. 1 shows 11pt recall-precision curves for baseline and proposed model as well. On both of experimental datasets, textual features proved to be predictive indicator since our proposed method outperformed the baseline approach that utilizes only non-textual features.
 In this paper, we presented supervised classification model for evaluating user-created documents. Four categories o f features have been defined in terms of stable and effective for capturing document quality. Features on formality were pointed out to be the most useful features in augmenting the performance. Read-ability features have no significant impact, while features on subjectivity show promising contribution in further improvement. Although our model dominated data sparseness and restricted-domain fe ature selection, it still has limitations. Our proposed method only concentrated on the quality regardless of relevance to the content. Also, our experimental data for reviews is a little small.
For future work, we plan to expand our work in several practical areas such as opinion search, blog spam filtering, and summarization. We aim to continue investigating on subjectivity features as well as verifying the effectiveness of our quality prediction model.
 This work was supported by Microsoft Research Asia. Any opinions, findings, and conclusions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsors.

