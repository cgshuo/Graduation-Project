 With the rapid advances in hardware technology, data stream-s are being generated daily in large volumes, enabling a wide range of real-time analytical tasks. Yet data streams from many sources are inherently sensitive, and thus providing continuous privacy protection in data streams has been a growing demand. In this paper, we consider the problem of private analysis of infinite data streams under differential privacy. We propose a novel data stream sanitization frame-work that periodically releases histograms summarizing the event distributions over sliding windows to support diverse data analysis tasks. Our framework consists of two modules, a sampling-based change monitoring module and a continu-ous histogram publication module. The monitoring module features an adaptive Bernoulli sampling process to accurate-ly track the evolution of a data stream. We for the first time conduct error analysis of sampling under differential privacy, which allows to select the best sampling rate. The publica-tion module features three different publishing strategies, including a novel technique called retroactive grouping to enjoy reduced noise. We provide theoretical analysis of the utility, privacy and complexity of our framework. Extensive experiments over real datasets demonstrate that our solution substantially outperforms the state-of-the-art competitors. H.2.7 [ DATABASE ADMINISTRATION ]: Security, in-tegrity &amp; protection Differential privacy, data stream, sampling-based monitor-ing, retroactive grouping
With the rapid advances in hardware technology, there have been numerous applications that create and/or con-sume data streams in real time. Examples include public c health surveillance, IoT device data, mobile data, GPS da-ta, call detail records (CDRs), etc. Sharing such streams enables a wide spectrum of data analysis tasks in a real-time manner. However, streaming data from many sources are inherently sensitive, making privacy protection an in-creasingly important demand [17].

One notable privacy risk of sanitizing streaming data is that individually sanitized data could jointly compromise privacy. Fortunately, this risk can be gracefully addressed by the differential privacy model [8], which provides com-posable privacy guarantees for continuous data publishing over time. In the context of data streams, there are two instantiations of differential privacy providing different ex-tents of privacy protection: event-level differential privacy and user-level differential privacy [9]. Event-level differen-tial privacy hides any single event of any user from attack-ers, while user-level differential privacy hides all events of any user. In this paper, we follow the conventional use of event-level differential privacy [3, 5, 6, 9] mainly due to its ability to support private analysis of infinite data stream-s with reasonable accuracy. Infinite monitoring/analysis is a practical requirement in many applications. Meanwhile, we stress that event-level differential privacy still provides strong and meaningful privacy protection in that it provides any individual plausible deniability for any event in the pub-lished results.

While there have been a few existing works [3 X 6,9 X 11,13] on applying differential privacy to data streams, our prob-lem setting is different from theirs and is oriented toward practical applications. We consider a more realistic data stream model that is widely used in real-world applications: periodically releasing synopsis structures (e.g., histograms) of sliding windows over practical infinite data streams (not simplified 0/1 strings as in some previous papers) [2]. Due to the consideration of practical usefulness, we advocate the data-dependent paradigm, which has been shown to be able to obtain significantly better utility in many cases [14]. The general idea of the data-dependent paradigm is to adaptive-ly make use of the properties of the underlying data to guide private operations that lead to lower error.

More specifically, our solution is inspired by the power of grouping-or clustering-based approaches on static his-togram publication [1, 18, 19]. These approaches group or cluster the bins of a histogram with similar counts to en-joy Laplace noise reduction proportional to the sizes of the groups or clusters. In the context of data streams, it is nat-ural to attempt a similar idea to the time dimension: group time units with similar trends to improve utility. However, applying this idea to a data stream faces several non-trivial t echnical challenges: (1) The data elements (or events) ar-rive in real time. At the time of publication, we do not have knowledge about future trends. (2) It is required to release the histogram of the current sliding window at each times-tamp. We cannot postpone publication to collect a batch of histograms to analyze their similarity. (3) Grouping with previously published histograms might increase the sensi-tivity and thus the magnitude of noise. In view of these challenges, we design a data stream sanitization framework equipped with novel sampling and publishing techniques to improve utility.
 Contributions. In this paper, we consider a practical set-ting for continuously analyzing infinite data streams with the differential privacy guarantee. In addressing the prob-lem, we first propose two first-cut solutions, accompanied with formal utility analysis, and then propose a novel data stream sanitization framework that is composed of two ma-jor modules: a sampling-based change monitoring module and a continuous histogram publication module.
 The novelty of the monitoring module lies in an adaptive Bernoulli sampling process that accurately detects changes of the data stream even with a small privacy budget. While allowing to reduce Laplace error, sampling itself introduces a new source of sampling error, which is neglected in pre-vious research. However, it needs to be carefully studied to properly adjust the sampling rate so as to improve accura-cy. We for the first time quantify the sampling error and identify the best sampling rate as an optimization problem. Such analysis is useful for many sampling-based algorithms under differential privacy.

The publication module features three different publish-ing strategies, including a novel technique called retroactive grouping . It sophisticatedly selects time units to passively publish (i.e., approximate their statistics without explicitly using the Laplace mechanism) and then retroactively groups these units with later time units to enjoy reduced noise. We formally prove that retroactive grouping satisfies differential privacy. We theoretically analyze the accuracy of each re-leased histogram and the space and time complexity of our solution to demonstrate its suitability for processing large-scale real-time data streams. We stress that while we focus on histograms in this paper, our general framework can eas-ily accommodate other synopsis structures.

We have conducted an extensive experimental study over real datasets. The results confirm that our solution achieves substantially better utility than different competitors.
Applying differential privacy to data streams is a very recent topic. The first research papers [6, 9] consider com-puting the running sum of 1 X  X  (i.e., the number of 1 X  X  seen so far) over a simplified stream of 0 X  X  and 1 X  X  whose length is T . They independently present similar results. The proposed schemes encode the partial sums in terms of a full binary tree, where each node stores a partial sum plus noise with scale logarithmic in T , and answer the running sum by using the minimum number of nodes in the binary tree. Bolot et al. [3] extends the previous papers by considering decayed sum. A dyadic tree structure is maintained in a non-uniform manner. Our solution is orthogonal to these methods, but they can be combined to enhance each other.

Fan et al. [10] consider user-level differential privacy on finite streams. They present the FAST framework whose key idea is to adaptively sample the timestamps and take d-ifferent publishing strategies for sampled and non-sampled timestamps in order to increase the privacy budget allocat-ed to sampled timestamps. For sampled timestamps FAST releases their perturbed values by using the Laplace mecha-nism, while for non-sampled timestamps FAST releases their predicted counts instead. FAST is not directly beneficial to our problem setting because not sampling a timestamp does not increase other timestamps X  privacy budgets. Also, the prediction accuracy could be low sometimes.

Kellaris et al. [13] propose a variant of event-level differen-tial privacy called w -event differential privacy. It hides the presence/absence of any consecutive event sequence of size w . They introduce two privacy budget allocation schemes, Budget Distribution ( BD ) and Budget Absorption ( BA ), to achieve w -event differential privacy over infinite streams. Their contributions are orthogonal to ours. We aim at de-veloping techniques to improve accuracy, which could be seamlessly incorporated into the w -event differential privacy framework.

Friedman et al. [11] study the problem of privately mon-itoring arbitrary threshold functions over distributed finite streams. This is achieved by the concept of Safe Zones, with-in which local vectors are guaranteed to satisfy the global privacy condition. Compared to theirs, our problem is more challenging in that we need to publish data, not merely sig-naling the exceeding of a given threshold. Similar to the distributed setting in [11], Chan et al. [5] study the problem of continual monitoring of heavy hitters from distributed streams by an untrusted aggregator.

A different problem setting on data streams is studied in [4]. Cao et al. consider to answer a given set of cor-related sliding window aggregate queries. They propose to first sample a subset of representative queries, which will be answered by adding Laplace noise, and then estimate the answers to the remaining queries from the answers to the representatives.

As mentioned earlier, our work is motivated by studies on static histogram publication [1,18,19]. Xu et al. [18] propose to differentially privately group a histogram X  X  bins based on the traditional histogram construction technique. One dis-advantage of their solution is the requirement of knowing the number of groups in advance. Acs et al. [1] circumvent this drawback by proposing a bisection-based grouping s-trategy that automatically finds a good number of groups. Zhang et al. [19] argue that global clustering can achieve better utility than the local grouping schemes in [1,18] and consequently propose a clustering framework that careful-ly balances the approximation error and the Laplace error. Unfortunately, such a global clustering scheme does not ap-ply to data streams due to the challenges mentioned in the previous section.
In this section, we first introduce the data stream model and the privacy notion in use and then give our problem statement.
In this paper, we consider an infinite data stream S con-sisting of an infinite number of events that arrive in real t ime. Each event belongs to a user and is associated with an attribute value. The data publisher (e.g., the online rec-ommender system in Example 1) continuously monitors the stream and publishes histograms summarizing the event dis-tributions in the latest sliding windows at discrete times-tamps. We consider publishing histograms as they have wide applications in a broad spectrum of problems [2]. Each bin B i in a histogram H corresponds to an attribute value, and its count H i is the number of events falling into B i within the sliding window. We denote the total number of bins in H by | H | . We call the time duration between two consecu-tive timestamps a time unit . The size of a sliding window is the number of time units it covers, which is denoted by  X  . We denote the segment of S between timestamps t i and t j to many real-life applications, as illustrated below. Example 1. For an online recommender system (e.g., Yelp), it is a business routine to continuously monitor and analyze users X  events (e.g., users X  visits to different types of restaurants). Periodically releasing histograms summarizing user events in the latest sliding windows (e.g., the distribu-tion of restaurant types being visited in the past two weeks) is beneficial in many aspects, most importantly as a building block for recommendations. Such a scenario is illustrated in Figure 1, where the recommender system publishes the his-tograms of visits to different types of restaurants within the past 4 time units at every discrete timestamp. Each bin in a histogram corresponds to a restaurant type and has a count equal to the number of visits within the sliding window.
In the context of data streams, differential privacy rests on the notion of indistinguishability between two neighboring streams. In this paper, we follow the previous studies [3,5, 6, 9] to define two data streams S and S  X  as neighbors if S can be obtained from S  X  by adding or removing at most a single event. Intuitively, differential privacy guarantees that an attacker with arbitrary background knowledge will not be able to (statistically) distinguish any computational result from S and S  X  . This definition is formalized below.
Definition 1. (Event-Level Differential Privacy [9]) A ran-domized algorithm K gives event-level  X  -differential privacy, if for any two neighboring streams S and S  X  , and for any O  X  Range ( K ), where the probability is over the coin flips of K .
Since two neighboring streams differ in a single event, it follows that an attacker is not able to distinguish the pres-ence or absence of any event. Therefore, this privacy notion entitles any individual plausible deniability for any event in the published results. The privacy level is controlled by the parameter  X  , also known as privacy budget . The smaller  X  is, the more privacy we gain.

A standard technique to achieve differential privacy is the Laplace mechanism [8]. It injects properly calibrated Laplace noise into a function X  X  output to mask the impact of any single tuple. The maximal impact of a tuple to the output of a function f is called its sensitivity .
Definition 2. (Sensitivity [8]) For a function f : S  X  R d the sensitivity of f is  X  f = max S , S  X  k f ( S )  X  f ( S neighboring streams S and S  X  .

The Laplace noise needed to satisfy differential privacy is precisely calibrated by a function X  X  sensitivity and the given privacy parameter, leading to the Laplace mechanism as follows.

Theorem 1. [8] For any function f : S  X  R d , the mech-anism K , variables with scale parameter  X  f  X  .

F or data streams, the graceful composition properties of differential privacy are particularly important for continuous privacy protection. The sequential composition property s-tates that when combined differentially private subroutines still provide differential privacy guarantees.

Theorem 2. [8] Let each mechanism K i provide  X  i -differential privacy. A sequence of K i ( D ) over the database D provide
A special case is when the mechanisms are applied to dis-joint databases, leading to the parallel composition property. Theorem 3. [8] Let K i provide  X  i -differential privacy. A sequence of K i ( D i ) over a set of disjoint databases D provides max(  X  i ) -differential privacy.
In this paper, we consider the following problem: Giv-en an infinite data stream, at each discrete timestamp we would like to publish an accurate histogram that summarizes the event distribution in the current sliding window while satisfying event-level differential privacy .

Following the convention in [18, 19], the accuracy (or u-tility) of a sanitized histogram is measured in terms of the expected sum of squared error (SSE). Given an original his-togram H and its sanitized version e H , the expected SSE is where H i and e H i are the true and perturbed counts in H and e H , respectively, and | H | is the number of bins. Accordingly, the error of a bin is measured by its expected squared error. In our problem, we publish a sanitized histogram at each timestamp summarizing the event distribution in the current sliding window, and therefore our utility goal is to minimize either the average or the total expected SSE of all published histograms.
I n this section, we provide two first-cut solutions that help better understand the problem and motivate our solution.
To publish T histograms over a data stream 1 , we can di-rectly perturb the histograms of the sliding windows by the Laplace mechanism. The most straightforward approach is to make use of the sequential composition property to di-vide the total privacy budget  X  into T portions, each being used for sanitizing a histogram. Since a single event can change exactly one bin X  X  count by 1, the expected SSE of a histogram introduced by this approach is E ( SSE ( H , e H )) = E F or an infinite data stream, we need to publish an infinite number of histograms (i.e., T  X  X  X  ), and therefore the above method can barely provide useful results. However, if we ob-serve that a single event can affect at most  X  sliding windows that cover it, where  X  is the sliding window size, we can do better. The sensitivity of all count queries needed to build all the histograms is  X  because a single event can affect at most  X  bin counts by 1. Therefore, the expected SSE of a histogram becomes 2 | H |  X  2  X  2 , where  X   X  T .
Another alternative is to add Laplace noise to the counts of each time unit and generate the histogram of a sliding window by summing the counts of all time units it covers. Since a single event can affect at most one bin X  X  count in at most one time unit by 1 (irrespective of the fact that the data stream is infinite), the sensitivity of all count queries is 1. Thus the expected SSE of the histogram corresponding to a time unit is 2 | H |  X  2 , leading to an expected SSE of f or a histogram of a sliding window with size  X  . It can be seen that this method is strictly better than the sliding win-dow based perturbation approach. Therefore, we consider the time unit based perturbation as the baseline approach (referred to as Baseline in the sequel) and propose novel techniques to further improve its utility.
In this section, we first provide an overview of our data stream sanitization framework and then elaborate its two key components. Our retroactive-grouping-based framework (referred to as RG ) consists of two novel modules: a sampling-based change monitoring module and a continuous histogram publication module. The former adaptively determines the best sam-pling rates to learn the underlying stream X  X  evolution, based on which the latter takes proper publishing strategies to im-prove utility.

We first summarize the notations that are most frequently used in the sequel. We denote a time duration starting at segment of the data stream S in [ t i , t j ] is then denoted by
H ere we introduce the parameter T just for the theoretical analysis. Our solution supports infinite streams. Algorithm 1 P rivate histogram publication at time t i Input: D ata Stream S Input: Current timestamp t i Input: Privacy budgets  X  1 and  X  2 s.t.  X  1 +  X  2 =  X  Input: Sliding window size  X  Input: Histograms e H t k  X  1 ,t k with i  X   X  + 1  X  k  X  i  X  1 Output: Sanitized histogram e H t i 1: Calculate the difference vector d between [ t i  X  1 , t 2: for each bin B j do 3: if d [ j ]  X  2 4 : e H j t 5 : G ( B j ) =  X  ; 6: else 7: Add H j t 8: if |G ( B j ) | X  m then //see Section 5.3 for m 9: e H j t 1 0: G ( B j ) =  X  ; 11: else 12: e H j t 13: e H j t 14: return e H t i ; S duration [ t i , t j ] by H t i ,t j and the count of a bin B its count of a bin B l is written as H l t clear, we may omit some subscripts or superscripts. We use tilded versions to denote differentially private variables.
At a timestamp t i , Algorithm 1 presents how our frame-work releases a sanitized histogram for the current sliding window. Since we concern with event-level differential pri-vacy, it is helpful to consider the events within different time units as disjoint databases. According to the parallel prop-erty, this allows us to use the full privacy budget on each time unit and offers us much flexibility in algorithm design. In Line 1, the monitoring module decides a proper sampling rate and calculates the difference vector d that measures the count differences of all bins between the current time unit and the previous time unit in terms of squared error. Since the monitoring module operates on the raw data, it needs to be guarded by differential privacy. For this purpose, the privacy budget  X  1 is assigned to it. Instead of comparing the SSE of the histograms in whole, we observe that the counts of different bins may change at different rates and therefore process each bin individually. Each element in d records the difference of the counts of a bin.

If the count difference of a bin is larger than the error introduced by the Laplace mechanism ( 2 for explanation), the best strategy is to employ the Laplace mechanism to generate the bin X  X  noisy count (Line 4). Oth-erwise, there is room to improve. A natural idea is to ap-proximate the current count with the previous (noisy) coun-t, a method used in [13], which we call passive publishing . Unfortunately, in our setting passive publishing alone may not be able to improve accuracy. In [13], passive publish-ing benefits by saving on the privacy budget for future time units, while in our case each time unit can use the priva-c y budget in full. This implies that we can always use the Laplace mechanism to generate a noisy count with error no more than that of passive publishing, which is the sum of the Laplace error of the same magnitude and the approxi-mation error (i.e., the actual difference between the current and the previous counts).

Here our key insight is that while not able to save on pri-vacy budget, the time unit passively released (referred to as passive time unit ) does help reduce sensitivity. To gain this benefit, we deliberately select some time units to passively publish and later group them with the time units with close counts, which we call retroactive grouping . At timestamp t if we can find a retroactive group with a size  X  m (a thresh-old determined by error analysis later) for a bin, we add reduced Laplace noise to the count (Line 9); otherwise we use passive publication (Line 12) to approximate the count. Finally, the noisy count of a bin in the current sliding win-dow is the sum of the noisy counts of all time units in the window (Line 13).
Monitoring the evolution of a data stream is key to taking the proper publishing strategy in our framework. At each timestamp, we compute the differences between the counts of the current time unit and those of the previous time unit. Sampling is a common tool used on data streams mainly for the reason of efficiency. In the context of differential privacy, sampling also has an impact on the utility aspect. Previous research [15] has indicated the amplification effect of sam-pling on the privacy budget, which is formalized below.
Theorem 4. [15] Let K be an  X  -differentially private al-gorithm. Let K  X  be another algorithm that first samples each tuple independently in its input dataset with probability  X  and then applies K to the sampled dataset. K  X  satisfies ln(1 +  X  ( e  X   X  1)) -differential privacy.

Theorem 4 suggests that if the sample generated is rep-resentative (i.e., the statistics obtained over it are approxi-mately the same as those of the original data), it is possible to obtain more accurate results over the sample by taking advantage of reduced Laplace noise. However, since sam-pling itself introduces a new source of error, sampling error , sampling may not always be helpful. To make a wise deci-sion on whether or not to employ sampling (and the proper sampling rate), we need to carefully quantify the sampling error. While sampling has been used in previous research on differential privacy [12, 15], to our best knowledge, the sampling error was never quantified before. In this paper, we provide theoretical analysis on the sampling error.
Algorithm 2 presents the pseudocode of our sampling-based monitoring module. At each timestamp t i , for each bin B j , we estimate the difference between its counts in the current time unit [ t i  X  1 , t i ] and the previous time unit [ t i  X  2 , t i  X  1 ]. The first step is to determine the proper sam-pling rate  X  j for B j (Line 2 of Algorithm 2). The sampling technique used in Theorem 4 is Bernoulli sampling [16]: For each coming event (of the population), perform a Bernoulli trial with the success probability 0 &lt;  X  j  X  1, which is the sampling rate. If a success occurs, include the trial event as part of the sample; otherwise discard the event. Under Bernoulli sampling, given a sample size n j , the population size N j (i.e., B j  X  X  true count in time unit [ t i  X  1 , t Algorithm 2 S ampling-based monitoring at time t i Input: D ata stream S Input: Current timestamp t i Input: Privacy parameter  X  1 Output: Difference vector d 1: for each bin B j do 2: Compute the sampling rate  X  j ; 3: Generate S j by sampling events of B j in S t i t 4: e N j = 5 : d [ j ] = e H j t 6: return d ; estimated by  X  N j = n j  X  Since we use squared error to measure the utility of sani-tized histograms, here we use mean squared error (MSE) to quantify the sampling error. Since the sample size n j is a binomial variable, we have where P ( n j = i ) = N j of having a sample size i . By solving Equation 1, we ob-introduced in our monitoring module.
 On the other hand, due to sampling, we enjoy reduced Laplace noise (see its construction in Line 4 of Algorithm 2), whose error can be quantified as per Theorem 4 as The total error is the sum of the sampling error and the Laplace error. Thus, we can express the total error as a function of  X  j and are interested in identifying the sampling rate  X  j that minimizes the total error, as formulated below:
Since  X  j is confined to the range (0 , 1], we can easily find the best rate by discretizing the range and applying the brute force method 2 . The above formulation is general to capture both the cases of sampling and not sampling. If  X  = 1 is chosen, it means that no sampling should be ap-plied. Therefore, our solution is general to make a decision on whether or not to apply sampling at all. Now the only unsolved problem is that in practice the population size N (and even its noisy version) is unknown before choosing  X  Here we estimate N j by the noisy count in the previous time unit due to the fact that the statistics of a data stream may not change substantially in successive time units, a key as-sumption made in previous research [13]. Our experiments confirm that our foregoing error analysis precisely identifies
I n the experiments, we set the granularity to 1%. the best sampling rate. In general, we observe that the ben-e fit of sampling is more visible when the population size is relatively small or the privacy budget is small. This allows us to use a very small  X  1 (e.g., 0 . 1  X  ) to accurately monitor the evolution of a data stream.

For each bin B j , after determining its sampling rate  X  j generate the sample S j by including each event associated with B j in the stream segment S t i t true sample size | S j | , estimate the count by the unbiased estimator introduced before, and record the squared error between this estimation and the noisy count of the previous time unit in the difference vector d . Since we keep track of the changes of bins independently, the same procedure needs to be applied to all bins.
As analyzed in Section 4, the time unit based perturba-tion method achieves strictly better accuracy than the slid-ing window based perturbation method. Thus we design our continuous histogram publication module based on this con-clusion. Depending on the output of the monitoring module, our framework takes three different strategies to calculate the bins X  noisy counts in the current time unit [ t i  X  1
We first quantify the error of the Laplace mechanism and use it as the sentinel to indicate which strategy to take. Since any single event can affect only one bin X  X  count by exactly 1 (i.e., the sensitivity of the corresponding count queries is 1) and  X  2 is reserved for publication, for any count H j squared error of its noisy version generated by the Laplace mechanism is
T his explains the threshold used in Line 3 of Algorithm 1. If the difference of a bin B j  X  X  counts between the current time unit and the previous unit is  X  2 ing the standard Laplace mechanism to perturb the current count H j t to perturb the count and that the sensitivity is 1, and thus the Laplace noise is generated with scale parameter 1  X  will formally analyze the privacy guarantee of our solution in Section 6.2.

If the difference is &lt; 2 sible to benefit from retroactive grouping and take the cor-responding strategies. In the context of data streams, the grouping must be done in a way that does not increase the sensitivity ; otherwise its benefit will be canceled out due to the increased sensitivity. It follows that the count of a bin in a time unit that has been previously released by the Laplace mechanism (using  X  2 3 ) should never be asked again. For this reason, we deliberately choose some time units for passive publishing (and thus their counts have not been answered by the Laplace mechanism using  X  2 ) and later group a bin X  X  cur-rent time unit with its previously passively published time units for reduced error.

We denote the retroactive group of a bin B j by G ( B j ) and its size by |G ( B j ) | . It contains the counts of the preceding passive time units up to (but not including) the most recent
T here is no need to worry about the queries issued using  X  . This point will be made clear in Section 6.2. non-passive time unit. After each non-passive release, we empty G ( B j ) (Lines 5 and 10 of Algorithm 1). It can be seen that such a construction does not increase the sensi-tivity. As a result, the counts of the first |G ( B j ) | X  1 mem-bers in G ( B j ) are approximated by the noisy count of the most recent non-passive time unit, and the last member, the count of the current time unit H j t Laplace noise. More specifically, H j t tive group, that is
W e give the sensitivity of the group average here and prove its privacy guarantee later.
 Theorem 5. The sensitivity of
P roof. Since a single event can change a bin X  X  count in at most one time unit by 1, it can change at most 1, and therefore can change the average by at most
For static histograms, we always prefer a larger group pro-vi ded that the approximation error is low (i.e., all counts in the group are close) because the Laplace noise reduction of every member in the group is proportional to the group size. Unfortunately, this is not the case for a data stream. We need more sophisticated design. For a data stream, it is mandatory to release a histogram at each timestamp and the histogram is consumed in real time. The previous-ly published counts (i.e., the first |G ( B j ) | X  1 members in G ( B j )) cannot benefit from the reduced Laplace noise due to retroactive grouping. This fact requires to carefully se-lect the retroactive group size, the parameter m in Line 8 of Algorithm 1.

As per the problem definition, for a bin B j , our goal is to minimize the average expected squared error of its retroac-tive group. To this end, we would like to express the average error as a function of m in order to identify the best m val-ue. Here we need to first understand the squared error of a passive publication. Let H i be a count from which H j is approximated by a passive publication. Note that H i and H j do not have to belong to two consecutive time units be-cause if the data stream evolves very slowly, H j could be well approximated by the count of a time unit long ago. In a passive publication, we approximate H j with the noisy version e H i of H i and thus the expected error is where  X  is the scale parameter of the Laplace noise added to e H i .  X  may not always be 1  X  with reduced Laplace noise by retroactive grouping. The first item ( H j  X  H i ) 2 represents the approximation error , while the second item 2  X  2 represents the Laplace error . The trade-off between these two types of errors lies at the core of our approach: we would like to trade a small approximation error for a large reduction of Laplace error . If the standard Laplace mechanism is applied to H i ( in this case  X  = 1  X  passive publishing cannot perform better than directly ap-plying the Laplace mechanism to H j because ( H j  X  H i ) Now we are ready to present the average expected squared error of noisy counts in a retroactive group.

Theorem 6. For a retroactive group G ( B j ) of size m , its average expected squared error is where H i with 1  X  i  X  m is the i th element in G ( B j ) , is the average of H i  X  X  , H 0 is the true count from which the first m  X  1 members in G ( B j ) are approximated, and  X  is the scale of the Laplace noise added to H 0 .

Proof. Since the first m  X  1 members are passively re-leased, all their noisy counts are approximated by H 0 + Lap (  X  ). Then, by definition, the average expected squared error of a retroactive group is E = 1 m E = 1 m = w here the first item represents the approximation error and the second item represents the Laplace error.

If we only consider the Laplace error, it is easy to derive t he best value of m : the approximation error often also increases, which requires to select a smaller m value. Since the approximation error is data-dependent and difficult to quantify, we empirically choose m = 2  X  X  which works well for all experimental datasets. Since  X  is a variable that changes with the data stream, the value of m also changes.

We illustrate how to apply different publishing strategies in the following example.

Example 2. Consider the data stream in Figure 2. We focus on a single bin with its counts given on top of the time units. At timestamp t 2 , since it is the first time unit, its true count 5 is perturbed by the Laplace mechanism. At t 3 let the difference between 75 and 5 detected by the monitoring module is  X  2 mechanism. Let the differences calculated from t 4 to t 9 &lt; 2 because the current retroactive group size is &lt; m = 2 . At t the true count 73 is published using retroactive grouping and the group is then emptied. After that, the counts at t 6 , t and t 8 are passively released because now m = 4 . Finally the Figure 2: An illustration of continuous histogram p ublication
After obtaining a bin X  X  noisy count in the current time unit using one of the publishing strategies, its noisy count in the current sliding window is the sum of the noisy counts of the time units covered by the sliding window (Line 13 of Algorithm 1). We repeat this procedure for all bins to produce the histogram to publish at t i .
In this section, we provide the theoretical analysis of RG . We give the major utility result in the following theorem.
Theorem 7. For (a segment of) a data stream with T time units, if T = 2 l +1  X  1 for some l  X  N + , the minimum average expected SSE of the histograms published by RG is otherwise its minimum average expected SSE is where k =  X  log 2 ( T + 1)  X  1  X  .

Proof. It is easy to see that the minimum average ex-pected SSE is achieved when the events in each bin arrive at a fixed speed. In this case, the approximation error is 0 and the SSE solely comes from the Laplace error. By de-sign, for any bin, its first time unit X  X  count will be published by the Laplace mechanism with the Laplace error 2 that, if T = 2 l +1  X  1 for some l  X  N + , the retroactive group are 2 1 , 2 2 ,  X  X  X  , 2 k , T  X  For both cases, the  X  value of the i th group is 2 2 i  X  pace reasons, we only prove the latter case. The former is a simplified case of the latter and can be similarly proved. By Theorem 6, the Laplace error of the i th retroactive group is can express the minimum average expected squared error of a bin in a time unit as = 2 = 2 Since each histogram covers  X  t ime units and has | H | bins, the minimum average expected SSE becomes This completes the proof.

Theorem 7 can be used to analyze the utility of not only a n entire data stream (even an infinite stream by setting T  X   X  ), but also a segment of a data stream. Since the minimum SSE is achieved when the approximation error is 0, Theorem 7 helps to identify the maximum Laplace error reduction that can be achieved by RG . For example, when T = 63, the minimum Laplace error is 0 . 14  X   X  | H |  X  2  X  = 0 . 9  X  , which is our experimental setting, the minimum Laplace error is just 8 . 6% of the Laplace error introduced
When analyzing the utility of RG , we cannot ignore the approximation error, which is another source of error in RG . Due to the data-dependent nature of RG , it is very difficult to precisely quantify the approximation error. But it is helpful to consider the Laplace error reduction as the tolerance of RG for the approximation error. As long as the approximation error is less than the reduction, it is beneficial to use RG . In Section 7, we experimentally show that this is often the case on real-life data.
To establish the privacy guarantee of RG , we mainly rely on the two composition properties of differential privacy. Now we give the formal privacy guarantee of RG .
 Theorem 8. RG is  X  -differentially private.
 Proof. By sequential composition property, to prove that RG is  X  -differentially private, it suffices to independently prove that the sampling-based change monitoring module satisfies  X  -differential privacy and that the continuous histogram publication module satisfies  X  2 -differential privacy, where  X  =  X  1 +  X  2 .

For the sampling-based change monitoring module, since any single event can appear in exactly one time unit, by definition of event-level differential privacy, we can consider the events within different time units as disjoint databas-es no matter how many time units there are in the data stream. By the parallel composition property, if the proce-dure for processing each time unit is  X  1 -differentially private, the monitoring module for the entire data stream is still  X  differentially private. For any time unit, since an event is associated with exactly one bin, we can again apply the par-allel composition property, which requires to prove that the noisy count of each bin in the time unit is  X  1 -differentially private. Note that since the output of the monitoring mod-ule is based on raw data, it needs to be properly sanitized even though it does not directly publish its output.
For any bin B j , we first prove that without sampling ln  X  j )-differential privacy. This directly follows the fact that the sensitivity of the count query is 1. Note that our method of learning  X  j does not consume any privacy budget as it is based on differentially private results. Now by Theorem 4, we learn that the sampling-based mechanism satisfies  X  1 -differential privacy because Dataset Event no. | H | Timestamp no.
 World Cup 1,352,804,107 88,762 [176, 2112]
Next we prove that the continuous histogram publication m odule is  X  2 -differentially private. Establishing the privacy guarantee of the publication module is more tricky as we issue queries that span over several time units, and therefore we cannot treat the events in different time units as disjoint databases. But the fact that an event can fall into exactly one bin still holds. Therefore, by parallel composition, it suffices to prove that all noisy answers to all queries over any bin gives  X  2 -differential privacy. We establish the proof by transforming all these queries into a standard Laplace mechanism. For each bin, we ask two types of queries: in Line 4 of Algorithm 1, we issue a count query to obtain the noisy count in the current time unit; in Line 9, we issue a query to obtain the noisy average of a retroactive group. By Theorem 5, the latter is equivalent to a sum query: what is the sum of the counts in a retroactive group, as the group size can be learned from the differentially private results. By design, no event can be involved in more than one of these queries because no time unit is involved in more than one query. It can be verified that for all these count and sum queries, a single event can change only one of the answers by exactly 1. Therefore, due to Theorem 1, the publication module is  X  2 -differentially private.
F or real-time stream processing, the space complexity and time complexity of an algorithm are very important factors to consider. For the space complexity, RG never requires to store the entire data stream. At any timestamp, it only needs to store the events in the current time unit, the noisy counts of the past  X   X  1 time units and the retroactive groups for all bins. Let n be the number of events in the current time unit and m max be the maximum group size over the data stream. It is easy to verify that the space complexity is O ( n + ( m max +  X  ) | H | ). In practice, m max and  X  are normally small numbers (compared to n and | H | ).
The time complexity of RG for each publication can be de-rived from Algorithm 1. In the monitoring module, for each bin, we calculate its sampling rate, where the runtime cost is O (1). For each coming event we perform a Bernoulli tri-al to decide whether to include it in the sample. Therefore, the time complexity of the monitoring module is O ( n + | H | ), where n is the number of events arriving in the current time unit and | H | is the number of bins. In the publication mod-ule, in the worst case, for each bin, we need to sum all ele-ments in its retroactive group and sum the noisy counts of the  X  time units in the sliding window. The time complexity of processing all bins is O (( m max +  X  ) | H | ). The total time complexity is then O ( n + ( m max +  X  + 1) | H | ).
As such, our sanitization framework introduces only a s-mall space and time overhead and is suitable for processing large-scale real-time data streams.
In this section, we empirically study the utility and scala-bility of our solution RG . For comparison reasons, we adapt the state-of-the-art techniques to our problem setting, in-cluding FAST [10] and BA [13]. We also implement Baseline introduced in Section 4.2. All methods are implemented in Java and tested on a machine with an Intel i7 2.40GHz CPU and 8GB RAM. In each experiment, every algorithm is executed 10 times, and the average is reported. We use two real datasets in our experiments. The World Cup dataset 4 contains 1,352,804,107 requests made to the 1998 World Cup Web site in 88 consecutive days. After removing some irregular URLs, it contains 88,762 unique URLs, each being considered a bin. The Traffic dataset [7] contains passengers X  8,107,643 visits to 342 stations in the Montreal transportation system within one week. Similar to [13], we consider the number of timestamps as a variable parameter for investigation. The statistics of the datasets are summarized in Table 1. Unless explicitly specified, the default values of the parameters are set as follows:  X  = 0 . 1,  X  = 0 . 1  X  ,  X  2 = 0 . 9  X  , w = 15, and the numbers of timestamps are 528 for World Cup and 504 for Traffic, respectively.
In our first set of experiments, we study the effect of pri-vacy budget on all approaches. The total and average SSEs under different privacy budgets are reported in Figure 3. Note that the y-axis has a logarithmic scale. In general, the error decreases with the increase of privacy budget. As can be observed, RG substantially outperforms all competitors in all cases. RG can improve the accuracy of the second best approach by up to 43%.

Figure 4 presents the errors under different sliding window sizes. With the increase of the sliding window size, the error increases. Since all these approaches are essentially time unit based perturbation, including more time units in a sliding window results in a larger error. Also for this reason, the improvement of RG becomes more obvious with the increase of the sliding window size.

In the next set of experiments, we study the effect of the number of timestamps in Figure 5. This is equivalent to examining the effect of the magnitude of changes over time.
P ublicly available at: http://ita.ee.lbl.gov/html/ contrib/WorldCup.html Since more timestamps means more publications, the to-tal SSE increases accordingly. More interesting observations are from the average SSE. The average SSE of Baseline is quite stable under different numbers of timestamps. This is because the Laplace noise injected by Baseline is inde-pendent of the underlying data (i.e., it is calibrated by the sensitivity and the privacy budget, not the underlying da-ta). In contrast, the other approaches all benefit from the increase of the number of timestamps or equivalently the reduced changes between consecutive time units. In par-ticular, for RG , having more timestamps implies a slower evolution, which in turn entitles more chances of applying retroactive grouping to reduce noise. Again, in all cases, RG performs significantly better than the other approaches.
According to our theoretical analysis, the space complexi-ty of RG is very small. Due to the space limit, we only report the results on the time complexity. We study the runtime of RG w.r.t the two dominating factors, namely the number of events and the number of bins. Figure 6 shows the runtimes of different approaches under different numbers of events. To form the test datasets, we uniformly select a percentage of total events at random. The runtime of Fast is too large to fit into the figures and therefore is omitted. The units of the y-axis are millisecond and microseconds in Figure 6(a) and (b), respectively. While larger than Baseline and BA , the runtime of RG is in general negligible: in the worst case the runtime per publication is less than 700 milliseconds on World Cup and less than 1 millisecond on Traffic. Also, the runtime of RG grows linearly with the number of events, conforming to our theoretical analysis. The runtimes for dif-ferent bin numbers are shown in Figure 7. Again the results of FAST are omitted for the same reason. Similarly, we gen-erate the test datasets by randomly extracting a percentage of bins. We can observe the expected trends on both World Cup and Traffic: the runtime is very small and increases linearly with the number of bins.
Finally, we would also like to note that since RG process-es each bin independently, we can easily and substantially improve its runtime by parallel computing techniques.
In this paper, we consider the problem of continuously publishing histograms over an infinite data stream while sat-isfying differential privacy. We propose a novel sanitization framework consisting of two modules. The sampling-based monitoring module performs Bernoulli sampling by careful-ly quantifying the sampling error; the continuous publica-tion module is equipped with a novel retroactive grouping technique to effectively reduce error. Our solution incurs a very small space and time overhead. Experiments over real datasets show that our framework outperforms the state-of-the-art competitors. [1] G. Acs, C. Castelluccia, and R. Chen. Differentially [2] C. C. Aggarwal. Data Streams: Models and [3] J. Bolot, N. Fawaz, S. Muthukrishnan, A. Nikolov, [4] J. Cao, X. Qian, G. Ghinita, N. Li, E. Bertino, and [5] T.-H. H. Chan, M. Li, E. Shi, and W. Xu.
 [6] T.-H. H. Chan, E. Shi, and D. Song. Private and [7] R. Chen, G. Acs, and C. Castelluccia. Differentially [8] C. Dwork, F. McSherry, K. Nissim, and A. Smith. [9] C. Dwork, M. Naor, T. Pitassi, and G. N. Rothblum. [10] L. Fan and L. Xiong. An adaptive approach to [11] A. Friedman, I. Sharfman, D. Keren, and A. Schuster. [12] G. Kellaris and S. Papadopoulos. Practical differential [13] G. Kellaris, S. Papadopoulos, X. Xiao, and [14] C. Li, M. Hay, G. Miklau, and Y. Wang. A data-and [15] N. Li, W. H. Qardaji, and D. Su. On sampling, [16] C.-E. Sarndal, B. Swensson, and J. Vretman. Model [17] N. Wang, J. Grossklags, and H. Xu. An online [18] J. Xu, Z. Zhang, X. Xiao, Y. Yang, and G. Yu. [19] X. Zhang, R. Chen, J. Xu, X. Meng, and Y. Xie.
