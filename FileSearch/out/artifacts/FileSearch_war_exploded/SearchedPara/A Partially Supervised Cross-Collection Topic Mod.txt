 Cross-domain text classification aims to automatically train a precise text classifier for a target domain by using labelled text data from a related source domain. To this end, one of the most promising ideas is to induce a new feature rep-resentation so that the distributional difference between do-mains can be reduced and a more accurate classifier can be learned in this new feature space. However, most ex-isting methods do not explore the duality of the marginal distribution of examples and the conditional distribution of class labels given labeled training examples in the source do-main. Besides, few previous works attempt to explicitly dis-tinguish the domain-independent and domain-specific latent features and align the domain-specific features to further im-prove the cross-domain learning. In this paper, we propose a model called Partially Supervised Cross-Collection LDA topic model (PSCCLDA) for cross-domain learning with the purpose of addressing these two issues in a unified way. Ex-perimental results on nine datasets show that our model outperforms two standard classifiers and four state-of-the-art methods, which demonstrates the effectiveness of our proposed model.
 I.2.6 [ Artificial Intelligence ]: Learning; I.5.1 [ Pattern Recognition ]: Models X  Statistical ; I.5.2 [ Pattern Recog-nition ]: Classifier Design and Evaluation; H.2.8 [ Database Management ]: Database Applications X  Data mining Algorithms, Experimentation, Performance Topic Modeling; LDA; Cross-Domain Learning; Text Clas-sification
Most supervised learning algorithms assume that training and testing data follow the identical distribution. However, in practice we may have a source domain with plentiful la-beled training data, but need to classify unlabeled data from a target domain with a different distribution from the source domain. For example, we might want to classify blog articles into some pre-defined categories (e.g. sports, politics, etc.). There are usually no labeled data in this target domain (i.e. blog articles) but abundant labeled data in another source domain such as news articles which are well-organized in news websites like CNN.com and BBC.com. The data dis-tributions in these two domains might be quite different because of different word usages or writing styles. In this scenario, the performance of supervised learning algorithms will normally deteriorate due to the violated assumption of identical distribution. To tackle this problem, cross-domain learning (also called domain adaptation or transfer learning ) methods have been recently proposed [10, 15].

Cross-domain learning methods aim to learn a more accu-rate model for the target domain by exploiting the labeled data from the source domain, as well as small amounts of labeled data and/or unlabeled data from the target domain. To this end, one of the most promising ideas is to induce a new feature representation so that the distributional dif-ference between domains can be reduced and a more ac-curate classifier can be learned in this new feature space. More formally, letting P s ( x ; y ) and P t ( x ; y ) denote the data distribution in the source and target domain respectively, where x is an example and y is the corresponding class la-bel, the goal is to transform the original feature space to a new one so that P s ( x ; y ) and P t ( x ; y ) are more similar. distribution of examples and P ( y | x ) is the conditional dis-tribution that could be viewed as the predictive model, it seems necessary to take both of these two factors into con-sideration. Ben-David et al. [1] formalize this intuition and theoretically demonstrate that the designed feature repre-sentation should simultaneously minimize the difference be-tween source and target domains (w.r.t P ( x )) and the em-pirical training error in the source domain (w.r.t P ( y | x )).
Many methods have been proposed for cross-domain learn-ing based on the aforementioned idea. However, to derive the new feature representation, most of them [4, 3, 18, 14, 11] only take into account the marginal distribution P ( x ) but ignore the conditional distribution P ( y | x ). Specifically, they assume that the conditional distributions will be simi-lar if the corresponding marginal distributions of examples in different domains are similar in the new feature space. In o ther words, they hypothesize that if two examples are close in the new feature space, their class labels should be same. As empirically demonstrated in [1], although these methods do not explicitly consider the conditional distribution, they do achieve low values for both marginal distribution differ-ence and training error in the source domain. However, Long et al. [13] recently demonstrate that these methods can not achieve optimal capability of knowledge transfer without ex-ploring the duality between the marginal distribution P ( x ) and conditional distribution P ( y | x ). They thus propose the so-called Dual Transfer Learning (DTL) which utilizes the duality between the two distributions to mutually reinforce each other during learning.

Another issue, when minimizing the difference of marginal distribution ( P ( x )), is that some induced common features might be specific to the source domain only and, conse-quently, the classifier relying on these features will be badly affected when tested on the target domain. Some previous works [17, 13] have noticed this issue and propose to only induce the domain-independent common features that can generalize between domains. More recently, Li et al. [11] demonstrate that the alignment of domain-specific latent features, in addition to the domain-independent features, can further improve the cross-domain learning.
 In this paper, we propose a novel model, called Partially Supervised Cross-Collection LDA Topic Model (PSCCLDA), for cross-domain learning, with the purpose of addressing the aforementioned issues in a unified way. Firstly, our model considers the duality between the marginal distribution and the conditional distribution so that they could reinforce each other during the learning. Specifically, we design an EM-style iterative learning algorithm where we seek to optimize the marginal distribution in the E-step and optimize the con-ditional distribution in the M-step. Secondly, our model ex-plicitly distinguishes the domain-independent and domain-specific latent features and seeks to align the domain-specific latent features to further improve the learning of new feature representation.

To evaluate the performance of our model for cross-domain text classification, we conduct experiments on nine datasets generated from two widely used datasets, including 20News-groups and Reuters-21578. Experimental results demon-strate that our method outperforms two traditional classi-fiers including Logistic Regression (LR) and Support Vec-tor Machine (SVM), and four state-of-the-art cross-domain learning methods including SFA, TPLSA, CDPLSA and TCA which will be reviewed in Section 2.

The rest of the paper is structured as follows. In Section 2, we briefly review some related works. In Section 3, we formally define the problem and present our proposed model. In Section 4, we present and discuss the experimental results. Finally, Section 5 concludes the paper and provides some possible future works.
In this section, we briefly review some previous works which are closely related to ours, including cross-domain learning and cross-collection topic modeling.
Cross-domain learning (also called domain adaptation or transfer learning ) has attract great attentions in recent years. As surveyed in [15], there are different settings of cross-domain learning. In this paper, we focus on the transduc-tive learning setting in which there are no labeled data in the target domain but abundant labeled data in the source domain, while the learning tasks in both domain are the same. The existing methods for transductive cross-domain learning can be roughly categorized into two types [15], in-cluding instance-based methods and feature representation based methods. Here, we only focus on the feature repre-sentation based methods.

Feature representation based methods aim to induce a common feature representation between the two domains to reduce the distributional difference. To this end, one type of algorithms attempts to make use of the domain-independent  X  X ivot features X  X o align the domain-specific features. For ex-ample, Blitzer et al. [4] propose the Structural Correspon-dence Learning (SCL) algorithm to learn a low-dimensional latent space by exploring the relationships between  X  X ivot features X  and  X  X on-pivot features X . Pan et al. [14] propose the Spectral Feature Alignment (SFA) algorithm to align domain-specific words from different domains into united clusters, with the help of domain-independent  X  X ivot fea-tures X . Then domain-independent and domain-specific fea-tures are co-clustered into a common latent space. However, the success of this kind of methods crucially depends on the auxiliary tasks for selecting  X  X ivot features X , which can be a non-trivial engineering problem for many different applica-tions. Different from these algorithms, our proposed model does not use any auxiliary tasks.

Another type of algorithms, which are closely related to our work, seeks to take advantage of the topic modeling tech-nique to induce the high-level common latent feature space. For example, Xue et al. [18] propose the Topic-bridged PLSA (TPLSA) model which extends the PLSA by intro-ducing the supervision of labeled data in training domain via the pair-wise constraints. However, TPLSA does not ex-plicitly model the domain-independent and domain-specific topics and simply assumes that the topics are shared by all domains. Zhuang et al. [21] propose the Collaborative Dual-PLSA (CDPLSA) model which extends the Dual-PLSA pro-posed in [19]. Dual-PLSA separately models the word topics and document topics. CDPLSA further assumes that word topics and document topics are respectively independent of the data domain, while the association between word top-ics and document topics is stable across domains. Although authors claim that word topics in different domains are se-mantically related to each other, CDPLSA actually only ex-tracts domain-specific (word) topics. Different from TPLSA and CDPLSA, Li et al. [11] propose the Topic Correlation Analysis (TCA) method which explicitly extracts the shared topics and domain-specific topics, and utilizes the correla-tions between them for cross-domain learning. Their ex-perimental results show that TAC outperforms TPLSA and CDPLSA. However, it requires an additional step to align domain-specific topics and the information of labels in the training domain is not utilized (as supervision) for learning the latent topics. Traditional unsupervised topic models, such as Latent Dirichlet Allocation (LDA) [2] and Probabilistic Latent Se-mantic Analysis (PLSA) [8], have been proved to be effec-tive in modeling unlabeled data and could uncover the un-derlying semantic structure of a collection. However, these t opic models are inadequate, as indicated in [20], for model-ing text from different collections for two reasons: (1) The structure of collections will be completely ignored. Con-sequently, the extracted common topics might only repre-sent some, but not all collections; (2) It is hard to identify which topic represents the common information across col-lections and which represents specific information to a par-ticular collection. To bridge this research gap, two variants of topic model were proposed to model multiple text col-lections. The first variant is the Cross-Collection Mixture Model (CCMix), proposed in [20]. It extends the PLSA model by explicitly distinguishing common topics that char-acterize common information across all collections from spe-cial topics that characterize collection-specific information. Common topics and collection-specific topics are aligned un-der the same set of indices and the number of topics in each collection are forced to be the number of common topics. The second variant is the Cross-Collection Latent Dirichlet Allocation model (CCLDA), proposed in [16]. It is further extended from CCMix by replacing the PLSA framework with that of LDA. Thus, it is actually a Bayesian version of CCMix model.

The cross-collection topic models have been successfully applied for many problems. The most direct application is the comparative text analysis. For example, it has been used for comparative text analysis of war news articles [20] and cross-cultural analysis of blogs articles [16]. Besides, they are also adapted for modeling multiple text streams with temporal dynamics [9].

It is also natural to adapt cross-collection topic models for cross-domain text classification, if we treat documents in each domain as a collection. However, as far as we know, there are no such attempts previously. In this paper, we extend the CCLDA model to a supervised version which could be directly applied for cross-domain text classification.
In this section, we first formally define our problem of cross-domain text classification. We then elaborate our pro-posed model and its learning algorithm. Finally, we demon-strate how to use our model for cross-domain text classifi-cation.
As surveyed in [15], there are different settings for cross-domain learning. Here, we focus on the transductive cross-domain learning setting for text classification. We formally define our problem of cross-domain text classification as fol-lows.

Given a source domain D s = { ( x s 1 ; y s 1 ) ; :::; ( x N s labeled documents, and a target domain D t = { x t 1 ; :::; x with N t unlabeled documents, our task is to assign a binary class label y  X  {  X  1 ; 1 } to each unlabeled document x t the target domain D t . We assume that training and testing documents are from related but different domains D s and D , and the feature spaces of source and target domain are different, X s 6 = X t .

Throughout the paper, we use the convention that lower-case letters denote values (e.g. y , z d i , x d i , etc.), and bold letters denote vectors (e.g. x ,  X  ,  X  , etc.). For convenience, we summary the frequently used notations in Table 1. W e now proceed to elaborate our proposed model called Partially Supervised Cross-Collection LDA (PSCCLDA). The goal of our model is to induce a new latent feature space so that we can achieve better performance for cross-domain text classification. On one hand, it is necessary to model documents in each domain as a separate collection because of the distributional difference between domains. To this end, we seek to take advantage of recently proposed CCLDA (Cross-Collection LDA) in [16] to model documents from multiple collections. On the other hand, as mentioned pre-viously, it is crucial to consider the duality between the marginal distribution of examples and the conditional distri-bution of class labels given examples. Specifically, it is the-oretically demonstrated that the induced latent feature rep-resentation should simultaneously minimize the difference between source and target domains and the empirical train-ing error in the source domain [1]. CCLDA is inadequate for this purpose since it is unsupervised and can not explic-itly model the observed response varibales (i.e. class labels) in the source domain. We thus propose our model which extends CCLDA to a supervised version. We call it  X  X ar-tially supervised X  since we only observe the class labels in the training source domain.

Figure 1: Graphical representation of our model. The graphical representation of our model is shown in F igure 1. The corresponding generative process associated is as follows: 1. For each topic z , draw a collection-independent multi-2. For each collection c , 3. For each document d ,
In our PSCCLDA model, we have a set of C collections, each collection c corresponds to a domain in the context of cross-domain learning. Note that our model could be easily applied for multiple source/target domains, although we assume that there is only one source domain and one target domain in our defined problem. The class labels are observed in the source domain collections but not in the target domain collections. Each collection c is associated with a set T S of collection-specific topics, and a set T common topics is shared by all collections. Similar to the models proposed in [16, 9], we assume that the number of elements in all topic sets are same (i.e. | T C | = | T S and the specific topics in different collections are forcibly common topics. This enables the alignment of the unrelated specific topics in different collections under the same topic index. Thus the total number of topics in our model is ( C + 1)  X  | Z | .

Each topic is defined as a multinomial distribution over a fixed vocabulary. Particularly, collection-specific topics  X 
S are drawn from a collection-specific Dirichlet distribu-tion Dir (  X  S ) while the common topics  X  C are drawn from a collection-independent Dirichlet distribution Dir (  X  C ). Each collection-topic pair ( c; z ) is associated with a Bernoulli dis-tribution with parameter  X  which further follows a Beta distribution Beta (  X  ).  X  indicates how likely a word is as-signed to a common topic, and the hyper-parameter  X  is our prior knowledge of  X  . For each word d i in document d , we draw a switching variable x d i  X  Bern (  X  ) which is a binary random variable for choosing collection-specific or common topic. Similar with LDA, each document d has a topic proportion  X  d  X  Dir (  X  c ) over the shared topic indices. Different from CCLDA, our PSCCLDA embeds the logistic regression model to explicitly model the response variables (i.e. class labels) in the training source domain. In partic-ular, each observed class label y d  X  { X  1 ; 1 } is drawn from Bern ( logistic (  X  y d  X  T z d ) ) where logistic ( t ) = logistic function.
Exact inference is often intractable in topic models. In practice, approximate inference methods, such as variational EM algorithm [2] and Gibbs sampling [7], are adopted. To learn our model, we employ a stochastic EM framework (similar to that used in [5, 9]) which incorporates the func-tional optimization problem with Gibbs sampling. Specif-ically, in E-steps, we fix the logistic coefficients  X  , and do Gibbs sampling to sample hidden variables z and x , gather useful counts and update new document representations z d I n M-steps, we update the logistic regression coefficients  X  by maximizing the joint likelihood of all observed data and hidden variables, which is equivalent to minimizing the ob-jective function of associated logistic regression model. In this way, our model follows the theory in [1] to induce the new latent feature representation by explicitly minimizing the difference between domains and the empirical training errors in the source domain.
 E-step:
Let hyper-parameters {  X  c ;  X  C ;  X  S ;  X  } be denoted as  X  , and hidden variables {  X  C ;  X  S ;  X  ;  X  } as  X  . The joint dis-tribution of observed variables and hidden variables, after integrating out  X  , is: Here, B (  X  ) denotes the function B ( v ) = v is a vector and  X (  X  ) is the gamma function. n x c;z is a 2-dimensional vector ( n x =0 c;z ; n x =1 c;z ) whose elements are the number of word tokens d i in collection c which satisfies z n d is a | Z | -dimensional vector where each element is the number of word tokens d i in document d which satisfies z z . n w z;x =0 is a | W | -dimensional vector where each element is the number of word token w assigned to common topic z . n c;z;x =1 is a | W | -dimensional vector where each element is the number of word token w assigned to collection-specific topic z in collection c .

In E-steps, we apply collapsed Gibbs sampling using the following updating rules which are derived based on equation 1 (Derivation is omitted due to space limitation.):  X  1 + e  X   X  Here, the superscript  X  i denotes a counting variable that ex-cludes the i -th word index in the corpus, and the superscript (  X  ) denotes a counting variable that sums over all elements in the corresponding vector. n z i ;  X  i d is the number of word tokens assigned to topic z i in document d , excluding the cur-rent word index i . n x i ;  X  i c signed to common (if x i = 0) or collection specific (if x topic z i in collection c d , excluding the current word index ken w i is assigned to common topic z i and collection-specific topic z i in collection c respectively. We assume that  X  C  X  S takes the identical value  X  C and  X  S respectively. M-step:
In m-steps, we update the logistic coefficients  X  by max-imizing the joint likelihood in equation 1. Since we fix the counts gathered in E-step, this is equivalent to learn a new logistic regression model where each document d is repre-sented by z d n ewly updated in e-step. Specifically, we learn a L2-regularized logistic regression model which solves the following unconstrained optimization problem: where R is regularization parameter and set to 1 : 0 in our model. We apply a trust region Newton method [12, 6] for optimization.
Having described our model and its learning algorithm, we now present how to leverage our model for cross-domain text classification.

The overall procedure is depicted in Algorithm 1. We first learn our model by alternately running E-step and M-step for T em iterations (or until convergence). Each domain is treated as a collection, and the model is initialized randomly. In E-steps, we run Gibbs sampling for T gibbs iterations. Dur-ing sampling, if a document is labeled (i.e. from training source domain), we use the exact update rules in equation 2 and 3 ; if a document is unlabeled, (i.e. from target domain), we use the update rules in equation 2 and 3 by removing the first term containing class label y d . When sampling is finished, we update the empirical topic frequency z d u sing the last sample obtained. In particular, z d i s a 2  X  | Z | di-mensional vector where the first | Z | dimensions correspond to common topics z C  X  { 1 :::Z } in turn, and the remain-ing | Z | dimensions correspond to collection-specific topics z
S  X  { 1 :::Z } in turn. Each element in z d i s the normal-ized frequency of the corresponding topic n z =n d where n the total number of tokens in document d . In M-steps, we update the logistic coefficients  X  according to the equation 4.

When the model is learned, we can directly use the last updated z d o f unlabeled documents and logistic coefficients Algorithm 1 P SCCLDA for Cross-Domain Text Classifi-cation Input: l abeled training data in source domain D s ; unla-Output: predicted class label of each unlabeled document 1: Initialize hidden variables z and x in the model 2: for t := 1  X  T em do 3: E-step: 4: for t e := 1  X  T gibbs do 5: Run collapsed Gibbs sampling for all documents us-6: end for 7: Update topic frequency z d f or each document d 8: M-step: 9: Update logistic regression coefficients  X  using equa-10: end for 11: Predict class label of each document d in target domain  X  f or predicting. Specifically, we predict the class label of unlabeled document d according to the following equation: If p ( y d = 1 | z d )  X  0 : 5 , the predicted class label is 1; other-wise, the predicted class label is  X  1.
In this section, we empirically evaluate the performance of our model for cross-domain text classification on nine datasets that are commonly used in cross-domain learning, and compare it with two baseline classifiers and four state-of-the-art approaches.
To evaluate the performance of cross-domain text classi-fication, nine datasets are generated from two widely used datasets, including 20Newsgroups 1 and Reuters-21578 2 , by utilizing their hierarchical category structures in the same way as many previous studies on cross-domain learning [11, 18, 21, 15, 10, 13]. In particular, suppose we have two top-categories A and B , which contain four sub-categories A ; A 2 ; A 3 ; A 4 and B 1 ; B 2 ; B 3 ; B 4 respectively. The task is defined as the top-category binary classification in the cross-domain context, where we aim to classify documents into either A or B . To generate training source domain, we randomly chosen two sub-categories for each top-category, and merge all documents in the chosen sub-categories (e.g. A ; A 2 ; B 1 ; B 2 ). The documents in the rest sub-categories are merged as testing target domain (e.g. A 3 ; A 4 ; B 3 In this way, the data in the source domain and target domain are related (since they come from the same top-category) but distributed differently (since they come from the differ-ent sub-categories).

Table 2 shows how the nine datasets are generated from 20Newsgroups and Reuter-21578 dataset. Specifically, four h ttp://people.csail.mit.edu/jrennie/20Newsgroups http://www.daviddlewis.com/resources/testcollections top-categories ( c omp , rec , sci , and talk ) in 20Newsgroups are used to generate six datasets and three top-categories ( orgs , people , places ) in Reuters-25178 are used to generate three datasets. We list the sub-categories of 20Newsgroups that are used for generating data. Since there are too many used sub-categories of Reuters-25178 dataset, we do not list them in the table.
 Table 2: Datasets generated from 20Newsgroups and Reuters-21578.

To make the fair comparisons with the results reported in [ 11], we directly use the processed data provided by the au-thors of this work 3 . The statistics of data are summarized in Table 3. In the  X  X nstances X  column, we present the num-ber of instances for both labeled data ( l ) in source domain and unlabeled data ( u ) in target domain. In the  X  X eatures X  column, we show the distributional difference between do-mains where | F s  X  t | is the number of features that exclusively appeared in the source domain, and | F s  X  t | is the number of features that appears in both source and target domain. The butional difference is.
To evaluate our PSCCLDA model for cross-domain text classification, we compare it with two standard classification algorithms, including Support Vector Machine (SVM) im-plemented in [6] and Logistic Regression (LG) implemented
T he first six datasets generated from 20Newsgroups are available on request. The rest three datasets generated from Reuters-21578 are available at http://www.cse.ust.hk/TL/ dataset/Reuters.zip in [6], and four state-of-the-art cross-domain classification methods reviewed in Section 2, including Spectral Feature Alignment (SFA) [14], Topic-bridge PLSA (TPLSA) [18], Collaborative Dual-PLSA (CDPLSA) [21] and Topic Corre-lation Analysis (TCA) [11].

For the standard algorithms SVM and LG, we perform the classification in a traditional way. Specifically, we train the classifier using the labeled documents in the source domain and directly use the trained model to predict class labels of unlabeled documents in the target domain. The parameters are all set to the default values as in [6]. The performance of these two standard classifiers serve as the baselines for our model and the other four competing methods.

For the competing methods, we choose the four state-of-the-art topic modeling based methods, including SFA, TPLSA, CDPLSA, and TCA, which have been demonstrated to be effective in cross-domain learning. Although there ex-ist many other state-of-the-art methods, we only choose the topic modeling based methods because we focus on explor-ing how to extend standard topic models for cross-domain learning in this paper. For all of these four methods, we use the same parameter settings as in the original papers.
To measure the performance of algorithms, we adopt the commonly used metric accuracy which is defined as the pro-portion of correctly classified examples.
Here, we show the performance comparison of our pro-posed PSCCLDA model with all baseline methods, includ-ing two standard classifiers and four competing methods, on all nine datasets. The results are shown in Table 4 where the best performance (measured by accuracy) is in bold font. Since our model depends on a random initialization, we run our model 3 times and report the  X  X ean  X  standard devi-ation X  in column  X  X SCCLDA X . For our model, the param-eters are tuned on  X  X omp vs rec X  dataset and then applied to all the other datasets. In particular, the number of topic indices | Z | is set to 5 and 6 for the datasets generated from 20Newsgroups and Reuters-21578 respectively;  X  is set to (20 ; 1), indicating that the common topics ( x = 0) are more likely to be chosen than collection-specific topics ( x = 1); the number of iterations T em is set to 50 and T gibbs is set to 6. Parameters of all other models are set to their default values as described in previous subsection.

As can be seen in Table 4, our PSCCLDA model per-forms best on 7 out of 9 datasets. Two exceptions are that SFA performs best on  X  X omp vs talk X  dataset and  X  X CA X  performs best on  X  X ec vs talk X  dataset. Not surprisingly, all cross-domain learning methods perform better than the standard classifiers (LG and SVM), demonstrating that the distributional difference between domains indeed deteriorate the performance of standard classifiers. On average, our model outperforms all the other methods with the classi-fication accuracy 0 : 880. We also conduct the t -test with 95% confidence level over all datasets for our model paired with each baseline method, and the tests show that the su-periority of our model over other methods are statistically significant.
In addition to the extraction of common topics, our model could also simultaneously extract and align the collection-specific topics. Here, we study the effect of these collection-s pecific topics on the model performance. Recall that the empirical topic frequency z d i s a 2  X  | Z | dimensional vec-tor where the first | Z | dimensions correspond to common topics while the remaining | Z | dimensions correspond to collection-specific topics. We exclude the dimensions cor-respond to collection-specific topics from z d a nd investi-gate how the performance will change. Specifically, we run model with/without topic alignment three times on all nine datasets and report the mean and standard deviation of clas-sification accuracy in Figure 2. As can be seen, the inclusion of collection-specific topics in z d c an slightly improves the model performance on all nine datasets. This implies that the alignment of specific topics in source and target domains could further improve the model performance in addition to the induced common topics. This finding is consistent with that in [11].
Since we use an EM-style algorithm to learn our proposed model, it is important to show that the performance of the model will converge. Figure 3 presents the model perfor-mance in terms of prediction accuracy by varying the num-ber of iterations. As can be seen, the model performance on all nine datasets increases fast during the first 10 itera-tions. After that, the performance tends to converge and is nearly constant. This result demonstrates that our model could ensure the convergence on all nine datasets used in our experiments.
There are two important parameters in our PSCCLDA model: (1) the number of topic indices | Z | ; and (2) the hyper-parameter  X  which could be interpreted as our prior beliefs on the proportion of domain-independent and domain-specific topics. To investigate the effects of these two param-eters, we show the performance of our model on all datasets by varying one of them meanwhile fixing the other one.
Firstly, we fix the parameter  X  to its default value (20 ; 1), and vary the number of topics from 2 to 20. The results are shown in Figure 4. As can be seen, the performance is relatively stable when the number of topics is larger than 5. This demonstrates that our model is not very sensitive to the parameter | Z | .

Next, we fix the parameter | Z | to 5 and 6 for datasets gen-erated from 20Newsgroups and Reuters-21578 respectively, and vary  X  x =0 from 0 : 5 to 100 while fixing  X  x =1 to 1. The results are shown in Figure 5. As can be seen, the per-formance is relatively stable when  X  x =0 is larger than 20. This demonstrates that our model is not very sensitive to the parameter  X  . It is interesting to notice that the perfor-mance of our model will drop when  X  x =0  X   X  x =1 , indicating that we should assign more weight to common topics than collection-specific topics during the learning of the model.
As demonstrated in [11], the alignment of domain-specific topics, in addition to domain-independent topics, can fur-
Figure 4: Performance by varying number of topics. Figure 5: Performance by varying parameter  X  .
 from  X  X omp vs rec X  dataset. (Topic labels are bold.)
Hardware Graphics and OS Motors and Autos Baseball and Hockey drive mb card disk system mem-o ry mhz video bit dos scsi drivers windows speed using monitor hard cards board pc hardware mac hardware graphics os moto auto baseball hockey ther improve performance for cross-domain learning. In t heir work, the alignment of domain-specific topics is achieved via an additional step to explore the topic correlations. In contrast, our model forces the mapping between topics dur-ing the learning and does not need the additional step.
Here, we conduct a qualitative analysis of extracted top-ics and their alignment. Table 5 presents four collection-independent topics and their corresponding collection-specific topics extracted by our model from  X  X omp vs rec X  dataset. As can be seen, each extracted topic is meaningful and collection-specific topics are well aligned with the correspond-ing common topic. For example, the first common topic in the first column in the table is regarding  X  X omputer hard-ware X . The corresponding collection-specific topic in source domain is regarding  X  X c hardware X  while that in target do-main is regarding  X  X ac hardware. X  As shown in Table 2, the sub-category  X  X omp.sys.ibm.pc.hardware X  X elongs to the source domain while  X  X omp.sys.mac.hardware X  belongs to the target domain. This hidden structure is recovered by our model nicely.
In this paper, we propose a partially supervised cross-collection topic model, called PSCCLDA, for cross-domain text classification. Guided by the theory proposed in [1], our model considers the duality between the marginal distribu-tion and the conditional distribution so that they could re-inforce each other during the learning process. Besides, our model could simultaneously extract and align the domain-specific latent features from different domains with the pur-pose of further improving cross-domain learning. We con-duct experiments on nine datasets which are previously used for evaluating cross-domain text classification. Experimen-tal results show that our model outperforms two baseline methods and four state-of-the-art methods, which demon-strates the effectiveness of our model.

Currently, we embed the logistic regression model in our proposed model for modeling the observed class labels in training data in the source domain. Hence, our model could only be used for binary classification. In the future, we in-tend to extend our model by replacing the logistic regression model with the softmax regression model so that it could be directly used for multi-class classification.
 We gratefully acknowledge the support of an internship grant from the National Institute of Informatics. [ 1] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira. [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [3] J. Blitzer, M. Dredze, and F. Pereira. Biographies, [4] J. Blitzer, R. McDonald, and F. Pereira. Domain [5] G. Doyle and C. Elkan. Accounting for burstiness in [6] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. [7] T. Griffiths and M. Steyvers. Finding scientific topics. [8] T. Hofmann. Unsupervised learning by probabilistic [9] L. Hong, B. Dom, S. Gurumurthy, and [10] J. Jing. A literature survey on domain adaptation of [11] L. Li, X. Jin, and M. Long. Topic correlation analysis [12] C. Lin, R. Weng, and S. Keerthi. Trust region newton [13] M. Long, J. Wang, G. Ding, W. Cheng, X. Zhang, and [14] S. Pan, X. Ni, J. Sun, Q. Yang, and Z. Chen. [15] S. Pan and Q. Yang. A survey on transfer learning. [16] M. Paul and R. Girju. Cross-cultural analysis of blogs [17] I. Titov. Domain adaptation by constraining [18] G. Xue, W. Dai, Q. Yang, and Y. Yu. Topic-bridged [19] J. Yoo and S. Choi. Probabilistic matrix [20] C. Zhai, A. Velivelli, and B. Yu. A cross-collection [21] F. Zhuang, P. Luo, Z. Shen, Q. He, Y. Xiong, Z. Shi,
