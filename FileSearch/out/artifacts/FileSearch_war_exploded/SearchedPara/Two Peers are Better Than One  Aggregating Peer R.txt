 Scientific peer review, open source software development, wikis, and other domains use distributed review to improve quality of created content by providing feedback to the work X  X  creator. Distributed review is used to assess or improve the quality of a work (e.g., an article). However, it can also provide learning benefits to the participants in the review process. We developed an online review system for begin-ning computer programming students; it gathers multiple anonymous peer reviews to give students feedback on their programming work. We deployed the system in an intro-ductory programming class and evaluated it in a controlled study. We find that: peer reviews are accurate compared to an accepted evaluation standard, that students prefer re-views from other students with less experience than them-selves, and that participating in a peer review process results in better learning outcomes .
 K.3.1 [ Computers and Education ]: Computer Uses in Education X  Collaborative learning Design, Experimentation collaboration, peer review, education
When one of Wikipedia X  X  thousands of volunteer editors examines a recently changed article, that volunteer improves the overall quality of the work by participating in a dis-tributed review process. Domains such as scientific peer re-view, industrial code review, open source software develop-ment, and information filtering on sites like Slashdot, Digg, or reddit also employ distributed review. We posit that the concept may improve education, too: student work (assign-ments) may be distributed to a set of reviewers who assess the work and provide formative feedback to the student. In an educational setting, another  X  and perhaps, central ben-efit  X  of distributed review is enhancing student learning. This paper systematically evaluates the use of distributed peer review in an in an introductory computer programming class.

Prior work has explored this subject. However, previous study of distributed review has left critical questions unan-swered. First, there have been no controlled studies demon-strating the benefits of distributed peer review in introduc-tory programming classes. Second, using distributed peer review in an educational setting raises a number of crucial challenges, such as preserving student privacy and avoiding  X  X ogue X  (e.g., retaliatory or collusive) reviews. This paper examines these important issues. We developed a system to facilitate the distributed review process for an introductory programming class. We then conducted a controlled, quan-titative study to determine if distributed peer reviews are accurate and effective, to determine the impact the various activities (reviewing, being reviewed) had on learning out-comes, to assess the acceptability of peer review to students, and to identify the extent to which problems like rogue re-views occurred.

Today X  X  educators face the challenge of delivering high-quality, individualized feedback to increasingly large classes. The traditional approach to this problem  X  using the same teaching and assessment techniques and requiring staff to cover the extra work  X  does not scale. This challenge forces us to look for new instructional and assessment techniques: hence, our interest in peer review.

Distributed peer review differs from traditional group pro-gramming projects because it uses a different time, different place learning environment. This paradigm may prove bene-ficial because computer programming assignments and long papers lack a structure conducive to same place, same time group learning; in fact, they have been identified as among the worst possible group assignments [28]. We view dis-tributed review as an alternate form of collaborative class-room activity. Since it is used successfully in a variety of real world contexts, we posit that it may work well in a classroom setting.

We organize our work around the following research ques-tions: 1. Accuracy of Peer Reviews: How accurate (relative 2. Review Effectiveness: Do students consider peer 3. Impact on Learning Outcomes: How does partic-
In the remainder of this paper, we discuss related work, describe our peer review system, discuss our experimental design, present and discuss our results in detail, and close with a brief summary.
This section presents related work in 2 parts: related re-search and existing system implementations.
Previous research examines distributed review in a num-ber of domains, including online communities. Lampe et al [21] demonstrate the basic utility of distributed review on Slashdot: they explore distributed comment moderation and find that distributed reviewers promote high quality com-ments and filter low quality ones. Cosley et al [6] study a more specific facet of online community distributed review: they examine the timing of the review process and conclude that the end result of post-review is just as good in terms of quality as pre-review. Like Lampe et al [21], we study the general utility of distributed review, but we do so in a classroom setting rather than in an online community. We take a broader view than Cosley et al [6] as we look at the end-to-end review process, rather than focusing the effects of review timing.

A number of researchers study the benefits of peer review in the classroom. Topping presents a comprehensive sur-vey of peer review techniques in higher education [26], while Bloom et al. [2] and Krathwohl et al. [20] posit that because evaluation and criticism are high level skills in Bloom X  X  Tax-onomy, students who participate in a peer review process achieve a high order educational goal. Hamer [17] summa-rizes benefits to using peer review in the classroom, including increasing the quantity and timeliness of feedback, exposing students to different techniques and styles, and encouraging reflection on the course objectives through the review task.
Sullivan and Collofello [25], [5] and Gehringer [9], [10], [11], [12] also examine peer review in the computer pro-gramming classroom. While Gehringer cites a number of benefits to using peer review as a classroom exercise, neither he nor other researchers perform a systematic quantitative evaluation of the accuracy (with respect to a standard) or of learning benefits of peer review as we do. Trytten proposes a design for team peer code review [29] and proposes that the structure of the review exercise is nearly as important as the exercise itself. We agree, and we incorporate this structured approach into our electronic system for facilitating the re-view process. Denning et al [7] examine in-class peer review for computer programming classes. They create a model for completing reviews quickly in a time-constrained class (a  X  X ame time, same place X  model). We instead employ a  X  X ifferent time, different place X  collaboration model that al-lows reviewers to complete their reviews at a time and place convenient for them.

One potential concern about using peer review in the classroom is shilling or rogue reviewers  X  those reviewers who, for a variety of reasons, give arbitrary scores regard-less of submission quality. Hamer et al [17] propose an algo-rithm for automatically calibrating peer review scores, how-ever they evaluate their algorithm on simulated data only. Here, we apply the algorithm to actual data.
 In addition to the work specifically mentioned above, Anewalt [1], Hamer [16], Wolfe [30], Trivedi [27], Silva [24], Liu [22], and Gotel [14], [15] all present research that de-scribes the design and implementation of systems and pro-cesses for peer review in computer programming courses. However, much of the previous work focuses on recounting lessons learned from the implementation of the systems and processes. Here, we design and conduct a controlled study designed to determine the accuracy of peer review, what makes a review effective, and the impact of the peer review process on learning.

Beyond distributed review, there are other methods for assessing the quality of work in a classroom setting. For example, Foltz [8] proposes a technique for automatically scoring essays using semantic analysis. However, it is not clear if this technique can be applied to computer program-ming or if it is widely used in the classroom.
A number of web-based systems facilitate distributed re-view. MyReview (myreview.lri.fr), ConfTool (www.conf tool.net), and Precision Conference (www.precision conference.com) support peer review for academic confer-ences. Peer Grader (PG) was an early web-based system for submitting of computer programming assignments for peer review [10]. In a later iteration, PG added resources for teaching computer architecture [11]. Later, it evolved into Expertiza, which used student peer review to improve an unpublished textbook [13]. It is currently in classroom use at North Carolina State University.

We considered using one of the aforementioned review platforms for our research. However, they left key require-ments unmet, for instance: (1) the existing classroom sys-tems were domain independent and did not support pro-gramming specific concepts like test cases and code char-acteristics (see below for definitions). Since evaluating test cases was a requirement for our peer review system, we built our own. (2) We wanted assignment authors to provide structured feedback for a review; we did not find any ex-isting tools that did this. (3) Many systems we evaluated are hosted at external web sites. Because we conducted our study in a classroom, we had to meet strict data privacy requirements, both legal (as specified in the United States Federal Educational Rights Privacy Act) and institutional (as specified by our University rules). Therefore, we decided to implement our own peer review application. Figure 1: Flowchart illustrating the submission and review process.
This section gives an overview of the application design and the process it supports.

Users (students) play two roles in the peer review pro-cess. Submission authors (hereafter called authors) create and submit solutions to programming assignments. Review-ers create structured reviews of assignments submitted by authors. In our study, students were assigned either, both, or neither of these roles.
 The peer review process consists of the following steps (see Figure 1): 1. Students submit their assignments via our web-based 2. The application assigns each submission to be reviewed 3. The application notifies all reviewers that the review 4. The TA and student reviewers complete and submit 5. The application notifies the submission author after all 6. The submission author then rates each review and pro-
The review assignment algorithm is a simple random al-gorithm with 2 obvious constraints: 1) students may not re-view their own submissions, and 2) no student may review the same submission more than once.

The reviewer provides feedback on the programming as-signment through test cases and code characteristics.
Test Cases. Test cases measure the performance of the submission against the assignment requirements. For ex-ample, one requirement may be to sum two numbers. A test case for this requirement may invoke the sum function with the numbers 5 and 10 and verify that the result is 15. One requirement may give rise to many test cases. Our application requires reviewers to write a minimum number of test cases (initially 3; later increased to 5 as assignment complexity increased). However, reviewers were allowed to write more. For each test case, the reviewer indicated an outcome from the following choices: does not compile, fa-tal error/crash, incorrect result(s), or correct result(s). We used the rankings to measure performance on test cases.
Code Characteristics. Not all aspects of a computer program can be assessed via test cases. For example, de-termining whether an interface design is  X  X ood X  or  X  X ad X  and whether documentation is sufficient require human judg-ment. Therefore, the application also lets reviewers rate the submission X  X  code characteristics including formatting, ef-fective use of conventions, documentation, interface design usability, and modularity.

For each code characteristic, the reviewer indicates his or her level of agreement with a statement about the charac-teristic using a 5-point Likert scale. For example, for rating documentation, the reviewer sees these statements:  X  X he code is well documented (commented). The comments are written in the problem language and will be easy to main-tain as the program changes over time. Comments are used judiciously. X 
The reviewer also can  X  and, in our study, was encouraged to  X  provide text comments for each test case and code char-acteristic. See Figure 2 for examples. We combined the out-comes of the test cases with the numerical rating of the code characteristics to produce a quantitative review score. The review in Figure 2 resulted in a score of 94/100. This score plays an important role in subsequent analysis; notably, it lets us compare student and TA reviews quantitatively.
Author Feedback to Reviewer. Upon receiving their review(s), the application required authors to give feedback on the review by rating and commenting on four aspects of the review: accuracy, helpfulness, reviewer knowledge, and fairness. These feedback ratings provide a mechanism for analyzing the effectiveness of different author/reviewer pairings. Figure 2 shows a typical completed review with feedback from the submission author.
We performed a controlled study to answer our research questions in the Fall of 2008. An introductory Information Systems class served as the backdrop for our study. The first author of this paper was the instructor for the course (which he had taught once previously.) To review, we summarize our research topics: Research Question 1 : we compare student reviews to those of the TA (the  X  X old standard X ); Research Question 2 : we measure re-view effectiveness (helpfulness); Research Question 3 :we measure the impact of participating in peer review on learn-ing outcomes.

We used a 2  X  2 experimental design. The two factors were writing reviews (yes or no) and receiving reviews (yes or no). Table 1 introduces names for each of the four re-sulting groups  X  G-control , G-review , G-receive ,and G-both  X  that we use throughout the paper. This exper-imental design let us test separately the effect of receiving and writing reviews as well as any added effect of doing both. We wanted the four groups of students each to be represen-tative of the course as a whole in terms of incoming grade point average and prior programming experience. However, we did not want to prejudice the course staff and researchers about student performance. Therefore, we gave instructions for creating groups to our university X  X  Office of Institutional Research, and they assigned students to groups. 51 students began the semester and 45 students completed it. This is a low drop-out rate for this course: for many students, this is the first course in their major (Information Systems), and some leave if they find it too challenging or not to their interest. Of the 51 initial students, 25 were in groups that received peer reviews ( G-Receive and G-Both ) and 25 were in groups that wrote peer reviews ( G-
Group Receives? Writes? Initial G-Control N N 13 13 G-Review N Y 13 10 G-Receive Y N 13 11 G-Both Y Y 12 11 Table 1: Our 2  X  2 design for analyzing how the differ-ent learning activities (reviewing, being reviewed) impact learning outcomes along with the initial and final sizes for each group.
 Review and G-Both ). Table 1 shows initial and final sizes for each group.

All students in the course used the review software to sub-mit six programming assignments. Students in G-Review and G-Both each reviewed 1 to 3 submissions for each of the six assignments. Students in G-Receive and G-Both received up to 3 peer reviews for each assignment. Having up to 3 student reviewers for each submission gave authors multiple sources of feedback and let us test how using dif-ferent number of peer reviews (one, two, or three) affected accuracy. Students in G-Control did not write reviews and did not receive peer reviews. However, like all students, they received reviews from the course Teaching Assistant (TA). Authors did not know whether a review came from the TA or a fellow student.

To measure the accuracy of student reviews, we used re-views from the TA as the  X  X old standard X . The TA was a Computer Science Master X  X  degree candidate with significant industry programming experience. Of course, TA grades are not a foolproof or an absolute measure of submission qual-ity; nonetheless, they are the accepted standard for course work at our University and generally throughout higher ed-ucation.

To measure student opinions about the process and to in-form the quantitative results, students completed pre-study and post-study surveys. The pre-survey was the same for all participants. We prepared somewhat different versions of the post-survey for each of the four study groups.
To ensure fairness to the students, assignment scores were based solely on the TA X  X  review. In addition, because our school has a median grading policy, final letter grades for the students were assigned with respect to their study group and not the class as a whole. This is because of the potential that review activities could lead to different learning outcomes for each group (which we report on below).
We organize the discussion of our results around our three research questions.
During the study semester, we collected 378 individual peer reviews for the six programming assignments. We ana-lyzed the accuracy of the reviews under several aggregation schemes.

Students rated submissions similar to the TA, but more harshly. We calculated the quantitative score for each review and then compared the scores of the TA and student reviews. We calculated the score by giving the test cases a 75% weight and giving the code characteristics a Figure 3: The correlation of of student review scores to those of the TA increases as you increase the num-ber of student reviewers.  X  X amer X  is the result of applying Hamer X  X  calibration algorithm. Note the y axis does not start at 0. 25% weight. If we correlate the TA score with each individ-ual student score, we get a Pearson coefficient ( r ) of 0.679, which is marked correlation. 24.6% of student review scores (93 out of 378) were higher than the TA X  X  score (for the corresponding author and assignment), 17.2% of student re-view scores (65 out of 378) were equal to the TA X  X  score (for the corresponding author and assignment), and 58.2% of student review scores (220 out of 378) were lower than the TA X  X  score. On average, student review scores were 2.6 points (out of 100) lower than the TA X  X  score.

Aggregating multiple student reviews improves ac-curacy. On average, the correlation between the mean of any two student review scores and the TA X  X  score (for the same assignment and author) was 0.750. Taking the mean of all three student reviews improved r even further to 0.769. Smart aggregation improves accuracy even more.
 A simple average of student review scores may not be the best technique. In addition, student reviewers could adopt a number of rogue strategies. For instance, they may give every submission a perfect score out of laziness, or they may be particularly aggressive in trying to  X  X reak X  the author X  X  program by giving it input they know will never be pro-cessed correctly. To avoid such problems, Hamer et al [17] designed an iterative algorithm for automatically calibrating peer review scores. Applying Hamer X  X  algorithm to our data improves the correlation to 0.787. The improvement is min-imal over the naive approach (3-student mean). However, Hamer X  X  algorithm provides additional information about the quality of student reviews that can be used to provide an incentive for students to write good reviews. In particu-lar, it computes weights that indicate each reviewer X  X  overall agreement with the consensus of all reviewers. These weights may provide a mechanism for rewarding the most accurate reviewers. (Note that reviewing systems for academic con-ferences sometimes present such data to reviewers as a self-assessment aid.) The algorithm also provides a mechanism for detecting potential plagiarism.

Figure 3 summarizes the correlation improvement as we aggregate more student scores and then apply Hamer X  X  al-gorithm.
Survey . The survey provided additional insight into the accuracy of peer reviews. 15 of 21 student reviewers agreed that they had the competence necessary to review other stu-dents X  work. In addition, 17 of 21 student reviewers indi-cated that they reviewed other students X  work fairly. Stu-dents receiving the reviews seemed to agree: 15 of 22 stu-dents who received peer reviews indicated the other stu-dents X  reviewed their assignments fairly, and 13 of 22 stu-dents who received peer reviews indicated that the other students had the competence necessary to do the reviews. Only 4 of the 22 students who received peer reviews indi-cated that, based on their experience in this course, they would not want to take another course that used peer re-view.
Our results showed that multiple peer reviews were accu-rate proxies for an individual expert review, even when the peers were novices. Previous research found that multiple peer reviews can even outperform individual expert review in certain situations [3]. Using peer reviews as a means to improve work quality and enhance learning seems relatively unproblematic. And the thought that peer reviews could be used to reduce the amount of work required from a teaching assistant is appealing. However, the last step  X  using peer reviews in computing course grades  X  raises some significant challenges, notably: rogue (dishonest or poor quality) re-views and comprehending or questioning grades. We now discuss this issue.

Taming the rogue review: incentives for honest re-views . In any system where participants review each other, there are a number of factors that may lead to untrustworthy or  X  X ogue X  reviews. Using peer reviews for grading purposes raises more factors. Here are the main reasons we consider: 1. Retaliation  X  participant X gives an undeserved nega-2. Collusion  X  two participants agree to give each other 3. Competition  X  participants compete for some limited 4. Laziness  X  it takes effort to write a good review. A stu-
We anticipated several of these situations and tried to cre-ate incentives to head them off. First, students who wrote reviews (i.e., groups G-Review and G-Both ) were required to submit their reviews before receiving their assignment score. This was designed to ensure timely completion of reviews. Second, authors rated each review they received. While we did not use these ratings in computing reviewers X  course grades, they could have been used for this purpose. This would be a strong incentive for writing honest and sub-stantive reviews. Further, the quality of a review could be estimated objectively, e.g., based on the amount of text and the distance between its score and the average score of all reviews. Finally, to try to prevent  X  X aziness X  due to the ef-fort of reviewing, we explicitly considered writing reviews as a course activity, adjusted the course workload accordingly, and communicated this to students.

In practice, we saw little or no evidence of rogue review-ing. A few students did express worries. Some reviewers said they were concerned that authors might retaliate if they (the reviewers) gave low scores. Four or five reviewers told the instructor that they suspected receiving retaliatory ratings in such cases. We did analysis to try to quantify whether retaliation was occurring; specifically, we correlated review scores and corresponding review ratings from the submission authors. The correlation was weak ( r =0 . 216). We also dis-covered several instances of collusion where pairs of students realized that they had received each other X  X  assignments to review and then agreed to give each other positive reviews and ratings. Collusion could be prevented by a number of methods, such as an algorithm that provides for specifying a minimum transitive distance between author/reviewer pairs. This is a likely candidate for further research.

Despite little evidence of rogue review, this issue clearly requires more work if peer reviews are to be used in assigning grades. Algorithms (like Hamer X  X ) can help identify rogue reviews. More importantly, inventive systems could prevent rogue reviews in the first place.
 Comprehending aggregated peer review grades.
 Under a traditional grading system, when students don X  X  understand or agree with their grades, they go complain to the TA or instructor. However, what should they do if their grade is based on some mechanical aggregation of scores from multiple anonymous peers?
Our post-survey illustrates that students are concerned about this issue. While only 3 of 22 students reported that they had problems raising grading issues with the TA, 10 of those same 22 students expressed concern that they would have no way to dispute or appeal peer review scores. In addition, students who received peer reviews disliked the idea of using them as a grade determinant: 15 of the 22 stu-dents disagreed with the statement,  X  X eceiving reviews from 3 other students on my programming assignments would be an acceptable substitute to receiving an assessment from the TA. X 
Next, while TA assessments are not perfect, replacing or complementing them with a complex aggregation of peer reviews may not provide sufficient understanding of why they received a particular score. While students compre-hend written feedback from the TA (e.g.  X  X ou lost 5 points because you did not do x X ), receiving multiple peer reviews from peers along with an aggregate score may leave the stu-dents without a clear understanding of what caused them to lose points or what actions they can take on a future assignment to improve.

Explanation systems might improve the comprehensibil-ity of grades based on peer reviews. Explaining decisions reached by computational systems in order to make them more acceptable to people is a well-established research topic. Herlocker et al. [18] developed and evaluated a number of explanation interfaces for recommender systems. More rel-evant to our concerns is the work of Clancey [4]. He showed how it wasn X  X  enough for a medical expert system to make accurate diagnoses; it also had to convince physicians that these diagnoses were right. It turned out that to compute convincing explanations, it wasn X  X  enough to just describe the expert system X  X  reasoning process; the reasoning pro-cess had to be re-designed so that it was comprehensible to people. Likewise, we will need explanations along with aggregation algorithms for peer review systems to be accept-able for classroom use.
On the post-survey, 14 of 22 student authors (i.e., mem-bers of groups G-Both and G-Receive ) reported receiving 3 or more effective reviews over the course of the study. To further understand review effectiveness, we carried out anal-ysis (described below) based on the ratings authors gave to the reviews they received. Recall that section 3 detailed the rating criteria: accuracy, helpfulness, reviewer knowledge, and fairness.

Using peer review in a classroom setting raises the ques-tion of whether students  X  who are just learning the material  X  are qualified to review the work of other students. A rea-sonable intuition would be that the more students know, the better reviewers they would be. Students seemed to share this intuition: the survey results showed that stu-dents ranked  X  X  reviewer with more experience than me X  as the second most important factor in judging review qual-ity. However, our results were not consistent with students X  intuitions: in fact, students preferred reviews from other students with similar experience.

We computed these results using authors X  ratings of re-views and the pre-survey. The pre-survey asked students to rate their own programming experience and to indicate the number of prior computer programming courses they had taken. We then divided all the review-ratings (i.e., au-thors X  rating of reviews) into four sets, based on the relative self-reported experience of the author and reviewer: ratings of the reviews they received into four groups based on the relative experience level of the reviewer and author: An ANOVA shows a statistically significant difference in rat-ings between the four groups ( p&lt; 0 . 01). Follow-up pair-wise Tukey tests show that all differences between groups were significant, with the exception of the Author and TA pair. Two major observations can be drawn from these re-sults. First, students X  self-reported preference for reviews from more experienced peers was not supported; in fact, stu-dents liked reviews from more experienced peers least of all. Second, students liked reviews from less experienced peers as much as reviews from the course TA. While the TA almost Figure 4: Author ratings of reviewers were divided into groups based on self-reported experience lev-els. Reviewers with more experience than the au-thor fared the worst. Note the y axis does not start at 0. certainly had a significantly higher experience level than all students, note that the TA has received training on writ-ing effective reviews and his or her continued employment at the school depends on doing the job effectively. Figure 4 shows the average ratings by group based on self-reported experience levels.

However, we wondered whether students did not accu-rately assess their experience level on the pre-survey. Or, put another way, perhaps self-rated experience level doesn X  X  cor-relate well with programming knowledge . Additional data analysis lets us investigate the effect of differential program-ming knowledge more objectively. The idea is to take final course performance (grade) as a measure of knowledge. We define P r to be the overall course performance of the re-viewer and P a to be the overall course performance of the author. Then  X  P = P r  X  P a represents the difference in course performance between the reviewer and the author (a proxy for differential knowledge). A positive  X  P repre-sents a case where the reviewer outperformed the author, and a negative  X  P represents a case where the author out-performed the reviewer. Figure 5 shows a regression of the review rating vs. this performance differential. While there are lots of data points ( n = 1412) that are fairly scattered ( r 2 is relatively low), the downward trend is obvious.
According to students, the first and third highest ranked factors contributing to an effective review were the quality and quantity of written feedback. Quality is subjective and thus hard to quantify. However, we did check whether the quantity of written feedback in a review correlated with the author X  X  rating. It did not ( r =  X  0 . 073 ,p =0 . 172).
Discussion. Students X  general intuition  X  that they want reviewers from more knowledgeable peers  X  did not correlate with their actual ratings of the reviews they received  X  they rated reviews from (apparently) less knowledgeable peers more highly. However, while we found this surprising, other research suggests that perhaps we should not have. Hinds identified a  X  X ognitive handicap X  that experts have when at-tempting to identify with the skill set of a novice [19]. Cho examined the impact of novice peer-based knowledge refine-ment in knowledge management systems [3]. In both cases, Figure 5: A regression of review ratings vs. per-formance differential. A high performance differ-ential indicates the reviewer significantly outper-formed the author in the course. Note the y axis does not start at 0. the researchers found fellow novices to be as good, if not bet-ter, than experts at performing certain tasks that involved assessing the quality or predicting the performance of other novices. Finally, Zhang et al [31] also speculate that ex-pertise networks based on the users with highest absolute expertise may encounter some strain. They propose a  X  X ust better X  model that balances the network using a hierarchical approach where users X  questions are answered by other users with slightly more experience than themselves. This model might provide the basis for an improved classroom review system.
To measure learning outcomes, we calculated an aggregate score (out of 100%) for each student across all assignments and exams. The average score for for the control group was the lowest of the four groups (87.7%); the average score for students who both wrote and received reviews ( G-Both was highest (95.2%). An ANOVA showed that the difference between the groups was not significant, although there was astrongtrend( p = . 1).

We also hypothesized that students who acted as peer reviewers would perform better in the course because (a) seeing other students X  approaches to the same programming assignments would help reviewers learn other paths to a vi-able solution, and conjectured that this knowledge, which one cannot get from TA-grading only or being reviewed by peers only, would prove valuable. (b) Assessing other students X  work requires significant cognitive effort: review-ers had to determine whether their peer X  X  work was cor-rect, and have some understanding of why it was right or wrong. We also thought this would boost learning. To test our hypothesis, we performed a t-test, comparing stu-dent performance in G-Review and G-Both (reviewer con-ditions) vs. performance in G-Control and G-Receive (non-reviewer conditions). Reviewers (mean performance 93.406) performed better than non-reviewers (mean perfor-mance 88.78), and the differences were statistically signifi-cant (p = 0.04, t=2.1167, df 43, std err of difference 2.185).
We also wondered whether receiving feedback from peer Figure 6: An interval plot of course performance vs. study group. Group 1 is the control group ( G-Control ) that did not participate in any peer review activities, while group 4 both wrote and received reviews ( G-Both ). Note the y axis does not start at 0. reviewers would boost performance, so we performed a t-test comparing students who received peer reviews verses those who did not, but differences were not statistically significant.
Figure 6 shows an interval plot illustrating the learning outcomes for each group.

Discussion. The study survey offered further support for the idea that participating in peer review activities en-hanced learning. Several students noted that the process of reviewing others X  work was surprisingly informative, indicat-ing that they would often look at the submissions they were assigned to review for ideas of how to improve their own programming. When asked what they learned by reviewing the work of other students, responses included: Overall, eight of 20 student reviewers agreed that they learned from reviewing other students X  work, and five of 20 dis-agreed. These findings suggest there is potential for peer re-view to enhance learning in computer programming classes.
This paper presents promising results concerning peer re-view for learning computer programming. We showed that peer reviews were accurate compared with a reasonable stan-dard (a TA with industry experience). We also showed that students find reviews from other students with less experi-ence to be most effective. Finally, we showed that reviewing other students X  programming assignments increases overall learning outcomes. We also identified key issues for future research, including developing incentive systems for elicit-ing honest reviews, algorithms for matching reviewers with authors, and review aggregation algorithms that are com-prehensible to students. These findings illustrate the vast potential for distributed review in the classroom and be-yond. [1] Anewalt, K. Using peer review as a vehicle for [2] Bloom, B., Englehart, M. D., Furst, E. J., Hill, [3] Cho, K., Chung, T. R., King, W. R., and [4] Clancey, W. J. From guidon to neomycin and [5] Collofello, J. S. Teaching technical reviews in a [6] Cosley, D., Frankowski, D., Terveen, L., and [7] Denning, T., Kelly, M., Lindquist, D., Malani, [8] Foltz, P. W., Laham, D., and Landauer, T. K.
 [9] Gehringer, E. Strategies and mechanisms for [10] Gehringer, E. F. Electronic peer review and peer [11] Gehringer, E. F. Electronic peer review builds [12] Gehringer, E. F., Chinn, D. D., Manuel A.
 [13] Gehringer, E. F., Ehresman, L. M., and Skrien, [14] Gotel, O., Scharff, C., and Wildenberg, A.
 [15] Gotel, O., Scharff, C., and Wildenberg, A.
 [16] Hamer, J., Kell, C., and Spence, F. Peer [17] Hamer, J., Ma, K. T. K., and Kwong, H. H. F. A [18] Herlocker, J. L., Konstan, J. A., and Riedl, J. [19] Hinds, P. J. The curse of expertise: The effects of [20] Krathwohl, D. R., Bloom, B. S., and Masia, B. [21] Lampe, C., and Resnick, P. Slash(dot) and burn: [22] Liu, E. Z.-F., Lin, S., Chiu, C.-H., and Yuan, [23] Resnick, P., Kuwabara, K., Zeckhauser, R., and [24] Silva, E., and Moreira, D. Webcom: a tool to use [25] Sullivan, S. L. Reciprocal peer reviews. In SIGCSE [26] Topping, K. Peer assessment between students in [27] Trivedi, A., Kar, D. C., and [28] Trytten, D. Progressing from small group work to [29] Trytten, D. A. A design for team peer code review. [30] Wolfe, W. J. Online student peer reviews. In CITC5 [31] Zhang, J., Ackerman, M. S., and Adamic, L.

