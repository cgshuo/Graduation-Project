 Event tracking is the task of discovering temporal patterns of popular events from text streams. Existing approaches for event tracking have two limitations: scalability and inability to rule out non-relevant portions in text str eams. In this study, we propose a novel approach to tackle these limitations. To de monstrate the approach, we track news events across a collection of weblogs spanning a two-month time period. H.1.0 [ Models and Principles ]: General Algorithms, Performance, Design, Experimentation, Theory LDA, topic models, relevan ce models, event tracking The task of Event Tracking is about discovering temporal intensities of events in text streams such as weblogs or newswires. The discovered temporal patterns reveal useful information about the behavior of the various topics in the data sets. For instance, they could show which popular news even ts are interesting to bloggers, and which ones are not. They could also indicate starting and ending points of events (or at least their discussions), as well as prime times when the events are intensively discussed. More over, discovering the temporal intensities of events is an important beginning for various further analyses, such as extracting relationships among events and news summarization. One of the issues of the tracking problem is scalability. Recent work on this direction [2] use variants of probabilistic topic models [1] to infer intensity of each event or topic documents. However, inference algorithms of these topic models often require hundreds of scans over the dataset. Tracking specific events is also hindered by the fact that documents in the text st ream (e.g. blog posts) often contain information that is non-relevant to the events of interest, such as personal stories. In this study, we propose a two-phase framework for tracking a set of given popular events. In the first phase, for each given event e we build a relevance model p(w|e k ) , which determines probability of observing a word w in documents relevant to the event e second phase, we scan through documen ts in the dataset. We use the relevance models estimated in the first phase to extract relevant terms from the documents, and then compute intensities of the events at different time stamps . So, our approach rules out non-relevant portions in the documents and takes only one scan over the dataset to track the events. In order to estimate the event relevance models, we first obtain a training set of documents using a te xt search engine Lucene [4]. For each event, we make a query of several (10 on average) keywords describing the event. Next we use the queries to retrieve the top 100 documents, and use thes e pseudo-relevant documents as training data to build the relevance models for the events. Unlike previous relevance models, our model supports a crucial fact that a document relevant to an event e k could still talk about topics other than e Specifically, each token in document d in the training set of event e is hypothesized to be generated by one of three topics: the e which the document is relevant, a background topic b representing the general language, and a third topic t_o(d) responsible for generating themes other than e k also mentioned in d (Fig. 1). Intuitively, tokens from words frequently appearing in most training sets are likely generated by th e background topic, tokens from words frequently appearing in the training set of an event e the other training sets are likely generated by this e from words frequently appearing in only a particular document d in extract the really relevant portions and rule out non-relevant portions generated by b or t_o(d) in training documents. Only the really relevant portions contribut e to the estimation of relevance model p ( w | e k ). The three components above are inferred by Gibbs sampling, which is formally described in [1, 3]. Given the relevance model for each event e k , we compute the intensity of this event at each time t with window size s as in (1). The intensity is essentially the log-likehood of the event in documents in the time period [t, t+s]. For each document, we use a threshold to rule out word tokens non-relevant to the event and sum over the relevant word tokens in the document (See formula 2). We add the term log(threshold) to normalize the log-likelihood and yield positive values. We demonstrate our model on a data set provided by weblog indexing service Spinn3r [5]. Us ing the inlink counts provided by Spinn3r, we collected a subset of one million of the most popular blog posts. Ten news events are c hosen to track over a two-month span (August and September 2008). These events are handpicked from the list of popular news events on Wikipedia [6]. The results are displayed in Fig. 2. Each topic peaks in intensity around the time of the event. For example, the Beijing Olympics st arted on August 8, and increased in popularity over the next few weeks. Three major Atlantic hurricanes struck over a two-week period in late August (Gustav  X  August 25, Ike  X  September 1, and Hannah  X  September 7). Additionally, the announcements of Presidential running mates of the two major political parties and their National Conventions occurred from August 23 through September 4, demonstrating a relative increase in intensity of US Presidential Election topic during those times. Sub-event tracking provides a more detailed look at what has happened within a particular event. For instance, given the temporal pattern of the event US Presidential Election in Fig. 2, users might be interested in seeing temporal trends of each party X  X  convention. To achieve this goal, we appl y relevance-based topic model (Section 2.1) on training sets for the two sub-events. Table 2 shows the ten most representative words for each sub-event. It is worth emphasizing that at sub-event leve l, the highly frequent words about the event such as convention, poll, vote, and campaign, should no longer play a significant role in representing sub-events. In our model, the background topic b will cover those words and the relevance model of each sub-event focuses on unique word features of the sub-events . So, by varying the specificity of the background topics, our approach allows us to track topics hierarchically.
 Given the relevance models for th e two sub-events, we use tracking method in Section 2.2 to track the sub-events. The results are shown in Fig. 3. We see that the peaks of the lines correspond to the dates of the conventions (August 25 X 28 for DNC and September 1 X 4 for the RNC), allowing clear separation of the US Presidential Election topic into subtopics occurring during each party X  X  convention. In this paper, we propose a novel approach for event tracking that overcomes issues of both scalability and inability to exclude non-relevant portions of documents. To demonstrate the approach X  X  efficiency, we hierarchically track popular news events with different levels of granularity over one million weblog documents spanning two months. The reported results confirm that our approach is able to extract key temporal patterns in news events. [1] Blei, M., Ng, A., Jordan, M., Latent Dirichlet Allocation , [2] Mei, Q., Zhai, C., Discovering Evolutionary Theme Patterns [3] Ha-Thuc, V., Srinivasan, P. Topic Models and a Revisit of [4] Apache Lucene. h ttp://lucene.apache.org/ [5] Spinn3r. http://spinn3r.com/ [6] Wikipedia. http://www.wikipedia.org/ 
