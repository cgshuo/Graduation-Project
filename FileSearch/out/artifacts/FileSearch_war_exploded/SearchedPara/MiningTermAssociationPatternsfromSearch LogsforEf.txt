 Searc h engine logs are an emerging new type of data that o ers interesting opp ortunities for data mining. Existing work on mining suc h data has mostly attempted to dis-cover kno wledge at the level of queries (e.g., query clusters). In this pap er, we prop ose to mine searc h engine logs for patterns at the level of terms through analyzing the rela-tions of terms inside a query . We de ne two novel term asso ciation patterns (i.e., con text-sensitiv e term substitu-tions and term additions) and prop ose new metho ds for mining suc h patterns from searc h engine logs. These two patterns can be used to address the mis-sp eci cation and under-sp eci cation problems of ine ectiv e queries. Exp eri-men t results on real searc h engine logs sho w that the mined con text-sensitiv e term substitutions can be used to e ec-tively rew ord queries and impro ve their accuracy , while the mined con text-sensitiv e term addition patterns can be used to supp ort query re nemen t in a more e ectiv e way. Categories and Sub ject Descriptors: H.3.1 [Con ten t Analysis and Indexing]: Linguistic pro cessing; H.3.3 [Infor-mation Searc h and Retriev al]: Query form ulation, Searc h pro cess General Terms: Algorithms Keyw ords: Term asso ciation patterns, searc h log mining, query reform ulation
As searc h engines are being used, they naturally accum u-late a lot of log data, including submitted queries, view ed searc h results, and clic ked URLs. Suc h searc h engine logs con tain a lot of valuable information suc h as patterns of query reform ulation. In general, a Web searc h engine an-swers millions of queries every day. Thus the huge amoun t of searc h engine log data o ers excellen t opp ortunities for data mining. Indeed, mining searc h engine logs has recen tly attracted much atten tion [22, 16, 11, 1, 8, 25, 21]. All these Cop yright 2008 ACM 978-1-59593-991-3/08/10 ... $ 5.00. studies have sho wn the promise of impro ving searc h accu-racy through mining searc h engine logs. However, virtually all the previous work has treated a whole query as a unit for analysis; as a result, the disco vered kno wledge is mostly at the level of queries. For example, clustering searc h queries is studied in [26, 4]. The similarit y of queries can be measured by the clic ked documen ts [26] or their temp oral correlations [6, 23]. Existing query suggestion works suc h as [19] and [12] also consider a whole query as a unit and they further rely on other resources suc h as Web snipp ets [19] or human-lab eled training data [12] to generate related queries. Furthermore, most of the work only suggests \related" queries and does not consider the e ectiv eness of the suggested queries, whic h is very crucial for successful query suggestions.

In this pap er, we look into patterns at the level of terms through analyzing the relations of terms inside a query and use the disco vered term asso ciation patterns for e e ctive query reform ulation. Our work is motiv ated from the fol-lowing observ ations about what types of kno wledge are use-ful to help a user form ulate an e ectiv e query . A query is ine ectiv e due to multiple reasons, but two of them are common: mis-sp eci c ation and under-sp eci c ation . (1) The mis-sp eci cation problem is caused by the fact that there may be multiple ways of expressing the same idea or describing the same thing, and a user may not kno w what exact terms have been used by the authors of the documen ts to be searc hed. This is also called \vocabulary mismatc h." For example, if a user wants to nd a place to wash his/her vehicle, a good query would be \car wash". If the user uses a query suc h as \auto wash" or \vehicle wash", the searc h results are generally not as good as those from using the query \car wash" even though all these queries have roughly the same meaning. This is because in most relev ant web pages, the authors used \car wash" rather than \vehicle wash" or \auto wash." In order to help a user in suc h a case, we need kno wledge of the form \auto ! car j wash" (i.e., in the con text \ wash", it is better to replace \auto" with \car"). This is an example of what we refer to as a con text-sensitiv e term substitution pattern. (2) The under-sp eci cation problem in a query may be because the user does not kno w much about the con ten t to be found or can not naturally think of additional speci c terms. For example, a query suc h as \auto quotes" can return mixed results with some about automobile insurance quotes and some about automobile sale prices. In suc h a case, it would be useful to suggest terms suc h as \insurance" and \sale" for a user to choose so as to mak e the query more discriminativ e. In order to do this, we need kno wledge of the form \+insurance j auto quotes" and \+sale j auto quotes" (i.e., in the con text of \auto quotes", \insurance" and \sale" are possibly useful terms to re ne the query at a speci ed position). This is an example of what we refer to as a con text-sensitiv e term addition pattern.

In this pap er, we rst formally de ne the two novel term asso ciation patterns in searc h logs { con text-sensitiv e term substitution and addition patterns. Then we prop ose new probabilistic metho ds to disco ver these patterns through an-alyzing term co-o ccurrences in query logs. Our basic idea is to analyze the co-o ccurrences of terms within multi-w ord queries in logs and obtain two kinds of term relations: (1) quasi-synon yms and (2) con textual terms. Quasi-synon yms are words that are synon yms (e.g., auto and car) or that are syn tactically substitutable (e.g., yaho o and google) [10]. Suc h terms tend to co-o ccur with the same or similar terms; for example, both \auto" and \car" often occur together with \ren tal", \pricing", etc. We prop ose to use probabilis-tic translation mo dels for capturing quasi-synon yms. Con-textual terms are terms that app ear together. For example, \car" and \insurance" often co-o ccur in the queries and they can help eac h other to re ne a topic { \car insurance" can be used to re ne both \car" and \insurance". We prop ose to use probabilistic con textual mo dels for capturing con tex-tual terms. Based on both translation mo dels and con tex-tual mo dels, we cast our con text-sensitiv e term asso ciation pattern mining as probabilit y estimation problems. Pat-terns with high probabilities are with high con dence and then used for query reform ulation. For example, \car" has a high probabilit y in the translation mo del of \auto" and high probabilit y to co-o ccur with \wash" in con textual mo dels, then the pattern \auto ! car j wash" will have a high prob-abilit y and thus is a pattern with high con dence.
To test the e ectiv eness of our prop osed algorithms, we conduct exp erimen ts on a sample of searc h logs. Exp eri-men tal results on the real searc h engine logs sho w that our prop osed metho ds can ecien tly and e ectiv ely mine term asso ciation patterns and all these patterns can be used for e ectiv e query reform ulation. These sho w that our prop osed metho ds can disco ver useful kno wledge based on the term relations inside queries. Our metho ds are totally orthogonal to, and thus can be enhanced by, other techniques whic h use other information suc h as clic k-through and user session data for query suggestions.

The rest of the pap er is organized as follo ws. We rst re-view the related work in Section 2. Then we formally de ne our mining problem in Section 3 and prop ose our mo dels to disco ver term asso ciation patterns in Section 4. Our searc h log data collection is describ ed in Section 5 and the exp eri-men ts are presen ted in Section 6. Finally we conclude this pap er and discuss future work in Section 7.
Our work is highly related to query suggestion works suc h as [19] and [12]. In [19], the similarit y between two queries are measured by their retriev ed snipp ets from a searc h en-gine. In [12], adjacen t query pairs from the same user ses-sions are used as candidates and mac hine learning algo-rithms are used to categorize query pairs into 4 classes, whic h re ect levels of relev ance between two queries. The main di erence of our work is that we primarily disco ver patterns in term level and use the disco vered pattern to rec-
Queries Clic ked URLs Time hotel taxes in las vegas http://xxx.xxx.xxx/ xxxx las airp ort NO CLICKS xxxx las vegas airp ort http://xxx.xxx.xxx/ xxxx ... ... ...
 ommend more e e ctive queries, while previous work does not consider the e ectiv eness of a query and only focuses on nding the generally related queries in the level of queries . Furthermore, they alw ays rely on external resources suc h as a Web corpus or training data, while our metho ds only need searc h logs.

Our metho ds to mining term asso ciation patterns are re-lated to translation mo dels for natural languages. Tradi-tional translation mo dels [5] are designed to learn trans-lating wor d pairs of di eren t languages (e.g., English and Frenc h) based on the training data whic h, in general, con-sists of translating sentenc e pairs. In this pap er, our trans-lation mo del between di eren t words is based on the bridge of their con texts. Based on similar ideas, there are sev eral related works, suc h as [13] and [10], whic h iden tify synon yms or near-synon yms in text corpus. Our two types of term as-sociation patterns are closely related to the syn tagmatic and paradigmatic word relations [17, 9]. The di erence is that our work is based on searc h logs and we further use them for query reform ulation.

Our work is also related to query mo di cation work in information retriev al comm unit y [20]. The study of query mo di cation can be traced bac k to the earliest relev ance feedbac k techniques suc h as the Rocchio metho d [18], in whic h queries are mo di ed based on the documen ts whic h are judged to be relev ant and irrelev ant. When all the top documen ts are irrelev ant, negativ e feedbac k can be em-ployed [24]. Pseudo-relev ance feedbac k is to sim ulate rele-vance feedbac k by assuming top rank ed documen ts of an ini-tial retriev al as relev ant ones [27]. In [2], a system Prisma is studied and it can recommend related terms and users can narro w the searc h results by selecting appropriate related ones to re ne their queries. All these approac hes dep end only on the original queries and their initial retriev ed doc-umen ts for query re nemen t. Our query reform ulation al-gorithms are based on the term asso ciation patterns mined from man y queries accum ulated by searc h engines, thus in a collective and collaborative way. Our metho ds rely on users' past activities recorded in searc h logs to disco ver term asso-ciation patterns and thus can re ect users' preferences more appropriately .

Our con text-sensitiv e query rew ording is also related to the spelling correction [7] and con text sensitiv e stemming [15]. But our metho d is to recommend a more appr opriate word to replace the original one, whic h is not necessarily a mis-spelling or a stem. In this sense, our work can be regarded as a \seman tic" extension of previous works whic h rely on \morphological" forms.
Searc h engine logs record the activities of web users, whic h re ect the actual general users' need or interests when con-ducting a searc h. Generally , searc h engine logs have the fol-lowing information: text queries that users submitted, the time when they searc hed, and the URLs that they clic ked after the queries. Searc h engine logs are separated by user sessions. A user session includes sev eral queries from the same user for a coheren t information need and the clic ked URLs for eac h query in the session. An example of user sessions is sho wn in Table 1. In this pap er, we focus on the pattern inside queries in searc h logs and we formally de-ne our problem of mining term asso ciation patterns in this section.

Definition 1 (Quer y). A query q of length n with vo-cabulary V is an ordered sequenc e of terms [ w 1 w 2 :::w wher e w i 2 V for all 1 i n . We use w 2 q if term w is containe d in q .

Definition 2 (Quer y Collection). A query collec-tion Q consists of a bag of N queries: Q = f q 1 ; q 2 ; :::; q The queries are not necessarily distinct from each other.
For example, all the queries submitted to a searc h engine in a certain perio d of time form a query collection. A query collection pro vides us data for mining term asso ciation pat-terns. We now de ne two interesting patterns in searc h logs. Definition 3 (Context-Sensitive Term Substitution). A context-sensitive term substitution pattern is in the form of [ w ! w 0 j c L c R ] . c L and c R are left and right context wor ds and this pattern means that term w should be substi-tute d by term w 0 given a speci c context.
 Definition 4 (Context-Sensitive Term Addition) .
 A context-sensitive term addition pattern is in the form of [+ w j c L c R ] . This pattern means that term w can be adde d into the context c L c R and thus forms a new sequenc e c
The de ned term asso ciation patterns can be easily ex-tended for query reform ulation. We de ne two types of query reform ulation, query rew ording and query re nemen t, in the follo wing and they are to address the mis-sp eci cation and under-sp eci cation problems of an ine ectiv e query re-spectiv ely.

Definition 5 (Quer y Rewording) . Given a query q = w w 2 :::w n , query rewor ding is to modify the query by replac-ing one term w i in q by its semantic ally similar term s , thus form a new query q 0 = w 1 :::w i 1 ; s; w i +1 :::w n .
Definition 6 (Quer y Refinement) . Given a query q = w w 2 :::w n , query re nement is to modify the query by adding one semantic ally relate d term r to q befor e a position i , thus form a new query q 0 = w 1 :::w i 1 rw i :::w n .

It can be seen that query rew ording and query re nemen t corresp ond to con text-sensitiv e term substitution and term addition patterns resp ectiv ely. In practice, query rew ording and re nemen t involve multiple terms. We only consider single terms in the consideration of complexit y.
In this section, we rst de ne two basic types of relation-ship between a pair of terms: syn tagmatic and paradigmatic relation [17], whic h corresp ond to our con textual and trans-lation mo dels resp ectiv ely. We then describ e our term as-sociation mining approac hes based on these two mo dels. In the follo wing, we use c ( x; X ) to represen t the coun t of x in collection X .
Our con textual mo dels are designed to capture syn tag-matic relations between terms. The syn tagmatic relation is for those terms whic h frequen tly co-o ccur together. For example \ren tal" has a stronger syn tagmatic relation with \car" than the word \bask etball" since \ren tal" co-o ccurs with \car" more frequen tly in queries. In general, seman ti-cally related terms have stronger syn tagmatic relation. This type of kno wledge is very useful for query re nemen t. We rst de ne term con texts.

Definition 7 (Term Contexts). Given a query col-lection Q and a term w , we have sever al di er ent types of contexts for w .

Gener al Context G is a bag of wor ds that co-occur with w in Q . That is, a 2 G , 9 q 2 Q , s.t. a 2 q and w 2 q .
The i-th Left Context L i is a bag of wor ds that occur at the i-th position away from w on its left side in any q 2 Q .
The i-th Right Context R i is a bag of wor ds that occur at the i-th position away from w on its right side in any q 2 Q .
For example, given that a query \national car ren tal" ap-pears in the query collection, \national" and \ren tal" are in the general con text G of \car"; only \national" is in the L and only \ren tal" is in the R 1 of \car". L i and R i are more precise con texts for eac h term. In the follo wing, given a type of con text C , we use C ( w ) to represen ts w 's C con text.
Our con textual mo dels are to capture the syn tagmatic relations probabilistically . Giv en a term w , di eren t terms have di eren t strength of syn tagmatic relation with w . We thus mo del this relation probabilistically and adopt language mo del approac hes here: Giv en a word w and its con text C ( w ), the con textual mo del is a uni-gram language mo del. The Maxim um Lik eliho od estimation (ML) is Intuitiv ely, a con text mo del tells us what words have high probabilities to app ear around a given word w (or at a spe-ci c position).

Smo othing techniques are usually used for language mo d-els due to the data sparseness problem. An e ectiv e ap-proac h is Diric hlet prior smo othing [28]: where P ( a j B ) is a prede ned reference mo del (usually set as the whole collection language mo del) and is the Diric hlet prior parameter to be set empirically (3000 in our exp eri-men ts). Note that we use ~ P C ( j w ) and P C ( j w ) to represen t the smo othed and non-smo othed con textual mo dels of w re-spectiv ely.
Our translation mo dels are designed to capture paradig-matic relations between terms. The paradigmatic relations capture words whic h are quasi-synon yms (e.g., \car" and \auto"). Our translation mo dels are built on con textual mo dels. The basic idea is that two terms have stronger paradigmatic relation if they have similar con texts. For example, \car" and \auto" may share a lot of con textual words suc h as \sales" and \insurance" and thus have strong paradigmatic relation. This type of kno wledge could be very useful in helping a user replace a query term (with a poten-tially better term) in a certain con text.

In our translation mo del, we use t ( s j w ) to denote the prob-abilit y of \translating" w to the word s . In the language mo deling approac h, we use the Kullbac k-Leibler div ergence (KL) D ( jj ) between two con textual mo dels to measure the similarit y between two con texts. Giv en two language mo dels p and q , their KL-div ergence is de ned as The KL-div ergence value is smaller if p and q are similar. We use KL-div ergence on con textual mo dels to de ne t C ( s j w ) as follo ws: After a few transformations, it can be seen that whic h is the likeliho od of generating s 's con text C ( s ) from w 's smo othed con textual mo del. The above form ula can be applied on any type of con textual mo dels. For example, we can use con texts G , L 1 , or R 1 . In this pap er, we use a com bination of L 1 and R 1 con texts since these two are most indicativ e of the word in consideration: ring in the L 1 ( R 1 ) con text of w .
The con text-sensitiv e term substitution patterns give us kno wledge about query rew ording. Recall that query re-wording is to substitute a term w i in q to be s and thus we get another query q 0 = w 1 :::w i 1 sw i +1 :::w n . The new query q 0 should have similar/related meaning to q . A good substitution should require that q 0 is better than q to re-triev e more relev ant documen ts. In other words, s is more appropriate than w i to capture the information need given the con text words in the query . For example, \car wash" is generally better than \auto wash" in the con text \ wash".
In order to disco ver term substitution patterns, the prob-abilit y we are interested in is: substituting the i-th posi-tion word w i by s given the con text w 1 :::w i 1 w i +1 P ( s j w i ; w 1 :::w i 1 w i +1 :::w n ). We use a shorthand t ( w s j q ) to represen t this probabilit y. Then
In Equation 2, eac h substitution candidate s is scored based on two factors: The rst factor t ( s j w i ) re ects the similarit y between the word w i and the candidate s . This is a glob al factor whic h tells us how globally similar these two words are. The second factor P ( w 1 :::w i 1 w i +1 :::w is the local factor based on other con text words in the query q . It tells us how likely s app ears in suc h a con text de ned by q . These two factors are com bined together to score eac h candidate in our metho d. To estimate the second factor, a simple approac h is assume the con text words are indep en-den t from eac h other given s . Using the general con text G , we have The general con text ignores the position information and also considers all the words in con text. In general, a word far away for the position in consideration should have lower impact. We thus use the more precise con textual mo dels L and R i and ignore those words whic h are far away: where k is the num ber of adjacen t terms to consider. For example, if we set k = 2, we have P ( w 1 :::w i 1 w i +1 as Note that we alw ays use smo othed con textual mo dels in the above form ulas. To mak e the distributions at di eren t po-sition i comparable, we use n -th root of the value in Equa-tion 3 and n is the total num ber of factors in the pro duct.
For term substitutions, we need a reliable translation mo del t ( s j w ). However, estimating t ( s j w ) based only on con textual mo dels need to be limited to ensure that s and w are seman-tically similar. For example, \American idol" and \Ameri-can express" are two popular queries. Thus the same word \American" can sho w up in the L 1 con texts of \idol" and \express" frequen tly; as a result, we will have a high trans-lation probabilit y of t (express j idol ), whic h is not desirable. To impro ve the translation mo dels, we rely on the user ses-sions in our searc h logs (see an example in Table 1). Since queries in a user session are usually coheren t, we would ex-pect \idol" and \express" would not app ear in the same sessions very often.

We use Mutual Information (MI) of the two words s and t over sessions to measure their correlation. MI is widely used to measure the mutual indep endency of two random variables in information theory , whic h intuitiv ely measures how much information a random variable tells about the other. In our case, MI can be computed as follo ws: where X s and X w are two binary random variables corre-sponding to the presence/absence of term s and term w in eac h user session. For example, P ( X s = 1 ; X w = 1) can be calculated as the prop ortion of the user sessions in whic h s and w are both presen t.

To mak e MI comparable across di eren t pairs of words, we use a normalized version of MI in our pap er, whic h is de ned as It is easy to verify that N M I ( w; w ) = 1 and 0 N M I ( s; w ) 1.

For our term substitution pattern mining, we com bine t ( s j w i ) and N M I ( s; w i ) as follo ws: (1) Giv en q and w i , we use t ( s j w i ) to nd the top N words whic h have the highest probabilities in t ( s j w i ). (2) For eac h of these N words, we calculate its N M I with w i using session information. (3) We use a threshold to remo ve a word s from the N words if N M I ( s; w i ) .

In our exp erimen ts, we set N = 20 and = 0 : 001. Since all the remaining words have high translation probabilities and frequen tly co-o ccur with w in user sessions, they are more reliable and we thus set all t ( s j w i ) = 1 for those remaining terms and compute t ( w i ! s j q ) only based on Equation 3. To decide when we need to replace w i by s , we larger than 1, we would recommend to replace w i by s .
A query may con tain multiple words and any one of them can be poten tially replaced/rew orded. In an interactiv e man-ner, a user can tell the system whic h term he/she wants to replace. In an automatic manner, our general strategy is to iterate all the words and try to replace eac h of them. Then we get a set of candidates whic h di er from the original query by one term. Eac h of these candidates has a prob-abilit y computed by Equation 3. We nally sort all these candidates by their corresp onding probabilities and recom-mend the top rank ed ones as substitutions.
A term addition pattern [+ w j c L c R ] is to add a word w given the con text c L c R . Formally , given a query q = w w 2 :::w n whic h con tains n words and a position i , our task is to recommend a term r whic h can be added to the original query to form a new query q 0 = w 1 :::w i 1 rw i :::w n . We for-malize this problem in a probabilistic way and we use i ( r j q ) to denote the probabilit y of a pattern [+ r j w 1 :::w i 1 P ( w 1 :::w i 1 w i :::w n j r ) can be estimated similarly to the es-timation of the local factor in Equation 2. Here we estimate it similarly to Equation 3 as follo ws: P ( r ) is the prior probabilit y of the app earance of the term r . In the simplest case, we can assume P ( r ) to be uniform thus it will not a ect the ranking of di eren t terms.
Intuitiv ely, the terms whic h have higher probabilities to be added to q are those whic h co-o ccur frequen tly in the query collection together with the words in q . Eac h word will be assigned a probabilit y based on Equation 7 and we then rank all the terms.

Giv en query q = w 1 :::w n , there are n + 1 positions in whic h we can add a term. In our exp erimen ts, we iterate over all these positions and get a list of new query candidates for position i . Eac h query has a corresp onding probabilit y estimated from i ( r j q ). A nal list of the recommended queries is the rank ed list merged from queries for all the positions.
We construct our data set based on the MSN searc h log data set released by the Microsoft Liv e Labs in 2006 [14]. Our log data spans 31 days from 05/01/2006 to 05/31/2006. In total, there are 8,144K queries, 3,441K distinct queries, 4,649K distinct URLs, and 7,470K user sessions in the raw data.

We separate the whole data set into two parts according to the time: the rst 2/3 data is used to sim ulate the history data and it is used as a query collection to mine the term asso ciation patterns. The queries in the last 1/3 data are retained to test our metho ds. In the history collection, we clean the data by only keeping those well-formatted English queries (queries only con taining characters from `a' to 'z' and space). We also use a prede ned stop word list to remo ve those common words suc h as \a" and \the" from our query collection. After cleaning, we get 4,431,152 queries in our query collection in total and 1,577,424 of them are distinct. The total num ber of unique words con tained by the queries in this collection is 199,629 and the media length of the queries is 2. This data set is used in our exp erimen ts to compute the con textual mo dels and translation mo dels. For the user sessions, we obtain 3,540K in total and 1,320K of them have at least two queries in our training data. We use these 1,320K user sessions to compute the normalized mutual information between two terms.

Based the queries in our query collection, we build G , L L , R 1 , and R 2 con texts for the 76,693 most frequen t words in the collection. All the con texts pro vide us the statistics of the necessary probabilities needed in our con textual mo dels. Furthermore, we also compute the words whic h have high translation probabilities to eac h of the 76,693 words and thus build their translation mo dels. All the con textual mo dels and translation mo dels are stored for online query rew ording and query re nemen t.
In this section, we describ e our exp erimen ts on mining term asso ciation patterns from the searc h engine logs. In all the follo wing exp erimen ts, we set smo othing parameter = 3000 and k = 2 in Equation 3.
In Table 2, we sho w the G , L 1 and R 1 con textual mo d-els of two words: \car" and \yaho o". From this table, we can see that all the con textual mo dels in this table app ear to be meaningful. We can also see that G con texts mix L 1 and R 1 con texts and that L 1 and R 1 con texts are much di eren t. This sho ws it is better to mo del these precise con texts for a given term in our query collection. Further-more, these con textual words may cover di eren t asp ects. In Table 3, We sho w the results of the disco vered asp ects of \car" and \yaho o". The asp ects are obtained by applying the star clustering [3] on the top words in the G con textual mo dels (see [3] for more details) and we sho w the top 5 clus-ters. Clearly , for the word \car", people usually care about \ren tal" and \pricing". In the example of \yaho o", people are interested in \searc h", or \games". All these asp ects corresp ond to di eren t information needs of end users and thus can be poten tially used for searc h result organization and query re nemen t.

We now sho w sev eral examples of our translation mo dels using Equation 1. In Table 4, we give 6 di eren t terms Table 3: Asp ects for words \car" and \yaho o".
 Table 5: Translation mo del and Normalized Mutual Information of w = \idol". from di eren t domains and their translation mo dels. For eac h term example, the top 5 words with highest translation probabilities are sho wn. We can seen that our prop osed translation mo dels are very e ectiv e to iden tify seman tically similar words. For example, \fox", \ab c", and \cnn" are all related to broadcast companies; \bm w", \honda", and \suzuki" are motorcycle/car brands. It is also interesting to note that words about di eren t languages and words about di eren t minerals can be iden ti ed to be similar. All these sho w the e ectiv eness of our prop osed translation mo dels to iden tify `related words". All these terms pro vide possibilit y for users to do exploratory searc h.
In this section, we study the e ectiv eness of our term sub-stitution patterns. We rst sho w sev eral examples. We then compare our metho ds with previous metho ds to sho w that our metho d can impro ve the e ectiv eness of a query .
In this section, we enhance our translation mo dels using user session information. Table 5 sho w an example. It can be seen that the words rerank ed using Normalized Mutual Information can indeed reduce those non-related words.
We sho w sev eral substitution patterns on query rew ord-ing. We decide to rew ord a query if the ratio t ( w ! s j q ) whic h means that s is more likely than w given query q . Table 6 sho ws sev eral examples of the patterns (1st col-umn) and the rew orded queries by our metho d (2nd col-
Table 6: Examples of term substitution patterns. umn). From the table, we have the follo wing observ ations: (1) Our metho d can recommend more e ectiv e queries. For example, \kids games" is usually more e ectiv e than \chil-dren games". (2) Term substitution patterns are con text-sensitiv e. For example, we substitute \auto" by \car" in the con text \ wash", while we substitute \car" by \auto" in the con text of \ trade". (3) We can see that queries from our metho ds are related to the original ones, but their mean-ings are not exactly equiv alen t (e.g., \birthda y cards" and \greeting cards"). This is because translation mo dels tend to nd words with similar concepts but not alw ays having the exactly same meanings, in general.
In this section, we study the e ectiv eness of query rew ord-ing by comparing with a previous metho d prop osed in [12].
Exp erimen t Design. In [12], related queries are gen-erated according to user sessions. For eac h query , they rst nd all its next queries in all user sessions and use Log-Lik eliho od Ratio (LLR) to iden tify those highly related queries. Then they rerank the queries based on a mo del learned from training data. In our pap er, we use their lin-ear regression mo del for reranking and use LLR to denote this metho d. The LLR metho d gives a rank ed list of queries. Some of the resulting queries is query rew ording, but they also con tain other types of query reform ulation suc h as query re nemen t. To compare fairly , we lter the rank ed list and only retain those queries whic h are rew ording of the original queries.

To compare di eren t metho ds, we construct our test cases from our hold-out logs as follo ws: 1) We merged all the sessions whic h have the same initial queries together as one test case. 2) For eac h test case, we use all the clic ked URLs, ex-cept those of the initial query , in all the merged sessions to appro ximate relev ance documen ts. These documen ts are to appro ximate relev ant documen ts whic h users obtained through query reform ulation. Our purp ose is to compare di eren t metho ds with resp ect to fetc hing additional rele-vant documen ts.

Giv en the test cases constructed above, we compare 3 metho ds: The rst metho d (denoted by Original query) is to use the initial/original queries to get a rank ed list from a searc h engine. The second metho d is the LLR metho d in [12] and the third is ours. For either of these two metho ds, we rst generate a list of recommended queries. We then fetc h a rank ed list of searc h results for eac h of the queries from the same searc h engine. Finally we use our relev ance judgemen t to evaluate these di eren t searc h results.
Our goal is to test whic h metho d can recommend more e ectiv e queries. Since both our and LLR metho ds can not generate recommended queries for every query , we thus lter the test cases and only retain those for whic h both LLR and our metho d can generate at least 5 queries and for whic h the rst generated queries by our metho d satisfy t ( w ! s j q ) This is to sim ulate the scenario in whic h the rst query is not very e ectiv e since it is precisely in suc h a scenario that a user would need help with reform ulation. From all the remaining test cases after ltering, we randomly sample 50 test cases for our evaluation.

We use Precision@5 (P@5) as our evaluation metric. Giv en m recommended queries for eac h test case, we select the best one whic h has the largest num ber of relev ant documen ts in its top 100 searc h results and use its P@5 as the accuracy of the corresp onding test case.

Results. We vary the num ber of recommended queries m from 1 to 5 and the best P@5 values are sho wn in Fig-ure 1. From this gure, we can see that both our metho d and LLR outp erform original query and thus can recommend more meaningful queries. Compared with LLR metho d, our metho d is more e ectiv e. For example, when we only con-sider the rst recommended query , our metho d can give P@5 = 0.08 while LLR metho d can only give P@5 = 0.05. This is because LLR metho d does not consider the e ectiv eness of a query while our metho d would recommend a more ef-fectiv e one based on the term substitution patterns mined from searc h logs. For example, our metho d can recommend \cheap tickets" for \cheap airfare", while LLR only suggests queries suc h as \discoun t airfare", whic h is not as e ectiv e as \cheap tickets".
In this section, we study the e ectiv eness of our metho d for queries with the same meanings. In order to ensure that the recommended queries have the same meanings as the original ones, we prop ose a lexical matc hing constrain t for the translation pairs. Figure 1: Comparison of term substitution patterns. We compare the best P@5 of the top m recom-mended queries by di eren t metho ds. Table 7: The categories and examples of translation pair after applying lexical matc hing constrain t.
Lexical Matc hing Constrain t. Giv en a word w , we rst get its top 3 words with the highest probabilities based on its translation mo del. This gives us 3 translation pairs. Our lexical matc hing constrain t only retains those pairs suc h that every character in the short word of the pair must app ear in the long word, in the same order. For example, \tx" and \texas" are a quali ed pair since \t" and \x" both app ear in \texas" and \t" is before \x" in both words. By apply-ing this lexical matc hing constrain t, we can hop efully get seman tically equiv alen t pairs.

Table 7 sho ws sev eral examples of the iden ti ed pairs after applying our lexical matc hing constrain t. We found that the results can be classi ed into three categories: plu-ral/singular, abbreviations, and others.

Using the translation pairs ltered by our lexical matc hing constrain t as candidate pairs, we apply our query rew ording algorithm on a set of queries sampled from our test data. Eac h of these queries q con tains at least a word w from our candidate pairs and our rew ording algorithm tries to replace w with its paired word s . If the ratio t ( w ! s j q ) we rew ord q by replacing w by s . Finally , we rewrite 1,437 queries. Table 8: Examples of con text-sensitiv e query re-wording using the pairs ltered by our lexical matc h-ing constrain t.

Table 8 sho ws sev eral examples of our query rew ording, ordered by their ratios. In this table, we can see that some queries are changed from singular to plural form, while some queries are changed from plural to singular form. All the changes are con text sensitiv e. For example, our algorithm changes \yaho o map" to \yaho o maps" (from singular to plural), but changes \maps quest" to \map quest" (from plural to singular). Intuitiv ely, our rew orded queries are more e ectiv e since the domain names of these two queries are maps.y aho o.com and map quest.com. A recen t work [15] has reac h similar conclusion that con text-sensitiv e stemming can impro ve clic k-through rate. Our results are consisten t with this conclusion.

E ectiv eness Exp erimen t Design. To study the ef-fectiv eness of query rew ording, we use the clic ked web pages in our searc h engine log data to evaluate. Giv en a query , we collect all the positions of its clic ked documen ts in our test log data and aggregate all these clic ks together. We treat all the clic ked positions as the positions of relev ant documen ts in a ranking list and evaluate and compare the accuracy of the original queries and our recommended queries. Since we use our lexical constrain t to force all the pairs to share equiv alen t meaning, comparing their results to sho w their e ectiv eness is reasonable. Intuitiv ely, a better query would retriev e more relev ant documen ts on the top of the rank ed list that users tend to clic k. In our exp erimen ts, for eac h query , we calculate the precision at 1, 5, 10, 15, and 20 documen ts.

E ectiv eness Results. Figure 2 sho ws the comparison between the original queries and the rew orded queries. The precisions are averaged over all the queries that our algo-rithm decides to rewrite. Clearly , we can see that, at ev-ery level of precision, our recommended queries can retriev e more relev ant documen ts and thus outp erform the original queries. For example, the P@10 of our recommended queries is 0.42, while the P@10 of the original queries is about 0.28. We achiev e 41 : 3% relativ e impro vemen t.
 of our query rew ording. A high ratio means that the re-worded query is more appropriate than the original query . To test this, we ordered the query pairs by the ratio in de-creasing order and we then evaluate the accuracy of top m pairs by varying m from 100 to 600. Figure 3 sho ws the in u-ence of ratio and the di erence is measured by the quotien t of P@10 of recommended queries over original ones. From this gure, we can see that when the ratio is higher, the Figure 2: The overall performance comparison of the original and rewritten queries. We compare their results by Precision@K. Figure 3: The impact of the ratio on the perfor-mance. The Di erence of P@10 is measured as the quotien t of P@10 of recommended queries over orig-inal ones. performance di erence is larger. This means that the ratio is a good indicator of the con dence of query rew ording.
In this section, we study our con text-sensitiv e term addi-tion patterns and use them for query re nemen t.
Table 9 sho ws sev eral examples of the mined term addition patterns and the re ned queries based on Equation (7). For eac h query , all the patterns are ordered by their probabilities in decreasing order. These examples sho w that our metho d can recommend very meaningful terms to re ne an origi-nal query . For example, for the query \wedding", we can recommend meaningful terms related to di eren t asp ects of \wedding", suc h as \dresses" and \cak es". All these terms can help users re ne their queries and thus nd more coher-ent results. Furthermore, all these terms give good guidance for a user when he/she wants to prepare a \wedding". Suc h a recommendation is more useful if a user is not satis ed in decreasing order. Figure 4: Comparison with the LLR metho d of term addition patterns. with the curren t results but lacks the necessary kno wledge to think of e ectiv e words to re ne his/her query .
We compare our metho d with LLR metho d in a similar way as in Section 6.2. We use the same test set and construct our test cases similarly . The only di erence is that we only retain those recommended queries whic h are re nemen ts of the original queries for the LLR metho d. We also use 50 test cases in this exp erimen ts and use P @5 as the ma jor evalu-ation metric. Figure 4 sho ws the comparison between dif-feren t metho ds. In this gure, we have similar observ ations to the term substitution patterns: Both our and LLR meth-ods can outp erform the baseline metho d. Compared with LLR metho d, our metho d can achiev e better results. This is because LLR metho d only consider the query re nemen t within user sessions. Our metho d can utilize information across sessions since our con textual mo dels are built over the whole collection.
The eciency of the algorithm is quite imp ortan t since a query collection is huge. We test the eciency of our metho d in this section.
 We implemen t our algorithm using the Lem ur toolkit 1 . The original data is a collection of queries. We build the standard Lem ur index by treating eac h query as a documen t. This part is as ecien t as the standard documen t indexing, http://www.lem urpro ject.org/ Figure 5: The time complexit y of building the con-textual mo dels and translation mo dels. thus it can be applied to very large data set prett y easily . The basic kno wledge we disco vered from the query collec-tion includes the con textual mo dels and translation mo dels. Both are pro cessed oine based on the Lem ur index of the query collection. Based on the translation mo dels and con-textual mo dels, the con text-sensitiv e term substitution and term addition patterns are disco vered in an online manner. Once the kno wledge is built, the online part needs only fetc h the corresp onding kno wledge we stored, and thus can be quite ecien t. Since all the online parts are very ecien t, in the follo wing, we only test the eciency of the oine part whic h is to compute the con textual mo dels and translation mo dels.

To study the eciency and scalabilit y, we randomly sam-ple f % of the original queries from the whole query collec-tion. We vary f from 10 to 100 with step 10. For eac h value of f , we record the time needed for our oine part. Figure 5 sho ws the time complexit y of our algorithms. In this gure, x -axis is the value of f % and y -axis is the time. It can be seen that both lines are roughly linear and thus our oine part is linearly scalable. This sho ws that our pro-posed metho ds can mine those patterns very ecien tly and can be applicable to very large query collections.
In the pap er, we studied the problem of mining term as-sociation patterns from the vast amoun t of searc h engine log data. We de ned two novel term asso ciation patterns (i.e., con text-sensitiv e term substitution and term addition patterns) and prop osed new metho ds for mining suc h pat-terns from searc h engine logs. Our metho ds are based on the con textual and translation mo dels whic h are mined from a query collection. The two types of disco vered term asso ci-ation patterns can be used to address the mis-sp eci cation and under-sp eci cation problems of ine ectiv e queries. Ex-perimen t results on searc h engine logs sho w the e ectiv eness of our prop osed metho ds.

There are a few limitations of our work. First, all the exp erimen ts are based on clic kthroughs instead of real rel-evance judgmen ts, so an interesting future work would be to further test the prop osed metho ds with real relev ance judgmen ts. Second, building an interactiv e user interface whic h can allo w a user to mo dify his/her queries using our suggested terms can help evaluate our algorithms. Third, searc h logs have more meaningful clic k-through information besides queries and sessions. We can extend our pattern mining algorithms to incorp orate this clic k-through infor-mation in the future.
We thank the anon ymous review ers for their valuable sug-gestions. This work is in part supp orted by the National Science Foundation under award num bers IIS-0347933, IIS-0713581, and a gift gran t from Microsoft Researc h. [1] E. Agic htein, E. Brill, and S. T. Dumais. Impro ving [2] P. Anic k. Using terminological feedbac k for web searc h [3] J. A. Aslam, E. Pelek ov, and D. Rus. The star [4] D. Beeferman and A. L. Berger. Agglomerativ e [5] P. F. Bro wn, V. J. D. Pietra, S. A. D. Pietra, and [6] S. Chien and N. Immorlica. Seman tic similarit y [7] S. Cucerzan and E. Brill. Spelling correction as an [8] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. Ma.
 [9] R. Green. Syn tagmatic relationships in index [10] D. Inkp en and G. Hirst. Building and using a lexical [11] T. Joac hims. Optimizing searc h engines using [12] R. Jones, B. Rey , O. Madani, and W. Greiner. [13] D. Lin. Automatic retriev al and clustering of similar [14] Microsoft Liv e Labs. Accelerating searc h in academic [15] F. Peng, N. Ahmed, X. Li, and Y. Lu. Con text [16] F. Radlinski and T. Joac hims. Query chains: learning [17] R. Rapp. The computation of word asso ciations: [18] J. J. Rocchio. Relev ance feedbac k in information [19] M. Sahami and T. D. Heilman. A web-based kernel [20] G. Salton, A. Wong, and C. S. Yang. A vector space [21] D. Shen, M. Qin, W. Chen, Q. Yang, and Z. Chen. [22] X. Shen, B. Tan, and C. Zhai. Con text-sensitiv e [23] M. Vlac hos, C. Meek, Z. Vagena, and D. Gunopulos. [24] X. Wang, H. Fang, and C. Zhai. A study of metho ds [25] X. Wang and C. Zhai. Learn from web searc h logs to [26] J.-R. Wen, J.-Y. Nie, and H. Zhang. Clustering user [27] J. Xu and W. B. Croft. Impro ving the e ectiv eness of [28] C. Zhai and J. D. La ert y. A study of smo othing
