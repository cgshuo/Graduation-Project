 Random projections have recently emerged as a powerful method for dimensionality reduction. Theoretical results indicate that the method preserves distances quite nicely; however, empirical results are sparse. We present experi-mental results on using random projection as a dimension-ality reduction tool in a number of cases, where the high dimensionality of the data would otherwise lead to burden-some computations. Our application areas are the process-ing of both noisy and noiseless images, and information re-trieval in text documents. We show that projecting the data onto a random lower-dimensional subspace yields re-sults comparable to conventional dimensionality reduction methods such as principal component analysis: the similar-ity of data vectors is preserved well under random projec-tion. However, using random projections is computationally significantly less expensive than using, e.g., principal com-ponent analysis. We also show experimentally that using a sparse random matrix gives additional computational sav-ings in random projection. random projection, dimensionality reduction, image data, text document data, high-dimensional data 
In many applications of data mining, the high dimen-sionality of the data restricts the choice of data process-ing methods. Such application areas include the analysis of market basket data, text documents, image data and so on; in these cases the dimensionality is large due to either a wealth of alternative products, a large vocabulary, or the use of large image windows, respectively. A statistically op-timal way of dimensionality reduction is to project the data *On leave at Nokia Research Center copics bear this notice and the lull citation on requires prior specific permission and/or a fee. KDD 01 San Francisco CA USA Copyright ACM 2001 1-58113-391-x/01/08...$5.00 onto a lower-dimensional orthogonal subspace that captures as much of the variation of the data as possible. The best (in mean-square sense) and most widely used way to do this is principal component analysis (PCA); unfortunately it is quite expensive to compute for high-dimensional data sets. A computationally simple method of dimensionality reduc-tion that does not introduce a significant distortion in the data set would thus be desirable. 
In random projection (RP), the original high-dimensional data is projected onto a lower-dimensional subspace using a random matrix whose columns have unit lengths. RP has been found to be a computationally efficient, yet suf-ficiently accurate method for dimensionality reduction of high-dimensional data sets. While this method has attracted lots of interest, empirical results are sparse. 
In this paper we give experimental results on using RP as a dimensionality reduction tool on high-dimensional image and text data sets. In both application areas, random pro-jection is compared to well known dimensionality reduction methods. We show that despite the computational simplic-ity of random projection, it does not introduce a significant distortion in the data. 
The data sets used in this paper are of very different na-tures. Our image data is from monochrome images of nat-ural scences. An image is presented as a matrix of pixel brightness values, the distribution of which is generally ap-proximately Gaussian: symmetric and bell-shaped. Text document data is presented in vector space [25], in which each document forms one d-dimensional vector where d is the vocabulary size. The i-th element of the vector indi-cates (some function of) the frequency of the i-th vocabu-lary term in the document. Document data is often highly sparse or peaked: only some terms from the vocabulary are present in one document, and most entries of the document vector are zero. Also, document data has a nonsymmetric, positively skewed distribution, as the term frequencies are nonnegative. It is instructive to see how random projection works as a dimensionality reduction tool in the context of these two very different application areas. 
We also present results on images corrupted by noise, and our experimental results indicate that random projection is not sensitive to impulse noise. Thus random projection is a promising alternative to some existing methods in noise reduction (e.g. median filtering), too. 
This paper is organized as follows. At the end of this intro-duction we discuss related work on random projections and similarity search. Section 2 presents different dimensionality reduction methods. Section 3 gives the experimental results of dimensionality reduction on image data, and Section 4! on text data. Finally, Section 5 gives a conclusion. 
Papadimitriou et al. [22] use random projection in the preprocessing of textual data, prior to applying LSI. They present experimental results on an artificially generated set of documents. In their approach, the columns of the ran-dom projection matrix are assumed strictly orthogonal, but actually this need not be the case, as we shall see in our experiments. 
Kaski [17, 16] has presented experimental results in using .the random mapping in the context of the WEBSOM 1 sys-tem. Kurimo [20] applies random projection to the indexing of audio documents, prior to using LSI and SOM. Kleinberg [19] and Indyk and Motwani [14] use random projections in nearest-neighbor search in a high dimensional Euclidean space, and also present theoretical insights. Dasgupta [6, 7] has used random projections in learning high-dimensional Ganssian mixture models. Other applications of random projection include e.g. [4, 28]. 
The problems of dimensionality reduction and similarity search have often been addressed in the information retrieval literature, and other approaches than random projection have been presented. Ostrovsky and Rabani [21] give a di-mension reduction operation that is suitable for clustering. Agrawal et al. [3] map time series into frequency domain by the discrete Fourier transform and only retain the first few frequencies. Keogh and Pazzani [18] reduce the dimension of time series data by segmenting the time series into sec-tions and indexing only the section means. Aggarwal et al. [2] index market basket data by a specific signature table, which easens the similarity search. Wavelet transforms ([12, 27] etc.) are a common method of signal compression. 
In random projection, the original d-dimensional data is projected to a k-dimensional (k &lt;&lt; d) subspace through the origin, using a random k  X  d matrix R whose columns have unit lengths. Using matrix notation where Xdx N is tile original set of N d-dimensional observations, is the projection of the data onto a lower k-dimensional subspace. The key idea of random mapping arises from the Johnson-Lindenstrauss lemma [15]: if points in a vec-tor space are projected onto a randomly selected subspace of suitably high dimension, then the distances between the points are approximately preserved. For a simple proof of this result, see [10, 8]. 
Random projection is computationally very simple: form-ing the random matrix R and projecting the d x N data matrix X into k dimensions is of order O(dkN), and if the data matrix X is sparse with about c nonzero entries per column, the complexity is of order O(ckN) [22]. 
Strictly speaking, (1) is not a projection because R is gen-erally not orthogonal. A linear mapping such as (1) can i See http ://websom. hut. fi/websom/ cause significant distortions in the data set if R is not or-thogonal. Orthogonalizing R is unfortunately computation-ally expensive. Instead, we can rely on a result presented by Hecht-Nielsen [13]: in a high-dimensional space, there exists a much larger number of almost orthogonal than or-thogonal directions. Thus vectors having random directions might be sufficiently close to orthogonal, and equivalently RTR would approximate an identity matrix. In our exper-iments, the mean squared difference between RTR and an identity matrix was about 1/k per element. 
When comparing the performance of random projection to that of other methods of dimensionality reduction, it is in-structive to see how the similarity of two vectors is distorted in the dimensionality reduction. We measure the similarity of data vectors either as their Euclidean distance or as their inner product. In the case of image data, Euclidean distance is a widely used measure of similarity. Text documents, on the other hand, are generally compared according to the cosine of the angle between the document vectors; if docu-ment vectors are normalized to unit length, this corresponds the inner product of the document vectors. 
We write the Euclidean distance between two data vec-tors xl and x2 in the original large-dimensional space as I]Xl -x211. After the random projection, this distance is ap-proximated by the scaled Euclidean distance of these vectors in the reduced space: where d is the original and k the reduced dimensionality of the data set. The scaling term V~ takes into account the decrease in the dimensionality of the data: according to the Johnson-Lindenstrauss lemma, the expected norm of a projection of a unit vector onto a random subspace through the origin is V/~ [15]. 
The choice of the random matrix R is one of the key points of interest. The elements r~j of R are often Ganssian dis-tributed, but this need not be the case. Achlioptas [1] has recently shown that the Gaussian distribution can be re-placed by a much simpler distribution such as In fact, practically all zero mean, unit variance distributions of r~j would give a mapping that still satisfies the Johnson-Lindenstrauss lemma. Achlioptas' result means further com-putational savings in database applications, as the compu-tations can be performed using integer arithmetics. In our experiments we shall use both Ganssian distributed random matrices and sparse matrices (3), and show that Achlioptas' theoretical result indeed has practical significance. In con-text of the experimental results, we shall refer to RP when the projection matrix is Gaussian distributed and SRP when the matrix is sparse and distributed according to (3). Oth-erwise, the shorthand RP refers to any random projection. 
In principal component analysis (PCA), the eigenvalue decomposition of the data covariance matrix is computed as E{XX T } ---EAE T where the .columns of matrix E axe the eigenvectors of the data covariance matrix E{XX T } and A is a diagonal matrix containing the respective eigenvalues. If dimensionality reduction of the data set is desired, the data can be projected onto a subspace spanned by the most important eigenvectors: where the d x k matrix Ek contains the k eigenvectors cor-responding to the k largest eigenvalues. PCA is an op-timal way to project data in the mean-square sense: the squared error introduced in the projection is minimized over all projections onto a k-dimensional space. Unfortunately, the eigenvalue decomposition of the data covariance matrix (whose size is d x d for d-dimensional data) is very expensive to compute. The computational complexity of estimating the PCA is O(d2N) + O(d z) [11]. There exists computa-tionally less expensive methods [26, 24] for finding only a few eigenvectors and eigenvalues of a large matrix; in our experiments, we use appropriate Matlab routines to realize these. 
A closely related method is singular value decomposition (SVD): X = USV T where orthogonal matrices U and V contain the left and right singular vectors of X, respectively, and the diagonal of S contains the singular values of X. Us-ing SVD, the dimensionality of the data can be reduced by projecting the data onto the space spanned by the left sin-gular vectors corresponding to the k largest singular values: where Uk is of size dx k and contains these k singular vectors. Like PCA, SVD is also expensive to compute. There exists numerical routines such as the power or the Lanczos method [5] that are more efficient than PCA for sparse data matrices X, and that is why we shall use SVD instead of PCA in the context of sparse text document data. For a sparse data matrix X4xN with about c nonzero entries per column, the computational complexity of SVD is of order O(dcN) 
Latent semantic indexing (LSI) [9, 22] is a dimensionality reduction method for text document data. Using LSI, the document data is presented in a lower-dimensional "topic" space: the documents are characterized by some underlying (latent, hidden) concepts referred to by the terms. LSI can be computed either by PCA or SVD of the data matrix of N d-dimensional document vectors. 
Discrete cosine transform (DCT) is a widely used method for image compression and as such it can also be used in dimensionality reduction of image data. DCT is compu-tationally less burdensome than PCA and its performance approaches that of PCA. DCT is also optimal for human eye: the distortions introduced occur at the highest frequen-cies only, and the human eye tends to neglect these as noise. DCT can be performed by simple matrix operations [23, 27]: an image is transformed to the DCT space and dimensional-ity reduction is done in the inverse transform by discarding the transform coefficients corresponding to the highest fre-quencies. Computing the DCT is not data-dependent, in contrast to PCA that needs the eigenvalue decomposition of data covariance matrix; that is why DCT is orders of mag-nitude cheaper to compute than PCA. Its computational complexity is of the order O(dNlog2(dN)) for a data ma-trix of size d x N [27]. 
The data set consisted of N = 1000 image windows drawn from 13 monochrome images 2 of natural scenes.The sizes of the original images were 256 x 256 pixels, and windows of size 50 x 50 were randomly drawn from the images. Each image window was presented as one d-dimensional column vector (d = 2500). 
When comparing different methods for dimensionality re-duction, the criteria are the amount of distortion caused by the method and its computational complexity. In the case of image data we measure the distortion by comparing the Euclidean distance between two dimensionality reduced data vectors to their Euclidean distance in the original high-dimensional space. In the case of random projection, the Euclidean distance in the reduced space is scaled as shown in (2); with other methods, no scaling is performed. 
We first tested the effect of the reduced dimensionality us-ing different values of k in [1,800]. At each k, the dimension-ality reducing matrix operation was computed anew. Figure 1 shows the error in the distance between members of a pair of data vectors, averaged over 100 pairs. The results of ran-dom projection with a Gaussian distributed random matrix (I~P), random projection with a sparse random matrix as in (3) (SRP), principal component analysis (PCA) and discrete cosine transform (DCT) are shown, together with their 95 per cent confidence intervals. Figure 1: The error produced by RP (+), SRP (*), PCA ( X ) and DCT (o) on image data, and 95 % con-fidence intervals over 100 pairs of data vectors. 
In Figure 1 it is clearly seen that random projection (RP and SRP) yields very accurate results: dimensionality re-duction by random projection does not distort the data sig-nificantly more than PCA. At dimensions k &gt; 600, random projection and PCA give quite accurate results but the error produced by DCT is clearly visible. At smaller dimensions also PCA distorts the data. This tells us that the variation in the data is mostly captured by the first 600 principal com-ponents, because the error in PCA is dependent on the sum of omitted eigenvalues, and k is equal to the number of eigen-2Available from http://www, cis .hut. fi/proj ect s/ica/data/images/ values retained. In contrast, the random projection method continues to give accurate results until k = 10. One expla-nation for the success of random projection is the J-L scaling term ~ (Formula (2)), which takes into account the de-crease in the dimensionality. In PCA, such scaling would only be useful in the smallest dimensions but a straightfor-ward rule is difficult to give. 
Another point of interest is the computational complex-ity of the methods. Figure 2 shows the number of Mat-lab's floating point operations needed when using RP, SRP, PCA or DCT in dimensionality reduction, in a logarithmic scale. It can be seen that PCA is significantly more burden-some than random projection or DCT. (In the case of DCT, only the chosen data vectors were transformed instead of the whole data set; this makes the number of floating point operations rather small.) Figure 2: Number of Matlab's floating point oper-ations needed when reducing the dimensionality of image data using RP (+), SRP (*), PCA (o) and DCT (o), in a logarithmic scale. 
From Figures 1-2 we can conclude that random projec-tion is a computationally inexpensive method of dimension-ality reduction while preserving the distances of data vectors practically as well as PCA and clearly better than DCT. Even more, at smallest dimensions RP outperforms both PCA and DCT. 
Dimensionality reduction on image data differs slightly from another common procedure, image compression, in which the image is transformed into a more economical form for e.g. transmission, and then transformed back into the original space. The transformation is often chosen so that the resulting image looks as similar as possible to the orig-inal image, to a human eye. In this respect, the discrete cosine transform has proven optimal. To see how an im-age whose dimensionality is reduced by RP would look like, the random mapping should be inverted. The pseudoin-verse of R is expensive to compute, but since R is almost orthogonal, the transpose of R is a good approximation of the pseudoinverse, and the image can be computed as new DT - X RP where X RP X~x N = is the result of the random projection (1). Nonetheless, the obtained image is visually worse than a DCT compressed image, to a human eye. Thus random projection is successful in applications where the distance or similarity between data vectors should be pre-served under dimensionality reduction as well as possible, but where the data is not intended to be visualized for the human eye. These applications include, e.g., machine vi-sion: it would be possible to automatically detect whether an (on-line) image from a surveillance camera has changed or not. 
In our second set of experiments we considered noisy im-ages. The images were corrupted by salt-and-pepper im-pulse noise: with probability 0.2, a pixel in the image was turned black or white. We wanted to project the data in such a way that the distance between two data vectors in the reduced noisy data space would be as close as possible to the distance between these vectors in the high-dimensional noiseless data space, even though the dimensionality reduc-tion was applied to high-dimensional noisy images. 
A simple yet effective way of noise reduction especially in the case of salt-and-pepper impulse noise is median fil-tering (MF) where each pixel in the image is replaced by the median of the pixel brightnesses in its neighborhood. The median is not affected by individual noise spikes and so median filtering eliminates impulse noise quite well [27]. A common neighborhood size is 3 x 3 pixels which was also used in our experiments. MF is computationally very efficient, of order O(dmN) for N image windows of d pixels, where m denotes the size of the neighborhood (in our case, m = 9). Also, MF does not require dimensionality reduction; thus its result can be used as a yardstick when comparing methods for dimensionality reduction and noise cancellation. 
Figure 3 shows how the distance between two noisy image windows is distorted in dimensionality reduction, compared to their distance in the original high-dimensional, noiseless space. Here we can compare different dimensionality re-duction methods with respect to their sensitivity to noise. We can see that median filtering introduces quite a large distortion in the image windows, despite that to a human eye it removes impulse noise very efficiently. The distor-tion is due to blurring: pixels are replaced by the median of their neighborhood, eliminating noise but also small details. PCA, DCT and random projection perform quite similarly to the noiseless case. From Figure 3 we can conclude that random projection is a promising alternative to dimension-ality reduction on noisy data, too, as it does not seem to be sensitive to impulse noise. There exists of course many other methods for noise reduction, too. Here our interest was mainly in dimensionality reduction and not noise re-duction. 
Next, we applied dimensionality reduction techniques on text document data from four newsgroups of the 20 news-groups corpus3: sci.crypt, sci.med, sci.space and soc.religion. christian. The documents were converted into term fre-quency vectors and some common terms were removed using McCallum's Rainbow toolkit 4 but no stemming was used. 
The data was not made zero mean, nor was the overall variance of entries of the data matrix normalized. The doc-ument vectors were only normalized to unit length. This kind of preprocessing was different from that applied to im-3Available from http ://www. cs. cmu. edu/'textlearning 4Available from http://www, cs. cmu. edu/~mccallum/bow Figure 3: The error produced by RP (+), SRP (*), PCA (o), DCT (o) and MF (-) on noisy image data, with 95% confidence intervals over 100 pairs of im-age windows. In MF dimensionality is not reduced. age data. Together with the distinct natures of image and text data, differences in preprocessing yielded slightly dif-ferent results on these different data sets. The size of the vocabulary was d = 5000 terms and the data set consisted of N = 2262 newsgroup documents. 
We randomly chose pairs of data vectors (that is, docu-ments) and computed their similarity as their inner product. The error in the dimensionality reduction was measured as the difference between the inner products before and after the dimensionality reduction. Figure 4: The error produced by RP (+) and SVD (o) on text document data, with 95% confidence in-tervals over 100 pairs of document vectors. 
Figure 4 shows the error introduced by dimensionality re-duction. The results are averaged over 100 document pairs. The results of SVD and random projection with a Gaus-sian distributed random matrix are shown, together with 95 per cent confidence intervals. The reduced dimensionality k took values in [1,700]. It is seen that random projection is not quite as accurate as SVD but in many applications the error may be neglectable. The Johnson-Lindenstrauss result [15] states that Euclidean distances are retained well in random projection. The case of inner products is a differ-ent one --Euclidean distances of document vectors would probably have been preserved better. It is a common prac-tice to measure the similarity of document vectors by their inner products; thus we present results on them. 
Despite using efficient SVD routines for finding a few sin-gular vectors of a sparse matrix, SVD is still orders of mag-nitude more burdensome than RP. 
Our results on text document data indicate that random projection can be used in dimensionality reduction of large document collections, with less computational complexity than latent semantic indexing (SVD). Similarly to what was presented in [22], RP can speed up latent semantic indexing (LSI): the dimensionality of the data is first reduced by RP and the burdensome LSI is only computed in the new low-dimensional space. In [22] the documents were generated artificially and the random matrix R was assumed strictly orthogonal; our experiments show that neither of these re-strictions is actually necessary. Another common problem in text document retrieval is query matching. Random projec-tion might be useful in query matching if the query is long, or if a set of similar documents instead of one particular document were searched for. 
We have presented new and promising experimental re-sults on random projection in dimensionality reduction of high-dimensional real-world data sets. When comparing dif-ferent methods for dimensionality reduction, the criteria are the amount of distortion caused by the method and its com-putational complexity. Our results indicate that random projection preserves the similarities of the data vectors well even when the data is projected to moderate numbers of dimensions; the projection is yet fast to compute. 
Our application areas were of quite different natures: noisy and noiseless images of natural scenes, and text documents from a newsgroup corpus. In both application areas, random projection proved to be a computationaUy simple method of dimensionality reduction, while still preserving the similar-ities of data vectors to a high degree. 
We also presented experimental results of random pro-jection using a sparsely populated random matrix intro-duced in [1]. It is in fact not necessary to use a Gaussian distributed random matrix but much simpler matrices still obey the Johnson-Lindenstrauss lemma [15], giving compu-tational savings. 
One should emphasize that random projection is benefi-cial in applications where the distances of the original high-dimensional data points are meaningful as such --if the orig-inal distances or similarities axe themselves suspect, there is little reason to preserve them. For example, consider using the data in neural network training. Projecting the data onto a lower dimensionM subspace speeds up the training only if the training is based on interpoint distances; such problems include clustering and k Nearest Neighbors etc. Also, consider the significance of each of the dimensions of a data set. In a Euclidean space, every dimension is equally important and independent of the others, whereas e.g. in a process monitoring application some measured quantities (that is, dimensions) might be closely related to others, and the interpoint distances do not necessarily bear a clear meaning. 
A still more realistic application of random projection would be to use it in a data mining problem, e.g. clustering, and compare the results and computational complexity of mining the original high-dimensional data and dimensional-ity reduced data; this is a topic of a further study. 
An interesting open problem concerns k, the number of dimensions needed for random projections. The Johnson-Lindenstrauss result [15, 10, 8] gives bounds that are much higher than the ones that suffice to give good results on our empirical data. For example, in the case of our image data, the lower bound for k on e = 0.2 is 1600 but in the ex-periments, k ~ 50 was enough. The Johnson-Lindenstrauss result, of course, is a worst-case one, and it would be inter-esting to understand which properties of our experimental data make it possible to get good results by using fewer dimensions. 
We conclude that random projection is a good alternative to traditional, statistically optimal methods of dimension-ality reduction that are computationally infeasible for high dimensional data. Random projection does not suffer from the curse of dimensionality, quite contrary to the traditional methods. [1] D. Achlioptas. Database-friendly random projections. [2] C. C. Aggarwal, J. L. Wolf, and P. S. Yu. A new [3] R. Agrawal, C. Faloutsos, and A. Swami. Efficient [4] R. I. Arriaga and S. Vempala. An algorithmic theory [5] M.-W. Berry. Large-scale sparse singular value [6] S. Dasgupta. Learning mixtures of Gaussians. In ,~Oth [7] S. Dasgupta. Experiments with random projection. In [8] S. Dasgupta and A. Gupta. An elementary proof of [9] S. Deerwester, S.T. Dumais, G.W. Furnas, and T.K. [10] P. Frankl and H. Maehara. The Johnson-Lindenstrauss [11] G.H. Golub and C.F. van Loan. Matrix Computations. [12] A. Craps. An introduction to wavelets. IEEE [13] R. Hecht-Nielsen. Context vectors: general purpose [14] P. Indyk and R. Motwani. Approximate nearest [15] W.B. Johnson and J. Lindenstranss. Extensions of [16] S. Kaski. Data exploration using self-organizing maps. [17] S. Kaski. Dimensionality reduction by random [18] E. J. Keogh and M. J. Pazzani. A simple [19] J.M. Kleinberg. Two algorithms for nearest-neighbor [20] M. Kurimo. Indexing audio documents by using latent [21] R. Ostrovsky and Y. Rabani. Polynomial time [22] C.H. Papadimitriou, P. Raghavan, H. Tamaki, and [23] K.R. Rao and P. Yip. Discrete Cosine Transform: [24] S. Roweis. EM algorithms for PCA and SPCA. In [25] G. Salton and M.J. McGill. Introduction to modern [26] L. Sirovich and R. Everson. Management and analysis [27] M. Sonka, V. Hlavac, and R. Boyle. Image processing, [28] S. Vempala. Random projection: a new approach to 
