 With the rapid development of Internet and E-commerce, abundant product reviews have been written by consumers who bought the products. These reviews are very useful for consumers to optimize their purchasing decisions. How-ever, since the reviews are all written by consumers who have bought and used a product, there are generally very few or even no reviews available for a new product or an un-popular product. We study the novel problem of retrieving relevant opinion sentences from the reviews of other prod-ucts using specifications of a new or unpopular product as query. Our key idea is to leverage product specifications to assess product similarity between the query product and other products and extract relevant opinion sentences from the similar products where a consumer may find useful dis-cussions. Then, we provide ranked opinion sentences for the query product that has no user-generated reviews. We first propose a popular summarization method and its mod-ified version to solve the problem. Then, we propose our novel probabilistic methods. Experiment results show that the proposed methods can effectively retrieve useful opinion sentences for products that have no reviews.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models ; H.4.0 [ Information Systems Applications ]: General Algorithms, Design opinion mining, probabilistic information retrieval c  X 
The role of product reviews has been more and more im-portant. Reevoo, a social commerce solutions provider, sur-veyed 1,000 consumers on shopping habits and found that 88 percent of them sometimes or always consult customer reviews before purchase. 1 According to the survey, 60 per-cent of them said that they were more likely to purchase from a site that has customer reviews on. Also, they consid-ered customer reviews more influential (48%) than advertis-ing (24%) or recommendations from sales assistants (22%). With the development of Internet and E-commerce, people X  X  shopping habits have changed, and we need to take a closer look at it in order to provide the best shopping environment to consumers.

Even though product reviews are considered important to consumers, the majority of the products has only a few or no reviews. Products that are not released yet or newly released generally do not have enough reviews. Also, un-popular products in the market lack reviews because they are not sold and exposed to consumers enough. How can we help consumers who are interested in buying products with no reviews? In this paper, we propose methods to au-tomatically retrieve review text for such products based on reviews of other products. Our key insight is that opinions on similar products may be applicable to the product that lacks reviews. For example, if products X and Y have the same CPU clock rate, then people X  X  opinion on CPU clock rate for product X may be applicable to that for product Y as well. The similarity between products can be computed based on product specifications which are often available, where an example of product specifications is shown in Fig-ure 1. %Here is an example of review text we manually retrieved for a certain product X  X  specification  X  X esolution: 12.1 megapixels X  from real reviews of products that have the same resolution. https://www.reevoo.com/news/half-of-consumers-find-social-content-useful-when-shopping-online/ Figure 1: A part of product specifications in CNET.com. Even though these sentences are not necessarily coherent opinions, they are clearly very useful for users to understand the product feature and get access to relevant discussion of opinions. Since a user would hardly have a clue about opin-ions on a new product, such a retrieved review text can be expected to be useful. As a minimum, it can be very useful to help users prioritizing what to read in the existing reviews of other products. Not only from a consumer X  X  perspective, but also from a manufacturer X  X  perspective, such techniques would be beneficial to collect opinions on its new or unpop-ular products. From the retrieved opinions, the manufac-turers would be able to predict what consumers would think even before their product release and react to the predicted feedback in advance.

This paper makes the following contributions: 1. We introduce and study a novel problem of relevant 2. To solve the problem, we propose a new probabilis-3. We create a new data set for evaluating the new prob-
In order to evaluate the automatically retrieved opinions for a new or unpopular product, we pretend that the query product does not have reviews and predict its review text based on similar products. Then, we compare the predicted review text with the query product X  X  actual reviews to eval-uate the performance of suggested methods. Experiment results show that our translation model effectively retrieves opinions for a product without reviews and it significantly outperforms baseline methods.
Reviews are one of the most popular sources in opinion analysis. Opinion retrieval and summarization techniques attracted a lot of attentions because of its usefulness in Web 2.0 environment. There are several surveys which summa-rize the existing opinion mining work [9, 21, 14]. Compared to text data in other general retrieval problems, opinionated articles such as product reviews have some different charac-teristics. In opinion analysis, analyzing polarities of input opinions are crucial. Also, majority of the opinion retrieval works are based on product feature (aspect) analysis. They first find sub-topics (features) of a target and show positive and negative opinions for each aspect. By further segment-ing the input texts into the smaller units, they showed more details in a structured way [7, 15, 16, 18, 25, 28, 10]. Mean-while, product reviews have been also employed to predict ratings [20, 5] or sales [3] of a product. However, no existing work addressed the problem of retrieving opinion sentences for new products yet.

In this paper, we also utilize unique characteristics of product data: specifications (structured data) as well as re-views (unstructured data). Although product specifications have been provided in many e-commerce web sites, there are only a limited number of studies that utilized specifi-cations for product review analysis. Zhou and Chaovalit [32] performed sentiment classification on reviews using do-main ontology database, which may be regarded as product specifications. Bhattattacharya et al . [2] employed IMDb X  X  structured data to categorize documents, and Yu et al. [30] built an aspect hierarchy using product specifications and reviews. Wang et al . [29] and Pe  X nalver-Mart  X  X nez et al . [23] also employed product specifications to summarize product features. Product reviews and specifications were jointly modeled using topic models by Duan et al. [4] to improve product search and by Park et al. [22] to generate aug-mented specifications with useful information. Park et al. [22] retrieved review sentences for each (feature, value) pair, but they did not study their model X  X  performance on prod-ucts with no reviews. In addition, their model does not consider similarity among products or specifications, which is an important factor for the problem. Likewise, there are a few studies that employed product specifications, but their goals are different from ours.

Our work is related to text summarization, which consid-ers centrality of text. Automatic text summarization tech-niques have been studied for a long time due to the need of handling large amount of electronic text data [19, 11, 6]. Automatic summarization techniques can be categorized into two types, extractive summary and abstractive sum-mary. Extractive summarization makes a summary by se-lecting representative text segments, usually sentences, from the original documents. Abstractive summarization does not directly reuse the existing sentences but generates sen-tences based on text analysis. Our work is similar to extrac-tive summarization in that we select sentences from original documents but different in that we retrieve sentences for an entity that does not have any text. Among the previous work, MEAD [26] is one of the most popular public extrac-tive summarization toolkits, which supports multi-document summarization in general domain. The goal of MEAD is dif-ferent from ours in that we want a summary for a specific product, and also MEAD does not utilize external structured data (specifications).

Cold start problem in recommendation systems [27], where no one has rated new items yet, is also related to our prob-lem. However, unlike rating connections between items and users, each user review carries its unique and complex mean-ing, which makes the problem more challenging. Moreover, our goal is to provide useful relevant opinions about a prod-uct, not recommending a product. XML retrieval [12] that utilizes structured information of documents is also related to our work, in that reviews and specifications can be rep-resented as a special XML. However, unlike general XML retrieval, in this paper, we propose more specialized meth-ods for product reviews using product category and specifi-cations. In addition, because we require the retrieved sen-tences to be central in reviews, we consider both centrality and relevance while general retrieval methods focus on rel-evance only. As far as we know, none of the existing work tried to solve the same problem as ours.
The product data consists of N products { P 1 ,...,P N Each product P i consists of its set of reviews R i = { r and its set of specifications S i = { s i, 1 ,...,s i,F } , where a spec-ification s i,k is a feature-value pair, ( f k ,v i,k ), and F is the number of features. Given a query product P z , for which R is not available, our goal is to retrieve a sequence of relevant opinion sentences T in K words for P z .

Note that our problem setup is a mixture of retrieval and summarization. On the one hand, it can be regarded as a ranking problem, similar to retrieval; on the other hand, it can also be regarded as a summarization problem since we restrict the total number of words in the retrieved opinions .

This is a new problem that has not been addressed in any previous work. The problem is challenging for several reasons. Retrieved sentences for P z should conform to its specifications S z while we do not know which sentences are about which specific feature-value pair. In addition, the retrieved sentences should be central across relevant reviews so that they reflect central opinions. Despite the challenges, we try to show that achieving the goal is feasible. In the next a few sections, we propose multiple methods to solve the problem.
When reviews are not available for a product, a consumer has no way to obtain opinions on it. In order to help con-sumers in such situation, we believe that product specifi-cations are the most valuable source to find similar prod-ucts. We thus leverage product specifications to find similar products and choose relevant sentences from their user re-views. In this approach, we assume that if products have similar specifications, the reviews are similar as well. For example, here is an actual review sentence from the review of a digital camera that takes a picture at high resolution:  X  X he best camera I have ever owned, takes unbelievable crisp sharp photos with it X  X  16.1 Megapixels. X  It is admitted that the consumer is very impressed with the feature-value pair, ( X  X esolution X ,  X 16.1 Megapixels X ), and we can expect that other digital cameras with the same feature-value pair could impress their consumers as well. The assumption may not be valid in some cases, i.e. , same specifications may yield very different user reviews. We thus try to retrieve  X  X en-tral X  opinions from similar products so that the retrieved sentences can become clearly useful.
We assume that similar products have similar feature-value pairs (specifications). In general, there are many ways to define a similarity function. We are interested in find-ing how well a basic similarity function will work although our framework can obviously accommodate any other simi-larity functions. Therefore, we simply define the similarity function between products as where w k is a weight for the feature f k , and the weights { w 1 ,...,w F } are assumed identical ( w k = 1) in this study, so the similarity function becomes where SIM f ( s i,k ,s j,k ) is a cosine similarity for feature f between P i and P j and is defined as where v i , k and v j , k are phrase vectors in values v i,k respectively. Both SIM p ( P i ,P j ) and SIM f ( s i,k ,s from 0 to 1.

In this paper, we define the phrases as comma-delimited feature values. SIM f ( s i,k ,s j,k ) is similar to cosine similar-ity function, which is used often for measuring document similarity in Information Retrieval (IR), but the difference is that we use a phrase as a basic unit while a word unit is usually adopted in IR. We use a phrase as a basic unit because majority of the words may overlap in two very dif-ferent feature values. For example, the specification phrases  X  X emory Stick Duo X ,  X  X emory Stick PRO-HG Duo X ,  X  X em-ory Stick PRO Duo X , and  X  X emory Stick PRO Duo Mark2 X  have high word cosine similarities among themselves since they at least have 3 common words while the performances of the specifications are very different. Thus, our similarity function with phrase unit counts a match only if the phrases are the same.
In this section, we suggest multiple methods for relevant opinion sentences retrieval. We first suggest a standard sum-marization tool, MEAD [26]. In order to make up for the MEAD X  X  weak points, we also suggest modified version of MEAD. Then, we propose our probabilistic models to solve the problem.
For our problem, text retrieval based only on query-relevance is not desirable. The retrieved sentences need to be central in other reviews in order to obtain central opinions about spec-ifications. For example, if there are more opinions that con-tains a word  X  X ig X  than a word  X  X mall X  for a certain feature-value pair, it is desired to assign higher score to the sentences having the word  X  X ig X . However, since the query contains only feature-value pair words, classic information retrieval approaches are not able to prefer such sentences. Therefore, we suggest using a method that considers centrality among sentences.

MEAD [26] is a popular centroid-based summarization tool for multiple documents, and it was shown to effectively generate summaries from a large corpus. It provides an auto-generated summary for multiple documents. For a corpus R , a score of i th sentence t in a document is computed by sum of centroid and position scores of words, which is defined as where C t is a sum of centroid scores of words in t , which is defined as C t = P w C w,t , and O t is a position score, which gives higher score to the sentences appearing earlier in a document and defined as O t = ( n  X  i +1) n  X  C max where n is the number of sentences in the document and C max is the maximum centroid score in the document. Centroid score of a word, C w,t , is a TFIDF value in the corpus R , and w and w o are weights for C t and O t , respectively. Please refer to [26] for more details.

In order to retrieve sentences that are likely to be rel-evant to the query product P z , which has no reviews, we employ specifications to find products similar to P z and use the similarity as a clue for finding relevant sentences. Since the score formula (4) utilizes only centrality and does not consider relevance to the query product, we augment it with product similarity to P z so that we can find sentences that are query-relevant and central at the same time. In addition, MEAD employs position score that is reasonable for news articles, but it may not be appropriate for reviews; unlike news articles, it is hard to say that the sentences appearing earlier in the reviews are more important than those appear-ing later. Thus, we remove position score term from formula (4), and we augment it with similarity to query. The new score function is defined as where t is a sentence in a review for product P y and SIM ( S y ,S z ) is a product similarity between P y and the query product P z , which is defined in equation (2).
To solve the problem in a more principled way, we intro-duce our probabilistic methods. Query likelihood retrieval model [1], which assumes that a document generates a query, has been shown to work well for ad-hoc information retrieval. Similarly, we attempt to generate the query specifications S from a candidate sentence t via several generative scenarios.
The generative story is described as follows. Each sen-tence t from reviews of its product P y first generates its specifications S y . The specifications S y then generates the query specifications S z . Following the dependencies among variables, the scoring function is defined as We can interpret p ( t,S y | S z ) as the probability that t and S y satisfy information needs of a user given S z . p ( S measures proximity of S y to S z . p ( S y | t ) measures proximity of t to S y , and p ( t ) is a general preference on t . Since we as-sume no preference on sentences, we ignore p ( t ) for ranking. p ( S z ) is also ignored because it does not affect the ranking of sentences for S z . Thus, the formula assigns high score to a sentence if its specifications S y match S z well and the sentence t matches its specifications S y well. p ( t,S y then defined as where a set of specifications such as S y is decomposed into feature-value pairs s y,k . We assume that a k  X  X h feature-value pair of one specification set generates only the k  X  X h feature-value pair of another specification set, not other feature-value pairs. This is to ensure that sentences not related to a specification s z,k are scored low even if their word score p ( s y,k | t ) is high. p ( s z,k | s y,k ), proximity of s estimated as follows. where Distinct ( k ) is a set of distinct feature-value pairs for a feature f k . p ( s y,k | t ) is defined as where U is a vocabulary set in corpus R , and c ( w,s y,k count of word w in the feature-value pair s y,k . p ( w | t ) follows t  X  X  unigram language model [24], and it means a word w  X  X  likelihood in a sentence t . One of the standard ways to es-timate p ( w | t ) is using maximum likelihood (ML) estimator, in t , and | t | is the number of words in t . Thus, p ( s likelihood of a feature-value pair s y,k in a sentence t , be-comes higher if more words in the feature-value pair appear often in t . To avoid over-fitting and prevent p ( s y,k being zero, we smooth p ( w | t ) with Jelinek-Mercer smooth-ing method [8], which is shown in [31] to work reasonably well. Using Jelinek-Mercer smoothing, p ( w | t ) is defined as: where p ml ( w | t ) and p ( w | R ) follow a sentence language model and a corpus language model, respectively, estimated with ML estimator. To smooth p ( w | t ), a reference language model p ( w | R ) is used so that we can have general word likelihood that nicely augments p ml ( w | t ). The resulting p ( w | t ) can be regarded as weighted average of p ml ( w | t ) and p ( w | R ).
Specifications Generation model in section 6.2.1 does not consider centrality among reviews. However, as explained in section 6.1, centrality as well as query-relevance should be considered for the task. Here, we assume that a candidate sentence t of product P y generates the product X  X  reviews R except itself t . This generation enables us to measure cen-trality of t among all other sentences in the reviews for P Then, t and R \ t y jointly generate its specifications S R y is a set of reviews for P y except the sentence t . Intu-itively, it makes more sense for S y to be generated by both t and R \ t y than by only t . S y then generates the query speci-fications S z . Following the dependencies, the score function is defined as score ( t,R \ t y ,S y ; R,S z )  X  p ( t,R \ t y ,S y | S where p ( t ) and p ( S z ) are ignored for the same reason as in section 6.2.1. Now, p ( R \ t y | t ), a proximity of t to the re-views R \ t y , is computed to consider centrality of t . Also, p ( S y | t,R \ t y ), a proximity of t and R \ t y to the specifications S , is computed to promote sentences from reviews that match its specifications well. Thus, a sentence t is pre-ferred if (1) its specifications S y is similar to S represent its reviews R y well, and (3) R \ t y represents t well. p ( t,R \ t y ,S y | S z ) can be re-written as p ( t,R \ t y ,S y | S z ) = p ( R \ t y | t ) where p ( w | t,R \ t y ) is smoothed to (1  X   X  )  X  ( w | t,R where  X  ( w | t,R \ t y ) is defined as We ignore w if w is not in t in order to require the retrieved sentences to contain words in s y,k . The proximity of t to R y , p ( R \ t y | t ), is estimated by TFIDF cosine similarity func-tion SIM ( R \ t y ,t ), where TFIDF cosine similarity between documents d and d 0 is defined as where IDF of word w is defined as where | R | is the number of reviews in the whole corpus, and DF ( w ) is the number of documents that contain w .
In Review and Specifications Generation model, we as-sumed a sentence t of product P y generates its reviews R and t and R y jointly generate their specifications S y . How-ever, we can also assume that t generates reviews of an arbi-trary product because there may be better reviews that can represent t and generate S y well. In other words, there may be a product P x that translates t and generates P z based on the translation with a better performance.

The generative story is described as follows. A candidate sentence t of a product P y generates each review set of all products, which will be used as translations of t . t and each of the generated review sets, R x , jointly generates t  X  X  specifications S y , and S y generates specifications of R and the query specifications S z . We intend S y to generate specifications of the translating product S x so as to penalize the translating product if its specifications are not similar to S y . Following the generative story, the score function is defined as where p ( S z ) and p ( t ) are ignored for the same reason as before. As described, the score function contains a loop over all products (except P z ), instead of using only t  X  X  review set R y , to get the votes from all translating products. The features in different specifications are paired together, which decompose p ( t,S y | S z ) as follows. where proximity between specifications are estimated using cosine similarity function SIM f as in specifications genera-tion model, and the proximity of t to arbitrary reviews R p ( R x | t ), is estimated by TFIDF cosine similarity function. In order to consider the case P y is the same as P x , we define p ( w | t,R x ) as p ( w | t,R x ) =
Meanwhile, looping over all non-query products is prob-ably too expensive in terms of computational complexity. We thus choose X translating products P X to reduce the complexity. Perhaps, the most promising translating prod-ucts may be those who are similar to the query product P z We want the retrieved sentences to be translated well by the actual reviews of P z , which means that those reviews of products not similar to P z are not considered important. Since we assume that products similar to P z are likely to have similar reviews, we exploit the similar products X  re-views to approximate R z , where we measure similarity using specifications. Therefore, we loop over only X translating products P X that are most similar to P z , where similarity function SIM p is employed to measure similarity between products. Since P x needs to be similar to P z , we further assume that P x generates P z , which yields proximity of P to P z , p ( P z | P x ), and it is defined as and this product-level similarity is used as a weight of P formula (17).
Since we study a new task that has not been studied be-fore, there is no existing test collection available to use for evaluation. We thus must solve the challenge of creating a test set. We address this problem by using products with known reviews as test cases. We pretend that we do not know their reviews and use our methods to retrieve sentences in K words; we then compare these results with the actual known review of a test product. This allows for evaluating the task without requiring manual work, and is a reasonable way to perform evaluation because it would reward a system that can retrieve review sentences that are very similar to the actual review sentences of a product.
 We now describe how to build our data set in detail. First, it is required for our problem to obtain reviews and specifications for products, and this kind of data is avail-able in several web sites such as Amazon.com, BestBuy.com, and CNET.com. Among them, we chose CNET.com be-cause they have reasonable amount of review data and rela-tively well-organized specifications. There are several prod-uct categories in CNET.com, and we chose digital camera and MP3 player categories since they are reasonably popu-lar and therefore the experiment results can yield significant impact. From CNET.com, we crawled product information for all products that were available on February 22, 2012 in both categories. For each product, we collected its user reviews and specifications.
 Table 1: Statistics of the data for digital camera and MP3 player categories.

We pruned out products that do not contain reviews or specifications. (We found that about two thirds of the prod-ucts didn X  X  have any user reviews.) To preprocess the re-view text, we performed sentence segmentation, word tok-enization, and lemmatization using Stanford CoreNLP [17] version 1.3.5. We lowered word tokens and removed punc-tuations. Then, we removed word tokens that appear in less than five reviews and stopwords. We also preprocessed specifications data. In general, specifications contain dozens or hundreds of distinct features, and many of them are not mentioned in the reviews. Therefore, we choose features that are considered important by users. In order to choose such key features, we simply adopt highlighted features pro-vided by CNET.com assuming that they chose the features based on importance. The highlighted features are listed in Table 2. We removed feature values that appear in less than five products. Then, we tokenized the feature and feature value words, and we lowered the word tokens. The statis-tics of the reviews and specifications data is shown in Table 1. While digital camera category has more products, more reviews are written for mp3 player categories. Also, in gen-eral, users wrote more texts per review for mp3 players than digital cameras. The number of highlighted features used for digital cameras is similar to that for mp3 players while there are much more distinct feature values for digital cameras. Table 2: CNET.com X  X  highlighted features for digi-tal camera and MP3 player categories.

In order to evaluate the performance of our methods for retrieving review sentences for a new or unpopular product, we perform the following experiment. To choose test prod-ucts, which will be regarded as products with no reviews, we selected top 50 qualified products by the number of reviews in each category in order to obtain statistically reliable gold standard data. Please note that we did not select (qualify) products that have their different versions such as colors or editions, in order to ensure that review sentences from the different version of the same product are not retrieved. For each of the top products, P z , all sentences of other products are regarded as candidate sentences. Pretending P z does not have any reviews, we rank those candidate sentences and generate a text of first K word tokens, and we compare it with the actual reviews of P z . We assume that if the gener-ated review text is similar to the actual reviews, it is a good review text for P z . The average number of reviews in the top 50 products is 78.5 and 152.2 for digital cameras and mp3 players, respectively. For the probabilistic retrieval models, we use  X  to control the amount of smoothing for language models, and we empirically set it to 0.5 for both product categories, which showed the best performance.
To evaluate a quality of the length-K retrieved text based on actual reviews for the query, we face another challenge: how should we measure the performance? We could con-sider using standard retrieval measures, but neither NDCG, nor MAP seems appropriate since we do not have multiple levels of judgments or even binary judgments. We thus de-cided to measure the proximity between the retrieved text and the actual reviews. Regarding the retrieved text as a summary for the query product, we can view our task as similar to multiple document summarization, whose goal is to generate a summary of multiple documents. Thus, we em-ploy ROUGE evaluation method [13], which is a standard evaluation system for multiple document summarization. In general, ROUGE evaluates the quality of an automatically generated summary by comparing it with one or more man-ually generated reference summaries. Assuming the actual reviews of the query product are manually generated ref-erence summaries, we can adopt ROUGE to evaluate the retrieved sentences. Among various ROUGE metrics, we employ ROUGE-1 and ROUGE-2, which are unigram and bigram matching metrics, respectively, and have been shown to perform well for the task. We compute precision, recall, and F1-score of each metric. For example, recall of ROUGE-n is defined as ROUGE-n ( r,s ) = where r and s are reference and retrieved summaries, re-spectively, gram n is n-gram text, Count match ( gram n ) is the maximum number of n-grams co-occurring in the retrieved summary and a reference summary. When there are multi-ple reference summaries are available, they use the following evaluation formula.
 Please note that each of the precision, recall, and F1-score takes the maximum from the reference summaries. More details about ROUGE can be found in [13].

However, the problem of ROUGE metrics is that it does not consider importance of words. All words have different level of importance; for example a word such as  X  X f X  is much less important than a word  X  X egapixel X  since  X  X f X  appears too often in documents and does not carry useful informa-tion. If a retrieved text contains many unimportant words, it may obtain a high score by ROUGE metrics, which is not desired. Therefore, we also employ TFIDF cosine sim-ilarity, which considers word importance by Inverse Doc-ument Frequency (IDF). TFIDF cosine similarity function between two documents is defined in equation (14). While the formula measures similarity based on bag of words, bi-gram provides important information about distance among words, so we adopt bigram-based TFIDF cosine similarity as well. Similar to ROUGE-n multi , we take a maximum from SIM ( r i ,s ) among different reference summaries because we still evaluate based on multiple reference summaries. For both ROUGE and SIM metrics, we use retrieved text length 100, 200, and 400, which reflect diverse users X  information needs. Table 3: Top ten sentences retrieved for Pentax *ist DS (Digital Camera) by Translation model with X =5. Table 4: Specifications for Pentax *ist DS. Note that some feature values are not available.

In order to see the usefulness of the sentences retrieved by our novel Translation model, we show the top retrieved sentences for query products and compare them with the actual review sentences for the query products. Table 3 lists top retrieved sentences for a product in each category, where the sentences are ordered by their scores, and the specifications of the product is listed in Table 4.We set the number of translating products to five, which is reasonable if we consider the computational cost of the model.
For the digital camera Pentax *ist DS, several top re-trieved sentences such as (2), and (8) mention about its compatibility with Pentax lenses. Surprisingly, there were several reviews for Pentax *ist DS that praise its lens com-patibility, and here are two actual examples from review sentences:  X  X lus the DS is backwards compatible with all old Pentax lenses, which have a well-deserved reputation among photographers. X  and  X  X  can use my pile of old (and very old) Pentax lenses including the m42 lenses. X  Also, the retrieved sentences such as (7), (8), (9), and possibly (3) mention about Pentax X  X  great picture quality, which is supported by the following actual review sentences:  X  X maz-ingly sharp lens. X  and  X  X t has a much better lens package than the Rebel and the base 20D kit. X  Sentences (2) and (6) claim the product X  X  good value, which is again supported by actual review sentences:  X  X etter value than you think X  and  X  X he camera is also cheaper than the comparable Nikon and Canon. X  The retrieved sentence such as (8) mentions about ease of use for the camera, and many users actually complimented the camera on its ease of use, indeed. The supporting sentences are as follows:  X  X ery easy to use right out of the box. X  and  X  X he controls are very easy to learn and are, for the most part, very intuitive. X  Meanwhile, the sentence (1) carries inconsistent opinion, which shows nega-tive sentiment on Pentax camera. Nevertheless, in a user X  X  perspective, who does not know much about Pentax *ist DS or other Pentax cameras, the listed information would be highly informative especially if the camera has no or few reviews. Although some of the retrieved sentences do not carry useful information, it is clear that some other retrieved sentences are indeed useful.

Our probabilistic retrieval models have a capability of re-trieving relevant sentences for a specific feature. For each of the probabilistic models, we can assume that the num-ber of features F is one so that the score functions compute only for one feature. Table 5 shows top retrieved sentences for the feature  X  X ens System  X  Type X  of Pentax *ist DS. As found in the top sentences for the whole product in Table 3, we can easily find that all the sentences except (2) praise the lens compatibility of Pentax, indeed. In addition, all sentences except (1) praises high quality of its lens, which is coherent with the top sentences for the whole product. From the sentences, users can learn much about the given product X  X  lens such as other consumers X  general sentiment and specific reasons why they like or dislike its lens. Table 5: Top sentences retrieved by Translation model ( X =5) specifically for the feature  X  X ens Sys-tem  X  Type X  of Pentax *ist DS.
Manually finding relevant opinions for a query product or its specific feature is extremely time-consuming for users; they need to find similar products by manually compar-ing specifications and extract relevant and central sentences from all the reviews of the similar products, which may take too much time. Here, we verified the automatically retrieved sentences can be indeed useful for users. In the next section, we quantitatively compare our Translation model with other suggested methods.
To retrieve review sentences that are likely to be writ-ten for a new or unpopular product, we employ several methods. In order to see the effectiveness of a standard ad-hoc retrieval method, we employ query likelihood (QL) language model approach [24], and we define the score func-0.333  X  X  0.0736  X  X  0.0743  X  X  0.0794  X  X  0.316  X  X  0.0458  X  X  0.0567  X  X  0.0649  X  X  tion as score ( t ; R,S z ) = P F k =1 Q w  X  s is smoothed as in equation (10). We suggested a modified version of one of the standard summarization tools, MEAD-SIM in formula (5), which considers both query-relevance and centrality. We employ MEAD-SIM as one of the base-line methods, and we also show results from the basic MEAD in formula (4) to see the effect of query-relevance addition to MEAD; we set w c = 1 and w o = 0 since position score is inappropriate for reviews. We also introduced several prob-abilistic retrieval methods for the task. Review and Specifi-cations Generation model (ReviewSpecGen) considers both query-relevance and centrality, so we use it as another base-line method. Specifications Generation model (SpecGen) fo-cuses on query-relevance, and we show its results to compare with ReviewSpecGen and QL. We then suggested our novel Translation model (Translation). We tuned X to be 100 for digital cameras and 10 for mp3 players, unless otherwise specified, which showed the best TFIDF cosine similarity values. The results from Translation model are mainly com-pared with the two baselines MEAD-SIM and ReviewSpec-Gen.  X  and  X  are used to mark if the improvement for Trans-lation model is statistically (paired t-test with p=0.05) sig-nificant in each measure from MEAD-SIM and ReviewSpec-Gen, respectively. We also record how much Translation model outperforms MEAD-SIM in parentheses.

Table 6 shows TFIDF cosine similarity evaluation results for both digital cameras and mp3 players. Both unigram (COS1) and bigram (COS2) measures are listed for the sug-gested methods. In general, models that exploit specifica-tions as query (MEAD-SIM, SpecGen, ReviewSpecGen, and Translation) except QL outperform MEAD, which does not compute query-relevance. QL outperforms MEAD in mp3 player data set, but it does not outperform other models in both data sets, since does not consider specifications similar-ity between products. MEAD-SIM outperforms MEAD in all cosine similarity measures (12/12), which means that cen-trality alone cannot perform well. ReviewSpecGen adds cen-trality computation to SpecGen, and the results show that its centrality helps it outperform SpecGen in most measures (9/12). ReviewSpecGen outperforms MEAD-SIM in all uni-gram measures (6/6) and most bigram measures (5/6). Trans-lation model significantly outperforms MEAD-SIM in all measures (12/12), and the average performance increase per-centage is 162%. It also significantly outperforms ReviewSpec-Gen in all measures (12/12), which means that choosing products similar to the query product as translating prod-ucts was more effective than choosing only one product the candidate sentence belongs. Translation model outperforms other models especially in bigram measures, which means that Translation model retrieves more connected fragments that are in the actual reviews. Figure 2: TFIDF cosine similarity evaluation results for Translation model with different number ( X ) of translating products. Upper figures are for digital cameras, and lower figures are for mp3 players. Left figures are results based on unigrams, and right fig-ures are those base on bigrams.
 We also evaluate retrieval results with ROUGE metrics. Although ROUGE does not consider importance of words, it is able to compute recall, precision, and F1 score in both unigram (ROUGE1-R, ROUGE1-P, and ROUGE1-F) and bigram (ROUGE2-R, ROUGE2-P, and ROUGE2-F) units.
 The ROUGE evaluation results for mp3 players are shown in Table 7. QL outperforms MEAD in all measures, but it is outperformed by MEAD-SIM in all measures since QL does not consider specifications similarity between products. SpecGen outperforms ReviewSpecGen in most measures (13/ 18) especially in bigram measures (9/9), which is different from the TFIDF cosine similarity results; this means that the sentences retrieved by SpecGen are more similar to ac-tual reviews than those retrieved by ReviewSpecGen, but ReviewSpecGen retrieved more  X  X mportant X  relevant words. Translation model outperforms all other models in all mea-sures (18/18), and the increase from MEAD-SIM and Re-viewSpecGen is statistically significant in most measures (17/18 and 18/18, respectively). Similar to TFIDF cosine similarity results, the performance difference in bigram is clearer than in unigram, which means Translation model re-trieves bigger fragments of actual reviews well. The increase in unigram ROUGE measures is not as big as that in uni-gram TFIDF cosine similarity measures, which means that the number of relevant words from Translation model is not very different from other models, but Translation model re-trieves much more important relevant words.

We also evaluated retrieved sentences for digital cameras with ROUGE metrics. In general, Translation model out-performs other models in all measures. More specifically, it significantly outperforms MEAD-SIM and ReviewSpecGen in most measures (16/18 and 18/18, respectively). We do not list ROUGE evaluation results for digital cameras since the other patterns are similar to those for mp3 players.
Overall, ROUGE evaluation results are similar to cosine similarity evaluation results in general. The difference be-tween the two metrics is that the TFIDF cosine similar-ity metric differentiates various models more clearly since they consider importance of word while the ROUGE metric does not; TFIDF cosine similarity metric prefers retrieved text that contains more important words, which is a desired property in such evaluation. On the other hand, ROUGE metric considers various evaluation aspects such as recall, precision, and F1 score, which can possibly help us analyze evaluation results in depth.

In order to reduce computation complexity of Translation model, we proposed to exploit X number of most promising products that are similar to the query product, instead of all products, under the assumption that similar products are likely to have similar reviews. We performed experiments with different X values to find how many translating prod-ucts are needed to obtain reasonably good performance. The results are evaluated with TFIDF cosine similarity @ K for unigrams and bigrams, and the results are shown in Figure 2. Surprisingly, only a few translating products (e.g., ten) are enough to perform reasonably well especially for mp3 players. These results mean that only a few  X  X ood X  translat-ing products are enough to translate a candidate sentence well, and the  X  X ood X  translating products may be selected by their similarity to the query product.
In this paper, we studied the problem of automatic rel-evant review text retrieval for products having no reviews. Relevant review sentences for new or unpopular products can be very useful for consumers who seek for relevant opin-ions, but no previous work has addressed this novel prob-lem. We proposed several methods to solve this problem, including summarization-based methods such as MEAD and MEAD-SIM and probabilistic retrieval methods such as Spec-ifications Generation model, Review and Specifications Gen-eration model, and Translation model. To evaluate relevance of retrieved opinion sentences in the situation where human-labeled judgments are not available, we measured the prox-imity between the retrieved text and the actual reviews of a query product. Experiment results show that our novel Translation model indeed retrieves useful sentences and sig-nificantly outperforms the baseline methods.

Our work opens up a new direction in text data mining and opinion analysis. The new problem of review text re-trieval for new products can be studied from multiple per-spectives. First, it can be regarded as a summarization prob-lem as the retrieved sentences need to be central across dif-ferent reviews. Second, as done in this paper, it can also be regarded as a special retrieval problem with the goal of retrieving relevant opinions with product specifications as a query. Finally, it can also be studied from the perspective of collaborative filtering where we would leverage related prod-ucts to recommend relevant  X  X pinions X  to new products. All these are interesting future directions that can potentially lead to even more accurate and more useful algorithms.
This work is supported in part by a gift fund from TCL and by the National Science Foundation under Grant Num-ber CNS-1027965. [1] A. Berger and J. Lafferty. Information retrieval as [2] I. Bhattacharya, S. Godbole, and S. Joshi. Structured [3] C. Dellarocas, X. M. Zhang, and N. F. Awad.
 [4] H. Duan, C. Zhai, J. Cheng, and A. Gattani.
 [5] G. Ganu, N. Elhadad, and A. Marian. Beyond the [6] E. Hovy and C.-Y. Lin. Automated text [7] M. Hu and B. Liu. Mining and summarizing customer [8] F. Jelinek. Interpolated estimation of markov source [9] H. D. Kim, K. Ganesan, P. Sondhi, and C. Zhai. [10] H. D. Kim and C. Zhai. Generating comparative [11] J. Kupiec, J. Pedersen, and F. Chen. A trainable [12] M. Lalmas. Xml retrieval (synthesis lectures on [13] C.-Y. Lin. Rouge: A package for automatic evaluation [14] B. Liu. Sentiment analysis and subjectivity. In [15] B. Liu, M. Hu, and J. Cheng. Opinion observer: [16] Y. Lu, C. Zhai, and N. Sundaresan. Rated aspect [17] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, [18] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai. [19] C. D. Paice. Constructing literature abstracts by [20] B. Pang and L. Lee. Seeing stars: Exploiting class [21] B. Pang and L. Lee. Opinion mining and sentiment [22] D. H. Park, C. Zhai, and L. Guo. Speclda: Modeling [23] I. Pe  X nalver-Mart  X  X nez, R. Valencia-Garc  X  X a, and [24] J. M. Ponte and W. B. Croft. A language modeling [25] A.-M. Popescu and O. Etzioni. Extracting product [26] D. R. Radev, H. Jing, and M. Budzikowska.
 [27] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. [28] I. Titov and R. McDonald. Modeling online reviews [29] T. Wang, Y. Cai, G. Zhang, Y. Liu, J. Chen, and [30] J. Yu, Z.-J. Zha, M. Wang, K. Wang, and T.-S. Chua. [31] C. Zhai and J. Lafferty. A study of smoothing [32] L. Zhou and P. Chaovalit. Ontology-supported
