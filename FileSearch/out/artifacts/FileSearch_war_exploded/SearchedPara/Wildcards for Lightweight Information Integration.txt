 We present a flexible information integration approach whic h addresses the dynamic integration needs in a personal desk-top environment where only partial mappings are defined between the sources to be integrated. Our approach is based on query rewriting using substitution rules. In addition to exploiting defined mappings, we employ substitution strate -gies, which are inspired by the idea of using wildcards in querying and filtering tasks. Starting from a triple based query language as used for querying RDF data, unmapped ontological elements are substituted in a controlled way wi th variables, leading to a controlled form of query relaxation . In addition, the approach also provides evidences for refin-ing the existing mapping based on the results of executing the relaxed queries. Different strategies for replacing non -matched ontology elements with variables are presented and evaluated over real-world data sets.
 H.3.3 [ Information Search and Retrieval ]: [Query for-mulation, Search process] THEORY, EXPERIMENTATION Virtual desktop information integration, Personal inform a-tion management
In the recent years the task of information integration clearly has become more dynamic due to the availability of a vastly growing number of information sources. In per-sonal information management(PIM), for example, ever new repositories for accessing, but also for storing and managi ng information in the personal context are used (see for exam-ple the success of Flickr and YouTube.)
This imposes new challenges for information integration in support of integrated information search and access. The definition of mappings between information sources, which i s required for information integration (see e.g. [1]) is an ex pen-sive and complex task and has to be repeated for every new information source to be integrated. With the high dynam-ics in many of today X  X  information integration settings, th e definition of complete mappings for all information sources quickly becomes a limiting factor for information integra-tion. Solutions for detecting mappings as they are devel-oped e.g. in the area of schema and ontology matching [2] have relaxed the situation, but are still not able to find all mappings automatically.

Thus, more lightweight approaches are required, which enable query answering over multiple information sources with reasonable result quality, even when only partial map-ping information is available. The approach of malleable schemata [3], for example tolerates and deals with the exis-tence of vagueness, ambiguity and diversity in schemata not requiring predefined mappings between different schemata. The approach of emergent semantic [4] goes even one step further, by considering the semantics in distributed syste ms and thus also the mappings between local ontologies as a result of a dynamic process of negotiations.

In this paper we present a light-weight approach for query rewriting in support of integrated information access, whi ch can deal with situations where only partial mapping infor-mation is available. In addition to the substitutions drive n by the given mappings, we perform additional substitutions which are inspired by the idea of using wildcards in infor-mation querying and filtering tasks. Starting from a triple-based query language as it is for example used when query-ing RDF, we substitute unmapped ontological elements, i.e. property and concept references in a controlled way by vari-ables. This leads to a controlled form of query relaxation, since the variables can be bound to arbitrary ontological el e-ments in the targeted information source. We experimented with different strategies for replacing non-matched ontolo gy elements with variables. The results of evaluating these di f-ferent strategies are reported in this paper.

Additionally, our approach supports a value-driven learn-ing of new mapping rules. The replacement of a property p by a variable, for example, and the subsequent successful binding of this variable to a target property q in query eval-uation provides an evidence for the assumption, that there might be a mapping between p and q . Such evidences can be accumulated and might finally result in the suggestion of new mapping rules. We also performed first experiments to evaluate the effects of this learning of mappings for the individual strategies.

The rest of the paper is structured as follows: Section 2 motivates the approach with an information integration sce -nario for personal information management. Section 4 gives an overview of our approach of query relaxation in the pres-ence of partial mappings. Section 5 discusses the suggested substitution rules for query rewriting in more detail. The e x-periments performed to evaluate our approach and the eval-uation results are described in Section 6 and 7, respectivel y. The paper concludes with a summary and a description of future work in Section 8.
Let us consider an integration scenario for the desktop of a project manager Peter, working in an European R&amp;D project. Peter has to manage, access and use a variety of personal desktop resources, such as emails, reports, con-tacts, know how about experts and expertise, publications, addition to resources on the desktop, there are further re-sources relating to his desktop including project wikis whe re, for example, all consortium telephone conferences (agenda and minutes) are collected and documented, and resources collected in the desktops of co-worker X  X  working in the same project. This leads to the notion of a Virtual Personal Desk-which extends the personal desktop by (project) related sou rces e.g. from project wikis and shared resources from co-worker  X  X  desktops.

If Peter is looking for information, he would like to access the entire VPD with one query and he would also like to be able to use more complex queries to formulate his very specific information needs such as:
An underlying assumption in our scenario is that every data source at Peter X  X  personal virtual desktop is describe d by a simple ontology. Additionally, Peter has  X  X is X  own schema for structuring information that can be obtained from the data sources which are embedded in its Personal Information Model (PIMO) [5]. Peter is not willing to re-member every ontology describing a different information source. He prefers specifying queries based on his personal view on the information, his PIMO.

Thus, a system which translates the queries to the avail-able data sources and returns the best obtainable results is required. For doing this mappings between the different ontologies are required. For this purpose, the system may http://www.foaf-project.org http://nepomuk.semanticdesktop.org/ compute initial simple mappings between the data source ontologies and the PIMO using schema or ontology match-ing technology. The computed matches are equipped with a confidence value reflecting the evidences found for the map-ping. This typically results in a partial mapping between the ontologies.

It is the goal of our approach to improve query answering, even when only partial mappings are available (light-weigh t integration) without asking the user to manually define map-pings. Furthermore, user feedback and the an analysis of query results are expected to contribute to a stepwise re-finement and extension of the mappings.

An overview of the steps required to query the VPD is presented in Algorithm 1. The work presented in this paper focuses on query rewriting and mapping refinement. Select-ing the right data source for a given query and the merging of the query results remain further challenges for implementi ng a solution for light-weight information integration for vi rtual personal desktops.
The scenarios presented in Section 2 require to align the schemata describing the sources in order to query them. For an initial computation of the correspondences between dif-ferent schemata, Schema Matching approaches can be used. There are several approaches for schema matching, each of them with different characteristics and strengths. As presented in Section 6.1 and exemplified in Section 6.1, the presented data sources are described by schemata being simple ontologies. A recent work overview and classifica-tion existing in the Ontology Matching area can be found in [2]. This overview presents not only a variety of systems and their details, but also a comprehensive classification o f all basic techniques currently used by the existing match-ing approaches. Approaches such as iPrompt [6] rely on syntactical, lexical and structural information. Others l ike GLUE [7] use in contrast mainly probabilistic approaches to derive matches. Furthermore, there are also Logical or SAT-based approaches. For example, the CTXMatch [8] ap-proach, and [9] which combines several methods for schema matching in the area of ontology reuse for ontology engineer -ing.

Following the scenarios presented in Section 2 and once the mappings between the user view on the data and the available schemata are available, querying a source requir es rewriting the query in order to adhere to the source X  X  schema . Query rewriting for Information Integration is a widely studied area, ranging from approaches in the relational wor ld such as the ones described in [10] [11], to approaches in the XML world such as [12], and to the ones in the RDF or log-ics world [13] [14]. These approaches have in common that the schema used to post queries is not the same as the one used at the sources, so a rewriting of the query is required. The difference with our approach is that we aim not only to answering the query as good as possible [15] (see [16] for a motivation), but also to discover or refine mappings between the user-schema and the schemata describing the sources by analyzing the results obtained when querying them. The work described in [17] presents a query relaxation approach which differs with ours in that a) it requires a training set, and b) it keeps as many possible constraints in the query in order to satisfy it, but, unlike our strategies it does not pr e-serve stated relation  X  X aths X  which we perform through the use of wildcards. The most important differences of our ap-proach to the widely studied local as view (LAV) and global as view (GAV) approaches (see [10] for an overview) are that i) we do not assume to have complete mappings, ii) the map-pings currently used in our work are very simple (opposite to most of the LAV approach) and, iii) since the user has a view on the content, we do not want to modify it explicitly when a new source is added to the system (as usually in the GAV approach), but it will evolve automatically depending on the user X  X  information need and interaction with the sys-tem through the feedback and mapping discovery process. Approaches in the area of Distributed Information Retrieval are closely related to our approach and deal with the i) discovery of the data source representation, ii) se-lection of a data source given a query, and iii) retrieval and merging of the obtained results. In this paper we do not deal with the automatic discovery of the data source X  X  schema since we assume that they are given. Several approaches like [18], [19] and [20] were proposed for the selection of the data source to be queried, given a specific query. The problem of retrieving and merging the results as described in [21] is highly related to the areas of reference reconcili a-tion or entity resolution (see e.g. [22]).
In heterogeneous settings as described above, both the user query (based on his view of the information space) and the various information sources to be queried are using on-tological elements (property and concept names) based on different ontologies. An important challenge in answering queries in such settings is to translate between the ontolog i-cal elements used in the query and the ontological elements available in the information source. A complete mapping would provide adequate translation rules for all ontologic al elements available in the user query.

However, we want to look into the cases, where only par-tial mappings are available, i.e. there can be ontological elements in the user query that cannot be translated into on-tological elements of the targeted information source. The re are different causes for the incompleteness of the mapping: 1) there might exist semantic mappings between the onto-logical elements of the query and of the information source which are not specified in the mapping (e.g. due to failures in the mapping detection process), and 2) there is no corre-sponding ontological element in the information source, i. e. the respective ontological element of the user query cannot be mapped to the information source. Our approach focuses on dealing with case 1). For dealing with case 2) methods are required for combining results from different informati on sources to jointly cover the user query.
 For querying an information source S i given a user query Q u there are several ways to deal with the situation where no mapping exists in M i for one or several of the ontolog-ical elements used in the query. For the discussion we as-sume that the queries consists of conjunctive combinations of query predicates. We neglect other query options such as projection and disjunctive combination of query terms. The following three strategies for dealing with partial mappin gs have been identified:
In this paper we follow the controlled query relaxation ap-proach presented above, and we explore different strategies in order to find a good balance between how much recall is gained and to which extent precision is lost. In addition, th e results of this strategy can also be used to refine the existin g mapping: The variable v introduced for a property p will be matched to another property, say q , in the targeted informa-tion source. In case results based on this match become part of the query result, this is an evidence for the assumption that there might be a mapping between p and q . Such evi-dences can be automatically accumulated, in order to learn further mappings, or presented to the user for getting user feedback on further possible mappings. This support for the refinement of existing partial mappings is a second impor-tant feature of the lightweight information integration an d query rewriting approach presented in this paper. Algorithm 1 Process overview for querying a new source
The core of the problem is the identification of a set of substitution rules for transforming a given query Q u into a source S i . The substitution rules will be based on the exist-ing (partial) mappings. In addition, they will also be drive n by a controlled substitution of ontological (or schema) el-ements such as properties and concept names by variables ( wildcard substitution rules ).

Since we want to focus on analyzing the effect of  X  X ild-card X  substitution on query rewriting we make the following simplifying assumptions: 1. We assume that no syntactical transformations be-2. The information sources to be accessed are described 3. We restrict the mappings to relate only concept to con-4. We only consider selective predicates combined by con-
Before discussing the substitution rules and their applica -tion to queries we introduce some underlying definitions.
Ontology: An ontology is defined as a set of concepts, binary relations over the concepts, and hierarchy over con-cepts and relations. Formally, an ontology O is a tuple O = C, R, H C , H R where C is a set of concepts, R is a set of relations and H C , H R are hierarchies over concepts and relations respectively. A fixed alphabet A contains all concept names ( A C ) and relation names ( A R ) (ontological elements), such that A = A C  X  A R and A C  X  A R =  X  . There is a function mapping every concept name in A C to a con-cept in C and every relation name in A R to a relation in R . Concepts c in C are over a domain  X  and relation names are mapped to binary relations r over  X   X   X . We will overload c and r to denote concept and relation names in A and the associated concept and relation in O . In conformance with the terminology used in RDF we will refer to the relations as properties.

Information Sources: The information sources to be accessed are described by simple ontologies. We assume to have a set S of n local sources and a set OS of n ontologies such that each S i is described by O S i with an alphabet A i  X  n . Each S i contains the set ext ( O S i ), the extension or instances of O S i .

Personal Information View: We assume that the view of the user on the domain is also given in form of an ontol-ogy, the personal ontology of the user O u . O u reflects the user X  X  mental organization of the data explicated as a sim-ple ontology. The alphabet of O u is denoted as A u and this alphabet is used by the user to construct any query Q u . We assume that A u  X  A S i =  X  for all i  X  n .

Queries: In our approach we assume a triple based query language as it is for example used for querying RDF. Indi-vidual query terms are expressed as triples &lt; s, p, o &gt; , which may contain properties (for p ), references to resources (for s and o ), and literals (for o ) from an infinite set of literals (constants) L . The properties used in the query are from the alphabet A u . In addition variables can be used in the query triples in all positions to express connections betwe en triples (e.g. paths), to cover irrelevant values and to name elements to be included into the query result. Let us denote as V AR an infinite set of variables that can be used in any query and as V AR Q u the set of variables actually used in query Q u .

Triple based query languages also contain statements that refer to the typing of the searched instances. In RDF the special property rdf : type is used for this purpose. It con-nects the instance to the schema level (or the A-Box to the T-Box). We assume that our query language also supports such a property  X  that is used for this purpose. A triple &lt; s,  X , o &gt; expresses the condition that s is an instance of the concept denoted by o .

As mentioned above we restrict our considerations to the part of the query language where the query conditions are defined and restrict ourselves to conjunctive combinations of predicates. Thus our query Q u is composed of a set of triples &lt; s, p, o &gt; , with the option for s , p and o as described above.
 M i . Each mapping M i consists of a set of triples of the dence value. The ontological element (concept or property) e u is from the alphabet A u , the element e S i from alphabet A mentioned above, we restrict the mappings to relate only concept to concept or property to property. The mapping does not have to be complete, i.e. not for all ontological el-ements in O u there have to be mappings in M i . Additional semantic mappings might exist between elements of O u and O i which are not specified in M i (e.g. due to failures in the mapping detection process). As described before, we assume that a set of mappings M i is defined between the personal information view of the user and the target information sources. These mappings are used to define a basic set of substitution rules.
The decision on which elements from M i to use for rewrit-ing the query Q u to query source S i is based on the confi-dence value for each pair-wise mapping in M i and the thresh-old  X  :
The symbol  X  represents the threshold value used for de-ciding on using a given mapping or not. The confidence val-ues can be modified based on the results of evaluating the query (feedback), so that mappings that were used in the past could cease to be used due to low confidence or non-used mappings could start to be used due to rise in their confidence (see below).

Having defined M 0 i , two first sets of substitution rules -the mapping-based substitution rules SUB M 1 and SUB M 2 for concepts and for properties, respectively -can be define d: These substitution rules directly apply the mappings define d in M i .
Ontological knowledge is a further source of substitution rules. For example, unmapped concepts and properties in the query Q u might be replaced by super-classes and super-properties from Q u , respectively, for which a mapping has been defined. In [14], Calvanese and colleagues present de-tailed approaches on how to use ontological knowledge and reasoning in query rewriting and query processing.
Our approach does not use substitution rules based on ontological knowledge, since we want to focus on the impact of using wildcard substitutions on query rewriting.
One of the core ideas of our approach is to replace ontolog-ical elements, i.e. concepts and properties, in the user que ry for which no matching exists, with variables. This is possi-ble since triple based query languages, as we consider them, can deal with variables in all triple positions (subject, pr ed-icate and object). For our approach it is important to find the right balance between substituting unmapped ontologi-cal elements by variables and omitting entire tuples, which contain unmapped ontological elements. Therefore differen t relaxation strategies have been designed and evaluated as part of the experiments in Section 6: 1. Strategy 1 -Substitute concepts without mapping: This 2. Strategy 2 -Substitute properties without mapping: Any 3. Strategy 3 -Substitute all unmapped elements: Replace 4. Strategy 4 -Substitute unmapped properties of mapped 5. Strategy 5 -Substitute unmapped properties if the prop-6. Strategy 6 -Substitute unmapped properties of mapped
Using the different strategies will result in different sets o f substitution rules. For a compact definition of the substitu -tion rules we assume the following underlying definitions: 1. The set of subjects for which the type information is 2. The set of unmapped properties: 3. The set of the un-mapped concepts:
For the variables introduced by the wildcard substitution rules some conditions have to be fulfilled. When ontological element e 1 and e 2 (property or concept) are replaced by variables v e 1 and v e 2 :
Using the definition above the substitution rules for the six strategies can be described as follows:
We have identified two groups of substitution rules, those based on the defined mappings (see Section 5.2) and those based on the introduction of wildcards (variables) (see Sec -tion 5.4). For the rewriting process (see below) both types o f substitution rules are combined. Depending on the selected wildcard strategy, there are six alternative sets of substi tu-tion rules, namely SUB 1 w to SUB 6 w (see Section 5.4). Thus, we also create six different combined sets of substitution rules SUB 1 to SUB 6 as (for j= 1, .. 6): that will be used to evaluate the impact of the different wildcard strategies on the query rewriting process and its results.
All sets of substitution rules defined in the previous sec-tion contain pairs of elements. Within the pairs, the first element refers to the element to be replaced in the query and the second element refers to the respective replacement . Thus, the presence of a pair ( e 1 , e 2 ) in a substitution set implies that Q u [ e 1 , e 2 ] will be computed for the considered user query Q u , i.e. all occurrences of e 1 in the query will be replaced by e 2 . For an entire set of substitution rules the substitutions have to be sequentially applied. Since we as-sume that A u  X  A S i =  X  and there are no rules that replace variables, the sequence of application will not influence th e result of the substitution.
 With each of the six sets of combined substitution rules rewritten into a target query in the following way: ( e i , sub i )  X  X UB j  X  ( i 6 = k  X  ( e i , sub i ) 6 = ( e
Q SUB j i denotes the target query created by applying the substitution rules of rule set SUB j to the query Q u .
In the previous discussion we focused on the application of our approach in query rewriting. However, the approach also has the potential of contributing evidences for the refineme nt and extension of the existing partial mapping M i .
If an ontological element, let X  X  say a property p , is re-placed by a variable v p the respective query triple is relaxed. During query execution the variable is iteratively bound to different properties in O u . Only some of these bindings will contribute to the query results, since also all other query conditions have to be fulfilled given the binding. We call ( p, q ) a successful coupling , when p is replaced by a variable v p and the binding of v p to property q contributes to the query result.

If a successful coupling happens frequently for a pair p and q of properties (or a pair of concepts) this suggests that ther e is a not yet specified mapping between these two ontological elements. To give the intuition behind this idea: Assume the information source contains a property q with literal values. A property p used in the query will often form a successful coupling with q , if the values specified for p in the query are often included in the values available for q in the information source S i . The assumption we make here is that properties with a similar set of employed values have a higher probability of being the same property. Of course, this is a heuristic, which will not work out in all cases.
In our approach, we aggregate individual (small) evidences , whenever successful couplings occurs and accumulate them over time. The collected evidences are then used as a basis for learning or suggesting new mappings, e.g. the mapping ( p, q ). The collected evidences can of course also be used to increase the confidence values for existing mappings (as the y have e.g. be computed by automatic mapping methods).
The feasibility of this approach has been evaluated as part of our experiments.
The different query rewriting strategies presented in the previous section have been evaluated with respect to their impact on precision and recall. Furthermore, the potential for mapping refinement has been systematically analyzed. This section describes the employed datasets and the setup of the experiments, whereas the next section discusses the evaluation results.
For the experiments we use desktop data or more pre-cisely a virtual personal desktop as introduced in section 2 . The queries are not directly evaluated on the desktop re-sources, but on a semantic layer of metadata that describes and relates the desktop resources. This layer is created by extraction technology as it is, for example, developed in th e Nepomuk project [23] and [24].
 In the experiments we use five information sources S 1 , S ,... S 5 , which reflect five areas of a VPD. The ontology used by the user O u for query formulation in the experiments is connected to the ontologies O S i describing the informa-tion sources S i by partial mappings. An excerpt of the user ontology O u which was used for constructing the different queries for this experiment is presented in Figure 1. Figure 1: Excerpt of the user X  X  ontology O u used for specifying Q u
In the following the information sources S i are described in more detail.

Information source S 1 contains metadata about Cal-endar Items , Email , Presentations and Documents . Exemplary aspects of the representation of this informatio n source are shown in the ontologies in Figures 2 and 3 which are part of a set of ontologies to describe desktop resources as designed in the project Nepomuk Information Ontologies 3 :
Information source S 2 contains information about Meet-ing Minutes . Project minutes are Documents with a pre-defined structure, containing the minutes of internal Nepo-http://nepomuk.semanticdesktop.org/ontologies/ muk project meetings at L3S as presented in Figure 4. The assigned tasks and their deadlines (see Figure 5), as well as informative items and events are contained (see Figure 6 and Figure 7 respectively). The metadata extracted from this special kind of documents does not follow the Nepo-muk ontologies previously presented in this section, but th e L3S Meeting Minutes ontology (mtm), which was created independently of the Nepomuk ontologies.
 Figure 4: Simple ontology for Nepomuk L3S internal Meeting Minutes
Information source S 3 contains information about Pub-lications . Scientific publications, usually as PDF files, are also present in the desktops. For this kind of publications, metadata depicting authors, editors, proceedings where th ey were published, and a publication date among others can be extracted. A partial view of the extracted metadata is shown in Figure 8.

Information source S 4 contains information about Per-sons extracted from FOAF Files . Another kind of data sources are the own FOAF profile and the FOAF profiles found when visiting web pages. Standard FOAF profiles usually contain information about the email address, the place of work, postal address, known people, etc. Also in-formation relating to  X  X riend X  profiles are provided, so the se are also considered as additional sources of information, b y traversing the user X  X  network of friends. The FOAF files Figure 5: Simple ontology for Nepomuk L3S internal Meeting Minutes: Task items Figure 6: Simple ontology for Nepomuk L3S internal Meeting Minutes: Information items crawled are files describing the current user of the system as well as the people he knows, obtained by traversing up to k steps the references to files of known people. For crawling FOAF files and storing them in the Nepomuk repository, a special crawler was built which stores the files by using the
Information source S 5 contains information about Tele-phone Conferences extracted from project Wiki Con-tent . In Nepomuk we employ XWiki 5 , a collaborative plat-form where all project relevant shared content is stored. In the Nepomuk XWiki there is a record of all telephone con-ferences held, each in a different page and with similar struc -ture. We access this pages on the wiki on a regular basis and crawl its contents, extracting metadata and storing it in th e local repository. The Wiki Telco Minutes (xwt) ontology used for representing this information was also developed independently of the Nepomuk ontologies and an excerpt of it is presented in Figure 9.
 All the generated or extracted metadata is stored in a RDF store, as available in the Nepomuk project.

In the experiment we used data from 14 different desktops of our colleagues at L3S. These desktops contain different fil e types such as MS-Office files, Emails, PDF X  X , structured files e.g. LaTeX and BibTeX files, as well as plain text files. Each different desktop was crawled separately with the technol-ogy available in Nepomuk, and, for each of them metadata in form of RDF triples was generated as a result of the crawl-ing process. In addition, the metadata available for each of the desktops was enriched with metadata automatically ex-tracted from internal L3S Nepomuk project meetings using, user-related FOAF descriptions, as well as metadata relate d to telephone conferences from the Nepomuk XWiki . The mean quantity of different file types available for each desk-top is presented in Table 1.
The goal of the experiments is to evaluate the different query substitution strategies presented in Section 5 over s ev-http://xmlns.com/foaf/0.1/ http://www.xwiki.com Figure 7: Simple ontology for Nepomuk L3S internal Meeting Minutes: Upcoming Events eral iterations, providing at each iteration feedback on th e confidence of the mappings by analyzing the results of the queries. The experiments are performed by using an evalu-ation framework interacting with e.g. the Nepomuk InfoIn-tegration component which is integrated into the Nepomuk infrastructure providing the functionality described in A lgo-rithm 1.
 Datasets: We used the datasets ( S i ) described in Section 6.1. Golden standards: A golden standard G S i was computed Mappings: A set of initial mappings is computed from the Queries: Eighty queries were defined on terms of the user-Table 1: Mean number of file types per desktop Figure 9: Simple ontology for TelCo Minutes on the Wiki
We performed the evaluation of the query substitution strategies ( SUB j as defined in Section 5) on each dataset, and then computed the overall quality measure of each sub-stitution strategy. Quality was measured in terms of preci-sion and recall, by comparing the results of evaluating the when querying the dataset with the rewritten query using a specific rewriting strategy. We present in Algorithm 2 an overview of the evaluation process by relying on the notatio n previously introduced.
 Algorithm 2 Experiment execution process
In the experiments presented here we simulated the user feedback. We compared the values of the variables for prop-erties or concepts in the evaluated queries, and, by compar-ing them with the expected ones, we decided on right or wrong answers. In case of a correct  X  X inding X  of variables to concepts or properties in a query, the corresponding map-ping confidence value was incremented by 0 . 1 for the next it-eration. In case no mapping existed between these elements, a mapping with confidence value 0 . 0 was added, this map-ping could then be increased in following query evaluations as already explained. It remains a future task to investigat e on using the evidence on false  X  X indings X  to lower existing mapping confidence values. For the experiments presented in this paper we used a value of  X  = 0 . 7. The results of our experiments are presented in Section 7.
The substitution strategies SUB j evaluated are the ones defined in Section 5. The reader might recall that different strategies imply different usage of wildcards in the query rewriting process, and in this way different relaxation stra te-gies.

As presented in Sections 6.1 and 6.2, we evaluated the different strategies over all datasets. In Table 2 the pre-cision values for each of the strategies without considerin g any mapping feedback are presented. This is, mappings were used as initially generated. It can be noticed that all strat e-gies present low precision values, and most of them also low recall values, meaning that the generated mappings were mostly not complete enough to allow the retrieval of the re-quested information. In the next paragraph we present the evolution of the precision and recall over several iteratio ns, showing that the feedback on the mappings is an important aspect in our presented approach.
 Precision 0.28 0.14 0.16 0.19 0.25 0.19 Recall 0.34 0.62 0.81 0.62 0.24 0.59
Figure 10 shows the variation of precision and Figure 11 of recall for the different strategies over the different iter a-tions. At each iteration, feedback on mappings was given for each strategy separately, so that the evolution of precisio n and recall values by using the different strategies with feed -back can be observed. Strategy SUB 1 and SUB 5 gave the best precision throughout the iterations, but the worst rec all when the mappings were sparse or low in confidence (cold start problem). After very few iterations this changed due to the feedback given to the mappings. This suggest that the best way to proceed would be to use a mix of strategies, starting with a strategy that returns less precise results b ut higher recall like SUB 2 , SUB 3 or SUB 6 and then, after some iterations when the mappings were improved based on the feedback, move to use SUB 1 or SUB 5 . The low recall that can be observed for some of the strategies is caused by the incompleteness of the mappings at the first iterations. Once the system learns and increases confidence of the correct mappings, recall also improves. An interesting aspect is th e small decrease in precision for some strategies at the last i t-eration. This happened because wrong mappings have been learned for those strategies. It remains an interesting tas k to evaluate the problem of detecting not only right but also wrong mappings via user feedback, and to use this informa-tion to avoid misslearning of mappings.
 Figure 10: Precision evolution for every strategy
Regarding the mappings and the discovery of new ones, evidence shows that all newly discovered mappings are added in the course of the first iterations. Subsequent feedback in -creases the confidence of these mappings, so that eventually after few iterations (the third in our case) all of them are used for rewritings. Regarding the existing mappings which had a too low confidence in order to be used in the query rewriting processes, positive feedback evidence makes the ir confidence raise in every iteration so that at some point they are used in the query rewriting process. Different strategie s make confidence of different mappings grow faster than oth-ers, which is the reason that, given the employed datasets and queries, some strategies perform better than others.
In this paper we presented an approach for query rewriting for triple based query languages to support of lightweight information integration. Our approach can deal with partia l mappings and supports stepwise refinement and extension of the mappings. The core idea is to introduce wildcards into queries for substituting unmapped properties and concepts , as they appear in case of partial mappings. This leads to a controlled form of query relaxation. Different strategies f or wildcard substitution have been designed and explored.
Evaluation of our approach for desktop data sets shows that the correct mappings to be used can be learned quickly after only few iterations, and that the learning phase is re-quired in order to obtain good results. In addition we show which strategies perform better in which user-iteration st ep and how they can be combined to minimize problems due to incomplete mappings.

Since these first evaluation results are promising, we plan to extend the approach in different directions. An important area is an extension to allow more complex query structures, going beyond the conjunctive combination of selection pred -icates as used in our current system. In addition, the impact of more complex mapping relationships -so far we only con-sider simple one-to-one mappings between properties and concepts -is a further direction for extending our approach . Last, we plan to investigate ways to extend query proces-sors to better support the recognition of successful proper ty and concept couplings (see 5.7), which are needed for au-tomatic mapping refinement as well as for the suggestion of new mappings to the user. While at present this is done as a post-processing step, better integration with query proce ss-ing should further improve efficiency.
This work was supported by the Nepomuk project funded by the European Commission under the 6th Framework Pro-gramme (IST Contract No. 027705). [1] Rousset, M.C., Reynaud, C.: Knowledge [2] Euzenat, J., Shvaiko, P.: Ontology matching.
 [3] Zhou, X., Gaugaz, J., Balke, W.T., Nejdl, W.: Query [4] Aberer, K., Cudr  X e-Mauroux, P., Ouksel, A.M., [5] Sauermann, L., van Elst, L., Dengel, A.: PIMO -a [6] Noy, N.F., Musen, M.A.: The prompt suite: [7] Doan, A., Domingos, P., Halevy, A.: Learning to [8] Bouquet, P., Serafini, L., Zanobini, S.: Semantic [9] Stecher, R., Nieder  X ee, C., Nejdl, W., Bouquet, P.: [10] Halevy, A.Y.: Answering queries using views: A [11] Pottinger, R., Halevy, A.: MiniCon: A scalable [12] Yu, C., Popa, L.: Constraint-based XML query [13] Chen, H., Wu, Z., Wang, H., Mao, Y.: [14] Calvanese, D., Giacomo, G.D.: Data integration: a [15] Sarma, A.D., Dong, X., Halevy, A.: Bootstrapping [16] Halevy, A., Franklin, M., Maier, D.: Principles of [17] Muslea, I.: Machine learning for online query [18] Si, L., Callan, J.: Unified utility maximization [19] Nottelmann, H., Fuhr, N.: Evaluating different [20] Baillie, M., Azzopardi, L., Crestani, F.: Towards [21] Shokouhi, M., Zobel, J., Bernstein, Y.: Distributed [22] Dong, X., Halevy, A., Madhavan, J.: Reference [23] Brunkhorst, I., Chirita, P.A., Costache, S., Gaugaz, J ., [24] Aperture: (Aduna aperture) [25] van Elst, L., Kiesel, M.: Generating and Integrating
