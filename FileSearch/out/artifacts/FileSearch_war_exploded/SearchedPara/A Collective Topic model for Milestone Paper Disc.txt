 Prior arts stay at the foundation for future work in aca-demic research. However the increasingly large amount of publications make it difficult for researchers to effectively discover the most important previous works to the topic of their research. In this paper, we study the automatic dis-covery of the core papers for a research area. We propose a collective topic model on three types of objects: papers, authors and published venues. We model any of these ob-jects as bags of citations. Based on Probabilistic latent se-mantic analysis (PLSA), authorship, published venues and citation relations are used for quantifying paper importance. Our method discusses milestone paper discovery in different cases of input objects. Experiments on the ACL Anthology Network (ANN) indicate that our model is superior in mile-stone paper discovery when compared to a previous model which considers only papers.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval X  clustering, retrieval models Topic Model; Milestone Paper; Paper Importance
Academic literature surveying plays a vital role in aca-demic research; researchers can learn what has been done, what research gaps might exist and what potential research directions to work on. Academic search engines such as Google Scholar 1 and CiteSeerX 2 enable researchers to find related literatures or prior arts. However, the overwhelm-ing number of publications makes it difficult to quickly ob-tain the most important set of previous work of a subject. http://scholar.google.com http://citeseerx.ist.psu.edu/index Some citation recommendation systems have been designed to recommend appropriate citations for academic works [5, 1]. However, academic search engines and recommendation systems might return qualified sets of papers based on se-mantic similarity; paper importance has not been consid-ered. It is essential to have some models for milestone paper discovery ; i.e., discover the core set of papers which can best represent previous works for a research topic. Wang et al. [8] studied topic milestone paper discovery and developed a generative model for theme and topic evolution. They used the idea of modeling each paper as a  X  X ag of citations X  and measured paper impacts based on co-citation relations. However, [8] only considered co-citation relations for topic milestone paper discovery. Therefore, their model could be biased against recently published papers (which are rarely cited by others). Thus, the issue of milestone paper discov-ery is not completely addressed.

In this paper, we propose a collective topic model for ob-jects of different types (i.e., papers, authors, and venues) in academic networks. The collective topic model quanti-fies paper importance based on authorship, published venue reputation and co-citation relationships in the academic col-lection. We investigate the identification of milestone papers in different cases. Our experimental results show that paper importance is well captured by our model; authorship and published venues have considerable influence on milestone paper discovery.
Previous work exists in citation recommendation [5, 1]; i.e., recommending appropriate references to researchers. For example, [5] designed a translation model between citation contexts and reference words, and recommended a list of ci-tations by using long queries such as sentences or a manuscript. Bethard and Jurafsky [1] designed a feature-based learning model for literature retrieval. A list of references are rec-ommended using the abstract of the input object (i.e., a pa-per) as query. These works perform recommendations based on semantic analysis, but paper importance or ranking is not considered. In addition, some works in topic evolution may enable researchers to see how the research in a par-ticular area evolves. Mei and Zhai [6] used temporal text mining techniques to discover latent themes from text and constructed theme evolution graphs. However, they cannot identify the  X  X icro-view X  of a research field, e.g., milestone papers for that area.

Wang et al. [8] used milestone paper discovery as an ap-plication when developing a generative topic model for re-
Figure 1: Overview of our collective topic model search theme evolution. They modeled a paper as a bag of citations and use the co-citation relationships for evaluating paper impact between topics. However, their results can be biased against recently published papers, since they did not take into account additional factors that influence the impor-tance of papers, such as authorship and published venues. In our work, we propose a topic model that considers these additional influence factors.
The importance of a paper depends on a variety of factors, including the authority of authors, the publication venue and co-citation relationship with other papers. We borrow the idea of modeling a paper as a bag of citations from [8] in order to consider the co-citation factor in the importance of a paper; thus each paper is represented as a bag of citation IDs. Since authors and venues are linked with documents in the academic document collection, we build a  X  X irtual doc-ument X  for each author and venue by aggregating all doc-uments associated with that author or venue (we call the result author document and venue document , respectively). This way, for each author or venue we also derive a bag of citation IDs. Based on [3], we assume that the multiple-typed documents (paper document, author document and venue document) have a common set of latent topics and each topic is represented as the distribution over citations. We model these documents using a probabilistic topic model which quantifies the probability of a paper to be cited as pa-per importance. Then, the problem about milestone paper discovery is defined as follows.

Milestone paper discovery : Given an academic docu-ment collection D (papers Q , author set A , published venues V and citations C are known), the model outputs a set of core papers M that includes citationIDs (paperID) ranking based on co-citations at top within a topic or  X  X  document X  (The document type might be a paper document, an author document or a venue document).
An overview of our model is shown in Figure 1. Table 1 describes meanings of the notations used in our model. We assume that for all documents there is a common set of k latent topics. Each document is represented as the distri-bution over topics and each topic is represented as the dis-tribution over citations. Then, the process of generating an academic document is as follows: for each citation in that document, firstly sample a topic z k according to the distri-bution from paper topic distribution  X  ( z ; d ) or author topic distribution  X  ( z ; a ) or venue topic distribution  X  ( z ; v ) based Table 1: Notations used in our collective model
Symbols Description d , a d for a paper, a for an author v , c , z v for a venue, c for a citation, z for a topic T , k T for topic set, k for topic number N The citation-paper Matrix U The citation-author Matrix
E The citation-venue Matrix  X  ( c ; z ) The topic-citation distribution  X  ( z ; d ) The paper-topic distribution  X  ( z ; a ) The author-topic distribution  X  ( z ; v ) The venue-topic distribution  X  ,  X  ,  X  relative weights for d , a , v on the document type. Then, draw a citation c from the sampled topic distribution  X  (:; z k ) in topic citation distri-bution  X  ( c ; z ).

We developed our model based on PLSA [4]. We can have the following joint model for citations based on documents in different types: and the parameter set  X  to be estimated is: In order to estimate the parameters  X  , we should maximize the likelihood of the document collection D given  X  . The loglikelihood function is represented as
L (  X  ) = X N ij indicates the occurrences of citation c i in paper d the citation-paper matrix, U in the occurrences of citation c cited by a n and E in the occurrences of citation c i cited by papers in venue v m .  X  ,  X  ,  X  indicate relative weights for three-typed documents d, a , v .
We use the Expectation-Maximization (EM) algorithm for parameter inference. EM iteratively executes two steps, an E-step and a M-step, until L (  X  ) converges [2].
 Each E-step computes the lower bound function Q of L (  X  ). In this process, the posterior probabilities p ( z k | c,o ) ( o can be d,a,v ) are re-computed using the new parameter values from the previous M-step: Figure 2: The perplexity over different k for our model
In the first E-step, the posterior probabilities are ran-domly initialized. The M-step that follows each E-step, re-estimates the  X  which maximizes Q as follows:  X  ( z k ; d j ) =  X  ( c i ; z k )  X   X 
The ACL Anthology Network (ANN) [7] was used in our experiments. There are 19408 papers by 15824 authors, pub-lished in 342 venues and having received 85367 citations. This dataset is also used in previous work [8]; thus, we can use it to perform some comparisons with [8].

Before testing our model, we have to determine the num-ber of topics k . Perplexity is commonly used to evaluate performance of topic modeling. Thus, we select the value of k which minimizes perplexity. Figure 2 shows the per-plexity scores during model estimation for different values of k . From this graph, we can see that a value of k around 150 is appropriate for this dataset, since it gives the low-est perplexity score among all tested values. Therefore, we perform our experiments using k = 150. In addition, we con-sider the three-typed documents equally and set the weights  X  =  X  =  X  = 1.
Each topic is presented as the mixture of citations in our model. Within each topic z k ,  X  ( c i ; z k ) indicates the impor-tance of citation (i.e., the cited paper) c i . Those citations can be ranked based on  X  ( c i ; z k ) and citations ranking at the top for each topic z k are considered as topic milestone papers. In order to compare with previous work [8], we use the top-10 papers for the topic Sentiment Analysis as an example. Table 2 presents topic milestone papers for Senti-ment Analysis in [8] while Table 3 shows our results. There are 5 overlapping papers and 5 different papers between the two models. The top-1 paper is the same, but the order of the overlapping papers is slightly different. The top-2 pa-per in our model is ranked highly, compared with that in [8] (top-4). We have more recently published papers. The rea-son behind these differences is that the previous model does not consider factors such as published venues and authorship that have influence on paper importance.
In previous work, only topic milestone paper discovery has been studied. Our model is more general in the sense that it can identify milestone papers also for a given venue or author. In the next experiment, the probability of a cita-tion given a venue is computed by Equation 3 and papers are ranked based on p ( c | v ). Here we only take the venue ACL as example and Table 4 reports important papers in ACL. These papers are mainly from three venues, e.g ACL, NAACL and JCL (Journal of Computational Linguistics).
Similarly, author milestone papers can be ranked, based on the probability of a citation given an author p ( c | a ) (see Equation 2). We used the author Bo Pang (the first author of the top-1 paper in Table 3) as an example. Table 5 shows the citations that Bo Pang has the highest probability to cite (* indicates self-citation). The result has high overlap with Table 2 (8 papers). This might indicate that author ci-tation patterns are regular and the author factor has similar influence as the citation relationships.
Finally, our model can also indicate popular topics for an author or a venue.  X  ( z ; v ) indicates how well a topic z represents a venue v and  X  ( z ; a ) shows how well a topic z can represent an author a . Therefore, we can find the most popular topics for a specific venue or an author. The top-3 topics for ACL are respectively Name entity extraction , Statistical parsing and Statistical machine translation . The top-3 topics for Bo Pang are Sentiment analysis , Opinion extraction and Paraphrases generation .
In this paper, we proposed a collective topic model for multiple-typed objects in the academic network, in order to address the issue of milestone paper discovery. The model is based on PLSA, and authorship, published venues and cita-tion relations have been included in it. Our method can not only discover topic milestone papers discussed in previous work, but also explore venue milestone papers and author milestone papers. In addition, it can find representative top-ics for an author or a venue. Experiments on a real dataset ANN show that our model can better evaluate the impact of papers and its result is not biased against new publica-tions. Directions for future work include the investigation of more complicated models with biased mechanisms and the integration of this model into existing academic literature search/recommendation systems. This work was supported by grant HKU 715711E from Hong Kong RGC. The authors would like to thank the anony-mous reviewers for their valuable comments. [1] S. Bethard and D. Jurafsky. Who should i cite: learning [2] C. M. Bishop. Pattern Recognition and Machine [3] H. Deng, B. Zhao, and J. Han. Collective topic [4] T. Hofmann. Probabilistic latent semantic indexing. In [5] W. Huang, S. Kataria, C. Caragea, P. Mitra, C. L. [6] Q. Mei and C. Zhai. Discovering evolutionary theme [7] D. R. Radev, P. Muthukrishnan, V. Qazvinian, and [8] X. W. Wang, C. Zhai, and D. Roth. Understanding
