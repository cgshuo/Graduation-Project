 principled algorithm for active learning with guaranteed c onsistency. favorably to usual active learning schemes. as general enough to accomodate many supervised learning task s [10], in particular: tributed (i.i.d.) data sampled from the distribution p hood population estimator  X  by: Given i.i.d data ( x minimizes P n all x  X  R d , E there exist  X   X  R d  X  k such that for all x  X  R d , y  X  X  , p ( y | x, X  ) = p potential misspecification while deriving asymptotic expa nsions. reduction procedure may be considered deterministic and ou r criteria may apply. points x to label, i.e., the points for which the corresponding y x , i = 1 ,...,n , the targets y conditional distribution p in many applications where the input distribution p For alternative scenarii, where the density p More precisely, we assume that the points x q w mum with respect to  X  of tributions: (1) the variables x variable x q performance for both sets of assumptions are identical. Section 5. Throughout this section, we assume that p a compact support included in the one of p We first make the assumption that the variables x butions q Section 4.4, we extend some of our results to the dependent ca se. 4.1 Bias and variance of ML estimator and var Proposition 1 We let  X  weight functions w w n ( x ) q n ( x ) to zero in probability and we have where J and I  X  I From Proposition 1, it is worth noting that in general  X  mum likelihood estimate  X  a non asymptotically vanishing bias in estimating  X  (a) our estimators have a low bias and variance in estimating  X  verge to  X  Propositions 2 and 3.
 There are two situations, however, where  X  then whatever the sampling distributions are,  X  consequence of the fact that E E Second, When w ing. We refer to the weights w u weights (see simulations in Section 6). 4.2 Expected generalization performance We let L u (  X  ) = E We now provide an unbiased estimator of the expected general ization error of  X   X  the Akaike information criterion [22] (for a proof, see [23] ): Proposition 2 In addition to the assumptions of Proposition 1, we assume th at E where w u E the minimum possible value L u (  X  the unbiased reweighting scheme (see Section 5). 4.3 Expected performance gain We now look at the following situation: we are given the first n data points ( x rent estimate  X   X  derivatives T distributions and weights of the ( n + 1) -th point: b with  X  g The following proposition shows that b H ( q mance gain of choosing a point x izing over y parameterized, this leads to a convex optimization problem . Proposition 3 We assume that E note  X   X  estimator obtained from the first n +1 points, i.e.,  X   X  then the criterion defined in Eq. (5) is such that E Moreover, for n large enough, all values of  X  n w 4.4 Dependent observations x brevity, we restrict ourselves to the unbiased reweighting scheme, i.e., w p leads to a simpler argument for the consistency of the estima tor. distribution q Proposition 4 (for a proof, see [23]) Let where w E
D b G = E D L u ( presented in Section 5, the distribution q on certain empirical moments and potential misspecificatio n. algorithms are composed of the following three ingredients : show empirically that usual heuristic schemes do not share t his property. s butions allows efficient sampling from a pool of samples of p with mixture density of the form s ( x |  X  ) = P non-negative and sum to one. The criterion b H ( q weighting scheme) which leads to H w n +1 ( x ) = p 0 ( x ) /q n +1 ( x ) The function H inverse functions, and is thus convex [14].
 Unless natural candidate distributions s clusters (e.g., 100 or 200), and then consider functions r where  X  y centroid y didate distributions r distributions.
 weight X ) which optimizes H which optimizes H order to select the new training data point x Eq. (7), then the appropriate cost function, H  X  is obtained, we sample x and w u and update weights accordingly.
 cross-validation every 10 new samples. is equivalent to finding x such that tr  X  ( x,  X   X  with the current model as an estimate of the unknown conditio nal probability p Figure 1: Proposal distributions: (Left) density p R  X  ( x ) q n +1 ( x ) dx (see text for details). other methods. R the maximum uncertainty framework.
 bad situation that our algorithms provably avoid. are scarce. (right) comparisons of methods.

