 pcarbone@bsd.uchicago.edu genetics and machine learning X  X ave focused on coming up with more accurate and more efficient approximate solutions to intractable probabilistic infer ence problems. To date, there are three widely-explored approaches to approximate inference in pr obabilistic models: obtaining a Monte Carlo estimate by simulating a Markov chain (MCMC); obtaini ng a Monte Carlo estimate by drawing samples from a distribution other than the target th en reweighting the samples to account for any discrepancies (importance sampling); and variatio nal inference, in which the original integration problem is transformed into an optimization pr oblem.
 The variational approach in particular has attracted wide i nterest in the machine learning commu-nity, and this interest has lead to a number of important inno vations in approximate inference X  Ravikumar [27] and Wainwright [31]. The key idea behind vari ational inference is to come up optimize some criterion in order to find the distribution par ameterized by  X  that most closely from a common objective, the Kullback-Leibler (K-L) diverg ence [9]. The major drawback of questionable factorization, leading to excessively biase d estimates (see Fig. 1, left-hand side). methodology of variational inference without being restri cted to tractable classes of approximate distributions (see Fig. 1, right-hand side). The catch is th at the variational objective (the K-L with Monte Carlo estimates of the gradient, and update these estimates over time with sequential Monte Carlo (SMC) [12] X  X ence, a stochastic approximation method for probabilistic infer ence . Large gradient descent steps may quickly lead to a degenerat e sample, so we introduce a mechanism safeguard mechanism does not make the standard effective sample size (ESS) approximation [14], hence it is likely to more accurately monitor the variance of the sample.
 Indirectly, the variance safeguard provides a way to obtain an estimator that has low variance in exchange for (hopefully s mall) bias. To our knowledge, our algorithm is the first general mea ns of achieving such a trade-off and, in so doing, it draws meanin g-ful connections between Monte Carlo and variational method s.
 The advantage of our stochastic approximation method with r e-spect to other variational methods is rather straightforwa rd: it does not restrict the class of variational densities by maki ng as-sumptions about their structure. However, whe advantage of our approach compared to Monte Carlo methods such as annealed importance sampling (AIS) [24] is less obvious. One key ad-vantage is that there is no need to design the sequence of SMC distributions as it is a direct product of the algorithm X  X  de riva-tion (Sec. 3). It is our conjecture that this automatic selec tion, when combined with the variance safeguard, is more efficient than setting the sequence by hand, say, via tempered transit ions [12, 18, 24]. The population genetics experiments we conduc t in Sec. 4 provide some support for this claim.
 We illustrate our approach on the problem of inferring pop-ulation structure from a cohort of genotyped sequences usin g the mixture model of Pritchard et al. [26]. We show in Sec. 4 that Markov chain Monte Carlo (MCMC) is prone to producing very different answers in independent simulations, and that it fails to adequately capture the uncertainty in its solution s. For many population genetics applications, such as wildlife co nser-vation [8], it is crucial to accurately characterize the con fidence in a solution. Since variational methods employing mean fiel d approximations [4, 30] tend to be overconfident, they are poo rly suited for this problem. (This has generally not been an issu e for semantic text analysis [4, 15].) As we show, SMC with a uniform sequence of tempered distributions fares little be tter than MCMC. The implementation of our approach on the population structure model demonstrate s improvements in both accuracy and reliability over MCMC and SMC alternatives, and at a compara ble computational cost. The latent Dirichlet allocation (LDA) model [4] is very simi lar to the population structure model of [26], under the assumption of fixed Dirichlet priors. Sinc e LDA is already familiar to the machine learning audience, it serves as a running example th roughout our presentation. 1.1 Related work of papers, but none of the existing literature resembles the approach proposed in this paper. De cent work on adaptive MCMC [1] combines ideas from both stoch astic approximation and MCMC to automatically learn better proposal distributions. Our work is also unrelated to the paper [20] with a similar title, where stochastic approximation is app lied to improving the Wang-Landau algorithm. Younes [33] employs stochastic approximation t o compute the maximum likelihood estimate of an undirected graphical model. Also, the cross-entropy method [10] uses importance sampling and optimization for inference, but exhibits no si milarity to our work beyond that. Its two key features are: the order of the words is unimportan t, and each document is drawn from a mixture of topics. Each document d = 1 ,...,D is expressed as a  X  X ag X  of words, and each word w di = j refers to a vocabulary item j  X  { 1 ,...,W } . (Here we assume each document has the same length N .) Also, each word has a latent topic indicator z di  X  { 1 ,...,K } . Observing the j th vocabulary item in the k th topic occurs with probability  X  kj . The word proportions for the observed data w and unknowns x = {  X , X ,z } given the hyperparameters {  X , X  } : The directed graphical model is given in Fig. 2.
 Implementations of approximate inference in LDA include MCMC [15, 26] and variational inference with a mean field approximation [4, 30]. The advantages of our inference ap-proach become clear when it is measured up against the variational mean field algorithm of [4]: first, we make no additional assumptions regarding the model X  X  factorizati on; second, the number of variational parameters is independen t of the size of the corpus, so there is no need to resort to coordinate-wise updates that are typically slow to converg e. From the importance sampling identity [2], we can obtain an unbiased estimate of (2) by drawing normalized importance sampling estimator [2] is used instead.) The Mon te Carlo estimator is astronomically high variance for all but the smallest probl ems. Instead, we construct a Monte Carlo es-timate (3) by replacing p ( x ) with an al-ternate target  X  p ( x ;  X  ) that resembles it, so that all importance weights are evaluated with respect to this alternate target. (We elaborate on the exact form of  X  p ( x ;  X  ) in Sec. 3.1.) This new estimator is biased, but we minimize the bias by solving a vari-ational optimization problem.
 Our algorithm has a dual interpretation: it can be interpreted as a stochastic approxi-mation algorithm for solving a variational optimization problem, in which the iterates are the paramet er vectors  X  k , and it can be equally sequence is chosen dynamically based on samples from the pre vious iteration. The basic idea is g k +1 at the next iteration. Since SMC is effectively a framework fo r conducting importance sam-pling over a sequence of distributions, we describe a  X  X aria nce safeguard X  mechanism (Sec. 3.5) too quickly. It is in this manner that we achieve a trade-off be tween bias and variance. Since this is a stochastic approximation method, asymptoti c convergence of  X  k to a minimizer of the objective is guaranteed under basic theory of stochasti c approximation [29]. As we elaborate There is no way to guarantee unbiased estimates under a finite number of samples, so convergence holds only as the number of iterations and number of samples b oth approach infinity. To recap, the probabilistic inference recipe we propose has five main ingredients: one, a family of approximating distributions that admits the target (Sec . 3.1); two, a variational optimization problem framed using the K-L divergence measure (Sec. 3.2); three, a stochastic approximation weights at each iteration of the stochastic approximation a lgorithm (Sec. 3.5). 3.1 The family of approximating distributions decomposition of the entropy [32]. Here, we impose no such re strictions on tractability; refer to Fig. 1. We allow any family of approximating distribution s so long as it satisfies these three densities are members of the exponential family [13] expres sed in standard form x can be partitioned into two sets A and B such that it is always possible to draw samples continuous-time Markov processes, and some Markov random fi elds are all models that satisfy add several complications to the description of the algorit hm. The restriction to the exponential For LDA, we chose a family of approximating densities of the f orm is recovered by setting  X  = 1 ,  X  = 0 and  X   X  =  X  . A sampling density with a tractable expression choice is meant to strike a balance between the mean field appr oximation [4] (with parameters  X   X  kj ) and the tempered distribution (with  X  X ocal X  temperature p arameters  X  and  X  ). 3.2 The variational objective The Kullback Leibler (K-L) divergence [9] asymmetrically m easures the distance between the target distribution p ( x ) =  X  p ( x ;  X   X  ) and approximating distribution  X  p ( x ;  X  ) , the exponential family, the gradient vector works out to be t he matrix-vector product algorithm X  X  performance hinges on a good search direction, so it is worth our while to reduce the variance of the gradient measurements when possible via Rao -Blackwellization [6]. Since we no longer have an exact value for the gradient, we appeal to the t heory of stochastic approximation. 3.3 Stochastic approximation Instead of insisting on making progress toward a minimizer o f f (  X  ) at every iteration, as in gradient descent, stochastic approximation only requires that progress be achieved on average . The Robbins-Monro algorithm [28] iteratively adjusts the c ontrol variable  X  according to unconstrained minimum. Due to poor conditioning, we advoca te replacing the gradient descent B k is a damped quasi-Newton (BFGS) approximation of the Hessia n [25]. To handle constraints  X   X  0 introduced in Sec. 3.1, we use the stochastic interior-poin t method of [5]. 3.4 Sequential Monte Carlo that the initial importance weights are uniform. After k steps the proposal density is  X  p in which we first draw a sample of x B (in LDA, the variables  X  and  X  ) given x A (the discrete variables z ), then update x A conditioned on x B . A Rao-Blackwellized version of the sub-optimal backward kernel [12] then leads to the following expression for updating the importance weights: 3.5 Safeguarding the variance  X  Let n ,  X  1 ,  X   X  , A , B , {  X  k } be given. A key component of the algorithm is a mecha-nism that enables the practitioner to regulate the variance of the importance weights and, by exten-sion, the variance of the Monte Carlo estimate of E [  X  ( X ) ] . The trouble with taking a full step (9) is that the Gibbs kernel (12) may be unable to effectively migrate the particles toward the new target, in which case the the importance weights will overcompensate for this failure, quickly lead-ing to a degenerate population. The remedy we propose is to find a step size  X  k that satisfies for  X   X  [0 , 1] , whereby a  X  near 1 leads to a strin-gent safeguard, and we X  X e defined scheme is myopic, the behaviour of the algorithm can be sensi tive to the number of iterations. The safeguarded step size is derived as follows. The goal is t o find the largest step size  X  k the safeguarded step size is the solution to the importance weights (13) was unstable as it occasionally recommended strange step sizes, but a naive importance weight update without Rao-Blackwelliza tion yielded a reliable bound on (14). computed in O ( n ) time. Since the importance weights initially have zero vari ance, no positive factor  X   X  (0 , 1) from the optimal sample. Resampling will still be necessary over long sequences to prevent the population from degenerating. The basic algo rithm is summarized in Fig. 4. Microsatellite genetic markers have been used to determine the genealogy of human populations, and to assess individuals X  an-cestry in inferring disease risks [16]. The problem is that a ll these tasks require defining a priori population structure. The Bayesian model of Pritchard et al. [26] offers a solution to th is conundrum by simultaneously identifying both patterns of p op-ulation subdivision and the ancestry of individuals from hi ghly variable genetic markers. This model is the same as LDA assum ing fixed Dirichlet priors and a single genetic marker; see Fig. 5 for the connection betwee n the two domains. This model, however, can be frustrating to work with because independen t MCMC simulations can produce remarkably different answers for the same data, even simulat ions millions of samples long. Such inference challenges have been observed in other mixture mo dels [7]; MCMC can do a poor job exploring the hypothesis space when there are several diver gent hypotheses that explain the data. Method. We used the software CoaSim [21] to simulate the evolution of genetic markers following their common ancestor allele, and the coalescent process is the stochastic process that generates the genealogy [17]. We introduced divergence events at vari ous coalescent times (see Fig. 6) so that we ended up with 4 isolated populations. We simulated 10 microsatellite markers each with a maximum of 30 alleles. We simulated the markers twice with s caled mutation rates of 2 and , and for each rate we simulated 60 samples from the coalescen t process (15 diploid individuals from each of the 4 populations). These samples are the words w in LDA. This may not seem like The goal is to obtain posterior estimates that re-cover the correct population structure (Fig. 6) and exhibit high agreement in independent simula-tions. Specifically, the goal is to recover the mo-ments of two statistics: the admixture distance , a measure of two individuals X  dissimilarity in their ancestry, and the admixture level where 0 means an individual X  X  alleles all come from a single pop-ulation, and 1 means its ancestry is shared equally among the K populations. The admixture dis-tance between individuals d and d  X  is and the admixture level of the d th individual is We compared our algorithm to MCMC as implemented in the softw are S tructure [26], and to another SMC algorithm, annealed importance sampling (AIS) [24], with a uniform tempering schedule. One possible limitation of our study is that the ch oice of temperature scehdule can be note that our intent was not to present an exhaustive compari son of Monte Carlo methods, so we did not compare to population MCMC [18], for example, which h as advantages similar to AIS. For the two data sets, and for each K from 2 to 6 (the most appropriate setting being K = 4 ), we carried out 20 independent trials of the three methods. For f air comparison, we ran the methods with the same number of sampling events: for MCMC, a Markov ch ain of length 50,000 and burn-in of 10,000; for both SMC methods, 100 particles and 50 0 iterations. Additional settings  X  quasi-Newton damping factor of 0 . 75 . We set the initial iterate of stochastic approximation to  X  =  X  =  X   X  kj =  X   X  j . We used uniform Dirichlet priors  X   X  j =  X  k = 0 . 1 throughout. no improvement over MCMC. The next step is to examine the accu racy of these answers. Fig. 8 shows estimates from MCMC and stochastic approximati on selected trials under a mutation The trials were chosen to reflect the extent of variation in th e answers. The mean and standard each matrix correspond to individuals sorted by their true p opulation label; the rows and columns on the sampled alleles w . As expected, the individuals from populations 3 and 4 were h ardest to distinguish, hence the high standard deviation in the bot tom-right entries of the matrix. The results of the second trial are less satisfying: MCMC failed to distinguish between individuals population 2. In all these experiments, AIS exhibited behav iour that was very similar to MCMC. Under the same conditions, our algorithm (bottom-left) failed to distinguish between the third and fourth populations. The trials, however, are more consiste nt and do not mislead by placing high confidence in these answers; observe the large number of dark squares in the bottom-right portion of the  X  X td. dev. X  matrix. This evidence suggests that these trials are more representative of the true posterior because the MCMC trials are inconsistent and occasionally spurious (trial #2). This trend is repeated in the more challenging inference sce nario with K = 3 and a mutation of the admixture distance: the estimates from the first trial are very accurate, but the second individuals in populations 3 and 4. What X  X  worse, MCMC placed disproportionate confidence in these estimates. The stochastic approximation method also exhibited some variance under these high standard deviation in the matrix entries correspondin g to the individuals from population 3. In this paper, we proposed a new approach to probabilistic in ference grounded on variational, Monte Carlo and stochastic approximation methodology. We d emonstrated that our sophisticated inference problem in population genetics. Some of the compo nents such as the variance safeguard beyond the motivation we already gave. More standard tricks , such as Rao-Blackwellization, were explicitly included to demonstrate that well-known techni ques from the Monte Carlo literature apply without modification to our algorithm. We have argued f or the generality of our inference approach, but ultimately the success of our scheme hinges on a good choice of the variational approximation. Thus, it remains to be seen how well our resul ts extend to probabilistic graphical models beyond LDA, and how much ingenuity will be required to achieve favourable outcomes. Acknowledgments We would like to thank Matthew Hoffman, Nolan Kane, Emtiyaz Kh an, Hendrik K X ck and Pooja
