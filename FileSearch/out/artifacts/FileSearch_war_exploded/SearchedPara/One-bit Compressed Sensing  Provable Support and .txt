 Sivakant Gopi gopisivakanth@gmail.com IIT Bombay, Mumbai, India Praneeth Netrapalli praneethn@utexas.edu The University of Texas at Austin, Austin, TX, 78705 USA Prateek Jain prajain@microsoft.com Microsoft Research India, Bangalore, India Aditya Nori adityan@microsoft.com Microsoft Research India, Bangalore, India Several machine learning tasks require estimating a large number of parameters using a small number of training samples. In general, this problem is degener-ate as many parameter vectors can be consistent with the same training data. However, recent works in the area of compressed sensing as well as high-dimensional statistics have shown that if the true parameter vector has certain structure (for example, sparsity, low-rank), then the estimation problem can be solved efficiently ban et al. , 2009 ).
 The above problem can be studied in a number of dif-ferent settings such as compressed sensing, statistical learning etc. In this paper, we mostly focus on the pop-ular compressed sensing setting, where the goal is to design measurement matrices and recovery algorithms to estimate sparse vectors using a few linear measure-ments ( Baraniuk et al. , 2010 ). While the key appli-cation of this problem has been in the area of signal acquisition, it has also found applications in several learning related problems ( Hsu et al. , 2009 ; Duarte et al. , 2008 ; Wright et al. , 2010 ).
 In compressive sensing, a k -sparse signal x  X   X  R n is encoded as so that given b and A , the sparse signal x  X  can be recovered exactly. In this paper, we mostly focus on recovering sparse signals; we briefly discuss extensions to other compressible signals in Section 4.1 . Several results in compressive sensing ( Cand`es &amp; Tao , 2005 ; Garg &amp; Khandekar , 2009 ) have shown that x  X  can be recovered using only m = O ( k log n ) linear measurements. However, all the above approaches re-quire the measurements b to be known exactly (up to infinite precision). Naturally, this requirement is not practical, e.g., image sensors cannot store mea-surements up to arbitrary accuracy. Furthermore, ar-bitrarily quantized measurements might lead to large errors in recovery.
 To address this issue and to simplify the signal acqui-sition process, ( Boufounos &amp; Baraniuk , 2008 ) intro-duced the problem of one-bit compressed sensing (or 1-bit CS) where only one bit of the linear measure-ments, specifically their signs are observed. In partic-ular, given A and we need to recover the k -sparse signal x  X  . Apart from ease of their implementation using comparators, the above measurements are also known to be more ro-bust to noise and non-linearity, and in certain situa-tions perform better than standard compressive sens-ing ( Laska &amp; Baraniuk , 2012 ).
 Note that using 1-bit measurements ( 1 ), we cannot re-cover the norm of x  X  from b because scaling x  X  does not change the measurements. Similarly, a small per-turbation in x  X  may not change b . Therefore, exact recovery of x  X  is in general not possible, even when x  X  is a unit vector.
 Instead, 1-bit CS is typically studied in these two settings: Support recovery: recover the support of x  X  , Approximate vector recovery: recover  X  x that is close to x  X  (up to normalization), i.e., approximation factor.
 For both of the above problems, a solution is evaluated on the following three critical parameters: 1) Number of measurements ( m ), 2) Running time of the recov-ery algorithm, and 3) Universality of the measurement matrix. A 1-bit CS method is universal if a fixed de-sign matrix A can be used to recover all sparse signals. Note that universality is a crucial requirement, as it is practically infeasible in several 1-bit CS applications (for instance, a single-pixel camera) to construct a new A for each signal.
 In this paper, we study one-bit compressive sensing in both the above mentioned settings and improve upon the state-of-the-art results in those settings. 1.1. Support Recovery Existing work : The best known solution for support recovery is by ( Haupt &amp; Baraniuk , 2011 ) that uses O ( k log n ) measurements. However, their solution is not universal, which is crucial for several real-world applications.
 Our Results : We propose the first universal mea-surement matrices for support recovery in the 1-bit compressed sensing problem. Our solutions are based on two combinatorial structures, called union free sets and expander graphs. Compared to existing work, our measurement schemes however require a factor of O ( k ) and O ( k 2 ) more measurements respectively. See Ta-ble 1 for a comparison of our methods with the method by ( Haupt &amp; Baraniuk , 2011 ) with respect to the above mentioned critical problem parameters. We would like to note that while expanders have previously been used in compressed sensing ( Jafarpour et al. , 2009 ), to the best of our knowledge, union free sets have so far not been used in this domain and might have applications to other related tasks as well.
 1.2. Approximate Recovery Existing work : ( Plan &amp; Vershynin , 2011 ) and ( Plan &amp; Vershynin , 2012 ) provide provable and ef-ficient recovery methods for this problem. In par-ticular, ( Plan &amp; Vershynin , 2012 ) provides a pow-erful framework for recovering a large class of com-pressible signals using only one-bit measurements. However, the number of measurements required by both ( Plan &amp; Vershynin , 2011 ) and ( Plan &amp; Ver-shynin , 2012 ) are sub-optimal in the dependence on  X  O  X   X  5 and O  X   X  6 respectively .
 Our Results : We propose a novel solution that ex-ploits well-known results for the standard compressed sensing problem to guarantee recovery using an opti-mal number of measurements, i.e., O  X   X  1  X  see sup-plementary material for a lower bound. See Table 2 for a comparison of our proposed method with the ex-isting methods.
 Finally, our experimental results show that our meth-ods are also empirically competitive with existing methods. Since the focus of this paper is on practical and provable methods for 1-bit CS, we draw a com-parison only against known state-of-the-art provable methods.
 Notation : We denote vectors using bold-faced letters (e.g., x ) and matrices using capital letters (e.g., A ). x i denotes the i -th element of x , and a ( i ) denotes i -th row of A . x ( S ) denotes elements of x restricted to set S , and A ( S ) denotes columns of A from set S . A  X  R m  X  n denotes a design matrix, and x  X   X  R n denotes the true signal. k x k p denotes the  X  p norm of x , and k x k 0 denotes the number of non-zeros in x . supp( x ) denotes the set of non-zero elements of x . We use e O ( ) to ignore poly (log k +log log n ) factors. Wlog stands for with out loss of generality, and s.t. stands for such that. Compressive sensing (CS) using precise linear mea-surements is a well-studied problem and several meth-farpour et al. , 2009 ) are known to achieve efficient re-covery using a near optimal number of measurements. In comparison, the problem of 1-bit CS is relatively new and the state-of-the-art algorithms still lack in certain regards. For support recovery, existing algo-rithms are not universal while for approximate recov-ery they do not have information theoretic optimal measurement complexity bounds (See previous section for more detailed discussion).
 Apart from provable recovery methods, several heuris-tics have also been proposed for this problem ( Boufounos , 2009 ; Laska et al. , 2011 ; Jacques et al. , 2011 ); these methods have good empirical performance but lack theoretical guarantees. Apart from the stan-dard 1-bit CS problem, several variants/extensions have also been studied. For instance, ( Davenport et al. , 2012 ) recently studied a similar problem called 1-bit matrix completion . ( Ai et al. , 2012 ) recently extended recovery results to measurement matrices A sampled from more general sub-Gaussian distribu-tions. Problem statement : Design a measurement matrix A  X  lem: given b = sign( A x  X  ) with x  X   X  R n , k x  X  k 0  X  k , find supp( x  X  ).
 For this problem, we propose two different approaches based on: a) union free family (UFF) of sets, b) ex-pander graphs. For both these approaches, we provide the design matrix as well as the corresponding recovery algorithm. 3.1. Support Recovery using Union Free In this section, we describe an efficient algorithm that recovers the support of any non-negative vector using O k 2 log n measurements. 3.1.1. UFF Background Let U be a fixed set, and let B i  X  U, 1  X  i  X  n . Then, the family of sets F = { B 1 , , B n } is said to be k -union free if no B i lies in the union of any other k sets from F .
 Definition 1. A family of sets F : = { B 1 , , B n } with underlying set U =  X  n i =1 B i is called a k -union-free family ( k -UFF) iff: B i for all distinct B i Definition 2. A k -UFF is called d -regular-k -union-free ( d, k ) -UFF if:  X  B i  X  F , | B i | = d. The following theorem from ( Erd  X os et al. , 1982 ) guar-antees existence of a large ( d, k )-UFF.
 Theorem 1. ( Erd  X os et al. , 1982 ) Let n ( m, k, d ) de-note the maximum cardinality of a ( d, k ) -UFF over an m -element underlying set ( | U | = m ). Let h =  X  d k  X  . Then, It is well known that such a family can be constructed using a randomized method: form subsets B i , 1  X  i  X  n by selecting d elements of the underlying set U uni-formly at random. The algorithm is formally given in Algorithm 1 .
 Algorithm 1 Probabilistic construction of a size n , ( d, k )-UFF from an m -element underlying set. input m, n, d 1: U  X  { 1 , 2 , . . . , m } , F  X   X  2: for i = 1 , , n do 3: Obtain B i by randomly sampling d distinct ele-4: F  X  F  X  X  B i } 5: end for output F The following theorem shows that with high probabil-ity, for appropriate choice of the parameters m and d , Algorithm 1 outputs a ( d, k )-UFF with high probabil-ity.
 Theorem 2. For m = 10 k 2 log 3 n  X  and d = k log 3 n  X  , Algorithm 1 outputs a ( d, k ) -UFF with probability greater than 1  X   X  .
 We provide a proof of Theorem 2 in the supplementary material. 3.1.2. UFF based sensing matrix We now provide a novel method of constructing mea-surement matrix A using a given ( d, k )-UFF. Using the randomized construction mentioned in Al-gorithm 1 , construct a ( k log 3 n  X  , k )-UFF F , where |F| = n with underlying set U = { 1 , 2 , . . . , m } . Note that, from Theorem 2 , we can choose m = Next, the sensing matrix A is defined as follows: That is, the j -th column of A is the incidence vec-tor of B j . Also, if x  X   X  R n is a non-negative vec-tor, then A x  X   X  0. Hence, the i th measurement b = sign( a ( i ) x  X  ) is given by: Note that A  X  { 0 , 1 } m  X  n , b  X  { 0 , 1 } m where m = O ( k 2 log n ).
 Support Recovery Algorithm : We now present our support recovery algorithm that estimates the support set b S of x  X  using measurements b constructed by the above described ( d, k )-UFF based design matrix A . Our algorithm proceeds in n steps: at the j -th step (1  X  j  X  n ), we add element j to the support set b S if pseudo code.
 The following theorem proved in the supplementary material establishes the correctness of Algorithm 2 . Algorithm 2 UFF based Support Recovery (UFF) input A : measurement matrix, b : measurement vec-1: b S  X   X  2: for j = 1 , , n do 4: b S  X  b S  X  X  j } 5: end if 6: end for output b S Theorem 3. Suppose x  X   X  R n is a non-negative vector s.t. k x  X  k 0  X  k , A is a sensing matrix con-structed according to ( 2 ) and b is computed using ( 3 ) . Then, the set b S returned by Algorithm 2 satisfies: b S = supp ( x  X  ) . 3.1.3. Discussion Above, we described our UFF based algorithm for the support recovery problem. Note that, the algorithm is universal, i.e., one design matrix A can be used to recover the support of every k -sparse non-negative vec-tor. Furthermore, the algorithm is efficient, with time complexity O ( nk log n ), and is easy to implement. Also, note that the measurements given by ( 3 ) are bi-nary measurements (i.e., in { 0 , 1 } m ) rather than signed measurements (i.e., in { X  1 , 1 } m ), but they are essen-tially of the same nature.
 Robustness to noise : Note that Algorithm 2 re-quires exact measurements without any noise. How-ever, we can easily extend our method for handling arbitrary adversarial noise in measurements b . That is, for the case where value of a small number of b i  X  X  can be flipped arbitrarily. To this end, we use the following robust version of UFFs: Definition 3. A family of sets F = { B 1 , B 2 , B n } is called a ( d, k,  X  ) -UFF if | B i  X  | B i each set in F has size d .
 Theorem 4. ( de Wolf , 2012 ) There exists a ( d, k,  X  ) -UFF F over an m -element underlying set such that Using a ( d, k,  X  ) UFF as in Theorem 4 , Algorithm 2 can be modified to make it robust up to 1 2  X   X  d ad-versarial errors, i.e., 1 2  X   X  d arbitrarily flipped mea-surements. See Algorithm 8 and Theorem 8 in the supplementary material for further details.
 Handling Vectors with Negative Elements : One drawback of our UFF based algorithm is that it cannot handle cases where the underlying signal x  X  has nega-tive entries. A solution to this problem is to select each non-zero A ij uniformly at random from [0 , 1] (instead of fixing it to be 1). This will ensure the following with probability 1:  X  if  X   X  S  X  then  X  if  X  /  X  S  X  then  X  j  X   X  B l such that The above observations along with proof of Theorem 3 shows that we can recover support of x  X  even if it has negative entries. However, a drawback of this solution is that the resulting algorithm is not universal as the values of A need to be sampled afresh for each x  X  . In the next section, we present a solution based on expanders that can handle vectors with negative ele-ments and is universal as well (although with higher measurement complexity). 3.2. Support Recovery using Expanders We now describe an expanders based algorithm that recovers the support using O k 3 log n measurements. 3.2.1. Expanders Background A left regular bipartite graph is called an expander if every small enough subset of the nodes on the left have a large enough neighborhood set on the right. Definition 4. A d -left-regular bipartite graph ( U, V ) , where N ( S ) is the neighborhood of nodes in set S . Expanders are closely related to UFF and hence can be constructed in the same way as in Algorithm 1 . For the sake of completeness, we recall the following result that establishes the existence of good expanders. Lemma 1. (Claim 1 , ( Berinde &amp; Indyk , 2008 )) For any n/ 2  X  k  X  1 ,  X  &gt; 0 there exists an ( n, m, d, k,  X  ) 3.2.2. Expander based Sensing Matrix In this section, we present a method to construct a sensing matrix A using a given expander. We first construct a ( n, m, d, k + 1 ,  X  )-expander with  X  = 1 16 k Using Theorem 1 , we can choose m = O ( k 3 log n k ) and d = O ( k log n k ). Let A  X  R m  X  n be the adjacency ma-trix of the expander: Then, we use A as the sensing matrix and observe b = sign( A x  X  ), with x  X   X  R n , k x  X  k 0  X  k . Support Recovery Algorithm : We now present our support recovery algorithm that estimates support set b S of x  X  using measurements b constructed using the Algorithm 3 Support recovery algorithm when A is constructed from a ( n, m, d, k,  X  )-expander. input A : measurement matrix, b : measurement vec-1: b S  X   X  2: for j = 1 , , n do 3: if | N ( j )  X  supp( b ) | &gt; d 2 then 4: b S  X  b S  X  X  j } 5: end if 6: end for output b S above described design matrix A . Our algorithm pro-ceeds in n steps: at the j -th step (1  X  j  X  n ), we add element j to b S if the measurement corresponding to at least half of the neighbors of j (i.e. N ( j )) are for a detailed pseudo code.
 The following theorem proved in the supplementary material shows correctness of Algorithm 3 .
 Theorem 5. Let b = sign ( A x  X  ) with k -sparse x  X   X  R n and A as constructed in Section 3.2.2 , then Al-gorithm 3 correctly identifies S  X  = supp ( x  X  ) , i.e., b S = S  X  .
 Discussion : Note that Algorithm 3 can exactly re-set sign of each element b x j ( j  X  b S ) to be the sign of the majority of elements in N ( j )  X  supp( b ). Robustness : for our choice of parameters, the algo-rithm can tolerate up to d 4 adversarial bit flips in b . Robustness up to d adversarial errors can be obtained by choosing graphs with better expansion property. Finally, observe that the computational complexity of Algorithm 3 is O nk log n k . 3.2.3. Divide and Conquer In this section, we present a  X  X ivide and Conquer X  ap-proach that in conjunction with our support recovery algorithms can achieve even lower measurement com-plexity than our support recovery algorithms. How-ever, the obtained approach is no longer universal. The key idea is to first partition the n coordinates of x  X  into k disjoint random sets of equal size; Wlog we can assume that k divides n . Since the sparsity of x  X  is k , on an average, each of the random partitions has sparsity 1. Using standard concentration bounds, with high probability, each of the partitions has at most O (log k ) non-zeros. We can then use our algorithms from Sections 3.1 or 3.2 to recover the support of each of the k subsets.
 Algorithm 4 Measurements for Algorithm 5 input m , n , d , k 1: P  X  random permutation matrix 3: B : generate block diagonal matrix using output B P Algorithm 5 Support Recovery for Divide and Con-quer Approach input A  X   X  :  X  -th block UFF-based sensing matrix, B : 1: b S  X   X  2: for l = 1 , , k do 3: b  X   X  b ((  X   X  1) m k , ,  X  m k  X  1) i.e.  X  -th block of 4: Run Algorithm 2 on b  X  and A  X   X  to recover b S  X  5: b S  X  b S  X  P  X  1 ( b S  X  ) 6: end for output b S Similar to the result by ( Haupt &amp; Baraniuk , 2011 ), we can show that the number of measurements needed by this approach is optimal up to poly (log k ), although the obtained approach is not universal. Algorithm 4 provides a pseudo-code for generating sensing matrix and Algorithm 5 provides the recovery algorithm. Construction of the measurement matrix: The measurement matrix A is given by A = B P where P is a random permutation matrix and B is a block diagonal matrix with k equal blocks, each block being a UFF-based matrix A  X   X  , constructed using Algorithm 1 with parameters ( m k , n k , d ).
 where A  X   X  =Algorithm 1 ( m k , n k , d ) Theorem 6. Suppose x  X   X  R n + s.t. || x  X  || 0  X  k , A = B P is a sensing matrix as in Algorithm 4 with m = e O k log n 5 returns supp ( x  X  ) in time e O n log n k with probability See the supplementary material for a detailed proof. Problem Statement : Design matrix A  X  R m  X  n and an algorithm to solve: given b = sign( A x  X  ) (where x  X   X  R n , k x  X  k 0  X  k and k x  X  k that: where  X  &gt; 0 is a given tolerance parameter. Note that assuming x  X  to be of unit norm entails no loss of generality since scaling x  X  doesn X  X  change b . In particular, we can never recover k x  X  k 2 from b . For this problem, we propose two novel solutions which are both universal, provide their measurement com-plexity and also provide their time complexity. Our first solution is based on combining standard com-pressed sensing techniques with Gaussian measure-ments (see Section 4.1 ). Our second method first re-covers the true support using methods of Section 3 and then uses Gaussian measurements to approximately recover elements of x  X  (see Section 4.1 ). 4.1. Two-stage Approximate Recovery In this section, we present our first approach for two-stage approximate recovery that exploits existing com-pressed sensing methods. Broadly, we design the measurement matrix A as a product of two matrices A select A 1 to be a standard compressed sensing matrix and A 2 to be an iid Gaussian matrix. So the measure-ments are: Now, let z  X  = A 1 x  X  and b = sign( A 2 z  X  ). The main idea is that given b and A 2 , we can find a vector b z that satisfies each of the measurement, i.e., sign( A 2 b z ) = b . Furthermore, using Theorem 10 (Theorem 2, Jacques et al. ( 2011 )), b z should closely approximate z  X  . Next, given b z and A 1 , using standard compressed sensing algorithms we estimate  X  x which should be a close ap-proximation to x  X  .
 Construction of the measurement matrix: Let A 1 is a matrix that satisfies the restricted isometry property (RIP) ( Cand`es &amp; Tao , 2005 ) with  X  2 k &lt; A 1 is said to satisfy k -RIP with constant  X  k if,  X  x  X  R n , k x k 0  X  k : Also, if m  X  = O ( k log n k ) and each entry of A 1 is sam-pled from a centered sub-Gaussian then A 1 satisfies Next, select m = O 1  X  m  X  log m  X   X  = e O 1  X  k log n k and sample each entry of A 2 independently from N (0 , 1). Using Theorem 10 (supplementary material) by ( Jacques et al. , 2011 ), with high probability such a Algorithm 6 Two-stage Approximate Recovery input A 1 , A 2 : measurement matrices (see Sec-1: Stage 1 : Run an LP solver for the following LP: 2: Stage 2 : Run GradeS algorithm ( Garg &amp; Khan-output b x measurement matrix ensures that:  X  x , y , sign( A 2 x ) = sign( A 2 y )  X  x Algorithm for approximate recovery: In this sec-tion, we present our two-stage algorithm for approx-imate recovery. As mentioned earlier, the algorithm first uses a half space learning algorithm to obtain an estimate b z of A 1 x  X  and then uses the GradeS al-gorithm, a robust compressed sensing algorithm by ( Garg &amp; Khandekar , 2009 ), on b z to obtain an estimate b x of x  X  . See Algorithm 6 for a pseudo-code of our approach. For completeness, we provide the GradeS algorithm in the supplementary material.
 Below, we provide proof of correctness of Algorithm 6 . Theorem 7. Let x  X   X  R n be a k -sparse vector with k x  X  k 2 = 1 and let A 1 and A 2 be chosen as described in the previous section. Also, let b = sign ( A 2 A 1 x  X  ) . Then for b x returned by Algorithm 6 , See the supplementary material for a detailed proof. Note that the computational complexity of solving the LP in Stage 1 of our algorithm can be bounded by Remarks : The above algorithm can be made robust to classification noise by repeating each measurement a fixed number of times and taking a majority vote. For instance, suppose each measurement is correct with probability 1 2 + p and is flipped with probability 1 2  X  p . Then repeating each measurement O log m p times, we can argue that a majority vote of measurements will give us the true measurements (with high probability). We can then use Algorithm 6 to recover x  X  .
 Extension to Other Compressible Signals: Note that, the second stage of Algorithm 6 is essentially just a  X  X tandard X  compressed sensing module, whose goal is used to recover x  X  from  X  X tandard X  (noisy) linear measurement of x  X  , i.e., b z = A 1 x  X  +  X  . Hence, we can Algorithm 7 Support Recovery based Approximate Recovery (S-Approx) input A 1 and A 2 , b = b 1 1: Stage 1 : Run Expanders algorithm (Algorithm 3 ) 3: Stage 2 : Run an LP solver for the following LP: output b x modify our second stage to recover other compressible signals as well, by directly using the corresponding re-covery method. Examples of such compressible signals include low-rank matrices, low-rank + sparse matrices, wavelet based sparse vectors etc.
 The framework by ( Plan &amp; Vershynin , 2012 ) can also recover a large class of compressible signals. However, as in the case of sparse vectors, their dependence on the error  X  is  X   X  6 while ours is only  X   X  1 . Furthermore, ( Plan &amp; Vershynin , 2012 ) needs to compute  X  X aussian width X  for each of these class of functions; in contrast, we can directly use the existing results for these class of signals to provide measurement complexity. Support Recovery based Approximate Recov-ery: In this section, we present another approach for approximate vector recovery that first recovers the support of x  X  using our Expanders algorithm (Algo-rithm 3 ) and then solves the resulting low dimensional problem. That is, we choose the design matrix to be: A = on expanders (as in Section 3.2 ) and A 2 is an iid stan-dard Gaussian matrix. Using the measurements corre-sponding to A 1 , we can first recover the support using Algorithm 3 .
 Once we have the support, we can solve an LP re-stricted to the support, to obtain b x that is consis-tent with the measurements. That is, sign( A 2  X  x ) = sign( A 2 x  X  ). Again using Theorem 10 (supplementary material) by ( Jacques et al. , 2011 ), we can conclude code of our approach.
 Now, the first step of support recovery re-quires O k 3 log n k measurements (Theorem 5 ) and O nk log n surements and e O k 5  X  5 time for recovery. So overall, the algorithm needs e O k 3 log n k + k  X  measurements In this section, we present empirical results for our al-gorithms for support recovery as well as approximate vector recovery. For support recovery, we evaluate our UFF algorithm (Algorithm 2 ) against the sketch based algorithm by ( Haupt &amp; Baraniuk , 2011 ) ( HB ). For support recovery, we evaluate our TwoStage algo-rithm and S-Approx algorithm against the algorithm by ( Plan &amp; Vershynin , 2012 ) ( PV ).
 Support Recovery : For these experiments, we gen-erate a k -sparse signal x  X   X  { 0 , 1 } n randomly and esti-mate its support using linear measurements proposed by each of the algorithms. We report the L 1 error in support estimation: Error Support ( b S, S  X  ) = | S  X   X  b We first compare recovery properties of different meth-ods using phase transition diagrams that are com-monly used in the compressive sensing literature (see Figure 2 (a), (b)). For this, we fix the number of mea-surements ( m = 500) while varying n and k . For each problem size ( k, n, m ) we generate 20 synthetic prob-lems and plot probability of exact support recovery; probability values in Figure 2 are color coded with red representing high probability of recovery while blue represents low probability of recovery. Figure 2 (a), (b) show the phase transition diagrams of our UFF method (Algorithm 2 ) and the HB method, respec-tively. Note that UFF is able to recover the support for a significantly larger fraction of problems than HB. Next, we study error incurred by different methods when the number of measurements are not enough for recovery. First, we fix n = 3000 , m = 500 and vary k . Figure 1 (a) compares the error incurred by our UFF algorithm against the HB algorithm. Clearly, our UFF based algorithm incurs smaller error than HB for large k . For example, for k = 20, UFF is able to recover the support exactly, while HB incurs around 20% error. Next, we fix n = 3000 , k = 20 while varying m . Figure 1 (b) shows that UFF is able to achieve exact recovery with around 400 measurements while HB requires around 800 measurements.
 Approximate Recovery : Here, we generate k -sparse signals x  X   X  R n where non-zeros are sampled using the standard k -variate Gaussian. We report er-ror in recovery, i.e., Error Approx =  X  x k  X  Here again, we first plot phase transition diagrams for different methods. We fix m = 500 and vary n, k ; for each problem size ( m, n, k ) we measure probability of success (out of 20 runs) where a method is considered to be successful for an instance if the error incurred is less than 0 . 3. Figures 2 (c), (d), (e) clearly show that S-Approx is significantly better than both TwoStage and PV; TwoStage is also marginally better than PV. Next, we fix n = 3000 and m = 500, while varying k . Figure 1 (c) compares TwoStage and S-approx algo-rithms with the PV algorithm. Here again, TwoStage and PV are comparable while S-approx incurs signif-icantly less error for k &lt; 24. For larger k , TwoStage and PV are significantly better than S-approx. Finally, we fix n = 3000 and k = 20, while varying m . Here again, for small number of measurements, S-approx in-curs more error compared to TwoStage and PV. But, for larger number of measurements, it is significantly more accurate.
 Ai, A., Lapanowski, A., Plan, Y., and Vershynin, R.
One-bit compressed sensing with non-gaussian mea-surements. arXiv preprint arXiv:1208.6279 , 2012. Baraniuk, Richard G., Cevher, Volkan, Duarte,
Marco F., and Hegde, Chinmay. Model-based com-pressive sensing. IEEE Transactions on Information Theory , 56(4):1982 X 2001, 2010.
 Berinde, Radu and Indyk, Piotr. Sparse recovery using sparse random matrices, 2008.
 Boufounos, Petros and Baraniuk, Richard G. 1-bit compressive sensing. In CISS , pp. 16 X 21, 2008. Boufounos, Petros T. Greedy sparse signal reconstruc-tion from sign measurements. In Proceedings of the 43rd Asilomar conference on Signals, systems and computers , pp. 1305 X 1309, 2009.
 Cand`es, Emmanuel J. and Recht, Benjamin. Exact matrix completion via convex optimization. Founda-tions of Computational Mathematics , 9(6):717 X 772, December 2009.
 Cand`es, Emmanuel J. and Tao, Terence. Decoding by linear programming. IEEE Transactions on Infor-mation Theory , 51(12):4203 X 4215, 2005.
 Davenport, M.A., Plan, Y., Berg, E., and Woot-ters, M. 1-bit matrix completion. arXiv preprint arXiv:1209.3672 , 2012. de Wolf, Ronald. Efficient data structures from union-free families of sets. http://homepages.cwi.nl/ ~rdewolf/unionfree_datastruc.pdf , 2012.
 Duarte, M.F., Davenport, M.A., Takhar, D., Laska, J.N., Sun, Ting, Kelly, K.F., and Baraniuk, R.G.
Single-pixel imaging via compressive sampling. Sig-nal Processing Magazine, IEEE , 25(2):83  X 91, march 2008. ISSN 1053-5888. doi: 10.1109/MSP.2007. 914730.
 Erd  X os, P  X eter L., Frankl, Peter, and F  X uredi, Zolt  X an.
Families of finite sets in which no set is covered by the union of two others. J. Comb. Theory, Ser. A , 33(2):158 X 166, 1982.
 Garg, Rahul and Khandekar, Rohit. Gradient de-scent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property. In ICML , 2009.
 Haupt, Jarvis and Baraniuk, Richard G. Robust sup-port recovery using sparse compressive sensing ma-trices. In CISS , pp. 1 X 6, 2011.
 Hsu, D., Kakade, S. M., Langford, J., and Zhang, T. Multi-label prediction via compressed sensing.
In Advances in Neural Information Processing Sys-tems , 2009.
 Jacques, Laurent, Laska, Jason N., Boufounos, Petros, and Baraniuk, Richard G. Robust 1-bit compressive sensing via binary stable embeddings of sparse vec-tors. CoRR , abs/1104.3160, 2011.
 Jafarpour, Sina, Xu, Weiyu, Hassibi, Babak, and
Calderbank, A. Robert. Efficient and robust com-pressed sensing using optimized expander graphs.
IEEE Transactions on Information Theory , 55(9): 4299 X 4308, 2009.
 Laska, Jason N. and Baraniuk, Richard G. Regime change: Bit-depth versus measurement-rate in com-pressive sensing. IEEE Transactions on Signal Pro-cessing , 60(7):3496 X 3505, 2012.
 Laska, Jason N., Wen, Zaiwen, Yin, Wotao, and Bara-niuk, Richard G. Trust, but verify: Fast and accu-rate signal recovery from 1-bit compressive measure-ments. IEEE Transactions on Signal Processing , 59 (11):5289 X 5301, 2011.
 Negahban, Sahand, Ravikumar, Pradeep D., Wain-wright, Martin J., and Yu, Bin. A unified framework for high-dimensional analysis of $m$-estimators with decomposable regularizers. In NIPS , pp. 1348 X  1356, 2009.
 Plan, Y. and Vershynin, R. One-bit compressed sensing by linear programming. arXiv preprint arXiv:1109.4299 , 2011.
 Plan, Yaniv and Vershynin, Roman. Robust 1-bit compressed sensing and sparse logistic regres-sion: A convex programming approach. CoRR , abs/1202.1212, 2012.
 Tropp, Joel A. and Gilbert, Anna C. Signal recovery from random measurements via orthogonal match-ing pursuit. IEEE Transactions on Information Theory , 53(12):4655 X 4666, 2007.
 Wright, J., Ma, Yi, Mairal, J., Sapiro, G., Huang, T.S., and Yan, Shuicheng. Sparse representation for com-puter vision and pattern recognition. Proceedings of the IEEE , 98(6):1031  X 1044, june 2010. ISSN 0018-
