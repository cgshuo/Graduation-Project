 Face Recognition (FR) has a wide range of applications, such as military, commercial algorithms have been proposed. Among these FR methods, the most popular methods are appearance-based approaches. 
Of the appearance-based FR methods, those utilizing Linear Discriminant Analysis (LDA) [4-16] techniques have shown promising results. Conventional LDA [4,5] algorithm aims to find an optimal transformation by minimizing the within-class scatter matrix and maximizing the between class scatter matrix simultaneously. The optimal transformation is readily computed by appl ying the eigen-decomposition to the scatter matrices. But an intrinsic limitation of conventional LDA is that its objective function within-class scatter matrix is often singular since the dimension of sample exceeds the number of sample and conventional LDA based methods suffer from the so-called  X  X mall sample size X  [6-8] problem. In the last decades, numerous methods ha ve been proposed to solve this problem. scatter matrix with its pseudoinverse. The perturbation method is used in [10], where a nonsingular. Cheng et al [11] proposed the Rank Decomposition method based on successive eigen-decomposition of the total scatter matrix and the between-class scatter matrix. However, the above methods are typically computationally expensive since the scatter matrices are very large. Swets and Weng [12] proposed a two stages PCA+LDA dimension reduction so as to make the within-class scatter matrix nonsingular before the application of LDA. By far, the PCA+LDA method is popular used. However, prevent this from happening, many extended LDA algorithms with null space problem and extract optimal classification features from original samples. But conventional DLDA [13-15] algorithm is often computationally expensive and not scalable. 
In this paper, first of all, we briefly recall the DLDA algorithm. Then we perform an in-depth analysis on DLDA algorithm and proposed a DLDA/QR algorithm. The steps. Thus we can implement the second stage of DLDA in a low dimensional space. foundation of the proposed method is revealed. Throughout the paper, C denotes the number of classes, m is the dimension, N is matrix, within-class scatter matrix and total class scatter matrix, respectively. 
DLDA [13-15] algorithm was proposed by Chen and Yang, which attempts to avoid the shortcomings existing in conventional solution to the  X  X mall sample size X  problem. The basic idea behind the DLDA algorithm is that the null space of w S may contain extracted by the DLDA algorithm is the intersection space w b N N  X  ' . 
The difference between Chen X  X  method and Yang X  X  method is that Yang X  X  method 
N . Although there is no significant difference between the two approaches, it may be algorithm. DLDA algorithm, the proposed algorithm can be realized through two stages. The first QR-decomposition. The second stage contains LDA algorithms that involve the concern of within-class distance. 
The first stage aims to solve the following optimization problem: maximizing between-class scatter matrix. The solution can be obtained by solving the also be obtained through the QR-decomposition on the matrix b H as follows, where and satisfies Let be the QR decomposition of b H , where t m X Q  X   X  1 , and optimization problem in Eq.(1). Note the rank t of the matrix b H , is bounded above In this case, the reduced dimension 
The second stage of DLDA/QR algorithm will concern the within-class distance. In this stage, the optimization problem is exactly the same one as in classical DLDA, but with matrixes of much smaller size, hence can be solved efficiently and stably. After we have obtained the matrix 1 Q , we assume that 1 1 this case, it is easy to verify that both t S should be noticed that the matrix b S
In this stage, we should find a matrix that simultaneously diagonalizes both b S Where  X  is a diagonal matrix whose diagonal elements are sorted in increasing order and I is a unitary matrix. symmetric matrix b S &lt;&lt; , it is easy to diagonalize the matrix. 
Assume that there exists the matrix U such that where I U U T = and b  X  is a diagonal matrix whose diagonal elements are sorted in decreasing order. Let Then we can obtain Next, let In a similar way, we can diagonalize the matrix t S  X  . 
Assume that there exists the matrix Y such that where I Y Y T = and t  X  is a diagonal matrix whose diagonal elements are sorted in increasing order. Let Then, we can obtain And 
Therefore, we obtain the matrix ZP V s = simultaneously diagonalizes both b S and t S
Thus, we can obtain the following transform matrix: where E is a s m  X  matrix. 
To a test image test x , the feature of this test image is found by which can be used to classify. 
Furthermore, the aforementioned simultaneous diagonalization can be further simplified by the following theorem [4]. Theorem 1. We can diagonalize two symmetric matrices t S is obtained by the matrix  X  . 
Based on the above discussion, the proposed DLDA/QR algorithm is described as follows: Step 2: Calculate the eigenvector matrix and the eigenvalue matrix of first s smallest eigenvalues that the s eigenvectors and eigenvalues form the matrix V and Step 3: Project samples into subspace acco rding to Eq.(16) and classify. To demonstrate the effectiveness of our method, experiments were done on the ORL face database (http://www.uk.research.att.com/facedatabase.html). Fig.1 depicts some images from the ORL face database. subject. In each round, the training samples are selected randomly from the gallery and the remaining samples are used for testing. This procedure was repeated 10 times by randomly choosing different training and testing sets. The number of training samples per subject,  X  , increases from 4 to 6 and the number of final discriminant vectors is 39 distance metric and cosine distance are used in our experiments. For each distance metric, mean and standard deviation of Fisherface, DLDA and DLDA/QR are listed in Table 1 and Table 2. From these results, we can conclude that the performance of the DLDA/QR algorithm is as well as (even slightly better than) that of conventional DLDA algorithm and superior to that of Fisherface algorithm. Then, recognition rates with different features numbers are shown in Fig.2. In this figure, as the features numbers varying from 20 to 39 with number of training samples per subject equals to 5 and number of classes equals to 40, the recognition rates are depicted. From that, with the QR decomposition, the DLDA algorithm becomes efficiency and algorithm can implement in a low dimension space, which avoids handling large matrices and improves the stability of the computation. In this paper, we proposed an extension of direct linear discriminant analysis algorithm, method does not require the whole data matrix in main memory. This is desirable for complexity of the DLDA/QR algorithm is linear in the number of the data items in the training data set as the number of classes and the number of dimensions. It is the QR supported by our experimental results. DLDA/QR algorithm is competitive with the ones achieved by DLDA and Fisherface algorithm. With efficiency and scalability, DLDA/QR algorithm is promising in real-time application involving extremely high-dimensional data. 
