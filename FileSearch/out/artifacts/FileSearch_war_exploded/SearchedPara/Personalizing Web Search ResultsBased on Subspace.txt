
Jingfei Li 1 , Dawei Song 1 , 2 , Peng Zhang 1 , Ji-Rong Wen 3 , and Zhicheng Dou 4 Over decades, modern search engines h ave transformed the way people access and interact with information. Users can easily search for relevant information by issuing simple queries to search engines. Despite the increasing popularity and convenience, search engines are faci ng some challenges. For example, given a query, a typical search engine usually returns a long list of URLs, usually dis-played in a number of pages. We call the results list as Original List . However, the top ranked URLs may not always satisfy users X  information needs well. Users may have to scroll down the current result page and even turn to the following pages to find desired information. This would affect the users X  search experience and satisfaction. One way to tackle this problem is through search personal-ization based on an individual user X  X  profile representing the user X  X  personal preferences and interests.

Personalized search has recently attrac ted much interest. Many personalized search strategies [5,6], [8], [10], [13] build on the users X  click-through data, where it is assumed that the clicked URLs are relevant [8]. This assumption is not rigorous, because users often go back quickly after clicking an irrelevant result. Previous research indicated that the clicks with short dwell time ( X  X uick backs X ) are unlikely to be relevant[10]. In this paper, we utilize the  X  X AT click X  criteria [10] ([i] the user dwelled on the result page corresponding to the clicked URL for at least 30 seconds; [ii] the click was the last click in current query session) to judge the relevance of a clicked document. Only the  X  X atisfied X  click data (URLs and corresponding documents) are used to build a user profile. The  X  X AT click X  data is also used as the ground truth when evaluating the proposed algorithms.
The classic Vector Space Model (VSM) has been a popular choice for user profile representation [21], in which the queries, documents and user profiles are all represented as vectors in a term space [12], [21]. Generally, the weight of each term (or keyword) in a user profile vector is calculated by its TF  X  IDF weight. However, representing user profiles as weighted keyword vectors has several in-herent limitations. As the number of keyw ords increases, the v ector representa-tion becomes ambiguous. Moreover, the traditional bag of words models in IR, such as VSM and unigram language model, are based on the term independence assumption . This assumption simplifies the development and implementation of retrieval models, but ignores the fact that some words are dependent on each other. Intuitively, two co-occurring words can convey more semantic information than the single words individually. For example, when  X  X bama X  and  X  X omney X  co-occur in a document, we may easily rec ognize that this document is about the American Election, but if we only observe one single word  X  X bama X  or  X  X om-ney X , the the topic of this document can b e different. To address this issue, term dependencies need to be mined and incorporated into IR models to improve re-trieval performance [1], [9], [21]. In t his paper, we propose to represent a user profile as a vector subspace, and make use of term dependence information, in the form of a word-correlation matrix, to generate the user profile subspace.
Our method is inspired by the idea of using a vector space basis for modeling context, originally proposed by Melucci [15] , where each basis vector refers to a contextual property. In linear algebra , a vector can be generated by a basis. In this way, an information object (e.g., an information need) represented by a vec-tor can be generated by the context model ed with the basis. Melucci [1] computed the probability that an information object has been materialized within a context. In this paper, we extend the idea to user profile representation. We systematically investigate and evaluate two novel algorithms based on the subspace projection for personalized re-ranking of Web search r esults. Specifically, we represent a user profile as a subspace spanned by a basis derived from a word-correlation matrix built from the user X  X  SAT clicked web pages. The personalized score for a docu-ment can be computed by projecting the document (represented as a vector in the first algorithm or another word-correlation subspace in the second algorithm) onto the user profile subspace. Then we re-rank the original list returned by a prominent search engine (Bing) based on the Borda X  rank fusion method [14]. In this section, we briefly review the related work on two areas, including per-sonalized information retrieval and the geometry underlying IR.

Personalized search aims to provide customized search results according to an individual user X  X  interests. Various personalized search methods have been proposed in recent years [4], [5], [10]. They are based on either explicit relevance feedback or implicit feedback through various user interaction behaviors, such as clicks, scrollings, adding pages to favorites, and so on. For example, Bennett et al. [4] utilized the position information of users to influence search results. Sontag et al. [5] proposed a generative model to predict the relevance of a doc-ument for a specific user. Collins-Thompson et al. [20] took the reading level of the users into considerations to improv e the effectiveness of retrieval. Xiang et al.[7] integrated various context information generated by user interaction into the learning to rank model to improve the IR performance. In [2], several per-sonalization strategies were proposed. It came to a conclusio n that personalized search can lead to a significant improvement on some queries but has little effect on other queries (e.g., queries with low click entropy).

In a seminal book about the geometry of IR [17], Hilbert X  X  vector spaces were used to represent documents. Similarly, Melucci [15] proposed an idea of using a basis to model the context in IR. In [16], a geometric framework is proposed to utilize multiple sources of evidence presented in current interaction context (e.g., display time, document retention) to develop enhanced implicit feedback models personalized for each user and tailored for each search task. The models we develop in this paper are inspired by the subspace projection method investigated in [1], which, in our opinion, is a general and principled theoretical framework for incorporating word dependencies and provides a unified representation for both user profiles and documents. 3.1 Probability of a Vector Out of Subspace Suppose B = { b 1 , ..., b k } is a basis of a k-dimension subspace defined over R n , where b T i  X  b i =1and b i  X  X  are mutually orthogonal. L ( B ) is the subspace spanned by B . x is a vector, and L ( { x } )isthesetofvectorsoftheform c x ,where c is a scalar. The vector x may or may not be generated by B .If x is generated by that every vector generated by B is entirely contained in L ( B ). The vectors that cannot be generated by B is not contained in L ( B ), but these vectors may be more or less close to L ( B ). Intuitively, if a vector x is close to L ( B ), the information object (e.g., a document) represented by x is likely within the context spanned by B . Similarly, the information object represented by a vector being far from L ( B ) is unlikely to be generated by the context spanned by B . Based on the notions just illustrated, we can model a user profile as a basis and documents as vectors, then comput e the inner product between a document vector and the projection of the vector onto the user profile subspace as the probability that the corresponding user is interested in the document, which formalized as Equation 1.
 wherewerestrict x T  X  x =1, P B is the projector to L ( B ), namely P B = B T  X  B , and P B  X  x is the projection of x onto the subspace B . Each basis vector in the subspace can be considered as a concept of a user profile. The projection of a document vector onto the subspace can t hen be interpreted as the concept of user profile which is most related to the document. This formula is different from the traditional VSM modeling of user profile as a single vector, which may contain more irrelevant noises for current search topic. 3.2 Probability of a Subspace Out of another Subspace In above theoretical framework, a docum ent is represented as a normalized vec-tor, which assumes that a document only contain one topic (corresponding to one basis vector). However the fact is that one document may have multiple topics, e.g., one document introduces bo th the beautiful scenery and the nota-bility of one place. To address this gap, we extend the projection-based method in Section 3.1 to the projection from one subspace to another subspace. In the extension, we represent a document as a subspace instead of a vector, denoted as L ( O ) spanned by a basis O = { x 1 , ..., x m } , where each dimension corresponds to a concept of the document. We then compute the probability of L ( O )outof L ( B ) according to Luders X  X  rule[1]: where P B ,P O are the projectors to L B ,L O respectively, tr (  X  )isthetraceofa matrix, and D is a density matrix (symmetric, positive definite and has trace one). In our work, the word correlation matrix built for user profile is regarded as the density matrix. We now present our concrete algorithms that implement the subspace projection based theoretical framework described in the previous section. In this paper, a  X  X uery instance X  refers to an information object that contains the user ID, query terms, query time stamp, original result list, clicked URL list, and so on. It is worth noting that different query instances may contain the same query terms. A user X  X  search process is captured by the user X  X   X  X uery trace X , which is a sequence of query instances sorted by query tim e stamp. A query trace is formalized as q and q n is the current query instance (for which the original search results are to be re-ranked). For each q i , we download the actual documents of the top 30 returned URLs from Bing search engine as the original list . When re-ranking the original list of q n , we compute the personalized score for each URL using our personalized algorithms and obtain the personalized ranked list according to the personalized scores. After that, a re-ranked list is gained by combining the original list with personalized ranked list using the Borda X  ranking fusion method [14]. 4.1 Algorithm 1: Document Vector Projection onto User Profile In this algorithm, we represent a user profile as a subspace that consists of the top K eigenvectors (corresponding to the top K eigenvalues) of a N  X  N word-correlation matrix, where N is the size of the vocabulary. Each eigenvector de-picts a distribution of words corresponding to a concept of user X  X  search interests. The top K eigenvectors constitute a basis of user profile subspace corresponding to the main aspects of user X  X  search history. A document is represented as a N dimensional column vectors and can be generated from the user profile subspace with a probability. For example, if a document can be totally generated from this subspace, the probability is 1; conversely, the probability is 0. We can rank documents based on such probabilities to get a personalized ranked list for the current query instance.
 Step1: Building the Word-Correlation Matrix for User Profile. In this work, we build a document collection f or each user, which are composed of all of the historical SAT clicked web pages. We preprocess each web page by segmenting it into a list of sentences. In this way, the document collection can be processed into a set of sentences, denoted as S = { s k } ,k =1 , ..., M .Theword-correlation matrix for the user is built based on the sentence set. We denote the user profile matrix as M P , each element of which is defined as: where M P ij is an element of M P corresponding to the i th row and the j th column. w i and w j are two words, and r ( w i ,w j ) reflects the correlation between them. Equation 3 aims to not only capture the dependency relationship between words, but also reflect the importance of each word. TFIDF ( w i ) is the product of term frequency (TF) of w i in the sentence set and its inverse document frequency (IDF) in a global document collection with 273298 web pages (not the user profile document collection). The mutual information (See Equation 4) between two variables is used to define the correlation between two words. Indeed, any other correlation measures can be applied here. A more systematic study of different correlation measurements will be carried out as future work. where X and Y are two random variables which indicate the existence of w i and w j respectively. P ( x )= P y } ,x,y  X  X  0 , 1 } . Here, P { X =1 } is the probability that w i occurs in the sentence set, and P { X =0 } is the probability that w i does not occur in the sentence set. P { Y =0 } and P { Y =1 } have similar definition corresponding to w the joint probability of X and Y . Note that, the Dirichlet smoothing method[19] has been used while estimating the word probability to avoid zero probability. I ( X ; Y ) is the mutual information of X and Y , which indicates the dependency relationship of the words. The diagonal elements of the matrix contains a factor, the self-information of X , which indicates the amount of information of X .
Step2: Computing the Personalized Score for each URL. The words-correlation matrix built in Step 1 is a N  X  N symmetric matrix. We can decom-pose it through the Singular Value Decomposition (SVD)[11] and get the top K of the user profile matrix corresponding to the i th eigenvalue of the all eigenvalues in a descending order. Then the projector for the user profile ( P P ) can be gained by the product of corresponding basis and its transposition, i.e., P P = B P  X  B T P . In order to obtain the personalized score for each URL, we represent each docu-ment (URL) as a N dimensional vector ( V d ) based on the vocabulary. We utilize the TF  X  IDF (denoted as TFIDF here), after normalization, as the weight of each element in the document vector.

The personalized score of each URL can be obtained naturally by projecting the document vector onto the user profile subspace (also see Equation 1): where PScore ( u ) is the personalized score for a URL. After this step, we can get the personalized rank list according to PScore ( u ).

Step 3: Re-Ranking the Query Instance. Since we cannot get the actual relevance score from the Bing search engine, we use the rank-based fusion method for re-ranking the original result list with the personalized ranked list. We denote the original result list of a query instance as  X  1 and the personalized ranked list gained in step 2 as  X  2 . Then we combine the rankings in  X  1 and  X  2 using the Borda X  ranking fusion method and sort the web pages with the combined rankings. Let u be one URL of the original result list of one query instance. Borda X  X  method first assigns a score B i ( u ) =  X  X he number of the URLs ranked below u in the rank  X  i  X , and then the total Borda X  score B ( u ) is defined as 2 i =1 B i ( u )[14]. Finally, we re-rank the result list according to the Borda score B ( u )togeta re-ranked list  X  . It should be noted that the different URLs may have the same Borda X  score in the actual experiment, which may lead to the uncertainty of ranking in the re-ranked list. To avoid this problem, we sort the URLs according to the relative order in  X  1 when the same Borda score occurs. 4.2 Algorithm 2: Document Subspace Projection onto User Profile This algorithm shares the same framework with the first algorithm described in Section 4.1, while the only difference is the method used for obtaining the personalized score for a URL. For this reason, we leave out the common parts of the two algorithm, and focus on how to get the personalized score. In this algorithm, the user profile subspace construction is the same as in Algorithm 1. Each document is also represented as a words-correlation matrix in the same way to build a user profile matrix. The document matrix is decomposed through SVD, so that the basis of the document subspace ( B d ) is generated (also the selection of top K eigenvectors as the basis). From the basis, we get the projector for the document subspace P d = B d  X  B T d . The personalized score is derived by projecting the document subspace onto the user profile subspace as introduced in Section 3: where M P is the word-correlation matrix for user profile as a density matrix, P
P is the projector corresponding to the user profile subspace. The advantages of this algorithm compared with the first algorithm are that (i) the correlation between words is taken into account in t he document representation; (ii) key concepts of a document with multiple topics are captured through SVD and considered in the document representation. 5.1 Baseline: Vector Space Model (VSM) In this paper we set the VSM as a baseline algorithm to compare with our algorithms described above. In SVM, bo th user profile and documents are rep-resented as vectors. The personalized score for a URL is the cosine similarity between the user profile vector V P and the document vector V d constructed with the same method as described in the first algorithm..
 5.2 Experiment Settings To test our personalized re-ranking algorithms, we conduct experiments on a real query log collection. In the experiments , we randomly sampled 107 users X  query logs as the training and testing data from a global query log with 1166 users over a certain period of time. Table 1 shows the detailed information of the global query log and sampled query log, which indicates that they have some similar statistical properties. In addition, we store a global document collection with 273,298 web pages downloaded from the Internet based on the URLs in the selected query log. We have preprocessed the web pages by ext racting the content data, segmenting them into sentences, removing stop wor ds and stemming the words with Porter Stemmer [18].
 We build a vocabulary for each user by selecting the top N words(we set N = 1000 in this paper) according to their TF-IDF weights in the SAT clicked document collection in the user X  X  search history. The vocabulary is updated dy-namically as user issuing new queries into t he search engine. In the representation of user profile, K , the number of selected eigenvectors in the basis of the user profile subspace, is an important paramet er which determines the number of the topics in the user profile used to personalize the web search results of the current query. We conduct systematic experiments to test the influence of different K on the algorithms X  performance.

The Click Entropy is a concept proposed in [2], which is a direct indication of query click variation. It is computed based on all of the clicks for a distinct query (i.e., which is unique in the query log).
 where U ( q ) is the collection of URLs that are clicked for the distinct query q ,and P ( u | q ) is the percentage of the clicks on the URL u among all the clicks for q . Dou et al.[2] pointed out that the smaller click entropy means that the majorities of users agree with each other on a small number of web pages for a query. It has been shown in the Literatures[2,3] that personalized search algorithms have different performance on query instances with different click entropies: generally speaking, the queries with low click e ntropy tend to have a less potential to benefit from personalization [2]. In this paper, we report the distribution of our experimental results over various d ifferent click entropy ranges. Note that, for statistical significance, we compute the click entropy for each distinct query based on a large scale global query log (see Table 1).
 In the real scenario of searching, users may skip the first SERP (Search Engine Results Page) and turn to following pages. Intuitively, this phenomenon indicates that a user may dissatisfy the search results returned by the search engine. From this view, the percentage of turning pages (= number of query instances that users turn to next pages / total number of queries) can reflect the users X  satisfaction with the search results to some extent. The larger the percentage is, the less the user X  X  satisfaction tend to be. Fig.1 (a) shows that users have relatively high satisfaction with the search results for queries with lower click entropy and there is less need of re-ranki ng results for these queries. With this consideration, we focus on re-ra nking the query instances with relatively high click entropy. This is a typical long tail task. Fig.1 (b) shows the distribution of the numbers queries over different c lick entropies for the test data. 5.3 Evaluation Metrics We utilize the evaluation metric introduced in Dou et al. [2] to evaluate the quality of a ranked URLs list for query instances. It is called Rank Scoring , denoted as R q for a query instance q . The average rank scoring for a set of query instances is denoted as R average .
 where j is the rank of a URL in the list;  X  ( q,j ) is 1 if URL j is relevant to user X  X  information need in query instance q and 0 otherwise; and  X  is set to 5, which follows the setting in[2]. The R Max q is the obtained maximum possible rank scoring for a query instance when all relevant URLs appearing at the top of the ranked list. A larger rank scoring value indicates a better quality of the ranked URLs list. Moreover, the  X  X AT click X  is used for relevance judgement of a URL. In our experiments, we evaluate the original result list given by Bing and the re-ranked list given by our algorithms in the same way. The performance of proposed algorithms can be measured by the improvement percentage of the re-ranked rank scoring compared with the Bing X  X  original rank scoring. The positive value (improvement percentage &gt; 0) means that the performance is increased after re-ranking, while zero value means performance staying unchanged and the negative value means the performance decreased.
 5.4 Experimental Results and Discussions Fig.2 shows the re-ranking performance of our algorithms in comparison with the baseline algorithm (VSM). (A) and (B) show the improvement percentage of our algorithms with different parameter K distributed on different click entropy intervals. The results show that the re-ranking performance of both V-S and S-S reach their peaks at a specific K value. A too small K value, e.g., K = 2, implies that too few topics are selected to pers onalizethewebsearchwhichmayleave out some important information for the current query. A too large K value, e.g., K = 20, may introduce too much noise. Only a proper K value can result in the most improvement of re-ranking performance. The results also show that the re-ranking performance of our algorithms (V-S and S-S) is relative poor for the queries with lower click entropy (less tha n 5.0), and is relatively good for queries with higher click entropy. One reason is that the Bing search engine has returned relative good results to users in the former case and thus there is little potential to improve it. There may even be a risk t o harm the users X  search experience when we personalize the queries with lower click entropy.

The average best performance of algorithms V-S and S-S appears in K =7 and K = 17 respectively. Table (C) compares the performance between VSM and our algorithms, namely S-S (K=17) and V-S (K=7). We observe that VSM demonstrates a better performance for lower click entropy queries ([3.0,3.5)); however, our two algorithms outperform VSM when the click entropy is large ( &gt; 4.0); especially, the S-S gain the best performance of a 35 . 20% improvement in click entropy interval [5.0,  X  ). Table (D) in Fig.2 shows the distribution of user numbers over different re-ranking perform ance ( X  X ncrease X ,  X  X tay X  and  X  X ecrease X ) for the 3 algorithms. This table indicates that the VSM helped slightly more users to improve the search results with the fewest harm (with the fewest number of users whose re-ranking performance is decreased) to user X  X  search quality. (E) gives another statistical analysis of the experimental result, i.e., the distribution of query numbers over different click entropy intervals and different re-ranking performance ( X  X ncrease X ,  X  X tay X  and  X  X ecrease X ) for different algorithms, showing that the VSM is more robust in lower click entropy intervals (less than 4.0) and the robustness in higher click entropy intervals for different algorithms is similar.
Overall, we find that our proposed algor ithms are effective, especially, for the queries with higher click entropy (which are queries worthwhile to person-alize [2]). The superiority of our methods are gained for four reasons: (i) we build word-correlation matrixes for user profile and documents, which not only captures the importance of single words, but also takes the correlation between words into consideration; (ii) we decompose the word-correlation matrix through SVD, and the dimensionality is reduced to a small value, so that the main as-pects of the user search history can be used to personalize the new query; (iii) a document is represented as subspace spanned by the top K eigenvectors of the document word-correlation matrix, which captures the main topics of the docu-ments and could serve as a denoising algorithm to some extent; (iv) the unified representation of the user profile and document as subspaces (or document as vector) well capture the geometrical fe atures of the user profile and documents, and based on this representation, we incorporate the well-principled subspace projection theory into our personalization framework. In this paper we propose two novel personalized re-ranking algorithms, based on subspace projection, to re-rank the original web search results which outperform the traditional VSM model especially for the queries with higher click entropy. It is noting that, in our work, we did not sel ect the most relevant historical queries for building the user profile, since we have utilized the subspace projection the-oretical framework that can automatically detect the most relevant concepts (topics) when computing the personalized scores. More specifically, the selected top K eigenvectors can be seen as some important and different topics of user X  X  search interests, and the projection from the document vector (subspace) to user profile subspace can map the most relevant topics to the retrieved documents. In the future, we will incorporate more information, such as similar queries and similar users, to further improve our model and algorithms.
 Acknowledgments. This work is funded in part by the Chinese National Pro-gram on Key Basic Research Project (973 Program, grant no. 2013CB329304 and 2014CB744604), the Natural Science Fo undation of China ( grant no. 61272265), and the European Union Framework 7 Marie-Curie International Research Staff Exchange Programme (grant no. 247590).

