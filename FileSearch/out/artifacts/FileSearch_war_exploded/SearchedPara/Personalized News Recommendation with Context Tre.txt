 The proliferation of online news creates a need for filter-ing interesting articles. Compared to other products, how-ever, recommending news has specific challenges: news pref-erences are subject to trends, users do not want to see mul-tiple articles with similar content, and frequently we have insufficient information to profile the reader.

In this paper, we introduce a class of news recommenda-tion systems based on context trees. They can provide high-quality news recommendations to anonymous visitors based on present browsing behaviour. Using an unbiased testing methodology, we show that they make accurate and novel recommendations, and that they are sufficiently flexible for the challenges of news recommendation.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering online recommender system, personalization, news
The first recommender systems were originally designed for news forums. Since then, they have been used with con-siderable success for products such as books and movies, but have found surprisingly little application in recommending news articles, due to the unique challenges of the area.
When users are identifiable as regular visitors to a news website, techniques from product recommendation can be adapted [8, 11]. However, most websites operated by indi-vidual newspapers do not have a strong base of electronic subscribers. Visitors to these websites are casual users, of-ten accessing them through a search engine, and little is known about them except what can be gathered through an ephemeral browsing history.
 Figure 1: A story with dynamic recommendations on the right side, and manually-generated recom-mendations on the bottom left (red-dashed areas).

The main page of a news website is already a set of recom-mended articles, which simultaneously addresses the needs of many users (Fig. 1). More specific recommendations are sometimes available to readers of individual articles. There are two shortcomings to this strategy: first, recommenda-tions are usually edited manually, and second, they only consider the last article read. Our goal is to construct rec-ommendations automatically and use the complete browsing history as a basis for giving personalized recommendations.
In principle, common recommender techniques such as col-laborative filtering could be applied to such a task, and have been adapted to temporal sequences [28, 24, 19]. However, they face several challenges specific to news. First, news are rapidly evolving: new stories and topics appear and disap-pear quickly, and old news are no longer interesting. Second, recommendations should provide added value, and not just consist of the most popular stories that the reader would have already seen on the front page.

To address these issues, we propose a class of online rec-ommendation algorithms based on Context Trees (CT) , which provide recommendations and are updated fully incremen-tally. A CT defines a partition tree organised in a hierarchy of increasingly precise partitions of a space of contexts. We consider as context the sequence of articles, the sequence of topics, or the distribution of topics. Each node in the tree is called context and corresponds to a set of sequences or topic distributions within a partition. The main idea is to give context-dependent recommendations, with contexts becoming progressively finer-grained deeper in the tree.
To make actual recommendations, we associate a set of prediction models, called experts , with each context. Their predictions are combined to make recommendations. We tailor our expert models to specifically take into account the idiosyncrasies of news. In particular, our expert models take into account the popularity and freshness of news items.
Using an unbiased testing methodology, emulating the process involved in implementing a system on a real web-site, we show that CT recommendations have state-of-the-art performance both with respect to prediction accuracy and to recommendation novelty, which is crucial for news articles since users want to read stories they do not know.
In general, there are two classes of recommender systems: collaborative filtering [25], which use similar users X  pref-erences to make recommendations, and content-based sys-tems [16], which use content similarity of news items.
The Grouplens project is the earliest example of collabo-rative filtering for news recommendation, applied to news-groups [21]. News aggregation systems such as Google News [8] also implement such algorithms. Google News uses the probabilistic latent semantic indexing and MinHash for clus-tering news items, and item covisitation for recommenda-tion. Their system builds a graph where the nodes are the stories and the edges represent the number of covisitations. Each of the approaches generates a score for a given news, aggregated into a single score using a linear combination.
Content-based recommendation is more common for news personalization [5, 1, 11]. NewsWeeder [12] is probably the first content-based approach for recommendations, but applied to newsgroups. NewsDude [5] and more recently YourNews [1] implemented a content-based system.

It is possible to combine the two types in a hybrid system [7, 14, 13]. For example, Liu et al. [14] extend the Google News study by looking at the user click behaviour in order to create accurate user profiles. They propose a Bayesian model to recommend news based on the user X  X  interests and the news trend of a group of users. They combine this ap-proach with the one by Das et al. [8] to generate personal-ized recommendations. Li et al. [13] introduce an algorithm based on a contextual bandit which learns to recommend by selecting news stories to serve users based on contextual information about the users and stories. At the same time, the algorithm adapts its selection strategy based on user-click feedback to maximize the total user clicks.

We focus on a class of recommender systems based on context trees. Usually, these trees are used to estimate Variable-order Markov Models (VMM). VMMs have been originally applied to lossless data compression, in which a long sequence of symbols is represented as a set of contexts and statistics about symbols are combined into a predictive model [22]. VMMs have many other applications [2].
Closely related, variable-order hidden Markov models [26], hidden Markov models [17] and Markov models [18, 23, 9] have been extensively studied for the related problem of click prediction. These models suffer from high state complexity. Although techniques [27] exist to decrease this complexity, multiple models have to be maintained, making these ap-proaches not scalable and not suitable for online learning.
Few works [28, 24, 19] apply such Markov models to rec-ommender systems. Zimdars et al. [28] describe a sequential model with a fixed history. Predictions are made by learn-ing a forest of decision trees, one for each item. When the number of items is big, this approach does not scale. Shani et al. [24] consider a finite mixture of Markov models with fixed weights. They need to maintain a reward function in order to solve a Markov decision process for generating rec-ommendations. As future work, they suggest the use of a context-specific mixture of weights to improve prediction ac-curacy. In this work, we follow such an approach. Rendle et al. [19] combine matrix factorization and a Markov chain model for baskets recommendation. The idea of factoring Markov chains is interesting and could be complementary to our approach. Their limitation is that they consider only first-order Markov chains. A bigger order is not tractable because the states are baskets which contain many items.
Because of the sequential nature of news reading, it is in-tuitive to model news browsing as a k -order Markov process [24]. The user X  X  state can be summarised by the last k items visited, and predictions can be based only on this informa-tion. Unfortunately, it is not clear how to select the order k . A variable-order Markov model (VMM) alleviates this problem by using a context-dependent order. In fact, VMM is a special type of context-tree model [2].

There are two key ideas behind a CT recommender sys-tem. First, it creates a hierarchy of contexts, arranged in a tree such that a child node completely contains the context of its parents. In this work, a context can be the set of se-quences of news items, sequence of topics, or a set of topic distributions. As new articles are added, more contexts are created. Contexts corresponding to old articles are removed as soon as they disappear from the current article pool.
The second key idea is to assign a local prediction model to each context, called an expert. For instance, a particular expert gives predictions only for users who have read a par-ticular sequence of stories, or users who have read an article that was sufficiently close to a particular topic distribution.
In the following, we first introduce the notion of context tree. Then, we describe various prediction models, how to associate them with the context tree and combine them in order to make recommendations.
When a user browses a news website, we track the se-quence of articles read.

Definition 1. A sequence s =  X  n 1 ,...,n l  X  is an ordered list of articles n i  X  N read by a user, and we denote s t sequence of articles read until time t . We write the set of all sequences by S .
 Note that a sequence can also be a sequence of topics of articles.

A context tree is built based on these sequences and their corresponding suffixes. Figure 2: VMM context tree for the sequence s =  X  n 1 ,n 2 ,n 3 ,n 2  X  . Nodes in red-dashed are active ex-perts  X   X  A ( s ) .

Definition 2. A k -length sequence  X  of is a suffix of a l -length sequence s , if k  X  l , and the last elements of s are equal to  X  , and we write  X   X  s when  X  is a suffix of s . For instance, one suffix  X  of the sequence s =  X  n 1 ,n 2 is given by  X  =  X  n 3 ,n 4  X  .

If two sequences have similar context, the next article a user wants to read should also be similar.

Definition 3. A context S = { s  X  X  :  X   X  s } , S  X  X  is the set of all possible sequences S ending with the suffix  X  . We can now give a formal definition of a context tree.
Definition 4. A context tree T = ( V , E ) with nodes V and edges E is a partition tree over the contexts S . It has the following properties: (a) The set of contexts at a given depth forms a partition : If V k are the nodes at depth k of the tree, then S i  X  S j =  X   X  i,j  X  X  k , while S i  X  X  sive refinement : If node i is the parent of j then S j  X  S
Thus, each node i  X  X  in the context tree corresponds to a context S i . Initially the context tree T only contains a root node with context S 0 = S . Every time a new article n is read, the active leaf node is split in a number of subsets, which then become nodes in the tree. This construction re-sults in a variable-order Markov model, illustrated in Fig. 2.
The main difference between news articles and products is that articles continuously appear and disappear, and the system thus maintains a current article pool that is always changing. The model for recommendation changes along with the article pool, using a dynamically evolving context tree. As new articles are added, new branches are created corresponding to sequences or topic distributions. At the same time, nodes corresponding to old articles are removed as soon as they disappear from the current pool.
Because of the large number of news items relative to top-ics, a context tree on topics might make better predictions. In particular, stories that have not been read by anyone can be recommended thanks to topic similarity. In this type of tree, each context represents a subset of the possible topic distributions of the last read article. The structure of the tree is slightly different and is modelled via a k -d tree.
A k -d tree is a binary tree that iteratively partitions a k -dimensional space S into smaller sets [4]. The i -th node cor-responds to a hyper-rectangle S i  X  S and has two children j,j 0 such that S j  X  S j 0 = S i and S j  X  S j 0 =  X  . In particular, the two children are always defined via a hyperplane splitting S in half, through the center of S i , and which is perpendic-ular to one principal axis. In practice, we simply associate each node to one of the k axes based on the depth such that we cycle through all possible axes: a = depth mod k . The set S is [0 , 1] k , the set of k -dimensional multinomial distributions on the possible topics.

In analogy to the sequence CT, a context is a hyper-rectangle S i and a suffix is the center  X  of S i in a topic distribution CT.

For instance, consider a node i with center  X   X  S i and associated axis a . Its two children correspond to two sets of topic distributions: Its left child j contains the distributions  X   X  S i with  X  0 a &lt;  X  a , while its right child j 0 is the set on the other side of the hyperplane: S j 0 = {  X  0  X  S i  X  } . When the system observes a new topic distribution  X  , the distribution is added to the tree, and possibly the tree expands.
We assign a local prediction model called expert to each context (node) in the tree. More formally,
Definition 5. An expert  X  i is a function associated with a specific context S i that computes an estimated probability of the next article n t +1 given that context, i.e. P i ( n
The user X  X  browsing history s t is matched to the context tree and identifies a path of nodes (see Fig. 2). All experts associated with these nodes are called active and are respon-sible for the recommendation.

Definition 6. The set of active experts A ( s t ) = {  X  i s } is the set of experts  X  i associated to contexts S i = { s :  X   X  s t } such that  X  i are suffix of s t .
The active experts A ( s t ) are combined by marginalizing to obtain a mixture of probabilities of all active experts: relevant for this context. These probabilities are derived as follows.

With each node i in the context tree we associate a weight w i  X  [0 , 1] that represents the usefulness of the correspond-ing expert. Given a path in the context tree, we consider experts in the order of the most specific to the most gen-eral context, i.e. along the path from the most specific node to the root. In this process, with probability equal to the weight w i we stop at a node without considering the more general experts. Thus, we take into account the relative usefulness of the experts.

Letting w j be the probability of stopping at j given that we have not stopped yet, we thus obtain the probability u that the i -th expert is considered as u i ( s t ) = w i Q w ) if s t  X  S i and 0 otherwise.

The calculation of the total probability can be made ef-ficiently via the recursion q k = w k P k ( n t +1 = x | s w ) q k  X  1 , where q k is the combined prediction of the first k experts. In Figure 2, the prediction of the root expert for the next item x is q 0 , while q 4 is the complete prediction by the model for this sequence.

The weights are updated by taking into account the suc-cess of a recommendation. When a user reads a new article x , we update the weights of the active experts corresponding to the suffixes ending before x according to the probability q ( x ) of predicting x sequentially via Bayes X  theorem [10]: No other weights are updated 1 . Finally, we also update the local models of the active experts (see Section 3.5).
Recommending news articles depends on multiple factors: the popularity of the news item, the freshness of the story, the sequence of news items or topics that the user has seen so far. Thus, each expert is decomposed into a set of local models , each modelling one of these properties. The first model ignores the temporal dynamics of the process. The second model assumes that users are mainly looking at pop-ular items, and the last model that they are interested in fresh items (i.e. breaking news).
A na  X   X ve approach for estimating the multinomial proba-bility distribution over the news items is to use a Dirichlet-multinomial prior for each expert  X  i . The probability of reading a particular news item x depends only on the num-ber of times  X  x it has been read when the expert is active. where  X  0 is the initial count of the Dirichlet prior.
The dynamic of news items is more complex. A news item provides new content and therefore has been seen by few users. News is subject to trends and frequent variations of preferences. We improve this simple model by augmenting it with models for popular or fresh news items.
A news item x  X  X  is in the set of popular items P when it has been read at least once among the last |P| read news items. We compute the probability of a news item x given that x is popular as: where c x is the total number of clicks received for news item x . Note that c x is not equal to  X  x (Eq. 3).  X  x number of clicks for news item x when the expert is active , while c x is the number of clicks received by news item x in total whether the expert is active or not.

The number of popular items |P| is important because it is unique for each news website. When |P| is small, the expert considers only the most recent read news. It is important to tune this parameter appropriately. w 0 = 1 since we must always stop at the root node.
A news item x  X  X  is in the set of fresh items F when it has not been read by anyone but is among the next |F| news items to be published on the website, i.e. a breaking news. We compute the probability of news item x given that x is fresh as:
P
The number of fresh items |F| influences the prediction made by this expert, and is unique for each news website. We combine the three expert models using this mixture:
There are two ways to compute the probabilities p  X  i : either by using a Dirichlet prior that ignores the expert prediction or by a Bayesian update to calculate the posterior probabil-ity of each expert according to their accuracy.

For the first approach, the probability of the next news item being popular is: where P j  X  j represents the number of times the expert  X  has been active,  X  pop and  X  notpop the number of read news items which were respectively popular and not popular when the expert  X  i was active.

Similarly, the probability of the next news item being fresh is given by: where  X  fresh is the number of read news items which were fresh when the expert  X  i was active.

Noting that P  X  X  =  X  , the probability of the next news item being neither popular nor fresh is: p i = P i ( n t +1 /  X  X  X  X  ) = 1  X  P i ( n t +1  X  X  )  X  P i ( n
It might happen that by using the Dirichlet priors, pre-dictions are mainly made by only one expert model. To overcome this issue, we compute the probabilities p { std,pop,fresh } via a Bayesian update, which adapts them based on the performance of each expert model:
We describe here the general algorithm to generate recom-mendations for the class of context-tree recommender sys-tems. This algorithm can be applied to domains other than news in which timeliness and concept drift are of concern. We then focus on the news domain and describe in more de-tails three VMM-based recommender systems and one based on the k -d context tree.
Algorithm 1 presents a sketch of the CT recommender algorithm. For simplicity, we split our system in two pro-cedures: learn and recommend . Both are executed for each read article x of a user with browsing history s in an on-line algorithm such as [20, 15], without any further offline computation. The candidate pool C is always changing and contains the popular P and fresh F stories. The system es-timates the probability of each candidate and recommends the news items with the highest probability. In order to es-timate the probability of a candidate item, the system 1) selects the active experts A ( s ) which correspond to a path in the context tree from the most general to the most spe-cific context, 2) propagates q from the root down to the leaf, i.e the most specific context. q at the leaf expert is the estimated probability of the recommender system for the candidate item x , i.e. P ( n t +1 = x | s t ) (see Eq. 1). Algorithm 1 CT recommender system 1: procedure Learn ( x, s , context set  X ) 2: q  X  0 and t  X  X A ( s ) | // loop from most general expert  X  0 to most specific expert  X  3: for i  X  0 ,t do 4: p i  X  P i ( n t +1 = x | s ) // i th expert prediction 5: q  X  w i p i + (1  X  w i ) q // combined prediction 7: update p std i ,p pop i ,p fresh i (Eq. 7-9 or Eq. 10) 8: if x  X  s /  X   X  then // is the context in the tree? 9:  X  =  X  S { x  X  s } // add a new leaf. 10: end procedure 11: procedure Recommend ( s ) 12: for all candidate n  X  X  do 13: q ( n )  X  0 and t  X  X A ( s ) | // loop from most general expert  X  0 to most specific expert  X  14: for i  X  0 ,t do 16: R X  sort all n  X  X  by q ( n ) in descending order 17: return first k elements of R 18: end procedure
We consider three VMM variants of recommender sys-tems, and one based on the k -d context tree.
The standard VMM recommender builds a context tree on the sequences of news items. That is s t =  X  n 1 ,...,n is a sequence of news items, and each active expert predicts n t +1 , the next news item.
In order to build a CT on topic sequences, we find a set of topics for each story, and assign the most probable topic to the news item. We then perform predictions on topics .
More precisely, we use the Latent Dirichlet Allocation (LDA) [6] as a probabilistic topic model. After concate-nating the title, summary and content of the news item to-gether, we tokenize the words and remove stopwords. We then apply LDA to all the news stories in the dataset, and obtain a topic distribution vector  X  ( n ) for each news item n .
We now define a context tree as follows. Let z t most probable topic of the t -th news item. Then the topic sequence is s t =  X  z 1 ,...,z t  X  and  X  is a suffix of topic se-quences. The context tree generates a topic probability dis-tribution P ( z t +1 = j | s t ), while the LDA model provides us with a topic distribution P ( z = j | n ) for each news item n . These are then combined into the following score: score ( n | s t ) = max The system recommends the articles with the highest scores.
We combine the standard VMM with the content-based system into a hybrid version. The context tree is built on topics , similarly to the CVMM system, but the experts make predictions about news items , like the VMM system.
HVMM system builds a tree in the space of topic se-quences . Each suffix  X  of size k is a sequence of most probable (Eq. 1 and later) are defined on the space of news items.
The CVMM and HVMM structures make predictions on the basis of the sequence of most probable topics. Instead, we consider a model that takes advantage of the complete topic distribution of the last news item. We use a k -d tree to build a context model in the space of topic distributions . In addition, we have the following baselines: Z-k is a fixed k -order Markov chain recommender similar to MinHash is the minhash system used in Google News [8]. MostPopular recommends a set of stories with the highest
We investigate whether the class of CT recommender sys-tems has an advantage over standard methods and if so, what is the best combination of partition and expert model.
We measure performance both with respect to accuracy and novelty of recommendations. Novelty is essential be-cause it exposes the reader to relevant news items that she would not have seen by herself. Obvious but accurate rec-ommendations of most-popular items are of little use.
We evaluate our systems on two datasets described be-low. We examine on the first dataset the sensitivity of the CT models to hyperparameters. The second dataset is used to perform an unbiased comparison between the different models. We select the optimal hyperparameters on the first dataset, and then measure the performance on the second dataset. This methodology [3] mirrors the approach that would be followed by a practitioner who wants to implement a recommender system on a newspaper website. We collected data from the websites of two daily Swiss-French newspapers called Tribune de Gen`eve (TDG) and 24 Heures (24H) 2 . Their websites contain news stories ranging www.tdg.ch and www.24heures.ch from local news, national and international events, sports to culture and entertainment.

The datasets span from Nov. 2008 until May 2009. They contain all the news stories displayed, and all the visits by anonymous users within the time period. Note that a new visit is created every time a user browses the website, even if she browsed the website before. The raw data has a lot of noise due to, for instance, crawling bots from search engines or browsing on mobile devices with unreliable internet con-nections. Table 1 shows the dataset statistics after filtering out this noise, and Figure 3 illustrates the distribution of visit length for each dataset.
We evaluate how good the systems are in predicting the future news a user is going to read. Specifically, we consider sequences of news items s =  X  n 1 ,n 2 ,...,n l  X  , n i  X  N , s  X  S read by anonymous users. The sequences and the news items in each sequence are sorted by increasing order of visit time. When an anonymous user starts to read a news item n the system generates 5 recommendations. As soon as the user reads another news item n 2 , the system updates its model with the past observations n 1 and n 2 , and generates a new set of recommendations. Hence the training set and the testing set are split based on the current time: at time t , the training set contains all news items accessed before t , and the testing set has items accessed after t .

We consider three metrics. The first is the Success@5 (s@5). For a given sequence s =  X  n 1 ,n 2 ,...,n t ,...,n rent news item n t in this sequence, and a set of recommended news items R , s@5 is equal to 1 if n t +1  X  X  , 0 otherwise.
The second metric is personalized s@5 , where we re-move the popular items R T from the set R , to get a reduced set R P = R\R T . This metric is important because it filters out the bias due the fact that data is collected from websites which recommend the most popular items by default. Figure 4: VMM recommender system: different mixtures of experts (Bayesian update, |F| = 10 ).

The final metric is novelty , defined by the ratio of un-seen and recommended items over the recommended items: novelty = |R X  X | / |R| . This metric is essential because users want to read about something they do not already know.
For all systems, we use a prior  X  0 = 1 / |N| for the Dirichlet models, and the initial weights for the experts as w k = 2 where k is the depth of the node. We evaluated experimen-tally the optimal number of topics in the range of 30 to 500, and found that 50 topics bring the best accuracy. We varied the size of candidate pool: number of popular items |P| from 10 to 500, and fresh items |F| from 10 to 100. When the can-didate set is small, the experts consider only the most recent read stories. We report averages over all recommendations with confidence intervals at 95%. We omit figures for the TDG dataset because we witnessed the same behaviours.
Although na  X   X ve, the approach of recommending the most popular stories is actually used very often on newspaper websites. This strategy does not pay off when the size of candidate pool increases.  X  X ood X  recommendations are drowned in popular items. This can also be seen by the fact that mixture of expert models integrating the popular-ity model are very sensitive to the number of popular items while others are more robust (e.g. Fig. 4 for VMM recsys).
We noticed that, when using the Dirichlet priors to update the mixture probabilities, the prediction was mostly made by the popularity model, resulting in the same behaviour as the most-popular recommender system as the size of candidate pool increases. However, as the Bayesian update (Eq. 10) adapts the probabilities based on the performance of each expert model, it is more robust when we increase the can-didate set. We also observed that as the number of fresh items increased, CT models were getting slightly better.
When we look at the general accuracy of CT recommender systems (Fig. 5(a)), their performance is close to the exist-ing techniques. However when we consider only personalized items (Fig. 5(b)), CT recommender systems outperform cur-rent techniques, showing that the order of the model is im-portant. Indeed, we observed that the weights of the experts are well distributed over the space even for long sequences. If the sequence is not important, the weights of the experts on performance on the tuning set, one should choose k CT for smaller values of  X  and VMM for larger values.
News recommendation is challenging due to the rapid evo-lution of topics and preferences. We introduced a class of recommender systems based on context trees that accommo-date a dynamically changing model. We considered context trees in the space of sequences of news, sequences of topics, and in the space of topic distributions. We defined expert models which consider the popularity and freshness of news items, and examined ways to combine them into a single model. We proposed an incremental algorithm that adapts the models continuously, and is thus better suited to such a dynamic domain as the context tree evolves over time and adapts itself to current trends and reader preferences. Our approach requires only one tree (the context tree), and thus scales very well. Our work is not restricted to the history of logged-in users, but considers a one-time session for recom-mendation, where users do not log in. Surprisingly, we do not know of any existing research that considers context-tree models for recommender systems.
 Each proposed variant has its strengths and weaknesses. To evaluate them, we used the expected performance curve methodology, whereby each method is tuned in a training set according to a parametrized utility metric. In doing so, we showed that if we are interested in accuracy in a static dataset, a context tree that implements a variable-order Markov model is ideal, while novelty is best served with a k -d tree on the space of topics. In addition, we showed that a large order is mainly important when we are not interested in recommending highly popular items. An open question is whether the results we obtained on a static trace will be qualitatively similar in an actual recommender system. In future work, we aim to perform an online test of the system on a real news website. [1] J. Ahn, P. Brusilovsky, J. Grady, and D. He. Open [2] R. Begleiter, R. El-Yaniv, and G. Yona. On prediction [3] S. Bengio, J. Mariethoz, and K. M. The expected [4] J. Bentley. Multidimensional binary search trees used [5] D. Billsus and M. Pazzani. A hybrid user model for [6] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [7] R. Burke. Hybrid recommender systems: Survey and [8] A. Das, M. Datar, A. Garg, and S. Rajaram. Google [9] M. Deshpande and G. Karypis. Selective markov [10] C. Dimitrakakis. Bayesian Variable Order Markov [11] W. IJntema, F. Goossen, F. Frasincar, and [12] K. Lang. Newsweeder: Learning to filter netnews. In [13] L. Li, W. Chu, J. Langford, and R. Schapire. A [14] J. Liu, P. Dolan, and E. Pedersen. Personalized news [15] N. Liu, M. Zhao, E. Xiang, and Q. Yang. Online [16] P. Lops, M. Gemmis, and G. Semeraro. Content-based [17] A. Montgomery, S. Li, K. Srinivasan, and J. Liechty. [18] J. Pitkow and P. Pirolli. Mining longest repeating [19] S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme. [20] S. Rendle and L. Schmidt-Thieme. Online-updating [21] P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and [22] J. Rissanen. A universal data compression system. [23] R. Sarukkai. Link prediction and path analysis using [24] G. Shani, D. Heckerman, and R. Brafman. An [25] X. Su and T. Khoshgoftaar. A survey of collaborative [26] Y. Wang, L. Zhou, J. Feng, J. Wang, and Z. Liu. [27] M. Zaki, C. Carothers, and B. Szymanski. Vogue: A [28] A. Zimdars, D. Chickering, and C. Meek. Using
