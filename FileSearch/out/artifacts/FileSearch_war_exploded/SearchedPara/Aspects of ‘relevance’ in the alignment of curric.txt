 1. Introduction
When K-12 teachers search digital library systems for lesson materials, they must find materials that support the educa-tional standards to which they must teach. These standards are plentiful X  X bout 60,000 for mathematics and science in the US alone X  X nd are revised by the governing states on a regular basis. Along with a rapidly growing supply of electronically available K-12 lesson materials, we therefore see a growing need for automated alignment services; i.e., services through which teachers can specify one or more standards and which will retrieve the aligned curriculum.

Assessment and validation of the performance of such retrieval mechanisms would benefit from the availability of a so-called gold standard; a ground-truthed, generally-accepted set of curriculum-standard alignments. However, not only are we unaware of the existence of such a set, the few attempts that have been made to create such a set have been thwarted by a lack of definition of the alignment concept as apparent from low alignment inter-rater reliability (IRR).

Inspired by a recent and comprehensive review of the concept of  X  X elevance X  in information science by Tefko Saracevic (2007, 2007a) , we set out applying his framework for operationalization by means of  X  X lues X  to the concept of standard align-ment. We had three goals in mind:
Given the latency and context-dependency of the alignment concept, we hypothesized that clue-based operationalization of alignment should result in high IRR.

We were interested in finding out how the various clues Saracevic recognizes in his work would manifest themselves in standard-based curriculum searching.

We were interested in finding out what is actually being measured when assessing alignment without using the clues approach.

In this paper we present the results of an experiment in which we attempt to define and validate the dimensions and meaning of standard-curriculum alignment using Saracevic X  X   X  X lues X  approach. We find that by operationalizing the align-ment concept in the situational context of K-12 teaching tasks, IRR is quite high. Also, in accordance with the Saracevic mod-el, the alignment model changes as a function of changing situational variables and these changes again may have consequences for how alignments are constructed. We also find that a statistically stable and significant dimensional model of overall or latent alignment can be formulated. This model informs what is being measured when using an overall rather than a clues approach to alignment.

We start out by briefly reviewing both the considerable promise and the limitations of (current) automated standard-cur-riculum alignment systems and their associated Web services. We then propose that the effectiveness of these tools and the likelihood of developing a widely used gold standard set of alignments can be increased using an operationalization of the alignment concept based on the Saracevic model and we present the results of an experiment involving such an operationalization. 2. Background: automated standards-curriculum alignment
Recent developments in digital library technology, combined with regulatory emphasis on standards-based teaching have resulted in a proliferation of K-12 curricular materials offered on the Web. These developments have been further strength-ened by initiatives such as the US National Science Foundation X  X  National Science Digital Library or NSDL ( Zia, 2002; Lagoze et al., 2005, 2005a ) and GK-12 programs ( NSF, 2008 ). Whereas only a few years ago schools and teachers had to depend al-most exclusively on traditionally published textbooks, nowadays rich repositories of Web-based curriculum are readily available. Examples of such collections include Teacher X  X  Domain (Blumenthal, 2003; Johnson, 2003 ), TeachEngineering ( Sul-livan et al., 2005 ), NetTrekker ( Breen, 2008; Felix, 2004 ), Engineering is Elementary (Cunningham and Hester, 2007; Lachap-elle and Cunningham, 2007 ), Curriki ( McAnear, 2007; Wallis &amp; Steptoe, 2006 ), the Middle School Portal (Saylor &amp; Minton-Morris, 2007 ) and the NSDL itself (Table 1 ).

This proliferation of resources presents a deepening challenge to teachers trying to navigate these collections in their search for curriculum that fits their field, their grade level, their pedagogical preferences and, increasingly, the standards to which they must teach. The latter in particular poses a serious challenge since increasingly US (public) school funding as well as teachers X  performance evaluations depend on the schools proving that their students master the learning out-comes specified in their state X  X  or district X  X  educational standards. Although there is no lack of analysis of and commentary on the role standards and standards-based teaching and testing play in today X  X  teaching practice, in this paper we do not question the validity or effects of standards-based teaching. Instead, recognizing that teachers must teach to standards, we concentrate on what it means to search for standard-appropriate curriculum in these quickly growing, digital collections of curricular materials.

The challenge for the designers of information retrieval (IR) mechanisms that query these collections is as simple to state as it is difficult to solve: how to align curriculum in these collections with the tens of thousands of standards in force across the United States so that when users single out a standard, the system can offer them the aligned curriculum. From an infor-mation retrieval perspective, such alignment is essentially a question of  X  X elevance X  (e.g., Borlund, 2003; Cosijn &amp; Bothma, 2005; Saracevic, 1975, 2007, 2007a; Toms, O X  X rien, Kopak, &amp; Freund, 2005 ); i.e., a problem of determining under which con-ditions standards and curriculum can be considered  X  X ligned X  and translating those conditions into an algorithm for associ-ating standards with curriculum. While this issue is of particular interest in education, similar problems exist in other collections such as law enforcement records, legal cases, and engineering reports where free-format text documents would be more useful were they matched to regulations, laws, work flows, production requirements, and other semi-structured resources.

Much work has been conducted on the formulation of theories, notions and models of  X  X elevance X  in IR X  X e refer to Sarac-evic (2007, 2007a) for a comprehensive review X  X nd some innovative work has been done on computationally aligning stan-dards and curriculum (e.g., Devaul, Diekema, &amp; Ostwald, 2007; Diekema &amp; Chen, 2005; Yilmazel et al., 2007 ). Unfortunately, little if any analysis of relevance in the context of standard-curriculum alignment has yet been conducted. This is problem-atic as, with the recent availability of electronic (XML) versions of all the US educational standards and the aforementioned rapid growth in electronically available curriculum, we essentially have both sides of alignment X  X tandards and curriculum X  readily available; yet we lack proper understanding of how they are connected through alignment. 2.1. Progress and problems with automated alignment Both the available curriculum and the size and complexity of the standards landscape are increasing. The Achievement Standard Network (ASN) lists about 60,000 state-specific educational standards for US science and mathematics alone ( Gate-way, 2007 ). Moreover, states revise their standards, on average, every 5 years (Jay and Longdon, 2003; Sumner et al., 2005).
The combinatorial nature of this rapidly growing collection of curricular materials with a large and ever-changing collec-tion of standards makes it impractical and costly to conduct and maintain these alignments manually. User-based align-ments and alignment annotations, such as the ones collected on Web sites such as www.hotels.com, www.epinions.com , or www.flickr.com , are a powerful source for developing alignment sets, but collecting a set with complete coverage this way would take significant time and would require a large audience of active, contributing users.

Collections can, alternatively, submit curriculum to automated services for alignment. Two such services are the Curric-ulum Alignment Tool (CAT) and the Standard Alignment Tool (SAT) (Devaul et al., 2007; Diekema &amp; Chen, 2005; Yilmazel et al., 2007 ). These services were developed to support manual alignment by providing machine-generated alignment sug-gestions. Using neural network technology, CAT automatically aligns curricular materials X  X  lesson, exercise, etc. X  X ith the standards of a state, whereas SAT aligns standards with each other. The purpose of the latter is to  X  X rosswalk; X  i.e., the tran-sitive inference that if standard A is similar to standard B and if we know that standard B aligns with curricular item C, then applied in various curriculum collections; e.g., Teachers X  X omain and TeachEngineering, and by standard alignment services such as Academic Benchmarks. An example from the TeachEngineering collection (www.teachengineering.org ) is displayed in Fig. 1 . Notice that for each curricular item found, check marks indicate its alignment source as CAT, SAT or  X  X xplicit, X  the latter indicating that the alignment was explicitly done by the curriculum author(s).

Although the obvious advantage of automated alignment is increased productivity X  X any thousands of alignments can be made in only a short amount of time X  X he validity of the alignments is not without problems. For instance, in a recent exper-iment Devaul et al. (2007) found low IRR between expert judgments of (semi) automated curriculum-standard alignments. On average only 32% of the alignments were shared by the two experts. Agreement was higher (40%) for standards address-ing more abstract subjects and lower (18%) for more precise, application-oriented standards. The authors thus conclude that being assigned are numerous and wide ranging in scope, ...strong IRR can be difficult to achieve.  X  Similar poor IRR was found by Voorhees (1998) and Bar-Ilan, Keenoy, Yaari, and Levene (2007) when they assessed the inter-subject judgments of noned-ucational search engine query results. Similarly, Kim (2006) found low IRR between term-document specificity ( X  X  X -spec X ) ratings, notwithstanding the use of only a three-point relevance scale X  Kek X l X inen and J X rvelin (2002) and Toms, O X  X rien, Kopak, and Freund (2005) recommend a multipoint scale. Work by Kulm and Grier (1998) and Roseman, Stern, and Koppal (in press) does show high alignment IRR but in both these studies the catalogers were highly trained in the application of a specific alignment mechanism.

Poor IRR between experts is problematic because it hinders the development and evaluation of effective retrieval systems ( X bersax, 2008 ) as it indicates a possible construct validity problem, i.e., improper operationalization of the theoretical con-cept meant to be measured, measurement error, or both. For instance, if different subjects attribute different meanings to the concept of alignment; i.e., alignment is a latent variable the meaning of which is multifaceted and context dependent, their judgment of the validity of machine-generated alignments might be quite different, resulting in low IRR. For the above cases, we hypothesize that at least a significant part of the cause is improper and/or unstable translation of the theoretical concept of relevance or alignment into measurable, observable quantities during the judgment process. In this paper we attempt to arrive at a more precise, less ambiguous operationalization of the concept. We show that if we anchor these operationaliza-tions in the daily practice of K-12 teaching, high IRR can be accomplished. In addition, improved measurement of document/ standard alignment should help in developing a better understanding of the alignment tasks and hence, contribute to the development of better automated retrieval tools. 3. Dimensions of alignment
The question seems simple enough:  X  X  X oes this curricular item align with this standard? X  Unfortunately, when asked this question, research subjects will quite happily answer it regardless of its ambiguity. Or as Pawson (1982) writes in his famous paper on methodological artifacts in social science measurement:  X  X  we can do so at will, especially if we are encouraged ... The technique will always produce results and so the presuppositions, if they ever come under question, are apparently exonerated . X 
The available automated alignment services seem to only worsen this problem because their definitiveness X  X hings align, do not align or align to a certain degree X  X dds to the impression that alignment is a clearly defined, objectively measurable con-cept. We offer that the poor IRR reported in the above-mentioned studies all suffered from this same problem of users being asked to make relevance judgments with excessive latitude in deciding what that actually meant. As long as this meaning is clear from the context of the question this might be acceptable, but as we shall argue shortly, whether or not a curricular item in a collection is relevant for a K-12 educational standard can mean a variety of different things to different teachers or to the same teacher in different circumstances. Relevance X  X r in our case alignment X  X ust be more clearly defined before we can hope to raise the IRR of alignment judgments.

In two recent papers Saracevic (2007, 2007a) reviews the literature on the conceptual and methodological aspects of rel-evance in IR. He reviews the duality of the concept discussing the works of those adhering to the content-centric view and those who put the user at the center, focusing in on the  X  X lues X  approach pioneered by Schamber, Eisenberg, and Nilan (1994) . Following his earlier work and that of Cosijn and Ingwersen (2000), Borlund (2003), Cosijn and Bothma (2005), Toms,
O X  X rien, Kopak, and Freund (2005) and a variety of other studies aimed at operationalizing the various aspects or dimensions of relevance, Saracevic distills seven  X  X lues X  or manifestations of relevance: content (e.g., topic, scope and concepts), object (unproblematic, inter-subjective characteristics of information objects; e.g., cost, required instruction time or grade level), validity (accuracy of the information provided), situational match (appropriateness of the materials for a given task), cognitive match (does the user understand the materials), affective match (emotional response to the material), and belief match (users X  credence given to the materials). Here we concentrate on the content, object, situational and affective clues as they can be readily operationalized into the daily practice of K-12 science and math teaching:
Content matches in K-12 teaching cover the more traditional alignment dimension of conceptual and topical content. Does the lesson contain and explain the key concepts and terms? Do the exercises and examples of the lesson convey the mean-ing of these concepts? Can examples from this lesson be used and perhaps combined with exercises and assessments from another one? Are the exercises and assignments valid proxies for applying these concepts?
Saracevic X  X  objective match criteria X  X ormat, availability, cost, etc. X  X o not seem to have any K-12-specific manifestations except perhaps for grade level. Almost all K-12 digital library materials and standards are indexed by grade band.
Situational match refers to the context-specific appropriateness of an information item. For K-12 teaching we can think of criteria such as the targeted grade band, whether or not the item X  X  assessment procedures are compatible with those pre-ferred or mandated by the school district, whether the lesson has troubleshooting tips or safety issues, or whether or not any of the required equipment can be reused (cost).

Affective match covers aspects of a curricular item X  X  appeal to a teacher. Does the item fit his or her style of teaching? Does it contain examples or exercises that might captivate students? Are the graphics and worksheets attractively and age-appropriately formatted? Does the subject matter appeal to the teacher?
From these considerations we conclude that meaningful empirical measurement of the abstract concept of alignment re-quires an operationalization in terms of context-specific; i.e., (teaching) task-specific aspects and indicators. We consider this position very much in line with two accomplished traditions in social science methodology. The first, context-specific measurement or latent variable analysis (e.g., Borsboom, 2008; Borsboom et al., 2003; Lazarsfeld, 1954 ) advocates separating the so-called structural model containing the theoretical variables from the measurement model in which the theoretical variables are operationalized differently depending on the context in which the measurement takes place. The second, rela-tional modeling (Hendriks &amp; van der Smagt, 1988; Lucardie, Hendriks, &amp; van Ham, 2008; Reitsma, 1990 ), argues for a func-tional equivalence approach. Applied to standard alignment, this approach would argue that alignment means that the curriculum can be used in the classroom by a given teacher and a specific class of students and hence, that it satisfies the real-world constraints of classroom teaching. This latter approach has significant overlap with Schamber et al. X  X  and Sarac-evic X  X   X  X ituational match. X  3.1. Towards an alignment gold standard?
As in many other domains of search and retrieval analysis, it would be attractive to have available a so-called gold stan-dard of standard-curriculum alignments; i.e., a set of collectively agreed upon or  X  X round-truthed X  standard-curriculum alignments (e.g., Baeza-Yates and Ribiero-Neto, 1999; Hripcsak &amp; Rothschild, 2005; Van Rijsbergen, 1979 ). Gold standards are quite common in domains where (a panel of) experts can determine the truth, falsehood or likelihood of an association between a query and its search result. Some recent examples of work employing such gold standards can be found in Toms, O X  X rien, Kopak, and Freund (2005), Voorhees and Harman (2005) and Smith, Cheung, Krauthammer, Schultz, and Gerstein (2007) .

We are not aware of any such gold standard for K-12 math and science standard-curriculum alignment. Of course, ser-vices such as NetTrekker, CNLP X  X  CAT, TeachEngineering, NSDL and Academic Benchmarks all provide alignments but we are unaware of the ground truthing of these alignments. Unfortunately, the problem of low IRR such as reported, for instance, by Devaul et al. (2007) does not bode well for the establishment of such a standard. Also, our interpretation of the Saracevic model of relevance for curriculum alignment is that alignment is defined in the everyday reality of teaching through a com-plex interaction between available curriculum, required standards, the perceptions and capabilities of teachers and other factors. A relativist interpretation of this would be to say that because of the sheer combinatorial complexity of these factors no general model for alignment can exist, and therefore the chances for developing a gold standard for K-12 math and sci-ence standard-curriculum alignment are exceedingly small. We should, however, be careful not to confuse a relational view of alignment with a relativist one. Teachers do not randomly pair teaching materials with standards, just as they do not ran-domly decide that one set of materials is superior to another. Likewise, teachers are not all different from each other as their training, education and, nowadays, their standard-driven practice of learning assessment provides for certain commonalities and partially-shared worldviews. For IRR and gold standards this implies essentially the same thing: that teachers share a common core of values and experiences which informs their view and conceptualization of alignment and that these com-mon views should manifest themselves in a carefully conducted IRR experiment. At the same time, however, sufficient dif-ferences exist both between the teachers themselves and the everyday realities in which they work, that a significant amount of variation around that core of views and values must be expected.

In summary, we offer first that the demonstrated lack of consistency in alignment judgments limits the assessment of effec-tiveness of automatic retrieval systems and hinders the development of useful gold standard collections to support continued improvement. Second, that a more nuanced model for educational standard alignment could help alleviate these problems. With the research reported here we aim to contribute to an improved methodology for collecting gold standard sets of align-ment assessments for educational digital libraries and to gather a sizeable set of such alignments to support further develop-ment of retrieval mechanisms. The experiment described in the balance of this article explores how assessments of different dimensions of alignment can be collected, particularly in an educational digital library context, with improved inter-rater reli-ability. The relevance clues suggested by Saracevic form the basis of our development of multiple scales for the survey instru-ment, and expert raters are provided with contextual information in the form of a hypothetical teaching assignment. 4. Hypotheses
To empirically assess the validity of the above theoretical considerations, we formulate the following hypotheses relating to K-12 math and science standard-curriculum alignment.

Hypothesis H-1 : Expressing curriculum-standard alignment following Saracevic X  X  typology of  X  X lues X  will result in dimen-sion-specific IRR that is significantly higher than IRR achieved when addressing overall relevance.

Previously, we offered that alignment is a complex concept the dimensions of which can be different and can interact with each other depending on the context in which they are applied and measured. From this we hypothesize that Hypothesis H-2 : Where a curricular item and standard may well align on one dimension, they may not on another.
Although empirical support for the previous hypotheses would strengthen the position that dimension-specific assess-ment of curriculum-standard alignment provides a more promising approach; i.e., higher IRR, than assessment based on overall alignment, it would still be useful to gain insight into the overall alignment concept as well. After all, whereas we would advocate dimension-specific alignments, overall alignments seem to be the norm both in academic and practical applications. Despite our essentially relational notion that alignment is created from the available information on the one hand and the role that information can play in a specific use context on the other, we expect that Hypothesis H-3 : Overall alignment can be at least partially modeled as a function of part-worth alignment dimensions.
If a retrieval system is to return relevant documents in response to a query, it must infer that relevance from available clues. In keeping with the organizational paradigm for education, nearly all teaching resources include a target grade level or band of grade levels; e.g., grades 4 X 6. This kind of labeling might be employed as an  X  X bject X  clue in computing algorithmic relevance. However, in our conversations with educators we noted that material prepared for one age group can often be adapted for another. Therefore, we hypothesize that
Hypothesis H-4 : Grade levels assigned to learning objects are less relevant to overall alignment than is an educator X  X  ability to scale or transpose material to the grade level of the standard.

While many other metadata constructs could be similarly challenged, we thought it useful to explore this one widely-em-ployed indicator. 5. Experiment
To test the above hypotheses, an experiment was conducted. Forty three science and mathematics K-12 teachers, stu-dents studying to become K-12 mathematics and science teachers and engineering students participating in K-12 teaching rotations participated in both the alignment of electronic lesson plans with specific teaching tasks and in the assessment of the alignments between lesson plan/teaching task pairs identified by their colleagues. We first present a general description of the experiment followed by a more detailed explanation of the experimental procedure. 5.1. Experiment  X  overview Experimental sessions consisted of two phases: a search phase followed by an evaluation phase:
Search : Given a hypothetical teaching task expressed in the form of an educational standard, respondents were asked to find and assess supporting learning documents; i.e., lesson materials in a Web-based digital library of K-12 curricula.
Evaluation : Respondents were asked to assess the appropriateness of existing alignments of the same hypothetical teach-ing tasks X  X gain expressed in the form of educational standards X  X nd one or more documents.

Teaching tasks were formulated as having to teach to a particular K-12 math or science standard. The standards were se-lected from a database of US science, mathematics and technology standards made available by the Achievement Standard
Network (ASN) project ( Gateway, 2007 ). Since all standards are associated with both a US state and a specific grade band, assignment of the standard implied teaching students of that grade band.

Documents were from the TeachEngineering.org digital library that was developed as part of NSF X  X  NSDL program and which contains over 800 classroom-tested, peer-reviewed, hands-on K-12 engineering learning objects. Use of these mate-rials is free of charge and all documents are aligned with educational standards (Sullivan et al., 2005 ). 5.2. Experiment  X  procedure Selection of standards for the experiment was subject to various constraints and considerations:
With the help of several K-12 curriculum developers, we selected standards for which the TeachEngineering collection was thought to provide a modest amount of coverage. Modest coverage prevents respondents from spending lots of time on failing searches while also not overwhelming them with search results.
 At the time of experimental design, the majority of TeachEngineering documents resulted from three states: Colorado,
Massachusetts and North Carolina. All of the documents from these states had been aligned with standards by human cat-alogers. Our curriculum experts preferred using these  X  X uman X  alignments over alignments made for standards of other states by automated standard alignment and assignment tools.

Since the contents and formulation of K-12 standards vary greatly across US states, we considered it desirable to have a wider coverage than just Colorado, Massachusetts and North Carolina.

Although not covered in this paper, we are interested in the validity of standard crosswalking. This consideration limits the availability of standards for experimental manipulation to those for which the TeachEngineering collection had some but not too many alignments and for which good alignments with other states X  standards could be found.
 To avoid grade band and topical bias, we needed representation of teaching tasks across multiple grade bands and topics.
In order to have statistically robust IRR measures we needed teaching tasks processed multiple times. This implied keep-ing the number of teaching tasks relatively small.

Taking the above considerations into account, we asked our catalogers to find 25 standards from the ASN covering a vari-ety of topics, states, and grade ranges, with each standard having modest coverage in TeachEngineering. The resultant set included standards from six states: Colorado (three tasks), North Carolina (five tasks), Massachusetts (five tasks), Virginia (seven tasks), Maryland (four tasks) and New Jersey (one task). Standards covered a variety of topics and grades four through grade 11. This set of 25 standards constituted our  X  X rimary X  set of standards. We then asked the catalogers to pair each of these 25 primary standards with two similar standards from different states. By pairing each primary standard with two sec-ondary ones we could subject respondents to three alignments for each document they had searched or evaluated, thereby leveraging the time needed by respondents to find and study those documents. The catalogers were successful in creating pairings for 23 of the 25 primary standards resulting in a set of 46 secondary standards. Table 2 contains a sample of three primary and their associated six secondary standards used in the experiment.

After receiving a short training on how to navigate and use the TeachEngineering system, respondents were asked to par-ticipate in a session lasting several hours during which they carried out the following sequence of activities. 5.2.1. Search phase
In the 90-min search phase of the experiments, respondents were asked to select any of the 25 primary tasks that were printed on index cards and laid out on a table in the experimental facility and to search the TeachEngineering collection for documents that they would consider using in preparation of teaching the task. Because each task was only listed on one card, the respondents as a group were guided to explore a range of tasks. Tools available to the respondents for finding supporting curriculum included the full functionality of the TeachEngineering system; i.e., hierarchical document browsing, full-text searching, educational standards-based searching, advanced searches, etc.

Once respondents selected a possibly interesting document, they were asked to score the document X  X  perceived align-ment relative to the task on four of Saracevic X  X  seven clues of relevance. These were operationalized X  X ith the help of an academic education expert X  X nto a total of nine  X  X imensions X  grounded in the everyday practice of K-12 science and math-ematics teaching (Table 3 ). An  X  X verall X  alignment score was also collected.

Scores per dimension were collected on a six-point Likert scale (strongly agree  X  agree  X  somewhat agree  X  somewhat disagree  X  disagree  X  strongly disagree). Each scale had a  X  X on applicable X  score to allow respondents to express their inabil-ity or unwillingness to score the scale. An additional control scale for testing respondents X  self-assessed ability to properly evaluate the document was included. Including this control scale in the rating form allowed us to exclude from analysis all scalings associated with tasks and documents the respondents believed they had insufficient background to effectively teach.
When respondents completed the task, the associated index card was put back on the table so that it became available again to other respondents. 5.2.2. Evaluation phase
In the subsequent 90-min evaluation phase, respondents were asked to judge the appropriateness of task-document pairs identified by other respondents. The goal of these evaluations was to collect the multiple scalings for document/task pairs needed to assess IRR. Respondents anonymously reviewed each other X  X  assignments only in the early sessions of the exper-iment. The searches and evaluations from those sessions generated a large number of additional document/task pairs used in the evaluation phase of later sessions.

Since pilot sessions had shown that respondents spent considerably more time studying and inventorying the contents of curricular documents than scoring the Likert scales, we decided to leverage the document reading time by asking respon-dents to score each document for alignment with one primary and two secondary teaching tasks. Although IRR assessment requires at least one pair of judgments per object of judgment, having additional pairs strongly increases the robustness of the IRR measures. Since the number of pairs increases exponentially with the number of judgments, collecting a few extra judgments for the same task-document alignments quickly firms up this robustness. Also, whereas many task-document alignments made in the search phase of an experimental session were assessed for IRR purposes in the evaluation phase of the session, the corresponding tasks (Ta ... ,Tb ... ) were assessed only in the evaluation phase of a session and hence, were in need of additional assessment is subsequent sessions. To account for the growing need for additional evaluation of previ-ously identified pairs, the search phase of the experiment was reduced from 90 to 30 min in the later experimental sessions and the evaluation phase was increased from 90 to 120 X 150 min.

Six experimental sessions were held over a 5-month period and at four locations (Table 4 ). Note the substantial variation in Alignments Scaled per Respondent across sites. The low rate (9.6) for the Oregon State University session reflects that is was a short (two times 30 min) pilot session. The results of this pilot, however, were so encouraging that we decided to include them in the experimental dataset. The disproportionately high number of evaluation scalings collected at Duke University in October 2008 reflects the above-mentioned technique of reducing the search phase duration and expanding the duration of the evaluation phase. 6. Results 43 Respondents processed 247 + 708 = 955 task-document combinations. Of these, 21 were eliminated from analysis be-cause the respondents disagreed with the control statement:  X  X  X  do have the math, science or engineering background to effec-tively use this document.  X  This resulted in a dataset of 934 10 = 9340 scored scales.

Collected search and evaluation scalings substantially cover the assigned tasks. For each of the 25 primary tasks, respon-dents selected at least one document and provided at least two ratings X  X ncluding all ten scales X  X or that document. Docu-ments were selected and ratings were performed for all 71 tasks (25 primary and 46 secondary). On average, 7.4 different documents were rated for each task with an average of 13 ratings per task and a maximum of 36. Of the 525 unique docu-ment/task pairs that were scored, 239 X 90 of which were search phase alignments X  X id not receive multiple ratings and hence, were excluded from IRR computations. Based on this analysis we do not believe that our results are inappropriately dominated by any subset of tasks, documents, or raters and have sufficient variation across tasks, raters, and documents to support compelling statistical results.
 To test hypothesis H-1 we defined four IRR measures: Distance : The distance between the scaled alignment values for a pair of respondents.
 Binary : Proportion of pairs of respondents that judge the alignment on the same half of the scale (agree vs. disagree).
Close : Proportion of pairs of respondents that judge the alignment on the same half of the scale and differ by no more than one scale point.
 Exact : Proportion of pairs of respondents that judge the alignment identically.

Table 5 shows the various IRR measures for each of the alignment dimensions and overall alignment. p -Values for statis-tical significance were computed using a t -test for the ratio variable Distance and using chi-squared tests for the proportional measures Binary , Close and Exact . All measures were computed for the entire set of paired judgments ( All ) including both search and evaluation phases and separately for the evaluation phase judgments ( Eval ).
 H-1 states that dimension-specific IRR will be higher than overall alignment IRR. Table 5 shows mixed results. Confirming H-1, the mean dimensional IRRs are high except for (unlikely) exact, six-point scale matches. Dimensional Distance IRR shows average values of 1.05 ( All ) and 1.36 ( Eval ). Binary has mean agreement values of 75.17% ( All ) and 68.94% ( Eval ), and Close shows 66.79% ( All ) and 60.37% ( Eval ) agreement. Comparing each dimension separately with overall alignment, we see that for Distance IRR, motivation, concepts, background , and examples significantly outscore overall alignment for both the All and Eval samples. For the Eval sample, this pattern is stable across the various IRR measures except for Exact matches.
Not confirming H-1 are the nonsignificant differences between dimension-specific IRR and overall alignment IRR for grade adjustability , and the presence of non-textual components, hands-on activities, attachments and external references and links ( ref_links ). The absence of significant differences for these dimensions suggests that rather than representing aspects other than overall alignment, they represent particular aspects of overall alignment. We will address this issue when assessing hypothesis H-3.

In addition to the differences between dimension-specific IRR and overall alignment IRR, we observe that the IRR values for overall alignment themselves are much higher than those reported by Devaul et al. (2007) and Bar-Ilan et al. (2007) . Although the design of our experiment does not allow us to test for this specifically, we suggest the following reasons:
On our questionnaires, the overall alignment scale was collected last; i.e., respondents scored the alignment on all dimen-sions before scoring overall alignment. We offer that this order might have elevated the overall scores, in particular because none of the dimensional IRR scores are low and because by asking respondents to scale the specific dimensions first, they might have consistently interpreted overall relevance as a balanced score card of the specific alignment dimensions.

All TeachEngineering materials are carefully reviewed, vetted and classroom-tested, and care has been taken to structure each of the learning objects following an accepted pedagogical model. This might have contributed to consistently high overall alignment scores.

Hypotheses H-2 and H-3 address the relationships between the various alignment dimensions. Whereas H-2 specifies that alignment results for the same task-document combination will be different across dimensions and that scores on one dimension may not be a good predictor of scores on other dimensions, hypothesis H-3 explores the possibility of mod-eling overall alignment as a composite function of various dimensions. We base the assessment of both these hypotheses on the dimensions X  correlation matrix. Fig. 2 shows the 45 correlations between the ten alignments X  X ine dimensions and over-all alignment X  listed in Table 3 . The 45 paired value scores are plotted as density graphs with lighter colors indicating higher frequencies and darker colors indicating lower ones. The product-moment correlation coefficients ( r ) are shown in the center of each plot. Naturally, the narrower the point cloud X  X he higher the density X  X he higher the correlation.

Although all correlations shown in Fig. 2 are statistically significant at p &lt; 0.01, correlation values vary substantially, con-firming hypothesis H-2. For instance, the correlation between how respondents judged the coverage of concepts present in the teaching task ( concepts ) and that of the required background material ( background ) is .76, whereas that between whether respondents considered the curriculum to be motivating for their students ( motivation ) and it containing useful references and Internet links ( ref_links ) is a mere .29.

To assess hypothesis H-3 X  X  partial but statistically stable model of overall alignment can be constructed X  X e estimated four multiple linear regression models with overall alignment as the dependent variable and the nine alignment dimensions as the independent ones. Table 6 displays these models with only statistically significant variables shown. Confirming hypothesis H-3, the model including all scalings ( All ) explains 76% of the variance in overall alignment. 6.1. Alignment in search differs from alignment in evaluation
Reviewing the data in Table 6 , it appeared that there were some important differences in the patterns of assessment be-tween the search and evaluation phases of the experiment. For instance, we observe that when searching, respondents con-sider the presence of hands-on activities and attachments to significantly contribute to overall alignment as together they add more than 13% to the explained variance of the model. This result is corroborated informally by evidence from the experimental sessions, during which some respondents reacted enthusiastically when they discovered that lesson materials included electronic versions of scoresheets and test materials. They commented several times on how much work it would save them not having to create these documents themselves. When evaluating, however, activities and attachments do not significantly contribute to overall alignment. Conversely, although both the coverage of concepts and the provision of background information contribute significantly to overall alignment in both search and evaluation, the coefficients and con-tributions to explained variance are higher among evaluators.

An interesting role is also reserved for the independent variable grade. Recall that grade represents the self-assessed abil-ity of the respondent to scale or transpose the content of a learning resource to the grade level given in the teaching task. This pertains to hypothesis H-4, as it states that in the context of our notion of alignment some of the traditional factors through which we often differentiate learning resources; e.g., grade level, might not apply in some contexts. Whereas most if not all collections of K-12 math and science curriculum offer a by-grade-level search option, a consequence of confirming hypoth-esis H-4 could be that such search options might do more harm than good in that by using them users might significantly reduce recall through an increase in false negatives.

To test H-4 in relation to grade level we computed the absolute difference between the grade level associated with the teaching task and the target grade level of the selected document for each scaling conducted by a respondent. Next, we com-puted the correlation between that difference and both overall alignment and the grade-level adjustability variable (grade).
Confirming H-4, neither of these correlations was statistically significant. Combining this result with the regression models all alignment and grade adjustability, it might not even be a meaningful alignment construct at all. What seems to matter instead is a teacher X  X  self-assessed capability to scale and apply materials to a teaching task at a particular grade level. But again, as a contributor to overall alignment searchers consider such grade-level adjustability more important than evalua-tors. Among searches it adds 8.9% to the explained variance whereas for evaluations it adds only a modest 3%. 6.2. Additional differences between search-based and evaluation-based alignment
Other important differences exist between alignment judgments collected during the search phase ( n = 245) and those collected during the evaluation phase ( n = 689) of the experiment. Fig. 3 shows two starplots of the 45 correlations between the 10 alignment dimensions; one set of correlations for alignments made during search (dashed line) and another set for alignments made during evaluation (solid line). Correlations are plotted starting at three o X  X lock, clockwise, in descending order of difference ( r search r evaluaton ). p -Values X  X omputed using Fisher X  X  Z transform X  X ive the probability that the differ-ence between correlations in both phases of the experiment are due to chance. We observe that 38 of the 45 correlations between alignment dimensions are smaller for judgments associated with experimental searches than for those collected in the evaluation phase of the experiment (dashed line on the outside). A t -test for differences in mean correlation X  X pplied on the Fisher X  X  Z transformed correlation coefficients X  X hows that the correlations for searches, on average, are indeed sig-consider the various alignment dimensions more independent from each other when searching for curricular documents than when evaluating other people X  X  search results.

Besides this overall pattern of correlations, specific regions of the starplots show interesting differences as well. A section on the right-hand side of the plot X  X etween one and three o X  X lock X  X ontains correlations that follow the opposite pattern; namely, correlations pertaining to searches that are larger than those pertaining to evaluations. Of the five correlations in this region, four involve overall alignment. This suggests that when searching for curricular documents, respondents consid-ered overall alignment significantly more associated with specific alignment aspects than when evaluating and vice versa . Whereas respondents associate overall alignment more with the availability of attachments  X  X orksheets, testing materials, etc. X  and references and links to associated teaching materials ( ref_links ) when searching, when evaluating other people X  X  alignments, they associate overall alignment more with background (seven o X  X lock) and concepts (11 o X  X lock).
The region between three and six o X  X lock shows another contrast between searching and evaluating. Whereas when eval-uating Saracevic X  X   X  X ffect X  dimension X  X ere operationalized as the degree to which the curricular materials are considered motivating X  X s associated with dimensions such as the presence of examples , the availability of hands-on exercises ( activi-ties ), non-textual materials and attachments , these correlations are much weaker in the searches. Similarly, when evaluating, ciations were significantly weaker when searching for documents.

After having found these differences in dimensional correlations between searches and evaluations, we checked to see if any such differences were present in the IRR scores as well. Although none were statistically significant, the number of IRR pairs obtained from the search part of the experiment was small ( n = 131) and hence, it is possible that with a larger set of search-borne IRR pairs significant differences would have been found.

We also see the results of Fig. 3 reflected in the contributions to explained variance of overall alignment by each of the alignment dimensions ( R 2  X  Table 6 ). Whereas when searching, the inclusion of hands-on activities and prepared attachments in the lesson materials contributed 13.5% to the explained variance of the model, these variables are entirely absent from the evaluation model. Similarly, whereas concepts and background material jointly explain 70% of the explained variations in evaluations, they explain only 56% of overall alignment in searches. 7. Discussion and conclusion The object of this work was to apply the Saracevic model of relevance clues to the alignment of curricular materials with K-12 educational standards in an attempt to boost the IRR of alignment judgments in support of the development of a gold standard set of such alignments. To this end we devised an experiment that asked respondents X  X ll essentially subject matter experts X  X o both formulate alignments (search phase) and to assess previously made alignments relative to a set of alignment dimensions modeled on the various  X  X lues X  of the Saracevic model (evaluation phase).

The results support our main thesis that by defining alignment in specific, teaching-related terms rather than in the form of an essentially undefined  X  X verall X  notion, respondents will better differentiate between the various aspects of alignment and poor IRR can be avoided. This, in turn, improves our potential for developing a commonly available, ground-truthed, gold standard set of alignments. Moreover, three of the dimensions with the highest IRR X  background, concepts and examples  X  X re the ones most frequently explored in the development of automated curriculum alignment systems through the use of con-trolled vocabularies, keywords, lexicons, ontologies, etc. and their mapping and relationships to the terms in educational standards. Hence, testing the performance of those systems X  X s in Devaul et al. (2007)  X  X an directly benefit from applying these more specific notions of alignment.

One of our hypotheses concerned the possibility of a structural model of overall alignment. The regression models indi-cate that, indeed, such a model can be formulated but with some qualifications. First and foremost, the models seem to be different depending on whether people are searching for aligned documents or assessing alignments made by others. The differences are both qualitative and quantitative; i.e., the part-worth alignment dimensions that factor into overall align-ment are different and the common dimensions have different coefficients. Moreover, these differences cannot be attributed to the usual difference between respondents conducting the searches and subject matter experts judging the search results, as in our experiment searches and evaluations were conducted by the same respondents! As all of these respondents were either K-12 teachers or teachers in training, they should all be considered subject matter experts. Hence, the data show strong evidence that although a core model of overall alignment can be constructed, overall alignment is a different concept dependent on whether respondents are searching vs. evaluating other people X  X  recommendations. Although both the search and evaluation models are dominated by content dimensions, other, practical dimensions such as the presence of class-ready worksheets and assignments added significant explanatory power to the search model.

What then, are the implications of these differences in alignment between when searching and evaluating? Based on the design and administration of the experiment, we conclude that the differences result from the difference between respon-dents having searched for and found the curriculum themselves vs. having them given to them by others. This then, has implications for the use and value of recommender services in curriculum alignment. These would include recommendations of users but also those made by automated alignment services such as CAT. One such implication could be to pair visual indi-cators of recommendations such as the ubiquitous five-star system, with comparable indicators of the presence of class-ready worksheets and hands-on exercises. Similarly, it might be wise to reduce the role of grade band information in cur-ricular search engines in order to reduce the likelihood of false negatives.

Perhaps the most important implication of both search-evaluation variation and the variation in alignment across dimen-sions is that we might have to make a difficult choice. Either develop better and more realistic notions and models of align-ment and embed these models in our automated retrieval mechanisms, or alternatively, recognize that automated retrieval mechanism are unlikely to be better equipped to specify context-dependent aspects of alignment than our users and hence, involve those users in the retrieval process itself. This could take the form of a stepped or guided search, users indicating their preference for certain dimensions over other ones or letting users specify specific search heuristics, or combinations of these. Of these two very different alternatives, we consider the second, user-driven search, the more promising one.
The differences in how alignment is conceptualized in a search task vs. an evaluation task and the differences in IRR be-tween alignment dimensions also forces a reconsideration of the way in which we must collect alignment examples in our efforts in developing a gold standard. The traditional method for doing this is to have subject matter experts judge (evaluate) the results of information retrieval mechanisms. This practice is based on the assumption that, at least intersubjectively, this body of experts has it  X  X ight. X  However, as our data show, significant differences exist in dimensional IRRs as expressed by our experts and the model of overall alignment changes depending on the task. Does this imply that a gold standard for K-12 math and science curriculum alignment is out of reach? If our conception of alignment is one-dimensional; i.e., if we con-tinue to aim for overall alignment only, then indeed, we should not expect much success. However, if we, as Saracevic ad-vises us, face the complexity of the alignment concept and break it down into different, task-specific contextual dimensions, we have a much better chance at constructing such a standard.

We believe that our results also provide relevant information for those developing automated curriculum alignment tools and mechanisms. Most existing alignment methodologies rely primarily on lexical analysis. Alignment services often main-tain carefully constructed taxonomies of terms to help with the process. Term-based analysis seeks to leverage the informa-tion stored in the concepts and background alignment dimensions of curricular documents. These are indeed powerful dimensions as they jointly contribute 70% and 56% of the explained variance of overall alignment among evaluations and searches respectively. On the other hand, the concepts and background dimensions do not address the remaining 30% and 44% of overall alignment. Whereas in evaluations our other dimensions only add another 3.5%, they explain a much larger 22.4% of variance among searches. Our survey questions attempt to measure alignment clues from four of Saracevic X  X  cate-gories. While sophisticated neural network tools such as CAT and SAT do theoretically allow for this kind of additional input, current systems appear to have been trained using only singular (overall) and binary (relevant/irrelevant) training data. Our results suggest that automated tools could benefit from such additional inputs, and development of additional clues and measures should be a profitable line of future work.

The observed variation between searchers and evaluators is also of interest in developing improved retrieval mechanisms for domains other than curriculum alignment. Given the vast and growing collections of materials provided in portals with search capability, it will become increasingly difficult to provide manually-generated expert alignments. If patrons of an edu-cational digital library are like the searchers in our experiment, they may have expectations that vary substantially from those of expert raters asked to establish that a particular document is appropriate for a particular teaching task. This, how-ever, raises interesting issues. For instance, how must we assess a document X  X  score on dimensions such as grade adjustabil-ity or the presence of data recording sheets ( attachments )? And if we can already do so X  X erhaps with interactive help of the user X  X ow do we combine these dimensions into an overall alignment score? These are interesting challenges for which we see no immediate and easy answers. Yet, if the interpretation of our data is correct and searching for aligned curriculum is a contextual activity that follows the Saracevic model of relevance, we have plenty of incentive to develop these mechanisms. Acknowledgement This work was sponsored, in part, by the US National Science Foundation, Grant #0532709.
 References
