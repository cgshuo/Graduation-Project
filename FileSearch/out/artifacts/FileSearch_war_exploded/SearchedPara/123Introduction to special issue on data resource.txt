 This special issue on Data Resources, Evaluation, and Dialogue Interaction is based on five thoroughly revised and extended papers from the sixth SIGdial Workshop held in Lisbon, Portugal, in September 2005. SIGdial is a special interest group on discourse and dialogue whose parent organisations are the Association for Com-putational Linguistics (ACL) and the International Speech Communication Asso-ciation (ISCA). SIGdial workshops accommodate a broad range of topics related to discourse and dialogue. Among these topics are data resources, evaluation, and dialogue interaction. The papers selected for this special issue have in common that they all deal with aspects of these topics and each paper has its focus on at least one of them.

Dialogue interaction is crucial to the end-user experience and there are many ways in which to improve this interaction and ensure its quality. The five papers only illustrate a few aspects of how it may be done. However, in all cases data resources and various kinds of evaluation play a central role. Data resources are the basis for training as well as for development and evaluation of (parts of) spoken (multimodal) dialogue systems. Evaluation may be done to see, e.g., if a resource is reliable, if an approach works or works better than some other approach, or how a system is perceived by end-users and other stakeholders.
 The five articles included in this issue are the following ones: A Corpus for Studying Addressing Behaviour in Multi-party Dialogues by Natasa Jovanovic, Rieks op den Akker and Anton Nijholt, Automatic Induction of Language Model Data for a Spoken Dialogue System by Chao Wang, Grace Chung and Stephanie Seneff, Evaluating the Markov Assumption in Markov Decision Processes for Spoken Dia-logue Management by Tim Paek and David Maxwell Chickering, Adaptation of an Laila Dybkj X r  X  Wolfgang Minker  X  Automotive Dialogue System to Users X  Expertise and Evaluation of the System by Liza Hassel and Eli Hagen, and DialogDesigner: Tools Support for Dialogue Model Design and Evaluation by Hans Dybkj X r and Laila Dybkj X r.

In the following we briefly describe the contents of each of the five articles and for each of them we highlight its relations to data resources, evaluation, and dialogue interaction.

The paper by Jovanovic, op den Akker, and Nijholt has its primary focus on data resources and their annotation. The authors describe the creation of a multimodal corpus of 12 meeting dialogues each with four participants. Containing about 75 min of recorded data, the corpus was designed for studying addressing behaviour in face-to-face conversations. So far it has been manually annotated with dialogue acts, adjacency pairs, addressees, and gaze directions. The coding schemes used for the four different levels of annotation are described and two tools used in support of the annotation are briefly presented. Six expert annotators were involved. Four anno-taters coded dialogue acts, addressees and adjacency pairs. Two other annotators coded gaze direction.

Annotated data resources should be evaluated for reliability before they are used for further purposes, such as studying features in conversation which we need in order to build systems that enable more natural (multimodal) dialogue interaction. The inter-annotator reliability is presented for each of the four coding schemes. For gaze annotation reliability was judged to be good while for dialogue acts and addressees it was lower though still acceptable. Also intra-annotator reliability has been measured for the annotation of dialogue acts and addressees. Each of the four annotators who coded dialogue acts, addressees and adjacency pairs annotated a meeting twice over three months. The intra-coder agreement was found to be good.
For Wang, Chung, and Seneff data resources are a means to creating better lan-guage models. When insufficient amounts of real data are available, artificial data may be generated and used. The authors discuss how to produce such an artificial corpus for language model training. The presented method is based on the gener-ation of large amounts of synthetic data and involves several steps. The approach has been evaluated in the context of recognition performance of a restaurant informa-tion system. The first step is to generate a seed corpus in the target domain by running simulations of the dialogue system. Then a synthetic corpus is generated by using the seed corpus together with a previously collected corpus from another domain (in the described case flight reservation) to transform the foreign corpus into the restaurant domain. Two transformation methods are presented, i.e. template-based transformation and transformation via formal generation rules. Transforma-tion generates a corpus which normally includes many inappropriate sentences for the target domain so a subsequent filtering process is needed. First a filtering based on syntactic constraints is performed followed by a semantic filtering based on topic-predicate relationships. The resulting corpus cannot be expected to have the same frequency distribution as real user data. To cope with this problem data sampling is used. Two methods are described. One method is based on simulation data while the other assumes that a small corpus of development interaction data is available.
To demonstrate the benefit of language models based on the synthetic data the authors have evaluated recognition performance in the restaurant system. Results from experiments are reported which show that synthetic training data helps improve recogniser performance in particular when combined with utterances from development data. Good recognition performance is crucial to dialogue interaction.
Paek and Chickering address reinforcement learning for spoken language dialogue management which is a kind of machine learning and thus needs training data. Reinforcement learning is concerned with an agent that through trial-and-error learns how to behave in a dynamic environment from which the agent receives a positive or negative reward for each action. The challenge consists in finding a policy which maximises the reward over the course of a dialogue. Often the environment is represented as a Markov Decision Process (MDP). The MDP assumes that the current dialogue state depends only on the previous state and action which may be a problematic assumption in a dialogue context. Furthermore, dialogue designers typically do not know the structure of the state space, i.e. they do not know in advance which variables are relevant for receiving a reward and how they relate to each other.

The primary focus of the paper is evaluation of the performance of the MDP model compared to other models. Ultimately the goal is to optimize dialogue interaction. To do the comparative evaluation the authors have performed a study based on a command-and-control, speech-enabled web browser. A finite horizon MDP can be represented as an influence diagram which is a directed acyclic graph. In the study three types of alternative influence diagrams were built that differ with respect to their temporal dependencies. These alternative models are described and the evaluation of their performance, including that of the MDP model, is reported. The study showed that not the MDP model but one of the alternatives, i.e. the total reward model, achieved the best performance.

The paper by Hassel and Hagen concerns evaluation of adaptation in a multi-modal spoken dialogue system implemented as part of BMW X  X  in-car system called iDrive. In addition to spoken input and output the evaluated system includes haptic input via a controller and push-to-talk button and graphics output. The system enables the user to carry out various tasks, e.g. navigation and air conditioning, via command and control. The system adapts to the user X  X  level of expertise. A user may be an expert in one kind of task while being a novice in another task type. The level of expertise is calculated based on, e.g., the number of help and option requests. System prompts for novices are more explicit than those for experts.

The system with prompt adaptation was evaluated together with a reference system which did not have prompt adaptation but otherwise had similar function-ality. Each system was evaluated with 22 novice users carrying out 11 scenarios whilst driving. A questionnaire was filled in subsequently. To evaluate the test results a modified version of the PARADISE framework was used. This framework claims that maximising task performance which means high user satisfaction, corresponds to maximising task success and minimising dialogue costs.

The data collected with the two systems was analysed and the results may be used to improve the dialogue interaction model. The analysis showed that user satisfac-tion as well as nearly all objective measures were better for the system with adap-tation than for the reference system meaning that adaptation was positively received. However, no correlation was found between user satisfaction on the one hand and task success and cost factors on the other hand.
 Dialogue interaction modelling is a core issue in the article by Dybkj X r and Dybkj X r . However, dialogue interaction modelling requires iterative data collection and evaluation so that new and improved versions of the dialogue model can be built. The authors present a tool called DialogDesigner in support of a modern iterative lifecycle process of developing and evaluating spoken dialogue systems. DialogDesigner has primarily been created for commercial application development, but so far it has only been used in a couple of commercial projects since it is very new. Compared to other tools its particular strength is its support for stakeholder communication but it also includes state of the art support for development of task-oriented spoken dialogue systems and for efficient code development.

DialogDesigner enables its user to create an electronic dialogue model and offers a suite of tools which operate on this dialogue model and contribute to its further development. These tools include a dialogue snippet design tool that allows the dialogue model developer to design entire or partial scenarios even before an electronic dialogue model has been created which is often how dialogue model design is initiated. The snippet tool may also be useful after a dialogue model has been created, and it is possible to automatically check if the snippets are consistent with the dialogue model. A simulation tool provides support for performing walk-throughs of the dialogue model and for making Wizard of Oz simulations to collect data for analysis and evaluation of dialogue interaction. There is a visualisation tool which enables the user to see graphical presentations of the dialogue model. Moreover, there is a tool which performs automatic analysis of aspects of well-formedness of the dialogue model. Automatic generation of HotVoice code is also included. Finally, it is possible to extract various presentations, such as a phrase list and a prompt list.

The five articles in this issue only demonstrate a few aspects of the topics of data resources, evaluation, and dialogue interaction, although from very different per-spectives. Nevertheless, we hope that the articles will stimulate further work on the mentioned topics which are all crucial to the discourse and dialogue community and also to wider communities.

