 Singular Value Decomposition (SVD)/Principal Component Analysis (PCA) have played a vital role in finding patterns from many datasets. Recently tensor factorization has been used for data mining and pattern recognition in high in-dex/order data. High Order SVD (HOSVD) is a commonly used tensor factorization method and has recently been used in numerous applications like graphs, videos, social networks, etc.

In this paper we prove that HOSVD does simultaneous subspace selection (data compression) and K-means cluster-ing widely used for unsupervised learning tasks. We show how to utilize this new feature of HOSVD for clustering. We demonstrate these new results using three real and large datasets, two on face images datasets and one on hand-written digits dataset. Using this new HOSVD clustering feature we provide a dataset quality assessment on many frequently used experimental datasets with expected noise levels.
 H.2.8 [ Database Management ]: Database Applications-Data Mining Algorithms [20]. Higher Order Orthogonal Iteration (HOOI) [14] used an iterative method to do tensor decomposition in all in-dices. Ding et al. provided D -1 tensor reduction algorithm and error bound analysis of tensor factorization [6].
Although many studies on tensor factorization appeared in data mining, pattern recognition, and machine learning areas, to our knowledge, this paper is the first one to propose and prove the equivalence between HOSVD and tensor clus-tering. In previous research, there are several papers that ever explored the relations between unsupervised dimension reduction and unsupervised learning [5, 24].

The main contributions of our paper are as follows: 1) We propose and prove the equivalence of HOSVD and si-multaneous subspace selection (compression) and K-means clustering with maintaining global consistency; 2) Experimental results on three public datasets demon-strate our theory; 3) Furthermore, our algorithm provides a dataset quality as-sessment method to help data mining and machine learning researchers to select the experimental datasets with their expected noise level.
Consider a set of input data vectors X = ( x 1 ,  X  X  X  , x n ) which can be also viewed as a 2D tensor X = { X ij } m i =1 n j =1 . PCA is the most widely used dimension reduction method by finding the optimal subspace defined (spanned) by the prin-cipal directions U = ( u 1 ,  X  X  X  , u k )  X  &lt; m  X  k . The projected data point in the new subspace are V = ( v 1 ,  X  X  X  , v n )  X  &lt; k  X  n . PCA finds U and V by minimizing In PCA or SVD, the Eckart-Young Theorem plays a funda-mental role. Eckart-Young Theorem[8] states the optimiza-tion problem has PCA/SVD as its global solution and the optimal (minimum) value is the sum of eigenvalues. where  X  m are eigenvalues of the covariance matrix XX T .
The input data is a 3D tensor: X = { X ijk } n 1 i =1 n 2 j =1 n 3 The rank-1 and HOSVD factorizations, treating every index uniformly, is where U, V, W are 2d matrices and S is a 3D tensor. Using explicit index, J 1 = is diagonal: S pqr = m p if p = q = r , S pqr = 0 otherwise. In many cases, we require that W, U, V are orthogonal: U T U = I, V T V = I, W T W = I . viewed as X = { X 1 ,  X  X  X  , X n 3 } where each X i is a 2d matrix ( e.g. an image) of size n 1  X  n 2 . Therefore, instead of treating matrix W gives out the clustering results on the data index direction.

Theorem 1. The HOSVD factorization of Eq.3 is equiv-alent to simultaneous 2DSVD data compression of Eq.5 and K-means clustering of Eq.8.
 Proof. We will prove the following: 1) Solution of W in HOSVD is the cluster indicator to K-means clustering (tensor clustering) of Eq.8; 2) ( U, V ) in 2DSVD of Eq.5 is the same ( U, V ) in HOSVD of Eq.3;
Given a three dimensional tensor X n 1 n 2 n 3 , or, equiva-lently a set of two dimensional images X (1) , X (2) ,  X  X  X  , X ( n 3 ) with size n 1  X  n 2 (we have X ijl = X ( ` ) ij ), the objective func-tion of tensor clustering is (please see Eq.8): where M ( ` ) = U T X ( ` ) V . We prove Eq.9 is equivalent to Eq.3.

Eq.9 can be written as: min where n k is the number of images in cluster C k . The solu-tion of tensor clustering is represented by K non-negative indicator vectors: where Let X  X  denote: We rewrite Eq.10 using Eq.11 and Eq.13: The first item is a constant. Thus min J K becomes and
The solution to Eq.3 can be derived using the following functions: After convergence, the iterations of A) X  X ) stop.
 Note that, F in Eq.20 is identical to  X  F of Eq.27 using Eq.16. Similar relation holds for G and  X  G . Therefore, ( U, V ) in 2DSVD of Eq.5 are equivalent to ( U, V ) in HOSVD of Eq.3. 2
In summary, we have proved that HOSVD is performing simultaneous 2DSVD and K-means clustering while main-taining global consistence. This is the major contribution of this paper.
According to Theorem 1, the matrix W in HOSVD so-lutions is the indicator to clustering results. The cluster-ing indicator can be found by applying clustering method into matrix W . In this paper, we apply K-means clustering method on matrix W to obtain the clustering indicator.
We demonstrate the equivalence theorem by using AT&amp;T face image database [1] (40 subjects, each subject has 10 dif-ferent images with size 112  X  96, please see the more detailed data description in section 4) to illustrate the visualization results of confusion matrix C = WW T . Later we will show more experimental results in next section. After performing the HOSVD on a 112  X  96  X  400 image tensor with reduced dimensions as 30  X  30  X  400, we get a 400  X  30 matrix W and use it to calculate the confusion matrix C . Since HOSVD does both compression and clustering simultaneously, differ-ent compression rates affect the clustering results. Here, we choose the commonly used 30  X  30 as the image size after compression.

In order to get a clear representation, we resize the original 400  X  400 matrix C to a new 40  X  40 matrix. Each square represents a 10  X  10 matrix in the original one and includes all images of one subject. The areas of squares are the sum of values in matrix. Fig. 1 shows the visualization of confusion matrix C . Compared to the small size squares, the large squares on the diagonal denote more images are clustered into one cluster. Therefore, there exists clustering results in matrix W of HOSVD. Here, we use K-means clustering method to find them from the HOSVD results.

The algorithm for finding clustering indicator from HOSVD results is presented in Table 1. Here we only use K-means clustering method to find the clustering results that already exist in HOSVD factorization results. In 2DSVD + K-means clustering, K-means clustering method really processes the non-clustered data into clusters.
 The clustering mechanism of HOSVD is similar to use PCA (each image is resized as one vector) to do data pro-jection, but HOSVD uses a more efficient way  X  tensor fac-torization.
In this section, we experimentally demonstrate the pro-posed new insights to understand relations among HOSVD, 2DSVD + K-means clustering, and PCA + K-means clus-3) HOSVD. We perform HOSVD on the 102  X  92  X  400 image tensor to do simultaneous compression and clustering with reduced dimensions 30  X  30  X  40. After that, K-means Clustering method helps us find the cluster indicator from matrix W .

Since there are 40 distinct subjects in AT&amp;T database, all previous papers using this public dataset consider all 400 images as 40 natural clusters and each cluster includes 10 different images of the same subject. After using three above methods, the final results are represented as 400  X  40 ma-trices Q . Each row labeled by one image and each column shows one cluster, e.g. , Q ij means image i is clustered into cluster j . We hope to group all ten images of the same sub-ject together to observe how many images of one subject are clustered into the same cluster (the remaining images are clustered into wrong clusters). But in matrix Q the subject label is not identical to the cluster label. In order to demon-strate the clustering results better, we perform the following steps to generate a new 40  X  40 matrix I : 1) We group every ten images of the same subject together along vertical axis and sum their values together by columns: I  X  j = 1 ; otherwise,  X  i k j = 0 . As a result, each row in new matrix represents one subject and one column represents one cluster; 2) Using the new matrix I , we build a bipartite graph G = ( V = V 1  X  V 2 , E ) with two sets of 40 vertices V 1 , V 2 and map V 1 to the 40 rows and V 2 to the 40 columns in matrix I ; 3) The value of edge e ( i, j ) in the bipartite graph are de-fined as the value of I ij , i, j = 1 ,  X  X  X  , 40. Using Hungarian algorithm [13], we do bipartite graph cut with maximizing weight matching.

The result is a new matrix P and the image group num-ber is identical to the cluster number if the images from that group are not clustered into other subjects X  clusters. We visualize P PCA in Fig. 2(a), P 2 DSV D in Fig. 2(b), and P
HOSV D in Fig.2(c), respectively. In these three figures, each row illustrates one subject and each column represents one cluster. The green squares show the number of images are clustered into one cluster. The areas of squares are pro-portional to the number of images clustered together, from 10 to 0. Because most images are clustered into the de-fault subject cluster and few images are clustered into other subjects X  clusters, the large squares mostly locate on the di-agonals and some small squares are scattered into the other areas.

We also calculate the clustering accuracy of these three methods. The clustering accuracy is defined by the ratio of the number of images that are clustered into the default subject clusters and the number of total images. The num-ber of images with correct labels can be directly computed by summing all the squares X  areas on diagonal in each figure of Fig. 2(a), Fig. 2(b), and Fig.2(c). Table 2 illustrates the clustering accuracy comparisons of three methods.
In the beginning, we use all 400 images to do cluster-ing and notice there are some misclustered images for every method. Because images from one subject are not always similar, some of them can even be treated as outliers (noise). Thus, we hope to select the subset of images from original one with fewer outlier images. We consider using the su-perset of correct clustering results from different methods. After running three methods on 400 images, we select the groups in which at least n (we use n = 8 and 10 for two Figure 4: Sample images of MNIST hand-written digits dataset. subsets of images selection) images are clustered into the default cluster by at least one of three methods and merge all of them together to create the new dataset. When n = 8, we have 30 distinct subjects with 300 images; n = 10, we have 22 distinct subjects with 220 images. Three methods are performed on the new datasets again and the cluster-ing accuracy values are summarized into column 3 and 4 in Table 2. Our experimental results show the clustering accu-racy is improved when the selection quality of subset images increases.
 PCA +
K-means 70.5% 74.5% 80.2% 2DSVD + K-means 73.5% 78.7% 83.6% HOSVD 74.0% 80.7% 84.5% Table 2: The clustering accuracy comparison of PCA + K-means clustering, 2DSVD + K-means clustering, and HOSVD methods using AT&amp;T datasets.

In order to understand the clustering results better, we also pay attention to the images that are not clustered into the default subject cluster. For example, results of three clusters are illustrated into Fig.3. Images on each row are clustered into the same cluster by HOSVD method. Every cluster includes images from more than one subject. The white lines are used to separate the images of different sub-jects. Because the images from AT&amp;T face database vary in the lighting, facial expression, and facial details, images of some subjects are far away to other images of the same subject and more closer to images of other subjects. The im-ages within the same subject group are not homogeneous. Therefore, if we use distinct subjects as the default labels, the original datasets don X  X  always have the correct cluster labels. We will discuss this issue more in next section.
Here, we present experimental results on the MNIST hand-written digits dataset, which consists of 60,000 training and 10,000 test digits [15]. The MNIST dataset can be down-loaded from  X  X ttp://yann.lecun.com/exdb/mnist/ X  with 10 classes, from digit  X 0 X  to  X 9 X . In the MNIST dataset, each image is centered (according to the center of mass of the pixel intensities) on a 28x28 grid. Fig.4 displays sample im-ages of hand-written digits. row) has different face directions in his ten images: his jaw changes directions from right to left, and then to center. These different face directions create misalignment errors during image matching. As a result, his ten images are clus-tered into five different clusters. The subject in the second example has different face directions and different face sizes in these ten images and those differences effect the clustering results. Both third and fourth examples are clustered into two clusters. One has two different face directions and the other one has different face details (the glass). The fifth and sixth examples show the consistent images within the same subject cluster without separation during clustering process.
In order to quantitatively observe the difference between those images, we calculate the average distance of each pair images of the subject. We call it as dispersion distance and define it as follows: where X i and X j are images of the same subject, and n = 10 . The dispersion distances of six examples in Fig.6 are 0.56, 0.68, 0.65, 0.51, 0.49, and 0.46. The fifth and sixth examples have small dispersion distances compared to other examples. These dispersion distances help us measure the consistency of high dimensional data under the same category.
So far many data mining and pattern analysis algorithms are tested on the public face databases, e.g. AT&amp;T databases, but the noise within those public datasets definitely bothers the evaluations and comparisons of their methods.
Now we show how clustering accuracy changes using dif-ferent image subsets. At first, the HOSVD method is per-formed on all images. Based on the results, we select the subjects which at least have n images been clustered into the default subject cluster. When n ranges from 1 to 10, we create ten image subsets. We run HOSVD method again on the selected subset images and the clustering accuracy re-sults are shown in Fig. 7. The numbers of x -axis denote the n values and the values on y -axis are the clustering accuracy. The red line is the clustering accuracy using all 400 images. This figure illustrates how images of the same subject are the images that are clustered into different clusters. Figure 8: Visualization of HOSVD method using images from the Yale Face Database B. in total without any corrupted images. We randomly select ten illumination conditions for all 31 subjects to create the experimental dataset with 310 images.

We employ HOSVD factorization on the 192  X  168  X  310 tensor with reduced dimensions as 30  X  30  X  31. Using the same way to draw clustering results in section 4.1, we vi-sualize the HOSVD clustering results in Fig.8. Although the main large green squares locate on the diagonal, several erful dimension reduction techniques. In this paper, we first propose a new theorem of HOSVD that it does simultaneous 2DSVD subspace selection (compression) and K-means clus-tering while maintaining the global consistence. A rigorous proof is provided for this novel insight of relation between HOSVD and tensor clustering. Using the same technique of proving Theorem 1, we can also prove that the ParaFac tensor decomposition is also equivalent to a K-means clus-tering.

We provide experiments to demonstrate our theoretical results on three public datasets. In experiments, we com-pare HOSVD method with PCA + K-means clustering and 2DSVD + K-means clustering methods. The experimental results show that HOSVD gives out both data compression and clustering results.

Furthermore, our experimental results suggest that the high dimensional data are not consistent in those public datasets, because of the outliers. In order to select the clean datasets with expected noise level for data mining and ma-chine learning research experiments, we provide a HOSVD based dataset quality assessment method and use it to find interesting results from two well known face image datasets. C. Ding is supported in part by a University of Texas STARS Award and T. Li is supported in part by NSF IIS-0546280. [1] http://www.cl.cam.ac.uk/research/dtg/attarchive/ [2] O. Alter, P. O. Brown, and D. Botstein. Singular value [3] D. Billsus and M. J. Pazzani. Learning collaborative [4] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. [5] C. Ding and X. He. K-means clustering via principal [6] C. Ding, H. Huang and D. Luo. Tensor Reduction [7] C. Ding and J. Ye. Two-dimensional singular value [8] C. Eckart and G. Young. The approximation of one
