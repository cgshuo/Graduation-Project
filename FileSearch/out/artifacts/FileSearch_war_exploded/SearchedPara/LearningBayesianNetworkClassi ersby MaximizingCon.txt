 Daniel Grossman grossman@cs.w ashington.edu Pedro Domingos pedr od@cs.w ashington.edu The simplicit y and surprisingly high accuracy of the naiv e Bayes classi er have led to its wide use, and to man y attempts to extend it (Domingos &amp; Pazzani, 1997). In particular, naiv e Bayes is a special case of a Bayesian net work, and learning the structure and pa-rameters of an unrestricted Bayesian net work would app ear to be a logical means of impro vemen t. How-ever, Friedman et al. (1997) found that naiv e Bayes easily outp erforms suc h unrestricted Bayesian net work classi ers on a large sample of benc hmark datasets. Their explanation was that the scoring functions used in standard Bayesian net work learning attempt to op-timize the likeliho od of the entire data, rather than just the conditional likeliho od of the class given the attributes. Suc h scoring results in sub optimal choices during the searc h pro cess whenev er the two functions favor di ering changes to the net work. The natural solution would then be to use conditional likeliho od as the objectiv e function. Unfortunately , Friedman et al. observ ed that, while maxim um likeliho od parameters can be ecien tly computed in closed form, this is not true of conditional likeliho od. The latter must be opti-mized using numerical metho ds, and doing so at eac h searc h step would be prohibitiv ely exp ensiv e. Fried-man et al. thus abandoned this avenue, leaving the investigation of possible heuristic alternativ es to it as an imp ortan t direction for future researc h. In this pa-per, we sho w that the simple heuristic of setting the parameters by maxim um likeliho od while choosing the structure by conditional likeliho od is accurate and ef-cien t.
 Friedman et al. chose instead to extend naiv e Bayes by allo wing a sligh tly less restricted structure (one paren t per variable in addition to the class) while still opti-mizing likeliho od. They sho wed that TAN, the result-ing algorithm, was indeed more accurate than naiv e Bayes on benc hmark datasets. We compare our algo-rithm to TAN and naiv e Bayes on the same datasets, and sho w that it outp erforms both in the accuracy of class probabilit y estimates, while outp erforming naiv e Bayes and tying TAN in classi cation error.
 If the structure is xed in adv ance, computing the maxim um conditional likeliho od parameters by gra-dien t descen t should be computationally feasible, and Greiner and Zhou (2002) have sho wn with their ELR algorithm that it is indeed bene cial. They leave op-timization of the structure as an imp ortan t direction for future work, and that is what we accomplish in this pap er.
 Perhaps the most imp ortan t reason to seek an im-pro ved Bayesian net work classi er is that, for man y applications, high accuracy in class predictions is not enough; accurate estimates of class probabilities are also desirable. For example, we may wish to rank cases by probabilit y of class mem bership (Pro vost &amp; Domin-gos, 2003), or the costs asso ciated with incorrect pre-dictions may be variable and not kno wn precisely at learning time (Pro vost &amp; Fawcett, 2001). In this case, kno wing the class probabilities allo ws the learner to mak e optimal decisions at classi cation time, what-ever the misclassi cation costs (Duda &amp; Hart, 1973). More generally , a classi er is often only one part of a larger decision pro cess, and outputting accurate class probabilities increases its utilit y to the pro cess. We begin by reviewing the essen tials of learning Bayesian net works. We then presen t our algorithm, follo wed by exp erimen tal results and their interpreta-tion. The pap er concludes with a discussion of related and future work. A Bayesian network (Pearl, 1988) enco des the join t probabilit y distribution of a set of v variables, f x 1 ; : : : ; x v g , as a directed acyclic graph and a set of conditional probabilit y tables (CPTs). (In this pap er we assume all variables are discrete, or have been pre-discretized.) Eac h node corresp onds to a variable, and the CPT asso ciated with it con tains the probabilit y of eac h state of the variable given every possible com bi-nation of states of its paren ts. The set of paren ts of x denoted i , is the set of nodes with an arc to x i in the graph. The structure of the net work enco des the asser-tion that eac h node is conditionally indep enden t of its non-descendan ts given its paren ts. Thus the probabil-ity of an arbitrary event X = ( x 1 ; : : : ; x v ) can be com-puted as P ( X ) = Q v i =1 P ( x i j i ). In general, enco ding the join t distribution of a set of v discrete variables requires space exp onen tial in v ; Bayesian net works re-duce this to space exp onen tial in max i 2f 1 ;:::;v g j 2.1. Learning Bayesian Net works Giv en an i.i.d. training set D = f X 1 ; : : : ; X d ; : : : ; X where X d = ( x d; 1 ; : : : ; x d;v ), the goal of learning is to nd the Bayesian net work that best represen ts the join t distribution P ( x d; 1 ; : : : ; x d;v ). One approac h is to nd the net work B that maximizes the likeliho od of the data or (more con venien tly) its logarithm: LL ( B j D ) = When the structure of the net work is kno wn, this re-duces to estimating p ijk , the probabilit y that variable i is in state k given that its paren ts are in state j , for all i; j; k . When there are no examples with missing val-ues in the training set and we assume parameter inde-pendence, the maxim um likeliho od estimates are sim-ply the observe d frequency estimates ^ p ijk = n ijk =n ij where n ijk is the num ber of occurrences in the train-ing set of the k th state of x i with the j th state of its paren ts, and n ij is the sum of n ijk over all k . In this pap er we assume no missing data through-out, and focus on the problem of learning net work structure. Cho w and Liu (1968) pro vide an ecien t algorithm for the special case where eac h variable has only one paren t. Solution metho ds for the gen-eral (intractable) case fall into two main classes: in-dep endence tests (Spirtes et al., 1993) and searc h-based metho ds (Co oper &amp; Hersk ovits, 1992; Hec ker-man et al., 1995). The latter are probably the most widely used, and we focus on them in this pap er. We assume throughout that hill-clim bing searc h is used; this was found by Hec kerman et al. to yield the best com bination of accuracy and eciency . Hill-clim bing starts with an initial net work, whic h can be empt y, random, or constructed from exp ert kno wledge. At eac h searc h step, it creates all legal variations of the curren t net work obtainable by adding, deleting, or re-versing any single arc, and scores these variations. The best variation becomes the new curren t net work, and the pro cess rep eats until no variation impro ves the score.
 Since on average adding an arc nev er decreases like-liho od on the training data, using the log likeliho od as the scoring function can lead to sev ere over t-ting. This problem can be overcome in a num ber of ways. The simplest one, whic h is often surpris-ingly e ectiv e, is to limit the num ber of paren ts a variable can have. Another alternativ e is to add a complexit y penalt y to the log-lik eliho od. For exam-ple, the MDL metho d (Lam &amp; Bacc hus, 1994) mini-mizes M DL ( B j D ) = 1 is the num ber of parameters in the net work. In both these approac hes, the parameters of eac h candidate net work are set by maxim um likeliho od, as in the kno wn-structure case. Finally , the full Bayesian ap-proac h (Co oper &amp; Hersk ovits, 1992; Hec kerman et al., 1995) maximizes the Bayesian Dirichlet (BD) score
P ( B S ; D ) = P ( B S ) P ( D j B S ) where B S is the structure of net work B , () is the gamma function, q i is the num ber of states of the Cartesian pro duct of x i 's paren ts, and r i is the num-ber of states of x i . P ( B S ) is the prior probabilit y of the structure, whic h Hec kerman et al. set to an exp o-nen tially decreasing function of the num ber of di ering arcs between B S and the initial (prior) net work. Eac h multinomial distribution for x i given a state of its par-ents has an asso ciated Diric hlet prior distribution with eters can be though t of as equiv alen t to seeing n 0 ijk occurrences of the corresp onding states in adv ance of the training examples. In this approac h, the net work parameters are not set to speci c values; rather, their entire posterior distribution is implicitly main tained and used. The BD score is the result of integrating over this distribution. See Hec kerman (1999) for a more detailed introduction to learning Bayesian net-works. 2.2. Bayesian Net work Classi ers The goal of classi cation is to correctly predict the value of a designated discrete class variable y = x v given a vector of predictors or attributes ( x 1 ; : : : ; x If the performance measure is accuracy (i.e., the frac-tion of correct predictions made on a test sample), the optimal prediction for ( x 1 ; : : : ; x v 1 ) is the class that maximizes P ( y j x 1 ; : : : ; x v 1 ) (Duda &amp; Hart, 1973). If we have a Bayesian net work for ( x 1 ; : : : ; x v ), these probabilities can be computed by inference over it. In particular, the naive Bayes classi er is a Bayesian net work where the class has no paren ts and eac h at-tribute has the class as its sole paren t. Friedman et al.'s (1997) TAN algorithm uses a varian t of the Cho w and Liu (1968) metho d to pro duce a net work where eac h variable has one other paren t in addition to the class. More generally , a Bayesian net work learned us-ing any of the metho ds describ ed above can be used as a classi er. All of these are gener ative mo dels in the sense that they are learned by maximizing the log likeliho od of the entire data being generated by the mo del, LL ( B j D ), or a related function. However, for classi cation purp oses only the conditional log likeli-hood CLL ( B j D ) of the class given the attributes is relev ant, where Notice LL ( B j D ) = CLL ( B j D ) + P n d =1 log P B ( x : : : ; x d;v 1 ). Maximizing LL ( B j D ) can lead to underp erforming classi ers, particularly since in practice the con tribution of CLL ( B j D ) is likely to be swamp ed by the generally much larger (in absolute value) log P B ( x d; 1 ; : : : ; x d;v 1 ) term. A better approac h would presumably be to use CLL ( B j D ) by itself as the objectiv e function. This would be a form of discriminative learning, be-cause it would focus on correctly discriminating between classes. The problem with this approac h is that, unlik e LL ( B j D ) (Equation 1), CLL ( B j D ) = P does not decomp ose into a separate term for eac h variable, and as a result there is no kno wn closed form for the optimal parameter estimates. When the structure is kno wn, locally optimal estimates can be found by a numeric metho d suc h as conjugate gradien t with line searc h (Press et al., 1992), and this is what Greiner and Zhou's (2002) ELR algorithm does. When the structure is unkno wn, a new gradien t descen t is required for eac h candidate net work at eac h searc h step. The computational cost of this is presumably prohibitiv e. In this pap er we verify this, and prop ose and evaluate a simple alternativ e: using conditional likeliho od to guide structure searc h, while appro ximating parameters by their maxim um likeliho od estimates. We now introduce BNC, an algorithm for learning the structure of a Bayesian net work classi er by maximiz-ing conditional likeliho od. BNC is similar to the hill-clim bing algorithm of Hec kerman et al. (1995), except that it uses the conditional log likeliho od of the class as the primary objectiv e function. BNC starts from an empt y net work, and at eac h step considers adding eac h possible new arc (i.e., all those that do not create cy-cles) and deleting or rev ersing eac h curren t arc. BNC pre-discretizes con tinuous values and ignores missing values in the same way that TAN (Friedman et al., 1997) does.
 We consider two versions of BNC. The rst, BNC-n P , avoids over tting by limiting its net works to a maxi-mum of n paren ts per variable. Parameters in eac h net work are set to their maxim um likeliho od values. (In con trast, full optimization of both structure and parameters, describ ed more fully in Section 4, would set the parameters to their maxim um conditional like-liho od values.) The net work is then scored using the conditional log likeliho od CLL ( B j D ) (Equation 3). The rationale for this approac h is that computing max-imum likeliho od parameter estimates is extremely fast, and, for an optimal structure, they are asymptotically equiv alen t to the maxim um conditional likeliho od ones (Friedman et al., 1997).
 The second version, BNC-MDL , is the same as BNC-n P, except that instead of limiting the num ber of par-ents, the scoring function CM DL ( B j D ) = 1 CLL ( S j D ) is used, where m is the num ber of param-eters in the net work and n is the training set size. The goal of BNC is to pro duce accurate class prob-abilit y estimates. If correct class predictions are all that is required, a Bayesian net work classi er could in principle be learned simply by using the training-set accuracy as the objectiv e function (together with some over tting avoidance scheme). While trying this is an interesting item for future work, we note that even in this case the conditional likeliho od may be preferable, because it is a more informativ e and more smo othly-varying measure, poten tially leading to an easier opti-mization problem. To gauge the performance of our BNC varian ts for the task of classi cation, we evaluated them and sev-eral alternativ e learning metho ds on the 25 benc hmark datasets used by Friedman et al. (1997). These in-clude 23 datasets from the UCI rep ository (Blak e &amp; Merz, 2000) and 2 extra datasets dev elop ed by Ko-havi and John (1997) for feature selection exp erimen ts. We used the same exp erimen tal pro cedure as Friedman et al., choosing either 5-fold cross-v alidation or hold-out testing for eac h domain by the same criteria. In all cases, we measured both the conditional log likeliho od (CLL) of the test data given the learned mo del, and the mo del's classi cation error rate (ERR). 4.1. Full Structure and Parameter We rst compared BNC to optimizing both struc-ture and parameters for conditional log likeliho od, to the exten t possible given the computational resources available. In this version, the parameters of eac h al-ternativ e net work at eac h searc h step are set to their (locally) maxim um conditional likeliho od values by the conjugate gradien t metho d with line searc h, as in Greiner and Zhou (2002). The parameters are rst ini-tialized to their maxim um likeliho od values, and the best stopping point for the gradien t descen t is then found by two-fold cross-v alidation (this is necessary to prev ent over tting of the parameters).
 Clearly , examining hundreds of net works per searc h step and performing gradien t descen t over all of the training data for eac h structure is impractical. For the smallest datasets in our exp erimen ts, full optimization was possible but took two orders of magnitude longer to complete than BNC (whic h requires sev eral min utes on a 1 GHz Pentium 3 pro cessor). To exp erimen t on larger datasets, we introduced two variations. First, we used only 200 randomly-c hosen samples for gra-dien t descen t, keeping the full training set to score net works and to t the nal parameters. This mak es the running time of gradien t descen t indep enden t of the training set size, while degrading the optimiza-tion only by increasing the variance of the objectiv e function. Second, we imp osed a maxim um num ber of line searc hes for the gradien t descen t on eac h net work. This tak es adv antage of the fact that the greatest im-pro vemen t in the objectiv e function is often obtained in the rst few iterations.
 These alterations sped up the full optimization ap-proac h to the point where small domains (i.e., those with few attributes) suc h as core, diab etes, and shuttle-small, could be completed within an hour or two. Medium-sized domains (e.g., heart, clev e, breast) required on the order of ve hours run time. One of the larger domains, segmen t, was tried on a 2.4 GHz ma-chine and was still running two days later when we terminated the pro cess. The completed exp erimen ts rev ealed that neither the full optimization nor the ac-celerated versions pro duced better results than those obtained by BNC. We thus conclude that, at least for small to medium datasets, full optimization is unlik ely to be worth the computational cost. 4.2. Structure Optimization We compared BNC with the follo wing algorithms: Mean negativ e-CLL and ERR data from BNC and the comp eting algorithms are presen ted in Tables 1 and 2, resp ectiv ely. One-standard-deviation bars for those data points app ear in the corresp onding graphs in Fig-ures 1 and 2. In both gures, points above the y = x diagonal are datasets for whic h BNC achiev ed better results than the comp eting algorithm. The signi cance values we rep ort for these results below were obtained using the Wilco xon paired-sample signed-ranks test. We also compared di eren t versions of BNC. As with ML-2P , we found that increasing the maxim um num-ber of paren ts per node in BNC-n P did not yield im-pro ved results beyond the two-paren t version. BNC-2P outp erformed BNC-MDL at the p &lt; : 01 and p &lt; : 001 signi cance levels for CLL and ERR, re-spectiv ely. A similar comparison sho wed that ML-2P likewise outp erforms ML-MDL in both measures. These observ ations suggest that the MDL penalt y is not well suited to the task of training Bayesian net-work classi ers, a conclusion consisten t with previous results (e.g., Chic kering and Hec kerman (1997), Allen and Greiner (2000)) sho wing that MDL does not per-form well for learning Bayesian net works. We thus used BNC-2P as the default version for comparison with other algorithms.
 We see in Figures 1a and 2a that BNC-2P outp er-forms NB on both classi cation error ( p &lt; : 06) and CLL ( p &lt; : 001). BNC-2P and TAN have similar er-ror rates (Figure 2b), but BNC outp erforms TAN in conditional likeliho od with signi cance p &lt; : 06 (Fig-ure 1b). On these 25 datasets, NB performs about as well as C4.5 in terms of classi cation error. TAN, originally suggested as a sup erior alternativ e to C4.5 (Friedman et al., 1997), attained a signi cance level of only p &lt; : 24 against it in our exp erimen ts, whereas BNC-2P fared sligh tly better with p &lt; : 20. HGC, Hec kerman et al.'s (1995) algorithm for learning un-restricted Bayesian net works, falls well below BNC in both the CLL and ERR metrics ( p &lt; : 015, p &lt; : 006) (Figures 1c and 2c). As men tioned above, ML-MDL substan tially underp erforms other algorithms. ML-2P (Figures 1d and 2d) is comp etitiv e with BNC-2P in terms of conditional log likeliho od, but BNC-2P out-performs it in classi cation accuracy in a large ma jor-ity of the domains ( p &lt; : 069).
 Finally , in Figures 1e,f and 2e,f we see that BNC-2P outp erformed NB-ELR and TAN-ELR in CLL, while obtaining similar error rates. We also applied ELR op-timization to the net works pro duced by BNC-2P . In all domains, ELR either con verged to the original max-imum likeliho od parameters or to worse-p erforming ones. This is consisten t with Greiner and Zhou's (2002) observ ation that parameter optimization is less helpful for structures that are closer to the \true" one, and further supp orts the use of BNC. An additional adv antage of BNC relativ e to ELR is that optimiz-ing structure is arguably preferable to optimizing pa-rameters, in that the net works it pro duces may give a human view er more insigh t into the domain. Maxi-mum likeliho od parameters are also much more inter-pretable than conditional likeliho od ones.
 We susp ect that there may be further performance dif-ferences between BNC and other algorithms that the benc hmark datasets used in these exp erimen ts are too small and simple to elicit. For example, the adv an-tage of discriminativ e over generativ e training should be larger in domains with a large num ber of attributes, and/or when the dep endency structure of the domain is too complex to capture empirically given the avail-able data. Conducting exp erimen ts in suc h domains is thus a key area for future researc h. The issue of discriminativ e vs. generativ e learning has receiv ed considerable atten tion in recen t years (e.g., Rubinstein and Hastie (1997)). It is now well un-dersto od that, although in the in nite-data limit an unrestricted maxim um-lik eliho od learner is necessar-ily optimal, at nite sample sizes discriminativ e train-ing is often preferable (Vapnik, 1998). As a result, there has been considerable work dev eloping discrim-inativ e learning algorithms for probabilistic represen-tations (e.g., the conditional EM algorithm (Jebara &amp; Pentland, 1999) and maxim um entrop y discrimination (Jaakk ola et al., 1999)). This pap er falls into this line of researc h.
 Ng and Jordan (2002) sho w that, for very small sam-ple sizes, generativ ely trained naiv e Bayes classi ers can outp erform discriminativ ely trained ones. This is consisten t with the fact that, for the same repre-sen tation, discriminativ e training has lower bias and higher variance than generativ e training, and the vari-ance term dominates at small sample sizes (Domingos &amp; Pazzani, 1997; Friedman, 1997). For the dataset sizes typically found in practice, however, their results, ours, and those of Greiner and Zhou (2002) all supp ort the choice of discriminativ e training.
 Discriminativ ely-trained naiv e Bayes classi ers are kno wn in the statistical literature as logistic regres-sion (Agresti, 1990). Our work can thus be view ed as an extension of the latter to include structure learning. Discriminativ e learning of Bayesian net work structure has receiv ed some atten tion in the speech recognition comm unit y (Bilmes et al., 2001). Keogh and Pazzani (1999) augmen ted naiv e Bayes by adding at most one paren t to eac h node, with classi cation accuracy as the objectiv e function. This pap er sho wed that e ectiv e Bayesian net work classi ers can be learned by directly searc hing for the structure that optimizes the conditional likeliho od of the class variable. In exp erimen tal tests, BNC, the resulting classi er, pro duced better class probabilit y estimates than maxim um-lik eliho od approac hes suc h as naiv e Bayes and TAN.
 Directions for future work include: studying fur-ther heuristic appro ximations to full optimization of the conditional likeliho od; dev eloping impro ved meth-ods for avoiding over tting in discriminativ ely-trained Bayesian net works; extending BNC to handle missing data, undiscretized con tinuous variables, etc.; apply-ing BNC to a wider variet y of datasets; and extending our treatmen t to maximizing the conditional likeliho od of an arbitrary query distribution (i.e., to problems where the variables being queried can vary from ex-ample to example, instead of alw ays having a single designated class variable).
 Ackno wledgemen ts: We are grateful to Jayant Mad-
