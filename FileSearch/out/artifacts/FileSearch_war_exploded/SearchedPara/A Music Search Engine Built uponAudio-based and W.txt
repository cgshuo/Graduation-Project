 An approach is presented to automatically build a search engine for large-scale music collections that can be queried through natural language. While existing approaches de-pend on explicit manual annotations and meta-data assigned to the individual audio pieces, we automatically derive de-scriptions by making use of methods from Web Retrieval and Music Information Retrieval. Based on the ID3 tags of a collection of mp3 files, we retrieve relevant Web pages via Google queries and use the contents of these pages to charac-terize the music pieces and represent them by term vectors . By incorporating complementary information about acous-tic similarity we are able to both reduce the dimensionality of the vector space and improve the performance of retrieval, i.e. the quality of the results. Furthermore, the usage of audio similarity allows us to also characterize audio pieces when there is no associated information found on the Web. Categories and Subject Descriptors: H.3.3 [ Information Systems ]: Information Storage and Retrieval; H.5.5 [ Information Systems ]: Information In-terfaces and Presentation  X  Sound and Music Computing General Terms: Algorithms Keywords: Music search engine, Music Information Re-trieval, music similarity, context-based retrieval, cross-medi a retrieval
Over the past years, using text-based search engines has become the  X  X atural X  way to find and access all types of multimedia content. While there exist approaches to auto-matically derive and assign semantic, natural language de-scriptors for images, videos, and  X  of course  X  text, the broad field of (popular) music has not drawn that much of atten-tion. Basically all existing music search systems, e.g. those offered by commercial music resellers, make use of manu-ally assigned subjective meta-information like genre or style (in addition to objective meta-data like artist, album name, track name, or year) to index the underlying music collec-tion. Thus, the person issuing the query (i.e. the potential customer) must already have a very precise conception of the expected result set. The intrinsic problem of these sys-tems is the limitation to a rather small set of meta-data, whereas the musical, or more general, the cultural context of music pieces is not captured.

However, as digital catalogs rapidly become larger and more inconvenient and inefficient to access, the need for more sophisticated methods that enable intuitive searching inside large music collections increases. For example, in-stead of just finding tracks that are labeled as rock , it would be valuable to be able to formulate a query like  X  X ock with great riffs X  to emphazise the importance of energetic gui-tar phrases. Another query could be  X  X elaxing music X  that should return relaxing pieces regardless of which  X  X enre X  they are assigned to. Clearly, music resellers with very large music databases or music information systems could benefit from such an engine, as it provides access to their catalog in the most common and most accepted manner.

Recent developments in music information systems re-spond to the fact that new and possibly unconventional ap-proaches are necessary to support the user in finding de-sired music. Most of them make use of content-based anal-ysis of the audio files (e.g. [12, 6]) or use collaborative rec-ommendations to point the users to music they might like there are techniques that incorporate information from dif-ferent sources to build interactive interfaces [15, 21].
In this paper, we present first steps toward the challeng-ing task of automatically building a search system that is capable of finding music that satisfies arbitrary natural lan-guage queries. For each track in a collection of mp3 files, we retrieve a set of relevant Web pages via Google. This enables us to represent music pieces in a traditional term vector space. Additionally, we make use of a state-of-the-art audio similarity measure. Thus, we combine information about the context of music with information about the con-tent . With the integration of acoustic similarity, we are able to (a) reduce the dimensionality of the feature space, (b) describe music pieces with no (or little) related information present on the Web, and (c) improve the quality of the re-trieval results. The evaluation carried out on a collection consisting of more than 12,000 music pieces gives strong in-dication that a natural language search engine for music collections is in reach. http://www.last.fm
Several methods to retrieve music from large databases have been proposed. Most of these approaches are based on query-by-example methods (e.g. [17]). Thus, the query must consist of a piece of information that has the same representation as the records in the database. For exam-ple, in Query-by-Humming/Singing (QBHS) systems [11] the user has to sing or hum a part of the searched piece into a microphone. In most cases, these systems operate on a symbolic representation of music (i.e. MIDI). Other query-by-example systems use a small, low quality recorded portion of a music piece as input, identify the piece, and re-turn the associated meta-data (artist, title, etc.). Typically, such services are accessible via cellular phone (e.g. from the
An even more challenging task is to design systems that enable cross-media retrieval. In our case, systems that al-low queries consisting of arbitrary natural language text, e.g. descriptions of sound, mood, or cultural events, and return music pieces that are semantically related to this query, are of interest. Unfortunately, the number of systems enabling its users to perform such queries is very little. The most elaborate approach so far has been presented by Baumann et al. [5]. Their system is supported by a semantic ontol-ogy which integrates information about artist, genre, year, lyrics, and automatically extracted acoustic properties like loudness, tempo, and instrumentation and defines relations between these concepts. Beside the correct extraction of the features, also the mapping of the query to the concepts in the ontology has to be accomplished. In the end, the sys-tem allows for semantic queries like  X  X omething fast from... X  or  X  X omething new from... X  . Also phonetic misspellings are corrected automatically.
 In [8], Celma et al. present the music search engine Search a set of manually defined  X  X udio blogs X , which can be ac-cessed via RSS links. In these blogs, the authors explain and describe music pieces and make them available for download (whether legally or illegally depends on the blog). Hence, the available textual information that refers to the music, to-gether with the meta-data of the files, can be used to match text queries to actual music pieces. Furthermore, for all re-turned pieces, acoustically similar pieces can be discovered by means of content-based audio similarity.

Another system that opts to enhance music search with additional semantic information is Squiggle [7]. In this frame-work, queries are matched against meta-data and also fur-ther evaluated by a word sense disambiguation component that proposes related queries. For example, a query for  X  X hcp X  results in zero hits, but suggests to search for the band  X  X ed Hot Chili Peppers X ; searching for  X  X ock DJ X  proposes the song by  X  X obbie Williams X , the genre  X  X ock X , as well as other artists (all  X  X Js X ). The underlying se-mantic relations are taken from the freely available com-although semantic relations are integrated, the system de-pends on explicit knowledge which is in fact a more extensive set of manually annotated meta-data. http://www.shazam.com http://www.searchsounds.net http://www.musicmoz.org http://www.musicbrainz.org
A system that is not restricted to a pre-defined set of meta-data is Last.fm. Last.fm integrates into music player software and keeps track of each user X  X  listening habits. Based on the collected data, similar artists or tracks can be recom-mended. Additionally, users can assign tags to the tracks in their collection. These tags provide a valuable source of information on how people perceive and describe music. A drawback of the system is that most tags are highly incon-sistent and noisy. We discuss this issue in more detail in Section 4 where we use part of the Last.fm data to evaluate our own approach.

Beside music information systems that deal solely with popular music, there exist a number of search engines that use specialized (focused) crawlers to find all types of sounds on the Web. The traced audio files are indexed using con-textual information extracted from the text surrounding the links to the files. Examples of such search engines are Arooo-
Finally, we briefly review approaches that aim to bridge the semantic gap for music by extracting information from the pure audio signal and assigning some sort of meta-data. In [15], music landscapes created from audio similarity infor-mation are labeled with music related terms extracted from the Web. [21] uses similar techniques to arrange a music collection around a circle for easy access. In [24], low-level characteristics of audio signals are mapped to semantic con-cepts to learn the  X  X eaning X  of certain acoustic properties.
In contrast, in this paper, we directly derive culturally as-sociated descriptors for audio files by extracting the desired information from related Web pages. Additionally, we make use of audio-based similarity to improve this technique. As a result, we obtain a representation of each music piece in a term vector space. To search for music pieces, queries are also transformed to this vector space where distances to all music pieces can be calculated.
In this section, we describe our technique to build a natu-ral language search engine for music. After a preprocessing step, related Web pages for each track are retrieved. Sec-ond, we compute acoustic similarity directly from the audio files which we use in the following steps to reduce the di-mensionality of the used vector space and modify the vector representations toward acoustically similar pieces. Finally, we describe how queries are processed in order to find the most adequate pieces in the collection.
To obtain descriptors for the tracks in an mp3 collection, we make use of the information found in the ID3 tags of the files. More precisely, we extract the values of the fields  X  X rtist X ,  X  X lbum X , and  X  X itle X . Very commonly, additional information is included in these tags, e.g. tracks that are a collaboration of two artists often contain the second artist in the title (indicated by feat. , and , with , etc.) or both artists are mentioned in the artist field. Also other meta-information (e.g. to indicate live versions or remixes) can be found. To avoid too constrained queries in the next step, this extra information is removed. One drawback of this preprocessing step is that it affects also artists like  X  X ke &amp; Tina Turner X , who are afterward represented only by  X  X ke X . http://www.findsounds.com mean 5,042,732 39,891 15,844 lower quartile 50,900 0 14 median 386,000 12 167 upper quartile 1,260,000 461 845 total number 1,200 2,073 12,601 | count = 0 | 8 (0.7%) 860 (41%) 2,430 (19%) | count  X  100 | 22 (1.8%) 1,360 (65%) 5,533 (44%) Table 1: Statistics of estimated page counts from Google for our evaluation collection.

Based on the meta-tags, also pieces that are likely to con-tain only speech are ignored (e.g. in Rap music this is often indicated by the word Skit ) as well as all tracks named Intro or Outro and tracks with a duration below 1 minute. Fur-thermore, all duplicates of tracks are excluded from the next steps to avoid unnecessary similarity computations and re-dundancies (different versions of tracks could be displayed, for example, as alternatives in the retrieval results). Among all duplicates, the version containing no meta-information like live or remix is chosen for further processing. In future work, we will also elaborate methods to deal with remixes, i.e. the fact that for remixes in general the remixing artist is more important than the original artist. The primary source of information for our approach is the Web. Previous work that uses Web data for Music Informa-tion Retrieval [25, 14] operates only on the artist level. The main argument for this limitation is the claim that there is not enough specific information present on the Web for each individual track. Table 1 shows some statistics on the esti-mated numbers of available Web pages returned by Google. It can be seen that the number of available pages decreases drastically when searching for album or title instead of artist alone. (Please note that the collection contains many tracks from samplers and compilations, which explains the low page counts for album related queries.) For our task, the number of queries that return zero or below 100 pages are of particu-lar interest. We can see that very low page counts occur only sporadically when searching for artist related pages (around 1%). On the track level for 19% no information is found at all. Thus, to gather as much track specific information as possible while preserving a high number of available web pages, we decided to combine the results of three queries issued to Google for each track in the collection: 1.  X  artist  X  music 2.  X  artist  X   X  album  X  music review 3.  X  artist  X   X  title  X  music review -lyrics
The first query is intended to provide a stable basis of re-lated documents. With the second query, we try to find re-views of the album on which the track was published, cf. [25]. The third query targets very specific pages and aims at pre-venting lyrics to be included. We plan to address song lyrics separately in future work. For each query, at most 100 of the top-ranked Web pages are retrieved and joined into a single
For each music piece m and each term t appearing in the retrieved pages, we count the number of occurrences tf tm (term frequency) of term t in documents related to m . Additionally, we count df tm the number of pages related to m the term occurred in (document frequency). All terms with df tm  X  2 are removed from m  X  X  term set. Finally, we count mpf t the number of music pieces that contain term t in their set (music piece frequency). For pragmatic reasons, we further remove all terms that co-occur with less than 0 . 1% of all music pieces, resulting in a vector space having about 78,000 dimensions. To calculate the weight w ( t, m ) of a term t for music piece m , we use a straight forward modification of the well-established term frequency  X  inverse document frequency ( tf  X  idf ) function [23]: where N is the number of music pieces in the collection. As can be seen, we treat all Web pages related to a music piece as one large document. The resulting term weight vectors are normalized such that the length of the vector equals 1 (Cosine normalization). This removes the influence of the length of the retrieved Web pages as well as the different numbers of retrieved pages per track.
To complement the extracted Web-based features with perceived acoustical similarity of the associated music, we follow a well-established procedure, e.g. [20]: For each au-dio track, Mel Frequency Cepstral Coefficients (MFCCs) are computed on short-time audio segments (called frames ) to describe the spectral envelope of each frame and model thus timbral properties. We use the definition given in [2]: As proposed, we calculate 19 MFCCs on each frame. Ignor-ing the temporal order of frames, each song is then going to be represented as a Gaussian Mixture Model (GMM) of the distribution of MFCCs [3]. According to Mandel and El-lis [18], a Single Gaussian Model with full covariance matrix is sufficient for representation, which facilitates computation and comparison of the models. Instead of estimating simi-larity of GMMs via Monte Carlo sampling, a symmetrised Kullback-Leibler divergence can be calculated on the means and covariance matrices [18].

However, as described in [1, 22], the Kullback-Leibler di-vergence has some undesirable properties. For example, it can be observed that some particular pieces, so called hubs, are frequently  X  X imilar X  (i.e. have a small distance) to many other pieces in the collection without sounding similar. On the other side, some pieces are never similar to others. Fur-thermore, the Kullback-Leibler divergence does not fulfill the triangle inequality. as we use information from Last.fm/Audioscrobbler later to measure the quality of our approach.
To cope with these issues imposed by a distance measure that is no metric, we developed a simple rank-based correc-tion called Proximity Verification [22]. As a consequence, all further steps presented here will be based on the ranking information of the audio similarity measure only, i.e. it will only be of interest whether a piece is most similar, second similar, third similar, etc. to a piece under consideration and not to which numerical extent the two pieces are considered to be similar.
To reduce the number of terms and remove irrelevant di-mensions from the term vector model, we use the  X  2 test which is a standard term selection approach in text classifi-cation, e.g. [26]. Usually, the  X  2 test needs some information on class assignments to measure the independence of a term t from a class c . In our case, no class information (e.g. genre) is available. Hence, we make use of the derived audio similarity. For each track, we define a 2-class term selec-tion problem and use the  X  2 test to find those terms that discriminate s , the group of the most similar tracks (which we assume to be homogeneous according to the audio sim-ilarity measure), from d , the group of the most dissimilar tracks (which is not necessarily homogeneous). We define s and d to comprise 100 tracks each, to have a solid number of documents for the  X  2 test. For each track, we calculate where A is the number of documents in s which contain t , B the number of documents in d which contain t , C the number of documents in s without t , D the number of docu-ments in d without t , and N is the total number of examined documents. The number of documents refers to the docu-ment frequency from Section 3.2. The n terms with highest  X  2 ( t, s ) values that occur more frequently in s than in d are selected because they are least independent from s and are thus best suited to discriminate the most similar tracks from the most dissimilar. The selected terms for each track are then joined into a global list. Throughout the experiments we use n  X  50 , 100 , 150 resulting in reduced features spaces with dimensionality of 4 , 679, 6 , 975, and 8 , 866, respectively.
The applicability of other methods for dimensionality re-duction, in particular techniques like probabilistic Latent Se-mantic Indexing [13], will be assessed in future work. Con-sidering the size of the evaluation collection and the dimen-sionality of the unpruned term vector space, for now, we decided to design and implement a method involving only moderate computational expenses, i.e. linear complexity in the number of tracks.
To further exploit the additional information of the au-dio similarity measure, we use acoustically similar pieces to adapt the term vector representations of pieces. This is par-ticularly necessary for tracks where no related information could be retrieved from the Web. For all other tracks, the intention is to enforce those characteristics, i.e. those dimen-sions, that are typical among acoustically similar tracks. To accomplish this, we perform a simple smoothing of the term vector weights depending on the similarity rank. The first vector adaptation approach uses a Gauss weighting. Modi-fied weights of term t for music piece m are defined as where sim i ( m ) is the i th most similar track to m accord-ing to audio similarity and sim 0 ( m ) is m itself. The second vector adaptation we examine is a simple linear weighting:
After adapting the term weights, vectors are again Cosine normalized. In Section 4, we will study the impact of re-weighting vectors based on the 5 and 10 nearest neighbors on the retrieval process.
After constructing a term vector representation for each piece in the music collection, we need a method to find those tracks that are most similar to a natural language query. The most simple approach would be to map the terms from the query to the respective dimensions and to calculate a score for each track over all query terms. While this could be performed very quickly, the problem is that the possible query terms would be restricted to the vocabulary of the fea-ture space. Hence, to obtain a vector space representation also for queries that consist of terms not in the vocabulary, we extend queries to the music search engine by the word music and send them to Google. 9 For reasons of efficiency and performance, only the 10 top-most Web pages are down-loaded. Using these pages, a query vector is constructed in the feature space that can now be compared to the music pieces in the collection by calculating Euclidean distances on the Cosine normalized vectors. Based on the distances, a relevance ranking can be obtained which forms the response to the query.

However, while this approach allows for a virtually unlim-ited number of queries, it is again dependent on the avail-ability of Google, i.e. to query a local database, the Internet must be accessible. Moreover, the response time of the sys-tem increases by the time necessary to perform the on-line retrieval. Furthermore, it is questionable whether only 10 pages are sufficient for proper query expansion. We will address these issues in future work, for which we plan to build an index of all Web documents retrieved during the Web-feature extraction. Query vectors could then be con-structed quickly based on this off-line index. Additionally, the comparison of the query vector to all feature vectors in the collection is very inefficient. This could be improved by implementing more sophisticated indexing and search algo-rithms, e.g. [10].
In this section, the performance of our music search engine approach is evaluated. For testing, we compiled a large mu-sic collection of mp3 files from the personal collections of the authors. We also added about 1,000 tracks from the on-line non-commercial use  X  to include tracks from not that well-known artists. In total, the complete collection consists of 9 During evaluation, we also add the constraint -site:last.fm . http://www.magnatune.com 14,342 mp3 files from which 12,601 remain after the prepro-cessing step (Section 3.1). The test set comprises tracks by 1,200 artists and 2,073 albums (with samplers and compila-tions counted as different albums for each artist). Thus, to obtain Web-based features for this collection, 15,874 differ-ent queries to Google are necessary.
Evaluating the quality of a retrieval system for music is a non-trivial task. The most common approach is to evaluate the results against genre information. Beside the labor in-tensive task of manually defining genres for a collection of more than 12,000 pieces, this method has several drawbacks, cf. [19]: First, in most cases musical pieces can not be as-signed to just one genre. Second, judgments whether a piece belongs to a genre or not are highly subjective. Finally, and most important, it is unclear how to match arbitrary queries to genre information. Since it is our goal to develop a natu-ral language search engine for music, we have to evaluate it on  X  X eal-world X  queries. Hence, we need a source for phrases which are used by people to describe music and which are likely to be used when searching for music. As mentioned be-fore, we utilize the track specific tag information provided by provides Web Services to access Last.fm data in a standard-ized manner. From our collection of 12,601 tracks, we find 7,148 to have Audioscrobbler tags assigned (56.7%). For 4,903 of the remaining tracks, tag information about the corresponding artist is available. Thus, in sum we can use 12,051 tracks for performance measurement. The remaining 550 tracks are ignored during the evaluation. Taking a look at the associated tags reveals that they are highly inconsis-tent and noisy. Many users use personalized identifiers to track their music collection, e.g. by using their nickname as tags for tracks they own. Frequently, tags contain typos, are redundant (e.g. HipHop -Hip Hop, or Hardrock -Hard Rock -Harder Rock), or simply not useful for our purpose ( X  X avorite artist X ,  X  X istagged X  etc.). However, for lack of a real golden standard, using Last.fm tags is still the best choice. In total, we could observe 43,472 different tags for our collection. Performing evaluation on this complete set is neither reasonable nor feasible. Instead, we make use of the most frequently used tags to describe tracks. From this list we remove useless entries like  X  X usic X ,  X  X isc X ,  X  X lbums i own X ,  X  X est1 X  etc., entries starting with  X  X y X  or  X  X avorite X , and one entry that never occurs in our collection. The re-maining 227 tags are used as test queries for our system. All music pieces labeled with the query tag are considered relevant wrt. the query. Table 2 shows the most and least frequent tags in our collection.
Using the Audioscrobbler ground truth, we evaluate the performance of the system with different parameters. Our goal is to study the impacts of the dimensionality of the feature space (cf. Section 3.4) and the vector modifications based on the audio similarity (cf. Section 3.5) on the re-trieval quality. In the following, we use the notation  X  2 /n to describe the strategy of selecting n most discriminating http://www.audioscrobbler.net http://ws.audioscrobbler.com/1.0/tag/toptags.xml Table 2: Most and least frequent Audioscrobbler top tags in our collection. Figure 1: Precision at 11 standard recall levels (av-erage over all 227 test queries) for different term selection settings on Web features without audio-based adaptations. terms per track in the term selection step. In the experi-ments we use n = 50 , 100 , 150, resulting in reduced features spaces of dimensionality 4 , 679, 6 , 975, and 8 , 866, respec-tively. Furthermore, the impact of gauss and linear weight-ing is examined (for both weightings, we evaluate usage of 5 and 10 neighbors).

To measure the quality of the obtained rankings, we cal-culate standard evaluation measures for retrieval systems, cf. [4]. Figure 1 shows the Precision at 11 standard recall levels for the modified tf  X  idf vectors according to Equa-tion 1 (without audio-based re-weighting) for the different  X  2 /n settings. This measure is useful to observe precision over the course of a ranking. Since we evaluate the system using a set of 227 queries, we calculate the average of the precision values at each recall level after interpolating to the 11 standard values. It can be seen that audio-based term selection has a very positive impact on the retrieval. The setting  X  2 / 50 yields best results, while term vectors without dimensionality reduction perform worst. In Figure 1, one can also see the baseline of all experiments, which has been empirically determined by repeated evaluation of random permutations of the music collection. All results obtained with our approach are far above the baseline.

To give stronger evidence for the effects of the vector space pruning, we also perform statistical significance tests. Since the precision values obtained for the different queries are not normally distributed, we have to apply a non-parametric test. To compare all different settings  X  X imultaneously X , we use Friedman X  X  test (pairwise significance tests would drasti-cally increase the probability of an alpha-error). For lack of a test that takes the development of precision values over the different recall levels into account, we perform a Friedman test at each separate recall level (with attached post-hoc tests; significance level = 0.05). At recall levels 0.0 to 0.4, the  X  2 / 50 setting is significantly better than both  X  2 / 150 and no  X  2 , at recall levels 0.1 to 0.3,  X  2 / 50 is even signif-icantly better than  X  2 / 100. For the remaining recall levels (0.5 to 1.0), no reasonable significant difference between the settings can be found.

While audio similarity-based term selection has an evident positive impact on the retrieval quality, the impact of audio-based vector re-weighting is only marginal. For reasons of lucidity, the precision values at 11 standard recall levels for tf  X  idf , gauss , and linear vectors (all  X  2 / 50) are shown in Table 3 instead of a graph (the curves would be too similar to gain insights). At the 0.0 recall level we can (theoretically) expect precision values around 0.6, at the 0.1 level, precision drops to about 0.4. A deeper look into the results of individ-ual queries reveals that especially sparsely distributed tags perform bad (cf. Table 2). The fact that queries with terms like  X  X orean X  result in very low precision is no surprise, since in reality there are no korean music pieces in the collection (although the Audioscrobbler tags suggest this).

Again, we perform Friedman tests to check for significant differences between the various re-weighting strategies. The mean values in Table 3 conceal the relations of the underly-ing distributions to some extent. Using the Friedman test, it can be seen that tf  X  idf performs significantly worse than gauss n =5 , gauss n =10 , and linear n =5 at the 0.0 recall level. At the 0.1 and 0.3 level, the tf  X  idf values are significantly below gauss n =5 , gauss n =10 , and linear n =10 , at 0.2, tf  X  idf is below all others. Between recall levels 0.4 and 0.8 there is no significant difference between tf  X  idf and the gauss weighted approaches. In general, we can observe a positive impact of vector re-weighting on the precision at lower recall levels (which are more relevant in practice). However, the achieved improvements are rather small.

Furthermore, we average single value summaries over all queries. Although single value summaries are more mean-ingful for the individual queries, as pointed out in [4], we still include the results to document the general tendencies. The average precision at seen relevant documents indicates the ability of the different settings to retrieve relevant doc-uments quickly (Table 4). The respective values are calcu-lated by averaging the current precision values at each ob-served relevant document. A similar measure is R-precision (Table 5). It corresponds to the precision at the R th po-sition in the ranking, where R is the number of relevant documents for the query. Finally, we calculate the precision after 10 documents . Since returning 10 results is the default for nearly all search engines, we think it is valuable to ex-amine how many relevant music pieces can be expected  X  X t first sight X  (Table 6). As can be seen for the best settings, in average, every second piece among the first ten is relevant. Table 4: Average precision at seen relevant docu-ments for different settings in percent (average over all 227 test queries). Table 5: R-precision for different settings in percent (average over all 227 test queries). Table 6: Precision after 10 documents for differ-ent settings in percent (average over all 227 test queries).

Finally, we want to present some search results from our test collection. Table 7 shows the top 10 music pieces for the queries  X  X ock with great riffs X  ,  X  X unk X  , and  X  X elaxing music X  . For  X  X ock with great riffs X  , all relevant music pieces are from the same artist (other artists with great riffs fol-low in the ranking).  X  X asy X  queries with relatively clearly defined musical styles like  X  X unk X  work very well. A query like  X  X elaxing music X  is difficult to judge because it is in fact very subjective. However, the majority of the returned pieces is the  X  X pposite of exciting X  and would probably be considered to be  X  X elaxing X  by many people. Interestingly, 9 out of 10 results are published by the label Magnatune.
Beside finding adequate music for descriptive queries, the system offers some simple but useful features for free. Based on the related Web pages,  X  X emantic relations X  are often im-plicitly available for queries. For example, the query  X  X a-mon Albarn X  returns tracks from the band  X  X lur X , as well as from the band  X  X orillaz X  (both bands of Damon Al-barn). Similarly, searching for  X  X eorge Harrison X  returns also tracks from  X  X he Beatles X .
 10. Wolfmother  X  Witchcraft 10. Electric Frankenstein  X  Home of the brave 10. Solace  X  Circle Table 7: Retrieval results for three queries. Bold entries indicate relevant music pieces.
We presented a first attempt to create a search engine for large music collections that can be queried via natural language text input. One of the central challenges of our method is to assign semantically related information to in-dividual music pieces. We opt to accomplish this by find-ing relevant information on the Web. The extracted text based information is complemented by audio-based similar-ity, which allows us to improve the results of the retrieval by reducing the dimensionality of the feature space. In-formation about the acoustic similarity is also mandatory to describe music pieces for which no related pages can be found on the Web.

Estimating the quality of the presented approach is diffi-cult since there exists no related approach that could serve as reference. Nevertheless, compared to the baseline, the results we obtained during evaluation were very promising. Considering the fact that the used ground truth has severe drawbacks, results can be interpreted as a  X  X onservative X  estimation of the real performance. However, there is ample space for improvements. In future work, we will elaborate methods to construct Web-based feature vectors more effi-ciently. Furthermore, we believe that there is much more potential in integrating audio-based similarity, especially if improved audio similarity measures become available. Fu-ture enhancements will also comprise special treatment of terms appearing in the meta-tags of the mp3 files and the search for phrases in lyrics. Finally, we can state that we are confident that these first steps point into the right di-rection and that a natural language search engine for music is feasible. This research is supported by the Austrian Fonds zur F  X orderung der Wissenschaftlichen Forschung (FWF) under project number L112-N04 and the Vienna Science and Tech-nology Fund (WWTF), project CIO10  X  X nterfaces to Mu-sic X . The Austrian Research Institute for Artificial Intel-ligence is supported by the Austrian Federal Ministry for Education, Science, and Culture and by the Austrian Fed-eral Ministry for Transport, Innovation, and Technology. [1] J.-J. Aucouturier. Ten Experiments on the Modelling [2] J.-J. Aucouturier and F. Pachet. Music Similarity [3] J.-J. Aucouturier, F. Pachet, and M. Sandler.  X  X he [4] R. Baeza-Yates and B. Ribeiro-Neto. Modern [5] S. Baumann, A. Kl  X uter, and M. Norlien. Using [6] P. Cano, M. Koppenberger, N. Wack, et al. An [7] I. Celino, E. Della Valle, D. Cerizza, and A. Turati. [8] O. Celma, P. Cano, and P. Herrera. Search Sounds: [9] O. Celma, M. Ramirez, and P. Herrera. Foafing the [10] P. Ciaccia, M. Patella, and P. Zezula. M-tree: An [11] A. Ghias, J. Logan, D. Chamberlin, and B. C. Smith. [12] M. Goto and T. Goto. Musicream: New Music [13] T. Hofmann. Probabilistic Latent Semantic Indexing. [14] P. Knees, E. Pampalk, and G. Widmer. Artist [15] P. Knees, M. Schedl, T. Pohle, and G. Widmer. An [16] I. Knopke. Aroooga: An audio search engine for the [17] N. C. Maddage, H. Li, and M. S. Kankanhalli. Music [18] M. Mandel and D. Ellis. Song-Level Features and [19] F. Pachet and D. Cazaly. A Taxonomy of Musical [20] E. Pampalk. Computational Models of Music [21] E. Pampalk and M. Goto. Musicrainbow: A new user [22] T. Pohle, P. Knees, M. Schedl, and G. Widmer. [23] G. Salton and C. Buckley. Term-weighting approaches [24] B. Whitman. Learning the meaning of music . PhD [25] B. Whitman and S. Lawrence. Inferring Descriptions [26] Y. Yang and J. O. Pedersen. A comparative study on
