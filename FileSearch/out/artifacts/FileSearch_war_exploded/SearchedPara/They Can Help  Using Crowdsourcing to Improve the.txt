 One of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007),  X  X ver a bil-lion people speak English as their second or for-eign language. X  This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 2010a), all in the last five years. In this year X  X  ACL conference, there are four long papers devoted to this topic.

Despite the growing interest, two major factors encumber the growth of this subfield. First, the lack of consistent and appropriate score reporting is an issue. Most work reports results in the form of pre-cision and recall as measured against the judgment of a single human rater. This is problematic because most usage errors (such as those in article and prepo-sition usage) are a matter of degree rather than sim-ple rule violations such as number agreement. As a consequence, it is common for two native speakers to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. In fact, to our knowledge, no two systems developed by different groups have been compared directly within the field primarily because there is no common corpus or shared task X  X oth commonly found in other NLP areas such as machine transla-tion. 1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures.

The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine transla-tion (Callison-Burch, 2009; Zaidan and Callison-Burch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), au-tomated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught lan-guages (Irvine and Klementiev, 2010), fact min-ing (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others.

In particular, we make a significant contribution to the field by showing how to leverage crowdsourc-ing to both address the lack of appropriate evaluation metrics and to make system comparison easier. Our solution is general enough for, in the simplest case, intrinsically evaluating a single system on a single dataset and, more realistically, comparing two dif-ferent systems (from same or different groups). We consider the problem of detecting an extraneous preposition error , i.e., incorrectly using a preposi-tion where none is licensed. In the sentence  X  They came to outside  X , the preposition to is an extrane-ous error whereas in the sentence  X  X hey arrived to the town X  the preposition to is a confusion er-ror (cf. arrived in the town ). Most work on au-tomated correction of preposition errors, with the exception of Gamon (2010), addresses preposition confusion errors e.g., (Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010b). One reason is that in addition to the standard context-based features used to detect con-fusion errors, identifying extraneous prepositions also requires actual knowledge of when a preposi-tion can and cannot be used. Despite this lack of attention, extraneous prepositions account for a sig-nificant proportion X  X s much as 18% in essays by advanced English learners (Rozovskaya and Roth, 2010a) X  X f all preposition usage errors. 2.1 Data and Systems For the experiments in this paper, we chose a propri-etary corpus of about 500,000 essays written by ESL students for Test of English as a Foreign Language (TOEFL R  X  ). Despite being common ESL errors, preposition errors are still infrequent overall, with over 90% of prepositions being used correctly (Lea-cock et al., 2010; Rozovskaya and Roth, 2010a). Given this fact about error sparsity, we needed an ef-ficient method to extract a good number of error in-stances (for statistical reliability) from the large es-say corpus. We found all trigrams in our essays con-taining prepositions as the middle word (e.g., marry with her ) and then looked up the counts of each tri-gram and the corresponding bigram with the prepo-sition removed ( marry her ) in the Google Web1T 5-gram Corpus. If the trigram was unattested or had a count much lower than expected based on the bi-gram count, then we manually inspected the trigram to see whether it was actually an error. If it was, we extracted a sentence from the large essay corpus containing this erroneous trigram. Once we had ex-tracted 500 sentences containing extraneous prepo-sition error instances, we added 500 sentences con-taining correct instances of preposition usage. This yielded a corpus of 1000 sentences with a 50% error rate.

These sentences, with the target preposition high-lighted, were presented to 3 expert annotators who are native English speakers. They were asked to annotate the preposition usage instance as one of the following: extraneous ( Error ), not extraneous ( OK ) or too hard to decide ( Unknown ); the last cat-egory was needed for cases where the context was too messy to make a decision about the highlighted preposition. On average, the three experts had an agreement of 0.87 and a kappa of 0.75. For subse-quent analysis, we only use the classes Error and OK since Unknown was used extremely rarely and never by all 3 experts for the same sentence.
We used two different error detection systems to illustrate our evaluation methodology: 2  X  LM : A 4-gram language model trained on  X  PERC : An averaged Perceptron (Freund and Recently,we showed that Amazon Mechanical Turk (AMT) is a cheap and effective alternative to expert raters for annotating preposition errors (Tetreault et al., 2010b). In other current work, we have extended this pilot study to show that CrowdFlower, a crowd-sourcing service that allows for stronger quality con-trol on untrained human raters (henceforth, Turkers), is more reliable than AMT on three different error detection tasks (article errors, confused prepositions &amp; extraneous prepositions). To impose such quality control, one has to provide  X  X old X  instances, i.e., ex-amples with known correct judgments that are then used to root out any Turkers with low performance on these instances. For all three tasks, we obtained 20 Turkers X  judgments via CrowdFlower for each in-stance and found that, on average, only 3 Turkers were required to match the experts.

More specifically, for the extraneous preposition error task, we used 75 sentences as gold and ob-tained judgments for the remaining 923 non-gold sentences. 3 We found that if we used 3 Turker judg-ments in a majority vote, the agreement with any one of the three expert raters is, on average, 0.87 with a kappa of 0.76. This is on par with the inter-expert agreement and kappa found earlier (0.87 and 0.75 respectively).

The extraneous preposition annotation cost only $325 (923 judgments  X  20 Turkers) and was com-pleted in a single day. The only restriction on the Turkers was that they be physically located in the USA. For the analysis in subsequent sections, we use these 923 sentences and the respective 20 judg-ments obtained via CrowdFlower. The 3 expert judgments are not used any further in this analysis. In this section, we provide details on how crowd-sourcing can help revamp the evaluation of error de-tection systems: (a) by providing more informative measures for the intrinsic evaluation of a single sys-tem (  X  4.1), and (b) by easily enabling system com-parison (  X  4.2). 4.1 Crowd-informed Evaluation Measures When evaluating the performance of grammatical error detection systems against human judgments, the judgments for each instance are generally re-duced to the single most frequent category: Error or OK . This reduction is not an accurate reflection of a complex phenomenon. It discards valuable in-formation about the acceptability of usage because it treats all  X  X ad X  uses as equal (and all good ones as equal), when they are not. Arguably, it would be fairer to use a continuous scale, such as the pro-portion of raters who judge an instance as correct or incorrect. For example, if 90% of raters agree on a rating of Error for an instance of preposition usage, then that is stronger evidence that the usage is an er-ror than if 56% of Turkers classified it as Error and 44% classified it as OK (the sentence  X  In addition classmates play with some game and enjoy  X  is an ex-ample). The regular measures of precision and recall would be fairer if they reflected this reality. Besides fairness, another reason to use a continuous scale is that of stability, particularly with a small number of instances in the evaluation set (quite common in the field). By relying on majority judgments, precision and recall measures tend to be unstable (see below).
We modify the measures of precision and re-call to incorporate distributions of correctness, ob-tained via crowdsourcing, in order to make them fairer and more stable indicators of system perfor-mance. Given an error detection system that classi-fies a sentence containing a specific preposition as Error (class 1) if the preposition is extraneous and OK (class 0) otherwise, we propose the following weighted versions of hits (H w ), misses (M w ) and false positives (FP w ):
In the above equations, N is the total number of instances, c i sys is the class (1 or 0) , and p i indicates the proportion of the crowd that classi-fied instance i as Error . Note that if we were to revert to the majority crowd judgment as the sole judgment for each instance, instead of proportions, p crowd would always be either 1 or 0 and the above formulae would simply compute the normal hits, misses and false positives. Given these definitions, weighted precision can be defined as Precision w = H w / ( H w + FP w ) and weighted recall as Recall w = H w / ( H w + M w ) .
To illustrate the utility of these weighted mea-sures, we evaluated the LM and PERC systems on the dataset containing 923 preposition instances, against all 20 Turker judgments. Figure 1 shows a histogram of the Turker agreement for the major-ity rating over the set. Table 1 shows both the un-weighted (discrete majority judgment) and weighted (continuous Turker proportion) versions of precision and recall for this system.

The numbers clearly show that in the unweighted case, the performance of the system is overesti-mated simply because the system is getting as much credit for each contentious case (low agreement) as for each clear one (high agreement). In the weighted measure we propose, the contentious cases are weighted lower and therefore their contribution to the overall performance is reduced. This is a fairer representation since the system should not be expected to perform as well on the less reliable in-stances as it does on the clear-cut instances. Essen-tially, if humans cannot consistently decide whether a case is an error then a system X  X  output cannot be considered entirely right or entirely wrong. 4
As an added advantage, the weighted measures are more stable. Consider a contentious instance in a small dataset where 7 out of 15 Turkers (a minor-ity) classified it as Error . However, it might easily have happened that 8 Turkers (a majority) classified it as Error instead of 7. In that case, the change in unweighted precision would have been much larger than is warranted by such a small change in the data. However, weighted precision is guaranteed to be more stable. Note that the instability decreases as the size of the dataset increases but still remains a problem. 4.2 Enabling System Comparison In this section, we show how to easily compare dif-ferent systems both on the same data (in the ideal case of a shared dataset being available) and, more realistically, on different datasets. Figure 2 shows (unweighted) precision and recall of LM and PERC (computed against the majority Turker judgment) for three agreement bins , where each bin is defined as containing only the instances with Turker agree-ment in a specific range. We chose the bins shown since they are sufficiently large and represent a rea-sonable stratification of the agreement space. Note that we are not weighting the precision and recall in this case since we have already used the agreement proportions to create the bins.

This curve enables us to compare the two sys-tems easily on different levels of item contentious-ness and, therefore, conveys much more information than what is usually reported (a single number for unweighted precision/recall over the whole corpus). For example, from this graph, PERC is seen to have similar performance as LM for the 75-90% agree-ment bin. In addition, even though LM precision is perfect (1.0) for the most contentious instances (the 50-75% bin), this turns out to be an artifact of the LM classifier X  X  decision process. When it must de-cide between what it views as two equally likely pos-sibilities, it defaults to OK . Therefore, even though LM has higher unweighted precision (0.957) than PERC (0.813), it is only really better on the most clear-cut cases (the 90-100% bin). If one were to re-port unweighted precision and recall without using any bins X  X s is the norm X  X his important qualifica-tion would have been harder to discover.

While this example uses the same dataset for eval-uating two systems, the procedure is general enough to allow two systems to be compared on two dif-ferent datasets by simply examining the two plots. However, two potential issues arise in that case. The first is that the bin sizes will likely vary across the two plots. However, this should not be a significant problem as long as the bins are sufficiently large. A second, more serious, issue is that the error rates (the proportion of instances that are actually erroneous) in each bin may be different across the two plots. To handle this, we recommend that a kappa-agreement plot be used instead of the precision-agreement plot shown here. Our goal is to propose best practices to address the two primary problems in evaluating grammatical er-ror detection systems and we do so by leveraging crowdsourcing. For system development, we rec-ommend that rather than compressing multiple judg-ments down to the majority, it is better to use agree-ment proportions to weight precision and recall to yield fairer and more stable indicators of perfor-mance.

For system comparison, we argue that the best solution is to use a shared dataset and present the precision-agreement plot using a set of agreed-upon bins (possibly in conjunction with the weighted pre-cision and recall measures) for a more informative comparison. However, we recognize that shared datasets are harder to create in this field (as most of the data is proprietary). Therefore, we also provide a way to compare multiple systems across differ-ent datasets by using kappa-agreement plots. As for agreement bins, we posit that the agreement values used to define them depend on the task and, there-fore, should be determined by the community.
Note that both of these practices can also be im-plemented by using 20 experts instead of 20 Turkers. However, we show that crowdsourcing yields judg-ments that are as good but without the cost. To fa-cilitate the adoption of these practices, we make all our evaluation code and data available to the com-munity. 5 We would first like to thank our expert annotators Sarah Ohls and Waverely VanWinkle for their hours of hard work. We would also like to acknowledge Lei Chen, Keelan Evanini, Jennifer Foster, Derrick Higgins and the three anonymous reviewers for their helpful comments and feedback.

