 y.shen2@aston.ac.uk Continuous-time diffusion processes, described by stochastic differential equations (SDEs), arise naturally in a range of applications from environmental modelling to mathematical finance [13]. In served, non-linear diffusion processes has been tackled using Markov Chain Monte Carlo (MCMC) approaches based on data augmentation [17, 11], Monte Carlo exact simulation methods [6], or Langevin / hybrid Monte Carlo methods [1, 3]. Within the signal processing community solutions filter/smoother [2, 5] and mean field analysis of the SDE together with moment closure methods [10] have also been proposed. In this work we develop a novel variational approach to the problem of approximate inference in continuous-time diffusion processes, including a marginal likelihood (ev-inference using naive methods is complicated due to dependencies between state and system noise parameters.
 We work in continuous time, computing distributions over sample paths 1 , and discretise only in our posterior approximation, which has advantages over methods based on discretising the SDE directly [3]. The approximate inference approach we describe is more computationally efficient than competing Monte Carlo algorithms and could be further improved in speed by defining a variety smoothing methods applied to non-linear systems [4]. Ultimately, we are motivated by the critical requirement to estimate parameters within large environmental models, where at present only a small number of Kalman filter/smoother based estimation algorithms have been attempted [2], and there have been no likelihood based attempts to estimate the system noise forcing parameters. sion processes with measurement noise and we provide the tools to estimate the optimal variational of the posterior distribution over parameters based on the variational free energy. Consider the continuous-time continuous-state stochastic process X = { X t , t 0  X  t  X  t f } . We assume this process is a d -dimensional diffusion process. Its time evolution is described by the following SDE (to be interpreted as an Ito stochastic integral):  X   X  R d  X  d is the system noise covariance. The diffusion is modelled by a d -dimensional Wiener with additive system noise. This might seem restrictive at first sight. However, it can be shown [13, 17, 6] that a range of state dependent stochastic forcings can be transformed into this form. { t simplicity, the measurement noise is modelled by a zero-mean multivariate Gaussian density,with covariance matrix R  X  R d  X  d . Our approximate inference scheme builds on [4] and is based on a variational inference approach upper bound to the negative log-marginal likelihood: with a system noise covariance chosen to be identical to that of the prior process induced by (1). The standard approach for learning the parameters in presence of latent variables is to use an EM noise covariance (see Appendix A) as the true posterior, the EM algorithm would leave this covari-ance completely unchanged in the M step and cannot be used for learning this crucial parameter. Therefore, we adopt a different approach, which is based on a conjugate gradient method. 3.1 Optimal approximate posterior process We consider an approximate time-varying linear process with the same diffusion term, that is the same system noise covariance: t is defined as follows: at time t . In the rest of the paper, we denote q ( X t |  X  ) by the shorthand notation q t . given by (see Appendix A) follows: where { Y t ,t 0  X  t  X  t f } is the underlying continuous-time observable process. 3.2 Smoothing algorithm The variational parameters to optimise in order to find the optimal Gaussian process approximation the time evolution of the means and the covariances are described by a set of ordinary differential equations [13, 4]: marginal means and the marginal covariances along sample paths. To enforce these constraints we formulate the Lagrangian gradient functions: The gradients  X  A E sde ( t ) and  X  b E sde ( t ) are derived in Appendix B.
 of the Lagrange multipliers, along with jump conditions when there are observations: The optimal variational functions can be computed by means of a gradient descent technique, such forward propagation of the means and the covariances, followed by a backward propagation of the Lagrange multipliers, and then to take a gradient step. The resulting algorithm for computing the optimal posterior q ( X |  X  ) over sample paths is detailed in Algorithm 1. Algorithm 1 Compute the optimal q ( X |  X  ) . 1: input ( m 0 , S 0 ,  X  ,  X  ,t 0 ,t f ,  X  t, X  ) 2: K  X  ( t f  X  t 0 ) /  X  t 4: repeat 5: for k = 0 to K  X  1 do 8: end for { forward propagation } 9: for k = K to 1 do 12: if observation at t k  X  1 then 15: end if { jumps } 16: end for { backward sweep (adjoint operation) } 17: update { A k , b k } k  X  0 using the gradient functions (12) and (13) 18: until minimum of L  X  ,  X  is attained { optimisation loop } 19: return { A k , b k , m k , S k ,  X  k ,  X  k } k  X  0 tion parameters and the system noise covariance. The estimation of the parameters related to the observable process are not discussed in this work, although it is a straightforward extension. The smoothing algorithm described in the previous section computes the optimal posterior process parts to make the boundary conditions explicit. This leads to 4.1 Initial state the following expressions: is taken sufficiently large to give a broad prior. 4.2 Drift SDE. Their general expression is given by where  X   X  f E sde ( t ) = D ( f  X  ( t,X t )  X  g ( t,X t )) &gt;  X   X  1  X   X  f f  X  ( t,X t ) E 4.3 System noise an MCMC approach because the efficiency is strongly dependent on the discrete approximation of the SDE and most methods break down when the time step  X  t gets too small [11, 6]. For example in a Bayesian MCMC approach, which alternates between sampling paths and parameters, the latent paths imputed between observations must have a system noise parameter which is arbitrarily close to its previous value in order to be accepted by a Metropolis sampler. Hence, the algorithm becomes extremely slow. Note, that for the same reason, a naive EM algorithm within our approach breaks down. However, in our method, we can simply compute approximations to the marginal likelihood of the marginal likelihood which is a time consuming method.
 The gradient of (16) with respect to  X  is given by In order to validate the approach, we consider the 1 dimensional double-well system: is driven by the system noise, which makes it occasionally flip from one well to the other. transition from one well to the other is highly unlikely in the window of roughly 8 time units that we consider and where a transition occurs.
 Figure 1(a) compares the variational solution to the outcomes of a hybrid MCMC simulation of the posterior process using the true parameter values. The hybrid MCMC approach was proposed in acceptance of new paths sufficiently high, the basic MCMC algorithm is combined with ideas from Molecular Dynamics, such that the MCMC sampler moves towards regions of high probability in the state space. An important drawback of MCMC approaches is that it might be extremely difficult to monitor their convergence and that they may require a very large number of samples before actually converging. In particular, over 100 , 000 sample paths were necessary to reach convergence in the case of the double-well system.
 The solution provided by the hybrid MCMC is here considered as the base line solution. One can ob-in approximately 180 conjugate gradient steps, each one involving a forward and backward sweep. The optimal parameters and the optimal initial conditions for the variational solution are given by Figure 1: (a) Variational solution (solid) compared to the hybrid MCMC solution (dashed), using the true parameter values. The curves denote the mean paths and the shaded regions are the two-curve and the dashed curve are respectively the approximations of the posterior shape based on the variational free energy and MCMC. the deviation of the system noise is worse. Deviations may be explained by the fact that the number a transition between the two wells within a small time interval and is thus highly untypical with on a sample path without transition, in a time window of the same size. In this case, we obtained Posterior distribution over the parameters we can approximate the posterior over the system noise variance via G (  X , X  ) , with  X  = 10  X  3 and  X  = 10  X  3 . A comparison with preliminary MCMC estimates for the variance of the density come out fairly well. We have presented a variational approach to the approximate inference of stochastic differential sional bi-stable system only. Comparison with a Monte Carlo approach suggests that our method can reproduce the posterior mean fairly well but underestimates the variance in the region of the transition. Parameter estimates also agree well with the MC predictions.
 In the future, we will extend our method in various directions. Although our approach is based on a Gaussian approximation of the posterior process, we expect that one can improve on it and obtain computation of a non-Gaussian shaped probability density for the system noise parameter using the free energy. An important extension of our method will be to systems with many degrees of freedom. dimensions.
 Acknowledgments This work has been funded by the EPSRC as part of the Variational Inference for Stochastic Dynamic Environmental Models (VISDEM) project (EP/C005848/1).
 dimensional) random variable described by the SDE in the time interval under consideration. Consider the Euler-Muryama discrete approximation (see for example [13]) of the SDE (1) and its linear approximation (4): true process and its approximation follow from the Markov property: do not restrict the variational posterior to factorise over the latent states.
 The Kullback-Leibler divergence between the two discretized prior processes is given by where we omitted the conditional dependency on  X  for simplicity. The second term on the right which defines an integral over the average sample path: tion q t = q ( X t |  X  ) is the marginal at time t for a given system noise covariance  X  . It is important to realise that the KL between the induced prior process and its approximation is finite because the system noise covariances are chosen to be identical. If this was not the case, KL  X  X  X  when  X  t  X  0 .
 If we assume that the observations are i.i.d., it follows also that Clearly, minimising this expression with respect to the variational parameters for a given system noise  X  and for a fixed parameter vector  X  is equivalent to minimising the KL between the vari-independent of sample paths. given by where  X  X  X  x f  X  ( t,X t )  X  q
