 reproducing kernel Hilbert space (see [1] for a discussion) .
 Waterman and BLAST scores are indefinite yet have provided hi nts for constructing useful positive measures for classification.
 generation technique similar to the analytic center cuttin g plane method we use here. numerical complexity of evaluating the exact positive semi definite kernel is too high and a proxy both cases, our method allows us to bypass these limitations . 1.1 Current results lem with an indefinite kernel as that of minimizing the distan ce between convex hulls formed from the two categories of data embedded in the pE space. The nonse parable case is handled in the same manner using reduced convex hulls (see [11] for a discussion of SVM geometric interpretations). eigenvectors in order to produce a positive semidefinite ker nel (see [12] and [2]). Yet another option is to reformulate either the maximum marg in problem or its dual in order to use the indefinite kernel in a convex optimization problem (s ee [13]). An equivalent formulation of SVM with the same objective but where the kernel appears in the constraints can be modified to a convex problem by eliminating the kernel from the object ive. Directly solving the nonconvex problem sometimes gives good results as well (see [14] and [1 0]). 1.2 Contribution tor weights and a proxy positive semidefinite kernel matrix, while penalizing the distance between can simply be formulated as a perturbation of the positive se midefinite case. Our formulation can also be interpreted as a worst-case robu st classification problem with uncer-a convex problem, and hence can be solved efficiently with gua ranteed complexity bounds. The paper is organized as follows. In Section 2 we formulate o ur main classification problem and detail its interpretation as a robust SVM. In Section 3 we des cribe an algorithm for solving this applications. Here, we introduce our robustification of the SVM classificat ion problem with indefinite kernels. 2.1 Robust classification Let K  X  S n be a given kernel matrix and y  X  R n be the vector of labels, with Y = diag ( y ) the n -vectors of real numbers. We can write the dual of the SVM clas sification problem with hinge loss and quadratic penalty as: problem is a convex quadratic program. Suppose now that we ar e given an indefinite kernel matrix K kernel matrix in some given neighborhood of the original (in definite) kernel matrix K in the variables K  X  S n and  X   X  R n , where the parameter  X  &gt; 0 controls the distance between the original matrix K classification problem with bounded uncertainty on the kern el matrix K . The above problem is infeasible for some values of  X  so we replace here the hard constraint on K by a penalty on the we solve is now: in the variables K  X  S n and  X   X  R n , where the parameter  X  &gt; 0 controls the magnitude of the penalty on the distance between K and K linear constraints and is therefore a convex problem in  X  .
 form. For a fixed  X  , the inner minimization problem is equivalent to the follow ing problem: in the variable K  X  S n . This is the projection of the K positive semidefinite matrices. The optimal solution to thi s problem is then given by: where X the i th eigenvalue and eigenvector of the matrix X . Plugging this solution into (3), we get: representation of X we get K  X  = V D where  X  the term k K  X   X  K finally becomes: in the variable  X   X  R n . 2.2 Dual problem Because problem (3) is convex with at least one compact feasi ble set, we can formulate the dual problem to (5) by simply switching the max and the min. The inn er maximization is a quadratic inner dual quadratic program into the outer minimization, t o get the following problem: minimize Tr ( K  X  1 ( Y ( e  X   X  + + y X  ))( Y ( e  X   X  + + y X  )) T ) / 2 + C T e +  X  k K  X  K subject to K 0 ,  X ,  X  0 in the variables K  X  S n ,  X ,  X  R n and  X   X  R . This dual problem is a quadratic program in the variables  X  and which correspond to the primal constraints 0  X   X   X  C and  X  which is the dual problem produces a corresponding kernel in (4), and pluggin g this kernel into the dual problem in a duality gap and track convergence. 2.3 Interpretation We noted that our problem can be viewed as a worst-case robust classification problem with uncer-kernel to use with SVM in our framework is the positive part of the indefinite kernel. full kernel corresponding to training instances by the rank -one update resulting from the optimal the test kernel values from the resulting positive semidefin ite matrix. We now detail two algorithms that can be used to solve Problem (5). The optimization problem is a simple projected gradient method which has numerically ch eap iterations but has no convergence bound. We then show how to apply the much more efficient analyt ic center cutting plane method whose iterations are slightly more complex but which conver ges linearly.
 -approximation as follows: and the gradient is given by  X   X  Gradient Calculating the gradient of our objective requires a full ei genvalue decomposition to with respect to  X  is given by: where v approximation above to get the gradient.
 We note that eigenvalues of symmetric matrices are not diffe rentiable when some of them have mul-subgradient methods, which are much slower, or use subgradi ents for analytic center cutting plane methods (which does not affect complexity). 3.1 Projected gradient method The projected gradient method takes a steepest descent, the n projects the new point back onto the an initial point  X  Projected gradient method The complexity of each iteration breaks down as follows.
 multiple times.
 Step 2. This is a projection onto the region A = {  X  T y = 0 , 0  X   X   X  C } and can be solved explicitly by sorting the vector of entries, with cost O ( n log n ) .
 Stopping Criterion. We can compute a duality gap using the results of  X  2.2: let K a convex kernel K current solution.
 Complexity. The number of iterations required by this method to reach a ta rget precision of  X  is typically in O (1 / X  2 ) . 3.2 Analytic center cutting plane method The analytic center cutting plane method (ACCPM) reduces th e feasible region on each iteration precision. This method does not require differentiability . We set A which we can write as { A method then works as follows (see [18] for a more complete ref erence on cutting plane methods): Analytic center cutting plane method The complexity of each iteration breaks down as follows.
 using interior point methods for example. Step 2. This simply updates the polyhedral description.
 Stopping Criterion. An upper bound is computed by maximizing a first order Taylor a pproximation of f (  X  ) at  X  i over all points in an ellipsoid that covers A i , which can be done explicitly. tion which keeps the complexity of the localization set boun ded. Other schemes are available with for example. In this section we compare the generalization performance o f our technique to other methods of performance on positive semidefinite kernels using the LIBS VM library. We finish with experiments showing convergence of our algorithms. Our algorithms were implemented in Matlab. 4.1 Generalization We finally also compare with using SVM on the original indefini te kernel (SVM converges but the solution is only a stationary point and is not guaranteed to b e optimal).
 We experiment on data from the USPS handwritten digits datab ase (described in [20]) using the indefinite Simpson score (SS) to compare two digits and on two data sets from the UCI repository and testing data. We apply 5-fold cross validation and use an accuracy measure (described below) optimal parameters and test on the independent test set.
 The main observation is that the USPS data uses highly indefin ite kernels while the UCI data use percentage of true positives that were correctly predicted positive.
 Our method is referred to as Indefinite SVM. We see that our met hod performs favorably among the USPS data. Both measures of performance are quite high fo r all methods. Our method does positive semidefinite maybe be seen as having a small amount o f noise. 4.2 Algorithm Convergence above. The average results with one standard deviation abov e and below the mean are displayed in Figure 1 with the duality gap in log scale (note that the codes were not stopped here and that the target gap improvement is usually much smaller than 10  X  8 ). As expected, ACCPM converges much precision, however each iteration only requires sorting th e current point. We have proposed a technique for incorporating indefinite ke rnels into the SVM framework with-out any explicit transformations. We have shown that if we vi ew the indefinite kernel as a noisy convex optimization problem. We give two convergent algori thms for solving this problem on rel-with other methods handling indefinite kernels in the SVM fra mework but provides a much clearer interpretation for these heuristics.
