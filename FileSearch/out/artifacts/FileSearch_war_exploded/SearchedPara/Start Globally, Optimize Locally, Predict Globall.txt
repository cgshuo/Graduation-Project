
Class imbalance is a ubiquitous problem in supervised learning and has gained wide-scale attention in the lit-erature. Perhaps the most prevalent solution is to apply sampling to training data in order improve classifier per-formance. The typical approach will apply uniform lev-els of sampling globally. However, we believe that data is typically multi-modal, which suggests sampling should be treated locally rather than globally. It is the purpose of this paper to propose a framework which first identi-fies meaningful re gions of data and then proceeds to find optimal sampling levels within each. This paper demon-strates that a global classifier trained on data locally sam-pled produces superior rank-orderings on a wide range of real-world and artificial datasets as compared to contem-porary global sampling methods.
Mining imbalanced datasets continues to be a pervasive problem in a large variety of applications [8, 22] including medicine, finance, and security . Traditional objective met-rics, such as accuracy, often fail to describe performance properly under these circumstances as a trivial classifier may produce high accuracy but perform poorly on the more interesting rare class. Instead, Area Under the ROC Curve ( AU ROC ) is typically used as a measure of the capabil-ity of a classifier to effectively rank the minority or posi-tive class instances above the majority or negative class in-stances. AU ROC captures the trade-off between true posi-tives and false positives, producing a robust metric for even the most imbalanced datasets.

The most significant direction of research on imbalanced datasets has focused on sampling strategies for countering the problem of class imbalance [3, 8]. Sampling methods focus on either adding minority (positive) class points or removing majority (negative) class points to improve accu-racy or AU ROC of the positive class [3, 15]. Sampling is a common approach to counter the curse of imbalance in many domains.

Figure 1. Data example for which the class distributions are generated through piece-wise, regional functions.

Sampling methods consider the class skew and proper-ties of the dataset as a whole. However, it is the contention of this paper that machine learning and data mining often face nontrivial datasets, which often exhibit characteristics and properties at a local, rather than global level. As an example, we consider Figure 1 where the class probability generating function is different within each region of the data, dictating that the relationship between classes is sub-stantially different within each discrete area. Even when two regions exhibit the same class skew, it is possible for them to exhibit other significant and disjoint characteristics. While a particular sampling level induces a strong classifier for the first region, it may well induce a poor classifier on the second region. This suggests that the typically global task of selecting sampling levels to improve classifier per-formance should in many contexts be considered locally.
With these concerns in mind, the primary contributions of this paper are as follows. 1) An analytic framework for supporting locally optimal mixtures of sampling . 2) An effective data partitioning or segmentation method based on the distribution invariant Hellinger distance [9, 10, 11, 21]. 3) Identifying optimal levels and types of sampling within each data segment. 4) A comprehensive evaluation using 20 real-world and UCI datasets.

For benchmarking, we will consider two popular sam-pling methods from the related work: Synthetic Minority Oversampling TechniquE (SMOTE) [6] and random under-sampling. In addition, we include Classification using lOcal clusterinG Over Sampling (COG-OS) [23]. SMOTE injects synthetic positive class examples to emphasize class bound-aries, while COG-OS uses k -means clustering to partition the complex majority class into simpler sub-classes and mi-nority class replication.
In the supervised learning task a labeled training data sample D is available for model training, presumably drawn randomly from some distribution p ( Y | X ) , for which each x i  X  X is a single feature vector and y i  X  Y is the associated class label. In addition, there is an available unlabeled sample T for testing purposes, which for the sake of this study is presumed from the same distribution p ( Y | X ) . The goal of this task is then to train some clas-sifier f : X  X  X  X  Y to estimate the probability for each x i in T to belong to a class in Y . The ideal for this clas-sifier is strong performance, either in minimizing loss or cost or maximizing accuracy, profit, or AU ROC . Thus, the model should maximize some objective function, O ,such that E ( X,Y ) [ O ( f ( X ) ,Y )] is maximized [4]. In the ideal case, f : X  X  X  X  Y represents the  X  X rue model X  of the data p ( Y | X ) . In practice, this is difficult if not impossible due to the limited representation D provides of p ( Y | X ) . Thus, the learning algorithms often approximate the true function. Due to the bias inherent in each learning algorithm X  X  rep-resentation and the particular emphasis of each objective function, it is often the case that D  X  , a resampling of D not necessarily drawn from p ( Y | X ) , will actually enhance the performance of the determined classifier. The task of finding some S : D  X  X  X  D  X  that improves a classifier X  X  performance is called sampling.

A primary question when employing sampling is how does one tune S : D  X  X  X  D  X  to yield optimal results? A suggested method is to use a heuristic wrapper [7] to set sampling levels for each class. The wrapper greedily explores the search space of sa mpling paramet ers until the  X  X otentially optimal X  amounts are discovered. An alternate method stems from cost sensitive learning. For instance, Elkan discusses a simple method to calculate optimal sam-pling levels based on the ratio of misclassification for each class [14]. However, evaluation typically occurs without explicit costs. In this case, Elkan X  X  calculation simply indi-cates to sample the classes to a balance point, which can be limiting. It is often necessary to explore a much larger sam-pling space, to address the question of how do the proper-ties of data (aside from class skew) affect optimal sampling levels? The wrapper exploits the properties of the data as it utilizes the performance of the learning algorithm as a guide.

We consider this question by constructing some artificial datasets, seen in Figure 2, as follows. To begin, there ex-ist two classes of examples with a skew ratio (+ :  X  ) of (50 : 1000) , positive examples depicted as +  X  X  and nega-tive examples as  X   X  X . Each class is centered at a fixed point  X  x + and  X  x  X  with radius r + and r  X  , respectively. Since the number of points for each class is fixed, the densities  X  and  X   X  are a product of the radii, reducing radius length in-creases density. The final consideration for this dataset is  X  , the distance between  X  x + and  X  x  X  . The higher the  X  ,the lower the class overlap. Of interest is the interaction be-tween the density  X  + , relative overlap between the regions, and end ability to discriminate between + and  X  as quan-tified by AU ROC . Since maintaining the relative skew is desired,  X  + will be affected by changing r + and the resul-tant region.

This experiment considers three discrete increments of both r + and  X  : High, Medium, and Low to determine the importance of degree of r + and  X  on end performance. We say that  X  + is High when r + = r  X  ,Mediumwhen r + =0 . 75 r  X  , and Low when r + =0 . 5 r  X  .Once r + is de-termined, overlap is High when  X  =0 . 5 r + ,Mediumwhen  X  =0 . 75 r + and Low when  X  = r + . When overlap is High, nearly all of the + region is engulfed within the  X  region. However, when overlap is Low, over half the + region is outside the larger  X  area. We note that  X  x  X  and  X   X  (and therefore r  X  ) remain fixed through all constructions. Based on these constructions, we now address how, if at all, do these constructed properties affect discriminative ability?
As we have conserved the relative skew ratio between classes at (50 : 1000) , Elkan X  X  solution would apply the same level of over and undersampling as a universal opti-mal solution to each of the nin e scenarios. However, the re-sults shown in Table 1 indicate otherwise, also establishing three general trends. First, i ncreased minority class density improves AU ROC , since increased density allows C4.5 to develop a more confident decision region. Second, in-creased separation between the two class centers (reduced class overlap) increases AU ROC , since this enables C4.5 to construct a more definitive border between the classes. Third, the optimal levels of sampling for each scenario is substantially different (and does not generate a class bal-anced sample), which is in contradiction with Elkan X  X  so-lution. Thus, it is observed that data properties aside from class skew may have a substantial effect on optimal sam-pling level.

However, datasets are often more complex than those in Figure 2, perhaps containing multiple points of inter-section. In such a scenario, even an optimized sampling of data may represent an incomplete solution from a local perspective. Thus, we would like to address what effect with High density and Medium class separation. The (+ :  X  does considering sampling locally have on global perfor-mance? We consider the data examples highlighted in Fig-ure 3. We note that the left and middle datasets are similar to the previous: they represent two-class overlapping radi-als, with varying distance bet ween centers and density. The left dataset is significantly easier to learn than the middle, since there is less overlap between the regions. This is re-flected by the determined AU ROC for left and middle of 0.868 and 0.531, using C4.5. Using sampling on the left dataset, the AU ROC can be improved to 0.906 using levels of ( Undersample,SMOTE ) of (50 , 350) found through the wrapper method in [7], while the middle can improve to 0.812 using (20 , 250) . However, there is some difficulty when the left and middle datas et concatenate to form the rightmost dataset in the figure. By default, C4.5 obtains an AU ROC of 0.817, which can be improved to 0.845 us-ing (100 , 50) . This is somewhat problematic. The com-ponents dictated different sampling levels, but when they were considered as a single dataset a distinct third sam-pling combination was used, indicating that there is a loss in efficiency of optimizing AU ROC via sampling globally. This is confirmed, since using the regional sampling level of (50 , 350) for the leftmost dataset and (20 , 250) for the cen-ter dataset improves global performance further to 0.874. Thus, our current methods of global sampling may be in-sufficient in some applications, since we observe a consid-erable improvement when lo cal sampling is considered.
Rather, the task of finding some S : D  X  X  X  D  X  to im-prove performance might be divided into components. It is our belief that much of data operates as a set of modalities guiding the construction of feature vectors and their associ-ated class labels, in accordance with P ( Y | X ) .
Presuming at least some data exists in such a modal fashion, it is therefore meaningful to derive samplings lo-cally. Such a procedure begins by decomposing D into m meaningful segments, D i . In Figure 3, the two compo-nents ( D 1 ,D 2 ) for the right dataset are the left and middle datasets. Once such segments have been determined, sam-
Figure 3. Two separate two class radials with different  X  ,  X  combination into a single dataset.

Table 1. AU ROC values for C4.5 and C4.5 with Sampling, in addition to optimal sam-pling levels, on each of the nine scenarios in Figure 2.
 pling optimizes locally to find each D  X  i . This is recombined as D  X  = m i =1 D  X  i , which is then used to formulate a global classifier. As can be seen from this example, a key aspect to local optimization is constructing a reliable partitioning which will be discussed in the next section.
This section introduces a supervised method for split-ting the data into segments or components. The partitioning builds a tree of maximum height 2 (see Algorithm 1), re-sulting in at most 4 segments. Our initial experimentation demonstrated that using a height greater than 2 resulted in much sparser segments, given the high imbalance of some of the datasets used in our paper. Thus, to be consistent we constrained the tree to the height of 2 for all datasets. This type of segmentation is used instead of unsupervised learning, such as k -means, since the known class labels pro-vide potentially useful information. The data will also be presumably be highly imbalanced, and we want to exploit the class skew information while constructing the partitions as well. To that end, we propose using Hellinger distance [9, 10, 11, 21] to guide the segmentation.

Hellinger distance is a measure of distributional diver-gence. Let ( X  , X  ) denote a measurable space with A and B as two continuous distributions measured with respect to  X  . Let a and b be the densities of A and B with respect to  X  . The definition of Hellinger distance can be given as: This is equivalent to: where  X  Hellinger distance does not de pend on the choice of param-eter  X  . It can also be defined for a countable space  X  ,as d
H ( A, B )=  X   X   X   X  ( A (  X  )  X  B (  X  )) 2 . The Hellinger distance carries these following properties: 1) d H ( A, B ) is in [0 , ing d H ( A, B )= d H ( B, A ) . Moreover, squared Hellinger distance is the lower bound of KL divergence.

Here, the A and B in Equation 1 are assumed to be the normalized frequencies of feature values across classes. This allows us to capture the notion of  X  X ffinity X  between the probability measures on a finite event space. If A = B, then distance = 0 (maximal af finity) and if A and B are com-pletely disjoint then distance = (2) (zero affinity). This dictates the partition splitting criterion for separability be-tween classes. We want to select a feature that carries the minimal affinity between the classes.

We assume a countable space, so we discretize all contin-uous features into p partitions or bins. Then, we are essen-tially interested in calculating the  X  X istance X  between the relative class frequencies for each feature value, normalized by the overall class frequency to reduce the effect of class skew. Thus, we apply Hellinger distance as Algorithm 1 Hellinger Partitioning Procedure Input: Dataset D with feature set F 1: if h == 2 or | D | == 0 then 2: return 3: end if 5: for each value v of f do 6: HellingerP artition ( T f = v ,h +1) 7: end for We note that bounds on this metric are unaffected by the class skew, making Hellinger distance ideal for separating regions of disjoint class distribution within highly imbal-anced data. To construct partitions, we use the procedure given in Algorithm 1. The Hellinger distance is applied to each feature with the highest distance used to generate a split in data to generate a two-level binary tree, as initial experimentation indicated this produced the best results. In the case of continuous features each potential feature split is explored, meaning p =2 . The procedure is then applied on each partition until a desired maximum depth is reached. With this partitioning method, we now propose a general-ized framework for local sampling aimed to leverage the observations of Section 2 into practical application.
Local Sampling ( LS ) provides a general framework into which it is possible to incorporate any sort of classifier, data partitioning, or sampling mechanism. This flexible structure allows for versatility, allowing a practitioner to readily adapt this framework to many application settings. In essence, this is a three step extension to any sampling scheme.

Algorithm 2 outlines the LS pseudo-code. While we use the proposed Hellinger distanc e based partitioning method, in principle any arbitrary partitioning method P is applied to divide the data. This phase may apply nearly any type of supervised or unsupervised method. With the set of discov-ered data partitions Z , the procedure moves to the sampling phase. Here, the chosen sampling method S is applied to each component. The algorithm checks the range of param-eters  X  for S and determines the optimal performer accord-ing to the objective function O . Once optimal sampling pa-rameters are applied to each component or segment, all the componentsare merged to create the new training set. Then, Algorithm 2 The Local Sampling (LS) Framework Input: D : Labeled Training Set Output: M : the model built 1: Z = P ( D ) 2: for each partition z  X  Z do 4: end for 6: M = f ( D  X  ) a classifier is learned. Thus, we go from global view of the data to local view (partitioning) and then again to a global view for learning the classifier.

We follow the wrapper framework [7] to discover the po-tentially optimal amounts of undersampling and SMOTE for S . Note that the same wrapper framework is used for the benchmarks used in this paper  X  COG-OS and combi-nation of undersampling and SMOTE on the entire dataset (without LS ). The sampling levels for each form the pa-rameters  X  and are { 0 , 10 , ... 100 } for undersampling and { 0 , 50 , 100 , ..., 500 } for SMOTE. For P , we apply the par-titioning method from Section 3. The wrapper then pro-ceeds as follows. First, a 5-fold cross-validation (cv) is ap-plied on the training set to determine the baseline AU ROC performance. Since, we use 5x2 cross-validation frame-work, this 5-fold cv is applie d to each of the training sets of 5x2, making it an independent validation framework. Then, the majority class examples are randomly removed 10% at a time, as long as the performance remains above the baseline. Once an undersampling level is selected, it results in the new improved baseline performance. Sub-sequently, SMOTE is applied in progressive steps, adding synthetic points in increments of 50% of the minority class size. Again, this is repeated as long as it beats the baseline performance. This procedure i s slightly adapted for local sampling by exploring the parameter space exhaustively for each partition in sequence. The order of optimization is the partition with the most majority class points to the one with the fewest, allowing us to gradually shift the bias towards the minority class. It is also likely that not all partitions will require either or both undersampling and SMOTE. The par-titions are merged after each optimization to determine the AU ROC via the 5-fold cv, allowing for measurement of the global relevance to any optimization done on the partition. Figure 4 depicts the practical application and benefits of LS on the Pima dataset [2], represented in two dimensions. While the + and  X  classes have disjoint regions, a signifi-Att 8 Att 8 (d), which presents the effects of global undersampling and SMOTE. cant class overlap is also observable in other regions. Using C4.5 alone yields an AU ROC of 0.765.

In Figure 4(b), Hellinger distance (discussed in Section 3) is used to form a four-region split of the dataset. Re-gion B is entirely comprised of  X  examples and A exhibits  X  example dominance. However, regions C and D have isolated regions in which + examples actually form the ma-jority of examples. Using AU ROC , each region is opti-mized with ( Undersample,SMOTE ) levels of (40 , 50) , (90 , 100) , (40 , 250) ,and (50 , 500) for A , B , C ,and D , respectively, seen in Figure 4(c). Doing so produces an AU ROC of 0.817, which outperforms a global undersam-pling and SMOTE combination, which finds an AU ROC of 0.790. The global sampling uses (70 , 400) , while the local sampling effectively applies (66 , 303) (based on the num-ber of examples actually added and removed for each class across all partitions). We have therefore observed a practi-cal case similar to Figure 3, where LS improves AU ROC . In this case, LS is more judicious in terms of requiring fewer examples for better results as it is able to focus use of undersampling and SMOTE to particular regions of the data.
In this section, we present experimental results to vali-date and understand the performance of LS on imbalanced datasets, particularly in comparison with other contempo-rary methods. We use 5x2-fold cross-validation (cv) [13] with AU ROC as the evaluation criterion. We chose 5x2 cv over 10-fold cv as the use of 10-fold cv, while resulting in in-dependent testing sets, carries sufficient overlap in train-ing data across folds. This makes application of typical t-tests assuming independent trials questionable. Dietterich [13] notes that this results in an elevated level of Type I er-ror, which can be corrected for by his 5x2 cv. This relies on the idea that learning curves rarely cross for algorithms as training set size varies. The elevated Type I error be-comes of particular concern w hen evaluating on imbalanced datasets because of the trade-off between false positive and false negative. Significance will be tested by the F-test as described by Alpaydin [1] at 95% confidence. We note that two datasets, Compustat and Intrusion, possess natural train/test split which are maintained to preserve the sanity of evaluation. Significant differences in results will not be determined on these datasets.

We also use the statistical analysis framework to com-pare classifier ranks outlined by Demsar [12]. This method uses the Holm procedure of the Friedman test to estab-lish the significance of classifier rankings across multiple data sets. This is a specialized, non-parametric procedure for testing the significance of differences between multiple means. Demsar notes that the Friedman test is  X  X ppropriate since it assumes some, but limited commensurability [12] (page 27). X  It is  X  X afer than parametric tests since it does not assume normal distributions or homogeneity of vari-ance. As such, it can be applied t o classification accuracies, error ratios or any other measure for evaluation of classifiers [12] (page 27). X  Both 5x2 cv and Demsar X  X  analysis are in-creasingly embraced as standard s for classifier comparison in the data-mining community.
We compare the proposed local sampling framework to other contemporary methods, notably the combination of undersampling and SMOTE, which has been shown to significantly improve performance on imbalanced datasets, and COG-OS [23], which is a recent method for countering class imbalance using clust ering analysis. COG-OS uses local clusterings of majority class points and minority class replication to enhance linear separability. These clusterings are derived using k -means. Since this algorithm is partially dependent on the initial random seedings of centroids, we used 10 different clusterings, and the clustering resulting in the lowest distance to centroid was preserved.

Each of the sampling met hods and COG-OS utilize the same framework for discovering the potentially optimal pa-rameters allowing for fair comparisons across the board. That is, for the vanilla SMOTE and undersampling combi-nation with C4.5, we used a 5-fold cv on the training fold of 5x2 cv to first determine the levels of sampling. For COG-OS, we empirically determined the appropriate number of clusters k , using the same 5-fold cv on top of 5x2 cv frame-work. Finally, for our LS approach we also used the 5-fold cv to determine the levels of sampling in each partition. The 5x2 training and testing sets remained the same across all the approaches. This ensured that all approaches were inde-pendently optimized, in the same fashion, to achieve their best performances.
 We use two base classifiers: C4.5 [19] and SVM
Table 2. All the datasets used in this pa-per. The Examples column for Compustat and Intrusion also includes the number of both training and testing examples.
 [5]. Here, decision trees are unpruned and use Laplace-smoothing at the leaves in keeping with the observations of Provost &amp; Domingos [18] that this provides improve-ments for AU ROC . We train the SVM using the param-eters  X  t 0  X  b 1 and build a probabilistic estimator. Since, SVM training has a much higher time complexity than de-cision tree induction, which is exacerbated to a prohibitive computational cost when combined with the wrappers for Undersample/SMOTE and Local Sampling, we restrict our experiments with Undersample/SMOTE and LS with C4.5, which is also the most commonly used classifier with sam-pling for imbalanced datase ts in the litera ture. Throughout the remainder of this paper, we will abbreviate undersam-pling/SMOTE as UnSM .
 The results include AU ROC for all five approaches  X  LS , UnSM , COG-OS, C4.5, and SVM. For complete-ness and repeatability, we also show the parameter values (amount of sampling for UnSM and LS , and number of clusters and replications of the minority class for COG-OS). As will be described later, we use robust statistical measures to evaluate the performance and difference among the dif-ferent methods.
Table 2 describes the characteristics of the datasets used in our experimental analysis of local sampling. Several datasets were acquired from various real-world scenarios originally considered in [6]. E-State contains electrotopo-[12].
 logical state descriptors for a series of compounds from the National Cancer Institute X  X  Yeast AntiCancer drug screen. Mammography is highly imbalanced and records informa-tion on calcification in a mammogram. Oil is provided by [17]; it is relatively small and very noisy. The Phoneme dataset originates from the ELENA project and is used to distinguish between nasal and oral sounds. Compustat North America is a financial database of U.S. and Canadian publicly held companies, accu mulated since 1995. Intru-sion is a subset of data acquired from the KDD X 99 Chal-lenge on intrusion detection with u2r as the minority class and normal as the majority class [16]. Boundary, Calmod-oulin, and PhosS are various biological datasets [20]. Four-Class, German.Numer, Sp lice, and SVMGuide1 all are available with LIBSV M [5]. The remaining datasets all originate from the UCI repository [2]. Some of these are originally multiple class datasets and were converted into 2-class problems by keeping the smallest class as minority and the rest as majority. The exception is Letter, for which each vowel became a member of the minority class, set against all of the consonants. Aside from stated modifications, each dataset is used  X  X s is. X  The AU ROC values for each dataset are presented in Table 3. When C 4 . 5+ LS significantly improves over a classifier, then that classifier X  X  value is in bold .Whena classifier X  X  value is in italics , it significantly improves over C 4 . 5+ LS . From these results, the following conclusions about the performance of each classifier may be drawn. From Table 3, we note that LS improves C 4 . 5 and C 4 . 5+ UnSM categorically. This indicates that iden-tifying local rather than global sampling levels is a very effective strategy. In fact, the AU ROC values produced through C 4 . 5+ LS are quite compelling, producing the best result on 17 of 20 datasets, with one result a tie with SV M + COGOS on Satimage. C 4 . 5+ LS is outper-formed by both SV M + COGOS and SV M on Boundary and Calmodoulin, datasets for which SV M naturally out-performs C 4 . 5 by a wide margin. Both datasets are highly dimensional, and as such SV M naturally lends itself to the high dimensional datasets. Despite the high dimensional-ity, Local Sampling is still able improve over all other C 4 . 5 based methods. For Pendigits, we note that there is not quite ( Undersample,SMOTE ) .
 as large a discrepancy in base classifier performance. We note that COG-OS improves both C 4 . 5 and SV M above Local Sampling. Since it does so for both classifiers, we presume that COG-OS is quite effective in extracting a sub-class for each digit forming th e majority class, which is more effective to improve C 4 . 5 than injecting synthetic examples. Overall, on the 18 datasets for which signifi-cance is tested, there is a significant improvement on 14 datasets when comparing to C 4 . 5 , 13 when comparing to SV M , 8 when comparing to C 4 . 5+ COGOS ,10when comparing to SV M + COGOS , and 10 when comparing to C 4 . 5+ UnSM . Considering the conservative nature of the 5x2 cv, these results are noteworthy. Finally, when we compare the performance ranking of different methods, C 4 . 5+ LS emerges as a clear winner, at 95% confidence per the Friedman tests using the Holm procedure .Thisisa compelling result, clearly establishing the performance im-provements achieved by local sampling.

We also point out that dividing the majority class into sub-classes through COG-OS is effective in improving the AU ROC values over the base classifier. While it often un-derperforms, as compared to C 4 . 5+ LS (and statistically significantly so), it improves the AU ROC of C 4 . 5 on 16 of 20 datasets and SV M on 18. COG-OS, using k -means clustering, is able to effectively exploit the inherent struc-tures in the majority class space to improve performance over the base classifier. However, we show that it is more impactful to decompose the minority class space using a su-pervised partitioning method.

Table 4 shows the sampling parameters producing opti-mal results. For COG-OS, the k value selected for C 4 . 5 and SV M are moderately similar, while r values are quite divergent. The Pearson correlation comparison for k values between C 4 . 5 and SV M is 0.46, showing moderate corre-lation; however, r values are weakly negatively correlated at -0.35. C 4 . 5 and SV M will tend to select r values at oppo-site ends of the spectrum. Fisher X  X  z-test suggests that these two correlations are significantly different. We note that the undersamplings produced by UnSM and LS are more sim-ilar with a Pearson of 0.74 than the determined SMOTE lev-els, which correlate at 0.61. However, both sampling levels are highly correlated and Fisher X  X  z-test suggests that these correlations are similar with 95% confidence. Therefore, we conclude the strength of Local Sampling derives from its ability to manipulate specific regions of samplings. These parameters will allow for the reproducibility of our results. We will also readily provide all the real-world datasets and source code to the interested readers.
There are various key conclusions to be drawn from this work. We elucidate the key observations on the behavior of classifiers in the presence of class imbalance, which also helped develop the thesis for the proposed framework of Local Sampling ( LS ). We observed how properties of the data, aside from class skew itself, affect end classifier per-formance and that different sampling levels may produce optimal performances on simila r datasets, even when the class skew is identical. We also noted that a classifier im-proved through global sampling levels may be insensitive to the peculiarities of different components or modalities in the data, resulting in a suboptimal performance. We demon-strated that by first discovering components or segments in data, and then applying sampling locally to each of the seg-ments, resulted in the best global performance  X  starting globally, optimizing locally, and predicting globally .
The main contribution of the paper general framework for Local Sampling ( LS ) and a class skew insensitive parti-tioning method, effective in this application. This frame-work is able to achieve significantly better performances than other contemporarymethods. As per the Friedman test, it achieves best performance, an d statistically so, across 20 datasets derived from different domains.

