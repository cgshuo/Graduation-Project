 Online metrics measured through A/B tests have become the gold standard for many evaluation questions. But can we get the same results as A/B tests without actually fielding a new system? And can we train systems to optimize online metrics without subjecting users to an online learning algo-rithm? This tutorial summarizes and unifies the emerging body of methods on counterfactual evaluation and learning. These counterfactual techniques provide a well-founded way to evaluate and optimize online metrics by exploiting logs of past user interactions. In particular, the tutorial unifies the causal inference, information retrieval, and machine learning view of this problem, providing the basis for future research in this emerging area of great potential impact.
 Supplementary material and resources are available online at http://www.cs.cornell.edu/  X adith/CfactSIGIR2016.  X  Information systems  X  Information retrieval; Re-trieval models and ranking; Evaluation of retrieval results; Learning to Rank; Counterfactual Estimation; Causal Infer-ence; Batch Learning from Bandit Feedback How many clicks will a new ad placement system get? Will a different news-ranking algorithm increase the dwell times of my users? What ranking function will minimize abandon-ment? These and similar questions are crucial to consider when building good search engines, recommendation sys-tems, ad placement systems and most other systems that interact with users.

One approach to address these questions is to field the new system in an A/B test. Unfortunately, this process is slow, costly and puts tight limits on the number of systems that can be fielded. However, we typically have large amounts of log-data from previous versions of our systems. Can we use this old data to evaluate new systems?
Trying to answer operational questions about a new sys-tem B with log-data from an old system A is essentially a counterfactual problem: how would system B have per-formed on some user-centered metric, if it had been oper-ational instead of system A? (e.g. how many clicks would the new ad placement system have gotten, if it had been deployed in the past?) While it may seem like the logs from system A will be insufficient for estimating the properties of system B, research over the past years [21, 3, 7, 13, 23] has shown the opposite: the logs of system A can provide unbi-ased counterfactual estimates for the performance of system B, even though B has never been fielded.

This opens up a wide area of research on new evaluation and learning techniques for information retrieval. Unlike deploying new systems in weeks-long A/B tests to get unbi-ased performance estimates [10], counterfactual estimators can provide unbiased estimates from existing logs without delay. Analogously, counterfactual learning techniques can use existing log files for learning improved systems, without any need for online learning methods that require interac-tive control. In this way, this new approach to evaluation and learning can revolutionize the process of developing and optimizing systems that interact with users.

Research on counterfactual estimation (also called esti-mation of treatment effects and off-policy evaluation) and counterfactual learning (also called policy optimization) has made rapid progress in recent years [8, 3, 7, 6, 2, 13, 22, 9, 11, 14, 15, 19, 24, 12, 18], and it is starting to have im-pact in commercial applications. The recent workshop at WWW2015 on Online and Offline Evaluation of Web-based Services [8] highlights keen interest in these techniques from academia and industry alike. However, this research has happened in many rather disjoint fields, including statistics [16], economics [1], reinforcement learning [20], contextual bandit learning [21], Monte Carlo estimation [17] and infor-mation retrieval [23, 5, 4]. It is an excellent time to aggregate and unify the existing works into one coherent tutorial that is broadly accessible to an information retrieval audience. This tutorial will provide an overview of the rapidly grow-ing body of research on offline evaluation and learning using online metrics. A confluence of developments in machine learning, causal inference, economics, and information re-trieval has recently pushed these methods to be well-founded and practical. This tutorial will provide a unifying view that will enable newcomers to enter the field, enabling broader research and further adoption of these methods in practice. The objectives of the tutorial are: 1. Provide a unified view of counterfactual estimation and 2. Translate the disparate terminology from economics, 3. Give an overview of counterfactual evaluation tech-4. Give an overview of counterfactual learning techniques 5. Demonstrate strengths and limitations of counterfac-6. Provide publicly available benchmark datasets to en-7. Outline directions for future research.
 This tutorial is aimed at an audience with intermediate ex-perience with information retrieval. It assumes the following prerequisites: All code samples demonstrating counterfactual analysis for IR will be in Python3. Participants who wish to run these demos locally must bring a device capable of running Python3 scripts. Supplementary material for this tutorial is available online at http://www.cs.cornell.edu/  X adith/CfactSIGIR2016. Thorsten Joachims is a Professor in the Department of Computer Science and in the Department of Information Science at Cornell University. His research interests center on a synthesis of theory and system building in machine learning, with applications in information retrieval and rec-ommendation. His past research focused on support vector machines, learning to rank, learning with preferences, and learning from implicit feedback, text classification and struc-tured output prediction. He is an ACM Fellow, AAAI Fellow and Humboldt Fellow.

Adith Swaminathan is a PhD candidate in the Depart-ment of Computer Science at Cornell University, advised by Prof. Thorsten Joachims. His research interests are at the core of this tutorial, focusing on principles and algorithms for off-policy evaluation and learning for retrieval and rec-ommendation systems. He received a BTech degree in Com-puter Science and Engineering from IIT Bombay in 2010 and MSc in Computer Science from Cornell University in 2014. We acknowledge and thank for the support under NSF Awards IIS-1247637, IIS-1217686, and IIS-1513692, as well as a gift from Bloomberg. [1] S. Athey and G. Imbens. Recursive Partitioning for [2] A. Beygelzimer and J. Langford. The offset tree for [3] L. Bottou, J. Peters, J. Q. Candela, D. X. Charles, [4] B. Carterette, E. Kanoulas, V. Pavlu, and H. Fang. [5] B. Carterette, E. Kanoulas, and E. Yilmaz. Advances [6] M. Dud  X  X k, D. Erhan, J. Langford, and L. Li. Doubly [7] M. Dud  X  X k, J. Langford, and L. Li. Doubly robust [8] N. Gupta, E. Koh, and L. Li. Workshop on online and [9] K. Hofmann, A. Schuth, S. Whiteson, and [10] R. Kohavi, R. Longbotham, D. Sommerfield, and [11] J. Langford, A. Strehl, and J. Wortman. Exploration [12] L. Li, S. Chen, J. Kleban, and A. Gupta.
 [13] L. Li, W. Chu, J. Langford, and X. Wang. Unbiased [14] L. Li, R. Munos, and C. Szepesvari. Toward minimax [15] J. Mary, P. Preux, and O. Nicol. Improving offline [16] P. Rosenbaum and D. Rubin. The central role of the [17] R. Rubinstein and D. Kroese. Simulation and the [18] T. Schnabel, A. Swaminathan, A. Singh, N. Chandak, [19] A. L. Strehl, J. Langford, L. Li, and S. Kakade. [20] R. S. Sutton and A. G. Barto. Reinforcement [21] A. Swaminathan and T. Joachims. Counterfactual risk [22] A. Swaminathan and T. Joachims. The [23] E. Yilmaz, E. Kanoulas, and J. A. Aslam. A simple [24] B. Zadrozny, J. Langford, and N. Abe. Cost-sensitive
