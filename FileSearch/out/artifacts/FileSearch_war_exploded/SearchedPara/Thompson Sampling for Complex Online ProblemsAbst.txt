 Yishay Mansour MANSOUR @ TAU . AC . IL The stochastic Multi-Armed Bandit (MAB) is a classical framework in machine learning and optimization. In the basic MAB setting, there is a finite set of actions, each of which has a reward derived from some stochastic pro-cess, and a learner selects actions to optimize long-term performance. The MAB model gives a crystallized ab-straction of a fundamental decision problem  X  whether to explore or exploit in the face of uncertainty. Bandit problems have been extensively studied and several well-performing methods are known for optimizing the reward ( Gittins et al. , 2011 ; Auer et al. , 2002 ; Audibert &amp; Bubeck , 2009 ; Garivier &amp; Capp  X  e , 2011 ). However, the requirement that the actions X  rewards be independent is often a severe limitation, as seen in these examples: Web Advertising: Consider a website publisher selecting at each time a subset of ads to be displayed to the user. As the publisher is paid per click, it would like to maximize its revenue, but dependencies between different ads could mean that the problem does not  X  X ecompose nicely X . For instance, showing two car ads might not significantly in-crease the click probability over a single car ad. Job Scheduling: Assume we have a small number of re-sources or machines, and in each time step we receive a set of jobs (the  X  X asic arms X ), where the duration of each job follows some fixed but unknown distribution. The la-tency of a machine is the sum of the latencies of the jobs (basic arms) assigned to it, and the makespan of the system is the maximum latency across machines. Here, the deci-sion maker X  X  complex action is to partition the jobs (basic arms) between the machines, to achieve the least makespan on average.
 Routing: Consider a multi-commodity flow problem, where for each source-destination pair, we need to select a route (a complex action). In this setting the capacities of the edges (the basic arms) are random variables, and the re-ward is the total flow in the system at each time. In this ex-ample, the rewards of different paths are inter-dependent, since the flow on one path depends on which other paths where selected.
 These examples motivate settings where a model more complex than the simple MAB is required. Our high-level goal is to describe a methodology that can tackle bandit problems with complex action/reward structure, and also guarantee high performance. A crucial complication in the problems above is that it is unlikely that we will get to ob-serve the reward of each basic action chosen. Rather, we can hope to receive only an aggregate reward for the com-plex action taken. Our approach to complex bandit prob-lems stems from the idea that when faced with uncertainty, pretending to be Bayesian can be advantageous. A purely Bayesian view of the MAB assumes that the model param-eters (i.e., the arms X  distributions) are drawn from a prior distribution. We argue that even in a frequentist setup, in which the stochastic model is unknown but fixed, working with a fictitious prior over the model (i.e., being pseudo-Bayesian ) helps solve very general bandit problems with complex actions and observations.
 Our algorithmic prescription for complex bandits is Thompson sampling ( Thompson , 1933 ; Scott , 2010 ; Agrawal &amp; Goyal , 2012 ): Start with a fictitious prior distri-bution over the parameters of the basic arms of the model, whose posterior gets updated as actions are played. A pa-rameter is randomly drawn according to the posterior and the (complex) action optimal for the parameter is played. The rationale behind this is twofold: (1) Updating the pos-terior adds useful information about the true unknown pa-rameter. (2) Correlations among complex bandit actions (due to their dependence on the basic parameters) are im-plicitly captured by posterior updates on the space of basic parameters.
 The main advantage of a pseudo-Bayesian approach like Thompson sampling, compared to other MAB methodolo-gies such as UCB, is that it can handle a wide range of information models that go beyond observing the individ-ual rewards alone. For example, suppose we observe only the final makespan in the multi-processor job scheduling problem above. In Thompson sampling, we merely need to compute a posterior given this observation and its like-lihood. In contrast, it seems difficult to adapt an algorithm such as UCB for this case without a naive exponential de-pendence on the number of basic arms 1 . Besides, the de-terministic approach of optimizing over regions of the pa-rameter space that UCB-like algorithms follow ( Dani et al. , 2008 ; Abbasi-Yadkori et al. , 2011 ) is arguably harder to apply in practice, as opposed to optimizing over the action space given a sampled parameter in Thompson sampling  X  often an efficient polynomial-time routine like a sort. The Bayesian view that motivates Thompson sampling also al-lows us to use efficient numerical algorithms such as parti-cle filtering ( Ristic et al. , 2004 ; Doucet et al. , 2001 ) to ap-proximate complicated posterior distributions in practic e. Our main analytical result is a general regret bound for Thompson sampling in complex bandit settings. No spe-cific structure is imposed on the initial (fictitious) prior, ex-cept that it be discretely supported and put nonzero mass on the true model. The bound for this general setting scales logarithmically with time 2 , as is well-known. But more interestingly, the preconstant for this logarithmic scali ng can be explicitly characterized in terms of the bandit X  X  KL divergence geometry and represents the information com-plexity of the bandit problem. The standard MAB imposes no structure among the actions, thus its information com-plexity simply becomes a sum of terms, one for each sepa-rate action. However, in a complex bandit setting, rewards are often more informative about other parameters of the model, in which case the bound reflects the resulting cou-pling across complex actions.
 Recent work has shown the regret-optimality of Thomp-son sampling for the basic MAB ( Agrawal &amp; Goyal , 2012 ; Kaufmann et al. , 2012 ), and has even provided regret bounds for a specific complex bandit setting  X  the linear bandit case where the reward is a linear function of the actions ( Agrawal &amp; Goyal , 2011 ). However, the analysis of complex bandits in general poses challenges that can-not be overcome using the specialized techniques in these works. Indeed, these existing analyses rely crucially on the conjugacy of the prior and posterior distributions  X  ei-ther independent Beta or exponential family distributions for basic MAB or standard normal distributions for linear bandits. These methods break down when analyzing the evolution of complicated posterior distributions which of -ten lack even a closed form expression.
 In contrast to existing regret analyses, we develop a novel proof technique based on looking at the form of the Bayes posterior. This allows us to track the posterior distributi ons that result from general action and feedback sets, and to express the concentration of the posterior as a constrained optimization problem in path space. It is rather surprising that, with almost no specific structural assumptions on the prior, our technique yields a regret bound that reduces to Lai and Robbins X  classic lower bound for standard MAB, and also gives non-trivial and improved regret scalings for complex bandits. In this vein, our results represent a gen-eralization of existing performance results for Thompson sampling.
 We complement our theoretical findings with numerical studies of Thompson sampling. The algorithm is imple-mented using a simple particle filter ( Ristic et al. , 2004 ) to maintain and sample from posterior distributions. We evaluate the performance of the algorithm on two complex bandit scenarios  X  subset selection from a bandit and job scheduling.
 Related Work: Bayesian ideas for the multi-armed ban-dit date back nearly 80 years ago to the work of W. R. Thompson ( Thompson , 1933 ), who introduced an el-egant algorithm based on posterior sampling. However, there has been relatively meager work on using Thomp-son sampling in the control setup. A notable exception is ( Ortega &amp; Braun , 2010 ) that develops general Bayesian control rules and demonstrates them for classic bandits and Markov decision processes (i.e., reinforcement learning) . On the empirical side, a few recent works have demon-strated the success of Thompson sampling ( Scott , 2010 ; Chapelle &amp; Li , 2011 ). Recent work has shown frequentist-style regret bounds for Thompson sampling in the standard bandit model ( Agrawal &amp; Goyal , 2012 ; Kaufmann et al. , 2012 ; Korda et al. , 2013 ), and Bayes risk bounds in the purely Bayesian setting ( Osband et al. , 2013 ). Our work differs from this literature in that we go beyond simple, decoupled actions/observations  X  we focus on the perfor-mance of Thompson setting in a general action/feedback model, and show novel frequentist regret bounds that ac-count for the structure of complex actions.
 Regarding bandit problems with actions/rewards more complex than the basic MAB, a line of work that de-serves particular mention is that of linear bandit optimiza -tion ( Auer , 2003 ; Dani et al. , 2008 ; Abbasi-Yadkori et al. , 2011 ). In this setting, actions are identified with deci-sion vectors in a Euclidean space, and the obtained rewards are random linear functions of actions, drawn from an un-known distribution. Here, we typically see regret bounds for generalizations of the UCB algorithm that show poly-logarithmic regret for this setting. However, the methods and bounds are highly tailored to the specific linear feed-back structure and do not carry over to other kinds of feed-back. We consider a general stochastic model X 1 , X 2 , ... of in-dependent and identically distributed random variables liv-ing in a space X (e.g., X = R N if there is an underlying N -armed basic bandit  X  we will revisit this in detail in Sec-tion 4.1 ). The distribution of each X t is parametrized by  X   X   X   X  , where  X  denotes the parameter space. At each time t , an action A t is played from an action set A , fol-lowing which the decision maker obtains a stochastic ob-servation Y t = f ( X t , A t )  X  Y , the observation space, and a scalar reward g ( f ( X t , A t )) . Here, f and g are general fixed functions, and we will often denote g  X  f by the func-tion 3 h . We denote by l ( y ; a,  X  ) the likelihood of observing y upon playing action a when the distribution parameter is  X  , i.e., 4 l ( y ; a,  X  ) := P  X  [ f ( X 1 , a ) = y ] . For  X   X   X  , let a  X  (  X  ) be an action that yields the high-est expected reward for a model with parameter  X  , i.e., a (  X  ) := arg max a  X  X  E  X  [ h ( X 1 , a )] . 5 . We use e note the j -th unit vector in finite-dimensional Euclidean space.
 The goal is to play an action at each time t to min-imize the (expected) regret over T rounds: R T := P number of plays of suboptimal actions 6 : P T t =1 1 { A t a } .
 Remark: Our main result also holds in a more general stochastic bandit model ( X  , Y , A , l,  X  h ) without the need for the underlying  X  X asic arms X  { X i } i and the basic ambient space X . In this case we require l ( y ; a,  X  ) := P  X  [ Y y | A 1 = a ] ,  X  h : Y  X  R (the reward function), a  X  (  X  ) := arg max a  X  X  E  X  [  X  h ( Y 1 ) | A 1 = a ] , and the regret R T  X  h ( Y 0 )  X  P T t =1  X  h ( Y t ) where P [ Y 0 =  X  ] = l (  X  ; a For each action a  X  X  , define S a := {  X   X   X  : a  X  (  X  ) = a } to be the decision region of a , i.e., the set of models in  X  whose optimal action is a . We use  X  a to denote the marginal probability distribution, under model  X  , of the output upon Algorithm 1 Thompson Sampling Input: Parameter space  X  , action space A , output space Y , likelihood l ( y ; a,  X  ) .
 Parameter: Distribution  X  over  X  .
 Initialization: Set  X  0 =  X  . for each t = 1 , 2 , . . . 1. Draw  X  t  X   X  according to the distribution  X  t  X  1 . 2. Play A t = a  X  (  X  t ) := arg max a  X  X  E  X  t [ h ( X 1 3. Observe Y t = f ( X t , A t ) . 4. (Posterior Update) Set the distribution  X  t over  X  to end for playing action a , i.e., { l ( y ; a,  X  ) : y  X  Y} . Moreover, set D  X  := ( D (  X   X  a ||  X  a )) a  X  X  . Within S a , let S  X  a be the models that exactly match  X   X  in the sense of the marginal distribu-tion of action a  X  , i.e., S  X  a := {  X   X  S a : D (  X   X  a  X  0 } , where D (  X  ||  X  ) is the standard Kullback-Leibler di-vergence between probability distributions  X  and  X  . Let S a := S a \ S We propose using Thompson sampling (Algorithm 1 ) to play actions in the general bandit model. Before formally stating the regret bound, we present an intuitive explanati on of how Thompson sampling learns to play good actions in a general setup where observations, parameters and actions are related via a general likelihood. To this end, let us as-sume that there are finitely many actions A . Let us also index the actions in A as { 1 , 2 , . . . , |A|} , with the index |A| denoting the optimal action a  X  (we will require this indexing later when we associate each coordinate of |A| -dimensional space with its respective action).
 When action A t is played at time t , the prior den-sity gets updated to the posterior as  X  t ( d X  )  X  tional expectation of the  X  X nstantaneous X  log-likelihood ra-a } D (  X   X  a ||  X  a ) . Hence, up to a coarse approximation, with which we can write with N t ( a ) := P t i =1 1 { A i = a } denoting the play count of a . The quantity in the exponent can be interpreted as a  X  X oss X  suffered by the model  X  up to time t , and each time an action a is played,  X  incurs an additional loss of essentially the marginal KL divergence D (  X   X  a ||  X  a ) . Upon closer inspection, the posterior approxi-mation ( 1 ) yields detailed insights into the dy-namics of posterior-based sampling. First, since exp  X  P a  X  X  N t ( a ) D (  X   X  a ||  X  a )  X  1 , the true model  X   X  always retains a significant share of posterior mass:  X  Thompson sampling samples  X   X  , and hence plays a  X  , with at least a constant probability each time, so that N ( a  X  ) =  X ( t ) .
 Suppose we can show that each model in any S  X  X  a , a 6 = a is such that D (  X   X  a  X  ||  X  a  X  ) is bounded strictly away from 0 with a gap of  X  &gt; 0 . Then, our preceding calcula-tion immediately tells us that any such model is sampled at time t with a probability exponentially decaying in t :  X  is negligible . On the other hand, how much does the algo-rithm have to work to make models in S  X  a , a 6 = a  X  suffer large (  X  log T ) losses and thus rid them of significant pos-terior probability? 7 A model  X   X  S  X  a suffers loss whenever the algorithm plays an action a for which D (  X   X  a ||  X  a ) &gt; 0 . Hence, several ac-tions can help in making a bad model (or set of models) suf-fer large enough loss. Imagine that we track the play count vector N t := ( N t ( a )) a  X  X  in the integer lattice from t = 0 through t = T , from its initial value N 0 = (0 , . . . , 0) . There comes a first time  X  1 when some action a 1 6 = a  X  eliminated (i.e., when all its models X  losses exceed log T ). The argument of the preceding paragraph indicates that the play count of a 1 will stay fixed at N  X  1 ( a 1 ) for the remain-der of the horizon up to T . Moving on, there arrives a time  X  2  X   X  1 when another action a 2 /  X  { a  X  , a 1 } is eliminated, at which point its play count ceases to increase beyond N To sum up: Continuing until all actions a 6 = a  X  (i.e., the re-gions S  X  a ) are eliminated, we have a path-based bound for the total number of times suboptimal actions can be played. If we let z k = N  X  k , i.e., the play counts of all actions at time  X  k , then for all i  X  k we must have the constraint z ( a k ) = z k ( a k ) as plays of a k do not occur after time  X  nated precisely at time  X  k . A bound on the total number of bad plays thus becomes The final constraint above ensures that an action a k is elim-inated at time  X  k , and the penultimate constraint encodes the fact that the eliminated action a k is not played after time  X  . The bound not only depends on log T but also on the KL-divergence geometry of the bandit, i.e., the marginal divergences D (  X   X  a ||  X  a ) . Notice that no specific form for the prior or posterior was assumed to derive the bound, save mass on the truth.
 In fact, all our approximate calculations leading up to the bound ( 2 ) hold rigorously  X  Theorem 1 , to follow, states that under reasonable conditions on the prior, the number of suboptimal plays/regret scales as ( 2 ) with high probabil-ity. We will also see that the general bound ( 2 ) is non-trivial in that (a) for the standard multi-armed bandit, it gives es-sentially the optimum known regret scaling, and (b) for a family of complex bandit problems, it can be significantly less than the one obtained by treating all actions separatel y. Our main result is a high-probability large-horizon regret bound 8 for Thompson sampling. The bound holds under the following mild assumptions about the parameter space  X  , action space |A| , observation space |Y| , and the ficti-tious prior  X  .
 Assumption 1 (Finitely many actions, observations) . |A| , |Y| &lt;  X  .
 Assumption 2 (Finitely supported,  X  X rain of truth X  prior) . (a) The prior distribution  X  is supported over a finite set: |  X  | &lt;  X  , (b)  X   X   X   X  and  X  (  X   X  ) &gt; 0 . Furthermore, (c) there exists  X   X  (0 , 1 / 2) such that  X   X  l ( y ; a,  X  )  X  1  X   X   X   X   X   X  , a  X  X  , y  X  X  .
 Remark: We emphasize that the finiteness assumption on the prior is made primarily for technical tractability, wit h-out compromising the key learning dynamics of Thompson sampling perform well. In a sense, a continuous prior can be approximated by a fine enough discrete prior without affecting the geometric structure of the parameter space. The core ideas driving our analysis explain why Thompson sampling provably performs well in very general action-observation settings, and, we believe, can be made general enough to handle even continuous priors/posteriors. How-ever, the issues here are primarily measure-theoretic: muc h finer control will be required to bound and track posterior probabilities in the latter case, perhaps requiring the des ign of adaptive neighbourhoods of  X   X  with sufficiently large posterior probability that depend on the evolving history o f the algorithm. It is not clear to us how such regions may be constructed for obtaining regret guarantees in the case of continuous priors. We thus defer this highly nontrivial task to future work.
 Assumption 3 (Unique best action) . The optimal ac-tion in the sense of expected reward is unique 9 , i.e., We now state the regret bound for Thompson sampling for general stochastic bandits. The bound is a rigorous version of the path-based bound presented earlier, in Section 3 . Theorem 1 (General Regret Bound for Thompson Sam-pling) . Under Assumptions 1 -3 , the following holds for the Thompson Sampling algorithm. For  X ,  X   X  (0 , 1) , there exists T  X   X  0 such that for all T  X  T  X  , with probabil-ity at least 1  X   X  , P a 6 = a  X  N T ( a )  X  B + C (log T ) , where B  X  B (  X ,  X , A , Y ,  X  ,  X  ) is a problem-dependent constant that does not depend on T , and 10 : The proof is in the Appendix of the supplementary mate-rial, and uses a recently developed self-normalized con-centration inequality ( Abbasi-Yadkori et al. , 2011 ) to help track the sample path evolution of the posterior distribu-tion in its general form. The power of Theorem 1 lies in the fact that it accounts for coupling of information across complex actions and give improved structural constants for the regret scaling than the standard decoupled case, as we show 11 in Corollaries 1 and 2 . We also prove Proposition 2 , which explicitly quantifies the improvement over the naive regret scaling for general complex bandit problems as a function of marginal KL-divergence separation in the pa-rameter space  X  . 4.1. Playing Subsets of Bandit Arms and Observing Let us take a standard N -armed Bernoulli bandit with arm parameters  X  1  X   X  2  X   X  X  X   X   X  N . Suppose the (com-plex) actions are all size M subsets of the N arms. Fol-lowing the choice of a subset, we get to observe the re-wards of all M chosen arms (also known as the  X  X emi-bandit X  setting ( Audibert et al. , 2011 )) and receive some bounded reward of the chosen arms (thus, Y = { 0 , 1 } M , A = { S  X  [ N ] : | S | = M } , f (  X  , A ) is simply the projec-tion onto coordinates of A  X  X  , and g : R M  X  [0 , 1] , e.g., average or sum).
 A natural finite prior for this problem can be ob-tained by discretizing each of the N basic dimen-sions and putting uniform mass over all points:  X  = n  X , 2  X , . . .  X  1  X   X  X  X  1  X  o  X   X   X   X  . We can then show, using Theorem 1 , that Corollary 1 (Regret for playing subsets of basic arms, Full feedback) . Suppose  X   X  (  X  1 ,  X  2 , . . . ,  X  N )  X   X  and  X  N  X  M &lt;  X  N  X  M +1 . Then, the follow-ing holds for the Thompson sampling algorithm for Y , A , f , g ,  X  and  X  as above. For  X ,  X   X  (0 , 1) , there exists T  X   X  0 such that for all T  X  T  X  , with probability at least 1  X   X  , P a 6 = a  X  N T ( a )  X  B B (  X ,  X , A , Y ,  X  ,  X  ) is a problem-dependent constant that does not depend on T .
 This result, proved in the Appendix of the supplemen-tary material, illustrates the power of additional informa -tion from observing several arms of a bandit at once. Even though the total number of actions N M is at worst exponential in M , the regret bound scales only as O (( N  X  M ) log T ) . Note also that for M = 1 (the standard MAB setting), the regret scaling is essentially P optimal regret scaling for standard Bernoulli bandits ob-tained by specialized algorithms for decoupled bandit arms such as KL-UCB ( Garivier &amp; Capp  X  e , 2011 ) and, more re-cently, Thompson Sampling with the independent Beta prior ( Kaufmann et al. , 2012 ). 4.2. A General Regret Improvement Result &amp; Using the same setting and size-M subset actions as be-fore but not being able to observe all the individual arms X  rewards results in much more challenging bandit settings. Here, we consider the case where we get to observe as the reward only the maximum value of M chosen arms of a standard N -armed Bernoulli bandit (i.e., f ( x, A ) := max i  X  A x i and g : R  X  R , g ( x ) = x ). The feedback is still aggregated across basic arms, but at the same time very different from the full information case, e.g., observ -ing a reward of 0 is very uninformative whereas a value of 1 is highly informative about the constituent arms. We can again apply the general machinery provided by Theorem 1 to obtain a non-trivial regret bound for observ-ing the highly nonlinear MAX reward. Along the way, we derive the following consequence of Theorem 1 , useful in its own right, that explicitly guarantees an improvement in regret directly based on the Kullback-Leibler resolvabili ty of parameters in the parameter space  X  a measure of cou-pling across complex actions.
 Proposition 2 (Explicit Regret Improvement Based on Marginal KL-divergences) . Let T be large enough such that for every a 6 = a  X  and  X   X  S  X  a , |{  X  a  X  A :  X  a 6 = a , D (  X   X   X  a ||  X   X  a )  X   X  }|  X  L , i.e., at least L coordinates of D  X  (excluding the |A| -th coordinate a  X  ) are at least  X  . Note that the result assures a non-trivial additive reducti on of  X  L  X  log T from the naive decoupled regret, when-ever any suboptimal model in  X  can be resolved apart from  X   X  by at least L actions in the sense of marginal KL-divergences of their observations. Its proof is contained i n the Appendix in the supplementary material.
 Turning to the MAX reward bandit, let  X   X  (0 , 1) , and suppose that  X  = { 1  X   X  R , 1  X   X  R  X  1 , . . . , 1  X   X  2 , 1  X   X  } N , for positive integers R and N . As before, let  X   X   X  denote the basic arms X  parameters, and let  X  min := action and observation spaces A and Y are the same as those in Section 4.1 , but the feedback function here is f ( x, a ) := max i  X  a x i , and g is the identity on R . An ap-plication of our general regret improvement result (Propo-sition 2 ) now gives, for the highly nonlinear MAX reward function, Corollary 2 (Regret for playing subsets of basic arms, MAX feedback) . The following holds for the Thomp-son sampling algorithm for Y , A , f , g ,  X  and  X  as above. For 0  X  M  X  N , M 6 = N 2 ,  X ,  X   X  (0 , 1) , there exists T  X   X  0 such that for all T  X  T  X  , with probability at least 1  X   X  , P a 6 = a  X  N T ( a )  X  B 3 + Observe that this regret bound is of the order of decoupled bound of |A| log T  X  2 tive factor of the regret scaling, the actual reduction is likely to be much better in practice  X  the experimental results in Section 5 attest to this. The proof of this result uses sharp combina-torial estimates relating to vertices on the N -dimensional hypercube ( Ahlswede et al. , 2003 ), and can be found in the Appendix in the supplementary material. We evaluate the performance of Thompson sampling (Al-gorithm 1 ) on two complex bandit settings  X  (a) Play-ing subsets of arms with the MAX reward function, and (b) Job scheduling over machines to minimize makespan. Where the posterior distribution is not closed-form, we approximate it using a particle filter ( Ristic et al. , 2004 ; Doucet et al. , 2001 ), allowing efficient updates after each play. 1. Subset Plays, MAX Reward: We assume the setup of Section 4.2 where one plays a size-M subset in each round and observes the maximum value. The individ-ual arms X  reward parameters are taken to be equi-spaced in (0 , 1) . It is observed that Thompson sampling outper-forms standard  X  X ecoupled X  UCB by a wide margin in the cases we consider (Figure 1 , left and center). The differ-ences are especially pronounced for the larger problem size N = 1000 , M = 3 , where UCB, that sees N M separate ac-tions, appears be in the exploratory phase throughout. Figure 2 affords a closer look at the regret for the above problem, and presents the results of using a flat prior over a uniformly discretized grid of models in [0 , 1] 10  X  the setting of Theorem 1 . 2. Subset Plays, Average Reward: We apply Thompson sampling again to the problem of choosing the best M out of N basic arms of a Bernoulli bandit, but this time re-ceiving a reward that is the average value of the chosen subset. This specific form of the feedback makes it possi-ble to use a continuous, Gaussian prior density over the space of basic parameters that is updated to a Gaussian posterior assuming a fictitious Gaussian likelihood model ( Agrawal &amp; Goyal , 2011 ). This is a fast, practical alterna-tive to UCB-style deterministic methods ( Dani et al. , 2008 ; Abbasi-Yadkori et al. , 2011 ) which require performing a convex optimization every instant. Figure 3 shows the re-gret of Thompson sampling with a Gaussian prior/posterior for choosing various size M subsets ( 5 , 10 , 20 , 50 ) out of N = 100 arms. It is practically impossible to naively ap-ply a decoupled bandit algorithm over such a problem due to the very large number of complex actions (e.g., there are  X  10 13 actions even for M = 10 ) 12 . However, Thompson sampling merely samples from a N = 100 dimensional Gaussian and picks the best M coordinates of the sample, which yields a dramatic reduction in running time. The constant factors in the regret curves are seen to be modest when compared to the total number of complex actions. 3. Job Scheduling: We consider a stochastic job-scheduling problem in order to illustrate the versatility o f Thompson sampling for bandit settings more complicated than subset actions. There are N = 10 types of jobs and 2 machines. Every job type has a different, unknown mean duration, with the job means taken to be equally spaced in [0 , N ] , i.e., iN N +1 , i = 1 , . . . , N . At each round, one job of each type arrives to the scheduler, with a random duration that follows the exponential distribution with th e corresponding mean. All jobs must be scheduled on one of two possible machines. The loss suffered upon scheduling is the makespan , i.e., the maximum of the two job dura-tions on the machines. Once the jobs in a round are as-signed to the machines, only the total durations on the ma-chines machines can be observed, instead of the individual job durations. Figure 1 (right) shows the results of applying Thompson sampling with an exponential prior for the jobs X  means along with a particle filter. We applied Thompson sampling to balance exploration and exploitation in bandit problems where the ac-tion/observation space is complex. Using a novel tech-nique of viewing posterior evolution as a path-based opti-mization problem, we developed a generic regret bound for Thompson sampling with improved constants that capture the structure of the problem. In practice, the algorithm is easy to implement using sequential Monte-Carlo methods such as particle filters.
 Moving forward, the technique of converting posterior con-centration to an optimization involving exponentiated KL divergences could be useful in showing adversarial regret bounds for Bayesian-inspired algorithms. It is reasonable to posit that Thompson sampling would work well in a range of complex learning settings where a suitable point estimate is available. As an example, optimal bidding for online repeated auctions depending on continuous bid re-ward functions can be potentially learnt by constructing an estimate of the bid curve.
 Another unexplored direction is handling large scale reinforcement learning problems with complex, state-dependent Markovian dynamics. It would be promising if computationally demanding large-state space MDPs could be solved using a form of Thompson sampling by policy iteration after sampling from a parameterized set of MDPs; this has previously been shown to work well in practice ( Poupart , 2010 ; Ortega &amp; Braun , 2010 ). We can also at-tempt to develop a theoretical understanding of pseudo-Bayesian learning for complex spaces like the X -armed bandit problem ( Srinivas et al. , 2010 ; Bubeck et al. , 2011 ) with a continuous state space. At a fundamental level, this could result in a rigorous characterization of Thomp-son sampling/pseudo-Bayesian procedures in terms of the value of information per learning step.
 Abbasi-Yadkori, Yasin, Pal, David, and Szepesvari, Csaba.
Improved algorithms for linear stochastic bandits. In Ad-vances in Neural Information Processing Systems 24 , pp. 2312 X 2320, 2011.
 Agrawal, Shipra and Goyal, Navin. Thompson sampling for contextual bandits with linear payoffs. In Advances in Neural Information Processing Systems 24 , pp. 2312 X  2320, 2011.
 Agrawal, Shipra and Goyal, Navin. Analysis of Thompson sampling for the multi-armed bandit problem. Journal of Machine Learning Research -Proceedings Track , 23: 39.1 X 39.26, 2012.
 Ahlswede, R., Aydinian, H., and Khachatrian, L. Maxi-mum number of constant weight vertices of the unit n-cube contained in a k-dimensional subspace. Combina-torica , 23(1):5 X 22, 2003. ISSN 0209-9683.
 Audibert, Jean-Yves and Bubeck, S  X  ebastien. Minimax policies for adversarial and stochastic bandits. In Con-ference on Learning Theory (COLT) , pp. 773 X 818, 2009. Audibert, Jean-Yves, Bubeck, S  X  ebastien, and Lugosi,
G  X  abor. Minimax policies for combinatorial prediction games. In Conference on Learning Theory (COLT) , pp. 107 X 132, 2011.
 Auer, Peter. Using confidence bounds for exploitation-exploration trade-offs. J. Mach. Learn. Res. , 3:397 X 422, 2003.
 Auer, Peter, Cesa-Bianchi, Nicol ` o, and Fischer, Paul. Finite-time analysis of the multiarmed bandit problem. Machine Learning , 47(2-3):235 X 256, 2002.
 Bubeck, S., Munos, R., Stoltz, G., and Szepesv  X  ari, C. X-armed bandits. J. Mach. Learn. Res. , 12:1655 X 1695, 2011.
 Chapelle, Olivier and Li, Lihong. An empirical evaluation of Thompson sampling. In NIPS-11 , 2011.
 Dani, Varsha, Hayes, Thomas P., and Kakade, Sham M. Stochastic linear optimization under bandit feedback. In
Conference on Learning Theory (COLT) , pp. 355 X 366, 2008.
 Doucet, A., Freitas, N. De, and Gordon, N. Sequential Monte Carlo Methods in Practice . Springer, 2001. Garivier, Aur  X  elien and Capp  X  e, Olivier. The KL-UCB algo-rithm for bounded stochastic bandits and beyond. Jour-nal of Machine Learning Research -Proceedings Track , 19:359 X 376, 2011.
 Gittins, J. C., Glazebrook, K. D., and Weber, R. R. Multi-Armed Bandit Allocation Indices . Wiley, 2011.
 Kaufmann, Emilie, Korda, Nathaniel, and Munos, R  X  emi.
Thompson sampling: An asymptotically optimal finite-time analysis. In Conference on Algorithmic Learning Theory (ALT) , 2012.
 Korda, Nathaniel, Kaufmann, Emilie, and Munos, Remi.
Thompson sampling for 1-dimensional exponential fam-ily bandits. In NIPS , 2013.
 Lai, T. L. and Robbins, Herbert. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathe-matics , 6(1):4 X 22, 1985.
 Ortega, P. A. and Braun, D. A. A minimum relative en-tropy principle for learning and acting. JAIR , 38:475 X  511, 2010.
 Osband, I., Russo, D., and Roy, B. Van. (More) efficient reinforcement learning via posterior sampling. In NIPS , 2013.
 Poupart, Pascal. Encyclopedia of Machine Learning . Springer, 2010. ISBN 978-0-387-30768-8.
 Ristic, B., Arulampalam, S., and Gordon, N. Beyond the
Kalman Filter: Particle Filters for Tracking Applica-tions . Artech House, 2004.
 Scott, S. A modern Bayesian look at the multi-armed ban-dit. Applied Stochastic Models in Business and Industry , 26:639 X 658, 2010.
 Srinivas, Niranjan, Krause, Andreas, Kakade, Sham, and
Seeger, Matthias. Gaussian process optimization in the bandit setting: No regret and experimental design. In ICML , pp. 1015 X 1022, 2010.
 Thompson, William R. On the likelihood that one unknown probability exceeds another in view of the evidence of
