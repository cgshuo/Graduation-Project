 This paper addresses the task of answering complex ques-tions using a multi-document summarization approach within a reinforcement learning setting. Given a set of complex questions, a list of relevant documents per question, and the corresponding human-generated summaries (i.e. answers to the questions) as training data, the reinforcement learning module iteratively learns a number of feature weights in order to facilitate the automatic generation of summaries i.e. answers to unseen complex questions. Previous works on this task have utilized a fully automatic reinforcement learning framework that selects the document sentences as the potential candidate (i.e. machine-generated) summary sentences by exploiting a relatedness measure with the avail-able human-written summaries. In this paper, we propose an extension to this model that incorporates user interac-tion into the reinforcement learner to guide the candidate summary sentence selection process. Experimental results reveal the effectiveness of the user interaction component in the reinforcement learning framework.
 H.3.m [ Information Storage and Retrieval ]: Miscella-neous; I.2.7 [ Artificial Intelligence ]: Natural Language Processing Algorithms, Performance, Experimentation Complex question answering, multi-document summariza-tion, reinforcement learning, user interaction
Users often ask questions in the context of a wider infor-mation need, for instance when researching a specific topic. This poses the problem of complex Question Answering (QA) that often relates to multiple entities, events and complex relations between them. For example, a complex question like  X  X ow was Haiti affected by the earthquake? X  often has a wider focus without a single or well-defined information need. Multi-document summarization techniques can be applied effectively to handle this type of questions [4]. In this paper, we consider the task of answering complex ques-tions using an extractive multi-document summarization ap-proach within a reinforcement learning setting. The major limitation of the current search engines is that they lack the way of measuring whether a user is satisfied with the in-formation provided. They also cannot improve their policy dynamically in real time [15]. This is the main motivation of applying the reinforcement approach [12] to the complex question answering domain. Given a set of complex ques-tions, a collection of relevant documents per question, and the corresponding human-generated summaries (i.e. answers to the questions), a reinforcement learning model can be trained to extract the most important sentences as system generated automatic summaries.

Previous works on this domain have used a reinforcement learning framework that verified the importance of an origi-nal document sentence by measuring its similarity with the abstract summary sentences using a reward function [1]. Their formulation was simplified with no user interaction as they took the assumption that the human-generated ab-stract summaries are the gold-standards and the users are satisfied with these summaries. Experiments in the complex interactive Question Answering (ciQA) task 1 at TREC-2007 demonstrate the significance of user interaction in this do-main. Motivated by the effect of user interaction shown in the previous studies [7, 14, 13], in this paper, we propose an extension to this reinforcement learning model by incorpo-rating user interaction into the learner and argue that the user interaction component can provide a positive impact in the candidate summary sentence selection process.
In [1], the complex question answering problem is for-mulated by estimating an action-value function. Given a complex question q and a collection of relevant documents D = { d 1 ,d 2 ,d 3 ,...,d n } , the task is to extract a summary (i.e. answer) automatically. The state is defined by the cur-rent status of the answer space. In each iteration, one sen-tence is added from the document to the answer pool that http://www.umiacs.umd.edu/  X  jimmylin/ciqa/ in turn changes the state. In each state, there is a possible set of actions where  X  X ction X  stands for selecting/choosing a particular sentence from the remaining document sentences that are not included so far in the candidate extract sum-mary. The value of taking an action a in the state s is defined under a policy  X  , denoted as Q  X  ( s,a ):
Here, E  X  denotes the expected value given that the agent follows policy  X  , R t is the expected return that is defined as a function of the reward sequence, r t +1 ,r t +2 ,  X  X  X  , where r is the numerical reward that the agent receives at time step, t . We call Q  X  the action-value function for policy  X  .  X  is the discount factor that determines the importance of future rewards. Once the optimal policy (  X   X  ) is found, the agent chooses the actions using the Maximum Expected Utility Principle [9]. As the number of states and actions are in-finite, the approximate action-value function is represented as a parameterized functional form with parameter vector, ~  X  . Corresponding to every state-action pair ( s,a ) , there is a column vector of features, ~ X  s = (  X  s (1) , X  s (2) ,..., X  with the same number of components as ~  X  t . The approxi-mate action-value function is given by: Q t ( s,a ) = ~  X  T t ~ X  s = P n i =1  X  t ( i )  X  s ( i )
In the training step of this reinforcement learning model, for computing the rewards, a fully automatic approach is used to select the document sentences as the potential can-didate (i.e. machine-generated) summary sentences by ex-ploiting a relatedness measure with the available human-written summaries using ROUGE (Recall-Oriented Under-study for Gisting Evaluation) [6]. A modified linear, gradient-descent version of Watkins X  Q (  X  ) algorithm [12] is applied to estimate the parameters of the model [1]. In this pa-per, we propose an extension to this reinforcement learning model by incorporating user interaction into the learner that can improve the performance of the reinforcement learning model by enhancing the efficiency of the candidate summary sentence selection process.
In our proposed model, for a certain number of itera-tions during the training stage of the reinforcement learning, the user is presented with the top five candidate sentences (based on the ROUGE similarity scores between the candi-date sentences and the human summaries). The user can also see the complex question being considered and the cur-rent status (content) of the answer space (i.e. state). The task of the user at this point is to select the best candidate among the five to be added to the answer space. In the reinforcement learning model of [1], the first candidate was selected to be added automatically as it was having the high-est similarity score. In this way, there was a chance that a potentially unimportant sentence could be chosen that is not of user X  X  interest. However, in our extended reinforcement learning model, the user interaction component enables us to incorporate the human viewpoint and thus, the judgment for the best candidate sentence is supposed to be perfect. The outcome of the reinforcement learner is a set of weights that are updated through several iterations until the algo-rithm converges. The user selects a sentence to add to the answer space and the feature weights are updated based on this response. The similar process runs up to three iterations for each topic during training. In the rest number of the it-erations, the algorithm selects the sentences automatically and continue updating the weights accordingly.
Each sentence of a document is represented as a vec-tor of feature-values. Our feature set includes two types of features, where one declares the importance of a sen-tence in a document and the other measures the similar-ity between each sentence and the user query. To mea-sure the importance of a sentence, we consider its position, length, and match with title, certain named entity and cue words. For query-related features, we consider n X  X ram over-lap, LCS, WLCS, skip-bigram, exact-word, synonym, hyper-nym/hyponym, gloss and Basic Element (BE) overlap, and syntactic features. These features have been adopted from several related works in the problem domain [3, 8, 10].
This paper deals with the complex question answering task defined in DUC 2 -2006. The task is as follows:  X  X iven a complex question (topic description) and a collection of relevant documents, the task is to synthesize a fluent, well-organized 250-word summary of the documents that answers the question(s) in the topic X  . We use an interactive reinforce-ment learning approach to generate topic-oriented 250-word extract summaries. Each topic and its document cluster was given to 4 different NIST 3 assessors, including the developer of the topic. Each assessor created a 250-word summary of the document cluster that satisfies the information need ex-pressed in the topic. In the reinforcement learning phase, these multiple reference summaries (also termed as human-generated abstracts) are compared with the original docu-ment sentences using ROUGE to rank the candidate docu-ment sentences in terms of similarity scores. For our experi-ments, we use the first 30 topics at most from the DUC-2006 data to learn the weights respective to each feature and then use these weights to produce extract summaries for the next 15 topics (test data).
The major objective of this research is to study the im-pact of the user interaction component in the reinforcement learning framework. To accomplish this purpose, we follow six different ways of learning the feature weights by vary-ing the amount of user interaction incorporated and the size of the training data: 1) SYS 0 20 , 2) SYS 10 20 , 3) SYS 20 0 , 4) SYS 20 10 , 5) SYS 30 0 , and 6) SYS 30 30 . The numbers in the system titles indicate how many user-interaction and non-user-interaction topics each system in-cluded during training, respectively. For example, the first system is trained with 20 topics of the DUC-2006 data with-out user interaction. Among these systems, the sixth sys-tem is different as it is trained with the first 30 topics of the DUC-2006 data without user interaction. The learned http://duc.nist.gov/ http://www.nist.gov/index.html weights that are found from the SYS 30 0 experiment are used as the initial weights of this system. This means that virtually the SYS 30 30 system is trained with 60 topics (30 topics with interaction from SYS 30 0 and 30 topics without interaction). The outcomes of all these systems are the sets of learned feature weights that are used to generate extract summaries (i.e. answers) for the last 15 topics (test data) of the DUC-2006 data set. So, after the six learn-ing experiments, we get six sets of learned feature weights which are used to generate six different sets of summaries for the test data (15 topics). We evaluate these six versions of summaries for the same topics and analyze the effect of user interaction in the reinforcement learning framework.
We evaluate the system generated summaries using the automatic evaluation toolkit ROUGE [6]. We report the two official ROUGE metrics of DUC-2006 in the results: ROUGE-2 (bigram) and ROUGE-SU (skip bigram). In Ta-ble 1, we compare the ROUGE-F scores of all the systems. In our experiments, the only two systems that were trained with 20 topics are SYS 0 20 and SYS 20 0 (the one has 20 unsupervised, the other has 20 supervised). From the results, we see that there is essentially no difference between their performance in terms of ROUGE-2 scores. However, the SYS 20 0 system improves the ROUGE-SU scores over the SYS 0 20 system by 0.47%. Again, we see that the SYS 20 10 system improves the ROUGE-2 and ROUGE-SU scores over the SYS 10 20 system (both systems had 30 topics but where SYS 20 10 had more human-supervised topics) by 0.96%, and 8.56%, respectively. We also find that the SYS 30 0 system improves the ROUGE-2 and ROUGE-SU scores over the SYS 20 10 system (both sys-tems had 30 topics with SYS 30 0 having more human su-pervision) by 0.25%, and 0.80%, respectively. So, the results show a clear trend of improvement when human interaction is incorporated. We can also see that the SYS 30 30 sys-tem is performing the best since it starts learning from the learned weights that are generated from the outcome of the SYS 30 0 setting. This denotes that the user interaction component has a positive impact on the reinforcement learn-ing framework that further controls the automatic learning process efficiently (after a certain amount of interaction has been incorporated). In table 2 and table 3, we report the 95% confidence intervals for ROUGE-2 and ROUGE-SU to show the significance of our results.

The automatic evaluation using ROUGE is not always reliable to all researchers [11]. So, we conduct an exten-sive manual evaluation of our systems. Two native English-speaking university graduate students judge the summaries for linguistic quality and overall responsiveness according to the DUC-2007 evaluation guidelines. They were blind to which system each output came from. The given linguis-
Table 3: 95% confidence intervals: ROUGE-SU tic quality score is an integer between 1 (very poor) and 5 (very good) and is guided by consideration of the following factors: 1. Grammaticality, 2. Non-redundancy, 3. Referen-tial clarity, 4. Focus, and 5. Structure and Coherence. The responsiveness score is also an integer between 1 (very poor) and 5 (very good) and is based on the amount of information in the summary that helps to satisfy the information need. The inter-annotator agreement of Cohen X  X   X  = 0 . 55 [2] was computed that denotes a moderate degree of agreement [5] between the raters. Table 4 presents the average linguis-tic quality and overall responsive scores of all our systems. Analyzing these results, we can clearly see the positive im-pact of the user interaction component in the reinforcement learning framework. The improvements in the results are statistically significant 4 ( p &lt; 0 . 05).
 Systems Linguistic Quality Overall Responsiveness SYS 0 20 2.92 3.20 SYS 10 20 3.45 3.40 SYS 20 0 3.12 3.39 SYS 20 10 3.50 3.72 SYS 30 0 3.68 3.84 SYS 30 30 3.96 4.10 Table 4: Linguistic quality and responsiveness scores
The main goal of the reinforcement learning phase is to learn the appropriate feature weights (See Section 4) that can be used in the testing phase. The effect of user feedback on the feature weights can be shown using a graph. We present the weights from different stages of the SYS 20 0 experiment in figure 1. Note that the SYS 20 0 system is trained with 20 topics of the DUC-2006 data with user in-teraction. The labels in the Y-axis refers to the features in the following order: 1) 1-gram overlap, 2) 2-gram overlap, 3) LCS, 4) WLCS, 5) exact word overlap, 6) synonym over-lap, 7) hypernym/hyponym overlap, 8) sentence length, 9) title match, 10) named entity match, 11) cue word match,
We tested statistical significance using Student X  X  t-test. 12) syntactic feature, and 13) BE overlap. Analyzing the graph, we find that at the end of the learning phase (end of topic-20), all the feature weights converge to zero except for 2-gram overlap and BE overlap. The zero weight val-ues suggest that the associated features can be eliminated because they do not contribute any relevant information to action (i.e. candidate sentence) selection. The graph veri-fies that the proposed reinforcement system is responsive to user interests and actions.
 Figure 1: Effect of user feedback on feature weights
We proposed an extension to the reinforcement learning model of answering complex questions by incorporating a user interaction component. Experiments revealed that the systems trained with user interaction perform considerably better and this trend continues with the increase of the train-ing data even using no interaction. This suggests that the system is capable to learn automatically (i.e. without inter-action) and effectively after a sufficient amount of user inter-action is provided as the guide to candidate answer sentence selection. A thorough automatic and manual evaluation of our systems proved this claim.
 The research reported in this paper was supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada X  X iscovery grant and the University of Lethbridge. [1] Y. Chali, S. A. Hasan, and K. Imam. A Reinforcement [2] J. Cohen. A Coefficient of Agreement for Nominal [3] H. P. Edmundson. New Methods in Automatic [4] S. Harabagiu, F. Lacatusu, and A. Hickl. Answering [5] J. R. Landis and G. G. Koch. The Measurement of [6] C. Lin. ROUGE: A Package for Automatic Evaluation [7] J. Lin, N. Madnani, and B. J. Dorr. Putting the user [8] M. Litvak, M. Last, and M. Friedman. A New [9] S. Russel and P. Norvig. Artificial Intelligence A [10] F. Schilder and R. Kondadadi. FastSum: Fast and [11] J. Sj  X  obergh. Older Versions of the ROUGEeval [12] R. S. Sutton and A. G. Barto. Reinforcement [13] M. Wu, F. Scholer, and A. Turpin. User preference [14] R. Yan, J. Nie, and X. Li. Summarize what you are [15] H. Zaragoza, B. B. Cambazoglu, and R. Baeza-Yates.
