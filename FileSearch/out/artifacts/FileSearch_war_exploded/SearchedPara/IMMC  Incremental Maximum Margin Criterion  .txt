 Subspace learning approaches have attracted much attention in academia recently. However, the classical batch algorithms no longer satisfy the applications on streaming data or large-scale data. To meet this desirability, Incremental Principal Component Analysis (IPCA) algorithm has been well established, but it is an unsupervised subspace learning approach and is not optimal for document categorization. In this paper, we propose an incremental supervised subspace learning algorithm, called Incremental Maximum Margin Criterion (IMMC), to infer an adaptive subspace by optimizing the Maximum Margin Criterion. We also present the proof for convergence of the proposed algorithm. Experimental IMMC converges to the similar subspace as that of batch approach. I.2.6 [ Artificial Intelligence ]: Learning G.1.6 [ Numerical Analysis ]: Constrained Optimization Algorithms, Theory Maximum Margin Criterion (MMC), Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA). In the past decades, machine learning and data mining research has applications, such as web document classification and face recognition. Among various subspace learning approaches, linear effectiveness. Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are two of the most widely used linear subspace learning algorithms. Furthermore, a novel efficient and robust subsp ace learning approach namely Maximum Margin Criterion (MMC) [4] was proposed recently. It can outperform PCA and LDA on many classification tasks. PCA is an unsupervised subspace learning algorithm. It aims at along the directions with maximal variances. However, it discards the class information, which is significant for classification tasks. On the other hand, LDA is a supervised subspace learning algorithm. It searches for the projection axes on which the data points of different classes limits the available subspace dimension in LDA, and the singularity problem limits the application of LDA. MMC is also a supervised subspace learning algorithm and it has the same goal as LDA. However the computational complexity of MMC is much lower than that of LDA due to the different form of object function. The original PCA, LDA, and MMC are all batch algorithms, which require that the data must be available in advance and be given once altogether. However, this type of batch algorithms no longer satisfies the applications in which the data are incrementally received from various data sources. Furthermore, when the dimension of the dataset is high, both the computation and storage complexity grow dramatically. Thus, an incremental method is highly desired to compute the adaptive subspace for the data arriving sequentially [5]. Incremental Principal Component Analysis (IPCA) [6] algorithms are designed for such a purpose and have been well established. However, IPCA ignores the valuable class label information. Accordingly, the most representative features derived from IPCA incremental supervised subspace learning algorithms have not been studied sufficiently. In this paper, we propose an incremental supervised subspace learning algorithm by incrementally optimizing the Maximum Margin Criterion called IMMC. It derives the online adaptive supervised subspace from sequential data samples and incrementally updates the eigenvectors of the criterion matrix. IMMC does not need to reconstruct the criterion matrix when it receives a new sample, thus the computation is very fast. We also prove the convergence of the algorithm in this paper. The rest of the paper is organized as follows. We introduce some background work on subsp ace learning, including PCA, IPCA, LDA, and MMC algorithms in Section 2. Then, we present the incremental subspace learning algorithm IMMC and the proof of its convergence real datasets are shown in Section 4. Finally, we concluded our work in Section 5, as well as some detailed proof in the appendix. Linear subspace learning approaches are widely used in real tasks such as web document classification and face recognition nowadays. It aims at finding a projection matrix, which could efficiently project the data from the original high dimensional feature space to a much lower dimensional representation under a particular criterion. Different criterion will yield different subspace learning algorithm with different properties. Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are two most widely used linear subspace learning approaches. Recently, Maximum Margin Criterion, a novel efficient and robust subsp ace learning approach has also been applied to many real tasks. original data into a p-dimensional ( p &lt;&lt; d ) subspace. The new low-the projection matrix and its column vectors correspond to the p minimizes the reconstruction error in the sense of least square error, and finds out the most representative features. Moreover, PCA is in algorithm, which could process large scale streaming data. However, it ignores the class label information; therefore, it is not optimal for general classification tasks. The computation cost of PCA, which is 3 () Om , mainly lies in the to conduct PCA on large scale dataset with high dimensional representations. Discriminant Analysis (FDA), was proposed to pursue a low dimensional subspace that can best discriminate the samples from LDA aims to maximize the so-called Fisher criterion, 
J WWSWWSW = ,where number of classes, m is the mean of all samples, i m is the mean of the samples belonging to class i and i p is the prior probability for a sample belonging to class i . The projection matrix W can be decomposition problem: application of LDA. Furthermore, it is difficult for LDA to handle large size datasets when the dimension of the feature space is high. real world problems. Incremental learning algorithms have attracted much attention in the past decades. Incremental PCA is a well-studied incremental leaning algorithm. Many types of IPCA have been proposed, and the main difference is the incremental representation of the covariance matrix. The latest version of IPCA [6] with convergence proof is called Candid Covariance-free Incremental Principal Component Analysis (CCIPCA) which does not need to reconstruct the covariance matrix at each iteration of the computation. It was designed based on the assumption that {()} E An A = , where dd AR  X   X  is of full rank and positive determined. Maximum Margin Criterion (MMC) is a recently pr oposed feature Discriminant Analysis. Using the same representation as LDA, the goal of MMC is to maximize the criterion () ( ) T bw J WWSSW = X  . Although both MMC and LDA are supervised subsp ace learning approaches, the computation of MMC is easier than that of LDA since MMC does not have inverse operation. The projection matrix decomposition problem: When computing, we can notice that the criterion matrix may even be negative determined. As discussed above, IPCA ignores the class label information. Thus the most representative features found by IPCA may not be the most discriminating ones which make IPCA not being optimal for general supervised subspace learning algorithm that can efficiently utilize the label information. In this work, we consider the scenario that maximizes the Maximum Margin Criterion proposed by Li [4] to make the data points in the same class as close as possible. In the following subsections, we will introduce the details on how to incrementally maximize the Maximum Margin Criterion. The convergence proof and algorithm summary are also presented. where b S and w S are the inter-class scatter matrix and intra-class scatter matrix respectively. Let C be the covariance matrix. In the above formulation, we exercised freedom to multiply W with some unit vectors, i.e. 12 [, , ] p Www w = L and 1 T kk optimization problem of the proposed object function () J W is transformed to the following constrained optimization problem: eigenvectors of the matrix bw SS  X  and the column vectors of W are orthogonal to each other. It shows that our problem is learning the p leading eigenvector of bw SS  X  incrementally. Before giving the incremental formulation of MMC, we analyze the criterion matrix bw SS  X  and transform it into a convenient form. Firstly, two lemmas are listed as: Lemma-1 : bw SS C += . Lemma-2 : if lim n Assume that a data sample sequence is presented as { ( )} n=1, 2... . The goal of MMC is to maximize the Maximum Margin of the transformed subspace. The Maximum Margin criterion can be transformed as () (2 ) T b J WW SCW = X  from Lemma-1. Then maximizing () J W means to find the p leading eigenvectors of 2 b SC  X  . The Inter-class scatter matrix of step n after learning from the first n samples can be written as below, From the fact that lim ( ) bb On the other hand, Assume that  X  is a positive real number and dd I R matrix, if  X  is an eigenvalue of matrix A and x is the corresponding eigenvector, then () () AIxAxIx x  X  X  X  X  +=+=+ , i.e. A should have the same eigenvectors with matrix AI  X  + . Further more, the eigenvalues are the same. Therefore, 2 b SC  X  should have the same eigenvectors as 2 b SC I  X   X + . From (2) and (3) we have the following conclusion: 2 lim (2 ( ) ( ( ) ( ))( ( ) ( )) )
SC I Si uimiuimi I where () 2 () ( () ())( () ()) T b Ai S i ui mi ui mi I Notice that we can consider matrix ( ) Ai as a random matrix, in other words we have The general eigenvector form is Ax x  X  = , where x is the eigenvector of matrix A corresponding to the eigenvalue  X  . By replacing matrix A with the Maximum Margin criterion matrix at step n , we obtain an approximate iterative eigenvector computation formulation with vx  X  = : vn S i ui mi ui mi I xi where () () () () xn is the th n step estimation of x . Once we obtain the estimation of v , eigenvector x can be directly computed as / xvv Let () xi = (1)/(1) vi vi  X  X  X  , we have the following incremental formulation: () ( 1) (2 () () () vn vn p n n n where () () ( 1) T jj nnvn  X  = X  X  and () (() ()) ( 1) Notice that different eigenvectors are orthogonal to each other. Thus it helps to generate  X  X bservations X  only in a complementary space for the computation of the higher order eigenvectors. To compute estimated th j eigenvector from the data, where 1 () () mn and ( ) j mn 1, 2,..., ic = . Since ( ) j i mn and ( ) combinations of ( ) lC  X  , i  X  are linear combination of i m and m , for convenience, we can only update  X  at each iteration step by: In this way, the time-consuming orthonormalization is avoided and the orthogonal is always enforced when the convergence is reached, although not exactly so at early stages. Through the projection procedure using (6) (7) at each step, we can get the eigenvectors of Maximum Margin criterion matrix one by one. It is much more efficient in comparison to the time-consuming orthonormalization process. The full algorithm consists of updating (5), (6) and (7) at each iteration. Theorem-1 shown as below guarantees the convergence of the proposed Incremental Maximum Margin Criterion algorithm 2 SC I  X   X + is non-negative determined. A similar theorem with proof can be found in [8]. Our convergence same as in [8]. Theorem-1 : If matrices sequence { ( )} An , () An &lt; X  converge to a matrix dd AR  X   X  , i.e. determined matrix and A &lt; X  , the eigenvalues of A satisfy 12 0 d  X  X  X  &gt; X  X  X  X  L , then the iterative process converges to corresponding eigenvector. Theorem-2 : Suppose () (1) ((1)) nnnnn vn vn ahvn ab a If A1 to A4 are all satisfied, let { v(n) } be bounded w.p.1. A 1 () h  X  is a continuous d R valued function on d R . A 2 {} n b is a bounded sequence of d R valued random variables such that 0 n b  X  when n  X  X  X  . A 3 {} n a is a sequence of positive real numbers such that 0 A 4 {} n  X  is a sequence of d R valued random variables and such that for some 0 T &gt; and each 0  X  &gt; Let * v be a locally asymptotically stable (in the sense of Liapunov) solution to equation () dX hX dt = with the domain of attraction () vn  X  X  infinitely often, we have * () vn v  X  as n  X  X  X  . (The origin form of this theorem and its proof can be found in [2].) Theorem-3 : Let * () vt v  X  be a locally asymptotically stable (in the sense of Liapunov) solution to the Ordinary Differential Equation as bellow: eigenvectors of A satisfies 12 0 d  X  X  X  &gt; X  X  X  X  () vt converges to 11 e  X  , 1 e is the eigenvector corresponding to The combination of theorem-2 and theorem-3 gives out the proof of theorem-1 which is in fact the convergence proof of our proposed Incremental Maximum Margin Criterion algorithm. It is easy to prove that the proposed algorithm satisfies the conditions of theorem 2. A 1, A 2 and A 3 are naturally satisfied and A.4 is satisfied due to lemma-3. The combination of Lemma-4 and the proof of [8] ends algorithm. Lemma-3 : ( ) vn is bounded, if (0) v is bounded. The proof of lemma-3 could be found in the appendix. Lemma-4 : dd AR  X   X  is a nonnegative determined matrix, corresponding to non-zero eigenvalues of A, if we expend {} normalized orthogonal basis of d R , 12 1 ,, , , mm d Ae = ,1,, jm d =+ L . Proof: set 0 jj yAe = X  ,1,, jm d =+ L , we have yspanei m  X = L and it conflicts with the fact that espanei m  X = L , this ends the proof of lemma-4. The time complexity of IMMC to train N input samples is ( ) ONcdp , factor. Furthermore, when handling each i nput sample, IMMC only need to keep the learned eigen-space and several first order statistics of the past samples, such as the mean and the counts. Hence, IMMC is able to handle large scale and continuous data stream. We performed three sets of experiments. Firstly, we used synthetic data to illustrate the subspaces learned by IMMC, LDA, and PCA intuitively. Secondly, we applied IMMC on some UCI subsets [1], and compared the results with the batch MMC approach that conducted by SVD, whose time complexity is 3 () Om , where m is the smaller number of the data dimension and the number of samples. Since the classification performance of MMC such as LDA has been discussed when it was proposed, we only focus on the convergence performance of IMMC to the batch MMC algorithm on UCI dataset. In the third dataset, the Reuters Corpus Volume 1 (RCV1) [3], a large scale dataset whose dimension is about 300,000, was used. We measured the performance of our algorithm by F1 value on it because the dataset is too large to c onduct the batch MMC on it. We generated a 2-dimension dataset of 2 classes. Each class consists of 50 samples from normal distribution with means (0, 1) and (0,-2), respectively; and the covariance matrices are diag (1, 25) and diag (2, lines are subspaces f ound by IMMC and PCA. Since the subsp ace found by MMC is the same as subsp ace by LDA in the case, we did not give out the LDA subspace. between two unit eigenvectors is represented by their inner product, and the larger the inner product is, the more similar the two eigenvectors are. Let us analyze this dataset to show the convergence of 2 b
ASC I  X  = X + are 0.25 and -84.42 and the corresponding eigenvectors are (0,-1) and (-1, 0). We choose 85  X  = that the criterion matrix is nonnegative determined. Figure 2 shows the convergence curve of our algorithm through inner product. UCI machine learning dataset is a repository of databases, domain theories and data generators that are used by the machine learning algorithms. Balance Scale Data was generated to model psychological experimental results. Balance Scale data set, the eigenvalues of 2 b ASC I  X  = X + -2.0 , -1.9774 , and 0.7067 . We choose 2.0  X  = to make sure the criterion matrix is nonnegative determined. Figure 3 shows the inner products of directions found by IMMC and CCIPCA [6]. In order to demonstrate the performance of IMMC on relative large scale data, we choose the Ionosphere database (figure 4). This radar data was collected by a system in Goose Bay, Labrador. This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. plus the class attribute. All 34 predictor attributes are continuous and the 35 th attribute is either  X  X ood X  or  X  X ad X  according to the definition. Since the smallest eigenvalue of this data set is very close to zero, we try taking the parameter 0  X  = in this experiment. Unfortunately, some experimental results show that IMMC could matrix 2 b ASC = X  is negative determined. This difficulty motivates us to propose a weighted Maximum Margin Criterion bw AS S Some advanced experiments show that the classical MMC ( 1 not usually optimal for classification tasks. In other words, a proper  X  could improve the performance of MMC and it could make sure that the criterion matrix is nonnegative determined. Then we could make the criterion matrix nonnegative determined by giving a smaller  X  instead of parameter  X  . To demonstrate the performance of IWMMC on a large scale dataset, we tested our algorithm on the Reuters Corpus Volume 1 (RCV1). Figure 2 Correlation between eigen-space of IMMC and batch The dimension of each data sample is a bout 300,000. We chose the data samples with the highest four topic codes in the  X  X opic Codes X  hierarchy, which contains 789,670 documents. Then we applied a five-fold cross validation on the data. We split them into five equal-shows the F1 value of different subspace learning approach by SVM classifier, where the number denotes the subspace dimension. For example, IG3 represents the 3-dimensional subspace calculated by Information Gain. It shows that IWMMC ( 0  X  X  == ) outperforms Information Gain and IPCA which could also be conducted on large scale dataset. In this paper, we proposed an incremental supervised subsp ace learning algorithm, called Incremental Maximum Margin Criterion (IMMC), which is a challenging issue of computing dominating eigenvectors and eigenvalues from incrementally arriving stream without storing the knowing data in advance. The proposed IMMC computational complexity. It can be theoretically proved that IMMC can find out the same subspace as batch MMC does. Moreover, batch MMC can approach LDA when there is a suitable constraint. But it remains unsolved that how to estimate and choose the parameter to make sure the criterion matrix is nonnegative determined. In the future work, we intend to give a rational function of  X  to make IMMC more stable. This work is accomplished in Microsoft Research Asia. The authors thank Ning Liu (the graduate student of Tsinghua University) for helpful discussions. Q. Yang thanks Hong Kong RGC for their support. [1] C.L., B. and C.J., M. UCI Repository of machine learning [2] Kushner, H.J. and Clark, D.S. Stochastic Approximation [3] Lewis, D., Yang, Y., Rose, T. and Li, F. RCV1: A new [4] Li, H., Jiang, T. and Zhang, K., Efficient and Robust Feature [5] Liu, R.-L. and Lu, Y.-L., Incremental Context Mining for [6] Weng, J., Zhang, Y. and Hwang, W.-S. Candid Covariance-[7] Yu, L. and Liu, H., Efficiently Handling Feature Redundancy [8] Zhang, Y. and Weng, J. Convergence analysis of Lemma-3 : ( ) vn is bounded, if (0) v is bounded. Proof: () ( 1) () vn vn An Since  X   X &gt; , From the fact that A &lt; X  and 1 () n when 2 nN  X  . Let 12 max{ , } NNN = , then () ( 1) vn vn  X   X  X  X + when nN  X  . Since we can choose  X  freely, we can draw the conclusion that () ( 1) vn vn  X  X  X  . when nN  X  . Since (0) v &lt; X  , When nN  X  So () vn is bounded when nN  X  and () ( 1) vn vn  X  X  X  that () vn w.p.1. End of proof. 
