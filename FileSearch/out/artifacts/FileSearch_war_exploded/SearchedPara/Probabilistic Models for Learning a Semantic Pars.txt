 Semantic parsing has recently gained popularity as a technique for mapping from natural language to a formal meaning representation language, e.g., in or-der to answer questions against a database (Zelle and Mooney, 1993; Zettlemoyer and Collins, 2005). In order to train a semantic parser, one must first pro-vide a lexicon , which is a mapping from words in the language to statements in the meaning representa-tion language. This mapping defines the grammar of the parser and thereby determines the set of meaning representations that can be produced for any given sentence. Therefore, a good lexicon is necessary to achieve both high accuracy and parsing speed. How-ever, the lexicon is unobserved in real semantic pars-ing applications, leading us to ask: how do we learn a lexicon for a semantic parser?
This paper presents several novel probabilistic models for learning a semantic parser lexicon. Ex-isting lexicon learning algorithms are heuristic in na-ture and require either annotated logical forms or manually-specified lexicon entry templates during training. In contrast, our models do not require such templates and can be trained from question/answer pairs and other forms of weak supervision. Train-ing consists of optimizing an objective function with Expectation Maximization (EM), thereby guaran-teeing convergence to a local optimum. Further-more, the objective function for our simplest model is concave , guaranteeing convergence to a global optimum. Our approach generates a probabilis-tic context-free grammar that represents the space of correct semantic parses for each question; once trained, our approach derives lexicon entries from the most likely parse of each question (Figure 1).
We present an experimental evaluation of our lex-icon learning models on a data set of food chain questions from a 4th grade science domain. These questions concern relations between organisms in an ecosystem and have challenging lexical diversity and question length. Our models improve semantic parser accuracy (35-70% error reduction) over prior work despite using less human input. Furthermore, our best model produces a lexicon that contains 40x fewer entries than the most accurate baseline, result-ing in a semantic parser that is 4x faster. Our models also obtain competitive results on GEO 880 without any dataset-specific engineering. Work on lexicon learning falls into two categories:
Pipelined approaches build a lexicon before training the parser, either by manually defining it (Lee et al., 2014; Angeli et al., 2012) or by us-ing a collection of heuristics. The heuristics of-ten take the form of lexicon templates , which are rules that create lexicon entries by pattern-matching training examples (Liang et al., 2011; Krishna-murthy and Mitchell, 2012; Krishnamurthy and Kol-lar, 2013). These approaches require new lexi-con templates for each application. More complex heuristic algorithms have been proposed based on word alignments (Wong and Mooney, 2006; Wong and Mooney, 2007) or common substructures in the meaning representation (Chen and Mooney, 2011); these algorithms all require annotated logical forms.
Joint approaches simultaneously learn a lexicon and the parameters of a semantic parser. Typically, these algorithms use lexicon templates to generate a set of lexicon entries for each example, then heuristi-cally select a subset of these entries to include in the global lexicon while training the parser (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Artzi and Zettlemoyer, 2013b; Artzi et al., 2014). UBL takes a different approach that performs top-down, iterative splits of an annotated logical form for each training example (Kwiatkowski et al., 2010; Kwiatkowski et al., 2011). Artzi et al. (2015) com-bine templates with top-down splitting. The heuris-tic search performed by these algorithms can be dif-ficult to control and we empirically found that these algorithms often selected overly-specific lexicon en-tries (see Section 4.4).

Other work has avoided the lexicon learning prob-lem altogether by searching over all possible mean-ing representations (Kate and Mooney, 2006; Clarke et al., 2010; Goldwasser et al., 2011; Berant and Liang, 2014; Pasupat and Liang, 2015; Reddy et al., 2014). The challenge of this approach is that the space of meaning representations for a sentence can be very large, making parsing less efficient and learning more difficult. A practical compromise is to combine a (possibly minimal) lexicon with flexible parsing operations (Liang et al., 2011; Zettlemoyer and Collins, 2007; Poon, 2013; Parikh et al., 2015).
Our lexicon learning models are closely related to machine translation word alignment models (Brown et al., 1993)  X  our key insight is that lexicon learning is equivalent to word alignment where the tokeniza-tion of one of the sentences is unobserved . Thus, our models simultaneously  X  X okenize X  the logical form  X  using a splitting process similar to UBL  X  and align the resulting logical form  X  X okens X  to words. This section describes our lexicon learning models. For concreteness, we focus on learning a lexicon for a Combinatory Categorial Grammar (CCG) seman-tic parser with lambda calculus logical forms as the meaning representation; however, our models are applicable to other semantic parsing formalisms and meaning representation languages.

Our models learn a CCG lexicon from a data set of question/label pairs { ( w i ,L i ) } n i =1 . Each question is a sequence of words, w i = [ w i 1 ,w i 2 ,... ] , and each la-bel is a set of logical forms, L i = { ` i 1 ,... } . Labeling each question with a set of logical forms generalizes many weak supervision settings, including ambigu-ous supervision (Kate and Mooney, 2007) and ques-learning is a collection of lexicon entries w := C : ` mapping word w to syntactic category C and logical form ` .

Our models are generative models of questions given a label, P ( w | L ) . The key component of each model is a probabilistic context-free grammar (PCFG) over correct logical form derivations. A parse tree in this grammar simultaneously represents (1) the choice of a logical form `  X  L , (2) the way ` is constructed from smaller parts, and (3) the align-ment between these parts and words in the question. Training each model amounts to learning the rule probabilities of this grammar, including which logi-cal forms are likely to generate which words. Pars-ing an example with the trained grammar produces an alignment between words and logical forms that is used to construct a lexicon. 3.1 Logical Form Derivation Grammar The logical form derivation grammar is a PCFG con-structed to represent the set of correct logical form derivations  X  i.e., correct semantic parses  X  of each training example. The grammar X  X  nonterminals are logical forms and its binary production rules repre-sent ways that pairs of logical forms can combine in the semantic parser. The grammar X  X  terminals are words and its terminal production rules represent lexicon entries. The complete grammar is a union of many smaller grammars, each of which is con-structed to represent the logical form derivations of a single example. Figure 2 shows a training exam-ple and a portion of the grammar generated for it, and Figure 1 shows a parse tree in the grammar.
Our algorithm for constructing the PCFG for a training example ( w ,L ) uses a top-down approach that iteratively splits logical forms in L . Assume we are given a procedure SPLIT ( f ) that outputs a list of ways to split f into a pair of logical forms ( g,h ) . Grammar generation performs the following steps: 1. Model weak supervision. Add L to the gram-2. Enumerate logical form splits. For all `  X  L , 3. Create lexicon entries. Add a terminal rule 4. Allow word skipping. Add a special SKIP non-
The SPLIT procedure required above depends on the meaning representation language, but is application-independent. For our lambda calculus representation, SPLIT ( f ) returns a list of logical forms g,h such that f = g ( h ) . We use similar constraints as Kwiatkowski et al. (2011) to keep the number of splits manageable. Note that SPLIT could also include composition by returning g,h pairs such that f =  X x.g ( h ( x )) ; however, we did not explore this possibility in this paper.

An important property of this grammar is that it excludes many logical form derivations that cannot lead to the label . For example, the grammar in Fig-ure 2 does not let us apply  X x. EATS ( x, BASS ) to BASS  X  even though this operation would be per-mitted by CCG  X  because there is no way to reach the label L from the result EATS ( BASS , BASS ) . This property reduces the number of possible parses of a question relative to a CCG parser with the same lexicon entries, making parsing more efficient.
The logical form derivation grammar G is con-structed by applying the above process to every ex-ample in the data set. Let P ( t | L ;  X  ) denote the prob-ability of generating tree t from G given ROOT ( t ) = L and parameters  X  . This probability factors into a product of production rule probabilities:
In the above equation, P ( f g h ;  X  ) and P ( f w ;  X  ) represent the conditional probability of select-ing a production rule given the nonterminal f . We use P ( w ,t | L ;  X  ) to denote P ( t | L ;  X  ) where the ter-minals of t are equal to the question w . In the fol-lowing, sums over trees t are implicitly over all trees permitted by G . 3.2 Independent Model The independent model assumes that each word w j of a question w is generated independently from a parse tree t chosen uniformly at random given the label L . This simple model allows two words in the same question to be generated by different trees. The probability of a question given a label is:
P ( w | L ;  X  ) =
The final term #( f,j,L, | w | ) is the fraction of trees with root L and | w | terminals where the j th terminal symbol is generated by nonterminal f . This term appears due to the assumption that trees are drawn uniformly at random. The parameters  X  of this model are the terminal production rule probabil-ities, which are modeled as a conditional probability table: P ( f w ;  X  ) =  X  f,w where The independent model is a generalization of IBM Model 1 (Brown et al., 1993) to the lexicon learning problem, and  X  like IBM Model 1  X  its loglikelihood function is concave (see Appendix A). Therefore, the EM algorithm will converge to a global optimum of the data loglikelihood under this model. 3.3 Coupled Model The coupled model generates the entire question w from a single parse tree t that is generated given L . This model removes the previous model X  X  na  X   X ve as-sumption that each word is generated independently. The probability of a question given a label under this model is:
Theoretically, we could learn both of the produc-tion rule distributions that compose P ( w ,t | L ;  X  ) in this formulation. However, in practice, the large number of nonterminals makes it challenging to learn a conditional probability table for the binary production rules. Therefore, we again assume the trees are drawn uniformly at random and only learn a conditional probability table for the terminal pro-duction rules. 3.4 Coupled Loglinear Model The coupled loglinear model replaces the con-ditional probability tables of the coupled model with loglinear models. Loglinear models can share parameters across different  X  but intuitively similar  X  production rules. For example, both  X x. EATS ( x, BASS ) and  X x. EATS ( x, FROG ) should have similar distributions over production rules when BASS is appropriately replaced by FROG . We can produce this effect with loglinear models by as-signing similar feature vectors to these rules.
This model uses three locally-normalized loglin-ear models to parameterize the distribution over pro-duction rules. First, a rule model decides whether or not to apply a terminal production rule. Next, given this model X  X  response, a second loglinear model de-cides which of the chosen kind of rules to apply. This approach ensures that each nonterminal sym-bol has a proper conditional probability distribution over rules. The production rule distributions are pa-rameterized as:
P ( f g h ;  X  ) =
In the above equation, the r,t and n subscripts indicate terms of the rule, terminal and nontermi-nal models, respectively. The Z r ,Z t ( f ) and Z n ( f ) terms denote the partition functions of each model, and  X  r , X  n and  X  t are functions mapping nontermi-nals and production rules to feature vectors. We use indicator features for logical form patterns, where each pattern is produced by replacing all of a logi-cal form X  X  subexpressions below a certain depth with their types. 3.5 Training with Expectation Maximization We train all three models by maximizing data log-likelihood with EM (Dempster et al., 1977). Train-ing the independent model is equivalent to training a mixture of multinomials where each word of each question has its own prior over cluster assignments. Let  X  m represent the model parameters on the m th training iteration. The E-step calculates expected count of each terminal production rule:
The term #( f,j,L i , | w i | )  X  representing the frac-tion of trees where nonterminal f generates the j th word of question i  X  can be calculated by parsing each example once using the inside/outside algo-rithm. The M-step re-estimates the terminal produc-tion rule probabilities using these expected counts:
Training the coupled models is a standard appli-cation of EM to learning the parameters of a la-tent probabilistic CFG. The E-step calculates the ex-pected number of occurrences of each production rule in each example:
In the above equation, the function #( f w ; t ) returns the number of occurrences of f w in t . These expected counts can be computed efficiently the coupled model is the same as that of the indepen-dent model above. The M-step of the coupled log-linear model solves an optimization problem to fit the loglinear models to the computed expectations (Berg-Kirkpatrick et al., 2010):  X 
This problem factors into three separate optimiza-tion problems: a binary logistic regression for the rule model, and estimating two conditional distri-butions over nonterminal and terminal production rules. We use L-BFGS to solve these problems. 3.6 Producing a Lexicon Given parameters  X  and a data set { ( w i ,L i ) } n i =1 produce a CCG lexicon from the terminal produc-tion rules of the most probable parse of each exam-ple. First, we parse each question w i conditioning on the parse tree root being L i . Second, we gen-erate lexicon entries from the highest scoring parse tree for each example. We identify the nonterminal f that generates each word w  X  w and, if f 6 = SKIP , create a lexicon entry w := C : f . As in previous work, we derive the syntactic category C from the semantic type of the logical form (Kwiatkowski et al., 2011). The argument directions of C are deter-mined by walking up the tree and noting the relative position of each of its arguments. Figure 1 shows an example of a predicted parse tree and the lexicon entries generated from it. We compare our lexicon learning models against several baselines on two data sets: FOODCHAINS , containing 4th grade science food chain ques-tions, and GEO 880, containing geography questions. These data sets each present different challenges for lexicon learning: FOODCHAINS has more difficult language  X  long questions and more lexical vari-ation  X  while GEO 880 has more complex logical forms. Our results demonstrate that our models per-form better than several baselines with difficult lan-guage while simultaneously performing reasonably well with complex logical forms.

Code, data and other supplementary material for this paper is available at naacl2016-lexicon .
 4.1 Data We collected a new data set, FOODCHAINS , that con-tains 774 food chain questions designed to imitate actual questions from the New York State Grade 4 Regents Exam. Each example in the data set con-sists of a natural language question and a food chain, which is a list of organisms that eat each other. The questions are multiple choice and the answer options are either animals from the food chain or a direction of change, e.g.,  X  X ncrease. X  Each question also has a logical form annotated by the first author, which is necessary to train some of the baseline systems. The denotation of each predicate  X  and therefore logical form  X  is a deterministic function of the food chain. Figure 3 shows some examples from this data set. FOODCHAINS was created using Mechanical Turk. We first manually created 25 distinct food chains of various lengths containing different organ-isms and a set of question templates  X  questions with a blank  X  based on real Regents questions. In the first task, Turk workers were shown a randomly-selected food chain, question template, and answer, and were asked to complete the question by filling in the blank. In the second task, workers paraphrased questions from the first task, thereby eliminating the templated structure and increasing lexical variation (similar to Wang et al., (2015)). In the third task, a worker validated each question by answering it.
Statistics of FOODCHAINS are presented in Ta-ble 1 alongside corresponding statistics of GEO 880 (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005). FOODCHAINS dif-fers from GEO 880 in two significant and interesting ways. First, although the data set contains relatively few predicates, there are many ways to reference each predicate  X  for example, consider the diversity in references to DECREASE in Figure 3. Second, the questions are long but contain many uninformative words that can safely be ignored by the parser. 4.2 Methodology We compare lexicon learning algorithms by per-forming an end-to-end evaluation, measuring the question answering accuracy of a CCG semantic parser trained with the learned lexicon. The parser has a rich set of features, including lexicon entry features, dependency features, and dependency dis-tance features. The parser is also permitted to skip words in the question for a learned per-word cost. We train the parser by optimizing data loglikelihood with 100 epochs of stochastic gradient descent.
All experiments on FOODCHAINS are performed using 5-fold cross validation. All questions about a single food chain appear in the same fold, ensuring that the questions in the held-out fold reference un-seen food chains. 4.3 Comparing Probabilistic Models Our initial experiment compares the three proba-bilistic models proposed in Section 3 on FOOD -CHAINS . We generated three lexicons by training each model using 10 iterations of EM. We used a smoothing parameter of 0.1 when estimating con-ditional probability tables, and an L2 regularization els. We also initialized the coupled model with the optimum of the independent model. All of these models are trained without labeled logical forms, in-stead using an automatically enumerated set of logi-cal forms that evaluate to the correct answer.
Table 2 presents the result of this evaluation. All three models perform roughly similarly, with the coupled loglinear model slightly outperforming the others. The competitive performance of the inde-pendent model is interesting because its concave ob-jective function is easy to optimize. The remaining experiments compare against the coupled loglinear model, which we dub PAL , short for  X  X robabilistic Alignments for Lexicon learning. X  4.4 Lexicon Learning Baselines Our second experiment compares PAL with four baseline lexicon learning algorithms. The first base-line, POS, defines a set of lexicon entries for each word in the training set based on its part-of-speech tag (Liang et al., 2011). We iteratively developed these templates to cover the data set, and the lexi-con generated by these templates can correctly parse 96% of the examples in FOODCHAINS . The re-maining three baselines, ZC2007 (Zettlemoyer and Collins, 2007), UBL (Kwiatkowski et al., 2011), and ADP2014 (Artzi et al., 2014), are joint lexi-con and parameter learning algorithms. We used the UW SPF (Artzi and Zettlemoyer, 2013a) implemen-tations of UBL and ZC2007. For these two models, we trained our parser using the learned lexicon to en-tialized UBL X  X  parameters using GIZA++ (Och and Ney, 2003) as in the original paper. The lexicon en-try templates for ZC2007 and ADP2014 are derived from the POS templates to ensure coverage of the questions; these algorithms allow each template to apply to 1-4 word phrases.

Table 3 compares the accuracy of semantic parsers trained with these baseline approaches to PAL . Our model outperforms all of the baselines, beating the most accurate baseline, POS, by more than 10 points. Note that all of these baselines also use more human input than our model, either in the form of lexicon templates or logical forms. Table 4 compares the average number of lexicon entries and semantic parser speed of the baselines with our mod-els. PAL produces the most compact lexicon and sec-ond fastest parser, which is 4x faster than POS, the baseline with the highest accuracy. The correlation between lexicon size and parse time is imperfect due to word frequency and co-occurrence effects.
The three joint lexicon and parameter learning al-gorithms perform poorly on our data set for two rea-sons. First, the long question length increases the difficulty of finding good lexicon entries. The algo-rithms with lexicon templates were frequently un-able to find a correct parse for long questions, even with a large beam size  X  we ran ADP2014 with a beam size of 10000. (Note that POS does not suf-fer from this problem because it only generates lex-icon entries for a few parts of speech, so most of the words in a question are ignored by default.) Sec-ond, these algorithms X  discriminative objectives in-herently prefer lexicon entries with highly specific word sequences. This preference interacts poorly with the uninformative words in the data set, leading these algorithms to produce many lexicon entries for long phrases that do not generalize well. The lexi-con sizes in Table 4 are suggestive of this problem for ZC2007 and UBL; ADP2014 also has this prob-lem, but its voting mechanism prunes lexicon entries much more aggressively. We tried to solve this prob-lem for ZC2007 and ADP2014 by restricting lexicon templates to apply to at most 1 word, but this change actually reduced accuracy. An investigation of this phenomenon found that reducing the length of the templates made it even more difficult for these mod-els to find correct parses for long questions.
In contrast to these baselines, our models do not suffer from either of these problems because the logical form derivation grammar restricts the search to correct derivations and our generative objective prefers frequently-occurring lexicon entries. Our models actually consider a larger space of possible lexicon entries than ZC2007 and ADP2014 with 1 word templates, yet find better lexicon entries. 4.5 Geo880 Evaluation We performed an additional evaluation on GEO 880 to demonstrate that our models can work with more complex logical forms. GEO 880 is a good data set for this evaluation because its logical forms contain, on average, about twice as many constants as FOOD -CHAINS . We applied PAL to generate a lexicon for this data set using its included logical form labels, then trained a CCG semantic parser with this lexi-con. Table 5 compares the accuracy of this parser with previous CCG lexicon learning results on this data set using the standard 600/280 train/test split. The comparison to PAL is inexact because both prior systems use CCG parsers with special extensions that improve performance on this data set. UBL uses factored lexicon entries that generalize better to certain infrequent entries, and ZC2007 includes relaxed parsing operators that fix common parsing errors. Examining the errors made by our parser, we found many cases where these extensions would help. We therefore trained another CCG parser us-ing a lexicon generated by postprocessing PAL  X  X  lex-icon to include factored lexicon entries. This parser achieves an accuracy close to previous work. We introduce several probabilistic models for learn-ing a semantic parser lexicon that can be trained from question/answer pairs and other forms of weak supervision. Our experimental results demonstrate that our models improve semantic parser accuracy and efficiency relative to prior work on data sets with more challenging language, despite using less hu-man input. Furthermore, we find that our indepen-dent model is nearly as effective as more complex models, but has a concave objective function that guarantees training converges to a global optimum.
A possible complaint about our approach is that, when training from question/answer pairs, it is not practical to enumerate all logical forms that pro-duce the correct answer. We believe this complaint is misguided because enumerating logical forms is unavoidable in the question/answer setting. Every algorithm uses an enumerate-and-test approach to identify correct logical forms; this process occurs in the gradient computation of semantic parser train-ing and in template-based lexicon learning algo-rithms such as ADP2014. The critical question is not whether enumeration is used, but rather how logical forms are enumerated. Many strategies are possible and different strategies are likely to be effective on different data sets. In fact, the failure of template-based algorithms on FOODCHAINS is largely a fail-ure of their template-guided enumeration strategy to find correct logical forms. Choosing an enumeration strategy and related questions  X  e.g., does semantic parser parameter learning affect the enumeration?  X  are empirical questions that must be decided on a task-specific basis.

A recent trend in semantic parsing has been to avoid lexicon learning, instead directly searching the space of all possible logical forms. However, we think that lexicon learning still serves a valuable pur-pose. Fundamentally, a lexicon constrains the space of logical forms searched by a semantic parser; these constraints improve efficiency and can improve ac-curacy as long as they do not exclude correct logi-cal forms. Thus, a promising approach to building a semantic parser is to automatically learn a lexicon using one of our models, then (perhaps) manually specify new parsing operations to correct any prob-lems with the learned lexicon. We plan to apply this approach in the future to construct semantic parsers for more challenging tasks.
 We gratefully acknowledge Oyvind Tafjord, Matt Gardner, Peter Turney, Oren Etzioni and the anony-mous reviewers for their helpful comments.
 The loglikelihood O (  X  ) of the independent model is: Each log term above is concave in  X  because log is a concave function applied to an affine function of  X  . (Note that the #( f,j,L i , | w i | ) terms do not depend on  X  .) Finally, O (  X  ) is concave because it is a sum of concave functions.
