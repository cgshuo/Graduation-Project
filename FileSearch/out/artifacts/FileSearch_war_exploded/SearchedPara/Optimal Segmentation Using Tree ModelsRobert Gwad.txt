
Sequence data are abundant in application areas such as computational biology, environmental sciences, and telecommunication. Many real-life sequences have a strong segmental structure, with segments of different complexities. In this paper we study the description of sequence segments using variable length Markov chains (VLMCs), also known as tree models. We discover the segment boundaries of a sequence and at the same time we obtain a VLMC for each segment. Such a context tree contains the probability distri-bution vectors that capture the essential features of the cor-responding segment. We use the Bayesian Information Cri-terion (BIC) and the Krichevsky-Trofimov Probability (KT) to select the number of segments of a sequence. On DNA data the method selects segments that closely correspond to the annotated regions of the genes.
We consider the problem of segmenting a sequence of symbols into contiguous homogeneous segments. The seg-mentation problem has many applications in areas such as computational biology, environmental sciences, and con-text recognition in mobile devices. The problem has been widely studied in different fields, for instance, in statistics it is known under the name change-point detection .
Many segmentation algorithms have been proposed in the data mining/algorithms community, ranging from on-line to offline, from heuristic to optimal (typically involving a dynamic programmingapproach), and from combinatorial to probabilistic.

We consider the sequence segmentation problem as a model selection process where we fit a variable-length Markov chain (VLMC) to segments of the input sequence. We use the Bayesian Information Criteria (BIC) and the Krichevsky-Trofimov (KT) probability criteria for: ( i )fit-ting an optimal VLMC fo r each segment; and ( ii ) deter-mining the optimal number of segments to partition the se-quence.

As a probabilistic model for a stationary categorical se-quence a d-order VLMC is a Markov chain (MC) whose contexts (memory) are allowed to be of variable length. Such reduced models are also called tree models , since they can be conveniently represen tedbyacontexttree,which can range from a full tree in the case of an ordinary d-order full MC to an empty tree in the case of a 0-order MC. The variable length memory of VLMCs has the potential of cap-turing complex phenomena that are present in real-life se-quences. VLMCs provide a sparse representation of a se-quence by reducing the number of parameters to be esti-mated. This flexibility of VLMCs is useful for the segmen-tation task because we can fit high order models to segments to maximize the likelihood, without being penalized for an exponential increase in the number of parameters (as in the case of ordinary MCs).

The fundamental question is whether the increased mod-eling power of VLMCs with respect to MCs really trans-lates into a better segmentation performance. As it turns out VLMCs can provide more accurate segmentations than MCs and are also capable of recognizing partition points in cases where MCs fail.

The tasks in our approach are the following: ( i ) fitting an optimal VLMC to data is a non-trivial task because it in-volves selecting among an exponential number of trees; ( ii ) many real sources have short segments and the algorithm has to fit VLMCs from sparse data; and ( iii ) the standard dynamic programming algorithm has a quadratic time com-plexity.

We solve task ( i ) by adapting known pruning algorithms of VLMC to be used with the BIC and KT criteria. We address ( ii ) by using VLMCs of variable order (maximum depth of the tree) with respect to segment length such that we fit taller trees to longer segments and shorter trees to shorter segments. Finally, we solve ( iii ) by applying nu-merous optimization techniques.

We conducted experiments on synthetic data and on a va-riety of DNA sequences. The results show that the method selects gene segments that closely correspond to the cur-rently known (annotated) gene regions. Our segmentation system, called TreeSegment , is available at the authors X  web pages.

The rest of this paper is organized as follows. In Sec-tion 2 we introduce the notion of tree models. Section 3 presents the details of the algorithm. In Section 4 we present our experimental results. Section 5 reviews related work and Section 6 is a short conclusion.
Let A = { a 1 ,a 2 ,...,a |A| } be an alphabet of cardinality |A| and let s j catenation of strings u and v is denoted by uv .Astring v is a suffix of s if there exists w such that s = wv .
Let S n 1 be a stationary ergodic stochastic process over alphabet A ,where P ( s n 1 )= P ( S n 1 = s n 1 ) .Astring c is a context for S if where c is a suffix of s i  X  1 1 and no proper suffix of c has this property.

A context tree T is a set of strings such that no string c  X  X  is a suffix of another string c  X  X  . Each string c = c d 1  X  X  can be visualized as a path from the root to a leaf consisting of d edges labeled by symbols c d c d  X  1
A parameter assignment  X  ( T ) assigns to each suffix c of a context c  X  X  (including to c itself) a vector of condi-tional probabilities  X  ( c )=[  X  ( c ,a 1 ) ... X  ( c ,a |A| a  X  X   X  ( c ,a )=1 and  X  ( c ,a )= P ( a
We assign a probability P ( s |T , X  ( T )) for the input se-quence s = s n 1 given the context tree T and the probability assignments  X  ( T ) by where c i is the longest suffix of s i 1 that belongs to T
Apair ( T , X  ( T )) is called a tree model or a d -VLMC if the longest string in T has length d .

In the case that all strings c  X  X  are of length d and |T| = |A| d the tree model is called d -MC (full d -order Markov chain model).

Given a tree model ( T , X  ( T )) and an input sequence s n we can compute the probability of observing the sequence s 1 given the model using (1). In the case that the tree known, but the parameter vector  X  ( T ) is unknown, one can compute the maximum likelihood estimator  X   X  ( T ) of the pa-rameter vector  X  ( T ) . In particular, the MLE of each condi-tional probability P ( a | c ) is expressed as follows: where N n ( c, a ) and N n ( c )= a  X  X  N n ( c, a ) is the num-ber of occurrences of strings ca and c in s n 1 , respectively.
Figure 1. An example of a 2-MC (top) and a 2-VLMC (bottom)
In practice, given an input sequence s ,a d -VLMC is built using a two stage process, where: first a context tree of depth d is built from a sequence and then the tree is pruned to obtain a variable-depth context tree that more accurately fits to the contextual structure of the input sequence. Fig-ure 1 shows an example of a 2-MC (top) and a 2-VLMC (bottom) for an alphabet of size 4. In the 2-VLMC case, the leaves { C, G } A and { C, G, T } C , that are connected using a dashed edge to their parents, represent virtual nodes .A virtual node is created when a parent node loses between 2 and |A| X  1 children nodes as a result of pruning. The idea of virtual nodes is that they represent the context of the pruned children by merging the pruned children contexts together. Clearly, we do not create a virtual node if there is only one pruned child, as this would not change the total number of children. Thus, the node { C, G } A represents contexts CA and GA .

Next we introduce some more notation. We use M T = {
M ( T , X  ( T )) :  X  ( T )  X   X ( T ) } to denote the model class defined as a set of all models sharing the same tree T ,where  X (
T ) is the set of all valid parameter assignments to T .If the input sequence has been generated by a VLMC, we de-note by T 0 the generating tree. Accordingly,  X  0 is the gener-ating parameter vector and d 0 is the order of the generating MC.

For a tree T we define D ( T ) to be the maximum depth, or equivalently the length of the longest context. If the tree T is implied then we use D instead. We also use T| d to denote a tree that is a truncation of T to depth d .
As we explained before,  X   X  is the maximum likelihood estimator of the parameter vector  X  ,foratree T ,andfora given input sequence s n 1 .Wealsouse ML = P ( s n 1 |  X   X  ) to denote the maximum likelihood of s n 1 .

A k -segmentation of a sequence is a partition of the se-quence in exactly k consecutive and non-overlapping seg-ments. Finally, a d -VLMC segmentation is a segmentation by fitting a d -VLMC. We also use the term BIC-or KT-segmentation with any of the above to specify the scoring function.

As an information criterion for model selection, we use variants of the Minimum Description Length (MDL) prin-ciple that says that the best model of the process given the observed sequence is the one that gives the shortest descrip-tion of the sequence, where the model itself is also a part of the description. For VLMCs, MDL has the following general form
MDL T ( s n 1 )= L ( C n ,s n 1 | M ( T , X  ( T ))) + L ( C where L (  X  ) is a real valued binary code-length function and C n is a uniquely decodable binary code. Thus, L ( C n ,s n 1 | M ( T , X  ( T ))) is the length of encoding of the data given the model and L ( C n , T ) is the length of encod-ing of the tree. BIC For a d -MC the BIC has the following form For a d -VLMC the BIC has the following form where Thus, the BIC estimator of a context tree of depth up to d is defined as follows: In coding terms BIC corresponds to the two-stage coding. The likelihood terms ML d and ML T correspond to the length of encoding of the data given the model while the correspond to the length of encoding of the parameters. In statistical terms, BIC has an interpretation as a maximum likelihood method. The first term measures the goodness of fit of the tree T to s n 1 , and the second term is the penalty term equal to the number of free parameters, which prevents BIC from overfitting.
 KT The KT probability [9] for a 0-MC binary sequence s n 1 is defined as the average probability over all possible p =  X  [0 , 1] weighted by the Dirichlet distribution D ( u ) with pa-rameters u =[ 1 2 , 1 2 ] and it can be expressed as where N n (0) is the number of zeros in s n 1 .Intermsof Bayesian statistics, KT 0 corresponds to the marginal likeli-hood [11] of s n 1 . Equation (5) can be generalized to a multi-alphabet case ( |A| &gt; 2 ) by using the multinomial distri-bution in place of the binomial. It can be shown that the integral has an exact solution The choice of prior parameter u in (5) is dictated by asymp-totic properties [1] and has an effect as pseudo-countsin (6).
For VLMCs, KT can be expressed using the fact that all symbols corresponding to the same context c  X  X  form a memoryless subsequence of s n 1 ,i.e, P ( s n 1 )= corresponding to context c . This leads to: KT is a minimizer of the worst case average redundancy R where R n ( T )= X  |T| ( |A| X  1) 2 log ( n ) [9].
In coding terms, KT corresponds to the mixture coding which consists of the encoding of s n 1 and the encoding of the tree. Thus, the MDL estimator of a context tree of depth up to d is defined as follows [20]:  X  T In statistical terms, KT is a mixture distribution that mea-sures the goodness of fit of the tree T to s n 1 in terms of the average probability in the model class M T .

Using the BIC (4) and KT (8) estimators directly by enu-merating all possible trees of a given maximum depth is infeasible in practice. Th erefore, variants of the Context al-gorithm [14] and the Context Tree Maximization (CTM) al-gorithm [20] can be used in practice. Such algorithms make local decisions as part of a search for an optimal solution. Algorithm Context and CTM work in two stages. They first build a context tree of depth d and then they recursively prune the tree.
 Algorithm Context Algorithm Context prunes a context tree as follows [4]. Starting from the leaves and p roceeding bottom-up for ev-ery parent node w its every child node uw is marked for pruning if N n ( uw ) &lt; |A| or  X  uw &lt;K ( n ) ,where and K ( n ) is a user defined threshold. If all children of w were marked for pruning then they are pruned and w be-comes terminal. If at least two children were marked for pruning but there is at least one non-marked child then the marked nodes are merged to create a virtual node, which represents the needed pruned contexts. In [14, 15] the min-imization of the stochastic complexity was used in place of Equation (9) as a pruning criterion.
 The Context Tree Maximization (CTM) We now present the original version of the CTM algorithm [20]. CTM finds a tree maximizing (7) by a local optimiza-tion in a recursive bottom-up way. For each node v in the tree CTM assigns two values: the maximum KT contribu-tion to (7) of the contexts in the subtree rooted at v called KT max ( v ) ; and an indicator I max ( v ) that marks nodes to be included in the ma ximizing tree. The algorithm proceeds bottom-up as follows: 1. if v is a leaf node then KT max ( v )= KT 0 ( s n 1 | v ) 2. if v is an internal node then After having visited all nodes, KT max ( root ) contains the maximized probability KT  X  T the indicators. To reconstruct  X  T 0 one has to recursively read them off top-down starting from the root. In terms of pruning the value of I max ( v ) has the following meaning: if I max ( v )=0 then all children are pruned at once (they are not part of  X  T 0 ); while if I max ( v )=1 then the children are not pruned (they are a part of  X  T 0 ). Comparing to Context , CTM has to visit all nodes in the tree. In this section we present the TreeSegment algorithm. We start with discussing our pruning criteria.
In this section we give a precise derivation for the thresh-old K ( n ) of the Context algorithm that locally minimizes (3). There is a consensus in the literature [18, 4] that K ( n ) should be of the form C log( n ) ;herewegiveaderivation for the value of C in detail.

We start with an example, illustrated in Figure 2, which shows a tree rooted at a node w for alphabet A = {
A, C, G, T } . The tree undergoes a pruning scenario ac-cording to the Context algorithm. For simplicity we assume in this example that the virtual node is created after the first node is pruned. We number the trees (1) -(5) from the left to the right. Tree (1) shows the situation before the pruning algorithm starts. Tree (2) shows the situation after prun-ing node Aw to w , which results in creating a virtual node {
A } w .Tree (3) shows the situation after pruning Cw to w , which results in updating the virtual node to represent con-text { A, C } w .Tree (4) shows the situation after pruning node Gw to w , which results in updating the virtual node to represent context { A, C, G } w . Finally, tree (5) shows the situation after the last node Tw has been pruned to w and w becomes terminal. Thus, in the presented scenario, the size of the tree |T| changes from |A| to 1 even though it does not decrease strictly monotonically after every prun-ing operation because of the need to create the virtual node. For simplicity we assume that every pruning operation de-creases |T| by one. Let T u be the tree T after the terminal node u has been pruned including a possible creation of a new virtual node.
 Thus, we want to prune uw to w if and only if BIC T u ( s n 1 ) &lt;BIC T ( s n 1 ) ,where |T| X  X T u | =1 ,which leads to and from (9) we have which finally gives us
We modify the CTM algorithm presented in Section 2.3 by considering all possible subsets of children for pruning instead of the two subsets consisting of all children (the par-ent node) versus none of the children as in (10). Thus, our KT max ( v ) is as follows: =max where: P ( A ) is the power set of A ; X is a subset of chil-dren; P ( A )  X  X  is any subset of cardinality at least 2; v is the parent node; {A X  X } v is the virtual node; and av is a child node. In the above equation, by enumerating all subsets of children we accomplish a finer fitting of the tree to s n 1 . In this section we present the details of the algorithm TreeSegment. The standard optimal segmentation algo-rithm can be expressed by the following dynamic program-ming equation, due to Bellman [2]: In the above equation, C [ k, i ] is the optimal k -segmentation cost of the prefix s i 1 and W [ j, i ] is the score of the segment s . In our case it is either the BIC or the KT score, i.e., portional to the length of the segment s i j leads to overall O ( n 3 ) running time, which is impractical for real-life se-quences.

However, Algorithm 1 achieves a linear speedup by com-puting W [ j, i ] in constant time (for a fixed alphabet size and depth of the tree). The main idea of the speedup is that for a fixed ending position i , a fixed-depth tree T 1 is being built (inductively) starting at position i and proceeding backward for j = i, i  X  1 ,..., 0 . Thus, for every pair ( j, i ) T tains counts of all context strings that occur in segment s and for the next starting position j  X  1 T 1 can be updated in constant time, since only one new context has to be added to it. After each updating of T 1 it is copied to T 2 ,whichis pruned to obtain the score for s i j . Clearly, the cost of copy-ing T 1 to T 2 and pruning of T 2 is proportional to the size |T 1 | , which is also constant for fixed values of the param-eters D and |A| . Since the computation of W [ j, i ] can be done in a constant time for all pairs ( j, i ) , the overall run-ning time of algorithm TreeSegment is  X ( n 2 ) . The space complexity of the algorithm is  X ( Kn ) .

We also find it very effective to compute the score C [ k, i ] for values of i that are a multiple of a parameter  X  .Using this modification, we obtain a suboptimal solution, but the running time of the Algorithm is  X (( n  X  ) 2 ) . Algorithm 1: Algorithm TreeSegment
Since we need to fit optimal trees to segments of vary-ing length bounding the maximum depth of the tree is of a particular importance in TreeSegment for the following reasons: ( i ) it decreases the probability of overestimation while estimating the tree from a short sequence; and ( ii )it reduces the unnecessary computational complexity of esti-mating a deeper tree. The only problem with the bound is that it may increases the pr obability of underestimation by restricting the context length. We use the following bound D order for a memoryless sequence to contain at least one oc-currence of every one of the |A| d contexts n should be at least equal to |A| d .
The border insertion penalty can be understood in terms of the Hidden Markov Model (HMM) as a transition proba-bility between hidden states of the generating source, where segments correspond to the hidden states. Also, in MDL terms each partition point should be treated as an additional parameter and penalized appropriately. Based on our exten-sive experiments we selected the following penalties for the BIC and the KT scoring methods: B BIC =( K  X  1) log 2 ( n ) and B KT = K k =2 log 2 n k  X  1 ,where K is the total num-ber of segments. Clearly, B BIC follows from the BIC as a parameter penalty. B KT follows from MDL by observing that to encode the following partition points we need pro-portionally fewer bits, i.e, we need roughly log 2 ( n ) bits for the first point, log 2 ( n 2 ) for the second and so on.
To compare the segmentations obtained by TreeSegment with the annotated segmentations of the genes, we used the following distance measure D seg ( A, B ) : where and A and B represent the sets of partition points of two segmentations. The measure D ( A, B ) captures the distance of each segmentation point in A to the closest segmentation point in B on average. The distance is measured as a frac-tion of the total length of the sequence. So, the measure D ( A, B ) takes values between 0 and 1, where the value
Figure 3. CDS region of Wisteria vein mosaic virus NC 007216 0 means that the two segmentations A and B are identi-cal, while the value 1 can be obtained only for segmenta-tions with one segmentation points (and being at opposite ends). In our experiments, for each gene we compute the measures D seg ( BIC, GENE ) and D seg ( KT,GENE ) , where GEN E , BIC ,and KT are the sets of partition points corresponding to the annotated gene structure, the BIC segmentation, and the KT segmentation, respectively.
In particular, we consider DNA sequences containing genes and their flanking regions. Within a gene we distin-guish the following structural regions [5, 21]: ( i ) 5 X  UTR (untranslated region); ( ii ) CDS (coding region); ( iii ) intron ; and ( iv ) 3 X  UTR , where CDSs and UTRs are part of an exon . Flanking regions are divided into the 3 X  flanking region and the 5 X  flanking region. Thus, we consider a total of six func-tional regions to be segmented by TreeSegment.

In our experiments, we start with studying single-exon genes (viruses) in Section 4.1, and then we consider multi-exon genes in Section 4.2. We obtained our viral gene sequences from http://www.ncbi.nlm.nih. gov and the eukaryotic gene sequences from http:// www.ensembl.org .
The first test was to check whether TreeSegment discov-ers any tree structure variation in the genes with the simplest possible structure. For this purpose we selected complete viral genomes from the family of ssRNA positive-strand viruses and segmented them. Our results revealed the fol-lowing facts: ( i ) for every genome TreeSegment delineates at most 3 segments, where for most of the genes it delineates exactly 3 segments; ( ii ) every CDS segment corresponds to atreeofdepth 1  X  D  X  2 ;( iii ) the UTR segments corre-spond to trees of depth D =0 ;and( iv ) all the CDS seg-ments have a common subtree of depth D =1 consisting of contexts C and T .

We now show a few more detailed results for a member of the family of ssRNA positive-strand viruses. Figure 3 shows a tree built from the CDS region of Wisteria vein mo-saic virus (accession point NC 007216). Notice that the tree in Figure 3 contains contexts C and T as a subtree. Also,
Table 1. Comparison of intron and exon trees for C. elegans we obtained the following values: D seg ( BIC, GENE )= 0 . 0005 and D seg ( KT,GENE )=0 . 0065 .

Our results demonstrate that the CDS regions have in-deed a contextual structure of non-zero order while the UTR regions are of order 0. We can also determine the partition points with high accuracy.
We now consider a more difficult task of segmenting multi-exon genes. Thus, before segmenting any multi-exon gene we investigate those differences. For this purpose, we repeated the following experiment for many eukaryotic or-ganisms. We first scanned the respective genomes and then extracted the corresponding exons and introns to separate sequences. Then we fitted context trees for those two kinds of sequences using algorithm Context. As an example we present results for Caenorhabditis elegans genome in Ta-ble 1. The results show that there is a structural difference between the intron and exon trees, where the intron tree is bigger ( |T| =23 versus |T| =3 )and( D =3 versus D =2 ). Also the exon sequence has a higher CG content while the intron sequence has a higher AT content [7].
Given the discovered differences in tree structures, we segmented 10 example genes. The results are presented in Table 2. By comparing the D seg distance measure for BIC and KT we can conclude that both methods perform com-parably.

A more detailed analysis of particular segmentations from Table 2 reveals two main properties of TreeSegment: ( i ) it tends to recognize boundaries exon-intron in cases where the corresponding segments are appropriately long; and ( ii ) it tends to merge consecutive smaller heterogeneous segments into one larger segment. Property ( i ) follows from the fact that long segments enable building appropriately large trees that may better fit to the segments in order to dis-cover a finer difference between them. Property ( ii ) follows from the fact that the MDL criteria employed in TreeSeg-ment favor a simpler model that spans a larger region in-stead of creating smaller segments to represent local fluctu-ations of the probabilistic behavior of the sequence. Below we present details of some segmentations form Table 2 that illustrate examples of the presented properties of TreeSeg-ment.

Figure 4 shows segmentation of Caenorhabditis ele-gans gene F33E11.3. The gene structure is as follows: GENE=[CDS1, Intron12, CDS2, Intron23, CDS3, In-tron34, CDS4, Intron45 and CDS5]. The BIC method merged two regions: Intron23, CDS3 into one region while the KT method recognized all gene regions.

Figure 5 shows segmentation of Caenorhabditis ele-gans gene Y50D4C.3. The gene structure is as fol-lows: GENE=[CDS1, Intron12, CDS2, Intron23, CDS3, In-tron34, CDS4, Intron45, CDS5]. The BIC method merged 7 consecutive regions starting from CDS2 while the KT method merged only 4 regions from CDS2 to Intron34. Also the KT method produced more segments than the annotated segmentation, while the BIC method produced fewer segments.

Figure 6 shows segmentation of Tetraodon nigroviridis gene GSTENT00014173001 for which the BIC method seems to have recognized all gene segments while the KT method merged 3 gene regions. Also unlike in the case of gene Y50D4C.3 here the BIC method produced more seg-ments than the KT method.

To check whether TreeSegment recognizes partition points between flanking regions and exons we segmented first Drosophila melanogaster gene CG10045-RA, and then we segmented a sequence composed of that gene and flank-ing regions of length 1000. The results are shown in in Fig-ure 7, where GENE=[UTR5 X , Intron12, CDS2, UTR3 X  X  and in Figure 8, where GENE=[Flanking5 X , UTR5 X , Intron12, CDS2, UTR3 X , Flnaking3 X  X . Clearly, after adding the flank-ing regions the origins of the first exon and the second exon have been properly recognized by the BIC and KT methods.
Figure 9 shows segmentation of Drosophila melanogaster gene CG5407-RA.

Finally, Figure 10 shows segmentation of Homo Sapiens gene ENST00000246662.
Tree models and the algorithm Context were introduced by Rissanen in [14]. Consistency results for tree models were provided by Weinberger et al in [18] and by B  X  uhlmann and Wyner in [4] who also defined the term VLMC. The CTM was introduced by Wilems et al in [19, 20]. In [9] Krichevsky and Trofimov in troduced KT and derived its asymptotic properties. In [1] Barron, Rissanen and Yu presented a comprehensive review of theoretical results on MDL in the context of coding and modeling. The Bayesian Information Criterin (BIC) was introduced by Schwarz in [16]. In [6] Ciszar and Talata proved consistency results for BIC and KT as estimators of the optimal context tree.
Segmentation algorithms have been central in the anal-ysis of genomic sequences. In [11] Liu and Lawrence pre-sented a Bayesian approach to DNA segmentation by as-
Table 2. Summary of segmentation results for multi-exon genes. For each gene we report the number of segments found by the BIC and KT methods and the distances D seg ( BIC, GENE ) and D seg ( KT,GENE ) .
 Figure 4. Segmentations obtained for
Caenorhabditis elegans gene F33E11.3; top: annotated sequence; middle: segmentation obtained by BIC-VLMC method; bottom: segmentation obtained by KT-VLMC method.

The numbers above each segment denote depths of the corresponding trees.
 Figure 5. Segmentations obtained for
Caenorhabditis elegans gene Y50D4C.3; top: annotated sequence; middle: segmentation obtained by BIC-VLMC method; bottom: segmentation obtained by KT-VLMC method.

The numbers above each segment denote depths of the corresponding trees. suming a 0-MC model and using the KT probability. The optimal number of segments was selected using Bayesian inference. Makeev et al [12] studied a Bayesian approach to DNA segmentation by extending the idea from [11] by using heuristic border insertion penalties and filtration of boundaries.

Orlov et al [13] presented a method for recognizing func-tional DNA sites and segmenting genomes. They developed a program  X  X omplexity X  for computing a context tree of a DNA sequence using the stochastic complexity [14, 15] as a pruning criterion. Using their program they analyzed DNA sequences of various functional classes (coding, non-coding and regulatory) and discovered that the DNA structure can be represented by trees.

The problem of DNA segmentation by model selection was posed by Li [10], where he considered a greedy top-down divide-and-conquer 0-MC segmentation approach to segment DNA by using the BIC and the Akaike information criterion but he did not consider gene segmentation. Also in [17] Szpankowski used 0-MC and the Shanon-Jensen dis-tance to segment DNA.

As far as gene segmentation the distinctive statistical properties of gene functional regions are well documented in the literature, e.g., see [7, 21, 8]. In particular the de-tection of genes has been based on the non-uniform codon usage in protein coding segments and has been modeled by non-uniform Markov models and HMMs [5]. Bernaola et
Figure 6. Segmentations ob-tained for Tetraodon nigroviridis gene
GSTENT 00014173001; top: a nnotated sequence; middle: segmentation obtained by BIC-VLMC method; bottom: segmentation obtained by KT-VLMC method. The numbers above each segment denote depths of the corresponding trees. al [3] proposed using entropic segmentation for finding bor-ders between coding and non-coding DNA regions.
We presented a segmentation method that uses tree mod-els to fit trees of different complexities to homogeneous regions of data. The MDL principle is used to guide the segmentation process by deciding the optimal tree model in each segment and by deciding the overall number of seg-ments. In our experiments on DNA we showed usefulness of our method for gene segmentation.
 Figure 7. Segmentations obtained for
Drosophila melanogaster gene CG10045-RA; top: annotated sequence; middle: seg-mentation obtained by BIC-VLMC method; bottom: segmentation obtained by KT-VLMC method. The numbers above each segment denote depths of the corresponding trees. Figure 8. Segmentations obtained for
Drosophila melanogaster gene CG10045-RA + flanking regions; top: annotated sequence; middle: segmentation obtained by BIC-VLMC method; bottom: segmentation obtained by
KT-VLMC method. The numbers above each segment denote depths of the corresponding trees. Figure 9. Segmentations obtained for
Drosophila melanogaster gene CG5407-RA; top: annotated sequence; middle: segmentation obtained by BIC-VLMC method; bottom: segmentation obtained by KT-VLMC method.

The numbers above each segment denote depths of the corresponding trees.
 Figure 10. Segmentations obtained for Homo
Sapiens gene ENST00000246662; top: anno-tated sequence; middle: segmentation ob-tained by BIC-VLMC method; bottom: seg-mentation obtained by KT-VLMC method. The numbers above each segment denote depths of the corresponding trees.

