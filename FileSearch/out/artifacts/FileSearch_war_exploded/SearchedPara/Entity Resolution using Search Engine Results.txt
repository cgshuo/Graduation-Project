 Given a set of automatically extracted entities E of size n , we would like to cluster all the various names referring to the same canonical entity together. The variations of each entity include acronyms, full name, and informal naming conventions. We propose using search engine results to clus-ter variations of each entity based on the URLs appearing in those results. We create a cluster C for each top search result returned by querying for the entity e  X  E assigning e to the cluster C . Our experiments on a manually created dataset shows that our approach achieves higher precision and recall than string matching algorithms and hierarchical clustering based disambiguation methods.
 I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing X  Text analysis ; H.3.3 [ Information Storage And Re-trieval ]: Information Search and Retrieval X  Clustering Algorithms Entity Resolution, Disambiguation, Search Engines
Organizations and companies have different forms by which they can be referred. Some forms stem from acronyms of the entity X  X  full name, while others are developed through cul-tural evolution. For example The United States Air Force can be referred to as USAF, Air Force, U.S Air Force and US Air Force . While some acronyms and name conventions are relatively straightforward to detect, it is not always the case. Consider the example of the Republican Party which entities are usually referenced only once in a document using e ither the acronym or the full form (one of them, if there are many). Therefore, looking for the canonical name in nearby text using pattern matching approach is not possible. The dataset is relatively large containing more than 4 million entities, hence labeling and evaluation become challenging. In addition, the data have some noise inherited from the automatic extraction and the text conversion process. The aforementioned characteristics of the data make it impera-tive to use an unsupervised method that can run relatively fast in proportion to the dataset. The applications of the problem and the approach we propose are not limited to acknowledgments, but can be used in entity resolution for mining documents in an enterprise repository. They also are applicable, with minor tweaks, to the problem of author disambiguation.
Algorithm 1: E ntity Disambiguation Algorithm input : Set of entities Entities and list of URLs L e
Clusters  X  X  X  HashT able ; for e  X  Entities do end for e  X  Entities do end return Clusters ;
The basic idea behind our approach is that name vari-at ions of the same entity should return similar search re-sults. For example, searching on Google or Bing for  X  X SF X  or  X  X ational Science Foundation X  returns www.nsf.gov in both queries. Search results were previously used to disambiguate authors in [10] using hierarchical agglomerative clustering with vectors whose features are induced from search results. These methods mostly use the content of the search result as feature vectors. Our approach, on the other hand, mainly treats the URL at the top search result as a classification re-sult based on the entity name as queries. Therefore, in our approach the search engine is the classifier, while in previous methods the search engine is a feature vector generator for some black box clustering algorithm. As such, our approach is usually faster than previous methods.

Ranking in information retrieval and search engines can be considered as a classification task, where the returned doc-uments are to be classified into the correct order by which they should be displayed to the user [5]. Therefore, it can be assumed that search engines are performing a classification task. A special type of queries that search engines receive cluster after applying any of the previous conversion func-t ions on it. The entity e i is assigned to the cluster L i [0]. If we are performing hard clustering, the algorithm terminates here. However, if we are performing soft clustering (the en-tity can be assigned to multiple clusters), then a parameter k controls the k th search result up to which e i is allowed to be clustered into. In this work we only consider hard clus-tering. The algorithm is linear in space and time, given that search results were retrieved in advance.
Our dataset is created from the entities within acknowl-uate students manually created 447 clusters from the top 3000 acknowledged entities. The total number of entities in the dataset is 1138. To evaluate our approach we have im-plemented multiple baseline methods including hierarchical agglomerative clustering (HAC) and longest common subse-quence (LCS).

The LCS algorithm is a common method for sequence alignment. It uses dynamic programming to compute opti-mal alignment between two sequences of characters. We use simple heuristics to separate acronyms from non-acronym entity names. For each non-acronym names we generate its potential acronym, by taking the first letter of each word in the name. To determine if an acronym name s matches a target name t , which can be either another acronym or a full name, we compute the longest common subsequence between s and t (or the potential acronym of t , if t is a full name). If the length of the longest common subsequence is greater than 0.7 of the length of the longer acronym, they are considered a match, , for instance, between  X  X SAF X  and  X  X S Air Force. X  On the other hand, for two non-acronym names to match, one has to be a substring of the other, e.g.  X  X S Department of Energy X  and  X  X epartment of En-ergy. X  We compute the pairwise LCS score between all the elements in our dataset. The clusters are created such that all the entities in cluster C have a pairwise LCS score &gt; 0.7. The clusters are built by greedily merging entity e i with C if the distance of any item e j  X  C is less than a threshold. The optimal threshold is selected through exhaustive grid search. highest precision, while using ST D-LTD results in the high-est recall. This is because exact URLs are too specific, and the top level and the second-level domain are too general, Netloc on the other hand provide a better compromise be-tween specificity and generality.

We also compute the purity and the inverse purity of each clustering method and find out that our approach with Net-loc or URL yields better results. The results are reported in figure 1 where Netloc, URL, SLD-TLD represent clustering with algorithm 1 and the corresponding conversion func-tion. LCS denotes using longest common subsequence for clustering, while HAC-EUCL and HAC-JACC represent the results of using HAC with Euclidean distance and Jaccard similarity respectively. In table 2 sC is the number of formed clusters, pP pR pF denote pairwise precision, recall, and F measure respectively. cP, cR, cF denote cluster precision, recall and F measure respectively. In the data column, lcs-n denotes LCS algorithm with threshold n  X  10  X  1 . And jacc in-dicate using HAC with Jaccard distance and the threshold mentioned after the hyphen. Similarly eucl denotes HAC with euclidean distance and the mentioned threshold.
The following list shows some of the correctly detected clusters:
Although our approach outperforms the baselines signif-icantly, it still makes mistakes. For example, the cluster princeton.edu contained the entities (1) Princeton , and (2) De-partment of Electrical Engineering . This is because when searching on Bing for Department of Electrical Engineering , the first result was www.princeton.edu/ee at the time of the API call. Such cases occur when the entity being queried for is very general, which leads us to assume that the static rank of the page is the factor behind bringing it to the top result.
Here, we introduced an efficient method for entity resolu-tion that outperforms related work in both accuracy and run time. Our work is unique in that search engines are used as classifiers with URLs as class labels, where previous search engine based disambiguation methods use search results as features in a black box clustering method. This translates to a linear running time for our method compared with usu-ally quadratic times for typical clustering algorithms. We believe that the effectiveness of our approach can be traced back to the numerous implicit signs that are utilized dur-ing query ranking such as: click through logs and context of incoming links. Such information cannot be captured if we just cluster the content of the pages returned by a search engine.

Future work could be experiments with a soft clustering approach that allows entities to belong to different URLs. In addition, one could experiment with results from different search engines and compare the performance.
