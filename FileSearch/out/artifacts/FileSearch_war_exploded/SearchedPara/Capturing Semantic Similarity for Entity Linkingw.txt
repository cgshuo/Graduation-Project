 One of the major challenges of entity linking is re-solving contextually polysemous mentions. For ex-ample, Germany may refer to a nation, to that na-tion X  X  government, or even to a soccer team. Past approaches to such cases have often focused on col-lective entity linking: nearby mentions in a docu-ment might be expected to link to topically-similar entities, which can give us clues about the identity of the mention currently being resolved (Ratinov et al., 2011; Hoffart et al., 2011; He et al., 2013; Cheng and Roth, 2013; Durrett and Klein, 2014). But an even simpler approach is to use context information from just the words in the source document itself to make sure the entity is being resolved sensibly in context. In past work, these approaches have typi-cally relied on heuristics such as tf-idf (Ratinov et al., 2011), but such heuristics are hard to calibrate and they capture structure in a coarser way than learning-based methods.

In this work, we model semantic similarity be-tween a mention X  X  source document context and its potential entity targets using convolutional neural networks (CNNs). CNNs have been shown to be ef-fective for sentence classification tasks (Kalchbren-ner et al., 2014; Kim, 2014; Iyyer et al., 2015) and for capturing similarity in models for entity linking (Sun et al., 2015) and other related tasks (Dong et al., 2015; Shen et al., 2014), so we expect them to be effective at isolating the relevant topic semantics for entity linking. We show that convolutions over mul-tiple granularities of the input document are useful for providing different notions of semantic context. Finally, we show how to integrate these networks with a preexisting entity linking system (Durrett and Klein, 2014). Through a combination of these two distinct methods into a single system that leverages their complementary strengths, we achieve state-of-the-art performance across several datasets. Our model focuses on two core ideas: first, that topic semantics at different granularities in a document are helpful in determining the genres of entities for entity linking, and second, that CNNs can distill a block of text into a meaningful topic vector.
Our entity linking model is a log-linear model that places distributions over target entities t given a mention x and its containing source document. For now, we take P ( t | x )  X  exp w &gt; f C ( x,t ;  X  ) , where f C produces a vector of features based on CNNs with parameters  X  as discussed in Section 2.1. Section 2.2 describes how we combine this simple model with a full-fledged entity linking system. As shown in the middle of Figure 1, each feature in f C is a cosine similarity between a topic vector asso-ciated with the source document and a topic vector associated with the target entity. These vectors are computed by distinct CNNs operating over different subsets of relevant text.

Figure 1 shows an example of why different kinds of context are important for entity linking. In this case, we are considering whether Pink Floyd might link to the article Gavin Floyd on Wikipedia (imagine that Pink Floyd might be a person X  X  nick-name). If we look at the source document, we see that the immediate source document context around the mention Pink Floyd is referring to rock groups ( Led Zeppelin , Van Halen ) and the target entity X  X  Wikipedia page is primarily about sports ( baseball starting pitcher ). Distilling these texts into succinct topic descriptors and then comparing those helps tell us that this is an improbable entity link pair. In this case, the broader source document context actu-ally does not help very much, since it contains other generic last names like Campbell and Savage that might not necessarily indicate the document to be in the music genre. However, in general, the whole document might provide a more robust topic esti-mate than a small context window does. 2.1 Convolutional Semantic Similarity Figure 1 shows our method for computing topic vec-tors and using those to extract features for a potential Wikipedia link. For each of three text granularities in the source document (the mention, that mention X  X  immediate context, and the entire document) and two text granularities on the target entity side (title and Wikipedia article text), we produce vector rep-resentations with CNNs as follows. We first embed each word into a d -dimensional vector space using standard embedding techniques (discussed in Sec-tion 3.2), yielding a sequence of vectors w 1 ,...,w n . We then map those words into a fixed-size vector using a convolutional network parameterized with a rectified linear unit (ReLU) and combine the results with sum pooling, giving the following formulation: where w j : j + ` is a concatenation of the given word volution granularity (mention, context, etc.) has a distinct set of filter parameters M g .

This process produces multiple representative topic vectors s ment , s context , and s doc for the source document and t title and t doc for the target entity, as shown in Figure 1. All pairs of these vectors be-tween the source and the target are then compared using cosine similarity, as shown in the middle of Figure 1. This yields the vector of features f C ( s,t e ) which indicate the different types of similarity; this vector can then be combined with other sparse fea-tures and fed into a final logistic regression layer (maintaining end-to-end inference and learning of the filters). When trained with backpropagation, the convolutional networks should learn to map text into vector spaces that are informative about whether the document and entity are related or not. 2.2 Integrating with a Sparse Model The dense model presented in Section 2.1 is effec-tive at capturing semantic topic similarity, but it is most effective when combined with other signals for entity linking. An important cue for resolving a mention is the use of link counts from hyperlinks in Wikipedia (Cucerzan, 2007; Milne and Witten, 2008; Ji and Grishman, 2011), which tell us how often a given mention was linked to each article on Wikipedia. This information can serve as a useful prior, but only if we can leverage it effectively by tar-geting the most salient part of a mention. For exam-ple, we may have never observed President Barack Obama as a linked string on Wikipedia, even though we have seen the substring Barack Obama and it un-ambiguously indicates the correct answer.

Following Durrett and Klein (2014), we introduce a latent variable q to capture which subset of a men-tion (known as a query ) we resolve. Query gen-eration includes potentially removing stop words, plural suffixes, punctuation, and leading or tail-ing words. This processes generates on average 9 queries for each mention. Conveniently, this set of queries also defines the set of candidate entities that we consider linking a mention to: each query gener-ates a set of potential entities based on link counts, whose unions are then taken to give on the possible entity targets for each mention (including the null link). In the example shown in Figure 1, the query phrases are Pink Floyd and Floyd , which generate Pink Floyd and Gavin Floyd as potential link targets (among other options that might be derived from the Floyd query).
 Our final model has the form P ( t | x ) = P linear way with three separate components: f
Q and f E are both sparse features vectors and are taken from previous work (Durrett and Klein, 2014). f
C is as discussed in Section 2.1. Note that f C has its own internal parameters  X  because it relies on CNNs with learned filters; however, we can compute gradients for these parameters with standard back-propagation. The whole model is trained to maxi-mize the log likelihood of a labeled training corpus using Adadelta (Zeiler, 2012).

The indicator features f Q and f E are described in more detail in Durrett and Klein (2014). f Q only impacts which query is selected and not the disam-biguation to a title. It is designed to roughly cap-ture the basic shape of a query to measure its de-sirability, indicating whether suffixes were removed and whether the query captures the capitalized sub-sequence of a mention, as well as standard lexical, POS, and named entity type features. f E mostly captures how likely the selected query is to corre-spond to a given entity based on factors like an-chor text counts from Wikipedia, string match with proposed Wikipedia titles, and discretized cosine similarities of tf-idf vectors (Ratinov et al., 2011). Adding tf-idf indicators is the only modification we made to the features of Durrett and Klein (2014). We performed experiments on 4 different entity link-ing datasets.  X  ACE (NIST, 2005; Bentivogli et al., 2010):  X  CoNLL-YAGO (Hoffart et al., 2011): This cor- X  WP (Heath and Bizer, 2011): This dataset con- X  Wikipedia (Ratinov et al., 2011): This We use standard train-test splits for all datasets ex-cept for WP, where no standard split is available. In this case, we randomly sample a test set. For all experiments, we use word vectors computed by running word2vec (Mikolov et al., 2013) on all Wikipedia, as described in Section 3.2.

Table 1 shows results for two baselines and three variants of our system. Our main contribution is the combination of indicator features and CNN fea-tures (Full). We see that this system outperforms the results of Durrett and Klein (2014) and the AIDA-LIGHT system of Nguyen et al. (2014). We can also compare to two ablations: using just the sparse features (a system which is a direct extension of Durrett and Klein (2014)) or using just the CNN-perform the sparse features and improve even further when stacked with them. This reflects that they cap-ture orthogonal sources of information: for example, the sparse features can capture how frequently the target document was linked to, whereas the CNNs can capture document context in a more nuanced way. These CNN features also clearly supersede the sparse features based on tf-idf (taken from (Rati-nov et al., 2011)), showing that indeed that CNNs are better at learning semantic topic similarity than heuristics like tf-idf.

In the sparse feature system, the highest weighted cosim ( s doc ,t doc ) 77.43 79.76 72.93 cosim ( s ment ,t title ) 80.19 80.86 70.25 All CNN pairs 84.85 86.91 82.02 features are typically those indicating the frequency that a page was linked to and those indicating spe-cific lexical items in the choice of the latent query variable q . This suggests that the system of Dur-rett and Klein (2014) has the power to pick the right span of a mention to resolve, but then is left to gener-ally pick the most common link target in Wikipedia, which is not always correct. By contrast, the full system has a greater ability to pick less common link targets if the topic indicators distilled from the CNNs indicate that it should do so. 3.1 Multiple Granularities of Convolution One question we might ask is how much we gain by having multiple convolutions on the source and tar-get side. Table 2 compares our full suite of CNN features, i.e. the six features specified in Figure 1, with two specific convolutional features in isola-tion. Using convolutions over just the source doc-ument ( s doc ) and target article text ( t doc ) gives a using convolutions over just the mention ( s ment ) and the entity title ( t title ). These represent two extremes of the system: consuming the maximum amount of context, which might give the most ro-bust representation of topic semantics, and consum-ing the minimum amount of context, which gives the most focused representation of topics seman-tics (and which more generally might allow the sys-tem to directly memorize train-test pairs observed in training). However, neither performs as well as the combination of all CNN features, showing that the different granularities capture complementary aspects of the entity linking task. 3.2 Embedding Vectors We also explored two different sources of embed-ding vectors for the convolutions. Table 4 shows that word vectors trained on Wikipedia outperformed Google News word vectors trained on a larger cor-pus. Further investigation revealed that the Google News vectors had much higher out-of-vocabulary rates. For learning the vectors, we use the standard word2vec toolkit (Mikolov et al., 2013) with vector length set to 300, window set to 21 (larger windows produce more semantically-focused vectors (Levy and Goldberg, 2014)), 10 negative samples and 10 iterations through Wikipedia. We do not fine-tune word vectors during training of our model, as that was not found to improve performance. 3.3 Analysis of Learned Convolutions One downside of our system compared to its purely indicator-based variant is that its operation is less in-terpretable. However, one way we can inspect the learned system is by examining what causes high ac-tivations of the various convolutional filters (rows of the matrices M g from Equation 1). Table 3 shows the n -grams in the ACE dataset leading to maximal activations of three of the filters from M doc . Some filters tend to learn to pick up on n -grams character-istic of a particular topic. In other cases, a single fil-ter might be somewhat inscrutable, as with the third column of Table 3. There are a few possible explana-tions for this. First, the filter may generally have low activations and therefore have little impact in the fi-nal feature computation. Second, the extreme points of the filter may not be characteristic of its overall behavior, since the bulk of n -grams will lead to more moderate activations. Finally, such a filter may rep-resent the superposition of a few topics that we are unlikely to ever need to disambiguate between; in a particular context, this filter will then play a clear role, but one which is hard to determine from the overall shape of the parameters. In this work, we investigated using convolutional networks to capture semantic similarity between source documents and potential entity link targets. Using multiple granularities of convolutions to eval-uate the compatibility of a mention in context and several potential link targets gives strong perfor-mance on its own; moreover, such features also im-prove a pre-existing entity linking system based on sparse indicator features, showing that these sources of information are complementary.
 This work was partially supported by NSF Grant CNS-1237265 and a Google Faculty Research Award. Thanks to the anonymous reviewers for their helpful comments.
