 Informal communication (e-mail, bulletin boards) poses a difficult learning environment because traditional grammat-ical and lexical information are noisy. Other information is necessary for tasks such as named entity detection. How topic-centric, or informative, a word is can be valuable in-formation. It is well known that informative words are best modeled by  X  X eavy-tailed X  distributions, such as mixture models. However, informativeness scores do not take full advantage of this fact. We introduce a new informativeness score that directly utilizes mixture model likelihood to iden-tify informative words. We use the task of extracting restau-rant names from bulletin board posts as a way to determine effectiveness. We find that our  X  X ixture score X  is weakly effective alone and highly effective when combined with In-verse Document Frequency. We compare against other in-formativeness criteria and find that only Residual IDF is competitive against our combined IDF/Mixture score. I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing; I.2.6 [ Artificial Intelligence ]: Learning Algorithms Named Entity Extraction, Inverse Document Frequency, Mix-ture Models, Term Frequency Distribution
We are interested in the problem of extracting information from informal, written communication. At the time of this writing, Google.com catalogs eight billion web pages. There are easily that number of e-mail, newsgroup and bulletin-board posts each day. The web is filled with information, but even more information is available in the informal com-munications people send and receive on a day-to-day basis. We call this communication informal because structure is not explicit and the writing is not fully grammatical. Web pages are highly structured. They use links, headers and tags to mark-up the text and identify important pieces of information. Newspaper text is harder to deal with. Gone is the computer-readable structure. But, newspaper articles have proper grammar with correct punctuation and capital-ization; part-of-speech taggers show high accuracy on news-paper text. In informal communication, even these basic cues are noisy X  X rammar rules are bent, capitalization may be ignored or used haphazardly and punctuation use is cre-ative. There is good reason why little work has been done on this topic: the problem is challenging and data can be difficult to attain due to privacy issues. Yet, the volume of informal communication that exists makes us believe that trying to chip away at the information extraction problem is a useful endeavor.

Restaurants are one subject where informal communica-tion is highly valuable. Much information about restaurants can be found on the web and in newspaper articles. Zagat X  X  publishes restaurant guides. Restaurants are also discussed on mailing lists and bulletin boards. When a new restau-rant opens, it often takes weeks, or months before reviews are published on the web or in the newspaper (Zagat X  X  guides take even longer). However, restaurant bulletin boards con-tain information about new restaurants almost immediately after they open (sometimes even before they open). They are also  X  X p X  on major changes: a temporary closure, new management, better service or a drop in food quality. This information is difficult to find elsewhere.

This timely information can be difficult to extract. Sys-tems that extract named entities from newspaper articles rely heavily on capitalization, punctuation and correct part-of-speech information. In informal communication, much of this information is noisy X  X ther features need to be in-corporated. An important sub-task of extracting informa-tion from restaurant bulletin boards is identifying restaurant names. It has been found that named entities, like restau-rant names, are highly relevant to the topic of a document [6]. If we had a good measure of how topic-oriented, or  X  X nformative, X  each word was, we would be better able to identify named entities. It is well known that informative words have  X  X eaked X  or  X  X eavy-tailed X  frequency distribu-tions [5]. Many informativeness scores have been introduced, including Inverse Document Frequency (IDF) [11], Residual IDF [4], x I [2], the z -measure [8] and Gain [12]. Only x makes direct use of the fit of a word X  X  frequency statistics to a peaked/heavy-tailed distribution. However, x I does a poor job of finding informative words. We introduce a new informativeness score that is based on the fit of a word X  X  fre-quency statistics to a mixture of 2 Unigram distributions. We find that it is effective at identifying topic-centric words. We also find that it combines well with IDF. Our combined IDF/Mixture score is highly effective at identifying informa-tive words. In our restaurant extraction task, only one other informativeness score, Residual IDF, is competitive. Using Residual IDF or our combined IDF/Mixture score, our abil-ity to identify restaurant names is significantly better than using capitalization, punctuation and part-of-speech infor-mation alone. In more formal or structured settings, infor-mativeness may be of marginal use, but here we find it to be of great value.
Inverse document frequency (IDF) is an informativeness score that was originally introduced by Jones [11]. It embod-ies the principle that the more rare a word is, the greater the chance it is relevant to those documents in which it appears. Specifically, the IDF score for a word, w , is The IDF score has long been used to weight words for in-formation retrieval. It has also been used with success in text classification [10, 13]. Recently, Papineni [12] showed that the IDF score can be derived as the optimal classifica-tion weight for a special self-classification problem using an exponential model and a generalized form of likelihood. In short, IDF has seen much success and has theoretical jus-tification. However, it is a weak identifier of informative words.

Since the introduction of IDF, many other informativeness scores have been introduced. Bookstein and Swanson [2] introduce the x I measure for a word w , where f w is the frequency of word w and d w is the docu-ment frequency of word w (number of documents in which w occurs). Informative words tend to exhibit  X  X eaked X  dis-tributions with most occurrences coming in a handful of doc-uments. This score makes sense at the intuitive level since, for two words with the same frequency, the one that is more concentrated will have the higher score. However, this score has a bias toward frequent words, which tend to be less in-formative.

Harter [8] noted that frequency statistics of informative or  X  X pecialty X  words tend to fit poorly to a Poisson distri-bution. He suggested that informative words may be iden-tified by observing their fit to a mixture of 2 Poissons ( X 2-Poisson X ) model; he introduced the z -measure as a crite-rion for identifying informative words. The z -measure, in-troduced earlier by [3], is a general measure between two distributions. It computes the difference between means di-vided by square-root of the summed variances: Harter found that this measure could be used to identify informative words for keyword indexing.

Twenty years later, Church and Gale [4] noted that nearly all words have IDF scores that are larger than what one would expect according to an independence-based model (such as the Poisson). They note that interesting or in-formative should tend to have the largest deviations from what would be expected. They introduced the Residual IDF score, which is the difference between the observed IDF and the IDF that would be expected: The intuition for this measure is similar to that of Bookstein and Swanson X  X  x I -measure. Words that are clustered in few documents will tend to have higher Residual IDF scores. However, whereas x I has a bias toward high-frequency words, Residual IDF has the potential to be largest for medium-frequency words. As such, it serves as a better informative-ness score. In our experiments, we find that Residual IDF is the most effective individual informativeness score.
Recently, Papineni [12] showed that IDF is the  X  X ptimal weight of a word with respect to minimization of a Kullback-Lieber distance. X  He notes that the weight (IDF) is different from the importance or  X  X ain X  of a feature. He suggests that the gain in likelihood attained by introducing a feature can be used to identify  X  X mportant X  or informative words. He derives the gain for a word w as where d w is the document frequency of word w and D is the total number of documents. Extremely rare and ex-tremely common words have low gain. Medium-frequency words have higher gain. A weakness of this measure is that it relies solely on document frequency X  X t does not take ac-count for  X  X eaked-ness X  of a word X  X  frequency distribution.
These informativeness measures represent a variety of ap-proaches to identifying informative words. Only Harter X  X  z -measure directly makes use of how a word X  X  frequency statistics fit a heavy-tailed mixture distribution. Yet, our study indicates that the z -measure is a poor identifier of in-formative words. In the next section, we introduce a new measure based on a word X  X  fit to a mixture distribution. It is a given that topic-centric words are somewhat rare. But we think that they also exhibit two modes of operation: (1) a high frequency mode, when the document is relevant to the word, and (2) a low (or zero) frequency mode, when the document is irrelevant. A mixture is well-suited to model this behavior. We think that we can identify informative words by looking at the difference in log-likelihood between a mixture model and a simple unigram model.

We propose that informative words can be effectively mod-eled with mixtures. For each word, we treat each document as a sequence of coin flips, heads (H) representing an occur-rence and tails (T) representing a non-occurrence. Consider the following four short  X  X ocuments X : The simplest model for sequential binary data (coin flips) is the unigram. For binary data, the unigram uses a single parameter which represents the chance of heads on each flip: We use h i for the number of heads and n i for the number of flips per document. The unigram is a poor model for the above data. The maximum likelihood unigram parameter is  X   X  = 0 . 5 and the data likelihood is 2  X  12 . The unigram has no capability to model the switching nature of the data. A mixture is a composite model. It randomly selects between a number of component models. The likelihood for a mixture of two unigrams is: p mix ( ~n, ~ h |  X , X  1 , X  2 ) = Y Here, the maximum likelihood parameters are  X   X  = 0 . 5, 1,  X   X  2 = 0 and the data likelihood is 2  X  4 . In effect, the mixture model makes 4 equi-probable decisions whereas the unigram makes 12 decisions. The two extra parameters of the mixture allow for a much better modeling of the data. When data exhibits two distinct modes of behavior, such as with our coin example, the mixture will yield a much higher data likelihood than the simple unigram.

Now we are ready to introduce our new informativeness score. For each word, we find maximum-likelihood parame-ters for both the unigram and mixture models. Our  X  X ix-ture score X  is then the log-odds of the two likelihoods: We use a ratio because we are interested in knowing the comparative improvement of the mixture model over the simple unigram. And, the log-odds ratio grounds the score at zero. The mixture is strictly more expressive than the simple unigram, so we can guarantee that the score will be non-negative.
We use Expectation-Maximization to maximize the like-lihood of the mixture model. We avoid a full discussion of EM because it is not essential to the understanding of our contributions. See Dempster et al. for more information [7]. EM uses a bound to iteratively update model parameters to increase likelihood. Since likelihood as a function of mixture parameters is not convex, the maximum EM finds may only be local. To increase our chances of finding a global max-imum, we use two starting points: (1) one slightly offset from the unigram model, and (2) one  X  X plit X  model where the first unigram component is set to zero and the second component and the mixing parameter (  X  ) are set to other-wise maximize the likelihood of the data. We found that this worked well to find global maxima X  X xtensive random sampling never found a higher likelihood parameter setting.
We think that the Mixture score can serve as an effective term informativeness score. To evaluate the correctness of our belief, we use the task of identifying restaurant names in posts to a restaurant discussion bulletin board. We treat
Token Score Rest. sichuan 99.62 31/52 speed 44.69 16/19 tacos 43.77 4/19 indian 41.38 3/30 tokyo 39.27 7/11 greek 38.15 0/20 Table 1: Top Mixture Score (left) and Residual IDF (right) Tokens. Bold-face words occurred at least once as part of a restaurant name. each thread as a document and calculate various informa-tiveness scores using word-thread statistics. Restaurants are often the topic of discussion and tend to be highly informa-tive words. The task of identifying them serves as a good test ground for any measure that claims to rate informative-ness. We collected posts from the board and hand-labeled them. The next section details our findings.
We used as a test-bed posts from a popular restaurant bul-letin board. The maintainers of the site moderate the board and lay out a set of ground rules for posting. The people who post are not professional restaurant reviewers. They simply enjoy eating and discussing what they have eaten. Information about restaurants can be found in the discus-sions that ensue. Major metropolitan areas each have their own bulletin board; other boards are grouped by region.
We collected and labeled six sets of threads of approxi-mately 100 posts each from a single board (615 posts total). We used Adwait Ratnaparkhi X  X  MXPOST and MXTERMI-NATOR 1 software to determine sentence boundaries, tok-enize the text and determine part-of-speech. We then hand-labeled each token as being part of a restaurant name or not. Labeling of the 56,018 tokens took one person about five hours of time. 1,968 tokens were labeled as (part of) a restaurant name. The number of restaurant tokens per set ranged from 283 to 436. We found 5,956 unique tokens. Of those, 325 were used at least once as part of a restaurant name. We used a separate set of data for developing and debugging our experiment code.
Here we explore how the various measures serve as infor-mativeness filters. First, we consider the density of restau-rant tokens in the top-ranked words. Both Gain and IDF serve as poor informativeness filters, at least with respect to restaurant names X  X nly occasional restaurant tokens are found in words ranked highest by Gain and IDF. The x I -measure ranks some restaurant tokens highly X  X ive of the top 10 words occur at least once as part of a restaurant. However, these tokens only appear in restaurant names very rarely. None of the top 30 x I words occur as part of restau-rant names at least 50% of the time. The z -measure serves as a reasonably good informativeness filter X  X hree of the top http://www.cis.upenn.edu/  X  adwait/statnlp.html
Rank Token Rest. 21 zoe 10/11 22 penang 7/9 23 pearl 11/13 26 dhaba 8/13 29 gourmet 23/27 30 atasca 9/10 Table 2: Top Mixture Score (left) and Residual IDF (right) Restaurant Tokens (50%+ restaurant usage) 10 words occur as restaurant tokens and nine of the top 30 words occur in restaurant names at least 50% of the time. Both the mixture score and Residual IDF have high densi-ties of restaurant tokens in their top ranks. Table 1 shows the top 10 words ranked by the Mixture score and Residual IDF. For both measures, seven of the top 10 words are used at least once as part of a restaurant name. Table 2 shows, for each measure, the top 10 words used a majority of the time in restaurant names. Most of the top-ranked Resid-ual IDF words occur a majority of the time in restaurant names. Fewer top Mixture score words are majority used in restaurant names, but those that are occur more often than the top Residual IDF words. Top-ranked words give only a partial view of the effectiveness of an informativeness filter. Next, we look at average and median scores and ranks across our entire corpus.
 Table 3: Average and Median Restaurant Token Ranks (lower is better)
So far, we have considered the upper-tail of informative-ness scores; we have done our counting over unique words, thus overweighting rare ones. Here, we compile statistics across the full set of data and count each restaurant to-ken occurrence separately. For each informativeness score, we compute the score for each unique word and rank the words according to score. Then, for each of the 1,968 to-kens labeled as (part of) a restaurant name, we determine the token X  X  rank. We compute both the average and median ranks of the restaurant tokens. Table 3 gives the average and median ranks of restaurant words for the various infor-mativeness scores. The Mixture score gives the best aver-age and median rank. The z -measure and x I -measure give slightly worse rankings. Residual IDF and Gain are better than the baseline 2 , while IDF yields worse rankings than the
Baseline average and median rank are what would be ex-pected from a score that assigns values randomly. Note that there are 5,956 unique words; 2,978 is half that number. baseline. The average and median rank give us a good feel for how well a score works as a filter, but not necessarily as a feature in a natural language system. Next, we discuss an evaluation that may better reflect performance on a real task.
 Table 4: Average and Median Relative Scores of Restaurant Tokens
Now we consider the average and median score of restau-rant tokens. For each of the 1,968 tokens labeled as (part of) a restaurant name, we compute the informativeness score. We then take an average or median of those scores. We di-vide by the average or median score across all 56,018 tokens to attain a  X  X elative X  score. We do this so that absolute mag-nitude of the informativeness score is irrelevant; i.e. multi-plication by a constant has no effect. Table 4 shows average and median relative scores for restaurant tokens. Of note is the fact that informativeness scores that produce good average/median ranks do not necessarily produce good av-erage/median scores (e.g. z and x I ). Residual IDF gives the best average relative score; IDF gives the best median rela-tive score. The Mixture score gives the second-best average relative score and second-best median relative score.
At this point, it seems sufficiently clear that the z -measure, the x I measure and Gain have relatively little to offer in terms of identifying informative words, at least when com-pared to Residual IDF, IDF and the Mixture score. We focus on RIDF, IDF and the Mixture score for the remainder of this paper.
To this point, both Residual IDF and the Mixture Score appear to be excellent informativeness scores. Both have a high density of restaurant tokens in their highest ranks; for both measures, average/median ranks/scores are much better than baseline. IDF, however, ranks restaurant words poorly, but yields the best median relative score. Since IDF seems so different from the other two scores, we postulate that it might work well in combination.

We look at how well correlated the scores are. If two scores are highly correlated, there is little use in combin-ing them X  X heir combination will be similar to either score individually. However, if two scores are uncorrelated, then they are measuring different sorts of information and may produce a score in combination that is better at identifying informative words than either score individually.
First, we consider a very simple test on our restaurant data set: how much overlap is there in highly-rated restau-rant words? For each of the scores, we choose a threshold that splits the restaurant words (approximately) in half. We then count the number of restaurant words that score above both thresholds. For scores that are independent of each other, we would expect the joint count to be about half of the individual count. Table 5 gives the individual and joint statistics. The Mixture/RIDF and IDF/RIDF combi-nations both show a substantial degree of dependence. This is not the case for Mixture/IDF. If the Mixture and IDF scores were independent, we would expect a joint count of 176  X  170 / 325 = 92, almost exactly the joint count that we do observe, 93. This gives us reason to believe that the Mix-ture and IDF scores may be uncorrelated and may work well in combination.
 Table 5: Number of restaurant tokens above score thresholds.

Our test provides evidence that the IDF and Mixture scores are independent, but it does not exclude the pos-sibility that there are pockets of high correlation. Next, we consider more traditional measures. Figure 1 shows scatter plots of the pairs of scores. Residual IDF (RIDF) and Mix-ture show a high degree of correlation X  X nowledge of RIDF is very useful for attempting to predict Mixture score and vice versa. IDF and RIDF show correlation, at least par-tially reflecting the fact that IDF bounds RIDF. IDF and Mixture show little relation X  X here is no clear trend in the Mixture score as a function of IDF. These observations are reflected in correlation coefficients calculated on the data, shown in Table 6. IDF and Mixture are practically uncor-related, while the other score pairs show substantial corre-lation.
 Table 6: Correlation coefficients for pairs of the IDF, Residual IDF and Mixture scores on restau-rant words. IDF and Mixture are effectively uncor-related in the way they score restaurant words.
 That the IDF and the Mixture scores would work well Table 8: Top IDF*Mixture Score Restaurant Tokens (50%+ restaurant usage) together makes sense intuitively. They capture very different aspects of the way in which we would expect an informative word to behave. IDF captures rareness; the Mixture score captures a multi-modal or topic-centric nature. These are both aspects that partially identify informative words. Next we investigate whether a combination score is effective for identifying informative words.
We use the relaxation of the conjunction, a simple prod-uct, to combine IDF and Mixture. We denote this by  X  X DF*Mixture. X  Table 7 shows the top 10 tokens accord-ing to the IDF*Mixture score. Eight of the top 10 are used as restaurant names. Worth noting is that the other two words ( X  X ibs X  and  X  X estival X ) were topics of discussions on the restaurant bulletin board. Table 8 gives the ranks of the top 10 tokens that were used regularly in restaurant names. Compared to the Mixture score, restaurant tokens more densely populate the upper ranks. Ten of the top 23 tokens are regularly used as restaurant names. The trend continues. 100 of the top 849 IDF*Mixture tokens are regu-larly used in restaurant names, while 100 of the top 945 Mix-ture tokens are regularly used in restaurant names. How-ever, Mixture catches up and and surpasses IDF*Mixture (in terms of restaurant density) as we continue down the list. This explains why Mixture has better average and me-dian ranks (next paragraph).
 Table 9: Average and Median Restaurant Token Ranks Table 10: Average and Median Relative Scores of Restaurant Tokens. Note that a superscript indi-cates that the score is raised to the given power. Here we give rank and relative score averages for IDF*Mixture. Table 9 gives the average and median ranks like before. Mix-ture still leads, but IDF*Mixture is not far behind. Ta-ble 10 gives the average and median relative scores. The relative score is affected by exponentiation, so we compare against squared versions of IDF, Mixture and Residual IDF. IDF*Mixture achieves the best median and is a close sec-ond for average relative score. IDF*Mixture appears to be a better informativeness score than either IDF or the Mix-ture score and very competitive against Residual IDF. In the next section, we describe the set-up for a  X  X eal X  test: a named entity (restaurant name) extraction task.
So far, we have focused on filtering. In this section, we consider on the task of detecting restaurant names. We use the informativeness scores as features in our classifier and report on how accurately restaurants are labeled on test data.
The F-measure [15], is commonly used to measure per-formance in problems where negative examples outnumber positive examples. We use the F1-measure ( X  X 1 X ), which equally weights precision, p = tp tp+fp , and recall, r = F1 varies as we move our classification threshold along the real number line. To eliminate any effects of selecting a Table 11: The contingency table for the binary clas-sification problem.  X  X p X ,  X  X n X ,  X  X p X , and  X  X n X  are the numbers of true positives, false positives, false neg-atives and true negatives, respectively. particular threshold, we report the maximum F1 score at-tained over all threshold values. We call this  X  X 1 breakeven X  in reference to a similarity it shares with precision-recall breakeven [10]; the breakeven F1 tends to occur when preci-sion and recall are nearly equal. However, unlike precision-recall breakeven, F1 breakeven is well-defined.
Given two classifiers evaluated on the same test sets, we can determine whether one is better than the other using paired differences. We use the Wilcoxon signed rank test [16]; it imposes a minimal assumption X  X hat the difference distribution is symmetric about zero. The Wilcoxon test uses ranks of differences to yield finer-grained distinctions than a simple sign test.

We use the one-sided upper-tail test, which compares the zero-mean null hypothesis, H 0 :  X  = 0, against the hypoth-esis that the mean is greater than zero, H 1 :  X  &gt; 0. We compute a statistic based on difference ranks. Let z i be the i th difference. Let r i be the rank of | z i | . Let  X  i be a an indicator for z i : The Wilcoxon signed rank statistic is: Upper-tail probabilities for the null hypothesis are calcu-lated for each possible value 3 . We reject H 0 (and accept H ) if the probability mass is sufficiently small. We use  X  = 0 . 05 as the threshold below which we declare a result to be significant. Table 12 gives the upper-tail probabilities for a subset of the possible values of T + . Values of 19 and higher are significant at the  X  = 0 . 05 level. Table 12: Upper-tail probabilities for the null hy-pothesis.
We used 6-fold cross-validation for evaluation: for each of the six sets, we used the other five sets as training data for
We use values from Table A.4 of Hollander and Wolfe [9]. Table 13: Named Entity Extraction Performance IDF*RIDF 20 18 18 16 -IDF*Mixture 21 21 21 21 15 Table 14: T + Statistic for F1 Breakeven. Each entry that is 19 or higher means that the score to the left is significantly better than the score above. For example, IDF*Mixture is significantly better than RIDF. our classifier 4 . No data from the  X  X est X  set was ever used to select parameters for the corresponding classifier. However, since the test set for one fold is used in the training set for another, we note that our significance calculations may be overconfident.

For classification, we used a regularized least squares clas-sifier (RLSC) [14] and used a base set of features like those used in [1]. Current, next and previous parts-of-speech (POS) were used, along with current-next POS pairs and previous-next POS pairs. We included features on the current, pre-vious and next tokens indicating various types of location, capitalization, punctuation and character classes (firstWord, lastWord, initCap, allCaps, capPeriod, lowerCase, noAlpha and alphaNumeric). Unlike HMMs, CRFs or MMM net-works, RLSC labels tokens independently (like an SVM does). We believe that using a better classifier would improve over-all classification scores, but would not change relative per-formance ranking.
Table 13 gives the averaged performance measures for six different experimental settings:
To select a regularization parameter, we trained on four of the five  X  X raining X  sets, evaluated on the fifth and selected the parameter that gave the best F1 breakeven.
 Table 14 gives the Wilcoxon signed rank statistic for pairs of experimental settings. IDF and the Mixture score both yield small improvements over baseline. The improvement for IDF is significant. Residual IDF serves as the best in-dividual informativeness score, yielding a significant, 2.39 percentage-point improvement over baseline and significant improvements over both IDF and Mixture. The IDF*Mixture score yields further improvement, 4.26 percentage-points bet-ter than baseline and significantly better than IDF, Mixture and Residual IDF. For completeness, we compare against the IDF*RIDF score (the product of IDF and Residual IDF scores). IDF*Mixture yields the larger average F1 breakeven, but we cannot say that the difference is significant.
These results indicate that the IDF*Mixture product score is an effective informativeness criterion; it is better than Residual IDF and competitive with the IDF*RIDF product score. The IDF*Mixture product score substantially im-proves our ability to identify restaurant names in our data.
We introduced a new informativeness measure, the Mix-ture score, and compared it against a number of other in-formativeness criteria. We conducted a study on identify-ing restaurant names from posts to a restaurant discussion board. We found the Mixture score to be an effective restau-rant word filter. Residual IDF was the only other measure found to be competitive. We found that the Mixture score and IDF identify independent aspects of informativeness. We took the relaxed conjunction (product) of the two scores, IDF*Mixture, and found it to be a more effective filter than either score individually. We conducted experiments on ex-tracting named entities (restaurant names). Residual IDF performed better than either IDF or Mixture individually, but IDF*Mixture out-performed Residual IDF.
 The authors acknowledge support from the DARPA CALO project. [1] D. M. Bikel, R. L. Schwartz, and R. M. Weischedel. [2] A. Bookstein and D. R. Swanson. Probabilistic models [3] B. C. Brookes. The measure of information retrieval [4] K. W. Church and W. A. Gale. Inverse document [5] K. W. Church and W. A. Gale. Poisson mixtures. [6] C. Clifton and R. Cooley. TopCat: Data mining for [7] A. P. Dempster, N. M. Laird, and D. B. Rubin. [8] S. P. Harter. A probabilistic approach to automatic [9] M. Hollander and D. A. Wolfe. Nonparametric [10] T. Joachims. Text categorization with support vector [11] K. S. Jones. Index term weighting. Information [12] K. Papineni. Why inverse document frequency. In [13] J. D. M. Rennie, L. Shih, J. Teevan, and D. R. [14] R. Rifkin. Everything Old Is New Again: A Fresh Look [15] C. J. van Rijsbergen. Information Retireval . [16] F. Wilcoxon. Individual comparisons by ranking
