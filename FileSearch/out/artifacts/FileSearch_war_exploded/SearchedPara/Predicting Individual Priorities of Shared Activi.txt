 Activity-centric collaboration environments help knowledge workers to manage the context of their shared work activities by providing a representation for an activity and its resources. Activity management systems provide more structure and organization than email to execute the shared activity but, as the number of shared activities increases, it becomes more and more difficult for users to focus on important activities that need their attention. This paper describes a personalized activity prioriti-zation approach implemented on top of the Lotus Connections Activities management system. Our prototype implementation allows each user to view activities ordered by her/his predicted priorities. The predictions are made using a ranking Support Vector Machine model trained with the user X  X  past interactions with the activities system. We describe the prioritization interface and the results of an offline experiment base d on data from 13 users over 6-months. Our results s how that our feature set derived from shared activity structures can significantly increase prediction accuracy compared to a recency baseline. H5.2 [ Information interfaces and presentation ]: User Interfaces. -Graphical user interfaces . G.3 [ Probability and Statistics ]: Experimental design, Probabilistic algorithms Design, Experimenta tion, Human Factors Activity-centric collaboration, prioritization, knowledge worker, ranking, support vector machine, activity flood Knowledge workers typically manage multiple tasks with different people at the same time. The idea of activity-centric collaboration [15] is to support knowledge workers in managing and switching between multiple tasks by providing the computational concept of a work ac tivity that represents the task and the resources involved. A se ries of activity management systems have been proposed, developed, or productized (e.g. [13], [15], [22], [27], or [28]). Although the definition of an activity varies across different authors, the many concepts are agreed-upon. For example, Geyer et al. [15] describe an activity as a  X  X ogical unit of work that incor porates all the tools, people, and resources needed to get the job done. Examples for activities are: preparing an executive meeting, planning a conference, closing a sale, [...], or writing or responding to an RFP . X  Activity management systems en courage knowledge workers to structure their everyday work as activities. Many systems support collaboration by allowing users to share content in the activity structure and by making changes or updates visible to other users. With an increasing number of activities, users experience an information overflow problem similar to the well-known problems of email: too many updates are coming in at the same time (e.g., [35]). Most activity management system s provide list views of activities ordered by recency of the latest modification to each activity or any of its component resources. A problem with this strategy is that all updates are treated equally, regardless of other factors such as the content of the update, the user X  X  role in the activity and the user X  X  relationship with the person who updated a component of the activity. To re solve this  X  X ctivity overflow X  problem, one solution is to rank activities by priority. Unlike ranking in web search engines, pr iorities of activities change more dynamically and may benefit from personalized models for each user. In this paper, we present a system and algorithm for prioritizing shared activities using machine learning techniques. We implemented our approach on top of the Lotus Connections Activities system (Lotus Activities) [26] and we integrated our user interface into Malibu ([16], [30]), a desktop client that provides access to Lotus Activities. We formalize the activity prioritization problem as a lear ning-to-rank problem, and we use a ranking Support Vector Machine (SVM) as our underlying model [21] Learning is supervised; first we retrieve activity data for each individual user from the centralized activity server. Then we create training data based on a user X  X  historic access and interaction patterns with his/her activities. During runtime, when new updates appear, the system predicts priorities of all activities based on the trained model and a feature set. The prediction results are shown in Malibu as a rank ordered list of activities. To verify the effectiveness of our system, we conducted an offline experiment with the data collected from 13 high-volume Lotus Activities users during a period of about 6 months. The experimental results indicate that our approach performs significantly better than conventional recency methods. The rest of the paper is organi zed as follows. Section 2 reviews related work. Section 3 introduces the Lotus Activities product and the Malibu client used for our work. Section 4 illustrates the architecture and the user interface of our prioritization system. Section 5 describes the ranking SV M and the feature set of our prioritization approach. Section 6 presents the experimental results, comparing our approach to two baseline recency methods. Section 7 summarizes the work a nd discusses future directions. Our goal of prioritizing activities can be reduced to the problem of finding a priority ordering or ranking across activities. Recently, machine learning approaches have treated ranking as a supervised learning problem. Res earchers have developed ranking methods based on the classical classification methods including Perceptron, neural networks a nd support vector machines (SVM) (e.g. [1], [4], [11], [21], [19], or [36]). Cohen [11] proposes an early approach to learning ranks in domain-specific research. Training is done by user feedback in the form of preference judgm ents on two items. Cohen X  X  algorithm then learns a pair-wis e linear preference function. The complete order of a set of items is learned by making it agree with the learned pair-wise preference function. The disadvantage of this approach is that finding the complete order with pair-wised preference function is NP-Complete. Herbrich et al. [19] and Crammer et al. [12] apply ordinal regression on SVMs and Perceptron models. Both of them create a connection between the classification and the ranking by modeling ranks as intervals of real numbers. Then the problem becomes learning a rank function that maps an input vector into a real number from the training data. In learning a rank model, one difficult part is to obtain training data. Training based on ranked pair s of items is more popular in existing work because it is easier to get a partial order than the whole order especially given a large number of items. For example, a partial order can be extracted from explicit user feedback such as the rating of pages (see [5], [36]). Other more user-friendly approaches employ implicit user feedback by capturing user behavior. For exam ple, Joachims [21] proposes an extension to SVMs using users X  clickthrough data on the search results to refine the ordering of future search results. Training pairs are generated based on the assumption that, in an ordered list of search results, a document selected by a user has a higher priority than other non-selected documents in the list before the selected document. The algorithm presented in this paper was inspired by Joachims [21]. We chose the Ranking SVM model fabecause it fits the pair-wise training data generated by users of our application very well. Also the kernel trick of it provides the ability to learn a non-linear decision boundary. Much research has been done on ranking web search results. (e.g. [1], [4], [5], [11], [21], [31], or [36]). Ranking web search results differs from ranking activities in that activity priorities are almost always personalized to the user, an activity is likely to change more frequently than a web page [9], and the set of individual activities to rank is much smaller than the number of web pages. Activity management systems can reduce information overflow, decrease search times, and improve interruption recovery because they support knowledge workers in maintaining multiple working contexts. Existing activity management systems fall into two categories: personal activity management systems and collaborative activity management systems. Personal activity management sy stems organize information on a user X  X  desktop in ways to provide information access in context, interruption recovery and info rmation finding. For example, Bellotti et al. [2] introduce TaskMast er, a threaded activity-centric email client which enables users to associate incoming email messages with existing activities and other resources. TaskTracer [13] is an activity management system that allows users to label resources on the desktop as being part of an activity. The system learns based on the labels and can automatically associate resources on the desktop with activities. Both TaskMaster and TaskTracer require user input to define their activities. Mitchell et al. [25] use an unsupervised clus tering approach to automatically extract activities from emails, online calendars, and contact names on the desktop. Collaborative activity management systems support activity coordination and collaboration am ong multiple users. Knowledge workers are typically involved in multiple activities and collaborations (e.g. [7], [18], [20], [28], [28]). The Haystack project [31] provides integrated presentations of diverse media that are assumed to be shared, such as email, instant messaging and web pages. It enables some awareness of how others are manipulating the artifacts. Activity Explorer [15], Unified Activity Management [27], and the Lotus Activities product [26] manage multiple types of shared items, support structured collec-tions of items, and provide access control at the level of items and collections. Activity Explorer al so supports notifications of updates to activities or their components ([29], [32]) Existing research in this area focuses mainly on information organization and interruption rec overy. The new work described in this paper helps knowledge workers to cope with activity overflow, i.e. with prioritizing an increased number of activities. This problem is similar to ma naging large numbers of email messages [35] or instant messages [18]. Our prototype leverages Lotus Activities, IBM X  X  activity management system ([22], [26]) which emerged from a multi-year research effort on activity-centric computing ([15], [27], [28]). Lotus Activities organizes and inte grates resources, tools, and people around the computational con cept of a work activity, with the goal of increasing work quality and efficiency. The system consists of a centralized, web-based service and includes many extensions for existing desktop applications. The design of Lotus Activities was driven by the goal of organizing work into shared or private activities. In this syst em an  X  X ctivity X  consists of a structured set of related, shared resources representing a task or project. The set of related resources is structured as a hierarchical thread called an  X  X ctivity outline X , representing the context of the task at hand. The activity outline (s ee (A) in Figure 1) is similar to working sphere [18] and greater diversity of resources than a thrask [2]. Users add items to activities by posting either a response to an existing item or a new resource, such as a file, comment, chat, URL, task etc. Complex tasks can be structured through subactivities. The system is extensible, i.e. new resource types can be added to customize the system to particular application scenarios. Activities and resources within them can have simple metadata associated with them: name, desc ription, tags, and optional due date (see (B) in Figure 1). Keyword-tagging allows users to find related resources in a single activity or across activities. Users can also search for terms within a specific activity, or across all their activities. Each activity has a web page associated with it (shown in Figure 1) so that users can see recent entries posted to the activity, navigate the activity outline, see all the entries in the activity organized by type, and see the history of the activity (C). Similar to Activity Explorer [15], the ac tivity outline (A) is automatically structured based on the response hierarchy in an activity, but it can also be reorganized post-hoc if a user wants to create their own structure within the activity. As noted above, a successful activity management system can lead to the problem of activity overflow. Malibu [16] was developed to assist knowledge wo rkers in their activity-centric work providing a peripheral list of current activities and related resources. It runs as a desktop side bar ( X  X alibu Board X ) that either slides out when users hover with their mouse at the left or right side of the screen or remains  X  X ticky X  and always visible on the deskt op. The Malibu Board contains a series of configurable views, each one displaying one data source (see Figure 2 ): My Activities (D) provides access to Lotus Activities as described in the previous section; Dogear Bookmarks (E) displays bookmarks from the Dogear social bookmarking system [24]; and My Feeds (F), is a general purpose feed reader. Dynamic views are frequently updated with the most recent or most relevant items. As such, Malibu provides peripheral access to and awareness of multiple data sources while the user focuses on her main work on the desktop. With this design, Malibu becomes an always-on companion that can display information that is contextually relevant to the current desktop activity of the user, draw attention to new and important events, and provide quick access to data sources that need attention. Because of these characteristics, we decided to integrate the prioritization scheme into the Lotus Activities view in Malibu. Also, since Malibu is a desktop front end to activities, it allowed us to offload computation onto a clie nt machine without modifying any of the Lotus Activities server code. In Malibu, the activity view (Figure 2 D) is the user interface to inspect and manipulate activities. The default version of Malibu is like other activity management approaches: All activities are listed in the order of their last update time by default. When there is a new update to one activity, the font of the activity becomes bold until the user accesses this activity. In Malibu, users can manually set an activity to be important or not important by right clicking an activity and selecting  X  X et Priority X  in the context menu. By default, each activity is neither important nor non-important. Important activities are displayed with a purple dot and non-important activities are displayed with a blue dot. Activities are sorted by recency per default. Users can filter the view to display only the important activities. Our activity prioritization system is implemented as a Malibu plug-in that enhances the activity view of the original Malibu version of [16] and [30]. Figure 3 shows the modified activity view. It supports both manually-specified priorities and machine predicted priorities using the algorithm described in this paper. We added new functionality to the Activities display through a prioritization feature based on m achine-learning. As shown in Figure 3, we decorate the top 5 activities with flags according to their predicted priority scores. In addition to recency, users can sort activities by predicted priory scores in each group (important, without-settings, non-important). Figure 4 depicts the architecture of our activity prioritizing system. It consists of the da ta retrieval component, ranking component and a local database. The data retrieval component retrieves activity data periodically from the Lotus Activities server via its Java and Atom AP I. For performance consideration, the activity data is cached locally in an embedded database. At the same time, activity data are sent to the ranking component. The ranking component updates its local activity representation including feature values and generates the new priority scores for the user X  X  activities. The computed priority scores are sent to the Malibu UI to update the list of activities in the Activities view. Our design imposes no extra processing costs on existing Malibu users. The data retrieval and ra nking component both work in the background with low computation overhead. Although the learning algorithm is supervised, we don X  X  bring extra user cost because the training data is from the logged history of activities stored in the centralized activity server. To build a personalized activity prioritization system, we formalize the problem as a lear ning-to-rank problem. We employ Ranking SVM [21] as the ranking algorithm. The learning centralized activity server and the training data are retrieved from users X  access history. The problem of prioritizing activities is defined as follows: For all m activities represented by an n-dimensional feature vector: Each feature k x indicates a specified attribute value at time We are trying to find a ranking function R whose input is the vector i a The larger the score is, the higher the priority of the activity. The problem of prioritizing activities is to find an appropriate ranking function R . 
Figure 3. The activity view of Malibu integrated with the In our setting, the training samples used to learn the ranking function are in the form of pairs of activities representing a partial order of priorities. For example, the input pair represents that the priority of activity i a priority of activity j a training sample can be inferred from users X  history and the Ranking SVM model works with pairs. The training pairs are generated as follows: We assume a daily time frame. At time if a user updates activity i a higher priority than all other activities not updated on the day of generate training samp les. We make the assumption that a user updates activities following their cognitive (i.e., non-modeled) priorities. Although this assumption is not  X  X erfect X , it is a good enough approximation and avoids imposing the extra cost on the users to supervise or correct the learning algorithm. Finding suitable features is essential to build an effective ranking system. Our activity prioritization system utilizes the ten features in Table 1 to create a feature vector representing an activity. features in Table 1 are intrinsic features, i.e. they are based on data from the activity only. Extrinsic features such as users X  recent desktop activities, emails or IM messages, are valuable too and we are looking into incorporating those in future work. Lotus Activities provides rich information to understand the context of each activity. Every update to an activity is logged including the type of action that produced the update, the user, the updated entries and tags, and the timestamp of the update. The history record of an activity shows all updates and provides a detailed record of how a user accessed the activity. The threaded (i.e., hierarchical) structure of activities shows the interactions between the user (responses) and other members of the activities. The user-assigned tags categorize ite ms and give clues about what the user has been working on. Ti me and date are very important since priorities of activities change over time. An activity X  X  priority can increase significantly when there is a new update. The feature construction criterion is to find scores that intuitively reflect the importance of an activity to the user at the current time. We based our model on the following features:  X  a count how active the activity is, based on items (2) and (3)  X  a measure of how actively the us er is involved in the activity,  X  a measure of collaboration a nd social measures based on  X  a simple indicator variable of the recency of a user X  X  joining  X  a measure of tag recency, based on item (10) in Table 1 These features are based in part in our experience with machine-learning models of user prefer ences for notifications in an activity-centric collaboration system [32]. To incorporate time as a factor, mo st features are either computed over time or weighted with an e xponential decay factor (see, e.g., item 5 in Table 1). Also, in order to avoiding numeric scale problems, we normalize each attribute across all activities. All features are constructed from the data on the centralized Lotus Activities server. Each feature can be regarded as a weak measure to sort priorities of activities. We performed a feature analysis on the data collected from 13 real users for 6 months. Figure 5 shows the distribution of the rank of the activity being updated by individual features in our experimental dataset. While user recency and activity recency perform well when used as individual features, more than 30% of the time these two features fail in predicting that an activity is among the top five most important activities. We employ Ranking SVM as our model to perform the task of learning the priorities of activities. The SVM approach has proved to be an effective learning met hod in classification and regression [34]. Ranking SVM [21] is an extension of standard SVM to support ranking. It can be directly applied to resolve the formalization of the activities prioritization problem that we described in Section 5.1. In Ranking SVM, each item is represented by an n -dimension vector be thought of as a data point in the n -dimension space. The training samples are pairs of items in the form of the rank of i a training samples, Ranking SVM finds a hyper-plane = where the distance for each data point to the hyper-plane has minimal discordance with the given training pairs. That means, for the training pair ) , ( j i a a the distance from the given sample to the learned hyper-plane. Similar to a classification SVM, to find the hyper-resolves the following optimization problem: Minimize : Subject to : For the k given training pairs ) , ( jk ik a a  X  is the slack variable and C is the trade-off parameter. Ranking SVM learns a linear ra nking function per default. However, it has been shown that different kernels can be applied to a Ranking SVM [21]. This allow us to create a nonlinear ranking function 1 User recency Number of minutes since the last update of the current user 2 Activity recency Number of minutes since the last update of any user. 3 Activity update frequency 4 User update frequency 5 User X  X  creative score 1 are the 6 Shared items within activity: 7 Shared activities 8 Become member recently 9 Co-Accessed activities accessed on the 10 Tag recency In our implementation, the Ra nking SVM is implemented based on the Java package of LibSVM [8]. We extended the package to support ranking in addition to cla ssification and regression. Since there is no efficient online update algorithm for SVMs, we retrain the Ranking SVM every 5 minutes. The training process runs in the background and is fast enough due to the small number of features. Our test on 13 subjects showed that the average training time on 6 month data took around 3 seconds and the average ranking time took less than 100 ms on an Intel Duo Core 1.6G HZ laptop with 1 GB memory. To evaluate the effectiveness of our activity prioritizing system, we ran an offline experiment with the data of the 13 internal users of Lotus Activities (also used to analyze our feature set in Section 5.1) We compared our Ranking SVM algorithm with the two baseline non-machine learning methods: Activity Recency and User Recency . In our experiments we measure prediction accuracy but also the cost for a user to scroll down in case an important activity is not visible in the activities view. The data were collected from a database of the Lotus Activities server deployed on the IBM Intranet. It contained data from 1000 registered Lotus Activities users. This number includes many users who only have very sm aller number of activities and updates on activities rarely. We selected 13 users with the largest number of activities. To avoid bi ased result, we also excluded users who worked on the Lotus Activities product team. The typical users were researchers, ma nagers and software developers. The activities created by the subjects were related to research activities, meetings, presentations and software projects etc. The average number of activities per user was 62.69 (median 62). The maximum number of activities per user was 117 and the minimum number was 20. Recall that our goal is to assist users to manage the large number of concurrent new updates of ac tivities that may overwhelm them (e.g. [28], [32]). Hence, an ideal activity prioritization system most likely to update, and (b) display that activity on the top of the Lotus Activities view in Malibu. This means that there is no extra cost of scrolling to find the right activity. On this basis, we performed an offline e xperiment as follows:  X  First we preprocessed the log data as follows: The actions  X  Then we sorted all the update actions by their timestamps.  X  Each time a user switched to a different activity, we treated  X  The entire set of data points was split into a training set and a  X  For each testing data point, the ranking SVM algorithm We used activity recency and user recency as a baseline in our experiment: Activity recency refers to sorting activities by their implementation in Lotus Activities and Malibu. User recency refers to sorting activities by the last update time of the current user (item 1 in Table 1). To evaluate the performance of our implementation, we measured the accuracy when true activity is among the top N (N=1, 3, 5) activities of the lists generated by our Ranking SVM algorithm and the two baseline methods 2 . Table 2 shows the prediction accuracy of the top 1, 3, 5 activities of the list and the mean and median rank of true activity in all test cases. In the experiment, Ranking SVM was configured with C=100 and a RBF kernel with  X  =0.25 according to [9] based on a 10-fold cross-validation test on the training set. The accuracy is shown with a 95% confidence intervals computed by two tailed signed binomial test. The mean and median of the rank of true activity are also provided. The results show that the Ranking SVM method enhances the two recency baseline methods. The difference is statistically significantly in all top 1, 3 and 5 cases at a 95% confidence level. The algorithm has 78.81% accuracy of predicting an activity to be within the top 5 most important activities and it also has the lowest mean and median values. For completeness, we note that the User recency presents a special case. Since the data points in our test are the moments before a user switches to an ac tivity, the last activity the user accessed will be always the first on the list according to user recency and it is never the true activity in our experiment. To fix this, we removed the first activity from the list of User recency in our evaluation process. In the user interface, the size of the activity view determines the number of activities visible without scrolling. If the true activity does not appear very high up on this list, or even worse fails to be visible, then an extra cost woul d be imposed on the user to find the activity. We assume that the larger the ranked position of an activity is, the higher the cost for the user. Figure 6 shows the percentage of failure cases in which the true activity failed to appear in the activity view as a function of size of the activity view. The error bars shows a 95% confidence interval computed using a binomial test. The number of failure cases (important activities not showing up in the activities view) is significantly lower compared to the two recency baseline methods. In Figure 6, user recency has fewer failure cases than activity recency when the number of displayed activities is small, but more failure cases when the number increases. Analysis show that users often go to an activity that they have not touched for a long time and also, there are many updates in activities that might not be important or relevant to a user in particular if activities have many members . This is an i ndication that the ranking based on user recency might have a higher variance whereas activity recency might have lower accuracy. Neither recency method performs well. The Ranking SVM algorithm described in this paper improves on the simple recency method by incorporating other factors that influence the priority of an activity. In this paper, we proposed an a pproach to prioritize activities via machine-learning methods to resolve the activity overflow problem ([28], [32], [35]) in an activity-centric collaboration environment. We proposed and an alyzed features used for our ranking algorithm, and we conducted an offline experiment with the data of a small number of high-volume Lotus Activities users. The results of this experiment ar e promising and indicate that the Ranking SVM model with the se lected features performs significantly better than the two baseline recency methods without adding any extra user cost. We also implemented and integrated our activity prioritization system into the user interface of Malibu, an experimental personal productivity tool. There are several future directions to improve the current activity prioritization system. Our current feature set is not complete and additioal features can be added. For example, using extrinsic data in the feature set of the learning algorithm might further improve prediction accuracy. In addition to the Lotus Activities metadata, potential clues about a user X  X  curre nt priorities could be found in emails, chat messages, or phone calls. Techniques including predicting the activities incoming emails belong to [14] and  X  X peech acts X  [6] could be applied to infer priorities of activities. For example, assuming that a user has an activity named  X  X ubmitting report X , if s/he receives an email and an email classification system identifies the activity  X  X ubmitting report X  as the associated activity, we could argue that this activity has a higher priority because of the potential need of the user to go to this activity now as a consequence of the email. Vice versa, if a user never opens that email, the activity might also have a very low priority. Another interesting direction is to include dates such as deadlines and milestones into the prioritization. Lotus Activities allows users to pick arbitrary names for their activities. We could automatically identify dates in names and incorporate them into the model. For example, an ac tivity named  X  X alibu demo and talk on August 16 X  could be ex tremely important and updated frequently on the days before Aug 16 but not after. This kind of knowledge could be used as a str ong, discriminating feature in our priority prediction model. We thank the Activities product team for their continuous support, Jianqiang Shen for insp irational research discussions, and all anonymous users of Activities who supported this study with their data. [1] Agichtein, E., Brill, E., and Dumais, S. Improving web [2] Bellotti, V., Ducheneaut, N., Howard, M., and Smith, I. [3] Boardman, R., &amp; Sasse, M.A.  X  X tuff goes into the computer, [4] Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., [5] Cao, Y., Xu, J., Liu, T., Li, H., Huang, Y., and Hon, H. [6] Carvalho, V. R. and Cohen, W. W. 2005. On the collective [7] Czerwinski, M., Horvitz, E., &amp; Wilhite, S.,  X  X  diary study of [8] Chang, C. and Lin, C. LIBSVM : a library for support vector [9] Hsu, C., Chang C and Lin, C.  X  X  practical guide to support [10] Cho, J. and Garcia-Molina, H. 2000. The Evolution of the [11] Cohen, W. W. Schapire, R. E. Singer, Y Learning to Order [12] Crammer, K.and Singer, Y. Pranking with ranking. In [13] Dragunov, A. N., Dietterich , T. G., Johnsrude, K., [14] Dredze, M., Lau, T., and Kushmerick, N. 2006. [15] Geyer, W., Muller, M., Moore, M., Wilcox, E., Cheng, L., [16] Geyer, W., Brownholtz, B., Muller, M., Dugan, C., Wilcox, [17] Google Desktop. http://desktop.google.com/. [18] Gonz X lez, V., and Mark, G. Managing currents of work: [19] Herbrich, R. Graepel, T. and Obermayer, K. Large margin [20] Hudson, J. M., Christensen, J., Kellogg, W. A., and [21] Joachims, T. Optimizing sear ch engines using clickthrough [22] Lotus Connections: Social Software for Business. [23] McCrickard D. S. &amp; Chewar C. M. (2003) Attuning [24] Millen, D. R., Feinberg, J., and Kerr, B. 2006. Dogear: [25] Mitchell, T. M., Wang, S.H., Huang, Y. and Cheyer. A. [26] Moore, M., Estrada, M., Finley, T., Muller, M. and Geyer, [27] Moran, T.P., Cozzi, A., &amp; Farrell, S.P. (2005),  X  X nified [28] Muller, M.J. (2004). Activity graphs of the microstructure of [29] Muller, M.J., Geyer, W., Brownholtz, B., Wilcox, E., and [30] Muller, M.J., Geyer, W., Brow nholtz, B., Dugan, C., Millen, [31] Quan, D., and Karger, D. R., (2003). 'Haystack: Metadata [32] Sen, S., Geyer, W., Muller, M.J., Moore, M., Brownholtz, [33] Ringel, M. and Hirschberg , J. 2002. Automated message [34] Vapnik, V. 1998. Statistical Learning Theory. Wiley-[35] Whittaker, S., Sidner, C.,  X  X mail Overload: Exploring [36] Xu, J., Cao, Y., Li, H., and Zhao, M. 2005. Ranking 
