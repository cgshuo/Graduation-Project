 Di  X  erentially private collaborative filtering is a challenging task, both in terms of accuracy and speed. We present a simple algorithm that is provably di  X  erentially private, while o  X  ering good performance, using a novel connection of di  X  er-ential privacy to Bayesian posterior sampling via Stochastic Gradient Langevin Dynamics. Due to its simplicity the al-gorithm lends itself to e ffi cient implementation. By careful systems design and by exploiting the power law behavior of the data to maximize CPU cache bandwidth we are able to generate 1024 dimensional models at a rate of 8.5 million recommendations per second on a single PC.
 Di  X  erential Privacy; Collaborative Filtering; Scalable Ma-trix Factorization
Privacy protection in recommender systems is a notori-ously challenging problem. There are often two compet-ing goals at stake: similar users are likely to prefer similar products, movies, or locations, hence sharing of preferences between users is desirable. Yet, at the same time, this exac-erbates the type of privacy sensitive queries, simply since we are now not looking for aggregate properties from a dataset (such as a classifier) but for properties and behavior of other users  X  X ust like X  this specific user. Such highly individualized behavioral patterns are shown to facilitate provably e  X  ective user de-nonymization [25, 37].

Consider the case of a couple, both using the same location recommendation service. Since both spouses share much of the same location history, it is likely that they will receive similar recommendations, based on other users X  preferences similar to theirs. In this context sharing of information is desirable, as it improves overall recommendation quality.
Moreover, since their location history is likely to be very similar, each of them will also receive recommendations to visit the place that their spouse visited (e.g. including places of ill repute), regardless of whether the latter would like to share this information or not. This creates considerable tension in trying to satisfy those two conflicting goals. c  X  2015 ACM. ISBN 978-1-4503-3692-5/15/09 ...$15.00.
Di  X  erential privacy o  X  ers tools to overcome these prob-lems. Loosely speaking, it o  X  ers the participants plausible deniability in terms of the estimate. That is, it provides guarantees that the recommendation would also have been issued with su ffi ciently high probability if another specific participant had not taken this action before. This is pre-cisely the type of guarantee suitable to allay the concerns in the above situation [ 8].

Recent work, e.g. by Mcsherry and Mironov [ 20] has fo-cused on designing custom built tools for di  X  erential private recommendation. Many of the design decisions in this con-text are hand engineered, and it is nontrivial to separate the choices made to obtain a di  X  erentially private system from those made to obtain a system that works well. Fur-thermore, none of these systems [ 20, 36] lead to very fast implementations.

In this paper we show that a large family of recommender systems, namely those using matrix factorization, are well suited to di  X  erential privacy. More specifically, we exploit the fact that sampling from the posterior distribution of a Bayesian model, e.g. via Stochastic Gradient Langevin Dy-namics (SGLD) [35 ], can lead to estimates that are su ffi -ciently di  X  erentially private [ 34]. At the same time, their stochastic nature makes them well amenable to e ffi cient im-plementation. Their generality means that we need not custom-design a statistical model for di  X  erential privacy but rather that is possible to retrofit an existing model to satisfy these constraints. The practical importance of this fact can-not be overstated  X  it means that no costly re-engineering of deployed statistical models is needed. Instead, one can simply reuse the existing inference algorithm with a trivial modification to obtain a di  X  erentially private model.
This leaves the issue to performance. Some of the best reported results are those using GraphChi [15], which show that state-of-the-art recommender systems can be built us-ing just a single PC within a matter of hours, rather than requiring hundreds of computers. In this paper, we show that by e ffi ciently exploiting the power law properties inher-ent in the data (e.g. most movies are hardly ever reviewed on Netflix), one can obtain models that achieve peak numerical performance for recommendation. More to the point, they are 3 times faster than GraphChi on identical hardware.
In summary, this paper describes the by far the fastest matrix factorization based recommender system and it can be made di  X  erentially privately using SGLD without losing performance. Most competing approaches excel at no more than one of those aspects. Specifically, 1. It is e ffi cient at the state of the art relative to other matrix factorization systems.  X  we develop a cache e ffi cient matrix factorization frame- X  we develop a fast SGLD sampling algorithm with book-2. And it is di  X  erentially private.  X  We provably show that sampling from a scaled posterior  X  We present a personalized di  X  erentially private method  X  We only privately release V to public, and design a local Experiments confirm that the algorithm can be implemented with high e ffi ciency, while o  X  ering very favorable privacy-accuracy tradeo  X  that nearly matches systems without dif-ferential privacy at meaningful privacy level.
We begin with an overview of the relevant ingredients, namely collaborative filtering using matrix factorization, dif-ferential privacy and a primer in computer architecture. All three are relevant to the understanding of our approach. In particular, some basic understanding of the cache hierarchy in microprocessors is useful for e ffi cient implementations. In collaborative filtering we assume that we have a set of U users, rating V items. We only observe a small number of entries r ij in the rating matrix R . Here r ij means that user i rated item j . A popular tool [14] to deal with inferring entries in R 2 R |U|  X  |V| is to approximate R by a low rank factorization, i.e.
 for some k 2 N , which denotes the dimensionality of the feature space corresponding to each item and movie. In other words, (user,item) interactions are modeled via Here u i and v j denote row-vectors of U and V respectively, and b u i and b m j are scalar o  X  sets responsible for a specific user or movie respectively. Finally, b 0 is a common bias.
A popular interpretation is that for a given item j , the el-ements of v j measure the extent to which the item possesses those attributes. For a given user i the elements of u i mea-sure the extent of interest that the user has in items that score highly in the corresponding factors. Due to the condi-tions proposed in the Netflix contest, it is common to aim to minimize the mean squared error of deviations between true ratings and estimates. To address overfitting, a norm penalty is commonly imposed on U and V . This yields the following optimization problem min A large number of extensions have been proposed for this model. For instance, incorporating co-rating information [28], neighborhoods, or temporal dynamics [13] can lead to improved performance. Since we are primarily interested in demonstrating the e ffi cacy of di  X  erential privacy and the interaction with e ffi cient systems design, we focus on the simple inner-product model with bias.
 Bayesian View. Note that the above optimization problem can be viewed as an instance of a Maximum-a-Posteriori estimation problem. That is, one minimizes where, up to a constant o  X  set and  X  log p ( U )= U  X  u U &gt; and likewise for V . In other words, we assume that the ratings are conditionally normal, given the inner product h u i ,v j i , and the factors u i drawn from a normal distribution. Moreover, one can also introduce priors for  X  r ,  X  u ,  X  v with a Gamma distribution G (  X |  X  ,  X  ).

While this setting is typically just treated as an afterthought of penalized risk minimization, we will explicitly use this when designing di  X  erentially private algorithms. The ratio-nale for this is the deep connection between samples from the posterior and di  X  erentially private estimates. We will return to this aspect after introducing Stochastic Gradient Langevin Dynamics.
 Stochastic Gradient Descent. Minimizing the regular-ized collaborative filtering objective is typically achieved by one of two strategies: Alternating Least Squares (ALS) and stochastic gradient descent (SGD). The advantage of the former is that the problem is biconvex in U and V respec-tively, hence minimizing U | V or V | U are convex. On the other hand, SGD is typically faster to converge and it also a  X  ords much better cache locality properties. Instead of ac-cessing e.g. all reviews for a given user (or all reviews for a given movie) at once, we only need to read the appropriate tuples. In SGD each time we update a randomly chosen rating record by: u i (1  X   X  t  X  ) u i +  X  t v j v j (1  X   X  t  X  ) v j +  X  t u i One problem of SGD is that trivially parallelizing the proce-dure requires memory locking and synchronization for each rating, which could significantly hamper the performance. [27] shows that a lock-free scheme can achieve nearly opti-mal solution when the data access is sparse. We build on this statistical property to obtain a fast system which is suitable for di  X  erential privacy.
Di  X  erential privacy (DP) [7, 9] aims to provide means to cryptographically protect personal information in the database, while allowing aggregate-level information to be accurately extracted. In our context this means that we protect user-specific sensitive information while using aggregate informa-tion to benefit all users.

Assume the actions of a statistical database are modeled via a randomized algorithm A . Let the space of data be X and data sets X, Y 2 X n . Define d ( X, Y ) to be the edit distance or Hamming distance between data set X and Y , for instance if X and Y are the same except one data point then we have d ( X, Y ) = 1.
 Definition 1 (Di  X  erential Privacy) . We call a randomized algorithm A (  X  ,  X  ) -di  X  erentially private if for all measurable sets S  X  Range ( A ) and for all X, X 0 2 X n such that the hamming distance d ( X, X 0 )=1 , If  X  =0 we say that A is  X  -di  X  erential private.
The definition states that if we arbitrarily replace any indi-vidual data point in a database, the output of the algorithm doesn X  X  change much. The parameter  X  in the definition controls the maximum amount of information gain about an individual person in the database given the output of the algorithm. When  X  is small, it prevents any forms of linkage attack to individual data record (e.g., linkage of Netflix data to IMDB data [25]). We refer readers to [8] for detailed in-terpretations of the di  X  erential privacy in statistical testing, Bayesian inference and information theory.

An interesting side-e  X  ect of this definition in the context of collaborative filtering is that it also limits the influence of so-called whales, i.e. of users who submit extremely large numbers of reviews. Their influence is also curtailed, at least under the assumption of an equal level of di  X  erential privacy per user. In other words, di  X  erential privacy confers robustness for collaborative filtering.

Wang et al. [34] show that posterior sampling with bounded log-likelihood is essentially exponential mechanism [ 21] there-fore protecting di  X  erential privacy for free (similar observa-tions were made independently in [23, 5]). Wang et al. [34] also suggests a recent line of works [35, 4, 6] that use stochas-tic gradient descent for Hybrid Monte Carlo sampling essen-tially preserve di  X  erential privacy with the same algorithmic procedure. The consequence for our application is very inter-esting: if we trust that the MCMC sampler has converged, i.e. if we get a sample that is approximately drawn from the posterior distribution, then we can use one sample as the private release. If not, we can calibrate the MCMC proce-dure itself to provide di  X  erential privacy (typically at the cost of getting a much poorer solution).
Akeydi  X  erence between generic numerical linear algebra, as commonly used e.g. for deep networks or generalized lin-ear models, and the methods used for recommender systems is the fact that the access properties regarding users and items are highly nonuniform. This is a significant advan-tage, since it allows us to exploit the caching hierarchy of modern CPUs to benefit from higher bandwidth than what disks or main memory access would permit.

A typical computer architecture consists of a hard disk, solid-state drive (SSD), random-access memory (RAM) and CPU cache. A good algorithm design should be pushing the data flow to CPU cache level and hide the latency from SSD or even RAM and amplify the available bandwidth.

The key strategy in obtaining high throughput collabora-tive filtering systems is to obtain peak bandwidth on each of the subsystems by e ffi cient caching. That is, if a movie is fre-quently reused, it is desirable to retain it in the CPU cache. This way, we will neither su  X  er the high latency (100ns per request) of a random read from memory, nor will we have to pay for the comparably slower bandwidth of RAM relative to the CPU cache.
We start by describing the key ideas and algorithmic frame-work for di  X  erentially private matrix factorization. The method, which involves preprocessing data and then sam-pling from a scaled posterior distribution, is provably dif-ferentially private and has profound statistical implications. Then we will describe a specific Monte Carlo sampling al-gorithm: Stochastic Gradient Langevin Dynamics (SGLD) and justify its use in our setting. We then come up with a novel way to personalize the privacy protection for indi-vidual users. Finally, we discuss how to develop fast cache-e ffi cient solvers to exploit bandwidth-limited hardware such that it can be used for general SGD-style algorithms.
Our di  X  erential privacy mechanism relies on a recent ob-servation that posterior sampling preserves di  X  erential pri-vacy, provided that the log-likelihood of each user is uni-formly bounded [34]. This simple yet remarkable result sug-gests that sampling from posterior distribution is di  X  eren-tially private for free to some extent. In our context, the claim is that, if 1 max U,V,R,i the method that outputs a sample from P ( U, V ) / exp preserves 4 B -di  X  erential privacy. Moreover, when we want to set the privacy loss  X  to another number, we can easily do this by simply rescaling the entire expression by  X  / 4 B . The question now is whether max is bounded. Since the ratings are bounded between 1  X  r ij  X  5 and we can consider a reasonable sublevel set { U, V | max i,j | u T i v j |  X   X  } , we have every summand to be bounded by (5 +  X  ) 2 . This does not a  X  ect the privacy claim as long as  X  is chosen independent to the data.

B could still be large, if some particular users rated many movies. This issue is inevitable even if all observed users have few ratings, since di  X  erential privacy also protects users not in the database. We propose two theoretically-inspired algorithmic solutions to this problem: Trimming: We may randomly delete ratings for those who Reweighting: Alternatively, one can weight each user ap-In addition, these procedures have their practical benefits for the robustness of the recommendation system, since they prevents any malicious user from injecting too much impact into the system, see e.g., Wang and Xu [33], Mobasher et al. [24]. Another justification of these two procedures is that, if the fully observed matrix is truly in a low-dimensional sub-space, neither of these two procedures changes the underly-ing subspace. Therefore, the solutions should be similar to the non-preprocessed version.

The procedure for di  X  erentially private matrix factoriza-tion (DPMF) is summarized in Algorithm 1. Note that this is a conceptual sketch (we will discuss an e ffi cient variant thereof later). The following theorem guarantees that our procedure is indeed di  X  erentially private.
 Theorem 1. Algorithm 1 obeys  X  -di  X  erential privacy if the sample is exact and (  X  , (1 + e  X  )  X  ) -di  X  erential privacy if the sample is from a distribution  X  -away from the target distri-bution in L 1 distance.

The proof in [17] shows that this procedure uses in fact the exponential mechanism [21] with utility function being the negative MF objective and its sensitivity being 2 B . Note that this can be extended to considerably more complex models. This is the strength of our approach, namely that a large variety of algorithms can be adapted quite easily to di  X  erential privacy capable models.
For convenience of notation we will omit the biases from the description below in favor of a slightly more succinct notation. Algorithm 1 Di  X  erentially Private Matrix Factorization Require: Partially observed rating matrix R 2 R m  X  n with 1: B max i =1 ,...,n min {  X  ,m i } w i (5  X  1+  X  ) 2 . . Compute 2: Trim all users with ratings &gt;  X  . 3: F ( U, V ) := 4: Sample ( U, V )  X  P ( U, V ) / e  X   X  4 B F ( U,V ) 5: while u T i v j / 2 [1  X   X  , 5+  X  ] for some i, j do 6: Sample ( U, V )  X  P ( U, V ) / e  X   X  4 B F ( U,V ) 7: return ( U, V )
Another interesting feature of the proposed procedure is that it allows us to calibrate the level of privacy protection for every user independently, via a novel observation that weights assigned to di  X  erent users are linear in the amount of privacy we can guarantee for that particular user.
We will use the same sampling algorithm, and our guar-antees in Theorem 1 still hold. The idea here is that we can customize the system so that we get a lower basic privacy protection for all users, say  X  =4 B . As we explained ear-lier this is the level of privacy that we can get more or less  X  X or free X . The protection of DP is su ffi ciently strong as to include even those users that are not in the database.
By adjusting the weight parameter, we can make the pri-vacy protection stronger for particular users according to how much they set they want privacy. This procedure makes intuitive sense because if some user wants perfect privacy, we can set their weight to 0 and they are e  X  ectively not in the database anymore. For people who do not care about pri-vacy, their ratings will be assigned default weight. Formally, we define personalized di  X  erential privacy as follows: Definition 2 (Personalized Di  X  erential Privacy) . An algo-rithm A is (  X  ,  X  ) -personalized di  X  erentially private for User i in database X if for any measureable set S in the range of the algorithm A for any X 2 X n and X 0 is either X [ { x i } or X \{ x i
Note that instead of replacing a user, we are now only adding or removing one specific user, such that the notion of personalized privacy is well-defined. We claim that Theorem 2. If we set w i for User-i such that then Algorithm 1 guarantees  X  B i 2 B -personalized di  X  erential pri-vacy for User i .

We show the proof in [17]. Note that if we set  X  =4 B (so we are essentially sampling from the posterior distribution), weget2 B i -Personalized DP for user i .

In summary, if we simply set  X  =4 B , the method protects 4 B -di  X  erential privacy for everybody at very little cost and by setting the weight vector w , we can provide personalized service for users who demands more stringent DP protec-tion. Also, personalized privacy o  X  ers a new perspective in interpreting DP beyond the worst case scenario. For in-stance, users who are more predictable by the model will be likely to have stronger privacy protection. If a model fits the data well, even a large worst-case privacy loss  X  could in fact provide meaningful protection to the large majority of users. Lastly, we recently become aware that personalized privacy (the exact same definition) has been independently developed in [10] along with a few basic properties that mir-rors the standard DP. The di  X  erence is that we focus on the weighted posterior sampling aspect of it.
Clearly, sampling from exp For a tractable approach we use a recent MCMC method named stochastic gradient Langevin dynamics (SGLD) [ 35], which is an annealing of stochastic gradient descent and Langevin dynamics that samples from the posterior distri-bution [26]. The basic update rule is ing only one or a small number of ratings. In other words, the updates are almost identical to those used in stochastic gradient descent. The key di  X  erence is that a small amount of Gaussian noise is added to the updates. This allows us to solve it extremely e ffi ciently. We will describe our e ffi cient implementation of this algorithm in Section 5.4 .
The basic idea of SGLD is that when we are far away from the basin of convergence, the gradient of the log-posterior b r ( u i ,v j ) F ( U, V ) is much larger than the additional noise so the algorithm behaves like stochastic gradient descent. As we approach the basin of convergence and  X  t becomes small, p  X  t  X   X  t so the noise dominates and it behaves like a Brow-nian motion. Moreover, as  X  t gets small, the probability of accepting the proposal in Metropolis-Hastings adjustment converges to 1, so we do not need to do this adjustment at all as the algorithm proceeds, as designed above.
This seemingly heuristic procedure was later shown to be consistent in [29, 31], where asymptotic X  X n-law X  X nd X  X lmost sure X  convergence of SGLD to the correct stationary distri-bution are established. More recently, Teh et al. [ 32] further strengthens the convergence guarantee to include any finite iterations. This line of work justifies our approach in that if we run SGLD for a large number of iterations, we will end up sampling from the distribution that provides us (  X  ,  X  )-di  X  erential privacy. By taking more iterations, we can make  X  arbitrarily small.
The performance improvement over existing libraries such as GraphChi are due to both cache e ffi cient design, prefetch-ing, pipelining, the fact that we exploit the power law prop-erty of the data, and by judicious optimization of random number generation. This leads to a system that comfortably surpasses even moderately optimized GPU codes.

We primarily focus on the Stochastic Gradient Descent solver and subsequently we provide some details on how to extend this to SGLD. Inference requires a very large number of following operations on data: To illustrate the impact of these operations consider train-inga2 , 048 dimensional model on the 10 8 rating triples of Netflix. Per iteration this requires over 3.2TB read/write op-erations to RAM. At a main memory bandwidth of 20GB/s and a latency of 100ns for each of the 200 million cache misses each pass would take over 6 minutes. Instead, our code accomplishes this task in approximately 10 seconds by using the steps outlined below.
To deal with the dataflow from disk to CPU, we use a pipelined design, decomposing global and local state akin to [1]. This means that we process users sequentially, thus reducing the retrieval cost per user, since the operations are amortized over all of their ratings. This e  X  ectively halves IO. Moreover, since the data cannot be assumed to fit into RAM, we pipeline reads from disk. This hides latency and avoids stalling the CPUs. The writer thread periodically snapshots the model, i.e. U and V to disk.
 Algorithm 2 Cache e ffi cient Stochastic Gradient Descent Require: parameters U , V ; ratings R ; P threads, 1: preprocessing Split R into B blocks; 2: procedure Read . Keep pipeline filled 3: while #blocks in flight  X  P do 4: Read: block b from disk 5: Sync: notify Update about b 6: procedure Update . Update U , V 7: while at least one of P processors is available do 8: Sync: receive a new block b from Read 9: for user i in b do 10: for each rating r ij 2 b from user i do 11: Prefetch next movie factor v j +1 from data 12: u i u i  X   X  t b r u i 13: v j v j  X   X  t b r v j 14: ( b r is either the exact or private gradient) 15: procedure Write 16: if B t blocks processed then save U, V
The previous reasoning discussed how to keep the data pipeline filled and how to reduce the user-specific cache misses by preaggregating them on disk. Next we need to address cache e ffi ciency with regard to movies. More to the point, we need to exploit cache locality relative to the CPU core rather than simply avoiding cache misses. The basic idea is that each CPU core exactly reads a cache line (commonly 64 bytes) from RAM each time, so algorithm designers should not waste it until that piece of cache line is fully utilized.
We exploit the fact that movie ratings follow a power law [11], as is evident e.g. on Netflix in Figure 1. This means that if we succeed at keeping frequently rated movies in the CPU cache, we should see substantial speedups. Note that traditional matrix blocking tricks, as widely used for matrix multiplications operations are not useful, due to the sparsity of the rating matrix R . Instead, we decompose the movies into tiers of popularity. To illustrate, considering a decom-position into three blocks consisting of the Top 500, the Next 4000, and the remaining long tail.

Within each block, we process a batch of users simultane-ously. This way we can preserve the associated user vectors u in cache and we are likely to cache the movie vectors, too (in particular for the Top 500 block). Also, parallelizing all the updates for multiple users does not require locks. Movie parameters are updated in a Hogwild fashion [27 ]. Figure 1: Distribution of items (Movies/Music pieces) as a function of their number of ratings. Many movies have 100 ratings or less, while the ma-jority of ratings focuses on a small number of movies.
This design is particularly e ffi cient for low-dimensional models since the Top 500 block fits into L1 cache (this amounts to 44% of all movie ratings in the Netflix dataset), the Next 4000 fits into L2, and ratings will typically reside in L3. Even in the extreme case of 2048 dimensions we can fit about 55% of all ratings into cache, albeit L3 cache.
To avoid the penalty for random requests we perform la-tency hiding by prefetching. That is, we actively request v j in advance before the rating r ij is to be updated. For dimensions less than 256, accurate prefetching leads to a dataflow of v j into L1 cache. Beyond that, the size of the latent variables could be too big to benefit from the lowest level of caching due to limited size of caches in modern com-puters. We provide a detailed caching analysis in Section 6 to illustrate the e  X  ect of these techniques. The data flow of SGLD is almost analogous to that in SGD, albeit with a number of complications. First o  X  , note that (4) applies to the whole parameter matrix U, V rather than just to a single vector. Following [3 ] we can derive an unbiased approximation of b r u i in ( 4) which is nonzero only for ( u i ,v j ) as follows: where N, N i denote number of rating data rated by all and rated by user i respectively. The parameters  X  r ,  X  u ,  X  not incur any major cost  X   X  u ,  X  v are diagonal matrices with a Gamma distribution over them. We simply per-form Gibbs sampling once per round. However, the most time-consuming part is to sample the remaining vectors, i.e. P( U  X  i ,V  X  i | R, rest) since it both requires dense updates and moreover, it requires many random numbers, which adds nontrivial cost.
 Dense Updates: Note that unless we encounter the triple Table Lookup: Drawing iid samples from a Gaussian is
We now investigate the e ffi ciency and accuracy of our fast SGD solver and Stochastic Gradient Langevin Dynam-ics solver, compared with state-of-the-art available recom-menders. We explore the di  X  erentially private accuracy by using our proposed method while varying di  X  erent privacy budgets.
We compare the performance of both the SGD solver and the SGLD solver to other publicly available recommenders and one closed-source solver. In particular, we compare to both CPU and GPU solvers, since the latter tend to excel in massively parallel floating point operations.
 GraphChi Most of our experiments focus on a direct com-GraphLab Create is a closed source data analysis plat-BidMach is a GPU based system [38]. It reports runtimes Spark is a distributed system (Spark MLlib) for inferring
We use two datasets  X  the well known Netflix Prize dataset, consisting of a training set of 99M ratings spanning 480k customers and their ratings on almost 18k, each movie be-ing rated at a scale of 1 to 5 stars. Additionally, we use their released validation set which consists of 1 . 4M ratings for validation purposes.

Secondly, we use the Yahoo music recommender dataset, consisting of almost 263M ratings of 635k music items by 1M users. We also use the released validation set which consists of 6M ratings for validation. We re-scale each rating at a scale of 0 to 5. http://github.com/BIDData/BIDMach/wiki/Benchmarks http://stanford.edu/~rezab/sparkworkshop/slides/ xiangrui.pdf , Slide 31 K SC-SGD GraphChi 16 2.84% 0.43% 12.77% 2.21% 256 2.85% 0.50% 12.89% 2.34% 2048 3.3% 1.7% 15% 9.8% Table 1: Cache miss rates in C-SGD and GraphChi.
We run all the experiments on an Amazon c3.8xlarge in-stance running Ubuntu 14.04 with 32 CPUs and 60GB RAM.
For SGD-based methods we grid search the best parame-ters [17] for initial learning rate  X  0 , decay rate  X  and regular-izer  X  [17]. For our fast SGLD solver, in addition to previous paramters, practically we multiply learning rate by a tem-perature parameter  X  [4] in the Gaussian noise N (0 ,  X   X   X  with
Since it is nontrivial to observe the test RMSE error in each epoch when using Graphlab Create, we only report the timing of Graphlab Create and all other methods in Figure 3. Note that we were unable to obtain performance results from BidMach for the Yahoo dataset, since Scala encountered memory management issues. However, we have no reason to believe that the results would be in any way more favorable to BidMach than the findings on the Netflix dataset. For reproducibility the results were carried out on an AWS g2.8xlarge instance.

To illustrate the convergence over time. We run all the methods in a fixed number of epochs. That is 15 epochs and 30 epochs respectively because we observe that our SGD solver can reach the convergence at that time. Figure 2 shows our timing results along with convergence while we vary dimensions of the models.

Both of our solvers, i.e. C-SGD and Fast SGLD benefit from our caching algorithm. C-SGD is around 2 to 3 times faster than GraphChi and Graphlab while simultaneously outperforming the accuracy of GraphChi.

Note that the algorithm required for Fast SGLD is rather more complex, since it performs sampling from the Bayesian posterior. Consequently, it is slower than plain SGD. Nonethe-less, its speed is comparable to GraphChi in terms of through-put (despite the latter solving a much simpler problem). One problem of SGLD is that the more complex the models are, the worse its convergence becomes (even though generates good samples on small dimensions Figure 4), due to the fact that we are sampling from a large state space. This is pos-sibly due to the slow mixing of SGLD, which is a known problem of SGLD [2 ]. Improving the mixing rate by consid-ering a more advanced stochastic di  X  erential equation based sampler, e.g. [ 4, 6], while keeping the cache e ffi ciency during the updates will be important future works. To our best knowledge we are the first to report the convergence results of SGLD at this scale.
We show the cache e ffi ciency of C-SGD and Graphchi in this section. Our data access pattern can accelerate the hardward cache prefetching. In the meanwhile we also use software prefetching strategies (with prefetching stride set to 2) to prefetch movie factors in advance.

We set the experiments as follows. In each gradient up-date step given r ij , once the parameters e.g. u i and v (3) been read they will stay in cache for a while until they be flushed away by new parameters. What we really care about in this section is if the first time each parameter be read by CPU is already staying in cache or not. If it is not in cache then there will be a cache miss and will push CPU to idle. We use Cachegrind [16] as a cache profiler and analyze Figure 3: Timing comparisons on Netflix (left, 15 epochs) and Yahoo (right, 30 epochs). Figure 4: Convergence of SGLD for 16-dimensional models on Netflix. cache miss (see Table 1 which shows it plays crucial e  X  ects on the final performance) for this purpose.
We now investigate the influence of privacy loss on ac-curacy. As discussed previously, a small rescaling factor B can help us to get a nice bound on the loss function. For private collaborative filtering purposes, we first trim the training data by setting each user X  X  maximum allow-able number of ratings  X  = 100 and  X  = 200 for the Net-flix competition dataset and Yahoo Music data respectively. We set B =  X  (5  X  1+  X  ) 2 and weight of each user as w i = min(  X  , B m i (5  X  1+  X  ) 2 ) where  X  is set to 1. According to di  X  erent trimming strength we have B = 2500 and B = 5000 for Netflix data and Yahoo data respectively. As such we get a dataset with 33M ratings for Netflix and 100M ratings for Yahoo Music data. We study the prediction accuracy, i.e. the utility of our private method by varying the di  X  erential privacy budget  X  for fixed model dimensionality K = 16. The parameters of the experiment are set as in [17].
While we are sampling ( U, V ) jointly, we essentially only need to release V . Users can then apply their own data to Figure 5: Test RMSE vs. privacy loss  X  on Netflix (left) and Yahoo (right). A modest decrease in ac-curacy a  X  ords a useful gain in privacy. get the full model and have a local recommender system: The local predictions, i.e. in our context the utility of di  X  er-entially private matrix factorization method, along with the di  X  erent privacy loss  X  are shown in Figure 5.

More specifically, the model (5)isa two-stage procedure which first takes the di  X  erentially private item vectors and then use the latter to obtain locally non-private user param-eter estimates. This is perfectly admissible since users have no expectation of privacy with regard to their own ratings.
Interpreting the privacy guarantees can be subtle. A pri-vacy loss of  X  = 250 as in Figure 5 may seem completely meaningless by Definition 1 and the corresponding results in Mcsherry and Mironov [ 20] may appear much better.
We first address the comparison to Mcsherry and Mironov [20]. It is important to point out that our privacy loss  X  is stated in terms of user level privacy while the results in Mc-sherry and Mironov [20] are stated in terms of rating level privacy, which o  X  ers exponentially weaker protection.  X  -user di  X  erential privacy translates into  X  /  X  -rating di  X  erential pri-vacy. Since  X  = 200 in our case, our results suggest that we almost lose no accuracy at all while preserving rating dif-ferential privacy with  X  &lt; 1. This matches (and slightly improves) Mcsherry and Mironov [20] X  X  carefully engineered system.

On the other hand, we note that the plain privacy loss can be a very deceiving measure of its practical level of pro-tection. Definition 1 protects privacy of an arbitrary user, who can be a malicious spammer that rates every movie in a completely opposite fashion as what the learned model would predict. This is a truly paranoid requirement, and arguably not the right one, since we probably should not protect these malicious users to begin with. For an average user, the personalized privacy (Definition 2) guarantee can be much stronger, as the posterior distribution concentrates around models that predict reasonably well for such users. As a result, the log-likelihood associated with these users will be bounded by a much smaller number with high prob-ability. In the example shown in Figure 5, a typical user X  X  personal privacy loss is about  X  / 25, which helps to reduce the essential privacy loss to a meaningful range.
In this paper we described an algorithm for e ffi cient collab-orative filtering that is compatible with di  X  erential privacy. In particular, we showed that it is possible to accomplish all three goals: accuracy, speed and privacy without any significant sacrifice on either end.

Moreover, we introduced the notion of personalized di  X  er-ential privacy. That is, we defined (and proved) the notion of obtaining estimates that respect di  X  erent degrees of privacy, as required by individual users. We believe that this notion is highly relevant in today X  X  information economy where the expectation of privacy may be tempered by, e.g. the cost of the service, the quality of the hardware (cheap netbooks deployed with Windows 8.1 with Bing), and the extent to which we want to incorporate the opinions of users.
Our implementation takes advantage of the caching prop-erties of modern microprocessors. By careful latency hiding we are able to obtain near peak performance. In particu-lar, our implementation is approximately 3 times as fast as GraphChi, the next-fastest recommender system. In sum, this is a strong endorsement of Stochastic Gradient Langevin Dynamics to obtain di  X  erentially private estimates in recom-mender systems while still preserving good utility.
Acknowledgments: Parts of this work were supported by a grant of Adobe Research. Z. Liu was supported by Sience Fund for Creative Research Groups (61221063); Min-istry of Education Innovation Research Team (IRT13035); NSF of China (91118005, 91218301, 61428206). Y.-X. Wang was supported by NSF Award BCS-0941518 to CMU Statis-tics and Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initia-tive and administered by the IDM Programme O ffi ce.
