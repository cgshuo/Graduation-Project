 The soldiers on a parade ground form a neat rectangle by inter acting with their neighbors. An officer decides where the rectangle should be, but he would be ill-ad vised to try to tell each individual sol-dier exactly where to stand. By allowing constraints to be en forced by local interactions, the officer enormously reduces the bandwidth of top-down communicatio n required to generate a familiar pat-tern. Instead of micro-managing the soldiers, the officer sp ecifies an objective function and leaves it to the soldiers to optimise that function. This example of pattern generation suggests that a multi-layer, directed belief net may not be the most effective way t o generate patterns. Instead of using shared ancestors to create correlations between the variab les within a layer, it may be more efficient for each layer to have its own energy function that is modulat ed by directed, top-down input from the layer above. Given the top-down input, each layer can the n use lateral interactions to settle on a good configuration and this configuration can then provide t he top-down input for the next layer down. When generating an image of a face, for example, the appr oximate locations of the mouth and nose might be specified by a higher level and the local inte ractions would then ensure that the accuracy of their vertical alignment was far greater than th e accuracy with which their locations were specified top-down.
 In this paper, we show that recently developed techniques fo r learning deep belief nets (DBN X  X ) can be generalized to solve the apparently more difficult proble m of learning a directed hierarchy of Markov Random Fields (MRF X  X ). The method we describe can lea rn models that have many hidden layers, each with its own MRF whose energy function is condit ional on the values of the variables in the layer above. It does not require detailed prior knowledg e about the data to be modeled, though it obviously works better if the architecture and the types o f latent variable are well matched to the task. The learning procedure for deep belief nets has now been desc ribed in several places (Hinton et al., 2006; Hinton and Salakhutdinov, 2006; Bengio et al., 2007) a nd will only be sketched here. It relies on a basic module, called a restricted Boltzmann machine (RB M) that can be trained efficiently using a method called  X  X ontrastive divergence X  (Hinton, 20 02). 2.1 Restricted Boltzmann Machines stochastic  X  X idden X  units via symmetrically weighted conn ections. A joint configuration, ( v , h ) of the visible and hidden units has an energy given by: where v the symmetric weight between them. The network assigns a pro bability to every possible image via this energy function and the probability of a training image can be raised by adjusting the weights and biases to lower the energy of that image and to raise the en ergy of similar, reconstructed images that the network would prefer to the real data.
 Given a training vector, v , the binary state, h the state of visible unit i , and w chosen for the hidden units, a reconstruction is produced by setting each v  X  ( b i + P j h j w ij ) . The states of the hidden units are then updated once more so t hat they represent features of the reconstruction. The change in a weight is giv en by where  X  is a learning rate, h v are on together when the hidden units are being driven by data and h v fraction for reconstructions. A simplified version of the sa me learning rule is used for the biases. The learning works well even though it is not exactly followi ng the gradient of the log probability of the training data (Hinton, 2002). 2.2 Compositions of experts A single layer of binary features is usually not the best way t o capture the structure in the data. We now show how RBM X  X  can be composed to create much more powerfu l, multilayer models.
 After using an RBM to learn the first layer of hidden features w e have an undirected model that defines p ( v , h ) via the energy function in Eq. 1. We can also think of the model as defining p ( v , h ) by defining a consistent pair of conditional probabilities, p ( h | v ) and p ( v | h ) which can be used to sample from the model distribution. A different way to expre ss what has been learned is p ( v | h ) and p ( h ) . Unlike a standard directed model, this p ( h ) does not have its own separate parameters. It is a complicated, non-factorial prior on h that is defined implicitly by the weights. This peculiar decomposition into p ( h ) and p ( v | h ) suggests a recursive algorithm: keep the learned p ( v | h ) but replace p ( h ) by a better prior over h , i.e. a prior that is closer to the average, over all the data vectors, of the conditional posterior over h .
 We can sample from this average conditional posterior by sim ply applying p ( h | v ) to the training data. The sampled h vectors are then the  X  X ata X  that is used for training a higher -level RBM that learns the next layer of features. We could initialize the hi gher-level RBM model by using the same parameters as the lower-level RBM but with the roles of the hi dden and visible units reversed. This ensures that p ( v ) for the higher-level RBM starts out being exactly the same as p ( h ) for the lower-level one. Provided the number of features per layer does not decrease, Hinton et al. (2006) show that each extra layer increases a variational lower bound on the log probability of the data. graphical model are a consequence of the the fact that we keep the p ( v | h ) but throw away the p ( h ) defined by the first level RBM. In the final composite model, the only undirected connections are between the top two layers, because we do not throw away the p ( h ) for the highest-level RBM. To suppress noise in the learning signal, we use the real-value d activation probabilities for the visible units of all the higher-level RBM X  X , but to prevent hidden un its from transmitting more than one bit of information from the data to its reconstruction, we alway s use stochastic binary values for the hidden units. For contrastive divergence learning to work well, it is impo rtant for the hidden units to be sampled from their conditional distribution given the data or the re constructions. It not necessary, however, for the reconstructions to be sampled from their conditiona l distribution given the hidden states. All that is required is that the reconstructions have lower free energy than the data. So it is possible to include lateral connections between the visible units and t o create reconstructions by taking a small step towards the conditional equilibrium distribution giv en the hidden states. If we are using mean-field activities for the reconstructions, we can move toward s the equilibrium distribution by using a few damped mean-field updates (Welling and Hinton, 2002). We call this a semi-restricted Boltz-mann machine (SRBM). The visible units form a conditional MR F with the biases of the visible units being determined by the hidden states. The learning pr ocedure for the visible to hidden con-nections is unaffected and the same learning procedure appl ies to the lateral connections. Explicitly, the energy function for a SRBM is given by and the update rule for the lateral connections is Semi-restricted Boltzmann machines can be learned greedil y and composed to form a directed hi-erarchy of conditional MRF X  X . To generate from the composit e model we first get an equilbrium sample from the top level SRBM and then we get an equilibrium s ample from each lower level MRF in turn, given the top-down input from the sample in the layer above. The settling at each interme-diate level does not need to explore a highly multi-modal ene rgy landscape because the top-down input has already selected a good region of the space. The rol e of the settling is simply to sharpen the somewhat vague top-down specification and to ensure that the resulting configuration repects learned constraints. Each intermediate level fills in the de tails given the larger picture defined by the level above. In a deep belief network, inference is very simple and very fa st because of the way in which the network is learned. Rather than first deciding how to represe nt the data and then worrying about in-ference afterwards, deep belief nets restrict themselves t o learning representations for which accurate variational inference can be done in a single bottom-up pass . Each layer computes an approximate sample from its posterior distribution given the activitie s in the layer below. This can be done with a single matrix multiply using the bottom-up  X  X ecognition X  connections that were originally learned by an RBM but are no longer part of the generative model. The re cognition connections compute an approximation to the product of a data-dependent likelihood term coming from the layer be low and a data-independent prior term that depends on the learne d parameters of all the higher layers. Each of these two terms can contain strong correlations, but the way in which the model is learned ensures that these correlations cancel each other out so tha t the true posterior distribution in each layer is very close to factorial and very simple to compute fr om the activities in the layer below. The inference process is unaltered by adding an MRF at each hi dden layer. The role of the MRF X  X  is to allow the generative process to mimic the constraints t hat are obeyed by the variables within a layer when the network is being driven bottom-up by data. Dur ing inference, these constraints are enforced by the data. From a biological perspective, it is ve ry important for perceptual inference to be fast and accurate, so it is very good that it does not involv e any kind of iterative settling or belief propagation. The MRF X  X  are vital for imposing constraints d uring generation and for whitening the learning signal so that weak higher-order structure is not m asked by strong pairwise correlations. During perceptual inference, however, the MRF X  X  are mere sp ectators. Data is often whitened to prevent strong pairwise correlati ons from masking weaker but more in-teresting structure. An alternative to whitening the data i s to modify the learning procedure so that it acts as if the data were whitened and ignores strong pairwise correlat ions when learning the next level of features. This has the advantage that perceptual in ference is not slowed down by an explicit whitening stage. If the lateral connections ensure that a pa irwise correlation in the distribution of the reconstructions is the same as in the data distribution, tha t correlation will be ignored by contrastive divergence since the learning is driven by the differences b etween the two distributions. This also explains why different hidden units learn different featur es even when they have the same connec-tivity: once one feature has made one aspect of the reconstru ctions match the data, there is no longer any learning signal for another hidden unit to learn that sam e aspect.
 Figure 1 shows how the features learned by the hidden units ar e affected by the presence of lateral connections between the visible units. Hidden units are no l onger required for modeling the strong pairwise correlations between nearby pixels so they are fre e to discover more interesting features than the simple on-center off-surround fetaures that are pr evalent when there are no connections between visible units. (A) (B) Figure 1: (A) A random sample of the filters learned by an RBM tr ained on 60,000 images of hand-written digits from the MNIST database (see Hinton et al. (20 06) for details). (B) A random sample of the filters learned by an SRBM trained on the same data. To pr oduce each reconstruction, the SRBM used 5 damped mean-field iterations with the top-down in put from the hidden states fixed. Adding lateral connections between the visible units chang es the types of hidden features that are learned. For simplicity each visible unit in the SRBM was con nected to all 783 other visible units, but only the local connections developed large weights and t he lateral connections to each pixel formed a small on-center off-surround field centered on the p ixel. Pixels close to the edge of the image that were only active one or two times in the whole train ing set behaved very differently: They learned to predict the whole of the particular digit tha t caused them to be active. To illustrate the advantages of adding lateral connections to the hidden layers of a DBN we use Sejnowski, 1997; Olshausen and Field, 1996; Karklin and Lew icki, 2005; Osindero et al., 2006; Lyu and Simoncelli, 2006). Using DBN X  X , it is easy to build overc omplete and hierchical generative models of image patches. These are able to capture much riche r types of statistical dependency than traditional generative models such as ICA. They also have th e potential to go well beyond the types of dependencies that can be captured by other, more sophisti cated, multi-stage approaches such as (Karklin and Lewicki, 2005; Osindero et al., 2006; Lyu and Si moncelli, 2006). 6.1 Adapting Restricted Boltzmann machines to real-valued data Hinton and Salakhutdinov (2006) show how the visible units o f an RBM can be modified to allow it to model real-valued data using linear visible variables wi th Gaussian noise, but retaining the binary stochastic hidden units. The learning procedure is essenti ally unchanged especially if we use the mean-field approximation for the visible units which is what we do. Two generative DBN models, one with and one without lateral c onnectivity, were trained using the updates from equations 2 and 4. The training data used con sisted of 150,000 20  X  20 patches extracted from images of natural scenes taken from the colle ction of Van Hateren 1 . The raw im-age intensities were pre-processed using a standard set of o perations  X  namely an initial log-transformation, and then a normalisation step such that eac h pixel had zero-mean across the training set. The patches were then whitened using a Zero-Phase Compo nents analysis (ZCA) filter-bank. The set of whitening filters is obtained by rotating the data i nto a co-ordinate system aligned with the eigenvectors of the covariance matrix, then rescaling e ach component by the inverse square-root of the correspoding eigenvalue, then rotating back into the original pixel co-ordinate system. Using ZCA has a similar effect to learning lateral connectio ns between pixels (Welling and Hinton, 2002). We used ZCA whitened data for both models to make it cle ar that the advantage of lateral connections is not just caused by their ability to whiten the input data. Because the data was whitened we did not include lateral connections in the bottom layer of the lateral DBN. The results presented in the figures that follow are all shown in  X  X nwhitened pixel-space X , i.e. the effects of the whitening filter are undone for display purposes.
 The models each had 2000 units in the first hidden layer, 500 in the second hidden layer and 1000 top layer to be quite large. In the case where lateral connect ions were used, the first and second hidden layers of the final, composite model were fully latera lly connected.
 Data was taken in mini-batches of size 100, and training was p erformed for 50 epochs for the first layer and 30 epochs for the remaining layers. A learning rate of 10  X  3 was used for the interlayer connections, and half that rate for the lateral connections . Multiplicative weight decay of 10  X  2 mul-tiplied by the learning rate was used, and a momentum factor o f 0 . 9 was employed. When training the higher-level SRBM X  X  in the model with lateral connectiv ity, 30 parallel mean field updates were used to produce the reconstructions with the top-down input from the hidden states held constant. Each mean field update set the new activity of every  X  X isible X  unit to be 0 . 2 times the previous ac-tivity plus 0 . 8 times the value computed by applying the logistic function t o the total input received from the hidden units and the previous states of the visible u nits.
 Learned filters Figure 2 shows a random sample of the filters learned using an R BM with Gaussian visible units. These filters are the same for both models. This representati on is 5  X  overcomplete.
 Figure 2: Filters from the first hidden layer. The results are generally similar to previous work on learning representations of natural image patches. The maj ority of the filters are tuned in location, orientation, and spatial frequency. The joint space of loca tion and orientation is approximately evenly tiled and the spatial frequency responses span a rang e of about four octaves. 6.1.1 Generating samples from the model The same issue that necessitates the use of approximations w hen learning deep-networks  X  namely the unknown value of the partition function  X  also makes it di fficult to objectively assess how well they fit the data in the absence of predictive tasks such as cla ssification. Since our main aim is to demonstrate the improvement in data modelling ability that lateral connections bring to DBN X  X , we simply present samples from similarly structured models, w ith and without lateral connections, and compare these samples with real data. Ten-thousand data samples were generated by randomly initi alising the top-level (S)RBM states and then running 300 iterations of a Gibbs sampling scheme betwe en the top two layers. For models without lateral connections, each iteration of the scheme c onsisted of a full parallel-update of the top-most layer followed by a full parallel-update of the pen ultimate layer. In models with lateral connections, each iteration consisted of a full parallel-u pdate of the top-most layer followed by 50 rounds of sequential stochastic updates of each unit in the p enultimate layer, under the influence of the previously sampled top-layer states. (A different rand om ordering of units was drawn in each update-round.) After running this Markov Chain we then perf ormed an ancestral generative pass down to the data layer. In the case of models with no lateral co nnections, this simply involved sampling from the factorial conditional distribution at ea ch layer. In the case of models with lateral connections we performed 50 rounds of randomly-ordered, se quential stochastic updates under the influence of the top-down inputs from the layer above. In both cases, on the final hidden layer update before generating the pixel values, mean-field updates were used so that the data was generated using the real-valued probabilities in the first hidden layer rath er than stochastic-binary states. Figure 3: (A) Samples from a model without lateral connectio ns. (B) Samples from a model with lateral connections. (C) Examples of actual data, drawn at r andom. (D) Examples of actual data, chosen to have closest cosine distance to samples from panel (B).
 Figure 3 shows that adding undirected lateral interactions within each intermediate hidden layer of a deep belief net greatly improves the model X  X  ability to gen erate image patches that look realistic. It is evident from the figure that the samples from the model wi th lateral connections are much more similar to the real data and contain much more coherent, long -range structure. Belief networks with only directed connections have difficulty capturing spatia l constraints between the parts of an image because, in a directed network, the only way to enforce const raints is by using observed descendants. Unobserved ancestors can only be used to model shared source s of variation. 6.1.2 Marginal and pairwise statistics In addition to the largely subjective comparisons from the p revious section, if we perform some simple aggregate analyses of the synthesized data we see tha t the samples from the model with lateral connections are objectively well matched to those f rom true natural images. In the right-hand column of figure 4 we show histograms of pixel inensities for r eal data and for data generated by the two models. The kurtosis is 8 . 3 for real data, 7 . 3 for the model with lateral connections, and 3 . 4 for the model with no lateral connections. If we make a histogram of the outputs of all of the filters in the first hidden layer of the model, we discover that the kurto sis is 10 . 5 on real data, 10 . 3 on image patches generated by the model with lateral connections, an d 3 . 8 on patches generated by the other model.
 Columns one through five of figure 4 show the distributions of t he response of one filter conditional on the response of a second filter. Again, for image patches ge nerated with lateral connections the statistics are similar to the data and without lateral conne ctions they are quite different. Figure 4: Each row shows statistics computed from a differen t set of 10,000 images. The first interactions. The third row is for patches generated withou t lateral interactions. Column six shows histograms of pixel intensities. Columns 1-5 show conditio nal filter responses, in the style suggested in (Wainwright and Simoncelli, 2000), for two different gab or filters applied to the sampled images. In columns 1-3 the filters are 2, 4, or 8 pixels apart. In column 4 they are at the same location but orthogonal orientations. In column 5 they are at the same loc ation and orientation but one octave apart in spatial frequency. Our results demonstrate the advantages of using semi-restr icted Boltzmann machines as the building blocks when building deep belief nets. The model with latera l connections is very good at capturing the statistical structure of natural image patches. In futu re work we hope to exploit this in a number of image processing tasks that require a good prior over imag e patches.
 The models presented in this paper had complete lateral conn ectivity  X  largely for simplicity in MATLAB. Such a strategy would not be feasible were we to signi ficantly scale up our networks. Fortunately, there is an obvious solution to this  X  we can sim ply restrict the majority of lateral interactions to a local neighbourhood and concomittently h ave the hidden units focus their attention on spatially localised regions of the image. A topographic o rdering would then exist throughout the various layers of the hierarchy. This would greatly reduce t he computational load and it corresponds to a sensible prior over image structures, especially if the local regions get larger as we move up the hierarchy. Furthermore, it would probably make the process of settling within a layer much faster. One limitation of the model we have described is that the top-down effects can only change the effective biases of the units in the Markov random field at eac h level. The model becomes much more powerful if top-down effects can modulate the interact ions. For example, an  X  X dge X  can be viewed as a breakdown in the local correlational structure o f the image: pixel intensities cannot be predicted from neighbors on the other side of an object bound ary. A hidden unit that can modulate the pairwise interactions rather than just the biases can fo rm a far more abstract representation of an edge that is not tied to any particular contrast or intensity (Geman and Geman, 1984). Extending our model to this type of top-down modulation is fairly straight forward. Instead of using weights w that contribute energies  X  v allows the binary state of h and Hinton (2007) show that the same learning methods can be a pplied with a single hidden layer and there is no reason why such higher-order semi-restricted Bo ltzmann machines cannot be composed into deep belief nets.
 Although we have focussed on the challenging task of modelin g patches of natural images, we believe the ideas presented here are of much more general app licability. DBN X  X  without lateral con-nections have produced state of the art results in a number of domains including document retrieval (Hinton and Salakhutdinov, 2006), character recognition ( Hinton et al., 2006), lossy image compres-sion (Hinton and Salakhutdinov, 2006), and the generation o f human motion (Taylor et al., 2007). Lateral connections may help in all of these domains.
 Acknowledgments We are grateful to the members of the machine learning group a t the University of Toronto for helpful discussions. This work was supported by NSERC, CFI a nd CIFAR. GEH is a fellow of CIFAR and holds a CRC chair.
 Bell, A. J. and Sejnowski, T. J. (1997). The  X  X ndependent com ponents X  of natural scenes are edge filters. Vision Research , 37(23):3327 X 3338.
 Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2 007). Greedy layer-wise training of deep networks. In B., S., Platt, J., and Hoffman, T., editors , Advances in Neural Information Processing Systems 19 . MIT Press, Cambridge, MA.
 Geman, S. and Geman, D. (1984). Stochastic relaxation, gibb s distributions and the bayesian restora-tion of images. IEEE Trans. Pattern Anal. Mach. Intell , 6.
 Hinton, G. E. (2002). Training products of experts by minimi zing contrastive divergence. Neural Computation , 14(8):1711 X 1800.
 Hinton, G. E., Osindero, S., and Teh, Y. W. (2006). A fast lear ning algorithm for deep belief nets. Neural Computation , 18.
 Hinton, G. E. and Salakhutdinov, R. (2006). Reducing the dim ensionality of data with neural net-works. Science , 313.
 Karklin, Y. and Lewicki, M. (2005). A hierarchical bayesian model for learning nonlinear statistical regularities in nonstationary natural signals. Neural Computation , 17(2).
 mixtures. In Advances Neural Information Processing Systems , volume 19.
 Memisevic, R. F. and Hinton, G. E. (2007). Unsupervised lear ning of image transformations. In Computer Vision and Pattern Recognition . IEEE Computer Society.
 Olshausen, B. A. and Field, D. J. (1996). Emergence of simple -cell receptive field properties by learning a sparse code for natural images. Nature , 381(6583):607 X 609. JUN 13 NATURE.
 Osindero, S., Welling, M., and Hinton, G. E. (2006). Topogra phic product models applied to natural scene statistics. Neural Computation , 18(2).
 Taylor, G. W., Hinton, G. E., and Roweis, S. (2007). Modeling human motion using binary latent variables. In B., S., Platt, J., and Hoffman, T., editors, Advances in Neural Information Processing Systems 19 . MIT Press, Cambridge, MA.
 Wainwright, M. and Simoncelli, E. (2000). Scale mixtures of Gaussians and the statistics of natural images. In Advances Neural Information Processing Systems , volume 12, pages 855 X 861.
 Welling, M. and Hinton, G. E. (2002). A new learning algorith m for mean field boltzmann machines.
In International Joint Conference on Neural Networks , Madrid.
