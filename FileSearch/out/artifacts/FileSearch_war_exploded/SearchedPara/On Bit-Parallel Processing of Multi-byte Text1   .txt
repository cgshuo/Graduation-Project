
Heikki Hyyr  X  o 1 , Jun Takaba 2 , Ayumi Shinohara 1 , 2 , and Masayuki Takeda 2 , 3 Different types of pair-wise string processing algorithms are fundamental in many information retrieval and processing tasks. Let the two processed strings be P and T , of length m and n , respectively. The most basic task is exact string matching, where P is a pattern string and T a text string, and one searches for occurrences of P inside T . Other typical examples include case-insensitive search, regular expression matching, and approximate string comparison. So-called bit-parallel algorithms have emerged as practical solutions for several of such string processing tasks. Let w denote the computer word size. We list here some examples of practical bit-parallel algorithms. Each of these has a run time O ( m / w n ). Baeza-Yates and Gonnet [3] proposed an algorithm for exact string matching, and that algorithm can handle also for example case-insensitive search. In [17] Navarro presents methods for allowing repeatable or optional characters in the pattern. Allison and Dix [2], Crochemore et al. [5], and Hyyr  X  o [11] have presented algorithms for computing the length of a longest common subsequence between P and T . Myers [15] presented an O ( m / w n ) algorithm for finding approximate occurrences of P from T , when Levenshtein edit distance is used as the measure of similarity. This algorithm can be modified to compute Lev-enshtein edit distance between P and T [12] as well as to use Damerau edit distance [10].
 ically use a size- X  table of match bit-vectors, where  X  is the size of the alphabet  X  , and the characters in  X  are mapped into the interval [0 ...  X   X  1]. Let us call the match table PM . For each character  X   X   X  , the bit values in the cor-responding match vector PM  X  mark the positions in P where the character  X  occurs. The cost of preprocessing and storing PM is reasonable with small al-phabets, such as the 7-or 8-bit ASCII character set. But in case of more general alphabets, perhaps most importantly Unicode text, the range of possible nu-merical character codes is much larger. To be specific, Unicode character codes fall into the range [0 ... 1114111]. This makes using a naively stored PM table impractical. A basic observation is that the value PM  X  needs to be explicitly computed only for those O ( m ) characters  X  that occur in P . All other characters share an identical  X  X mpty X  match vector. One quite straightforward solution is then to store only the non-empty vectors PM  X  into a hash table whose size is O ( m ) instead of  X  . Another solution, similar to the one proposed by Wu, Manber and Myers [25], is to sort into one size-O ( m ) table the character codes of those  X  that occur in P , and store the match vectors PM  X  , in corresponding order, into another size-O ( m ) table. The value PM  X  is then determined by doing an O (log m ) binary search in the code table (the bound assumes that two character codes can be compared in constant time). If the code of  X  is found at the i th position, then the vector PM  X  is in the i th position of the match vector table, and otherwise PM  X  is an empty match vector.
 vectors. The idea is to build an automaton that recognizes the alphabet character codes and whose accepting states identify the corresponding match vectors. The automaton reads the characters in byte-wise manner. We compare this method with the above described alternatives on UTF-8 encoded Unicode text and find that using an automaton is competitive. The results show that the choice of how to handle the match bit-vectors can have a significant effect in terms of the overall processing time: using the binary-search based method of Wu, Manber and Myers [25] may result in the overall processing time being almost three times as much as with the other two methods. We also include a basic direct table lookup in the comparison. This is done by using a restricted multi-byte character set that allows us to use a small table in storing the match vectors. The comparison provides a characterization about the feasibility of using bit-parallel algorithms with Unicode text. This is important as Unicode is becoming more and more widely used. This is true especially on the Internet, which allows people with very different cultures (and character sets) share textual information. XML (eXtensible Markup Language), which is an increasingly popular format for storing data for example on www-pages, uses by default UTF-8 encoded Unicode text. To our best knowledge, the current paper provides the first study about using bit-parallel algorithms in processing multi-byte encoded strings. The 7-bit ASCII (American Standard Code for Information Interchange) is a fun-damental computer character encoding. It, or some 8-bit extended ASCII form of it, is used with variations of the Latin (or Roman) alphabet. Many common computer systems/programs, such as the UNIX operating systems variants as well as the C programming language, are inherently designed to use such a single-byte ASCII code. For example a zero-byte is typically interpreted to mark an end of file. bets require a multi-byte character encoding. There are several specialized en-codings. For example Japanese Extended Unix Code (EUC), BIG5 (Taiwanese), shift-JIS (Japanese), EUC-KR (Korean), and so on. For compatibility with ASCII-oriented systems, such multi-byte encodings usually reserve the code range 0 ... 127 for the single-byte 7-bit ASCII characters, and the multi-byte characters consist of byte values in the range 128 ... 255. In terms of being able to recognize the characters, an important property in practice is also that the multi-byte code should be a prefix code . This means that no character code should be a continuation of another, or conversely, that no character code should be a prefix of another character code.
 ent languages and alphabets, the Unicode Consortium has created a common international standard character code, Unicode, that can express every char-acter in every language in the world [24, 23]. In its present form, Unicode can express 1114112 different characters. Out of these, currently more than 96000 areactuallymappedintosomecharacter.Unicodedefinesanumericcodefor each character, but it does not specify how that code is actually encoded. The following three are common alternatives:
UTF-32: A simple fixed-length encoding, where each character is encoded with UTF-16: A variable-length encoding: each character is encoded by 2 or 4 bytes. UTF-8: A variable-length encoding: each character is encoded by 1, 2, 3 or In this paper we concentrate on UTF-8 as it is the most compatible of these three choices and also serves as an example of a general variable-byte encoding. the structure of each byte as an 8-bit sequence, where the bit significance grows from right to left, and a value  X  X  X  means that the corresponding bit value is used in storing the actual numeric value of the encoded character. Below each 8-bit sequence we also show the corresponding possible range of numerical (base-10) values for the byte. The length of a UTF-8 code can be inferred from the most significant (here leftmost) bits of its first byte. If the first bit is zero, the code has a single byte. Otherwise the code has as many bytes as there are consecutive one bits when counting from the most significant bit towards the least significant bit. A byte is a continuation byte of a multi-byte UTF-8 code if and only if its value is in the range 128 ... 191. UTF-8 is clearly a prefix code. The number of available bits ( X  X  X ) for encoding a character code is 7 for a single-byte code, 11 for a 2-byte code, 16 for a 3-byte code, and 21 for a 4-byte code. Hence UTF-8 encoding can in principle express 2 7 +2 11 +2 16 +2 21 = 2164864 distinct characters. In this section we review three fundamental and much studied forms of string processing. They were chosen as typical representatives of string processing that can be solved by efficient bit-parallel algorithms. The motivation is to lay basic background: The discussed tasks are the ones we will concentrate on in the tests with multi-byte encoded text in Section 5. But let us first introduce some further basic notation. The length of a string A is denoted by | A | , A i is the i th character of A ,and A i..j denotes the substring of A that begins from its i th character and ends at the j th character. If j&lt;i , we interpret A i..j to be the empty string .If A is nonempty, the first character of A is A 1 and A = A 1 .. | A | . The substring A 1 ..j is a prefix and the substring A j.. | A | is a suffix of A . The string C is a subsequence of the string A if C can be derived by deleting zero or more characters from A . Thus C is a subsequence of A if the characters C 1 ...C | C | appear in the same order, but not necessarily consecutively, in A .
 Exact String Matching. Exact string matching is one of the most funda-mental string processing tasks. When one is given a length-m pattern string P and a length-n text string T , the task is to find all text indices j for which P = P 1 ..m = T j  X  1+ m..j . A common variant of this, and also the following two other tasks, is case insensitive matching , where no distinction is made between lower-and uppercase characters.
 Longest Common Subsequence. The string C is a longestcommonsubse-quence of the strings P and T ,if C is a subsequence of both P and T , and no longer string with this property exists. We denote a longest common subsequence between the strings P and T by LCS( P, T ), and LLCS( P, T ) denotes the length of LCS( P, T ). Both LCS( P, T ) and LLCS( P, T ) convey information about the similarity between P and T . This may be used for example in molecular biology (see e.g. [21]), file comparison (e.g. the Unix  X  X iff X  utility), or assessing how closely related two words are to each other (e.g. [20]).
 Edit Distance and Approximate String Matching. Edit distance is an-other measure of similarity between two strings. The edit distance ed ( P, T )be-tween the strings P and T is in general defined as the minimum number of edit operations that are needed in transforming P into T or vice versa.
 text substring is within a given edit distance from the pattern. A more formal definition is that the task is to find all text indices j for which ed ( P, T h..j )  X  k , where h  X  j and k is the given error threshold.
 tances are typical. We denote by ed s ( P, T ) a simple edit distance that allows one edit operation to delete or insert a single character. The values ed s ( P, T )and LLCS( P, T ) are connected by the equality 2  X  LLCS( A, B )= n + m  X  ed id ( A, B ) (e.g. [6]). Probably the most common form of edit distance is Levenshtein edit distance [14], which extends the simple edit distance by allowing also the opera-tion of substituting a single character with another. We denote Levenshtein edit distance between P and T by ed L ( P, T ). Damerau distance [8], which we denote by ed D ( P, T ), is used especially in spelling correction related applications. It ex-tends Levenshtein distance by allowing also a fourth edit operation: transposing (swapping) two adjacent characters. 3.1 Bit-Parallel Algorithms During the last two decades, so-called bit-parallel algorithms have emerged as practical choices for several string processing tasks. The principle of such algo-rithms is in general to take advantage of the fact that computers process data in chunks of w bits, where w is the computer word size (in effect the number of bits in a single register within the processor). Currently most computers have a word size w = 32, but also the word size w = 64 is becoming increasingly common. In addition, most current personal computers support specialized instruction extension sets, such as MMX or SSE, that allow one to use w =64oreven w = 128. Bit-parallel algorithms store several data-items into a single computer word, and then update them in parallel during a single computer operation. denotes bitwise  X  X r X ,  X   X   X  denotes bitwise  X  X or X ,  X   X   X  denotes bit complementation, and  X  &lt;&lt;  X  X nd X  &gt;&gt;  X  denote shifting the bit-vector left and right, respectively, using zero filling in both directions. Bit positions are assumed to grow from right to left, and we use superscripts to denote repetition. As an example let V = 1110010 be a bit vector. Then V [1] = V [3] = V [4]=0, V [2] = V [5] = V [6] = V [7] = 1, and we could also write V =1 3 0 2 10.
 as follows. First a size- X  match table PM is computed for the length-m string P . PM holds a length-m match bit-vector PM  X  for each character  X   X   X  . The bit-vector PM  X  identifies the positions in the string P where the character  X  occurs: the i th bit of PM  X  is set if and only if P i =  X  . For simplicity, we will assume throughout this paper that m  X  w . The case m&gt;w can be handled by simulat-ingalength-m bit-vector by concatenating m / w length-w bit-vectors, and thus the table PM occupies in general  X  m / w bits. Once PM is preprocessed and the data bit-vectors used by the algorithm have been initialized, the bit-parallel algorithm processes the string T sequentially. At each character T j the algorithm updates the data bit-vectors by using bit-operations. Depending on the task, the algorithm may at this point also update some score value and/or check whether a match was found at position j . Fig. 1 shows pseudocode for preprocessing PM and a skeleton for the actual processing phase. The sub-procedure  X  X ro-cessVectors X  encloses all steps that a particular algorithm conducts at character T . In what follows we will show some specific choices for the sub-procedures. Each bit-parallel algorithm that we discuss runs in O ( n )timewhen m  X  w andingeneralin O ( m / w n ) time. But a detailed discussion of any of these algorithms is outside the scope of this paper; the reader should look into the given references for more information about them. The algorithms are shown as examples of different types of bit-parallel algorithms, and they are the ones we use in testing.
 parallel algorithms can be easily modified to be case-insensitive. This is usually said more broadly: the algorithms can use classes of characters . For each char-acter  X  we may define a set of characters that are deemed to match with  X  .This can be done simply by setting the i th bit of PM  X  for all such  X  for which we wish to define P i =  X  =  X  .
 exact string matching. When m  X  w , its behaviour is similar, although much faster, than that of the well-known linear-time string matching algorithm of Knuth, Morris and Pratt [13]. Shift-and processes all text characters in sequential order, and thus it is typically somewhat slower than algorithms that try to skip quickly over such text areas that are seen not to contain a match (e.g. [4, 7, 18]). The latter approach is, however, more difficult in the case of variable-length encoded text. The pseudocode for the bit-parallel processing of the shift-and algorithm at the character T j is shown in the upper left part of Fig. 2. common subsequence problem. To our best knowledge, this was also the first bit-parallel approximate string processing algorithm. Later Crochemore et al. [5] and Hyyr  X  o [11] have proposed similar variants. The lower left part of Fig. 2 shows the pseudocode for the bit-parallel LLCS processing of [11] that makes four operations per character of T . As discussed in [11], these bit-parallel algorithms are very practical for LLCS-computation.
 matching under Levenshtein edit distance. The tests in [16] show that this algo-rithm is in many cases the fastest in practice. Here we refer to so-called  X  X erifi-cation capable X  algorithms that are based on actually computing edit distance. It is easy to transform Myers X  algorithm to compute edit distance [12], and it has also been modified to use Damerau distance [10]. The right side of Fig. 2 shows the pseudocode for the slightly simpler variant of Hyyr  X  o [9, 19]. As discussed in Section 1, storing the match vectors PM  X  intoasize- X  table is not practical in the case of Unicode encoded text or similar large alphabets. In this section we first propose an approach that uses a code automaton to overcome this problem. Then we also discuss two other options.
 Code Automaton. Our proposal is to build a minimized code automaton that uniquely recognizes the encoding of each character that appears in P ,andin addition accepts the encodings of all those characters that do not appear in the pattern. Let u be the number of different characters that appear in P . Then the code automaton has u + 1 accepting states: one for each different character in P , and one that represents all other characters in  X  . If the character  X  appears in P , we associate PM  X  with the state that accepts the encoding of  X  .The state that represents those characters that do not appear in P will be associated with a zero match vector 0 m . In our case of multi-byte character encoding, we will read the text T with the code automaton one byte at a time. Whenever the automaton recognizes a character, a bit-parallel algorithm can process the currently read text character T j by using the match vector that is associated with the current accepting state.
 over the encodings of all characters in the alphabet, and then minimizing it so that all leaves that correspond to characters that do not appear in the pattern P are merged into a single leaf. The leaves corresponding to the characters that appear in the pattern are not merged. When u has the same meaning as above, the resulting DAG (Directed Acyclic Graph) has u + 1 leaf nodes, which are the accepting states of the corresponding automaton. The final automaton is then composed by augmenting the DAG with Aho-Corasick failure links [1] and associating the match vectors with the accepting states. The process (except for the match vectors) is similar to how the pattern matching automaton used in [22] is built. The main difference is that here the  X  X et of patterns X  of the pattern matching automaton is formed by those character encodings that appear in P . Fig. 3 shows an example.
 Hash Table. The second approach is to use a hash table, which is a standard text-book procedure for storing keys. In this scheme the range of numerical values of the character encodings (for example 1 ... 1114112 in the case of the full range of Unicode encodings) is mapped onto a relatively small integer range 1 ...x .Let code (  X  ) denote the numerical value of the encoding of the character  X  , and let the function h as h ( code (  X  )) give the mapping onto the range 1 ...x .Foreach  X  that occurs in P ,thevalue code (  X  ) is stored into the position h as h ( code (  X  )) of the match vector table. If two non-equal characters in P have the same mapping, different mechanisms can be used. We describe here a simple linear hashing scheme. If the position h as h ( code (  X  )) in the table is already used when we are attempting to store code (  X  ) into it, we continue probing the next positions one-by-one until an empty position is found and store the value there. If the end of the table is reached, we continue from the first position of the table. This works as long as the table is not yet full, but the process takes h steps in the worst case, where h is the number of items currently in the table. But the scheme works well ifthenumberofstoreditemsissmallincomparisonto x . The match vectors are associated with the corresponding character encodings in the table. Finding the encoding value of a text character T j from the table works in similar fashion: first the mapping value h as h ( code ( T j )) is computed, and then the table is checked from the corresponding position onwards until either the value code ( T j )oran empty position is found. In the former case we use the associated match vector. Inthelattercasethetabledoesnotcontain code ( T j ), and we use an empty match vector 0 m .
 where u is again the number of distinct characters in P . For efficiency we use an extended table of size x + u so that we do not need to worry about reaching the end of the table. With multi-byte text we have tested a very simple mapping. It maps a multi-byte code onto the range 0 ... 255 (corresponds to x = 256) by using the value of the last byte in the code. As far as the encodings are random enough not to share too many identical last bytes, this works very efficiently. Binary Search. The third approach is derived from the proposition of Wu, Manber and Myers [25]. In it the numerical values of the character encodings of the u distinct pattern characters are stored into a size-u table, and the values are sorted. The match vectors are associated with the corresponding values. The value code ( T j ) is looked up from the table by doing an O ( l og 2 u ) binary search. Again we use the corresponding match vector if the value code ( T j ) was found from the table, and otherwise an empty match vector 0 m . We implemented and tested the three match table handling schemes from the previous section. In order to characterize their performance in conjunction with bit-parallel algorithms of various complexity, we did separate tests with each of the three bit-parallel algorithms discussed in Section 3.1. In order to evaluate hardware-dependency, we tested on four different computers: AMD Athlon64, Intel Pentium 4, AlphaStation XP1000 and Sparc Ultra 2. The code was exactly the same with all computers, and the different bit-parallel methods used the same file-handling framework. On the AlphaStation we used CC compiler, and on the other computers we used GCC. All code was compiled with the  X -O9 X  optimization switch. The tested strings were UTF-8 encoded, and they were generated randomly. The lengths of P were m = 4, 8, 16 and 32. The length of T was at least one million characters in the case of searching, and P and T were of equal length in the case of computing LLCS( P, T ). Each test included 100 different choices for P . In searching we used a single text T , and in computing the value LLCS( P, T ) we used as many T as was necessary for their combined length to be at least one million characters. In order to estimate the overhead of these match vector handling methods in comparison to the simple lookup from a size- X  table, we included also a test where the strings contain only UTF-8 characters that have distinct last byte values. This way our simple hash table method could be turned into a direct table lookup. Fig. 4 shows the results as a percentage of the running time of the direct table lookup.
 puters. In some cases the overall processing time was almost three times longer than with the code automaton. The relative performance of the hash table and the code automaton varied depending on m and the computer. On Pentium 4 the code automaton was always the fastest scheme, in fact even faster than the direct table lookup. We re-checked this with another compiler, and the situation remained the same. This is perhaps due to some pipelining effect etc. We note that this does not depend on the fact that the direct table lookup used restricted character encodings: we tested also the other schemes on the specially encoded strings, and their running times were practically the same as with the regular random strings. On Sparc and AMD the automaton and hash table performed fairly equally. With small m the hash table tended to be often a little faster (al-ways less than 10%), and with larger m the code automaton became the better of the two. On AlphaStation the hash table was up to roughly 20% faster than the code automaton, but still a little bit slower with m = 32.
 against the other methods. We also note that the overall penalty for not being able to use a direct table lookup is reasonably small: never more than roughly 40%. Since the advantage of the bit-parallel methods over other kinds of al-gorithms is often much larger than this, they seem to be practical also with multi-byte encoded text. In addition, also the other types of string processing algorithms will have to pay some penalty for having to deal with multi-byte encoding. We also point out that the automaton is quite insensitive to the value of m or the properties of the strings. Hence it is a feasible option for use with bit-parallel multi-byte string processing. In this paper we proposed a scheme that uses a code automaton for looking up match vectors of multi-byte encoded characters. We also discussed two other schemes for the same task, and compared the three quite extensively. The test results showed that using the automaton is often the fastest choice, and never more than roughly 25% slower than the next best of these schemes. The binary search based method proposed by Wu, Manber and Myers in [25] was found to perform very slow. Using it resulted always in the longest processing time, in one case almost three times longer than when using the code automaton. Overall the test results give an idea about the feasibility of processing multi-byte encoded text with bit-parallel algorithms. As the test indicated the penalty to be at most roughly 40%, bit-parallel algorithms are a viable option with multi-byte text.
