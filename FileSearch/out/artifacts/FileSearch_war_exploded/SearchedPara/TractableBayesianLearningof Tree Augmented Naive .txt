 Jes us Cerquides cerquide@maia.ub.es Gran Via 585, 08007 Barcelona, Spain Ramon Lopez de Mantaras mant aras@i iia.csic.es Campus UAB, 08193 Bellaterra, Spain Bayesian classi ers as Naive Bayes (Langley et al., 1992) or Tree Augmente d Naive Bayes (TAN) (Fried-man et al., 1997) have sho wn excellen t performance in spite of their simplicit y and hea vy underlying inde-pendence assumptions.
 Furthermore, it has been sho wn (Cerquides &amp; Lopez de Mantaras, 2003a; Kon tkanen et al., 1998) that Naive Bayes predictions and probabilit y estimations can bene t from incorp orating uncertain ty in mo del selection by means of Bayesian mo del averaging. In the case of TAN, a dev elopmen t inspired in this same idea is presen ted in (Cerquides, 1999), where to over-come the dicult y of exactly calculating the averaged classi er the idea of local Bayesian mo del averaging is introduced to calculate an appro ximation. In this case predictions are also impro ved.
 In this pap er we sho w that, under suitable assump-tions, the Bayesian mo del averaging of TAN can be integrated in closed form and that it leads to impro ved classi cation performance. The pap er is organized as follo ws. In section 2 we review Tree Augmente d Naive Bayes and the notation that we will use in the rest of the pap er. In section 3 we dev elop the closed ex-pression for the Bayesian mo del averaging of TAN and we construct a classi er based on this result whic h we will name tbma tan (from Tractable Bayesian Mo del Averaging of Tree Augmen ted Naiv e-Ba yes). In sec-tion 4 we notice that tbma tan has a ma jor dra wbac k that mak es dicult its usage for large datasets because it dep ends on the calculation of an ill-conditioned de-terminan t that requires the oating point precision to increase with the dataset size and hence increases the computing time. To solv e this dra wbac k we introduce sstbma tan , an appro ximation of tbma tan . In sec-tion 5 we study the empirical characteristics of tb-matan and sho w that it leads to impro ving classi -cation accuracy and to a better appro ximation of the class probabilities with resp ect to TAN. We also sho w that the empirical results for sstbma tan do not dif-fer signi can tly from the ones obtained by tbma tan while allo wing to deal with large datasets. We end up with some conclusions and future work in section 6. Tree Augmente d Naive Bayes (TAN) app ears as a nat-ural extension to the Naive Bayes classi er (Kon tka-nen et al., 1998; Langley et al., 1992; Domingos &amp; Pazzani, 1997). TAN mo dels are a restricted family of Bayesian net works in whic h the class variable has no paren ts and eac h attribute has as paren ts the class variable and at most another attribute. An example of TAN mo del can be seen in Figure 1(c).
 In this section we start introducing the notation to be used in the rest of the pap er. After that we discuss the TAN induction algorithm presen ted in (Friedman et al., 1997). Finally in this section we presen t also the impro vemen ts introduced to TAN in (Cerquides, 1999). 2.1. Formalization and Notation The notation used in the pap er is an e ort to put to-gether the di eren t notations used in (Cerquides, 1999; Hec kerman et al., 1995; Friedman et al., 1997; Meila &amp; Jaakk ola, 2000) and some con ventions in the ma-chine learning literature. 2.1.1. The Discrete Classifica tion Problem A discr ete attribute is a nite set. A discr ete do-main is a nite set of discrete attributes. We will note = f X 1 ; : : : ; X m g for a discrete domain, where X 1 ; : : : ; X m are the attributes in the domain. A classi-ed discr ete domain is a discrete domain where one of the attributes is distinguished as \class". We will use C = f A 1 ; : : : ; A n ; C g for a classi ed discrete domain. In the rest of the pap er we will refer to an attribute either as X i (when it is considered part of a discrete domain), A i (when it is considered part of a classi ed discrete domain and it is not the class) and C (when it is the class of a classi ed discrete domain). We will note as V = f A 1 ; : : : ; A n g the set of attributes in a classi ed discrete domain that are not the class. Giv en an attribute A , we will note # A as the num ber of di eren t values of A . We de ne # = # C = # C An observation x in a classi ed discrete domain C is an ordered tuple x = ( x 1 ; : : : ; x n ; x C ) 2 A 1 A n C . An unclassi e d observation S in C is an ordered tuple S = ( s 1 ; : : : ; s n ) 2 A 1 : : : A n . To be homogeneous we will abuse this notation a bit noting s
C for a possible value of the class for S. A dataset D in C is a multiset of classi ed observ ations in C . We will note N for the num ber of observ ations in the dataset. We will also note N i ( x i ) for the num-ber of observ ations in D where the value for A i is x i , N i;j ( x i ; x j ) the num ber of observ ations in D where the value for A i is x i and the value for A j is x j and simi-f noticing that f de nes a probabilit y distribution over A 1 : : : A n C .
 A classi er in a classi ed discrete domain C is a pro-cedure that given a dataset D in C and an unclassi-ed observ ation S in C assigns a class to S . 2.1.2. Bayesian Netw orks for Discrete Bayesian net works o er a solution for the discrete clas-si cation problem. The approac h is to de ne a ran-dom variable for eac h attribute in (the class is in-cluded but not distinguished at this time). We will note U = fX 1 ; : : : ; X m g where eac h X i is a random variable over its corresp onding attribute X i . We ex-tend the meaning of this notation to A i , C and V . A Bayesian network over U is a pair B = h G; i . The rst comp onen t, G , is a directed acyclic graph whose vertices corresp ond to the random variables X ; : : : ; X m and whose edges represen t direct dep en-dencies between the variables. The graph G enco des indep endence assumptions: eac h variable X i is inde-penden t of its non-descendan ts given its paren ts in G . The second comp onen t of the pair, namely , repre-sen ts the set of parameters that quan ti es the net work. It con tains a parameter i j eac h x i 2 X i and x Cartesian pro duct of every X j suc h that X j is a paren t of X i in G . i is the list of paren ts of X i in G . We will note i = U fX i g i . A Bayesian net work de nes a unique join t probabilit y distribution over U given by P
B ( x 1 ; : : : ; x m ) = The application of Bayesian net works for classi cation can be very simple. For example supp ose we have an algorithm that given a classi ed discrete domain C and a dataset D over C returns a Bayesian net work B over U = fA 1 ; : : : ; A n ; Cg where eac h A i (resp. C ) is a random variable over A i (resp. C ). Then if we are given a new unclassi ed observ ation S we can easily classify S into class argmax simple mec hanism allo ws us to see any Bayesian net-work learning algorithm as a classi er. 2.1.3. Learning with Trees Giv en a classi ed domain C we will note E the set of undirected graphs E over fA 1 ; : : : ; A n g suc h that E is a tree (has no cycles). We will use u; v 2 E instead of ( A u ; A v ) 2 E for compactness. We will note as E a directed tree for E . Every E uniquely determines the structure of a Tree Augmen ted Naiv e Bayes classi er, because from E we can construct E = E [ f ( C ; A i ) j 1 i n g as can be seen in an example in Figure 1. We note the root of a directed tree E as E (i.e. in Figure 1(b) we have that E = A 1 ). We will note as E the set of parameters that quan-tify the Bayesian net work M = h E ; E i . More con-cretely: where v j u;C ( j; i; c ) = P ( A v = j jA u = i; C = c; M ). 2.2. Learning Maxim um Lik eliho od TAN One of the measures used to learn Bayesian net works is the log likeliho od . An interesting prop erty of the TAN family is that we have an ecien t pro cedure (Friedman et al., 1997) for iden tifying the structure of the net-work whic h maximizes likeliho od. To learn the maxi-mum likeliho od TAN we should use the follo wing equa-tion to compute the parameters. It has been sho wn (Friedman et al., 1997) that equa-tion 2 leads to \over tting" the mo del. Also in (Fried-man et al., 1997) Friedman et al. prop ose to use the parameters as given by and suggest setting N 0 i j sults. Using equation 3 to x the parameters impro ves the accuracy of the classi er. In our opinion, no well founded theoretical justi cation is given for the im-pro vemen t. In the follo wing section we revisit the re-sults in (Cerquides, 1999) and sho w that we can get an alternativ e parameter xing equation with a well founded theoretical justi cation and equiv alen t classi-cation accuracy . 2.3. Learning Multinomial Sampling TAN In (Cerquides, 1999) we introduced an alternativ e ap-proac h to learning Bayesian net works whic h we named \multinomial sampling approac h" based on assuming that our dataset is a sample of a multinomial distribu-tion over A 1 : : : A n C .
 This multinomial sampling approac h was applied to TAN with the result that we should estimate the pa-rameters using: where # i = stands for the set of variables whic h are not paren ts of X i in the net work excluding X i .
 In (Cerquides, 1999) the usage of = 10 was found to be a good value after empirical tests, and the multi-nomial sampling approac h was compared to the max-imum likeliho od (equation 2) and softened maxim um likeliho od (equation 3) parameter estimations. The re-sults were that multinomial sampling is clearly better than maxim um likeliho od. When compared to soft-ened maxim um likeliho od, it was observ ed that multi-nomial sampling pro vides an equiv alen t classi cation accuracy but impro ves the qualit y of the probabilities assigned to the class. In the previous section we have review ed di eren t ways of learning a single TAN mo del from data. In this section we will dev elop a classi er based on the TAN mo del that does also tak e into accoun t the uncertain ty in mo del selection by means of decomp osable distri-butions over TANs. We start by introducing Bayesian mo del averaging, then we explain decomp osable dis-tributions over tree structures and parameters built upon the idea of decomp osable priors as prop osed by Meila and Jaak ola (Meila &amp; Jaakk ola, 2000) to end up sho wing that given a decomp osable distribution it is possible to calculate the probabilit y of an unseen observ ation and that given a prior decomp osable dis-tribution, the posterior distribution after observing a set of data is also a decomp osable distribution. We conclude the section by putting together these results to create tbma tan . 3.1. BMA Classi cation We are faced with the problem of de ning a good clas-si er for a classi ed dataset. If we accept that there is a probabilistic mo del behind the dataset, we have two alternativ es: 1. We kno w the mo del M (both structure and pa-2. We are given a set of possible mo dels M . In this In the follo wing we pro ve that if we x the set of mo d-els M to TAN mo dels and assume a decomp osable distribution as prior probabilit y distribution over the set of mo dels, the integral for P ( V = S; C = s C jD ; ) in equation 5 can be integrated in closed form. 3.2. Decomp osable Distributions over TANs In order to apply Bayesian mo del averaging, it is nec-essary to have a prior probabilit y distribution over the set of mo dels M . Decomp osable priors were intro-duced by Meila and Jaak ola in (Meila &amp; Jaakk ola, 2000) where it was demonstrated for tree belief net-works that if we assume a decomp osable prior, the posterior probabilit y is also decomp osable and can be completely determined analytically in polynomial time.
 In this section we introduce decomp osable distribu-tions over TANs, whic h are probabilit y distributions in the space M of TAN mo dels and an adaptation of decomp osable priors, as they app ear in (Meila &amp; Jaakk ola, 2000), to the task of learning TAN. Decomp osable distributions are constructed in two steps. In the rst step, a distribution over the set of di eren t undirected tree structures is de ned. Ev-ery directed tree structure is de ned to have the same probabilit y as its undirected equiv alen t. In the second step, a distribution over the set of parameters is de-ned so that it is also indep enden t on the structure. In the rest of the pap er we will assume implies a decomp osable distribution over M with hyperparam-eters ; N 0 (these hyperparameters will be explained along the dev elopmen t). Under this assumption, the probabilit y for a mo del M = h E ; E i (a TAN with xed tree structure E and xed parameters E ) is determined by:
P ( M j ) = P ( E ; E j ) = P ( E j ) P ( E j E ; ) In the follo wing sections we specify the value of P ( E j ) (decomp osable distribution over structures) and P ( E j E ; ) (decomp osable distribution over pa-rameters). 3.2.1. Decomposable Distribution over TAN One of the hyperparameters of a decomp osable dis-tribution is an n n matrix = ( u;v ) suc h that 8 u; v : 1 u; v n : u;v = v;u 0 ; v;v = 0. We can interpret u;v as a measure of how possible is un-der that the edge ( A u ; A v ) is con tained in the TAN mo del underlying the data.
 Giv en , the probabilit y of a TAN structure E is de-ned as: where Z is a normalization constan t with value: It is worth noting that P ( E j ) dep ends only on the underlying undirected tree structure E . 3.2.2. Decomposable Distribution over TAN Applying equation 1 to the case of TAN we have that P ( E j E ; ) = P ( C j E ; ) P ( A decomp osable distribution has a hyperparameter set N 0 = f N 0 A u ; c 2 C g with the constrain t that exist N 0 u;C ( i; c ), N
C ( c ), N Giv en , a decomp osable probabilit y distribution over parameters with hyperparameter N 0 is de ned by equation 9 and the follo wing set of Diric hlet distri-butions: P ( If the conditions in equations 6, 7, 8, 9, 10, 11, 12, 13, 14 and 15 hold, we will say that P ( M j ) follo ws a de-comp osable distribution with hyperparameters ; N 0 : 3.3. Calculating Probabilities with Assume that the data is generated by a TAN mo del and that P ( M j ) follo ws a decomp osable distribution with hyperparameters ; N 0 . We can calculate the probabilit y of an observ ation S; s C given by aver-aging over the set of TAN mo dels (equation 5). Let Q : R n n ! R n 1 n 1 . For any real n n matrix we de ne Q ( ) as the rst n 1 lines and columns of the matrix Q ( ) where The integral for P ( V = S; C = s C j ) can be calculated in closed form by applying the matrix tree theorem and expressed in terms of the previously introduced Q as: where The pro of for this result app ears in (Cerquides &amp; Lopez de Mantaras, 2003b). 3.4. Learning with Decomp osable Assume that the data is generated by a TAN mo del and that P ( M j ) follo ws a decomp osable distribu-tion with hyperparameters , N 0 . Then, P ( M jD ; ), the posterior probabilit y distribution after observing a dataset D is a decomp osable distribution with param-eters , N 0 given by: where The pro of app ears in (Cerquides &amp; Lopez de Mantaras, 2003b). 3.5. Putting it all Together Putting together the results from sections 3.3 and 3.4 we can easily design a classi er based on decomp os-able distributions over TANs. The classi er works as follo ws: when given a dataset D , it assumes that the data is generated from a TAN mo del and assumes a decomp osable distribution as prior over the set of mo d-els. Applying the result from section 3.4, the posterior distribution over the set of mo dels is also a decomp os-able distribution and applying the result of section 3.3 this decomp osable posterior distribution can be used to calculate the probabilit y of any observ ation S; s C . When given an unclassi ed observ ation S , it can just calculate the probabilit y P ( V = S; C = s C jD ; ) for eac h possible class s C 2 C and classify S in the class with highest probabilit y.
 We have men tioned that the classi er assumes a de-comp osable distribution as prior. Ideally , this prior will be xed by an exp ert that kno ws the classi cation domain. Otherwise, we have to pro vide the classi er with a way for xing the prior distribution hyperpa-rameters without kno wledge about the domain. In this case the prior should be as \non-informativ e" as pos-sible in order for the information coming from D to dominate the posterior by the e ects of equations 20 and 21. We have translated this requisite into equa-tions 23 and 24: 8 u; v ; 1 u 6 = v n ; 8 j 2 A v ; 8 i 2 A u ; 8 c 2 C De ning as in equation 23 means that we have the same amoun t of belief for any edge being in the TAN structure underlying the data. Fixed u; v , equation 24 assigns the same probabilit y to any ( j; i; c ) suc h that j 2 A v , i 2 A u and c 2 C and the value assigned is coheren t with the multinomial sampling approac h. is an \equiv alen t sample size" for the prior in the sense of Hec kerman et al. in (Hec kerman et al., 1995). In our exp erimen ts we have xed = 10. In the follo wing tbma tan will refer to the classi er describ ed in this section. tbma tan can theoretically be implemen ted by an al-gorithm with O ( N n 2 ) learning time and O (# C n 3 ) time for classifying a new observ ation. In spite of that, a straigh tforw ard implemen tation of tbma tan , even when accomplishing these complexit y bounds, will not yield accurate results, specially for large datasets. This is due to the fact that the calculations that need to be done in order to classify a new observ ation in-clude the computation of a determinan t (in equation 17) that happ ens to be ill-conditioned. Even worse, the determinan t gets more and more ill-conditioned as the num ber of observ ations in the dataset increases. This forces the oating point accuracy that we have to use to calculate these determinan ts to dep end on the dataset size. We have calculated the determinan ts by means of NTL (Shoup, 2003), a library that allo ws us to calculate determinan ts with the desired precision arithmetic. This solution mak es the time for classify-ing a new observ ation gro w faster than O (# C n 3 ), and hence mak es the practical application of the al-gorithm dicult in situations where it is required to classify a large set of unclassi ed data.
 We analyzed what mak es the determinan t being ill-conditioned and concluded that it is due to the W u;v factors given by equation 22. The factor W u;v could be interpreted as \ho w much the dataset D has changed the belief in that there is a link between u and v in the TAN mo del generating the data". The problems relies in the fact that W u;v are easily in the order of 10 200 for a dataset with 1500 observ ations. Furthermore, the pro viding the ill-condition of the determinan t. In order to overcome this problem, we prop ose to postpro cess the factors W u;v computed by equation 22 by means of a transformation that limits them to lie in the in-terv al [10 K ; 1] where K is a constan t that has to be xed dep ending on the oating point accuracy of the mac hine. In our implemen tation we have used K = 5. The transformation works as depicted in gure 2 and is describ ed in detail by the follo wing equations: Using f W u;v instead of W u;v to calculate the posterior hyperparameters u;v has the follo wing prop erties: 1. It is harder to get get ill-conditioned determi-2. It preserv es the relativ e ordering of the W u;v . 3. It does not exaggerate relativ e di erences in be-The posterior hyperparameters u;v can be interpreted as a represen tation of the a posteriori belief in the ex-istence of an edge ( u; v ) in the TAN structure. Using f W u;v , given the prop erties stated, means being more conserv ativ e in the structure learning pro cess, because the beliefs will be con ned to the interv al [10 K ; 1] whic h imp edes the represen tation of extreme proba-bilit y di erences between edges. We can interpret the transformation as applying some stubb ornness to the structure learning pro cess. Applying this transforma-tion allo ws us to implemen t an appro ximation of tb-matan that does not require the use of special oating point accuracy computations. We will refer to this ap-pro ximation of tbma tan as sstbma tan (from Struc-ture Stubb orn tbma tan ).
 It is worth noting that the problem describ ed in this section does only a ect the classi cation time. The learning pro cess for tbma tan does not need high pre-cision arithmetics. The learning time complexit y for tbma tan , O ( N n 2 ), is the same as the one for TAN. In spite of that, in practice, TAN learning time would be somewhat longer because the learning stage for tb-matan (calculating every N v;u;C ( j; i; c )) is only the rst step of the TAN learning pro cess. We tested four algorithms over 16 datasets from the Irvine rep ository (Blak e et al., 1998). The dataset characteristics are describ ed in Table 1. To discretize con tinuous attributes we used equal frequency dis-cretization with 5 interv als. For eac h dataset and algorithm we tested both accuracy and Log Scor e . Log Scor e is calculated by adding the min us logarithm of the probabilit y assigned by the classi er to the cor-rect class and gives an idea of how well the classi er is estimating probabilities (the smaller the score the better the result). If we name the test set D 0 we have
Log Scor e ( M; D 0 ) = For the evaluation of both error rate and Log Scor e we used 10 fold cross validation. We tested the algorithm with the 10%, 50% and 100% of the learning data for eac h fold, in order to get an idea of the in uence of the amoun t of data in the beha viors of both error rate and Log Scor e for the algorithm. In order to evaluate the statistical signi cance of the di erences we performed a paired t-test at 5%.
 Detailed exp erimen tal results can be found in (Cerquides &amp; Lopez de Mantaras, 2003b).
 The classi ers under comparison are: tbma tan classi cation times are very large for datasets with a large num ber of instances. For datasets over 5000 instances we have skipp ed the execution of tbma tan .
 5.1. Interpretation of the Results Statistical signi cance results can be seen in tables 2 and 3 where eac h entry in the table con tains the num ber of datasets for whic h the error rate (resp. Log Scor e ) for the classifer in the left column was bet-ter than the same measure for the classi er on the top row in a statistically signi can t way. For example, the impro vemen t in Log Scor e pro vided by sstbma tan with resp ect to tan+ms is statistically signi can t for 14 datasets when taking the 10% of the training data, for 11 datasets when taking the 50% and for 6 datasets when taking all of the learning data.
 In man y cases tbma tan impro ves both accuracy and Log Scor e with resp ect to tan+ms . The average rel-ativ e impro vemen t is around 10% for error rate and sligh tly higher for Log Scor e . The percen tage of im-pro vemen t is higher as we reduce the amoun t of learn-ing data. This is understandable, because it is reason-able to think that if we have enough data, the posterior is likely to be concen trated around the tree learned by tan+ms . sstbma tan performs even sligh tly better than tb-matan for man y datasets, so we can accept that the appro ximation introduced in section 4 is good for datasets of this size. Finally , if we compare sstb-matan and tan+ms we can see that its relativ e be-havior is very similar to the one of tbma tan with tan+ms . We have introduced tbma tan a classi er based on TAN, decomp osable distributions and Bayesian mo del averaging. We have seen that its implemen tation leads to the calculation of ill-conditioned determinan ts and have prop osed to use an appro ximated implemen ta-tion: sstbma tan . sstbma tan is, to the best of our kno wledge, the most accurate classi er rep orted with a learning time lin-ear on the num ber of observ ations of the dataset. The accuracy increase comes at the price of increasing the classi cation time, making it cubic on the num ber of attributes. The algorithm is anytime and incremen-tal: as long as the dataset observ ations are pro cessed randomly , we can stop the learning stage anytime we need, perform some classi cations and then con tinue learning at the only (ob vious) cost of the lower accu-racy of the classi cations performed in the middle of the learning pro cess. These characteristics mak e the algorithm very suitable for datasets with a large num-ber of instances.
 Being able to calculate some measure of the con-cen tration of the posterior distribution around the TAN learned by tan+ms (that is, some sort of \vari-ance") will probably allo w us to determine beforehand whether tbma tan will pro vide signi can t impro ve-men t over tan+ms in a dataset.
 Finally , we think that all of the classi ers review ed in (Friedman et al., 1997) that are based on the Cho w and Liu algorithm (Cho w &amp; Liu, 1968) can bene t from an impro vemen t similar to the one seen here by the use of decomp osable distributions and Bayesian mo del averaging. Formalizing the dev elopmen t for these classi ers and performing the empirical tests re-mains as future work.
 Blak e, C., Keogh, E., &amp; Merz, C. (1998). UCI rep osi-tory of mac hine learning databases.
 Cerquides, J. (1999). Applying General Bayesian Tech-niques to Impro ve TAN Induction. Proceedings of the International Confer enc e on Know ledge Disc ov-ery and Data Mining, KDD99 .
 Cerquides, J., &amp; Lopez de Mantaras, R. (2003a). The indi eren t naiv e bayes classi er. Proceedings of the 16th International FLAIRS Confer enc e .
 Cerquides, J., &amp; Lopez de Mantaras, R. (2003b).
Tractable bayesian learning of tree augmente d naive bayes classi ers (Technical Rep ort IIIA-2003-04). http://www.iiia.csic.es/ man taras/Rep ortI IIA-TR-2003-04.p df.
 Cho w, C., &amp; Liu, C. (1968). Apro ximatting Discrete Probabilit y Distributions with Dep endence Trees.
IEEE Transactions on Information The ory , 14 , 462{ 467.
 Domingos, P., &amp; Pazzani, M. (1997). On the Optimal-ity of the Simple Bayesian Classi er under Zero-One Loss. Machine Learning , 29 , 103{130.
 Friedman, N., Geiger, D., &amp; Goldszmidt, M. (1997).
Bayesian net work classi ers. Machine Learning , 29 , 131{163.
 Hec kerman, D., Geiger, D., &amp; Chic kering, D. (1995).
Learning bayesian net works: The com bination of kno wledge and statistical data. Machine Learning , 20 , 197{243.
 Hoeting, J., Madigan, D., Raftery , A., &amp; Volinsky , C. (1998). Bayesian model aver aging (Technical Re-port 9814). Departmen t of Statistics. Colorado State Univ ersit y.
 Kon tkanen, P., Myllymaki, P., Silander, T., &amp; Tirri,
H. (1998). Bayes Optimal Instance-Based Learn-ing. Machine Learning: ECML-98, Proceedings of the 10th Eur opean Confer enc e (pp. 77{88). Springer-Verlag.
 Langley , P., Iba, W., &amp; Thompson, K. (1992). An Analysis of Bayesian Classi ers. Proceedings of the
Tenth National Confer enc e on Arti cial Intel ligenc e (pp. 223{228). AAAI Press and MIT Press.
 Meila, M., &amp; Jaakk ola, T. (2000). Tractable bayesian learning of tree belief networks (Technical Rep ort CMU-RI-TR-00-15). Rob otics Institute, Carnegie Mellon Univ ersit y, Pittsburgh, PA.
 Shoup, V. (2003). NTL: A library for doing num ber
