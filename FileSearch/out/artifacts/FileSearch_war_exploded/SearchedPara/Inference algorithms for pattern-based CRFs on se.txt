 Rustem Takhanov takhanov@mail.ru Institute of Science and Technology (IST), Austria Vladimir Kolmogorov vnk@ist.ac.at Institute of Science and Technology (IST), Austria This paper addresses the sequence labeling (or the sequence tagging ) problem: given an observation z (which is usually a sequence of n values), infer label-ing x = x 1 ...x n where each variable x i takes values in some finite domain D . Such problem appears in many domains such as text and speech analysis, signal analysis, and bioinformatics.
 One of the most successful approaches for tackling the problem is the Hidden Markov Model (HMM). The k th order HMM is given by the probability distribution p ( x | z ) = 1 Z exp { X  E ( x | z ) } with the energy function where E k = { ( i,i + k ) | i  X  [1 ,n  X  k ] } and x i : j is the substring of x from i to j . A popular generaliza-tion is the Conditional Random Field model (Lafferty et al., 2001) that allows all terms to depend on the full observation z : We study a particular variant of this model called a pattern-based CRF . It is defined via where  X  is a fixed set of non-empty words, |  X  | is the length of word  X  and [  X  ] is the Iverson bracket . If we take  X  = D  X  D k then (3) becomes equivalent to (2); thus we do not loose generality (but gain more flexi-bility).
 Intuitively, pattern-based CRFs allow to model long-range interactions for selected subsequences of labels. This could be useful for a variety of applications: in part-of-speech tagging patterns could correspond to certain syntactic constructions or stable idioms; in pro-tein secondary structure prediction -to sequences of dihedral angles corresponding to stable configuration such as  X  -helixes; in gene prediction -to sequences of nucleatydes with supposed functional roles such as  X  X xon X  or  X  X ntron X , specific codons, etc.
 Inference This paper focuses on inference algorithms for pattern-based CRFs. The three standard inference tasks are (i) computing the partition function Z = P x exp { X  E ( x | z ) } ; (ii) computing marginal probabili-(iii) computing MAP, i.e. minimizing energy (3). The complexity of solving these tasks is discussed below. We denote L = P  X   X   X  |  X  | to be total length of patterns and ` max = max  X   X   X  |  X  | to be the maximum length of a pattern.
 A naive approach is to use standard message passing techniques for an HMM of order k = ` max  X  1. However, they would take O ( n | D | k +1 ) time which would become impractical for large k . More efficient algorithms with respectively were given by (Ye et al., 2009). 1 Our first contribution is to improve this to O ( nL ), O ( nL` max ) and O ( nL  X  min {| D | , log( ` max + 1) } ) respectively (more accurate estimates are given in the next section). We also give an algorithm for sampling from the dis-tribution p ( x | z ). Its complexity is either (i) O ( nL ) per sample, or (ii) O ( n ) per sample with an O ( nL | D | ) preprocessing (assuming that we have an oracle that produces independent samples from the uniform dis-tribution on [0 , 1] in O (1) time).
 Finally, we consider the case when all costs  X   X  ij are non-positive. (Komodakis &amp; Paragios, 2009) gave an O ( nL ) technique for minimizing energy (3) in this case. We present a modification that has the same worst-case complexity but can beat the algorithm of (Komodakis &amp; Paragios, 2009) in the best case. Related work The works of (Ye et al., 2009) and (Ko-modakis &amp; Paragios, 2009) are probably the most re-lated to our paper. The former applied pattern-based CRFs to the handwritten character recognition prob-lem and to the problem of identification of named enti-ties from texts. The latter considered a pattern-based CRF on a grid for a computer vision application; the MAP inference problem in (Komodakis &amp; Paragios, 2009) was converted to sequence labeling problems by decomposing the grid into thin  X  X tripes X . (Qian et al., 2009) considered a more general formu-lation in which a single pattern is characterized by a set of strings rather than a single string  X  . They pro-posed an exact inference algorithm and applied it to the OCR task and to the Chinese Organization Name Recognition task. However, their algorithm could take time exponential in the total lengths of input patterns; no subclasses of inputs were identified which could be solved in polynomial time.
 A different generalization (for non-sequence data) was proposed in (Rother et al., 2009). Their inference pro-cedure reduces the problem to the MAP estimation in a pairwise CRF with cycles, which is then solved with approximate techniques such as BP, TRW or QPBO. This model was applied to the texture restora-tion problem. (Nguyen et al., 2011) extended algorithms in (Ye et al., 2009) to the Semi-Markov model (Sarawagi &amp; Cohen, 2004). We conjecture that our algorithms can be ex-tended to this case as well, and can yield a better com-plexity compared to (Nguyen et al., 2011). First, we introduce a few definitions.  X  A pattern is a pair  X  = ([ i,j ] ,x ) where [ i,j ] is an interval in [1 ,n ] and x = x i ...x j is a sequence over alphabet D indexed by integers in [ i,j ] ( j  X  i  X  1).
The length of  X  is denoted as |  X  | = | x | = j  X  i + 1.  X  Symbols  X   X   X  denotes an arbitrary word or pattern (possibly the empty word  X  or the empty pattern  X  s , ([ s + 1 ,s ] , X  ) at position s ). The exact meaning will always be clear from the context. Similary,  X + X  denotes an arbitrary non-empty word or pattern.  X  The concatenation of patterns  X  = ([ i,j ] ,x ) and  X  = ([ j + 1 ,k ] ,y ) is the pattern  X  X  , ([ i,k ] ,xy ).
Whenever we write  X  X  we assume that it is defined, i.e.  X  = ([  X  ,j ] ,  X  ) and  X  = ([ j + 1 ,  X  ] ,  X  ) for some j .  X  For a pattern  X  = ([ i,j ] ,x ) and interval [ k,` ]  X  [ i,j ], the subpattern of  X  at position [ k,` ] is the pattern
If k = i then  X  k : ` is called a prefix of  X  . If ` = j then  X  k : ` is a suffix of  X  .  X  If  X  is a subpattern of  X  , i.e.  X  =  X  k : ` for some [ k,` ], then we say that  X  is contained in  X  . This is equivalent to the condition  X  =  X   X   X  . with interval [ i,j ]. We typically use letter x for pat-terns in D 1: s and letters  X , X ,... for other patterns.
Patterns x  X  D 1: s will be called partial labelings .  X  For a set of patterns  X  and index s  X  [0 ,n ] we denote
 X  s to be the set of patterns in  X  that end at position s :  X  s = { ([ i,s ] , X  )  X   X  } .  X  For a pattern  X  let  X   X  be the prefix of  X  of length |  X  | X  1; if  X  is empty then  X   X  is undefined.
 We will consider the following general problem. Let  X   X  be the set of patterns of words in  X  placed at all possible positions:  X   X  = { ([ i,j ] , X  ) |  X   X   X ) } . Let ( R,  X  ,  X  ) be a commutative semiring with elements O , 1  X  R which are identities for  X  and  X  respectively. Define the cost of pattern x  X  D i : j via where c  X   X  R are fixed constants. (Throughout the paper we adopt the convention that operations  X  and  X  over the empty set of arguments give respectively O and 1 , and so e.g. f (  X  s ) = 1 .) Our goal is to compute Example 1 If ( R,  X  ,  X  ) = ( R , + ,  X  ) then problem (5) is equivalent to computing the partition function for the energy (3) , if we set c ([ i,j ] , X  ) = exp { X   X   X  ij Example 2 If ( R,  X  ,  X  ) = ( R , min , +) where R , R  X  X  +  X  X  then we get the problem of minimizing en-The complexity of our algorithms will be stated in terms of the following quantities:  X  P = |{  X  | X   X   X  X  X   X  , X  6 =  X  }| is the number of distinct non-empty prefixes of words in  X . Note that P  X  L .  X  P 0 = |{  X  | X   X  +  X   X  }| is the number of distinct proper prefixes of words in  X . There holds P P 0  X  [1 , | D | ].
If  X  = D 1  X  D 2  X  ...  X  D k then P P 0 = | D | . If  X  is a sparse random subset of the set above then P P 0  X  1.  X  I ( X ) = {  X  | X   X   X  ,  X   X   X   X  , X  6 =  X  } is the set of non-empty words which are both prefixes and suffixes of some words in  X . Note that  X   X  I ( X ) and | I ( X ) | X  P . We will present 6 algorithms (omitting proofs due to space limitations; all proofs are given in (Takhanov &amp; Kolmogorov, 2012) ): Sec. 3 :  X ( nP ) algorithm for the case when ( R,  X  ,  X  ) is a ring, i.e. it has operation that satisfies ( a b )  X  b = a for all a,b  X  R . This holds for the semiring in Example 1 (but not for Example 2).
 Sec. 4 :  X ( nP ) algorithm for sampling. Alternatively, it can be implemented to produce independent samples in  X ( n ) time per sample with a  X ( nP | D | ) preprocess-ing.
 Sec. 5 : O ( n P  X   X  I ( X ) |  X  | ) algorithm for computing marginals for all patterns  X   X   X   X  .
 Sec. 6 :  X ( nP 0 | D | ) algorithm for a general commu-tative semiring, which is equivalent to the algorithm in (Ye et al., 2009). It will be used as a motivation for the next algorithm.
 Sec. 7 : O ( nP log P ) algorithm for a general commuta-tive semiring; for the semiring in Example 2 the com-plexity can be reduced to O ( nP log( ` max + 1)). Sec. 8 in (Takhanov &amp; Kolmogorov, 2012): O ( nP ) algorithm for the case ( R,  X  ,  X  ) = ( R , min , +), c  X  for all  X   X   X   X  .
 All algorithms will have the following structure. Given the set of input patterns  X   X  , we first construct another set of patterns  X ; it will typically be either the set of prefixes or the set of proper prefixes of patterns in  X   X  This can be done in a preprocessing step since sets  X  s will be isomorphmic (up to a shift) for indexes s that are sufficiently far from the boundary. (Recall that  X  s is the set of patterns in  X  that end at position s .) Then we recursively compute messages M s (  X  ) for  X   X   X  s which have the following interpretation: M s (  X  ) is the sum ( X   X   X ) of costs f ( x ) over a certain set of partial labelings of the form x =  X   X   X  D 1: s . In some of the algorithms we also compute messages W s (  X  ) which is the sum of f ( x ) over all partial labelings of the form x =  X   X   X  D 1: s .
 Graph G [ X  s ] The following construction will be used throughout the paper. Given a set of patterns  X  and index s , we define G [ X  s ] = ( X  s ,E [ X  s ]) to be a directed graph with the following set of edges: (  X , X  ) belongs to E [ X  s ] for  X , X   X   X  s if  X  is a proper suffix of  X  (  X  = +  X  ) and  X  s does not have an  X  X ntermediate X  that graph G [ X  s ] is a directed forest. If  X  s  X   X  s then G [ X  s ] is connected and therefore is a tree. In this case we treat  X  s as the root. An example is shown in Fig. 1. Computing partial costs Recall that f (  X  ) for a pattern  X  is the cost of all patterns inside  X  (eq. (4)). We also define  X  (  X  ) to be the cost of only those pat-terns that are suffixes of  X  : Quantities  X  (  X  ) and f (  X  ) will be heavily used by the algorithms below; let us show how to compute them efficiently.
 Lemma 1. Let  X  be a set of patterns with  X  s  X   X  for all s  X  [0 ,n ] . Values  X  (  X  ) for all  X   X   X  can be com-puted using O ( |  X  | ) multiplications ( X   X   X ). The same holds for values f (  X  ) assuming that  X  is prefix-closed, i.e.  X   X   X   X  for all non-empty patterns  X   X   X  . Sets of partial labelings Let  X  s be a set of patterns that end at position s . Assume that  X  s  X   X  s . For a pattern  X   X   X  s we define It can be seen that sets X s (  X  ;  X  s ) are disjoint, and their union over  X   X   X  s is D 1: s . Furthermore, there holds X (  X  ;  X  s ) = { x  X  X  s (  X  ) | x 6 =  X   X   X   X  = +  X   X   X  s } (9) We will use eq. (9) as the definition of X s (  X  ;  X  s ) in the case when  X  /  X   X  s . In this section we give an algorithm for computing quantity (5) assuming that ( R,  X  ,  X  ) is a ring. This can be used, in particular, for computing the partition function. We will assume that D  X   X ; we can always add D to  X  if needed 2 .
 First, we select set  X  as the set of prefixes of patterns in  X   X  : We will compute the following quantities for each s  X  [0 ,n ],  X   X   X  s : It is easy to see that for  X   X   X  s the following equalities relate M s and W s : These relations motivate the following algorithm. Since |  X  s | = P + 1 for indexes s that are sufficiently far from the boundary, its complexity is  X ( nP ) assum-ing that values  X  (  X  ) in eq. (13a) are computed using Lemma 1.
 Algorithm 1 Computing Z = L 1: initialize messages: set W 0 (  X  0 ) := O 2: for each s = 1 ,...,n traverse nodes  X   X   X  s of tree
M s (  X  ) :=  X  (  X  )  X 
W s (  X  ) := M s (  X  )  X  M 3: return Z := W n (  X  n ) Theorem 2. Algorithm 1 is correct, i.e. it returns the correct value of Z = L x F ( x ) . In this section consider the semiring ( R,  X  ,  X  ) = (
R , + ,  X  ) from Example 1. We assume that all costs c  X  are strictly positive. We present an algorithm for sam-pling labelings x  X  D 1: n according to the probability distribution p ( x ) = f ( x ) /Z .
 As in the previous section, we assume that D  X   X , and define  X  to be the set of prefixes of patterns in  X   X  (eq. (10)). For a node  X   X   X  s let T s (  X  ) be the set of nodes in the subtree of G [ X  s ] rooted at  X  , with  X   X  T s (  X  )  X   X  s . For a pattern  X   X   X  s +1  X  X   X  s +1 define set We can now present the algorithm.
 Algorithm 2 Sample x  X  p ( x ) = f ( x ) /Z 1: run Algorithm 1 to compute messages M s (  X  ) for 2: sample  X  n  X   X  n with probability p (  X  n )  X  M n (  X  n 3: for s = n  X  1 ,..., 1 sample  X  s  X   X  s (  X  s +1 ) with 4: return labeling x with x s : s = (  X  s ) s : s for s  X  [1 ,n ] We say that step s of the algorithm is valid if either (i) s = n , or (ii) s  X  [1 ,n  X  1], step s + 1 is valid,  X  s +1 6 =  X  s +1 and M s (  X  ) &gt; 0 for some  X   X   X  s (  X  (This is a recursive definition.) Clearly, if step s is valid then line 3 of the algorithm is well-defined. Theorem 3. (a) With probability 1 all steps of the algorithm are valid. (b) The returned labeling x  X  D 1: n is distributed according to p ( x ) = f ( x ) /Z . Complexity Assume that we have an oracle that produces independent samples from the uniform dis-tribution on [0 , 1] in O (1) time.
 The main subroutine performed by the algorithm is sampling from a given discrete distribution. Clearly, this can be done in  X ( N ) time where N is the number of allowed values of the random variable. With a  X ( N ) preprocessing, a sample can also be produced in O (1) time by the so-called  X  X lias method X  (Vose, 1991). This leads to two possible complexities: (i)  X ( nP ) (without preprocessing); (ii)  X ( n ) per sample (with preprocessing). Let us discuss the complexity of this preprocessing. Running Algorithm 1 takes  X ( nP ) time. After that, for each  X   X   X  s +1 we need to run the linear time procedure of (Vose, 1991) for distributions p (  X  )  X  M s (  X  ) , X   X   X  s (  X  s +1 ). The following theorem implies that this takes  X ( nP | D | ) time.
 Theorem 4. There holds P  X   X   X  |  X  To summarize, we showed that with a  X ( nP | D | ) pre-processing we can compute independent samples from p ( x ) in  X ( n ) time per sample. In this section we again consider the semiring ( R,  X  ,  X  ) = ( R , + ,  X  ) from Example 1 where all costs c  X  are strictly positive, and consider a probablity dis-tribution p ( x ) = f ( x ) /Z over labelings x  X  D 1: n . For a pattern  X  we define We also define the set of patterns Note that  X   X   X   X  and |  X  s | = | I ( X ) | for indexes s that are sufficiently far from the boundary. We will present an algorithm for computing Z (  X  ) for all pat-terns  X   X   X  in time O ( n P  X   X  I ( X ) |  X  | ). Marginal prob-abilities of a pattern-based CRF can then be computed In the previous section we used graph G [ X  s ] for a set of patterns  X  s ; here we will need an analogous but a slightly different construction for patterns in  X . For patterns  X , X  we write  X  v  X  if  X  =  X   X   X  . If we have  X  = +  X  + then we write  X  @  X  .
 Now consider  X   X   X . We define  X (  X  ) to be the set of patterns  X   X   X  such that  X  @  X  and there is no other pattern  X   X   X  with  X  @  X  v  X  .
 Our algorithm is given below. In the first step it runs Algorithm 1 from left to right and from right to left; as a result, we get forward messages messages  X  X  X 
W j (  X  ) = X Algorithm 3 Computing values Z (  X  ) 1: run Algorithm 1 in both directions to get messages  X  X  X  2: for  X   X   X  (in the order of decreasing |  X  | ) set The correctness of the algorithm is proved in (Takhanov &amp; Kolmogorov, 2012). Let us discuss its complexity. We claim that all values f (  X  ) used by the algorithm can be computed in O ( n ( P + S )) time where P and S are respectively the number of distinct non-empty prefixes and suffixes of words in  X . Indeed, we first compute these values for patterns in the set  X  X  X   X  , {  X  | X   X   X   X   X   X  } ; by Lemma 1, this takes O ( nP ) time. This covers values f (  X  ) used in eq. (19a). As for the value in eq. (19b) for pattern  X  = ([ i,j ] ,  X  )  X   X , we can use the formula where  X  c  X  = c  X  if  X   X   X   X  and  X  c  X  = 1 otherwise, and The latter values can be computed in O ( n ( P + S )) time by applying Lemma 1 in the forward and backward directions. (In fact, there were already computed when running Algorithm 1.) We showed that step 1 can be implemented in O ( n ( P + S )) time; let us analyze step 2. The following lemma implies that it performs O ( n P  X   X  I ( X ) |  X  | ) arith-metic operations; since P  X   X  I ( X ) |  X  |  X  P  X   X   X  |  X  |  X  max { P,S } , we then get that the overall complexity Lemma 5. For each  X   X   X  there exist at most 2 |  X  | patterns  X   X   X  such that  X   X   X (  X  ) .
 Remark 1. An alternative method for comput-In this section and in the next one we consider the case of a general commutative semiring ( R,  X  ,  X  ) (without assuming the existence of an inverse operation for  X  ). This can be used for computing MAP in CRFs con-taining positive costs c  X  . The algorithm closely resem-bles the method in (Ye et al., 2009); it is based on the same idea and has the same complexity. Our primary goal of presenting this algorithm is to motivate the O ( nP log( ` max + 1)) algorithm for the MAP problem given in the next section.
 First, we select  X  as the set of proper prefixes of pat-terns in  X   X  : For each  X   X   X  s we will compute message In order to go from step s  X  1 to s , we will use an extended set of patterns b  X  s : It can be checked that In step s we compute values M s (  X  ) in eq. (22) for all  X   X  b  X  s . Note, we now use the generalized definition of X (  X  ;  X  s ) (eq. (9)) since we may have  X  /  X   X  s . After completing step s , messages M s (  X  ) for  X   X  b  X  s  X   X  s be discarded.
 Our algorithm is given below. We have | b  X  s | = P 0 | D | +1 for indexes s that are sufficiently far from the bound-ary, and thus the algorithm X  X  complexity is  X ( nP 0 | D | ) (if Lemma 1 is used for computing values  X  (  X  )). Algorithm 4 Computing Z = L x  X  D 1: n f ( x ) 1: initialize messages: set M 0 (  X  0 ) := O 2: for each s = 1 ,...,n traverse nodes  X   X  b  X  s of tree 3: return Z := L  X   X   X  Theorem 6. Algorithm 4 is correct.
 Remark 2 As we already mentioned, Algorithm 4 resem-In the previous section we presented an algorithm for a general commutative semiring with complexity O ( nP 0 | D | ). In some applications the size of the input alphabet can be very large (e.g. hundreds or thou-sands), so the technique may be very costly. Below we present a more complicated version with complexity O ( nP log P ). If ( R,  X  ,  X  ) = ( R , min , +) then this can be reduced to O ( nP log( ` max + 1)) using the algorithm for Range Minimum Queries by (Berkman &amp; Vishkin, 1993). We assume that D  X   X .
 We will use the same definitions of sets  X  s and b  X  s as in the previous section, and the same intepretation of messages M s (  X  ) given by eq. (22). We need to solve the following problem: given messages M s  X  1 (  X  ) for  X   X   X  s  X  1 , compute messages M s (  X  ) for  X   X   X  s . Recall that in the previous section this was done by computing messages M s (  X  ) for patterns in the ex-tended set b  X  s of size O ( P 0 | D | ). The idea of our modi-fication is to compute these messages only for patterns in the set  X  s where Note that |  X   X  s | X  P + 1. Patterns in  X  s will be called special . To define them, we will use the following no-tation for a node  X   X  b  X  s :  X  b
 X  s (  X  ) is the set of children of  X  in the tree G [ b  X   X  b
T s (  X  ) is the set of nodes in the subtree of G [ rooted at  X  . We have  X   X  b T s (  X  )  X  b  X  s . We now define set  X  s as follows: pattern  X   X  b  X  s special if either (i)  X   X   X   X  s , or (ii)  X  has at least two children  X  1 , X  2  X  b  X  s (  X  ) such that subtree b T s (  X  { 1 , 2 } contains a pattern from  X   X  s , i.e. b T s (  X  i The set of remaining patterns b  X  s  X   X  s will be split into two sets A s and B s as follows:  X  A s is the set of patterns  X   X  b  X  s  X   X   X  s such that subtree b T s (  X  ) does not contain patterns from  X   X  s .  X  B s is the set of patterns  X   X  b  X  s  X   X   X  s such that  X  has exactly one child  X  in G [ b  X  s ] for which b T s (  X  )  X   X  Clearly, b  X  s is a disjoint union of A s , B s and  X  s . Consider a node  X   X  B s . From the definition,  X  has B s  X   X  s . If this child does not belong to  X  s , then it belongs to B s and the same argument can be repeated for it. By following such links we eventually get to a node in  X  s ; the first such node will be denoted as  X   X  . We will need two more definitions. For an index t and patterns  X , X  ending at position t with  X  = +  X  we denote We can now formulate the structure of the algorithm. Algorithm 5 Computing Z = L x  X  D 1: n f ( x ) 1: initialize messages: set M 0 (  X  ) := O 2: for each s = 1 ,...,n traverse nodes  X   X   X  s of tree 3: return Z := L  X   X   X  To fully specify the algorithm, we still need to describe how we compute quantities A s (  X  ) and B s (  X  ) defined by eq. (29a) and (29b). This is addressed by the the-orem below.
 Theorem 7. (a) Algorithm 5 is correct. (b) There holds |  X  s | X  2 |  X   X  s | X  1  X  2 P + 1 . (c) Let h be the maximum depth of tree G [ X  s  X  1 ] . (Note, h  X  ` max + 1 .) With an O ( P 0 log h ) prepro-cessing, values V s  X  1 (  X , X  ) for any  X , X   X   X  s  X  1 with  X  = +  X  can be computed in O (log h ) time. (d) Values A s (  X  ) for all  X   X   X  s can be computed in O ( P log P ) time, or in O ( P ) time when ( R,  X  ,  X  ) = (
R , min , +) .
 Clearly, the theorem implies that the algorithm can be implemented in O ( nP log P ) time, or in O ( nP log( ` max +1)) time when ( R,  X  ,  X  ) = ( R , min , +). To see this, observe that the sum in (29b) is effectively over a subset of children of  X  in the tree G [ X  s ] (see Fig. 2), and this tree has size O ( P ). Experimental evaluation of pattern-based CRF on dif-ferent tasks can be found in (Ye et al., 2009; Qian et al., 2009; Nguyen et al., 2011). Here we consider the prob-lem of protein backbone dihedral angles prediction, and compare with the HMMSTR method in (Bystroff et al., 2000) which is often used as a baseline method. It is based on the HMM approach where the states of an HMM ( X  X -sites X ) are computed by clustering fre-quently occuring sequence-structure motifs.
 We learn our pattern-based CRF using the Max-Margin approach ( X  X truct-SVM X ) (Tsochantaridis et al., 2004); it calls MAP inference as a subroutine. Experimental data was taken from the PDB database ( www.pdb.org ). The resulting sample contained 602 all-alpha proteins. Each protein in dataset is given as a pair of words: an amino-acid sequence z and a labeling x . Each amino-acid is labeled by two angles  X , X  . We discretized each angle into T = 18 levels with the step of 20 degrees, and added a special value  X  X nknown X  (since it is sometimes present in the data); thus, the label set has size | D | = ( T + 1) 2 = 361. We selected a set A of triplets a = ( z,k,x ) where x  X  D k is a subsequence of labels, k  X  [1 , | z | ] and z is the amino-acid at the k -th position of the correspond-ing amino-acids subsequence. 3 We then considered the distribution where h a ( x,z ) is the number of occurences of triplets a consistent with x and z .
 We use the Hamming loss function:  X ( x,x 0 ) = P i H ( x i ,x angles  X  and  X  (recall that x i = (  X  i , X  i )). 4 Note that struct-SVM requires the maximization of  X  ( x,x t ) + P by algorithms in sections 6 or 7.
 The sample set was divided into two subsets: a train-ing sample (500) and test sample (102). The training sample was used to train parameters for different regu-larization parameters C of struct-SVM and subsequent choice of the best one, and the resulting prediction ac-curacy was assessed on the test set. As a measure of ac-curacy, we used the MDA value from (Bystroff et al., 2000), which is the fraction of residues in segments of length 8 with no greater than 120 maximum deviation between observed and predicted backbone torsion an-gles. We obtained the result MDA = 59 . 0%. This approximately corresponds to the result of HMMSTR (57.1% on a training data of 618 randomly selected proteins, and 59.1% on a test data of size 61). How-ever, they used an additional trick, namely clustering frequently occuring sequence-structure motifs. This allowed them, in particular, to utilize more informa-tion, for example, the whole sequence of the amino-acids for a given motif was taken into account (while we look only at the first and the last amino-acids of a pattern).
 We thus expect that with a better  X  X eature selection X  step the results of pattern-based CRFs can be im-proved. In (Ye et al., 2009) this was done by boosting type methods (Dietterich et al., 2004) with sequen-tial selection of useful features. Another improvement can be made by considering  X  X uperpatterns X  by which we mean clusters of patterns (where patterns are not only sequences of labels, but also different local struc-tures like  X  X riplets X  that we used) that have the same weights in learning model. In this case a problem of clusterization of patterns set is to be addressed before learning of weights. It is also important to note that our algorithms can adapt to formulation with long su-perpatterns, since their complexities depend linearly on the number of patterns.
 A C++ implementation of our algorithms can be found at http://pub.ist.ac.at/ ~ vnk/software.html . The HMM/CRF models are a method of choice for many sequence labeling problems. Pattern-based CRFs generalize these models by providing more flex-ibility; they allow efficiently encoding certain long-range interactions. We provided a complete set of tools for inference in pattern-based CRFs. Importantly, all of our algorithms are linear in the number of patterns. Our preliminary experiments show a potential of this model for the protein dihedral angles prediction prob-lem. With a rather basic model were able to ob-tain results close to that in (Bystroff et al., 2000) who used many optimizations. To improve results fur-ther, we believe it is important to introduce techniques for parameterizing pattern-based CRFs, including ef-ficient algorithms for feature selection and superpat-terns clusterization. This will be our future work. Acknowledgements We thank Herbert Edelsbrun-ner for helpful discussions.
 Berkman, Omer and Vishkin, Uzi. Recursive star-tree parallel data structure. SIAM Journal on Comput-ing , 22(2):221 X 242, 1993.
 Bystroff, C., Thorsson, V., and Baker, D. HMMSTR: a hidden Markov model for local sequence-structure correlation in proteins. J Mol. Biol. , 301:173 X 190, 2000.
 Dietterich, T. G., Ashenfelter, A., and Bulatov, Y.
Training conditional random fields via gradient tree boosting. In ICML , 2004.
 Komodakis, N. and Paragios, N. Beyond pairwise ener-gies: Efficient optimization for higher-order MRFs. In CVPR , 2009.
 Lafferty, J., McCallum, A., and Pereira, F. Condi-tional random fields: Probabilistic models for seg-menting and labeling sequence data. In ICML , 2001. Nguyen, Viet Cuong, Ye, Nan, Lee, Wee Sun, and
Chieu, Hai Leong. Semi-Markov conditional ran-dom field with high-order features. In ICML 2011
Structured Sparsity: Learning and Inference Work-shop , 2011.
 Qian, Xian, Jiang, Xiaoqian, Zhang, Qi, Huang, Xuan-jing, and Wu, Lide. Sparse higher order conditional random fields for improved sequence labeling. In ICML , 2009.
 Rother, C., Kohli, P., Feng, W., and Jia, J. Minimiz-ing sparse higher order energy functions of discrete variables. In CVPR , 2009.
 Sarawagi, S. and Cohen, W. Semi-Markov conditional random fields for information extraction. In NIPS , 2004.
 Takhanov, R. and Kolmogorov, V. Inference algo-rithms for pattern-based CRFs on sequence data. ArXiv , abs/1210.0508, 2012.
 Tsochantaridis, I., Hofmann, T., Joachims, T., and Al-tun, Y. Support vector machine learning for inter-dependent and structured output spaces. In ICML , 2004.
 Vose, M. D. A linear algorithm for generating random numbers with a given distribution. IEEE Transac-tions on software engineering , 17(9):972 X 975, 1991. Ye, Nan, Lee, Wee Sun, Chieu, Hai Leong, and Wu,
Dan. Conditional random fields with high-order fea-
