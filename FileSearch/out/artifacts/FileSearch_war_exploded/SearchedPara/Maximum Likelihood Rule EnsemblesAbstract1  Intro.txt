 Krzysztof Dembczy X ski kdembczynski@cs.put.poznan.pl Wojciech Kot X owski wkotlowski@cs.put.poznan.pl Roman S X owi X ski rslowinski@cs.put.poznan.pl Decision rule is a logical statement of the form:  X  X f condition then response  X . It can be treated as a sim-ple classifier that gives a constant response for the ob-jects satisfying the condition part, and abstains from the response for all the other objects. Induction of decision rules has been widely considered in the early machine learning approaches (Michalski, 1983; Cohen, 1995; F  X urnkranz, 1996), and rough set approaches to knowledge discovery (Stefanowski, 1998). The most popular algorithms were based on a sequential cov-ering procedure (also known as separate-and-conquer approach). In this technique, a rule is learned which covers a part of the training examples, then examples are removed from the training set and the process is repeated until no examples remain.
 Although it seems that decision (classification) trees are much more popular in data mining and machine learning applications, recently we are able to ob-serve again a growing interest in decision rule mod-els. As an example, let us mention such algorithms as RuleFit (Friedman &amp; Popescu, 2005), SLIPPER (Co-hen &amp; Singer, 1999), Lightweight Rule Induction (LRI) (Weiss &amp; Indurkhya, 2000). All these algorithms follow a specific iterative approach to decision rule gen-eration by treating each decision rule as a subsidiary base classifier in the ensemble. This approach can be seen as a generalization of the sequential covering, be-cause it approximates the solution of the prediction task by sequentially adding new rules to the ensem-ble without adjusting those that have already been added (RuleFit is an exception since it generates the trees first and then transforms them to rules). Each rule is fitted by concentrating on objects which were hardest to classify correctly by rules already present in the ensemble. All these algorithms can be explained within the framework of boosting (Freund &amp; Schapire, 1997; Mason et al., 1999; Friedman et al., 2000) or forward stagewise additive modeling (FSAM) (Hastie et al., 2003), a greedy procedure for minimizing a loss function on the dataset.
 The algorithm proposed in this paper, Maximum Likelihood Rule Ensembles (MLRules), benefits from the achievements in boosting machines (Freund &amp; Schapire, 1997; Mason et al., 1999; Friedman et al., 2000; Friedman, 2001). Its main idea consists in rule induction by greedily minimizing the negative loglike-lihood (also known as logit loss in binary classification case) to estimate the conditional class probability dis-tribution. Minimization of such loss function with a tree being a base classifier has already been used in LogitBoost (Friedman et al., 2000) and MART (Fried-man, 2001), however, here we show a modified proce-dure, adapted to the case when the decision rule is a base classifier in the ensemble. In contrary to RuleFit (where trees are generated first), rules are generated directly; in contrary to SLIPPER and LRI, negative loglikelihood loss is used. Moreover, our approach is distinguished from other approaches to rule induction by the fact of estimating the class probability distri-bution instead of single classification and by using the same single measure (value of the negative loglikeli-hood) at all stages of the learning procedure: setting the best cuts (conditions), stopping the rule X  X  growth and determining the response (weight) of the rule. We derive the algorithm for two optimization techniques, depending on whether we expand the loss function to the first order (fitting to the gradient) or to the second order (Newton steps). We report experiments showing the performance of MLRules and comparing them with the competitive rule ensemble methods.
 The paper is organized as follows. In Section 2, the problems of classification is described. Section 3 presents a framework for learning rule ensembles. Sec-tion 4 is devoted to the problem of a single rule gen-eration. In Section 5 we discuss the issue of conver-gence of the method, and we propose a modification to the main algorithm. Section 6 contains experimen-tal results. The last section concludes the paper and outlines further research directions. In the classification problem, the aim is to predict the unknown class label y  X  X  1 ,...,K } of an object using known values of the attributes x = ( x 1 ,x 2 ,...,x m This is done by constructing a classification function f ( x ) that predicts accurately the value of y . The ac-curacy of a single prediction is measured in terms of the loss function L ( y,f ( x )) , while the overall accuracy of the function f ( x ) is measured by the expected loss (risk) over the data distribution P ( x ,y ) : Since P ( x ,y ) is unknown, the risk-minimizing function (Bayes classifier), f  X  = arg min f E [ L ( y,f ( x ))] , is also unknown. The learning procedure uses only a set of training examples { ( x 1 ,y 1 ) ,..., ( x n ,y n ) } to construct f to be a good approximation of f  X  . Usually, it is performed by minimization of the empirical risk: where function f is chosen from a restricted family of functions. The most commonly used loss function is 0-1 loss, L 0-1 ( y,f ( x )) = 1  X   X  y,f ( x ) , where  X  1 if i = j , otherwise  X  ij = 0 . If the correct class is predicted, classification function is not penalized, otherwise the unit penalty is imposed. Bayes classifier has the following form: The 0-1 loss has several drawbacks. Firstly, if we intro-duce unequal costs of misclassification, f  X  ( x ) does not longer have the form (2). Moreover, 0-1 loss is insensi-tive to the  X  X onfidence X  of prediction: minimization of 0-1 loss results only in finding the most probable class, without estimating its probability. On the contrary, probability estimation provides us with the conditional class distribution P ( y | x ) , by which we can measure the prediction confidence. Moreover, all we need to obtain the Bayes classifier for any loss function is the condi-tional probability distribution. Here we consider the estimation of probabilities using the well-known max-imum likelihood estimation (MLE) method. MLE can be stated as the empirical risk minimization by tak-ing the negative logarithm of the conditional likelihood (negative log-likelihood) as the loss function: We model probabilities P (1 | x ) ,...,P ( K | x ) with a vec-tor f ( x ) = ( f 1 ( x ) ,...,f K ( x )) using the multinomial logistic transform: Then (3) has the form: This expression (with the exception that vector func-tion f is used instead of scalar f ) has the form of (1) if we identify L ( y i , f ( x i )) = log P K k =1 e f k ( x It is worth mentioning that the Bayes function f  X  ( x ) is obtained by the inverse of (4). In this section, we describe the scheme of learning rule ensembles. Let X j be the set of all possible values of attribute j . Condition part of the rule consists of a conjunction of elementary expressions of the form x j  X  S j , where x j is the value of object x on attribute j and S j is a subset of X j , j  X  X  1 ,...,m } . We assume that in the case of ordered value sets, S j has the form of the interval [ s j ,  X  ) or (  X  X  X  ,s j ] for some s j  X  X that the elementary expressions take the form x j  X  s j or x j  X  s j . For nominal attributes, we consider ele-mentary expressions of the form x j = s j or x j 6 = s j . Let  X  be the set of elementary expressions constitut-ing the condition part of the rule and let  X ( x ) be an indicator function equal to 1 if x satisfies the condition part of the rule (all elementary expressions in the con-dition part), otherwise  X ( x ) = 0 . We say that a rule covers an object x , if  X ( x ) = 1 . The response of the rule is a vector  X   X  R K assigned to the region defined by  X  . Therefore, we define a decision rule as: Notice that the decision rule takes only two values, r ( x )  X  {  X  , 0 } , depending whether x satisfies the con-dition part or not. In this paper, we assume the clas-sification function is a linear combination of M rules: Using (4), we can obtain conditional probabilities from (7). Moreover, from (4) it follows that P ( y | x ) is a monotone function of f y ( x ) . Therefore, from (2) we have that object x is classified to the class with the highest f k ( x ) . Thus, combination (7) has very simple interpretation as a voting procedure: rules vote for each class k , and object x is classified to the class with the highest vote.
 The construction of an optimal rules ensemble mini-mizing the negative loglikelihood (empirical risk) is a hard optimization problem. That is why we follow here a forward stagewise strategy (Hastie et al., 2003), i.e. the rules are added one by one, greedily minimizing the loss function: r where r m is a rule obtained in the m -th iteration and f m  X  1 is the rule ensemble after m  X  1 iterations. It has been shown (Hastie et al., 2003) that  X  X hrinking X  the base classifier while adding it to the ensemble improves the prediction accuracy. That is why we set: where  X   X  (0 , 1] is the shrinkage parameter, which con-stitutes a trade-off between accuracy and interpretabil-ity. Higher values (  X   X  1 ) produce smaller ensembles, while low values (  X   X  0 . 1 ) produce larger but more accurate ones. In this section, we describe how the algorithm gener-ates single rules. In order to obtain a rule, one has to solve (8). The optimization procedure is still compu-tationally hard. Therefore, we restrict analysis to the rules voting for only one class, so that the response of the rule has the form  X  =  X  v , where v is a vector with only one non-zero coordinate v k = 1 , for some k = 1 ,...,K , and  X  is a positive real value.
 We propose two heuristic procedures for solving (8). The first, called gradient method (Mason et al., 1999), approximates ` ( f +  X  v  X ) up to the first order with respect to  X  : where Since the first term in (9) is constant, minimization of the loss for any positive  X  is equivalent to minimiza-tion of the second term. Thus, if we define: (we remind, that there are only K possible vectors v , so the arg min operation can be done by simply checking all K possibilities), then  X  m can be obtained by minimizing L m ( X ) .
 The second heuristic, Newton method, approximates ` ( f +  X  v  X ) up to the second order: ` ( f +  X  v  X ) ' ` ( f ) +  X ` 0 ( f , v  X ) + where ` 0 ( f , v  X ) is defined as before, and: Due to convexity of the loglikelihood, expression (12) is minimized by the Newton step: By substituting (14) into (12), and taking the square root, we get: and we can obtain  X  m by minimizing L m ( X ) .
 Algorithm 1 MLRules input: set of n training examples { ( y i , x i ) } n 1 , output: rule ensemble { r m ( x ) } M 1 . f 0 :=  X  0 . for m = 1 to M do end for Expressions ` 0 ( f , v  X ) and ` 00 ( f , v ) have a very simple form. Let v be such that v k = 1 . Then: where p ik = P ( k | x i ) and  X  i,j = 1 iff i = j . To calculate L m ( X ) , these expressions must be obtained for each k . What we still need for finding  X  m using both gradient and Newton techniques, is a fast procedure for mini-mizing L m ( X ) , regardless whether it is defined by (11) or (15). We propose the following simple iterative pro-cedure: at the beginning,  X  m is empty (no elementary expressions are specified) and we set L m ( X ) = 0 . In each step, an elementary expression x j  X  S j is added to  X  m that minimizes L m ( X ) (if it exists). Such ex-pression is searched by sequentially testing the ele-mentary expressions, attribute by attribute. For or-dered attributes, each expression of the form x j  X  s j or x j  X  s j is tested, for every s j  X  X j ; for nominal attributes, we test each expression of the form x j = s j or x j 6 = s j , for every s j  X  X j . Adding new expressions is repeated until L m ( X ) cannot be decreased. We also simultaneously obtain v m , i.e. the value of v for which the minimum is reached in (11) or (15). Notice that since L m ( X ) = 0 at the beginning, L m ( X ) must be strictly negative at the end, otherwise no rule will be generated. The procedure for finding optimal  X  is very fast and proved to be efficient in computational exper-iments. The ordered attributes can be sorted once be-fore generating any rule. This procedure resembles the way the decision trees are generated. Here, we look, however, for only one path from the root to the leaf. Moreover, let us notice that a minimal value of L m ( X ) is a natural stop criterion in building a single rule and we do not use any other measures (e.g. impurity mea-sures) for choosing the optimal cuts.
 Having found  X  m , we can obtain  X  m by solving the following convex line-search problem: To speed up the computations, we follow, however, simpler procedure and obtain  X  m by the Newton step (14). The whole procedure for constructing the rule ensemble is presented as Algorithm 1. We call this pro-cedure MLRules. Note that we start with f ( x ) equal to  X  0 , which is a  X  X efault rule X  with fixed  X ( x )  X  1 , while v 0 and  X  0 are obtained as usual. Since the re-sponse always indicates the majority class, such a rule serves as a default classification when no other rule covers a given object.
 In our implementation of the algorithm, we employed the resampling technique (Friedman &amp; Popescu, 2003), which is known to improve both accuracy and compu-tational complexity. To obtain less correlated rules, we search for  X  m , using (11) or (15), on a random subsample (drawn without replacement) of the train-ing set of size  X  &lt; n . Then, however, the response  X  is obtained using all of the training objects (includ-ing those objects, which have not been used to obtain  X  m ). This usually decreases |  X  m | , so it plays the role of a regularization method, and avoids overfitting the rule to the training set. In this section, we shortly discuss the problem of con-vergence and propose two simple extensions of the main algorithm. 5.1. Convergence The procedure of obtaining  X  m with the Newton step does not always decrease the empirical risk and does not guarantee the convergence of the algorithm. How-ever, using a simple backtracking line-search is suffi-cient for convergence: we start with  X  m obtained by the Newton step. If ` ( f +  X  v  X ) &lt; ` ( f ) , the procedure stops; otherwise repeat  X  m :=  X  m / 2 until the above condition is satisfied. This procedure ends after a fi-nite number of steps, since from the definition of L m , either (11) or (15), it follows that L m = 0 if and only if ` 0 ( f , v  X ) = 0 , so if a rule is generated, L m &lt; 0 and v  X  is a descent direction. Therefore, ` is decreased in each iteration. Since ` is bounded from below, the procedure converges, i.e.: lim m  X  X  X  ` ( f m ) = `  X  . In the implementation of the algorithm we do not use such a procedure since the algorithm is stopped after M rounds anyway.
 This raises the question, whether `  X  is the solution with the minimum achievable value of negative log-likelihood in the class of rule ensembles F , i.e. if `  X  equal to `  X  = inf f  X  X  ` ( f ) ? The answer is negative be-cause a greedy procedure is used to find the condition part of rule  X  . Then, even if a  X  X escent direction X  rule exists, the procedure may fail to find it (although the resampling strategy improves the procedure by ran-domly perturbing the training set in each iteration, which helps to avoid  X  X ocal minima X ). Nevertheless, this questions seems not to be of practical importance here, since we fix the maximal number of rules M . This is due to the empirical evidences showing that ensemble methods sometimes overfit on real-life data when the size of the ensemble is too large. In the next subsection, we describe another stopping condition, in-dependent of the parameter M . 5.2. Avoiding overfitting A decision rule has the form of m -dimensional rectan-gle. It can be shown, that the class of m -dimensional rectangles has Vapnik-Chervonenkis (VC) dimension equal to 2 m and the VC dimension does not depend on the number of cuts. This is contrary to the tree classifier, for which the VC dimension grows to infin-ity with increasing number of cuts (nodes). Therefore, in case of tree ensembles, one usually specifies some constraints on tree complexity, e.g. maximal number of nodes, while in case of a rule ensemble no such con-straints are necessary.
 The theoretical results (Schapire et al., 1998) suggest that an ensemble with a simple base classifier (with low VC dimension) and high prediction confidence (mar-gin) on the dataset generalizes well, regardless of the size of the ensemble. Nevertheless, we conducted the computational experiments which show that the per-formance of rule ensemble can deteriorate as the num-ber of rules grows, especially for the problems with high noise level. Similar phenomenon has been ob-served for other boosting algorithms, in particular for AdaBoost (Mason et al., 1999; Friedman et al., 2000; Dietterich, 2000). Therefore, we propose a procedure for stopping the ensemble growth, based on the simple  X  X oldout set X  analysis.
 Each rule is induced from the subsample of size  X  &lt; n without replacement. Thus, there are n  X   X  objects which do not take part in the induction procedure and can be used as a holdout set to estimate the quality of the induced rule. Since each rule votes for a sin-gle class, we calculate a simple 0-1 error (accuracy) of such a rule on the covered objects from the holdout set. A rule is acceptable if the holdout error is bet-ter (lower) than random guessing. Then, the stopping condition has the following form: in any p subsequent iterations at least q rules are not acceptable. Such  X  X veraging X  over the iterations removes variations and allows us to observe the longer-term behavior of rule acceptability. We set p = 10 and q = 8 , and those values were obtained by noticing, that when the null hypothesis states that rules are not worse than ran-dom guessing, at least 8 unacceptable rules must be obtained in 10 trials to reject the null hypothesis in the binomial test with confidence level 0 . 05 . Another possibility for stopping the ensemble growth is running the internal cross validation, but such procedure has not been used in the experiment due to computational complexity. 5.3. Ordinal classification It is often the case that a meaningful order relation be-tween class labels exists. For example, in recommender systems, users are often asked to evaluate items on five value ( X  X tars X ) scale. Such problems are often re-ferred to as ordinal classification problems. Here we show how the order relation can be taken into account in MLRules. Without loss of generality, we assume that the order between classes is concordant with the order between class labels coded as natural numbers Y = { 1 ,...,K } .
 To capture the ordinal properties of Y , we only take into account rules voting for  X  X t least X  and  X  X t most X  class unions, where by  X  X t least X  class union we mean set { k,...,K } for some k , while by  X  X t most X  class union we mean { 1 ,...,k } . Such rules can be incorporated by considering the vectors v in the response of the rule to be of the form: v = { X  1 ,...,  X  1 , 1 ,..., 1 } (vote for  X  X t least X  union) or v = { 1 ,..., 1 ,  X  1 ,...,  X  1 } (vote for  X  X t most X  union), so that the rule increases the probability of a class union, and not of a single class.
 The whole algorithm remains the same, apart from the formulas (16) and (17), which now takes the form: The experimental verification of the usefulness of such rule representation is postponed for future research due to the lack of space. In this section, we show the results of the computa-tional experiments on real datasets. First, we examine the behavior of the ensemble as the number of rules increases. Then, we compare our algorithm with ex-isting approaches to rule induction. 6.1. Error curves In Figure 6.1, we present the train and test error as a function of the ensemble size M for two real datasets, taken from the UCI Repository (Asuncion &amp; Newman, 2007). On the sonar dataset, both ensembles (gradient and Newton) do not overfit and the test error decreases even if the number of rules reaches 1000; this is a rather typical situation. An atypical one can be found on the second dataset ( haberman ), where from some point, test error starts to increase. However, then a stopping condition described in Section 5.2 is satisfied which prevents rule ensemble from overfitting. 6.2. Comparison with other rule ensemble To check the performance of MLRules on the real datasets, we compare them with three existing rule induction algorithms. SLIPPER (Cohen &amp; Singer, 1999) was proposed within the AdaBoost reweighting scheme and uses an induction procedure which involves pruning. LRI (Weiss &amp; Indurkhya, 2000) generates rules in the form of a DNF-formula and uses a spe-cific reweighting scheme based on the cumulative er-rors. RuleFit (Friedman &amp; Popescu, 2005) is based on FSAM framework (Hastie et al., 2003), but it uses the regression trees as base classifiers and then transforms them to rules. All three approaches are thus based on some boosting/reweighting strategy. According to our knowledge, RuleFit has not been compared with SLIPPER and LRI yet.
 We used 35 files taken from UCI Repository (Asun-cion &amp; Newman, 2007), among which 20 files are bi-nary classification tasks and 15 are multi-class tasks. We omit characteristics of the datasets due to lack of space. We tested four classifiers on each dataset (MLRules with gradient and Newton steps, LRI and SLIPPER) and RuleFit on binary datasets only (Rule-Fit does not handle multi-class case). We selected the following parameters for each method:  X  SLIPPER: we set maximum number of iteration  X  LRI: According to (Weiss &amp; Indurkhya, 2000), we  X  RuleFit: According to the experiment in (Fried- X  MLRules: We set  X  = 0 . 5 n, X  = 0 . 1 ,M = 500 , Each test was performed using 10-fold cross validation (with exactly the same train/test splits for each classi-fier) and average 0-1 loss on the test set was calculated. The results are shown in Table 6.2.
 We first restrict the analysis to binary-class problems only. To compare multiple classifiers on the multi-ple datasets, we follow Dem X sar (2006), and make the Friedman test, which uses ranks of each algorithm to check whether all the algorithms perform equally well (null hypothesis). Friedman statistics gives 33 . 28 which exceeds the critical value 9 . 488 (for confidence level 0 . 05 ), so we can reject the null hypothesis and state that classifiers are not equally good. Next, we proceed to a post-hoc analysis and calculate the criti-cal difference (CD) according to the Nemeneyi statis-tics. We obtain CD = 1 . 364 which means that al-gorithms with difference in average ranks more than 1 . 364 are significantly different. In Figure 6.2 aver-age ranks were marked on a line, and groups of classi-fiers that are not significantly different were connected. This shows that both MLRules algorithms are not sig-nificantly different to LRI, however they outperform both SLIPPER and RuleFit. On the other hand, none of three well-known rule ensemble algorithms (LRI, SLIPPER, RuleFit) is significantly better to any other. The situation remains roughly the same if we com-pare the algorithms using all 35 datasets. We exclude RuleFit (it does not work with multi-class problems) and SLIPPER (its results are very poor, the worst al-most every time 1 ). Thus, we end up with 3 algorithms. Friedman statistics gives 3 . 53 which does not exceed the critical value 5 . 991 , so that the null hypothesis cannot be rejected. Note that the difference in ranks decreased, mainly because LRI performs excellent on the largest datasets (letters and digits recognition). It is interesting to check how much of the improve-ment in accuracy of MLRules comes from shrinkage, resampling and regularizing the rule response, because those techniques can also be simply incorporated to SLIPPER and LRI. We plan to investigate this issue in our future research. We proposed a new rule induction algorithm for solv-ing classification problems, called MLRules, based on the maximum likelihood estimation method and us-ing boosting strategy in rule induction. In contrary to previously considered algorithms, it estimates the conditional class probability distribution and therefore can work with any cost matrix for classification. We considered two optimization techniques, based on gra-dient and Newton steps, and introduced a stopping condition to avoid overfitting. The performance of MLRules was verified on a large collection of datasets, both binary-and multi-class. Our algorithm is com-petitive or outperforms the best existing approaches to rule induction.
 We also suggested the way in which MLRules can cap-ture the order between classes and therefore can solve the ordinal classification problems. We plan to verify this issue experimentally in the future.
 Asuncion, A., &amp; Newman, D. J. (2007). UCI machine learning repository.
 Cohen, W. W. (1995). Fast effective rule induction.
ICML (pp. 115 X 123). Cohen, W. W., &amp; Singer, Y. (1999). A simple, fast, and effective rule learner. National Conference on Artificial Intelligence (pp. 335 X 342).
 Dem X sar, J. (2006). Statistical comparisons of classi-fiers over multiple data sets. Journal of Machine Learning Research , 7 , 1 X 30.
 Dietterich, T. G. (2000). An experimental comparison of three methods for constructing ensembles of de-cision trees: bagging, boosting, and randomization. Machine Learning , 40 , 139 X 158.
 Freund, Y., &amp; Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences , 55 , 119 X 139.
 Friedman, J. H. (2001). Greedy function approxima-tion: A gradient boosting machine. Annals of Statis-tics , 29 , 1189 X 1232.
 Friedman, J. H., Hastie, T., &amp; Tibshirani, R. (2000).
Additive logistic regression: a statistical view of boosting. Annals of Statistics , 337 X 407.
 Friedman, J. H., &amp; Popescu, B. E. (2003). Impor-tance sampled learning ensembles (Technical Re-port). Dept. of Statistics, Stanford University. Friedman, J. H., &amp; Popescu, B. E. (2005). Predic-tive learning via rule ensembles (Technical Report). Dept. of Statistics, Stanford University.
 F  X urnkranz, J. (1996). Separate-and-conquer rule learning. Artificial Intelligence Review , 13 , 3 X 54. Hastie, T., Tibshirani, R., &amp; Friedman, J. H. (2003).
Elements of statistical learning: Data mining, infer-ence, and prediction . Springer.
 Mason, L., Baxter, J., Bartlett, P., &amp; Frean, M. (1999).
Functional gradient techniques for combining hy-potheses , 33 X 58. MIT Press.
 Michalski, R. S. (1983). A theory and methodology of inductive learning , 83 X 129. Tioga Publishing. Schapire, R. E., Freund, Y., Bartlett, P., &amp; Lee, W. S. (1998). Boosting the margin: A new explanation for the effectiveness of voting methods. The Annals of Statistics , 26 , 1651 X 1686.
 Stefanowski, J. (1998). On rough set based approach to induction of decision rules. Rough Set in Knowledge Discovering (pp. 500 X 529). Physica Verlag.
 Weiss, S. M., &amp; Indurkhya, N. (2000). Lightweight
