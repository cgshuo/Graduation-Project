 Reducing high-dimensional data vectors to robust and inter pretable lower-dimensional representa-tions has a long and successful history in data analysis, inc luding recent innovations such as latent semantic indexing (LSI) (Deerwester et al, 1994) and latent Dirichlet allocation (LDA) (Blei, Ng, and Jordan, 2003). These types of techniques have found broa d application in modeling of sparse high-dimensional count data such as the  X  X ag of words X  repre sentations for documents or transaction data for Web and retail applications.
 Approaches such as LSI and LDA have both been shown to be usefu l for  X  X bject matching X  in their respective latent spaces. In information retrieval for exa mple, both a query and a set of documents can be represented in the LSI or topic latent spaces, and the d ocuments can be ranked in terms of how well they match the query based on distance or similarity in the latent space. The mapping to latent space represents a generalization or abstraction aw ay from the sparse set of observed words, to a  X  X igher-level X  semantic representation in the latent spa ce. These abstractions in principle lead to better generalization on new data compared to inferences ca rried out directly in the original sparse high-dimensional space. The capability of these models to p rovide improved generalization has been demonstrated empirically in a number of studies (e.g., Deerwester et al 1994; Hofmann 1999; Canny 2004; Buntine et al, 2005).
 However, while this type of generalization is broadly usefu l in terms of inference and prediction, there are situations where one can over-generalize. Consid er trying to match the following query to a historical archive of news articles: election + campaign + Camejo . The query is intended to find documents that are about US presidential campaigns and a lso about Peter Camejo (who ran as vice-presidential candidate alongside independent Ralph Nader in 2004). LSI and topic models are contain the words election or campaign ).
 However, a potential problem is that the documents that are h ighly ranked by LSI or topic models need not include any mention of the name Camejo . The reason is that the combination of words in this query is likely to activate one or more latent variabl es related to the concept of presidential campaigns. However, once this generalization is made the mo del has  X  X ost X  the information about the specific word Camejo and it will only show up in highly ranked documents if this wor d happens little media coverage compared to the coverage given to the c andidates from the two main parties). But from the viewpoint of the original query, our preference would be to get documents that are about the general topic of US presidential elections with the specific constraint that they mention Peter Camejo.
 Word-based retrieval techniques, such as the widely-used t erm-frequency inverse-document-frequency (TF-IDF) method, have the opposite problem in gen eral. They tend to be overly specific in terms of matching words in the query to documents.
 In general of course one would like to have a balance between g enerality and specificity. One ad hoc approach is to combine scores from a general method such as LS I with those from a more specific method such as TF-IDF in some manner, and indeed this techniq ue has been proposed in information retrieval (Vogt and Cottrell, 1999). Similarly, in the ad ho c LDA approach (Wei and Croft, 2006), the LDA model is linearly combined with document-specific word d istributions to capture both general as well as specific information in documents. However, neith er method is entirely satisfactory since it is not clear how to trade-off generality and specificity in a principled way.
 The contribution of this paper is a new graphical model based on latent topics that handles the trade-off between generality and specificity in a fully probabilis tic and automated manner. The model, which we call the special words with background (SWB) model, is an extension of the LDA model. The new model allows words in documents to be modeled as eithe r originating from general topics, or from document-specific  X  X pecial X  word distributions, or from a corpus-wide background distribu-tion. The idea is that words in a document such as election and campaign are likely to come from a general topic on presidential elections, whereas a name su ch as Camejo is much more likely to be treated as  X  X on-topical X  and specific to that document. Wo rds in queries are automatically inter-preted (in a probabilistic manner) as either being topical o r special, in the context of each document, allowing for a data-driven document-specific trade-off bet ween the benefits of topic-based abstrac-tion and specific word matching. Daum  X e and Marcu (2006) inde pendently proposed a probabilistic model using similar concepts for handling different traini ng and test distributions in classification problems.
 Although we have focused primarily on documents in informat ion retrieval in the discussion above, the model we propose can in principle be used on any large spar se matrix of count data. For example, transaction data sets where rows are individuals and column s correspond to items purchased or Web population behavior and the  X  X pecial word distributions X  c an capture the idiosyncracies of specific individuals.
 Section 2 reviews the basic principles of the LDA model and in troduces the new SWB model. Sec-tion 3 illustrates how the model works in practice using exam ples from New York Times news articles. In Section 4 we describe a number of experiments wi th 4 different document sets, includ-ing perplexity experiments and information retrieval expe riments, illustrating the trade-offs between generalization and specificity for different models. Secti on 5 contains a brief discussion and con-cluding comments. or LDA. There are D documents and document d has N symmetric Dirichlet priors for the D document-topic multinomials represented by  X  and the T topic-word multinomials represented by  X  . In the generative model, for each document d , the N Figure 1: Graphical models for (a) the standard LDA topic mod el (left) and (b) the proposed special words topic model with a background distribution (SWB) (rig ht). are generated by drawing a topic t from the document-topic distribution p ( z |  X  a word w from the topic-word distribution p ( w | z = t, X  (2004) the topic assignments z for each word token in the corpus can be efficiently sampled vi a Gibbs sampling (after marginalizing over  X  and  X  ). Point estimates for the  X  and  X  distributions can be computed conditioned on a particular sample, and pred ictive distributions can be obtained by averaging over multiple samples.
 We will refer to the proposed model as the special words topic model with background distribution (SWB) (Figure 1(b)). SWB has a similar general structure to t he LDA model (Figure 1(a)) but with additional machinery to handle special words and backgroun d words. In particular, associated with each word token is a latent random variable x , taking value x = 0 if the word w is generated via the topic route, value x = 1 if the word is generated as a special word (for that document) and value x = 2 if the word is generated from a background distribution spec ific for the corpus. The variable x acts as a switch: if x = 0 , the previously described standard topic mechanism is used to generate the word, whereas if x = 1 or x = 2 , words are sampled from a document-specific multinomial  X  or a corpus specific multinomial  X  (with symmetric Dirichlet priors parametrized by  X  a symmetric Dirichlet prior,  X  . One could also use a hierarchical Bayesian approach to intr oduce another level of uncertainty about the Dirichlet priors (e. g., see Blei, Ng, and Jordan, 2003) X  X e have not investigated this option, primarily for computati onal reasons. In all our experiments, we set  X  = 0.1,  X  The conditional probability of a word w given a document d can be written as: distribution for the corpus. Note that when compared to the s tandard topic model the SWB model can explain words in three different ways, via topics, via a s pecial word distribution, or via a back-ground word distribution. Given the graphical model above, it is relatively straightforward to derive Gibbs sampling equations that allow joint sampling of the z token w p ( x i = 0 ,z i = t | w , x  X  i , z  X  i , X , X  0 , X  )  X  and for x Figure 2: Examples of two news articles with special words (a s inferred by the model) shaded in gray. (a) upper, email article with several colloquialisms , (b) lower, article about CSX corporation. and for x where the subscript  X  i indicates that the count for word token i is removed, N words in document d and N latent topics, special words and background component, res pectively, C W T number of times word w is assigned to topic t , to the special-words distribution of document d , and to the background distribution, respectively, and W is the number of unique words in the corpus. Note that when there is not strong supporting evidence for x of this event is low), then the probability of the word being g enerated by the special words route, x = 1 , or background route, x i = 2 increases.
 One iteration of the Gibbs sampler corresponds to a sampling pass through all word tokens in the corpus. In practice we have found that around 500 iterations are often sufficient for the in-sample perplexity (or log-likelihood) and the topic distribution s to stabilize.
 We also pursued a variant of SWB, the special words (SW) model that excludes the background distribution  X  and has a symmetric Beta prior,  X  , on  X  (which in SW is a document-specific Bernoulli distribution). In all our SW model runs, we set  X  = 0.5 resulting in a weak symmetric prior that is equivalent to adding one pseudo-word to each document. Expe rimental results (not shown) indicate that the final word-topic assignments are not sensitive to ei ther the value of the prior or the initial assignments to the latent variables, x and z . We illustrate the operation of the SW model with a data set con sisting of 3104 articles from the New York Times (NYT) with a total of 1,399,488 word tokens. Th is small set of NYT articles was formed by selecting all NYT articles that mention the word  X  X  nron. X  The SW topic model was run with T = 100 topics. In total, 10 Gibbs samples were collected from the mo del. Figure 2 shows two short fragments of articles from this NYT dataset. The ba ckground color of words indicates the probability of assigning words to the special words topic X  X  arker colors are associated with higher probability that over the 10 Gibbs samples a word was assigne d to the special topic. The words with gray foreground colors were treated as stopwords and we re not included in the analysis. Figure 2(a) shows how intentionally misspelled words such as  X  X izn esmen X  and  X  X eeznessmen X  and rare Figure 3: Examples of background distributions (10 most lik ely words) learned by the SWB model for 4 different document corpora. words such as  X  X inkos X  are likely to be assigned to the specia l words topic. Figure 2(b) shows how a last name such as  X  X now X  and the corporation name  X  X SX X  that are specific to the document are likely to be assigned to the special topic. The words  X  X now X  a nd  X  X SX X  do not occur often in other documents but are mentioned several times in the example doc ument. This combination of low document-frequency and high term-frequency within the doc ument is one factor that makes these words more likely to be treated as  X  X pecial X  words. We use 4 different document sets in our experiments, as summa rized in Table 1. The NIPS and PATENTS document sets are used for perplexity experiments a nd the AP and FR data sets for re-documents from the U.S. Patents collection (TREC Vol-3), As sociated Press news articles from 1998 (TREC Vol-2), and articles from the Federal Register (TREC V ol-1, 2) respectively. To create the sampled AP and FR data sets, all documents relevant to querie s were included first and the rest of the documents were chosen randomly. In the results below all LDA/SWB/SW models were fit using T = 200 topics.
 Figure 3 demonstrates the background component learned by t he SWB model on the 4 different doc-ument data sets. The background distributions learned for e ach set of documents are quite intuitive, with words that are commonly used across a broad range of docu ments within each corpus. The ratio of words assigned to the special words distribution and the b ackground distribution are (respectively for each data set), 25%:10% (NIPS), 58%:5% (PATENTS), 11%:6 % (AP), 50%:11% (FR). Of note is the fact that a much larger fraction of words are treated as special in collections containing long documents (NIPS, PATENTS, and FR) than in short  X  X bstract-l ike X  collections (such as AP) X  X his makes sense since short documents are more likely to contain general summary information while longer documents will have more specific details. 4.1 Perplexity Comparisons The NIPS and PATENTS documents sets do not have queries and re levance judgments, but nonethe-less are useful for evaluating perplexity. We compare the pr edictive performance of the SW and SWB topic models with the standard topic model by computing t he perplexity of unseen words in test documents. Perplexity of a test set under a model is defin ed as follows: Figure 4: Average perplexity of the two special words models and the standard topics model as a function of the percentage of words observed in test documen ts on the NIPS data set (left) and the PATENTS data set (right).
 set, and D train is the training set. For the SWB model, we approximate p ( w d |D train ) as follows: where  X  s ,  X  s ,  X  s ,  X  s and  X  s are point estimates from s = 1: S different Gibbs sampling runs. The probability of the words w d in a test document d , given its parameters, can be computed as follows: where N test document.  X  s When a fraction of words of a test document d is observed, a Gibbs sampler is run on the observed words to update the document-specific parameters,  X  used in the computation of perplexity. For the NIPS data set, documents from the last year of the data set were held out to compute perplexity ( D documents were randomly selected as test documents.
 From the perplexity figures, it can be seen that once a small fr action of the test document words is observed (20% for NIPS and 10% for PATENTS), the SW and SWB m odels have significantly lower perplexity values than LDA indicating that the SW and S WB models are using the special words  X  X oute X  to better learn predictive models for individ ual documents. 4.2 Information Retrieval Results Returning to the point of capturing both specific and general aspects of documents as discussed in the introduction of the paper, we generated 500 queries of le ngth 3-5 using randomly selected low-frequency words from the NIPS corpus and then ranked documen ts relative to these queries using several different methods. Table 2 shows for the top k -ranked documents ( k = 1 , 10 , 50 , 100 ) how many of the retrieved documents contained at least one of the words in the query. Note that we are Table 2: Percentage of retrieved documents containing at le ast one query word (NIPS corpus). often specific query words occur in retrieved documents. TF-IDF has 100% matches, as one would expect, and the techniques that generalize (such as LSI and L DA) have far fewer exact matches. The SWB and SW models have more specific matches than either LD A or LSI, indicating that they have the ability to match at the level of specific words. Of cou rse this is not of much utility unless the SWB and SW models can also perform well in terms of retriev ing relevant documents (not just documents containing the query words), which we investigat e next.
 For the AP and FR documents sets, 3 types of query sets were con structed from TREC Topics 1-150, based on the Title (short), Desc (sentence-length) and Concepts (long list of keywords) fields. Queries that have no relevance judgments for a collection we re removed from the query set for that collection.
 The score for a document d relative to a query q for the SW and standard topic models can be com-puted as the probability of q given d (known as the query-likelihood model in the IR community). For the SWB topic model, we have p ( q | d )  X  Y We compare SW and SWB models with the standard topic model (LD A), LSI and TF-IDF. The TF-IDF score for a word w in a document d is computed as TF-IDF ( w,d ) = C W D wd LSI, the TF-IDF weight matrix is reduced to a K -dimensional latent space using SVD, K = 200 . A given query is first mapped into the LSI latent space or the TF-IDF space (known as query folding), and documents are scored based on their cosine distances to t he mapped queries.
 To measure the performance of each algorithm we used 2 metric s that are widely used in IR research: the mean average precision (MAP) and the precision for the to p 10 documents retrieved (pr@10d). The main difference between the AP and FR documents is that th e latter documents are considerably longer on average and there are fewer queries for the FR data s et. Figure 5 summarizes the results, broken down by algorithm, query type, document set, and metr ic. The maximum score for each query experiment is shown in bold: in all cases (query-type/ data set/metric) the SW or SWB model produced the highest scores.
 To determine statistical significance, we performed a t-tes t at the 0.05 level between the scores of each of the SW and SWB models, and the scores of the LDA model (a s LDA has the best scores overall among TF-IDF, LSI and LDA). Differences between SW a nd SWB are not significant. In figure 5, we use the symbol * to indicate scores where the SW and SWB models showed a statis-tically significant difference (always an improvement) rel ative to the LDA model. The differences for the  X  X on-starred X  query and metric scores of SW and SWB ar e not statistically significant but nonetheless always favor SW and SWB over LDA. Wei and Croft (2006) have recently proposed an ad hoc LDA appr oach that models p ( q | d ) as a weighted combination of a multinomial over the entire corpu s (the background model), a multino-mial over the document, and an LDA model. Wei and Croft showed that this combination provides excellent retrieval performance compared to other state-o f-the-art IR methods. In a number of exper-iments (not shown) comparing the SWB and ad hoc LDA models we f ound that the two techniques produced comparable precision performance, with small but systematic performance gains being achieved by an ad hoc combination where the standard LDA mode l in ad hoc LDA was replaced with the SWB model. An interesting direction for future work is to investigate fully generative models that can achieve the performance of ad hoc approaches .
 In conclusion, we have proposed a new probabilistic model th at accounts for both general and spe-cific aspects of documents or individual behavior. The model extends existing latent variable prob-abilistic approaches such as LDA by allowing these models to take into account specific aspects of documents (or individuals) that are exceptions to the broad er structure of the data. This allows, for example, documents to be modeled as a mixture of words genera ted by general topics and words generated in a manner specific to that document. Experimenta l results on information retrieval tasks indicate that the SWB topic model does not suffer from the wea kness of techniques such as LSI and LDA when faced with very specific query words, nor does it s uffer the limitations of TF-IDF in terms of its ability to generalize.
 Acknowledgements We thank Tom Griffiths for useful initial discussions about t he special words model. This material is based upon work supported by the National Science Foundat ion under grant IIS-0083489. We acknowledge use of the computer clusters supported by NIH gr ant LM-07443-01 and NSF grant EIA-0321390 to Pierre Baldi and the Institute of Genomics an d Bioinformatics.
 References
