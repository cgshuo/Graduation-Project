 This paper focuses on the problem of clustering data from a hidden or a deep web data source. A key characteristic of deep web data sources is that data can only be accessed through the limited query interface they support. Because the underlying data set cannot be ac-cessed directly, data mining must be performed based on sampling of the datasets. The samples, in turn, can only be obtained by querying the deep web databases with specific inputs.

We have developed a new stratified clustering method addressing this problem for a deep web data source. Specifically, we have de-veloped a stratified k-means clustering method. In our approach, the space of input attributes of a deep web data source is stratified for capturing the relationship between the input and the output at-tributes. The space of output attributes of a deep web data source is partitioned into sub-spaces. Three representative sampling methods are developed in this paper, with the goal of achieving a good esti-mation of the statistics , including proportions and centers, within the sub-spaces of the output attributes.

We have evaluated our methods using two synthetic and two real datasets. Our comparison shows significant gains in estimation ac-curacy from both the novel aspects of our work, i.e., the use of stratification(5%-55%), and our and representative sampling meth-ods(up to 54%).
 Categories and Subject Descriptors: H.2.8 [DATABASE MAN-AGEMENT ]: Database Applications General Terms: Algorithms.
 Keywords: Deep web, clustering, sampling.
In recent years, one mode of data dissemination has become ex-tremely popular, which is the deep web . Deep web is the term coined to describe the contents that are stored in the databases and can be retrieved over the internet by querying through HTML forms, which are also called query interfaces .

The deep web has been a subject of much investigation[20, 8, 33, 17, 27, 26]. Most existing effort have focused on providing deep web querying systems[20, 8, 33]. However, given the volume of informa-tion contained in the deep web, it is desirable to obtain summary or key insights from one or more deep web data sources. Unfortunately, mining a deep web data source involves several unique challenges, which have not yet been adequately addressed.
 Table 1: An Example of Back-end database for real-estate
ID Year Bedroom Bathroom Price($) Square Feet 10 2000 3 2 200,000 2,100 11 2000 4 1 215,000 2,200 12 2000 4 2 230,000 2,400 13 2008 3 1 250,000 2,400 14 2008 3 1 240,000 2,500 15 2008 4 2 280,000 2,500 16 2008 4 1 250,000 2,400
This paper specifically focuses on the problem of clustering data from a deep web data source. There are many examples where clus-tering data from a deep web data source can help obtain a useful summary of the entire data set. As one simple example, let us con-sider a county real-estate data source. Table 1 shows an example of the back-end database for a deep web data source of real-state. The database contains 16 properties described by 5 attributes, including ID , the Year of construction, the number of Bedrooms , the numbers of Bathrooms , Price , and the Square Footage of each property. Typi-cally, such web-sites make records on all residential properties in the county available to any interested party. A person considering relo-cation to the county may wish to obtain a summary of the residences in the county, in terms of their value and square footage. Such a sum-mary can be obtained through the clustering process, and can help a person know their options while looking to purchase a property.
Clustering in particular, and data mining in general, on the deep web is challenging because the databases cannot be accessed di-rectly. Instead, the data can only be accessed through query in-terface(s) , which are based on input attribute(s) . A user query in-volves specifying value(s) for these attributes, and in response to such a query, dynamically generated HTML pages are returned as the output, comprising one or more output attribute(s) . Download-ing the entire database is simply not practical. Thus, the only prac-tical method for mining a deep web source is to sample the dataset. The samples, in turn, can only be obtained by querying the deep web databases with specific inputs. In Table 1, the query interface con-tains three input attributes, year of construction, number of bedrooms and number of bathrooms. The returned output html web-page(s) provide the information of two output attributes, which are the price and the square footage.

The clustering problem is clearly one of the most widely studied problems in the data mining community [18, 28, 25, 37, 16], but there is no existing work on clustering a hidden or a deep web data source. Sampling for efficient clustering has also been quite widely studied [19, 7, 21, 29, 15]. These sampling methods have the goal of reducing the computation and memory costs. None of these methods can be applied to the problem of clustering data from a deep web data source, since the entire dataset is not accessible, and the samples can only be obtained by querying the interface.

The overall goal while clustering over a deep web data source is to discover the underlying clusters of output attributes so that the esti-mated centers for clusters are close to the true centers of underlying clusters. However, estimating the centers of underlying clusters is itself a challenge. Furthermore, unlike sampling based methods de-veloped for other types of data, computation or memory costs are not the dominant factor while mining the deep web. The key consider-ation, instead, is the sampling cost , which refers to the number of distinct queries that need to be issued in order to obtain the sample from the deep web. Thus, in developing sampling methods for clus-tering on a deep web data source, the two challenges are: 1) how to achieve a high estimation accuracy on mining results when the dis-tribution of data is unknown, and 2) how to achieve a high estimation accuracy with a low sampling cost.

This paper presents and evaluates a k-means clustering method based on stratified sampling. First, we have developed a scheme for stratifying the space of input attributes of a deep web data source. In our method, the stratification is done through a tree that also mod-els the relation between the input and output attributes. Second, for solving the challenge where the cluster identities of sampled data records are unavailable, the space of output attributes of a deep web data source is partitioned into sub-spaces based on a pilot sample. Third, we have developed sampling methods of obtaining a repre-sentative sample , considering the sub-spaces of output attributes of a deep web data source. Statistics , including proportions and cen-ters of sub-spaces, are chosen for describing sub-spaces of output at-tributes. Corresponding to these statistics, three representative sam-pling methods are proposed so that a good estimation of sub-spaces of output attributes is achieved. Focusing on the proportions of sub-spaces of output attributes, an active learning based sampling method is developed. For centers of sub-spaces of output attributes, two sampling methods are developed, including an optimized sampling method and a method that uses active learning. These sampling methods are used in a stratified k-means clustering algorithm, which weighs different sampled points differently. Also, unlike the original k-means algorithm, our approach includes a method for deciding the parameter k in the k-means clustering, specific to the challenge of clustering a deep web data source.

We have evaluated our method using two synthetic and two real datasets. Our comparison shows significant gains in estimation ac-curacy from our novel representative sampling methods. Compared against simple random sampling, our representative sampling meth-ods achieve 5%-55% improvement in accuracy of clustering. Com-pared with original k-means clustering, our stratified k-means clus-tering improves the accuracy of clustering by up to 54%.
In this section, we introduce our method of partitioning the space of output attributes.

The key challenge in mining (or even querying) a deep web data source arises because the data in the back-end database is inacces-sible. Typically, given an query composed of values of one or more of the input attributes, a deep web data source will return the num-ber of data records satisfying the input query. Thus, in this paper, we assume the a deep web data source provides the prior probabil-ity of different queries, since it can be derived from the number of data records satisfying the input query on the entire database, which is provided by a large number of deep web data sources. Further-more, using this information, the distribution, as well as clustering, on input attributes can be obtained. However, since the distribution of output attributes is unknown, discovering the clusters on output at-tributes is difficult. Still, as we will show later, the knowledge about prior probabilities of queries turns to be critical for output attribute clustering.

In traditional k-means clustering, the population to be clustered is partitioned into k clusters, where the distance between data objects Figur e 1: Partitioning the Space of Output Attributes of the Real-estate Data within each cluster is minimized. For a deep web data source, where the data is not accessible directly, our goal has to be to estimate k centers for the k underlying clusters, so that the estimated k centers based on the sample are close to the k true centers.

Estimating the k centers for a data source would be trivial if the k partitions on the entire underlying population are available. This is because the k partitions for the underlying population can be used to divide the sampled data objects, and then the centers for k clusters can be estimated. However, the cluster labels or the k partitions on the entire hidden population are not available directly, which makes the task of estimating the k centers difficult. A possible solution for estimating the k partitions on the entire underlying population would be to use a pilot sample , which would be a small set of data records randomly drawn from the deep web. Clustering can first be performed on the pilot sample, yielding the k partitions as estimator for the true k partitions over the entire population.

Unfortunately, this method may not be effective when the pilot sample is small, as the differences between the estimated k partitions and the true k partitions may be large. Thus, in order to preserve the underlying true partitions, we target at generating partitions with cardinality that is a multiple of k . Formally, in our method, we gen-erate c  X  k partitions over the space of output attributes, where c is an integer. The c  X  k partitions are generated by performing k-means clustering on a pilot sample and computing the corresponding c  X  k sub-centers . For a given data record, its distance with the c  X  k sub-centers is computed, and the data record is classified into the sub-space based on the sub-center with which it has the smallest distance.

To illustrate the idea described above, we revisit our running ex-ample. Figure 1 shows partitions on the output space of the real-estate data source which was presented earlier in Table 1. To explain our point, we assume that the dataset has many more data records (as compared to the 16 records shown in Table 1), but the same trends are maintained. The two axes of this figure correspond to the two output attributes, which are the price and the square footage. In this figure, in order to perform k-means clustering where k = 2 , the space of output attributes is partitioned into four sub-spaces. Our sampling methods will aim at obtaining a representative sample considering these four sub-spaces.
This section describes our approaches for stratification of the input space.

Unlike simple random sampling , which draws a sample from the population in entirety, stratified sampling picks separate samples from H groups, which are also called strata or sub-populations [10]. Strat-ified sampling helps improve performance when data varies consid-erably across sub-populations , whereas the variance within each sub-population is small. Thus, in our algorithm, stratification is per-formed so that data records contained in the same stratum are as similar as possible. On a deep web data source, where records can only be obtained by submitting queries, stratification clearly needs to be performed on the query space composed of input attributes in the query interface. The data in a deep web source can be consid-ered to correspond to the entire query (input) space, whereas the sub-populations correspond to query (input) sub-spaces . More precisely, a sub-population comprises of data records that can be obtained by submitting queries from the corresponding query (input) sub-spaces.
Overall, in our method, the stratification is achieved by building a query (input) sub-space tree recursively, i.e, each node in the tree represents a query (input) sub-space. Associated with the node are a set of potential splitting input attributes, one of which needs to be chosen to further stratify the query (input) sub-space of the node. This is done in a greedy fashion, with the goal being to create strata in a way that the radius of the data objects within the strata is small. Therefore, at each step, for a leaf node of the tree, the input attribute that would maximally decrease the radius of the corresponding sub-population is selected. This greedy stratifying process stops when the radius associated with all leaf nodes is smaller than a pre-specified threshold.

In the example of real-estate data source shown earlier in Table 1, we can observe the following trend. The square footage and the price of the properties have increased with the year of construction, reflect-ing the trend that lower interest rates prompted demand for bigger (and more expensive) new properties. In comparison, there is no clear trend with respect to the other two input parameters, which are the number of bedrooms and bathrooms. Thus, by creating different strata using the attribute year of construction , we will obtain more homogeneous query (input) sub-spaces, i.e., the radius within each stratum will be lower.

Formally, for a deep web data source DP , let IS = { I 1 ; : : : ; I denote the set of input attributes and OS = { O 1 ; : : : ; O the set of output attributes. For a leaf node LN in the tree, let Q represent the corresponding input query, which is composed of SI , a subset of input attributes. The potential splitting input attributes for the node LN , which is represented by P I = IS  X  SI , is composed of input attributes that are not contained in the query Q . Under the query (input) space of node LN , the radius for the corresponding sub-population, R , is estimated by: where, D R i ( OS ) = { DR i ( O 1 ) ; : : : ; DR i ( O q data record DR i of node LN  X  X  sub-population, and DR i ( O 1 ; : : : ; q denote values for output attributes in DR i denotes the center of the sub-population of the node LN , and N rep-resents the size of the sub-population corresponding to the node LN . For a deep web data source where the data is not directly accessible, the radius R is estimated based on a sample.

For the potential splitting input attribute P i  X  P I associated with the domain DM i = { a i; 1 ; : : : ; a i;t } , the decrease of radius is com-puted as: probability that the input attribute P i assigned by the value a the space of the input query Q . With the assumption that a deep web data source provides the prior probability of different values of each input attributes, such as p ( P i = a i;k ; Q ) and p ( Q ) , the conditional probability of p ( P i = a i;k | Q ) can also be obtained.
Using the potential splitting input attribute P i to stratify the query space of LN , there are t potential children generated, by assigning different values to the input attribute P i . R i;k denotes the radius for the k th potential child node generated by splitting input attribute P and the computation is similar to that of R . The potential splitting input attribute P i with the largest decrease of radius  X  R to split the subspace of node LF .

In our stratified sampling process, the strata are the leaf nodes of the tree built on the query space of input attributes. The obtained strata are later related to the sub-spaces of the output space obtained from the pilot sample, and used for the representative sampling we will describe in the next section.
In Section 2, we presented a method for solving the first challenge in performing clustering over a deep web data source. Specifically, we proposed to create a c  X  k partitions and generate c  X  of the output attributes. In this section, we discuss how we address the second challenge, which is how to obtain a sample so that the estimated centers are close to the true centers.

Our solutions involve obtaining a representative sample from the data source based on the statistics of the sub-spaces of output at-tributes. In our method, statistics , which are either the centers or the proportions , are used to describe the c  X  k sub-spaces of the out-put attributes. The centers of each sub-space here are different from the sub-centers of each sub-space introduced in Section 2. The sub-centers are computed based on a pilot sample, which is then used for generating c  X  k partitions on the space of output attributes. Once the initial pilot sample has been drawn, these sub-centers are fixed during the sampling process. In comparison, the center of each sub-space is the mean vector of output attributes of all the underlying data records in that sub-space, which will be estimated during the sam-pling process. The statistics of sub-spaces can be estimated based on the sample we have obtained. Particularly, a sample is considered to be representative of the entire population if the estimated statistics of sub-spaces are close to the true values. However, for a deep web data sources where the data is not available directly, the true values of statistics are not available. Thus, in our problem, we target at obtaining a good estimation of the statistics of sub-spaces. Further-more, as it is time consuming to obtain data from a deep web data source, we focus on developing efficient sampling methods. Consid-ering the centers of sub-spaces, two sampling methods, which are the optimized sampling method and an active learning based sam-pling method, are proposed. Next, focusing on proportions of sub-spaces, another active learning based sampling method is proposed for greedily reducing the risk of incorrect estimation for proportions of sub-spaces.
The overall goal of the clustering process is to estimate k centers that are close to the true k cluster centers. Thus, it is natural to choose the centers as the statistics for each sub-space. In this section, we describe two representative sampling methods which both focus on obtaining good estimation of centers of sub-spaces.

We initially develop a model of centers of sub-spaces. Then, we present our two sampling methods.
 Statistic Model for the Centers of Sub-spaces: Let sc i = sc i;q } , i = 1 ; : : : ; c  X  k denote the center of i th sub-space, where sc i;m corresponds to the m th output attribute. The center for the i th sub-space is computed as sc i = vector of output attributes for a data record belonging to the i th sub-space, and N i denotes the number of data records belonging to the i th sub-space in the data source. As we know, the output attributes of the deep web data source are not directly available, making the sum of vectors of output attributes of data records, and N size of the i th sub-space unavailable. However, the centers for the sub-spaces can be computed based on a sample drawn from the deep web.

In order to estimate the centers for sub-spaces, we define variables for the vectors of output attributes and classification for a data record with respects to each sub-space. Let the variable y i = { y ( O q ) } denote the vector of output attributes for a data record DR with respect to the i th sub-space. For a data object DR :
Let the variable x i denote the classification of data records with respect to the i th sub-space. For a data object DR :
Then, with the definition of variables y i ; x i , the center of the i th sub-space can be computed. Let t i = { t i; 1 ; : : : ; t corresponds to m th output attribute, and c i denote the sum of y for all data records of the data source. The center of i th sub-space, sc attribute, is computed as:
Since the data records are not available directly, t i , and c timated based on a sample. In the stratified sampling where the data records are drawn from the deep web data source independently from each stratum, t i and c i are estimated based on each stratum. Given a sample S = { S 1 ; : : : ; S H } obtained from each stratum, t mated by b t i = { b t i; 1 ; : : : ; b t i;q } : b t where N j denotes the size of j th stratum, and is available from the deep web data source. b y j i ( O m ) represents the estimated mean of y ( O m ) based on S j , the sample drawn from the j th stratum. Simi-larly, c i is estimated by: where b x i j represents the estimated mean of x i based on S sample drawn from the j th stratum.

T HEOREM 4.1. b t i;m and b c i are unbiased estimators for t c [32]: E ( b t i;m ) = t i;m ; m = 1 ; : : : ; q , E ( b c for b t i;m and b c i are: and x i in the j stratum and n j represents the size of sample drawn from j th stratum.

Based on the unbiased estimators b t i ; b c i for t i and c tain an estimator b sc i = { b sc i; 1 ; : : : ; b sc i;q subspace, sc i : T HEOREM 4.2. The estimator b sc i is a biased estimator [32]: where Cov ( b t i;m ; b c i ) is the covariance of estimator
As we can see that the bias of estimator b sc i;m depends on V ar ( which decreases with the sample size of each stratum (as shown in Theorem 4.1). Thus, when the sample size is large, the bias of esti-mator b sc i;m is negligible.
 Integrated Variance: To facilitate our presentation, we introduce the notion of integrated variance . For the i th sub-space in j th stra-tum, and corresponding to the m th output attribute: we define the integrated variance Q j i;m where S 2 ( x j i ), S 2 ( y j i ( O m ) denote the sample variance of x y ( O m ) based on the sample S j , Cov ( y j i ( O m ) ; x sample covariance of x i and y i ( O m ) based on sample S and b c i are the unbiased estimators for t i;m and c i introduced in The-orem 4.1.

L EMMA 4.3. Given a sample S = { S 1 ; : : : ; S H } , the variance of estimator b sc i is: where N j and n j denote the size of population and the size of sample for the j th stratum respectively.
 P ROOF . Please see [23].
 Distance Function: Our goal of sampling method is to obtain a sample so that the distance between the estimated centers { b sc i ; i = 1 ; : : : ; c  X  k } and the true centers SC = { 1 ; : : : ; c  X  k } of sub-spaces is small. To capture the distance between the centers SC and estimators d SC , a distance function is defined as the summation of the expected distance between each center and the corresponding estimator:
The Euclidean distance is utilized to evaluate the distance between sc i and its estimator, b sc i : dist ( c sc i ; sc i ) =
In order to obtain a representative sample from the deep web, we need to obtain a sample so that, Dist 2 ( SC; d SC ) , the distance be-tween estimated centers and the true centers of sub-spaces, is small.
L EMMA 4.4. For centers SC = { sc i ; i = 1 ; : : : ; c  X  spaces and its estimator d SC = { b sc i ; i = 1 ; : : : ; c function is:
Dist 2 ( SC; d SC ) = P ROOF . Please see [23].

As in Lemma 4.4, Dist 2 ( SC; d SC ) is composed of two terms, the variance denoted by SumV ar = the bias, Bias 2 = large, the bias is small compared to the variance of V ar ( Then, our goal of obtaining a sample where Dist 2 ( SC; d reduces to the problem of obtaining a sample where SumV ar is small. Combining Lemma 4.3, we have:
Since SumV ar depends on n j ; j = 1 ; : : : ; c  X  k , the size of sample as-signed to the j th stratum.

Now, we revisit our running example. Recall the data shown in Ta-ble 1, and assume that the input attribute Year of Construction is cho-sen to stratify the input space of the data source. To demonstrate our method, assume 2 strata are generated: Y ear  X  1990 and Y ear &gt; 1990 . Let the set of output attribute be OS = { P rice; SquareF eet A pilot sample of data records with ID 1, 4, 6, 8, 9, 10, 14, and 16, is drawn, and we assume c = 1 and k = 2 . Further, assume that the space of output attributes is partitioned into c  X  k = 2 sub-spaces by two sub-centers (115,000, 1,350) and (230,000, 2,100).

Table 3 shows the integrated variance, Q j i;m , for each stratum and sub-space computed based on the pilot sample that is shown in Ta-ble 2. Columns 2-3 show the value of Q j i;m of output attributes for the first sub-space, whereas, columns 4-5 show the value of Q the output attributes for the second sub-space. Column 6 shows the summation of Q j i;m for each stratum. We will later explain how these values are used by the two sampling methods we have developed. T able 2: A pilot sample for Sub-spaces of Output Attributes Y ear  X  1990 8.8E6 1.05E3 2.7E5 363 9.1E6 Y ear &gt; 1990 0.0 0.0 4.2E6 425 4.2E6 Optimized Sampling Method: We use this method to decide n the size of the sample drawn from j th stratum so that the variance of estimators d SC for centers of sub-spaces SC , SumV ar , is min-imized. As a result, the expected distance between estimated cen-ters d SC and centers SC will be minimized. The total number of data records drawn from all the strata is fixed, which is denoted by n =
L EMMA 4.5. The solution for the objective to be minimized above is
P R OOF . The problem can be solved by using Lagrange multipli-ers , a well know optimization method.

Our optimized sampling method can be viewed as a generaliza-tion of the Neymann method [10], a well-known sample allocation method for stratified sampling. Neymann allocation has been pro-posed in the context of of estimating results for an aggregation query, e.g. sum or mean, on a single statistical variable. It also works only on a population where each data object contains the variable. Our op-timized sampling method is proposed for estimating the mean vector of multiple output attributes within each sub-space, where the size of data records belonging to each sub-space is not known for each stratum.

Returning to our example, recall the integrated variance Q in Table 3. By using the optimized sampling method, with a sample size n = 5 , we will have n 1 = 3 and n 2 = 2 . This implies that three data records will be drawn from the first stratum, and two data records will be drawn from the second stratum. This is because the distribution of the output attributes in the first stratum is spread in a wider area, as compared to the second stratum. Thus, more data records are needed to represent the population of the first stratum. Active Learning based Sampling Method: The second method we propose is the active learning based method. Active learning [14, 11, 36] has been proposed in the context of machine learning, for cases where gathering data is costly and/or time-consuming. Com-pared with passive learning , which selects training data randomly from the entire population, active learning selects certain types of data records, to help build a better model faster. The choice of data records is based on the data that has been seen so far. In other words, the information that has already been learned about the data influ-ences the decision with respect to the data record to be chosen next by an active learner. While mining on the deep web, active learn-ing is clearly desirable, as samples can only be obtained by issuing queries over a network, making sampling cost a very important con-sideration.

The optimized sampling method we have already presented aims at optimally minimizing the expected distance between sc i c  X  k , the centers of sub-spaces and their estimators, in the long run . When the sample size is small, however, it might not have good per-formance. In comparison, our active learning based sampling method greedily reduces the distance between sc i ; i = 1 ; : : : ; c ters of sub-spaces and their estimators. Similar to optimized sam-pling method, our active learning based sampling method aims at reducing SumV ar ( n 1 ; : : : ; n H ) , the variance for estimators in For-mula 5, which is also defined as the risk function Risk ( n Data is sampled for reducing the risk function greedily. The stratum with largest reduction in the risk function value is selected and the as-sociated query is submitted to the data source. For the estimators for sc ; i = 1 ; : : : ; c  X  k , centers of sub-spaces, the estimated decrease of risk function for choosing a data record from the j th stratum is computed as:
Once a data record DR is drawn from the the j th stratum of the deep web, it is classified to one of the c  X  k sub-spaces based on the c  X  k sub-centers computed from the pilot sample. The parameters Q i;m ; i = 1 ; : : : ; c accordingly. The sample size of the j th stratum is also updated: n n j + 1 , while the sample size of other strata stay the same: n n ; r  X  = j .

Now, returning to our running example (Table 3), we have N N 2 = 8 and n 1 = n 2 = 2 . Based on Formula 6, the decrease in the distance function value from choosing the first stratum is 9.7E7, whereas the decrease of the distance function value for choosing the second stratum is 4.4E7. Thus, we will submit a query for the first stratum. As we can see, our active learning based sampling method is also focusing on the stratum where the distribution of output at-tributes is spread in a wider area. However, the difference is that it performs such calculation greedily at each step, rather than determin-ing the number of samples from each stratum separately.
In this subsection, we introduce our representative sampling method based on the proportions of sub-spaces. The proportion of a sub-space of output attributes is computed as the proportion of data records belonging to the sub-space in the entire population of the deep web data source.
 Statistic Model for the Proportion of Sub-Spaces: For i th sub-space, the proportion, which is denoted as pr i , is computed as where N i denotes the number of data records belonging to the i th sub-space, and N denotes the size of the entire population of the deep web data source. The computation for pr i is based on the en-tire population of the deep web. However, since the space of input attributes is stratified into H strata, the computation of the propor-tion of the i th sub-space, pr i , is also based on the H strata. Let pr i;j ; i = 1 ; : : : ; c  X  k; j = 1 ; : : : ; H denote the proportions of the i th sub-space in the j th stratum. Then the proportion of the i th sub-space, pr i = j th stratum, which is available from the information provided by the deep web data source. Since the output attributes of the deep web data source are not directly available, pr i;j ; i = 1 ; : : : ; c 1 ; : : : ; H are unknown, and can only be estimated based on a sample from the j th stratum, which is denoted by S j .

The goal of our sampling method is to obtain a sample where the estimation for proportion of sub-space pr i are good. In our previous work on frequent itemset mining on a deep web data source [22], we proposed an active learning based sampling method which aims at obtaining a sample where the estimation for the support of 1-itemset composed of output attributes are good. As we can see the goal of our sampling method based on the proportion of sub-spaces for cluster-ing on a deep web are similar to the task of frequent itemset mining on a deep web. So, we can develop a similar active learning based sampling method.

In using active learning for our goal, a risk function is defined to evaluate the loss of estimated proportions for sub-spaces, and deter-mine which queries are chosen to be submitted to the data source in the sampling process. Recall that we are focusing on pr 1 ; : : : ; c  X  k; j = 1 ; : : : ; H , the proportions of corresponding sub-spaces in each stratum. Dirichilet distribution [30], is used in our work. In the j th stratum, the proportions of sub-spaces P R { pr i;j ; i = 1 ; : : : ; c  X  k } X  Dirichlet ( 1 ;j ; : : : ; function is estimated based on the parameter of Dirichilet distribution for each stratum.

In the stratified sampling, after a data record is drawn from a stratum, the risk function Risk ( P R ) will be reduced. In our ac-tive learning based sampling method, data is sampled from the deep web for reducing the risk function in a greedy way, step by step. At each step, the stratum with largest reduction in the risk function is selected. The associated query is submitted to the deep web data source for sampling a data record. Once a data record DR is drawn from the deep web, the parameters of Dirichilet distribution of pr is modified, resulting the change in risk function, Risk ( P R ) , which determines the query to be submitted to the deep web in the next step.
Specifically, if a data record DR is drawn from the j th stratum of the deep web, it is classified to one of the c  X  k sub-spaces. The parameters for the Dirichilet distribution of P R j = { pr 1 ; : : : ; c  X  k } X  Dirichlet ( 1 ;j ; : : : ; c  X  k;j j;r + 1 if the sampled data record is classified to the r th sub-space;
We now introduce our method for clustering the data, based on the samples drawn using one of three methods introduced above.
The key observation in our method is that since we are using strati-fied sampling, the sample obtained is not a simple random sample on the entire population. Thus, an original clustering method, like the k-means clustering algorithm, cannot be directly applied on the sam-ple. Instead, we use a novel stratified k-means clustering method. The main idea in our algorithm is that each sampled data record is considered as a representative of the population of the stratum. Let the sample obtained be denoted by N S , and assume n j data records are drawn randomly from j th stratum, where the size of the corre-sponding sub-population is N j . Then, the n j data records are used to represent the N j data records, i.e., each data record in the sample represents N j n clustering process, each data record in the sample is associated with weight of N j n
Our algorithm works in the same way as the original k-means al-gorithm, with the following difference. At each iteration, the center of the i th cluster with the associate data objects is updated as: where ! r denotes the weight associated with data record DR i th cluster.

As in the original k-means clustering algorithm, the result of strat-ified k-means clustering depends on the initial k centers that are chosen randomly from the sample N S . To decrease the effect of bad initial k centers, we conduct the stratified k-means clustering on the sample multiple times with different random initial k centers. The result of stratified k-clustering with the smallest distance within clusters is chosen as the final clustering result for output. In our method, the distance within clusters of a clustering result for k cen-ters c i ; i = 1 ; : : : ; k is computed as: where C i denotes the set of data objects belonging to the i th cluster in the sample.

Similar to traditional k-means clustering, stratified k-means clus-tering is performed iteratively on the sampled data. Initially, k cen-ters are randomly chosen from the sampled data. At each iteration, sampled data objects are grouped into clusters with the smallest dis-tance between the data objects and the corresponding centers. The k centers are updated based on the newly generated k clusters ac-cording to the Expression 7. The process stops if the distance be-tween updated k centers and the current k centers is smaller than a user-defined threshold , or the number of iteration is larger than a user-defined threshold.
Initial experiments with our methods lead to the observation that our stratified k-means clustering method has a significantly lower av-erage square distance if the number of clusters k equals to the num-ber of natural clusters in the data. Moreover, more often than not, users are more interested in the natural clusters, rather than a specific number of cluster centers.

The problem of obtaining the number of natural clusters in datasets has been studied by several researchers [34, 2, 5]. We adapt the ap-proach based on the stability of clusters [2]. Specifically, we take the following approach. For each particular number k , stratified k-means clustering is performed on the data with different initial centers that are randomly drawn from the data. The distance between each pair of clustering results is computed. Then, p ( ) , the proportion of the pair of clustering results with similarity larger than is computed. The k corresponding to the largest value of p k ( )  X  p k +1 ( ) is considered to be the number of natural clusters of the population.
In this section, we evaluate our proposed methods. Particularly, we compare our three methods for representative sampling against two other methods, to evaluate the benefits from the use of stratified clustering and representative sampling.

Our evaluation has been performed using a combination of real and synthetic datasets.
 Synthetic data set: This is a data set generated by minitab tical software. This data set contains 4,000 data records with 6 cate-gorical attributes, including 4 input attributes and 2 output attributes. There are 4 clusters on the 2 output attributes which are generated by a Gaussian distribution. The output attributes are created to be dependent on the input attributes.
 Noisy Synthetic data set: This is a data set generated by adding noise data points , numbering 10% of the total data points, in the syn-thetic data set described above. The noise data points are uniformly distributed in the space of the output attributes.
 Yahoo! data set: The Yahoo! data set, which consists of the data crawled from a subset of a real-world hidden database at http://autos.yahoo.com/. Particularly, we download the data on used cars located within 50 miles of a zipcode address. This yields 8,000 data records. The data consists of 4 categorical input attributes and 1 numerical output attribute. The input attributes contain the age, mileage, brand, and the number of windows of the cars. The output attribute contains the price of the cars.
 US Census data set: This is a data set obtained from the 2008 US Census on the income of US households. This data set contains 40,000 data records with 13 categorical input attributes and 2 numer-ical output attributes. The input attributes contain the Metropolitan status of the household as well as the education, age, occupation, etc, of the husband and wife of each household. The output attributes are the income of husband and wife of the households.
 The criteria used for evaluation of all methods is Average Square Distance . It is computed based on the distance between the estimated centers and true centers of the clusters. For k clusters c puted as AsqDist = 1 k notes the Euclidean distance between c i and  X  c i . Obviously, a smaller average distance value implies that the method is working more ac-curately.
Please see http://http://www.minitab.com
In all results we report, sampling process is repeated 200 times, and the results we report are the average result for these 200 execu-tions.
In this subsection, we focus on evaluating the benefits from rep-resentative sampling and stratified clustering. For this purpose, we compare our methods that combine stratified clustering and repre-sentative sampling based on the partitioning over the space of output attributes. Specifically, our methods include active learning for pro-portions of sub-spaces, optimized sampling that focuses on centers of sub-spaces, and active learning based on centers of sub-spaces. These three methods are compared with two other methods, which are both based on simple random sampling. While both these meth-ods randomly choose data records from the deep web data source, they differ in how clustering is performed. In the first method , clus-tering is directly conducted on the simple random sample, applying the original k-means clustering method. In the second method , strat-ified k-means clustering is conducted on the simple random sample using the tree described in the Section 3.

While reporting the results from our experiments, our active learn-ing based sampling method that focuses on proportions of sub-spaces is referred to as prop_act , the optimized sampling method that fo-cuses on centers of sub-spaces is referred to as cent_opt , and the active learning based method that again focuses on centers of sub-spaces is referred to as cent_act . Among the two baseline meth-ods, the first method described above is referred to as rand , and the second method is referred to as rand_st ,
Figure 2 shows the average square distance achieved during clus-tering for the four datasets. The sub-figure a) shows the result for the synthetic dataset, with number of clusters, k , set to 4, and the number of sub-spaces per cluster, c , set to 3. A pilot sample of size 200 is used. We can see that compared with rand , the average square dis-tance achieved with algorithms rand _ st , prop _ act , cent _ opt , and cent _ act is smaller, and the average decrease of average square dis-tance over all sample sizes for rand _ st , prop _ act , cent _ opt , and cent _ act are 31.8%, 35.6%, 41.0% and 39.2% respectively, which shows the efficiency of stratified k-means clustering over the origi-nal k-means clustering. This also shows that the distribution of input attributes is helpful for identifying the clusters of output attributes, and the stratification on the population improves the performance of clustering. Furthermore, compared with rand _ st , the average square distance with our sampling methods prop _ act , cent _ opt , and cent _ act is smaller, and the average decrease of average square dis-tance over all sample sizes for prop _ act , cent _ opt , and cent _ act are 5.6%, 13.5%, and 10.8% respectively, which shows the benefit of sampling based on a partition over the space of output attributes. We can also observe that compared with prop _ act , cent _ opt and cent _ act have better performance in terms of the average square dis-tance. This is because cent _ opt and cent _ act aim at reducing the expected distance between the estimated centers and the true cen-ters of sub-spaces of output attributes. In addition, when the sam-ple size is large, cent _ opt has better performance than cent _ act , whereas, when the sample size is small, cent _ act has better perfor-mance than cent _ opt . The reason is that cent _ act greedily reduces the expected distance between the estimated centers and true centers of sub-spaces. But, at the same time, it is not an optimized sampling method on the long run, and thus, it does not have as good perfor-mance as cent _ opt when the sample size is large.
 Figure 2, sub-figure b), shows the result for noisy synthetic dataset. Again, we have k = 4 and c = 3 , and a pilot sample of size 200. The results also follow a very similar trend. The stratifica-tion based methods, prop _ act , cent _ opt , cent _ act , and rand _ st all have better accuracy than the original k-means clustering. Compared with rand , the average decrease of average square distance over all sample sizes for rand _ st , prop _ act , cent _ opt , and cent _ act are 26.9%, 35.5%, 37.4% and 38.6% respectively. The representa-tive sampling methods, prop _ act , cent _ opt , and cent _ act , further improves the accuracy over the simple random sampling rand _ st . Compared with rand _ st , the average decrease of average square dis-tance over all sample sizes for prop _ act , cent _ opt , and cent _ act are 11.8%, 14.4% and 16.1% respectively. Representative sampling methods that focus on a good estimation of the centers of sub-spaces, cent _ opt and cent _ act , have better performance than prop _ act . Optimized sampling method cent _ opt has better performance than the active learning based sampling method cent _ act when the sam-ple size is large.

Figure 2, sub-figure c) shows the results on the Yahoo! data set by setting k = 4 and c = 2 , with a pilot sample of 300. The result follows a very similar trend to those in the sub-figure a). The strat-ified sampling methods have better performance than the original k-means clustering. Compared with rand , the average decrease of av-erage square distance over all sample sizes for rand _ st , prop _ act , cent _ opt , and cent _ act are 7.2%, 13.2%, 15.0% and 16.8% respec-tively. But the improvements in accuracy are relatively lower. This is likely because the relationship between the input attributes and the output attributes is not as strong for this dataset. Furthermore, rep-resentative sampling methods improves the accuracy over the sim-ple random sampling. Compared with rand _ st , the average de-crease of average square distance over all sample sizes for prop _ act , cent _ opt , and cent _ act are 6.6%, 8.5%, 10.5% respectively. Fi-nally. the sub-figure d) shows the results on the US census dataset by setting k = 3 and c = 2 , with a pilot sample of 2000. We can also observe the same trend to those in the sub-figure a). Compared with rand , the average decrease of average square distance over all sam-ple sizes for rand _ st , prop _ act , cent _ opt , and cent _ act are -3.3%, 30.2%, 31.5% and 54.3% respectively. Compared with rand _ st , the average decrease of average square distance over all sample sizes for prop _ act , cent _ opt , and cent _ act are 32.5%, 33.8%, 55.8% re-spectively.
In this subsection, we focus on evaluating the scalability of our methods by applying them on data sets with different sizes. For this purpose, three data sets with size 40,000, 400,000 and 4000,000 are generated by performing sampling with replacement on the synthetic data set. Then we apply our three representative sampling meth-ods, which are denoted by prop _ act , cent _ opt and cent _ act . For each data set with population of size N , an initial sample with size 0 : 05  X  N is obtained and a further sample of size 0 : 1 based on the representative sampling methods. For each represen-tative sampling method on each data set, the time for stratification on the query space of input attributes, computing the representative sampling and stratified clustering is computed and reported in Ta-ble 4. The sampling process is repeated 10 times, and the results are the average result for these 10 executions.
 Table 4: Evaluation of Scalability of Representative Sampling and Stratified Clustering
As we can see, for each method, when the size of data set is in-creased by 10 times, the execution time for each method is increased by around 12 times, i.e. the increase is quite close to linear. This shows the scalability of our methods.
In this subsection, we evaluate our sampling methods with respect to different choices of k , the number of clusters. Table 5 shows the experimental data for the three datasets: the synthetic dataset, the Ya-hoo! dataset and the US. census dataset. In this table, the first column identifies the dataset. The second column shows the choice of k , and the third column shows the average value of p ( ) , the proportion of the pairs of clustering results with similarity larger than = 0 : 9 (as described in Subsection 4.4). The fourth to sixth columns show the average square distance for the three representative sampling meth-ods, i.e., prop _ act , cent _ opt , and cent _ act .

Rows 2-4 show the results for the synthetic dataset obtained by $YHUDJH6TXDUH'LVWDQFH $YHUDJH6TXDUH'LVWDQFH
US Census choosing c = 3 , taking a sample size of 500 and a pilot sample size of 200. From the table we can see that p ( ) has a large value at k = 4 and decreases sharply when we go from k = 4 to k = 5 . Based on this observation, we can consider k = 4 to the number of natural cluster in the dataset. On the contrary, when k = 3 and 5 , p ( ) is small. Another observation we can make is that at k = 4 , the average square distance is also small, whereas at k = 3 and k = 5 , the average square distance is larger.

These observations show that our methods have good performance when k is chosen to be the number of natural clusters, which can be detected during the execution of the algorithm by considering the value of p ( ) . Thus, we not only detect the number of natural clus-ters, but can also reduce the inaccuracy in estimating their centers.
Rows 5-7 show the results for the Yahoo! dataset, obtained by by choosing c = 2 , taking a sample size of 600 and a pilot sample size of 300. The results for the Yahoo! dataset are similar to the results for the synthetic dataset. The largest decrease in the value value p ( ) in group from k to k +1 clusters occurs when k = 2 . Correspondingly, the average square distance is also lowest at this point.
Similarly, rows 8-10 show the results for the US dataset, obtained by choosing c = 2 , sample size of 5000 and a pilot sample size of 2000. The result are similar to those obtained for the other two datasets. The largest decrease of value of p ( ) in going from k to k + 1 occurs where k = 3 . Our sampling methods also achieve the best accuracy at k = 3 , which also happens to be the number of natural clusters in the dataset.
We can make several observations from the experimental results we have presented. By comparing rand _ st , prop _ act , cent _ opt , and cent _ act with rand , we can see that stratified k-means clus-tering has better performance than traditional k-means clustering. Compared with rand , the decrease in the average square distance for stratified sampling methods is up to 54%. Furthermore, compared with rand _ st , the representative sampling methods, prop _ act , cent _ opt , and cent _ act improve the accuracy of clustering, with the de-crease in average square distance ranging from 5% to 55%.
In comparing the three representative sampling methods, we can see the following. As cent _ opt and cent _ act aim at obtaining a good estimation for centers of the sub-spaces, they have better per-formance than prop _ act , which aims at obtaining good estimation for proportions of sub-spaces. But since there are more parameters to be estimated in cent _ opt and cent _ act , when c is large, prop _ act has a better performance than cent _ opt and cent _ act . Furthermore, since cent _ act greedily reduces the risk function, it has better per-formance when the sample size n is small. When the sample size n is large, cent _ opt , the optimized method, has better accuracy.
We now compare our work with the existing work on sampling based methods for clustering, other data mining problems, and sam-pling work specifically targeting the deep web.
 Sampling for Clustering: As we had listed in the Introduction, sam-pling for clustering has been studied by several researchers [19, 21, 29, 15, 4]. CLARA (Clustering LARge Applications) [19] uses sam-pling to find the potential medoids of the data. The entire dataset is assigned to potential medoids, and the best system of medoids is computed according to a particular objective function. Kollios et al. [21] investigate the use of biased sampling according to the den-sity of the data set to speed up the operation of general data min-ing tasks, including clustering, on large multidimensional data sets. Meek et al. [29] examine the application of the learning-curve sam-pling method to the task of model-based clustering via the expectation-maximization (EM) algorithm. A fast and exact k-means cluster-ing [15] is proposed to compute the same cluster centers as those reported by the original k-means algorithm. Shai et al. [4] present a theoretic analysis of sample-based clustering algorithms and prove that clustering methods, including k-means and k-median, converge to optimal clusterings with approximation bounds that depend on the sample sizes and accuracy parameters, but are independent of the generating data distribution and the dimensionality of the data. Our work is different from these sampling methods, since we consider the problem of clustering on a hidden deep web. Because the data records are hidden under limited query interfaces in these systems and the sampling cost is high, sampling involves very distinct chal-lenges.
 Sampling for Other Data Mining Problems: Sampling for fre-quent itemset mining has been studied by several researchers [35, 31, 9, 6]. Toivonen [35] developed a random sampling method to iden-tify the association rules, which are then further verified on the entire database. Progressive sampling [31], which is based on equivalence classes, involves determining the required sample size for associa-tion rule mining. FAST [9], a two-phase sampling algorithm, has been proposed to select representative transactions, with the goal of reducing computation cost in association rule mining.

Sampling for outlier detection has also been studied [38, 1]. Wu et al. [38] have developed a sampling algorithm to efficiently de-tect distance-based outliers in domains where distance computation is very expensive. Abe et al. [1] present an approach to outlier detec-tion based on classification. Their method invokes a selective sam-pling mechanism based on active learning to the reduced classifica-tion problem.
 Hidden Web Sampling : There have been several recent research ef-forts [3, 12, 13, 24] on sampling from the deep web. Dasgupta et al. [12, 13] proposed HDSampler, a random walk scheme over the query space provided by the interface, to select a simple random sample from hidden database. Bar-Yossef et al. [3] proposed algo-rithms for sampling suggestions using the public suggestion inter-face. Our algorithm is different from their work, since our goal is sampling in the context of particular data mining tasks. We focus on achieving high accuracy for a specific task, instead of simple random sampling.
As a growing amount of useful information is available on the deep web, clustering such hidden data can provide useful summaries to various interested parties. This paper has presented a novel method-ology for clustering data over a deep web data source. The input space of a deep web data source is initially stratified for both im-proving the effectiveness of sampling and utilizing the relationship between the input and the output attributes. The space of output at-tributes of a deep web data source is partitioned into sub-spaces, and three representative sampling methods are proposed. The goal of each of these three methods is achieving a good estimation of the statistics , which is either proportions or the centers, of sub-spaces of output attributes. Two of three approaches we have presented involve a novel application of active learning.

We have evaluated our method using two synthetic and two real datasets. Our comparison shows significant gains in estimation accu-racy from both the novel aspects of our work, i.e., the use of stratified clustering and representative sampling.
