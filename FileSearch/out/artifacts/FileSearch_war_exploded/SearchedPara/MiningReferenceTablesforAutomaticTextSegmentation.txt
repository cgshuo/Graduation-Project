 Automatically segmenting unstructured text strings into structured records is necessary for importing the information contained in legac y sources and text collections into a data warehouse for subsequent querying, analysis, mining and inte gration. In this paper , we mine tables present in data warehouses and relational databases to develop an automatic segmentation system. Thus, we overcome limitations of existing supervised text segmentation approaches, which require comprehensi ve manually labeled training data. Our segmentation system is rob ust, accurate, and efficient, and requires no additional manual effort. Thorough evaluation on real datasets demonstrates the rob ustness and accurac y of our system, with segmentation accurac y exceeding state of the art supervised approaches.
 H.2.8 [ Database Management ]: Database Applications X  Data min-ing ; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, design, performance, experimentation Text segmentation, machine learning, text management, information extraction, data cleaning
Information in unstructured text needs to be con verted to a struc-tured representation to enable effecti ve querying and analysis. For example, addresses, bibliographic information, personalized web serv er logs, and personal media filenames are often created as unstructured strings that could be more effecti vely queried and analyzed when imported into a structured relational table. Building and maintaining lar ge data warehouses by inte grating data from such sources requires automatic con version, or segmentation of text into structured records of the tar get schema before loading them into relations.

Informally , the problem of segmenting input strings into a struc-tured record with a given n -attrib ute schema is to partition the string into n contiguous sub-strings and to assign each sub-string to a unique attrib ute of the schema. For instance, segmenting the input string  X  X e gmenting text into structured records V. Borkar , Deshmukh and Work done at Microsoft Research.
 Cop yright 2004 ACM 1-58113-888-1/04/0008 ... $ 5.00.
 Sara wagi SIGMOD X  into a bibliographic record with schema [Au-thors, Title, Conference, Year] requires the assignment of the sub-string  X  X  . Borkar , Deshmukh and Sara wagi X  to the Authors attrib ute, the sub-string  X  X e gmenting text into structured records X  to the Ti-tle attrib ute,  X  X IGMOD X  to the Conference attrib ute, and the NULL value to the Year attrib ute.

Current techniques for automatically segmenting input strings into structured records can be classified into rule-based and supervised model-based approaches. Rule-based approaches require a domain expert to design a number of rules and maintain them over time. This approach does not scale as deplo yment for each new domain requires designing, crafting, deplo ying, and maintaining a new set of rules. Supervised approaches alle viate this problem by automatically learning segmentation models from training data consisting of input strings and the associated correctly segmented tuples [5]. Ho we ver, it is often hard to obtain training data, especially data that is com-prehensi ve enough to illustrate all features of test data. This prob-lem is further exacerbated when input test data as in our tar get data warehouse scenario is error prone; it is much harder to obtain com-prehensi ve training data that effecti vely illustrates all kinds of errors. These factors limit the applicability and the accurac y of supervised approaches.

In this paper , we exploit refer ence relations | relations consisting of clean tuples | in typical data warehouse environments. For exam-ple, most data warehouses include lar ge customer and product tables, which contain examples that are specific to the domain of interest. Reference relations can be a source of rich vocab ularies and structure within attrib ute values. Our insight is to exploit such widely available reference tables to automatically build a rob ust segmentation system. Note that we do not attempt to matc h the newly segmented records with the tuples in the reference table. Record matching and dedupli-cation are dif ferent problems and are not the focus of this paper .
Our approach relies only on reference tables: we do not need the association between unse gmented input strings and the correspond-ing segmented strings (labeled data) that supervised systems require for training. Current operational data warehouses and databases do not maintain such associations between input data and the actual data in the database, because input data goes through a series of poten-tially comple x transformations before it is stored in the database.
Building segmentation models from clean standardized tuples in a lar ge reference table requires three critical challenges to be ad-dressed. The first challenge is that information in reference rela-tions is typically clean whereas input strings may contain a variety of errors: missing values, spelling errors, use of inconsistent abbre-viations, extraneous tok ens, etc. [18]. Therefore, the challenge is to learn from clean reference relations, segmentation models that are ro-bust to input errors. The second challenge is that we do not kno w the order in which attrib ute values in an input string are specified. In the data warehouse maintenance scenario, the order in which attrib ute values are concatenated by an address data source may be [State, Zip-code, City , Name, Address] while another source may concatenate it in the order [Name, Address, City , Zip, State]. Another common ex-ample is bibliographic data: some sources may order attrib utes for each article as [authors, title, conference, year , pages] while other sources may order them as [title, authors, conference, pages, year]. Therefore, for an unsupervised segmentation system to be deplo yed over a variety of data sources it has to deal with dif ferences in input orders by automatically detecting the attrib ute order . The third chal-lenge is that reference tables can usually be very lar ge and consist of millions of tuples. Therefore, to effecti vely exploit lar ge vocab-ularies and rich structural information in lar ge reference tables the algorithm for building a segmentation model from lar ge reference ta-bles has to be efficient and scalable. Note that training efficienc y is not usually an issue with supervised approaches: manually labeling data is time-consuming and expensi ve and hence training datasets are much smaller than sizes of available reference tables.

Due to the abo ve challenges, it is not possible to directly adapt ex-isting supervised approaches to the  X  X earn from reference table only X  scenario. First, we cannot use data in a reference table to prepare a training dataset for current supervised approaches because we do not kno w the order in which attrib ute values would be observ ed in test input strings. In fact, current supervised models hea vily rely on the kno wledge of input attrib ute order to achie ve high accurac y [5]. Second, data in reference tables is usually clean whereas input data observ ed in practice is dirty . And, making a design goal of rob ustness to input errors during training itself impro ves the overall segmenta-tion accurac y. Third, current supervised text segmentation systems are designed to work with small labeled datasets (e.g., by using cross validation) and hence tend not to scale to training over lar ge reference tables.

The architecture of our automatic segmentation system CRAM 1 is sho wn in Figure 1. We adopt a two-phased approach. In the first pre-pr ocessing phase, we build an attrib ute reco gnition model over each column in the reference table to determine the probability with which a (sub-)string belongs to that column. This process can be customized with a domain-specific tok enizer and a feature hierar -chy (i.e., tok en classification scheme such as  X  X umber X ,  X  X elimiter X , etc.). For example, the recognition model on the  X  X ip Code X  col-umn of an address relation indicates that the probability of a five-digit number (e.g., 12084 ) being a valid zip code is 0.95 whereas that of a word lik e Timb uktoo is only 0.005 . Models on all columns can be used together to determine the best segmentation of a given input string into sub-strings. In the second run-time segmentation phase, we segment an input string s into its constituent attrib ute val-ues s 1 ; : : : ; s n and assign each s i to a distinct column such that the quality of the segmentation is the best among all possible segmenta-tions.
 Contrib utions: In this paper , we develop a rob ust segmentation sys-tem which can be deplo yed across a variety of domains because it only relies on learning the internal attrib ute structure and vocab ular -ies from widely available reference tables. We introduce rob ust and accurate attrib ute recognition models to recognize attrib ute values from an attrib ute, and develop efficient algorithms for learning these models from lar ge reference relations (Section 4). We then develop 1 CRAM: C ombination of R obust A ttrib ute M odels an algorithm that effecti vely uses rob ust attrib ute recognition models to accurately and efficiently segment input strings (Section 5). Fi-nally , we present a thorough experimental evaluation of CRAM on real datasets from a variety of domains and sho w that our domain-independent segmentation system outperforms current state of the art supervised segmentation system (Section 6).
Extracting structured information from unstructured text has been an acti ve area of research, culminating in a series of Message Under -standing Conferences (MUCs) [2, 17] and more recently ACE eval-uations [21]. Named entity recognition systems extract names of en-tities from the natural language text (e.g., PERSON names, LOCA-TIONs, ORGANIZA TIONs). Detecting entities in natural language text typically involv es disambiguating phrases based on the actual words in the phrase, and the text conte xt surrounding the candidate entity . Explored approaches include hand-crafted pattern matchers (e.g., [1]), rule learners (e.g., [3, 23]), and other machine learning approaches (e.g., [11]). In contrast, strings in our segmentation sce-nario are not necessarily natural language phrases, and do not contain surrounding conte xtual text.

Work on wr apper induction (e.g., [12, 19, 24]) exploits layout and formatting to extract structured information from automatically gen-erated HTML pages, as well as heuristics and specialized feature hi-erarchies to extract boundaries between records [14]. In contrast, the input strings in our problem are short and have no obvious mark ers or tags separating elements. Furthermore, all of the string except for the delimiters must be assigned to some of the tar get attrib utes. As pre viously sho wn by Borkar et al. [5], the required techniques there-fore dif fer from traditional named entity tagging and from wrapper induction.

Hidden Mark ov Models (HMMs) are popular sequential models [4], and have been used extensi vely in information extraction and speech recognition. Since the structure of HMMs is crucial for effecti ve learning, optimizing HMM structure has been studied in the conte xt of IE and speech recognition (e.g., [15, 27].

The problem of rob ustness to input errors has long been studied in speech recognition. Some approaches include filtering out noise dur -ing pre-processing and training the system in artificially noisy con-ditions (error injection) [13]. Noise filtering from speech recognition cannot be adapted to text segmentation directly , since in our scenario the input errors are not separable from actual content. To the best of our kno wledge, we are the first to explicitly address input errors in the conte xt of text segmentation.

Past work on automatic text segmentation most closely related to ours is the DATAMOLD system [5] and related text segmentation approaches (e.g., [22] and [28]). These are supervised approaches and hence share the limitations discussed earlier .

In this paper , we present a novel, scalable, and rob ust text segmen-tation system CRAM. Our system requires only the tar get reference table and no explicitly labelled data to build accurate and rob ust mod-els for segmenting the input text strings into structured records.
In this section, we define the segmentation problem and introduce preliminary concepts required in the rest of the paper . We define the  X  X uality X  of segmentation of an input string into attrib ute value sub-strings as a function of the  X  X robabilities X  of these sub-strings belonging to corresponding attrib ute domains. Our goal now is to se-lect the segmentation with the best overall probability . The probabil-ity of a sub-string assignment to an attrib ute domain is estimated by a model called Attrib ute Reco gnition Model (described belo w) asso-ciated with each attrib ute. Consider an input string  X  X  almart 20205 S. Randall Ave Madison 53715 WI X  which has to be segmented into Or ganization Name , Str eet Addr ess, City , State , and Zipcode attrib ute values. For example, the attrib ute recognition model for Or ganiza-tion Name may assign respecti ve probabilities of 0.9 and 0.15 to sub-strings  X  X  almart X  and  X  X  almart 20205.  X  If the combination (say , product) of indi vidual probabilities of the segmentation  X  X  almart X  as Or ganization Name ,  X 20205 s. randall ave X  as Str eet Addr ess ,  X  X adi-son X  as City ,  X 53715 X  as Zipcode , and  X  X I X  as State has the highest numeric value, we output this segmentation of the given input string. The combination of probabilities has to be invariant to changes in the order in which attrib ute values were concatenated. This is in contrast to traditional (tagging) approaches, which rely hea vily on the order in which attrib ute values are concatenated (e.g, [5, 10]). In the rest of the paper , we use R to denote a reference relation with string-v alued attrib utes (e.g., varchar types) A 1 ; : : : ; A n .
 Attrib ute Reco gnition Model (ARM) : An attrib ute recognition model AR M i for the attrib ute A i is a model for the domain of AR M i ( r ) for any given string r is the probability of the domain of A i .
 Optimal segmentation of an input string : Let R be a reference re-lation with attrib utes A 1 ; : : : ; A n and AR M 1 ; : : : ; AR M respecti ve attrib ute recognition models. Given an input string segmentation problem is to partition s into s 1 ; : : : ; s them to distinct attrib utes A s 1 ; : : : ; A sn such that n is maximized over all valid segmentations of s into n substrings. A similar model that assumes partial independence of attrib ute segmen-tations was independently developed in [9].

Note that the order of attrib utes A s 1 ; : : : ; A sn may be dif ferent from the order of the attrib utes A 1 ; : : : ; A n specified in the reference table. Attrib ute constraints for R (e.g., maximum attrib ute length) can also be incorporated into this model. And, information about the order in which attrib ute values are usually observ ed can also be incorporated. For example, if we kno w that the street address value usually follo ws the name attrib ute value, we can potentially bias the assignment of consecuti ve sub-strings, say  X  X  almart X  and  X 20205 S. Randall Ave, X  to name and to street address attrib utes, respecti vely .
Let tok be a tok enization function which splits any string into a sequence tok(s) of tok ens based on a set of user -specified delimiters (say , whitespace characters). The tok en set of a string all tok ens in s . For example, tok(v[1]) of the tuple [Boeing compan y, Seattle, WA, 98004] is [boeing, compan y], and f boeing, compan y is the tok en set. Observ e that we ignore case while generating tok ens. The dictionary D i of the attrib ute A i of R is the union of tok en sets of all attrib ute values in the projection R [ i ] of R on of the paper , we assume that strings can be segmented only at tok en boundaries.
 Hidden Mark ov Models : A Hidden Mark ov Model (HMM) is a probabilistic finite state automaton encoding the probability distri-bution of sequences of symbols each dra wn from a discrete dictio-nary [25]. Figure 3(a) sho ws an example HMM. For a sequence of symbols each dra wn from the probability distrib ution encoded by a HMM, we can compute the probability of observing s . A HMM comprises a set of states and a dictionary of output symbols. Each state can emit symbols from the dictionary according to an emission probability distrib ution for that state and pairs of states are connected by directed edges denoting transitions between states. Further , edges are associated with transition probabilities. HMMs have two special states: a start state and an end state. The probability of observing a string s = o 1 ; : : : ; o k of symbols dra wn from the dictionary , is the Figur e 2: A sample generalized dictionary instantiated for the Street Address column sum of probabilities of all paths from the start state to the end state with k transitions. The probability of any path p is the product of all transition probabilities on each transition in p and the emission prob-abilities of observing the i th symbol o i at the i th state on with the highest probability is usually considered the path that gener -ated the string s . The set of states and the set of transitions constitute the topolo gy of a HMM. For any given application, the topology is usually fix ed a priori. 2 The emission and transition probabilities are then learned during a training phase over the training data. Featur e Hierar chy : A HMM built over a dictionary of an attrib ute cannot be directly used for computing probabilities of sequences with unkno wn tok ens. Ho we ver, the set of base tok ens in a dictionary can be generalized to recognize unkno wn tok ens [5]. For example, it may be suf ficient to see a 5 -digit number optionally follo wed by a number to recognize zip codes. The successi ve generalization of fea-tures (e.g., from 5 -digit numbers to all numbers) is usually encoded as a featur e hier archy . In this paper , we use a hierarchy which is similar to the feature hierarchy emplo yed in [5]. In our hierarchy the lower levels are more specific than higher levels. At the top level there is no distinction among symbols; at the next level the y are di-vided into classes  X  X  ords,  X   X  X umbers,  X   X  X ix ed,  X  and  X  X elimiters.  X   X  X  ords,  X   X  X umbers X  and  X  X ix ed X  are then divided into sub-classes based on their lengths. For example, the class of words consisting of 10 or less characters (denoted [a-z] f 1-10 g ) is abo ve the class of words consisting of 9 or less characters (denoted [a-z] All base tok ens are at the leaf levels of the feature hierarchy . To dis-tinguish base tok ens from the generalized elements in the discussion, we refer to the non-leaf elements in the feature hierarchy as featur e classes . We say that that a tok en t minimally belongs to a feature class f if t belongs to f but not to any feature class that is a descen-dant of f . For example, the Zipcode value 21934 is said to minimally belong to the feature class 5 -digit numbers.

Observ e that it is possible to input domain-specific feature hier -archies to CRAM in the pre-processing phase, but as we sho w ex-perimentally , the def ault hierarchy is suf ficient for a wide range of domains.
 Generalized Dictionary : The gener alized dictionary consists of all elements in the feature hierarchy in addition to the dictionary of base tok ens. Formally , the gener alized dictionary of an attrib ute is the union of the dictionary D i of A i and the set of feature classes in the feature hierarchy . Henceforth, we use the term dictionary to denote the generalized dictionary unless otherwise specified. Note that while the feature hierarchy itself is fix ed across domains, the generalized dictionary is instantiated for each attrib ute automatically during training from the pro vided reference tables. A sample of a 2 Recently , techniques based on cross validation were developed to identify a good topology from among a tar get class of topolo-gies [15].
Figur e 3: Example HMM Model (a), and our ARM Model (b) generalized dictionary instantiated for the Str eet Addr ess attrib ute is sho wn in Figure 2.
In this section, we discuss the efficient construction of rob ust at-trib ute recognition models from a reference relation. Recall that an attrib ute recognition model assigns probability with which a string or a sequence of tok ens belongs to the attrib ute domain. Therefore, we adopt the class of hidden mark ov models (HMMs), a popular class for modelling sequences of elements, for instantiating attrib ute recognition models. Instantiating an HMM requires us to define (i) the topology consisting of a set of states and the set of transitions among them, and (ii) the emission probabilities at each state and the transition probabilities between states. In this section, we describe the topology we adopt for instantiating ARMs and the computation of emission and transition probabilities. Our primary focus in this de-sign is (i) to impro ve the rob ustness of segmentation to input errors, and (ii) to develop an efficient and scalable algorithm for building rob ust attrib ute recognition models.

We now discuss the intuition behind the principle, which we call specificity , that we exploit to mak e ARMs more rob ust to input er-rors. A more  X  X pecific X  attrib ute recognition model assigns higher probabilities only to very few selecti ve tok en sequences. ARMs can be specific in three aspects: positional specificity , sequential speci-ficity , and tok en specificity . We illustrate these notions with an ex-ample. Consider an HMM example in Figure 3(a). That a tok en in the street address value ending in  X  X h X  X t X  can only be in the second position is an example of positional specificity . The probability of acceptance is much lower if such a tok en ending in  X  X h X  X t X  appears in the third position instead of the second position. A tok en ending in  X  X h X  or  X  X t X  can only follow a short word and tok ens  X  X t, rd, wy , blvd X  can only follow a tok en ending in  X  X h X  or  X  X t X  are examples of sequential specificity . Note that sequential specificity stresses the sequentiality | of a tok en follo wing another | and is orthogonal to the positionality of the tok ens. That the last state can only accept one of  X  X t, rd, wy , blvd X  is an example of tok en specificity .
Ev en though highly specific models may be required for some ap-plications, attrib ute recognition models need only be specific to the extent of being able to identify an attrib ute value as belonging to the correct attrib ute and distinguish it from other domains. Moreo ver, being overly specific in recognizing attrib ute values may cause the at-trib ute recognition model to reject (i.e., assign very low probability) attrib ute values with errors, thereby resulting in incorrect segmenta-tions. Often, we can trade specificity off for achie ving rob ustness to input errors. Ho we ver, the challenge is to mak e the tradeof f with-out losing segmentation accurac y and at the same time being able to build the model efficiently .

In this section, we instantiate an accurate and rob ust attrib ute recog-nition model. We first describe the topology of ARMs, and then de-scribe the techniques for relaxing sequential and tok en specificity . Finally , we describe a procedure for learning such a model from a lar ge reference table.
The topology of a hidden Mark ov model, consisting of the set of states and valid transitions between these states, has a big impact on the accurac y of the model. Recently , techniques based on cross-validation and stochastic optimization have been proposed to auto-matically decide good topologies [16, 5]. Ho we ver, these structure optimization techniques require several scans of the training data, and hence are slo w when training data is lar ge. Moreo ver, these techniques | if trained on  X  X lean X  data | can result in positionally specific topologies (e.g., Figure 3(a)), conditioned to accept tok ens in specific positions, say , first or third or last. Such topologies, even though accurate for segmenting clean input data, can be less rob ust towards erroneous input. In this section, we discuss a statically fix ed topology that (i) enables efficient model building and (ii) relax es po-sitional specificity in favor of rob ustness to input errors.
We observ e that collapsing positional information into a small number of distinct cate gories results in a more flexible, compact, and rob ust ARM topology . More specifically , we cate gorize tok ens in at-trib ute values into three positions: Be ginning , Middle , and Trailing positions, resulting in what we will call the BMT topology , sho wn in Figure 3(b). 3 In the example  X 57th nw 57th st X  string correspond-ing to a street address, the tok en  X 57th X  is cate gorized as beginning tok en,  X  X t X  as the trailing tok en, and the rest as middle tok ens.
Collapsing tok en positions into these three cate gories, we gain ef-ficienc y while building ARMs by avoiding a computationally expen-sive search for  X  X ptimal X  topology . We also gain rob ustness to sev-eral common types of input errors | tok en deletions, tok en insertions, and tok en re-orderings. For example, the probability of observing a tok en 57 th as the second or third tok en in  X  X w 57th 57th st X  is the same for both occurrences of the tok en. Observ e that we still are spe-cific about the positionality of the beginning and trailing tok ens be-cause these are critical for correctly recognizing boundaries between attrib ute values. 4 And, by not grouping boundary tok ens with the middle tok ens, we are able to collect more specific statistics on the emission and transition probabilities for boundary tok ens. Our em-pirical evaluation sho ws that the BMT topology captures the salient structure required for rob ust segmentation, and performs as well as, and sometimes better than, other topologies involving multiple mid-dle positions.

The cate gorization of tok ens into positions induces a cate goriza-tion on the (generalized) dictionary of an attrib ute. The dictionary D i corresponding to an attrib ute A i is now cate gorized into the be-ginning, middle, and trailing dictionaries D B example, a tok en occurring in the beginning position of an attrib ute value of any tuple in R belongs to the D B Set of States and Possible Transitions : The set of states in an ARM model is also cate gorized into beginning , middle , and trailing states. Each cate gory consists of a state s for each element e (base tok en or feature class) in the corresponding cate gorized (generalized) dic-tionary , and s emits only e with non-zero probability . The union of all three cate gorized states along with the special start and end states constitutes the set of states in an ARM. The broad structure of the set of allo wed transitions is sho wn in Figure 3(b). Each cate gory 3 The cate gorization can be generalized to more sophisticated topolo-gies with multiple middle positions. Ho we ver, our experiments (not included due to space constraints) sho wed that the simplest BMT topology is equally good. 4 This intuition is corroborated by the human ability to recognize words even if all but boundary characters are randomly rearranged:  X  X occdrnig to a rscheearch at an Elingsh uinervtisy , it deosn X  t mttaer in waht oredr the ltteers in a wrod are, the oln y iprmoetnt tihng is taht frist and lsat ltteer is at the rghit pclae. The rset can be a toatl mses and you can sitll raed it wouthit porbelm. Tihs is bcuseae we do not raed erv ey lteter by it slef but the wrod as a wlohe.  X  beginning, middle, and trailing | may consist of several states in the HMM but the transitions among these state cate gories are re-stricted to non-backw ard transitions, as indicated by the arro ws in Figure 3(b). That is, beginning states can only transition to either middle or to trailing or to the end states, middle states to middle or to trailing or the end states, and trailing states only to the end state.
Observ e that by assigning a state to each tok en or feature class, we encode transition probabilities more accurately than the usually adopted approach grouping all base tok ens into one state. Thus, we are better able to exploit lar ge sets of examples in reference tables. For example, grouping base tok ens  X  X t, hwy X  into one BaseT oken state also collapses all transitions from pre vious states (say ,  X 49th, hamilton, SR169 X ) to any of these indi vidual tok ens into one transi-tion. It is possible that the states (e.g.,  X  X R169 X ) transitioning into the tok en  X  X wy X  are very dif ferent from the states (e.g.,  X 49th, hamil-ton X ) transitioning into the tok en  X  X t.  X  Grouping several base tok ens into one BaseT oken state loses the ability to distinguish among tran-sitions. Therefore, we associate one base tok en per state. The cost of associating a tok en per state is that the number of transitions in-creases. Ho we ver, we sho w in Section 6.5 that the resulting transition matrices are very sparse, and hence the comprehensi ve ARM models easily fit in main memory .
 Emission and Transition Pr obabilities : To complete the instanti-ation of an attrib ute recognition model AR M i on attrib ute need to define the emission probabilities at each state and the tran-sition probabilities between states. Since we in include in a state s per element (base tok en or feature class) e in the cate-gorized feature hierarchy , the emission probability distrib ution at is: P ( x j e ) = 1 if x = e and the position of x within that of e within the attrib ute value are identical, and 0 Section 4.4, we describe an algorithm for learning, during the pre-processing phase, the transition probability distrib ution from the ref-erence table. In the next section, where we describe the relaxation of sequential specificity for rob ustness, we assume that these probabili-ties are kno wn.
Consider the example path in Figure 3(a) consisting of a  X  X hort word,  X  a  X  X umber ending in th or st, X  and a tok en in the set rd, wy , st, blvd, which accepts the string  X  X w 57th st X  with a high proba-bility . Ho we ver, its erroneous versions  X 57th st, X   X  X w57th st, X   X  X w 57th,  X   X  X w 57th 57th st X  have a very low acceptance probability due to the sequential specificity of the model in Figure 3(a): that a spe-cific tok en has to follo w another specific tok en. Therefore, erroneous transitions (from the state accepting  X 57th X  to the end state, from the start state to the state accepting  X 57th X ) have a very low probability .
Our approach for trading sequential specificity for rob ustness is to  X  X djust X  attrib ute recognition models trained on clean data by creat-ing appropriate states and transitions in an ARM to deal with some of the commonly observ ed types of errors: tok en insertions, tok en dele-tions, and missing values [18]. Our adjustment operations exploit the uniform BMT topology and are illustrated in Figure 4. In order to deal with erroneous tok en insertions, we cop y states along with transitions to and from these states in the beginning and trailing po-sitions to the middle position (as sho wn in Figure 4(a)). We add low probability (according to the Good-T uring estimate [8]) transitions to the states copied from the beginning position from all states in the beginning position. Similar transitions to trailing states are added from the states copied from the trailing position. In order to deal with erroneous tok en deletions, we cop y states and associated transi-tions from the middle position to the beginning and trailing positions (as sho wn in Figure 4(b)). In order to deal with missing attrib ute val-ues, we create a transition (as sho wn in Figure 4(b)) directly from the Figur e 4: Illustration of rob ustness operations: Reco ver Inser -tions (a), Deletions (b), Missing Values (c) start state to the end state. Observ e that the implementation of these operations on a non-uniform topology would be very comple x. Other common errors are addressed by other characteristics of ARMs. Our begin-middle-trailing topology is designed to be rob ust to tok en re-orderings, especially , those in the middle position. Spelling err ors are handled through tok en specificity relaxation as we describe next.
We now describe an approach for relaxing tok en specificity to ac-count for unkno wns or rare tok ens. Our approach is a departure from the smoothing approach used by Borkar et al. [5]: during training, we propa gate base tok en statistics to all matching feature classes. At runtime, if the observ ed tok en t is in the dictionary it is mapped directly to the appropriate state in the ARM; otherwise, the tok en is generalized to the minimal feature class that accepts t explicitly address spelling err ors , we have experimented with match-ing unseen base tok ens to states corresponding to kno wn base tok ens according to standard distance measures such as edit distance (e.g.,  X  X treet X  with  X  X treat X ). Ho we ver, our experiments did not sho w any accurac y impro vement over our original method.
We now describe the training procedure for computing transition probabilities between states in the BMT topology . The importance of each transition in recognizing whether or not a string exhibiting this transition belongs to the attrib ute depends on two factors: (i) gener ative factor: how often is the transition observ ed in the current attrib ute? (ii) discriminative factor: how unique is the transition to the current attrib ute?
Man y traditional approaches for learning HMM transition proba-bilities rely only the gener ative factors. That is, the (generati ve) tran-sition probability between two states s 1 and s 2 in AR M ability of observing tok en pairs t 1 and t 2 that can be emitted from and s 2 . The generati ve approach results in high probability transi-tions between higher level feature classes (e.g., transitions between the w + states that accept any tok en) even though such transitions may not discriminate an attrib ute from other attrib utes. For exam-ple, consider an erroneous bibliographic input string  X  X ditorial wil-fred hodges of logic and computation X  that has to be segmented into attrib utes [Title, Authors, Journal] 5 . In our experiments, purely gen-erati ve models segment this string as [ X  X ditorial X ,  X  X ilfred hodges of logic X ,  X  X nd computation X  X  because the tok en  X  X nd X  generalizes to a three-character string which is often observ ed as the beginning tok en for a journal name (e.g.,  X  X CM TODS X ).

We ensure that the transition probabilities depend on both the gen-erati ve and the discriminati ve factors. We now formalize this intu-ition to define the transition probabilities between pairs of states. In the follo wing description, let s 1 and s 2 be two states in the attrib ute recognition model AR M i . Let the position pos i ( s ) of a state note the position | beginning, middle, or trailing | of s The position pos ( t; v ) of a tok en t in an attrib ute value position | beginning, middle, trailing | of t in the string two states s 1 and s 2 , we say that the transition t ( s 5 The original record before corruption was [ X  X ditorial X ,  X  X  ilfred Hodges X ,  X  X ournal of Logic and Computation X  X . s is valid only if it is a non-backw ard transition. That is: Given an attrib ute value v from the attrib ute A i and the states of AR M i , we say that v supports a valid transition t ( s 1 ists a pair of consecuti ve tok ens t 1 and t 2 in v such that pos i ( s 1 ) , pos ( t 2 ; v ) = pos i ( s 2 ) , and either with non-zero probability by s 1 and s 2 , respecti vely or (ii) belong to the feature classes emitted by s 1 and s 2 , respecti vely . Positi ve Fr equency : Given a reference table R , the positive fre-quency f + trib ute A i is the number of attrib ute values in the projection of on A i that support t ( s 1 ; s 2 ) , and is 0 for all non-feasible transitions. Ov erall Fr equency : Given a reference table R , the over all frequency ues from any attrib ute that support the transition t ( s f ( t ( s 1 ; s 2 )) = i f + i ( t ( s 1 ; s 2 )) .
 Generati ve Transition Pr obability : Given a reference table gener ative transition probability GP ( t ( s 1 ; s 2 ) j A t ( s 1 ; s 2 ) with respect to an attrib ute A i is the ratio f Transition Pr obability : Given a reference table R , the transition probability P ( t ( s 1 ; s 2 ) j A i of a transition depends on its generati ve probability and its ability to distinguish attrib ute A i pendence between the two aspects, we compute the transition prob-ability as the product: A i : GP ( t ( s 1 ; s 2 ) j A i ) that we do not re-normalize the transition probabilities. An EM-style Baum-W elch [26] algorithm or other discriminati ve training methods (e.g., [10]) can be used to further optimize the transition probabili-ties. In our implementation, we used the efficient two-pass procedure described next (Figure 5).
 The pseudocode for the training procedure is sho wn in Figure 5. The overall procedure requires two passes: the first pass builds the dictionary and the second pass computes transition probabilities and applies rob ustness adjustments.
 ARMs SUMMAR Y : The crucial aspects of ARMs are (i) the adop-tion of a fix ed BMT topology that allo ws us to efficiently learn them from lar ge reference tables and to gain rob ustness to input errors, (ii) the association of a state per base tok en to accurately encode transi-tion probabilities and exploit lar ge dictionaries, (iii) the relaxation of sequential specificity by adjusting an ARM learned from clean ref-erence data, and (iv) the computation of transition probabilities to distinguish tar get attrib utes on which ARMs are built.
In this section, we describe an efficient algorithm for segmenting input strings. The segmentation problem has two components: first, determining the sequence in which attrib ute values are concatenated in an input string and second, determining the best segmentation of an input string into the corresponding ordered sequence of attrib ute values. Pre vious supervised approaches learned the attrib ute value order from the training data. For example, Borkar et al. [5] model the probabilistic attrib ute order using a hidden mark ov model. For instance, the author attrib ute immediately precedes the title attrib ute with probability 0 : 77 and the year attrib ute immediately precedes the booktitle attrib ute with probability 0 : 35 . Once such a probabilis-tic order is kno wn, a dynamic programming algorithm based on the Viterbi approximation can be emplo yed to determine the best seg-mentation [25]. Therefore, in the rest of this section, we only discuss the solution for the first sub-problem of determining the attrib ute value order . For the second sub-problem, a dynamic programming algorithm can be emplo yed. (Ho we ver, our implementation uses an exhausti ve search.) Figure 6 illustrates our approach: first learn the total order of attrib ute values over a batch of input strings, and then segment each indi vidual string using this (fix ed) attrib ute order .
We now describe an efficient algorithm for determining the at-trib ute value order in input strings. Our algorithm is based upon the follo wing observ ation. Attrib ute value orders of input strings usually remain same within batc hes of several input strings. For example, a data source for bibliographic strings may concatenate authors, title, conference name, year , pages in this order , preserving order across strings within the same page. Therefore, we need to recognize and reco ver this order only once for the entir e batch of input strings. We validated this assumption on real data sources on the web for the Media, Address, and Citation domains: content creators do tend to preserv e the order of attrib utes at least within the same page.
We now formalize the abo ve intuition. We first estimate the prob-ability of attrib ute A i preceding (not necessarily immediately) at-trib ute A j , and the use these estimates to determine the most lik ely total ordering among all attrib utes. We now describe these two steps of the process.
 Pairwise Pr ecedence Estimates Intuiti vely , the precedence estimate prec ( A i ; A j ) preceding attrib ute A j is the fraction of input strings where the at-trib ute value for A i is before the attrib ute value for dence order among attrib utes for a single input string is determined as follo ws. For each attrib ute, we determine the tok en in the input string s at which it is most lik ely to start. For a pair of attrib utes A i and A j , if the tok en at which A i is most lik ely to start precedes the tok en at which A j is most lik ely to start, then we say that precedes A j with respect to the input string s . We break ties by picking one of the two attrib utes with equal probability . For exam-ple, consider an input string consisting of 8 tok ens  X  X  almart 20205 s. randall ave madison 53715 wi.  X  We compute an 8 -coordinate vec-tor [0 : 05 ; 0 : 01 ; 0 : 02 ; 0 : 1 ; 0 : 01 ; 0 : 8 ; 0 : 01 ; 0 : 07] The first component 0 : 05 in the vector denotes the probability of the city attrib ute starting at the tok en  X  X  almart.  X  Because the dinate is the maximum among all coordinates, the city attrib ute is most lik ely to start at the tok en  X  X adison.  X  Suppose the vector for mum for the city vector occurs at the 6 th coordinate and that for the street occurs at the 5 th coordinate. Therefore, street attrib ute value precedes the city attrib ute value for this input string. The fraction of input strings in the entire batch where attrib ute A i precedes estimate for A i preceding A j .

Formally , let s be a given input string within a batch S We tok enize s into a sequence t 1 ; : : : ; t m of tok ens and associate with each attrib ute A i ( 1 i n ) a vector v ( s; A i ) = [ v The component v ij is an estimate of the attrib ute value for at the tok en t j ; v ij is the maximum probability with which accepts any prefix of [ t ij ; : : : ; t im ] . Let max ( v ( s; A coordinate corresponding to the maximum among values v i 1 That is, max ( v ( s; A i )) = argmax mate prec ( A i ; A j ) is: prec ( A i ; A j ) =
At the end of this phase, we possess the pairwise precedence es-timates between all pairs of attrib utes. Computationally , this proce-dure requires invoking the ARMs for determining acceptance proba-bilities of sub-sequences of tok ens from each input string in a batch. If the average number of tok ens in an input string is m , this computa-tion involv es O ( m 2 ) calls to ARMs. These acceptance probabilities can be cached and later used during the actual segmentation, thus avoiding repeated invocation of ARMs.
 Determining Total Attrib ute Order Using the directed attrib ute precedence probabilities estimated as de-scribed abo ve, we can now estimate the best total order among at-trib utes. The quality of an attrib ute order is the product of prece-dence probabilities of consecuti ve pairs of attrib utes in the given or-der . When the number of tar get attrib utes is small (say , less than we can exhausti vely search all permutations for the best total order . When the number of attrib utes is lar ge, we can use more efficient heuristic search techniques (e.g., [20]).
In this section, we evaluate CRAM using a variety of real datasets from real operational databases to sho w that CRAM is a rob ust, ac-curate, and efficient unsupervised domain-independent segmentation system. We first describe our experimental setup (Section 6.2). We present results on accurac y in Sections 6.3 and 6.4, and those on scal-ability in Section 6.5. We now describe the datasets and the evaluation metrics we adopted. Refer ence Relations : We consider reference relations from three dif ferent domains: addresses, media (music alb um records), and bib-liography domains.

Addr esses : The address relation consists of 1 ; 000 ; 000 standardized indi vidual and organization addresses from United States and Puerto Rico with the schema: [ Name , Number1, Number2, Ad-dress, City , State , Zip ].

Media : The media reference relation consists of 280 ; 000 and standardized records describing music tracks with the schema: [ ArtistName , Alb umName , TrackName ].

Bibliograph y : The bibliography relation consists of 100 ; 000 liography records from the DBLP repository and has the follo wing schema: [ Title , Author , Journal, Volume , Month, Year ]. Table 1: Err or Model: General attrib ute err or types used for corrupting the test datasets.
 Test Datasets : We evaluated CRAM over both naturally concate-nated strings obtained from the web and from internal compan y sources. Further , in order to allo w controlled experiments, we generated ex-tensi ve test sets by concatenating  X  X rror -injected X  real records into strings. All test datasets are disjoint from training datasets. from the RISE repository of information extraction sources (pub-licly available); Indi vidual addresses and Media filenames from in-ternal compan y sources (not publicly available); 100 most cited pa-pers from Citeseer (publicly available). We will refer to these collec-tively as Natural datasets.
 obtained by concatenating attrib ute values of records in a test rela-tion held aside for validation. Therefore, the training and test sets are disjoint. To automate evaluation, we fix the order in which attrib ute values are concatenated to be a randomly chosen permutation of all attrib utes. Erroneous input strings are generated from test tuples by passing them through an err or injection phase (as in [7]) before the y go into the concatenation phase described abo ve. The error injection phase is a controlled injection of a variety of errors, which are com-monly observ ed in real world dirty data [18], into the test tuples. The attrib ute value(s) to introduce an error into is chosen randomly from among all attrib ute values. The types of errors and their effects on attrib ute values are listed in Table 1. While generating input strings with errors, we introduce at least one error into every tuple. For the mix ed error model ( All 5 Err ors in Table 1), we assume that each error type occurs with equal probability .
 Systems Compar ed : We compare our system CRAM with a state of the art supervised text segmentation system DATAMOLD [5], which was designed for automatic text segmentation and was sho wn to out-perform other competing systems such as Rapier [6]. Thus, our com-parison is definiti ve, since by outperforming DATAMOLD, CRAM is virtually guaranteed to outperform other systems as well.
As discussed earlier in Section 5, we separate the problem of deter -mining attrib ute order from that of arri ving at the best segmentation given the order . Reflecting this separation, we also split the evalu-ation of the attrib ute order determination from that of segmentation accurac y given the order .
 Segmentation Accuracy : We measure segmentation accur acy as a fraction of attrib utes segmented correctly . From an input string (out of a maximum of n attrib utes) be the number of attrib utes correctly identified by a segmentation algorithm Alg . We define the segmentation accurac y acc ( s; Alg ) on the input string fraction of tar get attrib utes correctly identified. For a set strings, the overall segmentation accurac y of Alg is defined as the average fraction of correctly identified attrib utes over all strings in S, or, more formally as: Figur e 8: Relati ve accuracy gain of CRAM over Datamold.
 Attrib ute Order : We compute the accurac y of our order determina-tion as the fraction of attrib utes in the input batch of strings which were assigned the correct absolute position. For example, if the cor -rect order is Number , Addr ess, City and our algorithm proposes Num-ber , City , Addr ess , the order accurac y is 0 : 33 or 33%
We now evaluate the accurac y of CRAM over real datasets. We sho w that (i) the CRAM system substantially outperforms the state of the art supervised text segmentation system, (ii) the attrib ute order determination technique is accurate, and (iii) CRAM scales to lar ge reference tables.
 Segmentation Accuracy We compare the segmentation accurac y of CRAM with that of Data-mold over both erroneous and clean test datasets discussed in Sec-tion 6.1. For this experiment, we fix the order in which attrib ute values are concatenated to focus on the impro vements due to using ARMs. 6 We report the segmentation accurac y in Figure 7 (a, b, and c). Observ e that CRAM substantially impro ves accurac y, often by 8-15%, for all error types and over all datasets illustrating that ARMs are accurate in modelling attrib ute value domains. In order to put the accurac y impro vements in a relati ve perspecti ve, we report the same results in Figure 8 but in the form of the percentage relati ve error reduction over Datamold for the Addresses, DBLP , and Media datasets. For man y of the noisy datasets, CRAM reduces segmenta-tion errors by over 50%. These impro vements directly result in the significant reduction of required manual interv ention and increased accurac y while loading the data into relations.
 Accuracy of Attrib ute Order Determination We now evaluate our attrib ute order determination technique over batches of natur al (i.e., user -entered or web-deri ved) strings. Fig-ure 9 sho ws the high accurac y of our total order determination al-gorithm: in man y cases we can determine order with 100% for a relati vely small number (around 200 ) of tuples. This is a reasonable requirement, since man y personal media libraries, and address and citation collections, and legac y data sources will have that man y tu-ples.
 Exploiting Lar ge Refer ence Tables We now study the impact of reference table sizes on segmentation accurac y. Figure 10 sho ws the results. Observ e the increase in seg-mentation accurac y with the size of the reference table, especially for the Media dataset. In fact, note that accurac y of over 80% for the Media dataset is only achie ved when the reference table size ex-6 Because of pri vacy restrictions, we were not able to ship  X  X eal X  in-dividual addresses data to be evaluated by DATAMOLD. All of the remaining results are performed over the Natur al dataset, unless oth-erwise indicated. When this was not possible for pri vacy concerns, our All 5 Err ors dataset pro vides a good approximation of the seg-mentation accurac y of the real data entered by real users. Order Accuracy Figur e 9: Accuracy of attrib ute order determination for the web-deri ved Addr esses and Media, and the corrupted DBLP datasets. Figur e 10: Accuracy of segmentation using CRAM for incr easing refer ence table size (Addr esses, DBLP , and Media datasets). ceeds 40,000 tuples. Therefore, (i) exploiting rich dictionaries from lar ge reference tables is important to achie ve higher segmentation ac-curac y, and (ii) a segmentation system must scale to lar ge reference table sizes. CRAM tak es just a few minutes ( &lt; 5 ) to learn ARMs over a reference table of 200 ; 000 tuples. In contrast, supervised sys-tems relying on cross-v alidation approaches would be much slo wer .
In this section we evaluate the impact on segmentation accurac y of the BMT topology , the association of a state for each base tok en, and the rob ustness operations.
 Fixed ARM topology and Rob ustness Operations We now evaluate the effecti veness of our BMT topology | which al-leviates the need for expensi ve cross-v alidation approaches for opti-mizing HMM topology | to sho w that it results in high segmentation accurac y, and is often better than topologies obtained by using ex-pensi ve cross-v alidation approaches. We compare the accurac y of segmentation using two representati ve ARM topologies: (i) topology only models the sequential information between states by even collapsing all begin, middle, and trailing positions into one cat-egory and (ii) our BMT (the 3 -Pos) topology . In fact, we found that increasing the number of positions in the fix ed topology further does not increase accurac y. Due to space constraints, we omit the results. Figure 11 sho ws that the fix ed BMT topology is usually more accu-rate than the 1 -Pos topology that completely ignores positional speci-ficity .

Figure 11 also reports results of sequential specificity relaxation operations over ARMs. These include the adjustment operations (e.g., reco vering from tok en deletions) and the transition probability computation described in Section 4.4. As we can see, our rob ustness operations do pro vide a consistent impro vement in accurac y over both erroneous and clean data on all datasets. The impro vements on clean test datasets are the result of modified transition probability computation. and with rob ustness operations (BMT -rob ust). Figur e 12: Complete vs. Collapsed tok en states over Addr esses (A), DBLP (D), and Media (M) datasets.
 Modelling Indi vidual Base Tok ens As discussed in Section 4.1, we associate a state with each base to-ken that is retained in the ARM. A common alternati ve is to collapse man y base tok ens together into one state of the HMM, which results in the loss of transitional information by collapsing transitions to-gether . Figure 12 sho ws that collapsing base tok ens together into one state (as is done in Datamold), results in substantially lower (some-times by 10%) segmentation accurac y. The price of this accurac y gain is a lar ger model size. Ho we ver, we sho w in Section 6.5 that CRAM achie ves high accurac y even if we retain only important base tok ens. In related experiments, which we omit due to space con-straints, we studied the use of dif ferent hierarchies. Ho we ver, the y all resulted in similar segmentation accuracies.
 Hypothetical Super vised Appr oach We now demonstrate that making rob ustness to input errors a design criterion is essential. We consider a hypothetical scenario where we kno w the error model with the percentages of each error type ex-pected in the test input. (Of course, this kno wledge is not available in practice.) We use this kno wledge to prepare a training dataset that reflects the variety of errors and percentages. As sho wn in Figure 13, Datamold:Hypothetical (corresponding to Datamold trained over this hypothetical dataset) has significantly higher segmentation accurac y than Datamold trained over clean data, but is still outperformed by CRAM trained on clean reference data. Thus, rob ustness to input errors is essential for deplo ying a segmentation system on real data. Figur e 13: Segmentation accuracy of Datamold trained over clean and hypothetical err oneous training data. Figur e 14: Varying fraction of tok ens retained: Segmentation Accuracy (a), #T ransitions (b), and Total Model Size (MB) (c).
We now study the sizes of models with respect to reference table sizes and the impact of constraining model sizes on the segmentation accurac y. We validate our hypothesis that (i) the number of tran-sitions gro ws much slo wer than a function that is quadratic in the number of states, and (ii) that we can achie ve comparable accurac y by considering only a fraction of the most frequent base tok ens.
Figure 14 sho ws results of varying the fraction f of base tok ens retained while keeping the reference table size fix ed. As sho wn in Figure 14(a), retaining only a fraction of the base tok ens in ARMs gets us the same accurac y as that of retaining all base tok ens. At f = 1 , all tok ens are retained and the model size (sho wn in Fig-Figur e 15: Varying refer ence table sizes: Segmentation Accuracy (a), Total model size (in MB) on the primary axis, and #Model states on the secondary axis (b). ure 14(c)) is still less than a few MB for all datasets. And, the num-ber of transitions (sho wn in Figure 14(b)) is around 10 times that of the number of states | usually 100 ; 000| in the model; hence, it is orders of magnitude less than j S j 2 . Thus, we can significantly re-duce memory requirements without compromising on segmentation
We now study the impact on accurac y of increasing reference ta-ble sizes while constraining the size of the CRAM model to be under 2MB (using the Media dataset). Figure 15(a) sho ws the impro ve-ment in accurac y as the reference table size increases from to 200 ; 000 tuples. Figure 15(b) sho ws the corresponding model size (which is constrained to belo w 2MB) and the number of states. Note that even under constrained model size, the resulting segmen-tation accurac y of 86% for 200 ; 000 tuples matches that of the lar ger unconstrained model trained over the same reference table. Thus, CRAM scales to lar ge reference tables while maintaining a small memory footprint without compromising on segmentation accurac y.
In summary , we have sho wn CRAM to be rob ust, scalable, and domain independent. It is accurate on both clean and erroneous test data, and gracefully scales up to lar ge reference table sizes.
In this paper , we exploit widely available reference tables to de-velop a domain-independent, rob ust, scalable, and efficient system for segmenting input strings into structured records. We exploit rich dictionaries and latent attrib ute value structure in reference tables to accurately and rob ustly model attrib ute value domains. To easily de-plo y CRAM over a variety of data sources, we address the prob-lem of automatically determining the order in which attrib ute values are concatenated in input strings. Using real datasets from several domains, we established the overall accurac y and rob ustness of our system vis-a-vis state of the art supervised systems.
 Ackno wledgments : We thank Sunita Sara wagi for helping us com-pare our system with DATAMOLD. We also thank Theo Vassilakis for several thoughtful comments on the paper . 7 We did not have access to DATAMOLD code and so cannot report DATAMOLD X  s resulting model size or memory consumption.
