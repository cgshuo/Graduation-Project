 In this paper we introduce the task of tweet recommenda-tion , the problem of suggesting tweets that match a user X  X  interests and likes. We propose an Information-Retrieval-like model that leverages the content of the user X  X  tweets and those of her friends, and that effectively retrieves a set of tweets that is personalized and varied in nature. Our approach could be easily leveraged to build, for example, a Twitter or Facebook timeline that collects messages that are of interest for the user, but that are not posted by her friends. We compare to typical approaches used in similar tasks, reporting significant gains in terms of overall preci-sion, up to about +20%, on both a corpus-based evaluation and real world user study.
 H.4 [ Information Systems Applications ]: Miscellaneous; H.3.4 [ INFORMATION STORAGE AND RETRIEVAL ]: Systems and Software X  User profiles and alert services Algorithms Twitter recommendation, Information Filtering
Social media is having an unprecedented success in terms of popularity in recent years. Twitter and Facebook, the two most popular social media hubs, occupy high positions in the top-10 of the most visited sites, rivaled only by big search engines and by Wikipedia 1 . In January 2012 Twitter has been visited 2.5 billion times, more than double than h ttp://www.alexa.com/topsites 6 months before 2 . In October 2011, Twitter revealed that 250 million tweets are posted every day, with a user base of about 300 million people.

With a rate of roughly 3,000 tweets per seconds, social media are today facing the same challenge of information overloading that the Web faced more than 10 years ago. More than that, we are today also facing the challenge of information hiding . By no means can a Twitter user pos-sibly read all the messages s/he might consider important. Indeed, only a very small percentage of tweets spreads to a significant number of users [25]. It is quite likely that users miss lots of interesting tweets just because none of the peo-ple in their network retweet or mention them. Social search engines can help to solve the problem by providing a sim-ple means to retrieve information. However, these engines cannot predict what the user may like and push interesting information for her. The solution is therefore a combination of social search and algorithms for personalizing, summariz-ing and recommending social content.

In this paper we focus on the latter task of social content recommendation. We propose two Information Retrieval in-spired methods for recommending engaging tweets to a user, matching her interests and likes. The methods are based on the idea of building a user profile from her tweets and a weighted combination of those posted by her friends, and to match this profile on incoming tweets. Our extensive eval-uation performed on both an in-house dataset and on real users, shows that our methods successfully recommend in-teresting tweets when compared to other approaches. Our system differs from most previous work on social recommen-dation, where the focus is predominantly on recommending web content using social signals, and on recommending new connections. Our primary objective here is to recommend the social content itself, i.e., tweets. Although the methods we use to approach the tweet recommendation problem are derived from information retrieval techniques, their applica-tion to our context requires, as we shall see in the rest of the paper, some adjustments and additions to make them work in social media settings.

The most direct practical application of our system is to provide a social media user with a new timeline 3 that con-tains messages that strongly match her interests, but that have not been posted by any of her friends. The benefits of h ttp://www.quantcast.com/twitter.com
The user entry point in most social media hubs like Twitter and Facebook is the personal timeline, where the user is shown the recent messages posted by her friends. having this additional timeline are twofold. Firstly, the us er will not miss messages that are relevant to her, because our method will show them. Second, authors of recommended tweets may be considered relevant by the user who receives the recommendation. Our tweet recommendation system may therefore indirectly serve also as a mechanism to  X  im-plicitly  X  recommend accounts to follow.

Summing up, the original contributions of this paper are:
The paper is structured as follows. Section 2 presents an overview of the current state of the art in areas that are related to our work. Section 3 presents a formal definition of the problem, while Section 4 describes the proposed solu-tions. Experiments are presented in Section 5 along with a careful description of the datasets we are using and a defi-nition of the metrics we evaluate. We conclude the paper in Section 6 with a final analysis and future work.
Research on recommender systems has been widely stud-ied in the past decade (e.g., see [22, 4, 7, 21]). Recom-menders have been applied to many different items including movies, videos, music, users, restaurants, news stories, jour-nal articles, and much more (see [22] and references therein). To the best of our knowledge, tweet recommendation has not been yet explicitly defined. This task relates to four main ar-eas of research: tweet-based content recommendation , tweet ranking , tweet classification , and information spreading in social networks .
 Tweet-based content recommendation systems use Twitter for recommending other types of content. Among others, Abel et al. [1] propose a URL recommendation sys-tem for Twitter users. They build a linguistic model for the user, as the frequency weighted vector of the hashtags and entities that he/she mentions in his/her tweets. A simi-lar vector is computed from the content of candidate URLs. The web pages that have the highest cosine similarity with the user are recommended [1]. Chen et al. present similar models for the same task, where the user profile is repre-sented as the bag-of-word vector of all terms in the user X  X  tweets (or his/her friends X  tweets); and the URL model is built from the terms in the tweet that mention it [8]. Our work differs from the above two, in that we recommend tweets instead of URLs, which is a harder task since there is less linguistic material available. Yet, like them, we lever-age the linguistic content of the user X  X  tweets and of his/er friends X  tweets.

Tweet ranking is the task of ranking tweets given an input query. Duan et al. [11] use user and social features to feed a RankSVM algorithm. They report good DGC results by using a variety of information, including user authority features and cosine similarity between the texts of the re-trieved tweets. Similar ranking systems have been proposed at the TREC-2011 Microblog Track 4 . In general, typical models for web document ranking such as BM25 and text similarity, can be easily adapted to tweet ranking, show-ing good performance. Yet, microblogging specific features, such as social feedback and annotations (e.g., number of retweets and hashtags), the authority of the user posting the tweet, tweet length and freshness also play a key role. Tweets have also been used to improve document ranking: Dong et al. [10] show that integrating Twitter features (e.g., user and textual properties of the tweets mentioning the URLs) significantly improve state of the art web search al-gorithms, in ranking documents by relevance and freshness. The task presented in this paper differs from tweet rank-ing, in that our task is query independent and user centric. Instead of ranking tweets for a given query, we recommend tweets to a user according to his/her interests. There are however similarities between the two tasks, e.g., in both cases there is a specific user need to answer, and similar features can be used.

Short texts classification is a relatively new problem that is receiving more and more attention. Tweet classifica-tion is a specialization of this problem where text fragments propagate through social networks. The main issue for clas-sification of microblog messages is their small size. The same issue holds also in different domains where fragments of text are small. Take for instance search snippets, forum messages and Instant Message service texts. The main issue is that messages are small in size, and therefore difficult to classify. Several studies have approached this problem from different angles. Banerjee et al. [3] exploit Wikipedia as an exter-nal source for improving the accuracy of clustering short texts by enriching their representation with features com-ing from Wikipedia pages related to terms appearing in the short text. Schonhofen [24] presents a similar approach ex-ploiting Wikipedia as a taxonomy for topics showing signif-icant improvements on clustering accuracy. Phan et al. [20] use topic models for building short and sparse text classi-fiers. They build a a Latent Dirichlet Allocation [5] topic model from this dataset, and then use it to classify short texts.

Information spreading in social networks is another important and well-studied problem in social networks. This research area is important as results can be immediately ex-ploited in several ways, e.g., to spread a marketing campaign or to block spread diseases. One of the seminal papers deal-ing with this problem is that of Kempe et al. [16]. In the paper an approximation algorithm to find the most influen-tial nodes in a network is devised. The algorithm is further refined by Leskovec et al. [18] and by Goyal et al. [14]. A similar problem, but with the opposite purpose, is defined by Budak et al. [6]. The goal of their algorithm is to block a  X  bad  X  advertisement campaign as soon as it is initiated. As in the case of Kempe et al. [16] authors formulate the prob-lem as an optimization problem described by a sub-modular objective function. The standard greedy algorithm for this kind of functions [12] is used with the goal of identifying a subset of individuals that need to be convinced to adopt s ites.google.com/site/microblogtrack the competing (or  X  X ood X ) campaign so as to minimize the n umber of people that adopt the  X  X ad X  campaign at the end of both propagation processes.
We give here a definition of the notation and the prob-lems we shall study in the rest of the paper. Let T be a stream of tweets, each tweet t 1 , t 2 , . . .  X  T is indexed by the timestamp at which it arrives. For each user u , we assume to know the interestingness of a tweet t  X  T , denoted by I ( t ). Hereinafter, we consider a single user u and we omit the subscript u from I u whenever the user can be inferred from the context.

We shall define two problems whose goal is to select a subset of tweets S  X  T to be recommended to a given user. The first problem aims at selecting S  X  T of size k , without considering the goodness of the set in its entirety. Namely, each tweet in S is selected independently from the already selected tweets. We refine the idea of tweet subset selection in the second problem, where the set of tweets S  X  T is fur-ther required to maximize the overall information conveyed to u . To the best of our knowledge both problems are novel and have never been proposed before in the context of social media systems such as Twitter.
 We begin by introducing the first problem, namely the TweetRec problem, defined as follows.

Problem 1 (TweetRec). Given a user u and a pos-itive integer k , we aim at finding a set S of k tweets in T maximizing the overall  X  X nterestingness X . More formally, we would like to find
Essentially, we are looking for the set of k tweets S hav-ing the largest interestingness under the assumption that the tweets independently convey their interesting content. The nicest property of TweetRec is that its solution is optimally discoverable in O ( |T | log k ) time by the obvious greedy algorithm selecting the most interesting, (i.e., top-k ) tweets. The drawback of this solution is that we are assum-ing independence of tweet content in the solution set. This, indeed, is far from being satisfactory for a user. Suppose, in fact, that all the tweets selected for being included in the set S are highly correlated to each other. This represents a feasible solution of TweetRec but the user may be far from being satisfied. For instance, each of these two tweets  X  What X  X  new in Linux 3.2? #linux X  and  X  X ew features in Linux 3.2. #linux X  may be highly interesting for a user but report-ing both of them is useless.

The issue in TweetRec is given by the fact that we are not considering interdependency among selected tweets. The objective function in TweetRec assumes the indepen-dence of all the tweets and does not consider the overall interestingness of the identified set S in its entirety.
In order to overcome this limitation, we restate the tweet recommendation problem in an X  X dependency aware X  X etting, where shared (i.e., duplicated) information is avoided. Given S  X  T , we want to maximize the overall interestingness F ( S ) of the content of tweets t  X  S for the user u . Formally, F ( S ) = I F duplicate information across tweets is not going to increase the value of the objective function F ( S ). We also define I d formative content among all tweets in t  X  S .

Problem 2 (Interests-Spanning TweetRec). Given a user u , a positive integer k , and the stream of tweets T , we aim at finding the k tweets t in T maximizing the overall interestingness. More formally, we would like to identify the set T such that
The derivation of the equation in the objective function of Interests-Spanning TweetRec is an instantiation of the Inclusion-Exclusion principle to the set S  X  T of tweets. As in TweetRec , we are looking for the set of k tweets S having the largest overall interestingness for the user u . However, in this case we are taking into consideration all possible dependencies, i.e., shared content among the tweets in the set.

Interests-Spanning TweetRec is NP-Hard as proven in the following Theorem.

Theorem 1. The Interests-Spanning TweetRec prob-lem is NP-hard.

Proof. The reduction is from Independent Set [13]. As-sume we have a graph G = ( V, E ) with n = | V | and we want to compute the maximum independent set of G . The independent set can be computed by creating an appropri-ate instance of our problem. Vertexes in V are considered as tweets. Each vertex is assigned an interestingness equal to . Any set S has an overall interestingness equal to 1 n | S | i f and only if it does not exist a pair of vertexes in S con-nected by an edge in E . The interestingness is set to be smaller than 1 n | S | , otherwise. To conclude the reduction we observe that, fixed k , the set S identified by a solution of Interests-Spanning TweetRec has probability k n i f and only if G has an independent set of size k . In this case, nodes in S are exactly the nodes of one of these independent sets.

Despite being complex, the objective function of our prob-l em has the property of being a non-negative submodular function, as stated in the following Theorem 2. This theo-rem requires that the function F is monotone (namely, any set S of tweets is at most as interesting as any of its subsets). Obviously, whether I is monotone depends on its particular instantiation (i.e., the real formula used to its estimation). The monotonicity is a natural assumption to make and, in-deed, it holds in our estimation of I defined in Section 4.
Theorem 2 (Submodularity of F ( X ) ). Let k be a positive integer and let S be a set of k tweets. Then, the function is submodular.
Proof. W e have to show, by the definition of submodu-larity, that given A  X  B  X  T and a tweet c  X  T , F ( { c }  X  A )  X  F ( A )  X  F ( { c }  X  B )  X  F ( B ).
 The theorem is proven by observing that since F ( X ) = I F
Thus, F ( { c }  X  A )  X  F ( A ) = I ( c )  X  I c  X  F larly, F ( { c }  X  B )  X  F ( B ) = I ( c )  X  I c  X  F
The thesis easily follows by observing that I c  X  F I c  X  F
Given that our objective function is submodular, its maxi-m ization can be obtained by the greedy algorithm presented in [9, 19]. This algorithm incrementally builds the set S in k steps. At each step, the element that has the largest marginal gain is added. This simple algorithm guarantees to obtain a (1  X  1 /e ) approximation. The following Theorem summarizes this very well-known result.

Theorem 3 (Submodularity Approximation). [9, 19] For a non-negative, monotone submodular function F , let S be a set of size k obtained by selecting elements one at a time, each time choosing an element that provides the largest marginal increase in the function value. Let S  X  be a set that maximizes the value of F over all k -element sets. Then F ( S )  X  (1  X  1 /e ) F ( S  X  ) .
The TweetRec and Interests-Spanning TweetRec problem definitions give a good formalization for a tweet recommendation system. Yet, these formalizations assume that for any user u and tweet t  X  T , we know the score I ( t ) measuring the interestingness of the tweet t for the user u . In order to build a functioning recommendation system we therefore need an estimator of interestingness.

In this section, we present one example of an estimator based on a linguistic model. The main assumption is that the interests of a user u are implicitly expressed in his/her tweets. By comparing the linguistic content of the user X  X  tweets and that of a generic tweet t , we can therefore esti-mate the interestingness I ( t ).

In details, we estimate the interestingness as a linear com-bination of two distinct similarity measures between the set of tweets of the user and a candidate tweet (or set of tweets). In these two measures, sets are compared either by using item-wise or pair-wise comparisons between their enclosed terms. Our estimation is detailed in Section 4.1.
A drawback of the above-mentioned measure is that it heavily relies on the (textual content of the) set of tweets produced by the user. Indeed, there exist (a class of) users, namely passive users , that post a small number of tweets in their lifetime, and adopt Twitter mostly for reading news published by other users. In Section 4.2, we show how to extend the set of tweets written by a passive user u with other carefully chosen tweets that have been posted by users that are highly authoritative.
In this section, we show how to estimate I ( t ) by comput-ing the similarity between the set of user X  X  tweets T u and any subset of tweets from T .

First, given a generic set of tweets X , we define the bag-of-words BT ( X ) = {  X  1 ,  X  2 , ...,  X  n } as the set of distinct terms  X  i contained in at least a tweet t  X  X . Similarly, we define pairs of terms  X  i that co-occur (possibly not consecutively) in at least a tweet t  X  X .

We define two score functions for terms and pairs with respect to a set of tweets X .

Definition 1 (Term Score). Given a term  X  and a set of tweets X , we define the score of  X  given X as where TF T (  X , X ) is the number of tweets in X containing term  X  , IDF T (  X  ) = log |T | DF tweets in the stream T containing  X  .

As we will show in the experimental section, the sum of the tf  X  idf [23] of the terms used by the user is particularly useful for tweets. Indeed, apart from increasing the impor-tance of co-occurrences of rare terms, it privileges longer tweets with respect to shorter ones.

Similarly to what we have just done we define a score function for pairs of terms.

Definition 2 (Pair Score). Given a pair of terms  X  and a set of tweets X , we define the score of  X  given X as where TF is the number of tweets in X containing pair of terms  X  , IDF P = log |T | DF tweets in the stream T containing the pair of terms  X  .
The rationale behind the measure defined by Equation 3 is that since tweets are relatively small documents usually consisting of few sentences, there are good chances that two tweets having pairs of terms in common have higher corre-lation than tweets sharing only terms.

We observe that both scores implicitly consider the over-lapping content between sets of tweets only once. This is an important aspect of the scoring functions that complies with the concept of overall interestingness discussed in the previous section.

Finally, by combining Definitions 1 and 2, we define a score that establishes the similarity of a set of tweets with respect to another.

Definition 3 (Set Similarity). Given two sets of tw-eets X 1 and X 2 , we define the similarity of X 1 with respect to X 2 as where  X   X  [0 , 1] is the linear combination coefficient.
Observe that the similarity is computed by considering o nly terms whenever  X  is equal to 0. Conversely, the simi-larity is computed by considering only pairs of terms when  X  is equal to 1. Note that it is possible to define a score for triples of terms or more, similar to what we have done for terms and term pairs. In that case the function int  X  is defined by a linear combination of different scores, using coefficients  X  1 ,  X  2 ,  X  3 , . . . such that  X  1 +  X  2 +  X  Given the above definitions, we are now ready to estimate I ( t ). Given a user u and his set of tweets T u , for any value  X   X  [0 , 1] we estimate the interestingness of a set of tweets X for user u , by setting I u ( X ) = int  X  ( X, T u ).
We can now solve the TweetRec problem by selecting the k tweets in T having the largest values of int  X  with re-spect to T u . Obviously, this considers the contribution of each tweet t  X  X to the overall interest of X for a user independently. In fact, the content of two tweets may over-lap. When this overlap is non-negligible, the contribution of these two tweets to the overall interestingness of the pair is far from being the sum of the two independent contributions.
We can similarly solve the Interests-Spanning Twee-tRec problem, directly applying Definition 3 to Equation 2 to address this issue. It is straightforward to prove that int  X  satisfies all the requisites of Theorem 2 and, thus, it is submodular.
The main assumption of our approach is that the num-ber of tweets | T u | posted by a user is large enough to build a rich language model. Unfortunately a large portion of Twitter users are passive , i.e., they post only few or no tweets, as shown in Figure 1(a) and by other analytic stud-ies (e.g., [15]). According to Twitter, as of September 2011, 40% of the users  X  X ign in just to observe X . Building a lan-guage model for such users is thus prohibitive.

We therefore enhance our approach by including in the user model the tweets posted by the user X  X  friends. The intuition is that the user X  X  interests are expressed not only in his own feed, but also in the feeds of the people he follows. Indeed, contrary to other social networks, social connections in Twitter are based more on shared interests than on real world friendships [2, 17].

A further important observation is that some friends are intuitively more relevant than others when building a user model. Friends that are more authoritative should be weigh-ted more in the model, because they provide more contentful and relevant information. We model this intuition by inte-grating Definition 1 with the following new formulation of tscore : where in (  X , x ) is 1 if the tweet x contains term  X  , 0 other-wise; and auth ( u x ) is the authoritativeness of the user that originally posted x , obtained as described next. Likewise, we also modify Definition 2 by considering pairs of terms instead of single terms.

As a side note, we experimented with another similar ap-proach for solving the problem of passive user, where instead of considering authoritative users, we considered friends that the user often retweets, i.e., users that likely share her in-terests. This approach, obtained by replacing the auth ( u score in the above formula with a retweet score , showed no improvements over the authority approach, and has there-fore not been included in the paper for lack of space.
A clear definition of user authority in Twitter is very dif-ficult to draw and to model being often subjective and task dependent.

In our case, authoritative users are those that post rele-vant information that is engaging for other users. Author-itative users, often called information sources , are usually characterized by a high number of followers and a high ratio between followers and friends. Although other indicators of authority may serve in this context, e.g., number of retweets or PageRank scores, we prefer to keep the computation of the score as simple as possible, given the real-time high-load nature of our application 5 .

For each user u in our corpus, we define the authority score auth ( u )  X  [0 , 1] as a linear combination of two logistic functions 6 : auth ( u ) = A spectively the number of followers and friends of the user; A  X  [0 , 1] is the linear parameter;  X  and  X  are logistic param-eters. All the parameters have been set using a greedy search on a small manually ranked dataset of authoritative users. The resulting parameter values are as following: A = 0 . 5,  X  = 2 and  X  = 2000.
This section shows the experiments we conducted in order to assess the effectiveness of our recommendation methods.
After describing the datasets in Section 5.1, we assess the effectiveness of the int  X  function as an estimator of the in-terestingness of a tweet for a user, and we provide a compar-ison against two well-known methods (Section 5.2). These comparisons are done by using both an automatic evalua-tion and a user study. After assessing the effectiveness of int  X  , we use it to find solutions for the TweetRec and the Interests-Spanning TweetRec problems. Eventually, in Section 5.3 we compare the solutions obtained by approach-ing these two problems, and we show that results produced by solving Interests-Spanning TweetRec are of a higher quality.
Our corpus consists of about 182 , 000 tweets posted be-tween October 30 and November 4, 2011. The corpus was collected as follows. A large set of more than 14 million tweets was downloaded from Twitter using the Spritzer API, that provides access to a 1% random sample of all tweets. This set was then pruned to obtain our final corpus contain-ing informative and non-junk English tweets on which we run our experiments. In details, the pruning process was as follows. First, we discarded all the tweets shorter than 30
D espite other indicators, the number of followers and the number of friends are indeed directly available from the Twitter streaming API.
Normalization omitted for clarity. characters and having less than 8 tokens (i.e., terms, hash-tags, and usernames). Then, we removed tweets containing less than 3 English nouns and more than 5 English stop-words. To do that, we performed part of speech tagging using the NLTK 7 toolkit. Finally, we discarded directed tweets (i.e., tweets starting with the @ symbol), that are usually personal in nature, and therefore not interesting for recommendation purposes.
 Table 1 reports general statistics regarding our corpus. Figure 1(a) shows the distribution of the number of tweets per user. Each point on the x -axis corresponds to a bucket of 1,000 tweets. The corresponding y -axis value is the overall fraction of users that have posted that number of tweets. As expected the distribution follows a power-law. Almost 40% of users have posted less than 1 , 000 tweets since they joined Twitter. This confirms the need for strategies capable of handling  X  passive  X  users, as described in Section 4.2. Figure 1(b) shows the distribution of the friends and followers in Twitter for  X  passive  X  users with less than 200 tweets posted since they joined Twitter. Values on the x -axis are buckets of 100 friends or followers. We observe that about 80% has more than 100 friends, while around 40% has less than 100 followers.

In this section, we show the effectiveness of int  X  by com-paring it to two baselines: Cosine and Hashtags . Compar-isons are done both using an automatic evaluation and a user-study-based methodology.
We base this set of experiments on the assumption that a user is likely to find his own tweets more interesting than h ttp://www.nltk.org/ random tweets from other users. Building on this assump-tion we randomly select a set of 250 users from our corpus. For each one of these users u we use 90% of his tweets (we call it the set T u ) for building the user profile. The remain-ing 10% is mixed with all the tweets from other users in the corpus, forming the testing set.

Ideally, the correct recommendations for a given user cor-respond to tweets from the 10% of his own tweets. More formally, we divide the test set into a positive set P , con-sisting of the 10% of the user X  X  tweets, and a negative set N formed by all other tweets. We consider correct a tweet belonging to the positive set.

As evaluation metrics we use standard Information Re-trieval measures. For each user we compute:
To compute the final metrics X  scores we take the average of the above measures across all the 250 users.
 Optimizing  X  . The aim of our first experiment is to iden-tify the value of  X  in int  X  that provides the best performance. Figure 2 reports results of int  X  in terms of P@k (a), S@k (b), and MRR (c), for different  X  values.

Results show that adopting  X  values close to 1, corre-sponding to give more importance to pscore , the model achie-ves better performance. Maximum P@k and S@k values, in fact, are obtained when  X  = 1. In addition, in Figure 2(c) we observe that a value of  X  = 0 . 9 corresponds to a signif-icant MRR decrease, i.e., the rank of the best tweets in the positive set P significantly increases.

We can overall conclude that by setting  X  = 0 . 9 the int metric performs very well in terms of both precision and recall, and ranks interesting tweets high in the ranked list. Hereinafter, we therefore adopt the int 0 . 9 metric to estimate the interestingness of tweets for a user.
 Comparing the different methods. We are now ready to compare int 0 . 9 against the following two baselines. Cosine. A baseline model based on cosine similarity. For each tweet t in T , interestingness is estimated as the cosine similarity between the term vectors of t and T u , where each vector is obtained by using the classical information retrieval tf  X  idf [23] score.
 Hashtags. A baseline similar to the above, where the vec-tors are not filled with terms, but with hashtags, similar to what proposed by Abel et al. [1]. The intuition is that hash-tags explicitly summarize the topic(s) of a tweet (e.g.,  X  I am flying to #NY today! #travel  X ).

We also run experiments to compare our models with those generated by Latent Dirichlet Allocation ( LDA ) [5]. Surprisingly, despite being one at the state of the art model, the LDA method performs very poorly. An analysis of the topics discovered by LDA reveals that terms clustered in the same topic do not have a well defined semantic relation. For example one of the topics has as the top-most representa-tive words  X  art , box , winner , soul , alabama , adam , master , live  X . We conclude that the hypothesis that LDA topics can be mapped to user X  X  interests does not hold in our specific case. One possible explanation is that our dataset is proba-bly not big enough to correctly estimate the model parame-ters. Another issue is that LDA may be very sensitive to the short nature of the tweets. While a whole set of user X  X  tweets may suffice to infer the user X  X  topics of interest, a single tweet is too short to be reliably assigned topics. We will not re-port results of LDA given its extremely poor performance. However, we acknowledge that a deeper investigation is nec-essary to better understand the reasons behind poor LDA performances. We defer this to an extended version of this paper.
 Table 2: Comparison of i nt 0 . 9 with Cosine , and Hashtags .
Table 2 shows the comparative results among the tested methods. We observe that int 0 . 9 is the best performing method for all measures. In particular, for the P@1 met-ric, int 0 . 9 ranks a correct tweet in first position in 71% of the cases. Hashtags , which is the second best performing method, instead ranks a correct tweet in the first position in only 32% of cases. int 0 . 9 is the most effective method also in terms of S@k , with a 0 . 85 accuracy in finding correct results in the top-5 ranked tweets. The good performance of int 0 . 9 is confirmed also for the MRR metric. Correct tweets are ranked higher on average by int 0 . 9 than the other two methods. Table 3: Comparison of i nt 0 . 9 with Cosine , and Hashtags on a subset of highly related users.

The high results for int 0 . 9 may have been obtained because of bias in the data, i.e., a user might use a very peculiar writing style that makes his tweets highly distinguishable. In order to remove this bias we manually selected 20 pairs of users having very similar interests. The selection was done by using the Twitter user recommender system. The evaluation for this experiment works as follows: for each pair of users we add all the tweets of the second user in the stream; the task consists to re-retrieving them by using the set of tweets of the first user as the user profile.
Table 3 shows the results of this experiment, with a com-parison against Cosine , and Hashtags . Once again, it is con-firmed that int 0 . 9 performs better than the baselines. This suggests that the bias induced by the user X  X  writing style is very limited.
In addition to the automatic evaluation just shown, we also test the effectiveness of int 0 . 9 by running a user study. We experiment with the following methods: Cosine 8 , int 0 . 9 and int 0 . 9 enhanced with the authoritative friends strategy described in Section 4.2 (hereafter referred as int 0 . 9
The assessment is conducted by a group of 7 professional assessors that regularly post tweets ( Active users , in our ter-minology), and a group of 5 professional assessors that are using Twitter mainly for reading tweets ( Passive users ).
For each assessor and each method, we generate the top-20 tweet suggestions. Each assessor is provided with a random combination of tweets selected by the different methods, and is asked to state his personal interest on each tweet, using the following scale:
T o reduce the load on human assessors we did not evalu-ate Hashtags since in the most conservative experiments in Table 3, they were performing worse than Cosine . All the experimented models were built using a maximum of 200 tweets from the assessor X  X  Twitter stream. As evaluation metrics we use absolute Discounted Cumulative Gain ( DCG ), Precision computed over all assessed tweets of each method, and MRR .
 Table 4: User study results for i nt 0 . 9 and Cosine for Active and Passive users separately. We consider a tweet annotated as E, G, or F as useful.

Table 4 reports results for Active users , Passive users , and both classes combined. Table 5 reports more in detail the percentage of Excellent, Good, Fair and Bad judgments for each method. For Active users, both our methods out-perform Cosine in DCG and Precision , consolidating the re-sults obtained in the automatic evaluation. As expected, int 0 . 9 +auth is the best performing system. The addition of the authoritative friends X  tweets thus proves to be principled, as well as the criterion we used to select authoritative users, described in Section 4.3. For Active users , int 0 . 9 only reports 0 . 57 in MRR , i.e., the top recommended tweet is often not correct. A closer look at the errors shows that this happens when the user X  X  model shares with a bad tweet a term pair with a high pscore (thus boosting int 0 . 9 ), but the term pair is not representative of the user interests (e.g.,  X  have  X - X  today  X ). This effect disappears in int 0 . 9 +auth , where the larger size of the tweet set smoothes the impact of idiosyncratic term pairs.

In general, we verified that the most common errors across all methods are recommendations of  X  status update  X  tweets, e.g.,  X  Almost home! God can X  X  wait to get in my bed  X  Table 5: Percentage of Excellent, Good, Fair and Bad judg-m ents in the top-k tweets suggested to Passive and Active users.
 Table 6: Results of pairwise comparison of T weetRec and Interests-Spanning TweetRec , as judged by the asses-sors. or X  None of my dog friends around here raw feed  X . Ev-en if these tweets may contain terms that define an interest (e.g.,  X  raw feed  X  and  X  dog  X ), they are personal in nature and therefore not interesting. In future work we will explore heuristics to classify and discard status updates. int 0 . 9 +auth stands out even more for Passive users than Active users . This proposes int 0 . 9 +auth as a promising robust solution for tweet recommendation, for Active users as well as Passive users . In both cases, it is important to leverage the tweets of the users X  authoritative friends, that mostly post informative tweets.

The overall precision of our baseline Cosine is 0 . 52, which is relatively higher than expected w.r.t. our int 0 . 9 . We fur-ther investigate this issue by reporting in Table 5 the results of the user study for different scores. int 0 . 9 is almost always 1 . 5 times better than Cosine in ranking Excellent results. The difference is even higher if we consider the effectiveness of the top-10 Excellent or Good results. As a matter of fact, we have that only 27% of Cosine are Excellent or Good w.r.t. 66% of int 0 . 9 +auth . 5.3 TweetRec vs. Interests-Spanning TweetRec In this section, we compare the quality of the solutions of TweetRec versus Interests-Spanning TweetRec . As described in previous sections, a solution for the latter prob-lem should recommend a more heterogeneous, and therefore interesting, set of tweets to the user. As in the previous section, we both conduct an automatic and a user-based as-sessment. over TweetRec . Tweets are shortened for lack of space. Automatic Evaluation. Goal of the automatic evaluation is to assess if the Interests-Spanning TweetRec solution successfully recommends a set of tweets that spans a larger set of user X  X  interests, with respect to TweetRec . We proceed by randomly selecting 15 sets of 20 lists 9 each. The assumption is that each list represents a specific user interest. For each list we download 800 tweets. We then create 15 virtual users with 8 , 000 (= 20  X  400) tweets, one per each set, by selecting 400 tweets per list. We use the remaining 400 tweets per list to build a single virtual stream of tweets. The resulting dataset is a set of 15 virtual users spanning 20 different interests and having produced 8 , 000 tweets. Figure 3: Number of distinct interests (i.e., lists) present in the top-k recommended tweets.

We now evaluate how many different lists (i.e., interests) are present in the top-k tweets recommended by our func-tions. Figure 3 shows the number of distinct lists in the top-k recommended tweets by using the solutions of Twee-tRec and Interests-Spanning TweetRec . Results show that, given a user, the set of tweets reported by solving Interests-Spanning TweetRec spans multiple different interests, whereas the set returned by solving TweetRec is more biased towards a very small set (usually one) of interests. This experimentally proves the value of solving Interests-Spanning TweetRec .
 User study Evaluation. We further investigate the differ-ence between TweetRec and Interests-Spanning Twee-tRec , by evaluating which of the two solutions is preferred by real, i.e., human, users. We use the same group of as-sessors as in Section 5.2.2. For each assessor and for each
A T witter list aggregates the tweets of users that share a common interest. method, we generate the set of top-5 recommended tweets. We then ask the assessors to perform a pairwise compari-son of the top-5 sets from two different methods. We ask them to select the most interesting set or, alternatively, to indicate that the two sets are equally interesting.
Results are reported in Table 6, for two different configu-rations: int 0 . 9 and int 0 . 9 + auth . For both configurations, the top-5 recommended tweets are equally interesting in about half of the cases. If they are not, preference is mostly given to Interests-Spanning TweetRec , because in most cases it selects a set of tweets that spans several user X  X  interests.
For example in Table 7, the tweets selected by TweetRec are mostly focused on Barack Obama, while those selected by Interests-Spanning TweetRec are about a variety of political topics.
We presented different methods for recommending tweets to Twitter users. Results over a large experimental study show that our proposed solutions outperform existing base-lines, and that selecting tweets that are about a variety of the user X  X  interests, improves the user experience.
There is a large avenue for future work. First, we are interested in improving the precision of our methods by de-vising automatic strategies to filter out tweets that are un-interesting  X  X tatus updates X . We also plan to improve the se-lection power of Interests-Spanning TweetRec by pro-viding tweet deduping at higher levels of semantics (e.g., adopting Textual Entailment Recognition techniques).
Our recommendation system could work in several appli-cation scenarios, e.g., providing to users a more interesting timeline than the one currently available in Twitter. We plan to carry out simulation trials to test the computational cost of our methods when facing a real-time load of thou-sands of tweets per second. This research has been partially funded by the EU CIP PSP-BPN ASSETS Project, Grant Agreement no. 250527, and by the EU CIP-ICT PSP 2011.4.1 InGeoCloudS Project, Grant Agreement no. 297300. This work was completed before Marco Pennacchiotti joined eBay Inc..
 [1] F. Abel, Q. Gao, G.-J. Houben, and K. Tao.
 [2] E. Bakshy, J. M. Hofman, W. A. Mason, and D. J. [3] S. Banerjee, K. Ramanathan, and A. Gupta.
 [4] N. J. Belkin and W. B. Croft. Information filtering [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [6] C. Budak, D. Agrawal, and A. El Abbadi. Limiting [7] R. Burke. Hybrid recommender systems: Survey and [8] J. Chen, R. Nairn, L. Nelson, M. Bernstein, and [9] G. Cornuejols, M. L. Fisher, and G. L. Nemhauser. [10] A. Dong, R. Zhang, P. Kolari, J. Bai, F. Diaz, [11] Y. Duan, L. Jiang, T. Qin, M. Zhou, and H.-Y. Shum. [12] M. L. Fisher, G. L. Nemhauser, and L. A. Wolsey. An [13] M. R. Garey and D. S. Johnson. Computers and [14] A. Goyal, W. Lu, and L. V. Lakshmanan. Celf++: [15] P. Judge. Barracuda labs, annual report, 2009. [16] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing [17] H. Kwak, C. Lee, H. Park, and S. Moon. What is [18] J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos, [19] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An [20] X.-H. Phan, L.-M. Nguyen, and S. Horiguchi.
 [21] P. Resnick and H. R. Varian. Recommender systems. [22] F. Ricci, L. Rokach, B. Shapira, and P. B. Kantor. [23] G. Salton, A. Wong, and C. S. Yang. A vector space [24] P. Schonhofen. Identifying Document Topics Using [25] S. Ye and S. F. Wu. Measuring message propagation
