 and feature selection. Let G = ( V, E ) be a directed graph with weight A 0 motion b where  X  take V = [ p ]  X  { 1 , . . . , p } . In words, the rate of change of x represented by entries { A 0 this a mild restriction, since the system converges exponen tially to stationarity. A portion of time length T of the system trajectory { x ( t ) } with the number of its degrees of freedom? G fixed p , as T  X  X  X  , while we will tackle here the regime in which both p and T diverge. graphical model learning problems: Our results confirm in a detailed and quantitative way these i ntuitions. 1.1 Results: Regularized least squares discuss relations with existing literature in Section 1.3.
 row, A 0 where the likelihood function L is defined by is indeed related to least squares, one can formally write  X  x for the right hand side of Eq. (3), thus getting the integral R ( A  X  regularization term in Eq. (2) has the role of shrinking to 0 a subset of the entries A selecting the structure.
 the signed support of A 0 the maximum and minimum eigenvalue of a square matrix M respectively. Further, denote by A the smallest absolute value among the non-zero entries of ro w A 0 and one assumption on  X  high probability. This result is stated in the following the orem. Theorem 1.1. Consider the problem of learning the support S 0 of row A 0 sample trajectory { x ( t ) } then there exists  X  such that  X  Further, it is roughly inversely proportional to  X  since  X  1.2 Overview of other results with parameter  X  &gt; 0 : as above, and { w ( t ) } related to the fundamental fact that the samples { x ( t ) } as  X   X  0 ).
 as itself. 1.3 Related work A substantial amount of work has been devoted to the analysis of  X  Learning sparse graphical models via  X  conditions on a modified covariance. Also this paper focuses on i.i.d. samples. accurate probability estimates that capture the correct sc aling for small  X  . that these works focus once more on system with a fixed (small) number of dimensions p . oped here. Further, no model selection guarantee was proved in [16, 17]. which introduce parameters C 2.1 Learning the laplacian of graphs with bounded degree  X  deg( i ) on the diagonal [18]. (Here deg( i ) denotes the degree of vertex i .) of strength m to the origin. We obtain the following result.
 Theorem 2.1. Let G be a simple connected graph of maximum vertex degree k and consider the model (1.1) with A 0 =  X  m I +  X  G where  X  G is the laplacian of G and m &gt; 0 . If then there exists  X  such that  X  complexity for 90% probability of success vs. p. 2.2 Numerical illustrations process x n probability of success over randomly generated matrices.
 p Consider a system evolving in discrete time according to the model (6), and let x n be the observed portion of the trajectory. The r th row A 0 convex optimization problem for A where amount to a more precise version of this statement. Furtherm ore, L ( A log-likelihood of A of The process is generated for a small value of  X  and sampled at different rates. As before, we let S 0 be the support of row A 0 Lyapunov equation adopt the notations already introduced there. We use as a sho rthand notation  X  where  X  Theorem 3.1. Consider the problem of learning the support S 0 of row A 0 trajectory { x ( t ) } then there exists  X  such that  X  information that can be harnessed from a given time interval [0 , T ] . The r th row of W is denoted by W In order to lighten the notation, we will omit the reference t o x n simply write L ( A 4.1 Discrete time finally we sketch the outline of the proof.
 [12]. A proof of this proposition can be found in the suppleme ntary material. Proposition 4.1. Let  X , C signed support sign( A 0 and the hessian of the likelihood (3).
 taken with respect to a stationary trajectory, we have E { b G } = 0 , E { b Q } = Q 0 . 4.1.1 Technical lemmas the supplementary material provided.
 Our first Proposition implies concentration of b G around 0 .
 Proposition 4.2. Let S  X  [ p ] be any set of vertices and  X  &lt; 1 / 2 . If  X  then bounds on ||| b Q subsets of { 1 , ..., p } . We have, Then, we bound | b Q Proposition 4.3. Let i, j  X  { 1 , ..., p } ,  X  2 /D where D = (1  X   X  max ) / X  then, Finally, the next corollary follows from Proposition 4.3 an d Eq. (17). Corollary 4.4. Let J, S ( | S | X  k ) be any two subsets of { 1 , ..., p } and  X   X  &lt; 2 k/D and n X  &gt; 3 /D (where D = (1  X   X  max ) / X  ) then, 4.1.2 Outline of the proof of Theorem 3.1 of the theorem we have that the first two conditions (  X , C ( A We also combine the two last conditions on b Q , thus obtaining the following 4.2 Outline of the proof of Theorem 1.1 expressions are used for b G and b Q , namely hessian b Q n for any n  X  n b G theorem.
 to check that Q 0 (  X  )  X  Q 0 as  X   X  0 by the uniqueness of stationary state distribution. ( Q standard Brownian motion { b ( t ) } random walk to Brownian motion.
 and the NSF grant DMS-0806211 and by a Portuguese Doctoral FC T fellowship. [5] K. Zhou, J.C. Doyle, and K. Glover. Robust and optimal control . Prentice Hall, 1996.
