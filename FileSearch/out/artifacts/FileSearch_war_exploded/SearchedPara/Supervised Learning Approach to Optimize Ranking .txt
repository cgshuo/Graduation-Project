 As one of the major approaches for question answering system construction, FAQ-Finder system answers new question by s earching in a collection of previously-answered questions. Many researches have been carried out (e. g., [1, 2, 3, 4 and 5]). To build FAQ-Finder system needs consider two problems: 1) how to collect large QA pairs given new question query. 
To solve the collection problem, two resources come into researchers X  sight: the huge accumulations of FAQ pages on internet (e.g., [2]) and the significantly increased community-based question and answer services on the web where people answer other people X  X  questions (e.g., [3]). For example, the service on http:// zhidao.baidu.com has accumulated more than 8 million QA pairs in Chinese with considerable quality, which is the start point of this paper. most of them just focused on how to solve the word mismatching problem among questions, including utilizing lexical semantics dictionaries such as WordNet [1], conducting question type analysis [4] and employing machine translation technique [3] etc. Little research work paid attention to the more fundamental problem: ranking function. To our knowledge, only ranking functions such as TFIDF (e.g., [1]) are arbitrarily borrowed from traditional document retrieval without any customizations, which is obviously an unfortunate neglect. Therefore, based on the analysis of differences between FAQ-Finder and traditional document retrieval, we pursue to improve FAQ-Finder system by optimizing the ranking function in this paper. 
The rest of the paper is organized as follows: section 2 describes our experimental collection. Unified ranking function appro ach and supervised learning approach are proposed in section 3 and section 4 separately. Finally, we conclude in section 5. Baidu is one of the leading commercial search engines in China, and its question and answer service (http://zhidao.baidu.com) is also very popular. Over time, this service this paper are based on subsets of this archive (referred as Zhidao hereafter). 2.1 QA Pair Archive and Query Set Table 1 shows a QA pair example from Zhidao archive. The question part has two question title in more detail. The answer part always includes several answers, among For brief, question title, question description and best answer are referred as Q , D , and A respectively hereafter. For more examples, please refer http://zhidao.baidu.com. 
We successfully download 3.9 million QA pairs, and split them into two parts million QA pairs posted after that date. We utilize the first part as retrieval source in our experiments, referred as BaiduSet hereafter. The other part is employed to generate the experimental query set. The purpose of separation according posting time simply employ some word frequency based approach to select 2000 common questions, and employ one assistant to pickup 1000 meaningful questions to build the 2.2 Relevance Judgment To verify the performance of each retrieval technique, we first construct one evaluation dataset accord ing following steps: 1) Index BaiduSet on the Q , D and A field separately with Lucene.NET [6]; 2) For each query P in QuerySet : 
In manual relevance judgment, we consider two aspects: whether the question of pair is useful enough. Six grades of relevance scores are defined, as listed in Table 2. 
Score Specification
We regard one QA pair is useful to P iff its relevance score is equal or higher than 3. According to this specification, we employed four assistants to annotate the presents some distribution information about the relevance scores. 2.3 Measure Criterion Since multiple grades of relevance scores are defined, according to Kazuaki Kishida X  X  work [7], we employ one multi-grades relevance indicator: the precision-oriented modified sliding ratio v S X  , which is defined as following formula: Finder system. Because only QA pairs in the pool are manually annotated, for justice, according corresponding approach, regardless of all the other un-annotated QA pairs. 3.1 Characteristics of FAQ-Finder To retrieve relevant QA pair from millions of QA pairs given new question query is a differences include: 1) Different query: the query for FAQ-Finder is a natural language question 2) Different corpus to be retrieved from: the corpus for traditionally document 3) Different essential difficulties: for traditional document retrieval, the most 3.2 Unified Ranking Function Based on the above analysis, initialized from traditional TFIDF ranking model, we design one unified ranking function for FAQ-Finder as follows: Supervised Learning Approach to Optimize Ranking Function for Chinese FAQ-Finder 535 where, P denotes the query and F denotes one field of QA pair, such as Q , D , and A . four different influences to the ranking function: 1)  X  controls in what degree we ca re about the repeated word in F ; 2)  X  controls in what degree we emphasize the word with high idf value; 3)  X  controls in what degree we care about the words not found in P but in F ; 4)  X  controls in what percentage words with lowest idf value are discarded; The default values for these four parameters are 1.0, 1.0, 1.0 and 0.0 respectively. Note that the unified ranking function will reduce to TFIDF ranking function if all the will be investigated later. 3.3 Experimental Settings To investigate the effect of word segmentation on Chinese FAQ-Finder, we perform consisted of a dictionary of about 60,000 words, word frequency based segmentation disambiguation algorithm, and automatic proper name recognition. The word segmentation system achieves about 97% word segmentation accuracy evaluated on traditional newspapers corpus. These two experimental settings are referred as Dict_0K and Dict_60K respectively hereafter. And to evaluate each contribution of Q , D , and A field, we carry out experiments on each of the three fields separately. 3.4 Experimental Results 0.0 to 2.0 with 0.1 step. Six series of experiments were conducted under experimental the v S X  curves against different values for each parameter. From Figure 2, we can see that it is possible to improve the performance by tuning the four parameters in unified ranking function. By employing Hill-Climbing algorithm [8], we optimized four parameters under each experimental setting and evaluated the improvements by 4-folds cross validation. Table 3 shows the improvements. 
From the experimental results we can conclude: 1) Retrieval based on Q is significant better than D and A field; 2) Between the two word segmentation settings, different trends can be observed 3) Okapi (just BM25) is a little worse than TFIDF while the Language Model 4) The optimized value of each parameter is quite rational and generalizable:  X  536 G. Hu et al. 5) Significant improvement (6.67% improvement with p &lt;= 0.000539 under Q on each small figure denotes the experimental setting. columns. The  X  X aseline X  column denotes unified ranking function with the four parameters set each parameter in formation of minimum~maximum obtained in the 4-folds optimization. Field Dict. Okapi LM Baseline Optimized  X   X   X   X  Q 60K 0.3589 0.0162 0.4105 0.4270 0.2~0.4 1.0~1.1 0.5~0.7 0.0~0.2 D 0K 0.1056 0.0156 0.1852 0.1377 0.3~0.8 0.9~1.3 0.6~0.9 0.0~0.2 D 60K 0.1574 0.0099 0.1966 0.2019 0.4~0.8 1.2~1.6 0.6~0.8 0.0~0.2 A 0K 0.2310 0.0117 0.2383 0.2658 1.1~1.4 1.0~1.4 0.6~0.8 0.0~0.2 
A 60K 0.2958 0.0055 0.3237 0.3373 1.3~1.7 1.0~1.1 0.6~0.7 0.0~0.2 4.1 Supervised Ranking Function the ranking procedure in FAQ-Finder, and here we just employ simple linear model to v , where n is the dimension of feature vector. Then given a vector of feature weights ) ,..., ( Supervised Learning Approach to Optimize Ranking Function for Chinese FAQ-Finder 537 
Given the linear model and an annotated training corpus, there are also various supervised learning approaches to optimize th e weight vector. Similar to the work of Hu et al. on entity search [8], Hill-Climbing algorithm is employed for training here. 4.2 Features We totally extracted 264 features for each P and QA pair, as shown in Table 4. 4.3 Experimental Results presented in Table 5 and Figure 3. From these experimental results, we can conclude Chinese FAQ-Finder, and 2) there truly exist qu ite a lot of features that can contribute the retrieval performance. 0 (baseline of retrieval on Q field only) 0.4448 0~32 and 132~164 (all features related to Q only) 0.4704 (+5.76%) p &lt;= 2.82e-14 0,33,66, and 99 (baseline of retrieval on all fields) 0.4803 0~263 (all features) 0.5142 (+7.06%) p &lt;= 4.39e-11 538 G. Hu et al. We construct a Chinese FAQ-Finder system based on 3.8 million QA pairs in this paper. Unlike most published researches which lean to address word mismatching problem among questions, we focus on how to optimize the fundamental ranking function and two approaches are proposed. First we design a unified ranking function with four parameters for Chinese FAQ-Finder which achieves 6.67% ( p &lt;= 0.000539) improvement. Second, supervised learning approach together with 264 features extracted from the input query and QA pair are employed to further optimize ranking function, and 7.06% ( p &lt;= 4.39e-11) significant improvement is achieved again . 3. Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee: Finding similar questions in large question and 4. Lytinen, S..; Tomuro, N., The Use of Question Types to Match Questions in FAQFinder. In 5. Che Wanxiang, Liu Ting, Qin Bing, Li Sheng, Chinese Sentence Similarity Computing for 6. Lucene.NET, http://www.dotlucene.net/ 7. Kazuaki Kishida, Property of Average Precision and its Generalization: An Examination of 
