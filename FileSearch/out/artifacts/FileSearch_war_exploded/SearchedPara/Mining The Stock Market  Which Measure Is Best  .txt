 Inrecentyears,therehasbeenalotofinterestinthedatabase community in mining time series data. Surprisingly, little work has been done on verifying which measures are most suitableforminingofagivenclassofdatasets. Suchworkis of crucial importance, since it enables us to identify similar-ity measures which are useful in a given context and there-fore for which efficient algorithms should be further investi-gated. Moreover, anaccurate evaluationof theperformance of even existing algorithms is not possible without a good understanding of the data sets occurring in practice. Inthisworkweattempttofillthisgapbystudyingsimilarity measures for clustering of similar stocks (which, of course, is an interesting problem on its own). Our approach is to cluster the stocks according to various measures (including several novel ones) and compare the results to the  X  X round-truth X  X lusteringbasedontheStandardandPoor500Index. Our experiments reveal several interesting facts about the similarity measures used for stock-market data.
 I.5.3 [ Clustering ]: Similarity measures time series, stock, clustering, similarity measures, datamin-ing Inrecentyears,therehasbeenalotofinterestinthedatabase community in mining time series data. Such data naturally arise in business as well as scientific decision-support appli-cations; examples include stock market data (probably the most studied time series examples in history), production capacities, population statistics, and sales amounts. Since the data sets occurring in practice tend to be very large, most of the work has focused on the design of efficient algo-rithms for various mining problems and, most notably, the search of similar (sub)sequences with respect to a variety of measures [1, 2, 3, 4, 5, 7, 8, 10].
 Surprisingly, little work has been done on verifying which measuresaremostsuitableforminingofagivenclassofdata sets. Such work is of crucial importance, since it enables us to identify similarity measures which are useful in a given context and, therefore, for which efficient algorithms should be further investigated. Moreover, an accurate evaluation of the performance of theexisting algorithms is notpossible without good understanding of the data sets occurring in practice.
 Inthisworkweattempttofillthisgapbystudyingmeasures forclusteringofsimilarstocks(see[11]formoreinformation aboutminingfinancialdata). Weobtainedthestock-market data for 500 stocks from the Standard &amp; Poor (S &amp; P) in-dexfortheyear1998. Eachstockisaseries of252 numbers, representing the price of the stock at the beginning of an operational day. Every time series is assigned to one out of 102 clusters (e.g.  X  X omputers (Hardware) X ,  X  X il and Gas X , etc). Assuming this classification as a  X  X round-truth X , we try to re-create it by running a clustering algorithm on the stock data using a variety of similarity measures. Then the respective measures are evaluated by comparing the result-ing clustering to the original S &amp; P classification (we use other evaluation methods as well).
 Our experiments exhibited several very interesting proper-ties of stock market data. First of all, the best clustering results were obtained for a novel measure proposed in this paper which uses piecewise normalization . The main idea behind this approach is to split the sequences into blocks and perform separate normalization within each block. Our results indicate that this approach yields quite powerful re-sultsforstock-marketdata. Anotherinterestingobservation is that comparing the normalized derivatives of the price se-quences resulted in better clusterings than comparing the actualsequences(thisphenomenoniswidelyknowninthefi-nancial community). Inparticular, the combination of both of the above ideas results in the highest quality clustering obtained in this paper, depicted in Table 5. One can ob-serve that the majority of our clusters have a one-to-one correspondence with one of the original S &amp; P clusters, in permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 the sense that for each of these clusters (say C ) the S &amp; P cluster closest to C also chooses C as its closest cluster. All of the remaining clusters are not nearest neighbors of any S &amp; P cluster.
 Thehighqualityoftheclustering obtained usingderivatives hasveryinteresting implications, sincetheperformance anal-ysisformostofthetimesseriesdatastructures assumesthat the sequences are smooth 1 , which is clearly not the case for the derivatives. Therefore, our results suggest that new al-gorithmic techniques should be developed, to capture the scenarios in which non-smooth time series data are present. The Data. WehaveusedtheStandard andPoor500index (S&amp;P) historical stock data published at http://kumo.sw cp.com/sto cks/ . There are approximately 500 stocks which daily price fluctuations are recorded over the course of one year.
 Each stock is a sequence of some length d , where d  X  252 (the latter number is the number of days in 1998 when the stock market was operational, but d can be smaller if the company is removed from the Index). We used only the day X  X opening price; thedataalso containstheclosing price, and the low and high stock valuation for the day. Thedataalso containedtheofficial S&amp;Pclustering informa-tion which groups the different stocks into industry groups basedontheirprimarybusinessfocus. Thisinformation was also used in our experiments, with the assumption that it providesuswithabasis fora X  X round-truth X  withwhichwe cancompareandratetheresultsofourunsupervisedcluster-ing algorithm. We abstracted the 102 members of this S&amp;P clustering into 62  X  X uperclusters X  by combining closely re-latedonestogether, e.g.,  X  X utomobiles X  and X  X uto(Parts) X  or  X  X omputers (Software) X  with  X  X omputers (Hardware) X . Feature Selection. Our feature selection approach con-sists of three main steps, depicted on the following picture: 1. Represen tationchoice: inthisstepwemaptheoriginal fewelementsintheFourierrepresentationofasequence; the quality of approximation in this case relies on the fact that high frequency component of a signal have low amplitude, which is clearly not the case for the derivative sequence. 2. Normalization: in this step we decide if and how we 3. Dimensionalit y reduction: in this step we aim to re-Until now we described how we compare sequences of iden-tical length. In order to compare a pair of sequences of different lengths, we take only the relevant portion of the longer time series, andperform the aforemen tioned process-ing only on that part. However, in order to perform PCA, we need to assume that all points have the same dimension, so instead, we pad all shorter sequences with zeros. Similarit y measure. We use the Euclidean distance be-tween the feature vectors.
 The clustering method. We use Hierarchical Agglomer-ativeClustering (HAC),whichinvolvesbuildingahierarchi-calclassification ofobjectsbyaseriesofbinarymergers(ag-glomerations), initiallyofindividual objects,laterofclusters formed at previous stages; see [6] for more details. A single  X  X artition X  (slice across the hierarchy) can thenbe taken at any level to give the desired number of clusters. We exper-imented with several rules for agglomeration. Merging two clusterswhichhavethesmallestmaximumdistancebetween two inter-cluster elements proved to yield the best results. Rating and Comparing the Results. In order to eval-uate the various results we got from applying the different feature selection mechanisms, we need the  X  X round-truth X , i.e., someaprioriclassification whichwecanuseforcompar-isons. The S&amp;P clustering (provided in the input data as discussed above) serves exactly this purpose. This choice of  X  X round-truth X  is based on the reasonable assumption that thepricingofeachstockwillbemainlyinfluenced byfactors specific to the particular industry sector to which this stock belongs.
 We usethefollowing measure thatgiven thetwo clusterings C = C 1 . . . C k (say S &amp; P clusters) and C 0 = C 0 1 HACclusters), computes theirsimilarity usingthefollowing formula: and Note that this similarity measure will return 0 if the two clusterings are completely dissimilar and 1 if they are the same. The measure has been already used for comparing different clusterings, e.g., in [9]. Note that this measure is not symmetric.
 The above approach, although very natural, has the follow-ing disadvantage: the quality of the clusters depends not only on the similarity measure, but also on the clustering algorithm used. To avoid this problem, we complemen t our evaluationbyusingamethodakintoPrecision-Recall curves widely used in the Information Retrieval community. Here theyare definedasfollows. Foreachstock (say S )we  X  X ssue a query X , that is rank all of the other stocks S 0 according to the distance between S and S 0 , the closest ones first. We consider the stocks belonging to the same S &amp; P cluster as S to be X  X elevant X  and all other stocks to be X  X ot relevant X . Then we define a graph which for every rank i depicts the percentage of relevant stocks among the i stocks closest to P . The final curve is obtained by averaging the curves ob-tained for all stocks S . We started from finding the right parameters for the eigen-value decomposition for each particular feature-type. After applying SVD on the raw data, it turned out that 97 . 62% of the eigenvalue weight is in the first 5 values, and 98 . 88% is in the first 10 values. This suggests that a projection of this 252-dimensional data in 10-or even 5-dimensional space will result in a negligible loss of informa-tion (see Figure 2 and 3). Figure 2: All eigenvalues for raw data before global normalization Figure 3: First 11 eigenvalues for raw data before global normalization After global normalization of the data, we observe an in-crease in the error of dimensionalit y reduction. Now, in the first 10 eigenvalues is 90 . 47% of the weight, while 94 . 99% is in the first 20, and 98 . 19% in thefirst 50. Nevertheless, this could still allow us to achieve a significan t reduction (see Figure 4 and 5).
 After taking the first derivative, 61 . 95% of the eigenvalue weight is in the first 20 eigenvalues, 80 . 48% in the first 50, and 92 . 42% in the first 100 (see Figure 6). Figure 4: All eigenvalues for raw data after global normalization Figure 5: First 25 eigenvalues for raw data after global normalization Figure 6: Plot of all 252 eigenvalues after applying the first derivative The last transformation we used was taking the firstderiva-tive, then normalizing. Here, we observe the biggest dimen-sionality  X  X ispersal X . Only 75 . 23% of the eigenvalue weight is contained in almost half of the original dimensions, while 52 . 20% is inthefirst50, andonly21 . 47% inthefirst10 (see Figure 7). Figure 7: Plot of all 252 eigenvalues after applying globally normalized first derivative To summarize: we get tremendous dimensionalit y reduction on the raw data  X  97 . 62% of the information is only in 5 dimensions ! As we process the data (normalization and firstderivatives(FD)),weget adimensionalit y  X  X ispersal X . Themost X  X ispersed X  X sthedatathatwaspre-processedthe most.
 Therefore, for the further dimensionalit y reduction experi-ments, we choose thedimensionalit y for theabove scenarios to be equal to respectively 5, 10, 50 and 100.
 Clusters. Theresultsoftheclusterings (together withtheir parameters) are depicted in Tables 1, 2, 3 and 4. Table 1 wasobtained byclustering thestocksaccording using8vari-ants of the similarity metrics, from the space { raw data, first derivative } x { global normalization, no Thenumberofclusterswassetto62 (i.e., equaltothenum-ber of S &amp; P  X  X uperclusters X ).
 Thesecondtableshowstheresultsfordimensionalit yreduc-tion via aggregation. In this case, we vary the size of the aggregation window, the data representation (raw data or FD) and normalization (global or none).
 We also performed experiments where the dimensionalit y reduction was done via Fourier Transform. Since, by Par-seval X  X  Theorem [1], the Euclidean norm of a raw vector is equal to the norm of its spectral representation, we per-formed the experiments on trunc ated spectral representa-FD Norm Dims Sim(S&amp;P ,HAC) Sim(HAC,S&amp;P) N N all 0.183 0.210 N N 5 0.197 0.210 N Y all 0.222 0.213 N Y 10 0.211 0.212 Y N all 0.154 0.198 Y N 50 0.172 0.207 Y Y all 0.290 0.298 Y Y 100 0.310 0.310 Table 1: The clustering results, with PCA dimen-sionality reduction FD Norm AggWin Sim(S&amp;P ,HAC) Sim(HAC,S&amp;P) N N none 0.183 0.210 N N 5 0.192 0.217 N N 10 0.193 0.215 N N 20 0.192 0.213 N Y none 0.228 0.217 N Y 5 0.217 0.212 N Y 10 0.221 0.216 N Y 20 0.215 0.220 Y N none 0.152 0.197 Y N 5 0.190 0.211 Y N 10 0.195 0.217 Y N 20 0.178 0.208 Y Y none 0.288 0.294 Y Y 5 0.225 0.217 Y Y 10 0.230 0.231 Y Y 20 0.211 0.211 Table 2: The clustering results, with dimensionalit y reduction via aggregation tions, i.e., we kept only a few of its lowest frequencies. The resulting experiments are presented in Table 3.
 In the last table (Table 4), we show the results obtained when using piecewise normalization. Again, we vary the window size (note that the role of the window is different than in the previous set of experiments) and data represen-tation.
 One can observe that the best result (i.e., with the high-est Sim measure) was obtained when using the combination of piecewise normalization with window of length 15 and first derivative. The resulting clustering is depicted in Ta-ble 5. Unfortunately , the whole description of the clusters is too big to be included in the paper. However, we present the clusters in the following form. The i th row corresponds to one of the HAC clusters (say C i ). The second column shows the name of the S&amp;P cluster which is the closest to C i according to Sim(HAC, S&amp;P) measure. The remaining columns show the names of S&amp;P clusters which choose C to be the HAC cluster closest to them , listed in the order of similarity Sim(S&amp;P , HAC), the most similar first. Notice thatthecolumntwoandthreearealmostalwaysequal,with theexception ofclusters4, 35, 36, orwhenthethirdcolumn is empty.
 FD Norm Freqs Sim(S&amp;P , HAC) Sim(HAC,S&amp;P) N N 5 0.191 0.197 N N 10 0.203 0.204 N N 25 0.192 0.196 N N 50 0.193 0.202 N Y 5 0.215 0.217 N Y 10 0.210 0.208 N Y 25 0.221 0.229 N Y 50 0.225 0.224 Y N 5 0.202 0.215 Y N 10 0.189 0.209 Y N 25 0.191 0.217 Y N 50 0.190 0.212 Y Y 5 0.198 0.209 Y Y 10 0.235 0.236 Y Y 25 0.247 0.240 Y Y 50 0.232 0.234 Table 3: The clustering results after Fourier Trans-form Table 4: The clustering results, with piecewise nor-malization A few example clusters are depicted in Tables 6, 7, 8, 9 and 10.
 Precision-recall curves. In order to make our observa-tions independent from the clustering algorithms, we also computed Precision-Recall curves (PR-curv es) for a variety of measures andcompare them to the PR-curvefor normal-ized derivatives (see Figures 8, 9 and 10). This allows us to make a visual estimation of the influence of various pa-rameters (feature extraction algorithm, normalization etc) on theclustering quality. Ineach case, one can observe that thehigherqualityclustering correspondstoaPR-curvewith higher values at the beginning of the curve, which corre-sponds to higher precision at the beginning. The experiments described in the previous section support several interesting general observations. First of all, nor-malizing the input vectors in any form always improved the quality of the results. This behavior is very natural, since normalization enables us to reduce the effect of translation and scaling of the sequences. However, it turned out that Figure8: FourierTransform vs. globally normalized derivatives applying the same normalization to the whole sequence is notthebestsolution, andthatonecanobtainbetterresults byusingpiecewise normalization, foracarefully chosenwin-dow size. The fact that piecewise normalization behaves so good can be explained by observing that it greatly reduces the effect of  X  X ocal anomalies X  on the distance between two stockindices. This isduetothefactthatsuch X  X nomalous X  events affect only one or two windows, while others can still adjust to each other if their behavior is similar, even if the actual valuations are very different.
 Another observation is that using the normalized deriva-tives of the sequence data resulted usually in better results than using raw data. This phenomenon is widely known in thefinancial communityandcanbeexplained byarguments similar to theabove ones, i.e., that thelocal anomalies have onlylimitedinfluence onthedistancebetweenderivatives,as opposedto large influence incaseof therawdata. However, webelievethatmoreresearchhastobedoneinordertofully understand this behavior. For example, just computing the first derivatives without normalization actually worsens the performance (as compared to the raw data results). Another set of interesting observations comes from the at-tempts to reduce the dimensionalit y of the data. Although the raw, unprocessed data seems to have the lowest intrin-sic dimensionalit y (according to PCA), we are not able to build very good clusters out of it. It is actually interesting to note that in the case when we get best clusterings (i.e., we usenormalized derivatives)thedata doesnotseem tobe proneto dimensionalit y reduction, since theprojection onto a 100-dimensional space preserves only 75.23% of the total eigenvalueweight. However,eveninthiscase, theclustering was better than for the full data. There has been a significan t amount of recent research fo-cused on designing efficient algorithms for similarity search Figure 9: Globally normalized raw data vs. globally normalized derivatives and mining of time series. This work has been started by Agrawal et al [1], who showed that indexing time se-quences can be done very efficiently using Fourier Trans-form. This work has been extended in [7] to finding simi-lar subsequences. Another important contribution has been done by Agrawal et al in [2], who also addressed the issue of sequence transformations (like scaling and translations) as well as noise resilience. Further work in this area has been done by Rafiei-Mendelzon [10], Bollobas et al [3], Das et al [4] and Huang-Yu [8]. We would like to thank Prof. Carlo Tomasi and Kathy Pullen for their help. [1] R. Agrawal, C. Faloutsos, A. Swami,  X  X fficient [2] R. Agrawal, K. Lin, H. S. Sawhney, K. Shim,  X  X ast [3] B. Bollobas, G. Das, D. Gunopulos, H. Mannila, [4] G. Das, D. Gunopulos, H. Mannila,  X  X inding Similar [5] G. Das, K. Lin, H. Mannila, G. Renganathan, Figure 10: Piecewise normalization vs. global nor-malization (for derivatives) [6] W. Frakes and R. Baeza-Yates, editors.  X  X nformation [7] C. Faloutsos, M. Ranganathan and Y. Manolopoulos, [8] Y. Wu Huang, P. S. Yu,  X  X daptiv e Query Processing [9] B. Larsen, C. Aone,  X  X ast and effective text mining [10] D. Rafiei, A. Mendelzon,  X  X imilarit y-Based Queries [11] A. S. Weigend,  X  X ata Mining in Finance: Report from 2 Retail (0.518519) Retail (0.518519) 4 Agricultural (0.285714) Electrical (0.166667) 5 Computers (0.129032) 6 Biotechnology (0.25) Biotechnology (0.25) Office (0.222222) 7 Truckers (0.285714) Truckers (0.285714) 8 Oil (0.901961) Oil (0.901961) 9 Household (0.4) Household (0.4) 10 Personal Care (0.285714) Personal Care (0.285714) 11 Iron (0.428571) Iron (0.428571) Footwear (0.4) 12 Telephone (0.592593) Telephone (0.592593) Tobacco (0.190476) 13 Natural Gas (0.153846) 14 Homebuilding (0.333333) Homebuilding (0.333333) Air (0.133333) 15 Financial (0.142857) 16 Services (0.111111) 17 Photograph y/Imaging (0.25) 18 Electric (0.142857) 19 Retail (0.238095) 20 Chemicals (0.216216) 21 Auto (0.166667) 23 Housewares (0.4) Housewares (0.4) Engineering (0.222222) 24 Consumer (0.222222) Consumer (0.222222) 25 Auto (0.24) Auto (0.24) 26 Oil (0.129032) 27 Hardware (0.166667) Hardware (0.166667) Broadcasting (0.133333) 28 Health (0.55814) Health (0.55814) 29 Aerospace/Defense (0.222222) 30 Insurance (0.27027) 31 Homebuilding (0.333333) 32 Gaming (0.285714) Gaming (0.285714) 33 Insurance (0.368421) Insurance (0.368421) 34 Waste (0.333333) Waste (0.333333) 35 Restauran ts (0.210526) Specialty Printing (0.117647) 36 Chemicals (0.214286) Trucks (0.181818) 37 Services (0.222222) Services (0.222222) 38 Electric (0.96) Electric (0.96) 39 Photograph y/Imaging (0.3333) Photograph y/Imaging (0.3333) 40 Gold (0.909091) Gold (0.909091) 41 Publishing (0.266667) Publishing (0.266667) Building (0.2) 42 Natural (0.133333) 43 Paper (0.608696) Paper (0.608696) Containers (0.3) 44 Beverages (0.4) Beverages (0.4) Agricultural (0.4) 45 Distributors (0.222222) Distributors (0.222222) Foods (0.210526) 46 Telecommunications (0.153846) 47 Foods (0.133333) 48 Computers (0.0714286) 49 Iron (0.25) 50 Investment (0.25) Investment (0.25) 51 Restauran ts (0.333333) Restauran ts (0.333333) 52 Chemicals (0.307692) Chemicals (0.307692) 53 Railroads (0.333333) Railroads (0.333333) 55 Paper (0.428571) 56 Aerospace/Defense (0.666667) Aerospace/Defense (0.666667) 57 Telecommunications (0.333333) Telecommunications (0.333333) 58 Leisure (0.285714) Leisure (0.285714) Textiles (0.222222) 59 Savings (0.666667) Savings (0.666667) 60 Power (0.666667) Power (0.666667) Communications (0.2) 61 Computers (0.0689655)
