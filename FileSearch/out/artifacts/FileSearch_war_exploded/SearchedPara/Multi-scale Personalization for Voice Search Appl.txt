 Search engines are a powerful mechanism to find specific content through the use of queries. In recent years, due to the vast amount of information avail-able, there has been significant research on the use of recommender algorithms to select what information will be presented to the user. These systems try to predict what content a user may want based not only on the user X  X  query but on the user X  X  past queries, history of clicked results, and preferences. In (Tee-van et al., 1996) it was observed that a significant percent of the queries made by a user in a search engine are associated to a repeated search. Recom-mender systems like (Das et al., 2007) and (Dou et al., 2007) take advantage of this fact to refine the search results and improve the search experience.
In this paper, we explore the use of personaliza-tion in the context of voice searches rather than web queries. Specifically, we focus on data from a multi-modal cellphone-based business search application (Acero et al., 2008). In such an application, repeated queries can be a powerful tool for personalization. These can be classified into short and long-term rep-etitions. Short-term repetitions are typically caused by a speech recognition error, which produces an in-correct search result and makes the user repeat or reformulate the query. On the other hand, long-term repetitions, as in text-based search applications, oc-cur when the user needs to access some information that was accessed previously, for example, the exact location of a pet clinic.

This paper proposes several different user per-sonalization methods for increasing the recognition accuracy in Voice Search applications. The pro-posed personalization methods are based on extract-ing short-term, long-term and Web-based features from the user X  X  history. In recent years, other user personalization methods like deriving personalized pronunciations have proven successful in the context of mobile applications (Deligne et al., 2002).
The rest of this paper is organized as follows: Sec-tion 2 describes the classification method used for rescoring the recognition hypotheses. Section 3 de-scribes the proposed personalization methods. Sec-tion 4 describes the experiments carried out. Finally, conclusions from this work are drawn in section 5. 2.1 Log linear classification Our work will proceed by using a log-linear clas-sifier similar to the maximum entropy approach of (Berger and Della Pietra, 1996) to predict which word sequence W appearing on an n-best list N is most likely to be correct. This is estimated as
P ( W | N )= The feature functions f i ( W, N ) can represent ar-bitrary attributes of W and N . This can be seen to be the same as a maximum entropy formulation where the class is defined as the word sequence (thus allowing potentially infinite values) but with sums restricted as a computational convenience to only those class values (word strings) appearing on the n-best list. The models were estimated with a widely available toolkit (Mahajan, 2007). 2.2 Feature extraction Given the use of a log-linear classifier, the crux of our work lies in the specific features used. As a base-line, we take the hypothesis rank, which results in the 1-best accuracy of the decoder. Additional fea-tures were obtained from the personalization meth-ods described in the following section. 3.1 Short-term personalization Short-term personalization aims at modeling the re-pair/repetition behavior of the user. Short-term fea-tures are a mechanism suitable for representing neg-ative evidence: if the user repeats a utterance it nor-mally means that the hypotheses in the previous n-best lists are not correct. For this reason, if a hy-pothesis is contained in a preceding n-best list, that hypothesis should be weighted negatively during the rescoring.

A straightforward method for identifying likely repetitions consists of using a fixed size time win-dow and considering all the user queries within that window as part of the same repetition round. Once an appropriate window size has been determined, the proposed short-term features can be extracted for each hypothesis using a binary tree like the one de-picted in figure 1, where feature values are in the leaves of the tree.

Given these features, we expect  X  X een and not clicked X  to have a negative weight while  X  X een and clicked X  should have a positive weight. 3.2 Long-term personalization Long-term personalization consists of using the user history (i.e. recognition hypotheses that were con-firmed by the user in the past) to predict which recognition results are more likely. The assumption here is that recognition hypotheses in the n-best list that match or  X  X esemble X  those in the user history are more likely to be correct. The following list enumer-ates the long-term features proposed in this work:  X  User history (occurrences): number of times  X  User history (alone): 1 if the hypothesis ap- X  User history (most clicked): 1 if the hypothe- X  User history (most recent): 1 if the hypothe- X  User history (edit distance): minimum edit dis- X  User history (words in common): maximum  X  User history (plural/singular): 1 if either the  X  Global history: 1 if the hypothesis has ever  X  Global history (alone): 1 if the hypothesis is the
Note that the last two features proposed make use of the  X  X lobal history X  which comprises all the queries made by any user. 3.3 LiveSearch-based features Typically, users ask for businesses that exist, and if a business exists it probably appears in a Web docu-ment indexed by Live Search (Live Search, 2006). It is reasonable to assume that the relevance of a given business is connected to the number of times it ap-pears in the indexed Web documents, and in this sec-tion we derive such features.

For the scoring process, an application has been built that makes automated queries to Live Search, and for each hypothesis in the n-best list obtains the number of Web documents in which it appears. De-noting by x the number of Web documents in which the hypothesis (the exact sequence of words, e.g.  X  X andoor indian restaurant X ) appears, the following features are proposed:  X  Logarithm of the absolute count: log ( x ) .  X  Search results rank: sort the hypotheses in the  X  Relative relevance (I): 1 if the hypothesis was  X  Relative relevance (II): 1 if the the hypothesis 4.1 Data The data used for the experiments comprises 22473 orthographically transcribed business utterances ex-tracted from a commercially deployed large vocabu-lary directory assistance system.

For each of the transcribed utterances two n-best lists were produced, one from the commercially de-ployed system and other from an enhanced decoder with a lower sentence error rate (SER). In the exper-iments, due to their lower oracle error rate, n-bests from the enhanced decoder were used for doing the rescoring. However, these n-bests do not correspond to the listings shown in the user X  X  device screen (i.e. do not match the user interaction) so are not suit-able for identifying repetitions. For this reason, the short term features were computed by comparing a hypothesis from the enhanced decoder with the orig-inal n-best list from the immediate past. Note that all other features were computed solely with reference to the n-bests from the enhanced decoder.

A rescoring subset was made from the original dataset using only those utterances in which the n-best lists contain the correct hypothesis (in any po-sition) and have more than one hypothesis. For all other utterances, rescoring cannot have any effect. The size of the rescoring subset is 43.86% the size of the original dataset for a total of 9858 utterances. These utterances were chronologically partitioned into a training set containing two thirds and a test set with the rest. 4.2 Results The baseline system for the evaluation of the pro-posed features consist of a ME classifier trained on only one feature, the hypothesis rank. The resulting sentence error rate (SER) of this classifier is that of the best single path, and it is 24.73%. To evaluate the contribution of each of the features proposed in section 3, a different ME classifier was trained us-ing that feature in addition to the baseline feature. Finally, another ME classifier was trained on all the features together.

Table 1 summarizes the Sentence Error Rate (SER) for each of the proposed features in isolation and all together respect to the baseline.  X  X H X  stands for user history.
 Features SER
Hypothesis rank (baseline) 24.73% base + repet. ( seen ) 24.48% base + repet. ( seen &amp; clicked ) 24.32% base + repet. ( seen &amp; clicked ) 24.73% base + UH (occurrences) 23.76% base + UH (alone) 23.79% base + UH (most clicked) 23.73% base + UH (most recent) 23.88% base + UH (edit distance) 23.76% base + UH (words in common) 24.60% base + UH (plural/singular) 24.76% base + GH 24.63% base + GH (alone) 24.66% base + Live Search (absolute count) 24.35% base + Live Search (rank) 24.85% base + Live Search (relative I) 23.51% base + Live Search (relative II) 23.69% base + all 21.54% The proposed features reduce the SER of the base-line system by 3.19% absolute on the rescoring set, and by 1.40% absolute on the whole set of tran-scribed utterances.

Repetition based features are moderately useful; by incorporating them into the rescoring it is possi-ble to reduce the SER from 24.73% to 24.32%. Al-though repetitions cover a large percentage of the data, it is believed that inconsistencies in the user interaction (the right listing is displayed but not con-firmed by the user) prevented further improvement.
As expected, long-term personalization based fea-tures contribute to improve the classification accu-racy. The UH (occurrences) feature by itself is able to reduce the SER in about a 1%.

Live Search has shown a very good potential for feature extraction. In this respect it is interesting to note that a right design of the features seems critical to take full advantage of it. The relative number of counts of one hypothesis respect to other hypotheses in the n-best list is more informative than an absolute or ranked count. A simple feature using this kind of information, like Live Search (relative I), can reduce the SER in more than 1% respect to the baseline.
Finally, it has been shown that personalization based features can complement each other very well.
