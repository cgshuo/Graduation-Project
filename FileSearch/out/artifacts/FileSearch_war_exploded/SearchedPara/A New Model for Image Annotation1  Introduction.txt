 The phrase  X  X  picture is worth a thousand words. X  clearly depicts the difficulty in image annotation. The objective of this work is to build a system that can automatically provide an annotation that describes the input image.Table 2 shows example of annotated images used in this work.

Common approach to solve the image annotation problem (e.g. [1,2,3,4,5]) consists in partitioning the whole image into regions. From this segmentation, a set of local descriptors is extracted from each region to describe its visual and texture characteris tics. From a pool of local features extracted from all images, a clustering algorithm, usually K-means algorithm, is applied to select the set of features that will be used to represent the image. Indeed, each image will be represented by the set of these selected features or visterms in an analogous manner to the bag of words representation used in textual information retrieval.
Next, a learning technique is used to cap ture the relation between these blobs or visterms and the annotation label. Indeed, the annotation model computes the probability of a given label being present in the annotation, or label posterior for short, of an input image. In [1], the conditional probability of label given blob is trained using EM algorithm from all annotated images. We consider this as a global approach . When limited number of images is annotated the resulting model cannot efficiently capture all informatio n. Moreover, not every object or region that appears in the image is annotated and on the other hand several labels may be used to annotate the same image. Annotated objects may also appear on complex background. The probability estimated on this type of corpus is therefore noisy. As consequent, the annotation model should not rely on the global information solely.

The Cross-Media Relevance Model (deno ted as CMRM hereafter) [2] and sim-ilar models [2,3,6], apply a smoothing technique to combines the global probabil-ity function with the probability function computed locally on each annotated image. Integrating this local information allows better estimation of the label posterior leading to a mor e accurate annotation.

This work also relies on the similar idea, but pushing a bit further. Indeed, we expand the computation of local information from each image, to include its neighborhoods images as well. In fact, we compute the label posterior of an image as a function of three quantities namely; the label posterior on other images in the dataset, the similarity between other images and the input image, and the probability of label being assigned to this image if we know that it has also been assigned (or not) to other images. This framework can take advantage of un-annotated images in the training corpus as described in the next section.
Besides, it is easier to collect new images rather than annotating them. There-fore, it is interesting to see how to exploit these un-annotated images in the annotation model. This idea is shared by several semi-supervised learning algo-rithms. Another objective of this work is then to integrate these un-annotated images into the annotation model and to investigate how this additional data can improve the performance of the system.

In the following, Section 2 presents our image anno tation system. Section 3 and 4 presents the experimental resul ts and the conclusion respectively. Given an un-annotated image, we want to automatically select a set of annotation labels from a set of known labels that best describe this image. An annotation model is constructed for each label independently. This allows integrating new label easily. We follow the common approach describe in previous section by first segment the image into regions, extract local descriptors and use bag-of-visterms representation. Indeed, each image is represented by a normalized histogram of visterms. This histogram along with the histogram intersection are used as a building block to compute different probabilities involved in our system. 2.1 Image Annotation with Global Information In this subsection, we present a simple global image annotation model. The cal-culation used for this model is also used later in our proposed annotation model described in next subsection. For each label L , two histograms are computed from the set of annotated images namely L yes from images having this label in the annotation and L no from images without this label in the annotation. As these two histograms are computed from all annotated images, we consider it as global information.
For each image I i ,i =1 , ..., n in the training set, let L i denote the presence of label L in the annotation of this image. For an image I i , the posterior probability of having the label for I i using this global information or P G ( L i = yes/I i ) can be computed by where  X  denotes the histogram intersection operation. 2.2 Image Annotation Model Using Local Information For each image I i ,i =1 , ..., n in the training set, let L i denote the presence of label L in the annotation of this image. For an image I i , the posterior probability of having the label for I i or P ( L i = yes/I i ) can be computed by We further assume that the join probability P ( L i = yes, I j ,L j /I i ) can be fac-torized into To simplify the notation, let The equation 3 may be rewritten as
The last equation is used to update the label posterior for un-annotated images iteratively. For an annotated image I i , if the given label has been assigned to I i then f i is set to 1 and 0 otherwise. For an un-annotated images I i the value f i after convergence will be used to select the set of labels for I i .
The transition probability ( a ji in the equation 6) can be computed using the simple histogram intersection operation, i.e.
To compute the local posterior probability ( b ji in equations 7, and 8), we the following way; We know that I j has label L and we want to compute the probability of I i having label L too. If we consider the distribution of visterms for the class of images having the label, then around I j this distribution should be more similar to the distribution that represents I j rather than L yes . Therefore, we propose computing P ( L i = yes/I j ,L j = yes, I i ) in an analogous manner to P G ( L i = yes/I i ) (equation 2), but with a distribution that is biased toward I j . To this end, we define where  X  is a trade-off coefficient between local and global information, and b ( yes ) , b ji ( no ) are computed as follow: 3.1 Experimental Setup To construct the dictionary of visterms, each image is first partitioned into 10x10 pixels regions. The mean and standard deviation of three color channels R, G, and B were computed from each region. Four Gabor filters were applied to the image, the mean and standard deviation of these response were also computed. In total 14 features were used in this work. In the quantization step, K-means algorithm was used with K=1000. The trade-off parameter  X  was experimentally set to 0.9. The average precision (AP) which is as the average value of maximum precision of this system at different recall rates is computed for each label. The mean of AP for every label is used to measure the performance of the annotation model.

The experiments were performed on a set of 2360 photography images 1 .Every image is manually annotated. Fifty most frequent labels were retained for the ex-periment. Table 2 shows some images and their given annotations. This dataset is randomly split into a development set of 1860 images and a test set of 500 im-ages. To investigate how un-annotated images may help improving the system X  X  accuracy, we randomly selected 20% of development set (372 images) as training data with annotation. Then we randomly add 30% and 80% more images from the development set into the training set, but without their annotation. This evaluation was repeated 10 times. 3.2 Results The median of the mean AP with the confidence interval from the CMRM model [2], the simple annotation model with only global information and the proposed model are shown in Figure 1. From this figure, we can see that the CMRM model outperforms the simple global model. We believe this is due to the use of local information in CMRM. The proposed model outperforms both the CMRM model and the simple global model. Moreover, the performance of the proposed model increases as the size of available training data increases, even without given annotation. The Table 1 presents the results of the these models trained with all development data as training data. These results also underline the superiority of our model compared to the two others.

To annotate an image using this model, let L be the set of selected labels, its conditional probability given an input image I may be written as Using this equation, we may choose the set L of labels with maximum probability as the annotation for input image I . This strategy determines automatically the length of the annotation. Table 2 shows some example of annotation provide by the proposed model. An image annotation model is proposed. The proposed model integrates local information extracted from each image and its neighborhood within a proba-bilistic framework. This model can be used to select appropriate labels for an input image. The evaluation of this method on a more standard database like the Corel dataset will be investigated in our future work.

