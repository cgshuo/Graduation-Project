 Yichuan Tang tang@cs.toronto.edu Ruslan Salakhutdinov rsalakhu@cs.toronto.edu Geoffrey Hinton hinton@cs.toronto.edu Unsupervised learning is important for revealing struc-ture in the data and for discovering features that can be used for subsequent discriminative learning. It is also useful for creating a good prior that can be used for tasks such as image denoising and inpainting or tracking animate motion.
 A recent latent variable density model based on Markov Random Fields is the Gaussian Restricted Boltzmann Machine (GRBM) (Hinton &amp; Salakhutdi-nov, 2006). A GRBM can be viewed as a mixture of diagonal Gaussians with the number of components exponential in the number of hidden variables, but with a lot of parameter sharing between the exponen-tially many Gaussians. In (Hinton et al., 2006), it was shown that a trained RBM model can be improved by using a second RBM to create a model of the  X  X ggre-gated posterior X  (Eq. 9) of the first RBM, where the aggregated posterior is the equally weighted average of the posterior distributions over the hidden units of the first RBM for each training case. The second RBM is then used to replace the prior over the hidden units of the first RBM that is implicitly defined by the weights and biases of the first RBM. With mild assumptions on how training is performed at the higher layer, it was proven that a variational lower bound on the log-likelihood is guaranteed to improve. The second level RBM can do a better job of modeling the first RBM X  X  aggregated posterior than the first level RBM because its parameters are not also being used to model the conditional distribution of the data given the states of the units in the first hidden layer.
 A rival model for real-valued high-dimensional data is the Mixture of Factor Analyzers (MFA) (Ghahra-mani &amp; Hinton, 1996). MFAs simultaneously perform clustering and dimensionality reduction of the data by making locally linear assumptions (Verbeek, 2006). Unlike RBMs, MFAs are directed graphical models where a multivariate standard normal prior is speci-fied for the latent factors for all components. Learning typically uses the EM algorithm to maximize the data log-likelihood. Each FA in the mixture has an isotropic Gaussian prior over its factors and a Gaussian pos-terior for each training case, but when the posterior is aggregated over many training cases it will typi-cally be non-Gaussian. We can, therefore, improve a variational lower bound on the log probability of the training data by replacing the prior of each FA by a separate, second-level MFA that learns to model the aggregated posterior of that FA better than it is mod-eled by an isotropic Gaussian. Empirically, the average test log-likelihood also increases for models of both low and high-dimensional data.
 While it is true that a two layer MFA can be collapsed back into a standard one layer MFA, learning the two models is nevertheless quite different due to the shar-ing of factor loadings among the second layer compo-nents of the Deep MFA. Parameter sharing helps to reduce overfitting and greatly reduces the computa-tional cost of learning. The EM algorithm also bene-fits from an easier objective function due to the greedy layer-wise learning, so it is less likely to get stuck in poor local optima.
 Multilayer factor analysis was also part of the model in (Chen et al., 2011). However, that work mainly focused on learning convolutional features with non-parametric Bayesian priors on the parameters . By us-ing max-pooling and decimation of the first layer fac-tors, their model was designed to learn discriminative features, rather than a top-down generative model of pixel values. Factor analysis was first introduced in psychology as a latent variable model to find the  X  X nderlying factor X  behind covariates. The latent variables are called fac-tors and are of lower dimension than the covariates. Factor analyzers are linear models as the factor load-ings span a linear subspace within the vector space of the covariates. To deal with non-linear data distribu-tions, Mixtures of Factor Analyzers (MFA) (Ghahra-mani &amp; Hinton, 1996) can be used. MFAs approximate nonlinear manifolds by making local linear assump-tions.
 Let x  X  R D denote the D -dimensional data, { z  X  R d : d  X  D } denote the d -dimensional latent variable, and c  X  { 1 ,...,C } denote the component indicator vari-able of C total components. The MFA is a directed generative model, defined as follows: where I is the d  X  d identity matrix. The parameters of the c -th component include a mixing proportion  X  c , a factor loading matrix W c  X  R D  X  d , mean  X  c , and a diagonal matrix  X  c  X  R D  X  D , which represents the independent noise variances for each of the variables. By integrating out the latent variable z , a MFA model becomes a mixture of Gaussians with constrained co-variance: Inference For inference, we are interested in the posterior: The posterior over the components can be found using Bayes rule: Given component c , the posterior over the latent fac-tors is also a multivariate Gaussian: where Maximum likelihood learning of a MFA model is straightforward using the EM algorithm. During the E-step, Eqs. 7, 8 are used to compute the poste-rior over the latent variables given the current set-ting of the model parameters. During the M-step, the expected complete-data log-likelihood is maxi-mized with respect to the model parameters  X  = {  X  After MFA training reaches convergence, the model can be improved by increasing the number C of mix-ture components or the dimensionality d of the latent factors per component. This amounts to adjusting the conditional distributions p ( x | z ,c ). However, as we demonstrate in our experimental results, this approach quickly leads to overfitting, particularly when model-ing high-dimensional data.
 An alternative is to replace the standard multivari-ate normal prior on the latent factors: p ( z | c ) = N ( 0 , I ). The  X  X ggregated posterior X  is the empiri-cal average over the data of the posteriors over the specific aggregated posterior is: If each factor analyser in the mixture was a perfect model of the data assigned to it, the component-specific aggregated posterior would be distributed ac-cording to an isotropic Gaussian, but in practice, it is non-Gaussian. Figure 1 (left panel) shows a component-specific aggregated posterior (with d = 2), which is highly non-Gaussian. In this case, we wish to replace a simple standard normal prior by a more powerful MFA prior: Here,  X  (2) c emphasizes that the new MFA X  X  parameters are at the second layer and are specific to component c of the first layer MFA.
 More concretely, the variational lower bound on the log-likelihood of the model given data x is: L ( x ;  X  ) = = where H (  X  ) is the entropy of the posterior distribu-tion q and  X  represent the first layer MFA parame-ters. The DMFA formulation seeks to find a better prior log p ( z | c ) (using Eq. 10), while holding the first layer parameters fixed. Initially, when q ( z ,c | x ;  X  )  X  p ( z ,c | x ;  X  ), the bound is tight. Therefore, any increase in the bound will lead to an increase in the true like-lihood of the model. Maximizing the bound of Eq. 11 with respect to  X  (2) is equivalent to maximizing: averaged over the training data vectors. This is equiv-alent to fitting component-specific second-layer MFAs with vectors drawn from q ( z ,c | x ;  X  ) as data. The same scheme can be extended to training third-layer MFAs. With proper initialization, we are guaranteed to im-prove the lower bound on the log-likelihood, but the log-likelihood itself can fall (Hinton et al., 2006). Fig. 1 (middle panel) shows a schematic representation of our model. Using  X  (2) k mixing proportion of component k c , we have: A DMFA replaces the old MFA prior p MFA ( z ,c ) = p ( c ) p ( z | c ) with a better prior: Therefore, when sampling from a DMFA, we first sam-ple c using  X  c , followed by sampling the second layer component k c using  X  (2) k using the Gaussian of component k c , as in Eq. 4. A simpler, but completely equivalent DMFA formu-lation is to enumerate over all possible second layer components k c . We use a new component indicator variable s = 1 ,...,S to denote a specific second layer component, where S = P C c =1 K c . The mixing propor-c ( s ) and k c ( s ) denotes the first and second layer com-ponents c and k c to which s corresponds. For example c (2) = 1 and c (5) = 2. We note that the size of S is exponential in the number of DMFA layers. The generative process of this formulation is very intuitive and we shall use it throughout the remaining sections. Fig. 1 (right panel) shows the graphical model for a 2 layer DMFA. Specifically, Eq. 18 is fully deterministic as every s belongs to one R R d (2) diagonal matrices of the first and second layers respectively.
 DMFA has an equivalent shallow form, which is ob-tained by integrating out the latent factors. If we in-tegrate out the first layer factors z (1) , we obtain:
By further integrating out z (2) : p ( x | s ) = N ( x ; W (1) c  X  (2) s +  X  (1) c , (21)
From Eq. 20, we can see that a DMFA can be re-duced to a standard MFA where z (2) are the factors and s indicates the mixture component. This  X  X ol-lapsed X  MFA is regularized due to its parameter shar-ing. In particular, the means of the components s with the same first layer component c all must lie on a hyperplane spanned by W (1) c . The covariance of these components all share the same outer product factoriza-tion ( W (1) c W (1) c T ) but with different  X  X ore matrices X  ( X  s + W Assuming that the number of the second layer compo-nents are equal, i.e.  X  c : K c = K , a standard shallow MFA with S = C  X  K mixture components and d (1) factors per component would require O ( DKd (1) C ) pa-rameters. A DMFA with two layers, on the other hand, would require O ( Dd (1) C + d (1) d (2) CK ) = O (( D + d (2) K ) d (1) C ) parameters. Note that a DMFA requires a much smaller number of effective parameters than an equivalent shallow MFA, since d (2) &lt;&lt; D . As we shall see in Sec. 4.1, this sharing of parameters is critical for preventing overfitting. 3.1. Inference Exact inference in a collapsed DMFA model is of order O ( CK ) since the data likelihood must be computed for each mixture component. We can incur a lower cost by using an approximate inference, which is O ( C + K ). First, we compute the posterior p ( z (1) ,c | x ) = if we had a standard normal prior over z (1) , but it is an approximation of the exact posterior of the DMFA model. The entropy of the posterior p ( c | x ) is likely to be very low in high dimensional spaces. We therefore make a point estimate by selecting the component c with maximum posterior probability: For the second layer, we treat  X  c and z (1) as data, and compute the posterior distribution p ( z (2) ,s | z (1) ,  X  c ) in a similar fashion. 3.2. Learning A DMFA can be trained efficiently using a greedy layer-wise algorithm. The first layer MFA is trained in a standard way. We then use Eq. 23 to infer the com-ponent  X  c and the factors associated with that com-ponent for each training case { x n } . We then freeze the first layer parameters and treat the sampled first Algorithm 1 Learning DMFAs
Given data: X = { x 1 , x 2 ,..., x N } . //Layer 1 training
Train 1st layer MFA on X with C components and d factors using EM  X  MFA1. //Layer 2 training Create dataset Y c for each of the C components.
Y c  X  X  X  for i = 1 to N do end for d (2) and K c : # of 2nd layer factors and components. for c = 1 to C do end for layer factor values for every component { z (1) n } c as training data for the second layer MFAs. Algorithm 1 details this layer-wise training algorithm. After greedy learning,  X  X ackfitting X  by collapsing a DMFA and run-ning additional EM steps is also possible. However, more care is needed to prevent overfitting. We demonstrate the advantages of learning DMFAs on both low dimensional and high dimensional datasets, including face images, natural image patches, and speech acoustic data.
 Toronto Face Database (TFD): The Toronto Face Database is a collection of aligned faces from a variety of (mostly) publicly available face image databases (Susskind, 2011). From the original reso-lution of 108  X  108, we downsampled to resolutions of 48  X  48 or 24  X  24. We then randomly selected 30,000 images for training, 10,000 for validation, and 10,000 for testing.
 CIFAR-10 : The CIFAR-10 dataset (Krizhevsky, 2009) consists of 60,000 32  X  32  X  3 color images of 10 object classes. There are 50,000 training images and 10,000 test images. Out of 50,000 training images, 10,000 were set aside for validation.
 TIMIT Speech : TIMIT is a corpus of phonemically and lexically transcribed speech of American English speakers of different sexes and dialects 1 . The corpus contains a 462-speaker training set, a 50-speaker vali-dation set, and a 24-speaker core test set. For our pur-poses, we extracted data vectors every 10-ms from the continuous speech data. Each frame analyzes a 25-ms Hamming window using a set of filter banks based on the Fast Fourier Transform. Concatenating 11 frames, we obtain 1353 dimensional input vectors. We ran-domly selected 30,000 vectors for training, 10,000 for validation, and 10,000 for testing.
 Berkeley Natural Images : The Berkeley segmenta-tion database (Martin et al., 2001) contain 300 images from natural scenes. We randomly extracted 2 million 8  X  8 image patches for training, 50,000 patches for validation, and 50,000 for testing.
 UCI : We used 4 datasets from the UCI reposi-tory (Murphy &amp; Aha, 1995). These are low dimen-sional datasets and have relatively few training exam-ples. These were the only UCI datasets we tried. For all image datasets, the DC component of each im-age was removed: x  X  x  X  mean ( x ). This removes the huge illumination variations across data samples. No other preprocessing steps were used. For the TIMIT and UCI datasets, we normalize input vectors to zero mean and scale the entire input by a single number to make the average standard deviation be one. For evaluating the log probabilities of DMFAs, we always first collapsed it to a shallow MFA in order to obtain the exact data log-likelihood. 4.1. Overfitting We first trained a 20 component MFA on 24  X  24 faces until convergence 2 , which took 33 iterations. The number of factors was set to half of the input di-mensionality, d (1) = D/ 2 = 288. Fig. 2 shows the corresponding training and validation log-likelihoods 3 . We next stacked a second MFA layer with five second layer components ( K c = 5) for each of the first layer components and d (2) = 50 second layer factors. The DMFA (MFA2) model improved as learning continued for an additional 20 iterations (see red and blue lines in Fig. 2). As a comparison, immediately after we ini-tially formed the two-layer MFA, we collapsed it into its equivalent shallow representation and performed additional training (magenta and black lines in Fig. 2). Observe that the shallow MFA starts overfitting due to its extra capacity (5 times more parameters). MFA2, on the other hand, shows improvements on both the training and validation data. We note that training a shallow MFA with 100 components from random ini-tialization is significantly worse (see Table. 1). To give a sense of the computation costs, training the first layer MFA took 1600 seconds on a multi-core Xeon machine. The second layer MFA training took an ad-ditional 580 seconds. 4.2. Qualitative Results We next demonstrate qualitative improvements of the samples from a DMFA over a standard MFA model. As the baseline, we first trained a MFA model on 30,000 24  X  24 face images from the TFD, with 288 fac-tors and 100 components. We then trained a DMFA with 20 first layer components and 5 second layer com-ponents for each of the 20 first layer components. The DMFA has the same number of parameters as the base-line MFA. The two-layer MFA (MFA2) performs better compared to the standard MFA by around 20 nats on the test set. Fig 3 further shows samples from the two models. Qualitatively, the DMFA appears to generate better samples compared to the shallow MFA model. 4.3. High Dimensional Data Next, we explore the benefits of DMFAs on the high dimensional CIFAR and TIMIT datasets. We first trained a MFA model with the number of factors equal to half of the input dimensionality. The number of mixture components was set to 20. For MFA2, 5 com-ponents with 50 latent factors were used. For the 3rd layer MFA (MFA3) 3 factors with 30 latent factors were used.
 Table 1 shows the average training and test log-likelihood. In addition, we provide results for two types of RBM models that are commonly used when modeling high-dimensional real-valued data, including image patches and speech. The SSU model is a type of RBM with Gaussian visible and stepped sigmoid hid-den units (Nair &amp; Hinton, 2010). By using rectified linear activations, SSU can be viewed as a mixture of linear models with the number of components expo-nential in the number of hidden variables. A simpler Gaussian RBM (GRBM) model uses Gaussian visible and binary hidden units. It can also be viewed as a mixture of diagonal Gaussians with exponential num-ber of components. For both the GRBM and SSU, we used Fast Persistent Contrastive Divergence (Tiele-man &amp; Hinton, 2009) for learning and AIS (Salakhut-dinov &amp; Murray, 2008) to estimate their log-partition functions. The AIS estimators have standard errors of around 5 nats, which are too small to affect the conclusions we can draw from Table 1.
 The number of parameters for the GRBMs and SSU are matched to the MFA model, which means that approximately 6,000 hidden nodes are used. Increas-ing the number of hidden units did not result in any significant improvements of GRBM and SSU models. Hyperparameters are selected using the validation set. After MFA learning converged, a MFA2 model is ini-tialized. The means of the MFA-2 components were slightly perturbed from zero so as to break symme-try. Shallow1 results were obtained by collapsing these newly initialized MFA2 models and further training us-ing EM with early stopping. Shallow2 results were ob-tained by starting at random initialization (with mul-tiple restarts) with the equivalent number of parame-ters as the corresponding Shallow1 models. We note the significant gains by DMFAs for the TIMIT and TFD-48 datasets.
 Fig. 4 displays gains of 2 and 3 layer MFA as we vary the number of the first layer mixture components. It is interesting to observe that MFA and DMFA signif-icantly outperformed various RBM models. This re-sult suggests that it may be possible to improve many of the existing deep networks for modeling real-valued data that use GRBMs for the first hidden layer, though better density models do not necessarily learn features that are better for discrimination. 4.4. Low Dimensional Data DMFAs can also be used with low dimensional data. Following (Silva et al., 2011), we used 4 continuous datasets from the UCI repository. We removed the discrete variables from all datasets. For the Parkin-sons dataset, one variable from any pair whose Pear-son correlation coefficient is greater than 0.98 was also removed (for details see (Silva et al., 2011)). Table 2 reports the averaged test results using 10-fold cross validation. Compared to the recently introduced Cop-ula Networks, MFAs give much better test predictive performance. Adding a second layer produced signifi-cant gains in model performance. The improvements from adding a second layer on all datasets were statis-tically significant using the paired t-test at p = 0 . 01. 4.5. Natural Images One important application of generative models is in the task of image restoration which can be formulated as a MAP estimation problem. As confirmed by Zoran &amp; Weiss (2011), a better prior almost certainly leads to a better signal to noise ratio of the restored image. In addition, Zoran &amp; Weiss (2011) have shown that com-bining a mixture of Gaussians model trained on 8  X  8 patches of natural images with a patch-based denois-ing algorithm, allowed them to achieve state-of-the-art results. Following their work, we trained a two-layer MFA on 8  X  8 patches from the Berkeley database. Two million training and 50,000 test patches were extracted from the 200 training and 100 test images, respectively. Table 3 shows results. Note that the DMFA improves upon the current state-of-the-art GMMs model of Zo-ran &amp; Weiss (2011) by about 2 nats, while substan-tially outperforming other commonly used models in-cluding PCA and ICA. Finally, we trained a shallow equivalent to MFA-2 (5 times more parameters than MFA) from random initialization and achieved only 164.9 nats, thereby demonstrating that DMFAs are necessary in order to achieve the extra gain.
 4.6. Allocating more components to more Until now, we have given every higher level MFA the same number of components to model the ag-gregated posterior of its lower level factor analyser (  X  c : K c = K ). While simple to implement, this is not optimal. An alternative is to use more second layer components for the first layer components with bigger mixing proportions. We tested this hypothesis by first training a MFA model on 48  X  48 TFD faces, which achieved an average test log-likelihood of 5159 nats. For the two-layer MFA, instead of assigning 5 com-ponents to each of the first layer components, we let K c  X   X  c , with min ( K c ) = 2 and P With all other learning hyper-parameters held con-stant, the resulting DMFA achieved 5246 nats on the test set. Compared to 5242 nats of our previous model (c.f. Table 1), the new method accounted for a gain of 4 nats. As another alternative, a measure of Gaus-sianity of the aggregated posterior could be used to determine K c . As density models, MFAs significantly outperform undirected RBM models for real-valued data and by using second layer MFAs to model the aggregated pos-terior of each first layer factor analyser, we can achieve substantial gains in performance. Higher input dimen-sionality leads to bigger gains from learning DMFAs. However, adding a third MFA layer appears to be of little value. Another possible extension of our work is to train a mixture of linear dynamical systems and then to train a higher-level mixture of linear dynami-cal systems to model the aggregated posterior of each component of the first level mixture.
 We thank Iain Murray for discussions and Jakob Ver-beek for sharing his MFA code. This research was supported by NSERC &amp; CIFAR.

