 Kernel-based learning has been largely applied to semantic textual inference tasks. In particular, Tree Kernels (TKs) are crucial in the modeling of syntactic similarity between linguistic instances in Question Answering or Information Extraction tasks. At the same time, lexical semantic information has been studied through the adoption of the so-called Distributional Semantics (DS) paradigm, where lexical vectors are acquired automatically from large cor-pora. Notice how methods to account for compositional linguis-tic structures (e.g. grammatically typed bi-grams or complex verb or noun phrases) have been proposed recently by defining alge-bras on lexical vectors. The result is an extended paradigm called Distributional Compositional Semantics (DCS). Although lexical extensions have been already proposed to generalize TKs towards semantic phenomena (e.g. the predicate argument structures as for role labeling), currently studied TKs do not account for composi-tionality, in general. In this paper, a novel kernel called Composi-tionally Smoothed Partial Tree Kernel is proposed to integrate DCS operators into the tree kernel evaluation, by acting both over lexical leaves and non-terminal, i.e. complex compositional, nodes. The empirical results obtained on a Question Classification and Para-phrase Identification tasks show that state-of-the-art performances can be achieved, without resorting to manual feature engineering, thus suggesting that a large set of Web and text mining tasks can be handled successfully by the kernel proposed here.
Convolution Kernels [16] are well-known similarity functions among complex syntactic and semantic structures, largely used to solve complex NLP tasks such Question Answering [27], Semantic Textual Similarity [7] or Opinion Mining [18]. In particular Tree Kernels (TKs) introduced in [5] are used for their ability in captur-ing text syntactic information, directly from syntactic parse trees. They are a valid approach to avoid the difficulty of manually de-signing effective features from linguistic structures, by defining a similarity between data points in terms of all possible substructures in an implicit vector space.

In order to exploit lexical generalization automatically derived from the analysis of large scale document collections, i.e. Distribu-tional Semantic Models [21, 30, 29], a recent formulation of these kernel functions, called Smoothed Partial Tree Kernel (SPTK), has been introduced in [9]. Here the semantic information related to the lexical nodes of a parse tree is exploited for smoothing: lexical similarity is evaluated through a distributional semantic metric over vectors counterparts and than applied in the recursive tree kernel matching function. The main limitations of this approach are that a) lexical semantic information only relies on the vector metrics applied to the leaves in a context free fashion and b) the semantic compositions between words is neglected in the kernel computa-tion, that only depends on their grammatical labels.

In our perspective, semantic information is expressed by all nodes of the parse tree where specific compositions between the head and the modifier are expressed. For example in the sentence,  X  What instrument does Hendrix play?  X , the role of the word instrument is fully captured if its composition with the verb play is taken into account. Notice that in the parse tree depicted in Fig. 1 this cor-responds to model the contribution of the lexical node for instru-ments (i.e. instrument:: n ) as a function of the corresponding di-rect object relation ( dobj ) between play and instrument : we can say that instrument contributes to the sentence semantics by filling compositionally the slot dobj of its head play . In other words it seems that lexical semantic information should propagate over the entire parse tree, by making the compositionality phenomena of the sentence explicit. In recent years, Distributional Compositional Semantic (DCS) metrics have been proposed in order to combine lexical vector representations by algebraic operators defined in the corresponding distributional space [24, 11, 37, 3, 15, 14, 4, 1].
The purpose of this paper is to define a valid semantic kernel as a measure of similarity between lexically marked syntactic struc-tures in order to support complex textual inference tasks, such as Question Classification. The idea is to i) use the SPTK formula-tion in order to exploit the lexical information encoded in a tree, ii) define a procedure to encode head-modifier information in the parse tree nodes in order to support lexical similarity function be-tween subtrees, iii) apply compositional distributional metrics to enforce a context-dependent estimation of the similarity of individ-ual head-modifier information at the nodes. The resulting model has been called Compositionally Smoothed Partial Tree Kernel (CSPTK).

In Section 2, a summary of approaches for DCS and TKs is pre-sented. In Section 3, a lexical mark-up method for parse trees is described to support a richer semantic TK formulation. This lat-ter, is presented in Section 4, where the CSPTK similarity function is defined. Finally, in Section 5, the empirical evaluations of the CSPTK over the Question Classification and Paraphrase Identifica-tion tasks are reported.
Vector-based models typically represent isolated words [35] but mostly ignoring grammatical structures. A noticeable exception is [28]. They have thus a limited capability to model composi-tional operations over phrases and sentences. In order to over-come these limitations, Distributional Compositional Semantics (DCS) models have been investigated, to support synonymy and similarity judgments on phrases, rather than only on single words.
In [31] compositionality of two vector ~u and ~v is accounted by the tensor product ~u  X  ~v , while in [13] lexical vectors are summed, keeping the resulting vector with the same dimension of the latter. In [24] two general classes of compositional models have been de-fined: a linear additive model ~p = A ~u + B ~v and a multiplicative model ~p = C ~u~v . A and B are weight matrices and C is a weight tensor that project lexical vectors ~u and ~v onto the space of ~p , i.e. the vector resulting from the composition. This perspective clearly leads to a variety of efficient yet shallow models of compositional semantics compared in [24]. Two simplified models are derived from these general forms, thus the additive model became: that is the addition of the two lexical vectors to which apply the weights  X  and  X  1 and the multiplicative model became: that is the pointwise multiplication and the symbol represents multiplication of the corresponding components, i.e. p i = u Since the cosine similarity function is insensitive to the vector mag-nitude, in [24] a more complex asymmetric type of function called dilation is discussed. It consists in multiplying vectors ~v by the quadratic factor ~u  X  ~u and ~v by a stretching coefficient  X  as follows: Notice that either ~u can be used to dilate ~v , or ~v can be used to dilate ~u . The best dilation factor  X  is studied and tuned in [24]. The com-positional similarity judgment between two phrases p 1 = ( u,v ) and p 2 = ( u 0 ,v 0 ) with respect to Equation 1 is thus expressed as: where cosine similarity (  X  ) between the sum of the lexical pair vec-tors is computed. The same applies to Equation 2 and 3.

These models assume that composition is a symmetric function of the constituents. While this might be reasonable for certain structures, such as lists, a model of composition based on syntactic structure requires some way of differentiating the contributions of each constituent. In [11], the concept of a structured vector space is introduced, where each word is associated to a set of vectors corresponding to different syntactic dependencies. The noun com-ponent of a composition between verb and noun is here given by an average of verbs that the noun is typically object of. In [15] a re-gressor is trained for adjective-noun (AN) compositionality: pairs of adjective-noun vector concatenations are used as input in train-ing data, whilst corpus-derived AN vectors as output.
Notice that the simplifier additive model with the scalar weights set to 1 or to normalizing constants was a common approach to composition in the earlier literature [13, 20, 21]
A specific linear model of semantic composition is proposed in [1]. It is based on the idea that compositional similarity corre-sponds to cosine similarity computed over vectors projected in a subspace: the selected subspace dimensions are those that maxi-mize the overlapping features between the word pairs contributing to simple grammatical structures, i.e. head-modifier bi-grams. The subspace depends on a lexical pair ( u,v ) and tend to select the senses involved by their composition. In this way different senses, such as hard disk vs. intervertebral disk , are mapped into different spaces, where similarity with other noun phrases, e.g. mass storage or spinal column behaves differently. The resulting compositional operation is called the Support Subspace [1] and it will be hereafter summarized.

Given the source vectors of a compound ( u,v ) , space projection depends on both the two involved lexical items and selects only their  X  X ommon X  features: these concurrently constraint the suitable lexical interpretation local to the phrase. The core dimensions of a compound could be identified by the set where l = | C ( ~u,~v ) | components sharing the same sign have been selected. A k -sized Support Subspace is a k -dimensional space, with k  X  l , where the selected indexes are a subset of C , thus
A projection in a k -dimensional Support Subspace is obtained by the following diagonal matrix M
Given two phrases p 1 and p 2 , semantic similarity can be com-puted by first projecting the two pairs into the suitable Support Subspace and then applying the traditional cosine metrics. Differ-ent projections are discussed in [1] aimed at identifying suitable se-mantic aspects of the underlying head/modifier relationships such as in hard disk . The compositional similarity judgment between two phrases p 1 = ( u,v ) and p 2 = ( u 0 ,v 0 ) over the support sub-space of p i is thus expressed as: where first cosine similarity (  X  ) between the vectors projected in the selected support subspaces is computed and then a composition function  X  , such as the sum or the product, is applied. Notice how projection M i depends on the involved pair p i as the Support Sub-space derived from just one pair p i is used. It is imposed to the other pair with a corresponding asymmetric behavior of the  X  met-rics, denoted by  X  i . Alternatively, a symmetric measure  X  can be derived by projecting in both Support Subspaces, as derived for the two pairs p 1 and p 2 , and then combining them, as: where  X  1 , as well as  X  2 , projects both p 1 and p 2 into the Support Subspace of p 1 (and p 2 , respectively), and  X  i are then combined via the operator (e.g. sum). In [1] experiments over different variants of Eq. 7 and 8, i.e. different choices for projections M and compositions  X  and , are discussed. Best results are obtained over the dataset introduced in [25] when an additive operator  X  is used in Eq. 7.
Recently, Composition Semantics has been exploited in syntactic parsing, as shown in [32] where Compositional Vector Grammars (CVGs) are defined. These extend small-state PCFG based mod-els of the syntax and introduce distributional semantic constraints in statistical modeling. Interestingly, in CVGs, nodes in a parse tree are given a score of how plausible the corresponding syntactic constituent is, according to a Recursive Neural Network. Higher scores thus correspond to more plausible syntactic interpretations of grammatical constructs. Lexical vectors are used to choose se-mantically more plausible constituents and discard wrong ones (as in PP attachments).

Notice that a similar integrated contribution of lexical informa-tion (i.e. word vectors) and syntactic constituency is proposed in semantic extensions of Tree Kernels. As an example, the SPTK kernel function introduced in [9] is effectively used to train super-vised SRL systems. In [9] a framework to define similarity metrics strongly tied to the syntactic structure of entire sentences is defined: it will be shortly summarized hereafter.
Kernels are representationally efficient ways to encode similar-ity metrics in support of supervised learning algorithms. Tree Ker-nels (TK) as they have been early introduced by [5] correspond to Convolution Kernel [16] over the syntactic parse trees of sen-tence pairs. A TK computes the number of substructures (or their partial fragments) shared by two parse trees T 1 and T 2 . For this purpose, let the set F = { f 1 ,f 2 ,...,f |F| } be a space of tree frag-ments and  X  i ( n ) be an indicator function: it is 1 if the target f is rooted at node n and 0 otherwise. A tree-kernel function is a function TK ( T 1 ,T 2 ) = P n N 1 and N T 2 are the sets of the T 1  X  X  and T 2  X  X  nodes, respec-tively and  X ( n 1 ,n 2 ) = P |F| i =1  X  i ( n 1 )  X  i ( n cursively compute the amount of similarity due to the similarity among substructures. The type of considered fragments determines the expressiveness of the kernel space and different tree kernels are characterized by different choices. In early models, e.g. [5], lexical information has been neglected in recursive matching, so that only exact matching between node labels was given a non null weight: even when leaves are involved they had to be equal, so that no lexical generalization was early considered. A modeling of lexical information is proposed by [9], in the so called Smoothed Partial Tree Kernel (SPTK). In SPTK, the TK extends the similar-ity between tree structures allowing a smoothed function of node similarity. The aim of SPTK is to measure the similarity between syntactic tree structures, which are semantically related, i.e. par-tially similar, even when nodes, e.g. words at the leaves, differ. This is achieved by the following formulation of the function  X  over nodes n i  X  T i :
In Eq. 9 , ~ I 1 j represents the sequence of subtrees, dominated by node n 1 , that are shared with the children of n 2 all other non-matching substructures are neglected. Parameter  X  accounts for the decay factor penalizing embedded trees, whose contribution affects too many dominating structures towards the root. Moreover,  X  ( n 1 ,n 2 ) is a similarity function between nodes: for non terminals it can be strict (i.e. exact matching is imposed), while usually cosine measures between lexical vectors is adopted between leaves. More details about SPTK as well as its efficient computation are discussed in [9].

A further combination between lexical information and the struc-tural properties of sentences is introduced in the so-called Vector Tree Kernels (VTK), as recently introduced in [34]. In this work authors propose a walk-based graph kernel that generalizes the no-tion of tree kernel as a general framework for word-similarity, and in particular, incorporating distributed representations in a flexible way.

It has been observed that one main limitation of SPTK is the em-ployed lexical similarity, as it does not consider compositional in-teractions between words (i.e. lexical leaves of constituency trees). Given the phrase pairs ( np ( nn river )( nn bank )) and ( np ( nn sav-ings )( nn bank )), the SPTK would estimate the similarity by relying on a unique representation for bank , that gives wrong results as the compositional role of modifiers, river vs. savings , involve two different senses. Hereafter, a DCS operator will be adopted in or-der to model head/modifier specific similarity metrics to be used for smoothing the similarity at any non terminal node of the parse tree(s). Notice that this is the role of the function  X  in Eq. 9.
Compositional semantic constraints over a tree kernel computa-tion can be applied only when individual syntagms corresponding to nodes are made explicit. The Compositionally Smoothed Par-tial Tree Kernel, i.e. CSPTK, takes as input two trees where nodes express a lexical composition: this is used in the recursive composi-tional matching foreseen by the underlying convolution model, i.e. the same as in SPTK. Given the question  X  What instrument does Hendrix play?  X  and its dependency structure, the relative syntactic structure is shown in Figure 1 in terms of a Lexical Centered Tree (LCT) as proposed in [9].

Nodes in LCTs can be partitioned into:
An additional tree can be derived by emphasizing the impor-tance of syntactic information in the so-called Grammatical Rela-tion Centered Tree, i.e. GRCT in figure 3, where nodes are parti-tioned into:
General POS tags are obtained from the PennTreebank standard by truncating at their first char (as in instrument::n ). Figure 1: Lexical centered tree of the sentence  X  What instru-ment does Hendrix play?  X 
We aim at introducing lexical compositionality in these syntactic representation, through the explicit mark-up of every compositional (sub)structure.

Notice how grammatical dependencies in a LCT representation are encoded as direct descendants of a modifier non terminal. For example, the dependency dobj is a direct descendants of the lex-ical node instrument::n and expresses its grammatical role in the relationship with its parent node play::v . We propose to mark up such lexical nodes with the full description of the corresponding head/modifier relationship, denoted by ( h,m ) . In order to empha-size the semantic contribution of this relationship, the lexical in-formation about the involved head ( l h ) and modifier ( l represented: we denote it through a 4-tuple  X  l h ::pos h Therefore, in an extended (compositional representation for LCTs) every non terminal n  X  X T is marked as where d h,m  X  X  is the dependency function between h and m and l and pos i are lexical entries, and POS tags. Moreover, one lex-ical node is also added to represent the simple lexical information of the involved modifier, in order to limit data sparseness. Notice that this mark-up resembles closely the representation of immedi-ate dominance rules for grammatical phrases in AVM expressing feature structures in HPSG-like formalisms ([6]).

Figure 2 shows a fully compositionally labeled tree for the sen-tence whose unlabeled version has been shown in Figure 1. Now nodes are partitioned so that:
In a similar fashion, the GRCT in Figure 3 can be extended in the Compositional counterpart, i.e. the CGRCT in Figure 4. Nodes here are partitioned so that: Figure 3: Grammatical Relation Centered Tree (GRCT) of the sentence  X  What instrument does Hendrix play?  X 
Every compositional node supports the application of a composi-tional distributional semantic model, where lexical entries for heads ( l h :: pos h ) and modifiers ( l m :: pos m ) correspond to unique vec-tors. In fact, a shallow compositional function, independent from any dependency relation d h,m of an head-modifier pair ( h,m ) , can be straightforwardly defined over non terminal nodes, by adopting one of the DCS models discussed in Section 2. Given two subtrees T and T 2 , rooted at nodes n 1 and n 2 and the corresponding head-modifier pairs ( h 1 ,m 1 ) and ( h 2 ,m 2 ) , we can apply the similarity metric of Equation 4 so that: for the pairs p 1 = ( h 1 ,m 1 ) and p 2 = ( h 2 ,m 2 ) . The application of the above DCS model to the SPTK kernel computation is discussed in Section 4 where the definition of the compositional CSPTK ker-nel is given.
When the compositionality of individual lexical constituents is made explicit at the nodes, a compositional measure of similarity between entire parse trees can be computed through a Tree Kernel. We define here such a similarity function as the Compositionally Smoothed Partial Tree Kernel (CSPTK), by extending the SPTK formulation. Let us consider the application of a SPTK to a tree pair represented in their CLCT form: as an example, the trees derived from sentences such as  X  Bear market runs towards the end  X  or  X  The commander runs the offense  X  can be considered. Although the ker-nel recursively estimates the similarity among all nodes through the  X  function in Equation 9, it would not be able to distinguish differ-ent senses for the verb run , as it is applied on a unique distributional vector. The aim of the CSPTK is to cast the use of vectors within the compositional relationships exposed by a tree in order to make it sensible to the underlying complex constituents: even when the syntactic structure does not change, such as in  X  run the offense  X , i.e. attacking someone, vs.  X  run a company  X , the contribution of the lexical information (e.g. the vector for the verb  X  run  X ) must be correspondingly different.

The core novelty of the CSPTK is thus a new estimation of  X  as described in Algorithm 1. Notice that for simple lexical nodes, a lexical kernel  X  LEX , such as the cosine similarity between vectors of words sharing the same POS-tag, is applied. Moreover, the other non lexical nodes contribute according to a strict matching policy: they provide full similarity only when the same POS, or depen-dency, is matched and 0 otherwise. The novel part of Algorithm 1 correspond to the treatment of compositional nodes. The similarity function  X  Comp between such nodes is computed only when they exhibit the same d h,m and the respective heads and modifiers share the POS label: then, a compositional metric is applied over the two involved ( h,m ) compounds. The metric depends on the involved compounds that can be only partially instantiated, so that different strategies must be applied:
In all other cases are discarded, e.g. heads are unknown or differ-ent POS tags are observed for the heads h i of the n 1 and n 0 similarity is returned, as formally described in the following:
Notice that similarity is a function of the dependencies and POS labels at the nodes: these must be the same on both arguments Moreover, the factor lw can be adopted to reduce the contribution of non lexical nodes: it allows to make the similarity function ap-proximate more closely a strictly lexical kernel.
The role of compositional information encoded in the tree nodes is relevant to a variety of semantic NLP tasks, crucial for Ques-tion Answering processes, Web search or other ranking tasks. In this section we will introduce two fine grain tasks, largely used in
In line with more flexible extensions for text similarity, the exact grammatical matching constraint could be relaxed: suitable com-patibility scores could be devised to introduce non zero scores for specific grammatical mismatches.
 Algorithm 1  X   X  ( n x ,n y ,lw ) Compositional estimation of the lex-ical contribution to semantic tree kernel  X   X   X  0 , /* Matc hing between simple lexical nodes */ if n x =  X  lex x :: pos  X  and n y =  X  lex y :: pos  X  then end if /* Matching between identical grammatical nodes, e.g. POS tags */ if ( n x = pos or n x = dep ) and n x = n y then end if if n x = d h,m ,  X  li x  X  and n y = d h,m ,  X  li y  X  then end if return  X   X  NLP research to measure the impact and benefits brought by sta-tistical learning methods base on semantic kernels (e.g. [9, 22, 4, 34]). In order to study the behavior of the proposed compositional kernels onto the automation of semantic inference over texts, we carried out experiments over two tasks, such as Question Classifi-cation (QC) and Paraphrase Identification, that make available two consolidated and largely adopted resources. The experimental set-ting for both tasks is hereafter discussed, while, later in this section, the comparative evaluation of the proposed compositional kernels over the two tasks, is independently detailed.
Stanford CoreNLP 4 has been used to process texts and the result-ing dependency trees are extended with compositional information as discussed in Section 3. In all tests discussed in the next sec-tions, sentences (as well as questions) are represented by the Lexi-cal Centered Tree ( LCT ) or by the Grammatical Relation Centered Tree ( GRCT ) derived from their corresponding dependency tree: from the two source grammatical formats, the compositionally en-riched counterparts are correspondingly derived, called CLCT and CGRCT , respectively, according to the process discussed in Sec-tion 3. The lexical similarity function  X  LEX exploited in SPTK and CSPTK through the Algorithm 1 was based on a distributional anal-ysis of the UkWaC corpus [2], that gives rise to a word vector repre-sentation for all words occurring more than 100 times (i.e. the tar-gets ). The vector dimensions correspond to co-occurrences within a left or right window of 3 tokens between the targets and the set of the 20,000 most frequent words (i.e. features ) in UkWaC. Scores corresponds to Pointwise Mutual Information values between each feature and a target 5 across the entire corpus. The Singular Value Decomposition is then applied to the input matrix with a space di-mensionality cut at k = 250 . Further details on the distributional lexical models adopted are discussed in [8]. The similarity  X  required by Algorithm 1 corresponds to the cosine similarity in the resulting space, as in [9].

It is worth noticing that the word vector space generation is fully automatic, so that feature vectors for words as well as the corre-sponding tree representations related to the individual training ex-amples are made available for learning without resorting to any manual feature engineering: this makes the overall kernel method largely portable across collections, tasks and natural languages.
Question Classification has been largely used for limiting the re-trieval complexity in many Question Answering processes. Ques-tion classification is the task of assigning one (or more) class la-bel(s) to a given question written in natural language Usually it consists in predicting the class of the answer, so that it is also re-ferred as answer type prediction. Question classification plays an important role in QA [36], as it has been shown that its performance significantly influences the overall performance of a QA system, e.g. [17]. Thanks to availability of our kernel similarity models, the question classification (QC) task has been modeled directly in terms of the parse trees representing questions, i.e., the classifica-tion objects.

The reference corpus is the UIUC dataset [22], including 5,452 questions for training and 500 questions for test 6 six coarse-grained classes, i.e. ABBREVIATION, ENTITY, DE-SCRIPTION, HUMAN, LOCATION and NUMBER. SVM train-ing has been carried out over the UIUC by applying (i) the PTK and SPTK kernels over the LCT and GRCT representations and (ii) the compositional tree kernels (CSPTKs), according to different compositional similarity metrics  X  LEX , to the CLCT and CGRCT formats. For learning our models, we used two extensions of the
Left contexts of targets are treated separately from the right ones so that 40,000 features are used derived for each target SVM-LightTK software 7 [26] (which includes structural kernels, i.e., STK and PTK in SVMLight [19]): i) with the smooth match between tree leaves, i.e. the SPTK defined in [9] and ii) with the smooth match between compositionally labeled nodes of the tree, as in the CSPTK. Different tree representations are denoted in the pedices, so that CSPTK CLCT refers to the use of a compositional tree kernel over the compositionally labeled lexically-centered tree (as in Fig. 2). As every compositional kernel has to make use of a compositional similarity metrics at its lexically marked nodes, the different kernels are distinct according to the adopted metrics. The following similarity functions have been applied in the QC experi-ments: The accuracy achieved by the different systems is the percentage of sentences that are correctly assigned to the proper question class, and it is reported in Table 1. We run tests over a fixed set of dif-ferent values for the C factor (i.e. the trade off between hard vs. soft margin) and results correspond to the average and standard de-viation of the different outcomes for the accuracy. As a baseline, a simple bag-of-word model (denoted by BoW) is reported (row 1) representing questions as binary word vectors and resulting in a lexical overlap kernel. A first observation is that the introduc-tion of lexical semantic information in tree kernel operators, such as in SPTK vs. PTK, is beneficial, thus confirming the outcomes of previous studies, [9, 34]. Moreover, the compositional kernels proposed ( CSPTK ) seem to make an effective use of the lexi-cal semantic smoothing as they all outperform their non composi-tional counterpart. The achieved error reduction is very large rang-ing between 12% to 42%. Among the compositional kernels the ones adopting the simple sum operator, i.e. CSPTK + CLCT to outperform all the other compositional operators, such as dila-tion or support subspaces. This remains true independently from the type of tree adopted , i.e. LCT vs. GRCT . The Lexically Centered Tree seems to better exploit compositional information, as it achieves, on average, higher accuracy than GRCT , given that it emphasizes lexical nodes that occur closer to the tree root an thus contribute mostly to the kernel recursive computation.
Although the Question Classification experiments suggest that the proposed compositional extension of previously studied tree kernels, such as PTK and SPTK , better captures the syntactic and semantic properties of individual questions, more evidence is required to assess the above outcomes. A different task has been thus selected to benchmark the contribution of the CSPTK: it is the Paraphrase Identification (PI) task, introduced in [10]. The task is binary and corresponds to recognize if given a sentence pair they are in a paraphrase relation or not. Paraphrase can be seen as a restatement (or reuse) of a text giving its meaning in another form and it is of primary importance for a variety of Information Ex-traction, Machine Translation or Information Retrieval tasks. PI is relevant to our objectives, as it is inherently tied to rich lexical and grammatical properties of sentences, exactly the kind of phenom-ena that we wish to model by our CSPTK. Moreover, it provides a large data set for training computational natural language learning systems and it has been largely used in lexical semantic research. The corresponding corpus is in fact the Microsoft Research Para-phrase Corpus (MSRPC) discussed in [10] over which composi-tional models have been previously shown effective for the PI task, e.g. [4]. The corpus consists of sentence pairs s i 1 , s ual labels indicate whether they are in a paraphrase relationship or not. The tree kernel used over the MSRPC instances corresponds to the parse tree representations LCT and GRCT obtained as in the QC task. Overall, the MSRPC dataset contains 5,801 sentence pairs. We used the standard split of 4,076 training pairs (67.5% of which are paraphrases) and 1,725 test pairs (66.5% of which are paraphrases).

Given two sentence pairs ( s i 1 ,s i 2 ) and ( s j 1 ,s j 2 nels can be defined for capturing the structural and semantic analo-gies between the training and test sentence pairs: In all our experiments only the K 1 + K 2 combination of kernels is applied, as, after short initial explorations, they were giving rise to the best results. Notice how in turn each basic kernel k , can be the composition, e.g. the sum, of simpler kernels. For example, K 1 or K 2 can be derived by adopting over sentence pair s i s k ( k = 1 , 2 ) the sum of a BoW kernel with a tree kernel, e.g. SPTK : in this case every inner kernel computation k ( s corresponds to the sum k BoW ( s i k ,s j k ) + k SPTK ( s relies on both representations for each individual example.
In Table 2, the accuracy of the different systems is reported, as the percentage of the test sentence pairs whose classification is cor-rect, i.e. identical to the MSRPC gold standard labels.

The following noticeable results can be traced over the outcomes of Table 2. First, the results obtained through the BoWK confirm that lexical information is relevant. It is also noticeable given the relatively low performances of grammatical kernels, such as PTK. The generalization of lexical information within the SPTK shows a significant improvement with respect to PTK, while the further contribution of the compositional kernel CSPTK seems less signif-icant, although it outperforms SPTK. Interestingly, the combination of the basic lexical kernel with the compositional kernel boosts the performance of any compositional kernel used in isolation, in par-ticular for the CSPTK one. Again, the introduction of composition-ality, both in the tree (i.e. in the CLCT) and in the node similarity estimation is beneficial as it outperforms all other methods. Notice how among other state-of-art systems tested over the MSRPC, as reported in the last 4 lines of Table 2, the compositional tree kernel here proposed is the best one. Among the simple learning meth-ods applied to PI, the CSPTK when combined with the BoW, is the optimal solution. Other highly accurate PI methods evaluated over the MSRPC (e.g. [33]) improve over CSPTK (76.8% vs. 75.3%). However, these seem to rely on much more complex learning archi-tectures, based on Recursive Neural Auto-encoders that are stacked over a dynamic pooling layer. The resulting training and optimiza-tion is quite specific and the training data are also demanding, such as the Gigaword corpus used for the initial lexical self-encoding.
In this paper, a novel kernel function aiming at capturing com-positional aspects in natural language processing as a side effect of structural (i.e. grammatical) and lexical properties of sentences has been proposed. It capitalizes vector space representations as distributional models of lexical semantics and extends previously proposed tree kernels. The basic idea is to preserve the recursive matching properties of tree kernels as similarity metrics among sen-tence pairs, but at the same time extend them with lexical smooth-ing. As opposed to previous lexical extensions of tree kernels (e.g. [9]) the smoothing proposed here involves every node of the tree and not only lexical nodes. Non terminal nodes, that usually ex-press complex grammatical relations between lexical heads, give also a contribution to the similarity proportional to the composi-tional similarity of the corresponding subtree. This is obtained by marking nodes of a parse tree with lexical information about head and modifiers at each node. In this way the lexical information propagates over an entire parse tree, in a compositionally labeled parse tree, and the matching of nodes (i.e. subtrees) can contribute as a function of the word vectors related to head and modifiers marked at each node. This allows the adoption of different com-positional metrics over the same tree representation. The result-ing kernel is called Compositionally Smoothed Partial Tree Ker-nel (CSPTK) and corresponds to a Convolution Kernel that mea-sures the semantic similarity between complex linguistic structures where local matchings at subtrees are sensible to the head-modifier compositional properties.

The first empirical results of the CSPTK have been obtained across largely studied datasets for two NLP tasks, such as Question Classification and Paraphrase Identification. In both tasks, exper-iments confirm the robustness and the generalization capability of the proposed compositional tree kernel, and comparative analysis with respect to simpler lexical models as well as pure grammatical kernels (i.e. the ones like PTK that make no use of lexical general-ization) systematically shows that CSPTK is the best model. The compositional tree kernel CSPTK improves on previou work SPTK by making explicit the compositional information implicit in a dependency tree. The introduction of a new (  X  ) function in Algorithm 1, for the estimation of the similarity between com-positionally marked nodes, increases the involved training cost. However, complexity remains comparable with the SPTK yet, as a caching system of the distributional compositional information is made available by the framework. Moreover, scalability can be also improved by pre-computing pair similarities. The impact of the training test size on the CSPTK efficiency has not been studied, and it is certainly an important focus of future work.

The proposed CSPTK can be designed easily over new resources (i.e. corpora and annotated datasets), natural languages (as only a parser for new language is required) as well as tasks (as the vari-ety of semantic tasks can benefit from the proposed extended tree kernel is very large). It represents a viable and effective model for supervised kernel-based classifiers, such as SVM. Moreover, the inherent semantic nature of the kernel function allows to use it also in unsupervised scenarios where sentence encoding or clustering is required. Further experiments for assessing the methodology are also foreseen against other NLP-tasks, e.g. verb classification and, especially, SRL, that is actually one of task that early inspired most of our research on SVM-based tree kernel learning. [1] P. Annesi, V. Storch, and R. Basili. Space projections as [2] M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta. The [3] M. Baroni and A. Lenci. Distributional memory: A general [4] W. Blacoe and M. Lapata. A comparison of vector-based [5] M. Collins and N. Duffy. Convolution kernels for natural [6] A. Copestake, D. Flickinger, C. Pollard, and I. Sag. Minimal [7] D. Croce, P. Annesi, V. Storch, and R. Basili. Unitor: [8] D. Croce, S. Filice, and R. Basili. Distributional models and [9] D. Croce, A. Moschitti, and R. Basili. Structured lexical [10] B. Dolan, C. Quirk, and C. Brockett. Unsupervised [11] K. Erk and S. Pado. A structured vector space model for [12] A. Finch, Y. S. Hwang, and E. Sumita. Using machine [13] P. Foltz, W. Kintsch, and T. Landauer. The measurement of [14] E. Grefenstette and M. Sadrzadeh. Experimental support for [15] E. Guevara. A regression model of adjective-noun [16] D. Haussler. Convolution kernels on discrete structures. [17] E. Hovy, L. Gerber, U. Hermjakob, C.-Y. Lin, and [18] P. Jiang, C. Zhang, H. Fu, Z. Niu, and Q. Yang. An approach [19] T. Joachims. Estimating the generalization performance of an [20] W. Kintsch. Predication. Cognitive Science , 25(2):173 X 202, [21] T. Landauer and S. Dumais. A solution to plato X   X  A [22] X. Li and D. Roth. Learning question classifiers. In [23] R. Mihalcea, C. Corley, and C. Strapparava. Corpus-based [24] J. Mitchell and M. Lapata. Vector-based models of semantic [25] J. Mitchell and M. Lapata. Composition in distributional [26] A. Moschitti. Kernel methods, syntax and semantics for [27] A. Moschitti, S. Quarteroni, R. Basili, and S. Manandhar. [28] S. Pado and M. Lapata. Dependency-based construction of [29] M. Sahlgren. The Word-Space Model . PhD thesis, Stockholm [30] H. Sch X tze. Automatic word sense discrimination. Comput. [31] P. Smolensky. Tensor product variable binding and the [32] R. Socher, J. Bauer, C. D. Manning, and N. Andrew Y. [33] R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and C. D. [34] S. Srivastava, D. Hovy, and E. H. Hovy. A walk-based [35] P. D. Turney and P. Pantel. From frequency to meaning: [36] E. M. Voorhees. Overview of the trec 2001 question [37] F. M. Zanzotto, I. Korkontzelos, F. Fallucchi, and
