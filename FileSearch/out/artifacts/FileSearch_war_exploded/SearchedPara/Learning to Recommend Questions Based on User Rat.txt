 At community question answering services, users are usuall y encouraged to rate questions by votes. The questions with the most votes are then recommended and ranked on the top when users browse questions by category. As users are not obligated to rate questions, usually only a small proportio n of questions eventually gets rating. Thus, in this paper, we are concerned with learning to recommend questions from user ratings of a limited size. To overcome the data sparsity , we propose to utilize questions without users rating as well . Further, as there exist certain noises within user ratings ( the preference of some users expressed in their ratings diverge s from that of the majority of users), we design a new al-gorithm called  X  X ajority-based perceptron algorithm X  whi ch can avoid the influence of noisy instances by emphasizing its learning over data instances from the majority users. Ex-perimental results from a large collection of real question s confirm the effectiveness of our proposals.
 I.5.2 [ Pattern Recognition ]: Design Methodology; I.7.m [ Document and Text Processing ]: Miscellaneous Algorithms, Experimentation Question Recommendation, the Majority-based Perceptron Algorithm
Community question answering (referred to as cQA) is a web service where people can post questions and have their  X 
This work was done when Ke Sun and Xinying Song were visiting students at Microsoft Research Asia.
 questions answered by others. The growth in cQA has been one of the emerging trends at the age of Web 2.0. The typical examples provided by the commercial search engines include Yahoo! Answers 1 , WikiAnswers 2 , and Baidu Zhidao 3 .
Over time, cQA services have accumulated large archives of previous questions and their answers. Given the fact, the problem then raised for cQA services is how to efficiently help users re-use the archives. Usually there are two cat-egories of solution to the problem. One is on the basis of browsing. At cQA services, users are usually encouraged to rate questions by votes. The questions with the most votes are then recommended and ranked on the top when users browse questions by category. As users are not obligated to rate questions, usually only a small proportion of question s eventually gets rating. Thus, in this paper, we are con-cerned with learning to recommend questions from user ratings of a limited size . The other category of so-lution is based on a query interface. Typically, given a quer y (keywords or questions), a cQA service searches its archive to see if the same or related question has previously been asked. If the question is found, then the corresponding pre-vious answers can be provided to response the query. The solution is called  X  X uestion search X  [5, 9, 10, 22]. For ques -tion search, it has been proved that static rank can be useful when a large number of relevant results are found for a query [21]. The scores determined by the recommendation model can be used another kind of static rank for question search.
To the best of our knowledge, this is the first effort of learning to recommend questions from user ratings (or votes). Previously, Liu et al. [18] propose studying information se ek-ers X  satisfaction within the context of cQA, which is based o n user ratings, too. However, their ratings are about answers , while our ratings are about questions.

As aforementioned, usually only a small portion of ques-tions at cQA sites is associated with users ratings, which creates a data sparsity problem. To learn a  X  X uestion rec-ommendation X  model from the sparse data, we propose to take into consideration questions without user ratings as w ell when preparing the data for the model learning. The un-derlying intuition that we use is that, if two questions x and x (2) are viewed by a user u at the same time but x (1) is rated by u and x (2) not, it very possibly means that the user u recommends x (1) more than x (2) .

Obviously, different users might have different choices or preferences (recommend or not) when they provide rating http://answers.yahoo.com http://wiki.answers.com http://zhidao.baidu.com (or vote) for a question. However, in this paper, we as-sume there exists certain commonality of the choices of dif-ferent users (our experimental results in Section 4.4 will v al-idate the assumption). The  X  X uestion recommendation X  that we define is to discover the questions which can potentially be rated or chosen as  X  X ecommendation X  by the majority of users of a cQA service. Naturally it is required that an algorithm for learning a  X  X uestion recommendation X  model should avoid or mitigate the influence of the minority of users. To match the requirement, we design a new algorithm called  X  X he majority-based perceptron algorithm X  which ca n avoid the influence of the minority by emphasizing its learn-ing over the data instances from the majority of users.
We conduct extensive experiments to evaluate our propos-als using a large collection of real questions. The experime n-tal results show that (a) there does exist certain common-ality in users X  preferences about  X  X ecommending questions X  (Section 4.4); (b) our proposed method for preparing train-ing data can benefit the learned models as it is able to cover more questions; and (c) MBPA is effective in learning the commonality of users X  preferences.

The rest of the paper is organized as follows: In Section 2, we formalize the problem of  X  X uestion recommendation X  and then introduce two definitions on  X  X reference order X  which are used in our data construction. In Section 3, we present our approach to question recommendation. In Section 4, we empirically verify the effectiveness of the proposed approa ch. Section 5 introduces the related work. Section 6 summarizes our work and discusses the future work.
In this section, we first introduce user ratings for ques-tions at cQA services which lead to our definition on ques-tion recommendation and then we introduce two kinds of preference order by which we construct the data sets used for learning/evaluating the recommendation model.
In addition to asking or answering questions, cQA users form a social network and carry out various interactions as well. One kind of interaction is that users can rate or rec-ommend questions by their votes.

The mechanism of rating questions can be found in many cQA sites. For example, the users of Yahoo! Answers are allowed to vote questions for  X  X nteresting X . Specifically, the users can  X  X tar a question if you think it is interesting and would like to share it with others X . As another example, the users of WikiAnswers can recommend a question when they think  X  X ther contributors and visitors will be interested i n a given question X . Hereafter, we will use Yahoo! Answers as the example to describe our approach although our approach can be applied to the questions from other cQA sites as well.
A user vote for rating a question can be summarized as a quadruple ( u, x , v, t ) meaning that a user u ( u  X  U , here U denotes the set of users) provides a vote v (to recommend or not) for a question x which is posted at a specific time t (  X  R + ). Note that v  X  { 1 , 0 } where 1 means that a user provides a vote to recommend question x and 0 means oth-erwise. Then the set of quadruples with  X  X ecommendation X  votes can be expressed as Q + = { ( u, x , v, t ) : v = 1 } .
Usually, users don X  X  provide their identities (users accou nts) when they browse or search question. Thus, the  X  X uestion recommendation X  that we consider assume that users identi-ties are not available. Obviously, whether or not a question is worth being recommended depends on the users who read the question. That is, different users might have different choices (to recommend or not) for a question. Thus, it is relatively hard to predict whether a question is interestin g to a specific user when the identity of the user is not avail-able. Instead, in this paper, we are interested in predict-ing how likely a question is recommended by most users , which is characterized by a measure called question popularity . The higher is the popularity of a question, the more likely the question is recommended by most users of a service.

In the remaining of this paper, we are to describe how we estimate question popularity and then use it to recommend questions.
In this sub-section, we introduce our methods for prepar-ing the training data and the evaluation data. Obviously, it is hard to give a judgment that how likely a question is to be recommended. In contrast, it is much easier to judge which is more likely to be recommended given a pair of questions. Thus, we are to define a preference relationship  X  between any two questions such that x (1)  X  x (2) if and only if the popularity of question x (1) is greater than that of x (2) preference relationship is defined on the basis of user ratin gs. In the following, we are to explain two definitions of  X  .
Definition 1. A preference order x (1)  X  1 x (2) exists if and only if where  X { X  X  X  represents the size of a set.

One intuition is: the more votes a question receives, the more popular (or likely to be recommended) it is. Thus  X  1 defined on the basis of the number of votes that a question receives. The parameter  X  v is introduced to control the margin of separation in terms of votes between x (1) and x
The preference relationships derived according to  X  1 can be very reliable when  X  v is set to a larger value (e.g., 5). Thus, we use it to build our test set (see Section 4.1).
One disadvantage with  X  1 is that we can only use it to judge the preference order between questions already voted by users. Unfortunately, the data we collected for our exper -iments, i.e. Yahoo! Answers, does not have all of its questio ns voted. For example, within the category  X  X ravel X  of Yahoo! Answers, only around 13% questions are with  X  interesting  X  votes. The data sparsity makes that the training data built on the basis of definition 1 is not sufficient when used to learn a  X  question recommendation  X  model (see section 4.3).
One method for addressing the data sparsity is just to in-clude all the questions without user ratings (or votes) into definition 1 directly, which can be done simply by replac-ing Q + with Q . The method implicitly assumes that all the questions without user ratings (or votes) are  X  X ot rec-ommended X . However, the questions without votes can be worth being recommended as well. As users are not obli-gated to rate questions in cQA services, they may not rate a question even if they feel the question interesting or use-ful. Thus, to better use the questions without votes, we introduce definition 2.
Definition 2. A preference order x (1)  X  2 u x (2) exists if and only if
At cQA sites, questions are usually sorted by posting time when they are presented to users as a list of ranked items. Newer questions are shown higher in a question list than older ones. Thus, questions with nearby posting time have a higher chance to be viewed within a single page by a user, which means that they have equal chance of exposure to users for  X  recommendation  X  labeling. Based on this assump-tion, definition 2 can be translated as: x (1) and x (2) (in most cases) are viewed by a user u within a single web page while x (1) is rated by the user u but x (2) not. Therefore, it is rel-atively safe to believe that x (1) is more  X  popular  X  (or likely to be recommended) than x (2) .

By definition 2, we try to be more cautious in saying the questions without user votes  X  X ot recommended X . Particu-larly, as elaborated above, only the questions which do not have users X  votes and share similar user browsing contexts with questions having user votes are considered  X  X ot recom-mended X .

According to definition 2, we can build  X  U  X  (denoting the size of the set U ) sets of ordered (question) instance pairs: where z i equals 1 if x (1) i  X  2 u x (2) i and -1 otherwise. l number of instance pairs given by the user u . Let S denote the union  X  S u .

We next develop a statistical method that automatically learn a common preference given personalized preference sets S 1 , S 2 ,  X  X  X  X  , S  X  U  X  .

Hereafter, for simplicity, we use  X  to denote  X  2 u if there do not exist ambiguities.
In this section, we propose an algorithm called  X  X ajority-based perceptron algorithm X  to predict question popularit y through learning the commonality of users X  preferences. Th e resulted  X  X uestion popularity X  model is then used to deter-mine the likelihood that a question is  X  X ecommended X . Thus, the model is also called  X  X uestion recommendation model X . Assume that question x comes from an input space X  X  R n , where n denotes number of features. A set of ranking functions f  X  F exists and each of them can determine the preference relation between instances: The task here is to select the best function f  X  from F that respects the given set of instances S .

First, we assume that f is a linear function. where w denotes a vector of weights and  X  X  X  ,  X  X  X  stands for inner product. Plugging (3) into (2), we obtain
Note that the relation x i  X  x j between instance pairs x and x j is expressed by a new vector x i  X  x j . Next, given Algorithm 1 PAPL 1: Input : training examples { x (1) i  X  x (2) i z i } m i =1 2: learning rate  X  R + , 3: margin parameter  X  R + 4: w 0 = 0 ; t = 0; 5: repeat 6: for i = 1 to m do 7: if z i  X  w t , x (1) i  X  x (2) i  X  X  X  then 10: t  X  t + 1; 11: end if 12: end for 13: until no updates made within the for loop 14: return w t ; a training data set S , we create a new training data set S containing l (= P u l u ) labeled vectors.
 Similarly, we can create S  X  u for each u .

Next, we use S  X  as training data and construct a classi-fication model that assigns either positive label z = +1 or negative label z =  X  1 to a vector x (1) i  X  x (2) i .
Suppose that w  X  is the weight vector learned by the clas-sification model. Then we utilize w  X  to form a scoring func-tion f w  X  for evaluating popularity of a question x . The popularity score determines the likelihood that the questi on is recommended by many users.
In this paper, we are interested in adapting the Perceptron algorithm for this learning problem. In the following sub-sections, we first explain how the Perceptron algorithm can be used for this and then introduce our modification which makes the learned function f subjected to the majority of users.
The perceptron algorithm (PA) [20] is an on-line learn-ing algorithm for linear classifiers. In this paper, we adapt one of its extensions, the perceptron algorithm with margin s (PAM) [15], to the problem of preference learning. For the details on PAM, please refer to [15]. Algorithm 1 provides the adaptation which we call the perceptron algorithm for preference learning (PAPL).

Compared to PAM, PAPL makes two changes: (1) trans-formed instances (instead of raw instances) as given in equa -tion 4 are used as input; (2) the estimation of an intercept is no longer necessary (as line 9 which is commented out already). Note that the two changes don X  X  influence the convergence of the algorithm.

Certainly, for each user u , PAPL can learn a model (de-noted by weight vector w u ) on the basis of S  X  u . However, none of them can be used for predicting question popularity as they are personalized. An alternative is to use the model (denoted by w 0 ) learned on the basis of S  X  . The insuffi-ciency of the model w 0 comes from that it cannot avoid the influence of the minority of users (which diverges from the majority in terms of preference about  X  recommendation  X ). Algorithm 2 MBPA 1: Input : training examples { x (1) i  X  x (2) i z i } m i =1 2: users X  weight vectors { w u } k u =1 , 3: learning rate  X  R + , 4: margin parameter  X  R + 5: lower bound of correlation  X  R + , 6: initial weight vector w 0 satisfying  X  w 0  X  = 1 7: t = 0; 8: repeat 9: for i = 1 to m do 10: if  X  w t , w u ( i )  X  N  X  then 13: t  X  t + 1; 14: end if 15: end if 16: end for 17: until no updates made within the for loop 18: return w t ;
In the next sub-section, we introduce our proposal which boosts w 0 by mitigating the influence of the minority.
As discussed previously, different users might provide dif-ferent preference labels for the same instance pairs. What we want to achieve is to utilize the instance pairs from the majority while treating as noises those from the minority. The problem raised here is that how we can differentiate the minority from the majority automatically when learning to predict  X  question popularity  X .

One solution is associating different instance pairs with different weights. The larger the weight of an instance pair is, the more important it is. For example, Cao et al. [3] does this for adapting Ranking SVM for information retrieval. In this paper, we assume that all instance pairs from a user u share the same weight u . Then, the problem is transformed into how to determine a weight for each user.

Every w obtained by PAPL can be treated as a directional vector. Predicting the preference order between two ques-tions x (1) i and x (2) i is achieved by projecting x (1) onto the direction denoted by w and then sort them on a line. Thus, the directional vector w u denoting a user u who agrees with the majority should be close to the directional vector w 0 denoting the majority. Furthermore, the closer the user X  X  vector is to w 0 , the more important the training data from the user should be. We use cosine similarity to measure how close two directional vectors are. Then we can determine users X  weights { u } as follows:
Based on the discussion above, we propose the  X  majority-based perceptron algorithm  X  (MBPA) which is able to em-phasize its training on the instance pairs from the majority of users. Algorithm 2 provides the details.

In MBPA, at iteration 0 ( t = 0), we use the condition of line 10 to prevent the minority from participating in the training process. Note that u ( i ) represents the user who involves in generating the preference pair x (1) i and x (2) definition 2). Furthermore, at line 11, we emphasize the training over important instance pairs according to equa-tion (5). At iteration 1, we replace w 0 with w 1 and repeat the aforementioned procedure, and then iterate. By doing that, it is expected that w t +1 represents the majority better than w t .

As MBPA is an iterative algorithm, it is necessary to dis-cuss its convergence. Theorem 1 described below guarantees this.

Assuming that the training data S  X  is linearly separable, we then define the margin of a model f w as definition 3.
Definition 3. The margin of ( w , S  X  ) of a scoring function f w is minimal real-valued output on the training set S Specifically,
Theorem 1. Let S  X  = x (1) i  X  x (2) i , z i l i =1 be a set of training examples, and let r := max  X  X  X  x (1) i  X  x (2) i  X  X  X  . Suppose there exists w opt  X  R n such that  X  X  X  w opt  X  X  X  = 1 and Then if  X  w opt , w 0  X  &gt; 0 the number of updates made by the algorithm MBPA on S  X  is bounded by
This theorem is an extension of Novikoff X  X  theorem [19] as well as of the result of Krauth and Mezard [15]. Its proof can be easily derived by following the latter.
At cQA sites, a question is usually associated with three kinds of entity: (a) an asker who posts the question, (b) answerers who provide the answers to the question, and (c) answers to the question. In this paper, we are to predict popularity for not only questions with answers but also ques-tions without answers . Thus, when modeling question popu-larity , our proposed approaches explore only two aspects of features: features about question and features about asker . Table 1 gives the list of features.
 Features about questions come only from metadata of ques-Features about asker are extracted from historical behav-
In this section, we report our experimental results about question recommendation which is demonstrated by predict-ing question popularity . Particularly, we investigate (a) the use of different learning features and (b) the effectiveness o f our proposals. Dataset. We made use of the questions crawled from Yahoo! Answers for the evaluation. More specifically, we utilized 297,919 questions under the top-level category of  X  X ravel X  at Yahoo! Answers. The questions were posted withi n nine months between Aug. 1st, 2007 and Apr. 30, 2008. Each question consists of two fields:  X  X itle X  and  X  X escripti on X  (note that  X  X nswers X  are not always available). Each ques-tion is further associated with another entity: its asker wh o posted the question. Note that in Yahoo! Answers users rate or recommend questions by the label  X  X nteresting X .
We built the training sets, the development set, and the test set in the following steps. 1. We randomly separated all the questions into two sets 2. With Set-A, we built two training sets as follows. 3. We first extracted from Set-B all the questions voted
Among the crawled questions, only around 13% questions are voted by users with  X  interesting  X  label. The fact re-sults in the sparseness of TR-1. Figure 1 further illustrate s this by showing how many questions are voted as  X  inter-esting  X  for once, twice and so on within the  X  X ravel X  cate-gory.
 Table 2 provides the statistics about the above data sets. Note that TR-2 is much larger than TR-1. Here TR-1 was obtained by setting  X  v = 5. In Section 4.3, we will investi-gate the use of TR-1 when the value of  X  v varies.
Table 3 provides three example questions which are most popular in our data sets in terms of users X  votes. Figure 1: The distribution of users X  votes for  X  inter-esting  X  in  X  X ravel X  category
Table 3: The examples on  X  X nteresting X  questions
Evaluation Measure. We made use of  X  X rror rate of preference pairs X  [8, 13] for evaluation of the following ex -periments.

In this sub-section, we are to understand the use of differ-ent features while evaluating  X  question popularity  X . The use of features are studied in two ways: (a) calculating informa -tion gain of each feature, and (b) evaluating the contributi on of each feature in terms of predicting capability. Table 4: The use of the different learning features
Table 4 gives the list of learning features ranked by infor-mation gain. The information gains were calculated on the training set TR-2. We can see that the features about asker (denoted by AS) play major roles in predicting question pop-ularity . Particulary, the history of an asker posting starred questions is the most important (the first three features). I n comparison, the WH-words features are very weak features in predicting popularity .
 We calculated the information gains on the training set TR-1, too. The ranked order of the features exhibits the similar trends as Table 4 although the orders are not exactly the same.

The information gains just reflect how well each feature works when it is used alone. To further investigate the con-tribution of each feature when used with others, we trained a series of models with PAPL. The first model includes only the first feature in Table 4. Then the second model adds the second feature, and so on. The third column of Table 4 gives the error rates of the series of models. The error rates were calculated with the dataset DEV. From the results, we can see that the error rate doesn X  X  decrease monotonically. This means that the features are not independent from each other. Again, we find that the WH-word features do not help in terms of  X  X rror rate of preference pairs X .
In this sub-section, we present the evaluation on our pro-posals from two aspects: (a) how does the training set TR-2 help boost the performance? (b) how well does our proposed method MBPA perform when compared with PAPL? In the experiments, we used all the features in Table 1. We tuned the parameters for PAPL and MBPA with the development set DEV.
 Table 5 reports the experimental results. The training set TR-1 in the table was obtained by setting  X  v to 5 which is the same as that in TST. From the table, we can see that our proposed algorithm MBPA trained with the training set TR2 outperform both the PAPL trained with TR-1 and the PAPL trained with TR-2 significantly (sign-test, p -value &lt; 0.01). This suggests that (1) our proposed data constructio n method which takes into consideration questions without user rating (or votes) incorporates more evidences than the training set given by definition 1 (This can be observed from that the PAPL trained with TR-2 performs better than the PAPL trained with TR-1); (2) the majority-based percep-tron algorithm (MBPA) is effective in filtering noisy trainin g data (We will further illustrate this point in Section 4.4).
From Table 2, we see that the size of TR-2 is much larger than the size of TR-1. One may argue that we can use a larger TR-1 by setting  X  v smaller (e.g., &lt; 5) to achieve a better performance. Table 6 evaluates this scenario. Again , the test set is TST and the model is PAPL. We observe that the size of TR-1 becomes larger but the error rate of the corresponding PAPL increases when  X  v goes smaller. When  X  v = 1, the size of TR-1 is even comparable with TR-2, but the model learned with TR-1 still performs worse than that learned with TR-2 significantly. This further con-firms the use of TR-2 built with our proposed data construction method .

According to the results in Table 5, it seems that question popularity is hard to predict, since the error rates are still a bit higher. We believe that the prediction can be easier if finer categories of questions are considered. Intuitively, users tend to converge in their preference about  X  interesting  X  when topics of questions are constrained within a sub-category. For example, it can be easy for users to find the same pref-erence when only topics of Asian area are considered. Thus, we tried to predict popularity by sub-category under the cat-egory  X  X ravel X . Table 7 gives the error rates when predict-ing  X  popularity  X  within two popular sub-categories (46,541 questions under  X  X sia Pacific X  and 23,080 under  X  X urope X ). For each prediction, the training sets, the development set , and the test set are restricted within the corresponding sub -category. By comparing the results in Table 7 with those in Table 5, we can see that question popularity can be predicted much better within constrained category of topics .
In this sub-section, we provide some insights on our ap-proach MBPA by studying the relationship between the learne d preference and users X  preferences (represented by { w u } Figure 2: # users vs. cosine similarities between the learned preferences and users X  preferences
Figure 2 provides the accumulated count of users grouped by cosine similarity of users X  preferences compared to the learned preference. It was generated as follows: (1) let  X  w denote the weight vector learned by MBPA and then cal-culate the cosine similarities  X  w 0 , w u  X  N and  X   X  w , w each user u (note that w 0 denotes the weight vector learned by PAPL); (2) for each type of similarity, count the number of users whose similarities are less than  X  0 . 9, then  X  0 . 8, ..., and 1 . 0.

From Figure 2, we can observe that most users have larger cosine similarities whenever compared to MBPA or PAPL and only a small portion of users have smaller cosine sim-ilarities. This suggests that there does exist certain com-monality in users X  preferences.

In addition to that, we can also know that users tend to have larger cosine similarities compared to  X  w than com-pared to w 0 . Note that, for  X  w , we only care about the users whose similarities are larger than 0 (Line 10 of al-gorithm 2 assures this). This confirms that the preference learned by MBPA agrees with most users more than PAPL does. This also implies that MBPA can lower the influ-ence of the noisy data from the minority users automati-cally.
In this section, we first review the latest advances within the research area of community question answering and then survey some methods on preference learning.
In this sub-section, we mainly summarize recent advances within the research area of evaluating quality of cQA ques-tions or answers. The results of the research in this area can help filter low-quality content from cQA archives. Jeon et al. [11] tried to estimate cQA answer quality. They used 13 non-textual features and trained a maximum entropy model to predict answer quality. Their results showed that retrie val relevance is significantly improved when answer quality or question utility is integrated in a log likelihood retrieva l model. Later, Agichtein et al. [1] expands on [11] by ex-ploring a larger range of features including both structura l, textual, and community features. Agichtein et al. [1] also proposes identifying question quality as well as answer qua l-ity. In addition to those above, Song et al. [21] propose a measure called  X  X uestion utility X  used to evaluate questio n quality. Question utility can be estimated by either a lan-guage model based method or a LexRank [6] based method. Our work of rating the interesting questions falls into this research area, too.

Other previous work on cQA can be be categorized into three major areas:(1) how to mine questions and answers and how to find related questions given a new question [5, 9, 10, 12, 22], (2) how to find experts given a community network [16, 14], and (3) how to predict users X  satisfaction [18, 17].
Preference learning is to learn the underlying ordering over the instances from a set of pairwise preferences. This setting is exactly what we used for learning the question recommendation model.

Many methods have been proposed for preference learn-ing. For example, Ranking SVM [8] uses support vector machines to learn a ranking function from preference data. RankNet [2] applies neural network and gradient descent to obtain a ranking function. RankBoost [7] applies the idea of boosting to construct an efficient ranking function from a set of weak ranking function. Other methods for preference learning are proposed in [13, 3, 4].

None of the above methods, however, does the job of fil-tering noisy instances as MBPA does. Our proposal lowers the importance of noisy data automatically during learning . One of our future work is to incorporate the idea of MBPA into other preference learning methods.
In this paper we investigated the problem of learning to recommend questions based on user ratings. To the best of our knowledge, this is the first attempt to address the problem.

We proposed to enlarge the size of available training data by taking into consideration questions without user rating , which in turn benefits the learned model. We also proposed adapting the perceptron algorithm with margins to the prob-lem of preference learning defined by question recommenda-tion and extended it to a new algorithm which emphasizes the training over instances representing the preference of majority users. We have shown the effectiveness of our pro-posed approaches through intensive experiments.

Our future work includes: (a) providing a theoretical guar-antee on the performance of our proposed learning algorithm MBPA; and (b) extending the idea of MBPA (to emphasize the training over instances representing the preference of majority users) to other machine learning algorithms, such as SVM and ME.
We would like to thank Wen-Yun YANG for his helpful discussions about the methods. And we would also like to thank the anonymous reviewers for their valuable comments. [1] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and [2] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [3] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. [4] W. Chu and Z. Ghahramani. Preference learning with [5] H. Duan, Y. Cao, C.-Y. Lin, and Y. Yu. Searching [6] G. Erkan and D. R. Radev. Lexrank: Graph-based [7] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An [8] R. Herbrich, T. Graepel, and K. Obermayer. Support [9] J. Jeon, W. B. Croft, and J. H. Lee. Finding [10] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar [11] J. Jeon, W. B. Croft, J. H. Lee, and S. Park. A [12] V. Jijkoun and M. de Rijke. Retrieving answers from [13] T. Joachims. Optimizing search engines using [14] P. Jurczyk and E. Agichtein. Discovering authorities [15] W. Krauth and M. Mezard. Learning algorithms with [16] X. Liu, W. B. Croft, and M. Koll. Finding experts in [17] Y. Liu and E. Agichtein. You X  X e got answers: Towards [18] Y. Liu, J. Bian, and E. Agichtein. Predicting [19] A. B. Novikoff. On convergence proofs for perceptrons. [20] F. Rosenblatt. The perceptron: A probabilistic model [21] Y.-I. Song, C.-Y. Lin, Y. Cao, and H.-C. Rim. [22] X. Xue, J. Jeon, and W. B. Croft. Retrieval models
