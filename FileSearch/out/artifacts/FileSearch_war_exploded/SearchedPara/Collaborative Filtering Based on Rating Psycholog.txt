 Recommendation system aims at finding favorite items for users. In the past decades, many recommendation algorithms have been proposed, among of which collaborative filtering (CF) attracts much more attention because of its high recommendation accu-racy and wide applicability. These CF algorithms can be grouped into two categories, i.e., memory-based CF and model-based CF [1,2]. Memory-based CF mainly includes user-based CF and item-based CF. User-based CF supposes that the test user will like the test user for the test item is predicted based on the ratings of these similar users. Item-based CF assumes that the test user will like the items which are similar to the item is predicted based on the ratings of these similar items. The key step of memory-based CF is the calculation of similarity. Usually, the similarity is calculated based on the user-item rating matrix directly. In this matrix, each row denotes a feature vector of a user, while each column denotes a feature vector of an item. The similarity be-tween two vectors depends on the elements at which both the two vectors have rating. If the data is very sparse, the number of commonly rated elements of the two vectors is very small, which results in the computed similarity incredible and hence causes low recommendation accuracy. On the other hand, with the growth of users and items, restricts its scalability. 
To solve these problems, a variety of machine learning and data mining models are proposed, which lead to the appearance of model-based CF algorithms. The basic idea of model-based CF algorithms is trying to find the potential structure of rating matrix to cope with the scalability and data sparsity problems. Model-based CF mainly in-cludes clustering methods [3,4], regression methods [5], graph model based methods, transfer learning method [7], matrix factorization methods, and neural network me-thods. Graph model based methods include Bayesian network [8,9], PLSA (Probabil-istic Latent Semantic Analysis)[10,11], LDA (Latent Dirichlet Allocation)[12], and etc. Matrix factorization methods include SVD (Singular Value Decomposition)[13-17], PCA [18] (Principal Component Analysis), and NMF (Nonnegative Matrix Factorization) [19]. Neural network methods include RBM (Restrained Boltzmann Machine) [20], and etc. Usually, model-based CF has better performance than memo-ry-based CF, but the recommendation made by memory-based CF has good interpre-tability and is easier to convince users. 
However, all these algorithms do not fully consider user X  X  rating psychology. An algorithm based on user X  X  rating psychology can mine the latent structure of rating matrix more effectively. Moreover, it may be more tolerant against data sparsity. Based on this idea, a novel CF algorithm which is more consistent with user X  X  rating psychology is proposed in this paper. The parameters in the model are learned using stochastic gradient descent method. Experiment results show that this algorithm has better performance than state-of-the-art algorithms. The rest of this paper is organized as follows. Section 2 describes the related work. In section 3, the algorithm based on user X  X  rating psychology is introduced. Some experiments are designed and analyzed in section 4, followed by the conclusions in section 5. User X  X  feedbacks can be explicit or implicit (e.g., purchase of a product, rating on an represented by a user-item rating matrix. However, more than 90% of the data is missing in this matrix, i.e., many users only rate a few items. Moreover, there are a lot based on matrix low-rank approximation is applied. Low-rank approximation method can retain the main information of the matrix, and meanwhile, it can also remove some noises from the data. SVD is one of low-rank approximation methods, and it is ues of the matrix, which limits its scalability. S.Funk [15] suggests learning the latent factor matrices directly by fitting the observed data only, which can be expressed as: set beforehand, and it is usually less than M and N .  X  X or a given item i , the elements of  X   X  measure the extent to which the item possesses those factors, positive or nega-The resulting dot product,  X   X   X   X   X  captures the interaction between user u and item i  X  rating of item i . X   X  Y. Koren [17]. The parameters  X   X  and  X   X  are estimated by mi-by cross validation. Based on this model, Y. Koren [17] added user X  X  bias and item X  X  bias, and proposed a modified model which is called RSVD. It can be described as Eq. (3). of user u and item i from overall average respectively, and they need to be estimated using training set. RSVD achieved great success in Netflix Prize competition. 
However, both SVD [15] and RSVD [17] suppose that the interaction between a means that the user X  X  opinion of the item is the summation of the satisfactions that the item brings to the user in different aspects. Actually, a user gives bad evaluation to an item because the item cannot satisfy only few of the user X  X  demands. If the user does movie, it means that he or she may like the topic, the actor, or the director of the mov-ie [21]. It means that the rating value of a user for a product does not depend on what he or she got, but mainly depends on what he or she did not get. Based on this idea, this paper proposed a novel CF model. This model differs from the works described above in the following aspects. the vector denotes the number of user X  X  different demands. Each element of  X   X  measures the quantity of his or her specific demand. The supplies of item i are de-noted by a latent vector  X   X  , and each element of  X   X  denotes the quantity of its spe-demands and item i  X  X  supplies. 2. The distance between  X   X  and  X   X  is a weighted summation of the distance in each dimension. The weight in each dimension is user-specific which reflects that the user gives different importance to his or her different demands. discount to the rating for the item. In this paper, a negative exponential function is used to denote the discount. In this section, user X  X  rating psychology is described and a CF model based on rating psychology is proposed. Then, the parameter learning method is described. 3.1 Rating Psychology In fact, RSVD takes into account of user X  X  rating psychology to some extent. Under Thus, RSVD uses rating bias b u or b v to represent this individual psychology charac-ter. As for items, it uses b i to capture the bias of item i . Meanwhile, RSVD supposes that the user X  X  opinion of the item is the summation of satisfactions the item brings to the user in each aspect. 
However, nowadays, products are increasingly abundant and diverse, which makes usually advocate the advantages of the product and never mention the shortcomings of the product, which enlarges users X  expectations for the product X  X  benefits and reduces users X  tolerances to the product X  X  defects. Thus, when a user purchased a product and found its defect, the user usually could not tolerate it even the defect was negligible. Then, he or she would return it or give a bad evaluation to it. On the other hand, if a user does not like the product at all, he or she will not purchase it. So, a user gives high evaluation for a product usually because he or she likes all aspects of the product, and a user gives bad evaluation for a product usually because the product cannot satisfy only few of the user X  X  demands. It means that the rating value of a user depends on what he or she did not get. Moreover, only when the supplies of the prod-uct are just equal to user X  X  demands, it can satisfy the user. For example, a user likes horror films, but too scary movies will make him/her uncomfortable. 
Based on this rating psychology, a novel model is designed in this paper to mine the latent structure of rating matrix to deal with data sparsity problem. 3.2 Collaborative Filtering Model Based on Rating Psychology The CF model based on rating psychology is proposed as Eq. (4). where  X   X   X 0 and h  X  X  X   X 0 . Here,  X   X  denotes the score related to user u  X  X  de-mands. This equation indicates that the rating score of user u for item i can be divided into two parts. One is related to user u  X  X  demands, and it is denoted by  X   X  . The other is not related to his demand but related to rating bias. In extreme situations, if a user v  X  X  opinion on the items is completely under the influence of other user X  X  opinion and number of user X  X  latent demands, i.e., the dimensionality of user X  X  demand, which is a super parameter and should be set beforehand. Parameter p  X  X  X  denotes the quantity of fect their meanings endowed here, because if all of them are added with an enough big positive number, they will be positive and it does not change the estimated rating. from training set. 3.3 Parameters Learning function is designed as Eq. (5). frr bb pq h  X  X   X  X  X  = X + ++ +++  X  X  X  X   X  where the first term on the right side of the formula is the sum of square error, and the other four terms on the right side are the regularization terms applied to parameters to regularization constants which are determined by cross validation. Vectors  X   X  =  X   X  and they denote the latent demands vector of user u , the latent supplies vector of item i , and the weights vector of demands of user u respectively. Then, the gradient of each parameter is calculated by Eq. (6)-(13). For presentation simplicity, notations  X   X  X  X  and  X   X  X  X  are introduced. 660 H. Zhang et al. After that, every rating  X   X  X  X  in training set Tr is used to update the parameters by sto-chastic gradient descent method [16]. The update rules are expressed by the formulas (14)-(19), which can be deduced from the formulas (6)-(13).  X  ratio parameters  X   X   X ,  X   X ,  X  and  X   X  are determined by cross validation. 3.4 Workflow of the Algorithm Finally, the pseudo codes of our algorithm are shown in Figure 1, where we update the parameters using the observed rating values. For description simplicity, this algo-rithm is called CFBRP (Collaborative Filtering based on Rating Psychology). In the two experiments described in section 4, the initial values of the estimated parameters and q ik are generated according to two st eps as follows: Step 1, sample  X   X   X  X  X  random-ly from uniform distribution on [0, 1]; Step 2, normalize them, i.e., set  X   X  X  X  = be estimated by Eq. (4). In this section, two experiments are designed to evaluate the algorithm CFBRP. One is to compare it with RSVD on MovieLens 1 100k data set. RSVD is a well-known algorithm which has high prediction accuracy on the benchmark data sets. The other experiment is designed to compare CFBRP with other state-of-the-art algorithms reported in [22] to test its performance against data sparsity. 662 H. Zhang et al. 4.1 Experiment Designed To compare CFBRP with the algorithm RSVD, MovieLens 100k data set is used to evaluate the algorithms. MovieLens 100k data set has 100000 rating scores rated by 943 users on 1682 items (1-5 scales), where each user has more than 20 ratings. We randomly split MovieLens 100k data set into 5 parts which are used for 5-fold cross validation. This revised data set is called data set 1. 
We use MovieLens 10M data set to compare CFBRP with other state-of-the-art algo-validation. For each training set, 80%, 50%, 30%, 20% and 10% of the data are randomly selected to represent 5-level data sparsity degree, and they constitute 5 observed data sets respectively. Then, these observed data sets are used to train the algorithm respectively, level and on each test data set, run the algorithm CFBRP 5 times independently, and we report the average values in this paper. This revised data set is called data set 2. 
In order to compare the prediction quality of our method with other methods, we use MAE (mean absolute error) as evaluation metric. The MAE is calculated by aver-aging the absolute deviation between predicted values and true values: where | Te | is the number of tested ratings in the test set Te . The lower the MAE is, the better the performance is. 4.2 Comparison with Algorithm RSVD To compare algorithm CFBRP with algorithm RSVD, both of them are evaluated by a interval of 10) respectively. For CFBRP, the dimensionality means the number of which means that at each dimensionality each algorithm is run 25 times, and we re-port the average values. The initial values of parameters in RSVD are set similarly to that of Koren X  X  paper [16]. The initial values of learning ratio parameters and regula-rization constants in CFBRP are set as follows:  X   X  =0.015 ,  X   X  =0.5 ,  X   X  =0.01 ,  X   X  =0.1 , results are shown in Fig.2. 
From figure 2, it is shown that with the increasing of the dimensionality the MAE of both algorithms has a decrease tendency, and the performance of CFBRP is better than RSVD when the dimensionality is bigger than 30. When the dimensionality is set to 100, the MAE of CFBRP is 0.7137 while the MAE of RSVD is 0.7163. Algorithm CFBRP can get a better performance than RSVD, because CFBPR exploits user X  X  rating psychology more carefully and reasonably as described in section 2 and section 3. 4.3 Comparison with Other Stat e-of-the-Art Algorithms To evaluate the tolerance of CFBRP against data sparsity, we compare CFBRP with clude SVD [15], PMF [23], and TagRec [22]. For compare them consistently, the dimensionality K in CFBRP is set to 20, as done in [22]. The initial values of learning ratio parameters and regularization constants in CFBRP are set as follows:  X   X  = 0.015 ,  X   X  =0.5 ,  X   X  =0.01 ,  X   X  =0.1 ,  X   X  =0.01 ,  X   X  =0.001 ,  X   X  =0.015 , and  X   X  =0.001 . The reported MAE of CFBRP in table 1 is the average value over 25 independent runs. 
Table 1 shows that, when using 80% of the training data, CFBRP cannot do better than the other algorithms, and the reason may lie in the too small dimensionality. mance of CFBRP is less than RSVD. However, when less than 50% of the training data is used, CFBRP has better performance than all other algorithms. It implies that, by exploiting the rating psychology of users, CFBRP has better tolerance against data 664 H. Zhang et al. It hence means that, rating psychology is very important information implied in rating matrix, and it can be used to improve the performance of collaborative filtering. The process of rating an item can be considered as an expression of user X  X  psycholog-ical behavior. How to use user X  X  psychology characteristics to improve recommenda-tion is a meaningful research field. In this paper, a novel recommendation algorithm is proposed by utilizing the rating psychology. Experiment results show that the pro-posed algorithm has a better performance than other competitors. But, how to prove ture. In future works, we will also explore other useful psychology information to improve the performance of recommendation algorithms. Some optimization processes are also need to be studied, such as how to speed up the algorithm, and how to deploy it in a distributed computing environment to cope with big data. Acknowledgement. This work was supported by the National Natural Science Foun-dation of China [grant number 60973105, 90718017, 61170189, and 61202239], the Research Fund for the Doctoral Program of Higher Education [grant number 20111102130003], and the Fund of the State Key Laboratory of Software Develop-ment Environment [grant number KLSDE-2011ZX-03]. 1. Adomavicius, G., Tuzhilin, A.: Towards the Next Generation of Re-commender Systems: 2. Su, X., Khoshgoftaar, T.M.: A survey of collaborative filtering techniques. In: Advances in 7. Li, B., Yang, Q., Xue, X.: Can movies and books collaborate? X  X ross-domain collabora-8. Su, X., Khoshgoftaar, T.M.: Collaborative Filtering for Multi-class Data Using Belief Nets 10. Hofmann, T.: Latent semantic models for collaborative filtering. ACM Transactionson In-12. Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent Dirichlet allocation. Journal of Machine Learn-13. Sarwar, B.M., Karypis, G., Konstan, J.A., Riedl, J.: Application of Dimensionality Reduc-14. Sarwar, B., Konstan, J., Riedl, J.: Incremental singular value decomposition algorithms for 16. Koren, Y.: Factorization Meets the Neighborhood: A Multif aceted Collaborative Filtering 18. Goldberg, K., Roeder, T., Gupta, D., et al.: Eigentaste: A Constant Time Collaborative Fil-20. Salakhutdinov, R., Mnih, A., Hinton, G.: Restricted boltzmann machines for collaborative 22. Zhou, T.C., Ma, H., King, I., et al.: Tagrec: Leveraging tagging wisdom for recommenda-23. Salakhutdinov, R., Mnih, A.: Probabilistic matrix factorization. In: Advances in Neural In-
