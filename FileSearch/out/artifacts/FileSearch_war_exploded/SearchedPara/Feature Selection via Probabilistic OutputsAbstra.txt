 Nicholas A. Arnosti narnosti@stanford.edu 314X Huang Engineering Center, 475 Via Ortega, Stanford, CA 94305 Andrea Pohoreckyj Danyluk andrea@cs.williams.edu Williams College, 47 Lab Campus Drive, Williamstown, MA 01267 USA Data sets used to perform classification often contain redundant and/or irrelevant information. Eliminat-ing unhelpful features can reduce the computational complexity of many learning algorithms, increase the interpretability of the models they produce, and de-crease the risk of over-fitting. For these reasons, a great deal of work has been dedicated to the task of feature selection (Guyon &amp; Elisseeff, 2003). This paper explores probabilistic feature selection techniques. We present a feature-scoring criterion based on the work of Shen et al. (2008) and develop a novel theoretical framework to analyze their score and ours. We show that their score approximates an upper-bound for the improvement in accuracy that each fea-ture offers to the Bayes-optimal classifier. We demon-strate that both their scoring method and ours esti-mate the spread (across all values of a given feature) of the probability that an example x belongs to the positive class. Finally, we begin to characterize when each scoring technique proves advantageous over the other.
 In the next section we introduce notation, and fol-low with a discussion of feature selection methods. In Section 4 we present the scoring criteria and our the-oretical analysis. Section 5 outlines our predictions for the relative performance of the scores and gives preliminary empirical results. We close by discussing directions for future work. Here we introduce notation that will be used in the remainder of this paper. We say that a training set consists of examples of the form ( x i , y i ). Each x i vector of feature values taken from the space X , and y i is a class label taken from the set Y . The goal is to find a function g : X  X  Y such that for unseen examples ( x , y )  X  X   X Y , g ( x ) = y . We use n to refer to the number of training examples and d to represent the number of real-valued features for each example. We use p ( x ) to represent the density of the distribution from which feature values are drawn. Given a vector x  X  R d , x j refers to the value of the j th element of this vector and x  X  j  X  R d  X  1 is the vector x with the j th feature removed. Thus we can equivalently express P ( y = 1 | x ) represents the true probability that the example x belongs to class 1, and  X  P ( y = 1 | x ) is an estimate of this probability. For binary classification, we take Y = { X  1 , +1 } .
 Though all of our notation assumes real-valued fea-tures, the analysis presented applies equally well to variables that take on discrete values.
 Feature selection approaches generally fall into three broad categories: filter , wrapper , and embedded meth-ods. Filters are effectively pre-processing steps that score features according to some criterion and select those with the highest scores. They are fast and simple, but tend to perform less well than other ap-proaches, partly because they measure the impact of a feature without taking into account the way the classi-fier will use that feature. Embedded methods involve modification to the training algorithm itself so that features can be selected as part of the training pro-cess. They are usually classifier-specific, and thus less general than other selection techniques. For more on these approaches, see Guyon &amp; Elisseeff (2003). In this paper we focus on wrapper methods, which score a set of features according to the loss (on a test set) of a classifier trained using only these features (Guyon &amp; Elisseeff, 2003). This approach considers variables in the context of others, can apply to virtu-ally any classifier, and explicitly measures the use of the feature to the chosen classification algorithm. A commonly-implemented wrapper approach, known as recursive feature elimination , greedily constructs nested subsets of features. Starting with the full set of features, a series of classifiers are trained. The vari-ables that cause performance to suffer least when not used are eliminated, and the process repeats.
 The most common criticism of wrapper methods is that naive implementations tend to be quite slow. Even using the greedy method described above, for a data set with d features, O ( d 2 ) classifiers must be trained. This can be prohibitively expensive, so a va-riety of methods have been developed to approximate the result of this process (Guyon et al., 2002; Maldon-ado &amp; Weber, 2009). In classification tasks, the most commonly used loss function is simply the number of classification errors on the test set, or some close variant such as F -measure. This approach can have difficulty identifying signif-icant features in high-dimensional spaces, where the influence of each feature tends to be small and remov-ing any single feature is unlikely to notably affect clas-sification performance.
 One way to address this concern is to use algorithms that output estimated class probabilities. Since a feature may influence probability estimates without changing the predicted class label, scoring features ac-cording to their effect on probability estimates is more sensitive than considering only the misclassification rate.
 In this section we examine a feature-scoring method that incorporates class probabilities proposed by Shen et al. (2008), and introduce our modified feature-scoring criterion. Both scores are intended to be used as part of a recursive feature elimination scheme. For ease of exposition, this section assumes a binary clas-sification problem. 4.1. Two Feature Scoring Criteria As discussed above, the sensitivity of class probabili-ties provides a natural measure of each feature X  X  impor-tance. Shen et al. (2008) propose the following feature ranking criterion based on this idea.
 S S ( j ) = Because it is impossible to measure the above quan-tities, the joint density p ( x ) and the probabilities P ( y = 1 | x ) and P ( y = 1 | x  X  j ) must be estimated. Shen et al. propose four techniques to approximate (1). They report the best results when using the fol-lowing estimate: The motivation for the scoring system implied by  X  S S is clear: features that cause significant changes in the estimated class probabilities are ranked higher than those that do not. At the same time, it seems that an ideal importance measure should take into account not only the magnitude of the change in probability estimates, but also its sign.
 To illustrate this point, suppose that we have the following classification task with two binary attributes: x 1 x 2 p ( x ) P ( y = 1 | x ) P ( y = 1 | x 1 ) P ( y = 1 | x 0 0 10 / 22 0 . 495 0 . 45 0 . 66 0 1 1 / 22 0 . 000 0 . 45 0 . 00 1 0 10 / 22 0 . 825 0 . 75 0 . 66 1 1 1 / 22 0 . 000 0 . 75 0 . 00 Note that for an example with features x = (0 , 0), P ( y = 1 | x ) = 0 . 495, whereas if information about x 1 is not available, we see that P ( y = 1 | x  X  1 ) = 0 . 66. Though including the first feature in our model changes our estimated probabilities by 0 . 165 for all ex-amples with feature vector (0 , 0), this change is only beneficial 50 . 5% of the time, since in the other 49 . 5% of cases, the example belongs to the positive class. Equa-tion (2) does not take this fact into account.
 In light of this observation, we propose the following scoring criterion:
S A ( j ) = This score rewards features when their inclusion moves estimated probabilities towards the correct class and punishes for examples such that including the feature worsens our prediction.
 We now have two proposed feature scoring functions given by  X  S S and  X  S A . Our next goal is to develop a theoretical framework to assist in analyzing and un-derstanding these measures. 4.2. Analysis of the score S S In this section, we analyze the quantity S S given in (1). This analysis is motivated by the thought that before dedicating too much effort to approximating S S , we would like to ensure that it is a reasonable surrogate for the importance of a feature. The contributions of this section are two-fold. First, we demonstrate that S
S provides an upper-bound for the improvement in accuracy exhibited by the Bayes-optimal classifier due to the inclusion of the j th feature. We also show that S S measures the expected mean absolute deviation of P ( y = 1 | x ) as the j th feature varies.
 Ideally, S S should correspond in some way to the util-ity of the j th feature. Of course, the utility of a feature depends on the procedure by which the data are used (i.e., the classification algorithm of choice). Though Shen et al. X  X  work focuses on support vector machines (SVMs), we proceed with a general analysis in this section. Rather than measuring the utility of a fea-ture to any particular classifier, we consider the utility provided by that feature to the best possible classifier. If the true function P ( y = 1 | x ) were known, predic-tion error could be minimized by always predicting the most likely class. This decision rule defines the Bayes-optimal classifier, and for binary classification its expected accuracy is given by It is natural to measure each feature X  X  importance by the improvement in accuracy that it offers the Bayes-optimal classifier: S B ( j ) =  X  Z which can be re-expressed using the identity max { p, 1  X  p } = 1 / 2 + | p  X  1 / 2 | as S B ( j ) = (6) Z The reverse triangle inequality implies that S B ( j )  X  (7) Z =
Thus, S S ( j ) provides an upper-bound for S B ( j ) that holds regardless of the number of features, the shape of their joint density, or the form of P ( y = 1 | x ). The following computations serve to provide addi-tional intuition for the quantity measured by S S ( j ). We can rewrite S S ( j ) as: The quantity inside of parentheses is the expected value of | P ( y = 1 | x )  X  P ( y = 1 | x  X  j ) | given x the mean absolute deviation (MAD) of P ( y = 1 | x ) given x  X  j (where M AD [ Z ] = E | Z  X  E [ Z ] | ). Thus, A visualization of this fact is presented in Figure 1. Intuitively, the most important features are those that cause significant fluctuation in P ( y = 1 | x ), so using a measure of the spread of P ( y = 1 | x ) as a feature ranking criterion seems reasonable. For entirely irrel-evant features, P ( y = 1 | x ) does not vary as x j does, so MAD[ P ( y = 1 | x ) | x  X  j ] = 0. 4.3. Analysis of Our Alternative Score Having provided two reasons that S S seems like a rea-sonable feature scoring criterion, we now analyze our modified score  X  S A given in (3).
 tion  X  P ( y = 1 | x ). Then, this estimate is used to com-pute the sums in (2) and (3), which approximate the density p ( x ). It came to our attention that because  X  S
S does not use the labels of each example in this sec-ond step, the sum in (2) can be taken over any mix of labeled and unlabeled examples. In many domains, large sets of unlabeled examples are available while labeled training data is relatively sparse, so using un-labeled data in this way could substantially improve the performance of  X  S S .
 Both  X  S S and  X  S A approximate the term P ( y = 1 | x )  X  P ( y = 1 | x  X  j ) by assuming a functional form for P ( y = 1 | x ) and fitting parameter values to the training data. Even with a large training set, these estimates may be significantly biased if P ( y = 1 | x ) does not have the assumed form. We argue that including y i in  X  S A may alleviate this problem. To illustrate our point, suppose that x 0 is such that P ( y = 1 | x 0 ) = 1 / 2. Then even if the estimates  X  P ( y = 1 | x 0 ) and  X  P ( y = 1 | x  X  j ribly wrong, as the number of training examples with feature vector x 0 grows, we see a law of large numbers effect: the 50% of positive examples with feature vec-tor x 0 should cancel the corresponding negative exam-ples in the sum from (3). By contrast, bad estimates contribution when computing  X  S S ( j ) via (2), regardless of the training set size.
 Our work from Section 4 and the above discussion lead us to the following predictions:  X  Because variance is the square of standard devia- X  When the training set is sampled disproportion- X  In cases where the assumed model does not fit We test the first two predictions in Sections 5.1 and 5.2, while leaving the third for future work. Addition-ally, we present preliminary results from applying  X  S S and  X  S A to a real-world data set in Section 5.3. 5.1. Relationship Between  X  S S and  X  S A : a To confirm the prediction that  X  S A should vary approx-imately as the square of  X  S S , we ran a trial on the syn-thetic variables X c,p as described earlier, with c rang-ing from 0 to 3 in steps of 0.25 and p in [0 , 1] with steps of 0.1. For each ( c, p ) pair, we trained a linear SVM on a two-variable training set, where the first variable was X c,p and the second was Gaussian noise.
 Support vector machines typically do not provide probability estimates, but Vapnik (1998), Hastie &amp; Tibshirani (1998) and Platt (1999) have all proposed methods for using SVMs to generate probabilities. For tests in this section, we obtained and modified source code from Shen et al., which derives probability esti-mates using the technique proposed in (Platt, 1999) and an SVM implementation from LIBSVM. Results from these trials are shown in Figure 3, and confirm our prediction beautifully.
 5.2. Incorporating Unlabeled Examples We designed the next test to explore the potential ben-efit of using unlabeled examples when computing  X  S S . In this section, we use  X  S SU to represent the score (2) augmented by additional unlabeled examples and  X  S S to stand for the same score applied only to the labeled training instances.
 For the purposes of this experiment, we constructed a synthetic data set with five nominal features. Each feature is independent of all others and takes on three possible values, say a , b , and c . The frequency with which each variable takes each value is given below: All but the first feature are noise (i.e. they do not affect the probability that the example belongs to the positive class). We chose P ( y = 1 | x 1 = a ) = 1 / 4, P ( y = 1 | x 1 = b ) = 1 / 2, P ( y = 1 | x 1 = c ) = 3 / 4. Tests described in this section and 5.3 were conducted with a Naive Bayes classifier designed for nominal fea-tures. We made this choice because Naive Bayes clas-sifiers explicitly provide estimated class probabilities. Additionally, they afford a natural (and efficient) way to compute  X  P ( y = 1 | x  X  j ): missing values are handled by simply not including probabilities from that feature (Kononenko, 1991). Note that on this data set the fea-tures are independent, so with adequate training data a Naive Bayes classifier should replicate the true class probabilities.
 Experiments were conducted in three trials. Each trial contained tests on nine different sizes of training sets. For each training size and each trial, 500 runs were performed. A single run for a specified trial and train-ing size consisted of sampling an appropriate number of training examples, training a Naive Bayes classifier on these examples, and using this classifier (along with a collection of unlabeled examples when appropriate) For each algorithm, we recorded the number of times (out of 500 runs) that it successfully ranked the first feature as the most informative.
 The difference between the three trials was the man-ner in which training examples were selected. During the first trial, examples were drawn from the original distribution. During the second, the x 1 training val-ues were equally likely to be a , b , and c (effectively undersampling from b and oversampling from a and c , thereby making x 1 seem more informative than it actually is). In the third trial, x 1 took the values and c each with probability 1/8, and took the value b with probability 3/4. This has the effect of understat-ing the importance of x 1 . In all trials, the dependency P ( y = 1 | x ) remained unchanged and the unlabeled ex-amples provided to  X  S S were drawn from the original distribution. This design allowed us to explore the question of how each scoring system fares when there is bias in the process of selecting training examples. The results, shown for different training set sizes, are displayed in Figure 4.
 When sampling proportionately from the data set, the three scores performed comparably. On the second trial (which we refer to as the  X  X versampling X  trial due to the fact that it overstates the importance of X 1 ),  X  S S and  X  S A identified x 1 as the top feature more often than in the first trial (as expected), while  X  S SU performed nearly identically to the first trial. This suggests that our estimate for P ( y = 1 | x ) is sufficiently accurate that incorporating unlabeled examples yields scores as if the training examples had been sampled proportionately. On the third ( X  X ndersampling X ) trial, both  X  S S and  X  S A identified x 1 as the most important feature in fewer cases than either of the other trials. The degradation is most notable for  X  S S . Meanwhile,  X  S
SU performed at approximately the same level as on the other trials. The amount by which  X  S SU outper-forms the other methods on the third trial appears to be independent of the number of training examples. These results suggest that when the training data is not representative of the entire domain and unlabeled examples are available, using them in scoring can be very beneficial. When no unlabeled examples are avail-able,  X  S A provides a more reliable measure of each fea-ture X  X  importance than  X  S S . 5.3. Breast Cancer Results Here we test the performance of  X  S S and  X  S A on a real-world data set: Breast Cancer, available from the UCI repository ( http://archive.ics.uci.edu/ml/ datasets/ ). The task is to predict, based on 9 discrete features (each taking values in the set { 1 , 2 , . . . , 10 } ), whether a tumor is malignant or benign.
 The distribution of feature values is far from uniform: averaged across all features, 46.2% of values are 1, while only 1.1% take the value 9, and the values 6,7,8 each occur with frequency below 4%. Because we wished to see how  X  S S and  X  S A performed when pre-sented with limited training data (in particular, as few as ten labeled examples), this meant that for any given training set, it was likely that most of the possible fea-ture values would not be present in the training data. To alleviate this problem, we converted each feature into a binary attribute according to the mapping f given by: f ( x ) = I ( x 1  X  5) , I ( x 2 = 1) , I ( x 3 = 1) , I ( x
I ( x 5  X  2) , I ( x 6 = 1) , I ( x 7  X  3) , I ( x 8 = 1) , I ( x Our basic classifier achieved a leave-one-out error rate of 23/699 on the original data set. On the transformed data with binary features, this rate was 24/699, indi-cating that for a Naive Bayes classifier, effectively no predictive power is lost by our transformation. In order to easily evaluate  X  S S and  X  S A , we augmented the data set by adding three purely noisy binary fea-tures. We conducted experiments on sets of size 10 , 20 , . . . , 100. For each size, 20 runs were conducted. A run consisted of sampling a set of positive and nega-tive examples with replacement from the full data set. Given a training set of n examples, n classifiers were trained, each using all but one example to compute estimates for  X  P ( y = 1 | x ) and  X  P ( y = 1 | x  X  j held-out data point. These estimates were used to compute  X  S S and  X  S A , thereby obtaining a ranking of the features.
 For each training size, we computed the aggregate rank for each feature by ranking them on the basis of their average rank across all 20 runs. As shown in Table 1, for all training sizes and both scores, the three unin-formative features were among the five features with the lowest aggregate rank. Additionally, across all runs and training sizes, neither score ever ranked one of our dummy features as its top choice.
 When provided with at least 30 training examples,  X  S S ranked all three noisy features among the bottom four on all 20 runs.  X  S A did not perform quite as well: even with 100 training examples, on two of 20 runs one of the uninformative features was ranked as highly as fifth.
 It is difficult to infer much from the ranking of the orig-inal features, because all of them are at least weakly predictive of the class label.  X  S S and  X  S A generally agreed that feature 9 was the least useful of the original features. As one might hope, when we tested the per-formance of classifiers trained on each possible pairs of features, those using x 9 performed least well. In this paper, we have considered the feature scoring algorithm presented by Shen et al. (2008) and pro-posed our own related score for use in feature selec-tion tasks. We focused on these techniques because they consider the importance of each variable in the context of others and can score variables even in high-dimensional contexts where each feature X  X  impact on the final prediction is small. Our primary contribution is a careful analysis of Shen et al. (2008) X  X  score, S S , and our alternative criterion S A .
 We demonstrated that the quantity S S ( j ) is the ex-pected conditional mean absolute deviation of P ( y = 1 | x ) given x  X  j , and that S A ( j ) is the expected condi-tional variance of P ( y = 1 | x ) given x  X  j . These proofs suggest that each score is a reasonable criterion for fea-ture selection, as S S and S A select as important the features whose values have the greatest influence on P ( y = 1 | x ).

S S 10 20 30 40 50 60 70 80 90 100 x 10 9 11 12 12 12 12 12 12 12 12 x 11 11 12 11 11 11 11 11 11 11 11 x 12 10 9 9 10 10 10 10 10 10 10  X 
S A 10 20 30 40 50 60 70 80 90 100 x 10 10 9 10 10 10 9 8 11 9 10 x 11 12 10 9 12 9 11 11 12 10 9 x 12 8 11 8 10 12 8 12 9 8 11 As alternative justification for S S we proved that S S ( j ) provides an upper-bound for the improvement in ac-curacy of the Bayes-optimal classifier due to the infor-mation provided by the j th feature. Additionally, we hypothesized that the approximation  X  S S could benefit from unlabeled examples. For a simple synthetic task, we confirmed that this data improved the robustness of  X  S
S to variations in the way that the labeled examples were sampled.
 We motivated our score,  X  S A ( j ), by observing that it measures both the magnitude and the sign of changes in estimated class probabilities due to the j th feature. We proved that using  X  S A to eliminate features from the data set is equivalent to minimizing total loss on the training set when using an L 1 loss function. For the problem described in Section 5.2, we concluded that when there was no unlabeled data available to supplement the training set,  X  S A was less sensitive than  X  S
S to sampling variations in the training data. We view this paper as a beginning, rather than con-clusive, investigation of feature selection using proba-bilistic outputs. As such, there are many interesting directions for future work.
 Much of the analysis here pertains to the quantities S
S and S A , but in practice we are forced to use ap-proximations. An open question is the extent to which the approximations used in this paper are  X  X ood. X  One way to quantify this would be to give sufficient condi-tions for these estimates to converge to S S and S A as the number of training examples grows.
 We argued in Section 5 that the fact that  X  S A incorpo-rates the sign of changes in predictions should help to mitigate the presence of bias due to modeling assump-tions. One major goal for the future is to validate this prediction, either empirically or theoretically. The eventual goal of this work is to develop the theory of feature selection using probabilistic outputs to the point where, given a data set, we can choose a feature scoring algorithm that is likely to perform well in the specified domain. In order to accomplish this, we hope to perform tests on real-world data to determine the extent to which the theory developed in this paper extends to different learning algorithms and domains. Guyon, Isabelle and Elisseeff, Andre. An introduction to variable and feature selection. J. Mach. Learn. Res. , 3:1157 X 1182, March 2003. ISSN 1532-4435. Guyon, Isabelle, Weston, Jason, Barnhill, Stephen, and Vapnik, Vladimir. Gene selection for cancer classification using support vector machines. Ma-chine Learning , 46:389 X 422, 2002. ISSN 0885-6125. 10.1023/A:1012487302797.
 Hastie, Trevor and Tibshirani, Robert. Classification by pairwise coupling. The Annals of Statistics , 26 (2):pp. 451 X 471, 1998. ISSN 00905364.
 Kononenko, Igor. Semi-naive bayesian classifier. In
Kodratoff, Yves (ed.), Machine Learning -EWSL-91 , volume 482 of Lecture Notes in Computer Sci-ence , pp. 206 X 219. Springer Berlin / Heidelberg, 1991.
 Maldonado, Sebasti  X an and Weber, Richard. A wrapper method for feature selection using support vector machines. Inf. Sci. , 179:2208 X 2217, June 2009. ISSN 0020-0255. doi: 10.1016/j.ins.2009.02.014.
 Platt, J. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers , 10 (3), 1999.
 Shen, Kai-Quan, Ong, Chong-Jin, Li, Xiao-Ping, and
Wilder-Smith, Einar. Feature selection via sensi-tivity analysis of svm probabilistic outputs. Ma-chine Learning , 70:1 X 20, 2008. ISSN 0885-6125. 10.1007/s10994-007-5025-7.
 Vapnik, Vladimir N. Statistical Learning Theory . Wiley-Interscience, 9 1998. ISBN 9780471030034. URL http://amazon.com/o/ASIN/0471030031/ .
 Weston, J. and Watkins, C. Support vector machines for multi-class pattern recognition. European Sym-
