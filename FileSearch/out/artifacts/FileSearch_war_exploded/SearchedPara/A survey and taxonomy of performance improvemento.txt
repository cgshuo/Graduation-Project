 Peyman Kouchakpour  X  Anthony Zaknich  X  Thomas Br X unl Abstract The genetic programming (GP) paradigm, which applies the Darwinian principle of evolution to hierarchical computer programs, has been applied with breakthrough success in various scientific and engineering applications. However, one of the main drawbacks of GP has been the often large amount of computational effort required to solve complex prob-lems. Much disparate research has been conducted over the past 25 years to devise innovative methods to improve the efficiency and performance of GP. This paper attempts to provide a comprehensive overview of this work related to Canonical Genetic Programming based on parse trees and originally championed by Koza (Genetic programming: on the programming of computers by means of natural selection. MIT, Cambridge, 1992). Existing approaches that address various techniques for performance improvement are identified and discussed with the aim to classify them into logical categories that may assist with advancing further research in this area. Finally, possible future trends in this discipline and some of the open areas of research are also addressed.
 Keywords Genetic programming  X  Computational effort  X  Efficiency  X  Performance improvement  X  Taxonomy 1 Introduction In the natural world, there is a wealth of complex and intelligent biological organisms and creatures. Biological processes have offered countless inspirations, presenting novel ideas and metaphors to scientists for artificial systems. Evolutionary processes have been the vital force for the progression of the smallest virus to the most complicated creature. This has lead researchers to view evolution as a powerful concept and to validate whether the Darwinian principles of natural selection can be applied in the silicon world as they appear in the carbon world, resulting in the birth of evolutionary computation (EC). EC has attracted the attention of many researchers from diverse backgrounds, motivating new developments and applica-tions [ 46 , 77 , 78 , 143 , 198 ], to name a few. As shown in Fig. 1 , Evolutionary Computation has four main traditional variants; evolutionary programming (EP) invented in the mid 1960s, evolution strategies (ES) developed in the 1970s, genetic algorithms (GA) devised in the mid 1970s and the youngest stream the genetic programming (GP) formalised and championed in the 1990s.

All the EC variants are based on the same concept of Darwinian evolution and thus the underlying idea is the same for all of them. That is, they start off with an initial random popu-lation of individuals and process this set of candidate solutions simultaneously, using natural selection and operations such as crossover and mutation to produce new candidate solutions. All the various dialects of EC are population based and stochastic. They use random initiali-sation together with architecture altering operations, fitness function, selection mechanisms and termination conditions to discover a solution. The primary feature that characterises each EC system into its own stream is how the chromosomes are encoded. In fact, the dif-ferent EC systems were organized in [ 9 ] based on their type of representation. This point is graphically illustrated as illustrated in Fig. 2 . In other words, the main difference is in the structure undergoing adaptation, i.e. the representation or the genotypes. For example, the data structures encoding the solution are typically fixed length binary characters in GA, finite state machines in EP and trees with varying shapes and sizes in GP. Consequently, the definition of their respective variational operators becomes specific and different. Each discipline therefore differs in its application area. For example, GP is typically positioned in machine learning whilst the other disciplines generally pertain to optimisation problems.
This work attempts to present an overview and taxonomy of previous research conducted to enhance the performance of Genetic Programming. Although this paper very briefly covers the other variants of the Genetic Programming paradigm, its main focus is on the canonical or standard GP which was championed by Koza in [ 124 , 125 ] based on tree structures. The creative ideas proposed by researchers to improve the effectiveness and efficiency of GP are reviewed and categorised in the following sections. Section 2 briefly discusses the basic properties that influence evolving populations. Section 3 discusses the overall modifi-cations and improvements. Section 4 looks at Improved GP with the proposed modifications that various researchers have put forward and forms the bulk of this paper. In Sect. 5 ,someof the GP variants and hybrids are highlighted and briefly discussed, followed by conclusions in Sect. 6 . 2 Evolving populations Although there exist many variants of evolutionary algorithms, their underlying idea is vir-tually the same. Given a population of candidate solutions, their fitness can be increased by environmental pressure through natural selection. Selection and variation operators form the two basic forces in this system. The role of selection is to distinguish amongst individuals based on their quality to either allow the better individuals to become parents or to replace the existing individuals with individuals that have higher quality or fitness. Every individual has an associated fitness value which represents how good an individual is in solving the prob-lem at hand. The fitness function forms the basis for selection and facilitates improvements. Therefore, selection can act as a force pressing on quality improvements. On the other hand, the role of the variation operators is to create new individuals and hence explore the solutions X  search space. It is important to note that this iterative process is not always guaranteed to produce individuals with higher fitness. Due to the highly stochastic nature of this system, it is quite likely to witness the phenomenon of genetic drift, where individuals with higher fitness could be lost from the population or the population could experience a loss of variety of certain characteristics.

Candidate solutions within the original context of the problem are referred to as phe-notypes whilst their encodings in the problem solving space are termed as genotypes. The representation, which is one of the distinguishing differences amongst the various evolution-ary algorithms, involves specifying the mapping between the phenotype and genotype space. Consequently, a range of encoding strategies has been developed for the various evolution-ary algorithms. For example, the encoding of individuals is via fixed-length character strings (typically binary) in Genetic Algorithms, real-valued vectors in Evolutionary Programming and trees in canonical Genetic Programming.

Convergence can be generally interpreted as the point at which the population contains a considerable number of similar individuals. In this instance, the algorithm is either not progressing satisfactorily or is approaching a local optimum (premature convergence). At times, convergence has been interpreted as the point at which the algorithm is approaching the global or optimal solution. In the former, convergence could be considered to be a serious problem or weakness as it could be expected that after repeated cycles of the evolutionary process uniformity may arise sooner or later. Maintaining diversity may be considered a possible remedy to this problem. The diversity of a population is a measure of the number of different solutions present in the current generation. There exist numerous possible defi-nitions of diversity in GP. The term diversity can be referred to as the diversity of genotypes (structural diversity) or behavioural difference (phenotypes).

The no free lunch theorem (NFL) [ 226 ] states that no search algorithm is superior to any other algorithm on average across all possible problems. For example, GA may perform better than random search for certain problems, whereas random search may outperform GA for other different problems. The erroneous conclusion should not be made from this that there is no point in designing better algorithms or improving algorithms. We are not typi-cally interested in solving all possible problems but rather a certain class of problems that are suitable for the evolutionary algorithms to tackle. The broad implication of the NFL in regards to performance improvement of the algorithms is that the modifications may only be appropriate for certain form of representations or the modifications may provide improved performance for a specific class of problems. This would mean that the stated improvements are always accompanied with certain assumptions and limitations. 3 Overview of modifications and improvements The modifications that have been made to the GP to improve performance or save compu-tational effort generally fall into three categories, GP variants, hybrids and improved GP as depicted in Fig. 3 and described below.

The structures within the canonical or standard GP are tree-based. However, the structures that undergo adaptation within the GP variants are no longer tree-based and significantly devi-ate from the original canonical GP. As previously mentioned, since the genome or structure is the primary feature that distinguishes the different variants within EC, the same argument can be applied here. In other words, these variants of GP can be viewed as separate minor dialects of EC because the representation is changed but they have the same motivation or application of a GP. Hybridization of EC with other techniques, generally known as memet-ic algorithms (MA) [ 51 ], are considered to be problem tailored methods. The combination of GP with other algorithms or problem specific techniques enriches the GP with knowl-edge and thereby improves its performance. All the other innovative methods to improve the performance of GP fall in the last category, termed here as improved GP (IGP), which pre-serves the tree-based structure of the canonical GP. Although GP variants and hybrids are very briefly discussed herein, the focus will be based on Improved GP.

Before continuing with the survey of the proposed methods that improve the performance of GP, a brief note needs to be made of the work performed on the theoretical aspects of GP. Although these studies investigate why GP performs poorly or well, it is essential to briefly note that the main work on the theoretical aspects of GP encompasses tree schemata and the schema theorem of GP. Schemata were traditionally used to explain why GAs work but have recently been used to show the workings of GP [ 184 , 185 , 202 ]. The definition of a schema for GP is much less straightforward than for GAs and several alternative definitions of GP schema have been proposed in the literature (for details, please refer to [ 185 , 186 ]). A schema is defined as a similarity template composed of one or multiple trees or fragments of trees. In some definitions [ 231 ] schema components are non-rooted leading to considerable mathematical difficulties. In this case, a schema can be present multiple times within the same program departing from the original concept of GA schemata. Rather than representing subsets of the search space, such definitions focus on the propagation of program compo-nents within the population. Recently, schemata have been represented by rooted trees or tree fragments [ 184 , 185 , 202 ], resulting in easier schema calculations. In these studies, it was hypothesized that solution outcomes are determined by rooted-tree schema y [ 202 ]andthat GP identifies these structures first during a run and then builds upon these structures to form a particular solution. The efforts highlighted above endeavour to build a theory for GP, based on the concept of schema. In one case, groups of components that propagate within the pop-ulation and how their occurrences vary over generations are modelled. In another instance, subsets of the search space and how the amount of individuals in such subsets varies over generations are modelled. 4 Improved GP Figure 4 shows how the various improvements on Standard GP may be organized into possible categories. Generally the improvements are either implemented to remedy an acknowledged issue within Genetic Programming, such as the bloat phenomenon, lack of diversity or premature convergence etc., or the improvements provide pioneering modifica-tions to the components of the GP algorithm. The proposed modifications can usually be further subdivided into three classes. They could be fixed and predetermined prior to the run and unaltered during the run. For example, one may determine that a certain variation opera-tion is beneficial and recommend its use for the entire run. Secondly, the modifications could be deterministic and static X  X here is a predetermined rule which specifies how and when a certain modification will take place. The rule f ( g ) , which specifies how and when a modifi-cation takes place, is a function of time or generation g . For example, it may be suggested that the mutation operation should only be used after generation number g &gt; 30. Lastly, the mod-ifications could be based on some feedback mechanism. For example, if a certain condition occurs, then a particular selection mechanism or operation should take effect. 4.1 Improvements on components of GP Genetic Programming has a number of main components which define its operation, namely variation operators, initialization, selection, control parameters, fitness and termination. Researchers have looked into improving or altering all these components in turn to enhance the performance of GP, as detailed in the following subsections. 4.1.1 V a ri a tio n oper a tors Variation operators are used to create new candidate solutions and are typically divided based on their arity, e.g. mutation and crossover being unary and binary variation operators respectively. Although crossover shoulders a great responsibility for the evolution of the GP algorithm [ 52 , 53 ], the mutation operator plays a significant role in the genetic convergence process by preventing loss of genetic diversity in the population [ 140 ]. The main search operator in GP is the crossover operator and all the other variation operators are often termed as secondary operations. The crossover operator is discussed in the following sub-subsection and the next subsection reviews the secondary operators and all the other newly proposed operators. 4.1.1.1 Crosso v er The primary operation for modifying genetic structures in GP is cross-over. Crossover is a stochastic operator which merges information from generally two parents to create offspring genotypes. The first crossover operator for GP was defined in [ 124 ]. The effect of crossover was investigated in [ 172 ]. It was argued that it had the disadvantage of producing a high computational cost due to growth of individuals in size and complexity during the evolution process [ 223 ]. This effect, which is known as code bloat, is formed by an excessive exploration capability of the crossover [ 155 ]. It was argued [ 16 ] that 75% of crossover event could be termed as lethal and can result in disruption of building blocks. With recombination or crossover being considered as the primary operator in GP, many researchers have looked into ways of modifying it to improve the efficiency.

As crossover was viewed to be destructive, the brood recombination aimed to decrease this effect and preserve good building blocks. The  X  X oft brood selection X  method [ 1 ] generated a brood by performing crossover over the selected parents N times and then introduced the best of the brood in the next generation by holding a tournament. The  X  X rood recombination X  was introduced in [ 220 , 221 ], which was a refinement of the soft brood selection. The Brood Selection Recombination Operator R B ( n ) produced n pairs of offspring but only kept the best two of the 2  X  n produced offspring using a selection function. As the brood selection performs multiple samplings of the crossover operator and keeps the best 2 offspring, it can essentially be viewed as a hill-climbing crossover operator. Although, the brood selection increases the computational cost per generation it can however increase the selection pressure and therefore the rate of convergence towards an optimal solution for some problems. To reduce the computational cost a clever approach was implemented [ 220 ], where the eval-uation of the new 2  X  n offspring is on a small portion of the training set rather than all the test cases. The brood size was further investigated [ 252 ] for the brood recombination crossover method. It was shown that as the brood size increased, the performance improved and the brood recombination method outperformed the standard crossover method for the three object classification problems studied. The disruptive nature of crossover was reduced by the brood recombination, as the children of the destructive crossover events were rejected by this operator and as a result the building of larger building blocks was promoted. Other approaches [ 187 , 246 ] choose good sub-trees or crossover points to swap. Context-aware crossover [ 104 , 153 ] discovers the best possible crossover site for a sub-tree and is shown to consistently attain higher fitness. There are similarities between the con-text-aware crossover and the Brood Crossover in that multiple children are produced during each crossover event. The destructive effects of standard crossover was minimized in [ 151 ] by placing the selected sub-tree in its best context in the parent tree. The best context was calculated by using the effect of the placement of the selected sub-tree on the overall fit-ness of the parent tree and then selecting the placement, which produced the maximum final fitness.
 Some heuristics were added to the standard crossover operator [ 99 ] to make it smart. Here, a form of intelligent heuristic guidance for the GP crossover was proposed. The smart crossover computed the performance values for sub-trees and used this information to decide which sub-trees are potential building blocks to be inserted into another sub-tree and which sub-trees are to be replaced due to their poor performance value.

A homologous crossover operator was introduced in [ 16 ], where the exchange is strongly biased towards very similar chunks of genome. Structural distances are measured by com-paring genotypes and functional distances by comparing phenotypes. These two measures are used to determine the probability that the trees are crossed over at a specific node. In this way, the crossover probabilities are biased by structural and functional features of the trees. Similarly, a GP 1-Point crossover operator was introduced in [ 184 ], which had homologous overtones. This was based on the 1-point crossover for GAs, where the selection process involved checking for structural similarities of trees to find points with structural homology.
The Ripple Crossover, examined in [ 114 ], was shown to outperform the traditional sub-tree crossover on two benchmark problems. Although the Ripple Crossover was more explorative than the single tree node crossover, it was a more disruptive crossover operator and its disrup-tive nature resulted in a slower convergence. A one-point crossover was introduced in [ 185 ], in which the same crossover point is selected in both parents. Two trees are aligned from the root nodes and recursively and jointly traversed. Recursion is stopped as soon as an arity mismatch between the corresponding nodes in the two trees is observed. A random crossover point is selected from the above identified nodes and the two sub-trees below the common crossover point are swapped. Some of the interesting features of the one-point crossover are that it is a simpler form of crossover for GP and it facilitates population convergence by searching for good partial upper part or structure solutions. Moreover, it does not increase the depth of the offspring beyond that of their parents.
 Crossover points are conventionally selected randomly. A depth-dependent crossover for GP was proposed [ 103 ], in which the depth selection ratio was varied according to the depth of a node. Shallow nodes were favoured as the crossover points and hence larger sub-trees were swapped. This promoted the accumulation of useful building blocks via the encapsula-tion of a larger part of a tree. The behaviour of the uniform crossover and point mutation was examined in [ 178 ] presenting a novel representation of function nodes, which allowed the search operators to make smaller movements around the solution space. It was shown that the performance on the even-6-parity problem was improved by three orders of magnitude when compared to the standard GP.

The headless chicken crossover operator, which was studied in [ 132 ], uses a selected pro-gram P and a newly randomly generated program R to produce an offspring by replacing a sub-tree of P with a replaced sub-tree from R until it finds an offspring with greater or equal fitness to P . The crossover-hill climbing [ 16 ] operator is another form of the headless chicken crossover. In [ 2 , 88 ], the fitness of the individual is the sum of its fitness components or genes. A gene is periodically added to the individual during the evolution and if it improves its fitness it is kept, otherwise discarded. Between gene additions, the population evolves by intergene crossover.

A novel crossover method was proposed [ 113 ] using the usage frequency of nodes. Three crossover techniques were investigated, namely a crossover with crossover points in both nodes having high usage frequency, with crossover points in a node having high usage fre-quency and a node having low usage frequency, with crossover points in both nodes having low usage frequency. It was discovered that their method was promising for speedup in GP. Many researchers looked into determining crossover points that are likely to be more advan-tageous. For example, a higher-level analysis of the population as a whole was used in [ 199 ] utilizing statistics gathered over all sub-trees to determine the crossover points or in [ 10 ] selective self-adaptive crossover (SSAC) and self-adaptive multi-crossover (SAMC) meth-ods were used. The depth-fair crossover (DFC) was introduced in [ 117 ], which allowed for weighting crossover points. It assigned an equal weight to each depth of the tree. Each node within each depth was given an equal amount of the depth weight. Improvements were also made to the crossover operator [ 250 ] using a measure called looseness to guide the selection of crossover points rather than choosing them randomly. Improvement was shown over the headless chicken crossover [ 132 ] and the standard crossover.

The latest developments imply that the crossover operator is on its way to becoming a more powerful and robust operator. It is believed that there is still room for the crossover operator to improve the quality and efficiency of the search it conducts. This can be achieved by either combining the current approaches or devising new ways for improvement. 4.1.1.2 Other Oper a tors In addition to the primary genetic operation of crossover, there are various secondary operations such as mutation, permutation, encapsulation etc. The effect of various operators, namely mutation, permutation, encapsulation and editing, on GP perfor-mance was first investigated in [ 125 ]. Although it was argued that the subject was certainly not solved and required further work for a general conclusion, it was shown that for some selected problems there was no substantial difference in performance when these operators were included.

The performance improvements in GP provided by automatic defined functions (ADF) and decimation were compared in [ 164 ] using the Santa Fe ant, the lawnmower, the even 3-bit parity and symbolic regression problems. It was concluded that decimation provided superior improvement in performance over ADF. It should however be noted that it was con-cluded that ADF was not effective for simple problems [ 127 ] and its benefits only became increasingly evident for complex problems.

To overcome the disruption of building-blocks due to crossover and mutation, an adaptive program called STructured Representation On Genetic Algorithms for Non-linear Function Fitting (STROGANOFF) was introduced [ 96 , 97 ]. In [ 98 ] an adaptive recombination for a numerical GP was proposed which was guided by a measure called minimum description length (MDL). The application of mutation or crossover operators was adaptively controlled to improve efficiency.

A new operator was introduced in [ 52 , 53 ] to minimize the number of evaluations required to find an ideal solution by evaluating the observed strengths and weaknesses of selected individuals within areas of the problem. The motivation in [ 53 ] was to intelligently per-form crossover by discriminating between the portions of each parent that lead to success and failure. A new GP operator, the memetic crossover, was introduced, which allowed for an intelligent search of the feature-space. The proposed process involved the identifi-cation of specific areas of importance within the problem (sub-problem) and in tracking the nodes executed while observing the individual X  X  performance as it was evaluated. The information gathered was then organized by ranking the nodes. Nodes that were executed were said to participate in the sub-problem. Bad and good nodes were associated with one or more sub-problem failures and successes respectively. Using the memetic crossover method [ 53 ] individuals were examined to ensure compatibility with respect to sub-problem perfor-mance. The individuals were regarded as compatible, when a significant sub-problem match occurred between one of the worst performing nodes in the recipient and one of the best performing nodes in a potential donor. In this instance, crossover was performed with the recipient replacing its bad node with the donor X  X  good node. The Los Altos trail and the royal tree problem, which can easily be decomposed into well-defined sub-problems, were used as benchmark problems. For this approach to be significantly advantageous, it was required that the problem be able to be methodically decomposed into sub-problems. Although me-metic crossover incurred additional processing cost, they were considered negligible when compared to the time saved through the reduction in the number of evaluations.

The macro-mutation operator (headless chicken crossover) was shown in [ 132 ] to out-perform the traditional GP crossover operator. The pruning genetic operator was proposed in [ 168 ] for removing useless structures from the GP individual. The operation was applied to randomly selected sub-trees. Redundant node patterns, which are problem dependant and uniquely defined for each problem, were searched and replaced with an effective terminal node resulting in efficiency improvement in the GP X  X  search.

In [ 14 ], crossover was between a member of the population and an ancestor tree, which was a fixed collection of trees. The crossover operator generated only one tree, the population member with one of its sub-trees replaced by a sub-tree of the ancestor. This variation oper-ator, which was neither a crossover nor a mutation, used information from two individuals with only one member belonging to the population. Analysis of mean tree size growth dem-onstrated that this operation limited parse tree growth because the ancestors did not grow. The genetic material in the ancestor set did not change and was available indefinitely, implying that building blocks or information was never lost.

It is suggested that new possible operators or methodologies should be devised that either promote finding good building blocks or reduce the destructive nature of the currently pro-posed GP operators. Care should however be exercised that that the reduction of disruptive effects of current operators, that generate new candidate solutions, should not be overin-dulged as it may simply transform the GP paradigm to a simple hill climber, which is not desirable. 4.1.2 I n iti a liz a tio n Initialization involves the random generation of individuals for the initial population. The traditional GP tree-creation algorithms GROW, FULL and Ramped Half-and-Half were intro-duced in [ 125 ]. It was shown that Ramped Half-and-Half was the best observed for the Quartic polynomial, 6-multiplexer, Artificial Ant and Linear equations problems. In [ 138 ], a random initialization, which produced programs of random shapes, was defined.

The RAND_tree algorithm was introduced in [ 100 ]andin[ 21 ] where trees were initial-ized with exact uniform probability from a tree-derivation grammar. Using diverse random seeds, multiple abbreviated runs were made in [ 181 ]. An enriched population was created using the best member from each abbreviated run. This enriched population was then loaded together with a full set of randomly generated unique members at the start of a consolidated run.

Two new tree-creation algorithms probabilistic tree-creation (PTC 1 and PTC 2) were offered by [ 145 ], where an average tree size or a distribution of tree sizes could be specified with guaranteed probabilities of occurrence for specific terminal and non-terminal functions within the generated trees. PTC 1 &amp; 2 had very low computational complexity and had comparable results with the GROW technique.

Further research in formulating novel ways of generating new individuals per run should be called for, as it is believed that the overall fitness and diversity of the initial population in the first generation plays a significant role in the success of the later generations within that run. The main goal should be to enrich the initial population and increase its structural and behavioural diversity without introducing a large computational effort. Finer initialization techniques for new runs may emerge by either exploring memory or learned behaviour from previous runs or introducing problem specific innovations into the initialization stage. 4.1.3 Selectio n The role of selection is to differentiate among individuals based on their quality. Individu-als with higher quality are favoured to be parents participating in a variation operation or be replacements of an existing individual in the case of recombination. Selection is gener-ally responsible for driving for quality improvements and is probabilistic. The performance characteristics of a repertoire of selection methods, namely proportional selection, ranking selection, and tournament selection, were investigated for time series prediction in [ 118 ].
In [ 189 ], the sampling behaviour of tournament selection over multiple generations was analysed, where the analysis was focused on individuals which did not participate in any tournament at all, due to not being sampled during the creation of the required tournament sets. A new selection scheme was proposed in [ 69 ], which was based on standard tournament selection, to encourage genetically dissimilar individuals to undergo genetic operation. It demonstrated performance improvements of GP for the algebraic symbolic regression prob-lem. An automatic selection pressure for the tournament selection was investigated in [ 242 ] to improve the efficiency of GP. The number of tournament candidates was dynamically changed in response to the changing evolutionary process. Using the symbolic regression and the even-6 parity problems it was shown that this approach could improve the effec-tiveness and efficiency of GP systems. In [ 244 ], the relationship between population size and tournament size was investigated and a new fitness evaluation saving algorithm, evalu-ated-just-in-time (Ejit), was proposed which resulted in constant computational savings by avoiding the evaluation of not-sampled individuals.

It is believed that a large training set of fitness cases could slow down GP. Dynamic subset selection based on a fitness case was proposed in [ 139 ], where an appropriate topology-based subset selection method allowed individuals to be evaluated on a smaller subset of fitness cases. A topology relationship on the set of fitness cases was created during the evolutionary search by increasing the strength of the relation between two fitness cases that an individual could successfully solve. The proposed selection method selected a subset, where the fit-ness cases were distantly related with respect to the induced topology. Using four different problems, it was shown that dynamic topology-based selection of fitness cases progressed on average faster than the stochastic subset sampling.

In the canonical GP, selection pressure is only applied in the selection of parents and the offspring are simply propagated into the next generation without any selection. In [ 243 ], a many-offspring breeding process with selection pressure applied to the selection of offspring was investigated. A many-offspring breeding process can be viewed as a standard crossover that generates a large number of poor offspring in the search for good offspring. Two cross-over operators were proposed. Firstly, the Ideal Crossover considers all the possible ways of recombining two selected parents to produce all the possible offspring. It then evaluates all the offspring and keeps the best two offspring with the highest fitness values. Secondly, the Partial Crossover, which is similar to the context-aware crossover operator [ 151 ], selects a random point for crossover in one parent P 1 but considers all the other nodes in the other parent P 2 to produce offspring. The focus of these techniques is to optimise the offspring X  X  fitness and thereby increasing selection pressure.

A theoretical and empirical study that will increase our understanding of which selection methodologies can be deemed superior during different stages of the evolutionary process is recommended. This study can then become the basis for implementing new schemes that explore dynamic selection of various proposed selection techniques during the run. In addi-tion, further innovative ways that can result in reducing the computational burden of selection can be very beneficial for enhancing the performance of GP. 4.1.4 Co n trol of p a r am eters The genetic programming paradigm is controlled by various control parameters such as the maximum number of generations ( G ), the population size ( M ), the probability of crossover ( P c ), recombination ( P r ), mutation ( P m ) and the maximum tree depth ( D ), to name a few.
The issue of parameter control and setting was discussed in [ 51 ]. The algorithm param-eters can either be tuned or adapted. Parameter tuning involves the empirical investigation of parameter values which will result in good performance before the run. Once the best suited parameter value for the specific problem is determined, its value is set in advance and remains unchanged for the duration of the run. Alternatively, the parameters could be deterministically altered as a function of time/generation during the run or adaptively con-trolled through some heuristic feedback mechanism resulting in explicit adaptation. On the other hand, The actual parameters could be encoded into the data structures of the algo-rithm and evolve with the adaptation being entirely implicit. This can be summarized as per Fig. 5 . 4.1.4.1 Tree size There has been substantial amount of work performed concerning the solution X  X  shape (dynamics of tree size and depth) such as [ 43 , 44 , 47 , 128 , 211 ], to name a few. The tree size parameter is used to impose a size restriction on individuals. Typically the tree sizes for the random initial population and evolved individuals are restricted differently, namely through the maximum initial tree size D i and the maximum created initial size D c . These parameters impose restrictions on the maximum allowable depth for a tree. The cor-relation between average parent tree size and the modification point (crossover or mutation) was shown in [ 148 ]. Both of these were directly linked to the size of the resulting child.
A dynamic tree depth limit was explored in [ 206 , 207 ]. The dynamic limit was initially set as high as the maximum depth of the initial random trees [ 206 ]. Trees which exceeded this threshold were rejected and replaced by one of their parents, unless the tree in question was the best individual found so far. In this instance, the dynamic limit was increased to match the depth of this new best-of-r un individual. The dynamic limit was lowered as the new best-of-run individual allowed such reduction [ 207 ]. Moreover, a dynamic tree depth was adopted in [ 253 ] to constrain the complexity of programs. The proposed method was applied to data fitting and forecasting problems with results indicating improvement over GP.

Some researchers have looked into limiting the total amount of tree nodes of the entire population [ 228 ], rather than imposing limits at the individual level. The concept of resource-limited GP, which was a further development to [ 228 ], was introduced in [ 208 ]. As the total number of nodes in the population exceeded a predefined limit, resources became scarce and not all offspring were guaranteed to progress. The candidates were queued by fit-ness and progressed into the next generation on a first come first serve basis. The trees that required more resources than the amount still available would not survive. The relationship between size and fitness was not explicitly defined and was a product of the evolutionary process. A natural side effect of this approach was that the population was automatically resized. Although these approaches used the same rationale, they operated at different levels of the GP paradigm, namely acting at the individual level and at the population level. Tree depth limits imposed a maximum depth to each individual and Resource-limited GP limited the total amount of resources that the entire population could use.

The two different approaches, tree depth limits and resource-limited GP, were compared in [ 210 ] using symbolic regression, even parity, and artificial ant problems. It was shown that the resource-limited GP was superior to tree depth limits [ 210 ]. A dynamic approach to resource-limited GP was developed in [ 209 ]. The dynamic resource limit was initially set as high as the amount of resources required for the first generation. The allocation of trees, sorted according to fitness, continued into the next generation until the resources were exhausted (as per original resource-limited GP). The rejected individuals would be consid-ered as candidates for the next generation if the mean population fitness was improved. Hence the dynamic resource limit was raised as a function of mean population fitness providing the additionally needed resources. It was shown that the dynamic approach to resource-limited GP achieved better performance when compared with the static approach and with the tradi-tional depth limits, using the symbolic regression polynomial problem and Santa Fe artificial ant problem. 4.1.4.2 Oper a tor an d selectio n prob a bilities As GP is a completely stochastic process, it is controlled by various probabilistic control parameters, such as the probability of selecting an internal point ( P ip ) as a node for the crossover operation or probability of crossover and reproduction ( P c and P r ) which determine by which process the fraction of individuals are created for the next generation.

In [ 172 ], explicitly defined introns (EDIs) were introduced as instruction segments that were inserted between two nodes of useful code. EDIs changed the probability of crossover between the two nodes on either side of the EDI improving the convergence properties of a GP algorithm. Similarly in [ 30 ] the probability of selection of every node for crossover was indirectly adapted through the evolutive introns (EIs). Evolutive introns are explicitly defined introns, which are artificially generated, with the aim to increase the probability of selecting good crossover points as the evolutionary process continues. The automatic growth and shrinking of non-coding segments in the individuals are promoted, thereby adapting the probabilities of groups of code being protected.

The adaptation of operator probabilities in genetic programming was investigated in [ 167 ] with an attempt to reduce the number of free parameters within GP. Two problems from the areas of symbolic regression and classification were used to show that the results were bet-ter than randomly chosen parameter sets and could contest with parameters set as based on empirical knowledge. 4.1.4.3 F un ctio nan dter m i na lsets In the conventional GP, the structures are typically com-prised of the set of N func functions from the function set F = f 1 , f 2 ,..., f Nf un c and the set of N term terminals from the terminal set T = { a 1 , a 2 ,..., a Nter m } forming the combined set C = F  X  T . This combined set defines the set of all the possible structures or elements. The choice of function and terminal sets can have a significant effect on the GP X  X  performance and if the sets are not sufficient to express a solution for a given problem, then GP would not be able to solve the problem.

The effect of extraneous variables and functions was first studied by [ 125 ] and it was shown that a linear degradation in performance was observed as additional extraneous variables were added for the cubic polynomial problem. Similar results were obtained for extraneous functions; nevertheless it was shown that for some specific problems, extraneous functions improved performance. Furthermore, no substantial difference in performance was observed for extraneous ephemeral random constants. It was concluded that the question of extraneous sets was not definitely answered in this study and further experimental and theoretical work was recommended to be carried out leading to general conclusions. In [ 229 ], a systematic study was conducted of how to select appropriate function sets to optimize performance. They classified functions into function groups of equivalent functions and showed that a set that was optimally diverse (that included one function from each function group) was most appropriate. 4.1.4.4 Pop u l a tio nan dge n er a tio n num ber Population size ( M ) and the maximum number of generations ( G ) are the two major numerical control parameters in GP, with their values generally dependant on the difficulty of the problem. The role of population size in GP was first very briefly studied by [ 125 ]. In this study, the 6-multiplexer problem was solved with various population sizes. The study concluded that larger population size M increased the cumulative probability P ( M , i ) of satisfying the success predicate of a problem for GP, for generations between generation 0 and generation i . In this study, the population size was maintained at a constant level and was not varied throughout the run. However, there was a point after which the cost of a larger population surpassed the benefit achieved from the increase in P ( M , i ) .

It was shown that using a large population was not always the best way to solve problems [ 70 ]. It was demonstrated in [ 182 ] that as problem complexity increases, determination of the optimal population size becomes more difficult. The control of population size in GP was first implemented by [ 215 ] to improve the algorithm X  X  robustness and reliability. The plague operator (first experimented within GA [ 42 ]) was introduced in [ 57 , 58 ] to fight bloat in GP, where individuals were removed at a linear rate to compensate for the increase in individual size. It was shown that computational effort could be saved and plague allowed a given fit-ness level to be reached with a smaller computational effort. The decrease in population size was also studied in [ 149 ], where the population size was gradually decreased throughout the GP run. In Virtual Ramping, the size of the population and the number of generations were continuously increased [ 61 ] reducing premature convergence.

In [ 122 ], the population variation (PV) scheme was introduced, where the population could be increased and/or decreased with a variable profile. The increment or reduction of population size could take on any flexible profile such as linear, exponential, hyperbolic, sinusoidal or even random. It was demonstrated that PV significantly improved performance and showed that the optimum profile was dependant on the problem domain. An investigation was carried out to determine whether the nature of the  X  X opulation variation X , i.e. the way the population was altered during the run, had any significant impact on GP performance in terms of computational effort. In addition, a novel concept was introduced to ensure that the computational effort for an unsuccessful run in the PV scheme would never exceed that of the Standard Genetic Programming. It was shown that the PV algorithm outperformed the plague operator.

Static population variation employs a deterministic adaptation approach using simple time-varying schedules, where the population size is varied according to a deterministic function. One of the shortcomings of the static population variation scheme is that the population size is varied by a blind deterministic function. It is more desirable to vary the population size in an informed way during the run. Using a heuristic feedback mechanism, the population size can be dynamically varied by taking into account the actual progress of GP in solving the problem. A technique was introduced in [ 62 ] to dynamically vary the size of the population during the execution of the GP system. The population size was varied  X  X n the run X  accord-ing to some particular events occurring during the evolution. In [ 123 ] various new ways to dynamically vary the population size during the run of the GP system were proposed. The proposed approach, referred to as dynamic population variation (DPV), extended the work of [ 62 ] and it was shown that DPV was superior to Standard Genetic Programming (SGP), the plague operator, the dynamic population modification technique in [ 62 ] and all the static PV schemes reported in [ 122 ].

Some possible potential research in this area may be a further study into population vari-ation based on structural and behavioural diversity, which may yield interesting results. 4.1.5 Fit n ess an d objecti v ef un ctio n Fitness is the driving force of natural selection and measures the quality of an individual with respect to how well an individual can solve a given problem. It is generally defined by an objective or fitness function forming the basis for selection, defining and facilitating improvements.

Multi-objective techniques [ 31 ], which allow the concurrent optimization of several objec-tives by searching the so-called Pareto-optimal solutions, were investigated in [ 20 ]toevolve compact programs. There are various multi-objective optimization techniques such as strength pareto evolutionary algorithm (SPEA) [ 256 ], SPEA2 (an improved version of SPEA) [ 258 ], SPEA2+ [ 119 ], NSGA-11 [ 45 ], Adaptive Parsimony Pressure [ 248 ]. The program size was considered to be a second independent objective in addition to the program functionality in [ 20 ] and an enhanced version of SPEA proposed in [ 257 ] was used. A multi-objective GP was used in [ 13 ] to make improvements on the results.

Novel strategies based on elastic artificial selection (EAS) and improved minimum descrip-tion length (IMDL) were investigated in [ 121 ] for fitness evaluation and selection to create shorter programs and prevent premature convergence. The effect of tournament selection and fitness proportionate selection with and without over-selection for particular problems was investigated in [ 125 ]. It was shown that for many problems it was possible to enhance the performance of GP by greedily over-selecting the fitter individuals in the population.
In [ 241 ], the whole population was clustered to reduce the fitness evaluations and improve the effectiveness of GP. The clustering was performed by a heuristic called fitness-case-equiv-alence. For each cluster, a cluster representative was selected and its fitness calculated and directly assigned to other members in the same cluster. Using a clustering tournament selec-tion method and a series of experiments of symbolic regression, binary classification and multi-class classification problems, it was shown that the new GP system outperformed the standard GP on these problems.

A new methodology was proposed [ 162 ] to create a new training set of randomly-gener-ated fitness cases prior to each generation of the GP run instead of using a fixed set of fitness cases. It was shown that, this methodology was mainly useful to reduce the brittleness of GP when the fixed training population does not adequately represent the full range of difficult situations of the problem. The fitness function was scaled over time in order to improve performance in [ 71 ]. The motivation behind this approach was that it is often easier to learn difficult tasks after simpler tasks have been learned.

As the majority of the computational effort in GP is expended in the fitness function, it is beneficial to avoid invoking the fitness function whenever possible. As the reproduction operator produces an identical copy of its parent, it is then quite obvious that the fitness evaluation can be safely avoided. This can result in considerable savings in computation because reproduction in GP conventionally accounts for the creation of ten percent of the new individuals. This flagging and caching of the already computed fitness of reproduced individuals was proposed in [ 125 ], provided that there are no varying fitness cases from generation to generation. A technique was devised in [ 105 ], which allowed the GP system to determine many instances in which invocation of the fitness function could be avoided. This was achieved through the consideration of the program nodes executed during fitness evaluation to establish whether a newly generated individual has the same fitness value as its parent. This could be realized through the identification of dormant nodes [ 106 ], which are program nodes that are never executed, extending a marking method described in [ 18 ]. It was shown that this technique, when applied to the multiplexer problem and even-parity problem, resulted in significant savings in execution time. 4.1.6 Ter m i na tio n In traditional GP, a fixed number of generations are usually used as the condition for termi-nating the evolution process. To address the CPU time-consuming issue and the large amount of computational resources required for GP, the three different termination criteria of effort, time and max-generation were examined by [ 67 ]. An improved termination criterion was implemented in [ 131 ] to prevent premature termination, when further search may continue to pay off, or to prevent unnecessarily continuing to search dead-ends when further progress seems implausible. Here, the run will continue as long as improvements continue to be made. A maximum number of unproductive generations is used to terminate a run.

Examination of different measures for stagnation and premature convergence could be most promising together with newly invented methodologies to abruptly terminate a run and commence a new run with the knowledge gained from the previous run. Moreover, a thor-ough study on a dynamic termination method that is based on the combined structural and behavioural diversity is suggested. 4.2 Innovative ideas The improvements detailed in this section contain some pioneering modifications to the canonical GP algorithm to enhance its performance. 4.2.1 Si m plific a tio n An approach to online simplification was introduced in [ 236 , 251 ], where programs were auto-matically simplified during the evolution using algebraic simplification rules and algebraic equivalence. The proposed method was tested on the regression and classification problems, showing its superior performance when compared with the standard GP systems. 4.2.2 Mod u l a riz a tio n The technique of automatic function definition (ADF) was introduced by [ 126 ] to potentially define useful functions dynamically during a run and to accelerate the discovery of solutions in GP. The number of fitness evaluations that must be executed [ 127 ] can be considered as a reasonable measure of computational burden. In [ 127 ], ADF was shown to allow the discov-ery and exploitation of regularities, symmetries, similarities and modularity of the problem environment. It was shown that for simpler versions of problems ADF was not effective but as the problems were scaled up the increasing benefits became evident.

Evolving modular programs was also investigated in [ 6 , 8 ], where special mutation oper-ators (compress and expand) defined modules from the developing programs at random, allowing modular programs to emerge using the genetic library builder (GLiB). An approach for reusability was proposed in [ 92 ] based on ADF, incorporating a library for keeping sub-routines acquired by ADF. This library preserved knowledge and ensured reusability so that the acquired subroutines could be shared and reused. The most frequent sub-trees, which were expected to contain useful partial solutions, were grouped as modules [ 195 ]. Such sub-trees were encapsulated by representing them as atoms in the terminal set. Additionally, a random sub-tree selection and encapsulation was examined and empirical results illustrated performance improvement over standard GP. A method for automatically generating useful subroutines by systematically considering all small trees was presented in [ 34 ]. This algo-rithm moved progressively and systematically through the best trees of a given size and considered them as candidates for subroutine generation. This algorithm was successfully tested on the artificial ant problem.

Layered learning has been used for solving GP problems in a hierarchical fashion. The layered learning approach [ 39 , 79 , 94 , 95 , 107 , 217 ] decomposes a problem into subtasks, each of which is then associated with a layer in the problem-solving process. It is believed that the learning achieved at lower layers when solving the simpler tasks directly facilitates the learning required in higher subtask layers. Two program architectures are proposed in [ 108 ] for enabling the hierarchical decomposition based on the division of test input cases into subsets, each dealt with by an independently evolved code segment. The main program branch includes calls to these new entities via an expanded terminal set. The proposed tech-nique offered substantial performance improvements over the more established methods such as the ADF for the even-10 parity problem.

A sub-tree was randomly selected in module acquisition (MA) [ 7 , 120 ] from an individual and then a part of this sub-tree was extracted as a module and preserved in a library defined as a new function. This module was protected against blind crossover operations and could be referred to by other individuals. In adaptive representation GP (AR-GP) [ 200 , 201 ], an effective sub-tree was selected and added as a new function to the function set to improve learning efficiency.

For GP to be able to address more demanding larger and more complex solution programs, it is inevitable for GP to have the ability to scale up. One way to achieve this is through more efficient modularization practices. Further theoretical and empirical studies that aid in under-standing the concept of building blocks within GP, their early detection and their further development and enrichment would certainly be a promising way to explore new schemes or possible improvements of current methods. In addition, a study that combines the current approaches may provide new insights in this area. 4.2.3 Other i nn o va ti v eide a s Double-based genetic algorithm (DGA), which improves the performance of a GA, was shown to be relevant for the GP paradigm [ 36 ]. Two types of doubles were defined based on permutations on the arguments and permutations on the terminals of the terminal set, introducing doubles in the population set. The Double-based Genetic Programming para-digm provided a useful extension of the GP standard search procedure and demonstrated its advantages for Genetic Programming.

The best subtree genetic programming (BSTGP) [ 163 ] selects the best sub-tree in order to provide the solution of the problem. This is different from the canonical GP, where the fitness of a tree is given by its root node. BSTGP also produces smaller trees as nodes that do not belong to the best sub-tree are deleted. The proposed approach was tested using a number of symbolic regression and classification problems showing comparable results to standard GP.
 An approach using a clustering method was described [ 12 ] to reorganize subpopulations in GP, with the goal of producing more highly fit individuals. The initial population P is divided into number of subpopulations S i after a nominated clustering frequency and according to the genetic similarity of the individuals. The sizes of the subpopulations are proportional to the average fitness of the individuals they contain. It was shown that a slight speedup over the canonical GP was observed for the multiplexer, parity and artificial ant problems. The evaluation of a generation is widely accepted to be the most expensive process in GP. Sub-tree caching and vectorized evaluation [ 115 ] attempted to make this less expensive and more efficient. Two types of bottom-up and top-down caching were introduced, where the latter encouraged the caching of big sub-trees and the former encouraged the caching of small sub-trees. Although the caching of big sub-trees made the evaluation process more efficient, it was less likely that it could be matched and used again during the evaluation process due to its larger size.

It should be noted that the other main innovative ideas studied herein and proposed by various researchers, which are specific to either the GP components and operators or the GP aspects and concepts that they pertain to, have been grouped, categorized and discussed in the other sections of this paper. 4.3 Solutions to known issues or problems within GP The improvements detailed in this section endeavour to remedy an acknowledged issue within GP, such as the bloat phenomenon or lack of diversity etc. 4.3.1 Clos u re The initial population-generating algorithms may not always generate valid individuals. The grammar-guided genetic programming (GGGP) attempted to address this known closure problem. The reader is referred to Sect. 5.1 . 4.3.2 Pre ma t u re co nv erge n ce Various studies have been conducted to address the issue of premature convergence [ 172 , 240 , 253 ]. The issue of Premature Convergence is mainly addressed by making improvements on the components of GP. The reader is referred to Sect. 4.1 . 4.3.3 Di v ersity There has been much work which has focused on diagnosing or remedying the loss of diver-sity within Evolutionary Computation. A new method of approximating the genetic similarity between two individuals was presented in [ 69 ], which used ancestry information to exam-ine the issue of low population diversity. By defining a new diversity-preserving selection scheme, genetically dissimilar individuals were selected to undergo genetic operation. This provided the means to alter the perceived fitness of individuals. The study of how multi-population GP helps in maintaining phenotypic diversity was conducted in [ 225 ]. In [ 158 ], negative correlation was examined to improve diversity and prevent premature convergence. A study to evaluate the influence of the parallel GP in maintaining diversity in a population was conducted in [ 66 ].

A two-phase diversity control approach was proposed by [ 240 ] to prevent the common stage through a refined diversity control (RDC) method with automatically defined functions (ADF) and a fully covered tournament selection (FCTS) method. RDC was an extension to general diversity control (GDC), which passed the diversity check if two whole program trees, with the main tree and ADFs treated as a whole, were not exactly identical in genotype. RDC treated the main tree and ADF as individual objects and hence both were required to be unique for the diversity check to pass. FCTS was an extension to the standard tournament selection (STS). It was argued in [ 240 ] that due to the randomness of STS, individuals with bad fitness may be selected multiple times, where an individual with good fitness may never be selected. FCTS avoided this issue by excluding individuals that had already been selected. The proposed methods effectively improved the GP X  X  performance resulting in the reduction of number of generations needed to reach an optimal solution and decreased incidences of premature convergence. 4.3.4 Blo a t an d code growth Many researchers have highlighted the problem of bloat, which is the uncontrolled growth of the average size of an individual in the population. There exist numerous studies of code were summarized in [ 133 ] to prevent bloat, namely (i) Limiting tree depth to some maxi-mum value, (ii) Using parsimony pressure, with the use of multi-objective (MO) methods and (iii) Tailoring genetic operators such as size-fair crossover [ 135 , 138 ]orfairmutation[ 136 ].
In the standard GP this issue is indirectly dealt with by limiting individual tree X  X  maxi-mal allowed depths. This can be viewed as unsatisfactory as this will require knowledge of the maximum necessary depth in advance of solving the problem. The effects and biases of size and depth limits on variable length linear structures were explored using empirical and theoretical analyses in [ 157 ]. It was argued in [ 253 ] that the increasing size of trees would reduce the speed of convergence towards a solution and thus affect the fitness of the best solution. Consequently, the dynamic maximum tree depth was proposed to avoid the typical undesirable growth of program size. A technique was demonstrated in [ 161 ], which signifi-cantly constrained the growth of solutions, i.e. bloat. This method imposed a maximum size on the created individuals within the population, which solely depended on the size of the best individual of the population. It was shown that the combination of depth limiting and methods which punish individuals based on excess size, were effective [ 150 ].

One mechanism for limiting code size is the Constant Parsimony Pressure, where larger programs are penalized by adding a size dependent term to their fitness [ 212 ]. This tech-nique incorporated the program size as an additional constraint, but a hidden objective. The application of parsimony pressure was investigated in [ 72 ] in order to reduce the complexity of the solutions. Their results reported that while the accuracy on the test sets were pre-served for binary classification setup, the mean tree size was significantly reduced. Parsimony pressure has also been used in [ 146 , 147 ] to fight Bloat. Parsimony pressure incorporated in a multi-objective framework has been used by many researchers [ 197 , 254 , 255 ]. The use of multi-objective optimization for size control was studied in [ 40 ]. Multi-objective tech-niques in the context of GP were also investigated in [ 20 ] to reduce the effects caused by bloating. The inclusion of the tree size measure as one of the objectives has been found to be extremely effective at controlling bloat. It was shown in [ 15 ] that mutation can be used to prevent population collapse, a phenomenon where the population in Multi-objective GP rapidly degenerates to just trees of a single node because it tends to produce a positive mean increase in tree size per generation counterbalancing the parsimony pressure exerted by the fitness-based selection process. In [ 111 ], the functionality was first optimized and then after-wards followed by size (ranking method-two stage). The advantage of this was that pressure on size would not deter GP from discovering good solutions. This was because pressure is only applied when the individual has already reached the desired performance. However, bloating continued in solutions that had not attained the aspired performance.

The genetic operators could be modified to address the problem of bloating such as Delet-ingCrossoverin[ 19 ]. In [ 50 ], speed improvements were observed by removing introns. In [ 183 ], the issue of too long solutions and bloat were addressed by maximum homolo-gous crossover (MHC). Equivalent structures from parents were preserved by aligning them according to their homology. MHC was tested on a symbolic regression problem and demon-strated its abilities in bloat reduction without inducing any specific biases in the distribution of sizes, allowing efficient size control during evolution. It was evident from the results that control of the size was possible while improving performance. The use of multiple cross-overs was explored as a natural means to contain code growth [ 216 ]. Multiple crossovers were performed between two parent trees, where the total number of crossovers occurring between the two selected parents is dependent on the sum of the sizes of the parents involved. Three similar multi-crossover algorithms were shown to be a viable choice for containment of code growth.

Itwasarguedin[ 238 ] that a significant problem with GP was the continuous growth of individual X  X  size without a corresponding increase in fitness. A self-improvement (SI) operator was applied in combination with a characteristic based selection strategy to reduce the effects of code growth. Instead of simply editing out non-functional code the proposed method selected sub-trees with better fitness. The SI operator selects individuals that have at least one sub-tree that has a higher fitness value than its original tree (mother individual). This will result in a fitter individual to replace a less performing individual with the reduction in depth and node count from its original tree. In other words, the sub-tree is upgraded to a new individual by removing the branches of the original mother tree. The performance of the proposed method was validated by testing it on a symbolic regression and a multiplexer problem showing a substantial reduction of code growth while maintaining the same level of fitness. It may be argued that the proposed approach is suitable for the above problems because they are simply and decomposable.

It was claimed in [ 247 ] that code bloat slowed down the search process, destroying pro-gram structures, and exhausting computer resources. Non-neutral offspring (NNO) operators and non-larger neutral offspring (NLNO) operators were proposed to deal with these issues. An offspring could be distinguished as improved, neutral or worsened with respect to their fitness in comparison with their parents. The neutral offspring that are larger in size than their parents (LNO) could be further separated from those that are smaller in size than their parents (SNO). An LNO has more introns while an SNO has less. But they both have the same exon structure as their parents. It is argued that evolutionary processes favour LNOs, resulting in intron growth with no performance improvement. The proposed approach discarded all neutral offspring, named non-neutral offspring operators (NNO). Another approach, called non-larger neutral offspring (NLNO), kept SNOs and discarded only LNOs. Both approaches confined intron growth to different degrees. These two kinds of neutral offspring control-ling operators were tested on two GP benchmark problems, namely symbolic regression and multiplexer problems to verify whether they could successfully apply parsimony pressure. It was concluded that NLNO was only able to confine code bloat and simultaneously improve performance.
 It was shown in [ 218 ] that by eliminating bloat, the performance of GP could be improved. Some modifications to the selection procedures were presented to eliminate bloat without deteriorating performance. The relationships of the bloat phenomenon with parallel and distributed GP has been investigated by many researchers and positive results have been obtained, where the bloat phenomenon could be controlled by parallelizing GP [ 56 , 73 ]. It was shown in [ 41 ] that the parallel evolutionary model, specifically the island model, helped to prevent the bloat phenomenon.

A simple theoretically-motivated method for controlling bloat was introduced in [ 188 ], which was based on the idea of dynamically and strategically creating fitness  X  X oles X  in the fitness landscape repelling the population. These holes were created by zeroing the fitness of a certain proportion of above average length offspring. This meant that only a fixed proportion of offspring, those which violated the length constraints, were randomly penalized.
Three methods for bloat control were presented [ 179 ], biased multi-objective parsimony pressure (BMOPP), the Waiting Room, and Death by Size. BMOPP was a variation on the pareto-optimization theme which combined lexicographic ordering, pareto dominance and a proportional tournament. The latter two methods do not consider parsimony as a part of the selection process, but instead penalize for parsimony at other stages in the evolutionary process. In the Waiting Room, newly created individuals were only permitted to enter the population after having sat in the  X  X aiting room X  or queue for period of time proportional to their size. This provided smaller individuals a greater opportunity to spread. Death by Size chose individuals to die and be replaced based on their size.

Parsimony pressure is traditionally used to reduce the complexity of solutions. In [ 219 ] however, a negative parsimony pressure was applied for a financial portfolio optimization problem in GP, preferring complex solutions rather than simpler ones. Negative parsimony pressure presumed that the principle of Occam X  X  Razor inhibits evolution [ 219 ]. Favour-able results were shown; where in some instances it was better to apply negative parsimony pressure.

Recent studies support the hypothesis that introns emerge predominantly in response to the destructive effects of the variation operators. Although, it may be argued that introns can be considered as useful because they protect good building blocks, nevertheless it can at the same time run the entire population into stagnation due to bloat, which is the explosive and exponential growth of introns. In this instance, no feasible improvements can be observed as the population is merely exchanging introns during recombination. A potential future direction within this area would be the formulation of more efficient variation operators or methodologies that can reduce the destructive nature of existing operators. 5 GP variants and hybrids It should be noted that this paper is focused on the canonical GP and does not delve into the other variants and hybrids. A very brief summary and introduction to GP variants and hybrids (entire Sect. 5 ) has been included to introduce the reader to these new GP variants in the hope that it may lead to insights or clues for possible ideas that may guide future improvements in canonical GP.

Variants of GP are differentiated by their different structures [ 87 , 125 ]. There are many different GP structures such as tree, linear and graph structures with many other forms of representations being investigated and continuously emerging. For example, in [ 112 ]anew kind of GP structure called linear-tree-structure together with its own crossover and mutation operations was introduced. A novel genetic parallel programming (GPP) paradigm, with a linear genetic programming representation, was introduced in [ 141 ] for evolving parallel pro-grams. It was observed that parallel programs were more evolvable than sequential programs. In [ 32 ], considerable speed up in evolution was observed using the GPP paradigm, running on a multi-arithmetic-logic-unit (Multi-ALU) processor (MAP) evolving parallel programs and then serializing them into a sequential program.

A low level modularization strategy, called compressed GA (cGA), was presented for linear genetic programming based on a substring compression/substitution scheme. The pur-pose was to protect building blocks and foster genetic code reuse. There are many more other variants explored by various researchers such as the gene expression programming (GEP), gene estimated gene expression programming (GEGEP) an extension to GEP [ 49 ], Multi niche parallel GP [ 74 ], directed acyclic graphs (DAGS) [ 82 ], parallel automatic induction of machine code with genetic programming (parallel AIM-GP) [ 174 ], genetic network program-ming (GNP) [ 89 ], grammar model-based program evolution (GMPE) [ 102 ] and many others [ 3 , 11 , 101 , 144 , 203 ]. There are also many various hybrids that have been researched, such as genetic programming neural network (GPNN) [ 193 , 194 ], Ant Colony Programming [ 22 ], traceless genetic programming (TGP) [ 175 ]. Many researchers have looked into improving the newly proposed hybrid. For example, in [ 23 ] the problem of eliminating introns in ant colony programming (paradigm based on genetic programming and ant colony system) was investigated. 5.1 Grammar-guided genetic programming Grammar-guided genetic programming (GGGP) is an extension to standard GP with the aim to address the closure problem [ 130 , 233 , 235 ]. GGGP employs a context-free grammar (CFG) establishing a formal definition of the syntactical restrictions. The grammar allowed the declarative introduction of language bias that could result in a reduction in the search space. Individuals are derivation trees that represent solutions belonging to the language defined by the context-free grammar [ 230 ]. GGGP always generates valid individuals (points or possible solutions) that belong to the search space. In [ 232 ], the influence of program gram-mars on the efficiency of GP was described. In [ 249 ], a new method of representing based on CFG was used to separate search space from solution space through a genotype to phe-notype mapping and this technique was applied to a symbolic regression problem showing improvement over a basic GP without a grammar. In [ 259 ], extensions to the operators were made to improve grammar-based evolutionary algorithms.

Many other variants of GGGP have also been investigated. For example in [ 90 , 91 ],anew grammar guided genetic programming system called tree-adjoining grammar guided genetic programming (TAG3P+) was proposed. It is argued in [ 165 ] that standard GP is unable to search for all tree shapes, namely solutions that require very full or narrow trees. A different tree-based representation was used by [ 165 ] together with new local structural modification operators (point insertion and deletion) to eliminate this problem. The new representation was based on tree adjoining grammars (TAGs), which were first proposed in [ 109 ], to remove the fixed-arity limitation of standard GP. 5.1.1 I n iti a liz a tio n A new initialization method was introduced for GGGP in [ 33 ]. Random Branch tree-gen-eration algorithm [ 33 ] guaranteed the generation of trees of requested size. However, this algorithm could not produce a well distributed set of trees, thus resulting with a negative impact on the convergence speed [ 81 ]. The Uniform Tree Generation algorithm [ 21 ] guar-anteed the uniform creation of trees of requested tree size but was known to be too complex [ 75 ]. The Grow algorithm was modified in PTC1 and PTC2 to ensure that the trees were generated around an expected size [ 145 ].

A new tree-generation algorithm for GPPP, grammar-based initialization method (GBIM), was proposed [ 75 ]. A parameter was included to control the maximum size of the trees to be generated and thereby the initial populations generated were distributed in terms of tree size. It was shown that the proposed method had a higher convergence speed when compared with Ramped Half-and-Half, Basic, Random Branch, Uniform and PTC2 tree generation algorithms for an arithmetical equalities problem and the real-world task of breast cancer prognosis. 5.1.2 V a ri a tio n oper a tors The strong context preservative crossover operator (SCPC) was proposed in [ 38 ] to preserve the context in which the sub-trees occur in the parent trees and to control code bloat. Nodes with matching coordinates can be selected as crossover points in SCPC, in other words, restricting crossover to nodes that reside in similar contexts within the individual. In this case, emphasis was placed on the genotype of the individual (locations of the nodes), not the phenotype.

Crossover in GP has been blind [ 87 ] in contrast to biological crossover, where chro-mosomes exist with a matching and aligned homologous partner, in a process referred to as meiosis. The Fair crossover operator [ 35 ] was designed to prevent code bloat, which is a modified version of the operator proposed by Langdon [ 133 ]. Two original genetic operators, crossover and mutation, were proposed in [ 37 ] for the grammar-guided genetic programming (GGGP) paradigm. The grammar-based crossover operator (GBC) improved the GGGP per-formance, by providing a good balance between search space exploration and exploitation. The grammar-based mutation (GBM) operator generated individuals that matched the syntac-tical constraints of the CFG that defined the programs. The proposed operators were tested in two experiments demonstrating a higher convergence speed and a lesser likelihood of being trapped in local optima.

A new grammar-based crossover (GBX) operator was introduced in [ 154 ] for the gram-mar-guided genetic programming system to prevent code bloat. Moreover, GBX provided trade-off between exploration and exploitation of the search space. Grammatical evolution (GE) is an extension of GP and evolves complete programs by using a Backus Naur Form (BNF) grammar or style of notation. An important aspect of GE, which distinguishes itself from other GGGP approaches, is its representation of the individual as a linear string, which is decoded to produce a derivation tree. Various different crossover operators were proposed in [ 83 , 84 ]andin[ 85 ] a meta-grammar was introduced into GE allowing the grammar to dynamically define functions. 5.2 Parallel genetic programming The measurement and computation of population fitness consumes a large amount of com-putational effort and is generally considered time-consuming. Many researchers have looked into distributing the computational effort needed to calculate fitness, hence Parallelizing GP which is frequently the focus of the parallel computing community. In addition, many researchers have investigated the spatial distribution of GP models. The two fields of parallel GP and spatially-distributed GP models had different goals. The main goal for parallel GP was often to speed up computation of the fitness evaluations, by either having each individual or some subpopulation evaluated on a separate processor. In this instance, the population as a whole was often treated as a panmictic population. The spatially-distributed GP constructed some form of spatial structure with the intention of maintaining diversity, which could be executed on the same processor or multiple processors.

Two basic approaches to parallelization were discussed in [ 125 ]. In Distributed GP the population is divided into sub-populations (island model), each assigned to a processor. The Distributed GP can be implemented on a network of workstations or a parallel computer where the GP operates on each sub-population separately. A specified percentage of individ-uals within each sub-population are selected for migration after a certain designated number of generations. There are many variations of distributed models such as demes, islands and niching methods. For example, in the island method a population P of M individuals is divided into N subpopulations (called demes) D 1 ,..., D N of M / N individuals. A standard GP works on each deme and the subpopulations are interconnected according to various com-munication topologies and information is periodically exchanged by migrating individuals from one subpopulation to another. As a result various new parameters such as the number of subpopulations N , number of individuals to be migrated (migration rate), the number of generations after which migration should occur (frequency) and the migration topology are needed in this methodology. In the second approach, there are no sub-populations or migrations, where steps are executed locally and asynchronously on a distributed basis. The independent algorithm tasks are distributing to separate processors.

The behaviour of distributed GP with respect to sequential GP was firstly analysed in [ 191 ]. Three levels of parallelization for the determination of fitness were described by [ 125 ] namely, at fitness cases, at fitness evaluation for different individuals and at independent runs. In [ 4 ], each processor was responsible for the fitness evaluation and breeding of a sub-population increasing the efficiency of GP. It was hypothesized in [ 225 ] that distributed GP outperforms the panmictic GP due to maintaining diversity.

In [ 129 ], it was argued that increases in computational power can be realized by paralle-lizing the application. The parallel implementation of GP on a network of processing nodes was described in [ 5 ] that achieved super-linear performance. A divide and conquer strategy was introduced to increase the probability of success in GP [ 63 ], where the search space was partitioned in smaller regions that were explored independently of each other.

In [ 190 ], multi-populations were examined and in [ 54 , 59 ] various control parameters for multi-population models were systematically studied. Layered genetic programming (LAGEP) was proposed in [ 142 ], which was based on multi-population genetic program-ming (MGP). This method employed layer architecture to arrange multiple populations. A layer contains a number of populations. In addition, an adaptive mutation rate tuning method was proposed to increase the mutation rate. LAGEP achieved comparable results to single population GP in much less time. The GP was used with several isolated subpopulations, where the individuals among the several populations were not allowed to communicate [ 55 ]. This methodology was referred to as isolated multipopulation genetic programming (IMGP). It was shown that although IMGP was not always helpful in obtaining better results, in some instances better results were obtained than in the classic method.

A fine-grained parallel implementation of GP through cellular model on distributed-mem-ory parallel computers with good performances was presented in [ 65 ]. In the fine-grained (grid or cellular) model, each individual is associated with a spatial location on a low-dimen-sional grid, interacting only with their direct neighbours. Different neighbourhoods can be defined for the cells. Some examples of the two-dimensional (2-D) neighbourhoods are the 4-neighbour (von Neumann neighbourhood) and 8-neighbour (Moore neighbourhood).

Various researchers have also attempted to make improvements to the canonical paral-lel evolutionary algorithms, e.g. in [ 80 ] the speciating island model (SIM) was explored. In [ 54 , 55 ], the aspect of population size was investigated for the multi-population Parallel Genetic Programming. It was discovered that an optimal range of values exists to speed up the search for solutions. In [ 60 ], the plague operator was used to enhance the performance of parallel GP (based on the island model). Individuals were removed every generation, altering the population size. This compensated for the increase in size of individuals and hence saved computational effort. Changing population size was also investigated for distributed GP in [ 196 ] to reduce bloat. It was shown that by keeping their size as small as possible and the amount of resources needed was decreased. There have been many other approaches in par-allelizing GP [ 48 , 110 , 169 , 177 , 204 , 222 ]. An extensive survey on the subject can be found in [ 224 ]. 5.3 Graph genetic programming In graph genetic programming (GGP) system the GP operates on graphs. In [ 166 ], the notion of graph isomorphism was discussed and it was empirically shown how using a canonical graph indexed database (fitness database) can improve the performance by reduc-ing the number of fitness evaluations and thus saving considerable evaluation time. 5.4 Cartesian genetic programming A new form of GP called Cartesian genetic programming (CGP) was introduced in [ 159 ] in which programs were represented as indexed graphs (rather than as parse trees), encoded in the form of a linear string of integers. In [ 237 ], the CGP programs were represented as directed acyclic graphs (DAGs), enabling outputs from previous computations to be reused. An implicit context representation for CGP was described in [ 28 ] showing the beneficial effects of recombination to outperform the conventional Cartesian GP. The computational efficiency of graph-based Cartesian Genetic Programming was described in [ 160 ]. The Cartesian genetic programming was extended by utilizing automatic module acquisition in [ 227 ]. 5.5 Page-based genetic programming Page-based GP [ 170 ] is a linearly structured GP (L-GP), where individuals take the form of a  X  X inear X  list of instructions. A Page-based linear GP was proposed in [ 86 , 87 ]where individuals were described in terms of a number of pages. It was shown that page-based linear GP evolves solutions better than the block-based linear GP [ 173 ]. 5.6 Other representations The performance of GP was improved by using a data structure coded by binary decision dia-grams (BDDs), reducing storage requirements and accelerating the fitness calculation [ 245 ]. BDDs are a compact representation of Boolean functions using directed acyclic graphs. The entire population was stored as a shared BDD and all genetic operations and fitness calcu-lations were performed on the BDD. This technique is suitable for problems where only Boolean variables and functions are involved. BDD-based GP is not practical for problems where real variables, such as symbolic regression are used. Nevertheless, it can also be used for integer-based programs by encoding the integers as binary vectors. New crossover, mutation and evaluation algorithms were developed for BDD [ 245 ].

Linear GP, which makes use of linear phenomena and resembles conventional GA with the exception of the chromosome length being allowed to evolve, is also generally used as an alternative representation to the tree based GP. Some of the works that have used linear GP are [ 24 , 25 , 64 ]and[ 26 ], to name a few.

A technique to reduce the time and space requirements of GP was proposed in [ 82 ]. The population of parse trees was stored as a directed acyclic graph (DAG). However, it was stated that this technique cannot be applied to all problems due to restricted program encoding and bounded fitness cases, such as the Artificial Ant and Cart Centering problems. The number of nodes stored and evaluated was reduced by a significant factor resulting in less space requirements to store a population of computer programs. In addition, time savings were also observed as a result of caching. In the standard sub-tree crossover it is difficult to make changes near the root, occasionally causing runs to become trapped in local maxima. Based on these structural limitations a different tree representation, AppGP, was proposed [ 68 ]. The representation of trees and the tree manipulation algorithms were modified in AppGP. All non-terminal nodes were represented as application (APP) nodes and the AppGP represen-tation had more nodes than the standard GP representation, providing more potential points for the application of recombination operators. It was shown that on all of the test problems, AppGP did no worse than standard GP, and in several instances it outperformed standard GP. 5.7 Memetic algorithms: hybrids Researchers have attempted to capitalize upon the strategies of other methods by incorpo-rating them into an enhanced version of GP that outperforms the canonical GP. The GA-P [ 93 ], which is a genetic algorithm and genetic programming hybrid, performed symbolic regression by combining the conventional GA function optimization strength with the GP paradigm to evolve complex mathematical expressions. The GA-P was extended in [ 205 ]. A genetic algorithm (GA) was embedded into a genetic programming (GP), where each paradigm operated at different levels within the problem domain [ 29 ].

The genetic programming paradigm was hybridized with statistical analysis in [ 1 ]to derive systems of differential equations. A framework for combining GP and inductive logic programming (ILP) was proposed in [ 234 ], which is another form of GGGP. A memetic algorithm was proposed in [ 27 ] evolving heterogeneous populations, in which a GA was used to optimize the numeric terminals of programs evolved using GP. The GP algorithm was hybridized with hill climbing and the nature of new crossover algorithms, crossover hill climbing (XOHC) and crossover with simulated annealing (XOSA), was investigated [ 176 ]. It was shown that the hybrids offer added search power and hybridizing GP with hill climbing yields better results than the standard GP. 6 Conclusions An overview has been provided of the research work done to improve the performance of Canonical Genetic Programming based on parse trees. The various techniques of existing approaches to improve performance were briefly discussed, with an attempt to classify these proposed methods into various categories. The improvements made to the GP were catego-rized based on whether the improvements pertained to the various components of GP or if they contained some innovative ideas enhancing GP X  X  performance or whether they resolved known issues highlighted within GP. Each of these categories was then further subdivided as appropriate.

A variety of modifications to the crossover operator have been introduced to reduce the destructive nature of crossover. There have been diverse investigations to discover methodol-ogies for controlling and setting GP parameters to enhance effectiveness. The issue of bloat and code growth has been given a lot of attention. Many researchers have proposed different representations to canonical GP and thereby have departed from the traditional tree based GP, inventing their own GP variants. In addition, hybrids involving other methods such as GA and Neural Networks have been successfully implemented.
 Although there exist a multitude of proposed methods for improving various aspects of GP, it should be noted that no single  X  X est X  method can exist to improve the performance of GP. Some promising areas of future research, according to the authors X  opinions, are outlined as follows. Further investigations into the issue of diversity may still be worth pursuing in conjunction with performing some new systematic operation to remedy the lack of diversity. Examination of different measures for stagnation and premature convergence could be most promising together with newly invented methodologies to either bring the population out of stagnation or abruptly terminating a run and commencing a new run with the gained experi-ence and knowledge of the previous run, i.e. incorporated memory. A complete theoretical analysis of evaluating and measuring difficulty in GP could be pivotal. An understanding of how to highlight and flag sub-trees that are the main reason for success of the individual is crucial and new operators could be used to expose these highly competitive genes by keeping an updated genes library and incorporating them into the new individuals. Little research has been conducted in the field of implicit adaptation or self-adaptive control, i.e. modifications and parameter control are encoded as a genome and are evolved implicitly with the individ-ual. The authors believe that further breakthrough successes may be achievable by exploring self-adaptive control. It should, however be noted that such research has been conducted in GA. This suggests that possible future advances in GP may stem from examining other similar fields. In addition, using the insights from other similar fields such as theoretical population genetics and evolutionary biology may help extend GP approaches.
 References Author Biographies
