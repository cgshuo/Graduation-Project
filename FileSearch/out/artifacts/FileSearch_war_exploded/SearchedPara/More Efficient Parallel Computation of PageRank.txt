 Categories and Subject Descriptors: H.3.3 Information Search and Retrieval: Information filtering General Terms: Algorithms, Experimentation, Performance, Theory.
 Keywords: Web graph, Power ite ration, Pagerank. The first order, homogeneous, linear recurrence w n +1 = Aw n + b occurs in various settings. When A 1 &lt; 1, it is well-known that w n = A n w 0 + P urally when computing PageRank [4] via power iteration. Specifically, given a web graph matrix, M  X  0, with  X  X or-malized X  columns (i.e., each column sums to 1), a (nor-malized) personalization vector, v  X  0, and a teleporta-tion probability, , define the perturbed Markov matrix, M v, =(1  X  ) M + vJ ,where J is a row of 1 X  X . Power it-eration takes an arbitrary, normalized initial vector, v 0 computes r n +1 = M v, r n ,with r 0 = v 0 ,andterminates when r n  X  r n  X  1 1 &lt; X  .Since M v, and v 0 are normalized, so is r n ,  X  n . This implies that r n +1 =(1  X  ) Mr n + which is just the linear recurrence from above with A = (1  X  ) M and b = v . Therefore, r n converges to r = is the unique positive, normalized eigenvector of M v, with eigenvalue 1, which is the usual definition of PageRank.
If we instead take the unnormalized initial vector, v 0 = the partial sums, r n = quence converging to r . This sequence can be computed by the pair of recurrence equations: t n +1 =(1  X  ) Mt n and r n +1 = r n + t n ,with t 0 = r 0 = v .Since M, v  X  0, the termination condition becomes simply  X &gt; r n  X  r n  X  1 1 t n 1 = J [(1  X  ) M ] n v = (1  X  ) n . We refer to this modi-fied algorithm as GeoRank . The computationally intensive step in both GeoRank and power iteration is the matrix-vector multiplication, Aw , of the recurrence.

Kamvar et al. [2] have observed that M is sparse, and when pages are grouped by top-level domain (TLD) name, the matrix is almost block diagonal, where the blocks cor-respond to TLD X  X . Kohlsch  X  utter et al. [3] represent the web graph as a block-structured matrix with relatively few large blocks, by merging groups of TLD blocks together. They exploit this block structure to distribute the computation-ally intensive multiplication in each step of power iteration among a small number of processors.

To illustrate, suppose for simplicity that w is partitioned into three segments, w j ,and A is partitioned into 9 cor-responding blocks, A i,j , i, j =0 ,..., 2. Three processors may then compute w = Av , as follows. Processor j stores A i,j and w j , computes  X  w i,j = A i,j w j , i =0 ,..., 2, sends  X  w i,j , i = j to processor i , and accumulates the results, w technique with power iteration to obtain a  X  X arallel X  compu-tation of PageRank. Their key contribution was to observe that, since the off-diagonal blocks are sparse, the segments transmitted among the processors are sparse vectors.
Since they are computationally equivalent, differing only in initial condition, we use the corresponding distributed version of GeoRank as a proxy for Kohlsch  X  utter et al. X  X  algorithm in our experiments. As we will see, while these al-gorithms are distributed, they are not truly parallel, in that they do not scale particularly well as the number of proces-sors increases. We present a modified algorithm, FastRank , which scales more efficiently.
Assume that M is partitioned into blocks, as described above, where each partition corresponds to a union of TLD X  X . Define M 0 to be the block-diagonal matrix consisting of the diagonal blocks of M ,andlet M 1 = M  X  M 0 .Thatis, M 0 consists (primarily) of links within any given TLD (in-tralinks), while M 1 consists entirely of links between TLD X  X  (interlinks). Multiplication by M = M 0 + M 1 is effectively multiplication by M 0 plus multiplication by M 1 .Thefor-mer can be performed in parallel, since M 0 is block-diagonal. While the latter can be distributed using the technique de-scribed above, this computation is not truly parallel. In par-ticular, runtime does not decrease as 1 / (# of processors). Although the time to perform each block multiplication de-creases, the amount of data sent and received by each pro-cessor actually increases! Hence, we arrive at the main idea of this poster: By reducing the number of M 1 multiplications relative to M 0 multiplications, we can increase the amount of computation done in parallel, thus obtaining a more effi-cient algorithm to compute the PageRank vector, which we cal l FastRank .

Expanding powers of [(1  X  ) M ] j =[(1  X  )( M 0 + M 1 )] j we may express r = product of , v , and a sum over words in M i  X  (1  X  ) M i other words, r = dominates M 1 , the terms with fewer M 1 factors dominate the sum. Now we group terms according to the number of M 1 factors, using the fact that 1 M sum over arbitrary length words in M 0 only. Since  X  r 0 is precisely Kamvar X  X  BlockRank [2], Equation 1 illustrates nicely how BlockRank approximates PageRank.
Like GeoRank , FastRank halts when | t n | 1 &lt; X  .How-to a desired tolerance. Since 1  X  M  X  0 M 1 magnifies errors in t by at most a factor of 1  X  ,ateachstepweapply Geo-Rank with a tolerance of 1  X   X  . More precisely, in our dis-tributed implementation, we perform GeoRank in parallel
Since the actual error magnification factor is most likely much smaller, this is probably an unnecessarily stringent termination condition. We have yet to obtain theoretical er-ror estimates, but in experiments these stopping conditions achieved the desired accuracy. We expect that better error analysis will lead to more appropriate stopping conditions, fewer multiplications, and even faster performance.
To see how FastRank and GeoRank compare in prac-tice, we used the (decompressed) version of Stanford X  X  web graph (http://webgraph.dsi.unimi.it/), from a 2001 crawl as part of its WebBase project. This graph has on the order of 10 8 nodes and 10 9 links. We re-indexed the pages so that those within common TLD X  X  were contiguous. We im-plemented both algorithms as distributed systems with one master and k slaves. The (normalized) web graph matrix and ranking vector were partitioned to respect TLD X  X , in the manner of the example in Section 1, so that the num-ber of links assigned to the j th slave, approximately equal across slaves. 1 We used a uniform dis-tribution for v , i.e., v i = 1 dim M ,  X  i .
 In the following experiments, each slave ran on a different Apple PowerPC G5 (3.0) with dual 2GHz processors and 2G of RAM, running OSX Server 10.4.7 in the Brown Internet Lab. Timings are in CPU-seconds and do not include time spent in loading the initial graph partitions from disk nor writing the final rankings to disk.

First, consider a typical run of each algorithm with k = 20 slaves. Each partition was roughly 6  X  10 6 dimensional and, on average, nnz ( M j,j )  X  5  X  10 7 , while nnz ( M 10 5 , which confirms that M 0 dominates M 1 . Likewise, on average, nnz ( s i )  X  3  X  10 5 , which confirms the sparsity of transmitted vectors noticed by Kohlsch  X  utter et al. [3]. FastRank converged at  X  =10  X  3 by i = 3 in 265 sec. Each GeoRank call took, on average, 2.2 sec./mult., with 41 M 0 multiplications at i = 0, 24 multiplications at i =1, 11 multiplications at i = 2, and 2 multiplications at i =3. Since t i  X  0 quickly, the number of M 0 multiplications decreases rapidly. The remaining computational time was nnz ( M ) = number of non-zero entries of M spent performing just 3 M 1 multiplications (i.e., computing s ). These calculations include not only matrix multiplica-tions, but buffer allocation, socket i/o, and sum accumula-tion, as well; in total, this took, on average, 29 sec. iteration. In comparison, GeoRank converged by i = 31 in 641 sec. Thus, it required 31 multiplications by M ,eachofwhich took, on average, 20 sec. While the total number of mul-tiplications is less for GeoRank (effectively, 31 multiplica-tions each by M 0 and M 1 )than FastRank (78 by M 0 and 3 by M 1 ), GeoRank required many more M 1 multiplications, and thus took significantly longer.

The average time to multiply by M (20 sec.) was less than multiplication by M 0 and M 1 (31.2 sec.), only because the time spent by GeoRank doing buffer allocation was amor-tized over a greater number of iterations. Corresponding individual timings were, in fact, comparable.

Table 1 shows how the two algorithms compare as we vary the number of slaves with  X  fixed at 10  X  3 .Thenumbersof M 0 , M 1 ,and M multiplications were roughly independent of k . Notice that time per M 0 multiplication continued to decrease as k increased, since the size of dim M j,j decreased and these multiplications were done in parallel. However, as the time spent in data transmission began to dominate, the cost of M 1 and M multiplications leveled off. Thus, FastRank outperformed GeoRank at an increasing rate.
By employing sufficiently many slaves, FastRank enables us to efficiently compute the PageRank vector. Coupled with the compression techniques of [1], it may be feasible to do so for much larger graphs, such as the Deep Web [5]. Further numerical analysis is needed to determine precise error estimates for FastRank . As indicated at the end of Section 2, this may lead to even better performance.
