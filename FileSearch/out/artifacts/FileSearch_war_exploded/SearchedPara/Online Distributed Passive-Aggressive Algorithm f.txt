 Structured learning [1] becomes a popular research topic recently. In the situation of structured learning, the labels of the data are not independent from each other, instead they form a mutually associated structure.
 since structured learning algorithms are usually polynomial complexity in the number of labels. The larger the label set is, the more complex the algorithm will be. Tradi-tional methods reduce the search space of the structural labels by pruning to improve computation efficiency [2]. However, these methods have a more or less performance loss.
 We construct a parallel version of standard PA algorithm via weights averaging strategy. We also give a theoretic proof of the convergence of the training process and show that the distributed algorithm gives the same cumulative loss upper bound as the standard PA algorithm.
 ter accuracy than the standard PA algorithm [3] with less training time. If the standard PA algorithm suffers from a scalability problem in both memory space and computa-tional time when the size of a dataset is too large, our distributed PA algorithm will overcome these difficulties with less space and time using a parallel strategy and just a few machine nodes.
 essary background in the Passive-Aggressive algorithm in Section 2. Then we describe in detail our implementation of the distributed PA algorithm in Section 3 and give a the-oretic analysis in Section 4. Finally, we report our results and analysis of experiments. The online algorithms do not define an object function on the entire sample set because they need not obtain all samples at once. They update the parameters only depending on the observing sample. Perceptron algorithm [4] is a famous online algorithm, which perceptron, it is guided by the margin maximum idea.
 for various prediction tasks [3]. The difference between this algorithm and perceptron is that PA uses the margin of the samples to update the current classifier. Online PA algorithm updates the weights of features by solving a constrained optimization prob-lem. It requires that the updated weights must stay as close as possible to the previous weights and on the other hand the updated weights correctly classify the current exam-ple with a sufficiently high margin. [6] applied this algorithm to the dependency parsing task which is a typical structured learning problem.
 f 0 , 1 g d as a feature vector of d dimensions on the sample ( x , y ) . The value in each dimension of the  X  ( x , y ) is a 0 1 value which indicates whether this feature occurs in the current sample. w 2 R d is the parameter vector of the classifier and each dimension of it is the weight of one feature. In order to learn a proper weight vector w online PA algorithm adopts an iterative method until the convergence of w or the number of loops exceeds the predefined maximum iteration number.
 we define the margin is: From above the margin is positive if and only if all of the relevant labels are ranked higher than all of the irrelevant labels. However online PA algorithm is not satisfied by a mere positive margin as it requires the margin of every predication to be at least a loss classifier gives such a prediction is defined as: On round t online PA algorithm sets the new weight vector w t +1 to be the solution to the following constrained optimization problem. where C is a positive parameter which controls the influence of the slack term on the objective function.
 satisfying the following set of linear constraints. then the large enough margin between the correct labels and the other labels is guaran-teed. Solving this constraint problem leads to a parameters update formula, here, Here C is a positive parameter which controls the influence of the slack term on the objective function. In order to prevent noise samples to make the  X  t too large to depart from the classification face, so  X  t is forced to be smaller than C . In the other word T are absolutely separable because such u exists. So from Eq. 3 for jj
 X  ( x , y ) jj 2 6 R/ 2 is satisfied. Then [3] proves the cumulative squared loss of PA algorithm on this sequence of examples is bounded by Since the online algorithms update the parameters only depending on the observing sample, there is a straight-forward way for distributed training.
 domly. After dividing samples, all pieces are evenly assigned to different cores 1 . Each core trains the parameters with PA algorithm on its assigned samples. After all cores finish training, a summarization system averages the S parameter vectors to get a new parameter vector and send it to each core for the next training iteration. The above process is repeated until the parameters converge.
 duce [7] framework. The Mapper process trains parameters with each piece of samples using the Passive-Aggressive algorithm and transfer these parameters to the Reducer. The Reducer is responsible for gathering these parameters, averaging them and finally sending them to each Mapper.
 Algorithm 2,  X  i,n is a distribution which averages the parameters vector returned by each classifier. w ( avg,n 1) is the initial parameters vector on round n . 3.1 Parameters Averaging Strategy An unsolved problem is how to mix the parameters in each iteration. In other words, we need assign a proper value of  X  i,n for each piece i .
 form distribution, ing X  strategy, if a machine or core is arranged to deal with a more difficultly-treated sample portion than others, the parameters obtained from it should be more important. The updating speed will be dragged down due to uniform averaging strategy. Therefore, we wish the touchy parts to contribute more in parameters updating. In the other word, we want to increase the influence of the parameters trained from those pieces with more errors. So we use the following formula to average the parameters from all portions. where  X  i,n is defined as the number of wrong classified samples in the i th training portion on the n th round. We also need to estimate the cumulative loss produced by the distributed Passive-Aggression algorithm.
 introduce a notation  X  X  =  X  ( x , y )  X  ( x , ^ y ) .
 final classification hyperplane produced by the classifier is u 2 R d then we denote the between the square of the distance w t apart from u and the square of the distance w t +1 apart from u on round t ,  X  = 0 is satisfied for any sample. Then from above, So the upper bound of the loss of a sample is determined by the difference of the pa-rameters and the final classification face u before and after each round.  X  inequality.  X  2 is denoted as jj w ( avg,n ) u jj 2 , then So the cumulative loss in N iterations is so Refer to the section 2 we can find that the online distributed Passive-aggression algo-rithm has the same cumulative loss upper bound. We verify the efficiency of our method with joint segmentation and POS-tagging prob-lem, which is a common task in natural language processing (NLP). Structured learn-ing methods are usually adopted to solve this problem nowadays but suffers from long training time.
 for natural language processing.
 are 37 kinds of POS tags in this corpus. We use cross-label method for this task. The cross-label consists of two parts: the first part is the label of segmentation, and the second part is the label of POS tag. We use four segmentation labels ( X  X  X ,  X  X  X ,  X  X  X ,  X  X  X ) to represent the beginning, middle, end of a word and a single character word respectively. For example, a label  X  X -NN X  means the beginning of a noun word. After processing the corpus, we get 108 different cross-labels.
 common for the segmentation and POS-tagging task.
 used for testing. We split the training set into 2 , 5 , 10 , 20 and 50 pieces randomly and train each piece in each machine or core.
 ods. In this experiment we split the entire sample set into 2 pieces and 5 pieces respec-tively and inspect the training errors at each iteration.
 same for both 2 pieces and 5 pieces training strategy. This is because training corpus is randomly and uniformly split so the probability of hard tagging sentences gathering in a single piece is rare. So the number of training errors of each machine is very close. The difference between the two averaging strategies is very small in this situation. However a better averaging strategy maybe speeds the process of training up.
 experimental settings. It shows that our algorithm can increase the speed of the training without accuracy loss.
 online PA algorithm. As the result from Table 2 the distributed algorithm get approxi-mate or even better result of the standard PA algorithm.
 property of the normal online PA algorithm with our distributed implementation. Figure 2 displays relations between the training precision and the number of iterations under different pieces of samples. We set the maximum iterations to be 150 . We can see that it needs the more iterations to convergence as the number of pieces increases. However, since each piece use less time for the larger number of pieces, we can reduce the training time in total.
 as the time goes on. In this section we ignore the time of network I/O operations and only consider the accumulated CPU time. The comparisons are shown in Figure 3. Ap-parently, the distribution PA algorithm is faster than the original algorithm to achieve the same test precision. The parallel PA algorithm is also faster to converge with com-parable precision to the normal PA algorithm. We can see that the test precision of distributed PA algorithm is a little higher. 5.1 Experimental Discussion There are some things to be worth noting from our experiments.
 the standard one mentioned above. We suspect this happens for the season that the distributed method is a form of parameter averaging which has the same effect as the average perceptron [5]. Although training time is dramatically increased after we add tributed algorithm. Maybe this happens because the original version overfits the training data.
 testing data is almost the same to converge and to achieve the best performance. This is shown in Figure 3 that lines in the left up corner cover each other. Further consid-ering the network I/O waste, an unlimited split of the sample set is undesirable. The segmentation with 10 pieces is a better choice for POS tagging in our paper. The distributed machine learning is attracting increasing attention in recent years. For example, Mahout 2 is a famous package of scalable parallel machine learning libraries based on MapReduce [7] framework. The distributed computing framework also pro-vides a new way to deal with the large computation cost in structured learning. is easily applied to many different learning algorithms. They have also investigated parallel implements of many batch algorithms and modified these algorithms in the map-reduce framework. For online algorithms, parameters updating process is a serial procedure. In order to guarantee the accuracy and convergence of online algorithms we need a particular strategy to make the serial process parallel.
 which reduces memory use through performing a row-based, approximate matrix fac-torization.
 their implementation every node only interacts with parameters relevant to its data and sends messages to other nodes along a junction-tree topology.
 nation mechanism among processors. However, there is no detailed theoretic discussion in their paper.
 ceptron implementation. Their work is very similar to ours, but PA and Perceptron are different online algorithms. So the theoretic proofs of the convergence are actually quite different. In this paper we propose a distributed training strategy with the online PA algorithm for structured learning. We guarantee its convergence with the proposed averaging strategy. out decreasing the accuracy. Although the number of iterations increases, less time cost in each iteration also make the training time still less.
 such as parsing. We would like to thank the anonymous reviewers for their valuable comments. This work was funded by NSFC (No.61003091 and No.61073069).

