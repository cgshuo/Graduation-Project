 We prop ose and test an objectiv e criterion for evaluation of clustering performance: How well does a clustering algo-rithm run on unlab eled data aid a classi cation algorithm? The accuracy is quan ti ed using the PAC-MDL bound [3] in a semisup ervised setting. Clustering algorithms whic h nat-urally separate the data according to (hidden) lab els with a small num ber of clusters perform well. A simple extension of the argumen t leads to an objectiv e mo del selection metho d. Exp erimen tal results on text analysis datasets demonstrate that this approac h empirically results in very comp etitiv e bounds on test set performance on natural datasets. Categories and Sub ject Descriptors: I.5.3 [Pattern Recog-nition]: Clustering General Terms: Algorithms, Exp erimen tation Keyw ords: Clustering, Evaluation, PAC bounds, MDL
Clustering is perhaps the most widely used exploratory data analysis tool using in data mining. There are man y clustering algorithms that have been prop osed in the liter-ature based on various requiremen ts. Eac h one optimizes some unsup ervised internal qualit y measure suc h as min-imizing the intra-cluster distances, maximizing the inter-cluster distances, maximizing the log-lik eliho od of a para-metric mixture mo del, minimizing the cut-v alue of a pair-wise similarit y graph, etc. The di erences between these internal qualit y measures mak e it practically imp ossible to objectiv ely compare clustering algorithms. Due to the lack of an objectiv e measure, choosing the \righ t" algorithm for a particular task is confusing.

We prop ose a metho d whic h eliminates some of these con-fusions in some settings. Clustering is often used as an in-termediate exploratory data analysis step for a prediction problem. Hence, the answ er to what is a good clustering algorithm for my problem? often dep ends on how much pre-diction power the clustering step pro vides, or, more directly , what is the predictive accuracy of a clustering algorithm? In other words, since clustering is often used as a metho d for gaining insigh t into a dataset, our metho d here evaluates the degree to whic h clustering aids the understanding of a dataset for the purp ose of classi c ation . On natural semi-sup ervised learning datasets, if the clustering \agrees well" with a set of hidden lab els using a small num ber of clusters, we say that the clustering algorithm is good for prediction. The precise de nition of \agrees well" is mediated by the PAC-MDL bound [3] on the accuracy of a test set. Our prop osed criterion has sev eral natural prop erties: 1. It applies to every clustering algorithm. 2. It is inheren tly normalized to the interv al [0 ; 1]. 3. All possible values are exercised on natural datasets. 4. The metric can exibly incorp orate prior information by prop er design of a description language . 5. It can be used for mo del selection. 6. It is directly related to a concrete goal (go od prediction). It is imp ortan t to note that this is not the only possible objectiv e criterion for evaluation of clustering algorithms. Clustering algorithms may be used for other purp oses, and when so used, other measures may be more appropriate. Our results here are only directly applicable when the goal in clustering is related to prediction.

The rest of the pap er is organized as follo ws. First, we explain the PAC-MDL bound [3] in section 2. In section 3, we discuss how the PAC-MDL bound can be applied to a clustering setting for performance evaluation and mo del se-lection. We presen t exp erimen tal results on benc hmark text datasets in section 4 to demonstrate the prop osed approac h. Related work is presen ted in section 5. We end with a dis-cussion on the prop osed criterion in section 6.
The PAC-MDL bound is the core mec hanism used to trade o between a sucien tly rich represen tation to capture the data and over-tting on the data. Clustering algorithms with a small bound must have a small num ber of clusters whic h agree well with a set of (hidden) lab els.

Consider the follo wing learning setting: Let D be any distribution over ( X; Y ) where X denotes the input and Y denotes the lab el. We assume Y can tak e one of ` &gt; 1 possible values, i.e., Y 2 f 1 ; ; ` g . Consider a train set S = ( X m ; Y m ) and a test set S 0 = ( X n ; Y n ), where ( X denotes the set of i indep enden tly dra wn samples from the (unkno wn) join t distribution D over ( X; Y ). The PAC-MDL bound applies to a transductiv e learning algorithm, T : ( X Y ) m X n 7! , where : X m + n 7! ^ Y m + n is a transductiv e classi er whic h pro duces a sim ultaneous la-beling ^ Y m + n for all of the data points X m + n . Giv en the description complexit y of the transductiv e classi er mea-sured by its bit description length j j , and the prediction error coun t on the train set, ^ S = jf Y m 6 = ^ Y m gj , the bound limits the error coun t on the test set ^ S 0 = jf Y n 6 =
The precise bound is de ned in terms of a cum ulativ e hy-pergeometric distribution. To understand this distribution, imagine a buc ket with m red balls and n blue balls, from whic h ( a + b ) balls are dra wn without replacemen t. Now, de ne Buc ket( m; n; a; b ) to be the probabilit y that at least b blue balls are dra wn. That is, The bound is actually de ned in terms of a \worst case" over the value of b de ned according to: Thus, for any b &gt; bmax( m; n; a; ), if ( a + b ) balls are dra wn out of the ( m + n ) balls, the probabilit y of getting at least b blue balls is less than . In the PAC-MDL bound, a plays the role of ^ S , the num ber of errors on the train set, and b plays the role of ^ S 0 , the num ber of errors on the test set, whic h is exactly the num ber we wish to bound.
 Theorem 1 (PAC-MDL bound [3]) For any distribution D , for any label description language L = f g , with proba-bility at least (1 ) over the draw of the train and test sets Intuitiv ely, the theorem says that if a transductiv e classi er with a short description length achiev es few errors on the train set, then the num ber of errors on the test set is small with high probabilit y. For details on this theorem, its pro of, and its connection to other PAC bounds, see [3]. For now, it is imp ortan t to note that the applicabilit y of this theorem is only limited by the assumption that the train and test sets are eac h dra wn indep enden tly from the distribution D , and that L = f g is a \valid" description language. In this section, we discuss the application of the PAC-MDL bound to clustering. Tw o imp ortan t issues are rele-vant for clustering bounds: A. How does a clustering algorithm pro duce a prediction? B. What is a \valid" description language for transductiv e For A, recall that the PAC-MDL bound is applicable to a transductiv e classi er. It turns out that any clustering can be con verted into a transductiv e classi er. Giv en the en-tire train set ( X m ; Y m ), and X n from the test set, consider X m + n as the input to the clustering algorithm. Say the clustering algorithm partitions X m + n into k subsets. To nd the lab els ^ Y m + n on all points, rst compute the most common lab el in eac h cluster using Y m . Then, i. if the most common is of class i; i 2 1 ; ; ` , lab el all ii. if there is a tie between two or more class lab els, pick iii. if there are no lab eled points in a cluster, choose a lab el After the assignmen t of the new lab els, all points in eac h cluster have the same lab el.

For issue B above, note that the description of a clas-si er can be considered a binary code that con tains all the relev ant information for generating the lab els ^ Y m + n formally speaking, there is a Turing mac hine that tak es as an input, pro duces the lab els ^ Y m + n as output and halts. Therefore, the description language L = f g must be a pre x-free code (by the halting prop erty) and hence sat-isfy Kraft's inequalit y (see [4], Theorem 5.2.1, Lemma 7.3.1, for details), i.e., P 2 L 2 j j 1. This is the only condition a lab el description language L = f g must satisfy for the prop osed bound to hold.
Consider the problem of clustering a dataset X m + n , where m is the train-set size and n is the test-set size. For any xed 1 clustering algorithm, we can construct a description language L = f g by letting eac h description have a con-stan t lab el on eac h cluster, as discussed earlier. Giv en the clustering, this description is sucien t to generate the new lab els ^ Y m + n over the entire data. Now, if c is the num ber of clusters and ` is the num ber of lab els, then the set of descriptions, i.e., the language L = f g , has size at most ` whic h can be indexed using only j j = c log ` bits (\frac-tional bits" are ok here). Since j L j ` c , it is straigh tforw ard to see that Kraft's inequalit y is indeed satis ed, and hence L is a valid description language. On a more general note, if one assigns a probabilit y measure p ( ) to all 2 L , since j j = log 1 p ( ) alw ays gives a valid bit description com-plexit y for . Here, since j L j = ` c , we have essen tially as-signed a uniform probabilit y of p ( ) = 1 ` c ; 8 2 L .
We call the above description language Simple . Using a direct application of the PAC-MDL bound, we get: With probabilit y at least (1 ) over the dra w of the train and test sets S; S 0 D m + n , the test-set error is bounded by:
In practice, clustering algorithms are highly dep enden t upon random initializations, so the algorithm is run multi-ple times with the best 2 performing run chosen. Note that the description length of this scheme is higher since the de-scription language must specify whic h random initialization to use. If the best initialization is chosen from r random initializations, the description length is j j = c log ` + log r . We call this language Init . Applying the PAC-MDL bound for Init , we get with probabilit y at least (1 ) over the dra w of the train and test sets:
We mean xed initialization and xed cluster num ber here. \Best" migh t be de ned with resp ect to some algorithm-speci c metric or with resp ect to bound performance, de-pending on what you want to evaluate.
Most clustering algorithms need the num ber of clusters as an input to the algorithm. The PAC-MDL bound pro vides a natural mec hanism for cluster num ber selection when some lab el information is available. Consider running a cluster-ing algorithm on a dataset over a range of cluster num bers and picking the cluster num ber with the tigh test bound on the test-set error. This pro cess increases the size of our set of descriptions again. We use a description language for the cluster num ber c requiring log c ( c + 1) bits. This is \legal" because it does not violate the Kraft inequalit y since: P 1 c =1 1 c ( c +1) = 1. One sligh t optimization is pos-sible here: the nature of our metric disallo ws the c = 1 case, implying that we can substitute c ! c 1. With this description language, the length of our description is j j = c log ` + log r + log( c ( c 1)) bits. We call the language Cluster . Applying the PAC-MDL bound for Cluster , we get that with probabilit y at least (1 ) over the dra w of the train and test sets, the optimal cluster num ber c achiev es a test-set error of:
In practice, for a given dataset, it is not clear whic h clus-tering algorithm is appropriate for use. Typically an algo-rithm is chosen using domain kno wledge, and there is nor-mally no objectiv e way to verify whether the choice made was good or bad. To cop e with this, we can extend our description language to specify one of multiple algorithms. More precisely , if we are choosing the best among s cluster-ing algorithms, an extra log s bits are required to send the index of the clustering algorithm that performs the best. Hence, j j = c log ` + log r + log(( c 1) c ) + log s . Being the best over all algorithms, we call this language Algo . Ap-plying the PAC-MDL bound, we see that with probabilit y at least (1 ) over the dra w of the train and test sets, the best algorithm with the optimal cluster num ber and optimal initialization achiev es a test-set error of:
We presen t an empirical study of the prop osed evalua-tion technique on the problem of text clustering for sev eral benc hmark datasets using various algorithms.
The datasets that we used for empirical validation and comparison of our algorithms were carefully selected to rep-resen t some typical clustering problems: (a) classic3 is a well kno wn collection of documen ts that con tains 3893 doc-umen ts, among whic h 1400 Cranfield documen ts are from aeronautical system pap ers, 1033 Medline documen ts are from medical journals, and 1460 Cisi documen ts are from information retriev al pap ers; (b) classic2 is a subset of 2860 documen ts from the classic3 collection formed with the 1400 Cranfield documen ts and the 1460 Cisi documen ts; (c) cmu-newsgroup-clean-1000 , or, the CMU 20 newsgroups dataset is a widely used text analysis dataset that is a collection of appro ximately 20,000 messages from 20 di eren t USENET newsgroups, with appro ximately 1000 messages per group; (d) cmu-newsgroup-clean-100 was formed by sampling 100 messages per group from the full 20 newsgroup dataset; (e) cmu-di erent-1000 is a subset of the original 20 news-groups dataset consisting of 3 groups on very di eren t topics: alt.atheism , rec.sp ort.b aseb all , sci.sp ace ; (f) cmu-di erent-100 is a subset of (e) formed by sampling 100 documen ts per topic; (g) cmu-simila r-1000 is a subset of the original 20 newsgroups dataset consisting of 3 groups on similar topics: talk.p olitics.guns , talk.p olitics.mide ast , talk.p olitics.misc ; (h) cmu-simila r-100 is a subset of (g) formed by taking 100 docu-men ts per topic; (i) cmu-same-1000 is a subset of the original 20 newsgroups dataset consisting of 3 groups on the same topic viz computers, with di eren t subtopics : comp.gr aphics , comp.os.ms-windows , comp.windows.x ; (j) cmu-same-100 is a subset of (i) formed by sampling 100 documen ts per topic; and, (k) yaho o , or, the Yaho o News (K-series) dataset that has 2340 Yaho o news articles from 20 di eren t categories.
We exp erimen t with 6 algorithms that have been applied to text datasets with varying degrees of success. Since the motiv ation behind the exp erimen ts is to establish the e-cacy of the prop osed criterion in evaluation, comparison and mo del selection for clustering, we have not tried to be ex-haustiv e in the list of algorithms that have been used. How-ever, we have chosen algorithms that represen t the state-of-the-art and have been applied to text clustering in the liter-ature. The algorithms we consider are: SPKMeans [6], better kno wn as spherical kmeans, that emplo ys the widely used co-sine similarit y; FSKMeans [2], a frequency sensitiv e version of spherical kmeans; Hard-moVMF [1], a generativ e mo del based clustering that uses a mixture of von Mises-Fisher (vMF) distributions to mo del the data; Soft-moVMF [1], that also uses a mixture of von Mises-Fisher distributions to mo del the data with soft-assignmen ts that are nally con verted to hard assignmen ts by the standard metho d assigning a data point to the highest probabilit y cluster; KMeans [9], the stan-dard kmeans clustering algorithm; and KLKMeans [5], better kno wn as information theoretic clustering, that uses KL-div ergence between L 1 normalized documen t vectors instead of squared Euclidean distance in the kmeans framew ork.
Before performing any exp erimen ts, an indep enden t train-test split needs to be made. All exp erimen ts rep orted in this pap er were performed on 5 di eren t train-test splits: 10-90, 30-70, 50-50, 70-30, 90-10. On eac h train-test split, we per-formed 4 sets of exp erimen ts for eac h dataset corresp onding to the 4 bounds discussed in section 3: (a) Exp erimen ts for a particular algorithm, with a xed (b) The best results for a particular algorithm with a xed (c) The best results for a particular algorithm, where the Figure 1: Test set error rate bounds for KMeans and SPKMeans on cmu-di erent-1000 and cmu-same-1000 : 10 runs with di eren t initializations for 3 clusters (d) The best performance for a given dataset, where the Now we are ready to presen t results on the various datasets. We start with the simplest language and using (1) presen t represen tativ e results comparing individual runs of a par-ticular algorithm for a xed cluster num ber with di eren t random initializations (Fig 1). Taking the best over 10 ini-tializations for eac h cluster num ber, we then compare per-formance of a particular algorithm over a range of cluster num bers (Fig 2) using (2). Next, by taking the best over the entire cluster num ber range considered, we compare the performance of various algorithms (Fig 3) using (3). Fi-nally , by taking the best over the best of all the algorithms considered, using (4), we presen t the best results on partic-ular datasets for various train-test splits (Fig 4-5). Unless otherwise stated, all results are on a 50-50 train-test split.
In Fig 1, we presen t test-set error-rate bounds for KMeans and SPKMeans on cmu-di erent-1000 and cmu-same-1000 for 10 runs with di eren t initializations for 3 clusters. All bounds are computed using (1). cmu-di erent-1000 is a relativ ely easy dataset in that its lab els are reasonably separated being samples from 3 quite di eren t newsgroups. As a result, both algorithms achiev e low bounds on the error-rate. SPKMeans performs particularly well since it was designed to be a text clustering algorithm [6]. Over the 10 runs, KMeans achiev es a lowest bound of 34.13 % with probabilit y 0.9 in run 4, and SPKMeans achiev es a lowest bound of 4.93 % with probabil-ity 0.9 in run 6. The best constan t classi er has error-rate 66.67 %. cmu-same-1000 is a relativ ely dicult dataset since the true lab els have signi can t overlaps. Again, SPKMeans achiev es lower bounds than KMeans in most runs, although the bounds are in general higher than those for cmu-di erent-1000 . Over the 10 runs, KMeans achiev es a lowest bound of 50.8 % with probabilit y 0.9 in run 7, and SPKMeans achiev es a lowest bound of 31 % with probabilit y 0.9 in run 3, the best constan t classi er has error-rate 66.67 %.

Next, we consider the best performance over all the 10 Figure 2: Test-set error-rate bounds for KMeans and SPKMeans on cmu-di erent-1000 and cmu-same-1000 : Best over 10 runs with di eren t initializations over a clus-ter num ber range of (2 to 20). iterations for eac h cluster num ber and compare them over a range of cluster num bers. The bound calculation is done using (2). In Fig 2, we presen t test-set error-rate bounds for KMeans and SPKMeans on cmu-di erent-1000 and cmu-same-1000 over a range of cluster num bers (2 to 20). We observ e that SPKMeans achiev es a lower bound than KMeans for most cluster num bers for reasons describ ed above. Over the en-tire range, SPKMeans achiev es a lowest bound of 5.46 % with a probabilit y of 0.9 for 3 clusters. This is marginally higher than the lowest bound of 4.93 % in Fig 1, since the bound calculation uses (2) after incorp orating the extra log 10 bits required to index the best over the 10 runs with di eren t random initializations. This is the extra cost for not kno w-ing upfron t whic h random initialization is going to perform the best. This demonstrates the trade-o between impro ve-men t in prediction accuracy and considering more runs with di eren t random initializations. On the other hand, KMeans achiev es a lowest bound of 17.2 % with a probabilit y of 0.9 for 13 clusters. For cmu-same-1000 , over the entire range, SPKMeans achiev es a lowest bound of 32.2 % with probabil-ity 0.9 for 3 clusters, whic h is marginally higher than the lowest of 31 % in Fig 1 due to extra description complexit y of log 10 bits in indexing the best. KMeans achiev es a lowest bound of 40.3 % with probabilit y 0.9 for 10 clusters.
From the results in Fig 2, we mak e an interesting obser-vation. Note that for SPKMeans , the optimal numb er of clus-ters , as dictated by the lowest bound over the entire range of cluster num bers considered, is 3 for both the datasets. Interestingly , the num ber of true lab els in both the datasets is 3. This demonstrates how the prop osed criterion can be used for model-sele ction , the \righ t" num ber of clusters in this particular case, for a given algorithm and a dataset.
Next, we compare the best performance of eac h of the algorithms, with best tak en over all cluster num bers and initializations, on various datasets. This comparison is of great practical interest since this determines the appropri-ateness of an algorithm for a given dataset. Since the best performance of eac h algorithm over cluster num bers and ini-tializations is considered, (3) is used to compute the bounds in Fig 3. In Fig 3, we compare the test-set error-rate bounds Figure 3: Test-set error-rate bounds for eac h algorithm on all the 11 datasets: classic2 , calssic3 , cmu-di erent-1000 , cmu-simila r-1000 , cmu-same-1000 , cmu-di erent-100 , cmu-simila r-100 , cmu-same-100 , cmu-newsgroup-clean-1000 , cmu-newgroup-clean-100 , yaho o : Best over all num ber of clusters and initializations. for all the 6 algorithms on all the 11 datasets under con-sideration. For datasets suc h as cmu-di erent-100 , cmu-simila r-100 , cmu-same-100 , the low num ber of samples in high-dimensions mak e the clustering problem hard for most algorithms. Among the algorithms considered, Soft-moVMF performs quite well, e.g., it achiev es the lowest test-set error-rate bound of 18.6 % with a probabilit y of 0.9 on cmu-di erent-100 . On the other hand, datasets suc h as cmu-di erent-1000 has more samples from the same distribution that mak es the clustering problem reasonably simple for quite a few algorithms. We note that 4 algorithms have comparativ e performances, with Soft-moVMF achieving the lowest bound of 5.67 % with a probabilit y of 0.9. It is in-teresting to note that SPKMeans achiev es a bound of 5.87 % whic h is marginally higher than the bound of 5.46 % we ob-serv ed in Fig 2. The marginal increase is due to the extra description length of log((3 1)3) bits used to describ e the fact that the optimal cluster num ber is 3. As for the relativ e performance of the algorithms, as exp ected, there is no clear winner across all datasets although Soft-moVMF app ears to win quite often.

We now presen t best bounds on the test-set error-rate by taking the best over all algorithms, cluster num bers and initializations. We use (4) to compute the bound. Results over 5 di eren t train-test splits on all the datasets considered are presen ted in Figs 4-5. classic2 is a relativ ely simple dataset with only 2 reason-ably separate classes. As we see in Fig 4, for all train-test splits, the bound on the test-set error-rate is very low. For the 50-50 train-test split, with probabilit y 0.9 we get a PAC bound of 1.68% on the error-rate on the test-set. (The best constan t classi er has error-rate of 48.95%.) This is a re-mark ably low error-rate bound by PAC standards 4 . classic3 is also a relativ ely simple dataset with 3 classes. As sho wn in Fig 4, for the 50-50 train-test split, with probabilit y 0.9 we get a bound of 2% on the error-rate on the test-set. (The best constan t classi er has error-rate 62.50%.)
Although cmu-di erent-100 consists of samples from 3 rel-ativ ely di eren t classes, the small num ber of samples and high dimensionalit y mak e the problem dicult. As sho wn in Fig 4, with a probabilit y of 0.9, we get a bound of 20.6% on the test-set error-rate for the 50-50 train-test split. (The
Note that increasing the train-set fraction need not increase prediction accuracy since the clustering algorithms nev er ac-tually look at the lab els. Figure 4: Test set error rate bounds for classic2 , clas-sic3 , cmu-di erent-100 , cmu-di erent-1000 : Best over all algorithms, cluster num bers, initializations best constan t classi er has error-rate 66.67%.) In cmu-di erent-1000 , the extra samples mak e nding structure in the data easier | a bound of 6 % is obtained.

Due to a large overlap between the underlying lab els, both cmu-same-100 and cmu-same-1000 are dicult datasets to get good predictions on by just clustering. As sho wn in Fig 5, with a probabilit y of 0.9, we achiev e error-rate bounds of 64% and 28.4% resp ectiv ely, while the best constan t clas-si er has error-rate 66.67%. cmu-newsgroup-clean-100 is a dicult dataset since there are 20 underlying classes with signi can t overlaps and small num ber of samples per class. As Fig 5 sho ws, the lowest bound on the test-set error-rate is 84 % with probabilit y 0.9 on a 50-50 train test split, whereas the best constan t clas-si er has an error-rate of 95 %. For cmu-newsgroup-clean-1000 , with increased num ber of samples from the same prob-lem, a lowest bound of 47.94 % is achiev ed with probabilit y 0.9. Figure 5: Test set error rate bounds for cmu-same-100 , cmu-same-1000 , cmu-newsgroup-clean-100 and cmu-newsgroup-clean-1000 : Best over all algorithms, cluster num bers, initializations Figure 6: Test-set error-rate bounds on 11 datasets for 4 languages: Simple , Init , Cluster , and Algo . yaho o is a dataset with 20 underlying classes and 2340 ex-amples. The class portions are highly skewed ranging from as low as 0.0038 to as high as 0.2111. Naturally , unsup er-vised prediction is non trivial. The best performance (not sho wn) achiev es a bound of 73.84% with probabilit y 0.9 on the 50-50 train-test split. (The best constan t classi er has error-rate 78.89%).

At this point, we mak e two observ ations: (i) on all the datasets considered, the test-set error-rate bound is always better than the best constan t classi er; and, more inter-estingly , (ii) for most datasets, the bound on the test-set error-rate for the best clustering algorithm is comp arable , perhaps even better than man y sup ervised learning bounds. Clustering app ears to capture the structure of lab eling in natural datasets.

Finally , we presen t a comparison between the bounds ob-tained from the 4 language families considered: Simple , Init , Cluster and Algo , corresp onding to Eqns (1)-(4). It is di-cult to directly compare languages because eac h optimizes over a di eren t set of possibilities. For example, it would be unfair to compare the best bound (across all possibilities) for Init to the best bound (across all possibilities) for Clus-ter , since Cluster tak es into accoun t the optimization over all num ber of clusters while Init does not. To mak e the compar-ison fair, we compare the average bound of Init to that for Cluster . Similar argumen ts apply to other languages. Fig 6 displa ys comparisons of the 4 languages on all the datasets under consideration. As sho wn in Fig 6, there seems to be some adv antage to using a more complicated language, i.e., trying to optimize over sev eral iterations, cluster num bers and algorithms. In practice, using a more complicated lan-guage of course implies more computational e ort.
A clustering algorithm typically tries to optimize its inter-nal qualit y measure, thereb y making objectiv e comparison of various algorithms practically imp ossible. Note that sev-eral unsup ervised metho ds for comparing clusterings, e.g., Jaccard index, Rand index, Fowlk es-Mallo ws index, Mirkin metric, variation of information etc., exist in the literature (for details, see [9, 10] and references therein). These com-pletely unsup ervised metho ds are incapable of measuring performance in sup ervised tasks, suc h as prediction.
Since the predictiv e abilit y of clustering algorithms is of-ten key to their successful application, external prediction-related qualit y measures are often appropriate. Sev eral su-pervised measures suc h as purit y, entrop y, normalized mu-tual information, sup ervised F-measure etc. have been used in the literature (see [8] for details). However, it is not clear how these measures are related to the error-rate of the clus-tering algorithm when used for prediction. Furthermore, none of these sup ervised measures help with cluster num-ber selection, whic h is often a big issue for these sup ervised measures.

An information theoretic external validit y measure moti-vated by the minimum description length (MDL) principle has been recen tly prop osed [7]. In spite of having sev eral desirable prop erties, this measure has a few dra wbac ks with resp ect to the measure prop osed here: (i) the measure is not normalized to the interv al [0,1] (and not easily normal-ized to exercise that interv al) whic h is desirable in sev eral settings, and man y other qualit y measures actually satisfy this; and, (ii) the measure does not pro vide guaran tees of prediction abilit y for test data. On a more general note, by attempting to measure the bits required to precisely specify all the class lab els in the dataset, the metho d [7] overlo oks the more general possibilit y of lossy compression, whic h ap-pears useful and is hea vily utilized in our prop osed criterion.
The PAC-MDL bound pro vides an objectiv e criterion for evaluation, comparison and mo del selection for clustering that is applicable when the goal of clustering is related to prediction. Exp erimen tal results sho w that this criterion is practically and exibly useful.

It is particularly striking (and perhaps even sho cking) to notice test-set error-rate bounds achiev ed by the best clustering algorithms are very comp etitiv e with various su-pervised learning bounds on the true error rate of learned classi ers. This good performance suggests that clustering algorithms are doing something fundamen tally \righ t" for prediction purp oses on natural datasets.
