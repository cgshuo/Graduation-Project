 WENLIANG CHEN, DAISUKE KAWAHARA, KIYOTAKA UCHIMOTO, YUJIE ZHANG, and HITOSHI ISAHARA National Institute of Information and Communications Technology 10: 2  X  W. Chen et al. 1. INTRODUCTION Dependency parsing has become increasingly popular for a surge of interest lately for applications such as machine translation [Ding and Palmer 2005] and question answering [Cui et al. 2005]. Dependency parsing attempts to build dependency links between words in a sentence. The results of the shared tasks of CoNLL-2006 [Buchholz and Marsi 2006] and CoNLL-2007 [Nivre et al. 2007] show that for sufficient labeled data, it is easy to train high-performance dependency parsers by using supervised learning methods.

However, currently used statistical dependency parsers provide poor results when the dependency length increases [McDonald and Nivre 2007]. Here, the length of a dependency from word w i and word w j is equal to | i  X  j | . Figure 1 shows the F 1 score 1 obtained by using a deterministic parser rela-tive to the dependency length on our testing data. The figure indicates that the F 1 score decreases when the dependency length increases as observed by [McDonald and Nivre 2007]. We also notice that the parser provides quite good results for short dependencies (94.57% for dependency length = 1 and 89.40% for dependency length = 2). Figure 2 shows the percentages of differ-ent dependency lengths in the data. We can find that more than 58% of all the dependencies are associated with either length = 1 or length = 2. In this article, short dependency refers to the dependencies with a length of either 1 or 2. We expect that the information on short dependency can help to parse words separated by longer distances.

Labeled data is expensive to create; however unlabeled data can easily be obtained. In this article, we present an approach of incorporating unlabeled data in order to improve Chinese dependency parsing. First, all the sentences in the unlabeled data are parsed by a dependency parser, which provides state-of-the-art performance. Subsequently, information on short dependency relations is extracted from the parsed data, because the accuracies for short dependencies are relatively higher than those for others. Finally, we train an-other parser by using the extracted information as features.

The proposed method can be regarded as a semi-supervised learning method. Compared with the semi-supervised methods employed by Sagae and Tsujii [2007] and Reichart and Rappoport [2007], our approach employs infor-mation on word pairs in auto-parsed data instead of selecting entire sentences as newly labeled data for training new parsers. It is difficult to detect reliable parsed sentences; however relative reliable parsed word pairs can be obtained according to the dependency length. In our experiments, the results show that our approach significantly outperforms the baseline system and current state-of-the-art techniques.

The rest of this article is as follows: Section 2 introduces the motivation and previous work. Section 3 describes the basic parser based on a deterministic parsing algorithm. Section 4 proposes an approach of incorporating short de-pendency relations from unlabeled data. Section 5 explains the experimental results. Section 6 explains the analysis. Finally, in Section 7 we draw the conclusions. 2. MOTIVATION AND PREVIOUS WORK 2.1 Motivation The goal in dependency parsing is to tag dependency links that show the head-modifier relations between words. A simple example is provided in Figure 3, where the link between  X   X  (saw) X  and  X  = P (hat) X  denotes that  X  = P (hat) X  is the dependent of the head  X   X  (saw) X .
 10: 4  X  W. Chen et al.

In general, the two words in a head-dependent relation in one sentence can be adjacent words (word distance = 1), neighboring words (word distance = 2), or words with greater distance (word distance &gt; 2) in other sentences. Here, we define that the word distance of words w i and word w j is equal to | i  X  j | . In Chinese, some modifiers can be added between the two words in a head-dependent relation. For instances, a prepositional phrase can be added be-tween a noun and a verb that act as a subject-predicate relation. And a noun can be added between an adjective and the modified noun. Figure 4 shows that  X   X   X  (Specialist-level) X  and  X  (discussion) X  have a head-dependent rela-tionship with different distances in the sentences, where  X   X   X  (Specialist-level) X  is an adjective (JJ) and  X  (discussion) X  is a noun (NN).  X  X ependent X  is optional to a head-dependent structure [Nivre and Kubler 2006]. We expect that the information obtained from word pairs with different distances can be shared with each other and thus the parser can be improved.

First, we demonstrate the method to use the dependency between adjacent words in unlabeled data to parse two words whose word distance is 2. Here, the string  X   X   X  JJ(Specialist-level)/  X  \ NN(working)/ NN(discussion) X  should be tagged as solution (a) in Figure 5. However, our current parser may select solution (b) in Figure 5 without using any additional information. The question is: how to assign the head for  X   X   X  (Specialist-level) X . Is it  X   X  \ (working) X  or  X  (discussion) X ?
As Figure 1 suggests, the current dependency parser is good at tagging the relationship between adjacent words. We expect that dependencies of adjacent words can provide useful information for parsing words whose word distances are longer. By searching the string  X   X   X  (Specialist-level)/ (discussion) X  at Google X  X  Web site, 2 many relevant documents can be retrieved. If we have a good parser, we may assign the relationships between two adjacent words in the retrieved documents, as shown in Figure 6. We can find that  X  (discussion) X  is the head of  X   X   X  (Specialist-level) X  in many cases.
Now, consider what a learning model could do to assign the appropriate rela-tionship between  X   X   X  (Specialist-level) X  and  X  (discussion) X  in the string  X   X   X  (Specialist-level)/  X  \ (working)/ (discussion) X . In this case, we pro-vide additional information to  X  (discussion) X  by saying that it is the pos-sible head of  X   X   X  (Specialist-level) X  in the unlabeled data. In this manner, the learning model may use this information to make correct decisions.
Thus far, we have demonstrated how to use the dependency relation be-tween adjacent words in unlabeled data to help parse two words whose word distance is 2. Similarly, we can provide information for parsing two words whose word distance is longer.

On the basis of the above observations, we propose an approach to exploit-ing information from large-scale unlabeled data in order to improve Chinese dependency parsing. We use a parser to parse the sentences in the unlabeled data. Another parser makes use of the information on short dependency rela-tions in the newly parsed data to improve its performance. 2.2 Previous Work Our study is associated with incorporating unlabeled data into a model for Chinese parsing. Several other studies relevant to ours have been conducted, as described below.

Chinese dependency parsing is a difficult task, and a considerable amount of research has been devoted to this topic [Cheng et al. 2005a,b; Yu et al. 2007; Wang et al. 2005, 2007]. Most of the studies have focused on training depen-dency parsers on labeled data by using supervised learning methods such as Nivre X  X  [2003] model, Yamada and Matsumoto X  X  [2003] model, and structured boosting [Wang et al. 2007].

A simple method is that of self-training in which the existing model first labels unlabeled data, and the newly labeled data is then treated as hand an-notated data for training a new model. However, it appears that self-training is not so effective. Steedman et al. [2003] reports only a small improvement by using self-training for phrase structure parsing on small labeled data. The reason may be that errors in the original model can be amplified in the new 10: 6  X  W. Chen et al. model. [McClosky et al. 2006] presents a successful instance of parsing with self-training by using a re-ranker. As Figure 1 suggests, the dependency parser exhibits poor performances for parsing the words with long distances. In our approach, we select partial reliable information that is obtained from short dependency relations for the dependency parser.

Smith and Eisner [2006] present an approach to improve the accuracy of dependency grammar induction models by using EM from unlabeled data. They obtain consistent improvements by penalizing dependencies between two words that are farther apart in the string. Kawahara and Kurohashi [2006] present an integrated probabilistic model for case structure analysis of Japanese parsing. Case structure analysis of Japanese is very different com-pared to those of English and Chinese. In Japanese, postpositions are used to mark cases. Thus they construct case frames from a huge raw corpus to facilitate Japanese case structure analysis. The large corpus is automatically parsed and case frames are constructed from modifier-head examples in the resulting parses. Then they incorporate the case frames into a fully-lexicalized probabilistic model and select the syntactic structure that has the highest structure probability. Our work differs in that we consider general dependency relations while they construct case frames for the verbs from auto-parsed data. Further, we represent additional information as the features for learn-ing models while they use the case frames as one component for a probabilistic model. 3. BASIC PARSER In this article, we select a deterministic parser based on the model described by Nivre [2003]. This model is simple and works very well in the shared-tasks of CoNLL2006 [Nivre et al. 2006] and CoNLL2007 [Hall et al. 2007]. In fact, our approach can be applied to other parsers, such as Yamada and Matsumoto X  X  [2003] parser and McDonald et al. X  X  [2006] parser. 3.1 Preliminaries A dependency structure can be defined as a directed graph G , consisting of a dence order &lt; on V [Nivre and Kubler 2006]. The nodes in V are the words with their properties (such as part-of-speech tags and lemma) and the arcs in E are labeled with dependency types. There are four formal conditions on de-pendency graphs [Nivre and Kubler 2006]: 1) G is connected, for every node v i v of node v j .

We denote a sentence with n words by W = w 1 ,...,w n and the correspond-ing dependency graph by g . A dependency graph is represented with a set of ordered pairs ( i , j )  X  g in which w j denotes a dependent and w i denotes a head. The parser tries to find a dependency graph g for each sentence W . 3.2 The Parsing Model The Nivre [2003] model is a shift-reduce type algorithm, which uses a stack to store processed tokens and a queue to store remaining input tokens. It can perform dependency parsing in O(n) time. The dependency parsing tree is built from atomic actions in a left-to-right pass over the input. The behaviors of the parser are defined by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input string):  X  Left-Arc: Add an arc from NEXT to TOP; pop the stack.  X  Right-Arc: Add an arc from TOP to NEXT; push NEXT onto the stack.  X  Reduce: Pop the stack.  X  Shift: Push NEXT onto the stack.
 The two actions (Left-Arc and Right-Arc) will add a dependency relation be-tween TOP and NEXT.

These actions are determined by the parser configurations C , which are rep-remaining input tokens, and A is the current arcs from the dependency graph. set of arcs A ). For a configuration c , the parser takes the optimal action t  X  = It represents the likelihood of taking action t out of configuration c . The actions Left-Arc and Right-Arc are subject to the condition that ensures the graph con-dition single head is satisfied. In contrast, the action Reduce can be applied only if TOP has a head. For Shift, the condition is that Q is non-empty.
Figure 7 shows the four actions and their conditions, where w i denotes TOP (the token on top of the stack S ), and w j denotes NEXT (the first token in the w into A and pop w i from the stack S . Similarly, the parser can take the action Right-Arc for w i and w j if w j does not have a head. For Right-Arc, we add an arc w i  X  w j into A and push w j onto S . The action Reduce can be taken only if w i has a head. For Reduce, we pop The parser uses a classifier to produce a sequence of actions for a sentence. For the classifier, we can use several machine learning algorithms such as ME (Maximum Entropy), SVM (Support Vector Machines), and MBL (Memory-Based Learning). Thus far, the best results achieved by using the parser with the SVM model [Nivre et al. 2006]. In our experiments, we use the SVM model as the classifier. More specifically, our parser uses LIBSVM [Chang and Lin 2001] with a polynomial kernel (degree = 3) and the built-in one-versus-all strategy for multi-class classification.
 10: 8  X  W. Chen et al.
 3.3 Basic Features We represent the basic features extracted from the fields of data representa-tion, including word and part-of-speech (POS) tags. The basic features used in our parser are listed as follows:  X  Features based on words: The words of TOP and NEXT, the word of the head of TOP, the words of the leftmost and rightmost dependent of TOP, and the word of the token immediately after NEXT in the original input string.  X  Features based on POS: The POS of TOP and NEXT, the POS of the token immediately below TOP, the POS of the leftmost and rightmost dependent of TOP, the POS of next three tokens after NEXT, and the POS of the token immediately before NEXT in original input string.

By using the above basic features, we can train a parser on labeled data. In the following sections, we call this parser the Basic Parser. 4. OUR APPROACH In this section, we describe our approach of exploiting reliable features from unlabeled data that have been parsed by the Basic Parser. First, we preprocess the unlabeled data to obtain an auto-parsed data. Subsequently, we collect word pairs having short dependencies from the auto-parsed data. Finally, we represent the labeled data on the basis of the collected word pairs and train another parser on new data representation. Figure 8 shows the architecture of our approach. The rest of this section describes the proposed approach in Sections 4.1, 4.2, and 4.3 corresponding to the parts  X  X reprocessing, X   X  X ollect-ing word pairs, X  and  X  X raining new parser, X  respectively.
 4.1 Unlabeled Data Preprocessing The input in our approach is unlabeled data, which can easily be obtained. For the Basic Parser, the corpus should have part-of-speech (POS) tags. Therefore, we should assign the POS tags by using a POS tagger. For Chinese sentences, we should segment the sentences into words before POS tagging. After data preprocessing, we obtain the word-segmented sentences with the POS tags. We then use the Basic Parser to parse all sentences in the unlabeled data. Finally, we obtain an auto-parsed data. 4.2 Collecting Reliable Word Pairs The Basic Parser can provide complete dependency parsing trees for all sen-tences in unlabeled data. As shown in Figure 1, short dependencies are more reliable. To offer reliable information for the model, we extract word pairs having short dependency relations from the newly auto-parsed data. 4.2.1 Extracting Word Pairs from Auto-Parsed Data. Suppose that the cur-rent two words are w i (TOP) and w j (NEXT) and the word pair p t is  X  w i -w j  X . Because POS tags are too generic, we only consider word pairs. Thus, in this section we describe the method to extract word pairs that have short depen-dency relations in the sentences.

In a parsed sentence, if the dependency length of two words ( w i and w j ) is either 1 or 2, we add this word pair p t to a list named DepList and then count its frequency. We also consider the direction D I R = { L A , RA } and length L EN = { L 1 , L 2 } of the dependency. L 1 refers to the pairs with de-pendency length 1, L 2 refers to the pairs with dependency length 2, RA refers to right arc, and L A refers to left arc. For example,  X   X   X  (specialist-level) X  and  X  (discussion) X  are adjacent words in a sentence  X   X  (We)/ &gt; L (held)/  X   X  (specialist-level)/ (discussion)/  X  and have a left dependency arc as-signed by the Basic Parser. Hence, we have the word pair  X   X   X  (specialist-level)-(discussion) X  with  X  X A-L1 X . The pair with  X  X A-L1 X  and its frequency is added to the DepList. We use Freq ( p t , D I R , L EN ) to denote the frequency. 10: 10  X  W. Chen et al.
 4.2.2 Classifying into buckets. We group word pairs into different clusters according to the following equation:
In our experiments, we group the pairs into four clusters corresponding to four levels:  X  X igh-frequency X ,  X  X iddle-frequency X ,  X  X ow-frequency X , and  X  X nfre-quency X . We select the threshold values( f 1 , f 2 , f 3 ) by tuning on development data. Finally, we obtain four clusters by setting f 1 = 1, f 2 = 7, and f 3 = 14. Among all the collected pairs, the pairs in C 1 , C 2 , C 3 , and C 4 take approxi-mately 70%, 24%, 3%, and 3% respectively. To avoid the change in threshold values when using different sizes of unlabeled data, we can use the percent-ages as threshold values.

We then form buckets by combining clusters with the dependency length and dependency direction. For example, if the frequency of the pair  X   X   X  (specialist-level)-(discussion) X  with  X  X A-L1 X  is 20, it belongs to the bucket  X  C 4 LA L1 X . Additional examples are provided in Table I. 4.3 Training the New Parser 4.3.1 New data representation. Based on the buckets, we represent new features for training or parsing the current two words: TOP and NEXT. We consider word pairs from the context around TOP and NEXT, and we obtain the buckets of the pairs in the DepList. According to dependency lengths, we divide the pairs into two sets: L1 pairs and L2 pairs.
 First, we represent the features based on L1 pairs. We name these features L1 features. The L1 features are listed according to different word distances between TOP and NEXT, as follows: (1) Word distance is 1: (TN0) the bucket of the word pair of TOP and NEXT, (2) Word distance is 2: (TN0) the bucket of the word pair of TOP and NEXT, (3) Word distance is 3 and 3+: (TN0) the bucket of the word pair of TOP and
Thus, we obtain eight types of L1 features, including two types in item (1), three types in item (2), and three types in item (3). The feature is format-ted as  X  X ordDistance:Position:PairBucket X . For example, we have the string  X   X   X  (specialist-level)/ w 1 / w 2 / w 3 / (discussion) X . Here,  X   X   X  (specialist-level) X  is TOP and  X  (discussion) X  is NEXT. Thus, we obtain the fea-ture  X  X 3+:TN0: C 4 LA L1 X  for TOP and NEXT, because the word distance is 4(3+) and  X   X   X  (specialist-level)-(discussion) X  belongs to the bucket  X  C 4 LA L1 X . A pair can belong to two buckets because there exists two directo-ries (LA and RA). Here, we use the bucket whose pair has higher frequency.
Similarly, we represent the features based on L2 pairs. We name these features L2 features. The L2 features are listed as follows: (1) Word distance is 1: (TN1) the bucket of the word pair of TOP and next (2) Word distance is 2: (TN0) the bucket of the word pair of TOP and NEXT, We obtain three types of L2 features, including one type in item (1), and two types in item (2). 4.3.2 Training with new data representation. By using the basic features (represented in Section 3.3) and the new features (represented in Section 4.3.1), the data is represented in a new feature space. A new parser is then trained on the new data representation. 5. EXPERIMENTS For the labeled data, we used the Chinese Treebank (CTB) version 4.0 3 in the experiments. We used the same rules for conversion and created the same data split as Wang et al. [2007]: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used the gold standard segmentation and POS tags in the CTB.

For the unlabeled data, we used the PFR corpus. 4 It includes documents from the People X  X  Daily at 1998 (12 months). There are approximately 290,000 sentences and 15 million words in the PFR corpus. To simplify, we used its seg-mentation. We discarded the POS tags because PFR and CTB used different 10: 12  X  W. Chen et al.
 POS sets. We used the package TNT [Brants 2000], a highly efficient statisti-cal part-of-speech tagger, to train a POS tagger on training data from the CTB. To know whether our POS tagger was good, we tested the TNT package on the standard training and testing sets for full parsing [Wang et al. 2006]. The TNT-based tagger provided 91.52% accuracy, which was comparable with the results obtained by [Wang et al. 2006].

We measured the quality of the parser by the unlabeled attachment score (UAS), that is, the percentage of tokens (excluding all punctuation tokens) with the correct HEAD. We also reported the ROOT accuracy. 5.1 Experimental Results In the experiments, we trained the parsers on training data and tuned the pa-rameters on the development data. In the following sessions,  X  X aseline X  refers to the Basic Parser (the model with the basic features), and  X  X URS X  refers to our proposed parser (the model with all features). 5.1.1 Our approach. Table II shows the results of the parser with different feature sets, where  X +L1 X  refers to the parser with the basic features and L1 features, and  X +L2 X  refers to the parser with all features(basic features, L1 fea-tures, and L2 features). From the table, we found that our approach achieved significant improvement (1.12% for UAS and 1.02% for ROOT) by adding L1 features. The L2 features provided a further but small improvement, 0.12% for UAS and 0.13% for ROOT. The reason may be that the information from the dependency length 2 data contains more noise. As shown in Figure 1, the score of dependency length 2 was about 5% lower than that of dependency length 1. The other reasons may be that the number of L2 pairs is much less than that of L1 pairs and that 35.8% of the L2 pairs are included in the list of L1 pairs. Hence, the L2 features could not provide much further information. Totally, we achieved a 1.24% improvement for UAS and 1.15% for ROOT. The improve-ment is significant in the one-tail paired t-test ( p &lt; 10  X  5 ).
We also attempted to discover the effect of using different numbers of un-labeled sentences. Table III shows the results obtained when using different numbers of sentences. Here, we randomly selected different percentages of sentences from unlabeled data. When we used 1% sentences of unlabeled data, the parser achieved a large improvement of 0.4%. With the addition of sen-tences, the parser provided better results. 5.1.2 Comparison of other systems. In this section, we compare our parser with the state-of-the-art parsers. We used the same testing data as those used by Wang et al. [2005], which sentences with lengths up to 40 were selected. Table IV shows the results of our method and other studies, where Wang05 refers to Wang et al. [2005] and Wang07 refers to Wang et al. [2007]. From the table, we found that our parser performed the best. The baseline system exhibited better performance as compared to those of Wang05 and Wang07.
We turned to compare our approach with other semi-supervised learning methods. We implemented two systems: SelfTrain and CoTrain. The SelfTrain system was similar to the method described by Reichart and Rappoport [2007], and new auto-parsed sentences were randomly selected. The CoTrain system was similar to the method described by Sagae and Tsujii [2007]. First, we trained a forward parser (same as our baseline system) and a backward parser. Then, we selected the sentences that had been identically parsed by the two parsers as newly labeled data. Finally, we retrained the forward parser with new training data. We selected the sentences having about 200k words from the PFR data as newly labeled data for the SelfTrain and CoTrain systems.
Table V shows the experimental results. SelfTrain showed an improvement of 0.43%, and CoTrain gave a 0.55% improvement as compared with the base-line system. Our approach performed the best among all the systems. The time required for training the SelfTrain and CoTrain systems increased be-cause they employed almost double the training data. We compared the run times (in minutes) for training four systems. Table V shows that OURS re-quired 1,126 minutes, the similar time by the baseline system. SelfTrain and CoTrain required almost three times as much as the time required by the base-line system.

Finally, we implemented a system that used the short dependency relations in auto-parsed data as new training data for a self-training method. We called the system SelfTrainShort. The SelfTrain system used all dependency rela-tions in the auto-parsed sentences as new data, whereas the SelfTrainShort 10: 14  X  W. Chen et al.
 used only the relations involving distances of 1 and 2. SelfTrainShort pro-vided 85.40 UAS, slightly better than the baseline system. This score was the best one among the results obtained by varying the number of selected word pairs of newly labeled data. 6. ANALYSIS 6.1 Improvement Relative to Dependency Length We consider the improvement relative to the dependency length. We conducted the experiments by performing a 10-cross validation. The training data and testing data were merged into one set. Then it was randomly divided into 10 parts. We performed the experiments 10 times. Each time, we used one of them as testing data and the others as training data. Figure 9 shows the average scores relative to the dependency length. From the figure, we found that our method provided better performance for all lengths. In particular, our parser performed well on lengths greater than 4, and the Basic Parser showed poor performance. 6.2 Improvement Relative to Unknown Words The unknown word 5 problem is an important issue for parsing. Our approach can partially release the unknown word problem. We calculated the number of unknown words in a sentence and listed the accuracies of the sentences having unknown words, from 0 to 5. We discarded the sentences having more than five unknown words, because their number was very small. We grouped each sentence into one of three classes: (Better) those where our approach X  X  score increased relative to the baseline X  X  score, (NoChange) those where the score remained the same, and (Worse) those where the score had a relative decrease. We added another class (NoWorse) by merging Better and NoChange.
Figure 10 shows the experimental results, where the x axis refers to the number of unknown words in one sentence and the y axis shows the class in percentage. For example, for the sentences having three unknown words, about 38.46% improved, 15.38% became worse, 46.15% were unchanged, and 84.61% did not become worse. The NoWorse curve showed that regardless of the number of unknown words in a sentence, there was more than an 80% chance that our approach did not harm the result. The Better curve and Worse curve showed that our approach always provided better results. The results also indicated that our approach can achieve larger improvement in parsing sentences having unknown words than parsing sentences without any unknown words. The reason may be as follows: We collect word pairs including unknown word pairs and known word pairs in Section 4.2, and group them into one of the buckets by using Equation (1). Unknown word pairs in the testing data are also mapped into one of the buckets by using Equation (1). Hence, known word pairs can share the features with unknown word pairs. 6.3 Improvement Relative to POS Pairs In this section, we listed the improvements relative to POS tags of paired words having a dependency relation. Table VI shows the accuracies of baseline and OURS on TOP 30 POS pairs (ordered by the frequencies of their occur-rences in testing data), where A b refers to the accuracy of baseline, A o refers to the accuracy of OURS, and  X  X airs X  is the POS pairs of dependent-head. For example,  X  X N-VV X  means that  X  X N X  is the POS of the dependent and  X  X V X  is the POS of the head. Baseline yielded 84.35% accuracy and OURS yielded 87.87% (3.52% higher) on  X  X N-VV X . The example provided in Section 2.1 was relative to  X  X J-NN X  for which OURS provided an improvement of 2.46%. The table shows that our approach worked well for most POS pairs (better for nine-teen pairs, no change for four, and worse for seven).

In Figure 9, we found that our proposed method performed very well for lengths greater than 4. Considering POS pairs and dependency distances, we investigated the improvement relative to these two factors. We divided the lengths into two ranges:  X  X 1-D4 X  for lengths ranging from 1 to 4, and  X  X 5+ X  for lengths no less than 5. Table VII shows the accuracies of baseline and OURS on TOP 10 POS pairs with two ranges, where  X  X LL X  refers to all the 10: 16  X  W. Chen et al.
 pairs. For example, for the pair  X  X N-VV X , there are 70.27% in the range  X  X 1-D4 X  and 29.73% in the range  X  X 5+ X . OURS provided 2.04% improvement for the range  X  X 1-D4 X  and 7.02% for the range  X  X 5+ X . From the table, we found that OURS provided larger improvement for  X  X 5+ X  on 7 pairs out of 10. For the pairs  X  X N-VV X  and  X  X -VV X , the rates of  X  X 5+ X  were higher than the average and OURS provided an improvement greater than 5%. 6.4 The Effect of Word Segmentation To simplify and make our results easily to reproduce, we used the word-segmented PFR corpus in our experiments. However, the word segmentation specifications of CTB and PFR are different. To verify the effect of this dif-ference, we removed the word boundary from the PFR corpus and used a tool named CJMA 6 [Nakagawa and Uchimoto 2007] that was trained on the CTB data to perform word segmentation and POS tagging at once. Then we used the Basic parser to parse these sentences. We applied our approach on the newly auto-parsed data. The new system provided 86.42% for UAS, compa-rable to that with the word-segmented PFR corpus. The reason was that the errors caused by CJMA affected the results of the new system and the word-segmented PFR data provided another type of  X  X rror X  due to the difference in the specifications. 6.5 Case Study of Neighborhood Ambiguities In Chinese dependency parsing, there exists a large number of ambiguities in neighborhoods, such as  X  X J NN NN X ,  X  X D VV VV X ,  X  X N NN NN X , and  X  X J NN CC NN X . They have possible parsing trees as shown in Figure 11. For such ambiguities, our approach can provide additional information for the parser. For example, we have the following case in the data set:  X   X  } JJ(friendly)/ \ NN(corporation)/ s  X  NN(relationship)/ X . We can provide additional in-formation about the relations of  X   X  } JJ(friendly)/ \ NN(corporation) X  and  X 
 X  } JJ(friendly)/ s  X  NN(relationship)/ X  in unlabeled data to help the parser make the correct decision.
 Our approach can also work for longer constructions such as  X  X J NN NN NN X  and  X  X N NN NN NN X .

For the construction  X  X J NN1 CC NN2 X , we do not define special features to solve the ambiguity. However, based on the current DepList, we can pro-vide additional information about the relations of JJ/NN1 and JJ/NN2. For example, for the string  X   X  e JJ(further)/ 9  X  NN(improvement)/  X  CC(and)/  X  U NN(development)/ X , the parser often assigns  X  9  X  (improvement) X  as the head of  X   X  e (further) X  instead of  X   X  U (development) X . There is an entry  X   X  e (further)- X  U (development) X  in the DepList. Here, we require a coor-dination identifier to identify these constructions. Thus, we can provide the information for the model. 6.6 Case Study for Other Ambiguities For Chinese dependency parsing, there exists several special characteristics that cause ambiguities. The  X   X  /DE X  structure is a special case. As shown in Figure 12(a), it becomes a very difficult problem when combining with the PP structure. The Basic Parser incorrectly assigns  X  ( (in) X  as the head of 10: 18  X  W. Chen et al.
  X  -M (seat) X . Our approach can provide the additional information of the pairs  X  ( (in)-T  X  (the United Nations) X  and  X  b (recover)--M (seat) X  to facilitate making correct decisions. For the sequence  X  X V NN VV X , we can not obtain sufficient number of clues in Chinese to identify whether or not it is a noun clause or an infinitive clause (such as  X  X hat X  and  X  X o X  in English). Figure 12(b) shows the above mentioned difficulty. The list of our collected word pairs includes the pair  X  W (comets)- X   X  (collide) X , but does not include the pair  X 
 X  K (forecast)-W (comets) X . By using this information, our parser makes cor-rect decisions. Similarly, our approach can partially help parse verb sequences ( X  X V+VV X ) by using the information of the pairs of surrounding words and the verbs. 7. CONCLUSION This article presents an effective approach to improve Chinese dependency parsing by using unlabeled data. We extract the information on short de-pendency relations in an automatically generated corpus parsed by the Basic Parser. We then train a new parser by using this extracted information. The new parser achieves an absolute improvement of 1.24% over the state-of-the-art parser on the Chinese Treebank (from 85.28% to 86.52%). We compare our proposed method with other methods. The experimental results show that our new parser exhibits the best performance.

We analyze the parsing results. The proposed method improves the per-formance for longer dependencies. The results show that our approach can partially release the unknown word problem.

There are many ways in which this research should be continued. First, the feature representation needs to be improved. Here, we use a simple feature representation on short dependency relations. We may use a combined rep-resentation to use the information obtained from long dependency relations, although they are not so reliable. Second, we can attempt to select sentences that are parsed with higher accuracy. In this manner, we may collect more reliable information. Third, we may attempt to apply the proposed method to other languages, such as English.
 10: 20  X  W. Chen et al.

