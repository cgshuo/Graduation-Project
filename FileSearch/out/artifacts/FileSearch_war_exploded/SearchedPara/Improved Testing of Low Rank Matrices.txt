 We study the problem of determining if an input matrix A  X  R m  X  n can be well-approximated by a low rank ma-trix. Specifically, we study the problem of quickly estimat-ing the rank or stable rank of A , the latter often providing a more robust measure of the rank. Since we seek signifi-cantly sublinear time algorithms, we cast these problems in the property testing framework. In this framework, A ei-ther has low rank or stable rank, or is far from having this property. The algorithm should read only a small number of entries or rows of A and decide which case A is in with high probability. If neither case occurs, the output is allowed to be arbitrary. We consider two notions of being far: (1) A requires changing at least an -fraction of its entries, or (2) A requires changing at least an -fraction of its rows. We call the former the  X  X ntry model X  and the latter the  X  X ow model X . We show: We also give an empirical evaluation of our rank and stable rank algorithms on real and synthetic datasets.
 F.2.1 [ Analysis of Algorithms and Problem Complex-ity ]: Numerical Algorithms and Problems X  Computation on matrices ; G.2.3 [ Discrete Mathematics ]: Applications Algorithms, Theory dimensionality reduction, principal component analysis, prop-erty testing, robustness, stable rank
Low rank approximation is a popular tool in computer science with applications to computer vision, information retrieval, and machine learning. In many of these appli-cations, such as image, video, multimedia processing, web data, and bioinformatics the dimensionality of the data is very large. This makes designing algorithms for processing such data more challenging, requiring very low memory and extremely fast processing time.

A saving grace of large-scale data is that it is often of low intrinsic dimension. For example, in Principal Component Analysis (PCA) [6, 7, 13] the data points are column vectors of a matrix A with the assumption that A can be expressed as L + N for L a matrix of low rank and N a matrix of small Frobenius norm, which could typically model noise that has been added to A . Replacing A with the matrix L provides a good low rank approximation to A . PCA has a wide range of applications, including non-negative matrix factorization [9], latent dirichlet allocation [1], clustering [3], and geometric shape fitting problems [4]. There is a large body of work on randomized algorithms for low rank approximation; we refer the reader to Section 5 of the survey by Mahoney [11].
Recently, a new form of PCA called robust PCA was intro-duced [2]. In this problem, the data points are again column vectors of a matrix A = L + N , where L is a low rank matrix, but now N is only guaranteed to be a sparse matrix. Un-like classical PCA, the entries of N can be arbitrarily large provided there are a small number of non-zero entries (the locations of the non-zero entries of N are unknown). This makes robust PCA less sensitive to outlier contamination. We refer the reader to [2] in which applications of robust PCA to video surveillance, face recognition, latent semantic indexing, ranking and collaborative filtering are given. In typical applications, such as recommender systems [15], L is a matrix of a small constant rank. Surprisingly, under cer-tain assumptions there are efficient algorithms for recovering L and N . One assumption is that the number of non-zero entries of N is at most a sufficiently small constant fraction of the total number of entries.

Independently of the work above, the property testing community has also studied whether a matrix can be ex-pressed as a small perturbation of a low rank matrix [8, 12]. In the property testing model there is an unknown, typically very large object, such as a graph, a matrix, or a vector. This object is queried in certain positions in order to determine if it satisfies a property P or is far from sat-isfying P . For an introduction to property testing, we refer the reader to a survey by Goldreich [5]. The relevant results in the property testing literature for robust PCA are those for what we refer to as the Rank property. In this problem, the input matrix A is either of rank at most d , or requires changing an -fraction of its entries in order to become a ma-trix of rank at most d . Note that this is a decision version of the robust PCA problem: either A = L in the notation above, or if A = L + N for a matrix L of rank at most d , then necessarily more than an -fraction of entries of N are non-zero. Distinguishing these two cases allows one to decide whether the assumptions required for a robust PCA algorithm to succeed hold. If the input A is in neither case, then it is allowed for the algorithm to output an arbitrary answer, which is acceptable for the robust PCA application since robust PCA is guaranteed to work if N has at most an -fraction of non-zero entries.

The Rank problem was studied by Krauthgamer and Sas-son [8], who showed there exists a randomized algorithm succeeding with 99% probability on every input matrix A and reading only O ( d 2 / 2 ) entries of A . This bound is inde-pendent of the dimensions of the matrix A . This provides a quick, provably correct method for determining whether robust PCA procedures will work on A , without having to run them in case A is not well-approximated by a low rank matrix. Other methods such as clustering and recommenda-tion systems can also benefit by first running an algorithm for Rank to determine if A is close to a low rank matrix.
Despite this progress, there are several natural questions that remain: 1. In machine learning problems a quadratic dependence 2. In differential equation applications, one often has a 3. It is often more common for a matrix to have low stable Our Contributions: In this paper we thoroughly study both the Rank and StableRank problems. We answer the questions above, providing new theoretical and empirical guarantees for these problems.
 Results for the Rank Problem : 1. In the entry model, by allowing queries ( i,j ) to be 2. We show that, for constant d , adaptivity is neces-3. We further study the problem when d = 1, which has 4. In the row model, we show that any, possibly adap-Results for the StableRank Problem : 1. We show in the row model that reading a total of 2. We also show an  X ( d/ 2 ) lower bound in the row model. We experimentally validate our algorithms for Rank and Sta-bleRank on several natural input distributions on A and spar-sity patterns N .
 For the StableRank problem, we use real datasets from the University of Florida Sparse Matrix Collection. We show that for a large fraction of the matrices in this dataset, our algorithms only need to sample a very small fraction of rows to solve the StableRank problem. We parameterize the num-ber of rows that need to be read as a function of the stable rank parameter d for these datasets.

For the Rank problem, we use synthetic datasets. Our experiments show particularly noticeable improvements for adaptive query algorithms over non-adaptive query algo-rithms for small . For example, for = 0 . 01 and d = 1, for one of our input distributions the number of adaptive queries is 7% of the number of non-adaptive queries required. Paper Outline: We give our adaptive algorithm for the Rank problem in the entry model in Section 2, and show that adaptivity is essential by proving a lower bound for non-adaptive algorithms in Section 3. In Section 4, we give a new non-adaptive algorithm for the important case of d = 1, which comes close to the lower bound we prove for non-adaptive algorithms in Section 3. In Section 5 we consider the row model, and prove a lower bound on the number of rows read for the Rank problem. In Section 6 we give an algorithm for the StableRank problem and show a nearly matching lower bound, both in the row model. Finally, we present our experimental results in Section 7.
In this section we study the Rank problem with adaptive queries. We assume that min( m,n ) =  X  ( d/ ), that is, that min( m,n ) is larger than cd/ for any fixed constant c &gt; 0. This is consistent with our goal of testing if A has small rank.

We first review the algorithm for Rank in [8]. Suppose that the input matrix A has rank greater than d . That algorithm tries to find a submatrix with rank greater than d . The algorithm starts with an empty submatrix and iteratively grows the submatrix by appending one random row and one random column. Let B t be the submatrix maintained at step t and X t = rank( B t ). It was shown in [8] that Pr { X X | X t  X  d } X  / 3 and thus by a Chernoff bound, t = O ( d/ ) suffices to reach X t &gt; d with constant probability. Algorithm 1 Our Algorithm for the Rank problem 1: I  X  X  X  ,J  X  X  X  2: for t = 1 to O ( d 2 / ) do 3: Pick ( i,j ) uniformly random from I c  X  J c 6: I  X  I  X  X  i } ,J  X  J  X  X  j } 7: end if 9: if rank( B t ) &gt; d then 10: return  X  A is -far from rank d  X  11: end if 12: end for 13: return  X  A is of rank d  X 
In our adaptive algorithm, we also augment B t in each step until rank( B t ) &gt; d . We formally write our algorithm in Algorithm 1. Suppose at step t , rank( B t ) &lt; d and I and J are the index sets of the rows and columns of B t , respec-tively. Consider the index pairs I c  X  J c , where I c = [ m ] \ I We claim that at least an  X ( ) fraction of the index pairs in I c  X  J c would increase rank( B t ). Assume that this is true for the moment. Then in expectation, O (1 / ) random samples in I c  X  J c suffice for there to exist a sample index pair that would increase the rank B t after augmenting with respect to that index pair. We can find one such pair by checking each chosen possible augmentation of B t . Call the pair found B t +1 . By linearity of expectation and a Chernoff number of entries read is, in expectation, bounded by Now we prove our claim above to complete the proof. We can assume, without loss of generality, that B t consists of an up-per left submatrix of A . Since we assume that min( m,n ) =  X  ( d/ ), and B t has at most d rows and columns, we can change all the entries of A in the first t columns and first t rows so that the rows restricted to the first t columns are in the row span of B t , and the columns restricted to the first t rows are in the column span of B t . This only changes at most an / 2-fraction of the total number of entries of A . Next, for each entry ( i,j ) not among the first t columns or rows, we can change the value of A i,j so that augmenting B t by the pair ( i,j ) does not increase the rank of B t . Since we must change at least an -fraction of overall entries of A to reduce the rank to at most d , and B t has rank at most d , the number of index pairs in I c  X  J c that would increase rank( B t ) must be at least mn/ 2.

Our algorithm is optimal for constant d , because it re-quires  X (1 / ) queries just to distinguish a zero matrix from a matrix with mn randomly placed non-zero entries.
In this section, we start with a simple example to demon-strate that it is generally hard to improve the non-adaptive upper bound of O (1 / 2 ) for Rank even for d = 1, for a class of natural non-adaptive algorithms which query submatri-ces and make their decision based on the maximum rank of them. Next, we give a proof that any randomized non-adaptive algorithm requires  X ((log 1 / ) / ) queries for d  X  1.
To design non-adaptive algorithms, a natural way is to se-lect some submatrices of A to query, namely A 1 ,  X  X  X  ,A t then make a decision based on whether max i  X  [ t ] rank( A d . However, there is an example of A such that the number of queries required is at least  X (1 / 2 ) for such algorithms, even when d = 1. In the following we fix d = 1. One can easily extend the result to any d .
 Denote where 1 r,c is an r -by-c matrix whose entries are all 1s. Let A be the matrix obtained from uniformly randomly permuting the rows and columns of M .

In order to find a fully queried submatrix whose rank is more than 1, one must query an entry in A corresponding to an entry of the top-left submatrix in M (we call such an entry critical), whose size is just n  X  n . Therefore, if the total query size is o (1 / 2 ), the probability is o (1) that one has queried a critical entry in order to find that rank( A ) = 2 instead of 1. Hence, a lower bound of  X (1 / 2 ) holds for non-adaptive algorithms which query a set of submatrices and decide on whether the maximum rank of those submatrices is more than d .

In fact, for more complicated algorithms, it is possible to reduce the non-adaptive query size when d = 1. We shall study it in Section 4.

The example here also illustrates the superiority of adap-tive queries over non-adaptive ones. An adaptive algorithm needs O (1 / ) queries (in expectation) to find an entry of value 1, and based on the position of that entry, the algo-rithm can then extend it to a matrix of rank 2 with O (1 / ) more queries; while a non-adaptive algorithm does not know which rank-1 matrix to extend.
In this subsection, we prove the following theorem, which can be automatically extended to arbitrary d .

Theorem 1. Any randomized non-adaptive algorithm for the Rank problem with d = 1 requires  X ((1 / ) log(1 / )) queries.
To give a lower bound for non-adaptive queries for any randomized algorithm, we apply Yao X  X  Lemma, and (1) de-fine two distributions D 0 ,D 1 , such that D 0 is a distribution of matrices of rank at most d (or Pr M  X  D 0 { rank ( M )  X  d } = 1, the same below), while D 1 is a distribution of matrices which are -far from rank d ; (2) prove that with high prob-ability, any deterministic non-adaptive set of ( c/ ) log(1 / ) entries cannot distinguish D 0 from D 1 , where c &gt; 0 is a constant.
 Algorithm 2 Hard Distribution 1: Let i be uniformly sampled in [ k ]. 2: Let r = n/ 2 i  X  1 , c = n  X  2 i , and x 1 ,x 2 ,y 1 3: Let M 0 ,M 1  X  R n  X  n be 4: Let P r ,P c  X  R n  X  n be two uniformly random permuta-5: Let D 0 be the distribution of P r M 0 P c and D 1 the dis-
We define the distributions D 0 and D 1 on R n  X  n in Al-gorithm 2. Notice that D 0 is a distribution of matrices of rank 1 with probability 1 while D 1 is a distribution such that a random sample is -far from a rank-1 matrix with probability 1.

Now consider a deterministic algorithm for testing the ma-trix A sampled from either of the two distributions with equal probability. The queries of the algorithm can be writ-ten as a deterministic subset S  X  [ n ]  X  [ n ]. The following lemma is straightforward by the construction of the distri-butions, together with the property of normal distributions that N (  X  1 , X  2 1 ) + N (  X  2 , X  2 2 )  X  N (  X  1 +  X  2 , X 
Lemma 1. If for each row and column of A the number of observed non-zero entries is at most 1 , then the algorithm cannot determine whether A is  X  X f rank 1 X  or  X  -far from any rank-1 matrix X  better than a random guess. Formally, To upper-bound the probability that two or more non-zero observations are in a query row or column, we need the following lemma. It follows from a union bound argument and simple inequalities.

Lemma 2. Suppose that there are n bins, m of which con-tain a ball each. Then choosing b bins uniformly at random collects at least 2 balls with probability at most ( bm/n )
Proof. We pick b bins one by one. The probability that two particular bins both contain balls is at most ( m/n ) Also notice that if at least 2 balls are picked, it must be the case that there exist two attempts both of which have balls. Applying a union bound, we obtain the probability that we collect at least 2 balls is at most b 2  X  m n 2  X  bm n 2 .
The next is the most important lemma, which is a bit technical. It says that if the number of non-adaptive queries is small, then the probability will be small that there exists one column such that the number of non-zero observations on that column is larger than 1.
 Lemma 3. If | S | X  1 192 log 1 , then
Proof. We start with some definitions. For every i  X  [ k  X  1], let x i be the number of columns in [ n ] such that the number of entries observed on that column is larger than 2 i  X  1 but no more than 2 i . Let x k be the number of columns in [ n ] such that the number of entries observed on that col-umn is larger than 2 k  X  1 . More formally, for i  X  [ k  X  1], let and for i = k , We know that For i  X  [ k ], let P i bet the probability that there exists one column containing 2 or more observed non-zero entries, con-ditioned on the event that A has an ( n/ 2 i  X  1 )  X  ( n 2 trix of non-zero entries (i.e., i is chosen when it is generated in Algorithm 2). By Lemma 2, we obtain that for all j  X  [ k ],
Notice that the factor 2 j comes from the fact that there are only 2 j n columns that are non-zero in A . If we visit 2 entries on a column of n/ 2 j  X  1 non-zero entries, the prob-ability that we hit at least 2 non-zero entries is at most 1 since it is a probability. Therefore, Summing over all j  X  [ k ] yields that Therefore, if | S | X  1 192 log 1 , then i.e.,
Extending Lemma 3 to rows and combining with Lemma 1, we can prove Theorem 1, i.e., any non-adaptive algorithm that solves our problem takes  X  1 log 1 queries. In this section, we give a non-adaptive algorithm for the Rank problem with O ( 1 log 2 1 ) queries when d = 1 and  X  1 /e . Let  X  be such that  X  log(1 / X  ) = and  X  &lt; 1 / 2. Also let k = log 1 / X  . The proposed algorithm is as follows. We describe it for an n  X  n matrix A , though it immediately extends to rectangular matrices as well.

Choose R 1 ,  X  X  X  ,R k and C 1 ,  X  X  X  ,C k from [ n ] uniformly at random such that and where c 0 is a sufficiently large constant to be determined later. Denote Q = S k i =1 ( R i  X  C i ), the overall set of entries the algorithm will query. Then, the algorithm computes the minimum possible rank of the matrix, and decides that  X  A is -far from being rank-d  X  iff the minimum possible rank is more than d .

Notice that the total number of entries the algorithm justify the correctness of the proposed algorithm for d = 1.
For fixed A  X  R n  X  n which is -far from being rank -d , call ( r,c ) an augment for R  X  C  X  [ n ]  X  [ n ] if r  X  [ n ] \ R , aug ( R,C ) be the set of all the augments, that is, aug ( R,C ) = { ( r,c )  X  ([ n ] \ R )  X  ([ n ] \ C ) : For fixed R , C and A , define count r ( r  X  [ n ] \ R ) to be the number of c  X  X  such that ( r,c )  X  aug ( R,C ). Let count be the non-increasing reordering of the sequence ( count r following lemma follows from the fact that the number of augments is at least n 2 if A is -far from being rank-d and rank( A R,C )  X  d , as argued in Section 2.

Lemma 4. If A is -far from being rank-d and rank ( A R,C d , then We define the concept of an augment pattern below.

Definition 1. For M , R , C and i  X  [log(1 / X  )], we say that ( R,C ) has augment pattern i on A iff count  X  n/ 2 i  X  2
Following the definition, we show the existence of at least one augment pattern for ( R,C ) when A is -far from being rank-d and rank( M R,C )  X  d .

Lemma 5. If A is -far from being rank-d and rank( A R,C )  X  d , then there exists i such that ( R,C ) has augment pattern i .

Proof. We prove the lemma by contradiction. Suppose that ( R,C ) does not have augment pattern i for all i  X  [log(1 / X  )], i.e., It follows that which contradicts Lemma 4.

Note that if ( R,C ) has augment pattern i on A , a uni-formly random rectangle sample of dimension c 2 i  X  c/ 2 will hit at least one augment with high probability, which is at least We conclude this fact as Lemma 6. Suppose that ( R,C ) has augment pattern i on A and j  X  { i  X  1 ,i } . Let R 0 ,C 0  X  [ n ] be uniformly random ( R 0 ,C 0 ) contains at least one augment of ( R,C ) on A is at least 1  X  2 e  X  c/ 2 .

Now we are ready to show the correctness for the proposed algorithm.

Theorem 2. Suppose that  X  1 /e . For any matrix A (either of rank at most d = 1 , or at least -far from it), the probability that the proposed algorithm is erroneous is at most 1 / 3 , provided that c 0  X  12 .
Proof. If A is of rank at most 1, the algorithm will never be wrong. Now we analyze the case that A is -far from being rank-1. We discuss the two cases based on the number of augment patterns for (  X  ,  X  ) on A .
 Case (i) (  X  ,  X  ) has only one single augment pattern. Let i denote the only augment pattern that (  X  ,  X  ) has. We divide R i uniformly at random into two even parts, R (1) and R (2) i . Do the same with C i , obtaining C (1) i and C Lemma 6, the probability that A one non-zero entry is at least 1  X  2 e  X  c 0 / 4 . Let us condition on this event.

Let ( r,c )  X  ( R (1) i ,C (1) i ) be such that A r,c ( { r } , { c } ) has augment pattern i by Lemma 5, while on the other hand it is impossible that ( { r } , { c } ) has augment pat-tern other than i , since (  X  ,  X  ) does not have the augment pattern. Now consider the probability that ( R \{ r } ,C \{ c } ) contains an augment for ( { r } , { c } ). We claim that this prob-ability is also at least 1  X  2 e  X  c 0 / 4 . Since R (2) uniformly random given R (1) i and C (1) i , we can use a coupling argument to show that the probability that ( R i \{ r } ,C contains at least one augment for ( { r } , { c } ) is greater than a uniformly random sample of dimension c 0 2 i / 2  X  c 0 / 2 in ([ n ] \{ r } )  X  ([ n ] \{ c } ) does.

Therefore, the probability to augment one empty matrix to a 2  X  2 full rank matrix is at least 1  X  4 e  X  c 0 / 4 the algorithm answers correctly in this case.
 Case (ii) (  X  ,  X  ) has multiple augment patterns.

In this case, suppose that (  X  ,  X  ) has augment patterns i and j ( i &lt; j ). Divide R i uniformly at random into two even parts R (1) i and R (2) i , and C j into C (1) j and C divide R j \ R i evenly into R (1) and R (2) , C i \ C j and C (2) . According to Lemma 6, the probability that cuss two cases based on whether ( R (1) i ,C (1) j ) intersects with aug (  X  ,  X  ).

Case (ii.1): ( R (1) i ,C (1) j )  X  aug (  X  ,  X  ) =  X  . Let ( r ( R rectly.
 ( R case (ii.1), we can prove that the probability is at least 1  X  2 e  X  c 0 / 4 that ( r,c ) could be augmented with augment by ( R j \{ r } ,C j \{ c } ). So the overall probability is at least 1  X  6 e  X  c 0 / 4 &gt; 2 / 3 that the algorithm answers correctly in this case by finding a submatrix of rank 2.
In this section, we discuss the Rank problem in the row model. Recall that we say A is -far from having property P if at least n rows of A have to be changed for A to have property P . The Rank problem in this model is to test whether the matrix has rank at most d or is -far from having rank at most d .
 In this model, the algorithm of [8] gives an upper bound of O ( d/ ) rows. Next we show a matching lower bound when the entries of A come from any field F , e.g., the real numbers. Assume that n  X  2 d/ throughout this section.

First assume F is a finite field. Let D 1 be a distribution over n  X  n matrices defined as follows. Choose a random d -dimensional subspace W in F n and then choose 2 n uni-formly random vectors from W . Place these 2 n vectors on 2 n uniformly random rows of an n  X  n matrix. The result-ing distribution is D 1 . We define D 2 similarly, except that W is a uniformly random ( d + n )-dimensional subspace in F . Clearly rank( A )  X  d when A  X  D 1 . When B  X  D 2 , with probability 1  X  o (1), one needs to change at least n rows of B to reduce its rank to d .

By construction, adaptively choosing rows does not help in distinguishing D 1 from D 2 , and so we may assume the query algorithm is non-adaptive. Fix Q  X  { 1 ,...,n } with Each defines a distribution on q  X  n matrices, denoted by L ( A Q ) and L ( B Q ), respectively.

Lemma 7. Suppose that F is a finite field. When q  X   X d/ (8 ) , it holds that d TV ( L ( A Q ) , L ( B Q ))  X   X  + | o (1) , where d TV denotes total variation distance.
Proof. When q  X   X d/ (8 ), by a Markov bound, with probability  X  1  X   X  at most d/ 4 vectors of the chosen 2 n ones are read. For distribution D 1 , with probability  X  1  X  |
F |  X  d/ 4 , the vectors are linearly independent. For distribu-tion D 2 , with probability  X  1  X  o (1), the vectors are linearly independent. The conclusion follows immediately from the observation that conditioned on the vectors being linearly independent, they are distributed as a set of uniformly cho-sen d/ 4 linearly independent vectors in F n .

For F = R , we define D 1 and D 2 similarly, except that the 2 n random vectors are chosen subject to the multidi-mensional Gaussian measure on W . Similarly to the lemma above, we have,
Lemma 8. Suppose that F = R and  X  &gt; 0 . When q  X   X d/ (8 ) , it holds that d TV ( L ( A Q ) , L ( B Q ))  X   X  + o (1) .
Proof. When q  X   X d/ (8 ), by a Markov bound, with probability  X  1  X   X  at most d/ 4 vectors of the chosen 2 n ones are read. For both distributions, the randomly chosen vectors are linearly independent almost surely. The conclu-sion follows immediately from the observation that condi-tioned on the vectors being linearly independent, they are distributed as a set of uniformly chosen d/ 4 linearly inde-pendent vectors in R n .
 The lower bound follows immediately as a corollary. Corollary 1. In the row model, any algorithm for the Rank problem needs to sample  X ( d/ ) rows.
Definition 2. (stable rank) Let A  X  R n  X  n be a non-zero matrix. The stable rank of A is srank( A ) = k A k 2 F / k A k We will design an algorithm for the StableRank problem for n  X  n matrices. We denote the i -th row of A by A i,  X  . Algorithm 3 Algorithm for the StableRank problem 2: Sample q rows of A , forming  X  A 3: X  X  n q k  X  A k 2 F 4: if X  X  9 10 (1  X  1 d ) n then 5: output  X  X table rank  X  d  X  6: else 7: if n q k  X  A k 2  X  X 8: output  X  X table rank  X  d  X  9: else 10: output  X  n/d -far from having stable rank  X  d  X  11: end if 12: end if
Lemma 9. Suppose that d/  X  2 . If A is ( /d ) -far from having stable rank  X  d , then
Proof. Suppose that x  X  S n  X  1 satisfies k A k = k Ax k 2 Without loss of generality, assume that  X  A 1 ,  X  ,x  X  2  X  X  A  X  X  X   X   X  A n,  X  ,x  X  2 . Let m = d n/d e  X  1, so that n &gt; 2 m . Changing each A i,  X  (1  X  i  X  m ) to x forms a new matrix B , and it must hold that srank( B ) &gt; d .

It is clear that k B k 2  X  m and k B k 2 F  X k A k 2 F + m , so whence (1) follows.
 Next we prove the second conclusion. It is clear that S Observe that and It follows that whence (2) follows.
 Lemma 10. In Algorithm 3, it holds that | X  X  X  A k 2 (1  X  1 d ) 2 n with probability  X  9 / 10 .

Proof. Let  X  = 1 8 (1  X  1 d ) 2 . By a Chernoff bound, sam-pling q rows uniformly gives failure probability 2 e  X  2 q (  X  ) 0 . 1, that is, q =  X (1 / (  X  ) 2 ).

Lemma 11 ([10]). Let  X  A be a matrix formed by r in-dependent row samples of A according to probability p ity at least 1  X   X  , it holds that (1  X   X  ) k A k 2  X  n (1 +  X  ) k A k 2 .

Lemma 12. Let X  X  Unif( S n  X  1 ) then k x k  X   X  with probability  X  1  X  n  X  2 .

Theorem 3. Suppose that k A k row = 1 , then Algorithm 3 is a correct algorithm for the StableRank problem with suc-cess probability  X  0 . 6 in the row model. It reads O ( d log n d log n log( d log n )) rows.

Proof. By Lemma 1, if A is far from having stable rank at most d , it must hold that k A k 2 F  X  (1  X  1 /d ) n . Con-ditioned on the event that X is a good estimator to k A k that is, X satisfies the conclusion of Lemma 10, it holds that X  X  9 10 (1  X  1 d ) n . Hence the algorithm is correct on Line 5. Now we assume that k A k 2 F  X  (1  X  1 /d ) n . Let  X  = cn
Now suppose that srank( A ) &gt; c 1 d . Let U be a uniformly random n  X  n orthogonal matrix. Since we only care about norms of  X  A we can replace  X  A with  X  AU , which is a random sample of q rows of AU . Observe that ( AU ) i,  X  is a random i with probability  X  1  X  1 /n . Conditioned on this event, 1.8], and thus with probability  X  0 . 9, k  X 
A 0 k X  10 C 1 On the other hand, when srank( A )  X  d , it holds with prob-ability  X  0 . 9 that k  X 
A 0 k X  1 By our choice of parameters, 1 2 provided that c 1 is less than a constant times 1 / (1  X  1 c =  X  . Hence we can distinguish the two cases.

Now we assume that srank( A )  X  c 1 d . Let  X  = k A k 2 F satisfies the assumption of Lemma 11. It then follows from Lemma 11 that with probability at least 0 . 9, it holds that Conditioned on this event; in the first case, k A k 2  X k A k in the second case, by Lemma 9, k A k 2  X  (1+ d (1  X  1 d )) (1  X  1 d )( n d  X  1) and thus n q It is not difficult to establish that (1+  X  ) when c =  X  = 1 8 (1  X  1 d ) 2 . Therefore we can distinguish the two cases. Combining with the discussion above for the case where srank( A ) &gt; c 1 d , we see that Line 8 and Line 10 are correct.
Let D 1 be a distribution over n  X  n matrices defined as follows. Choose a random x 0  X  S n  X  1 and place x 0 in n/d randomly chosen rows of an n  X  n matrix A . Place the first n  X  n/d rows of a random orthogonal matrix in the remaining n  X  n/d rows of A . Let D 1 be the distribution of A . We define D 2 similarly as follows. Choose random x 0  X  S n  X  1 and place x 0 in (1  X  2 ) n/d randomly chosen rows of an n  X  n matrix B . Place the first n  X  (1  X  2 ) n/d rows of a random orthogonal matrix in the remaining n  X  (1  X  ) n/d rows of B . Let D 2 be the distribution of B .

Suppose that A  X  D 1 and B  X  D 2 . It is clear k A k 2 F and k A k 2  X  n/d , and so srank( A )  X  d . Now we upper bound k B k 2 . With probability 1, we know that x 0 does not lie in the span of the orthogonal rows, and so k Bx k 2 &lt; 1 + (1  X  3 ) n/d , that is, k B k 2 &lt; 1+(1  X  3 ) n/d  X  (1  X  2 ) n/d . Chang-ing  X n/d rows of B forms a new matrix B 0 with srank( B 0 d . We know that k B 0 k 2 F  X  k B k 2 F  X   X n/d = (1  X   X /d ) n and k B 0 k 2  X  k B k 2 +  X n/d . It follows from k B 0 k 2 that 1  X   X /d  X  1  X  2 +  X  , thus  X   X  2 (1 + 1 /d ) &gt; , and we conclude that with probability 1, the matrix B is ( /d )-far from having stable rank  X  d .
 and define B Q similarly. Each defines a distribution on q  X  n matrices, denoted by L ( A Q ) and L ( B Q ), respectively. Also denote by B ( n,p ) the binomial distribution of n trials and success probability p .

Lemma 13. The Hellinger distance between two binomial distributions is given by d
H ( B ( n,p ) ,B ( n,q )) =
Lemma 14. Suppose that d  X  2 . When q  X   X  2 d/ (18 2 ) , it holds that d TV ( L ( A Q ) , L ( B Q ))  X   X  + o (1) .
Proof. Observe that A Q and B Q contain the same num-ber of rows of x 0 then the conditional distributions are the same. Note that the distance between L ( A Q ) and B ( q, 1 /d ) is o (1) and a similar result holds for L ( B Q ) and B ( q, Therefore using that least as large as the variation distance, we have, = d TV ( B ( q, 1  X   X  2 1  X  It is not difficult to verify that whenever d  X  2 and 0 &lt; &lt; 1 3 . Therefore, it holds that The lower bound follows immediately as a corollary.
Corollary 2. Suppose that d  X  2 . Under the row sam-pling model, any algorithm that is correct on the StableRank problem needs to sample  X ( d/ 2 ) rows.
All programs are written in MATLAB and the source code can be found at http://www.mpi-inf.mpg.de/~yli/codes.pdf .
Algorithm 3 takes  X  O ( d 2 log n ) row samples with a theoret-ical guarantee, however, a literal interpretation of the bound makes it less useful in practice, since for d = 2, = 0 . 1, it holds that d log n/ 2 &gt; n for n  X  1500. Indeed, the theo-retical upper bound is too pessimistic, i.e., very often we do not need so many samples for real data sets. We justify our thoughts in the following experiment.
 We test our algorithm with the University of Florida Sparse Matrix Collection 1 . There are 628 square real matrices with dimension between 100 and 1000 (inclusive). Among them, there are 220 matrices at least 0 . 1-far from having stable rank 2. There are also 35 square matrices with stable rank  X  2. For each matrix A of the 255 matrices, we determine the minimum q such that our algorithm, when sampling q rows at random, succeeds with probability  X  0 . 9 in distin-guishing whether its stable rank is at most 2, or it is at least 0 . 1-far from having stable rank 2. The probability is determined by 100 independent trials. The cumulative dis-tribution of q/n (where n is the dimension of A ) is plotted in Figure 2. We can see that our algorithm needs to sample only at most 15% of the rows for 90% of the matrices. The remaining 10% have relatively small stable rank and it is natural to expect that more rows are needed.

We conducted similar experiments for d = 3 and d = 5, too. The results are also plotted in Figure 2. Regarding d = 3, there are 174 matrices at least 0 . 1-far from having stable rank 3 and 67 matrices with stable rank  X  3. We ran our algorithm on each of the 81 matrices and plotted the cumulative distribution of q/n . We can see that our algorithm needs to sample only at most 15% of the rows for 90% of the 241 matrices. Regarding d = 5, there are 105 matrices at least 0 . 1-far from having stable rank 5 and 161 matrices with stable rank  X  5. We can see that our algorithm needs to sample only at most 10% of the rows for 90%, and 15% of the rows for 95%, of the 266 matrices.
We have seen there is a gap of a 1 / factor in the theoret-ical results between O ( d 2 / 2 ) samples for the non-adaptive http://www.cise.ufl.edu/research/sparse/matrices/ algorithm and O ( d 2 / ) samples for the adaptive one. As above, both bounds could be too pessimistic as well. Thus we design the following experiments to show that the adap-tive tester has a real advantage over the non-adaptive algo-rithm even when both algorithms read much fewer samples than the respective theoretical upper bound.

We conducted three sets of experiments on different ma-trix distributions as follows. In each case it holds that rank( A ) = d + 1 with probabil-ity 1. We consider three cases of d : d = 1 , 2 , 5. For both the strip and the square distribution, we set n = 1000 and 0 . 35, 0 . 40, 0 . 45, 0 . 5; for the rectangular distribution we set n = 1024 and = 1 / 128, 1 / 64, 1 / 32, 1 / 16, 1 / 8, 1 / 4, 1 / 2. For each configuration of d and and each matrix distribu-tion, we ran both the non-adaptive query algorithm [8] and the adaptive query algorithm (Algorithm 1) for 1000 times independently to obtain the number of queries needed to conclude rank( A ) &gt; d with a success probability of at least 0 . 9. The results are shown in Figure 1 in logarithmic scale.
In all settings above, adaptive queries outperform non-adaptive ones, and particularly heavily for small . It is also notable that the strip distribution is especially adversarial for the non-adaptive tester, which needs to makes at least 1 / 2 queries. When = 0 . 01, the number of adaptive queries is only 7 . 1%, 9 . 4%, 12 . 4% of that of non-adaptive queries for d = 1, 2, 5, respectively. Even when = 0 . 5, the number of adaptive queries is less than 1 / 3 of that of non-adaptive queries. The difference between non-adaptive and adaptive queries is less pronounced under the other two distributions, still the number of adaptive queries is at most a half of that of non-adaptive ones.
David Woodruff would like to acknowledge the support from the XDATA program of the Defense Advanced Re-search Projects Agency (DARPA), administered through Air Force Research Laboratory contract FA8750-12-C0323. [1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [2] E. J. Cand`es, X. Li, Y. Ma, and J. Wright. Robust [3] C. H. Q. Ding and X. He. K -means clustering via [4] D. Feldman, M. Schmidt, and C. Sohler. Turning big [5] O. Goldreich. A brief introduction to property testing. [6] H. Hotelling. Analysis of a complex of statistical [7] I. T. Jolliffe. Graphical Representation of Data Using [8] R. Krauthgamer and O. Sasson. Property testing of [9] D. D. Lee and H. S. Seung. Algorithms for [10] M. Magdon-Ismail. Row sampling for matrix [11] M. W. Mahoney. Randomized algorithms for matrices [12] M. Parnas and D. Ron. Testing metric properties. In [13] K. Pearson. On lines and planes of closest fit to [14] M. Rudelson and R. Vershynin. Sampling from large [15] B. M. Sarwar, G. Karypis, J. A. Konstan, and [16] J. A. Tropp. Column subset selection, matrix [17] C. Yang, T. Han, L. Quan, and C.-L. Tai. Parsing respectively, from left to right.
