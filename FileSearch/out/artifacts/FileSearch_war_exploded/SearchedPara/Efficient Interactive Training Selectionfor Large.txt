 Entity resolution (ER), also known as data matching, record linkage, or duplicate detection, is the process of identifying and matching records that correspond to the same entities from one or more databases [ 6 ]. As the databases to be matched generally do not include entity identifiers, ER has to be based on the available attributes, for example, personal names, addresses and dates of birth. In the past decades, ER has attracted much interest from various application domains, the most prominent being health, census statistics, e-commerce, national security and digital libraries. For recent surveys see [ 6 , 14 ].
 The core steps in ER in their most basic form consist of the pair-wise com-parison of records using functions that calculate numerical similarities between attribute values, followed by either an unsupervised or supervised classification of pairs of records into matches and non-matches [ 6 , 14 ]. The comparison of attribute values used in ER is commonly based on approximate string comparison func-tions that return a normalized similarity between 0 (totally different values) and 1 (exact matching values). For each compared record pair, a weight vector is cal-culated with the similarities over the different attributes of that pair [ 6 ]. been proposed for ER in past years. While supervised techniques generally result in much better matching quality, these techniques require training data in the form of labeled examples of true matching pairs of records that refer to the same entity, and true non-matching pairs of records that refer to different entities. While in certain, mostly academic, situations such training data are available, in most practical applications of ER actual truth data are difficult to obtain. In many cases training data have to be manually generated, a task that is known to be difficult both in terms of cost and quality [ 6 ]. The traditional way of selecting training data is to use random sampling. However, from a robust statistical point of view, random sampling needs to select a significantly large number of examples for guaranteeing the quality of training data, which was also verified in our experiments discussed in Sect. 5 . Another difficulty of using random sampling is caused by the imbalance of the ER problem, as the vast majority of record pairs will correspond to non-matches [ 6 ]. Two challenges thus stand out in particular when training data are to be manually generated over large-scale data sets: (1) How can we ensure  X  X ood X  examples are selected for training? (2) How can we minimize the user X  X  burden of labeling examples? The central idea is to reduce the labeling efforts through actively choosing infor-mative or representative examples [ 16 ]. In doing so, instead of choosing a large quantity of examples to label as is required for fully supervised learning, active learning only selects examples based on the hints from previously labeled exam-ples, which can often yield a training set that is small but still sufficient for supporting accurate classification.
 in achieving efficient training for large-scale data sets. Most of these methods are grounded on a monotonicity assumption  X  a record pair with higher similarity is more likely to represent the same entity than a pair with lower similarity. This assumption is valid in some real-world applications but does not generally hold, as we will illustrate in Sect. 3 . Thus, two difficult issues arise in selecting training data: (1) How do we know whether the monotonicity assumption holds on a data set since training data are not available? (2) How can we effectively select training data when the monotonicity assumption does not hold? ing ER training data over large data sets. Unlike other works, we do not rely on the monotonicity assumption. Instead, our method exploits the cluster struc-ture in data through active learning, which can circumvent the first issue above, meanwhile solving the second issue. The basic idea of our method is illustrated in Fig. 1 , where (a) shows weight vectors that are generated from pair-wise record comparisons, and the labels of these weight vectors are unknown. Then, (b) to (d) show how the weight vectors are interactively selected and manually classi-fied, and how the set of weight vectors is recursively split into smaller clusters until each cluster is classified as being pure or fuzzy (to be formally defined in Sect. 4 ) based on the label purity of its informative weight vectors. During this process, the training set is interactively constructed by gathering the weight vectors from pure clusters.
 We make the following contributions in this paper. (1) We develop an inter-active training method which can be applied to ER training tasks without prior knowledge of the match and non-match distributions of the underlying data sets. (2) Our training method incorporates a budget-limited noisy human oracle, which ensures: (i) the overall labeling efforts can be controlled at an acceptable level and as specified by the user, and (ii) the accuracy of labeling provided by human experts can be simulated. This is in contrast to existing active learning methods for ER which often assume a perfect and unlimited labeling process [ 20 ]. (3) We experimentally evaluate our method on four real-world data sets from different application domains.
 In the following section we discuss related work. In Sect. 3 we present the problem and building blocks of our approach, which we describe in detail in Sect. 4 . We experimentally evaluate our approach in Sect. 5 , and conclude the paper in Sect. 6 with an outlook to future work. Active learning has previously been studied in many problem domains, such as text classification and speech recognition [ 20 ]. In the area of ER, active learning has been explored for learning ER classifiers, which classify pairs of records as matches or non-matches through actively selecting a reduced number of examples that relates to our study in this paper.
 sentative examples typically used disagreement between multiple classifiers. For example, a committee of classifiers was used to identify the most representative examples for labeling [ 19 , 22 ], i.e., labeling is iteratively required for pairs of records where the classifiers return contradictory labels for the same example. Sampling based active learning and its bias have been discussed in [ 11 ]. cally has a linear combination of the two measures precision and recall as the learning objective. For example, in [ 1 , 2 ], given a minimum precision specified by the user, a learned classifier aims to have a precision greater than the minimum precision and a recall close to the best possible. Compared with these active learning techniques, our algorithm has several interesting properties: (1) pro-viding an integrated view on labeling budget control and quality guarantee, (2) using interactive purity-based classification to reduce examples for labeling, and (3) not relying on the monotonicity assumption for improving quality. been used. One is to incorporate blocking or indexing [ 6 ] into the learning pro-cess with parameters that are tuned manually [ 1 , 2 ] or semi-automatically [ 10 ]. Blocking in ER is the process of dividing data sets into smaller blocks according to some criteria so that only records within the same block are compared with each other. In principle, existing blocking techniques can be easily incorporated into active learning algorithms as a pre-processing step before learning. The second technique is to optimize active learning algorithms under certain distri-bution assumptions, such as the monotonicity [ 1 ] and low noise [ 2 ] assumptions. Our training method is completely independent of any assumption concerning the data set or any blocking technique used, which makes our method more generally applicable.
 tain strategies. Repeated labeling strategies were investigated in [ 21 ], includ-ing round-robin repeated labeling and selective repeated labeling based on the uncertainty of labels. In [ 12 ], a combined strategy was proposed, which selects examples that are more likely to be correctly labeled yet still provide high quality information, and examples that are most likely to have been incorrectly labeled. In [ 23 ] the most reliable oracle was selected among multiple noisy oracles for labeling. In this paper, we explore active learning in the presence of a noisy human oracle, which allows us to simulate the challenging manual clerical label-ing process in real-world ER applications. We study the problem of reducing the labeling costs for selecting training data, while keeping the quality of ER classification at a high level. In contrast to the works of [ 1 , 2 ], we do not rely on the monotonicity assumption since it does not generally hold for ER. This is evident from the plots in Fig. 2 , which show the non-matches with the highest similarity and matches with the lowest similarity from three of the real-world data sets we used in our experimental evaluation in Sect. 5 . To address this problem, we propose an active learning approach that, given a set of weight vectors and a classifier, recursively splits the weight vectors into clusters, and classifies these as being matches or non-matches if the purity of informative weight vectors in a cluster is higher than a specified threshold. In the following we present the building blocks of our proposed approach. Let R be a set of records from one or more data sets, each ing a set of attributes. We use r.A to refer to the value of an attribute a record r . Given two records r 1 ,r 2  X  R and an attribute a similarity weight of A between r 1 and r 2 is a value in [0 f ( r .A, r ity between r 1 .A and r 2 .A . Taking the edit distance similarity function example [ 6 ], f ed ( r 1 .f name, r 2 .f name )=1 . 0  X  3 and r For a set A = { A 1 ,...,A n } of attributes selected for performing ER tasks, each compared pair ( r 1 ,r 2 ) of records that has the attributes A results in a weight vector a 1 ,...,a n  X  [0 , 1] n over A , where a i A between r 1 and r 2 ( i =1 ,...,n ). For example, the pair ( the attributes { f name, sname, age } with r 1 .f name = Rob r .age = 30, r correspond to a weight vector 0 . 5 , 1 , 0 . 5 .A weight vector set W over A consists of all the weight vectors over A to which the pairs of records in R correspond. A cluster W i  X  W is a subset of weight vectors in W .A partition of W is a set {
W ,..., W m } of pairwise disjoint clusters whose union contains all the weight vectors in W , i.e. W i  X  W j =  X  for 1  X  i = j  X  m ,and We consider a noisy human oracle that simulates a non-perfect manual cleri-cal labeling process. The main reason behind such noisy human oracles is due to the fact that human experts often have different levels of expertise for labeling matches and non-matches [ 6 ]. Thus, depending on which human expert is asked for labeling an example, the labeling accuracy varies. A human oracle takes a set of record pairs and their corresponding weight vectors as input, and based on manual inspection of the attribute values of these records assigns each weight vector with a label. Let W i be a weight vector set. Then a human oracle over W is a function  X  : W i  X  X  M, N } , where M and N are the two labels indicating match and non-match of a weight vector, respectively. Moreover, each human oracle  X  is associated with a pair bud (  X  ) ,acc (  X  ) , where limit ( b tot ) indicating the maximal number of weight vectors that can be labeled by  X  ,and acc (  X  )  X  [0 , 1] is indicating the accuracy of labels provided by acc (  X  ) = 1 then the oracle is perfect .
 matches and non-matches through their corresponding weight vectors [ 6 ]. More specifically, an ER classifier takes as input a weight vector set W of labeled (with M and N ) weight vectors W T i  X  W i as the training set, and generates a partition of W i into W M i of matches and W N be used in our approach. In this section we discuss the details of our approach. A high-level description of our interactive training approach is provided in Algorithm 1. Let W be a weight vector set, and T M and T N be the subsets of W that are selected into the match and non-match training sets, respectively, with T queue Q of clusters to be processed (line 2). The main iteration (line 4) loops as long as the queue is not empty (i.e. there are clusters to process) and the total oracle budget b tot has not been fully used ( b  X  b first cluster W i in the queue is being processed. In the first loop (with indicating no manual labeling has been done), the init select used to select a first set of weight vectors S i  X  W i to be manually classified by the oracle, while in sub-sequent iterations the main select Different approaches for these selection functions will be described in Sect. 4.1 . In general, a selection function selects k informative weight vectors S cluster W i (lines 7 or 9).
 (line 11) into a match set T M i and non-match set T N i , which are added to the final training sets T M and T N , respectively (line 12). The used budget is also increased (line 10) by the number of manually classified weight vectors the purity p i of the cluster is calculated (line 13), as will be described further below. All weight vectors in the cluster are added into one of the training sets (lines 14 to 17) if the cluster is pure enough ( p i  X  p min requires further splitting if it is larger than a minimum cluster size the total oracle budget b tot has not been fully used, and if T not empty (lines 18 to 22). If T M i and T N i are both not empty, they will be used to train a classifier for the current cluster W i (line 20). The splitting of W (line 21) leads to two smaller clusters W M i and W N i of matches and non-matches, respectively, which are then added to the queue (line 22). In principle, the two smaller clusters W M i and W N i should have a higher purity compared to W . Clusters that are small ( | W i | X  c min ) and not pure are not considered for inclusion into the final training sets.
 The algorithm thus generates a partition of W such that the weight vectors in each pure cluster are selected into the training sets, i.e., the weight vectors from a match cluster into T M and the ones from a non-match cluster into T while the weight vectors in fuzzy clusters (those too small for further splitting and not pure enough) are discarded.
 The purity p i of a cluster W i is calculated based on the classification done by the human oracle (line 11) using the manually classified weight vector set S as the proportion of classified weight vectors that have the majority label: where | T M i  X  T N i | = | S i | . For a given purity threshold W i is labeled as pure if purity ( W i ) &gt;p min ; otherwise W 4.1 Weight Vector Selection Methods The informativeness of selected weight vectors crucially influences the quality of the final generated training sets T M and T N . Therefore, the selection methods in Algorithm 1 need to be carefully chosen. Here we propose three methods for the init select function and four methods for the main select the initial selection (line 7 in Algorithm 1) using INIT SELECT we consider: (1) Far: Farthest-first weight vectors with random initialization [ 15 ], based on the farthest first clustering algorithm which selects the k weight vectors from W that are farthest apart from each other. The idea of this approach is to start with a selection of weight vectors with the highest possible variety. (2) 01: Weight vectors that are closest to the two corners [1] | A | and [0] likely to represent matches and non-matches, respectively, as they correspond to weight vectors closest to exact matching and totally different record pairs [ 7 ]. (3) Corner: Weight vectors that are closest to all corners { 0 , 1 } for i =1 ,..., | A |} , where there are 2 | A | corners in total. This approach combines the ideas of both Far and 01 , selecting weight vectors with the highest possible variety in terms of all the attributes in A .
 ing the main iteration of Algorithm 1 (line 9) using MAIN SELECT we consider: (1) Ran: A random selection of k weight vectors. We use this as a baseline in our experiments to evaluate the effectiveness of the other selection methods. (2) Far: Farthest-first weight vectors selection within a cluster, as done in the Far initial selection method. This will give us weight vectors at the outer boundary of a cluster. (3) Far-Med: Here we select the k  X  1 farthest apart weight vectors from W i , and additionally we add the medoid weight vector closest to the center of the cluster. The idea is to not just manually classify pairs at the boundary of a cluster, but also one weight vector in its center to get a better picture of the distribution of matches and non-matches in the cluster.
 with different parameter settings on several real-world data sets. We conducted experiments on four data sets: ACM-DBLP [ 17 ], CORA Google Scholar (DBLP-GS) [ 17 ], and the North Carolina Voter Registration (NCVR) database 2 . The characteristics of these data sets are summarized in Table 1 . As can be seen, all data sets exhibit a high to very high class imbalance between true matches and true non-matches. We used the Febrl open source record linkage system for the pair-wise linkage step, together with a variety of blocking/indexing and string comparison functions [ 8 ]. The output of this step are sets of weight vectors of the compared record pairs, and their known true labels (match or non-match).
 The following parameter variations were used in our experiments: minimum purity threshold p min =[0 . 95, 0 . 9, 0 . 85, 0 . 8, 0 . 0 . 95, 0 . 9, 0 . 85, 0 . 8, 0 . 75], total budget b tot = [100, 200, 500, 1 10 , 000], number of weight vectors selected k = [9, 19, 49, 69, 99], and the different initial selection ( Far , 01 and Corner ) and selection ( Ran , Far and Far-Med ) methods discussed in the previous section. The classifiers used for splitting weight vectors were decision trees (DTree) with entropy and information gain [ 18 ]. Default values for the parameters were set to minimum purity 0 . 95, oracle accuracy acc (  X  )=1 . 0, number of weight vectors selected the CORA data set and k = 69 for the other data sets, total budget for the CORA data set and b tot =5 , 000 for the other data sets, minimum cluster size c min = 50, initial selection method 01 and selection method Far , as these settings resulted in the best quality based on a set of pre-experiments. We evaluated the effectiveness of our approach using the F-measure [ 6 ], and the efficiency using the time required for the classification. The baseline approaches we used to compare with our approach (which we refer as DTree-AL) were: (1) fully supervised decision tree (DTree-S), (2) fully supervised support vector machines with linear and polynomial kernels (SVM-S), (3) unsu-pervised automatic k-nearest neighbor clustering (kNN-US) [ 7 ], (4) unsupervised k-means clustering (kMeans-US), and (5) unsupervised farthest first clustering (Far-US) [ 8 ]. Our proposed active learning approach and the baseline approaches are implemented in Python 2.7.3, and we ran all experiments on a server with 6-core 64-bit Intel Xeon 2.4 GHz CPUs, 128 GBytes of memory and running Ubuntu 14.04. The programs and test data sets are available from the authors. We first evaluated how different values for the six main parameters of our app-roach (i.e., p min , acc (  X  ), b tot , k ,INIT SELECT methods, and MAIN SELECT methods) affect the quality of the classification results. Fig. 3 (a) shows the F-measure of our approach for different minimum purity thresholds ( F-measure increases with an increasing p min since a higher purity of cluster requirement results in more accurately classified clusters. As expected the F-measure also increases when the accuracy of the oracle ( (Fig. 3 (b)).
 vectors selected ( k ) as can be seen from Fig. 3 (c) and (d), respectively. Larger budgets allow more vectors to be manually labeled, and a larger number of weight vectors selected from each cluster can represent the clusters more effectively, resulting in increased F-measure. However, as can be seen when number of clusters can be manually assessed with larger k to lower F-measure. An interesting result is that a high F-measure (of achieved on all data sets even with a small budget size of though all three methods achieve high F-measure on all four data sets except the Far method on the ACM-DBLP data set, as shown in Fig. 3 (e). The selection methods Far and Far-Med perform equally well on all four data sets, while Ran does not consistently perform well, particularly over two relatively large data sets DBLP-GS and NCVR, due to its random selection. (see Fig. 3 (f)). Finally, we compared our approach with five baseline approaches as described above. Fig. 4 (a) shows the F-measure (effectiveness) of all six approaches and results illustrate that our active learning approach achieves significantly higher F-measure results compared to unsupervised approaches, and comparable results to fully supervised approaches, while requiring significantly lower runtime than all other approaches on all four data sets. We have developed an active learning approach for reducing the labeling costs in ER while achieving high linkage quality results. Experiments conducted on four real data sets validate the efficiency and effectiveness of our approach compared to both existing fully supervised and unsupervised ER classifiers.
 As future work we plan to study the following two issues. First, how does the ordering of clusters (line 5 in Algorithm 1) in the queue affect the training quality? Since only a limited labeling budget is available, the number of weight vectors a human oracle can manually label is restricted. Once the labeling budget is run out, the training selection process terminates. Thus, the cluster selected for manual labeling at each iteration should be the one that can provide an optimal improvement in the quality, coverage and representativeness of the training data set. Second, how can our approach be improved if the accuracy of a human oracle is known? Knowing this accuracy may significantly affect the purity calculation of clusters. It is thus plausible to enhance the performance of our approach by taking the accuracy of a human oracle into account.

