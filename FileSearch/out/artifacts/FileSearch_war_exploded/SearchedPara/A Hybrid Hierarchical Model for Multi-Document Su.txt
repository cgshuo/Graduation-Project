 Extractive approach to multi-document summa-rization (MDS) produces a summary by select-ing sentences from original documents. Doc-ument Understanding Conferences (DUC), now TAC, fosters the effort on building MDS systems, which take document clusters (documents on a same topic) and description of the desired sum-mary focus as input and output a word length lim-ited summary. Human summaries are provided for training summarization models and measuring the performance of machine generated summaries.

Extractive summarization methods can be clas-sified into two groups: supervised methods that rely on provided document-summary pairs, and unsupervised methods based upon properties de-rived from document clusters. Supervised meth-ods treat the summarization task as a classifica-tion/regression problem, e.g., (Shen et al., 2007; Yeh et al., 2005). Each candidate sentence is classified as summary or non-summary based on the features that they pose and those with high-est scores are selected. Unsupervised methods aim to score sentences based on semantic group-ings extracted from documents, e.g., (Daum  X  e III and Marcu, 2006; Titov and McDonald, 2008; Tang et al., 2009; Haghighi and Vanderwende, 2009; Radev et al., 2004; Branavan et al., 2009), etc. Such models can yield comparable or bet-ter performance on DUC and other evaluations, since representing documents as topic distribu-tions rather than bags of words diminishes the ef-fect of lexical variability. To the best of our knowl-edge, there is no previous research which utilizes the best features of both approaches for MDS as presented in this paper.

In this paper, we present a novel approach that formulates MDS as a prediction problem based on a two-step hybrid model: a generative model for hierarchical topic discovery and a regression model for inference. We investigate if a hierarchi-cal model can be adopted to discover salient char-acteristics of sentences organized into hierarchies utilizing human generated summary text.

We present a probabilistic topic model on sen-tence level building on hierarchical Latent Dirich-let Allocation (hLDA) (Blei et al., 2003a), which is a generalization of LDA (Blei et al., 2003b). We construct a hybrid learning algorithm by extract-ing salient features to characterize summary sen-tences, and implement a regression model for in-ference (Fig.3). Contributions of this work are:  X  construction of hierarchical probabilistic model designed to discover the topic structures of all sen-tences. Our focus is on identifying similarities of candidate sentences to summary sentences using a novel tree based sentence scoring algorithm, con-cerning topic distributions at different levels of the discovered hierarchy as described in  X  3 and  X  4,  X  representation of sentences by meta-features to characterize their candidacy for inclusion in sum-mary text. Our aim is to find features that can best represent summary sentences as described in  X  5,  X  implementation of a feasible inference method based on a regression model to enable scoring of sentences in test document clusters without re-training, (which has not been investigated in gen-erative summarization models) described in  X  5.2.
We show in  X  6 that our hybrid summarizer achieves comparable (if not better) ROUGE score on the challenging task of extracting the sum-maries of multiple newswire documents. The hu-man evaluations confirm that our hybrid model can produce coherent and non-redundant summaries. There are many studies on the principles govern-ing multi-document summarization to produce co-herent and semantically relevant summaries. Pre-vious work (Nenkova and Vanderwende, 2005; Conroy et al., 2006), focused on the fact that fre-quency of words plays an important factor. While, earlier work on summarization depend on a word score function, which is used to measure sentence rank scores based on (semi-)supervised learn-ing methods, recent trend of purely data-driven methods, (Barzilay and Lee, 2004; Daum  X  e III and Marcu, 2006; Tang et al., 2009; Haghighi and Vanderwende, 2009), have shown remarkable im-provements. Our work builds on both methods by constructing a hybrid approach to summarization.
Our objective is to discover from document clusters, the latent topics that are organized into hi-erarchies following (Haghighi and Vanderwende, 2009). A hierarchical model is particularly ap-pealing to summarization than a  X  X lat X  model, e.g. LDA (Blei et al., 2003b), in that one can discover  X  X bstract X  and  X  X pecific X  topics. For instance, dis-covering that  X  X aseball X  and  X  X ootball X  are both contained in an abstract class  X  X ports X  can help to identify summary sentences. It follows that sum-mary topics are commonly shared by many docu-ments, while specific topics are more likely to be mentioned in rather a small subset of documents.
Feature based learning approaches to summa-rization methods discover salient features by mea-suring similarity between candidate sentences and summary sentences (Nenkova and Vanderwende, 2005; Conroy et al., 2006). While such methods are effective in extractive summarization, the fact that some of these methods are based on greedy algorithms can limit the application areas. More-over, using information on the hidden semantic structure of document clusters would improve the performance of these methods.

Recent studies focused on the discovery of la-tent topics of document sets in extracting sum-maries. In these models, the challenges of infer-ring topics of test documents are not addressed in detail. One of the challenges of using a pre-viously trained topic model is that the new docu-ment might have a totally new vocabulary or may include many other specific topics, which may or may not exist in the trained model. A common method is to re-build a topic model for new sets of documents (Haghighi and Vanderwende, 2009), which has proven to produce coherent summaries. An alternative yet feasible solution, presented in this work, is building a model that can summa-rize new document clusters using characteristics of topic distributions of training documents. Our approach differs from the early work, in that, we combine a generative hierarchical model and re-gression model to score sentences in new docu-ments, eliminating the need for building a genera-tive model for new document clusters. Our MDS system, hybrid hierarchical summa-rizer, HybHSum , is based on an hybrid learn-ing approach to extract sentences for generating summary. We discover hidden topic distributions of sentences in a given document cluster along with provided summary sentences based on hLDA described in (Blei et al., 2003a) 1 . We build a summary-focused hierarchical probabilistic topic model, sumHLDA, for each document cluster at sentence level, because it enables capturing ex-pected topic distributions in given sentences di-rectly from the model. Besides, document clusters contain a relatively small number of documents, which may limit the variability of topics if they are evaluated on the document level. As described in  X  4, we present a new method for scoring candidate sentences from this hierarchical structure.

Let a given document cluster D be represented human summary be represented with sentences S = V = w 1 ,w 2 ,..w | V | in { O  X  S } .
Summary hLDA (sumHLDA): The hLDA represents distribution of topics in sentences by organizing topics into a tree of a fixed depth L (Fig.1.a). Each candidate sentence o m is assigned to a path c o m in the tree and each word w i in a given sentence is assigned to a hidden topic z o m at a level l of c o m . Each node is associated with a topic distribution over words. The sampler method alternates between choosing a new path for each sentence through the tree and assigning each word in each sentence to a topic along that path. The structure of tree is learnt along with the topics us-ing a nested Chinese restaurant process (nCRP) (Blei et al., 2003a), which is used as a prior.
The nCRP is a stochastic process, which as-signs probability distributions to infinitely branch-ing and infinitely deep trees. In our model, nCRP specifies a distribution of words into paths in an L -level tree. The assignments of sentences to paths are sampled sequentially: The first sentence takes the initial L -level path, starting with a sin-gle branch tree. Later, m th subsequent sentence is assigned to a path drawn from the distribution: path old and path new represent an existing and novel (branch) path consecutively, m c is the num-ber of previous sentences assigned to path c , m is the total number of sentences seen so far, and  X  is a hyper-parameter which controls the probability of creating new paths. Based on this probability each node can branch out a different number of child nodes proportional to  X  . Small values of  X  suppress the number of branches.

Summary sentences generally comprise abstract concepts of the content. With sumHLDA we want to capture these abstract concepts in candidate sen-tences. The idea is to represent each path shared by similar candidate sentences with representative summary sentence(s). We let summary sentences share existing paths generated by similar candi-date sentences instead of sampling new paths and influence the tree structure by introducing two sep-arate hyper-parameters for nCRP prior:  X  if a summary sentence is sampled, use  X  =  X  s ,  X  if a candidate sentence is sampled, use  X  =  X  o .
At each node, we let summary sentences sample a path by choosing only from the existing children of that node with a probability proportional to the number of other sentences assigned to that child. This can be achieved by using a small value for  X  s ( 0 &lt;  X  s  X  1 ). We only let candidate sentences to have an option of creating a new child node with a probability proportional to  X  o . By choos-ing  X  s  X   X  o we suppress the generation of new branches for summary sentences and modify the  X  of nCRP prior in Eq.(1) using  X  s and  X  o hyper-parameters for different sentence types. In the ex-periments, we discuss the effects of this modifica-tion on the hierarchical topic tree.

The following is the generative process for sumHLDA used in our HybHSum : (1) For each topic k  X  T , sample a distribution  X  k v Dirichlet(  X  ). (2) For each sentence d  X  X  O  X  S } , ( a ) if d  X  O , draw a path c d v nCRP(  X  o ), ( b ) Sample L -vector  X  d mixing weights from ( c ) For each word n , choose: ( i ) level z d,n |  X  d
Given sentence d ,  X  d is a vector of topic pro-portions from L dimensional Dirichlet parameter-ized by  X  (distribution over levels in the tree.) The n th word of d is sampled by first choosing a level z d,n = l from the discrete distribution  X  d with probability  X  d,l . Dirichlet parameter  X  and  X  o con-trol the size of tree effecting the number of topics. (Small values of  X  s do not effect the tree.) Large values of  X  favor more topics (Blei et al., 2003a).
Model Learning: Gibbs sampling is a common method to fit the hLDA models. The aim is to ob-tain the following samples from the posterior of: ( i ) the latent tree T , ( ii ) the level assignment z for all words, ( iii ) the path assignments c for all sen-tences conditioned on the observed words w .
Given the assignment of words w to levels z and assignments of sentences to paths c , the expected posterior probability of a particular word w at a given topic z = l of a path c = c is proportional to the number of times w was generated by that topic: Similarly, posterior probability of a particular topic z in a given sentence d is proportional to number of times z was generated by that sentence: n ( . ) is the count of elements of an array satisfy-ing the condition. Note from Eq.(3) that two sen-tences d 1 and d 2 on the same path c would have different words, and hence different posterior topic probabilities. Posterior probabilities are normal-ized with total counts and their hyperparameters. The sumHLDA constructs a hierarchical tree structure of candidate sentences (per document cluster) by positioning summary sentences on the tree. Each sentence is represented by a path in the tree, and each path can be shared by many sen-tences. The assumption is that sentences sharing the same path should be more similar to each other because they share the same topics. Moreover, if a path includes a summary sentence, then candi-date sentences on that path are more likely to be selected for summary text. In particular, the sim-ilarity of a candidate sentence o m to a summary sentence s n sharing the same path is a measure of strength, indicating how likely o m is to be in-cluded in the generated summary (Algorithm 1):
Let c o m be the path for a given o m . We find summary sentences that share the same path with o m via: M = { s n  X  S | c s n = c o m } . The score of each sentence is calculated by similarity to the best matching summary sentence in M : If M=  X  , then score( o m )=  X  . The efficiency of our similarity measure in identifying the best match-ing summary sentence, is tied to how expressive the extracted topics of our sumHLDA models are. Given path c o m , we calculate the similarity of o m to each s n , n =1.. | M | by measuring similarities on: ? sparse unigram distributions ( sim 1 ) at each topic l on c o m : similarity between p ( w o l,c o ?? distributions of topic proportions ( sim 2 ); similarity between p ( z o m | c o m ) and p ( z s n | c o  X  sim 1 : We define two sparse (discrete) un-igram distributions for candidate o m and sum-mary s n at each node l on a vocabulary iden-tified with words generated by the topic at that node, v l  X  V . Given w o m = w 1 ,...,w | o let w o are generated from topic z o m at level l on path c m . The discrete unigram distribution p o m l p ( w o ity over all words v l assigned to topic z o m at level l , by sampling only for words in w o p words w s n in s n of the same topic. The proba-bility of each word in p o using Eq. (2) and then normalized (see Fig.1.b).
The similarity between p o obtained by first calculating the divergence with information radius-IR based on Kullback-Liebler(KL) divergence, p=p o is transformed into a similarity measure (Manning and Schuetze, 1999): IR is a measure of total divergence from the av-erage, representing how much information is lost when two distributions p and q are described in terms of average distributions. We opted for IR instead of the commonly used KL because with IR there is no problem with infinite values since like KL , IR is symmetric, i.e., KL(p,q) 6 = KL(q,p) .
Finally sim 1 is obtained by average similarity of sentences using Eq.(6) at each level of c o m by: The similarity between p o is weighted proportional to the level l because the similarity between sentences should be rewarded if there is a specific word overlap at child nodes.  X  sim 2 : We introduce another measure based on sentence-topic mixing proportions to calculate the concept-based similarities between o m and s n . We calculate the topic proportions of o m and s n , p ( z s distributions is then measured with transformed IR as in Eq.(6) by: sim 1 provides information about the similarity between two sentences, o m and s n based on topic-word distributions. Similarly, sim 2 provides in-formation on the similarity between the weights of the topics in each sentence. They jointly effect the sentence score and are combined in one measure: sim ( o m ,s n ) = sim 1 ( o m ,s n )  X  sim 2 ( o m ,s n ) (9) The final score for a given o m is calculated from Eq.(4). Fig.1.b depicts a sample path illustrating sparse unigram distributions of o m and s m at each level as well as their topic proportions, p z o m , and p s n . In experiment 3, we discuss the effect of our tree-based scoring on summarization performance in comparison to a classical scoring method pre-sented as our baseline model. Each candidate sentence o m , m = 1 .. | O | is rep-resented with a multi-dimensional vector of q fea-tures f m = { f m 1 ,...,f mq } . We build a regression model using sentence scores as output and selected salient features as input variables described below: 5.1 Feature Extraction We compile our training dataset using sentences from different document clusters, which do not necessarily share vocabularies. Thus, we create n-gram meta -features to represent sentences instead of word n-gram frequencies: (I) nGram Meta-Features (NMF): For each document cluster D , we identify most fre-quent (non-stop word) unigrams, i.e., v freq = { w i } r i =1  X  V , where r is a model param-eter of number of most frequent unigram fea-tures. We measure observed unigram proba-bilities for each w i  X  v freq with p D ( w i ) = n
D ( w i ) / P number of times w i appears in D and | V | is the total number of unigrams. For any i th feature, the value is f mi = 0 , if given sentence does not con-tain w i , otherwise f mi = p D ( w i ) . These features can be extended for any n -grams. We similarly include bigram features in the experiments. (II) Document Word Frequency Meta-Features (DMF): The characteristics of sentences at the document level can be important in sum-mary generation. DMF identify whether a word in a given sentence is specific to the document in consideration or it is commonly used in the document cluster. This is important because summary sentences usually contain abstract terms rather than specific terms.

To characterize this feature, we re-use the r most frequent unigrams, i.e., w i  X  v freq . Given sentence o m , let d be the document that o m be-longs to, i.e., o m  X  d . We measure unigram prob-abilities for each w i by p ( w i  X  o m ) = n d ( w i  X  o m ) /n D ( w i ) , where n d ( w i  X  o m ) is the number of times w i appears in d and n D ( w i ) is the number of times w i appears in D . For any i th feature, the value is f mi = 0 , if given sentence does not con-tain w i , otherwise f mi = p ( w i  X  o m ) . We also include bigram extensions of DMF features. (III) Other Features (OF): Term frequency of sentences such as SUMBASIC are proven to be good predictors in sentence scoring (Nenkova and Vanderwende, 2005). We measure the average unigram probability of a sentence by: p ( o m ) = unigram probability in the document collection D and | o m | is the total number of words in o m . We use sentence bigram frequency, sentence rank in a document, and sentence size as additional fea-tures. 5.2 Predicting Scores for New Sentences Due to the large feature space to explore, we chose to work with support vector regression (SVR) (Drucker et al., 1997) as the learning algorithm to predict sentence scores. Given training sen-tences { f m ,y m } | O | m =1 , where f m = { f m 1 ,...,f is a multi-dimensional vector of features and y m = score( o m )  X  R are their scores obtained via Eq.(4), we train a regression model. In experi-ments we use non-linear Gaussian kernel for SVR. Once the SVR model is trained, we use it to predict the scores of n test number of sentences in test ( un-seen ) document clusters, O test = o 1 ,...o | O
Our HybHSum captures the sentence character-istics with a regression model using sentences in different document clusters. At test time, this valu-able information is used to score testing sentences.
Redundancy Elimination: To eliminate redun-dant sentences in the generated summary, we in-crementally add onto the summary the highest ranked sentence o m and check if o m significantly repeats the information already included in the summary until the algorithm reaches word count limit. We use a word overlap measure between sentences normalized to sentence length. A o m is discarded if its similarity to any of the previously selected sentences is greater than a threshold iden-tified by a greedy search on the training dataset. In this section we describe a number of experi-ments using our hybrid model on 100 document clusters each containing 25 news articles from DUC2005-2006 tasks. We evaluate the perfor-mance of HybHSum using 45 document clusters each containing 25 news articles from DUC2007 task. From these sets, we collected v 80K and 25K sentences to compile training and testing data respectively. The task is to create max. 250 word long summary for each document cluster.
We use Gibbs sampling for inference in hLDA and sumHLDA. The hLDA is used to capture ab-straction and specificity of words in documents (Blei et al., 2009). Contrary to typical hLDA mod-els, to efficiently represent sentences in summa-rization task, we set ascending values for Dirichlet hyper-parameter  X  as the level increases, encour-aging mid to low level distributions to generate as many words as in higher levels, e.g., for a tree of depth=3,  X  = { 0 . 125 , 0 . 5 , 1 } . This causes sen-tences share paths only when they include similar concepts, starting higher level topics of the tree. For SVR, we set = 0 . 1 using the default choice, which is the inverse of the average of  X  ( f ) T  X  ( f ) (Joachims, 1999), dot product of kernelized input vectors. We use greedy optimization during train-ing based on ROUGE scores to find best regular-izer C = 10  X  1 .. 10 2 using the Gaussian kernel.
We applied feature extraction of  X  5.1 to com-pile the training and testing datasets. ROUGE is used for performance measure (Lin and Hovy, 2003; Lin, 2004), which evaluates summaries based on the maxium number of overlapping units between generated summary text and a set of hu-man summaries. We use R-1 (recall against uni-grams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams).

Experiment 1 : sumHLDA Parameter Analy-sis: In sumHLDA we introduce a prior different than the standard nested CRP (nCRP). Here, we illustrate that this prior is practical in learning hi-erarchical topics for summarization task.

We use sentences from the human generated summaries during the discovery of hierarchical topics of sentences in document clusters. Since summary sentences generally contain abstract words, they are indicative of sentences in docu-ments and should produce minimal amount of new topics (if not none). To implement this, in nCRP prior of sumHLDA, we use dual hyper-parameters and choose a very small value for summary sen-tences,  X  s = 10 e  X  4  X  o . We compare the re-sults to hLDA (Blei et al., 2003a) with nCRP prior which uses only one free parameter,  X  . To ana-lyze this prior, we generate a corpus of v 1300 sen-tences of a document cluster in DUC2005. We re-peated the experiment for 9 other clusters of sim-ilar size and averaged the total number of gener-ated topics. We show results for different values of  X  and  X  o hyper-parameters and tree depths.
As shown in Table 1, the nCRP prior for sumHLDA is more effective than hLDA prior in the summarization task. Less number of top-ics(nodes) in sumHLDA suggests that summary sentences share pre-existing paths and no new paths or nodes are sampled for them. We also observe that using  X  o = 0 . 1 causes the model to generate minimum number of topics (# of top-ics=depth), while setting  X  o = 10 creates exces-sive amount of topics.  X  0 = 1 gives reasonable number of topics, thus we use this value for the rest of the experiments. In experiment 3, we use both nCRP priors in HybHSum to analyze whether there is any performance gain with the new prior. Experiment 2 : Feature Selection Analysis Here we test individual contribution of each set of features on our HybHSum (using sumHLDA).
 We use a Baseline by replacing the scoring algo-rithm of HybHSum with a simple cosine distance measure. The score of a candidate sentence is the cosine similarity to the maximum matching sum-mary sentence. Later, we build a regression model with the same features as our HybHSum to create a summary. We train models with DUC2005 and evaluate performance on DUC2006 documents for different parameter values as shown in Table 2.
As presented in  X  5, NMF is the bundle of fre-quency based meta-features on document cluster level, DMF is a bundle of frequency based meta-features on individual document level and OF rep-resents sentence term frequency, location, and size features. In comparison to the baseline, OF has a significant effect on the ROUGE scores. In addi-tion, DMF together with OF has shown to improve all scores, in comparison to baseline, on average by 10%. Although the NMF have minimal indi-vidual improvement, all these features can statis-tically improve R-2 without stop words by 12% (significance is measured by t-test statistics). Experiment 3 : ROUGE Evaluations We use the following multi-document summariza-tion models along with the Baseline presented in Experiment 2 to evaluate HybSumm . ? PYTHY : (Toutanova et al., 2007) A state-of-the-art supervised summarization system that ranked first in overall ROUGE evaluations in DUC2007. Similar to HybHSum , human gener-ated summaries are used to train a sentence rank-ing system using a classifier model. ? HIERSUM : (Haghighi and Vanderwende, 2009) A generative summarization method based on topic models, which uses sentences as an addi-tional level. Using an approximation for inference, sentences are greedily added to a summary so long as they decrease KL-divergence. ? HybFSum (Hybrid Flat Summarizer): To investigate the performance of hierarchical topic model, we build another hybrid model using flat LDA (Blei et al., 2003b). In LDA each sentence is a superposition of all K topics with sentence specific weights, there is no hierarchical relation between topics. We keep the parameters and the features of the regression model of hierarchical HybHSum intact for consistency. We only change the sentence scoring method. Instead of the new tree-based sentence scoring (  X  4), we present a similar method using topics from LDA on sen-tence level. Note that in LDA the topic-word dis-tributions  X  are over entire vocabulary, and topic mixing proportions for sentences  X  are over all the topics discovered from sentences in a docu-ment cluster. Hence, we define sim 1 and sim 2 measures for LDA using topic-word proportions  X  (in place of discrete topic-word distributions from each level in Eq.2) and topic mixing weights  X  in sentences (in place of topic proportions in Eq.3) respectively. Maximum matching score is calcu-lated as same as in HybHSum . ? HybHSum 1 and HybHSum 2 : To analyze the ef-fect of the new nCRP prior of sumHLDA on sum-Table 3: ROUGE results of the best systems on DUC2007 dataset (best results are bolded .) marization model performance, we build two dif-ferent versions of our hybrid model: HybHSum 1 using standard hLDA (Blei et al., 2003a) and HybHSum 2 using our sumHLDA.
 The ROUGE results are shown in Table 3. The HybHSum 2 achieves the best performance on R-1 and R-4 and comparable on R-2. When stop words are used the HybHSum 2 outperforms state-of-the-art by 2.5-7% except R-2 (with statistical significance). Note that R-2 is a measure of bi-gram recall and sumHLDA of HybHSum 2 is built on unigrams rather than bigrams. Compared to the HybFSum built on LDA, both HybHSum 1&amp;2 yield better performance indicating the effective-ness of using hierarchical topic model in summa-rization task. HybHSum 2 appear to be less re-dundant than HybFSum capturing not only com-mon terms but also specific words in Fig. 2, due to the new hierarchical tree-based sentence scor-ing which characterizes sentences on deeper level. Similarly, HybHSum 1&amp;2 far exceeds baseline built on simple classifier. The results justify the per-formance gain by using our novel tree-based scor-ing method. Although the ROUGE scores for HybHSum 1 and HybHSum 2 are not significantly different, the sumHLDA is more suitable for sum-marization tasks than hLDA.

HybHSum 2 is comparable to (if not better than) fully generative HIERSUM . This indicates that with our regression model built on training data, summaries can be efficiently generated for test documents (suitable for online systems).
 Experiment 4 : Manual Evaluations Here, we manually evaluate quality of summaries, a common DUC task. Human annotators are given two sets of summary text for each document set, generated from two approaches: best hierarchi-cal hybrid HybHSum 2 and flat hybrid HybFSum models, and are asked to mark the better summary according to five criteria: non-redundancy (which summary is less redundant), coherence (which summary is more coherent), focus and readabil-ity (content and not include unnecessary details), responsiveness and overall performance.

We asked 4 annotators to rate DUC2007 pre-dicted summaries (45 summary pairs per anno-tator). A total of 92 pairs are judged and eval-uation results in frequencies are shown in Table 4. The participants rated HybHSum 2 generated summaries more coherent and focused compared to HybFSum . All results in Table 4 are statis-tically significant (based on t-test on 95% con-fidence level.) indicating that HybHSum 2 sum-maries are rated significantly better. In this paper, we presented a hybrid model for multi-document summarization. We demonstrated that implementation of a summary focused hierar-chical topic model to discover sentence structures as well as construction of a discriminative method for inference can benefit summarization quality on manual and automatic evaluation metrics.
 Acknowledgement Research supported in part by ONR N00014-02-1-0294, BT Grant CT1080028046, Azerbaijan Min-istry of Communications and Information Tech-nology Grant, Azerbaijan University of Azerbai-jan Republic and the BISC Program of UC Berke-ley.
 R. Barzilay and L. Lee. Catching the drift: Proba-bilistic content models with applications to gen-eration and summarization. In In Proc. HLT-NAACL X 04 , 2004.
 D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.
Hierarchical topic models and the nested chi-nese restaurant process. In In Neural Informa-tion Processing Systems [NIPS] , 2003a.
 D. Blei, T. Griffiths, and M. Jordan. The nested chinese restaurant process and bayesian non-parametric inference of topic hierarchies. In Journal of ACM , 2009.
 D. M. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. In Jrnl. Machine Learning Research, 3:993-1022 , 2003b.
 S.R.K. Branavan, H. Chen, J. Eisenstein, and
R. Barzilay. Learning document-level seman-tic properties from free-text annotations. In
Journal of Artificial Intelligence Research , vol-ume 34, 2009.
 J.M. Conroy, J.D. Schlesinger, and D.P. O X  X eary.
Topic focused multi-cument summarization us-ing an approximate oracle score. In In Proc. ACL X 06 , 2006.
 H. Daum  X  e III and D. Marcu. Bayesian query fo-cused summarization. In Proc. ACL-06 , 2006. H. Drucker, C.J.C. Burger, L. Kaufman, A. Smola, and V. Vapnik. Support vector regression ma-chines. In NIPS 9 , 1997.
 A. Haghighi and L. Vanderwende. Exploring con-tent models for multi-document summarization. In NAACL HLT-09 , 2009.
 T. Joachims. Making large-scale svm learning practical. In In Advances in Kernel Methods -Support Vector Learning. MIT Press. , 1999.
 C.-Y. Lin. Rouge: A package for automatic evalu-ation of summaries. In In Proc. ACL Workshop on Text Summarization Branches Out , 2004. C.-Y. Lin and E.H. Hovy. Automatic evaluation of summaries using n-gram co-occurance statis-tics. In Proc. HLT-NAACL, Edmonton, Canada , 2003.
 C. Manning and H. Schuetze. Foundations of sta-tistical natural language processing. In MIT Press. Cambridge, MA , 1999.
 A. Nenkova and L. Vanderwende. The impact of frequency on summarization. In Tech. Report
MSR-TR-2005-101, Microsoft Research, Red-wood, Washington , 2005.
 D.R. Radev, H. Jing, M. Stys, and D. Tam.
Centroid-based summarization for multiple documents. In In Int. Jrnl. Information Process-ing and Management , 2004.
 D. Shen, J.T. Sun, H. Li, Q. Yang, and Z. Chen.
Document summarization using conditional random fields. In Proc. IJCAI X 07 , 2007.
 J. Tang, L. Yao, and D. Chens. Multi-topic based query-oriented summarization. In SIAM Inter-national Conference Data Mining , 2009.
 I. Titov and R. McDonald. A joint model of text and aspect ratings for sentiment summarization. In ACL-08:HLT , 2008.
 K. Toutanova, C. Brockett, M. Gamon, J. Jagarla-mudi, H. Suzuki, and L. Vanderwende. The ph-thy summarization system: Microsoft research at duc 2007. In Proc. DUC , 2007.
 J.Y. Yeh, H.-R. Ke, W.P. Yang, and I-H. Meng.
Text summarization using a trainable summa-rizer and latent semantic analysis. In Informa-tion Processing and Management , 2005.
