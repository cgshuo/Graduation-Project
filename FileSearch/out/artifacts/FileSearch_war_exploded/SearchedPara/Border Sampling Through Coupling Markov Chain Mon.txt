 proposed for sample selection in supervised learning by progressively learning an augmented full border from small labeled datasets. However, this quadratic learning algorithm is inapplicable to large datasets. In this paper, we incorporate the PBS to a state of the art technique called Coupling Markov Chain Monte Carlo (CMCMC) in an attempt to scale the original algorithm up on large labeled datasets. The CMCMC can produce an exact sample while a naive strategy for Markov Chain Monte Carlo cannot guarantee the convergence to a stationary distribution. The resulting CMCMC-PBS algorithm is thus proposed for border sampling on large datasets. CMCMC-PBS exhibits several remarkable characteristics: linear time complexity, learner-independence, and a consistent convergence to an optimal sample from the original training sets by learning from their subsamples. Our experimental results on the 33 either small or large labeled datasets from the UCIKDD repository and a nuclear security application show that our new approach outperforms many previous sampling techniques for sample selection. Progressive Border Sampling (PBS) was proposed to learn a small sample from original populations by incorporating a novel method called the Border Identification in Two Stages (BI 2 ) algorithm with progressive learning techniques [12]. Border sampling by PBS tends to find a theoretically minimally sufficient training set given any training set. Its main advantage is that it is learner-independent. As a result, many classic learning algorithms can build successful classifiers on the resulting sample without any loss of performance with respect to the classifiers built on the full training sets [12]. On the other hand, despite this advantage, the algorithm is still inapplicable to large datasets due to its quadratic learning time. For example, the algorithm could not efficiently process the UCIKDD [2] Letter datasets (with twenty thousand data points) or even the UCIKDD Splice datasets (with three thousand data points). large datasets [9][15]. However, there exist some vital drawbacks to this research. For example, within the classification branch of machine learning, Progressive Sampling techniques (PS) [9][15][24] are subject to a failure in converging to an optimal sample due to the bias of a base learner. Active learning techniques or semi-supervised learning whose goal is to reduce learning cost for labeling [3][22] suffer from the same difficulty as PS to converge to an optimal sample due to the high bias of the selected learner. Conversely, we believe that reducing the variance of the data caused by redundancies can help reduce the learning cost without loss of performance. theoretical issues related to the previous PBS technique. In terms of this theoretical foundation, we investigate and discuss the scalability of PBS to large datasets. A natural way is to adopt the standard Markov Chain Monte Carlo (MCMC) [1] for border sampling on large datasets. Among the variant MCMC techniques, the state of the art Coupling Markov Chain Monte Carlo (CMCMC) can produce an exact sample while the standard MCMC cannot guarantee to converge to the stationary distribution [1][14]. border sampling on large datasets, and propose a new approach, called Coupling Markov Chain Monte Carlo-based PBS (CMCMC-PBS), in which two interactive Markov chains, called the B chain for border points and the R chain for redundant data points are evolved by using PBS as an oracle. Correspondingly, the convergence detection for the B chain and the collapsing condition for the R chain are heuristically defined. There are three main advantages to the CMCMC-PBS. First, it is independent of inductive algorithms as PBS itself. Therefore, it can unlimitedly learn an optimal sample by reducing the variance of the data due to redundancies from the original large population. Second, the CMCMC-PBS is a linear algorithm and can efficiently converge to a perfect sample by calling many small subsamples with a rapid mixing time related to the CMCMC techniques. Therefore, it is feasible to use in practical applications. Third, CMCMC-PBS is not restricted for use in either small or large datasets because it is not sensitive to the sampling window. At the extreme case, the whole training set is fitted in the sampling window. with previous sampling techniques for reducing learning cost in large labeled training sets [9][15] by conducting experiments on benchmark datasets from UCIKDD repository [2] , as well as on a problem of nuclear explosion detection through the monitoring of radioxenon levels in the atmosphere [20]. This work is conducted in the context of the Comprehensive Nuclear-Test-Ban Treaty (CTBT) whose purpose is to ban the testing of nuclear explosive devices worldwide. The remainder of this paper is organized as follows. In Section 2, we review the theoretical foundation for Border Identification (BI) and Markov Chain Monte Carlo techniques proposed in previous research. Our main work for border sampling on large datasets is described in Section 3. In Section 4, we describe our experimental design and results. We conclude and suggest future work in Section 5. Identification in Two Stages, denoted as BI 2 proposed for full border identification by avoiding the limitation of the traditional BI, which only discusses partial borders [12]. We give definitions and a formal description of BI 2 and PBS in the following sections followed by a review of MCMC. the nearest neighbor of a data point p among all data points (the entire domain); which returns the nearest neighbor or the informative data point of a data point p in the given domain D; given data point p; p, denoted as C p . Definition 1. Given a labeled dataset D and its subset B  X  D, any point p  X  D  X  B is a redundant data point with respect to B if p  X  = 1NN(p, B) and l (p) = l (p  X  ). A set of redundant points R with respect to B, denoted as B) and l(p) = l(p  X  )}, denoted as R(B), without any confusion. Definition 2 . Given a labeled dataset D, the full border B of D can be defined recursively as follows: 1) B = B  X  B n , where B n = {q |  X  p  X  D,  X  q  X  D, q = 2) B = B  X  B f , where B f = {q |  X  p  X  B,  X  q  X  D, q = redundant data point with respect to the full border B is always near data points of the same category and far from data points of different categories. The related proofs are omitted due to space limitation. i.e., in the first stage, the BI 2 identifies the near border between any two categories. In the second stage, the BI 2 will iteratively identify new far borders in the two categories. For example, a simple XOR function can be visualized by 4 labeled data points in 2D. The BI 2 can identify two near border points and two far border points from the XOR domain while the depth of the recursion for far border points is 1. Empirically, the maximum depth of the recursion is shown with a bound (&lt;&lt; n) in many practical applications [12] (see  X 3.2.3). insufficient for statistical learning, Progressive Border Sampling (PBS) [12] has been proposed to progressively learn an augmented full border in the pairwise strategy [7][21] for multiclass domains such that the resulting border points can be used for training in supervised learning tasks [12]. emphasize that PBS can be equivalent to the BI 2 only for full border identification by ignoring convergence detection for an augmented full border, and it can be regarded as a forward selection for border sampling. border sampling on large datasets. In this paper, we use the PBS as an oracle for border sampling on large datasets by adopting the CMCMC. is a sampling technique such that selecting sample x (i+1) only depends on sample x (i) , where the superscript i is a nonnegative integer, and the chain is expected to converge to a stationary distribution  X  with two properties: irreducible and aperiodic. time, which measures how quickly much a Markov chain takes to eliminate the bias of the starting point x (0) . The mixing rate measures how fast a Markov chain converges. Ideally, the stationary distribution of a good chain is reached quickly starting from an arbitrary position, i.e., low burn-in time and mixing rate. design a transition matrix, e.g., a Markov chain transition graph for webpages and links [1], for guiding the evolution of a chain only if the transition matrix follows the two properties. Essentially, we are required to design a function or algorithm to establish the transition matrix for the evolution of a chain. MCMC, the Coupling From The Past (CFTP) is an exact sampling technique, which consists of the following three main components [14]: oracle, which is a random map procedure which generates a subsample from the original population; the composition of maps, which can be used to simulate the flow for many time-steps; the convergence detection, which is used for ascertaining whether total coalescence has occurred. ..., f -N , where N is how far we have to go into the past, and is determined at run-time. We can define a composite map by N 3 2 1 the composite map must bring in collapsing with respect to some N. i.e., the PBS, on large datasets can be depicted as follows. Given a large training set D and the specified size N of a partition, called the sampling window , we can obtain M subsamples, S i , i = 1,..., M, where M = |D| / N, by stratified sampling. The PBS can be executed as BI 2 to identify each local full border B i on each subsample S i , and the resulting border is given by i M i B B 1 = =  X  . Standard stratified sampling techniques are used for reducing the variance in estimation of the Monte Carlo analysis. The MCMC technique iteratively produces successive samples containing border points from the previously identified borders. The successive samples evolve from a large population with many redundant data points to a small population with few redundant data points. As a result, the MCMC can be regarded to as a backward elimination method for border sampling. above, is insufficient to converge to the stationary distribution  X  because we cannot guarantee the monotonicity of the states, i.e., P(B (i) )  X  P(B (i+1) i represents the i th state B of the Markov Chain. composite B containing border points identified by the oracle from subsamples defined by a specified sampling window in the na X ve strategy. B does not contain sufficient border points while D  X  = D  X  B does not purely contain redundant points. The na X ve strategy can be used again on D  X  for new border points. The B can be augmented by adding the new border points while D  X  is reduced by removing the new border points identified from it. As a result, the iterative procedure can cause D  X  collapsing to some star convex graph with possible fewer border points. For example, a simple XOR domain is thought to satisfy the collapsing condition for a star convex graph with 4 data points. defined by c(c  X  1) + 2 by assuming that each pair of classes has at least two border points and a simple XOR structure, where c is the number of classes. sequence of B for the sets of border points, called the B chain , and the sequence of D  X  for the sets of redundant points, called the R chain . The iterative procedure can be regarded to as a Coupling MCMC (CMCMC) consisting of the B chain and multiple R chains. [12] is shown in Fig. 1, where some states of the B chain and the R chains of the CMCMC generated by the CMCMC-PBS algorithm (see  X 3.2) are depicted. complicated XOR binary domain with 640 data points, the sampling window for stratified sampling is set to 100. The original dataset is the beginning state of the B chain, and it becomes the beginning state of the first R Fig. 1. A B chain and two successive R chains of the CMCMC from a to h . Fig. 2. The third state of the B chain obtained by removing data points in e from h in Fig. 1. chain; ( b ) a state of the first R chain; ( c ) the state of the first R chain ahead of the collapsing state; ( d ) the collapsing state, which is nearly a star convex graph with many redundant data points and fewer border points; the ending state through 7 states of the first R chain; ( e ) the new (second) state of the B chain, which has 485 data points fewer than the original 640 data points. It was obtained by removing redundancy in ( d ) from the first state of the first R, i.e., ( a ), and becomes the beginning state of the second R chain; ( f ) a state of the second R chain; ( g ) the state of the second R chain ahead of the next collapsing state; ( h ) collapsing due to the presence of fewer redundant data points fitting in the sample window; the ending state through 4 states of the second R chain. Correspondingly, the third state of the B chain is obtained by retaining border points after redundant data points identified in ( h ) are removed from ( e ), as shown in Fig. 2, which is coalescent to e , and leads to the occurrence of convergence. The resulting sample in e is returned through 3 states in B chain and 11 states in two R chains of the CMCMC. propose a new method, called Coupling Markov Chain Monte Carlo-based PBS (CMCMC-PBS), see below, for border sampling on large datasets. Given the two inputs of the algorithm, the training set D and the sampling window W, it returns the result in Border. The algorithm produces the B chain in the while loop from Step 2 to Step 6 while the R chain is generated in the while loop from Step 11 to 21. base learner for convergence detection of the B chain at Steps 4 and 5. NB has been successfully used for progressive learning for convergence detection [9][12], i.e., ValidateNBModel() is used for building a NB classifier for estimating the current sample D  X  , and the downside or the beginning points of the plateau (not fringe) of the generated adaptive learning curve saved in LearningCurve is used as a convergent point. while loop beginning at Step 11 while initially the condition of the forward selection of the PBS is defined at Step 10 (see  X 3.2.4). The floor function is used for specifying a sampling window at Step 12, and the stratified sampling technique helps reduce the variance of MCMC at Step 13. The PBS is used as an oracle for identifying local full borders at Step 17, and the algorithm tests collapsing at Step 19 according to the collapsing condition. Initially, cg is false at Step 9. It leads to the oracle performing as BI 2 only for full border identification at Step 17. Because S will be shrunken at Step 18, when S is fitted in the sampling window W, PBS searches for an augmented full border in forward selection. The while loop exits at Step 20 if the conditions are met. The result is returned at Step 22 by removing redundancies in S  X  from the input D. 3.2.1. Relation to CFTP. We propose CMCMC by adapting CFTP to border sampling. PBS is suggested as our oracle, and a state in the B chain corresponds to a composition map defined by those states in the related R chain. Essentially, the oracle establishes the transition matrices related to the B chain and the R chain for their evolutions. In a sense, these transition matrices obey the laws of irreducibility and aperiodicity. Collapsing can be observed by approximately testing the occurrence of some star convex graph from the states of the R chain. The NB is assumed for convergence detection for the B chain. 3.2.2. Similarity measure s . Generally, any distance metric or similarity measure can be used in the oracle of the CMCMC-PBS for searching for border points, e.g., Radial-Based Function (RBF), Cosine, Euclidean distance or normalized Euclidean distance, Pearson Coefficient, Mahalanobis distance, and Extended Jaccard similarity [18], etc. However, different effects have been observed in the oracle, e.g., RBF is bias to the class contour while Cosine is bias to the class core [12]. Instead of developing an ideal similarity, we empirically show that RBF, e.g., in Fig. 1, has an asymptotical effect for Monte Carlo integration in CMCMC-PBS in most cases. 3.2.3. Linear time complexity. Clearly, the space complexity of the CMCMC-PBS is a linear increase. We analyze its time complexity as follows. Considering the two while loops in the CMCMC-PBS, the time complexity can be first simply given by O(T  X  K  X  N/W  X  C 0 ), where T is the number of tries for convergence detection in the while loop beginning at Step 2; K is the number of iteration in the while loop beginning at Step 11 in Coupling; N is the size of a given training set D and W is the sampling window; C 0 is the time complexity of the oracle with the sampling window W, and C 0 = O(T 0 K 0 FW 2 ), where F is the number of features; K 0 is the depth of the recursive far borders; T 0 is the number of tries for convergence detection [12]. Therefore, the time complexity of the CMCMC-PBS is given by where the term O(TFN) [4][13] is the time complexity for learning a NB in the CMCMC-PBS. T 0 and K 0 are empirically analyzed in previous research by a bound with a small number ( X n) given a domain [12]. Especially, T 0 = 1 if the oracle runs as BI 2 . strategy for border identification on multi-domains, K 0 has nothing to do with the number of classes. 8 data points in 2D is constructed as the worst case, as shown in Fig. 3. The depth of the recursive far borders is 3. Further, we obtain an upper bound, i.e., K 0  X  2F  X  1, by a constructed XOR of dimension F. It is just equal to the size of the boundary of an F-cube minus one. Empirically, K 0 is much smaller than F [12]. Fig. 3. The extended XOR with 8 data points in 2D. points on the entire dataset by the oracle until a collapsing occurs. K is a little domain-related, e.g., redundancies, while it is related to collapsing test. But its small value ( X n) has been observed. efficient learning method in linear time complexity with respect to the sample size N for border sampling. 3.2.4. Convergence detect ions and collapsing. The NB is used for convergence detection in the B chain from Step 2 to Step 6 of the CMCMC-PBS algorithm. Empirically, it is not always effective to track the adaptive learning curve of this linear machine for convergence detection if random sampling is used. However, it has been shown that the effectiveness can be obtained precisely by border sampling [12]. convergence detection by setting cg = true at Step 14, T &gt; 1. As a result, the CMCMC-PBS performs two convergence detections (T times for the CMCMC in the B chain and T 0 times in the oracle [12]). On the other hand, collapsing detection (K times for the R chain) is heuristically defined by assuming the occurrence of a star convex with less border points. 1 Border =  X  , i = 0..K, D  X  = D, LearningCurve[0] = 0; 3 D  X  = Coupling (D  X  , W) linear functions approaching to the class boundary in the CMCMC-PBS. In some cases, the oracle X  X  convergence detection can lead to the reduction of the CMCMC X  X  convergence detection. We emphasize that the CMCMC X  X  backward elimination is more efficient for convergence than the oracle X  X  forward selection in multiclass domains. sampling window and the number of classes  X  5, the oracle performs convergence detection for progressive border sampling, e.g., ( h ) in Fig. 1. 3.3.5. Learning measures. The ROC curve is drawn for selecting an optimal classifier [5] and assessing the ranking in terms of separation of the classes by Area under ROC curve (AUC). The AUC has been used for evaluating the performance of classifiers on class imbalanced domains because it is more discriminate than other learning measures, e.g., accuracy, and is not sensitive to imbalance [5]. The AUC is suggested as a learning measure for border sampling. As a result, the adaptive learning curve of NB is an AUC curve. and attempts to address its scalability on large datasets. The original BI 2 for full border identification and the original PBS for an augmented full border [12] have been adapted according to th e formalization in Def2. 1NN(p, C q ) and l (p)  X  l (q)} and B 2 = {q |  X  p  X  D,  X  q  X  D, q = 1NN(p) and l (p)  X  l (q)} are not equivalent. As a result, 1NN(.) is subject to failure for defining a border while 1NN(.,.) should be used to define the near border. This observation is used for explaining the main difference between the border sampling techniques shown in the BI 2 and the techniques for the reduction of training sets in those algorithms by the nearest neighbor editing rule [23], whose purpose is the reduction of training sets for Instance-Based Learning. We emphasize that the reduction of training sets should be one of the tasks of border sampling. Bayes and Decision Trees, etc, can be very fast at learning a good classifier even with a large training set of, say, ten thousand examples, reducing the sample size by simply selecting a small sample as per previous research is not expected to reduce the learning cost without loss of performance, e.g., active learning for sample selection [22]. On the other hand, we claim that our current research for border sampling in supervised learning can be easily migrated to active learning or semi-supervised learning. can be regarded as a learning method that builds a theory from examples available over time [6][19]0. For example, incremental SVM for online application has been studied in [11]. Clearly, the CMCMC-PBS can be easily used for incremental learning by suggesting and focusing on incremental sampling. Bayesian decision rule can define a Bayesian decision boundary by discriminating functions g i (x) = p(x|w i )p(w i ) [4]. Our research suggests that the new method tends to learn optimal class conditional probability distributions p(x|w i ) by border sampling such that the related prior probability distribution p(w i and Bayesian decision boundary can be obtained. border data points lying close to the class boundary, it can produce an optimal sample for training. Thus, it provides a promising treatment for the class imbalance problem as compared with previous techniques for under-sampling and oversampling [8][10]. and results as follows. one obtained from a nuclear security application and 32 chosen from the UCIKDD repository [2]. For the application, a possible method of explosion detection for the Comprehensive nuclear-Test-Ban-Treaty [20] consists of monitoring the amount of radioxenon in the atmosphere by measuring and sampling the activity concentration of Xe-131m, Xe-133, Xe-133m, and Xe-135 by radionuclide monitoring. Several samples are synthesized under different circumstances of nuclear explosions, and combined with various levels of normal concentration backgrounds so as to synthesize a training dataset, called Explosion, for use with machine learning methods. Table 1, where the columns are the names of the datasets, the number of attributes (#attr), the number of instances (#ins), the number of classes (#c), the number of data points selected from training sets by CMCMC-PBS (#CPBS), the percent (%) of data selected by CMCMC-PBS over the training sets, the average number of trials (T) in CMCMC-PBS for convergence detection in the B chain, the average number of iterations (K) for the collapsing test in the R chain. similarity measure selects samples from the training sets with a specified sampling window. Several inductive algorithms are used for training classifiers on either the resulting samp les generated by CMCMC-PBS, or the full training sets (Full), or those generated by previous approaches, i.e., static (Static), arithmetic (Arith), and geometric PS with LRLS (Geo) [9][15]. The performances of these classifiers with respect to the AUC are used for evaluation between the CMCMC-PBS and the other algorithms. either small datasets or large datasets with different sampling windows, these datasets are divided into three groups. The sizes of the sampling window for the datasets in the first, second, and third group are set to 10, 100, and 1000, respectively. Decision Tree (DT, i.e., J48), Support Vector Machine (SVM, i.e., SMO [16]), and IB1 for Instance-Based Learning (IBL) from the Weka data mining package [25]. They have been widely used for many practical applications. The classifiers are built with their default settings, e.g., NB with Gaussian estimator, DT with no reduced error pruning and no C4.5 pruning [17] and no Laplace smoothing, SVM with polynomial of 1 for kernel function and constant C of 1 for soft margins, and IB1 with normalized Euclidean distance for IBL. CMCMC-PBS can select a small sample from the original training set after redundancies are removed, e.g., samples with only 15.54 and 0.74 percents of the original training sets are selected in the Sick and Explosion datasets, respectively, while it can retain most instances in the original training sets if little redundancies can be found, e.g., on Vowel and Splice. The average number of trials T can be 2 in Shuttle or 22 in Waveform. The average number of iterations K for the collapsing test in Coupling can be 1 in Audiology or 70 in Shuttle. The Ks are much smaller than the #ins, though. CMCMC-PBS for sample selection on large datasets, e.g., Adult and Shuttle, by comparing CMCMC-PBS with Arith, Geo, and Full for NB and DT while SVM and IB1 are ignored due to their intractability on large datasets. In more detail, we employed 10 fold cross validation. Both the average elapsed time and the AUC over the 10 runs on the large datasets are computed. Static is executed by resampling with replacement using the same sample size as that of the resulting sample identified by CMCMC-PBS from the training sets with the same class distribution. Arith and Geo, on the other hand, are executed according to their specified schedules on each run, and the curves of the elapsed times and the AUCs obtained on 10 runs are averaged. by NB and DT on Adult, as shown in Fig. 4. Arith has a higher sampling cost than CMCMC-PBS (at 24665) after the queried sample size by NB or DT exceeds 6300 or 9400, respectively. No matter how a sample is queried, however, Arith degrades the performance of DT as compared with CMCMC-PBS since the AUC of CMCMC-PBS is Arith X  X  upper bound. On the other hand, Arith can approximately obtain the same performance with NB as CMCMC-PBS by selecting a small sample, i.e., 3200, in less time. can sample data efficiently with unavoidable failures in selecting an optimal sample as compared to CMCMC-PBS. We omit the details due to space limitation. CMCMC-PBS (CPBS), Full, and Static for NB and DT on averages of the results of 20 runs on the datasets in the second group, and 10 runs on the datasets in the third group. These results are listed in Table 2, where  X  X  X  and  X  X  X  represent the corresponding methods in terms of the paired t-test and the Wilcoxon signed rank test at significance levels of 0.05 the CMCMC-PBS wins and losses, respectively, and two statistical test Fig. 4. The comparison between the CMCMC-PBS and Arith on Adult in the third group for NB and DT with respect to the elapsed time and the AUC. results are possible pairs separated by a comma. We can see that CMCMC-PBS consistently outperforms Static for NB and DT, and outperforms Full for NB. It is very competitive with Full for DT in terms of the resulting AUC, the paired t-test, and the Wilcoxon signed rank test except in the case of Adult for DT. 
The same results with respect to AUC, the paired t-test, and Wilcoxon signed rank test are also obtained by averaging of 20 runs on the datasets in the first group, as shown in Table 3. Moreover, the average AUCs are shown at the bottom of Tables 2 or 3, respectively. According to these results, CMCMC-PBS (CPBS) outperforms Static by overall upgrading the performance of all selected classifiers, and even outperforms Full by upgrading the performance of NB and SVM and by reducing the training set size without degrading the performance of either DT on the datasets in the first and second group or IB1 on the datasets in the first group. There are some exceptions, however. For example, CMCMC-PBS degrades the performance of DT on Adult as compared with Full, and degrades the performance of NB on Ionosphere as compared with Full and Static. This suggests that the pr oposed algorithm suffers a failure on these domains. In addition, on Vowel, due to little redundancy the CMCMC-PBS still wins the Static, which performs resampling with 100% of the training set with the replacement and same class distribution. The Explosion is a synthesized domain. The experimental results on Explosion in Table 2 reveal that CMCMC-PBS is superior to Static with respect to the AUC of NB and DT while it is competitive with Full with respect to the AUC of NB and DT. In all cases, it requires quite a small sample for training. Table 2. The comparison between the CMCMC-PBS and Full, Static for NB and DT with respect to AUC on the datasets in th e first and second groups. 
We repeated our experiments with incremental window sizes on the same datasets. The increments for sample selection by CMCMC-PBS on the datasets in the first, second, and third groups are set to 10, 100, and 100, respectively. We obtained 9 additional results related to the AUC, the resulting sample size, and the number of iterations K during the Coupling. For example, as shown in Fig. 5, we plotted the curves of the AUC as a function of window size for DT on Anneal and Shuttle. We observed only negligible impacts of the sampling windows on AUC on these two datasets while no evident result shows any significant effect of the sampling window on the performance of CMCMC-PBS in other cases. Similarly, we observed an insignificant effect of the sampling window on K. The related results are omitted due to space limitation. previously proposed PBS algorithm for border sampling on large labeled datasets. This scalability is achieved by a novel method incorporating PBS with the CMCMC technique. The CMCMC-PBS algorithm is proposed for border sampling on either small datasets or large datasets. It can efficiently and effectively converge to an exact sample by learning on many subsamples in linear time complexity. For example, CMCMC-PBS speeds up PBS by 60% and 40% for border sampling and helps improve the performance of NB on Letter and Splice, respectively. 
We conducted experiments on 33 datasets and showed that CMCMC-PBS consistently outperforms three earlier methods, Static, Arith, and Geo for sample selection although it needs a little more time expenses only by a little bit than Arith and Geo. It helps train NB in most cases as compared with Full while it is consistent with Full for training SVM, DT and IB1 in all cases. No evident result shows an impact of sampling window on the resulting samples for training. Therefore, CMCMC-PBS is efficient and effective for sample selection in many practical applications. Two exceptions in our experiments show that CMCMC-PBS needs further improvement in the direction of performing a more precise geometric computation for collapsing test than the current method for collapsing. These are issues for either reducing the learning cost of active learning [3][22] or for dealing with the class imbalance problem [5][10] by using CMCMC-PBS. These are natural focuses in our future. [1] C. Andrieu, N. D. Freitas, A. Doucet, M. I. Jordan. An [3] D. Cohn, Z. Ghahramani, and M. Jordan. Active [4] R.O. Duda and P.E. Hart. Pattern Classification and [5] T. Fawcett. ROC graphs: Notes and practical [6] C. Giraud-Carrier. A Note on the Utility of Incremental [7] T. Hastie and R. Tibshirani. Classification by pairwise [8] N. Japkowicz and S. Stephen. The Class Imbalance [9] G. John and P. Langley. Static versus dynamic sampling [10] M. Kubat, S. Matwin. Addressing the Curse of [11] P. Laskov, C. Gehl, S. Kr  X uger, K. M  X uller. Incremental [12] G. Li, N. Japkowicz, T. J. Stocki, and R. K. Ungar. Full [13] T. Mitchell. Machine Learning . McGraw-Hill [14] J. G. Propp and D. B. Wilson. Coupling from the past: a [16] J. Platt. Fast training of support vector machines using [17] J. R. Quinlan: C4.5: Programs for Machine Learning. [18] A. Strehl and J. Ghosh. Value-based customer grouping [19] M. J. A. Strens. Evolutionary MCMC sampling and [20] J. D. Sullivan. The comprehensive test ban treaty. [21] J. Sulzmann, J. F X rnkranz, and E. H X llermeier. On [22] S. Tong and D. Koller. Support vector machine active [23] D. R. Wilson and T. R. Martinez. Reduction Techniques [24] G. M. Weiss and F. Provost. Learning when training [25] WEKA Software, v3.5.2. University of Waikato. http://-
