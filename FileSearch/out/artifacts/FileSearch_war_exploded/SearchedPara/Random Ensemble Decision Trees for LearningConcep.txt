 Peipei Li 1 , Xindong Wu 1 , 2 , Qianhui Liang 3 , Xuegang Hu 1 , and Yuhong Zhang 1 With the development of Web Services technology, streaming data from the In-ternet span across a wide range of domains, including shopping transactions, Internet search requests, and etc. In co ntrast to the traditional data sources of data mining, these streaming data present new characteristics as being contin-uous, high-volume, open-ended and concept drifting. As an important branch of data mining, classification can be solved by inductive learning models[20,23]. Decision trees, due to their advantages, are one of the most popular techniques. They are widely used as a basic model in ensemble classifiers. However, it is still a challenge to learn from data streams with these traditional models. Thus, new decision-tree-based algorithms have been developed for data streams, including a continuously-changing data stream algorithm-CVFDT[16], a Streaming En-semble Algorithm-SEA[24], a weighted ensemble algorithm[25], a boosting-like method[22] and new variants of Bagging for concept drifting data streams[3]. However, few of them focus on tackling different types of concept drifts while considering the effect from noise i n the concept drifting detection.
Contrary to these existing efforts, we introduce a new Ensemble algorithm based on random Decision Trees for Concep tdriftingdatastreamswithnoise (EDTC). Our major contributions are as follows. First, the incremental gener-ation of random decision trees in EDTC is different from all existing random decision trees. Second, we develop three variants of random feature selection to determine the split-information. Observations in [10,19] reveal that algorithms with random feature selection are more effective and efficient compared to those with informed split-tests in heuristic methods. Lastly, we develop a double-threshold-based mechanism of concept drifting detection in Hoeffding Bounds inequality[13]. Experiments show that EDTC enables tackling concept drifting data streams with certain noisy data compared to other online algorithms.
The rest of this paper is organized as fo llows. Section 2 reviews the related work. Our EDTC algorithm is described in detail in Section 3. Section 4 provides the experimental study an d Section 5 is the summary. Many efforts based on ensemble rando m decision trees have emerged for data stream handling, which are summarized below. An ensembling method [9] based on random decision trees [10] was propose d to improve the accuracy of classifi-cation and adapt quickly to sudden drifts. An online algorithm[1] of Stream-ing Random Forests extended from Random Forests was presented for data streams. To further apply this algorithm to tackle concept drifts, an algorithm using an entropy-based drift-detection t echnique[2] was developed for evolving data streams. In addition, an algorithm of Multiple Semi-Random decision Trees called MSRT[18] was also presented for concept-drifting data streams. It adopts a double-threshold-based mechanism to detect concept drifts.

In contrast to the aforementioned al gorithms based on random decision trees, our EDTC algorithm presents three differen t characteristics. i) Instead of gener-ating all trees with fixed heights in advance[9,10,18], decision nodes are scaled up incrementally after seeing real trai ning data. ii) Various versions of random fea-ture selection are explored to solve the split-information in the growing of trees. All variants are different from the random-feature-selection mechanism in [2]. iii) A double-threshold-based drift-detect ion technique is utilized to distinguish concept drifts from noise. This is differ ent from MSRT regarding the evaluation method for error rates of classification and the values of thresholds. Our EDTC algorithm aims for concept dri fting detection in data streams with noisy data. The processing flow is shown in Algorithm 1. With the continuous incoming of data streams, EDTC will generate N -random decision trees incre-mentally in various strategies of random feature selection , called the component of GenerateIncrTree . If the total number of training data arrived amounts to a drifting check period-DP , EDTC will distinguish concept drifts from noisy data streams and adjust the detection mechanism for adaptation to concept drifts.
This is called the component of DetectDrifts . Meanwhile, if the number of training instances arrived is up to the memory check period-MP , it will release the space to avoid spa ce overflow, called AdjustMemory . Last, it will adopt a simple voting method to validate the performance itself after training, called Voting . In the following subsections, we will describe each component in detail. Incremental Generation of Decision Trees In the component of GenerateIncTree , to avoid growing sub-branches blindly, each node is generated only if a real training instance arrives. Meanwhile, to ob-tain the maximum utility of each decision path, the split-test mechanism in the growing of each tree follows that discrete attributes should not be chosen again in a decision path while numerical attributes can be chosen multiple times. In addition, with the continuous incoming data streams, a sliding window is intro-duced to store training data for future reconstruction of sub-branches. Instances in the window follow the forgetting mechanism of  X  X irst in and first out X . More details refer to Function 1. In this function, the statistical information involved in Step 12 includes the count of instances, the distribution of class labels and the attribute values relevant to this split-attribute only. Attribute values will be stored in an ascending order and discritized into min(10, the number of these sequential attribute values) intervals for future split-tests. In Steps 14 and 17, a doubly linked list of Parent-Children is created for future bottom-up search, called PCList . Each element is a triple, consisting of a self-pointer, a parent pointer and a sibling pointer. It is used in the concept drifting detection. Random Feature Selection To solve cut-points for growing nod es with numerical attributes, three random feature selection strategies are adopted in this paper. The first one refers to Randomization with the heuristic method in Hoeffding Bounds inequality, called RHB. The second one specifies Randomi zation with the heuristic method of Information Gain , named as RIG, and the third one indicates RANdomization without any heuristic method, called RAN. Details are as follows. In the first strategy, a cut-point is selected from the statistic attribute values of the current split-attribute (e.g., a j ). It is determined in the evaluation function (denoted as H (  X  )) relevant to Information Gain and Hoeffeding Bounds inequality. The definition of Hoeffeding Bounds inequality is given below. Consider a real-valued random variable r whose range is R . Suppose we have made n independent observations of this variable, and computed their mean  X  r , which shows that, with probability 1- X  , the true mean of the variable is at least  X  r - X  , where R is set to log 2 (the number of class labels), and n indicates the number of instances required in a split-test (using n min instead). According to Eq.(1), suppose  X  H = H ( a j ( x ))-H ( a j ( y )) refers to the difference between two candidate cut-points with the highest gain for attribute-a j .Foragiven  X  1 , if the condi-tion of  X  H &gt; X  or  X  H  X   X &lt; X  holds, the attribute value of the x th cut-point with the highest gain will be selected as the final split-point. In the second strategy, the evaluation function is only related to Information Gain .Inotherwords,if satisfying H ( a j ( x )) &gt; H ( a j ( y )), the attribute value of x th cut-point with the highest gain will be selected as the final sp lit-point. However, the third strategy evaluates the cut-point without any heuristic method. That is, it first randomly selects a discretization interval index of attribute values for the current numerical split-attribute, and then specifies the aver age value of the selected interval as the split-point. In brief, because each strat egy implies a certain random character-istic in the feature selection, we call them variants of random feature selection 2 . Concept Drifting Detection A double-threshold-based concept driftin g detection mechanism presented in this section is developed to improve the perfor mance in the tracking of concept drifts under noise. Our thought is enlightened from the drifting detection in [11], but a difference is that thresholds are determined in the Hoeffding Bounds inequality. Details of drifting detection in DetectDrifts are as follows. In terms of the struc-ture PCList , a bottom-up scanning in each tree starts on decision nodes with the 2 nd highest level and ends at the root of the current tree for each drifting check period. More precisely, for each av ailable decision node, we first calculate thesumerrorrateofclassificationinNa  X   X ve Bayes at its children nodes. Sup-pose this is the p th check period. Estimation values in the p th and ( p  X  1) th periods are denoted as e f and e s respectively. Both variables are considered as independent ones, which follow the Normal distribution. According to the re-producibility of Normal distribution, the linear transformations of these variants (e.g.,  X  e = e f  X  e s ) also follow this distribution. Considering that the probability distribution is unchanged when the context is static, then the 1- X  confidence interval for  X  e is approximately (  X  e f  X   X  e s )  X  c  X  according to the inequality of Ho-effding Bounds, where c is a constant,  X  e f =1 / p  X  p k ,and e k indicates the error rate in the k th checking period. In our algorithm, we ignore the deviation between  X  e f and  X  e s , because with the increasing of the value of p , the difference of  X  e f  X   X  e s approaches to zero. In the analysis of the relation among c ,  X  e and  X  , we find that the larger value of c indicates the larger value of  X  e and the smaller value of  X  while the larger value of  X  e indicates the larger deviation in the data distribution during different periods, namely, the more possibility that concept drifts occur (i.e., the larger value of 1  X   X  ). In addition, regarding the noise impact in the concept drifting detection, we use two different values of c ( c 1 and c 2 , c 1 &lt; c 2 ) to distinguish concept drifts from noise. Hence, we could obtain three cases rel evant to concept drifts below. i) If  X  e  X  c 1  X  , we only consider that a potential concept drift is occurring with the confidence 1 - X  l ,where c 1  X  =1 / exp[  X  l 2  X  2 n / R 2 ]. ii) If  X  e  X  c 2  X  , it indicates that a true concept drift is involved with the confidence 1 - X  h ,where c 2  X  =1 / exp[  X  h 2  X  2 n / R 2 ]. In this case, sub-trees will be reconstructed using the instances in SW for better tracking concept drifts. iii) Otherwise, a plausible concept drift is consider ed due to the impact from noise. In the first two cases, n indicates the instance count at the current node. Handling of Space Overflow To avoid space overflow in the growing of a decision tree, we provide an ap-proach to space relief in the component of AdjustMemory . That is, firstly, stop all undergoing splits of growing nodes and change them into leaves. Secondly, release the storage space of attribute values at decision nodes. It is implemented by traversing from bottom to top using the structure of PCList . Lastly, to per-form simple pruning, cut off several sub-trees from bottom to top with the roots whose error rates of classification are more than 50%.
 Majority Voting for Classification Regarding the classification in our algorithm, the strategy of majority voting is utilized to classify each test instance. It is implemented after the individual decision from each tree in the Na  X   X ve Bayes method, namely choosing the majority class label as the classification result.
 Analysis: Estimation on Generalization Error In the theorem of generalization errors for a model of random tree ensembling, an upper bound [6] is given in Eq.(2), where  X  p indicates the correlation among trees while s indicates the strength of each individual tree.
 This is also suitable for our random ensem ble decision trees[15]. According to the definitions on three strategies of random feature selection mentioned above, all trees in the ensemble model are created in a completely random selection method on split-attributes, the correlation among trees is hence lower, namely ensuring a smaller value of  X  p . However, considering the value of s , it is non-deterministic due to the different randomization in each individual model. To evaluate the strength of our model, it is sufficient to take into account the probability that this model is optimal, as the higher predictive accuracy in a model infers the more likelihood that this model is optimal[8]. More specifically, supposing a database has only one relevant attribute a i and the remaining | Attr | -1 attributes are irrelevant, there are two extreme cases as follows.

Case1 ( Best Case): There are pure discrete-on ly attributes in the database and all of the discrete attributes are binary. Each decision path from the root to a leaf is completely independent.

Case2 ( Worst Case): All attributes in the database are numerical. Each node in a tree only generates two children branches at most. The sampling mechanism with replacement is used in the generation of trees.

In both cases, probabilities of an attribute to appear in the L th level of a decision path are defined in Eqs.(3) and (4) respectively (let K = | Attr | ). Thus, we have the least probability for at least one path to involve the only relevant attribute in Eq.(5), namely, our model is optimal with the value of LP at least. The generalization errors mentioned above are obtained on the assumption that the distribution of training data is i.i.d. However, it is hard to hold in the concept drifting data streams. Thus, regarding the impact from concept drifts, we give an infimum bound of generalization errors for our random ensemble model in Eq. (6), where T t specifies the current decision tree ensembling generated over the sequential data chunks {  X  k ,1  X  k  X  t } . Each data chunk  X  k contains all instances in the k th drifting check period. Due to the page limitation, the detailed inference of Eq. (6) is omitted.
 To validate the efficiency and effectiv eness of our EDTC algorithm, several single and ensemble algorithms for concept drifting data streams, including CVFDT[16], HT-DDM (a single Hoeffding Tree with a Drift Detection Method) [11], HT-EDDM (a single Hoeffding Tree with an Early Drift Detection Method) [4], Bag10-ASHT-W+R 3 [3] and MSRT[18], are select ed to compare with our al-gorithm on benchmark concept drifting databases and real streaming data. All experiments are performed on a P4, 3.00GHz PC with 2G main memory, running Windows XP Professional. Algorithms of CVFDT, MSRT and EDTC are devel-oped in C++ while others are coded in Java from the open source MOA 4 [14]. Thus, only those algorithms coded in the same platform with EDTC on the overheads of space and time are contrasted for fair comparisons. In addition, because three strategies of random feature selection in EDTC are designed for cut-points of nodes with numerical attributes, only a copy of experimental val-ues is presented for databases with pure dis crete-only attribut es. All denotations involved in this section are summarized as shown in Table 1.
 Parameter Estimation With respect to the parameters of h 0 and N in EDTC, it is sufficient to ensure better performance for the current ensemble model if satisfying h 0 = | A | /2 (or 5) and N =10[8,19]. Considering the parameters of n min and  X  in the growing of a tree, an experimental conclusion[19] reve als that the model performs sufficiently well if n min =200 and  X  =10  X  7 . Regarding the values of c 1 and c 2 ,wealsouse the control limit of 2- X  or 3- X  [11] to partition concept drifts and noise. In EDTC, let  X  l equal to 0.05, namely, the confidence level for drifts is set to 95% while the confidence of 1- X  h could be more than 99%. In addition, for other parameters in EDTC, they are set to DP =1k 5 , MP =500k and SW =10k. However, for the pa-rameters in the comparison algorithms, they follow the default settings involved in [16,11,4,3,18] respectively.
 Evaluations on Concept Drifting Data Sets SEA. It is a well-known data set of concep t shift with numerical attributes only[24]. This database consists of a three-dimensional feature space with two classes and four concepts. We use the d ata generator of SEA in MOA to generate a test set with 100k-sized instances and a training set with 400k-sized instances containing four concepts. Both data sets contain 5% class noise.
 STAGGER. It is a standard database of concept-shifting data streams to test the abilities of inductive algorithms[21]. This database consists of three attribute values and three alternative underlying concepts. We also use the database gen-erator of STAGGER in MOA to generate a training set with instances of 1000k and a test set with 500k-sized instances respectively. The training set includes 0.1k concepts and each concept contains 10k-sized instances.
 KDDCup99. It is a database for network intrusion detection[17], containing 24-class labels and 41-attribute dimensions in total with 34 dimensions of numerical attributes. We select this database because it has b een simulated as streaming data with sampling change [27]. The times of concept changes is 36.
 Predictive accuracy In this subsection, two aspects are concerned to evaluate the predictive ability of EDTC. In one dimension, Figures 1-3 6 present the cases of tracking con-cept drifts on benchmark databases. We can see that false alarms are prone to appear at the beginning of learning from training data. For instance, all false alarms appear before the first concept shift in SEA. 64.3% false alarms appear when learning from only 1/10 training data of STAGGER. This is because there are larger fluctuations of error rates classified in the model learning from in-sufficient training data. Concept drifts are hence probably considered falsely. Furthermore, Table 2 reports the statistical results of drifting detection, which show that our algorithm with random feature selection could detect most of the concept changes in a few instances afte r a drift occurs. Meanwhile, regarding values of estimation metrics[12], i.e., FA , Miss and AE , EDTC-RAN performs best. In the other dimension, Figure 4 presents predictive results on the test data. The observations are as follows. i) On SEA, EDTC-RHB performs worse than EDTC-RIG and EDTC-RAN on the predictive accuracy by 2%. The best predictive accuracy in EDTC-RIG is l ower than CVFDT by 3% while the devi-ation from Bag10-ASHT-W+R, HT-DDM and HT-EDDM is limited to 1%. ii) On STAGGER, EDTC is superior to all other algorithms. The predictive accu-racy is improved by 10% at least. iii) On KDDCup99, predictive accuracies in EDTC-RIG and EDTC-RAN are only lower than that of Bag10-ASHT-W+R by around 4% while they are improved by the range of [14%, 45%] compared to the remaining algorithms. However, it seems abnormal for EDTC-RHB with a more than 80% error rate. This is resulted from the fact that KDDCup99 presents a heavily skewed distribution of class labels. Meanwhile, the constraint of Hoeffd-ing Bounds inequality used in EDTC-RHB impedes the growing of trees. It is hence prone to generate tree stumps, which leads to a poor performance on the predictive ability.
 Speed and Space A set of experiments is conducted to evaluate the overheads of runtime and space in EDTC. Experimental results shown in Table 3 present that our algorithm demands in a light weight way compared to other algorithms. More precisely, on SEA, the lowest time consumption is only 1/2 of that in MSRT and 1/16 of that in CVFDT respectively while the space overhead is no more than 1/38 of those in CVFDT and MSRT. On STAGGER, EDTC performs as approximately as CVFDT, especially on the time consumption while it outperforms MSRT explicitly on the overheads of runtime and space. However, on KDDCup99, the heaviest overhead of time in EDTC is twice more than that in CVFDT while the space consumption is less than 1/7 of that in CVFDT.
 Robustness In another dimension, we adopt the noisy database of LED (with 24 dimen-sions of attributes containing 17 irrelevant attributes and 7 drifting attributes) generated by MOA[14] to validate the robustness to noise in EDTC. This data set contains 1000k-sized training data and 500k-sized test data. Experimental results shown in Figure 5 7 present that EDTC is super ior to other algorithms in the resilience to noise. As the noise rates vary from 5% to 30%, the predictive accuracy in EDTC is improved from 2% to 7%. The reason is that EDTC makes use of a semi-random strategy or compl etely random strat egy to select split-features. The process of selecting attributes is independent of the distribution of classes, which reduces the impact of noisy data on classification largely. Application on Web Shopping Data In this last subsection, a real data stream from Yahoo shopping databases is obtained via Yahoo web services[26] to verify the feasibility in EDTC. The data set contains 113k-sized records and 22-dimension attributes with 16-dimension numerical ones. To mine the relation between the credibility of merchants and possible factors, we define the attribute of  X  OverallRating  X  as the class label and divide its values into five class labels. In our experiments, we randomly select 2/3 of total records as the training set and the remaining 1/3 as the test set corresponding to the original distribution of class labels. Experimental results shown in Figure 6 reveal that EDTC-RIG and EDTC-RAN perform as well as Bag10-ASHT-W+R on the predictive accuracy while all of them outperform other algorithms very much. For instan ce, the lowest predictive accuracy and the highest one in EDTC-RIG are improved by 1.28% and 70.26% respectively. Meanwhile, the maximum time consumption in a single tree is no more than 20s and the space overhead is only 4M at most. These data confirm that EDTC is suitable for handling real streaming data. In this paper, we have proposed an algorithm of random Ensemble Decision Trees for Concept drifting data streams ca lled EDTC. Unlike other random decision trees, three variants of random feature selection are developed to determine the cut-points in the growing of decision trees. Meanwhile, two thresholds defined in Hoeffding Bounds inequality are adopted to track concept drifts from noisy data streams. Experimental evaluations show that EDTC performs better than several state-of-the-art online algorithms. An application of EDTC on a real-world Yahoo shopping data has also shown promising results. However, how to adapt to gradual concept drifts is an issue for our future work.
 This research is supported by the 973 Program of China under award 2009CB326-203, the National Natural Science Foundation of China (NSFC) under grants 60828005, 60975034 and 61005007, the Natural Sc ience Foundation of Anhui Province under gra nt 090412044 and Speci al Funds of Thousand Talents Program unde r grant 2010HGXJ0715.

