 ORIGINAL PAPER K. Pramod Sankar  X  R. Manmatha  X  C. V. Jawahar Abstract In this paper, we present a practical and scal-able retrieval framework for large-scale document image col-lections, for an Indian language script that does not have a robust OCR. OCR-based methods face difficulties in charac-ter segmentation and recognition, especially for the complex Indian language scripts. We realize that character recognition is only an intermediate step toward actually labeling words. Hence, we re-pose the problem as one of directly performing word annotation. This new approach has better recognition performance, as well as easier segmentation requirements. However, the number of classes in word annotation is much larger than those for character recognition, making such a classification scheme expensive to train and test. To address this issue, we present a novel framework that replaces naive classification with a carefully designed mixture of indexing and classification schemes. This enables us to build a search system over a large collection of 1,000 books of Telugu, con-sisting of 120K document images or 36M individual words. This is the largest searchable document image collection for a script without an OCR that we are aware of. Our retrieval system performs significantly well with a mean average pre-cision of 0.8.
 Keywords Image retrieval  X  Automatic annotation  X  Document images  X  OCR-free annotation 1 Introduction Traditionally, digital libraries of scanned English books are made searchable by converting images to text using an opti-cal character recognizer (OCR). Example projects include Google Books [ 1 ], Internet Archive [ 2 ] and Universal Digi-tal Library (UDL) [ 3 ]. However, for Indian language scripts, there are no robust OCR engines available, as yet [ 4 ]. While there is active research on building OCR engines for Indian languages [ 4 ], their performance is not yet good enough for use. The fundamental challenges in building an accurate Indic-OCR are an extended character set, complex script lay-out [ 5 ], conjoined vowel/consonant modifiers for each con-sonant (see Fig. 1 ), etc. Additionally, degradations such as cuts and merges have a more pronounced effect (than on Eng-lish) with regard to character and component segmentation. Finally, with a digital-library-scale dataset, the framework enabling retrieval needs to be efficient and scalable.
In this paper, our goal is to build a searchable library of printed books in Indian languages such as Telugu .We overcome many of the issues with Indian language character recognition, by re-posing the problem to one of word anno-tation . By focusing on recognizing words, instead of char-acters, better character disambiguation is possible as well as difficult character segmentation is avoided. The scalability of this approach is achieved by proposing an efficient index-ing based classification scheme. The specific contributions of our paper include 1. A novel direction in building document retrieval systems 2. A unique framework to scale recognition systems to large 3. A thorough evaluation of features and indexing sche-mes 4. The largest collection of Indic document images, 120K The rest of the paper is organized as follows. In the fol-lowing section, we outline the datasets that we work with. In Sect. 2 ,wediscussthepreviousworktowarddocumentimage retrieval, and we shall elaborate on why previous methods are not adequate to address our problem. We present our new solution direction in Sect. 3 and detail the processes involved in our framework in Sects. 4 and 5 ; especially in Sect. 5 ,we outline a scheme for scaling our approach to large collec-tions. We analyze the subtleties of our approach in Sect. 6 and present experimental details and results in Sect. 7 .We compare this work with other similar previous work in Sect. 8 and conclude in Sect. 9 . 1.1 Datasets Our datasets come from document images of printed Telugu books from the Digital Library of India [ 9 ]. The collection has a variety of print quality, font and style, at varying levels of ink and page degradation. We shall refer to three sets of data in this paper: 1.1.1 Test data Our Te s t data consists of 1,000 books, including many classical poetic, prose and fiction works in the language of Telugu X  X  language with 75 million speakers. The Te s t dataset amounts to 120K document images with 36 million word images. This is the largest Indic dataset ever attempted for a document retrieval system. This is the dataset that we would like to build a retrieval system for, as a consequence of this work. There is a significant variation in the fonts, typeset styles and print quality in the dataset. Varying levels of degradations, such as cuts, merges, salt-and-pepper noise, are also present in the data. Figure 2 shows this variation for one word across the collection. 1.1.2 Validation data This is a hand-labeled groundtruth dataset of 33,000 word images, belonging to 1,000 distinct words. Each of these word images is labeled with the corresponding text label. The number of occurrences of each label in the groundtruth dataset varies from 5 to 500. The size of the word images ranges from 30  X  30 to 500  X  300 pixels. We shall use the Validation data to optimize the settings of our framework. The Validation dataset, which we call IIIT-H Telugu Word Dataset, is available for download from [ 8 ]. 1.1.3 Labeled/training data The Training data are a set of 33 books consisting of 3,000 document images. Unlike the Validation data that consists of 33,000 accurately labeled word images, the training data is a much larger set of word images with a slightly erroneous labeling (about 80% accuracy). The labeling is obtained by aligning page-level transcriptions to their corresponding word images. This was achieved by performing a text-driven segmentation of the document image. From the text, we are aware of the number of lines in the page, number of words in each line and the (approximate) number of characters in each word. This information can be used to drive the page segmentation.

The process begins by performing line segmentation of the document image. If the number of lines from the segmenta-tion is less than the number of lines in the text, it means two text lines have been erroneously merged by the segmenta-tion algorithm. This often happens because of overlapping ink pixels between the two lines, due to the presence of consonant-conjuncts (see Fig. 1 ). To correct this, the tallest lines are split until the number of lines match between the image and the text. On the other hand, if the number of lines from segmentation is more than those in text, it indicates that some lines were split by the segmentation. This is cor-rected by merging the smallest lines together. In the second step, each line is similarly segmented into words depending on the known number of words in the given line. Labeled exemplars for the keywords are obtained from this dataset. 2 Previous work Early document retrieval addressed the problem of document retrieval as a whole. Such work uses document level features or local features that collectively represent the document image [ 10 , 11 ]. An application of such work was applied for document tracking on a camera-based desk [ 12 ]. However, the shortcoming of whole-document retrieval is that users typically prefer to query and retrieve documents at content level,morespecificallywith words asqueries,anaturalexten-sion of web-search.

There have been two major approaches in literature toward building content-level retrieval systems. The first is a recognition-based approach involving the building of an optical character recognizer (OCR) along with its post-processing modules. The other is the recognition-free Wo rd -Spotting approach. 2.1 OCR OCRs have had a long history, and surveys can be found in [ 13 , 14 ]. OCRs begin by segmenting the document image to connected components, each of which is considered as an isolated character. These components are represented using features such as patch-based, PCA, LDA or other statistical features. The classifiers used to recognize characters have evolved over time from Neural Networks [ 15 , 16 ] to Support Vector Machines (SVM) [ 17 ]. Since SVMs are mostly suit-able for two-class classification problems, a chaining archi-tecture such as a directed acyclic graph is used to combine multiple classifiers [ 18 ]. Errors in classification are typically corrected using post-processing based on character error models [ 19 ], dictionaries [ 20 ] or statistical language mod-els [ 15 , 21 ]. OCRs such as the Byblos system [ 21 , 22 ]use HMMs to encode the n-gram statistics toward refining char-acter labels. However, such methods inherently use gradient-based features, which we shall show (in Sect. 7.1 ) are not reliable in the presence of severe degradations present in our data.

In the case of Indic OCRs, despite significant efforts [ 4 ], their performance is not comparable to that of English. The major roadblocks toward building Indic OCRs are  X  An extended character set, almost 5 times that of English,  X  A complex style of writing, where each character is a  X  A number of similar-looking character pairs, distin- X  A large lexicon that is almost 10  X  that of English, making
Moreover, a poor OCR performance translates to unac-ceptable word-level accuracies. Typical character recogni-tion accuracies over document images similar to our collec-tions are less than 90%. Assuming an average word length of 6, the word recognition accuracy would be ( 0 . 90 ) 6 only 53%. Retrieval from such erroneous recognition results would result in poor precision and recall. 2.2 Word-spotting As an alternative to OCR, a number of authors have proposed techniques that avoid the use of character recognition. One such approach is based on character shape codes for word detection [ 24 ]. A set of 5 character shape codes encode each connected component, which is prone to errors from com-ponent segmentation and degradations. A few shape coding works avoid character segmentation, by proposing novel fea-tures such as stroke coding [ 25 ], topological shapes and char-acter extremum points [ 26 ]. However, these techniques are limited by the shape-code vocabulary and are typically not invariant to font and style variations (for e.g. italics).
The more popular recognition-free approach is called word-spotting , where word images are holistically repre-sented in a visual feature space and matched with the queries in the feature space itself. Queries are either given as word image examples or are synthetically generated [ 7 , 27 ]. Features proposed for representing words include pro-files [ 28 ], gradient-based binary features [ 29 ], HoG [ 30 ] or shape-context [ 31 ]. In most cases, the features used to represent the word images are of nonuniform length, depending on the word image X  X  width. With variable-length features, word images are matched using Dynamic Time Warping (DTW) [ 28 , 32 ]. Apart from DTW, matching tech-niques using HMMs [ 33 ] (that needed to be pretrained) and Hough transform of distance matrix images [ 34 ]were proposed. More recently, BLSTM neural networks [ 35 , 36 ] have been used to obtain a pseudo-character representation, which is then matched with the query using edit distance. In the case of historical documents, a segmentation-free word-spotting was presented [ 37 ] where features are extracted on blocks of the document images, which are then matched with the query. Word-spotting approaches have been extended to retrieve/recognize from hand-written documents as well [ 38  X  40 ].

The more successful DTW-based approach is computa-tionally intensive, with an estimated query time of 416 days for searching a single query within our test collection of 36M words. A suggested approach to overcome the impractical retrieval times is to perform an offline-indexing using either clustering [ 28 ] or with Locality Sensitive Hashing [ 41 ]. How-ever, indexing the features from large datasets (about 210GB for 1,000 books) is infeasible even on advanced computers. Also, some of the features such as the split-style HoG fea-ture [ 30 ] are of a very high dimension, making it unsuitable to manage for large collections. Further, word-spotting sys-tems can only be queried by example, while users prefer text querying. Text querying requires that text labels be provided to the word images. In [ 42 ], such labels were obtained over handwrittendocuments,byapplyingarelevancemodellearnt from transcribed documents. 2.3 Dataset size comparisons With regard to scalability aspects, much of the previous work has been restricted to laboratory settings or small document collections. Due to the unavailability of a large, labeled cor-pus, most of the experiments were done only on a limited number of pages [ 43 ]. The UNLV dataset [ 44 ] evaluated OCRs on a set of 2,000 pages. In word-spotting techniques, the test set sizes range from a few tens of images [ 28 , 45  X  48 ] to a few books [ 49 ]. The largest handwritten collec-tion of about 1,000 pages comes from George Washing-ton X  X  diary consisting [ 42 ]. One of our previous papers [ 7 ] was the first to build a retrieval system over a collection of 500 (printed) Telugu books, consisting of 75,000 doc-ument images. In this paper, we shall extend our previ-ous work to a much larger collection, while significantly boosting performance and slashing computational require-ments. 3 Our approach At the outset, we realize that in order to replicate Web search characteristics and performance on document images, a text-based retrieval system is the best option. A textual representation also allows for many breakthroughs in text retrieval, summarization and translation to be applied to doc-ument images in the future. Since OCRs are not the effective approach to obtain the corresponding text, we re-pose the problem to one of word annotation .
 3.1 Toward annotating words The move from character to word labeling is motivated by these observations: 1. Word images contain a lot more information, than indi-2. Segmentation of the document to words is much more 3. For the purpose of retrieval, character/component level
Further, not all words in document images are informa-tive toward indexing them. Since it only suffices to label those words that are useful from a search perspective, our process is one of annotation instead of full-fledged recogni-tion. The comparison of our approach with OCR and word-spotting is given in Table 1 . Our approach is distinct yet combines the advantages of both these directions into a new framework. 3.2 Problem setting We begin with two inputs: (i) a set of test document images D images W 1 i , W 2 i ,..., W n i i , and (ii) a set of words (in text) forming the vocabulary-of-interest, V 1 , V 2 ,..., V M .The word image set W j i and the text vocabulary V k are drawn from different datasets, and hence, there is no knowledge if a given word image W j i corresponds to a text label V The word annotation problem now boils down to finding the appropriate word V k , if it exists, for each word W j i . Without any loss of generality, the same problem can also be posed as one of finding all the words W j i that correspond to the given word V k in the vocabulary. Such a labeling will directly result in a search index over the documents. Since, the task of find-ingdatumcorrespondingtoagivenkeywordistheconceptual opposite of the traditional annotation scheme, we call such a process Reverse Annotation [ 7 ].
 Algorithm 1 Word Annotation Framework A conceptual overview of our framework is presented in Algorithm 1 . We shall now proceed to elaborate on the design choices for each of the steps in this algorithm. For each section, we indicate the corresponding step of the algorithm in parentheses. 4 Classifier design for word annotation ( offline phase: training ) 4.1 Identifying the vocabulary (classes) In retrieval systems, the choice of vocabulary { V k } that can be searched for is an important design criterion. Practical text search engines do not index (and answer) all possible queries, since such an index could span all possible unique words in the language. For example, search engines such as Lemur [ 50 ], and Galago [ 51 ] do not index strings made up of special characters. The goal of building a search index is to optimize the percentage of queries that can be answered and documents that can be retrieved. Similarly in our work, we could limit the vocabulary size to optimize the query-document coverage.

The distribution of the keywords in our training dataset is shown in Fig. 4 . It follows the well-known Zipf X  X  law [ 52 ] that states that the frequency of any word is inversely pro-portional to its rank in the frequency table. At the head of the distribution, we find what are called stop words, such as { the , a , an , and ,... } . Stop words do not contribute to the retrieval task, and hence, it would only be a waste of index space. The distribution also has a long-tail , consisting of a large number of words that occur only a few times.
Consequently, we design the vocabulary-of-interest V k , such that a large percentage of documents are indexed and large percentage of queries are answered. We first remove stop words, by thresholding on the inverse document fre-quency (IDF) measure. The IDF is defined as log | D | | d where | D | is the number of documents in the collection and the denominator is the number of documents containing the given term. We consider all the words that occur in one-tenth of the document collection, as stop words. This corre-sponds to an IDF threshold of 2.3. Rare words are removed by thresholding the term frequency (TF) at 10. Our vocabulary-of-interest consists of 100K words. 4.2 Building the classifier When we re-pose the problem from character recognition to word annotation, the number of classes that need to be learnt expands from the character set (of about 300) to the size of the vocabulary (100K in our case). Such a class size is quite large compared to typical machine learning problems addressed in the literature [ 53 , 54 ]. This results in two issues: (i) the classifiers need to be designed to scale to the large number of classes and (ii) significant amount of reliable training data is required to learn the classifiers.

In picking a classifier, apart from high performance, the major requirement is that the classifier scheme be scal-able to huge test collections. In traditional machine learning work, the focus has been on improving classification accu-racy and/or the training times. We believe however, that for our problem, scalability in the test phase is more important. Accordingly, we examine two distinct classifier schemes: (i) Nearest Neighbor and (ii) Support Vector Machines (SVM), to measure suitability of each for word image annotation.
SVMs are inherently two-class classifiers. Multi-class classification with SVMs is performed by either building | one-vs-rest classifiers, or | C | X | C  X  1 | / 2 one-vs-one classi-fiers. Each data point is classified as belonging to the classi-fier with biggest margin (for one-vs-rest) or the class that is selected by most classifiers (for one-vs-one). The major issue with one-vs-rest classifiers is the lack of a reliable mecha-nism to combine classifier scores, especially when the inter-nal parameters of the individual SVMs depend on the data or class distributions. The second approach is obviously pro-hibitive to train and test for our problem where | C | is 100K. Each data point would then have to be classified by 5  X  10 SVMs, which would be infeasible. Moreover, SVMs require large amounts of training data to learn the classifier accu-rately.

We experimentally verify this intuition over our valida-tion dataset of 32K word images belonging to 1,000 classes. SVMs were built for each of the 1,000 classes using half the validation data as training set and the other half for testing. The classifier with the maximum score is chosen as the class for the test point. Results from this experiment are shown in Table 2 . As can be seen, SVMs perform much poorer than a Nearest Neighbor classifier using the same features. Apart from performance, SVMs are also slow to train and test, due to the 1,000 classifiers (one for each word) that needed to be learned and classified against. 5 Annotation on a large scale ( offline phase: testing ) The challenge in the testing phase is scalability of the classi-fier scheme to large test collections. For example, the NN classifier requires about 0.77s per test sample. The time required to classify our test dataset against all 100K clas-sifiers would require about 90 years of compute time, which is clearly not the optimal classification scheme. We shall address the scalability challenge by proposing a novel mech-anism based on this unique premise: One need not classify every given data point.
 Classification can be re-used across similar data.
In a typical machine learning scheme, each test case is considered isolated and classified independent of each other. With such a naive classification, the complexity of classifi-cation is O ( N Test  X  O ( C )) , where N Test is the number of data points to classify and O ( C ) is the inherent complex-ity of the classifier for the classes of interest. However, we observe that this is quite unnecessary, since the test samples follow an inherent distribution. A number of samples are either repetitions or near-duplicates of other samples. Con-sider a scenario where one could identify if a data point has the same (or similar) features as another data point. In such a case, only one of these features needs to be classified, and the classification result can be re-used for the other point. Such a scheme would be effective under the following fairly generic conditions: 1. The size of the test data is much greater than the number 2. The test samples follow a distribution that is not arbitrary, 3. Smoothness Assumption: If two points x 1 , x 2 are close,
For the case of document images, we find that all these assumptions are valid. It is quite obvious that the list of classes in the word annotation is limited by the known vocab-ulary size of the language. The data, however, that can be indexed with such a vocabulary could be limitless. It is also known that the test data follows a Zipf X  X  distribution, similar to the training set. With Zipf X  X  distribution, a large amount of the mass is present in a small percentage of the distribution. This is an ideal candidate to re-use classification scores, when multiple instances of the same data are seen often. The third condition only requires that the matching scheme be robust to noise. In cases where condition 3 is false, it would imply that two features that are close in the given feature space, actually belong to different categories. This would mean that the features and/or the distance measure are highly sensitive to noise, and hence not the right setting to learn the classifier. As we shall show in Sect. 7 , our features and classifiers are indeed robust, hence satisfying condition 3.

Given the validity of the above assumptions, as a conse-quence, we now make the  X  X luster Assumption X : if points { x to be of the same class. More precisely, for a given point x and its cluster centroid C ( x i ) , for some reasonable  X  y ( x i ) = y ( C ( x i )) if d ( x i , C ( x i )) &lt;  X  where y (.) is the label for the given data point.
Thus, if we were able to cluster the word images, such that the maximum radius of the cluster is less than a certain threshold  X  , then it would suffice to classify only one point, namely the centroid of the cluster. We could then propagate the class label of the centroid to the rest of the cluster, without having to classify every single point.

In the case of word images, the centroid is typically a very clean representative candidate of the various instances of word image occurrences across the data collection. Thus, the classification of the centroid is much more accurate than classification of those points at the fringe of the cluster. 5.1 Clustering training data With the clustering scheme, the computational cost of using a classifier reduces from O ( N Test  X  O ( C )) to O ( K  X  Here, K is the number of clusters in the test data, K is typically a small multiple of the number of classes, C .In the specific case of Nearest Neighbor classifiers, a further cost saving could be achieved. We observe that the keyword-exemplars themselves are not totally isolated in the feature space. While multiple exemplars of the same word would be clustered together at the lower levels of the cluster hierarchy, at a higher level, similar words that vary in only a few charac-ters would be found together. Thus, keyword-exemplars can also be clustered based on the similarity in their representa-tive features. 5.1.1 Matching clusters through trees Imagine we are given a pair of trees T D and T K , one belong-ing to the unlabeled word image dataset and another from the keyword exemplar set. The problem of word annotation is now the task of matching the clusters at the leaf node of T to the specific exemplar belonging to T K . Unlike an exhaus-tive matching between all the leaf nodes between the pair of trees, we could exploit the paths between the leaf nodes to prune out unsuccessful matches beforehand. We assume that two clusters will not match at a lower level, unless their parent nodes match. This is a fair assumption, even in cases where the clusters in the two trees partition the feature space differ-ently. This technique is depicted in Fig. 5 . Using this method, the complexity of matching exemplars with features is now reduced to O ( logN 1  X  logN 2  X  B 2 ) , instead of O ( N a regular Nearest Neighbor (NN) classifier, where B is the branching factor (see below). 5.2 Efficient clustering schemes In the previous section, we introduced the concept of clus-tering for avoiding repeated classification of similar points. However, the process of clustering itself is expensive, typi-cally requiring O ( N Test . K ) time, where K is the number of clusters. Another limitation of the K-Means clustering algo-rithm is the necessity to fix K beforehand. If K is less than the number of unique words in the collection, it would mean that some of the clusters would have different words in them. A larger K reduces the chance that images corresponding to different words will be part of the same cluster. The trade-off is that a larger K will lead to multiple clusters for the same word. However, this would increase the compute time. We shall look at two approaches to perform this process in reasonable time.
 Hierarchical K-Means (HKM) is a way to approximate K-Means fast. The idea [ 55 ] is that a small number of clusters X  X ay B  X  X re created at the top level, B is known as the branching factor. In the next step, each of these B clusters is expanded to B more clusters giving B 2 clusters at the second level. This process is repeated up to a cer-tain depth D so that B D clusters or leaf nodes are formed at depth D , typically D = log B ( N Test ) . To build the entire HKM, tree requires O ( N Test  X  K  X  D ) time while K-Means would require O ( N Test  X  K D ) . For a million data points, the speedup is from 31 yrs for K-Means to 13h for HKM. Once theHKMtreeisbuilt,givenanewpoint,ittakes B  X  D compar-isons to find out which cluster it belongs to, unlike traditional K-Means which would take B D comparisons. For example, if B = 10 , D = 6, then there are one million leaf nodes. While K-Means requires 10 6 comparisons while HKM only needs 60.

Another approach to speeding up clustering is by using approximate nearest neighbor search. In K-Means, the most expensive step is the mapping of data to the closest provi-sional centroid, at each iteration. This is essentially the prob-lem of nearest neighbor matching, which can be approxi-mated using indexing schemes such as Locality Sensitive Hashing or Random Forest of KD-Trees. 5.2.1 Locality sensitive hashing In Locality Sensitive Hashing [ 56 ] (LSH), data points are projected onto random subspaces, such as a line or hyper-plane. The projected subspace is divided into spatial bins; each point is assigned to the bin it falls into. The assumption is that points close to each other in a high-dimension space will most likely fall into the same bin in the low-dimensional projection. Multiple such hash functions are generally used to collect additional evidence to determine NNs. A feature vector x is mapped onto a set of integers by each hash func-tion h a , b ( x ) . For a predefined bin-width w , the hash function h , b is given by, h Featurevectors that arecloseinthefeaturespaceareexpected to fall into the same hash bin. As a corollary, data points that have the same hash value over multiple hash functions are expected (but not necessary) to be close in the feature space. Following this, for each test feature, the hashed values in the same bins as the test feature are returned as the approx-NNs. 5.2.2 Random forest of KD-trees Another approach to addressing the approximate NN prob-lem uses a random forest of KD-Trees [ 57 ]. A KD-Tree partitions the feature space with axis-parallel hyperplanes. The algorithm splits the data in half at each level of the tree on the dimension for which the data exhibits the great-est variance. The tree is looked up for NNs, by comparing the query with the bin-boundary at each level of the tree(s). Building a KD-tree from n points takes O ( nlogn ) time, by using a linear median finding algorithm. Inserting a new pointtakes O ( log n ) andqueryingfornearestneighborstakes O ( n 1  X  1 / k + m ) , where k is the dimension of the KD-Tree and m is the number of nearest neighbors. The algorithm [ 57 ] first traverses the KD-tree and adds the unexplored branches in each node along the path to a priority queue. It then finds the closest center in the priority queue to the given query and uses this node to restart the traversal. The process is stopped when a predetermined number of nodes are visited. Further, a random forest of multiple KD-Trees is used to improve the ANN evidence for better clustering quality. When building a random forest, the split dimension is chosen randomly from the first D dimensions on which the data has the greatest vari-ance. Multiple trees define an overlapping split of the feature space. These trees are looked up for NNs, by comparing the query with the bin-boundary at each level of the tree(s). By using a voting scheme over the different trees, we obtain very reliable clusters. 5.2.3 Incremental indexing Irrespective of the indexing scheme used to speedup cluster-ing, memory limitations are easy to breach in large datasets. To begin with, the features for our test dataset (36M word images) occupy a diskspace of 210GB, which cannot be loaded in one instance on a single machine. Further, all index-ing schemes have large memory overheads, where memory is traded for speedup. For example, the storage complexity of LSH is of the order O ( N Test . | H | ), n being the number of features hashed and H is the set of hash functions. Since the optimal | H | depends on the size of the dataset N Test memory required for a large dataset could be more than what modern computers can handle [ 58 ]. We address this issue, by dividing the dataset into subsets, say M of them, and build separate indexes over each subset. The size of the subset is determined by the maximum number of features that can be indexed in one instance. Once the indexes are built, the approximate nearest neighbors are obtained by looking up each subset against the index of every other subset (includ-ing itself). Thus, the process of clustering would involve M indexing steps and M 2 lookup steps. 5.3 Indexing of document images ( indexing phase ) At the end of the word annotation procedure, we have an index of the word images against the keywords. However, the index does not lend itself to ranking the word images for retrieval purposes, since the associations in the index are binary. The ranking is based on a probability that consists of two parts. The first part measures the quality of labeling of a cluster (i.e. its centroid) with the given keyword-exemplar. The second term measures the association of the word images to the cluster.

Let us assume that the word image t i is the centroid of a certain cluster, and the set of word images in this cluster are cluster is given as V k , then the probability that word image t matches this keyword is given as: p (
V k | t j i ) = where s ( x || y ) is a similarity measure (inverse of a distance measure). Since we assign a single label to each cluster, for V
The denominator of the first term involves the summation over all keywords and all clusters (a modified version of that used in [ 7 ]). While computing the denominator is expensive, we observe that it remains a constant for all word image and keyword cluster pairs. For ranking purposes, we can ignore thedenominatorwhilecomputingthescore.Thecomputation of the score can thus be performed only between the pairs of exemplar and test-set clusters which match in the word-image annotation phase, hence preserving the computational advantages of the framework. 5.4 Ranking of retrieved documents ( online phase ) Given the ranking of word images against the query, the relevant documents are ranked using a function similar to the Term Frequency/Inverse Document Frequency (TF-IDF) measure. In the ranking function, the sum over the number of words is replaced with their probabilities given by Eq. 1 .TF measures the importance of the keyword for the image and is normalized by the document length to avoid bias to longer documents. The document length is replaced by the sum of the probability scores of all the words within the document, against their respective keywords. This scheme ignores the words that are not labeled in the annotation step. For the query word V k , the TF for a document D i containing word images { W j i } is defined as TF ( D i | V k ) = j where the denominator is a sum of all the scores between the word images and their matched keywords.

When the search query consists of multiple words, the rel-ative importance of the words in the query is an important distinguishing factor during ranking. This is estimated by the inverse document frequency (IDF) measure, which indicates the overall importance of the given keyword in the entire col-lection. It is basically the logarithm of the total number of documents over those containing the given term. The modi-fied IDF measure is defined as IDF ( V k ) = log The  X  functionensuresthatmultipleword-occurrenceswithin a document are counted only once:  X ( x ) = The TF  X  IDF provides the final ranking of documents for the given query-words. 6 Analysis of the solution 6.1 Effect of clustering errors Clustering of data is crucial to our solution. It is possible that some of the clusters are not  X  X ure X . This could be because of multiple reasons: (i) the words are inherently similar looking, (ii) degradations cause the words to appear similar or (iii) the cluster centers are initialized in the intra-class region in the feature space. One of the solutions that we suggest to improve cluster purity is by allowing multiple clusters for each word in the vocabulary. Ideally, if the vocabulary size | V | is known, the number of clusters  X  X  X  should be equal to the number of unique words. Using K &lt; | V | will inevitably result in clustering errors. When the vocabulary size is unknown, it is suggested to use a large K , thereby allowing multiple clus-ters for some of the words, but not constraining the different words to fall into a limited number of clusters.

The effect of cluster size on the purity of the clustering was analyzed over the validation dataset (described in 1.1 ). The dataset consists of 32K word images belonging to a vocab-ulary size of 1,000. The precision, recall and F-measure of various cluster sizes is shown in Fig. 6 . As can be seen, when K &lt; | V | , the precision of the clusters is very poor. The preci-sion improves as K increases, with the most precision of 1.0 reached when each data point is a separate cluster. However, the recall value of the clustering would tend to 0.0 in such a case. Consequently, we choose an operating point where there is sufficient trade-off between precision and recall as defined by the F-measure. From our experiments, we observe that an acceptable F-measure was when the number of clus-ters was around 5 times the vocabulary size. We use this observation while building the clusters over our test data.
Further, instead of creating a single cluster tree over the data, multiple such trees are generated for the same set of data, and a voting scheme is used to identify clusters that are reliable. In cases where errors remain in the cluster, we do end up with errors in their labeling. However, as we shall see in Sect. 7.2 , the drop in recognition performance due to the clustering scheme is less than 3%, while providing tremendous speedup. 6.2 Refuse to label and soft labeling In our problem definition, we restrict ourselves to a subset of the vocabulary, which allows us to focus more on label-ing those words that would be useful for retrieval. Conse-quently, there would be many word image clusters that do not belong to the vocabulary of interest. In order to avoid labeling such clusters, we assign labels only if the classifier score for the closest keyword is greater than a predefined threshold. The reject threshold is chosen by analyzing the effect of the threshold on the precision of classification and the accept-rate (or recall) of the classification.
The effect of the threshold over classification of the vali-dation dataset is shown in Fig. 7 . As observed from the plot for 1-NN classification, the precision is high across various recall points. We choose the threshold where recall is atleast 0.8, i.e. atleast 80% of the points are assigned a suitable label. At this threshold, the accuracy of classification is more than 95%. Further, we observe that with the k  X  NN clas-sifier, better performance is observed with smaller k .The 1-NN classifier gives the best recognition accuracy across different k .

On the other hand, instead of assigning a single label to a cluster (or word image), one could also label the word images with multiple matching keywords, along with storing their match score. This would partially address the presence of similar-looking but distinct word images within a clus-ter. By assigning the top-m word labels to a given cluster, one could improve recall for rare words that are not large enough to form a distinct cluster in the feature space. This would require a small change in the computation of prob-abilities of Eq. 1 , by computing probabilities over all valid V k for a given cluster centroid t i . The results in this paper were obtained without assigning soft labels, owing to mem-ory constraints on storing and accessing the large indexes. Soft labeling could also be employed by allowing each word image to belong to multiple clusters, drawing from success of such methods in object retrieval [ 59 ]. Some of the advan-tages of soft-clustering are implicitly obtained by building multiple cluster trees. Explicit soft labeling, however, would hurt the speedup that we gain from assigning word images toasinglecluster. 6.3 Effect of segmentation errors Though the segmentation requirements at the word level are much easier to handle than character segmentation, errors occur occasionally in word segmentation. This could be due to overlapping ink pixels across multiple lines of text, or due to insufficient inter-word spaces. In our framework, we match words as a whole, refraining from partial matching (using DTW), to save on computational expense. This results in words with segmentation errors to be rejected or mis-labeled in the annotation phase. 6.4 Scaling to larger collection Once the index is built over a 1,000 books, one can examine the prospect of adding more books to the searchable col-lection. Given another 1,000 books, the same process can be repeated: (i) divide the collection into M subsets each, (ii) build hierarchical clusters over each of the M subsets and (iii) match these trees with those built over the Training set. It can be inferred that the process is linear in collection sizes, and thus, the framework is scalable to bigger digital libraries. 7 Experimental results In this section, we shall evaluate various choices avail-able for building our framework. The evaluation of features (Sect. 7.1 ), indexing schemes (Sect. 7.2 and labeled data quantity (Sect. 7.3 ) is based on the groundtruthed validation dataset (described in 1.1 ). The performance of our approach on the Te s t dataset is presented in Sect. 7.4 . 7.1 Feature evaluation The features that represent the word images are required to be robust to variations in font, style and degradations. We examine three holistic features that have been popularly used in the document recognition community:  X  Profile features  X  Scale Invariant Feature Transform (SIFT) +  X  Pyramid Histogram of Oriented Gradients (PHOG) 7.1.1 Profile features Profile features were popularized by Rath and Manmatha [ 60 ], where these features were used to represent words from handwritten documents. The profile features we extract include (see [ 60 ] for more details):  X  The Projection Profile is the number of ink pixels in each  X  Upper and Lower Profile measures the number of back- X  Transition Profile is calculated as the number of ink-
Each profile is a vector whose size is the same as the width of the word. The dimensionality of the feature, there-fore, varies with the word used. Such variable-length fea-tures are typically matched using Dynamic Time Warping (DTW) [ 60 , 61 ]. However, owing to the computational cost of DTW,analternativefixed-lengthrepresentationwasobtained by scaling the word images to a canonical size before extract-ing the Profiles. 7.1.2 SIFT + BoW An alternative to profile features is to use a point-based rep-resentation for the word images. SIFT [ 62 ] (Scale Invari-ant Feature Transform) has proved to be very successful in many computer vision tasks. The SIFT operator contains two parts X  X n interest point detector and a descriptor. The inter-est point detector is based on the difference between multi-scale Gaussians of the given image. The descriptor is a his-togram of oriented gradients. The scale-invariance property of SIFT and its high repeatability across various affine trans-formations makes it a good candidate to be used in document images [ 45 ].

Pairwise matching of SIFT features between word images is time consuming. To alleviate this, the features are mapped toauniqueID,calledthe visterm .Imagescannowbematched by counting the overlap between corresponding visterms. The visterms are obtained by vector quantizing the SIFT fea-tures using K-Means clustering. Each feature in a word image is represented as the index of the cluster visterm. The word image is then represented as a histogram of the occurrences of each visterm. Given this representation, two words are compared by finding the distance between the correspond-ing histogram feature vectors. 7.1.3 PHOG + BoW Unlike SIFT, which is a sparse detector, we also examine a dense representation with similar descriptors such as His-tograms of oriented gradients (HoG) [ 63 ]. We use a pyrami-dal HoG [ 64 ] to represent each word image. To compute this feature, a fixed-size window is moved across the word image, and at each instance, a HoG descriptor is computed. Once the entire word image is scanned by the window, the window size is doubled and the process is repeated, till the window size exceeds the word image X  X  size. The set of extracted features are vector quantized, and the word image is represented as a histogram of the occurrences of each visterm.

Performance of the features is reported in Table 3 , which shows that the best performing feature was the scaled-profiles with L1 distance matching (83%). Thus, our setting outper-forms the state-of-the-art DTW matching of profile features (81%), which also has the additional disadvantage of high time complexity. The recognition over the validation dataset set alone takes around 75h with DTW, as opposed to 3.5h using scaled-profiles. Further, the gradient features X  X IFT and PHOG X  X erform rather poorly in spite of their great suc-cess with generic vision tasks. This is not surprising, given the large amount of degradations present in our document images which drastically affect the oriented gradients .The profile features are thus more robust to degradations.
Example results from our annotation module are given in Fig. 8 . As we observe, a large number of words are cor-rectly recognized in spite of heavy degradations and font variations. We benefit from the presence of similar-looking-labeled exemplars in the training set. It is important to note that the best performing Telugu OCR does poorly on the kind of degraded words that we successfully label.

In cases where our procedure fails, the features are unable to distinguish between different words. For example, con-sider the results for the word man X  X hi (row2)ofFig. 8 . The first erroneous word is man X  X a , which differs in the last (third) character. Notice the strong similarity between the first correct and the first erroneous word. In the sec-ond erroneous example, the second half of the word ( lin X  X hi ) matches with the label and is hence misclassified. We also observe some errors due to erroneously labeled training images. 7.2 Performance of indexing schemes Our next experiment evaluates the performance of vari-ous indexing schemes. The evaluation results from this experiment are given in Table 4 . The features used here are scaled-profiles, and the distance measure is either Euclidean or Manhattan. As can be seen, the indexing schemes give a large speedup of over 500 times as compared to a brute-force NN classifier. The small loss in accuracy of 2.5%, from the use of indexing is an acceptable trade-off for such a speedup. The best performing scheme is the composite of KD-Tree and HKM, used with the L1-Norm. For KDTrees and HKM, we use the FLANN software provided by [ 57 ].
 7.3 Effect of labeled data quantity Another aspect that we evaluate, is what amount of labeled wordimages,isrequiredtoreliablyrecognizeunlabeledword images. Unlike a 50:50 split that was used in the experiments of Sects. 7.1 and 7.2 , we shall vary the split proportions here. We stick to the scaled-profiles and the Euclidean distance based NN classifier with HKM index. The recognition accu-racy for different percentage of training data is shown in Fig. 9 . For the NN classifier, it can be seen that for a labeled dataset of as small as 20%, the recognition performance is a respectable 71%. The performance improves with additional data until it tapers around 76.5% (for the 50:50 split), after which there is no significant improvement. For the HKM approach with B = 8, we can achieve 70% accuracy with just 30% of the data. 7.4 Word-retrieval performance on test data Popular information retrieval evaluation measures such as precision, recall and f-measure, assume that the number of relevant documents is known in the collection. However, given our massive collection, it is impossible for us to obtain the exact recall value for the queries. To evaluate our word-retrieval performance, we propose using an approximate esti-mate of average precision. We manually label the top 1,000 retrieved results, as relevant or irrelevant to the query. By labeling the search results, we are effectively evaluating results over the entire collection; instead of evaluating over a small groundtruthed subset, like in other previous work. The number of true-positives in the top 1,000 is considered as an approximation of the true recall. Hence, when the top-1,000 retrieved results are considered, the recall value is always 1. This is very similar to the evaluation performed in the TREC Web retrieval challenge [ 65 ]. We evaluate the pre-cision across various values for this approximate recall. The Precision-Recall curve averaged over 100 queries is shown in Fig. 10 .

The Average Precision (AP) is computed as the average of precision at each relevant retrieval for the given query. The Mean Average Precision (mAP) is the mean of the AP for multiple queries. We obtained the labels for about 100 queries, from which the mAP was calculated to be 0.8 . A few qualitative examples of document retrieval are shown in Fig. 11 . The query was  X  X ishvadaabhiraama vinuraveima X , a characteristic phrase used by a famous Telugu poet called Vemana . Among the 1,000 books in the collection, the rele-vant books and pages containing or discussing these poems were retrieved. 7.5 Computational time The superiority of our method is further highlighted by the reasonable time it takes for enabling search. The times taken by each step of the pipeline is given in Table 5 . The total compute time for processing the 1,000 books was close to 2,700h, or only 2.7h per book. 8 Comparison with previous work In this paper, we extend some of the ideas that we previously presented (to a limited extent) in Pramod and Jawahar [ 7 ] and in Pramod et al. [ 6 ]. In Pramod et al. [ 6 ], approximate nearest neighbor techniques were used to match exemplar word images with a set of unlabeled word images. This approach was used to build a collection OCR over a smaller set of 33 books (the same set of 33 books was used to generate the training data for this paper). On the other hand, in [ 7 ], word images from the document images were matched against synthetic-labeled exemplars that were obtained by font-rendering with a single canoni-cal font. The word images and exemplars were represented using the profile features [ 66 ], which were matched with a Dynamic Time Warping (DTW) based distance measure. Due to this distance measure, clustering of word images was needed to be performed using hierarchical agglom-erative clustering. The approach was demonstrated over a collection of 500 books consisting of 75,000 document images in the Telugu script. However, the clustering was computationally intensive, requiring close to 21,000h of computing (30 machines running for 30 days). This was thus not scalable to much larger collections; doubling the dataset would have required twice the computing time. Though the complexity of scaling is linear in collection size, the constant is very large, making it quickly unaccept-able.

In this paper, we have modified the techniques, by which we reduced the clustering and annotation process to less than 500h, a speedup of more than 85 times. This was achieved by representing word images using fixed-length representa-tions, which allowed the use of simpler distance metrics. The clustering is speeded up using approximate NN techniques such as Locality Sensitive Hashing, KDTrees and Hierarchi-cal K-Means. Further, the synthetic-labeled exemplars are replacedbytrainingsamplesobtainedfromdocumentimages. Due to the similarity in appearance with word images, the new training exemplars significantly improved the labeling accuracy. 9 Conclusions In this work, we have presented a successful and scalable framework to enable retrieval from a large collection of doc-ument images. Our approach is applicable to many languages for which robust OCRs are not available. In future work, we could explore methods to allow indexing of a much larger vocabulary, in the absence of training data. This would enable retrieving for rare-word queries, especially nouns. Further, we could consider the possibility of applying similar tech-niques for other visual recognition tasks.
 References
