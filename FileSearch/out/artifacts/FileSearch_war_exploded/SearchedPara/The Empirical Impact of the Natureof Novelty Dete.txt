 Novelty detection asks the question, whether a sentence is new (compared to previously acquired knowledge). The first contribution to the study of large scale text novelty detection (experiments were performed on a document level novelty detection dataset) was made by [7]. After that, in the last three years, from 2002 to 2004 [1] [4] [3], there were three Novelty tracks held by the Text REtrieval Conference (TREC). The focus was on sentence level query-specific (intra-topic) novelty detection.
 between theory and observations) were raised regarding the redundancy measure and the novelty detection procedure: symmetric or asymmetric redundancy mea-sure, sentence-to-sentence or sentence-to-multiple-sentences comparison model. We try to answer the questions both theoretically and empirically.
 larity, summarizes the widely used overlap method and one discrepancy between theory and experiments. Section 3 quickly reviews the two relations underlying novelty detection proposed by the accompanying paper. In section 4, we reveal more cases of discrepancy in evaluation of the experiments of the selected pool method. Furthermore, in section 5, we discuss the empirical importance of the evaluation methodologies for Novelty. Section 6 concludes the paper. We know that if all the meanings of a sentence are the same as some known facts, the sentence is redundant. So a symmetric similarity measure between sentences can be used to estimate the symmetric  X  X ame X  relation between meanings. A sentence sufficiently similar to a previous one is considered redundant. Taking a first look at the novelty task, anyone would probably come up with a similarity measure.
 if we think twice, when one sentence X  X  meanings are covered by another, this relation is not necessarily symmetric, because sentences may contain different numbers of meanings. An asymmetric overlap measure should be used eventually, ([6] [7] mentioned such belief).
 empirically better than asymmetric methods like the overlap method, as exper-imental results from [6] [7] indicated.
 similarity, p for pool, and sp for selected pool. In Table 1,  X  X 0.7 X  is the overlap method with a threshold of 0.7;  X  X 0.4 X  is the similarity method with threshold 0.4.
 ter than overlap on both 03 and 04 TREC Novelty collections. The F-measure difference was small because overlap and similarity differ slightly. First, the partial order relation &gt; co , we called the complete overlap relation (CO) . One sentence A &gt; co B, if A contains all the meanings of sentence B. This relation is a partial order relation. It is transitive and antisymmetric. A &gt; po B, if A and B have meanings in common. This relation is non-transitive and symmetric. As the PO relation is symmetric, we called the sentences that are PO related to one sentence its PO relatives. (e.g., for sentence A in { A: A &gt; po BandA &gt; po C alsoB X  X andC X  X POrelative.) Following the two relations, in the selected pool method, only sentences that are PO-related to the current sentence are included in the pool (approximating the PO relation), then, a pool-sentence overlap judgment (approximating the CO relation) follows. In the experiments, if the TFIDF [2] overlap score of the current sentence by a previous sentence exceeded a selection threshold, that previous sentence was considered PO-related to the current sentence. By setting the threshold to be 0, we include all previous sentences in the pool -the selected pool turns back into the simple pool method. Setting the threshold to be the threshold for pool-sentence overlap judgment, the selected pool becomes the overlap method. Experiments showed, in F-measure, selected pool (sp0.7s2.0) is significantly better than overlap (o0.7) by a paired t-test (0.620 vs. 0.608, significant at p=0.000006).The results are presented in Table 2.
 turned novel sentences in the totality of the extra sentences returned. In F-measure s2.0 is better than p0.7, and s5.0 is almost the same as o0.7. But for the additional returned sentences, only a small part (about 1/3 to 1/4) of the addi-tional returned sentences were novel. Simple derivation showed that to increase the F-measure of a set of results, addi tionally returning a set with precision higher than P  X  (P+R) is sufficient, where P and R are the precision and recall of the original result set. For example, if P=0.5 and R=0.9, including a set with precision greater than 0.36 already increases F-measure. This strange property of the F-measure can be misleading when we compare different Novelty meth-ods only using F-measure, and this motivated us in devising new evaluation measures.
 worse performance of the theoretically advantaged overlap measure (comparing to the similarity measure) arose. As novelty detection could be treated as a two step classification task (PO and CO judgments). We propose an evaluation method, in which the error rate of the classifications is used, as in the following Pairwise Sentence Measure (PSM): pairs and the false overlapping pairs are usually called misses and false alarms in the classification terminology (face detection for example), whereas the task here is to detect the overlapping pairs. We define the PSM more clearly; for a run A, suppose A R is the set of redundant sentences judged by A, and A N the novel sentences judged by run A (the sentences finally returned by A). Immediately, A
R A N = C (disjoint union), where C is the collection of all sentences. i is a sentence, is the set of redundant sentences by judgment (true redundant sentences), PO i is the set of true PO relatives of sentence i ,and SPO i is the set of sentences run A judges as the PO relatives of i . For any X set of sentences, suppose, | X | is the usual measure of X (the number of elements in X ), | X | R is the redundancy measure of X (the number of redundant ones in X ), and | X | N is the novelty measure of X (the number of novel ones in X ). Here, we have |
X | correct pairs for PSM evaluation for all the 50 topics; there are thousands of sentences and tens of thousands of sentence pairs in the Novelty datasets (quite a tremendous task for the limited labor force available). Also, not all methods can return the PO relatives of a sentence, so we adopted an alternative that only made use of the human assessments currently available.
 every sentence has only one previous PO relative in the list, i.e. Redundancy-Mistake measure in [7] -an increase in SPSM always corresponds to a decrease in the Redundancy-Mistake. If a run A returns more novel sentences on the basis of B (i.e. A N  X  B N ), | A N | N  X  X  B N | N = | A N  X  B N | N ,then #novel in A N  X  B N (number of novel ones in the extra returned sentences) corresponds to the decrease in #false alarms, and #redundant in A N  X  B N corresponds to the increase in #misses. This is shown in the last column of Table 2. Under SPSM, in Table 2, selected pool (sp0.7s5.0) on Novelty 03 dataset is slightly better than overlap, and simple pool is worse than both selected pool and overlap. For the 04 collection the improvement of selected pool is consistent under both SNM and SPSM.
 rable to the similarity method. Actually, similarity is better than overlap on both 03 and 04 datasets under SNM because of the use of SNM. Till this point we have both theoretically and empirically answered the questions in the introduc-tion proposed by [7]: symmetric or asymmetric measure (the SPSM evaluation for similarity and overlap pointed out where the problem truly lies: the evaluation method; symmetric measures are not always better under SPSM), one-to-one or one-to-multiple comparison (the selected pool method solved the mystery of this discrepancy between theory and practice). These questions arose because of the unclear perception of the nature of novelty detection. The PO-CO framework and related discussions as the characteristics of novelty detection is important because they provide new insights into Novelty theoreti-cally (detailed discussion provided in the accompanying paper) and empirically. crepancies between theory and observation) proposed by [7]. This work has pro-posed a new approach to novelty detection (a new family of methods)  X  selected pool, which has been proved effective empirically. In the selected pool theme, not only can sentences be treated as sets of weighted terms like in section 4, but also can be treated as vectors or language models or sequences of terms. The nature of novelty detection takes effect independent of the representation of individual sentences. We leave the discovery of an optimal representation of sentence to the future work. In addition to the above empirical achievements, this paper pro-vides as well insights into the characteristics of the datasets, and the evaluation methodologies. All these factors together contributed to the empirical impact of the nature of novelty detection.

