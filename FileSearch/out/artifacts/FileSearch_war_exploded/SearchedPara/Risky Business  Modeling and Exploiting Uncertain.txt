
Most retrieval models estimate the relevance of each docu-ment to a query and rank the documents accordingly. How-ever, such an approach ignores the uncertainty associated with the estimates of relevancy. If a high estimate of rele-vancy also has a high uncertainty, then the document may be very relevant or not relevant at all. Another document may have a slightly lower estimate of relevancy but the cor-responding uncertainty may be much less. In such a cir-cumstance, should the retrieval engine risk ranking the firs t document highest, or should it choose a more conservative (safer) strategy that gives preference to the second docu-ment? There is no definitive answer to this question, as it depends on the risk preferences of the user and the infor-mation retrieval system. In this paper we present a general framework for modeling uncertainty and introduce an asym-metric loss function with a single parameter that can model the level of risk the system is willing to accept. By adjustin g the risk preference parameter, our approach can effectively adapt to users X  different retrieval strategies.

We apply this asymmetric loss function to a language modeling framework and a practical risk-aware document scoring function is obtained. Our experiments on several TREC collections show that our  X  X isk-averse X  approach sig-nificantly improves the Jelinek-Mercer smoothing language model, and a combination of our  X  X isk-averse X  approach and the Jelinek-Mercer smoothing method generally outperform s the Dirichlet smoothing method. Experimental results also show that the  X  X isk-averse X  approach, even without smooth-ing from the collection statistics, performs as well as thre e commonly-adopted retrieval models, namely, the Jelinek-Mercer and Dirichlet smoothing methods, and BM25 model.
H.3 [ Information Storage and Retrieval ]: H3.1Content analysis and Indexing; H.3.3 Information Search and Re-trieval Algorithms, Experimentation, Measurement, Performance
The aim of information retrieval is to find information rel-evant to users X  needs. Probabilistic retrieval models prim ar-ily focus on building the correspondence (relevance) betwe en users X  information needs (queries) and documents. These models have led to various document ranking algorithms, including the language modelling approaches [14] and the Divergence from Randomness (DFR) model [1].

However, most estimates of the relevance of documents to queries only provide point estimation and ignore the uncer-tainty associated with the estimates. As such, they provide a  X  X est guess X  by maximum likelihood estimation (MLE) or maximizing a posterior probability estimation of relevanc y. These approaches do not consider two fundamental research issues, namely, (i) the uncertainty associated with the mat ch between queries and documents, and (ii) development of a utility-based ranking function that considers the corresp ond-ing uncertainty in the matches.

To investigate these issues, we have conducted both theo-retical and empirical investigations. Theoretically, we r eturn to the basic question in information retrieval: estimating the probability of relevance. We argue that this probability es -timation should consider both the associated uncertainty, and the fact that these uncertain probability estimates wil l be used to rank documents. Our approach uses a Bayesian formulation to model the uncertainty associated with the estimation. An asymmetric loss function is used to model the risk associated with the estimation uncertainty. The resulting ranking formula incorporates both the mean and variance of the estimate and provides a single parameter that allows us to adjust the desired level of risk. By risk ad-justment, our approach can adapt to a range of IR metrics that reflect different user information search strategies.
We apply the approach to a language modelling frame-work, and a novel risk-aware document scoring function is presented. Empirically, the proposed approach has been studied and evaluated on several TREC collections. The experiments demonstrate that significant performance gain can be achieved if we take a  X  X isk-averse X  approach. Our experiments also show that the risk-adjustment parameter can effectively adapt to different levels of risk associated w ith users X  different retrieval strategies.

In the following, we describe previous work in Section 2, discuss our risk-aware ranking approach in Section 3, test our approach on five TREC test collections in Section 4, and conclude in Section 5.
Formal retrieval models have formed the basis of informa-tion retrieval research since the early 1960 X  X . The two dif-ferent document-oriented and query-oriented views on how to assign a probability of relevance of a document to a user need have resulted in several different types of practical mo d-els [17]. The classic probabilistic model of information re -trieval (the RSJ model) [18] takes the query-oriented view (or need-oriented view), assuming a given information need and choosing the query representation in order to select rel -evant documents. A further development of that model led to the widely-adopted term weighting function known as the BM25 formula [19]. In the document-oriented view, first proposed by Maron and Kuhn [13], the objective is to choose the appropriate document representation to match queries and judge its relevance. The language modelling approach [14] builds upon the document-oriented view. In the ba-sic language models, a unigram model is estimated for each document and the likelihood of the document model with respect to the query is computed. Many variations and ex-tensions have been proposed [6, 10, 24]. The third type of model is called the Divergence from Randomness (DFR) model [1]. In this model, the weight of a query term is cal-culated on the basis of the hypothesis that the more diver-gence there is between the within-document term-frequency and the term X  X  frequency within the collection, the more the information is carried by the term in the document.
To resolve the uncertainty with the estimation, recent studies have focused on building a more accurate document model. Examples include smoothing from collection statis-tics [6, 26], the latent models [2, 7], and Dirichlet models [12]. Alternatively, a full Bayesian treatment has been in-troduced into the language modelling framework [24]. How-ever, the uncertainty directly associated with the ranking problem has received much less attention.

The most relevant work can be found in [10, 27], where a risk minimization framework is proposed and documents are ranked based on an ascending order of the expected risk of a document. It has been applied to subtopic retrieval by mod-elling not only relevance but also redundancy, novelty, and subtopics [27]. But, nonetheless, in the resulting impleme n-tation, the studies [10, 27] still result in a point estimati on, and use the mode of the posterior as opposed to integrat-ing out model parameters. Therefore, the uncertainty of the estimation is still not fully addressed.

In previous work [28], we have conducted early study of utilizing the variance of the prediction, where we as-sumed the relevance score follows a Gaussian distribution. In this paper, we derive a more general and practical form of the risk-adjustable ranking function, and apply the rankin g function to language models.
The Probability Ranking Principle (PRP) of information retrieval [16] implies that ranking documents in descend-ing order by their probability of relevance produces optima l performance under a  X  X easonable X  assumption, i.e. the rele -vance of a document is independent of other documents in the collection [22].

A classic solution to estimating the probability of rele-vance is to treat it (or the parameters of the assumed distri-bution) as a fixed unknown constant that does not have an associated distribution. Existing relevance-based retri eval models, such as the RSJ model [18], two-Poisson model [19], and resulting BM25 formula [20], all belong to this category . The language modelling approaches, although not designed to directly estimate the relevance of documents, also con-sider the model parameters as unknown fixed constants.
The main drawback of this approach is that exact mea-sures of the uncertainty associated with the estimation are not handled in a principled manner, either for the probabil-ity of relevance or the model parameters. As a consequence, unreliably-estimated documents may be ranked highly in the ranked list, reducing the retrieval performance of the top-N returned documents.
 To address this problem, this paper takes an alternative Bayesian view point. We consider the parameters, either the probability of relevance or the model parameters (in the language models), as random variables, and calculate their posterior probability given the observed data. Without los -ing generality, let us denote  X  as the estimation of the cor-respondence between a query q and a document d . In the relevance models, the estimation  X  is equal to P ( r | d , q ) ( r denotes relevance), while in the language models, it is equa l to p ( q |  X  d ) (  X  d is the estimated document model for d ). For-mally, the posterior probability of  X  is written as: where O denotes the observations we obtained so far. It may include the features from the document and the query, or relevance feedback from the user.

By associating an uncertainty with each document X  X  es-timate of relevance, we are able to consider more sophisti-cated ranking algorithms. For example, consider two doc-uments, the first of which has the highest estimated rele-vancy but with a corresponding high uncertainty, and the second document having a slightly lower relevancy but with a corresponding low uncertainty. If we can only choose one document, which should we choose? If the first document is chosen, then there is a chance that it is  X  X ery relevant X , but equally, there is a chance that it is much less relevant ( X  X lightly relevant X ) than we expected. Herein lies the ris k. If we choose the first document, the result may be  X  X ery relevant X  or  X  X lightly relevant X . By choosing the second do c-ument, we reduce the variability (risk), but now the chance that the document is  X  X ery relevant X  is smaller, as is the chance that it is  X  X lightly relevant X . Thus, there is a choic e to be made between consistency, at the expense of relevancy, or relevancy at the expense of consistency.

To explicitly model such uncertainty with ranking, this paper introduces a loss function L (  X   X ,  X  ). It denotes the loss when we choose  X   X  as our ranking score while the true value is  X  . We do not take a point estimation, e.g., maximizing a posterior probability, as in [6, 26]. Instead, we integrat e out the unknown model parameters. This provides a natural and principled way to deal with the uncertainty both related to the ranking and parameter estimation. Marginalizing out the unknown true  X  , we obtain the following expected loss for a given  X   X  : where E denotes the expectation. The optimal ranking score should minimize the expected loss function in Eq. (2). We therefore derive our Bayesian optimal ranker as follows: where  X   X  B is our Bayesian ranker. It is indeed the Bayes estimator of  X  with respect to the loss function L . Notice that the development so far is similar in spirit to the frame-work introduced in [10, 27]. However, we shall see later that our full Bayesian treatment results in a completely differen t document scoring function. On the other hand, the aim and formulation of our Bayesian ranker also depart from those in [24]. We intend to address the ranking uncertainty, while the authors in [24] aimed at dealing with the uncertainty when building a document model (the document model is marginalized out when generating a query).

In the remainder of this section, we will introduce an asymmetric loss function, and derive an approximation to Eq. (3). The loss function will then be applied to language models. 3.1 Asymmetric loss function
One way to model ranking uncertainty is to introduce an asymmetric utility function that penalizing under-estima ting less that over-estimating the rank score, or vice versa. We introduce one such function called the linear/exponential (LINEX) asymmetric loss function [25] given by: where b  X  R is the parameter to balance the loss. Roughly speaking, the loss of LINEX increases exponentially on one side of zero and increases linearly on the other side. One of the advantages of the LINEX loss function is that we can easily balance the loss on both sides by adjusting a single parameter.
 Substituting Eq. (4) into the expected loss in Eq. (2) gives:
E  X  ( L (  X   X ,  X  )) = e b  X   X  E  X  ( e  X  b X  | O )  X  b (  X  where we use  X   X  as the ranking score while the true value is  X  . Minimizing Eq. (5) with respect to  X   X  yields the optimal ranking score (For detailed information, we refer to [25]):
In Eq. (6), E  X  ( e  X  b X  | O ) is the moment generating function (MGF), which generates the moments of the probability dis-tribution  X  , and ln( E  X  ( e  X  b X  | O )) is the cumulant-generating function of  X  . Based on [9, 11], we define the cumulant gen-erating function via the characteristic function as: where the cumulants  X  n are given by derivatives of the cumulant-generating function [9, 11] as:  X  1 =  X ,  X  2 =  X  2 =  X  2  X  the mean,  X  2 is the variance, and  X  n is the n -th moment about the mean of  X  .

The risk-adjustable ranking function then becomes: where  X   X  B is our Bayesian optimal ranker,  X  denotes the mean of the posterior probability of  X  ,  X  2 is its variance,  X  measure of the lopsidedness of the posterior distribution and  X  4 is a measure of whether the distribution is tall and skinny or short and squat.

Eq. (8) gives a general formula to rank documents when considering asymmetric loss. It shows that to address the uncertainty, the final ranking is equal to the mean adjusted by a function of the variance (weighted cumulants). De-pending on the specific probability distribution  X  , one can
The third central moment is called the skewness. A distri-bution that is skewed to the left (the tail of the distributio n is heavier on the left) will have a negative skewness. A distr i-bution that is skewed to the right (the tail of the distributi on is heavier on the right), will have a positive skewness. derive different forms of Eq. (8). For example, if  X  conforms to a normal distribution, the cumulants are  X  1 =  X  ,  X  2 and  X  3 =  X  4 = ... = 0. 3.2 Risk-aware language models
In this section, we present an application of the proposed document ranking approach under the language modelling framework. However, it is worth mentioning that the pro-posed method is generally applicable to any probabilistic retrieval model. 3.2.1 Unigram language models: In the language modelling framework, document ranking is primarily based on the following two steps. First,  X  X hoose X  a generative model for the target document P (  X  | d ), and then generate the query terms using that model p ( q |  X  ) [26], where d and q can be formally represented as vectors of indexed term counts as: where q i ( d i ) is the number of times the term i appears in the query (document) and | V | is the size of a vocabulary.
The unigram language models consider a Multinomial dis-tribution  X  for each document, where  X  = (  X  1 , ...,  X  i and ument, to estimate  X  i = p ( i |  X  ) for each query term i . A straightforward approach is to apply the maximum likeli-hood estimation (MLE) method: where | d | X 
However, estimating from one single document is unreli-able due to small data samples. One of the common solu-tions is to use the posterior probability as opposed to the likelihood function. Using the conjugate prior of the Multi -nomial distribution (the Dirichlet) results in the followi ng posterior probability: where the prior p (  X  |  X  ) is deployed to incorporate prior knowl-edge of the model parameters, and  X   X  (  X  1 , . . . ,  X  | V | parameter of the Dirichlet prior. Maximizing the posterior probability (taking the mode [5]) gives a general form of the language modelling approaches: where, for simplicity, we denote c i  X  d i +  X  i and  X  c  X  P i ( d i +  X  i ). The so-called hyper-parameter  X  acts as pseudo-counts, and can be used to integrate collection statistics f or smoothing the estimation [6, 26]. A further discussion on this can be found in Section 3.3. 3.2.2 Our risk-aware approach: Previous studies have demonstrated that introducing the pseudo-counts in Eq. (12) alleviates the problem of small data samples to some extent. Yet, using such point estimation alone to rank documents is insufficient.

In this section, we derive an alternative, risk-aware lan-guage model from Eq. (8). As an approximation, we apply Eq. (8) to the posterior probability distributions of indi-vidual terms, and leave the application of Eq. (8) to the whole query X  X  posterior probability distribution to futur e work. Therefore, the cumulants of the Dirichlet distribu-tion can be obtained on the basis of the moments of the Dirichlet distribution as follows: where  X   X  i and  X  2 i are the mean and variance of  X  i , respectively.
For language models of documents from a reasonable large dataset, c i is generally much smaller than  X  c . Therefore, we can easily show that the value of  X  n decreases sharply as n increases. Our experiments (not shown) also show that our risk-adjustment approach is effective when only taking into account the first and second moments, i.e., mean and variance, and the introduction of higher moments does not affect the result significantly. Therefore, an approximatio n that considers both the first and second moments provides an excellent trade-off between accuracy and efficiency:
Assuming term independence in a unigram language model, we reach our final ranking score of a document d for a given query q as where  X   X  B denotes a risk-aware ranker in the language mod-elling framework. Again c i  X  d i +  X  i and  X  c  X  Therefore, a single parameter b adjusts the risk in ranking. A positive b produces a risk-averse (conservative) ranking where unreliably-estimated documents (with big variance) are given a lower rank (in the fear that they are less rele-vant than estimated). The bigger the parameter b is, the more conservative the ranking is. On the other hand, a neg-ative b gives a risk-inclined ranking. In this case, unreliably-estimated documents are given a higher rank (in the hope that they are more relevant than estimated). 3.3 Discussions
Eq. (15) consists of three types of parameters. { d i } | V | accounts for the document features; parameter b provides a way to address risk, and hyper-parameter  X  is a place to integrate subjective or prior knowledge. The discussions o n the parameters, variance, and their relationships now foll ow. 3.3.1 The hyper-parameter: Various choices for the hyper-parameter  X  lead to different ways to smooth the es-timation [24]. The proposed ranker has the same ability to incorporate collection statistics into the estimation. Je linek-Mercer smoothing [8, 26] uses a single coefficient  X  to linearly interpolate the maximum likelihood model with the collec-tion model. It is straightforward to adopt Jelinek-Mercer n ( i, D ) denotes the number of occurrences of term i in the collection, and | D | is the collection size.

Alternatively, the Dirichlet smoothing approach [26] ad-justs the parameter  X  on the basis of document length | d | by setting  X  =  X  | d | +  X  , where  X  is the Dirichlet smoothing pa-rameter, and the parameter  X  i for the Dirichlet smoothing 3.3.2 Factors influencing variance: In previous work, the relationship between the variance of parameters and col -lection statistics such as IDF (inverse document frequency ) [4] has been studied. In this section, we show that the vari-ance is also related to the document length. Incorporating Figure 1: Effect of background smoothing on vari-ance. (a)Under Dirichlet smoothing parameter  X  . (b)Under Jelinek-Mercer smoothing parameter  X  . variance in our framework provides an alternative justifica -tion for using the document length. To understand this, let us set  X  i = 0 and substitute c i = d i and  X  c = | d | into Eq. (13), to obtain: This equation illustrates that the variance monotonically de-creases with respect to the document length. In a risk-avers e setting ( b &gt; 0), the ranker will favor longer documents over shorter ones, if both documents have the same estimated mean. This is a good feature because short documents may yield unreliable estimates.

To further study the effect of document length on the vari-ance  X  2 i , and the influence of the background smoothing pa-rameters, we plot the relationship between document length and variance in Fig. 1 (a) and (b) for the Jelinek-Mercer and Dirichlet smoothing language models, respectively.
For simplicity, we set d i = 3 and n ( i, | D ) | D | = 0 . 0001, which are typical values for a dataset. We plot the variance for dif -ferent values of Dirichlet parameter  X  as shown in Fig. 1 (a). We see that large values of  X  significantly reduce variance, especially for short documents. And the longer a document, the smaller the variance. When  X  is typically set around 2000 [26], the variance remains relative small irrespectiv e of the document length. Conversely, for the Jelinek-Mercer smoothing method (shown in Fig. 1 (b)), we see that the variance has a different trend when  X  is typically set be-tween 0.1 and 0.3 [26].

This explains why the Dirichlet smoothing method that utilizes the document length generally outperforms the Jel inek-Mercer method for document ranking. Since the variance under the Dirichlet smoothing method remains small irre-spective of document length, it seems more preferable to apply our approach to the Jelinek-Mercer smoothing lan-guage model. By combining our risk-aware approach with Table 1: Overview of the five TREC test collections. the Jelinek-Mercer language model, we provide an alterna-tive way to consider both collection statistics such as IDF and document length. Our experimental results in Section 4 demonstrate that the combined approach can generally outperform the Dirichlet smoothing language model. 3.3.3 The risk-adjustable parameter: In Eq. (15), the final ranking score relies on not only the estimated mean, but, equally importantly, the variance associated with the estimate. To the best of our knowledge, this has not been previously studied. By setting the parameter b we adjust how much risk we are willing to take when ranking docu-ments. When subtracting a function of the weighted vari-ance from the mean ( b &gt; 0), the ranker takes a more conser-vative approach which effectively reduces the overall uncer -tainty (variability) in the top-N ranking list. This is at the expense of dropping documents that are potentially  X  X ery relevant X  but also very uncertain. Conversely, when adding a function of the weighted variance to the mean ( b &lt; 0), the ranker takes more risk, and thereby increasing the uncer-tainty (variability) in the top-N ranking list. In this case, it is hoped that a relevant document is X  X ery relevant X , but it i s just as likely to only be  X  X lightly relevant X  document. When b is set to 0, the variance is ignored and the ranking score is equivalent to conventional language models. Certainly, there are many factors influencing the optimal setting of b . In Section 4, we limit our study to investigating what is the best ranking strategy for the TREC evaluation. An investi-gation into formal optimization is left for future work.
We examined five TREC collections described in Table 1. TREC2004 robust track is evaluated with an emphasis on the overall reliability of IR systems, i.e. minimizing th e number of queries for which a system performs badly. 50  X  X ifficult X  topics among the TREC2004 robust track topics can help us understand whether our approach is effective for both  X  X rdinary X  and  X  X ifficult X  topics.

Documents were stemmed using the Porter stemmer, but not stopped at indexing time. Instead, stopping is carried out at query time using a standard list of 421 stopwords. In all our experiments, only the title portion of the TREC topics are used as queries.
 Recall that in Fig. 1 (a), for large values of  X  in the Dirichlet smoothing, the variance becomes relatively smal l, and consequently, our risk adjustment becomes less effec-tive. Therefore, it is more favorable to combine our approac h with the Jelinek-Mercer smoothing language model. There-fore our experiments in Section 4 focus on the evaluation of the risk-averse approach combined with the Jelinek-Mercer smoothing language model. 4.1 The risk-adjustable parameter
In this section, we first investigate whether a risk-averse o r risk-inclined approach is more helpful in improving retrie val performance. We then study what the key factors are that affect the optimal choice of the risk adjustment parameter.
Language models provide theoretical justification for the use of IDF in document ranking [26]. Figure 2: Plots of the percentage of gain on met-rics against parameter b on five collections (under Jelinek-Mercer smoothing). (a) MRR (b) MAP 4.1.1 Risk-averse or risk-inclined? Let us first look at the effect of the risk adjustment parameter over different TREC collections. The performance is measured by MRR (Mean Reciprocal Rank) and MAP (Mean Average Preci-sion), the two main performance measures in TREC. We vary the value of b between -100 and 400, and the percent-age improvement 3 for these b values and fitted curves based on the data points, are shown in Fig. 2. The results are re-ported on the risk-adjusted Jelinek-Mercer smoothing lan-guage model where  X  = 0 . 1. Similar results were obtained for  X  =0.2, 0.3, and 0.4. When we apply our approach to a model without background smoothing (assign 0.5 to the zero counts), we also obtain similar results, and the improv e-ments for MRR and MAP for b &gt; 0 are even greater.
We can see from Fig. 2 that risk aversion applies to all five collections. When taking a risk-inclined approach, i.e. b &lt; 0, the MAP and MRR for all five collections degrade. In con-trast, a risk-averse approach where b &gt; 0 can improve the MRR and MAP on all five collections. The improvements on MRR for four of these collections are statistically signific ant , and on MAP are statistically significant for two collec-tions. It is worth noting that the improvements on MRR for the WT10g collection can be above 40%, and on MAP for the WT10g collection can be above 15%. This suggests that for a larger collection like the WT10g, risk adjustment is even more favorable than for smaller collections.
Note that our risk adjustment approach is robust/stable for a value of b anywhere between 100 and 400, for all collec-
The percentage of improvement (or gain) on MRR and other metrics is based on the improvement of the risk ad-justed model over the model where b = 0.
We tested statistical significance with t tests (one-tail crit-ical values for significance levels  X  =0.05). tions. Our approach is effective for the Robust2004 and Ro-bust2004 50 difficult topics since the improvements on MRR for these two can be above 10% and 20%, respectively. 4.1.2 Key factors affecting the ranking risk: We now investigate how our model behaves under a risk-sensitiv e metric called k -call at 10, or simply k -call, proposed in [3]. Given a ranked list, k -call is one if at least k of the top-10 documents for a query are relevant. Otherwise, k -call is zero. Averaging over multiple queries yields mean k -call. The two extremes are 10-call, an ambitious metric of per-fect precision: returning only relevant documents, and 1-c all as a conservative metric that is satisfied with only one rel-evant document. Therefore, a risk-averse approach, which can reliably find one relevant document with small variance, is preferred for 1-call, while a risk-inclined approach, which gives small weight to the variance, is favored for 10-call [3 ].
Fig. 3 (a) illustrates the relationship between the optimal values of b and 1 to 10-call on the WT10g collection. The data points are the optimal b values versus these metrics, and the curve is fitted based on the data points. The figure demonstrates that when k decreases, the optimal value of b tends to increase. It confirms that the risk adjustment parameter b controls how much risk we are going to take when ranking documents. A bigger value of b gives a more risk-averse (conservative) ranking.
 Next we study the effect of ranking positions on b . In Fig. 3 (b) and (c), we plot the optimal b value for NDCG at 1 to 25, and Precision at 1 to 100 on the Robust2004 collec-tion, respectively. The curves are fitted based on the data points. The figures illustrate that when the cut-off point is under 14 for NDCG, and 5 for Prec, respectively, the optimal values of b for the two metrics are both large, i.e. around 500, which estimates ranking scores conservatively by weightin g variance highly. However, when the cut-off point is above 17 for NDCG, and 10 for Precision, respectively, the opti-mal values of b for the two metrics are both relatively small, i.e. around 200, which subtracts a lower function of varianc e from the mean. Such behavior suggests that lower rank po-sition favors more conservative ranking (bigger b ). 4.2 Performance 4.2.1 Risk adjustment with Jelinek-Mercer smooth-ing: We compare our risk-averse approach with both the Jelinek-Mercer and Dirichlet smoothing language models. We carried out 5-fold cross validation on each of the five collections. For each collection, we randomly divided the topics into five partitions. For each partition, topics in th e other partitions were used to estimate the parameters on the basis of the MAP, and performance for the partition is eval-uated with the trained parameters. Results were averaged over the 5 partitions, and reported in Table 2.
 The results on a number of metrics including MRR, MAP, NDCG, Precision, and k -call are reported in Table 2. When comparing our approach with the Jelinek-Mercer smooth-ing method, 57 out of the 60 reported improvements are positive, and 45 of these improvements are statistically si g-nificant. Note that 17 of these improvements exceed 20%. When compared with the Dirichlet smoothing method, 52 out of the 60 reported improvements are positive, 28 of these improvements are statistically significant, and 13 of these improvements exceed 15%. We therefore conclude that a combination of our risk adjustment approach with Jelinek-Mercer smoothing can largely outperform both the Jelinek-Mercer and Dirichlet smoothing methods on all collections. 4.2.2 Risk adjustment without background smooth-ing: The current best performing ranking algorithms, such as BM25 and the language models, all use collection statis-tics, such as the IDF of query terms, the collection size, and the average document length. However, the proposed ranker reveals that incorporating the moments, e.g. variance, int o document ranking provides an alternative way to  X  X orrect X  the estimation without using collection data. Independenc e from collection statistics is beneficial when access to the e n-tire data set is prohibited in situations such as peer-to-pe er network environments [15], and meta-search engines.
In this section, we compare our approach without back-ground smoothing with the state-of-the-art language model s and the BM25 model. We used 5-fold cross-validation, and the results are reported in Table 3.

Table 3 shows that when comparing our approach without smoothing to the Jelinek-Mercer smoothing language model, the improvements for 53 out of the 60 reported results are positive, 43 of the improvements are statistically signific ant, and 14 improvements are above 20%. When comparing our approach with the Dirichlet smoothing language model, 46 out of the 60 reported improvements are positive, 26 of these improvements are statistically significant, and 13 improve -ments are above 15%. When comparing our approach with the BM25 model, 25 out the 60 reported improvements are positive, 10 of these improvements are statistically signi fi-cant, and 8 improvements are 9% or above. 4.2.3 Discussion: The strong performance of our ap-proach even without background smoothing demonstrates the effectiveness of our risk-adjustment approach.
Our risk-averse approach conservatively estimates doc-ument relevance score, therefore, putting documents with both high relevance and low variance higher up in the ranked list. This risk-averse approach effectively improves the MR R metric as evidenced by the fact that our method outperforms all three state-of-the-art models on all collections in ter ms of the MRR metric as shown in both Table 2 (when com-bined with the Jelinek-Mercer smoothing language model) and Table 3 (when without background smoothing).

On the other hand, we got mixed results when compar-ing our risk-averse approach with the Dirichlet smoothing method and BM25 model in terms of the MAP and NDCG metrics. This is due to the fact that our method is more ef-fective to retrieve the first relevant document, while metri cs such as MAP and NDCG aim to reflect the overall retrieval performance [21] by taking into account the top-N docu-ments, where N is set as 1000 in our experiments.
The above findings confirm our assumption in Section 3.3.2 that improvements can be made by naturally integrat-ing variance to address uncertainty without using collecti on statistics. As we discussed, this is especially beneficial w hen information about the entire dataset is inaccessible.
In our cross-validation for our risk-adjustable approach both with and without background smoothing reported in Section 4.2.1 and 4.2.2, respectively, the range of the trai ned risk-adjustment parameter b is always between 100 and 400, showing that a risk-averse approach is preferable.
We have presented an approach to document ranking in information retrieval that explicitly models the associat ed uncertainties. The proposed approach led to a risk-aware retrieval model. One of the merits of our approach is that such uncertainty is addressed in a principled way. Further, by using an asymmetric loss function, based on a LINEX cost function, only a single parameter is needed to adjust the risk preference. We have applied our method to the language modelling framework, and a practical risk-aware document scoring function was derived.

The empirical evaluations on five TREC collections have shown that a risk-averse approach significantly improves th e retrieval performance over a number of IR metrics. The pro-posed approach dramatically improves the performance of the Jelinek-Mercer smoothing language model on a number of metrics; a combination of our approach and the Jelinek-Mercer smoothing approach can outperform the Dirichlet smoothing approach on all five collections. We also demon-strated that adding a function of the variance into document ranking provides an alternative way to improve the perfor-mance without smoothing from collection data, and the per-formance of our approach without background smoothing was comparable with that of the state-of-the-art models, namely, Jelinek-Mercer and Dirichlet smoothing language models, and the BM25 model.

Since our approach is more effective to the top-ranked positions, our approach achieved significant overall perfo r-mance gain in terms of the MRR metric over the current models. Furthermore, by varying the risk adjustment pa-rameter, our approach can effectively adapt to different user search behaviors, such as risk-inclined or risk-averse.
We hope that our analysis will increase the awareness of using the second moment for information retrieval mod-elling. The research challenges that need to be addressed in the future include: optimizing risk adjustment on the basis of IR metrics and ranking positions, incorporating term or document correlation in the analysis, and investigating ho w the correlation can reduce our ranking risk [23]. Statistically significant improvements are marked with  X * X  .
