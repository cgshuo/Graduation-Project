 An optimally diverse ranking should achieve the maximum coverage of the aspects underlying an ambiguous or under-specified query, with minimum redundancy with respect to the covered aspects. Although evaluation metrics that re-ward coverage and penalise redundancy provide intuitive ob-jective functions for learning a diverse ranking, it is unclear whether they are the most effective. In this paper, we con-trast the suitability of relevance and diversity metrics as ob-jective functions for learning a diverse ranking. Our results in the context of the diversity task of the TREC 2009 and 2010 Web tracks show that diversity metrics are not neces-sarily better suited for guiding a learning approach. More-over, the suitability of these metrics is compromised as they try to penalise redundancy during the learning process. Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval General Terms: Experimentation, Performance Keywords: Web Search, Learning-to-rank, Diversity
An ambiguous query can pose additional difficulties for a search engine, since it may not be clear which interpreta-tions or aspects underlying this query are of interest to the user [5]. An effective approach for tackling query ambigu-ity is to diversify the search results, in order to maximise the chance that different users will find at least one relevant result to their particular information need [3].

In this paper, we investigate whether a diverse ranking can be automatically learned from training data by optimising an evaluation metric that rewards diversity. In particular, an optimally diverse ranking should achieve the maximum coverage of the aspects underlying an ambiguous query, with minimum redundancy with respect to the covered aspects. Hence, it seems intuitive to guide a learning-to-rank algo-rithm by optimising an evaluation metric that directly ac-counts for both aspect coverage and redundancy. Although several such metrics have been proposed and shown to ef-fectively reward diversity in the search results [1], it is not clear whether they are any better suited than traditional relevance-oriented metrics for learning a diverse ranking. To investigate this, we contrast two relevance-oriented metrics and their diversity counterparts as objective functions for learning-to-rank for search result diversification.
Our investigation is conducted in the context of the diver-sity task of the TREC 2009 and 2010 Web tracks, henceforth WT09 and WT10 tasks, respectively. In particular, WT09 provides 50 queries, while WT10 provides 48 queries, all with relevance assessments at the sub-topic level. Our ex-periments are based on the TREC ClueWeb09 (cat. B) cor-pus, which comprises 50 million English documents, aimed to represent the first tier of the index of a commercial search engine. We index this collection using Terrier, 1 with Porter X  X  weak stemmer and without removing stopwords.

For our learning setup, we produce a sample comprising the top 5,000 documents retrieved for each of the 98 consid-ered queries, using the Divergence From Randomness DPH weighting model, as implemented in Terrier. Besides being effective, DPH is a parameter-free model, and hence requires no training. Based on this initial sample, we compute a total of 61 document features typically used in the learning-to-rank literature [2]. These include standard weighting mod-els (e.g., DPH itself, BM25), field-based (e.g., BM25F) and dependence (e.g., Markov Random Fields) models, spam de-tection, URL and link analysis (e.g., PageRank) features.
To enable learning by directly optimising an evaluation metric, we use Metzler X  X  Automatic Feature Selection (AFS) listwise learning-to-rank algorithm, which has been shown to perform effectively on a web setting [4]. As optimisa-tion metrics, we consider two standard relevance-oriented metrics: expected reciprocal rank (ERR) and normalised discounted cumulative gain (nDCG). Additionally, we con-sider their diversity counterparts: ERR-IA and  X  -nDCG [1]. While ERR and nDCG are established metrics for web search evaluation, ERR-IA and  X  -nDCG are the primary evalua-tion metrics in the diversity task of the TREC 2010 Web track. Moreover, besides rewarding aspect coverage and penalising redundancy, both ERR-IA and  X  -nDCG allow for tuning how much redundancy should be penalised [1]. To assess how penalising redundancy impacts the learning outcome, we consider variants of these two metrics X  X alled ERR-IA  X  and  X  -nDCG  X  , respectively X  X hat do not penalise redundancy at all. To enable a deeper analysis, all optimi-sation metrics are computed at rank cutoffs 20 and 1000. Finally, we report the diversification performance attained by the learned models optimised to each of these metrics in http://terrier.org (ERR, nDCG) or diversity-oriented (ERR-IA,  X  -nDCG, ERR-IA terms of ERR-IA@20 and  X  -nDCG@20 under training and test scenarios. In particular, the latter scenario uses the WT09 and WT10 queries as separate training and test sets (i.e., we train on WT09 and test on WT10, and vice versa).
In this section, we investigate the suitability of relevance-and diversity-oriented metrics for guiding a listwise learning-to-rank approach in order to learn a diverse ranking. Ta-ble 1 shows the diversification performance of the models learned by AFS by optimising the previously described met-rics. Statistical significance is verified using the Wilcoxon signed-rank test. The symbols ( )and ( )denotea significant increase (decrease) at the p&lt; 0 . 01 and p&lt; levels, respectively, while = denotes no significant difference. A first instance of these symbols denotes the significance (or lack thereof) of ERR-IA and ERR-IA  X  compared to ERR, as well as of  X  -nDCG and  X  -nDCG  X  compared to nDCG. A second instance denotes the significance of ERR-IA  X  and  X  -nDCG  X  compared to ERR-IA and  X  -nDCG, respectively.
From the training results in Table 1 (left half), we ob-serve that diversity-oriented metrics generally lead to an improved diversification performance compared to relevance metrics. However, these improvements are not always sig-nificant, particularly for metrics that penalise redundancy. Indeed, on the WT10 queries, neither ERR-IA nor  X  -nDCG can significantly outperform their relevance-oriented coun-terparts, and are generally worse than ERR-IA  X  and nDCG  X  (except for  X  -nDCG  X  @20), which do not penalise redundancy. These results show that, although models that promote diversity can be learned via listwise learning-to-rank, metrics that penalise redundancy do not seem to be particularly suited for guiding the learning process.
The test scenario in Table 1 (right half) complements these observations. In particular, the test performance at-tained by using ERR-IA and  X  -nDCG as optimisation met-rics is not significantly better than that attained using ERR andnDCG,respectively. Infact,  X  -nDCG@1000 (for WT09) and ERR-IA@1000 (for WT10) si gnificantly underperform compared to their relevance-oriented counterparts. On the other hand, when ERR-IA and  X  -nDCG are compared to their variants that do not penalise redundancy, we observe a significantly superior performance of the latter, particu-larly for ERR-IA  X  @1000 and  X  -nDCG  X  @1000. Neverthe-less, although generalising better across different query sets in the test scenario, these variants still cannot consistently and significantly improve compared to relevance metrics.
We have investigated the suitability of diversity metrics as objective functions for learning effective models for search result diversification. Our results show that, contrarily to our intuition, deploying a diversity metric does not necessar-ily help produce learned models with superior diversification performance compared to those produced using standard rel-evance metrics as objective functions. This highlights the need to develop features that better indicate not only the relevance, but also the diversity of individual documents.
Our results also show that models learned by optimising for metrics that penalise redundancy seem to overfit to the training queries, with a poor generalisation across different query sets. This observation c orroborates related research that shows that the target evaluation metric is not necessar-ily the best suited to guide a learning-to-rank approach [6]. Indeed, since standard listwise learning-to-rank approaches assume that the relevance of a document is independent of other documents, optimising for metrics sensitive to redun-dancy (i.e., metrics that consider the relevance of a particu-lar document in light of the other documents) may actually introduce noise to the learning process. [1] C. L. A. Clarke, N. Craswell, I. Soboroff, and [2] T.-Y. Liu. Learning to rank for information retrieval. [3] R. L. T. Santos, C. Macdonald, and I. Ounis.
 [4] R. L. T. Santos, C. Macdonald, and I. Ounis. Intent-[5] K. Sp  X  arck-Jones, S. E. Robertson, and M. Sanderson. [6] E. Yilmaz and S. E. Robertson. On the choice of
