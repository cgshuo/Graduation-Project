 With the rise in the amount information of being streamed across networks, there is a growing demand to vet the qual-ity, type and content itself for various purposes such as spam, security and search. In this paper, we develop an energy-efficient high performance information filtering system that is capable of classifying a stream of incoming document at high speed. The prototype parses a stream of documents using a multicore CPU and then performs classification us-ing Field-Programmable Gate Arrays (FPGAs). On a large TREC data collection, we implemented a Naive Bayes classi-fier on our prototype and compared it to an optimized CPU based-baseline. Our empirical findings show that we can classify documents at 10Gb/s which is up to 94 times faster than the CPU baseline (and up to 5 times faster than previ-ous FPGA based implementations). In future work, we aim to increase the throughput by another order of magnitude by implementing both the parser and filter on the FPGA. H.3.4 [ Information Storage and Retrieval ]: Systems and Software: Performance evaluation Efficiency, Classification, Filtering, Parsing, FPGA
Ever increasing amounts of data and information are be-ing generated and transmitted across the web. A recent report [1] estimated (conservatively) that enterprise servers processed and moved over 9 zettabytes (1 zettabyte = 10 21 bytes) in 2008; this number is projected to double every two years. An increasing amount of this data growth is attributed to unstructured or semi-structured data  X  e.g. emails, web pages, images, videos, etc. For example, Face-book estimates that more than 100 TB of log data are gen-erated every day. This explosive growth in data has led to a corresponding growth in new data analysis needs. e.g. search, live business analytics, social correlation, collabora-tive filtering, spam detection, etc. A common characteristic of these emerging tasks is the need to achieve better perfor-mance at larger scale  X  and lower costs.

Meanwhile, power and cooling have become important constraints in the design and operation of data centers. Many cloud data centers ( X  X arehouse-scale computers X  [2]) report-edly spend millions of dollars every month in electricity costs, and face significant challenges in cooling the infras-tructure. Since energy consumption has a significant en-vironment impact it has become the priority of both gov-ernment agencies and industry consortiums to reduce the impact of operating data centres.

The combination of these two trends motivates the need for new energy-efficient system designs for future data cen-tres. Given the inherent parallelism in many data-centric algorithms, FPGA-based acceleration has the potential to help achieve these goals. This is because they combine mas-sive parallelism with very low power consumption. While FPGAs have been widely used in many application areas, their uptake in high-performance computing has been lim-ited [3]. However, with the recent improvements to the size and processing capabilities of new FPGA platforms (e.g. recent products such as Fusion-io X  X  storage solutions and IBM X  X  Netezza line of data warehousing products) mean that they are being adopted by the high-performance computing community as a viable solution [4, 5, 6].

In this paper, we present an FPGA-accelerated informa-tion filtering system where incoming documents are matched against profiles and classified for some purpose or another [7]. When faced with large volumes of incoming documents, pro-cessing needs to be performed in real time, and therefore time based efficiency is paramount. We have implemented our design on a commercial FPGA platform (GiDEL PROCStar-IV) and have compared it to a multicore CPU based imple-mentation. Compared to a reference software-only imple-mentation, our FPGA implementation, on average, shows 34  X  higher full-system performance and 51  X  higher scoring performance, whereas the power consumption only increases by about 10%. These results show that even a conservative FPGA design can achieve high performance at low power.
Recently, there has been a spate of work investigating different ways to improve the efficiency of various informa-tion processing tasks using novel hardware, either FPGAs (e.g. [8, 9, 10, 11]) or Graphic Processing Units (GPUs) (e.g. [12, 13].) For the purposes of this work, we provide a brief overview of the related work on FPGAs and GPUs. In [10], an FPGA system is used for mapping Language Classification algorithm using n-grams was developed. Lock-wood [10] implemented a Language Classification algorithm to differentiate 255 languages. The design was improved by Jacob [11]. This work uses similar ideas as our work: it performs classification and uses n-grams and Bloom filters. The application and its design are however very different, and our system also performs parsing in real-time.
Ding et al. [14] have investigated the use of GPUs for high-performance Information Retrieval. In principle GPUs are very interesting target platforms for IR because many IR problems are parallelizable. In this work, the authors report on the design of a basic system architecture for GPU-based query processing. Query-based search is actually the dual of document filtering (as explained e.g. [15]): in query-based search, the query is dynamic (and typically small) and the collection is static, and abstracted into an  X  X nverted index X  datastructure. In filtering/classification, the query is static (and typically large) and the collection is dynamic. The work in [14] is a framework for query-based search, so a di-rect comparison with our work is not possible; we can how-ever note that the obtained speed-ups for query processing are moderate only (2-3  X  ).

Most closely related to this current work is a system de-veloped by Azzopardi et al [8] which implemented a doc-ument filtering system similar to the one proposed here, but for indexed documents in bag-of-words format. In their work, they used two RC100 blades with two Xilinx Virtex-4 FPGAs running on a SGI Altix 4700 machine, and imple-mented a relevance feedback based classifier to score docu-ments. This was compared to a reference implementation on a dual-core Itanium processor. From their experiments they reported improvements of 10-20 times faster than the refer-ence implementation. Given the age of the FPGAs used, it is not possible to directly compare our work against their system. In this work we use a more modern and faster CPU (3GHz quad-core as opposed to 1.6GHz dual-core) and FPGA (Stratix-IV), but actually report larger improvements up to 94 times and on average 50 times faster.
The purpose of our system is to classify a stream of unin-dexed text documents based on significant terms in the doc-uments. For example, we might want to scan email traffic for certain words in the messages. A key assumption for our work is that the majority of the documents in the stream will not be of interest to us, i.e. the profiles we use for classification are highly discriminating.
 For this work, we combine a Naive Bayes classifier with Laplace smoothing with n-grams similar to the work in [16]. However, our main interest is not in the effectiveness of the classifier but the efficiency of the term scoring. Our ap-proach in this work is similar to our previous work [17] in that we create a lookup table of weights for the relevant terms (called a profile ). The score for a document is com-puted by adding the weights for each term. To limit the size of the profile, we only include weights with an absolute value above a given threshold.

Using a different classifier  X  for example in [17] we use a relevance feedback model  X  only results in different weights for the terms in the profile. Consequently, our system is suit-able for a large class of classification tasks. In our previous work we used a bag-of-words representation of the docu-ments. This requires the document collection to be indexed, a process which is difficult to perform in real time. In this work, we parse and score a stream of documents (e.g. emails or web pages) in real time, i.e. we score based on the string values rather than on numerical term identifiers. Conse-quently, the profile is a set of (string, weight) tuples rather than (integer identifier, weight) . To support string-based lookup, considerable modifications to the hardware imple-mentation were required. It is important to note that our FPGA-accelerated system performs functionally in exactly the same way as our software reference system, i.e. there is no loss of accuracy.
 Figure 1: Schematic of FPGA-accelerated classifica-tion system
The classifier consists of three main blocks (Figure 1): a very fast Finite State Machine-based parser for text, sup-porting email and HTML/XML content; an n-gram gen-erator; a scoring kernel. The parsed document stream is transferred to FIFO buffers on the FPGA board, the FPGA reads the collection from the buffers and scores it against the profile. Finally, the scores for each document are returned to the host. For this work we used a server with a quad-core Intel Xeon E5450 CPU (clock speed 3GHz, memory bus speed 533 MHz) and a GiDEL PROCStar-IV FPGA boards with four Altera Stratix-IV FPGAs (clocked at 150MHz) and 64 GB of on-board DRAM, clocked at 667 MHz, connected via a 16-lane PCIe bus. The server OS was Fedora 15.
The main contributions of this work are the implemen-tation of the very fast Finite State Machine based parser and the n-gram generator, as well as the adaptation of the scoring design to use n-grams.
 For this work we have implemented the parser in C++. However, the parser was designed to be equally suitable for implementation on FPGAs. The C++ code makes full use of the CPU X  X  available parallelism (8 hardware threads) us-ing POSIX threads. The code was compiled using both gcc (v4.6) and Intel X  X  icpc (v12.1). We found Intel X  X  compiler to be slightly faster with all optimisations enabled. Our tests show that the parser on our platform is capable of parsing document streams at 426MB/s.

The GiDEL PROCStar-IV board contains four FPGAs with each two banks of DRAM used as FIFOs for the doc-ument collection, and one bank of SDRAM for the profile. The profile is copied to each of the SDRAMs, and the collec-tion is written into the FIFOs by each thread of the software parser. As the PCI-Express bus has 16 lanes, there is no con-tention between the transfers from the CPU threads to the FPGA board memory.

The n-gram generator and scoring kernel are implemented on FPGA using VHDL. The n-gram generator design is pipelined and consequently does not affect the throughput of the system. The current version creates bi-grams and tri-grams, but can easily be extended to support larger n-grams.
The scoring design is a modified version of our previous design [17] that used a Bloom filter to reject negative ex-amples. A Bloom filter is a data structure that allows fast testing of membership of a set. This data structure can be implemented very efficiently in the very fast on-chip memory of the FPGA, see [18] for details. If a term is not present in the Bloom filter, the expensive lookup of the term weight in the off-chip memory is avoided. Consequently, this approach can result in very high throughput for highly selective pro-files.

Parsed strings are much longer than the term identifiers in a bag-of-words representation: the vocabulary of a typical collection is usually much smaller than four billion, so the term identifiers can be expressed using 32-bit integers. How-ever, tri-grams consisting of e.g. English words will usually contain around 10-20 characters, which even in 7-bit ASCII would require 70-140 bits. It is therefore not possible to use the term directly as a memory address, as was the case with the term identifiers used in [17]. Instead, we use two keys obtained by applying two hashes to the terms. The hashing is pipelined and does not affect the throughput. To minimise the effect of lookups in the Bloom filter on the throughput, we use our novel Bloom filter architecture described in [18].
Using a standard TREC Test collection, the Aquaint new corpus, we compared the performance of the FPGA-accelerated system against an optimized CPU reference implementation. To do this, we created profiles by combining the TREC top-ics into categories given the subject of the news. Profiles for each category were created from topic descriptions and used by both the CPU and FPGA classifiers.

For the CPU-based reference implementation we followed the same approach as for the parser, using C++ and POSIX threads. For the actual scoring algorithm we used the high-efficiency unordered map from the C++11 standard library.
We performed twelve binary classification experiments us-ing as the first training set one of the TREC Aquaint subcat-egories entertainment , sports and usa , and for the second, one of political , financial , international and washington . We parsed the collection (247MB) and classified it against the profiles for each experiment. For each case, we compared the performance of the FPGA-accelerated system with the CPU-only reference system. We compared both the scoring performance and the full-system performance, because the current system uses a CPU-based parser and FPGA-based scorer. The results are shown in Table 1.
 From these results we make two interesting observations. First, scoring on the FPGA vastly outperforms scoring on the CPU, the median of the speed-up is 51  X  . It is also clear that for the full system, the CPU-based parsing step dom-inates the performance: the median throughput for scoring on the FPGA is 749MB/s, but 426MB/s for the full system. Figure 2: Impact of test set size on FPGA scoring performance
Second, the performance of the reference system does not depend on the size of the test sets, whereas the FPGA X  X  performance shows a clear dependency (Figure 2). This is because the reference system uses a hash-based lookup ta-ble, so lookup times to not depend on the size of the table; the FPGA system uses a Bloom filter as a first step, and performs an external memory lookup when the Bloom filter returns a hit. The average hit rate of the Bloom filter de-pends on its occupancy. The occupancy of the Bloom filter is proportional to the size of the profile. The number of en-tries in a profile depends on the size of the training sets: the profile is built from the terms with sufficiently high weights, so the profile size is not necessarily directly proportional to the size of the test sets; but as is clear from the figure, the variation in the FPGA performance correlates with the size of the test sets.

The current FPGA-accelerated full system performance (426MB/s) is already 34  X  faster than the reference sys-tem. As typical Ethernet utilisation is rarely above 30%, this means the system could process traffic from a 10Gb/s link (i.e. on average 3Gb/s or 375MB/s) in real time.
Yet it is clear from the results that by implementing the parser on the FPGA we could achieve a much higher full-system performance, very close to the scoring throughput of 749MB/s. Moreover, the current FPGA performance is sub-optimal because the Bloom filter was dimensioned for single-term lookups, whereas in the current system every term results in 6 lookups because of the n-gram generation and the need of two keys per term. TREC Aquaint CPU Scoring Speed-up (entertainment,political) 12.5 1167.1 93.2 11.2 426.2 37.9 (entertainment,financial) 13.3 790.8 59.6 11.8 426.2 36.0 (entertainment,international) 13.0 1088.8 83.9 11.6 426.2 36.7 (entertainment,washington) 14.0 963.5 69.0 12.4 426.2 34.4 (sports,political) 14.1 778.5 55.2 12.5 426.2 34.1 (sports,financial) 14.5 514.2 35.5 12.8 426.2 33.3 (sports,international) 14.1 726.2 51.4 12.5 426.2 34.1 (sports,washington) 13.1 644.4 49.3 11.7 426.2 36.5 (usa,political) 14.9 766.6 51.3 13.2 426.2 32.4 (usa,financial) 15.0 522.0 34.9 13.2 426.2 32.4 (usa,international) 14.2 730.8 51.3 12.6 426.2 33.8 (usa,washington) 13.2 660.5 50.0 11.8 426.2 36.2 Median performance 14 749 51 12 426 34
We have presented a novel system for high-throughput streaming document classification, aimed at real-time classi-fication. The system uses Field-Programmable Gate Arrays (FPGAs) to perform the n-gram generation and scoring of the parsed documents against a profile. The parser is im-plemented in C++ on a multicore CPU. Our results using a Naive Bayes classifier on the TREC Aquaint collection show that the system is capable of performing real-time classifi-cation on 10Gb/s internet traffic streams.

For future work, we aim to create a system that can han-dle 100Gb/s by integrating the parser onto the FPGA and improving the scoring performance.
 Acknowledgments This project was funded by an EP-SRC KTA award, and also supported by HP. [1] J. Short, R. Bohn, and C. Baru,  X  X ow Much [2] L. Barroso and U. H  X  olzle,  X  X he datacenter as a [3]  X  X ybrid-core: The big data computing architecture, X  [4] J. Coyne, J. Allred, V. Natoli, and W. Lynch,  X  X  field [5] N. A. Wood,  X  X pga acceleration of european options [6] W. Vanderbauwhede and K. Benkrid, [7] N. J. Belkin and W. B. Croft,  X  X nformation filtering [8] L. Azzopardi, W. Vanderbauwhede, and M. Moadeli, [9]  X  X etezza appliance architecture, X  [10] S. Eick, J. Lockwood, R. Loui, A. Levine, J. Mauger, [11] A. Jacob and M. Gokhale,  X  X anguage classification [12] S. B. Mane, S. B. Bansode, and P. K. Sinha, [13] H. He, J. Lin, and A. Lopez,  X  X assively parallel suffix [14] S. Ding, J. He, H. Yan, and T. Suel,  X  X sing graphics [15] R. Baeza-Yates and B. Ribeiro-Neto, Modern [16] F. Peng and D. Schuurmans,  X  X ombining naive bayes [17] S. Chalamalasetti, M. Margala, W. Vanderbauwhede, [18] W. Vanderbauwhede, S. Chalamalasetti, and
