 Feature extraction is an effective tool in data mining and ma-chine learning. Many feature extraction methods have been investigated recently. However, few methods can achieve or-thogonal components. Non-orthogonal components distort the metric structure of original data space and contain re-ductant information. In this paper, we propose a feature extraction method, named as incremental orthogonal basis analysis (IOBA), to cope with the challenging endeavors. First, IOBA learns orthogonal components for original data, not only theoretically but also numerically. Second, an in-novative way of training data selection is proposed. This se-lection scheme helps IOBA pick up numerically orthogonal components from training patterns. Third, by designing a self-adaptive threshold technique, no prior knowledge about the number of components is necessary to use IOBA. More-over, without solving eigenvalue and eigenvector problems, IOBA not only saves large computing loads, but also avoids ill-conditioned problems. Results of experiments show the efficiency of the proposed IOBA.
 I.2.6 [ Artificial Intelligence ]: Learning X  X nduction; I.5.4 [ Pattern Recognition ]: Application Theory, Algorithm, Performance
Feature extraction is an effective technique to handle the high dimension problem. It has been widely used in many areas.  X 
This work was supported in part by the China NSF grant (#60573157, #60723003, and #60775046).

Principal Component Analysis (PCA) is the most famous feature extraction algorithm. It attempts to search for di-rections of original data along which variations are extremal. During recent years, some variances of PCA have been pro-posed. For instance, Bishop [1] adopts some prior infor-mation to obtain the intrinsic dimension and the optimal number of clusters for PCA in the latent variable model. Weng et al. [2] proposes an incremental principal compo-nent analysis (CCIPCA) to realize online learning for PCA. Tang et al. [3] uses traditional PCA and nonorthogonal binary feature extraction method to get components. Inde-pendent Component Analysis (ICA) is another category of feature extraction algorithms. ICA, along with some ICA-based methods [4] [5], tries to transform original patterns into uncorrelated data in feature space. There are also some other categories of feature extraction methods[6][7].
However, such feature extraction learning models have some shortcomings. (1) Few of those methods can achieve orthogonal components. Even for ICA-based methods, they can not obtain orthogonal components. Cai et al. [8] in-dicated that non-orthogonality can not preserve the metric structure of original data space and this would cause the de-grading of learning performance. Liu et al. [9] dictated that some redundant information would be incorporated into the learned non-orthogonal components. Therefore, orthogonal components are more desired. (2) Nearly all methods need a user to predetermine the number of components. Although for some methods [1], the target dimension estimation can be done. But it deserves noting that such estimation step needs prior knowledge and aggravates the computing load of the algorithms. Therefore, it is necessary to find an approach to automatically generate the proper number of components for feature space. (3) Many of these feature extraction al-gorithms have to solve eigenvalue and eigenvector problems, which aggravates the computing load to some extent. More-over, it is very likely for such methods to meet ill-conditioned problem.

In this paper, we introduce a fast method named as in-cremental orthogonal basis analysis (IOBA) to handle the above problems of typical feature extraction methods. By adjusting the Gram-Schmidt (GS) algorithm [10], we ex-tract an orthogonal representation for the input data, not only theoretically but also numerically. By proposing a data selection scheme, IOBA learns the components efficiently using part of training data rather than all the training pat-terns. By designing a self-adaptive threshold scheme, IOBA automatically generates the proper number of components with little extra step and no prior knowledge. Moreover, IOBA obviates solving eigenvalue and eigenvector problems, thus the computation load is low and the algorithm is stable.
As mentioned in section 1, we try to propose a method to cope with the shortcomings of those typical feature ex-traction methods. Here, we summarize the targets of the proposed IOBA as follows: (1) To learn orthogonal compo-nents (base vectors). (2) To automatically determine and adjust the number of components needed without any prior knowledge. (3) To realize a fast and robust incremental pro-cedure.

To fulfill the goals, we propose the IOBA as follows: for training data that come in an online manner, first, we em-ploy the Gram-Schmidt method to incrementally extract components. Then, we design a way to measure the inde-pendence between the income data and learned components based on linear dependence theorem, and select those train-ing data that are independent of learned components. And a threshold scheme is proposed to automatically determine the number of components. In the rest of the section, we analyze IOBA algorithm in detail.
To learn orthogonal components, we need to consider the independence between learned components and input pat-terns. Suppose we have learned base vectors { b 1 , b 2 , ..., b If an input pattern  X  k +1 is dependent on the learned feature space, the corresponding computed vector b k + 1 is unsuit-able to be accepted as a new component. It is because that if a component is transformed from a pattern that has strong dependence on learned components, the computation error would arise and affect the orthogonality among components [11]. In this case, if we accept b k + 1 as a new component, the numerical orthogonality between { b 1 , b 2 , ..., b k } and b can X  X  be guaranteed. Therefore, it is necessary to consider the independence between learned components and input patterns.

But simply considering the independence from theoretical aspect is not enough. Take the famous Hilbert Matrix for example: the n  X  n Hilbert matrix H ( n ) is a positive definite symmetric matrix. Thereby, theoretically speaking, the di-mension of the space spanned by its column vectors should be n . However, in Matlab 6 . 1, we validate that there are only about 18 numerically independent base vectors in the space spanned by H (100) X  X  100 column vectors. The exam-ple dictates that it may cause large bias if we merely take theoretical independence into consideration. Here, based on the linear dependence theorem [11], we propose a technique to measure the numerical independence of between input patterns and learned components.

The linear dependence theorem indicates that the inde-pendence between a space S and a vector x can be measured by the projection distance of x upon the basis space spanned by the basis vectors of S .

Now we apply this theorem to address our problem. We presume that the already learned basis vectors (components) are b 1 , b 2 , ..., and b k . The basis vector b k + 1 is learned from the input pattern  X  k + 1 . We attempt to measure the linear dependence between the input pattern  X  k + 1 and the space span { b 1 , b 2 , ..., b k } according to their distance. In addition, the distance between  X  k + 1 and span { b 1 , b can be rewritten as the following expression. we define matrix B k = ( b 1 , b 2 , ..., b k )  X  R d  X  k written in a matrix representation.
The column vectors in B k are orthogonal. Thereby, we can construct an orthogonal matrix Q in the following form:
Since orthogonalization transformation never changes the 2-norm of a vector, (2) is equal to the follow expression:
We let  X  = B k &gt;  X  k + 1 . Thereby, the value of expression (2) is equal to k Q 1 &gt;  X  k + 1 k 2 . The theorem below will denote is obtained in the process of learning components. (See the detailed algorithm in section 2.3.)
Theorem 1. The 2-norm of Q 1 &gt;  X  k + 1 shown in (4) is equal to r k +1 ,k +1 achieved in the process of learning compo-
Proof. The proof is quite straightforward and we omit it here for brevity.

Based on the theorem portrayed above, we can measure the numerical independence between the already learned ba-sis spanned by components span { b 1 , b 2 , ..., b k } and input
To obtain orthogonal components, we need to select or-thogonal components into basis M according to the measure-ment discussed in section 2.1. Those computed base vectors should be ruled out of basis M if they are learned from input data that are dependent on M . Otherwise, the numerical orthogonality of learned components can X  X  be guaranteed. Therefore, we adopt the value of r k +1 ,k +1 as a criterion to select training data. If r k +1 ,k +1 is large, i.e., the indepen-dence between b k + 1 and learned components is strong, we accept b k + 1 into basis M . Otherwise, we reject computed component b k + 1 . Through the designed training data selec-tion scheme, only those data that are independent of learned components can be selected to learn components.

The number of components is a crucial factor worth con-sidering in feature extraction task. If it is too small, the learned components will have a large distinction from the space spanned by the original data. On the contrary, a too large target dimension will render it difficult in learning com-ponents and waste storage capacity. Many methods need a user to predetermine the target dimension for feature space. Although for PCA and some PCA based methods, the tar-get dimension can be determined based on the accumulation ratio whose lower bound comes from the tolerable approxi-mation error, this lower bound has to be predetermined by users as parameters. Therefore, traditional methods may learn an improper dimension for feature space.

In IOBA, we address the issue by a self-adaptive thresh-old policy: if r k +1 ,k +1  X  T , we accept potential component b k + 1 into basis M . In this case, the number of learned components is automatically determined based on the inde-pendence. It means that all the new accepted components are transformed from patterns independent of learned com-ponents; we learn the proper number of components. And the target dimension is completely data-dependent. d + 1 vectors of d dimension must be linearly dependent. For that reason, the number of vectors in the basis is no more than d . Thereby, if the compression ratio, i.e., the dimension of basis dim ( B ) dividing the dimension of input pattern d , is small currently, it is imperative to accept potential basis vectors and vice versa.

As described above, the proposed threshold T should sat-isfy the following two constraints. i) The orthogonality of components should be ensured. ii) The difficulty of ac-cepting components increases with the increase of the basis dimension dim ( B ) for feature space. To satisfy the con-straints, we propose a self-adaptive threshold policy: if and component.

Because dim ( B ) d is confined to the interval of [0 , 1], we
Based on the threshold policy, IOBA can select those training data that has little dependence on learned com-ponents.
According to the analysis presented above, we give the detailed IOBA in Algorithm 1 as shown below.
 Algorithm 1 Incremental Orthogonal Basis Analysis 1: Initialize basis B =  X  and its dimension k = dim ( B ) = 2: For each input new pattern x j ( j &gt; k ): 8: Accept b k + 1 as a component and add it into basis B . 9: Update basis dimension k  X  k + 1. 10: end if 11: Goto step 2 to continue the learning process.
The time complexity of IOBA is O ( Ndk ), where N is the training set size, d is the dimension of original patterns, and k is the number of components learned by the algorithm. For comparison with IOBA, we analyze the time complex-ity of PCA, BPCA (Bayesian PCA) [1], CCIPCA [2], and IICA [5] (an incremental version of ICA) as following: the time complexity of PCA is O ( d 3 + Nd 2 + kd 2 ); BPCA is O ( Nd 3 k ); CCIPCA is O ( Ndk ); IICA is O ( Ndkh ). Here, h is the iteration time in the stage of IICA.

The whole procedure of IOBA needs not solve the eigen-value and eigenvector problems. Therefore, the proposed method can enjoy a low computing load and high stability. In the following section, we use some datasets from the UCI Machine Learning Repository [12] to test the proposed algorithm. Such datasets include Hill, Ionosphere, Musk, Optical Digits, Sonar, and Waveform.
In this section, we try to test the quality of the compo-nents generated using the proposed method. Here, we take this policy: the more patterns in the training set that can be represented linearly by the components, the better the components are. In other words, the degree to which the original data are represented by components reflects their component quality.

If a pattern  X  can be represented completely using a group of components b 1 , b 2 , ..., b k , the vector  X  is in the space span { b 1 , b 2 , ..., b k } that is spanned by the components. It means that the distance between vector  X  and the space ponent quality by defining a cost function E ( B ) as follows : E ( B ) = 1 N of the function E denotes the average distance from every pattern in a dataset to the space spanned by components.
We perform IOBA under the each database sequentially to learn the components. Then we compute the cost func-tion of learned components and present the result in Ta-ble 1. In addition, PCA, BPCA, CCIPCA, and IICA are executed to make a comparison with the proposed IOBA. IOBA and BPCA automatically learn the number of com-ponents, as listed in Table 3. But for PCA, CCIPCA, and IICA, users must predetermine the target dimension as a parameter. The target dimension can affect the linear rep-resentation quality to some extent. Therefore, to guarantee fair comparison, for PCA, CCIPCA, and IICA, we set them up with the same value in IOBA.

Table 1 shows that the cost function of components learned by the IOBA is averagely smaller than that of components learned by any other algorithm. The components learned by IOBA are orthogonal among each other. Therefore, the components obtained by IOBA contain the least redundant information and soundly reflect the structure of original data space under the same condition. Hence it is no wonder that IOBA can achieve the best component quality.
Next, we apply the recognition rate as a benchmark to compare different feature extraction methods. For each dataset, a group of components for transformation matrix is learned by Algorithm 1. Then we adopt the learned components to transform original data into low dimension feature space data. In the transformed feature space, 1-nearest neigh-bor (1-NN) rule is used to classify the test patterns. PCA, BPCA, CCIPCA, and IICA are also used as comparison.
The result is listed in Table 2. The best recognition rate is highlighted in bold typeface. To demonstrate that IOBA can achieve a better recognition performance, we also list the recognition rate that is obtained merely by NNC in Table 2. According to the analysis presented in section 3.1, for IOBA and BPCA, the target dimension is generated automatically. However, in PCA, CCIPCA, and IICA, users need to prede-termine it as a parameter. Here, we tune these parameters for PCA, CCIPCA, and IICA by ten times 10-fold cross-validation of training set to achieve an optimal recognition performance for the respective parameter setting. Further-more, we show the target dimension and the compression ratio, i.e., the target dimension divided by the dimension of input vectors for PCA, BPCA, CCIPCA, and IICA in Table 3.
 Table 1: E ( B ) : The best and near best performance are highlighted in bold typeface.
 Optical Digits 0.233 0.151 0.182 0.175 0.625 Table 2: Recognition rate of the experiment: the best recognition rate is highlighted in bold typeface. Ionosphere 88.0% 87.1% 87.4% 86.6% 87.9% 86.1% Optical Digits 97.6% 98.1% 97.9% 97.9% 98.1% 98.0% Table 3: Target Dimension ( Dim ) and Compression Ratio ( CR ) Ionosphere 22/ 0.65 30/0.88 26/0.76 28/0.82 29/0.85 Waveform 10/ 0.48 14/0.67 15/0.71 16/0.76 12/0.57 Average CR 0.40 0.56 0.52 0.65 0.61
From Table 2, we know that for Ionosphere, Musk, Sonar and Waveform, IOBA has the best or near the best recog-nition performance. The average recognition rate of IOBA is higher than that for any other classifier for all databases. For the Hill and Optical Digits datasets, IOBA works a little worse than some methods. But Table 3 shows that, the com-pression ratio in IOBA under the two database is 0 . 09 and 0 . 41, which is much smaller than that in other methods. That means, from the aspect of compression ratio, IOBA is better than other algorithms under the two databases. Therefore, we can conclude that IOBA could reach better recognition rate and compression ratio.
As described in this paper, we propose an incremental al-gorithm for feature extraction tasks. The proposed method learns orthogonal components, not only theoretically but also numerically. Based an self-adaptive threshold criterion, the target dimension can be generated automatically and adjusted adaptively during learning. Users no longer need to predetermine the target dimension. The incremental iter-ation method without solving eigenvalues and eigenvectors guarantees a low computing load and robustness.

In the experiments described herein, we compare IOBA with several typical feature extraction methods. Results show that the quality of components generated by IOBA is the best among these algorithms. With respect to the recognition rate and compression ratio, IOBA performance is good. Although another method might sometimes achieve slightly better recognition rate, IOBA fulfills the best com-promise among component quality, classification power, and storage efficiency. When considering time complexity, the performance of IOBA is good. [1] C. Bishop. Bayesian PCA. In the Advances in Neural [2] J. Weng, Y. Zhang, and W. S. Hwang. Candid [3] F. Tang, R. Crabb, and H. Tao. Representing images [4] J. Chien and B. C. Chen. A new independent [5] I. Dagher and R. Nachar. Face recognition using [6] X. He, D. Cai, and J. Han. Learning a maximum [7] C. J.Veenman and D. M. J. Tax. Less: A model-based [8] D. Cai and X. He. Orthogonal locality preserving [9] H. Liu and L. Yu. Toward integrating feature selection [10] G. H. Golub and C. F. V. Loan. Matrix Computations, [11] X. He. Numerical dependence theorem and its [12] C. L. Blake and C. J. Merz. Uci repository of machine
