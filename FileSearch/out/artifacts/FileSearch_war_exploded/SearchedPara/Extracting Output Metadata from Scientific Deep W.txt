 Department of Computer Science and Engineering
Increasingly, many data sources appear as online databases, hidden behind query forms, thus forming the deep web . The popularity of this new data dissemination medium is creating new problems for data modeling and data integration. Lately, there has also been a lot of work on integrating and mining useful information from the deep web [1], [2], [3], [4], [5], [6]. For a data integration system that integrates heterogeneous deep web data sources, discov-ering the metadata for each data source is the prerequisite for many other integration tasks. With rapidly increasing number of data sources, the metadata of deep web data sources must be obtained automatically, and not manually. For example, currently there are more than 1000 online databases in the biological domain and the number is still increasing rapidly every year [7].

The metadata of a deep web data source that we consider is the input and output schemas of the data source. In the literature, there has been a lot of research on understanding input forms and extracting input schemas of deep web data sources [8], [9], [10]. For example, sub-figure (a) in Figure 1 shows the input query form of dbSNP 1 , which is a biological data source providing data on Single Nucleotide Polymorphisms (SNPs). We can know that Reference cluster ID(rs#) is the attribute in the input schema of dbSNP .

However, a difficult and as yet unsolved problem is of extracting the output schema of scientific deep web data sources, where the results of a query do not always contain the complete set of output schema attributes. To further explain this problem, consider the following exam-ple. After a user submits an input query form, an output page corresponding to the input query is returned as the answer. The sub-figure (c) in Figure 1 shows a partial output page from dbSNP after specifying Reference cluster ID(rs#) to be rs7412 (the ID of a SNP).
 The labels that are in bold font and underlined are the output schema attributes. The values shown in italic font are the corresponding data values. If a data source returns values for all of its output schema attributes every time, the output schema can be extracted from the output page(s), using any of the many existing techniques [11], [12], [13], [14]. Unfortunately, many deep web data sources, especially in the scientific domain, only return a partial set of output schema attributes in response to a query, i.e., the ones that have non-NULL values for the particular input.

To further elaborate on this, we consider the following two examples. Motivating Example 1: Let us reconsider the dbSNP data source, and consider two input examples, rs12903896 and rs7412 . The partial output page of these two input examples are shown in sub-figures (b) and (c) in Figure 1. For the input rs7412 , the output contains population diver-sity related data. However, when the same source is queried using rs12903896 as the input, the output page does not contain any attributes related with population diversity. This is because this SNP_ID does not have any population data associated with it.
 Motivating Example 2: To further understand the complex-ity of this problem, we closely examined 20 data sources we had been working with. For each source, we randomly create 50 input cases, and analyze the corresponding 50 output pages. We count the number of distinct output schema attributes . Detailed results from 4 of these data sources is shown in Table I. We can observe that none of the output pages covers the complete output schema, and on the average, each output page only covers about 60% to 70% of the complete output schema. Furthermore, the high standard deviation values also show the coverage of output schema varies considerably between different output pages. A. Related Problems and Existing Work
The problem we are considering may appear similar to many problems that have been extensive studied. Among these, the existing output schema extraction methods [11], [12], [13], [14] focus on label extraction from HTML web pages. They have mostly been driven by data sources in the E-commerce domain, where most data sources return the complete set of output schema attributes for every input. In considering the data sources that return only a subset of output schema attributes, we need to intelligently obtain a list of output pages, so that these output pages could cover the complete output schema. In the process, the existing approaches cited above can be adopted to extract the output schema attribute labels .

Extracting data from deep web data sources is another problem somewhat related with ours [15], [16]. However, our problem is distinct from this problem from the following aspects. First, we focus on extracting metadata , not data . Second, the approaches proposed in [15], [16] assume that every output page contains the complete set of output schema attributes.

It may be argued that an intuitive way to find the output schema is to use the documentation from data sources. However, this approach is not applicable in many cases, and for other cases, there are other difficulties. For example, among the 20 biological data sources we focused on, only 10 had accessible documentations, and furthermore, the documentation that is available is hard to use. This is because most documentation is designed for human readers, and not for an automated system. For example, some data sources ( dbSNP , Alfred 2 ) use non-text documents (such as an ER diagram) to state their schemas, while some other sources ( Gene 3 , KEGG 4 , Uniprot 5 , GeneCards 6 ) only provide a high-level schema information, without giving a complete list of attributes.
 B. Our Approach
In this paper, we propose two approaches for auto-matically extracting the set of output schema attributes from scientific deep web data sources. The first approach, which is called the Sampling Model Approach , is based on a distribution model of output schema attributes which we discovered from a preliminary study on scientific data sources. We argue that a modest sized sample of output pages could discover output schema attributes with relatively high recall. Mixture Model Approach is our second proposed approach. It is motivated by the observation that many scientific data sources share output schema attributes [17]. For example, SNP Frequency related schema attributes can be obtained through 3 data sources, which are dbSNP , SNP500Cancer , and SeattleSNP . Based on this char-acteristic of scientific deep web data sources, we consider that an output schema attribute is generated from a mixture model, which is composed of multiple related data source models.

The rest of the paper is organized as follows. In Section II, we formulate our problem and give an overview of our solutions. The sampling model approach and the mixture model approach are described in Sections III and IV, respectively. We report an evaluation study in Section V. We review related work in Section VI and conclude in Section VII.

We have a set of deep web data sources D = {
D 1 ,D 2 ,...,D n } . Each data source D i has an input an output schema OS D i = { o D i 1 ,o D i 2 ,...,o D i m which we want to determine. An input instance for the data source D i is a tuple with valid values for each input schema attributes. A naive way of obtaining the complete output schema of D i is to use all possible input instances. However, this method is clearly infeasible because, first, it is very hard to obtain all possible input instances for a data source, and second, querying the data source using all input instances will be extremely time consuming. As a result, we need a more efficient approach.
 Problem Formulation: For each data source D i , we want to select s i number of input instances, where s i is the sample size. Each input instance II D i j will return an output page containing a partial output schema, POS D i j . Then we will have s i partial output schemas for D i , which can be denoted mately, for D i , we want the union of all the partial output schemas to be equal to the complete output schema of D i ,
Given the problem formulated above, for a data source, finding a list of output pages containing the complete output schema is the focus of our paper. When these output pages are obtained, we adopt existing methods to extract schema attribute labels from the page.
 Sampling Model Approach: Since we take input samples to obtain a list of output pages instead of an exhaustive approach, we need to answer three questions: 1) what is the appropriate value of the sample size s i ?, 2) what is the possibility of missing some output schema attributes using a sample of this size?, and 3) where do we draw the samples from? These three questions are answered in Section III. In the sampling model approach, we first establish a model to show that for deep web data sources, most output schema attributes can be discovered using a modest sized sample. Then we describe an input entity sample pool from which we draw our input samples, and propose a rejection sampling method to draw a simple random sample from the sample pool. We also estimate the sampling error and construct a sample size estimator.
 Mixture Model Approach: In some cases, it is hard to obtain a sampling pool of the input instances for a data sources. In such cases, the sampling model approach would not be applicable. Because the input interfaces of deep web data sources are designed for human users, there are often input examples on the input interface that can be extracted automatically. One such example is shown as a highlighted rectangle in sub-figure (a) Figure 1. However, the number of input samples obtained in this way would be too small to provide an effective approach. We can exploit another observation, which is that there is likely some redundancy across data sources. Thus, output schema attributes could be shared among different data sources. Since the output page of the deep web data sources are dynamically generated from its back-end database, we model a data source as a probabilistic data source model that generates output schema attributes with certain probabilities. We also model the borrowability between similar data sources. Since a schema attribute could be shared by multiple data sources, we consider the probability of a schema attribute generated by a data source D as being determined by a mixture model composed of the probabilistic data source models of similar data sources. Section IV describes this method in more details.

In this section, we introduce the sampling model ap-proach. First, we empirically build a distribution model for output schema attributes of deep web data sources, to understand the underlying assumptions that can be valid. Then, we introduce the rejection sampling method. Finally, we construct a sample size estimator to bound the sampling error within a confidence interval.
 A. Distribution Model
Given two randomly selected output pages from a deep web data source, we often observe that some schema at-tributes only appear in one output page, but most schema attributes appear on both. This observation points to an important fact about the relationship between output schema attributes and output pages. To further study this relation-ship, we downloaded 100 randomly selected output pages over 20 data sources we are targeting. Then, for each source, we draw a random sequence of output pages, and count the number of output schema attributes and the number of distinct output schema attributes, cumulatively. In Figure 2 we show the results for the data sources dbSNP, Gene and JSNP. The results for other data sources are similar. Here, the x-axis is the total number of output schema attributes seen. The y-axis is the number of distinct output schema attributes. We observe that with the increase of the number of output schema attributes, which also corresponds to the increase in the number of output pages, the number of newly discovered distinct output schema attributes reduces. In other words, as more output pages are gathered, there will be diminishing returns in terms of discovery of distinct output schema attributes.

The above observation makes a sampling approach fea-sible. It shows that a large percentage of output schema attributes can be discovered using a relatively small number of output pages. The pattern shown in Figure 2 is very similar to the Heaps law [18] in linguistics. Heaps law is an empirical law which describes the portion of a vocabulary that is represented by an instance document. It states that as more instance text is gathered, there will be diminishing returns in terms of the discovery of the full vocabulary. Our data fits the Heaps law very well. If we use V to represent the number of discovered distinct output schema attributes, and m to show the number of output schema attributes, we will have V = km  X  . We observe from our data (Figure 2) that, in the context of the deep web data we are working with, the parameter k and  X  are typically 5  X  k  X  6 and 0 . 3  X   X   X  0 . 4 .

Another necessity for a sampling based technique is the availability of a sampling pool . Our work assumes that such a pool is available. In practice, many data sources use entity names as input schema attribute and a sample pool may arise from a query log of entity names associated with a data source, or could be generated using a domain-specific ontology. A problem with a sampling pool is that it may not have a complete list of valid input entities, which in reality is hard, if not impossible, to obtain. The under-coverage of the sampling pool may bring a non-sampling error. But we argue that the way our sampling pool is constructed decreases the effect of the non-sampling error. We first define a rare output schema attribute to be an attribute which seldom appear in output pages, formally, it is output in response to only a small number of potential inputs, these inputs are infrequently used in practice. The performance of a data integration system mainly depends on the non-rare output schema attributes. Since our sampling pool is constructed using a large number of frequently used inputs, we claim that it decreases the probability of missing non-rare output schema attributes, and thus the effect non-sampling error. B. Sampling Algorithm
Simple random sample (SRS) is a widely used sample form which reduces bias in the sampling procedure. SRS requires that each unit in the sampling pool should have an equal probability of being sampled. Since our sample pool is composed of query logs, the more frequently used input instances are likely to appear more often. As a result, if we draw random samples from our sampling pool, the probability of a unit being sampled, which is defined as the selection probability , is proportional to its frequency. Therefore, an sample formed from a random walk on our sample pool could be skewed and the skew is resulted from the variance among the selection probabilities .
To counter the skew, we use rejection sampling . Rejec-tion sampling probabilistically accepts or rejects the input instances that have been selected by the random walk on our sampling pool. Let X  X  consider the following example. We assume that the size of our sampling pool is | Q | , i.e. the total number of input instances in the sampling pool is | Consider an input instance x appears k times in the sampling pool and x is selected from a random walk on our sampling pool. Ideally, since we want a simple random sample from the sampling pool, the probability of x being sampled should be 1 | Q | . But, actually the selection probability of x will be | Q | . To compensate for the skew caused by the selection probability , our rejection sampling algorithm will toss a coin with a head probability of 1 k , which is equal to the selection probability , and accepts x if the head faces up. This decreases the possibility of x being selected, and ensures the probability of x being sampled is k | Q |  X  1 k = 1 | Q | detailed description of rejection sampling can be seen from the recent work by Bar-Yossef and Gurevich [19]. The rejection sampling method we use is illustrated as Algorithm III.1.
 Sampling Error: Using rejection sampling, we can draw a SRS from our sampling pool to estimate the number of output schema attribute V . Suppose the variance of the number of output schema attributes in the output pages of the input samples is s (the values in the last column in Table I). The sampling error of our method is given by the lemma below.
 Lemma III.1. If the size of our sampling pool is | Q | ,we draw a sample of size n , and the sampling error will be
The proof of Lemma III.1 is straightforward, using the properties of SRS [20]. The practical impact of the sampling and non-sampling errors will be shown in Section V-B. C. Sample Size Estimation
Given the sampling algorithm as in Section III-B and the sampling error as in Lemma III.1, we need to determine the size of sample we need. Let V be the total number of distinct output schema attributes in a data source, and m be the total number of output schema attributes found in all the sampled output pages. From our distribution model, as we had introduced in Section III-A, we know that V = km  X  . In order to estimate the sample size, we need to first compute the sampling error of V as shown in Lemma III.1. To facilitate the computation, we make the following transformations. Let  X  V = log ( V ) ,  X  k = log  X  m = log ( m ) , and  X   X  =  X  . Now our distribution model can be expressed as  X  V =  X  k +  X   X   X   X  m .
 Lemma III.2. The sampling error  X  V is SE (  X  V )= s 2  X  m The proof of Lemma III.2 is omitted due to lack of space.
To estimate the required sample size, for a variable Y needs to be estimated, we need to define its margin of error e and (1  X   X  )% confidence interval CI .
 Definition 1. We have a variable Y needs to be estimated. The true value of Y is y and the estimated value is  X  y . The margin of error e is defined as P ( | y  X   X  y | X  e )=1  X   X  .  X  is the confidence of the estimation.
 Definition 2. If the estimate of Y ,  X  y , falls into the confidence interval of true value of Y , we have | y  X   X  y z
Combining Definition 1 and Definition 2, for a variable Y , we can relate the margin of error e of Y and the confidence interval CI of Y as follows:
In our scenario, the variable that needs to be estimated is  X 
V . If we combine the formula (1) with Lemma III.2, we can obtain Lemma III.3. Let e  X  V be the error rate of  X  V , and s be the standard derivation  X  m . The sample size needed in Algorithm III.1 is n = the equation (2).

In statistical sampling, the confidence level  X  is usually set to be 0 . 05 [20], which makes z  X  2 to be 1 . 96 . In our system, s 2  X  m is obtained through our experimental study that we had described earlier. As mentioned in Section III-A, for the deep web data sources we are focusing on, the value of  X   X  is usually set to be between 0 . 3 and 0 . 4 . In Lemma III.3, e is only user specified parameter. In our system, we allow a margin of error of 5% , which means that we allow the number of estimated output schema attributes to be 5% lower than the real number of output schema attributes, denoted of error in Definition 1, the margin of error is the deviation between the estimated value and the true value. As a result, D. Obtaining output schema attributes
For a data source D , based on the sample size estimator we described above, we select a list of inputs with our sampling algorithm. These inputs are used to query the data source D and obtain a list of output pages. Now, for each output page, we need to extract output schema attributes using an extraction algorithm. There are many existing methods [14], [9] can be used for extracting labels from HTML web pages, and in our implementation, we designed label classifier to extract schema attributes based on the ideas in [9]. For each output page of D , our classifier returns a schema attribute candidate set SC , where SC = { ( sc 1 ,prob 1 ) , ( sc 2 ,prob 2 ) ,..., ( sc n ,prob sc i is a schema attribute candidate and prob i indicates the probability that sc i is a true schema attribute in the output page. For each data source D , we can obtain a list of SC sets, each of which is obtained from a sampled output page from D . Next, we merge all its SC sets to obtain a final set of schema attribute candidates. During this merging, if a sc i has probability values from different output pages, we keep the the larger one for sc i for the final SC set. After merging, we delete all the schema attribute candidates with probability values smaller than a threshold. We notice that the classifier may bring new errors into our entire process. It is possible that a sampled output page contains a true output schema attribute, but the classifier misclassifies it. The effect of the classifier on the accuracy of our overall approach is showninSectionV-B.

This section describes a different approach for extracting the output metadata of a deep web data source. The main idea in this approach is as follows. Let us say that for a data source D , an schema attribute a does not appear in any of the output pages obtained from the sampled inputs we use. However, let us say that the schema attribute a appears in the output of several other data sources, such that each of these data sources have many overlapping schema attributes with the data source D . In this case, there is a good chance that the schema attribute a is in the output schema of the data source D , but has been missed because of the input sample we have used. In this case, we should try validating if a does appear in output of D .

The rest of this section describes this approach in more details. We first introduce the model of data sources we use in this approach and then describe our method to measure the borrowability of a data source with respect to another. Finally, we introduce the validation algorithm we use. A. Modeling of Data Sources Probabilistic Data Source Model: The output page of a deep web data source is dynamically generated from its back-end database, based on the input. If we consider an output page as a document, then the data source can be considered as a collection of such documents, comprising all possible output pages that can be generated. As each such document has a certain number of schema attributes, we can consider a data source D i as a probability model  X  that generates schema attribute with certain probabilities. The challenge for us is how to estimate this probability. For a data source D , if we have only one sample output page and the schema attribute candidates are extracted using the classifier introduced in Section III-D, an intuitive way to model the probability is to use the probability score from the schema candidate set .

Formally, if our classifier predicts a schema attribute candidate sc ij to be a true schema attribute of a data source D i with the probability prob ( sc ij ,D i ) , then the probabilistic data source model  X  says that the data source D i generates schema attribute sc ij with the probability prob ( sc ij ,D which is denoted as p ( sc ij |  X  i )= prob ( sc ij ,D i ) Mixture Model: An important observation of scientific deep web data sources is that many output schema attributes are common across distinct data sources. This suggests that an output schema attribute could be generated from a mixture model that is comprised of a set of probabilistic data source models. If a schema attribute sc ij is covered by the data source D i with a probability prob ( sc ij |  X  i ) , and D of similar data sources D i ,D 2 ,...,D i  X  1 ,D i +1 ,...,D we consider the probabilistic data source model of D i as the main model and other similar data sources X  models as background models . Along this line, the probability that a schema attribute candidate sc ij is a true schema attribute of data source D i can be written as follows: p ( sc ij | D i )= prob ( sc ij |  X  i )+
The first component of the above expression represents the contribution from the main model and the second component represents the contribution from the background models .  X  is the weight or borrowability given to the background model  X  u with respect to main model  X  i . Computation of this factor is explained next.
 B. Computing Data Source Borrowability
In the discussion above,  X  ui indicates the weight we put on the contribution from another data source D u with respect to D i , which is also stated as the borrowability score of D with respect to D i .

The borrowability of D i with respect to D j is computed using their SC sets, SC i and SC j which is the schema attribute candidate set returned by the label classifier as introduced in Section III-D. We define the intersection of the two SC sets from two data sources as SC i  X  SC j = { ( sc k ,prob k ) | sc k  X  SC i ,sc k  X  SC j , prob k = prob SC i ( sc k )  X  prob SC j ( sc k ) }
In other words, the set SC i  X  SC j contains the schema attribute candidates in both D i and D j and the probability is computed as the candidates appear in both sources. Then the borrowability of D i with respect to D j is defined as From this formula, we are more likely to borrow a schema attribute of D i to D j if more attributes in D i overlap with attributes in D j .

Note that borrowability is not symmetric, i.e., the bor-rowability of D i with respect to D j may not be the same as the borrowability of D j with respect to D i . For example, consider the situation where D i covers data about biological concepts SNP, Gene and Chromosome, and D j only focuses on SNP data. If we have a schema attribute about SNP, we should believe that the possibility of borrowing the attribute from D j to D i is higher than that of borrowing it from D to D j .

Another issue in borrowing schema labels from other data sources is that we need to match the labels from different data sources together to compute the set SC i  X  SC j . In our current system, we use some simple word level matching and instance based matching methods to achieve this goal. For generalizing our work in the future, we will incorporate the more powerful methods proposed in the literature [21]. Using the mixture data source model, for each data source D , a schema attribute candidate sc has a new probability score, which is obtained by gathering contribution from multiple similar data sources, as the probability of sc being a true attribute of D . If this probability is greater than a threshold, we predict it to be a true schema attribute of D . C. Validation Of Borrowed Attributes
As we borrow output schema attributes from other data sources, we need to make sure that the borrowed attributes are not false-positives . Without having the access to the back-end databases, we use search engines and data source documentation to perform such validation. A reasonable assumption is that if a borrowed schema attribute is a true schema attribute of D , some documentation or HTML pages located within the domain of D may contain this schema attribute. In our validation algorithm, for each borrowed schema attribute ba with respect to the data source D , which has a corresponding web address URL , we issue a query (
Input, ba, U RL ) on Google, where Input includes the input schema attributes of D . If any documents or HTML pages are returned by the search engine, we believe that ba is a true schema attribute of D .
This section reports the results from the experiments we conducted to evaluate our strategies. In our experiments, besides the 6 data sources (AlfredGene, AlfredSNP, dbSNP, Gene, SNP500 and KEGG) we have introduced in previous sections, we also considered 4 other deep web data sources in the biological domain, which are HGNC 7 ,JSNP 8 ,MGI-Gene 9 , and MGISNP 10 Initially, we report the performance of the label classifier and the validation heuristic we have used in our work. Quantifying the performance of these is important, as they impact the accuracy of our two proposed methods. Next, we compare the performance of two ap-proaches proposed in this paper and point out the limitations of each approach. Finally, we show that a hybrid approach that combines the sampling model and the mixture model approach yields the best results.
 A. Label Classifier and Validation Algorithm Evaluation
We use the precision (P) and recall (R) metrics to evaluate the performance of the classifier and the validation algo-rithm. Further, we report the F-measure, that incorporates both precision and recall with the following expression F =2 PR/ ( P + R ) . The results are shown in Table II.
The performance of the classifier is shown in the left side of the table. For most data sources, our classifier has a F-measure about or above 0 . 8 . This corresponds well to the results from the original paper from which our classification method was adapted [9]. The classifier does not perform well on the data source Gene. This is because unlike other data sources, which mainly use table-like format to represent the results, Gene uses text-like formats. The classifier misclassifies some labels in this case.
The validation algorithm works effectively on all data sources. All data sources have F-measure greater than 0 . and only one data source has the F-measure lower than 0 . 85 . We also observe that with the exception of dbSNP, the precision is 1 for all data sources. This indicates that our validation algorithm can recognize the true schema attributes with a high accuracy. In terms of the recall, we can observe that 5 data sources have high recall (greater than 0 . 85 whereas the other 4 have a moderate value (above 0 . 75 ). This is because these 4 data sources use some common English words, such as Method, Center and Resources, as schema attributes. These common words always appear in the documentation pages of the data sources, as a result, some non-attributes are accepted as false positives. B. Evaluation Of Our Approaches
We now evaluate the performance of the two proposed approaches. As mentioned in Section III-D, in the sampling model approach, the output schema attributes are extracted using a label classifier. As a result, the performance of the classifier may negatively impact the performance of the sampling model approach. In order to separate the perfor-mance of the classifier with the sampling model approach, we assumed a perfect classifier just for the purpose of our experiments. This perfect classifier always has a recall and precision of 1.

Now, based on the two proposed approaches, we consider four different cases, which are as follows.
 SamplePC: It refers to the sampling model approach with the perfect classifier. We use the sampling model approach to obtain the sampled output pages from data sources, and then we use the perfect classifier to extract schema attributes. SampleRC: It refers to sampling model approach with the real classifier. We want to show the performance of the sampling model approach in a real data integration system and point out the limitations of the sampling model approach.
 Mixture: It refers to the mixture model approach with the real classifier. In our experiments, we show the advantage of this algorithm and also point out the limitations. Sample+Mix: It is a hybrid algorithm combining SampleRC and Mixture. In this algorithm, we first use the sampling model approach to select a list of sample output pages for each of the data sources. Next, we use the idea of the mixture model approach to share attributes, but with an important modification. In this hybrid algorithm, we not only share schema attributes across data sources , more importantly, we share schema attributes across different output pages from the same data source. Recall that the original mixture model approach assumes that for a data source D , the number of sample output pages is quite limited (usually 1). As a result, schema attribute sharing can only be performed among D and its similar data sources. But in the hybrid algorithm, since we have multiple sample outputs from a data source D obtained using input samples, we can consider each output page from D as a pseudo-probabilistic data source model . We extend the mixture model to not only include the data sources similar to D , but also all sampled output pages of D .
 Algorithm Performance and Analysis: We report the detailed experiment results of the above four algorithms on the data sources dbSNP, KEGG, JSNP, and Gene. The precision and recall of the discovered output schema labels are used as the evaluation metrics. The results are shown in Figure 3. The experiment results of other 6 data sources we worked with are similar to the results shown here, and are not included because of space limitations. In the figures, the x-axis is the number of samples used, and the y-axis is the recall of the discovered schema attributes. The number near to each data point is the precision.

We have the following observations. 1). Using the SamplePC algorithm (line with a triangle pointing right), the recall increases rapidly with the increas-ing number of samples, but only till we have around 10 samples. Subsequently, as more samples are used, the rate of improvement of the recall slows down, and finally reaches about 95  X  97% . The pattern is about the same for all data sources. This pattern is consistent with the distribution model as we introduced in Section III-A. We can see that with a perfect classifier, we can obtain almost all output schema attributes with a modest sized sample. For dbSNP, only 3% of the attributes are missed due to sampling and non-sampling errors, and this error rate ( 3% ) is within the error rate specified in the sample size estimator (which is 5% ). We can also see from the sub-figures (b), (c) and (d) that the percentage of missed attribute of the SamplePC algorithm is always below or about 5% . This shows that the sampling model approach achieves the specified error rate. 2). Using SampleRC algorithm (line with a triangle pointing down), the recall has the same pattern as in the SamplePC algorithm. However, the highest recall achieved is much lower than the recall achieved from SamplePC. This shows that in a real system, the performance of the sampling model approach is degraded severely due to the performance of the classifier. Across the four data sources, the best results are obtained from dbSNP, where the highest recall is 90%. The results are the worst for Gene data source, because the classifier has poor performance, as we had shown earlier in Table II. This observation shows an important limitation of the sampling model approach, which is that the accuracy of this approach, to some extent, depends on the accuracy of the label classifier. 3). To achieve the same recall with the Mixture model approach (line with circle), we need significantly fewer samples than the sampling model approach. For example, with dbSNP, only 4 samples are needed instead of 13. For KEGG data source (sub-figure (b)), the Mixture algorithm achieves a recall of 75% using 4 samples, but the highest recall of SampleRC is only about 70% . The same pattern also appears for JSNP and Gene. Another observation is that after the recall reaches some level, it does not change much with the increase in number of samples. Furthermore, the precision now decreases. The reasons are the follows. At the beginning of the Mixture algorithm, it successfully finds common schema attributes and shares them among different data sources. When all common schema attributes have been discovered, the algorithm cannot find more schema attributes. Since it is possible that the Mixture algorithm borrows false positives from other data sources, the precision goes down. This shows a limit to what can be achieved by the Mixture approach, which depends on the number of common schema attributes shared among data sources. 4). Using the Sample+Mix algorithm (line with square), for dbSNP (figure(a)), we can achieve a recall as high as the SamplePC algorithm, while using much less number of samples (10 v.s. 20). In the sub-figure (b), for KEGG, although the recall of Sample+Mix does not achieve the level achieved through SamplePC, the hybrid algorithm outperforms the SampleRC and Mixture algorithm. In the sub-figure (c), for JSNP, the recall of the hybrid algorithm is even higher than the SamplePC algorithm. This shows that the hybrid algorithm overcomes the limitations of the sampling model approach and the mixture model approach.
The reasons that the hybrid algorithm outperforms the sampling and the mixture model approach are as follows. First, in the hybrid algorithm, we use the sampling approach to select more samples than in the mixture model approach, so the discovery of output schema attributes does not solely depends on the shared schema attributes across data sources. Second, hybrid algorithm can correct the misclassified non-rare attribute. For example, in a sample output page, a true schema attribute sc is misclassified (has low probability score). If sc appears multiple times in other output pages from the same data source (i.e. sc is non-rare), since we also combine the contribution of different output pages in the hybrid algorithm, the gathered weighted probability for sc would be higher than the cut-off threshold.

We compare our work with existing work on related top-ics, which are label extraction, retrieving data from hidden databases, and schema discovery for structured data. Label Extraction and Template Mining: Deep web data source input interface label extraction [9], [8], [10] and out-put page template mining [11], [13], [12], [14] are two well studied areas. Several of the label extraction algorithms learn the layout of input interfaces using pre-specified rules [8]. Nguyen et al. [9] use a supervised learning strategy to train a two layer classifier to classify labels and non-labels. For learning the output page templates, most algorithms explore the repeated patterns appearing in the returned data records [11], [12] or identify the mapping between label and data value using heuristics [13]. The approach proposed by Gatterbauer [14] used model based method to learn web table layout. The above work focuses on extracting data or labels from a given web page, however, we are interested in wisely finding a list of output pages which could cover the complete output schema.
 Retrieving Data from Hidden Databases: The work pro-posed in [22], [23], [24] focuses on retrieving data from hidden text data sources. This is different from our work because there is no metadata in text data sources, and the result from text data source for a keyword query is composed of a list of matched documents. Lidden et al [15] and Dasgupta et al [16] proposed systems on extracting data from hidden relational databases. Lidden X  X  system can extract all the data from a hidden data source and Dasgupta X  X  system is capable of finding a random data sample from a hidden data source. Madhavan et al [25] proposed a scalable system for sampling web pages generated from deep web data sources to build index for Google. However, their systems are different from ours from the following aspects. First, they focus on extract the data , rather than metadata from hidden data sources. Second, they assume that given an input query, the returned output page has the complete metadata of the output schema. Third, Lidden X  X  approach mainly depends on a default query which has default value for each input schema attribute, while such default input query seldom appears for scientific data sources in our scenario.
 Schema Discovery for Structured Data: Schema discovery is the problem of constructing a relational schema that best describes the extracted data. Most of the work in this area utilizes heuristics, ontology, or instance-based matching methods [26], [27]. Our work is clearly distinct from the above work, since we do not construct schemas.
In this paper, we considered the problem of extracting output metadata from deep web data sources. We proposed two approaches, which are the sampling model approach and the mixture model approach. Our experiments show that using a perfect classifier, the sampling model approach achieves high recall while not exceeding the pre-specified error rate. But, with a real classifier, the performance of the sampling model approach degraded. The mixture model approach achieves relatively high recall using significantly fewer samples. However, there is a limit to what can be achieved by this approach, which depends on the number of common attributes shared among data sources. A hybrid approach that combines the above two approaches gave the best results.

