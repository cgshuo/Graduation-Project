 In this paper, we discuss a prot otype application deployed at the U.S. National Science Foundation fo r assisting program directors in identifying reviewers for proposals. The application helps program directors sort proposals into panels and find reviewers for proposals. To accomplish these tasks, it extracts information from the full text of proposals both to learn about the topics of proposals and the expertise of review ers. We discuss a variety of alternatives that were explored, the solution that was implemented, and the experience in using the solution within the workflow of NSF. H.2.8 [ Database Applications ]: Data Mining Algorithms, Human Factors, Emerging applications, technology, and issues Keyword extraction, similarity f unctions, clustering, information retrieval. proposals a year. Each proposal is reviewed by several external reviewers. It is critical to the mission of the agency and the integrity of the review process th at every proposal is reviewed by researchers with the expertise necessary to comment on the merit of the proposal. If there is not a good match between the topic of a proposal and the expertise of th e reviewers, then it is possible that a project is funded that will not advance the progress of science or that a very promising proposal is declined. We explore the problem of using data mini ng technology to assist program directors in the review of proposal s. Care is taken to match the technology to the existing workflow of the agency and to use technology to offer suggestions to program directors who ultimately make all decisions. Although this paper reports on reviewing proposals, we argue th at the lessons and technology would also apply to the reviewing of papers submitted to conferences and journals. typically 8-15 reviewers who meet to discuss a set of 20-40 related proposals, with each panelist typically reviewing 6-10 proposals. Most proposals are submitted in response to a particular solicitation (e.g.,  X  X nformation Technology Research X ) or to a specific program (e.g.,  X  X  uman Computer Interaction X ). Individual program directors, or for larger solicitations teams of program officers, perform a number of tasks to insure that proposals are reviewed equitably. These tasks include: proposals to create panels. conflict of interest with proposal s they are reviewing (e.g., they may not be from the sa me department as the proposal X  X  author), and a diverse group of panelists (both scientifically and demographically) is desirable to in sure that multiple perspectives are represented in the review process. Furthermore, due to scheduling or workload conflicts, not every invited reviewer accepts the invitation, requiring an iterative process of inviting a batch of reviewers and then inviting others to fill in gaps after the initial reviewers respond to the invitation. multidisciplinary, e.g., mining genome data. To determine if such a proposal is meritorious, it is im portant to consult some experts with backgrounds in data mining (to insure that the methods proposed are likely to work) and in the biological sciences (to insure that the problem addressed is an important open problem). If all reviewers have expertise in one area, it X  X  possible that an important problem would be addre ssed by a technique that isn X  X  very promising or that very promising technology would be applied to a problem that is already solved. mining technologies to NSF to help with the reviewing process. The most common technology proposed is automated text clustering to help organize proposals into panels. A variety of alternative approaches (e.g., hierar chical [1] or k-means [2]) have been explored. While these pres ent interesting views of proposal submission data, they do not produce results that fit easily into the workflow of NSF or that have gained universal acceptance by program officers who organize pane ls and assign reviewers. Automated clustering approaches suffer from a number of flaws that have reduced their utility in dividing proposals into panels. solutions is that they don X  X  l eave room for human input of preferences or constraints. There has been some research that addresses issues raised. For example, the simplest k-means clustering algorithm is incremental and would allow for the late additions to the existing clusters . However, the results of k-means are not stable so it results in different partitioning of the same data on different runs. Se veral investigators (e.g., [4] and [5]) have looked at adding constrai nts to the clustering process so that constraints are approximately the same size. However, none of these address the lack of alignment with the organization structure and workflow. In Section 3, we discuss an approach to  X  X luster checking X  in which algorithms related to text clustering and classification are used to s uggest improvements to clusters produced by people and new proposals are added to existing panels. assigning reviewers to proposals. One approach is to create a database of reviewers with keywor ds indicating user expertise. These databases are populated by users filling out a form with their expertise. Experience within NSF on prototypes of reviewer databases have found mixed results. Common problems include: when they are used within NSF, they are limited to suggesting a pool of candidates for a panel on a given topic. While Computer and Information Science a nd Engineering at NSF has experimented with a keyword system (e.g., in the 2001 ITR competition), it was not used in subsequent years. panelists to indicate preferences for reviewing proposals within a panel. In such systems, panelists indicate their preference for reviewing a proposal on a numeric s cale. Many conferences also use similar systems such as Cyberchair [9]. In Cyberchair, a constraint satisfaction algorithm assigns people proposals they are most interested in. These systems only address part of the reviewer assignment problem. They do not assist with identifying panelists but only assigning proposal s to panelists once they have been identified. There has b een an issue with compliance on these systems as well, i.e., not every panelist promptly enters preferences data and a single person not replying can delay the assignments for all others. In addition, it isn X  X  clear what the preference scores mean or how much thought goes into the assignments. While the intent is to judge how well qualified a reviewer is to review a proposal, we have observed many panelists having a strong preference for proposals by well known researchers and fewer having a preference for proposals by less established researchers. While NSF typically asks for preferences on 20-30 proposals, some conferences ask for preference data on 200-300 papers. The second author admits that when presented with 300 papers in Cyberchair , not as much time is spent reviewing the abstracts of the last batch of papers as the first to determine preferences. Finally, there is also a problem with multidisciplinary proposals if people from one discipline have a preference for a paper. It can occur that all computer scientists and no biologists give high prefer ence scores to a bioinformatics proposal, in which case a preference-based system will result in one aspect of the proposal not being reviewed. that addresses the problems w ith previous fully autonomous systems. The philosophy behind the system is to assist program directors and not replace their judgment with a black box system. One key design criteria is that Re vaide offers suggestions that may be accepted or declined individually. In this section, we introduce Revaide, its tasks and solution, and evaluate the utility of using Revaide. We introduce a measure to evaluate how well the expertise of a group of reviewers is suited for a proposal. Following the discussion of the key components of Revaide in this section, we will report on the experiences using the algorithm. converts the proposals to ASCII and represents proposals in the standard TF-IDF vector space [10] as term vectors in the space of all words in the document collecti on. The entire proposal is used including the references and resume of the investigator. One simple use of Revaide is to annotate spreadsheets of proposals with the 20 terms with highest TF -IDF weights. These keywords are often more informative to program directors than the title to determine what a proposal is a bout. While early versions of Revaide used stemming [11] to c onvert words to root forms, we found that stemming reduced the human comprehensibility of the resulting term vector representa tion. Experience showed that using stemming did not increase the quality of the suggestions made by Revaide. Therefore, we no longer use stemming. of the resulting term representation. We augmented the stoplist of items that should not be used as keywords. While most stoplists include common words such as articles and prepositions, we augmented the stoplist to include words that appeared in proposals that were not descriptive of the proposal content, including the e-mail addresses of PI s and the name and city of the university. These words frequently occur within a few proposals and not in many others giving them high TF-IDF weights, but they confused program directors when used as keywords and degraded the quality of Revaide X  X  suggestions. Revaide for one proposal. The term s with the highest weights and their weights were image: 0.031, judgments: 0.028, To preserve the privacy of the submitter, we cannot provide the title or abstract, but we find that the automatically extracted keywords do indeed provide a compact representation that makes sense to program directors a nd provides a basis to assist reviewers. IDF representation of the proposals they have submitted to NSF in the past. While it would be possible to use published papers of authors downloaded from Citeseer [12] or Google Scholar as measures of expertise, there are advantages in using NSF proposals in a practical system de ployed at NSF. First, all proposals are similar in style and length. These conditions are ideal for keyword extraction with TF-IDF. Second, the proposals have a variety of meta-data that is useful in other aspects of the process. This meta-data includes the PI X  X  name, e-mail address and other contact information, and an NSF ID for the PI X  X  university. This meta-data simplifies contacting the PI and checking for conflicts of interest between proposals and reviewers. Third, NSF has a strong preference for using people with PH.D. degrees as reviewers, and one can X  X  distinguish new graduate students from professors on published papers. By using people who have submitted to N SF as a reviewer pool, this problem is avoided since those eligible to apply to NSF are eligible to review. Finally, using proposals also avoids the problem of disambiguating peopl e with common names. Finally, it automatically creates a large pool of potential reviewers. A disadvantage of this approach is that it does include people who do not submit to NSF, such as researchers from industry or from outside the US. Of course, program directors may identify such people through usual means, such as checking the editorial board of journals and program co mmittees of conferences. authors of proposals that have been judged as  X  X undable X  by the review process to insure that th e reviewers were thought by their peers to have expertise in the area. We also leave out proposals with more than one author so that it is clear who has the expertise in a proposal. When more than one past proposal is available for a given author, all of the proposals are combined by adding and then re-normalizing the term vectors to form a model of the expertise. The example proposal representation in the previous section would also serve as the expertise representation of the author that submitted the proposal. directors to form panels. The most help is needed in large competitions where 500-1500 proposals may be submitted at a time. NSF X  X  system produces a spreadsheet that includes columns containing information such as th e author X  X  name, institution, the title of the proposal and links to the abstract and the PDF of the entire proposal. Teams of program directors manually sort these proposals first into general areas and then into panels of 20-30 proposals. Due to the short time and large number of proposals, it is possible that a proposal could be put into a panel with only a loose relationship to the majority of the proposals. Due to the distributed nature of the work, it is also possible that no one claims responsibility for a proposal. failed at this task when program directors didn X  X  accept the results of the clustering system. Instead of automatically clustering, Revaide checks the clusters produced by program directors for coherence and suggests improveme nts. In addition, Revaide suggests panels for  X  X rphan X  propos als that are not assigned to a panel. Furthermore, before pr ogram directors form panels, the spreadsheet they use is augmented first with the terms that have the highest TF-IDF weights 1 of each proposal. Although the weights are not included, the terms are ordered by weight. of the important terms of the cluste r. In Revaide, this is done by finding the centroid [10] of the proposals that are in each cluster, essentially creating a term vector for each cluster that is the  X  X verage X  of the term vectors of the proposals. Next, the cosine similarity [10] is found between each proposal X  X  term vector and each cluster X  X  term vector. REVAIDE produces a summary of the important terms in each cluster. These terms are chosen based on a weighted TF/IDF score. The example below illustrates such a summary. In addition to the TF-IDF weight of each term Revaide also prints out the number of proposals in the cluster that contain each term. which a proposal has been assigned, that is a sign that a proposal is potentially in the wrong cluster. Such discrepancies are pointed out to the program director w ith a suggestion to move the proposal to another panel. Belo w, the output of cluster checking is shown omitting any identifying information from the output. approximately 5% of the proposals. We have received comments from program directors that include,  X  X hanks, I don X  X  know how I overlooked that, X  in response to Revaide X  X  cluster checking. Often, Revaide finds a better pane l that is a matter of emphasis within a proposal, e.g., determin ing that a proposal will make a contribution to computer vision fo r astronomical applications as opposed to making a contribution to astronomy using existing computer vision techniques. not been put into any panel. This can occur if no member of the distributed team of program directors has identified that a proposal falls within the scope of th e panel. In this case, the panel that is most similar to the proposal is found, together with the next three, as determined by cosine similarity between the orphan This example shows an earlier version of Revaide that used stemming [9], perhaps also illustrating why we turn stemming off in later versions. proposal vector and the centroids of the panels. The output below illustrates this process. related to Rocchio X  X  algorithm for text classification [13]. The MailCAT system [14] used the idea of displaying a few possible folders for filing e-mail messages analogous to the way that Revaide finds a few possible panels . In both cases, the idea is to cope with the reality that text classification is not 100% accurate while providing benefit by focusing a person on a few possibilities out of the many that are available. The algorithm for recommending a panel for orphan proposals is one use of text classification. Th is section describes another use: performing an initial assignment of proposals to program directors. Recall that teams of program directors sort through proposals to identify the major area before further subdividing into panels. Revaide can use a text classification algorithm to perform this initial sort. In this case, the training data is the previous year X  X  proposals and the cl ass is the name of the program officer who organized the review panel the previous year. That is, the goal of the text classifica tion is to find the person who will assume initial ownership of this year X  X  proposals based upon their responsibilities in the prior year 3 . The initial program director either places a proposal into a panel they will organize or passes it to another program officer who is a better match for the proposal. submitted to Information and Intelligent Systems, the classification accuracy was 80.9%. This clearly is not good enough for a fully automated system. However, it provides tremendous benefits within the ex isting workflow. For example, rather than having 10 people each sort through 1000 proposals to find proposals of interest, each person is initially assigned approximately 100 by the text cl assification algorithm. Each program director then reviews those 100 proposals and on average needs to find a better program director for 20 proposals. This has greatly reduced the amount of effort required to identify the best program officer for each proposal. process, first by recommending an initial program officer. Once the final program officer is decided upon for each proposal proposals are manually subdivided into panels and the panels are checked for coherence. A proposal might be  X  X rphaned X  if it was initially misrouted or delayed or if no program officer claimed Because many program officers are rotators who spend a short time at NSF, the initial assignment may be based upon the program officer X  X  predecessor X  X  proposals. This overview slightly simplifies the process. Two program directors may decide to hold a joint panel, e.g., at the intersection of databases and artificial intelligence. responsibility in the initial sort. It is then assigned to a program director in the panel checking stage. In the next section, we discuss assisting in the assignm ent of reviewers to proposals. proposal would simply be to select the N authors of the previous proposals that are the most similar to the new proposal to be reviewed. This is the approach that has been used in some past efforts at automatic reviewer assignments (e.g., [15]). This approach does a fair job but has some important drawbacks. The main problem occurs when a proposal has more than one topic (a fairly common occurrence) and one topic dominates the match with other proposals. This leads to a set of reviewers that all have the same expertise, often leaving other topics in the target document uncovered. For example, consider a document about data mining using Gaussian mixt ure models to predict outcomes in a medical context. Ideally you would want a mix of reviewer expertise for this document: general data mining, the specific technique being used, as well as the field it is being applied to. Simply selecting reviewers by doc ument similarity would tend to select reviewers who matched most closely to the primary topic of the paper (as determined by the TF-IDF weighting process) possibly failing to select any reviewers at all for an important secondary topic of the document. differently. Instead of finding the N closest matches for the target proposal, we look for the set of N proposals that together best match the target document. We define a measure that indicates the degree of the overlap between the terms in a proposal vector and a set of expertise vectors. terms: normalized vector: weight of term i in a reviewer X  X  expertise vector. We define a residual term vector to represent the relevant terms in the proposal that are not in the expertise of the reviewer. The weight of each of the residual term vectors is the difference between the weight in the proposal and expertise vector with a minimum of 0. and we define the residual term vector when there are k reviewers to be where  X  controls the amount of overlap in expertise desired in the reviewers. If  X  is 1, then it is sufficient to have one reviewer whose expertise about a term equa ls the importance of that term to the proposal. If  X  is 0.5, then two reviewers should have expertise on every term in the proposal. approaches for finding reviewers we define a measure called Sum of Residual Term Weight (SRTW) to be: of reviewers that reduces the sum of residual terms to be 0 and the one set of reviewers is better suited to review a proposal than a number if that set of reviewers has a lower SRTM. find a set of reviewers for each proposal. We start by finding the  X  X est X  reviewer and then iteratively select another reviewer until N are found. At each step, the revi ewer that minimizes SRTM is selected. This iterative process will reduce the residual term weight. The residual term weight with no reviewers is 1.0 (since we work with normalized vectors). As each reviewer is selected, the term weights are adjusted according to the expertise of the reviewer. By subtracting the e xpertise vector from the document vector, the sum of residual term weights in the document vector will decrease. reduced by selecting reviewers. The row shows the most important terms in the term vector of a proposal and the remaining table shows the residual term vector after subtracting each expertise vector (with  X  =0.5). A proposal on relevance feedback for image retrieval is to be reviewed. The first reviewer selected is an expert on image retrieval. Once that contribution has been accounted for, we see terms such as  X  X mage X  have a lower term weight, reducing th eir impact on finding the next reviewer. The second reviewer has greater experience in image relevance judgments and these term s are reduced in weight. The process repeats until the desired number of reviewers are found. An important aspect of this al gorithm is that it can easily be started from a partial solution. This turns out to be a very useful property when considering the context in which the system is used. By allowing program directors to provide a partial solution that will then guide the system towards its final solution, we allow the experts to use Revaide as a tool to assist them to complete their jobs rather than using it to completely replace their judgments. whether a proposal has reviewers w ith adequate expertise. When there is no reviewer with exper tise on an aspect of the proposal, the value of SRTM for that proposal would be higher than others. This might occur if the pool of reviewers is too small or if the proposal is on a topic that had not received submissions in the past. One way to find a reviewer in this case is to use the terms with the highest residual weights as query to a specialized search engine such as Google Scholar. Figure 1 illustrates the results of Google Scholar using the three terms with the highest residual weights from table 1. Although G oogle Scholar is not integrated with the entire workflow of Reva ide (e.g., it doesn X  X  identify the e-mail address and affiliation of the authors), it still provides a useful way of recommending reviewers. the goal is to find a set of reviewers for a single proposal. However, at NSF panels, review ers typically review several proposals in a panel. Revaide can easily be used to recommend panelists for a set of proposals. Recall that in cluster checking, Revaide creates a term vector for each panel that is the centroids of the proposals in the panel. This cluster term vector represents the terms that are most important to the proposals in the panel. To invite panelists, Revaide si mply finds the panelists whose expertise best reduces the SRTM of the centroid of the panel. In this case, rather than assigning four reviewers to a proposal, 12 reviewers might be selected for a panel of 24 proposals. A lower value of  X  is used when selecting reviewers for a panel. For example, a value of 0.2 will bias Revaide toward finding 5 reviewers with expertise in th e major areas. In reality not everyone who is invited to review actually agrees to. Therefore, we typically ask 20 with the expectation of getting a 50% yield. Once many reviewers have accepted, Revaide can be run again using the confirmed reviewers as a starting point and finding reviewers to complement their expertise. a set of utilities run remotely at UC Irvine to a prototype deployed within the CISE directorate at NSF. In the first year, Revaide was used only in parallel with existing systems and only had the ability to find the N most similar to the previous proposals. This showed the promise of the technique, but the utility was diminished by the loose coupling with NSF X  X  systems and workflow. For example, the firs t version of Revaide could find the closest proposals but didn X  X  have the meta-data to automatically associate a title, author and the author X  X  contact info with the proposal. These were later manually added to spreadsheets. At this point, Revaide also could not perform conflict of interest checking and would recommend a reviewer from the same institution as the pr oposal X  X  author, a violation of NSF X  X  policies. Furthermore, it would even recommend that an author review a proposal by that author based on the author X  X  prior proposal. lead to a serendipitous finding: Revaide could be used to spot probable revisions of prior year X  X  proposals. A new proposal was typically much more similar to a prior version of that same proposal than any other previous proposal. In a small number of instances, we found a proposal that was  X  X oo similar X  to a previously submitted funded proposal, a clear violation of NSF policy. In other cases, we found t oo much similarity to a proposal currently under review at another part of NSF, another violation of NSF policy. Revaide merely alerted program directors to these possible violations. Program dir ectors decided whether there was a probable violation, which in th e case of resubmission of funded work was then investigated by NSF X  X  Inspector General. While not emphasized in this paper, Revaide still retains these capabilities. meta-data on proposals so that it can do conflict of interest checking and produce output that in cludes names and contact info of potential reviewers. Revaide also has access to the previous summary reviews rankings and f unding decisions on proposals so only those whose expertise has been validated by the peer-review process were considered as potential reviewers. Revaide also helped NSF achieve its diversity goals by including some demographic data on reviewers. If a proposal or panel did not include female reviewers, reviewers from underrepresented groups, or reviewers from EPSCOR states (i.e, states that do receive much federal research funding), additional reviewers were recommended from these groups, insuring that proposals are not just reviewed by an  X  X ld boys club X  and that a diverse group of investigators has the benefits of participating in funding decisions. similarity for selecting reviewers to using the residual term weight approach described earlier. This was done in response to the problem of cosine similarity on interdisciplinary proposals leading to recommending proposals only from a single discipline. However, Revaide was still used remotely from California when the results and data were in Ar lington, VA. Delays caused by computation to converted proposal s from PDF to ASCII and index proposals, transferring gigabytes of data, minor errors in the meta-data 5 and the difference in time zone typically resulted in a two-or three-day turnaround in running the system. Nonetheless, the system illustrated its utility by finding proposals that were obviously assigned to the wrong panel and suggesting qualified reviewers that were overlooked by program officers. year, NSF purchased the appropria te computer equipment and ran Revaide in house. Furthermore, this enabled tighter integration with NSF X  X  databases, e.g., pr oposal meta-data was accepted in the exact format produced by NSF X  X  systems rather than requiring Furthermore, processes were put into place to accurately record and maintain the data used by Revaide. This reduced the time required to get results from Reva ide from a few days to a few hours. Plans are now being evaluated to have a contractor fully integrate Revaide with NSF X  X  internal systems and build a web interface to Revaide. In the next section, we summarize the experiences in the third year of using Revaide. empirically evaluate the utility of the residual term weight approach in assigning reviewers. We also report on the lessons we have learned in deploying Re vaide in the government context. proposals. In particular, for each proposal submitted to the 2004 Information Technology Research program in the division of Information and Intelligent Systems, a total of approximately 1,500, we compare finding the three closest reviewers as determined by cosine similarity to the three that best reduce SRTM. In each case the pool of reviewers is the people who submitted proposals to the division in the prior three years. The average sum of residual term weights (with  X  = 0.5) decreases from 0.636 for the three closest to 0.569 for Revaide X  X  approach. Note that this average does not tell the entire story. For more than five percent of the proposals, perhaps the most interdisciplinary proposals, there was a difference of greater than 0.15 in the sum of residual terms, demonstrating the importance of finding a set of reviewers with complementary expertise. Of course, it may seem like a tautology to show that a system that attempts to minimize SRTM has a lower SRTM. However, this also puts a number behind the intuition that similarity alone isn X  X  sufficient for finding reviewers for interdisciplinary proposals. for two panels of proposals submitted to the 2005 Universal Access solicitation. For each panel, we compare using six randomly selected people funded in the prior year as reviewers (analogous to the common conference practice of inviting a program committee before papers are submitted or the NIH practice of having a standing panel), the six reviewers closest to the centroid of the proposals in the panel, and the six reviewers For example, an unexpected carriage return in a proposal title resulted in an ill-formed tab separated file. that best reduces the sum of residual term weights from the centroid of the panel (with  X  = 0.5). Once the panelists are selected, then four panelists are assigned to proposals by Revaide using SRTM with  X  = 0.5. The mean residual term weight under these conditions is shown in Tabl e 2. It is apparent from this figure that both approaches that examine the proposals to select panelists have a benefit over pick ing panelists who are experts in the general subject area (Standing Panel). Furthermore, selecting panelists with complementary expe rtise (SRTM) has an advantage of selecting panelists whose expe rtise is most similar to the central theme of the pr oposals (Similarity). upon heavily in the Information and Intelligent Systems (IIS) with the evaluation of a competition that received slightly over 1000 proposals and several other comp etitions with 200-500 proposals. It was also used in competitions in the Computer and Communication Foundations Divi sion and a Computer and Information Science and Engineering interdisciplinary competition. In IIS, Revaide was relied upon to initially dispatch proposals to program officers, to check panels for coherence, to find panels for orphan proposals, and to recommend reviewers for most panels. Some summary resu lts and lessons learned included: success of Revaide: [16]. The main technical contribution of Revaide is the use of the sum of residual term weights measur e in reviewer assignment. In implementation, we used a well established but simple document model: TF-IDF weights on words. The residual term weight approach is independent of the document model and could just as easily be used with hand-select ed keywords, LSI terms (e.g., [17]), or author and topic models (e.g., [18] and [19]). We did indeed consider using LSI in Re vaide but have decided against it because LSI doesn X  X  produce terms that are easily understood by people and can easily be used as queries for a text search engine. If we had access to only abstracts, LSI might prove particularly useful, but in longer documents such as full proposals, the benefits of LSI are less dramatic and not worth the lack of comprehensibility in this application. in a proposal left uncovered by a partial set of reviewers. One approach to this problem is Maximal Marginal Relevance (MMR) [20]. MMR provides a way to adjust the ranking (or re-rank) the retrieved results of a query to produce a diverse set of documents. MMR is based on comparing retrieved documents to each other in order to select a diverse group. In contrast, SRTW is a more focused measure that seeks to ach ieve diversity to satisy the goal of covering terms in a source document. into its data infrastructure and workflow. Revaide would then be able to directly access NSF databases rather than going through intermediate files. We plan on conducting further research on the general topic of reviewer assignment. In particular, we are exploring approaches that will balance reviewer assignments across reviewers on a panel. We believe such an approach will need to consider the residual term weights, the number of proposals assigned to a reviewer, and the distance between a proposal and a reviewer X  X  exper tise (because in our experience reviewers have a strong aversion to reviewing proposals outside their expertise). deployed at NSF as prototype. While much of Revaide relies upon existing technology for representing documents, Revaide makes two contributions to the prac tice of text mining. First, we have defined a new measure of similarity suited for insuring that expertise is found for all aspects of a proposal to be reviewed. Second, we have shown that text mining technology can be deployed to augment rather than replace human judgment. development and deployment of Revaide. Feedback of early users helped in the design of later versions. Many also helped navigate the approval. [1] Willett, P. (1998). Recent Trends in Hierarchic Document [2] Larsen , B. and Chinatsu A. (1999). Fast and effective text [3] Hopcroft, J., Khan, O., Kulis, B. &amp; Selman, B. Tracking [4] Bradley, P., Bennett, P and. Demiriz., A. (2000) [5] Banerjee , A. &amp;G hosh,, J. (2002). Frequency Sensitive [6] Furnas, G.W., Landauer, T.K., Gomez, L.M., Dumais, S.T. [7] Ding, W., and Marchionini, G. A Study on Video Browsing [8] Furnas, G.W., Landauer, T.K., Gomez, L.M., Dumais, S.T. [9] van de Stadt, R. (2000). CyberChair, an Online Submission [10] Salton, G., &amp; McGill, MJ (1983). Introduction to modern [11] Porter, M.F., (1980), An algorithm for suffix stripping, [12] Giles, C. Bollacker, K., Lawr ence, S. (1998). CiteSeer: An [13] Rocchio, J. (1971) Relevance feedback in information [14] Segal, R and Kephart, J (1999).. MailCat: An Intelligent [15] Basu, C., Hirsh, H., Cohen, W., and Nevill-Manning, C., [16] Geller, J. and Scherl, R., 1997 Challenge: Technology for [17] Dumais, S., Nielsen, J. (1992, Automating the Assignment of [18] Steyvers, M., Smyth, P ., Griffiths, T. (2004) Probabilistic [19] Mann, G., Mimno, D. and McCallum, A (in press). [20] Carbonell, J. and Goldstein, J (1998). The Use of MMR, 
