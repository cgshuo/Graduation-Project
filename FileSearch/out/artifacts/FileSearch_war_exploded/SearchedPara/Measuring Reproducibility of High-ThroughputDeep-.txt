 During the past years, the biological technology revolution, next-generation high-throughput deep-sequencing, has produced mountains of data of DNA-Seq, RNA-Seq and ChIP-Seq. This mega-data allows biologists to observe the signals from tens of thousands of genes or related genomic elements in a single experi-ment, a way that was not possible before. One question arises here: how many of these signals are real biologically relevant? Generally, to avoid experiment noise or error, two experiment replicates of on e biological sample should be produced, each of which has a collection of individual elements or signals such as a list of genes or transcripts. Then we need to verify the reproducibility of each individ-ual signal. Only the individual signals with high reproducibility are considered as reliable results for further analysis such as differential gene expression iden-tification or GO analysis. Here reproducibility of a signal X  X  two observations in two replicates is a measure of the confidence that the two observations are con-sistent with each other. Hereinafter it is shorted as  X  reproducibility of signal  X . We propose a posterior probability to characterize the confidence and also de-fine irreproducibility = 1 -reproducibility. By choosing a specific critical value, each signal can be determined whether or not to be confident. It is worth noting that we only have two replicates, two lists of observations of individual signals but need to detect the reproducibility of each individual signal. Such extremely small samples also lead to a big and difficult challenging task to traditional data mining where data are generally of remarkably larger size and density estimation can be effectively calculated based on these data.

IDR (Irreproducible Discovery Rate) which measures the reproducibility in high-throughput experiments has been put forth by Li [1]. They proposed to use copula, which can separate the dependency structure of random variables, and a measurement based on copula to det ect the high reproducible signals. A remarkable advantage of copula is that it provides an effective way to infer the dependency structure between biological signals without knowing their respec-Reproducibility measure has been adopted as the standard of ENCODE (The Encyclopedia of DNA Elements) project and has been carried on each signal of all samples before these data are submitted to public database. The strategy of IDR has been generalized to use on other data types such as RNA-Seq.

Although IDR has shown its ability of distinguishing the bona fide signals from artificial signals, it employed the Gaussian Copula, which assumes that the dependence structure of random variables follows multivariate Gaussian distri-bution. This assumption causes that the Gaussian Copula is sensitive to extreme events and can not capture the asymmetric dependence structure [2]. In fact, de-spite its simplicity, Gaussian Copula often leads to an underestimation of the risk of the occurrence of joint extreme events [3,4].Therefore, it is necessary to develop a novel and efficient approach to measure the reproducibility of replicate data without stronger assumption to data distribution.

In this paper, we propose a S elf-a daptive Mi xture C opula, called SaMiC, to measure the reproducibility of the high-throughput deep-sequencing experi-ments. Unlike IDR, SaMiC doesn X  X  assume the dependence structure of random variables to follow Gaussian distribution. SaMiC mixes several copulas and auto-matically determines the mixture coeffici ents based on the fitness of the data and the copulas. We prove theoretically that the new mixture copula is still a copula so that it can be used to measure the reproducibilities. Simple and easy to imple-ment, SaMiC is effective and suitable for general distributions. Experiments in both simulated data and real biological data of RNA transcripts expression from human cells show that compared with ID R, SaMiC attains better performance in distinguishing bona fide signals from artificial signals.

The remainder of this paper is organized as following. In Section 2, we in-troduce the development and preliminary of copulas. In Section 3 we detail our proposed self-adaptive mixture copul a and a novel measurement to detect the reproducibility of experiments. In Section 4, we perform experiments in simu-lated data and real data. In Section 5, we conclude the paper. As a tool of extracting the dependence structure from joint distributions of ran-dom variables, copula was first proposed by Sklar [5]. In his work, copula is obtained by a two-stage procedure, i.e. , estimating the marginal distribution of each random variable followed by measuring the dependence structure between different random variables. Deheuvels proposed several empirical functions, i.e. , the empirical copula of samples, to estimate the copula of population and con-structed different non-parametric dependence tests from samples [6]. However, a well-recognized definition of copula hasn X  X  been given until Nelsen X  X  work [7].
There are two major categories in studying copulas: parameter estimation and test of goodness of fit. In the former category, Oakes and Genest proposed a common strategy to estimate the parameters of bivariate copula [8,9]. Later, Joe investigated the maximum likelihood estimation of parametric marginal distri-bution and parametric copula [10]. Furthermore, Chen studied two stage semi-parametric maximum likelihood estimation [11], and Abegaz derived asymptotic properties of the marginal and copula parameter estimators [12].

In the aspect of test of goodness of fit, an important goal is to measure how well a copula describes the dependence structure among random variables since it is closely related to the correctness of the proposed copula. According to the copula model, test of goodness of fit can be transformed into test of univariate distribution. Then Kolmogorov-Smirnov test can be used to test the goodness of fit of copula. In this manner, Klugman used Q-Q plot to measure the rationality of copula model [13]. Hu introduced M -statistics, which follows the chi-square distribution, to measure goodness of fit of copula model [14]. Engle proposed a test method named  X  X it X  [15], and Patton expanded the test method  X  X it X  to the nonlinear density model for checking the goodness of fit [16]. Theses methods can evaluate both the copula and the marginal distribution.

Since the function is useful to obtain the dependence structure of multivariate random variables with few assumptions, it has been applied in financial field. Embrechts employed copula for financial risk management [17]. Hu proposed to use mixed-copula to analyze the financial data [14]. However, such a mixture is not automatic and depends on expert s X  experience. Recently, copula was also employed in bioinformatics field. For example, Kim discussed the application in genetic data [18], Zhang used copula model to analyze ChIP-Seq data [19], and Li proposed a new method based on copula model to measure reproducibility of bioinformatics data [1]. A main reason of using copula in financial and bioin-formatics fields is that it can obtain the dependence structure without knowing marginal distribution of random variables in advance. It is worth noting that the above-mentioned copulas still require more or less assumptions to data dis-tribution, which may limit its effectiveness and extension. It is also noticeable that in bioinformatics field, copula is still a new tool of data analysis. For better understanding, we introduce some preliminaries of copula as follows. Theorem 1 (Sklar X  X  Theorem). [5] Let H be a joint distribution function with margins F 1 and F 2 . Then there exists a copula C such that If F 1 and F 2 are continuous, then C is unique; otherwise, C is uniquely deter-mined on RanF 1  X  RanF 2 .Here RanF refers to the range of F . Conversely, if C is a copula and F 1 and F 2 are distribution functions, then the function H is a joint distribution function with margins F 1 and F 2 .
 Sklar X  X  Theorem shows that a joint distribution function can be divided into each variable X  X  marginal distribution function and a copula which present their statistical consistence. Th erefore, it is possible to calculate copulas from joint distribution function and its marginal distribution functions. From Sklar X  X  The-orem we can get the follow properties, which are important to measure the reproducibility of high-throughput deep-sequencing experiments:
Property 1. Let G ( X 1 ,X 2 ,  X  X  X  ,X n ) be a joint distribution function of n tribution functions of these random variables and C ( u 1 ,u 2 ,  X  X  X  ,u n )isthecor-F ( z )  X  u i } .

Property 2. Let G ( X 1 ,X 2 ,  X  X  X  ,X n ) be a joint distribution function of n ran-the random variables, we can get that density functions of F i ( X i )and G ( X 1 ,X 2 ,  X  X  X  ,X n ), respectively.
Currently, there are two main types of commonly-used copulas: Elliptical cop-ulas and Archimedean copulas. Elliptical copulas are a kind of copulas with contoured elliptical distributions, such as Gaussian copula and t -copula. Easy to construct, Elliptical copulas don X  X  have a closed form of function expression, and all of them are radially symmetric and hard to extend to high-dimensional situa-dependence structures or adapt to complex situations.

Different from Elliptical copulas, Archimedean copulas are an associative class of copulas which satisfy the following equations: where  X  (  X  ) is usually called the generator of Archimedean copula. Among a lot of Archimedean copulas, three frequent ly used Archimedean copulas are Frank Copula, Clayton Copula and Gumbel Copula. Specifically, the reproducibility structures of Frank Copula and the variables drawn from Frank Copula are symmetric in both tails of their distribut ions. So any asymmetric consistence between random variables can X  X  be captured using Frank Copula. Clayton Copula is sensitive to the low tail dependence of random variables, and can easily capture the changes around the low tail. Finally, Gumbel Copula is sensitive to the upper tail dependence of random variables. It is obvious that each copula has its respective pros and cons in different dis-tributions. To deal with more general distributions, we propose self-adaptive mixture copula, which is a linear combination of several copulas. For simplifica-tion, we discuss the proposed copula in two-dimensional situation.
 Definition 1 (Mixture Copula). A function is called mixture copula if it C cients of C M ( u,v ) .
 We prove that C M ( u,v ) is a copula, which will be introduced in a extended version due to the length limitation. To self-adaptively estimate the linear coef-ficients of the proposed mixture copulas, we utilize Pearson  X  2 statistic pro-posed by Hu to measure the goodness of fit of each copula [14], i.e., M = dicted frequencies in cell ( i,j ) of a contingency table, respectively. The details on the contingency table can be referred to as Hu [14]. M follows  X  2 distribution bution, we can get probability  X  i from M C i .Infact  X  i is the probability that there is no significant difference between copula C i and the data. Because of the additivity of chi-squared distribution, let  X  i =  X  i m of C i , then the proposed self-adaptive mixture-copula is Since our self-adaptive method chooses coefficients automatically, it can deal with more general distributions. By contrast, the ordinary mixture copula man-ually selects the coefficients, heavily depending on human experience and need lots of tuning for each new group of data [14]. Consequently, it is only applicable to some specific distributions.

To measure the statistical consistency or reproducibility based on the pro-posed self-adaptive mixture copula, we here consider the situation with two rows of observations for simplification. The reason is that when observations subject to independent identically distribution, it is not difficult to expand to multivariate situation if we use a pairwise analysis to them.
 replicates of n random signals. We assume that the observations consist of a more reproducible group and a less reproducible one, and  X  0 and  X  1 denote the proportion of the less reproducible group and the more reproducible group, respectively. Let parameter K i be an indicator to identify whether or not a signal belong to the more or less reproducible group. K i = 1 if the i -th signal belong to the more reproducible group, and K i = 0 if it is in the less reproducible group.
Obviously, the signals in the more or less reproducible group have different probability distributions. We assume that the dependence structures of the two observations of signals in the more and less reproducible groups are induced by z any multivariate probability distribution can be divided into its marginal proba-bility distributions and its copula. In other words, it provides a way to infer the reproducibility among several random variables without knowing their marginal distributions in advance. Thus, we construct our parametric model as follows: S ( u,v ) ,k =0 , 1, and tively. And  X  k denotes the relevant parameter of copula. Then considering (6), the total distribution function is In this equation, our actual observations ( x i, 1 ,x i, 2 ) are used to estimate the functions of S 0 ( z i, 1 ,z i, 2 )and S 1 ( z i, 1 ,z i, 2 ) as follows: Up to now, our model is parametrized by  X  =(  X  0 , X  0 , X  1 )and F 1 , F 2 .The parameters can be attained using maximum likelihood estimation as: Note that selecting different Archimedean copulas in (6) will lead to different forms of S ( z i, 1 ,z i, 2 ). Since F selection of Archimedean copulas in (6).
 attain the final mixture-copula as: C R ( u,v ) can be decomposed into
C R ( u,v )= v ;  X  ( i ) , 1 ) are the more and less reproducible groups, respectively.
Once C R ( u,v ) is determined, it is easy to update the total distribution func-tion of the data as and thus Finally, we can estimate the irreproducibility of each signal based on: P { K ( x Algorithm 1. The Proposed SaMic Approach tive two-stage one-dimensional optimization method. It X  X  worth noting that we rable and the rank statistic tends to cope better with real-world systematic biases and errors. The pseudo-code of our estimation procedure is shown in Alg. 1.
Compared with the IDR proposed by Li [1], a remarkable advantage of our algorithm is that it is more effective since it only needs to do one-dimensional optimization search for no more than 2 km times, where k is the threshold of it-erations. So its asymptotic time complexity is O ( mn ). The actual running time of our algorithm is also affected by the selected threshold of precision and iterations. To evaluate the effectiveness of our proposed SaMiC approach, we compare it with IDR proposed by Li [1] in two simulated data with remarkably different marginal distributions and reproducibility structures and one real biological data. Note that although IDR has four values to be initialized, we found that they have less influence to the analysis of the final results. We chose Frank Cop-ula, Clayton Copula and Gumbel Copula that all from Archimedean family as base copulas since these copulas have high potential of extending from bivariate Archimedean copulas to multivariate ones. 4.1 Simulated Data In the first experiment, we generate two rows of 10,000 numbers which follow normal distributions N (0 , 1) and N (2 , 12), respectively. Then we consider these numbers as 10,000 signals X  two observations to detect their (ir)reproducibilities. For the two rows of numbers generated from different distributions, there X  X  little chance that these signals X  two observations are confidently consistent. So we expected the (ir)reproducibilities are low (high). Then we use both IDR and SaMiC to measure the (ir)reproducibilities of these signals. Note that both IDR and SaMiC output the irreproducibilities in [0 , 1]. The results shown in Fig. 1 indicate that IDR has a lower recognition rate to discover the irreproducible signals. For example, if we regard those signals whose irreproducibilities are less than 0.5 are reproducible, then many irreproducible signals will be classified to be reproducible. In contrary, our SaMiC approach can correctly classify most irreproducible signals even when the cutoff value is set to be 0.9.

In the second experiment, we wish to test whether our SaMiC can be suit-able for a more general distribution. Thus, we generated two rows of 10,000 random numbers combined from two different types of distributions. Firstly, tion T  X   X  (2 , 14), and 10,000 random numbers ( a 1 ,a 2 ,  X  X  X  ,a 10000 )frombeta Fig. 1 it X  X  obvious that IDR failed to distinguish the irreproducible signals. In contrast, SaMiC can estimate the (ir)reproducibilities of signals in experiment 2 with high confidence. The reason is that SaMiC makes less assumption to the dependence structure of observations, and the self-adaptive mixture copula is helpful to be suitable for general distributions.

We also use both kernel and Gaussian density estimation on these data of the two above experiments. As demonstra ted in Fig. 1, the results from density estimation can show the differences between two rows of numbers only in an overall perspective. So it X  X  hard to d ecide whether or not to trust some spe-cific signals that are reproducible by using density estimation. In contrast, our method attains the (ir)reproducibility of each signal, which can distinguish bona fide signals from artificial signals. 4.2 Real Data We also use real biological data to test the performance of SaMiC. The data can be downloaded from  X  X ttp://genome.ucsc.edu/cgi-bin/hgFileUi?db=hg19 &amp;g=wg EncodeCshlLongRnaSeq X  (selected categories: Cell Line = HeLa-S3, Lo-cation = cell, RNA Extract = Long Pol yA+ RNA, View: Transcript Gencode V7). This data was generated by ENCODE project [20] and they are biological experiments to detect the expression level of HeLa-S3 cell X  X  long RNA tran-scripts, which were sequenced by RNA-Seq. The downloaded data file contains 161,999 annotated transcript individuals X  expression values  X  the normalized RPKM values. As need, each transcript h as two values from different exper-iment replicates respectively. They are estimated by SaMiC and IDR to test their performance. Different from some cl assical data mining problems, it X  X  dif-ficult to verify the experiment results on real data because of lacking test data or labels. So we need to analyze the results in some indirect ways.

Intuitively, the larger (or smaller) the proportion of a signal X  X  two observa-tions is, the smaller probability that the signal is reproducible. In fact, SaMiC scores are different from proportions b ecause proportions on ly consider local information while SaMiC scores rely on both the entire distribution and the dependence structure of the observations. Nevertheless, it still can demonstrate some differences between IDR and SaMiC by using figure of proportions versus irreproducibilities. As shown in Fig. 2, we draw this figure by putting logarithm of proportion on x -axis and irreproducibility on y -axis. From Fig. 2 we can see that SaMiC is more sensitive and has a stronger recognizing ability. Take signals in (  X  X  X  ,  X  2]  X  [2 ,  X  ) with irreproducibilities lower than 0 . 2 for example. The num-ber of irreproducible signals estimated by IDR is remarkably larger than that estimated by SaMiC. It shows that SaMiC is more sensitive to (ir)reproducibility and can identify the irreproducible signals which are ignored by IDR.
For the convenience of subsequent data analysis, such as keeping specific pro-portion of data or choosing different criti cal values, we expect the irreproducibil-ities to be smooth. In order to compare IDR and SaMiC from this viewpoint, we produce the cumulative distribution curves of the results from both IDR and SaMiC. From Fig. 3 we observe that the curve of SaMiC is smoother than that of IDR. Besides, compared with IDR, SaMiC can provide more detailed data for keeping specific proportion of data. For example, if we want to get the sig-nals with the lowest 20% irreproducibilities, it X  X  easy while using SaMiC but unavailable while using IDR. This is because that there are almost 40% irrepro-ducibilities that are 0 in the result of IDR, which means IDR fails to discriminate the signals with 40% lowest irreproducibilities while SaMiC succeeds. So SaMiC performs better on selecting the most reliable signals with a specific proportion.
Furthermore, we perform more experiments on another three different types of cells including GM12878, H1-hesc and K562, which are downloaded from the same website as HeLa-S3. For saving spa ce, the detailed results can be found in future extended version. Based on thes e experiments, we give a comparison on running time. As shown in Fig. 3, SaMiC works faster than IDR on all of the four data. The computing environment is Intel Core2 2.93GHz with 4G memory. In this paper, we have proposed a Self-adaptive Mixture Copula to measure the reproducibility of high-throughput deep-sequencing experiments, which is a difficult and challenging data mining problem since the number of samples is extremely small. The proposed SaMiC ca n effectively separate the dependence structure from joint distribution of signals without priori assumption. Compared with IDR, SaMiC can discover the irreproducible signals in a more reliable way.
SaMiC features no parameters that n eed to be tuned and can calculate the (ir)reproducibilities in an automatic way. It can self-adaptively choose the most suitable parameters for given data and is thus robust for different datasets. Besides, SaMiC works faster than IDR on all data we tested.

SaMiC can be used in all high-throughput deep-sequencing experiments that produce over one replicate to avoid reducing the confidence of experimental results. Actually, the reproducibility issue exists for a great number of researches so that the method of estimating reproducibility has a wide application. In the future, we will compare SaMiC with other methods such as FDR.
 Furthermore, we will do more experime nts with labeled data and investigate more application fields of SaMiC.
 Acknowledgements. This work was supported in part by the National Natural Science Foundation of China (NS FC No. 61273299, 60975044) and Ministry of Education (No. 20120071110035).
 Nowadays, the use of many electronic devices in real world applications has led to an increasingly large amount of data containing moving object information. One of the interesting moving object clusters. A moving object cluster can be def ned as a group timestamps. In this context, many recent st udies have been define such as flock [5], [6], traveling companions [13], gathering patterns [16], etc...

Nevertheless, after the extraction, the end user can be overwhelmed by a huge num-ber of movement patterns although only a few of them are useful. However, relatively few researchers have addressed the problem of reducing movement pattern redundancy. description length (MDL) principle [4], proposes to reduce the amount of itemsets by patterns.

In this paper, we adapt the MDL principle for mining representative movement pat-terns. However, one of the key challenges in designing an MDL-based algorithm for movingobjectdata is that theencodingschemeneedsto deal with differentpattern struc-
Furthermore, although patterns express different kinds of knowledge, they can over-lap each other as well. Thus, enforcing non-overlapping patterns may result in los-Krimp algorithm does not allow overlapping patterns then it has to select one and ob-viously loses the other one. However, they express very different knowledge and thus, by removing some of them, we cannot fully understand the object movement behavior. Therefore, the proposed encoding scheme must to appropriately deal with the pattern overlapping issue.

Motivated by these challenges, we propose an overlapping allowed multi-pattern structure encoding scheme which is able to compress the data with different kinds of patterns. Additionally, the encoding scheme also allows overlapping between different kinds of patterns. To extract compression patterns, a naive greedy approach, named N AIVE C OMPO , is proposed. To speed up the process, we also propose the S MART -C
OMPO algorithm which takes into account several useful properties to avoid useless and eff ciency of the proposed approaches by comparing different sets of patterns. 2.1 Object Movement Patterns Object movement patterns are designed to group similar trajectories or objects which tend to move together during a time interval. In the following, we briefl present the def nitions of different kinds of movement patterns.
Database of clusters. Let us consider a set objects occurring at different times-tamps. A database of clusters, C DB = { C t 1 ,C t 2 ,...,C t c  X  C as a preprocessing step.

After generating C DB , the moving object database ( O DB ,T DB ) is define such as for the associated timestamp. For inst ance, Figure 1 presents the database O DB and object o 1 can be represented as o 1 = c 1 c 4 c 6 c 7 c 8 .

From this set different patterns can be extracted. In an informal way, a closed swarm in cs . Then a closed swarm can be formally define as follows: For instance, see Figure 3, cs = c 1 c 3 c 4 is a closed swarm with min t =2 , X  =2 . disjointed convoys which are generated by the same group of objects in different time intervals. In this paper, we only consider closed swarm instead of convoy and group pattern since closed swarm is more general [10].
 next cluster. A rGpattern can be def ned as follows: trajectory pattern if: C Essentially, we have two kinds of rGpatterns, rGpattern  X  and rGpattern  X  .Forin-stance, see Figure 1, rGpattern  X  = c 1 c 4 c 6 and rGpattern  X  = c 7 c 8 . 2.2 Problem Statement this principle in the following def nition: Given a scheme S ,let L S ( P ) be the description length of hypothesis P and L the hypothesis and an encoding scheme S . Informally, the MDL principle proposes that the best hypothesis always compresses the data most. Therefore, the principle suggests that we should look for hypothesis P and the encoding scheme S such that L when the encodingscheme is clear fromthe context.Additionally,the description length of O DB given P is denoted as L P ( O DB )= L ( P )+ L ( O DB |P ) .
 In this paper, the hypothesis is considered as a dictionary of movement patterns P . bit representation which requires a unit memory cell. In our context, the description number of patterns (i.e. L ( P )= O
DB when encoded with the help of dictionary P can be calculated as L ( O DB The problem of f nding compressing patterns can be formulated as follows: Definition 4. (Compressing Pattern Problem). Given a moving object database O DB , which contains at most K movement patterns so that: A key issue in designing an MDL-based algorithm is: how can we encode data given a overlapping allowed multi-pattern structures encoding scheme for moving object data. 3.1 Movement Pattern Dictionary-Based Encoding Before discussing our encoding for moving object data, we revisit the encoding scheme used in the Krimp algorithm [14]. An itemset I is encoded with the help of itemset patterns by replacing every non-overlapping instance of a pattern occurring in I with a pointer to the pattern in a code table (dictionary). In this way, an itemset can be encoded to a more compact representation a nd decoded back to the original itemset. o o object can involve in only a part of a rGpattern and viceversa.
 at and a pattern p = c 1 ...c n . p occurs in o or o contributes to p if: o  X  o ( c Case (3), we have o  X  O ( cs )= rGpattern  X  , rGpattern  X  ), we need to store with the pointer an additional index to ending involving point) of the object o in a rGpattern  X  (resp. rGpattern  X  ).
As an example, consider dictionary P in Table 1. Using P , o 1 can be encoded as o 1 o = p 2 ,i.e. p 2 is a closed swarm. 3.2 Overlapping Movement Pattern Encoding encoding an object o given a pattern p . In this section, the encoding scheme will be completed by addressing the pattern overlapping problem so that overlapped patterns can exist in the dictionary P .
 overlapping but both of them can be included in the dictionary P . Note: in our con-text, overlapped clusters are counted only once.

Main idea. Given a dictionary P and a chosen pattern p (i.e. will be added into P ), given pattern p . Secondarily, we propose to encode all candidates p  X  F given p in kinds of pattern candidates which are encode d candidates and non-encoded candidates. Next, the best candidate in F will be put into P andusedtoencode O DB and F .The process will be repeat until obtaining top -K patterns in the dictionary P .
Let us consider the correlations between a pattern p  X  X  and a candidate p  X  F to identify whenever encoding p given p is needed. The correlation between p and p is illustrated in Table 2. First of all, we do not allow overlap between two patterns of the same kind since they represent the same knowledge that may lead to extracting redundant information.
Next, if p is a closed swarm then p do not need to be encoded given p . This is because there are objects which contribute to gradual trajecto-ries p but not closed swarm. These objects cannot be encoded using p and therefore p needs to be re-mained the same and the regular en-coding scheme can be applied. Oth-erwise, p will never be chosen later since there are no objects in O DB which match p . For instance, see Figure 2, the objects o 1 and o 4 do not contribute to the closed swarm p . Thus, if the gradual trajectory p is encoded given p to indi-o ,o 4 and the gradual trajectory p .

Until now, we already have two kinds of candidates p  X  F (i.e. non-encoded and encoded candidates). Next, some candidates will be used to encode the database O DB . To encode an object o  X  O DB given a non-encoded candidate p , the regular encoding scheme mentioned in Section 3.1 can be app lied. However, given an encoded candidate p , we need to perform an additional step before so that the encoding scheme can be p and thus o cannot be encoded given p .

For instance, see Figure 2, given a gradual trajectory pattern rGpattern  X  p = c p o
To deal with this issue, we simply recover uncommonclusters between the two point-o encoded given p such that o 3 = p c 4 .
 Definition 5. (Uncommon Clusters for rGpattern  X  ). Given a rGpattern  X  , p = c c that [ p,k ]= c k c k +1 ...c l  X  1 [ p,l ] .
 are able to recover uncommon clusters betw een two pointers which refer to a pattern. Now, we start proving that given an object o  X  O DB and a candidate p  X  F ,if p occurs in o then o can be encoded using p even though they contain many pointers to other patterns. First, let us consider if p is a rGpattern  X  and p is a closed swarm. Lemma 1. Given a rGpattern  X  , p = c 1 ...c n , an object o and a closed swarm p  X  x ,y o ,x p and y p are lists of clusters. If o contributes to p then: have o = x o uncom ( p,k,l )[ p,l ] y o .
 Then we can apply the regular encoding scheme to encode o given p . let us as-sume that each object o  X  O p has a common list of pointers to other patterns as  X  X  X  X  X  X  ( p ,o )= Lemma 1 on each pointer in have the other lemmas for other pattern types.

Data description length computation. Until now, we have def ned an encoding scheme for movement patterns. The description length of the dictionary in Table 1 is calculated as L ( P )= | p 1 | +1+ | p 2 | +1+ | p 3 | +1+ |P| = 3+1+4+1+2+1+2= 14 . Note: for each pattern, we need to consider an extra memory cell of pattern type. Additionally, for any given dictionary P and the data O DB , the cost of storing the timestamp for each cluster is always constant regardless the size of the dictionary. In this section we will present the two greedy algorithms which have been designed to extract a set of top -K movement patterns that compress the data best. 4.1 Naive Greedy Approach K of
NaiveCompo , we select candidate p which compresses the database best. Next, p will be added into the dictionary P and then the database O DB and F will be encoded given p . The process is repeated until we obtain K patterns in the dictionary. To select the best candidate, we generate a duplication of the database O d for each candidate p  X  F , we compress O d smallest data description length will be considered as the best candidate. Note that p = argmin p  X   X  F L p  X  ( O DB ) .TheN AIVE C OMPO is presented in Algorithm 1. 4.2 Smart Greedy Approach Algorithm 1. NaiveCompo tern type has its own compression gain computation function. Let us start presenting the process by proposing the property for a closed swarm p .
 Proof. After construction we have L P X  p ( O DB )= L ( P X  p )+ L ( O DB |P  X  p )= Furthermore,  X  o  X  O p : L ( o |P  X  p )= L ( o |P )  X  X  p | +1+ L ( O p |P  X  p )= o  X  O k | | p L ( O p |P ) . Consequently, we have gain ( p , P )= | O p | X | p | X  O p o k | + | p | + | O By applying Property 2, we can compute the compression gain when adding a new puted by scanning p with objects o  X  O ( p ) without encoding O DB . Due to the space
To select the best candidate at each iteration, we need to chose the candidate which returns the best compression gain. S MART C OMPO is presented in the Algorithm 2. Algorithm 2. SmartCompo Furthermore, DBScan [2] ( MinPts =2; Eps =0 . 001 ) is applied to generate clus-ters at each timestamp. In the comparison, we compare the set of patterns produced by SmartCompo with the set of closed swarms extracted by ObjectGrowth [10] and the set of gradual trajectories extracted by ClusterGrowth [6].

Effectiveness. We compare the top-5 highest support closed swarms, the top-5 high-est covered area gradual trajectory patterns and the top-5 compression patterns from
Top-5 closed swarms are very redundant since they only express that Swainsonies move together from North America to Argentina. Similarly, top-5 rGpatterns are also redundant. They express the same knowledge that is  X  X rom 1996-10-01 to 1996-10-25, the more time passes, the more objects are following the trajectory { Oregon Nevada Utah Arizona Mexico Colombia }  X  .

Figure 6 illustrates 3 patterns among 5 extracted ones by using SmartCompo. The rGpattern  X  expresses the same knowledge with the mentioned rGpattern in the top highest covered area. The closed swarm expresses new information that is  X  X fter ar-riving South America, the Swainsonies tend to move together to Argentina even some of them can leave their group X  . Next, the rGpattern  X  shows that  X  X he Swainsonies return back together to North America from Argentina (i.e. 25 objects at Argentina) Guatemala) since they are only 2 objects at the last stop, i.e. Oregon State X  .
Compressibility. We measure the compressibility of the algorithms by using their top -K patterns as dictionaries for encoding the data. Since NaiveCompo and Smart-Compo provides the same results, we only show the compression gain of SmartCompo.
Regarding to SmartCompo, the compression gain could be calculated as the sum of the compression gain returned after each greedy step with all kinds of patterns in
F . For each individual pattern type, compr ession gain is calculated according to the greedy encoding scheme used for Smart Compo. They are respectively denoted as SmartCompo CS (i.e. for closed swarms), SmartCompo rGi (i.e. for rGpattern  X  ) and SmartCompo rGd (i.e. for rGpattern  X  ). Additionally,to illustrate the difference between MDL-based approaches and standard support-based approaches, we also em-gradual trajectories patterns.

Figure 7 shows the compression gain of different algorithms. We can consider that top -K highest support or covered area patterns cannot provide good compression gain since they are very redundant. Furthermore, if we only consider one pattern type, we cannot compress the data best since the compression gains of SmartCompo CS, Smart-Compo rGi and SmartCompo rGd are always lower than SmartCompo. This is because terns and not good compression gain. By propos ing overlapping allowed multi-pattern structure encoding scheme, we are able to extract more informative patterns. faloes is very diff cult to increase in a group and thus SmartCompo rGi is lower than the two other ones.

Running Time. In our best knowledge, there are no previous work which address mining compression movement pattern issue. Thus, we only compare the two proposed approaches in order to highlight the dif ferences between them. Running time of each algorithm is measured by repeating the experiment in compression gain experiment.
As expected, SmartCompo is much faster than NaiveCompo (i.e. Figure 8). By ex-sequently, the process eff ciency is speed up. Mining informativepatterns can be classifi d into 3 main lines: MDL-based approaches,
The idea of using data compression for data mining was fir t proposed by R. Cilibrasi who propose to use compressi bility as a measure of distance between two sequences. use swap randomization to generate random transactional data from the original data. A similar method is proposed for graph data by R. Milo et al. [11]. Another research MDL principle). Examples of this direction include the Krimp algorithm [14] and Slim algorithm [12] for itemset data and the algorithms for sequence data [9]. We have explored an MDL-based strategy to compress moving object data in order to: 1) select informative patterns, 2) combine different kinds of movement patterns with overlapping allowed. We supplied two algorithms NaiveCompo and SmartCompo. The latter one exploits smart properties to speed up the whole process obtaining the same proaches are able to compress data better than considering just one kind of patterns.
