 The past decades have witnessed the rapid development of academic resea rch, which results in a growing nu mber of scholarly papers. As a result, paper recommender systems have been proposed to help resea rchers find their interest ed papers. Most previous studies in paper recommendations mainly concentrate on pa per-paper or user-paper similar-ities without taking users X  reading purposes into accoun t. It is common that different users may prefe r to different aspects of a paper, e.g., the focused problem/task or the proposed solution. In this paper, we propose to satisfy user-specific reading purposes by recommending the most problem-related papers or solution-related papers to users separately. For a target paper, we use the paper citation graph to generate a set of potential relev ant papers. Once getting the cand idate set, we calculate the problem-based similarities and solution-based similarities between cand i-dates and the target paper through a concept based topic model, respectively. We evaluate our models on a real aca-demic paper dataset and our experimen ts show that our ap-proach outperform s a traditional similarity based model and can provide highly relev ant paper recommendations acco rd-ing to different reading purposes for resea rchers. H.3.1 [ INF ORMATION STOR AGE AND RETRIEVAL ]: Content Analysis and Indexing X  Lingu istic processing ; H.3.3 [ INFORMATION STO RAGE AND RETR IEVA L ]: Information Search and Retri eval X  Information filter ing Algo rithms,Experimen tation Content-Based Filtering, Paper Recommendation, Topic Model, Concept Terms
Researchers are searc hing academic pub lications that are related to their interests all the time. However, the rapid development in the volumes of resea rch papers leads to pub -lication overloa d. As a result, resea rchers have to spend more and more time on finding related papers. Informa-tion Retrie val (IR) techn olog y[11] are used to relieve the case to some extent, but sometimes users, especially junior resea rchers, may have no idea on ho w to c hoose appropri-ate queries fed into a search engine. On the other hand , Recommender System(RS) is designed to sugg est items for users by capturing their interests and need s. This motivates an emerg ing area, academic paper recommendation, which aims to help users find a set of pub lications that might be useful for their current researc h according to their profiles.
Some exist ing pub lication recommender systems recom-mend papers by calculating the relev ance between papers. They use content-based features to generate the paper rele-vance. However, the similarity between academic papers is not just like other similarities such as movie or commodity. In many cases, after reading an academic article, users prob-ably want to find more relat ed papers which solve the same problem or use the same solution. For example, when we are reading the article  X  X and om walk based enti ty ranking on graph for multidimensional recommendation X  X 8], we may be curious to know more abou t the multidimensional recom-mendation problem or the random walk model. For most existing systems, they recommend similar papers in only one list which means users need to distinguish the prob lem-related and solution-related papers by them selves.
To address this prob lem, we satisfy users  X  specific reading purposes by recommending the most problem-relevant pa-pers and solution-relevant papers to users separately. Due to the copyright issues, most paper recommender systems cann ot access full text information. In this case, we only use abstracts to analyze the focused problem and solution aspects discussed b efore. As we observed, the traditional abstract forma t includes three parts: a). They first intro-duce the background of their work and what problems they intend to solve. b). And then describe how they solved this prob lem and what specific meth od they finally used. c). Some abstracts also mention the experimen ts in the end. According to the traditional abstract style, we can fin d the transition p oint to split an ab stract into a problem part and a solution part. We then build LDA models on each of them, which are in turn utilize d to compu te the problem-based and solution-based similarities between the target and candidate papers.

In the rest of the paper, we first review related works on recommender syste m. In the next section, we detail our app roach on ho w to recommend academic papers via re-searchers X  different reading purposes. In Section 4, we de-scribe the experimental setup and discuss our resul ts in de-tail. We conclude this paper in Section 5 and discuss possible directions for future work.
Traditional recommender systems are dominated by col-laborative filtering based approach and content-based ap-proach.

Collaborative filtering[9] is one of the most successful rec-ommendation approaches which make automatic predictions about a user X  s interest s by collecting preferen ces or taste in-formation from similar users. This approach has been widel y so on. However, it suffers from the cold-start problem where a system cann ot generate accurate recommendations for new users or new items without enough initial rating history.
Content-based method[4] is also a popular choice in rec-ommender systems, where an item is suggested according to the similarity between user profile and this item. This method has been applied mostly in textual domains such as news recommendation and in hybrid approaches with col-laborative filtering[3].

Nitin et al[2] build a recommender system based on sub-space clustering approach. The idea is that resea rchers from the same area tend to be interested in the same article. Hence it is possible to improve searc h results by recommend-ing previous searc h queried by other people with similar in-terests. Since this system is built by collaborative filtering, it cann ot avoid the cold-start problem. We are alleviating this problem by recommending papers based on ab stract similarity, which can be used to recommend new articles.
Adomavicius et al.[1] find that since content-based sys-tems are designed most ly to recomme nd text-based items, the content in these systems is usually described with key-words. For example, a content-based component of the Fab system[3], which recommends Web pages to users, represen ts the page content with its 100 most important words. Basu et al.[5] model the task of assigning techn ical papers to confer-ence review ers as a problem of recomme nd ing techn ical pa-pers to the authors based on their interests and background . They show that their content-based retriev al methods can outperform the collaborative filtering methods. This moti-vates us to adopt the content-based paradigm for paper rec-ommendation, and use concepts to better capture the main content of an academic paper.

Chand rasekan et al.[7] present a concept-based approach which is similar to our work. Their system builds users pro-files based on the users X  previously pub lished papers and 1 ht tp://www .amazon.com/ compute the similarities between the user profile and the concept profiles of articles in the collec tion. Papers other than their own are then recom men ded to the users . This work has the shortcoming that it can only serve the authors who have pub lished papers before. Kodakateri et al. im-proved this work in[10]. They create user profiles based on the users  X  previously viewed papers rather than their au-thored papers, extending the recommendations to CiteS eer users as well as authors. Both Chand rasekan et al.[7] and Kodakateri et al.[10 ] are building concept profiles based on a predefin ed set of concepts, which is ACM X  X  Com pu ting Clas-sification System(CCS ). However the nu mber of concepts they used is limited, and these concepts are sometimes too general to fully distinguish different ideas. We utilize user tagg ed academic term s instead of CCS in our model.
Figure 1 shows the architecture of our system which mainly consists of four components: 1. Abstract Splitting: focuses on splitting abstracts into 2. Paper Similarity Model: extracts features from aca-3. Candidate Selection: selec ts cand idate papers given a 4. Paper Ranking: ranks the candidate papers given the Both Abstract Splitter and Paper Similarity Model are time-consuming procedures. They need to process every pub lica-tion in the academic paper dataset, which mean s we need to run the first t wo steps offline. The Cand idate Selection components narrows down the nu mber of related papers, and the Paper Ranking step only need to compu te the paper similarities within a small nu mber of candidate papers. The computing complexity of the last two parts is acceptable. As a result it can meet the requirement of online serving. Each of these modules will be explained in the following sections.
This module splits an abstract into two parts: problem description and solution description. It is observed that many researc hers first introduce the background and prob-lem definition in the abstract, and then describe the solu-tion they used. In order to validate this observation, we condu ct a pilot experime nt by randomly selecting 200 pa-pers from four international computer science conferen ces including SIGIR, SIGKDD , RecS ys and CIKM, and man-ually investiga te whether these papers contain a problem-solution abstract. The result shows that abou t 71% of the paper abstracts obey the observed rule and can b e spitted into two parts.

According to this convention, we can find a transition point or sentence in the abstract to perform the split. For example, when we see a sentence starting with  X  X n this pa-per X , it probably mean s that the following content tells us about the proposed solution. Based on these observations, we found that most transition sentences can b e matched with certain patterns and summarize them into six regular expression patterns to capture the transitions in an ab stract.
As we have successfully split an ab stract into two parts, we can get a problem content and a solution content for each abstract. We assign the problem content to the prob lem-based training corpus and solution content to the solution training corpus. We then build similarity models on these two corpora repectively.
The tf*idf weight is a nu merical statistic which reflects how important a word is to a document in a corpus. We use F i = ( w 1 , w 2 , ..., w k ) to repre sent the feature vector of paper i where w j is the tf*idf score of term j in i , Here we uses term importances to represe nt the paper con-tent.
Topic models are based upon the idea that a document may be viewed as a mixt ure of various topics, where a topic is a multinomial distribu tion over words. The latent dirich-let allo cation (LDA) model[6] is a generative model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. LDA estimates the topic-word distribution P(t | z ) and the document-topic distribu tion P(z | d) from an unlabeled cor-pu s using Diric hlet priors for the distribu tions with a fixed nu mber of topics. As a resul t, we get P(z | d ) for each docu-ment and further build the feature vecotr as This approach can find similar documents even if they share little common words.
Our TF*IDF and LDA models are generated up on the whole vocabu lary. However, for a paper abstract, academic topics are often repres ented by a group of techn ical terms such as Latent Dir ichl et Allocation . If we treat the three words individu ally, we will never know they are actually a model name. To alleviate this, we gule these academic term s into one word with  X   X  to form Latent Diric hl et Allocation . We leverage social tagging resou rces to define academic terms by extra cting users  X  tag ging information from citeulike 3 dataset. In citeulike, users label their interested papers using tags. We believe most tags in this dataset can b e used as academic concepts. We coun t all tags and filter out those appearin g less than 50. We match word sequences in the abstracts with the citeulike dataset vocabu lary, and merg e matched sequence into one academic concept. We then use this pro-cessed abstracts to build the LDA similarity model intro-duced in Section 3.2.2.
Considering the large size of a paper database, it is time consuming to compu te similarities between the target pa-per and all other papers. There are only a smal l portion of papers that relate to the target paper. We need to nar-row down the scope of cand idates. Fortunately, the citation graph of academic pub lications provides meaningful rela-tionship among papers. The fact that resea rchers cite other papers intend ing to tell us the referen ced papers are similar to their own work in some aspect. For a paper i , we rep-resen t its referen ce set as Ref i , its citation set as Cit define i  X  X  candidate set as: CandidateS et = Ref i  X  Cit i  X  ( [
After splitti ng abstracts and bu ilding similarity models on both problem and solution parts, we can obtain weighted vectors for each paper. The Candidate Selection step helps us reduce the nu mber of papers for ranking. We then calcu-late the cosine similarity between the target paper and each paper in cand idate list: sim = cos  X  = A B k A kk B k = where A and B are featu re vectors of two papers. We rank papers by their cosine similarities for both problem-and solution-related candidates. As a result, we get a problem related list and solution relat ed list for each paper.
We evaluate our models and compare with baselines in this section. tains 1.3 million pa pers with necessary meta data. But some of the abstracts and referenc es are missing. Our experiments are thus condu cted over the subset which contain b oth ab-stract and referen ce inform ation for each paper, coun ting 349,231 papers in tota l.

We randomly select 30 papers from the subset and make sure their abstracts can b e divided into two parts. For each paper, we recommend b oth most problem-relev ant and most 3 http:/ /www. citeu like.org/ NDC G@5 Problem Relevance Solution Relevance Model Name Problem All Solution All TF*IDF 0.83 68 0.5 9 28 0.4 9 10 0.7 9 85 LDA 0.80 12 0.7 0 68 0.8 3 36 0.7 1 76 LDA+Co ncept 0.84 03 0.8 3 20 0.8 867 0.7 3 24 MAP@5 Problem Relevance Solution Relevance Model Name Problem All Solution All TF*IDF 0.83 94 0.5 0 87 0.4 4 83 0.7 3 71 LDA 0.76 01 0.8 0 31 0.8 3 13 0.6 6 35
LDA+Co ncept 0.84 27 0.7 3 58 0.8 642 0.7 0 12 solution-relevant papers to users. Besides these two recom-mended lists, we also build baselines based on the similarity models that are trained by all abstracts instead of problem part or solution part. For problem/solution-relev ant recom-mendations, we ask volunteers to label whether the recom -mended article describes the same problem/solution with the target paper. The volu nteers are required to mark their choice with integer scores ranging from 0 to 3. Whil e 0 rep-resen ts no similarity and 3 repres ents highly relev ant. And for the baseline, they will make two scores for both problem and solution simil arities.
To properly acco un t for the correctness of top ranked doc-uments and the accuracy of recommendation, we employ the normalized discoun ted cumulative gain (NDCG ) and mean average precision (MA P).

Since users may just notice the top items, we concern mainly abou t whet her the top ranked papers are relev ant or not. Therefore, in this work, we use NDCG@N and MAP @N (N = 5) for evaluation where N is the nu mber of top-N papers recommended by our proposed approaches. Table 1 and table 2 are the resul ts on NDCG@ 5 and MAP@5, respectively. The Problem/ Solution in the sec-ond row means using problem/solution parts only to train models and All means using the whole abstract content. We set topic nu mber as 80 to train LDA and LDA+CON CEPT models.

According to the table, we can see that similarity models trained on problem/solution splits outperform those trained on full abstracts. In particular, the highest accuracy is ob-tained both on NDCG @5 and MAP@5 when we using con-cept based topic model. For short text, TF*IDF does not perform as well as on long text. In this case, topic models achieves better accuracy. Wha t X  X  more, introducing aca-demic concepts can help LDA und erstand papers X  gists bet-ter.
In this article, we describe a novel way to recom mend aca-demic papers to users from problem-related and solution-related aspects. And our experimen ts show that the con-cept based topic model meth od can achieve a higher ac-curacy. The key contributions of our work are threefold: 1) we argue that, beyond conventional similarity computa-tions, a better academic paper recommender syste m should take user X  X  different reading purposes into consideration and provide more focused recommendations accordingly. 2) We propose a topic model based LDA method to flexibly cap-ture the similarity between short texts given different read-ing purposes, which outperform s a traditional tf*idf meth od. 3) Our experimen ts prove that the academic concepts play an important role in conveying the main idea of an abstract, which is potentially crucial for context-based academic rec-ommendations.

Currently, in the Abstract Split step, we analyze the writ-ing patterns for abstracts and furth er extra ct temp late term s by empi rical observations. It would be of importance to au-tomatically classify the extra cted terms into problem-related or solution-related according to the context information and their positions. It is also necessary to extend our proposed model by further generating user profiles with user clic k-through history and predicting whet her a user prefers to problem-centered papers or method-centered ones. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] N. Agarwal, E. Haque, H. Liu, and L. Parsons. [3] M. Balabanovic and Y. Shoham. Fab: content-based, [4] C. Basu, H. Hirsh, and W. W. Cohen.
 [5] C. Basu, H. Hirsh, W. W. Cohen, and C. G.
 [6] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [7] K. Chandrasekaran, S. Gauch, P. Lakkaraju, and H. P. [8] W. Ding and G. Marchionini. A study on video [9] D. Goldb erg, D. A. Nichols, B. M. Oki, and D. B. [10] A. K. Pudhiyaveetil, S. Gauch, H. P. Luong, and [11] A. Singhal. Modern information retriev al: A brief [12] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su.
