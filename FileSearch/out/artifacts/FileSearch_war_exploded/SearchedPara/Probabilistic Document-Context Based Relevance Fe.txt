 This paper presents our novel relevance feedback (RF) algorithm that uses the probabilistic document-context based retrieval model with limited relevance judgmen ts for document re-ranking. Probabilities of the document-context based retrieval model are estimated from the top N (=20) documents in the initial retrieval. We use document-context based cosi ne similarity measure to find similar data for better probability estimation in order to reduce the data scarcity problem and the negative weighting problem. Our RF algorithm is promising because its mean average precision is statistically significantly better than the baseline using TREC-6 and TREC-7 data collections. Categories and Subject Descriptors H.3.3 [ Information Search and Retrieval ]: Relevance Feedback General Terms: Experimentation, Performance Keywords: Document-Context, Model Relevance feedback (RF) has been extensively studied in information retrieval (IR) in order to increase the retrieval effectiveness while requiring minima l user effort (e.g., [1-2]). In this paper, we propose our novel RF algorithm that modifies the existing probabilistic document-context based model proposed by Wu et al. [3] with a single iteration of RF and limited relevance. Many past RF algorithms focus on query expansion (e.g., [4]). By contrast, we do not expand the query. We use the contexts in the top 20 retrieved documents to estimate the probabilities of the document-context retrieval mode l for re-ranking the retrieved documents. There are also previ ous works on using RF for re-weighting query terms (e.g., [5]) . Model-based feedback using language modeling approaches was al so investigated [6]. Instead of using the whole document, we use contexts of query terms inside a document for RF. Alt hough using the contexts/windows in documents to explore term co-occurrence relationship is not new (e.g., [7]), as far as we know, modeling the contexts/windows explicitly in the retrieval model for model-based RF is novel. Let | d i | be the total number of terms in the i -th document d d [ k ] be the k -th term with k  X  [1, | d i |]. Let c ( d of the term d i [ k ] with size 2 n +1. The context c ( d the terms surrounding and including d i [ k ]. More specifically, c ( d contexts. We ignore contexts wher e the middle term is not a query they do not contain relevance information. A context score is computed for each remaining contexts. The context scores for the remaining contexts centered at the k -th position in the i -th document is denoted by w ( d i , k ). The maximum of the context scores is the document score for ranking similar to some passage-based retrieval [8]: Taking the maximum context scor e conforms to the Disjunctive Relevance Decision principle (see [9]) which states that a document is relevant to q if any document part is relevant. This is because taking the maximum is equivalent to a fuzzy disjunction. Given a query, let M r and M  X  r be the relevant and irrelevant models defined as the term dist ributions in the relevant and irrelevant sets, respectively. P (  X  | M r ) and P ( probabilities of term  X  given the relevant and irrelevant models, respectively. This is similar to the relevance-based language models [10], but we use contexts for constructing M (Sec. 3). Given M r and M  X  r , the context score is the log-odds [11]: We assume the linked-depe ndence assumption [12] for simplifying (1) and for avoiding the problem of data inconsistency pointed out by Cooper [12], as follows: M and M  X  r are initialized to be empty sets. The initial retrieval list is obtained by an automatic run. The relevance judgments of the top 20 documents from this initial retrieval list are obtained and they are used for training M r and M  X  r . Training here refers to constructing M r and M  X  r using the contexts in the top 20 retrieved documents. That is, M r contains the terms occurred in the contexts of the top 20 retrieved relevant documents and the weight of each term  X  is the occurrence frequency of  X  in the contexts of the top 20 retrieved relevant documents, similarly for M  X  r which uses the contexts of the top 20 retrieved irrelevant documents. When all top 20 documents from the initial retrieval are relevant/irrelevant, then there ar e singularity problems in (1) as number of relevant documents is small in the top 20 retrieved documents, the estimation of w ( d i , k ) may be biased to the irrelevant information obtained. This is the negative weighting problem [13]. In order to solve both problems, we expand M M  X  r by the retrieved documents not in the top 20 before document re-ranking. The contexts of thes e retrieved documents are ranked by the difference of their cosine similarity with M r and M where CosSim (.) is the cosine similarity defined as follows: where  X  is the dot product, term we ights are TF-IDF weights and Euclidean length is used. Similarity for M  X  r is defined similarly. After ranked the contexts not in the top 20 retrieved documents in descending order, the terms from the top 10% ranked contexts are added to M r and the terms from the bottom 10% ranked contexts are added to M  X  r (i.e., both M r and M  X  r are non-empty). A second retrieval is performed with the expanded M r and M document-context based model descri bed in Sec. 2. Note that our RF algorithm needs only the relevance judgments of the top 20 retrieved documents. This is less than 21% of all the relevance judgments averaged over per topic in TREC-6 and -7 (Table 1). The context size was determined em pirically. It should not be too small to exclude relevant information or too large to include irrelevant information. From pr evious experiments, we observed that the performances were similar over the range of context sizes from 71 to 101. For efficiency, the context size is set to 71. The experiment used the TREC-6 and -7 data collections with 50 title (short) queries each. The initial (baseline) retrieval used the BM11 term weighting of the 2-Po isson model [14] with passage-based retrieval and pseudo-releva nce feedback (PRF). Passage and PRF were used for higher precision at 20 (P@20) in the initial retrieval so that more relevant information is available for training. The relevance judgments of the top 20 documents are provided by TREC. For simplicity, Laplace smoothing was used to handle unseen terms in M r and M  X  r . It is difficult to compare our results with others X  RF (e.g., [15]) as the number of relevance judgments used may vary (e.g., [16]). So, we compare our RF results with those in the retrospective (RE) experiments by Wu et al. [3] where the retrieval model used full relevance judgments. By comparing with RE, we can carry out statistical significance tests of performance. Therefore, we did not evaluate based on rank freeze or based on the residue collection. Apart from using less relevance j udgments, the model for our RF experiments used the document-c ontexts for training whereas the model in Wu et al. [3] used the whole document for training. Table 1 shows the results of the initial baseline (BASE) retrieval, retrospective (RE) and RF r uns. Using Wilcoxon matched-pairs signed-ranks test, the MAPs of RF are statistically significantly better than BASE at 99.9% confidence interval (i.e., p  X  0.001) and do not show significant difference with RE (i.e., p  X  0.1). The results suggest that training using c ontexts is more preferable than using whole documents which furt her supports that the influence of terms is limited within their document-contexts. We presented a new single-itera tion RF algorithm using the top 20 documents from the initial retrieval. The MAPs of the proposed RF algorithm using the document-context based model are not statistically significantly different from a similar model with full relevance information. This suggests that the document-context based model works with limited relevance information. This work is supported by the CERG Project # PolyU 5226/05E. Robert thanks the CIIR, UMASS, for taking his leave there. [1] J.J. Rocchio, Relevance feedback in information retrieval, The [2] M. Iwayama, Relevance Feedback with a small Number of [3] H.C. Wu, R.W.P. Luk, K.F. W ong, K.L. Kwok and W.J. Li, A [4] R.H. Warren and T. Liu, A Re view of Relevance Feedback [5] C. Buckley and G. Salton, Optimization of Relevance Feedback [6] C. Zhai and J. Lafferty, Model-based Feedback in the Language [7] O. Vechtomova, S.E. Robertson and S. Jones, Query Expansion [8] J.P. Callan, Passage-based evidence in document retrieval, Proc. [9] Y.K. Kong, R.W.P. Luk, W. La m, K.S. Ho and F.L. Chung, [10] V. Lavrenko and W.B. Croft, Relevance based language models, [11] S.E. Robertson and K. Sparck Jones, Relevance Weighting of [12] W.S. Cooper, Some inconsiste ncies and misidentified modeling [13] D. Harman, Relevance Feedback Revisited, Proc. ACM SIGIR , [14] S.E. Robertson and S. Walker, Some Simple Effective [15] K. Sparck Jones, Summary Performance Comparisons TREC-2 [16] G.V. Cormack, C.L.A. Clarke, C.R. Palmer and S.S.L. To, 
