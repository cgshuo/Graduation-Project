 Recently many data types arising from data mining and Web search applications can be modeled as bipartite graphs. Ex-amples include queries and URLs in query logs, and authors and papers in scientific literature. However, one of the issues is that previous algorithms only consider the content and link information from one side of the bipartite graph. There is a lack of constraints to make sure the final relevance of the score propagation on the graph, as there are many noisy edges within the bipartite graph. In this paper, we propose a novel and general Co-HITS algorithm to incorporate the bipartite graph with the content information from both sides as well as the constraints of relevance. Moreover, we investi-gate the algorithm based on two frameworks, including the iterative and the regularization frameworks, and illustrate the generalized Co-HITS algorithm from different views. For the iterative framework, it contains HITS and personalized PageRank as special cases. In the regularization framework, we successfully build a connection with HITS, and develop a new cost function to consider the direct relationship between two entity sets, which leads to a significant improvement over the baseline method. To illustrate our methodology, we apply the Co-HITS algorithm, with many different set-tings, to the application of query suggestion by mining the AOL query log data. Experimental results demonstrate that CoRegu-0.5 (i.e., a model of the regularization framework) achieves the best performance with consistent and promising improvements.
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  retrieval models, search process ; H.2.8 [ Database Management ]: Database Applications X  data mining Algorithms, Performance, Experimentation Co-HITS, bipartite graphs, mutual reinforcement, score prop-agation, regularization
Bipartite graphs have been widely used to represent the relationship between two sets of entities (which we refer to as two kinds of data to avoid ambiguity) for Web search and data mining applications. The Web offers rich relational data which can be represented by bipartite graphs, such as queries and URLs in query logs, authors and papers in scien-tific literature, and reviewers and movies in a movie recom-mender system. Taking the query-URL bipartite graph as an example, although there is no direct edges between two queries, the edges of the bipartite graph between queries and URLs may lead to hidden edges within the query set as shown in Fig. 1. Previous work [5] shows that there is a natural random walk on the bipartite graph, which demon-strates certain advantages comparing with the traditional approaches based on the content information. Many link analysis methods have been proposed, such as HITS [12] and PageRank [4], to capture some semantic relations within the bipartite graph.

The problem we address is how to utilize and leverage both the graph and content information, so as to improve the precision of retrieved entities. One good example is the query suggestion by mining a query log, in which we have a query-URL bipartite graph, and the queries and URLs. In addition, the queries and URLs can be represented as term vectors with the content information. The objective of the query suggestion is to find semantically similar queries for the given query q . Traditionally, we can identify ini-tial similar queries based on the content information, then utilize HITS or personalized PageRank [10] for further mu-tual reinforcement on the bipartite graph. However, one of the issues is that there is a lack of constraints to make sure the final relevance of the score propagation on the graph, as there are many noisy edges within the bipartite graph. For example, let us consider the following two queries: map and Yahoo , where they may be co-linked by some URLs such as  X  X ww.yahoo.com X  ( Yahoo! ). As the general URL Yahoo! is associated with many queries, it can aggregate large rele-vance scores by the mutual reinforcement, which may prop-agate the score to the highly connected query Yahoo and lead to the high relevance score between map and Yahoo . In this case, if we consider the content information of the URL Yahoo! , the relevance score of the URL Yahoo! against the query map will be very low. Thus, when incorporating the Figure 1: Example of a bipartite graph. The edges between U and V are represented as the transition matrices W uv and W vu . Note that the dashed lines represent hidden links when considering the vertices in one side, where W uu and W vv denote the hidden transition matrices within U and V respectively. low relevance of the URL into the mutual reinforcement on the bipartite graph, the final relevance score between map and Yahoo would be constrained to a lower, but more rea-sonable score. In order to avoid the adverse effect of noisy data, we argue that the initial relevance scores, from both sides of the bipartite graph, provide valuable and reinforced information as well as the constraints of relevance, which should all be incorporated in a unified framework.
In this paper, we propose a novel and general algorithm, namely generalized Co-HITS, to incorporate the bipartite graph with the content information from both sides. Con-sequently, we investigate the following two frameworks, i.e., iterative framework and regularization framework, for the generalized Co-HITS algorithm from different views. The basic idea of the iterative framework is to propagate the scores on the bipartite graph via an iterative process with the constraints from both sides. The iterative framework contains HITS, personalized PageRank, and the one-step propagation algorithm as the special cases. Furthermore, we develop a joint regularization framework instead of the above iterative algorithm. In the regularization framework, we suc-cessfully build the connection with HITS, and develop a new cost function to consider the direct relationship between two entity sets, which leads to a significant improvement over the baseline method. To illustrate our methodology, we apply the generalized Co-HITS algorithm with different settings to the query suggestion task using the real-world AOL query log data [20]. Experimental results show that the CoRegu-0.5 (i.e., a model of the regularization framework) achieves the best performance, and its improvements are consistent and promising.

In a nutshell, our major contributions of this paper are: (1) the introduction of the generalized Co-HITS algorithm to incorporate the bipartite graph with the content infor-mation from both sides; (2) the investigation of two frame-works, including the iterative and the regularization frame-works, for the generalized Co-HITS algorithm from different perspectives; and (3) a new smoothness function in the reg-ularization framework to consider the direct relationship be-tween two entity sets as well as the smoothness within the same entity set, which leads to a significant improvement over the baseline method.

The rest of this paper is organized as follows. We first introduce the preliminaries in Section 2. In Section 3 we present the proposed Co-HITS algorithm, including the iter-ative framework and the regularization framework. Section 4 describes the application to bipartite graphs. We then de-scribe and report the experimental evaluation in Section 5, and briefly review some related work in Section 6. Finally, we present our conclusions and future work in Section 7.
Consider a bipartite graph G = ( U  X  V, E ), its vertices can be divided into two disjoint sets U and V such that each edge in E connects a vertex in U and one in V ; that is, there is no edge between two vertices in the same set. Let U = { u 1 , u 2 , ..., u m } and V = { v 1 , v 2 , ..., v two sets of m and n unique entities. Generally, a bipartite graph can be modeled as a weighted directed graph. Given i  X  U and j  X  V , if there is an edge connecting u i and v , the transition probabilities w uv ij and w vu ji are positive, where w uv ij denotes the transition probability from u i and w vu ji denotes the transition probability from v j to u otherwise, w uv ij = w vu ji = 0. Since the transition probability from state i to some state must be 1, we have and
For a bipartite graph, there is a natural random walk on the graph with the transition probability as shown in Fig. 1. Let W uv  X  R m  X  n denote the transition matrix from U to V , whose entry ( i, j ) contains a weight w uv ij from u i to v W vu  X  R n  X  m be the transition matrix from V to U , whose entry ( j, i ) contains a weight w vu ji from v j to u i . To consider the vertices in one side, such as the query-to-query graph in query logs, then a hidden transition probability w uu ij from u i to u j , corresponding to a dashed line in Fig. 1, can be introduced as: and
X Similarly, for the transition probability from v i to v j show that w vv ij = use W uu  X  R m  X  m and W vv  X  R n  X  n to denote the hidden transition matrices within U and V , respectively.
In addition to the graph information, each entity (such as a query or a document) may be represented as a term vector with its content information. For a given query q , the relevance scores of the entities can be calculated using a text relevance function f , such as the vector space model [1] and the statistical language model [23, 27]. The initial relevance scores x 0 i and y 0 j are respectively defined by x 0 i = f ( q, u and y 0 j = f ( q, v j ) for u i and v j .
Given a query q and the above information, the ultimate goal is to find a set of entities which are most relevant to the query q . The problem we address is how to utilize and leverage both the graph and content information, so as to Figure 2: Score propagation on the bipartite graph: (a) score y k is propagated to u i and u j , and (b) score x is propagated to v k . improve the precision of the results. In this section, we pro-pose a novel and general algorithm, namely generalized Co-HITS, to incorporate the bipartite graph with the content information from both sides.
The basic idea of our method is to propagate the scores on the bipartite graph via an iterative process. As shown in Fig. 2(a), the score y k of v k is propagated to u i according to the transition probability. Similarly, additional scores are propagated from other vertices of V to u i , then the score of u is updated to get a new value x i . In Fig. 2(b), it shows that the new value x i is propagated to v k . The intuition behind the score propagation is the mutual reinforcement to boost co-linked entities on the bipartite graph. In addition, the initial relevance scores based on the content informa-tion provide invaluable information, which should also be considered in the framework.

In order to incorporate the bipartite graph with the con-tent information, the generalized Co-HITS equations can be written as where  X  u  X  [0 , 1] and  X  v  X  [0 , 1] are the personalized param-eters, x 0 i and y 0 k are the initial scores for u i and v tively. In this model, the initial scores are normalized to be P operation, the sum of x i and the sum of y k will also be equal to 1 without further normalization. If only considering the vertices in one side, by substituting Eq. (4) for y k in Eq. (3), the generalized Co-HITS equation can be represented as the following The final scores of every entities can be obtained through an iteratively updating process. From our empirical testing, we find in most cases the equation can converge after about 10 iterations.

The proposed Co-HITS framework is general, and it con-tains a large algorithm space as shown in Table 1, in which HITS and personalized PageRank are actually two special cases in this space. If  X  u is set to be 0, the algorithm returns the initial scores as the baseline . If  X  u and  X  v are all equal to 1, Eq. (5) becomes the ordinary HITS equation, If one of the parameters  X  u and  X  v is set to be 1, it can be re-garded as the personalized PageRank (PPR) algorithm [10]. Suppose  X  v = 1, it becomes When  X  v is set to be 0, the algorithm becomes a general hybrid method which aggregates the initial scores X 0 and Y 0 as follows, which can be viewed as an one-step propagation algorithm.
Here we investigate a joint regularization framework for the above iterative framework. Let us first consider the ver-tices in one side, and imagine the personalized PageRank algorithm within the graph U as Eq. (7). For each itera-tion, every node receives the score from its neighbors (sec-ond term), and also retain its initial score (first term). The iteration process continues, and finally converges with the scores that are determined by their neighbors on the graph and their initial scores. A regularization framework can be developed for the personalized PageRank algorithm, by reg-ularizing the smoothness of relevance scores over the graph along with a regularizer on the initial ranking scores. The cost function R 1 , associated with U , is defined to be
R where  X  &gt; 0 is the regularization parameter, and D is a diagonal matrix with entries d ii = tion. Intuitively, the first term of the cost function defines the global consistency of the refined ranking scores over the graph, while the second term defines the constraint to fit the initial ranking scores, and the trade-off between each other can be controlled by the parameter  X  . When  X   X  +  X  , R 1 puts all weights on the second term, and the regularization framework boils down to the baseline which corresponds to  X   X  = 0 in Eq. (7). If  X  = 0, the regularization framework dis-cards the initial ranking scores, and only takes into account the global consistency on the graph, which corresponds to  X   X  = 1 in Eq. (7) (i.e., HITS as Eq. (6)). Similarly, for the cost function R 2 associated with V , we can show that The intuition behind this framework is the global consis-tency, i.e., similar entities are most likely to have similar relevance scores with respect to a query.

Until now, R 1 and R 2 have defined the consistency based on the hidden links within U and V individually. However, the direct links between U and V may have more significant effect on the score propagation and mutual reinforcement. In this paper, we investigate and develop a new cost function R 3 to consider the direct relationship between U and V : The intuition behind R 3 is the smoothness constraint be-tween two entity sets, which penalizes large differences in rel-evance scores for vertices between U and V that are strongly connected.
 Formally, the cost function R , associated with both U and V , is defined to be where  X  &gt; 0 and  X  r  X  [0 , 1]. By minimizing the cost func-tion R , we obtain the general regularization framework as-sociated with the general Co-HITS equation as Eq. (5). In this paper, we simply set  X  = 1 and focus on investigating the effect of parameter  X  r . Then the original optimization problem min F ( R ) can be rewritten as follows: min s.t. W = where X and Y are the score vectors for U and V respec-tively. Differentiating Eq. (12) [30, 32], we have where S = D  X  1 2 WD  X  1 2 , then Eq. (13) can be transformed into After simplifying, a closed-form solution can be derived, where I is an identity matrix. Note that  X   X  ranges from 0 to 1, and  X   X  +  X   X  = 1. In this paper, we consider the normalized Laplacian in [30], and S is positive-semidefinite. Details about how to calculate the matrix W and S will be introduced in Section 4.1. Given the initial ranking scores F 0 and the matrix S , we can compute the refined ranking scores F  X  directly.
  X  (0 , 1) = 1 Personalized PageRank as Eq. (7)  X  (0 , 1) = 0 One-step propagation as Eq. (8)  X  (0 , 1)  X  (0 , 1) General Co-HITS as Eq. (3)
In this section, we establish connections between the gen-eralized Co-HITS algorithm and other methods in Table 1. The iterative framework contains HITS, personalized PageR-ank, and the one-step propagation algorithm as the spe-cial cases. When looking at the regularization framework, its variations are controlled by the parameters  X   X  and  X  When  X   X  = 0 (  X   X  +  X  ), R puts all weights on the sec-ond term, and the regularization framework boils down to the baseline . If  X   X  = 1 (  X  = 0), the regularization frame-work discards the initial ranking scores, and only takes into account the global consistency on the graph, which corre-sponds to the HITS algorithm. Moreover, a different selec-tion of  X  r leads to a different smoothing strategy. If  X  it only considers the single-side regularization within U and V . If  X  r  X  (0 , 1), it utilizes the double-side regularization to make full use of the bipartite graph.

For the large-scale information retrieval, the matrix S is usually very large but sparse, which can be loaded in a rel-atively small storage space. However, the inverse matrix ( I  X   X   X  S )  X  1 will be very dense, and may need a huge space to save it. To balance the storage space and the computation time of the inverse matrix, we suggest to approximate the Eq. (15) in a specific subgraph with a submatrix  X  S , which consists of the top-n entities according to the initial ranking scores  X  F 0 . It can be found that the top ranking scores usu-ally outnumber the very low ranking scores. Theoretically, if the ranking scores after n are close to 0, the following approximate solution is equivalent to Eq. (15), In this equation, we eliminate the parameter  X   X  as it does not change the ranking. Accordingly, it needs to calculate the inverse matrix ( I  X   X   X   X  S )  X  1 online. Fortunately, the matrix is usually very sparse, then the complexity time of the sparse matrix inversion can be reduced to be linear with the number of nonzero matrix elements. In our experiments, we extract the top 5,000 entities for approximation.
To illustrate our proposed method, we use the statistical language model as the baseline to calculate the initial rele-vance scores based on the content information, and specify the application in query suggestion base on the query-URL bipartite graph. In this section we introduce the bipartite graph construction and the statistical language model, then show the overall algorithm of our framework.
Bipartite graphs are widely used to describe the relation-ship between queries U and URLs V when mining the query logs, such as query suggestion and classification. The edges of the query-URL bipartite graph can capture some seman-tic relations between queries and URLs. For each edge ( q , d j )  X  E we associate a numeric weight c ij , known as the click frequency , that measures the number of times the URL d j was clicked when shown in response to the query q . The transition probability w uv ij [5, 22] from the query q to the URL d j is defined by normalizing the click frequency from the query q i as w uv ij = c ij P probability w vu ji from the URL d j to the query q i is defined matrices W uv , W vu , W uu and W vv .

In practice, it is sometimes unnecessary to apply our learn-ing algorithms to a very large bipartite graph constructed from the entire collection. Since our task is to find the most relevant queries as suggestion for a given query, it would be more efficient to apply our algorithm only to a relatively compact query-URL bipartite graph that covers the relevant queries and related URLs. We utilize the same method used in [15] for building a compact query-URL bipartite graph and iteratively expanding it in the following, 1. Initialize a query set  X  U = U L (seed query set), and 2. Update  X  V to add the set of URLs that are connected 3. Update  X  U to be the set of queries that are connected 4. Iterate 2 and 3 until  X  U and  X  V reach a desired size; The final bipartite graph  X  G to which the algorithms are ap-plied consists of  X  U ,  X  V and edges  X  E connecting them. Ac-cording to the relevance scores, we initialize the top-10 rel-evant queries and top-10 relevant URLs as the seed sets. Generally, it only needs one iteration to reach 5,000 entities in our experiments. In this paper, we employ the widely used k -nearest neighbor ( k -NN) graph, where each node is connected to its k nearest neighbors under the transition probability measure and the edges can be weighed by the transition matrices. It has been shown to be effective when k = 10 in [7]. Then, the matrix  X  W is constructed with max-imum 50,000 (5 , 000  X  10) entries. After normalization, we can obtain the matrix  X  S . Fortunately, the matrix is usually very sparse, and the complexity time of the sparse matrix inversion can be reduced to be linear with the number of nonzero matrix elements.
Using language models for information retrieval has been studied extensively in recent years [23, 27, 28]. To determine the probability of a query given a document, we infer a doc-ument model  X  d for each document in a collection. With query q as input, retrieved documents are ranked based on the probability that the document X  X  language model would generate the terms of the query, p ( q |  X  d ). The ranking func-tion f 0 ( q, d ) can be written as where p ( t |  X  d ) is the maximum likelihood estimation of the term t in a document d , and n ( t, q ) is the number of times that term t occurs in query q . The likelihood of a query q consisting of a number of terms t for a document d un-der a language model with Jelinek-Mercer smoothing [28] is calculate the initial ranking scores of the documents with respect to a query.

In our proposed method, we employ the language model to determine the initial relevance scores F 0 for the queries and URLs. Note the queries from the query log are very short, but it still can be viewed as a document in the lan-guage model. We can get better initial relevance scores if we perform the query expansion and construct the document model with the expanded queries. For each URL, although its exact content information is not included in the query log, it can be represented as a document by the aggregation of connected queries [21].
 Algorithm 1 Generalized Co-HITS Algorithm Input: Given a query q and the bipartite graph Perform: 1. Calculate the initial ranking scores based on the sta-2. Expand and extract the compact bipartite subgraph 3. Get the weight matrix  X  W or  X  S , and normalize the cor-4. Solve Eq. (5) or Eq. (16) and get the final scores  X  F Output: Return the ranked queries
By unifying the Co-HITS algorithm in Section 3 and the application to bipartite graphs, we summarize the proposed algorithm in Algorithm 1. In the algorithm, note that we first perform preprocessing in a collection to construct the bipartite graph, and calculate the transition matrices. In the algorithm, we calculate the initial ranking scores using the language model, extract the compact bipartite subgraph, and perform the Co-HITS algorithm.

To implement the Co-HITS algorithm, we employ a sparse matrix package, i.e., CSparse [6], to solve the sparse matrix inversion efficiently. To deploy the efficient implementations of our scheme, all of the other algorithms used in the study are programmed in the C# language. We have implemented the language modeling approach to obtain the initial rele-vance scores with the Lucene.Net 1 package. For these ex-periments, the system indexes the collection and does to-kenization, stopping and stemming in the usual way. The http://incubator.apache.org/lucene.net/ testing hardware environment is on a Windows workstation with 3.0GHz CPU and 1GB physical memory.
In the following experiments we compare our proposed al-gorithm with other methods on the tasks of mining query logs through an empirical evaluation. We define the follow-ing task: Given a query and a query-URL bipartite graph, the system has to identify a list of queries which are most similar or semantically relevant to the given query. In the rest of this section, we introduce the data collection, the assessments and evaluation metrics, and present the exper-imental results.
The dataset that we study is adapted from the query log of AOL search engine [20]. The entire collection consists of 19 , 442 , 629 user click-through records. These records con-tain 10 , 154 , 742 unique queries and 1 , 632 , 789 unique URLs submitted from about 650 , 000 users over three months (from March to May 2006). As shown in Table 2, each record of the click contains the same information: UserID, Query, Time, Rank and ClickURL. This dataset is the raw data recorded by the search engine, and contains a lot of noises. Hence, we conduct a similar method employed in [26] to clean the raw data. We clean the data by removing the queries that ap-pear less than 2 times, and by combining the near-duplicated queries which have the same terms without the stopwords and punctuation marks (for example,  X  X oogle X  X  image X  and  X  X oogle image X  will be combined as the same query). After cleaning, our data collection consists of 883 , 913 queries and 967 , 174 URLs. After the construction of the click graph, we observe that a total of 4 , 900 , 387 edges exist, which in-dicates that each query has 5 . 54 distinct clicks, and each URL is clicked by 5 . 07 distinct queries. Moreover, taken as a whole, this data collection has 250 , 127 unique terms which appear in all the queries.
It is difficult to evaluate the quality of query similar-ity/relevance rankings due to the scarcity of data that can be examined publicly. For an automatic evaluation, we uti-lize the same method used in [2] to evaluate the similarity of retrieved queries, but engage the Google Directory 2 in-stead of the Open Directory Project 3 . When a user types a query in Google Directory, besides site matches, we can also find category matches in the form of paths between direc-tories. Moreover, these categories are ordered by relevance. For instance, the query  X  X nited States X  would provide the hierarchical category  X  X egional &gt; North America &gt; United States X , while one of the results for  X  X ational Parks X  would be  X  X egional &gt; North America &gt; United States &gt; Travel and Tourism &gt; National Parks and Monuments X . Hence, to measure how similar two queries are, we can use a notion of similarity between the corresponding categories provided by the search results of Google Directory. In particular, we measure the similarity between two categories Ca i and Ca as the length of their longest common prefix P ( Ca i , Ca divided by the length of the longest path between Ca i and Ca r . More precisely, the similarity is defined as: where | Ca i | denotes the length of a path. For instance, the similarity between the above two queries is 3/5 since they share the path X  X egional &gt; North America &gt; United States X  and the longest one is made of five directories. We evaluate the similarity between two queries by measuring the simi-larity between the aggregated categories of the two queries, among the top 5 answers provided by Google Directory.
To give a fair assessment, we randomly select 300 distinct queries from the data collection, then retrieve a list of sim-ilar queries using the proposed methods for each of these http://directory.google.com/ http://www.dmoz.org/ queries. For the evaluation of the task, we adopt the preci-sion at rank n to measure the relevance of the top n results of the retrieved list with respect to a given query q r , which is defined as where Sim ( q i , q r ) means the similarity between q i and q our experiments, we report the precision from P @1 to P @10, and take the average over all the 300 distinct queries.
We consider the question whether our proposed method can boost the performance using the generalized Co-HITS algorithm for query suggestion. First the experiments are performed to compare the iterative framework of Co-HITS with different parameters  X  u and  X  v . Then we examine the performance of the regularization framework by varying the parameters  X   X  and  X  r . Finally, we investigate and compare the detailed results of different methods, which shows that the regularization framework CoRegu-0.5 achieves the best results.
For the iterative framework, the generalized Co-HITS con-tains HITS, personalized PageRank (PPR), and the one-step propagation (OSP) algorithms as the special cases. In this subsection, we compare the performance of general Co-HITS (CoIter) with the above special cases, and report the preci-sions of P@5 and P@10 in Fig. 3.
 First of all, we evaluate the performance of personalized PageRank after setting  X  v = 1. Figure 3(a) illustrates the experimental results for different  X  u , in which the solid curves indicate the precisions of P@5 and P@10 for different param-eters, and the dashed curves denote the precisions for the baseline . We can see that the performance has only a slight increase when compared to the baseline if  X  u is set close to 0. With the increase of  X  u , the performance becomes worse, and even underperforms the baseline. It is because of the lack of relevance constraints from both sides of the bipartite graph, so the score propagation on the graph may be influ-enced easily due to some noise edges. When  X  u is equal to 1, it corresponds to the HITS algorithm that discards the initial relevance scores.

When  X  v = 0, the Co-HITS algorithm boils down to sim-ply aggregation of the initial scores from both sides. As shown in Fig. 3(b), we notice that the simple aggregation method (i.e., one-step propagation when  X  u is set from 0.1 to 0.9) benefits from both sides, and outperforms the method that only considers from one side. This observation supports the intuition of our Co-HITS algorithm that the initial rele-vance scores from both sides provide valuable and reinforced information as well as the constraints of relevance.
To illustrate the performance of general Co-HITS algo-rithm, we choose to set  X  u = 0 . 7 and vary the parameter  X  from 0 to 1, and then show the results in Fig. 3(c). From this figure, we can observe that its improvement over the baseline is promising when compared to the personalized PageRank, and it is comparable with the one-step propagation when  X  is set to be 0.4.
For the regularization framework, we first evaluate the single-sided regularization (SiRegu) by varying the parame-Figure 4: The effect of varying parameters (  X   X  and  X  ) in the regularization framework: (a) single-sided regularization, and (b) double-sided regularization. ter  X   X  , then we fix  X   X  and perform the double-sided regu-larization (CoRegu) with different  X  r .

As mentioned in Table 1, the parameter  X   X  is used to con-trol the balance between the global consistency and the ini-tial ranking scores in the unified regularization framework as Eq. (9), and it ranges from 0 to 1. The experimental results for the single-sided regularization are illustrated in Fig. 4(a). When  X   X  = 0, SiRegu boils down to the ini-tial baseline . We can see that the performance is improved over the baseline when incorporating the global consistency (  X   X  &gt; 0) in the framework. With the increase of  X   X  , the performance becomes better until it puts too much weight on the term of global consistency (  X   X   X  1). If  X   X   X  1, SiRegu discards the initial ranking scores, and only takes into account the global consistency on the graph. As shown in Fig. 4(a), when the parameter  X   X  is equal to 0.99, the performance of our method becomes worse than the initial baseline due to the overweighted global consistency. Ac-cording to the theoretical analysis in Section 3.2, SiRegu corresponds to the personalized PageRank in the iteration framework. By comparing Fig. 4(a) with Fig. 3(a), both re-sults are improved first and then degraded with the increase of  X   X  and  X  u , which shows that the parameters  X   X  and  X  have similar impact on SiRegu and PPR, respectively.
We have shown that SiRegu can improve the performance over the initial baseline, and achieves the best performance when  X   X  is set to be 0.1. Now we fix  X   X  = 0 . 1, and ex-amine whether CoRegu can further boost the performance by incorporating a direct smoothness constraint between two entity sets. According to Fig. 4(b), it is obvious that CoRegu (  X  r &lt; 1) performs better than SiRegu (  X  r = 1). The improvement over the SiRegu method owes to the direct Table 3: Comparison of different methods by P@5 and P@10. The mean precisions and the percentages of relative improvements are shown in the table.
 CoIter-0.4 0.7 0.4 0.388 ( 8.6%) 0.352 (11.2%) SiRegu-0.1 1 0.1 0.381 ( 6.5%) 0.343 ( 8.5%)
CoRegu-0.5 0.5 0.1 0.396 (10.8%) 0.357 (12.8%) smoothness constraint as Eq. (10) which is incorporated in the CoRegu framework. This observation supports the the-oretical analysis of the proposed regularization framework. Moreover, CoRegu is relatively robust and may achieve the best results when the parameter  X  r is set to be 0.2-0.6.
To gain a better insight into the proposed Co-HITS algo-rithm, we compare the best results of different models using P@5 and P@10 in Table 3. The mean precisions and the percentages of relative improvements over the baseline are shown in the table. A quick scan of the table reveals that CoRegu-0.5 achieves the best performance. When looking at the relative improvements of those models, we can see that CoRegu-0.5 improves over the baseline by 10 . 8% (for P@5) and 12 . 8% (for P@10) respectively, while CoIter-0.4 over the baseline by 8 . 6% and 11 . 2%. In addition, SiRegu-0.1 performs better than PPR-0.1. These results confirm that the regularization framework outperforms the iterative framework.

Figure 5 illustrates the precisions of six models from P@1 to P@10. In general, we can see that the performances of all the models, except the PPR-0.1, are better than the base-line. It is comparable for the precisions of OSP-0.7, CoIter-0.4 and SiRegu-0.1. The double-sided regularization model, i.e., CoRegu-0.5, achieves the best performance, whose im-provements are consistent. After looking into the details, one important observation is that the improvements of our method over the baseline are increased for larger n (of the evaluation matric P@n). This is because the mutual rein-forcement can boost the semantically relevant entities which have low initial scores. According to all the the experimental results, we can argue that it is very essential and promising to consider the double-sided regularization framework for the bipartite graph.
The work is related to the category of link analysis meth-ods. In [9], the authors have tried to model a unified frame-work for link analysis, which includes the two popular rank-ing algorithms HITS [12] and PageRank [4]. Several nor-malized ranking algorithms are studied which are intermedi-ate between HITS and PageRank. Our method differs from this unified framework as we integrate the graph information with the content information.

According to some generalization of PageRank and HITS, a family of work on the structural re-ranking paradigm over a graph was proposed to refine the initial ranking scores. Kurland and Lee performed re-ranking based on centrality within graphs, through PageRank-inspired algorithm [13] and HITS-style cluster-based approach [14]. Zhang et al. [29] proposed a similar method to improve Web search re-sults based on a linear combination of results from text search and authority ranking. In addition, PopRank [19] is developed to extend PageRank models to integrate het-erogenous relationships between objects. Another approach suggested by Minkov et al. [18] has been used to improve an initial ranking on graph walks in entity-relation networks. However, those methods does not make full use of the con-tent and graph information as they treat the content and graph information individually.

The regularization framework we proposed is closely re-lated to graph-based semi-supervised learning [32, 30, 25, 31], which usually assume label smoothness over the graph. Mei et al. [17] extend the graph harmonic function [32] to multiple classes. However, our work is different from theirs, as their tasks are mainly used in query-independent settings (i.e., semi-supervised classification, topic modeling), while we focus on query-dependent ranking problems. With the advance of machine learning, graph-based models have been widely and successively used in information retrieval and data mining. Diaz [8] use score regularization to adjust ad-hoc retrieval scores from an initial retrieval. Deng et al. [7] propose a method to learn a latent space graph from mul-tiple relationships between objects, and then regularize the smoothness of ranking scores over the latent graph. More re-cently, Qin et al. [24] use relational objects to enhance learn-ing to rank with parameterized regularization models. But those three methods only consider the regularization from one side of the bipartite graph or within a single graph, while our regularization framework takes into account not only the smoothness within the same entity set but also the direct relationship between two entity sets.

This work is also related to query log analysis [2], as we apply our Co-HITS algorithm to the application of query suggestion by mining the query logs. A common model for utilizing query logs from search engines is in the form of a query-URL bipartite graph (i.e., click graph) [5]. Based on the click graph, many research efforts in query log anal-ysis have been devoted to query clustering [3], query sug-gestion [11, 16] and query classification [15]. Craswell and Szummer [5] used click graph random walks for relevance rank in image search. Li et al. [15] presented the use of click graphs in improving query intent classifiers. In this work, we combine the click graph with the content information from queries and URLs to improve the precisions of the results, which differs from the previous methods.
In this paper we have presented the generalized Co-HITS algorithm for bipartite graphs, whose basic idea is to in-corporate the bipartite graph with the content information from both sides. We not only formally define the iterative framework, but also investigate the regularization frame-work for the generalized Co-HITS algorithm from different views. For the iterative framework, it has been shown that HITS, personalized PageRank, and the one-step propaga-tion algorithm are special cases of the generalized Co-HITS algorithm. In the regularization framework, we successfully build the connection with HITS, and develop a new cost function to consider the direct relationship between two en-tity sets, which leads to a significant improvement over the baseline method. We have applied the proposed algorithm to mine the query log and compare with many different set-tings. Experimental results show that the improvements of our proposed model are consistent, and CoRegu-0.5 achieves the best performance. In future work, it would be interesting to investigate the performance of our Co-HITS algorithm in other bipartite graphs to see if the proposed method might have an impact on any bipartite graphs.
This work described in this paper is supported by grants from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. CUHK4128/08E and Project No. CUHK4158/08E). This work is also affili-ated with the Microsoft-CUHK Joint Laboratory for Human-Centric Computing and Interface Technologies. [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern [2] R. A. Baeza-Yates and A. Tiberi. Extracting semantic [3] D. Beeferman and A. L. Berger. Agglomerative [4] S. Brin and L. Page. The anatomy of a large-scale [5] N. Craswell and M. Szummer. Random walks on the [6] T. Davis. Direct Methods for Sparse Linear Systems . [7] H. Deng, M. R. Lyu, and I. King. Effective latent [8] F. Diaz. Regularizing ad hoc retrieval scores. In [9] C. H. Q. Ding, X. He, P. Husbands, H. Zha, and H. D. [10] T. Haveliwala, S. Kamvar, and G. Jeh. An analytical [11] R. Jones, B. Rey, O. Madani, and W. Greiner. [12] J. Kleinberg. Authoritative sources in a hyperlinked [13] O. Kurland and L. Lee. Pagerank without hyperlinks: [14] O. Kurland and L. Lee. Respect my authority!: Hits [15] X. Li, Y.-Y. Wang, and A. Acero. Learning query [16] H. Ma, H. Yang, I. King, and M. R. Lyu. Learning [17] Q. Mei, D. Cai, D. Zhang, and C. Zhai. Topic [18] E. Minkov, W. W. Cohen, and A. Y. Ng. Contextual [19] Z. Nie, Y. Zhang, J.-R. Wen, and W.-Y. Ma.
 [20] G. Pass, A. Chowdhury, and C. Torgeson. A picture of [21] B. Poblete and R. A. Baeza-Yates. Query-sets: using [22] B. Poblete, C. Castillo, and A. Gionis. Dr. searcher [23] J. M. Ponte and W. B. Croft. A language modeling [24] T. Qin, T.-Y. Liu, et al. Learning to rank relational [25] A. Smola and R. Kondor. Kernels and regularization [26] X. Wang and C. Zhai. Learn from web search logs to [27] C. Zhai and J. D. Lafferty. Two-stage language models [28] C. Zhai and J. D. Lafferty. A study of smoothing [29] B. Zhang, H. Li, Y. Liu, L. Ji, W. Xi, W. Fan, [30] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and [31] D. Zhou, B. Sch  X  olkopf, and T. Hofmann.
 [32] X. Zhu, Z. Ghahramani, and J. D. Lafferty.

