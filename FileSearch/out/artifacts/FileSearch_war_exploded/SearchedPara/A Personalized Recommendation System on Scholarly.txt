 Researchers, as well as ordinary users who seek information in diverse academic fields, turn to the web to search for publications of interest. Even though scholarly publication recommenders have been developed to facilitate the task of discovering literature pertinent to their users, they (i) are not personalized enough to meet users X  expectations, since they provide the same suggestions to users sharing similar profiles/preferences, (ii) generate recommendations pertain-ing to each user X  X  general interests as opposed to the specific need of the user, and (iii) fail to take full advantages of valu-able user-generated data at social websites that can enhance their performance. To address these problems, we propose PubRec , a recommender that suggests closely-related refer-ences to a particular publication P tailored to a specific user U , which minimizes the ti me and efforts imposed on U in browsing through general recommended publications. Em-pirical studies conducted using data extracted from CiteU-Like (i) verify the efficiency of the recommendation and rank-ing strategies adopted by PubRec and (ii) show that PubRec significantly outperforms other baseline recommenders. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information Filtering Algorithms
Researchers, as well as ordinary users, who seek materials in diverse academic fields, can turn to customized search en-gines, such as Google Scholar or ACM Portal, to locate pub-lications of interest. These tools, however, are designed to retrieve articles matching the information need specified by auserinakeywordqueryandareinadequateforperforming personalized and contextual searches [11]. Scholarly publica-tion recommenders [4] are context-dependent and thus infer users X  interest to suggest articles that potentially match the preferences of a user. Although these recommenders have been thoroughly studied over the past decade, a shortcoming of their design methodology is the X  X ne-size-fits-all X  premise. Due to their assumption on group preference, which leads to the suggestion of same publications to users sharing sim-ilar profiles regardless of the individual interest of the users, their recommendations are not personalized enough [7]. In addition, these recommenders provide suggestions pertain-ing to the users X  general interests, which unlikely match the instant need of an individual user and yield extraneous rec-ommendations the user is required to browse through.
With the increasing popularity of social websites, such as Delicious(.com) and CiteULike(.org), which archive user-generated data (i.e., social connections and tags), a new era of research on advancing the design of recommender sys-tems is emerging [10]. Even though recently-developed rec-ommenders consider the rich resource of information avail-able on social websites, to the best of our knowledge, none of them generate personalized recommendations on schol-arly publications . For this reason, we introduce PubRec ,a personalized recommender that addresses the shortcomings of traditional recommenders and takes advantage of users X  data archived on CiteULike to enhance the quality of its recommendations on scholarly publications. The proposed recommender facilitates the process of discovering academic citations related to a particular publication to provide rel-evant references to its users, a common inquire conducted very frequently within the academic setting. While existing recommenders suggest publications for a user U basedonthe (contents of) publications included in U  X  X  profile/personal li-brary, PubRec recommends articles tailored to the specific information need of U captured on a particular publication P . Based on the premise that U treasures recommendations made by people with whom (s)he has an explicit connection, PubRec recommends articles among the publications book-marked by U  X  X  connections on CiteULike that are relevant (in content) to P to a certain degree.
PubRec extracts data from CiteULike to generate person-alized recommendations of scientific references, i.e., schol-arly publications or simply publications, for CiteULike users. Given a CiteULike user Cusr and a publication P (that Cusr is interested in), PubRec first identifies Cusr  X  X  con-nections . Hereafter, using word-correlation factors, PubRec determines the set of publications, denoted CandidateP , among the ones included in the personal libraries of Cusr  X  X  connections that are similar in content to P to a certain degree. PubRec computes a ranking score for each publi-cation in CandidateP and recommends the Top-10 publica-tions with the highest ranking scores to Cusr . The process of PubRec is illustrated in Figure 1.
CiteULike, which is one of the leading social web sys-tems developed for managing and sharing bibliographic ref-erences, includes 5,237,158 indexed articles (as of May 24, 2011) and allows its users to search, organize, and share publications. A CiteULike user X  X  personal library includes a number of bibliographic references that the user has book-marked. Besides bookmarking publications and maintaining their metadata, abstracts, and links to the publishers X  web-sites, CiteULike users can add personal comments and tags to publications in their personal libraries [4]. Each publica-tion P indexed in CiteULike is associated with a list of tags that are provided by the CiteULike users who have book-marked P . The list is used by PubRec to infer the tag cloud of P , i.e., a global visual representation of the tags and their frequencies assigned to P . CiteULike users can also assign a reading priority to a publication using a 5-star rating scale. In this rating system, one star (five stars, respectively) is as-sociated with the label  X  X  don X  X  really want to read it X  ( X  X op priority! X , respectively), which indicates that the user is not eager to read (very interested in reading, respectively) the corresponding publication.

Serving as a social website, CiteULike offers its users an infrastructure to establish explicit communication chan-nels with other CiteULike users. Explicitly-connected users, called connections in CiteULike, can exchange private mes-sages and share bibliographic references of interest with one another. (To find out all the social features offered by Ci-teULike, see wiki.citeulike.org/index.php/Social Features.)
PubRec relies on the pre-computed word-correlation fac-tors in the word-correlation matrix [8] to determine the simi-larity between any two tags assigned to their respective pub-lications, which capture and represent their contents.
Each correlation factor, which was calculated using a set of approximately 880,000 Wikipedia(.org) documents, in-dicates the degree of similarity of the two corresponding words 1 basedontheir(i) frequency of co-occurrence and (ii) relative distances in each Wikipedia document.
Words in the Wikipedia documents were stemmed af-
As the number of publications bookmarked by Cusr  X  X  connections can be large, it is inefficient to compare each publication with P , a publication of interest to Cusr ,to identify the ones similar to P to be recommended to Cusr , since the comparisons significantly prolong the processing time of PubRec . To minimize the number of comparisons and thus reduce the processing time required in generating recommendations, PubRec applies a filtering strategy on articles included in the personal libraries of Cusr  X  X  connec-tions to generate a subset of articles, denoted CandidateP , to be considered for recommendation. Each publication in CandidateP contains at least one tag exactly matching or highly similar to one of the tags of P assigned by Cusr .As publications in CandidateP and P share same (or analo-gous) tags, PubRec expects they are similar (to a degree) in content and address the same or similar topic.

To identify highly similar tags, PubRec employs a re-duced version of the word-correlation matrix (introduced in Section 2.2) which contains 13% of the most frequently-occurring words (based on their frequencies of occurrence in the Wikipedia documents), and for the remaining 87% of the less-frequently-occurring words only the exact-matched correlation factor, i.e., 1.0, is used.
PubRec ranks each publication CP in CandidateP to pri-oritize them for recommendation using (i) the degree of sim-ilarity between P and CP , (ii) the number of connections who include CP in their personal libraries, and (iii) the ad-justed rating (or simply rate )scoregivento CP by each connection of Cusr who has bookmarked CP . To determine the degree of similarity between P and CP , PubRec adds the word-correlation factors between each tag in the tag cloud of P and CP in CiteULike, respectively. We consider the tags in the tag cloud of P ( CP , respectively), since these tags provide a more comprehensive description of (the content of) P ( CP , respectively), as opposed to the personal tags assigned to P ( CP , respectively), which only reflect the personal opinion of and vocabulary used by Cusr (one of Cusr  X  X  connections, respectively) in describing (the content of) P ( CP , respectively). Besides computing the (content) similarity between P and CP , PubRec considers the popularity of CP , which denotes the number of Cusr  X  X  connections who include CP in their personal libraries.

Publications that have attracted the attention of a num-ber of Cusr  X  X  connections are more likely bookmarked in the personal libraries of Cusr  X  X  connections. PubRec weights the fact that publications frequently-bookmarked by Cusr  X  X  connections may also be of interest to Cusr ,since Cusr and his/her connections share common interests to a certain de-gree. While solely relying on the popularity of an item in performing the recommendations task (which does not ap-ter all the stopwords , such as articles and prepositions, which do not play a significant role in representing the content of a document, were removed. From now on, un-less stated otherwise, (key)words/tags refer to non-stop, stemmed (key)words/tags. ply to PubRec ) can lead to less diverse, i.e., less person-alized, recommendations, Adomavicius and Kwon [1] claim that the accuracy of the recommendations can be enhanced by considering the popularity of an item during the recom-mendation process.
PubRec considers the number of stars given by Cusr  X  X  connections to each publication in CandidateP and assign higher weight to those given high ratings by the connections, since users tend to favor highly-rated items more than the ones assigned lower ratings [5].

A connection, who includes in his/her library a significant number of articles on the same (or similar to the) topic T of P , is interested in T , which can be interpreted as the connection is likely more knowledgeable on or familiar with T and thus is more reliable in rating articles on T .Based on this premise, if the rating R of a candidate publication CP is assigned by the connection who is familiar with the topic of P , which is the same or similar to the topic of CP , then R should be perceived as more trustworthy than the rating assigned to CP by a connection who is less interested in or familiar with the topic of P . PubRec adjusts the rat-ing provided by each Cusr  X  X  connection, denoted Ccon ,on CP based on the reliability of Ccon in assigning ratings to publications on the topic related to the one addressed in P . Having determined the (i) degree of similarity between P and each CP in CandidateP , (ii) the popularity score of CP , and (iii) the adjusted rating score assigned to CP by each Cusr  X  X  connection, PubRec computes the ranking score of CP by adopting CombMNZ [9], a popular linear combination strategy . CombMNZ combines multiple exist-ing lists of rankings on an item into a joint ranking ,atask known as rank aggregation or data fusion [9].

The rank aggregation strategy adopted by PubRec ac-counts for the fact that not all of the publications are as-signed a non-zero score in each (input) ranked list of pub-lications. By adopting CombMNZ, PubRec considers the strength of each evidence, i.e., scores in each of the ranked lists, as opposed to simply positioning higher in the ranking publications with a non-zero score in all of the ranked lists of publications, regardless of the values of the corresponding scores in each list. As a result, PubRec can position higher in the ranking of generated recommendations a candidate publication CP that is more closely related (in content) to P , highly-popular among Cusr  X  X  connections, and without a rating score than a publication that is less similar to P , less popular, and poorly-rated.
In this section, we introduce the dataset, evaluation proto-col, and metrics (in Sections 3.1, 3.2, and 3.3, respectively) which are used for assessing the performance of PubRec . We detail the empirical study conducted for evaluating the effectiveness of PubRec and compare its performance with existing baseline recommenders (in Section 3.4).
We constructed a dataset with data extracted from Ci-teULike. We first identified users who have recently book-marked articles on CiteULike and randomly selected a set of fifty of them 2 , denoted  X  X ctive users. X  For each active user U we extracted U  X  X  connections. Thereafter, we retrieved the personal tags and ratings of each article in U  X  X  personal library (posted under the personal libraries of U  X  X  connec-tions, respectively). The dataset also includes the set of tags (along with their frequencies of occurrence) in the (inferred) tag cloud of each scholarly publication bookmarked by ei-ther an active user or one of his/her connections. The resul-tant dataset includes 183 distinct users (fifty of which are active users and the remaining ones are their connections), 103,723 distinct scho larly publications, and 35,034 distinct tags. Since, as previously stated, PubRec generates person-alized recommendations for an active user on a particular publication, we evaluate PubRec based on the recommenda-tions generated for each of the 21,867 user-publication pairs, called Test Pairs , in the constructed dataset.
We measure the overall performance of PubRec ,usingthe metrics to be introduced in Section 3.3, on the recommen-dations generated for each user( U )-publication( P )pairin Test Pairs .Asa ground truth , i.e., the (non-)relevance of a recommendation R generated by PubRec for a U -P pair, we rely on the publications bookmarked by U on CiteULike. R is relevant if it is included in U  X  X  personal library (exclud-ing P )andis non-relevant , otherwise, a commonly-employed protocol for assessing recommendation systems [3, 4].
We treat PubRec as a content retrieval system that rec-ommends to its users a list of ten publications relevant to a submitted query, a publication in our case, which is a commonly-adopted evaluation strategy [3, 4], and apply Pre-cision@K , Mean Reciprocal Rank ,and Normalized Discounted Cumulative Gain [6] to evaluate the recommendation accu-racy of PubRec , in addition to its ranking strategy.
To demonstrate and verify the effectiveness of our pro-posed recommender, we comp are the performance of PubRec with two well-known, widely-adopted baseline recommender systems: SocialRecommender ( SR ) [3] and TagVectorSimi -larity ( TVS ) [2]. While the former adopts a collaborative filtering strategy, the later is a content-based recommender.
Given that PubRec and SR recommend publications in-cluded in the personal libraries of active users X  connections to make recommendations, we restricted the publications to be considered for recommendations by TVS to those book-marked by active users X  connections and thus conduct a fair, i.e., comparable, assessment among the recommenders.
Prior to comparing the aforementioned recommenders, we determined the relevance of each publication recommended by SR ( TVS , respectively) for each user-publication pair in Test Pairs according to the evaluation protocol detailed in Section 3.2 and the metrics introduced in Section 3.3. Note that since only publications existing in a user X  X  library are considered relevant, it is not possible to account for po-tentially relevant publications that the user has not book-marked. Thus, the computed precision scores are underes-timated, a well-known limitation of the protocol introduced in Section 3.2. As this limitation affects PubRec , SR ,and
CiteULike users lacking explicit connections were excluded. Figure 2: The P @1 , P @10 , MRR ,and NDCG scores of SR , TVS ,and PubRec , respectively TVS , the precision values are consistent for comparative purposes [3].

As shown in Figure 2, the P @1 scores achieved by SR and TVS are 0.15 and 0.23, respectively, which are at least 36% lower than the P @1 score of PubRec . Also shown in Figure 2 are the P @10 scores of SR , TVS ,and PubRec , which are 0.11, 0.18, and 0.45 respectively. The P @1 scores indicate that more than 1 2 (close to 1 7 and 1 4 , respectively) of the time, the first publication recommended by PubRec ( SR and TVS , respectively) is relevant. In addition, the P @10 values show that close to half of the scholarly publi-cations recommended by PubRec are relevant, as opposed to the approximately one-tenth (one-fifth, respectively) rec-ommended by SR ( TVS , respectively).
 Figure 2 also shows the MRR scores of SR , TVS ,and PubRec . These scores reflect that while on the average PubRec users are required to browse through less than two (  X  = 1 0 . 69 =1.44 &lt; 2) recommended publications before locat-ing one that is related to a scholarly publication that (s)he is interested in, users relying on SR and TVS are required to scan through at least four (  X  = 1 0 . 24 = 4.16) and three ( 3.03) recommended publications, respectively before locat-ing one that is of interest.

The NDCG scores calculated for the evaluated recom-menders are also shown in Figure 2. The NDCG score of PubRec , which is 0.72, is over 40% higher than the NDCG scores computed for SR or TVS , which are 0.29 and 0.30, respectively. A significantly higher NDCG value indicates that PubRec is notably more effective than SR and TVS in ranking higher in the list of recommendations scholarly publications that are relevant.
We have introduced a personalized publication recom-mender, denoted PubRec , which is simple and requires nei-ther supervision nor domain-specific information in gener-ating recommendations. PubRec is based on the premise that a user U values recommendations made by people with whom U has an explicit connection, a design methodology that differs from existing recommenders which suggest rec-ommended items inferred from users unknown to U .Recom-mendations suggested by PubRec are personalized , since it considers the personal preference of U to make suggestions, instead of providing the same recommendations to users who share the same or similar profile information/common interests. Furthermore, while existing user -centric recom-menders provide recommendations pertaining to U  X  X  general interests, PubRec recommends articles relevant to a particu-lar publication P given U . The unique design methodologies employed by PubRec facilitate the process of identifying aca-demic publications relevant to an article A of interest to an individual user who conducts a search of references for A ,a task performed in the academic setting on a regular basis.
To assess the performance of PubRec and existing base-line recommenders, we have conducted an empirical study using data extracted from CiteULike. The study has demon-strated (i) the effectiveness of the recommendation and rank-ing strategies adopted by PubRec and (ii) the superiority of PubRec over baseline recommenders. The results of the conducted experiments verify that by considering (i) collab-orative annotations, i.e., tags and ratings, extracted from a social website, along with (ii) connections established among the users in a social environment we can enhance the quality of recommendations on publications.

While PubRec is currently applied for recommending schol-arly publications, we intent to further enhance our proposed recommender so that it can simultaneously suggest any num-ber of multimedia items, i.e., songs or movies, provided that collaborative data describing items of interest and explicit connections among users can be extracted from a social net-working environment. [1] G. Adomavicius and Y. Kwon. Improving Aggregate [2] C. Basu, H. Hirsh, W. Cohen, and C. Nevill-Manning. [3] A. Bellogin, I. Cantador, and P. Castells. A Study of [4] T. Bogers and A. van den Bosch. Recommending [5] P. Cremonesi, Y. Koren, and R. Turrin. Performance [6] W. Croft, D. Metzler, and T. Strohman. Search [7] J.Jung,K.Kim,H.Lee,andS.Park.AreYou [8] J. Koberstein and Y.-K. Ng. Using Word Clusters to [9] J. Lee. Analyses of Multiple Evidence Combination. In [10] H. Ma, D. Zhou, C. Liu, M. Lyu, and I. King. [11] R.White, P. Bailey, and L. Chen. Predicting User
