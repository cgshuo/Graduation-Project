 This paper addresses several key issues in the ArnetMiner system, which aims at extracting and mining academic social networks. Specifically, the system focuses on: 1) Extracting researcher pro-files automatically from the Web; 2) Integrating the publication data into the network from existing digital libraries; 3) Modeling the entire academic network; and 4) Providing search services for the academic network. So far, 448,470 researcher profiles have been extracted using a unified tagging approach. We integrate pub-lications from online Web databases and propose a probabilistic framework to deal with the name ambiguity problem. Further-more, we propose a unified modeling approach to simultaneously model topical aspects of papers, authors, and publication venues. Search services such as expertise search and people association search have been provided based on the modeling results. In this paper, we describe the architecture and main features of the system. We also present the empirical evaluation of the proposed methods. H.3.3 [ Information Search and Retrieval ]: Text Mining, Digital Libraries; H.2.8 [ Database Management ]: Database Applications Algorithms, Experimentation Social Network, Information Extraction, Name Disambiguation, Topic Modeling, Expertise Search, Association Search
Extraction and mining of academic social networks aims at pro-viding comprehensive services in the scientific research field. In an academic social network, people are not only interested in search-ing for different types of information (such as authors, conferences, and papers), but are also interested in finding semantics-based in-formation (such as structured researcher profiles).

Many issues in academic social networks have been investigated and several systems have been developed (e.g., DBLP, CiteSeer, and Google Scholar). However, the issues were usually studied separately and the methods proposed are not sufficient for mining the entire academic network. Two reasons are as follows: 1) Lack of semantics-based information. The social information obtained from user-entered profiles or by extraction using heuristics is some-times incomplete or inconsistent; 2) Lack of a unified approach to efficiently model the academic network. Previously, different types of information in the academic network were modeled individually, thus dependencies between them cannot be captured accurately.
In this paper, we try to address the two challenges in novel ap-proaches. We have developed an academic search system, called ArnetMiner ( http://www.arnetminer.org ). Our objective in this system is to answer the following questions: 1) how to au-tomatically extract researcher profiles from the Web? 2) how to integrate the extracted information (e.g., researchers X  profiles and publications) from different sources? 3) how to model different types of information in a unified approach? and 4) how to provide powerful search services based on the constructed network? (1) We extend the Friend-Of-A-Friend (FOAF) ontology [9] as the profile schema and propose a unified approach based on Condi-tional Random Fields to extract researcher profiles from the Web. (2) We integrate the extracted researcher profiles and the crawled publication data from the online digital libraries. We propose a uni-fied probabilistic framework for dealing with the name ambiguity problem in the integration. (3) We propose three generative probabilistic models for simul-taneously modeling topical aspects of papers, authors, and publica-tion venues. (4) Based on the modeling results, we implement several search services such as expertise search and association search. We conducted empirical evaluations of the proposed methods. Experimental results show that our proposed methods significantly outperform the baseline methods for dealing with the above issues.
Our contributions in this paper include: (1) a proposal of a uni-fied tagging approach to researcher profile extraction, (2) a pro-posal of a unified probabilistic framework to name disambiguation, and (3) a proposal of three probabilistic topic models to simultane-ously model the different types of information.

The paper is organized as follows. In Section 2, we review the related work. In Section 3, we give an overview of the system. In Section 4, we present our approach to researcher profiling. In Section 5, we describe the probabilistic framework to name disam-biguation. In Section 6, we propose three generative probabilistic models to model the academic network. Section 7 illustrates sev-eral search services provided in ArnetMiner based on the modeling results. We conclude the paper in Section 8.
Several research efforts have been made for extracting person profiles. For example, Yu et al. [32] propose a two-stage extraction method for identifying personal information from resumes. The first stage segments a resume into different types of blocks and the second stage extracts the detailed information such as Address and Email from the identified blocks. However, the method formalizes the profile extraction as several separate steps and conducts extrac-tion in a more or less ad-hoc manner.

A few efforts also have been placed on the extraction of contact information from emails or from the Web. For example, Kristjans-son et al. [19] have developed an interactive information extraction system to assist the user to populate a contact database from emails. In comparison, profile extraction consists of contact information extraction as well as other different subtasks.
A number of approaches have been proposed to name disam-biguation. For example, Bekkerman and McCallum [6] present two unsupervised methods to distinguish Web pages to different per-sons with the same name: one is based on the link structure of the Web pages and the other is based on the textural content. However, the methods cannot incorporate the relationships between data. Han et al. [15] propose an unsupervised learning approach using K-way spectral clustering. Tan et al. [27] propose a method for name disambiguation based on hierarchical clustering. However, this kind of methods cannot capture the relationships either.
Two supervised methods are proposed by Han et al. [14]. For each given name, the methods learn a specific classification model from the training data and use the model to predict whether a new paper is authored by a specific author with the name. However, the methods are user-dependent. It is impractical to train thousands of models for all individuals in a large digital library.
Considerable work has been conducted for investigating topic models or latent semantic structures for text mining. For example, Hofmann [17] proposes the probabilistic latent semantic indexing (pLSI) and applies it to information retrieval (IR).
 Blei et al. [8] introduce a three-level Bayesian network, called Latent Dirichlet Allocation (LDA). The basic generative process of LDA closely resembles pLSI except that in pLSI, the topic mixture is conditioned on each document while in LDA, the topic mixture is drawn from a conjugate Dirichlet prior that remains the same for all documents.

Some other work has been conducted for modeling both author interests and document contents together. For example, the Author model [21] is aimed at modeling the author interests with a one-to-one correspondence between topics and authors. The Author-Topic model [25] [26] integrates the authorship into the topic model and can find a topic mixture over documents and authors.

Compared with the previous topic modeling work, in this pa-per, we propose a unified topic model to simultaneously model the topical aspects of different types of information in the academic network.
For academic search, several research issues have been inten-sively investigated, for example expert finding and association search.
Expert finding is one of the most important issues for mining so-cial networks. For example, both Nie et al. [24] and Balog et al. [4] propose extended language models to address the expert finding problem. From 2005, Text REtrieval Conference (TREC) has pro-vided a platform with the Enterprise Search Track for researchers to empirically assess their methods for expert finding [13]. Association search aims at finding connections between people. For example, the ReferralWeb [18] system helps people search and explore social networks on the Web. Adamic and Adar [1] have investigated the problem of association search in email networks. However, existing work mainly focuses on how to find connections between people and ignores how to rank the found associations.
In addition, a few systems have been developed for academic search such as, scholar.google.com, libra.msra.cn, citeseer.ist.psu, and Rexa.info. Though much work has been performed, to the best of our knowledge, the issues we focus on in this work (i.e., profile extraction, name disambiguation, and academic network modeling) have not been sufficiently investigated. Our system addresses all these problems holistically.
Figure 1 shows the architecture of our ArnetMiner system. The system mainly consists of five main components: 1. Extraction : it focuses on extracting researcher profiles from 2. Integration : it integrates the extracted researchers X  profiles 3. Storage and Access : it provides storage and index for the ex-4. Modeling : it utilizes a generative probabilistic model to si-5. Search Services : based on the modeling results, it provides It is challenging in many ways to implement these components. First, the previous extraction work has been usually conducted on a specific data set. It is not immediately clear whether such methods can be directly adapted to the global Web. Secondly, it is unclear how to deal with the disambiguation problem by making full use of the extracted information. For example, how to use the rela-tionships between publications. Thirdly, there is no existing model that can simultaneously model the different types of information in the academic network. Finally, different strategies for modeling the academic network have different behaviors. It is necessary to study how different they are and which one would be the best for academic search.

Based on these considerations, for profile extraction, name dis-ambiguation, and modeling, we propose new approaches to over-come the drawbacks that exist in the traditional methods. For stor-age and access, we utilize the classical methods, because these is-sues have been intensively investigated and the existing methods can result in good performance in our system.
Profile extraction is the process of extacting the value of each property in a person profile. We define the schema of the researcher profile (as shown in Figure 2) by extending the FOAF ontology [9].
We perform a statistical study on randomly selected 1 , 000 re-searchers from ArnetMiner and find that it is non-trivial to perform profile extraction from the Web. We observed that 85 . 62% of the researchers are faculty members from universities and 14 . 38% are from company research centers. For researchers from the same company, they may share a template-based homepage. However, different companies have different templates. For researchers from universities, the layout and the content of their homepages vary largely. We have also found that 71 . 88% of the 1 , 000 Web pages are researchers X  homepages and the rest are pages introducing the researchers. Characteristics of the two types of pages significantly differ from each other.

We also analyze the content of the Web pages and find that about 40% of the profile properties are presented in tables/lists and the others are presented in natural language text. This suggests a method without using global context information in the page would be in-effective. Statistical study also unveils that (strong) dependencies exist between different profile properties. For example, there are 1 , 325 cases ( 14 . 54% ) in our data of which the extraction needs to use the extraction results of other properties. An ideal method should consider processing all the subtasks holistically.
The proposed approach consists of three steps: relevant page identification, preprocessing, and extraction. In relevant page iden-tification, given a researcher name, we first get a list of web pages by a search engine (we use the Google API) and then identify the homepage/introducing page using a binary classifier. We use Sup-port Vector Machines (SVM) [12] as the classification model and define features such as whether the title of the page contains the person name and whether the URL address (partly) contains the person name. The performance of the classifier is 92 . 39% by F1-measure. In preprocessing, (a) we separate the text into tokens and (b) we assign possible tags to each token. The tokens form the ba-sic units and the pages form the sequences of units in the tagging problem. In tagging, given a sequence of units, we determine the most likely corresponding sequence of tags by using a trained tag-ging model. Each tag corresponds to a property defined in Figure 2, e.g.,  X  X osition X . In this paper, we make use of Conditional Ran-dom Fields (CRFs) [20] as the tagging model. Next we describe the steps (a) and (b) in detail. (a) We identify tokens in the Web page using heuristics. We define five types of tokens:  X  X tandard word X ,  X  X pecial word X ,  X &lt;im-age&gt; X  token, term, and punctuation mark. Standard words are un-igram words in natural language. Special words include email, URL, date, number, percentage, words containing special terms (e.g.  X  X h.D. X  and  X .NET X ), special symbols (e.g.  X === X  and  X ### X ). We identify special words by using regular expressions.  X &lt;image&gt; X  tokens (used for identifying person photos and email addresses) are  X &lt;image&gt; X  tags in the HTML file. Terms are base noun phrases ex-tracted from the Web page by using a tool based on technologies proposed in [30]. (b) We assign tags to each token based on the token type. For ex-ample, for a standard word, we assign all possible tags correspond-ing to all properties. For a special word, we assign tags indicating Position, Affiliation, Email, Address, Phone, Fax, Bsdate, Msdate, and Phddate. For a  X &lt;image&gt; X  token, we assign two tags: Photo and Email, because an email address is sometimes shown as an image).
After each token is assigned with several possible tags, we can perform most of the profiling tasks using the tags (extracting 19 properties defined in Figure 2).
We employ Conditional Random Fields (CRF) as the tagging model. CRF is a conditional probability of a sequence of tags given a sequence of observations [20]. For tagging, a trained CRF model is used to find the sequence of tags Y  X  having the highest like-lihood Y  X  = max Y P ( Y | X ) . The CRF model is built with the labeled data by means of an iterative algorithm based on Maximum Likelihood Estimation.

Three types of features were defined in the CRF model: con-tent features, pattern features, and term features. The features were defined for different kinds of tokens. Table 1 shows the defined fea-tures. We incorporate the defined features into the CRF model by defining Boolean-valued feature functions. Finally, 108,409 fea-tures were used in our experiments.
For evaluating our profiling method, we randomly chose 1 , 000 researcher names in total from our researcher network. We used the method described in Section 4.2.1 to find the researchers X  home-pages or introducing pages. If the method cannot find a Web page for a researcher, we removed the researcher name from the data set. We finally obtained 898 Web pages (one for each researcher). Seven human annotators conducted annotation on the Web pages. A spec was created to guide the annotation process. On disagree-ments in the annotation, we conducted  X  X ajority voting X . In the experiments, we conducted evaluations in terms of precision, re-call, and F1-measure for each profile property.

We defined baselines for profile extraction. We used the rule learning and the classification based approaches as baselines. For the former, we employed the Amilcare tool, which is based on a rule induction algorithm: LP 2 [11]. For the latter, we trained a classifier to identify the value of each property. We employed Sup-port Vector Machines (SVM) [12] as the classification model.
Experimental results show that our method results in a perfor-mance of 83.37% in terms of average F1-measure; while Amilcare and SVM result in 53.44% and 73.57%, respectively. Our method clearly outperforms the two baseline methods. We have also found that the performance of the unified method decreases (  X  11 . 28% by F1) when removing the transition features, which indicates that a unified approach is necessary for researcher profiling.

We investigated the contribution of each feature type in profile extraction. We employed only content features, content+term fea-tures, content+pattern features, and all features to train the models and conducted the profile extraction. Figure 3 shows the average F1-scores of profile extraction with different feature types. The results unveil contributions of individual features in the extraction. We see that solely using one type of features cannot obtain accurate profiling results. Detailed evaluations can be found in [28].
We integrate the publication data from the online database in-cluding DBLP bibliography, ACM Digital library, CiteSeer, and others. For integrating the researcher profiles and the publications, we use the researcher name and the publication author name as the identifier. The method inevitably has the ambiguity problem.
We give a formal definition of the name disambiguation task in our context. Given a person name a , we denote all publications having the author name a as P = { p 1 , p 2 ,  X  X  X  , p n } . Each publi-cation p i has six attributes: paper title ( p i .title ), publication venue ( p .pubvenue ), published year ( p i .year ), abstract ( p i authors ( { a (0) i , a (1) i ,  X  X  X  , a ( u ) i } ), and references ( p
For the authors of a paper { a (0) i , a (1) i ,  X  X  X  , a author name we are going to disambiguate as the principal author (denoted as a (0) i ) and the others secondary authors. Suppose there are k actual researchers having the name a , our task is then to assign papers with the author a to their actual researcher y h , h  X  [1 , k ] . We define five types of relationships between papers (Table 2). Relationship r 1 represents two papers are published at the same venue. Relationship r 2 means two papers have a secondary author with the same name, and relationship r 3 means one paper cites the other paper. Relationship r 4 indicates a constraint-based relation-ship supplied via user feedback. For instance, the user can specify that two specific papers should be assigned to a same person. We use an example to explain relationship r 5 . Suppose p i  X  X avid Mitchell X  and  X  X ndrew Mark X , and p j has authors  X  X avid Mitchell X  and  X  X ernando Mulford X . We are to disambiguate  X  X avid Mitchell X . If  X  X ndrew Mark X  and  X  X ernando Mulford X  also coauthor a paper, then we say p i and p j have a 2-CoAuthor relationship. In our currently experiments, we empirically set the weights of rela-tionships w 1  X  w 5 as 0 . 2 , 0 . 7 , 0 . 3 , 1 . 0 , 0 . 7
The publication data with relationships can be modeled as a graph comprising of nodes and edges. Each attribute of a paper is attached to the corresponding node as a feature vector. In the vector, we use words (after stop words filtering and stemming) in the attributes as features and use their numbers of occurrences as the values. We propose a probabilistic framework based on Hidden Markov Random Fields (HMRF) [5], which can capture dependencies be-tween observations (with each paper being viewed as an observa-tion). The disambiguation problem is cast as assigning a tag to each paper with each tag representing an actual researcher.

Specifically, we define a-posteriori probability as the objective function. We aims at maximizing the objective function. The five types of relationships are incorporated into the objective function. According to HMRF, the conditional distribution of the researcher labels y given the observations x (papers) is where D ( x i , y h ) is the distance between paper x i and researcher y and D ( x i , x j ) is the distance between papers x i and x denotes a relationship between x i and x j ; w k is the weight of the relationship; and Z is a normalization factor.
Three tasks are executed by the Expectation Maximization method: estimation of parameters in the distance measure, re-assignment of papers to researchers, and update of researcher representatives y We define the distance function D ( x i , x j ) as follows: here A is defined as a diagonal matrix, for simplicity. Each element in A denotes the weight of the corresponding feature in x .
The EM process can be summarized as follows: in the E-step, given the researcher representatives, each paper is assigned to a researcher by maximizing P ( y | x ) . In the M-step, the researcher representative y h is re-estimated from the assignments, and the dis-tance measure is updated to maximize the objective function again.
In the E-step, assignments of data points to researchers are up-dated to maximize P ( y | x ) . A greedy algorithm is used to sequen-tially assign each paper x i to its new assignment y h ( h  X  [1 , k ] ) that minimizes the function (equivalently to maximize P ( y
The assignment of a paper is performed while keeping assign-ments of the other papers fixed. The assignment process is re-peated after all papers are assigned. This process runs until no paper changes its assignment between two successive iterations.
In the M-step, each researcher representative is updated by the arithmetic mean of its points: y h =
Then, each parameter a mm in A is updated by (only parameters on the diagonal): a mm = a mm +  X 
To evaluate our method, we created a data set that consists of 14 real person names (six are from the author X  X  lab and the others are from [31]). Statistics of this data set are shown in Table 3. Five human annotators conducted disambiguation for the names. A spec was created to guide the annotation process. The labeling work was carried out based on authors X  affiliations, emails, and publications on their homepages.

We defined a baseline based on the method from [27] except that [27] also utilizes a search engine to help the disambiguation. The method is based on hierarchical clustering. We also compared our approach with the DISTINCT method [31]. In all experiments, we suppose that the number of persons k is provided empirically.
Table 4 shows the results. We see that our method significantly outperforms the baseline method for name disambiguation ( +10 . 75% in terms of the average F1-score). The baseline method suffers from two disadvantages: 1) it cannot take advantage of relation-ships between papers and 2) it relies on a fixed distance measure. Figure 4 shows the comparison results of our method and DIS-TINCT [31]. We used the person names evaluated in both [31] and our experiments for comparison. We see that for some names, our approach significantly outperforms DISTINCT (e.g.,  X  X ichael Wagner X ); while for other names our approach underperforms DIS-TINCT (e.g.  X  X in Yu X ).
 We further investigated the contribution of each relationship type. We first removed all relationships and then added them to our ap-proach one by one: CoPubvenue, Citation, CoAuthor, and  X  -CoAuthor. At each step, we evaluated the performance of our approach (cf. Figure 5). We see that without using the relationships the dis-ambiguation performance drops sharply (  X  44 . 72% by F1) and by adding the relationships, improvements can be obtained at each step. This confirms us that a framework by integrating relationships for name disambiguation is worthwhile and each defined relation-ship in our method is helpful. We can also see that the CoAuthor relationship is the major contributor ( +24 . 38% by F1).
Modeling the academic network is critical to any searching or suggesting tasks. Traditionally, information is usually represented based on the  X  X ag of words X  (BOW) assumption. The method tends to be overly specific in terms of matching words.
Recently, probabilistic topic models such as probabilistic La-tent Semantic Indexing (pLSI) [17], Latent Dirichlet Allocation (LDA) [8], and Author-Topic model [25] [26] have been proposed as well as successfully applied to multiple text mining tasks such as information retrieval [29], collaborative filtering [8] [16], and paper reviewer finding [22]. However, these models are not sufficient to model the whole academic network, as they cannot model topical aspects of all types of information in the academic network.
We propose a unified topic model for simultaneously modeling the topical distribution of papers, authors, and conferences. For simplicity, we use conference to denote conference, journal, and book hereafter. The learned topic distribution can be used to fur-ther estimate the inter-dependencies between different types of in-formation, e.g., the closeness between a conference and an author.
The notations used are summarized as follows. A paper d is a vector w d of N d words, in which each w di is chosen from a vo-cabulary of size V ; a vector a d of A d authors, chosen from a set of authors of size A ; and a published conference c d . A collection of D papers is defined by D = { ( w 1 , a 1 , c 1 ) ,  X  X  X  , ( w denotes an author, chosen from a d , responsible for the i -th word w di in paper d . The number of topics is denoted as T .
The proposed model is called Author-Conference-Topic (ACT) model. Three different strategies are employed to implement the topic model (as shown in Figure 6).

In the first model (ACT1, Figure 6 (a)), each author is associ-ated with a multinomial distribution over topics and each word in a paper and the conference stamp is generated from a sampled topic.
In the second model (ACT2, Figure 6 (b)), each author-conference pair is associated with a multinomial distribution over topics and each word is then generated from a sampled topic.

In the third model (ACT3, Figure 6 (c)), each author is associated with a topic distribution and the conference stamp is generated after topics have been sampled for all word tokens in a paper.
The different implementations reduces the process of writing a scientific paper to different series of probabilistic steps. They have different behaviors in the academic applications. In the remainder of this section, we will describe the three models in more detail.
In the first model (Figure 6(a)), the conference information is viewed as a stamp associated with each word in a paper. Intuition behind the first model is: coauthors of a paper determine topics written in this paper and each topic then generates the words and determines a proportion of the publication venue. The generative process can be summarized as follows: 1. For each topic z , draw  X  z and  X  z respectively from Dirichlet 2. For each word w di in paper d :
Following [26], we choose Gibbs sampling for inference. As for the hyperparameters  X  ,  X  , and  X  , for simplicity, we take a fixed value (i.e.,  X  = 50 /T ,  X  = 0 . 01 , and  X  = 0 . 1 ). In the Gibbs sampling procedure, we first estimate the posterior distribution on just x and z and then use the results to infer  X  ,  X  , and  X  . The posterior probability is calculated by the following: where the superscript  X  di denotes a quantity, excluding the current instance (e.g., the di -th word token in the d -th paper).
After Gibbs sampling, the probability of a word given a topic  X  , the probability of a conference given a topic  X  , and the probability of a topic given an author  X  can be estimated as follows:
In the second model (cf. Figure 6(b)), each topic is chosen from a multinomial topic distribution specific to an author-conference pair, instead of an author as that in ACT1. The model is derived from the observation: when writing a paper, coauthors usually first choose a publication venue and then write the paper based on themes of the publication venue and interests of the authors. The corresponding generative process is: 1. For each topic z , draw  X  z from Dirichlet priors  X  z ; 2. For each word w di in ppaer d :
Similarly, we can calculate the posterior conditional probability using a Gibbs sampling procedure analogous to that in ACT1.
In the third model (cf. Figure 6(c)), the conference stamp is taken as a numerical value. Each conference stamp of a paper is chosen after topics have been sampled for all word tokens in the paper. Intuitively, this corresponds to a natural way of publishing the scientific paper: authors first write a paper and then determine where to publish the paper based on the topics discussed in the paper. The corresponding generative process is: 1. For each topic z , draw  X  z from Dirichlet priors  X  z ; 2. For each word w di in paper d : 3. Draw a conference c d from z 1: N d using a normal linear model In this model, the conference comes from a normal linear model. The covariates  X  in this model are the frequencies of the topics in the document. The regression coefficients on these frequencies constitute  X  . The difference of parameterization from ACT1 is that the conference stamp is sampled from a normal linear distribution after topics were sampled for all word tokens in a paper. For inference in ACT3, there is a slight difference from that in ACT1 and ACT2, as we also need to estimate the parameters  X  and  X  . We use a Gibbs EM algorithm [2] for inference of this model.
In the E-step, for sampling the topic, the posterior probability is calculated by where
In the M-step, given the sampled topics z , the optimal  X  and  X  Specifically,  X  is updated by and  X  2 is updated by where E [ . ] is the expectation of the variables; A is a D  X  T matrix. The d -th row of the matrix is E [  X  ] =  X   X  := (1 /N d ) E [ A &gt; A ] = defined as: with diag {  X  di } denoting a matrix with diagonal as the vector of  X  di . Note that  X  di denotes a vector of probabilities of topics gen-erating word w di . We omit details of derivation of Equations (12) and (13). Interested reader is referred to [7] and [23].
In expertise search, the objective is to find the expertise authors, expertise papers, and expertise conferences for a given query.
Based on the proposed models, we can calculate the likelihood of a paper generating a word using ACT1 as the example as following:
The likelihood of an author model and a conference model gen-erating a word can be similarly defined. However, the learned top-ics by the LDA-style model is usually general and not specific to a given query. Therefore, only using ACT itself is too coarse for aca-demic search [29]. Our preliminary experiments also show that em-ploying only ACT or LDA models to information retrieval hurts the retrieval performance. In general, we would like to have a balance between generality and specificity. Therefore, we derive a combi-nation of the ACT model and the word-based language model: where P LM ( w | d ) is the generating probability of word w from pa-per d by the language model. It is defined as: where tf ( w, d ) is the frequency of word w in d , tf ( w, D ) is the frequency of word w in the collection D , and N D is the number of word tokens in the collection D .  X  is the Dirichlet prior and is commonly set based on the average paper length in the collection. Finally, given a query q , P ( q | d ) can be computed by P ( q | d ) =  X  P ( q | c ) for conferences.
We collected a list of the most frequent queries from the log of ArnetMiner for evaluation. We conducted experiments on a subset of the data (including 14 , 134 persons, 10 , 716 papers, and 1 , 434 conferences) from ArnetMiner. For evaluation, we used the method of pooled relevance judgments [10] together with human judgments. Specifically, for each query, we first pooled the top 30 results from three similar systems (Libra, Rexa, and ArnetMiner). Then, two faculty members and five graduate students from CS provided human judgments. Four-grade scores (3, 2, 1, and 0) were assigned respectively representing definite expertise, exper-tise, marginal expertise, and no expertise. Finally, the judgment scores were averaged to obtain the final score.
 In all experiments, we conducted evaluation in terms of P@5, P@10, P@20, R-pre, and mean average precision (MAP) [10] [13].
We used language model (LM), LDA [8], and the Author-Topic (AT) model [25] [26] as the baseline methods. For language model, we used Equation (17) to calculate the relevance between a query term and a document and similar equations for an author/conference (an author is represented by his/her published papers and a confer-ence is represented by papers published on it). For LDA, we used a similar equation to Equation (16) to calculate the relevance of a term and a document. For the AT model, we used similar equations to Equation (16) to calculate the relevance of a query term with a paper or an author. For the LDA and AT models, we performed model estimation with the same setting as that for the ACT models. We empirically set the number of topics as T = 80 for all models. Table 6: Performance of six expertise search approaches ( % ). Table 5 shows five topics discovered by ACT1.

Table 6 shows the experimental results of retrieving papers, au-thors, and conferences using our proposed methods and the base-line methods. We see that our proposed three methods outperform the baseline methods. LDA only models documents and thus can support only paper search; while AT supports paper search and au-thor search. Both models underperform our proposed unified mod-els. Our models benefit from the ability of modeling all kinds of in-formation holistically, thus can capture the dependencies between the different types of information. We can also see that ACT1 achieves the best performance in all evaluation measures.
For comparison purposes, we also evaluate the results of two similar systems: Libra.msra.cn and Rexa.info. The average MAP obtained by Libra and Rexa on our data set are 48 . 3% and 45 . 0% . We see that our methods clearly outperform the two systems. Association Search : Given a social network G = ( V, E ) and an as-sociation query ( a i , a j ) (source person, target person), association search is to find and rank possible associations {  X  k ( a a to a j . Each association is denoted as a referral chain of persons.
There are two subtasks in association search: finding possible as-sociations between two persons and ranking the associations. Given a large social network, to find all associations is an NP-hard prob-lem. We instead focus on finding the  X  X hortest X  associations. Hence, the problem becomes how to estimate the score of an association and one key issue is how to calculate the distance between persons. We use KL divergence to define the distance as:
We use the accumulated distance between persons on an associ-ation path as the score of the association. We call the association with the smallest score as the shortest association and our problem can be formalized as that of finding the near-shortest associations. Our approach consists of two stages: 1) Shortest association finding. It aims at finding shortest asso-ciations from all persons a  X  V \ a j in the network to the target person a j (the score of the shortest association from a i noted as L min &gt; 0 ). We use a heap based Dijkstra algorithm to find the shortest associations. 2) Near-shortest associations finding. Based on the shortest asso-ciation score L min and a parameter  X  , the algorithm uses a depth-first search to find associations whose scores are less than (1 +  X  ) L min . We constrain the length of an association to be less than a pre-defined threshold. Finally, the obtained associations are ranked according to the scores.

Our approach can find the near-shortest associations for a query in less than 3 seconds on a commodity machine with a network of researchers. In the following, we list two associations ranked by our approach for the query ( X  X ang Li X ,  X  X iang Yang X ).
Our model can support many other applications, e.g., author in-terest finding and academic suggestion.

For example, Table 7 shows top 5 words and top 5 authors as-sociated to two conferences found by ACT1. Table 8 shows top 5 words and top 5 conferences associated to two researchers found by ACT1. The results can be directly used to characterize the con-ference themes and researcher interests. They can be also used for prediction/suggestion tasks. For example, one can use the model to find the best matching reviewer for a paper submitted to a spe-cific conference. Previously, such work is fulfilled by only keyword matching or topic-based retrieval such as [22], but not considering Table 7: Top 5 representative words and top 5 authors associ-ated to two conferences found by ACT1.
 Table 8: Top 5 representative words and top 5 conferences associated to two researchers found by ACT1.
 the conference. One can also use the model to suggest a venue to submit a paper based on its content and authors X  interests. Or one can use it to suggest popular topics when authors prepare a paper for a conference.
In this paper, we describe the architecture and the main fea-tures of the ArnetMiner system. Specifically, we propose a uni-fied tagging approach to researcher profiling. About a half million researcher profiles have been extracted into the system. The sys-tem has also integrated more than one million papers. We propose a probabilistic framework to deal with the name ambiguity prob-lem in the integration. We further propose a unified topic model to simultaneously model the different types of information in the academic network. The modeling results have been applied to ex-pertise search and association search. We conduct experiments for evaluating each of the proposed approaches. Experimental results indicate that the proposed methods can achieve a high performance.
There are many potential future directions of this work. It would be interesting to further investigate new extraction models for im-proving the accuracy of profile extraction. It would be also interest-ing to investigate how to determine the actual person number k for name disambiguation. Currently, the number is supplied manually, which is not practical for all author names. In addition, extending the topic model with link information (e.g., citation information) or time information is a promising direction.
The work is supported by the National Natural Science Founda-tion of China (90604025, 60703059), Chinese National Key Foun-dation Research and Development Plan (2007CB310803), and Chi-nese Young Faculty Research Funding (20070003093). It is also supported by IBM Innovation funding.
