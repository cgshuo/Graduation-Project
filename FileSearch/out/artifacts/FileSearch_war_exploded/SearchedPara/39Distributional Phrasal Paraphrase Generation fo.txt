 Paraphrase generation, or paraphrasing , is defined as finding alternative phrasing (words, phrases, sentences, etc.) to convey the same X  X r nearly the same X  X eaning as that of a given word or phrasing. Hereafter the paraphrased phrasing is called the focal phrasing. For example, given the focal word deal , good paraphrases might be agreement , or accord , but not sky ; given the focal phrase to provide any , good paraphrases might be to give any ,or to give further , but not yellow submarine . Paraphrase generation serves various Natural Language Processing (NLP) applications, such as Natural Language Generation (NLG), summarization, Information Retrieval (IR), Question Answering (QA), and Statistical Machine Translation (SMT) [Madnani and Dorr 2010; Androut-sopoulos and Malakasiotis 2010]. This work focuses on paraphrasing for SMT, for which it is useful because it increases translation coverage. Untranslated words and phrases, and bad reordering of known words and phrases in unseen larger sequences, remain a major problem [Callison-Burch et al. 2006] for phrase-based SMT systems, flat and hierarchical alike [Koehn et al. 2003, 2007; Koehn 2004a; Chiang 2005, 2007, inter alia ], in spite of much progress since statistical word-based translation models were introduced [Brown et al. 1993]. This article is based on parts of the author X  X  doctoral dissertation and past publications [Marton 2009, 2010; Marton et al. 2009a]. It extends this previous work with technical improvements, leading to further gains in translation quality, and with implementation details, complexity analysis, optimization issues, and a new set of experiments aimed to explore various ways of incorporating paraphrases in SMT phrase tables.

According to Callison-Burch et al. [2006], an SMT system with a training corpus of 10,000 words learned only 10% of the vocabulary (i.e., 10% of the types, not of the tokens); the same system learned about 30% of the types with a training corpus of 100,000 words; and even with a large training corpus of nearly 10,000,000 words, it only reached about 90% coverage of the source vocabulary. Coverage of higher-order n-grams is even harder than unigram coverage. This Out-Of-Vocabulary (OOV) problem plays a major part in reducing machine translation quality, as reflected by both automatic measures such as B LEU [Papineni et al. 2002], METEOR [Banerjee and Lavie 2005], TER [Snover et al. 2006], and human judgment tests such as HTER [Snover et al. 2006; Specia and Farzindar 2010]. Reducing the Out-Of-Vocabulary (OOV) word and phrase rate is therefore important for SMT systems.

Recent work proposed augmenting the training data with paraphrases generated by pivoting through other languages [Bannard and Callison-Burch 2005; Callison-Burch et al. 2006; Callison-Burch 2008]. This indeed alleviates the vocabulary coverage prob-lem, especially for the resource-poor, so-called  X  X ow density X  languages. However, these approaches require additional parallel texts (or translation tables) where one side con-tains the original source language. Such parallel texts are uncommon, with the notable exception of the EuroParl corpus [Koehn 2005]. (Moreover, Callison-Burch [2008] also requires parsing information). Most other recent methods require supervised training (see Section 3), resources for which are also scarce in  X  X ow density X  languages.
To overcome this resource constraint, we subsequently proposed to augment the training data with paraphrases generated by using distributional methods on a large monolingual corpus, a relatively abundant resource [Marton et al. 2009a]. The method constructs monolingual Distributional Profiles (DPs; see Section 2.2) of out-of-vocabulary words and phrases in the source language. It then generates paraphrase candidates from phrases that cooccur in same contexts, and ranks them with semantic distance measures, using cosine of vectors containing log-likelihood ratios. The highest ranking paraphrases are used to augment the translation phrase table (Section 4). However, this approach lacks the human linguistic knowledge that is implicit in the sentence alignment of the parallel texts. Therefore, it is unclear a priori which approach should yield higher gains in translation quality.

The base distributional paraphrasing method was reimplemented or extended in various ways (e.g., Mirkin et al. [2009], Marton [2010], and Marton et al. [2011]). Here we concentrate on providing more implementation details of the base method, mainly the use of a suffix array, augmented with a prefix tree with suffix links [Manber and Myers 1993; Lopez 2007, 2008], and an analysis of the paraphrasing algorithm complexity when using this data structure. We also report further gains in translation quality as a result of limiting the paraphrase length to be in the vicinity of the length of the paraphrased phrase.

In the rest of this article we describe Distributional Profiles (DPs) and semantic distance measures in Section 2, paraphrasing methods (including Marton et al. [2009a] with its algorithmic complexity) in Section 3, and translation model augmentation methods in Section 4. We report our experiments and results in Section 5, and conclude by discussing the implications and future research directions in Section 6. Since this article brings together various subfields, we discuss related work in each of the relevant sections.
 Semantic distance measures aim to detect words (or morphemes) with similar and/or related meaning. Semantic distance measures of larger units X  X igrams, phrases, sen-tences, passages, documents X  X ave been developed too, as an extension or combination of the word-level measures. We use semantic distance measures to recognize para-phrases (see Section 3). 1 These measures are grouped here as follows: lexical resource-based, corpus-based, and hybrid. WordNet [Fellbaum 1998] is a manually created taxonomy, 2 where each node represents a concept or word sense. An edge between two nodes represents a lexical semantic relation such as hypernymy ( is-a ) and troponymy ( has-part ). WordNet-based measures consider two terms to be close if they are connected by only a few arcs [Lee et al. 1993; Rada et al. 1989]), if their definitions share many terms [Banerjee and Pedersen 2003; Patwardhan and Pedersen 2006], or if they share a lot of information [Lin 1998; Resnik 1999] which are in fact hybrid methods, described in Section 2.3). The distance between nodes can be the number of arcs in the connecting path, or it can be computed from corpus statistics.

Within WordNet, the is-a hierarchy is much more well-developed than that of other lexical semantic relations. So, not surprisingly, the best WordNet-based measures are those that rely only on the is-a hierarchy. Therefore, they are good at measuring seman-tic similarity (e.g., doctor X  X hysician ), but not semantic relatedness (e.g., doctor X  X calpel ). Further, these measures can only be used in languages for which a (sufficiently devel-oped) WordNet exists. WordNet sense information has been criticized to be too fine-grained or inadequate for certain NLP tasks [Agirre and Lopez de Lacalle Lekuona 2003; Navigli 2006]. See Hirst and Budanitsky [2005] for a comprehensive survey of WordNet-based measures.

Lesk [1986] introduces a WSD method which relies on maximal word overlap of each of the word X  X  senses X  glosses (dictionary definitions) with the glosses of the surrounding words: If a word has several senses listed in the dictionary, the gloss of each sense is compared with the glosses of the surrounding words, and the sense whose gloss has the most overlap in number of words is chosen. Banerjee and Pedersen [2003] generalize this approach to a semantic relatedness measure, based on word overlap in the glosses of two words of interest. Hashimoto et al. [2011] scale this approach to paraphrase extraction from the World Wide Web. Corpus-based measures of distributional similarity rely on the distributional hypothe-sis [Harris 1954; Firth 1957]: Words tend to have a typical Distributional Profile (DP); they repeatedly appear next to specific other words in a typical rate of cooccurrence. Moreover, words close in meaning tend to appear in similar surrounding contexts, where context is taken to be the surrounding words in some proximity, typically a fixed size sliding window around the word X  X  occurrences. The DP of word u is a feature vec-tor whose dimensions are the surrounding context words ( collocates ), and the values represent Strength-of-Association (SoA) between u and each collocate. 3 Beside simple cooccurrence counts within sliding windows, other SoA measures include functions based on TF/IDF [Fung and Yee 1998], mutual information (PMI) [Lin 1998], condi-tional probabilities [Schuetze and Pedersen 1997], chi-square test [Gale and Church 1991], and the log-likelihood ratio [Dunning 1993]. An example DP for the word cord is given in Table I.

Profile similarity measures. A DP similarity function psi m ( DP u , DP v ) is typically de-fined as a two-place function, taking vectors as arguments (the DP of some word/phrase u and the DP of some word/phrase v ) whose size is the known vocabulary size. These vectors X  cell i contains the SoA of u (or v ) with each word ( X  X ollocate X ) w i in the known vocabulary. The vector representation allows for using well-studied similarity mea-sures, and also to intuitively think about the distance in geometric analogs. Similarity can be estimated in several ways, for example, the cosine coefficient, the Jaccard coeffi-cient, the Dice coefficient (all proposed by Salton and McGill [1983]),  X  -skew divergence [Dagan et al. 1999], and the City-Block measure [Rapp 1999]. The cosine is especially appealing. It is competitive, easy to compute, requires simple data structures (vectors) as input, and can be intuitively visualized: cosine of two two-dimensional vectors is inversely proportional to their angle  X  . 4 Although cosine is not a probability, it uses the same convenient range [0..1], which makes it easy to combine or interpolate with other measures, if so desired. The formula for the cosine function for similarity measure is given in Eq. (1).
In principle, any SoA measure can be used with any profile similarity measure, but in practice only some combinations do well; and finding the best combination is still more art than science. Some successful combinations are cos CP [Schuetze and Pedersen 1997], Li n PMI [Lin 1998], City LL [Rapp 1999], and Jensen X  X hannon divergence of conditional probabilities ( JSD CP ; a.k.a. Information Radius, in Manning and Schtze [1999]). Other measures are directional (textual entailment) in u and v [Kotlerman et al. 2009]. See Weeds et al. [2004], Curran [2004], Mohammad [2008], and Turney and Pantel [2010] for surveys of distributional measures. Resnik [1999] introduced a hybrid model for calculating  X  X nformation content X  by traversing the concept X  X  subtree in WordNet. This measure is hybrid in that it uses both a linguistic knowledge source and a large corpus of text, although it doesn X  X  use the distributional contexts of the words in the corpus. Lin [1997] and Jiang and Conrath [1997] improved on this idea by incorporating the distance of each word from the lowest common subsumer, following the intuition that words that are closer to this subsumer are likely to be more semantically similar than those that are far below it in the WordNet hierarchy.

A corpus-based DP of a word u conflates information about the potentially many senses of u [Mohammad and Hirst 2006]. For example, assume the noun bank has bank and wave will be some average of the semantic distance between all their senses. However, for various NLP tasks, what is often needed is the distance between their closest senses, in this case, the W ATER /R IVER senses. Both Mohammad and Hirst [2006] and Patwardhan and Pedersen [2006] proposed measures that are not only distribu-tional in nature but also rely on a lexical resource to exploit the manually encoded information therein as well as to overcome the sense conflation problem. Mohammad and Hirst [2006] generate separate DPs for the different senses of a word by using the categories in a Roget-style thesaurus as coarse senses or concepts: A word may be found in more than one category if it has multiple meaning. They use a simple unsupervised algorithm to determine concept-based DP( c ): a vector containing the SoA between the category c and each of the words w i in a corpus vocabulary, based on the number of times w i occurred next to an instance of c in the corpus. We observed that if words u and v appear under the same concept c , the semantic distance between u and v would be in-distinguishable (i.e., zero distance), since the concept-based similarity measure returns the semantic distance of the closest sense pair (here: sim ( DP ( c ) , DP ( c ))). Therefore, we adopted in Marton et al. [2009b] a hybrid approach with fine-grained soft constraints, discounting cooccurrence counts according to the counts in the concept-based DP of the desired word sense, and then calculating SoA measures over the discounted counts: from the cooccurrence frequencies in the concept-based DPs, and the cooccurrence count f ( u ,w i ) is calculated from word-based DPs. We also extended this method there to handle any word in the corpus, even if it didn X  X  have a concept-based DP. We do not address word senses here.

Erk and Pad  X  o [2008] represent a word sense in context by biasing the word X  X  DP according to the context surrounding a specific occurrence of that word. The advantage of their approach is that it does not rely on a thesaurus or WordNet. Its disadvantage is that it relies on dependency relations and selectional preferences information, which might not be available, or be of low quality, in a low-density language.

The lexical resource-based and hybrid semantic distance measures rely on resources that might not exist, or not be sufficiently developed, in a resource-poor  X  X ow density X  language. Therefore, their applicability is limited; or, in the best case, their quality would degrade to corpus-based measures X  level. In the rest of this article we use corpus-based measures, and concentrate on the distributional paraphrasing method. Note, however, that this method may be used with hybrid semantic distance measures as well [Marton 2009, 2010]. Paraphrasing is the act of replacing linguistic utterances (typically text) with other linguistic utterances, bearing similar meaning but different form. Hereafter the para-phrased text to be replaced is called the focal word or text, or simply, the focal . Paraphrasing research is quite diverse, and can be characterized by many axes, in-cluding the following.
 Focal unit. This can be a word,  X  X hrase X  (any word sequence), syntactic constituent,  X  X appy phrase X  a.k.a.  X  X attern X  (as in XgaveYtoZ ), paragraph, sentence, document, etc.
 Paraphrased elements. These can be lexical (different words with similar meaning), structural (e.g., switching between active and passive voice), or both.
 Use of linguistic knowledge. This type of knowledge can include syntactic information (parses; refer to Lin [1997], Callison-Burch [2008], and Das and Smith [2009]), semantic information (WordNet hierarchy, thesaurus concepts, or sense-annotated corpora as in Shutova [2010]), and/or morphological analysis [Nakov and Ng 2011], or none.
 Resource type. The reference type can include sentence-aligned parallel text (a.k.a. bitext), comparable text (e.g., global news from different sources covering the same period, refer to Munteanu and Marcu [2005]), stand-alone text (one monolithic corpus), or crowdsourcing (refer to Resnik et al. [2010]).
 Paraphrasing method. These include SMT (translating from and to the same lan-guage; Barzilay and McKeown [2001]), pivoting (translating to other languages and back [Bannard and Callison-Burch 2005; Callison-Burch 2008; Madnani et al. 2007]), distributional (relying on similar contexts in which the paraphrases tend to occur [Marton et al. 2009a; Marton 2010]), morphological and character-based analysis (compounds, edit distance), or other (e.g., correlating time-locked bursts of terms such as earthquake in several resources). 5 Paraphrasing object. The paraphrasing object can be the source-language elements in SMT, the translation references (target-language) elements in SMT, or something else (non-SMT-related, e.g., for document summarization).
 Input and output languages. Paraphrasing can be monolingual (typically), or multi-lingual (if translation is viewed as an instance of paraphrasing [Fung and Yee 1998; Rapp 1999; Diab and Finch 2000; Haghighi et al. 2008]).

Paraphrasing may be somewhat  X  X ossy X  in number of words and/or content, with the extreme cases of summarization and translation. This work uses monolingual, stand-alone text in order to generate (non-gappy) phrasal paraphrases with distributional methods, capable of incorporating lexical resources, and extensible to using syntactic information as well. Out-Of-Vocabulary (OOV) phrases in the source language are paraphrased and then used to augment an SMT translation model (details are given in Sections 4). Due to space limitations, we only mention here by name the most similar and recent work. For more information on kinds of paraphrasing, see Madnani and Dorr [2010] and Androutsopoulos and Malakasiotis [2010].

Moving along the paraphrasing method axis, Barzilay and McKeown [2001] use di-rect translation in order to generate paraphrases, in contrast with our method. They extract paraphrases from a monolingual parallel corpus, containing multiple trans-lations of the same source. Their method is limited by the small size of monolingual parallel corpora, if such exist, which are even scarcer than bilingual parallel resources, which are used in the pivoting method described next.

A leading SMT-related paraphrasing method currently is the pivoting approach, es-pecially Bannard and Callison-Burch [2005], Callison-Burch et al. [2006], and Callison-Burch [2008], with the latter using syntactic information.  X  X ivoting X  means translating the focal phrases to additional language(s) and back to the source language. The qual-ity of these paraphrases is estimated by marginalizing translation probabilities to and where f 1 and f 2 are the phrase and its paraphrase candidate, respectively. A major disadvantage of the approach is that it relies on the availability of parallel corpora in other languages. While this works for English and many European languages, it is far less likely to help when translating from other languages, for which bitexts are scarce or nonexistent. Also, pivoting is inherently noisy in both the paraphrase candidates X  correct sense and their translational likelihood, because of the double translation step. (More on that in Section 6). The problem of incorrect sense translation is likely to be exacerbated with out-of-domain translation, that is, when the test set is of a different genre than the bitexts. One of its advantages, however, is the use of linguistic knowl-edge that is encapsulated in the parallel sentence alignment. Still, we argue in Marton et al. [2009a] that the ability to use much larger resources for paraphrasing should trump or at least match the human knowledge advantage. Moreover, as mentioned in Section 2.3, some human linguistic knowledge can be harnessed from readily available lexical resources, which group words according to common semantic properties. This information can be used to approximate word senses, and model word meaning more accurately, still without relying on parallel text.

Zhao et al. [2008] apply SMT decoding for paraphrasing, using several log-linear weighted resources (phrase table, thesaurus, etc.), while Zhao et al. [2009] filter out paraphrase candidates and weight paraphrase features according to the desired NLP task: sentence compression, simplification, or similarity computation. Malakasiotis [2009] propose paraphrase recognition using machine learning techniques to combine similarity measures. Chevelu et al. [2009] introduce a new paraphrase generation tool based on Monte-Carlo sampling. Mirkin et al. [2009], inter alia , frame paraphrasing as a special, symmetrical case of (WordNet-based) textual entailment. Distributional paraphrase generation relies on the Distributional Hypothesis much like distributional distance measures do (see Section 2.2). The key idea is that good para-phrases are likely to be found in same (or similar) contexts as the focal word/phrase.
The DIRT system [Lin and Pantel 2001] uses unigram contexts and dependency parsing paths for paraphrasing relations such as  X  X wrote Y  X . Paraphrase candidate gathering is done with a hash table containing for each word in the vocabulary all the paths it serves as X or Y . Context overlap ratio between paths is used for candidate ranking. This approach requires parsing the entire monolingual text resource.
Pasca and Dienes [2005] use slightly longer, but still predefined context length (2 X 3 tokens). They find it useful to use contexts with specific templates containing Named Entities (NE) or relative clauses. They discard paraphrase candidates that are clauses, with pronouns, having no verbs, or having only function words, or merely showing up-per/lower case alternation from the focal term. Paraphrase candidate gathering is done similarly (via shared contexts). The frequency of the candidate in the same context(s) is used for candidate ranking. This approach requires POS and NE tagging of the entire resource.

Bhagat and Ravichandran [2008] also use POS tagging, and are interested in para-phrasing relations  X  X rel Y  X . They start with a seed set representing each relation, then download top 1000 Google search hits to extend it. They find and rank candidates using nearest neighbors with locality-sensitive hashing (cosine-preserving fingerprint of the DP, so that the cosine can be computed fast).

Like other distributional methods, our paraphrasing method requires a large mono-lingual corpus of text in the source ( X  X rom X ) language, a relatively abundant resource. This resource is used both for generating paraphrase candidates and for ranking them according to their semantic distance from the focal (paraphrased) phrase. We use contexts of dynamic length, without relying on POS tagging or parsing (although these can be added in principle), and we calculate vectors (DPs) on-the-fly (so no pruning or huge matrix precalculation needs to take place).

The outline of our method is as follows. (1) Upon receiving focal phrase phr , build distributional profile DP phr . (2) For each occurrence of phr , keep the surrounding (left and right) context L R . (3) For each such context L R , gather all paraphrase candidates cand , such that (4) Rank all candidates cand according to their semantic distance from phr by building (5) Optionally: Filter out every candidate cand that textually entails phr . (6) Output up to k-best candidates above a certain similarity score threshold.
The input phrases used to evaluate our method are source-language phrases un-known to an SMT model (out-of-vocabulary phrases). This is illustrated in Figure 1. 3.2.1. Build Phrasal Profile DP phr . Build a distributional profile of the OOV phrase phr , enlisting all collocating words, and their cooccurrence count or strength-of-association with phr , as described in Section 2.2. The cooccurrence counts are collected using a sliding window of size Max Pos tokens to each side of each occurrence of phr in the monolingual training corpus. If phr is very frequent (above some threshold of s occurrences), uniformly sample only s occurrences, multiplying the gathered co-counts by factor of count ( phr ) / s .Soif phr occurs 30,000 times and the threshold is s = 10000, than count cooccurring words in a sliding window around only every third occurrence of phr , but multiply these cooccurrence counts by 3. 3.2.2. Gather Context. Example contexts are shown in the left half of Table II. The challenge in deciding how much context to keep to the left and right of each occurrence of phr is a familiar recall-precision tension: if the context is very short and/or very frequent (e.g.,  X  X he is X ), then it might not be very informative, in the sense that many words can appear in that context (in this example, practically any noun); however, if the context is too long (too specific), then it might not occur enough times elsewhere (or not at all) in the training corpus. Therefore, to balance between these two extremes, we use the following heuristics. Start small: Start with setting the left context L to be a single word/token to the left of phrase phr .If count ( L ) &gt; mc c , where mc c is a maximal context count limit, append the next word to the left (now having a bigram left context instead of a unigram), and repeat until the condition is met. 6 Repeat similarly for R , the context to the right of phr . Add the resulting L R context to a context list. 3.2.3. Gather Candidates. For each gathered context in the context list, gather all para-phrase candidate phrases cand that connect left-hand side context L with right-hand side context R , that is, gather all cand such that the sequence Lcand R occurs in the corpus. Example candidates, appearing in same contexts as the focal phrase phr , are shown in the right half of Table II. In practice, to keep search complexity low, limit cand to be up to length Max PhraseLen . 3.2.4. Rank Candidates. For each candidate cand , build distributional profile DP cand and evaluate psi m ( DP phr , DP cand ) as in Section 2.2. Recall that since the DP is represented as a vector, any vector similarity function can be used here, for example, cosine. 3.2.5. Textual Entailment Filtering (Optional). Filter out every candidate cand that textually entails phr : This is approximated by filtering cand if its words all appear in phr in the same order. For example, if phr is spoken softly ,then spoken very softly would be filtered out. The idea behind this step is not to introduce novel or more specific information that was not present in the source. 3.2.6. Output k-Best Candidates. Output up to k -best paraphrase candidates for phrase phr , in descending order of similarity. Filter out paraphrases with score less than has a similarity score . 25, it will be filtered out because its score is too low, even though it is in the top 20 list. Conversely, if the 25 th best paraphrase has score . 76, it will be filtered out because it is not in the top 20, even though its score is above the threshold. Our paraphrase generation is implemented using a suffix array-based data structure [Manber and Myers 1993], augmented with pattern matching of word sequences [Lopez 2007, 2008]. 7 A Suffix Array (SA) stores all the suffixes in text corpus T in lexicographic order, where  X  X uffix X  here means a sequence of tokens ending at the end of T (not a sequence of letters ending a word). Populating the SA takes O ( | T | log | T | ) time, which is the cost of sorting the suffixes. This data structure only keeps indices to the beginning of each suffix (the end of each suffix being the end of T by definition), so it requires O ( | T | ) space.

Searching all occurrences of phrase phr in text T takes O ( r +| phr |+ log | T | )time, where r is the number of occurrences of phr : Finding the first occurrence of the first word of phr takes O (log | T | ); finding the first occurrence of the rest of the phrase takes additional O ( | phr | ); the other r  X  1 occurrences of phr follow it immediately in the sorted suffix array. 8
The SA can be augmented with a prefix tree with suffix links [Lopez 2007, 2008]. This additional data structure stores all prefixes of the (potentially gappy) search patterns over the SA X  X nd their suffixes, motivated by the observation that there is no point in searching for pattern abc in its entirety if its prefix ab was not found in the SA, which in turn should not be searched for if its suffix b was not found. The prefix and its suffix are linked, and marked as  X  X ead-ends X  if the latter is not found. However, if found, the tree node representing the suffix is annotated with the (contiguous) range of the suffix in the SA. This fact turns out to be very useful for DP calculation, which involves finding the counts (number of occurrences) of many collocates. This is because the size of this range (last index minus first index) is actually the count of the suffix or pattern (or collocate) of interest, and this can be quickly calculated without even having to access the corpus T itself again. The pattern matching code was originally developed for searching and extracting SMT rules in parallel text, but it was adapted here for building phrasal DPs and searching for their paraphrase candidates in similar contexts. 9
Since using SA frees one from the need to precalculate and hold in memory a huge collocational matrix, we can now compute DPs without pruning the vocabulary (as is often done when using LSA, for example), and are able for the first time to scale up these distributional methods to usage in an end-to-end state-of-the-art SMT system.
The focal phrases and their paraphrase candidates are contiguous (gapless) word sequences in this work. However, the search of candidates in context is done as a gappy pattern matching of the form L ... R for the left and right contexts. Whenever the gappy pattern is matched, the token sequence in the gap is taken to be a paraphrase candidate.

In the analysis that follows we assume that text corpus T has a vocabulary V con-sisting of | V | unique tokens (a.k.a. types ), and that the maximal phrase (or paraphrase candidate) length is m tokens. The maximal number of occurrences of any phrase is 0  X  r  X | T | X  X  V |+ 1; due to the Zipfian nature of natural language, r is, more often than not, very small. The algorithmic complexity of distributional paraphrasing is derived from the following steps. (1) Build distributional profile DP phr . A somewhat naive implementation would tra-(2) Gather left and right context L R for phrase phr. This step involves, for each (3) Gather paraphrase candidates cand occurring in the afore-mentioned L R contexts . (4) Rank all candidates cand. Let d be the number of candidates cand . First, build (5) Filter textually entailed candidates. This is linear in the length of the phrase and (6) Output k-best candidates. This is trivial: O (1) per phrase (assuming k is limited to The total complexity is, then, initial cost: O ( | T | log | T | ) time, and per-phrase cost. This expression can be simplified, and also pushed further down doing the following.  X  X niformly sample every phrase whose count is above some threshold s &lt;&lt; | V | ,for example, count ( phr ) &gt; s = 10 , 000. This means skipping count ( phr )  X  s indices in the suffix array, and visiting only s loci in T when collecting cooccurrence counts, multiplying each collocate occurrence by count ( ph r ) / s .  X  X s mentioned before, max( | L | , | R | ), | phr | ,and | cand | are small, negligible constants.  X  X he maximal context occurrence count is also limited to some constant: max( c ( L ) , c ( R )) &lt; mc c = 2000.
 These speedups are important for real-world SMT applications, since increasingly, a translation wait time of minutes or even deca-seconds is becoming unacceptable. From our experience, while most phrases are processed in mere milliseconds, others (at the left of the Zipfian curve) may take hours if parameter values are set carelessly. Taking the preceding points into account, we can rewrite the complexity expression as follows.
Note that d , the number of candidates, dominates the complexity, but is dependent in part on mc c , the maximal context count. Besides decreasing mc c , one can devise heuris-tics for subsampling the number of candidates, perhaps according to cooccurrence or Strength of Association (SoA) of certain contexts with phr . We leave this for future re-search. Further improvements in both initial and per-phrase runtime can be achieved if T is broken into parts that can be searched in parallel (followed by combining the respective search results). This is not the first to attempt to ameliorate the Out-Of-Vocabulary (OOV) words problem in statistical machine translation, and other natural language processing tasks. These attempts can be roughly divided into the following categories.  X  X ugment current resources (typically parallel texts or the derived phrase tables) with paraphrases of their elements.  X  X reate additional resources of same type (additional parallel texts).  X  X se alternative resources (lesser or no reliance on parallel texts).
 This work belongs to the first category, and therefore we mainly focus here on para-phrasing work. It most resembles Callison-Burch et al. [2006] (and its improved variant in Callison-Burch [2008]) in augmenting translation models with source-side para-phrases of the OOV phrases, using weighted log-linear features.

Habash and Hu [2009] show, with a similar pivoting method to Callison-Burch et al. [2006] and a trilingual parallel text, that using English as a pivot language between Chinese and Arabic can actually outperform translation using a direct Chinese-Arabic bilingual parallel text. The authors suggest that this might be due to the fact that English is  X  X alf-way X  between the other two languages in terms of word order prop-erties. Other attempts to reduce the OOV rate by augmenting the source side of a translation phrase table include Habash [2008, 2009], providing an online tool for paraphrasing OOV phrases by lexical and morphological expansion of known phrases and dictionary terms, and transliteration of proper names.

Bond et al. [2008] also translate and back-translate in order to generate paraphrases, but they do not use another language. They improve SMT coverage by using a manually crafted monolingual HPSG grammar for generating meaning-and grammar-preserving paraphrases by parsing the English side and then converting it to an abstract semantic representation and back to English. This grammar allows for certain word reordering, lexical substitutions, contractions, and  X  X ypo X  corrections. The paraphrases are then used to augment the training set. They test this method on both Japanese-to-English and English-to-Japanese translation tasks, and achieve modest B LEU score gains in most cases. Kuhn et al. [2010], too, do not use external resources. Instead, they pivot within the same phrase table, generating paraphrases this way, and soft clustering semantically similar translation rules (phrase-table entries) with new cluster-based probabilistic features.

Also recently (after the publication of this article X  X  core work), source-side paraphrase lattice has been used to augment SMT [Du et al. 2010; Onishi et al. 2010]. Pivot paraphrases were used, although this is not crucial for the lattice core idea. Max [2010] augments SMT models by using paraphrases to improve also the distribution estimates of existing (but mainly infrequent) translation entries. Each of our paraphrase-augmented models is identical to its corresponding paraphrase-less baseline model (Section 5), with the exception of additional paraphrase-based phrase-table entries (translation rules), and extra weighted log-linear feature or fea-tures, as in Callison-Burch et al. [2006].
As noted in Marton et al. [2009a] and Marton [2009], it is possible to construct a new translation rule from f to e via more than one pair of source-side phrase and its paraphrase; for example, if f 1 is a paraphrase of f ,andsois f 2 , and both f 1 , f 2 translate to the same e , then both lead to the construction of the new rule translating f to e ,but with potentially different feature scores. To illustrate this, suppose a Spanish-English phrase table has the following rules, all with the same target-side translation and suppose further that the source-side phrase a disponer de los is unknown (not in the table), and that among its top paraphrases are the following. Then there are three paths to construct a new translation rule from a disponer de los to to leave the , each going through one of the phrase-table entries given before. See Figure 2 for a visual example.

There are different possible approaches to the multiple-path phenomenon: A default approach might create a separate new f , e rule for each path, making these new rules compete with one another in order to enter the final sentence translation derivation during  X  X ecoding X  time (denoted hereafter as aggregation method NONE); another approach might generate only a single rule from f to e , using only one randomly chosen path (aggregation method RAND); or using only the  X  X est path X  X  X he path with the highest paraphrase similarity score X  X he upper path in the example previous (with highest similarity score .74; hereafter, aggregation method BEST). However, it is also possible to have all paths reinforce the model X  X  confidence in using a single new translation rule from f to e , either by a simple addition (aggregation method ADD, which is similar to the probability marginalization applied by Callison-Burch et al. [2006], except that the sum here might exceed 1), or by averaging the paths X  similarity scores (aggregation method AVRG). Yet another way would be to increase the new rule X  X  associated semantic score in proportion to the paraphrase scores of f to f 1 ,then f to f 2 , and so on (aggregation method UPDT). More formally, for each paraphrase f of some source-side phrases f i , with similarity scores sim ( f i , f ), calculate an aggregate score asim : with a  X  X uasi-online-updating X  method as where asim 0 = 0. The aggregate score asim is updated in an  X  X nline X  fashion with each pair f i , f as they are processed, but only the final asim k score is used, after all k pairs have been processed. Simple arithmetics can show that this method is insensitive to the order in which the paraphrases are processed. In this aggregation method, we only augment the phrase table with a single rule from f to e , and in it are the feature values of the phrase f i for which the score sim ( f i , f ) was the highest. 10 In Section 5.3 we compare the performance of afore-mentioned methods (except for RAND, which seems less principled). We examined augmenting translation models with paraphrases generated distribution-ally and ranked by distributional semantic distance measures. We tested our system on handling unknown phrases when translating from English into Chinese (E2C), and from Spanish into English (S2E).
 For all baselines we used the phrase-based Statistical Machine Translation system Moses [Koehn et al. 2007], with the default model features: 11  X  X  phrase translation probability,  X  X  reverse phrase translation probability,  X  X  lexical translation probability,  X  X  reverse lexical translation probability,  X  X  word penalty,  X  X  phrase penalty,  X  X ix lexicalized reordering features,  X  X  distortion cost, and  X  X  Language Model (LM) probability.
 The phrase translation probabilities were determined using maximum likelihood es-timation over phrases induced from word-level alignments produced by performing intersection of Giza++ training [Och and Ney 2000] on the source and target sides of the parallel training sets. All features were weighted in a log-linear framework [Och and Ney 2002]. Feature weights were set with minimum error rate training [Och 2003] on a development set using B LEU [Papineni et al. 2002] as the objective function. Test results were evaluated using B LEU and TER [Snover et al. 2006]: The higher the B LEU score, the better the result; the lower the TER score, the better the result. This is denoted with B LEU  X  and TER  X  in Table IV. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output, as in Callison-Burch et al. [2006].

Statistical significance for the B LEU results was calculated using Koehn X  X  paired bootstrap resampling test [Koehn 2004b], with a sample size of 2000 pairs. Statistical significance was determined in case the 95% Confidence Interval (CI) of the systems X  B
LEU score difference did not include zero. For conciseness, this is denoted as p &lt;. 05 hereafter. Similarly, a 99% CI is denoted as p &lt;. 01, and so on for other CIs. The word  X  X ignificant X  is used hereafter as a shorthand for  X  X tatistically significant X  (at p &lt;. 05 unless specified otherwise).
 The paraphrase-augmented models were created as described in Section 4, with the UPDT path aggregation method. See Section 5.3 for exploration of alternative path aggregation methods. We used the following parameter settings for the experiments reported throughout this section, unless otherwise specified: For generating the mono-lingually derived distributional paraphrases, we used a sliding window of size  X  6, a sampling threshold of 10000 occurrences, and a maximal paraphrase length of 6 tokens. Also, we arbitrarily limited the number of occurrences (in which to look for paraphrase candidates) of each context of phrase phr to no less than 250 (if there are more than that), and no more than 2,000 occurrences, in order to keep the runtime short, but still give a reasonable chance to any context to contribute candidates. For each phrase phr , we output no more than the top k = 20 best-scoring paraphrases. We generated para-phrases for phrases up to six tokens in length, with an arbitrary similarity threshold of 0.3.
We experimented with three variants:  X  X dding a single additional feature for all paraphrases ( 1-6grams-distrib );  X  X sing only paraphrases of unigrams ( 1grams-distrib );  X  X nd adding two features, one only sensitive to unigrams, and the other only to 2 X 6-grams ( 1&amp;2-6grams-distrib ).
 All features had the same design as described earlier, and each model X  X  feature weight set, including the baseline X  X , was tuned using a separate minimum error rate training. We repeated this process with pivot paraphrases for comparison. For the English-Chinese (E2C) baseline model, we trained on the LDC Sinorama and FBIS tests (LDC2005T10 and LDC2003E14), and segmented the Chinese side with the Stanford Segmenter [Tseng et al. 2005]. After tokenization and filtering, this bitext contained 231,586 lines (6.4M + 5.1M tokens). We trained a trigram language model on the Chinese side, with the SRILM toolkit [Stolcke 2002], using the modified Kneser-Ney smoothing option. We then split the bitext into 32 even slices, and constructed a reduced set of about 29,000 sentence pairs by using only every eighth slice. The purpose of creating this subset model was to simulate a resource-poor language.

For development we used the Chinese-English NIST MT 2005 evaluation set. In order to use it for the reverse translation direction (English-Chinese), we arbitrarily chose the first English reference set as the development  X  X ource X , and the Chinese source as a single  X  X eference translation X . For testing we used the English-Chinese NIST MT evaluation 2008 test set with its four reference translations. We calculated the B LEU score using shortest reference length, and using the NIST-provided script to split the output words to Chinese characters before evaluation, as is standardly done in the NIST English-Chinese translation task official evaluation. 12
We augmented the E2C baseline models with paraphrases generated as described earlier, training on the British National Corpus (BNC) v3 [Burnard 2000] and the first 3 million lines of the English Gigaword v2 APW, totaling 187M tokens after tokenization, and number and punctuation removal. See Table III for training set sizes.
 Results are shown in Table IV, and paraphrasing examples in Section 6, Table XIII.
Augmentation with pivot-based paraphrases. In order to compare the monolingual distributional paraphrasing method with the multilingual pivot method, we augmented the translation model with the pivot-generated English paraphrases used in Callison-Burch [2008]. 13 Due to memory (RAM) constraints, it was not possible to use the full list. We therefore chose to filter it with a score threshold of p &lt;. 3, similarly to the one used for the distributional paraphrases. Note, however, that a . 3 pivot-based estimated paraphrase probabilistic score is not equivalent to a . 3 distributional paraphrase vector similarity score. In addition to using all available lengths of paraphrased phrases (unigram to 5-gram) as done in Callison-Burch [2008], we also experimented with novel 1grams-pivot and 1&amp;2-5grams-pivot models, equivalent to 1grams-distrib and 1&amp;2-6grams-distrib , respectively. All pivot models showed significant B LEU gains over the baseline. They also showed slight TER gains, except for the unigram pivot model (but recall it was threshold-filtered). The novel 1&amp;2-5grams-pivot model also significantly outperformed its unigram counterpart.

Augmentation with distributional paraphrases. The E2C 29k-line augmented mod-els showed significant gains over the baseline, up to 1.7 B LEU points. All these 29k-line distributional models also showed higher B LEU and TER gains than their pivot counterparts (except for the TER score of 1-6grams-distrib ). For the 232k-line models, results were negative. TER scores generally followed the B LEU pattern. Note that the E2C 232k-line baseline is reasonably strong: Its character-based B LEU score is slightly higher than the JHU-UMD system that participated in the NIST 2008 MT evaluation (constrained training track), 14 although we used a subset of that system X  X  training materials, and a smaller language model. Results there ranged from 15.69 to 30.38 B
LEU (ignoring a seeming outlier of 3.93). For a successful attempt at augmenting the 232k-line model, see Section 5.4. In order to permit a more direct comparison with the multilingual pivoting method, we also experimented with Spanish-to-English (S2E) translation, following Callison-Burch et al. [2006]. For baseline we used the Spanish and English sides of the publicly available Europarl multilingual parallel corpus [Koehn 2005], with the standard train-ing, development, and test sets. We created training subset models of 10,000, 20,000, and 80,000 aligned sentences, as described in Callison-Burch et al. [2006]. For better comparison with their pivoting system, we used the same 5-gram language model, development, and test sets: For tuning, we used the Europarl dev2006 Spanish and English sides, and for testing we used the Europarl 2006 test set. 15
We trained the Spanish paraphrase generation model on the Spanish corpora avail-able from the EACL 2009 Fourth Workshop on Statistical Machine Translation 16 (the Spanish side of the Europarl-v4, news training 2008, and news commentary 2009), the JRC-Acquis-v3 corpus 17 , and the AFP part of the LDC Spanish Gigaword (LDC2006T12), and truncating the resulting corpus after the first 150M lines. We low-ercased these training sets, tokenized and removed punctuation marks and numbers, and this resulted in training set sizes as detailed in Table V.

Results are shown in Table VI. 18 In order to evaluate the S2E models, we used B LEU [Papineni et al. 2002] over lowercase output. Not recasing the output avoids possible recaser-originated scoring  X  X oise X . We used Koehn X  X  [2004b] significance test as before. Paraphrase examples are given in Section 6, Table XII.
 The distributional paraphrasing models achieved gains of up .6 B LEU points on the S2E 10,000-line subset (not all significant), and diminishing gains on the 20,000-line and 80,000 subsets. 1&amp;2-6grams-distrib was best performer in all cases. In the S2E experiments, the pivot method yielded slightly higher gains. However, note that pivot paraphrases for named entities were filtered out. Note also that the S2E baselines X  scores reported here are higher than those of Callison-Burch et al. [2006]. We attribute this to evaluating lowercased outputs instead of recased ones, and also possibly due to improvements in the Moses decoder over the three years separating the experiments reported in Callison-Burch et al. [2006] and those reported here. We concluded from a manual evaluation of the 10,000-line models that the two major weaknesses of the baseline model were (not surprisingly) number of untranslated (OOV) words/phrases, followed by number of superfluous words/phrases in the translation output. This section explores two issues that were raised in Sections 3.3 and 4.2: choosing the best path aggregation method, and limiting the maximal context count mc c (in order to reduce d , the number of paraphrase candidates, and hence algorithmic complexity and runtime). We therefore explored the interaction of several path aggregation methods (NONE, ADD, BEST, AVRG, UPDT; see Section 4.2) with several mc c values (from 256 to 4096 in multiples of 2). We first experimented with the E2C 1&amp;2-5grams-distrib model (left half of Table VII), since it was the best performer in the experiments reported in Section 5.1 and in Marton [2009]. The difference between the lowest and highest results was 1.6 B LEU . Limiting mc c to 512 clearly yielded best results in almost all cases. This is good news, since it means that mc c can be lowered from its previous value around 2000 down to around 500 without loss in performance (in fact, this would result in performance gains in addition to faster runtime).

Which is the best path aggregation method? This is less clear from the table. Gen-erally, it seems that NONE and ADD are the worst performers, and that AVRG and UPDT are the best performers. In order to decide which method to use in the future, we counted how many times each method was among the top three performers for each mc c value (rightmost column in the table). UPDT was the winner, albeit by a small margin.
In order to get an impression of the magnitude of the multiple path phenomenon, we counted the number of alternative paths per each new f , e rule 19 in the E2C 1grams-distrib and 1-5grams-distrib models, in both 29k and 232k training sizes. An in many other linguistic phenomena, this turned out to be a Zipfian curve (see Table VIII) 20 for each model: high number of rules with two alternative paths, which decreases as the number of alternative paths increases, with a long  X  X ail X , in some cases reaching over 20 alternative paths for very few rules. At first thought, one might find the differences between the 29k and 232k models surprising: One might expect that the larger models would have fewer OOV phrases, and therefore fewer augmentative paraphrastic rules, and smaller values in each alternative path bin. However, the opposite holds: The number of rules in each 232k model path bin is greater than that of the corresponding path bin of the counterpart 29k model. This can be explained by the fact that these augmentative rules depend on finding an existing  X  X nchor X  item in the phrase table X  X  source side. Naturally, the larger the model X  X  training set, the larger the phrase table, and hence the higher the chances of finding suitable  X  X nchor X  items.

Finally, we repeated a this experiment in a narrower mc c range with the S2E 1-gram model (right half of Table VII). Here, however, the differences between the lowest and highest performers were only about 0.1 B LEU , and therefore, not supporting (but also not undermining) the conclusions from the E2C experiment given before. So far, the distributional paraphrases proved useful only in models trained on small datasets. We hypothesized that larger models didn X  X  show gains because they already covered many of the good paraphrases. To test this, we conducted experiments with other paraphrase semantic distance thresholds. 21 We settled on a lowered 0.05 thresh-old, which helped some models (but not all). For these models, the increased coverage resulted in B LEU score gains, even though the average paraphrasing quality dropped (the additional paraphrases had lower scores by construction). With this setting, two of the 232k models showed for the first time a significant gain over the baseline: 0.7 B
LEU (but still worse TER scores). See Table IX. 22 In order to address the concern that in the experiments reported previously in this section, the distributional models had an unfair advantage over the pivot models in that they paraphrased unigrams to 6grams, here we experimented with paraphrasing unigrams to 5grams, as in the pivoting models. In preliminary experiments, the longer n-gram models always out-performed their shorter n-gram counterpart models (0.3 X 0.4 B LEU , and except for one case, about 0.1 X 0.2 TER), putting this fair comparison concern to rest, and making our claim for superior performance stronger.

A manual inspection of the paraphrases (before augmenting the translation tables) revealed that longer paraphrases tend to have lower quality. Therefore, we repeated some experiments, only allowing the usage of paraphrases whose length difference from the paraphrased phrase was no longer and no shorter than two tokens. We denote these experiments with  X  2 in Table IX. Threshold of 0.05 was used here as well. This time almost all models yielded further gains: The 29k models yielded up to 1.9 B LEU and 0.5 TER over the 29k baseline, and the 232k models yielded up to 1.2 B LEU and 0.2 TER over the 232k baseline. 1grams-distrib  X  2 was consistently the best performer in both training set sizes. Following the phrase-table augmentation insights gained in Section 5.3 and the re-vised experiments in Section 5.4, we next turn to translating into a morphologically rich language. Unlike translating into English or Chinese (Sections 5.1 X 5.2), both of which having a relatively impoverished morphological inflection, Modern Standard Arabic exhibits rich morphology and complex agreement patterns. This fact may pose additional challenges, both because richer morphology implies higher data sparsity (and therefore desired target word forms may be missing from the table), and because it entails more complex syntactic agreement patterns (and therefore some translation derivations with augmented rules may not be used since they would be penalized by elements such as the language model, even though translation with these augmented (but imperfect) rules may be superior to all other derivations available to the model).
The settings in the following experiments were the same as in Section 5.4, includ-ing the monolingual corpus for paraphrase generation, consisting of over 500 million tokens. The English-Arabic training parallel corpus consisted of about 135,000 sen-tences (4 million words) and a subset of 30,000 sentences (1 million words) from Arabic News (LDC2004T17), eTIRR (LDC2004E72), English translation of Arabic Treebank (LDC2005E46), and Ummah (LDC2004T18). Testing was done on the NIST Arabic-English MT05 and MEDAR 2010 English-Arabic four-reference evaluation sets.
English-to-Arabic translation results are given in Table X. Augmenting the larger model seems beneficial, although the improvement in B LEU was found statistically significant only when measured in lemma-based B LEU on MT05. Interestingly, the subset (reduced) model yielded negative results. The lemma-based scores generally portray the augmented models in better light than the respective standard (word-based) B LEU and TER measures, in terms of difference from baseline. This supports the concern that morphologically rich target languages require additional research in order for a method such as ours to work well. For more details on the data, tokenization, evaluation metrics, additional related experiments, and further discussion, see El-Kholy and Habash [2010a, 2010b], and Marton et al. [2011]. Schroeder et al. [2009] recently showed that the upper bound for gains by paraphrase augmentation (using human-generated paraphrases in a lattice of the source language) is high, and has not been not reached yet. We take their work as another validation of this research direction. Pivot paraphrasing methods (translating to other languages and back) rely on limited resources (bitexts), and are subject to shifts in meaning and inaccurate translation probability estimation due to their inherent double translation step. A related potential problem is a probability mass  X  X eakage X : The more polysemous the focal phrase, the more likely a higher rate of inadequate paraphrase candidates; even if each inadequate candidate score is low, together they might take away substantial probability mass, resulting in lowering the probability estimates for the better candidates, making the paraphrase probability estimate less reliable. Table XI demonstrates the potential pivot-related problems in the extreme case of identity paraphrases, for which one might intuitively expect a relatively high probability: trivially, the phrase itself should be a high scoring paraphrase candidate, but in reality, the estimated probabilities are often quite low. 23 In contrast, large monolingual resources are relatively easy to collect, the paraphrasing method described here involves only a single translation/paraphrasing step per focal phrase, and the identity paraphrasing score always equals 1 (unless the focal phrase is not in the monolingual corpus). In addition, the Callison-Burch et al. [2006] paraphrases were reported to filter out named entities and numbers, while here named entities were not filtered out (but digits and punctuation were).

It is unclear how to fairly compare the pivoting method to ours. Should the monolin-gual and bilingual training resources be equivalent in some way? (But large bitexts are rare; EuroParl-based pivoting is only applicable to European languages). Should the lengths of the phrase or its paraphrase be the same in both methods? (This was done in Section 5.4). Should pivot paraphrases be threshold filtered as the distributional ones are? (But recall that a .3 vector similarity score is not equivalent to a .3 probability score). Or should the number of augmentative paraphrases be similar in both? Per-haps each method should be presented in its best light. But finding the best running parameters for each method is not a simple matter either. Therefore, the comparisons here should be regarded as a first stab at this problem, inviting further research.
One potential advantage of using bitexts for paraphrase generation is the usage of implicit human knowledge, that is, sentence alignments. The concern that not using this knowledge would turn out to be detrimental to the performance of our paraphrase-augmented SMT systems was largely put to rest: In both S2E and E2C language pairs, the original pivot paraphrase augmentation 1-5grams-pivot models resulted in about 0.3 B LEU gains over the baseline, similar or lower than their distributional counterparts. Our novel improved pivot models yielded higher gains: E2C 1&amp;2-5grams-pivot yielded 1B LEU  X  X ut its distributional counterpart yielded a higher 1.7 B LEU gain, and S2E 1grams-pivot yielded 0.6 B LEU  X  X lightly higher than its distributional counterpart; however, this is a rather small gain.

To look at some specific examples, the top part of Table XII shows that for the Spanish baile , the top four distributional paraphrases are all appropriately dance related, while an unrelated function word a made its way into the top four pivot candidates. However, in the bottom part of the table, for the Spanish source phrase a favor del informe , several antonymous paraphrase candidates (containing contra instead of favor , i.e., against the report instead of for the report ) made their way to top places on the list, while the pivot candidates seem all to carry similar meaning (if perhaps only partially so in some cases), except for the function word su . This is typical of both methods: pivoting seems to rank high the alignment-wise and collocationally  X  X romiscuous X  function words, while the distributional method tends to rank high antonymous candidates, since they appear in similar contexts as the source phrase. 24 Table XIII contains additional examples of good and bad top paraphrase candidates, in English. All top paraphrases of the focal deal are semantically close to it ( agreement, accord, . . . ), and so is the case for the five best paraphrases of the focal fall , except for the one-best ( rise ). This is another example of the tendency of distributional mea-sures to rank antonyms high, which is undesired for SMT. The sixth-best paraphrase ( fall tokyo ap stock prices fell ) demonstrates another weakness of this method: This paraphrase seems to have been ranked high due to the collapsing of two separate para-phrase candidates at its edges ( fall and fell ), benefitting from the context to the left of fall tokyo. . . and the context to the right of ...fell . Such cases can be ameliorated with incorporation of syntactic parsing information [Callison-Burch 2008] or other struc-tural cues that would help filter out these cases. The third part of the table shows semantically close top paraphrases of the focal phrase to provide any other . It seems that in general, paraphrases of longer focal phrases are of lower quality than those of unigrams, as can be seen at the bottom right, fourth part of the table. There, only the second-best paraphrase is somewhat semantically close to the focal we have a situation that , but the overall quality is clearly lower.

The paraphrase quality remains an issue with this method (as with all other para-phrasing methods). Some possible ways of improving it, besides using larger corpora, are: using syntactic information [Callison-Burch 2008]; using semantic knowledge such as thesaurus or WordNet to perform Word Sense Disambiguation (WSD) [Resnik 1999; Mohammad and Hirst 2006; Marton et al. 2009b; Marton 2010]; using context to help sense disambiguation [Erk and Pad  X  o 2008]; improving the semantic distance measure (see also Footnote 24); and optimizing the paraphrase similarity threshold for use in SMT, for example, on a held-out dataset. As for the latter, note that the higher the threshold the lower the coverage, while the lower the threshold the lower the paraphrases and translation quality. We showed gains by lowering the threshold (Section 5.4), but it remains to be seen how these two opposite effects play out and where the optimum lies. We would like to explore ways of incorporating syntactic knowledge that do not sacrifice coverage as much as in Callison-Burch [2008]. We would also like to integrate context with lexical resource-based semantic knowledge. Scaling up to larger monolingual corpora, although potentially promising in terms of quality and coverage, poses some challenges of disk space, RAM, and processing time. It remains to be seen if it is feasible, with or without enabling techniques such as sampling, cloud computing, and Map/Reduce (e.g., Lin [2008]), which have been applied in related subfields. Currently, distributional semantic distance measures tend to become less accurate when comparing profiles (DPs) of words (or phrases) with a large difference in occurrence frequency in the monolingual corpus. This problem is expected to exacerbate with larger corpora, and needs to be taken up in future research.
Scaling up our method so that it would improve models trained on larger bitexts is another challenge. Generally, the larger the bitext, the lower the OOV rate; moreover, it seems that the larger the bitext, the harder it is to paraphrase focal phrases, perhaps due to their lower frequency on average, resulting in potentially impoverished DPs, and less reliable cooccurrence statistics and similarity scores. However, for translating low-resourced specialized domains and genres, lowering the OOV rate is likely to remain an important issue.

How much monolingual data would be  X  X ufficient X ? Currently we have only little evidence to support the claim that increasing resource size is helpful, although it would be surprising if it were not the case. Giving a more detailed account for this question is not necessarily simple: using artificially reduced resources is not of much interest, since current monolingual data size seems to yield useful paraphrases, but with only modest translation quality gains; using additional monolingual data might involve mixing different domains or genres in an unbalanced manner, which may affect paraphrase quality either way, and which one would have to control and tease apart from mere data size when assessing the contribution of resource size. Future research should further investigate paraphrase quality as a function of monolingual data size. One of the advantages of our novel usage of SA for semantic representation that is mentioned in Section 3.3 is not having to precompute a large matrix. Therefore, SA can be useful in a fast-response application such as SMT. Our method can be tuned to be faster (potentially sacrificing paraphrasing quality), and the code can be further optimized for speed. Another advantage is not having to prune vector or matrix size, and this richer representation is likely to contribute to paraphrasing quality. But the potential advantages of using SA for semantic representation go beyond what is mentioned in Section 3.3: SA enables the search and DP construction of any number of arbitrarily long phrases in reasonable time and space, which is either entirely prohibitive in standard methods, or is only partially possible at best. Moreover, several new research efforts apply higher-order matrices (a.k.a. tensors ) to semantic representation; the memory limitations for such representation are even more acute, which often forces pruning and pairwise projections to two dimensions (matrix) at a time, most likely resulting in a performance degradation (e.g., Van de Cruys [2009]). Last, since SA stores all suffixes in lexicographic order, it naturally lends itself to paraphrase extraction for all n-grams of arbitrary length (e.g., 1 X 5grams), simply by traversing the SA and paraphrasing each desired suffix. Doing this for our purposes here was unnecessary, but arguably it is useful for building lexical resources such as a thesaurus or paraphrase bank. Future research is likely to benefit from further utilizing SA. Fine-grained features proved advantageous in many cases, that is, having a dedicated feature for paraphrases of unigrams, and another feature for paraphrases of longer phrases, compared with a single (and hence, coarser) feature for all paraphrases. How-ever, paraphrasing unigrams only ( 1grams. . . ) was a best performer even more consis-tently. So it seems that paraphrasing longer phrases is useful, at least in some cases, but not as reliable as paraphrasing unigrams only. This fact leads us to conjecture that modeling the semantics and semantic distance of larger-than-unigram phrases is still far from being well understood. We are not the first to notice it and take interest in this issue; see for example Sag et al. [2001] and recent MultiWord Expression (MWE) workshops. Further experimentation is required to better understand and tease apart the issues of longer phrase semantics and fine granularity.

When the length of the distributional paraphrases was limited (hard constrained) to the vicinity of the length of their focal phrase  X  2 tokens, further gains were achieved: up to 1.9 B LEU for both 1grams-distrib  X  2 and 1-5grams-distrib  X  2 . This novel element can be further developed as a set of additional soft constraint features: | focal | : | cand | length ratio, | focal | X  X  cand | length difference, and/or | focal | and | cand | lengths as separate features. 25 Alternatively, rules with focal and/or paraphrases of certain length can have a dedicated paraphrase similarity feature with a separate weight, similarly to, and extending, the 1&amp;2-5grams model approach. Yet another alternative (as was suggested by an anonymous reviewer), is to experiment with 1-2grams, 1-3grams, etc.
Note that there is a trade-off between finer granularity and data sparseness. The longer the unknown phrase, the fewer the generated paraphrases above some sim-ilarity threshold. Also, from preliminary experiments, it seems that the number of generated paraphrases per unknown phrase (above a fixed similarity threshold) drops in proportion to the length of the unknown phrase. Therefore, separate soft constraint log-linear features for longer phrases are likely to be of low quality or marginal impact, while increasing runtime. If using the de facto standard MERT [Och 2003] X  X s opposed to, say, the newer MIRA [Chiang et al. 2008, 2009] X  X or feature weight optimization, the mere increased number of features might be prohibitive by itself. It remains to be explored what the optimal split to separate features is, in interaction with better mod-eling of longer phrases. It will most likely also depend on the available monolingual corpus size. The paraphrasing method presented here is quite general, and therefore different semantic distance measures X  X ncluding other corpus-based, resource-based, or hybrid measures X  X an be plugged in to generate phrasal paraphrases. Although our method is largely language-independent, English-Arabic results (Section 5.5) indicate that some language-specific issues in paraphrasing or SMT (such as lemmatization of the monolingual paraphrasing corpus, the training bitext, and/or the language model) are needed in order to yield higher gains, at least in such SMT tasks, and likely with any morphologically rich language.

A further goal in the future would be to create a distributional semantic-distance-based, high-performance SMT system, with reduced or even no dependency on manually aligned parallel texts. Such a system would be especially beneficial to the  X  X ow-density X , resource-poor languages, but has potential to benefit all languages and language pairs, at least on low-resourced specialized domains and genres. 26 The para-phrases generated by our method can also help in other tasks beside SMT, be it query expansion in information retrieval, document summarization, dialog systems and nat-ural language generation, or other NLP tasks. We showed that augmenting translation models with monolingually derived para-phrases, generated distributionally, and ranked with distributional semantic distance measures over a large text in the source-side language, yields best improvements over a nonaugmented baseline in almost all cases. Our method is largely language-independent, and has the advantage of not relying on bitexts in order to generate the paraphrases, and therefore it clears the way for accessing large amounts of (monolin-gual) training data, for which creating bitexts of equivalent size is generally unfeasible.
The smaller models, emulating a resource-poor language setting, showed even higher gains than larger models (which were trained on supersets of the smaller models X  data), when augmented using the same paraphrase resources. The best smaller model X  X  gain was 1.9 B LEU , compared with 1.2 B LEU for the best larger model X  X  gain.

We also showed that the phenomenon of multiple alternative paths for generating a new translation rule is pervasive, with a Zipfian distribution. We explored several ways of aggregating the multiple alternatives, and settled on the UPDT method.
Last, we provided implementation details of our method, and analyzed its algorithmic complexity. We identified d , the number of paraphrase candidates per phrase, and mc c , the related maximal context count, as the main factors driving the complexity. We then explored how much mc c (and hence indirectly d as well) can be limited without hurting translation performance.

Our novel use of suffix array for paraphrasing enabled us to generate paraphrases on-the-fly (without collocation matrix precalculation, which would require knowledge of the test set X  X  source side), and to use full (non-pruned) distributional profiles without encountering memory limitation issues. We believe that using suffix array for semantic representation has a great potential that hasn X  X  been fully exploited yet, for exam-ple, in building multidimensional (tensor) representation, and searching meaning in context.

