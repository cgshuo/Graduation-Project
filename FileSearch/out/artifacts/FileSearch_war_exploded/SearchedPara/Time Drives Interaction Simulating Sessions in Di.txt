 Real life information retrieval takes place in sessions, where users search by iterating between various cogn i tive, perceptual and motor subtasks through an interactive interface. The sessions may follow diverse strategies, whi ch, together with the interface cha r acteristics, affect user effort (cost), experience and session effe c tiveness. In this paper we propose a pragmatic evaluation a p proach based on scena r-ios with explicit subtask costs. We study the limits of effectiveness of diverse interactive searching strat e gies in two searching env i-ro n ments (the scenarios) under overall cost constraints. This is based on a comprehensive sim u lation of 20 million sessions in each scenario. We analyze the effectiv e ness of the session strat egies over time, and the prope r ties of the most and the least effective sessions in each case. Fu r thermore, we will also contrast the proposed evaluation a p proach with the trad i tional one, rank based evaluation, and show how the latter may hide essential f actors that affect users X  pe r formance and satisfa c tion -and gives even counter -intuitive results. H.3.3 [ Information Search and Retrieval ]: Search process Session -based evaluation, simulation, time -based evaluation Interaction through search interface and environment greatly affects the user behavior, user experience, and user pe r formance.
 Many earlier studies have extended t he traditional Cra n field view of IR and discussed various aspects of interactive searching (see, e.g., [4], [5], [6], [13], [21]), user interaction, and query modific a tion (see, e.g., [3], [10], [14], [28]).
 During interaction the user selects b etween subtasks , e.g., whether to scan the result or launch a new query instead, and how to co n-struct the query. Such selections obviously affect session gains. However, different subtasks also have costs, e.g., they take time. This is i m portant because re al life IR often takes place under (time) constraints. In particular, keeping the overall session cost reasonable may be esse n tial for end users. The costs of subtasks may vary for many reasons between searc h ing environments. For example, regarding the qu ery side, small devices and touch screens are inconvenient for typing [11]. R e cently, novel kinds of searching devices, including personal phone -based mobile devi c es, have become increasingly popular.
 In order to minimize the overall session costs, a mobil e phone user might e.g., avoid typing and prefer result scanning. Low input costs might change the situation from the user X  X  point of view, leading to longer queries. Therefore, if we assume two users having identical needs and identical cost co n straints r e garding the overall session time, it is possible that different devices render different subtask comb i nations optimal in searc h ing. Traditional IR evaluation focuses on the quality of the ranked ou t-put. In this view, the costs of posing queries are non -p roblematic, even uninteresting. In this paper we will utilize simple scenarios to bring time factors into the research setting. Scenarios formalize and quantify the gains and costs of inte r a c tive sessions. We construct two cases  X  a personal desktop co m put er (PC) and a smart phone (SP) case, with subtask costs d e rived from the literature. We will sim u late session interaction involving multiple queries based on prototypical but empirically grounded query modification strategies using a test collection. We th en e x plore the effectiveness of searc h-ing via the exhaustive set of querying -scanning combinations poss i-ble, and evaluate the effe c tiv e ness of both scenarios in terms of Cumulated Gain (CG) [16] under time constraint (overall session time). We use non -norm alized metrics, because normalized metrics may yield mi s leading results, esp e cially if time is taken into a c-count.
 Early papers on IR evaluation had a comprehensive approach to i n teractive IR evaluation. Cleverdon et al. [8] pointed out, among others, phys i cal and intellectual user effort as an important factor in IR evalu a tion. Salton [24] identified user effort measures in the context of IR evaluation. More recently Su [30] gave a co m par i son of 20 different evaluation measures for interactive IR, inclu d in g actual cost of search, several utility measures, and worth of search r e sults vs. time expended. The interactive aspect of IR requires attention because previous stu d ies have repeatedly shown that di s crepancy exists between interactive and non -interactive evalu a-tion results. Hersh et al. [12] showed that a weighting scheme gi v-ing maximum improvement over the bas e line in non -interactive batch evaluation failed to surpass others when real users performed a simulated task. Turpin and Hersh [31] o b served that a system sup e rior over the baseline in batch evalu a tion, measured by mean average prec i sion, was not super i or in an interactive situation. Turpin and Scholer [32] found no signif i cant relationship between the search engine effectiveness mea s ured by mean average prec i-sion and real user success in a prec i sion -oriented task. Smith and Kantor [25] observed that users of degraded sy s tems were as su c-cessful as those using non -degraded systems. They suggested that users achieved this by altering their b e havior. Dunlop [9] proposed  X  X ime -to -view X  graphs, which incorp o rate user interface and system as well as the time component into the same framework for evaluation of system effectiveness. However he did not analyze time constraints, query modification strategies and different d e vices.
 Smucker [26] brought time factors into the traditional Cranfield setting by augmenting it with the use of the GOMS [7] model (a c-ronym for Goals, Operators, Methods, and Selections). He su g-gests a user model for IR where the search process is seen as a sequence of actions (e.g., typing; clicking; evaluating a su m mary; waiting for the results to load) with associated times and pro b abil i-ties (e.g., whether the simulated user will click on a rel e vant su m-mary). He used the model in a sim ulated study to demonstrate the impact of changes in the IR system interface (e.g., when the speed and accuracy of the summary evaluation is varied) on user perfor m-ance (the number of relevant doc u ments read within a given time frame). While his experimen t was li m ited to single query situations, the approach can be e x tended to multiple qu e ry scenarios, e.g., for computing the costs of specific query r e formulations.
 Azzopardi [2] addressed the cost aspect by treating intera c tive IR as an economical problem and studied the trade -off b e tween querying and browsing while maintaining a given level of no r ma l ized CG (NCG) [15] in sessions. His analysis focused on quer y ing  X  sca n-ning depth combinations for various formal r e trieval met h ods that deliver a given level of NCG. Our approach in the present paper differs from earlier studies. Our study is based on the simulation of multiple -query sessions gene r-ated with various query modification and scanning strat e gies in diffe r ent searching environments.
 In the next sec tion we start by discussing session generation with costs, and present the research questions. In Section 3 we d e scribe the research setting. In Section 4 we will run an exper i ment in a test collection based on scenarios and discuss the r e sults. We close t he paper by discussing the signif i cance of our approach in the last section. A use case is  X  X  relatively informal description of system X  X  b e havior and usage, intended to capture the functional requirements of the system by describi ng the interaction between the outside actors and the system, to reach the goal of the primary actor" [19]. We utilize simplified use cases, which we call scenarios, to pr e sent an altern a-tive way to look at the effectiveness of IR a p proaches based on the user viewpoint. The next subsections will first e x plain the session generation formally, and then e x plain the specific query modific a-tion (QM) and sca n ning strat e gies utilized in the scenar i os. For session simulation, we first formally g enerate all possible se s-sions under constraints. We will represent sessions as s e quences of actions with costs. For example the tuple &lt;(a 1 (a ,c n )&gt; is a session of n actions and each pair (a i representation represents an action a i and its cost c i action types are:  X  Initial query, represented as ( X  X q X , ic), where  X  X q X  is the action  X  Query reformulation ( X  X  X , qc), where  X  X  X  is the action label and  X  Document snippet scan ( X  X  X , sc), where  X  X  X  is the action label  X  Next page request ( X  X  X , nc), where  X  X  X  is the action label and nc The constraints are:  X  MaxSLen, maximum session length in terms o f elementary  X  MaxSCost, maximum session cost (seconds), here 60, 90 or  X  A session always begins with an initial query.  X  All queries (initial and reformulation) are followed by at least  X  The longest scan sequence we consider is a scan of 10 sni p pets In effect, the shortest possible session therefore is initial action IA = &lt;(iq, ic),(s, sc)&gt;, consisting of an initial query followed by the scan of one snippet (with costs). To generate longer sessions, we define the set NA for the possible subsequent actions: Note here that the next actions are tuples of one or two eleme n tary actions; a scan may appear individually, while a reformul a tion / next page requires a scan to follow. Sessions are generated by co n-cat e nating next actions to the initial action. Concatenation of two tuples S 1 = &lt;e 1 , e 2 ,..., e n &gt; and S 2 = &lt;f S , S 2 &gt; = &lt;e 1 , e 2 ,..., e n , f 1 , f 2 ,... , f over a set of session tuples S i , denoted as: The cost of a session S is, informally, the sum of its action costs. More formally, we derive this cost by the function s -cost as fo l low s: [N.B. we extend the definition of the set membership operator from sets to tuple components in an obvious way.] For example, the cost cost (S1) = ic+sc+qc+sc.
 The condition of maximum scan length of n in a session S is e n-forced by the Boolean predicate max -scan (S, n ). It yields  X  X rue X  for a given session S if S does not contain a subsequence of scan actions tion here omitted for brevity). To generate sessions, we first generate all sessions up to the max number of actions MaxSLen. This session set is MLS: We then select the subset of sessio ns fulfilling the time co n straint MaxSCost and the scan length constraint as follows. All sessions in MLS with maximal cost MaxSCost (or less) form the set MCS:
MCS = {S  X  MLS | s -cost (S)  X  MaxSCost  X  max -scan (S, 11)} Note that this approach does not defin e the query contents or mod i-fications in sessions. However, it keeps them within co n straints and guarantees that the last action is a document sni p pet scan. In our experiments, we excluded the next page action from NA due to the max scan length constraint of 10. The next two sub -sections explain and justify the query modification and sca n ning strategies used in the experiment. We will simulate interactive search sessions as querying -scanning iterations having a goal, a procedu re to reach the goal, and co n-straints regarding the procedure. We define the goal in terms of maximi z ing CG during the session under the constraint on the overall session time available. The procedure is d e fined in terms of QM and scanning strategies.
 The previous section did not define any particular QM strategies. We assume that a set of individual words { w 1 , w available for each particular topic, and QM strategies d e termine how elements from this set are combined to form queries (either the initial query, or one of the subsequent queries). In other words, given a set of individual search words for the topic, the QM stra t egy defines how to form a sequence of queries. Five QM strategies (S1  X  S5) were used in the experiment. Th e se prototyp ical strategies are based on term level changes which have grounding in the observed real life b e havior and are just i fied by literature (see [1], [20], [33]):  X  S1 : an initial one -word query ( w 1 ) is followed by repea t edly  X  S2 : an initial two -word query ( w 1 w 2 ) is followed by queries  X  S3 : an initial three -word query ( w 1 w 2 w 3 ) is followed by q u e- X  S4 : an initial one -word query ( w 1 ) is followed by adding one  X  S5 : an initial two -word query ( w 1 w 2 ) is followed by adding This means that the sessions consist of at most 3 to 5 queries; this reflects real life behavior [22]. Generall y speaking, co n structing a query entails a cost due to the cogn i tive user load plus the edit costs. We will return to the cost factors in Se c tion 2.4. The user may simply scan one or more documents after each query before formulating th e next query candidate or ending the session. After a single query Q i a sequence of one or more doc u ment sni p-pets may be scanned: Q -&gt;s 11 -&gt;s 12 -&gt;s 13 -&gt;...
 The cost of this session manifests as: qc + sc 11 + sc 12 + sc 13 + ...
 When a set of queries is available for one topic, the user can scan varying numbers of document snippets after any particular query, lea d ing to a vast number of possible querying -scanning sessions , e.g., Q -&gt;s 11 -&gt;Q 2 -&gt;s 21 -&gt;Q 3 -&gt;s 31 -&gt; ... or Q -&gt;s 11 -&gt;s 12 -&gt;Q 2 -&gt;s 21 -&gt;... or Q -&gt;s 11 -&gt;s 12 -&gt;s 13 -&gt;Q 2 -&gt;s 2 1 -&gt; s In real life a session typically continues until the user has found what he was looking for, at least partially, and/or when he runs out of time or queries. The scanning lengths may fluctuate for many reasons. In this paper we stud y the properties of optimal and less optimal interactive behaviors in sessions below the given overall time constraint. Therefore we produced all possible se s sions as follows. For all five QM strategies we formed all possible combin a-tions of scanning lengt hs exhaustively (from 1 to 10 documents) using a sequence of all possible queries avai l a ble per topic (cf. equation MCS in Section 2.1). We focus on the top documents because only few top do c uments may be inspected by the user in real life [14], [23], and only these may matter for the u s er [1]. As we had 5 words for each topic, sessions had at most 5 queries, co n-trolled by the QM strategy and time co n straint. As the query words were ordered by qual i ty (see 3.1), the query words were used in that particular order, not pe r muted. There is a cost involved with the subtasks of formulating the query and scanning. We assume that the absolute cost is partia l ly dete r-mined by the scenario. Empirical studies show that it takes signif i-cantly more time to en ter queries by using a small smart phone keypad than by using an ordinary keyboard [17]. To study the si g-nificance of subtask costs under overall session cost co n straint we define two scenarios, i.e., a Desktop PC scenario (PC) and a Smart phone sc e nario ( SP). These scenarios have different subtask costs. termine the subtask costs [17]. Obviously, also forming queries under different QM strategies S1  X  S5 have very different relative costs . All queries in strat e gies S1, S2 and S3 have a fixed query length in sessions (one, two or three words, correspondingly) while in strat e gies S4 and S5 the queries experience and knowle dge of the person, the size of the ke y board, the layout of the keyboard (e.g., nine -key multi -tap vs. qwerty ke y board) [17], [18], and whether predictive text feed is available and used. We used liter a ture to derive the cost values in scenarios PC and SP r egarding the initial query cost and the su b sequent query cost (Table 1). The query costs in S1  X  S5 in the Desktop PC case are based on the typing costs of 3.0 seconds per word. The corr e-sponding Smart Phone costs are based on [17]. The authors pe r-formed a large -scale log analysis of cell phone usage and observed that an average smart phone query length was 2.56 words and the ave r age query -entry time was 39.8 seconds (ave r age typing cost of 15.5 seconds per word). We assume in our simul a tions that the cost of adding one word to a query (that is, S4 and S5) or repla c ing one word at the end of the previous query (that is, S1, S2, S3) is a co n-stant, i.e., either 3.0 or 15.5 seconds depending on the sc e nario. Table 1. Average subtask costs (in seconds) of fiv e QM stra t e-gies (S1 -S5) for two scenarios: (i) initial query cost, (ii) subs e-quent query cost, and (iii) the cost of scanning one doc u ment snippet To check whether these costs are reasonable we also pe r formed a small -scale exper i ment where four test persons typed the initial and subsequent qu eries accor d ing to strategies S1 -S5 using two types of interfaces (Desktop PC and Smart Phone) for three test topics. The experiment corroborated that the query time est i mates were reaso n-a ble.
 The document snippet scanning costs in real life are affected b y the motor and perceptual costs plus the cognitive load related to the task. In this study we assume that the document snippet sca n ning cost is constant in both scenarios and across the searc h ing strat e gies S1  X  S5 (see Table 1). In the SP case we defined a scanning cost of three seconds per snippet. We justify this by an o b servation by Kamvar and Baluja [17] that the average cell phone user used 30 seconds to scan the search results before selecting one, after recei v-ing 10 search results. For the snippet sca n ning costs in the Desktop PC case we decided to use the same value. Obviously, our metho d-o l ogy is well -suited to expe r iment with different costs. The overall cost constraint of a se s sion was d e fined as 60, 90, or 120 seconds. In the simulations all sub tasks (querying and scanning) had to be performed within this time constraint. We excluded the eventual thinking time in pr o ducing query words. We set forth the following research questions: 1. How effective are the five QM strategies (S1 t o S5) in terms of 2. What are the characteristics of the best and the worst se s sions 3. How st able are the observed trends when the overall time co n-4. What is proper evaluation methodology when time is part of t he We used a subset of the TREC 7 -8 document collection with 41 topics for the experiment. The documents have graded relevance assessments on a four -point scale with respect to the to pics. [27] The present authors obtained query words for session generation for the test topics from [20] where the authors used real test pe r sons to suggest keywords of various lengths for qu e ries on the 41 to p ics. The test persons were asked to directly propose good search words from topic descriptions (descriptions and narr a tives) in a stru c tured way. Among others, they produced query versions of var i ous lengths: (i) one word, (ii) two words, and (iii) three or more words. These were collected per topic as ordered word lists of 5 words for each topic. During the query formul a tion exper i ment the test pe r-sons did not interact with a real retrieval system. While this may have affected negatively the quality of queries, Keskustalo and co l leagues [20] suggest that the test persons were able to construct the query words in a descending order of effe c tiveness. Retrieval system Lemur with language modeling and two -stage smoothing options was used in the experiment. For each topic we utilized a minim um of 1 query and a maximum of 5 queries in each session. A minimum of 1 document snippet and a maximum of 10 document snippets were scanned per query. In Table 2, the number of possible scanning paths is given for co n-secutive queries. If the session comp rises at most 2 queries, first there are 10 possible paths after the first query, and for every path there are 10 possible paths after the second query. So the comb i n a-tions of these at most two queries sum up to 10+10*10 =110 poss i-ble paths. In our experim ent design, users can pose up to 5 queries depending on session strategy; this presents alt o gether 111,110 possible paths, which are taken into consideration. 
Table 2. Number of possible sessions per number of queries, when at most 10 documents can be s canned after each query Queries 1 2 3 4 5  X  We ran all 41 topics * 5 QM strategies * Q queries, Q  X  depending on the strategy, and collected their results. Then we generated all 111K possible sessions from the query results, pruned the ones exceeding the time constraint in each scenario, and by using the recall base (qrels), evaluated the CG of the scanned sni p-pets for each session. For example, for the session Q &gt;s the snippet sequence s 11 , s 12 , s 13 , s 21 , s 22 million sessions (41 topics * 5 QM strat e gies* 111,110 possible scanning sessions * 2 scenarios) were evaluated. As the collection has graded relevance assessments, CG was incr e m ented by 3 points for the highly relevant doc u ments, 2 points for the fairly relevant documents and 1 point for the ma r ginal ones. Whenever a dupl i cate was retrieved by a subsequent query in a session, its gain was null i-fied. Finally, we ranked all session s within a topic and a strategy by their CG scores. In this data set per topic, strategy and time co n-straint, each session is represented by its tuple of actions (see 2.1) and its gain. The action tuples allow the analysis of the number of queries and the length of each scan in a session. The ranked order of sessions allows identification of the best and the worst session across topic s , strat e gy, scenario , and time constraint. We analyze the sets of 10 best sessions, and 10 worst sessions p er topic as ave r a g es instead the single best or worst session. This approach smoothes minor random variations in human behavior and thus the set of top (bo t-tom) 10 sessions provide more reliable mea s urements co m pared to the single best/worst session when w e explore their properties under varying conditions. Since the pr e sent study does not aim to prove one r e trieval method better than another, we report the findings without tests on significance of statistical differences. First we discuss the CG results under the two scenarios, PC and SP. We present the best case and worst case results r e garding all quer y-ing -scanning sessions based on the five QM strategies: S1 ( s e-quence of individual words); S2 ( two -word s; last word va r ied); S3 ( three -words; last word varied); S4 (incr e mental exte n sion starting from one word); and S5 (incr e mental extension starting from two words). Table 3 gives the averaged CG values, the nu m ber of qu e-ries and scans per query for 10 best and 10 worst cases for every QM strategy for the 60 second time co n straint, which are utilized in the following figures in this se c tion. Table 3. Averaged CG, number of Queries (#q) and Scans per Query (s/q) for sc enarios PC and SP , for 5 strategies for the 10 best (b) and 10 worst (w) sessions, t ime constraint 60 se c onds (60 s) avg. CG avg. #q avg. s/q Table 4 and Table 5 are equivalent to Table 3 but for the time co n-straints 90 and 120 seconds, respe c tively. Figure 1 shows the CG of the best (worst) sessions for each strategy in both scenar i os under the overall cost constraint of 60 seco nds. Note that all sessions require 60 seconds or less if no fu r ther action fits in (the absolutely worst imaginable session without any time requir e ment, in terms of the CG, would naturally consist of the initial action (IA) only). In other words, regardi ng the worst results, we report CG for the worst possible 60 second perfo r mance.
 When the best sessions of the PC and SP cases are compared in Figure 1, the PC case performs at a considerably higher level (ave r-age CG is above 8 in three strategies) than th e SP case (ave r age CG is below 5 in all strategies).
 Fig 1. Cumulated Gain under cost constraint of 60 seconds.
 Second, when the best and the worst cases are compared within the scenarios, not surprisingly, the best case results are typically clearly be tter than the worst case results except in SP case for S3. In the latter case both the best and the worst session may not co n tain more than one query because of high query entry cost. Third, among the best cases for PC the strategies S2 and S3 are almost equally good. For the SP case, the strategy S2 (varying the second word), S4 (extending from one word), and S5 (exten d ing from two words) lead to much higher gain than S1 and S3. An interes t ing trade -off in the SP scenario can be observed when the scannin g length is considered. In the best case the gain reached i n creases from S1 to S2. However, the average scanning length decreases (Fig. 2). In other words, a better result is achieved using the longer queries although a smal l er number of documents are scan ned on the ave r age; the ranking is simply better.
 Fig 2. Average number of scanned document snippets per qu e ry under cost constraint of 60 seconds. Table 4. Averaged CG, number of Queries (#q) and Scans per Query (s/q) for scenarios PC and SP , for 5 s trategies for the 10 best (b) and 10 worst (w) sessions, t ime constraint 90 se c onds (90 s) avg. CG avg. avg. Figure 3 shows the CG results when the sessions take 90 s e conds. In this case, the observations comply with the 60 second case. Di f fe r-ence between S3 X  X  best and worst CG values is clo s ing in the PC scenario; this is becau se of lacking further scanning o p tion s, there is now enough time to scan almost all 10 doc u ments for each query . S3 strategy has a maximum of 3 queries to ex e cute before the 5 keywords run out. This in turn confines the possible scanning space. It is also conspicuous that the di f ference between best and worst CG values in SP case is much larger than in PC case.
 Fig 3. Cumulated Gain under cost constraint of 90 seconds.
 When scanning in the best sessions of the PC and SP cases is co m-pared (Fig. 4), we noti ce that even though the scans per query va l-ues for SP case are higher than or similar to the PC case, the CG values are always poorer (Fig. 3). This is due to the smaller number of posed queries in SP case than in PC case. This fo l lows from the trade -off between query vs. scan costs.
 Fig 4. Average number of scanned document snippets per qu e ry under cost constraint of 90 seconds.
 Interestingly, the difference between the best and the worst se s sions both in terms of gain and average scan length remains gr eat in SP case, but fades away in PC case. In the latter, 90 seconds allows the searc h er to launch almost all queries and scan the best results in all cases. When the results are compared between di f ferent strat e gies, the strategy S4 with on average 5 scan s in PC case and approx i-mat e ly 6 scans in SP case (Fig. 4) produce sim i lar CG values as the other QM strat e gies (Fig. 3). Again, larger queries yield better rankings. On the other hand, S3 in SP case has less than 5 scans per query, and still achieves slig htly better CG results than S1 strat e gy. Table 5. Averaged CG, number of Queries (#q) and Scans per Query (s /q) for scenarios PC and SP , for 5 strategies for the 10 best (b) and 10 worst (w) sessions, t ime co n straint 120 seconds (120 s) Figure 5 shows the CG values under the cost constraint of 120 s e conds. In the PC case, the gaps between the best and worst CG values are diminishing. This can be explained so th at every stra t egy except S1and S4 has enough time to pose all the queries and employ much sca n ning. According to the experiment design, worst cases must also use up the allocated time, and this results in that there is enough time to launch all queries and scan the results. When the best sessions of the PC and SP cases are compared, we notice that there are no large di f ferences. Again, in Figure 6 we can see as many scans per query (S/Q) for S1 and S4 in the SP case as in the PC case for best sessions. Bes ides all the stra t egies for PC case show the same S/Q for 10 best and 10 worst sessions. Al t hough in SP case S/Q values diverge from each other, Figure 6 exhibits similar pa t terns as Figure 4. From Figure 5 one can conclude that, if there is enough time fo r searching, one should use at least two word queries for good results. If the queries are of lower quality like S1, then sca n ning matters. In short, the more you scan, the more you get.
 Fig 5. Cumulated Gain under cost constraint of 120 seconds.
 Fig 6. Average number of scanned document snippets per qu e ry under cost constraint of 120 seconds. We had three empirical and one methodological research que s tion. The three empirical ones were about effectiveness of diffe r ent QM strategies under time constraints, characteristics of the best and the worst QM sessions, and the stability of the o b served trends. The met h odological one was about proper evalu a tion of sessions under time constraints. We will consider each of the questions below. Strate gy effectiveness. Given a stringent time frame in the PC scenario, the user cannot use the entire vocabulary (all qu e ries) and perform exhaustive scanning for all queries. Short queries (strat e gy S1) are clearly inferior regarding session effectiveness. It seems reason a ble to invest on two to three word queries (S2, S3) because the evidence thereby added for ranking signif i cantly improves the quality of the results. This can also be seen in strat e gies S4 and S5, when they have enough time to advance b e yond the first query. When more time is allocated to searching, the weaker strategies catch up because there is more time for sca n ning the results and the weaker ranking effectiv e ness is not that critical. In the SP scenario the rules of the game change a bit. In a stri n gent time frame there is no time for tedious query input, and one must compromise toward short scanning of weaker quality ran k ings. The more effective strategies cannot be applied at all due to high query input cost. Again, when more time is all ocated, wea k er strategies catch up. In the longest sessions of S2 -S5, the gap between the best vs. worst sessions begins to close.
 Session characteristics. In the PC scenario, under stringent time constraints, the best sessions involved less queries and l onger scans than the worst sessions (Table 3). However, as the time allocation grows, the differences disappear. Between the best strat e gies in the PC case, both the number of queries and the average scan lengths increase as time allocation grows (Tables 3 -5). Corr e spondingly, in the worst sessions, the number of queries does not change as time grows, but the scan lengths grow. This is because the worst se s sions consume all possible queries even under the shortest time frame. Similarity with best sessions grows.
 In the SP scenario, under stringent time constraints, the best se s-sions also involved less queries and longer scans than the worst sessions (Table 3). As the time all o cation grows, the differences remain, probably due to shortage of time even in th e longer se s-sions. B e tween the best strategies in the SP case, both the number of queries and the average scan lengths increase as time allocation grows, the latter dramatically between 60 and 90 seconds (Tables 3 -4). Corr e spondingly, in the worst sessions , the number of qu e ries grows along time, but the scan lengths remain low. The worst b e-havior here means investing the effort in query input. Also here there were interesting differences in scan lengths between queries in sessions. All in all, if time all ows, two to three first query words that one identifies, followed by a longer scan, seem to provide reasonable perfo r mance, no matter what the strategy among S2 -S5 is.
 Effect of time. With limited time allowance, it seems important to make a good compromis e between providing evidence for ranking (longer queries) and scanning the search results. The compromise depends on the overall cost levels related to the stringency of the time frame and on the relations between cost types. This depends on the searching device. Expensive input favors scanning at length, cheap input favors better queries. The more time is avail a ble the less it matters how one searches  X  there will be time to identify the rel e vant documents.
 Evaluation methodology. Typical IR evaluation met rics are based on the quality of ranking alone. In session -based evalu a tion they must be applied with great care because they may be insuff i cient or even misleading. They may be partially insensitive to the user X  X  experience and observed costs and benefits . This is partic u larly critical, when user X  X  costs (time expenditure) are taken into a c count and the metric employs normalization, i.e. scaling the measurements to a predefined range such as [0, 1]. For example, the popular NDCG metric [15] and its non -dis counted cou n terpart NCG should be avoided in any compar i sons between searching environments, and between strategies within a given searching environment when input costs are taken into account. This is because the ideal gain vector used for no r malization i s read to vastly different lengths between strategies or enviro n ments. For example, consider Figure 7, which plots NCG over time for stra t egy S2 in the two scenarios. Fig 7. NCG vs. time comparison of PC and SP for S2 (41 To p-ics).
 Due to normalization (d ivision by the ideal cumulated gain ve c tor) the SP scenario seems to have better performance in the time frame from 40 to 135 seconds. This is due to (a) ranking being somewhat effective, and (b) the number of documents seen in each session: in the PC case the user sees 15 to 35 documents, but in the SP case only 5 to 20 documents in the indicated time frame. Figure 8 plots CG with the corresponding data and makes the difference clear. Fig 8. CG over time for S2 in scenarios PC and SP (41 To p ics). Simil ar pitfalls also plague the most classic metric, MAP. Co n sider the following two rankings observed for a given topic in two scena r-ios and/or strategies under the same time constraint (say, one mi n-ute; queries omitted and binary relevance for si m plicity): Further, assume that there are three relevant documents for the topic. The MAP for the ranking r1 is (1/9 + 2/10 + 0)/3 = 0.103 and for r2 (1 + 0 + 0)/3 = 0.333. Arguably, r2 is the better ran k ing, but if both requir e one minute, what is the user X  X  opinion? The first se s-sion co l lected twice as many relevant documents. Even within the un -normalized metric, such as CG, incorporating time in session -based evaluation has profound effects. Consider Figures 9 and 10. The f ormer gives traditional cumulated gain over ranks for strategies S1 and S3 for the 41 topics. The latter gives CG over time in the two scenarios. Fig 9. Traditional View, CGs over ranks for 41 topics, sc e na r ios PC and SP for strategies S1 (allowing 5 qu eries) and S3 (a l lo w-ing only 3 queries).
 In Figure 9, both scenarios PC and SP have the same observed effe c tiveness, because the evaluation focuses on the gain (CG) over the result ranks, no matter how long it takes to retrieve the doc u-ments. The two strat egies S1 and S3 differ in effectiveness, S3 providing far better effe c tiveness than S1. However, when time is taken into account (Fig. 10), the scenarios and strategies differ greatly from each other. Up to 60 seconds, S3 in the SP case is the long query. With enough time (180 sec.), S3 in SP catches up S3 in PC case. Also, PC and SP do not much differ for S1 due to the rel a tively low input cost and weak result qual i ty. Comparing Fi g-ures 9 and 10, it is easy to see that time drives interaction and pr o-foundly affects both user experience and effe c tiveness in se s sions in different scenarios.
 Fig 10. Time based View, CGs over time for 41 topics, sc e na r ios PC and SP for strategies S1 and S3 .
 Limitations. In our study we did not take into account the time, which users spend for pondering about possible query words. One might argue that the more words one needs to identify, the harder (and slower) per word it comes. However, the thinking time is the same between sessions using the same number of words. In add i-tion, this could be taken into account by revising subsequent query costs (Table 1). We have chosen to short -cut here in order to avoid too much complexity at this stage. Fu r the r more, we d o not consider the time users spend in examining documents. This may depend on the device used. This can be seen as an artificial lim i tation. Tackling it would, however, complicate analysis, and this is ther e fore left for later study. We did not simulate u ser X  X  learning during a session. Admittedly, learning from snippets and seen documents take place. This is not impossible to simulate but some challenges remain to be solved.
 We employed in the evaluation relatively limited query vocab u la r-ies, simple bag -o f -word queries, and relatively short time frames. The query vocabularies and structure are justified by query length statistics in many search environments [14], [29], and the time frames by our simulation capabilities. However, the time frames are for eff ective search time in sessions, excluding thinking and doc u-ment examination time. While the query v o cabularies are short, they are human -generated for this collection, and therefore more realistic than words mined, e.g., from known relevant doc u ments (in q rels). We did not cover all the imaginable complex sessions. However we employed idealized and literature -based sessions, which shed the light on the peculiar evaluation problems beyond the trad i tional rank -based eva l uation. This is a step forward while we are not su g gesting that anyone follows a single strategy consistently in real life.
 Our initial results are promising. First, the scenario, and to a large extent the device itself, dictate what kind of interactive behavior can be successful. Because real users do have limited resources and they use various devices having different prope r ties, our methodology has unquestionable user relevance and potential pragmatic value for the industry. Measuring the effe c tiveness of systems from the pragmatic point of v iew may i n crease the vali d ity of the results achieved. This may lead to greater user satisfa c tion. Secondly, our experimental r e sults suggest that strict time constraints determine some session stra t egies as the best strategies as they maximize CG. The str engths of our approach are:  X  The QM strategies S1 -S5 have an empirical real life groun d ing  X  The query vocabularies were generated by real test persons,  X  We were able to evaluate over 20M sessions in each sc e na r io; We have only taken the first steps. In future, we will study the d i mensions of variation related to users, systems, information sources and sessions t o construct more fine -grained scenarios expl i-cating hypotheses about user goals, learning, and behaviors to val i date evaluation measures used. [19] In this study, we have shown the necessity of a pragmatic evalu a-tion approach based on scenarios with explicit subtask costs under an overall time constraint. Effectiveness of various query modif i c a-tion and scanning strategies for two scenarios, namely, PC and SP is analyzed. Furthe r more, the characteristics of the best and the worst interactive sear ch sessions are examined. Expensive input favors scanning at length, cheap input favors better queries. The more time is available the less it matters how one searches  X  there will be time to identify the relevant doc u ments. We have shown that the effort r equired by searching devices and the ove r all search time allocation drive interaction and profoundly affect both user experience and effectiveness in se s sions in different scenarios. Moreover, we have also pointed out the inapt use of all normalized rank -b ased mea s-ures. Thus, we hope we could inst i gate new evaluation metrics for time -based co m parisons. This research was funded by Academy of Finland grant number 133021. [1] Azzopardi, L. 2007. Position Paper: Towards Evaluating [2] Azzopardi, L. 2011. The economics of interactive info r m a tion [3] Bates, M. J. 1979. Information search tactics. Journal of the [4] Bates, M. J. 1989. The Design of Browsing and Berrypic k ing [5] Beaulieu, M. 2000. Interaction in Information Searching and [6] Belkin, N. L. 1980. Anomalous States of Knowledge as a [7] Card, S. K., Moran, T. P., and Newell, A. 1983. The Ps y cho l-[8] Cleverdon, C.W., Mills, L., and Keen, M. 1966. Factors d e-[9] Dunlop, M. D. 1997. " Time Relevance an d Interaction Mo d e l-[10] Fidel, R. 1985. Moves in online searching. Online R e view , 9 [11] Hearst, M. A. 2011.  X  X atural X  Search User Interfaces. Co m-[12] Hersh, W., Turpin, A., Price, S., Chan, B., Kraemer, D., S a-[13] Ingwersen, P. and J X rvelin, K. 2005. The Turn: Integration of [14] Jansen, M. B. M., Spink, A., and Saracevic, T. 2000. Real [15] J X rvelin, K. and Kek X l X inen, J. 2002. Cumulated Gain -Based [16] J X rvelin, K. and Kek X l X inen, J. 2000. IR evaluation methods [17] Kamvar, M. and Baluja, S. 2007. Deciphering Trends in M o-[18] Karat, C -M., Halverson, C., Horn, D., and Karat, J. 1999. [19] Karlgren, J., J X rvelin, A., Eriksson, G, and Hansen, P. 2011. [20] Keskustalo, H., J X rvelin, K., Pirkola, A., Sharma, T. and Ly k-[21] Kuhlthau, C. C. 1991. Inside the Search Process. Jou r nal of the [22] Price, S.L., Nielsen, M.L., Delcambre, L.M.L., and Ve d sted, P. [23] Ruthven, I. 2008. Interactive Information Retrieval. In A n nual [24] Salton, G. 1970. Evaluation Problems in Interactive Info r m a-[25] Smith, C. L. and Kantor, P. B. 2008. User Adaptation: Good [26] Smucker, M. D. 2009. Towards Timed Predictions of H u man [27] Sormunen, E. 2002. Liberal Relevance Criteria of TREC  X  [28] Spink, A. 1997. Study of Interactive Feedback during Med i-[29] Stenmark, D. 2008. Identifying Clusters of User Behavior in [30] Su, L.T. 1992. Evaluations Measures for Interactive Info r m a-[31] Turpin, A. and Hersh, W . 2001. Why Batch and User Eva l u a-[32] Turpin, A. and Scholer, F. 2006. User Performance ver sus [33] Vakkari, P. 2000. Cognition and changes of search terms and 
