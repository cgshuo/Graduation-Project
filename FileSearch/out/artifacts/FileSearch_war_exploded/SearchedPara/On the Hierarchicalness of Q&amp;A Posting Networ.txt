 Many internet users turn to online knowledge exchange com-munities to get information they cannot find elsewhere. Ques-tion&amp;Answer sites are one of the largest hosts of such com-munities where users reciprocally answer their questions. Research on expert identification in online communities tries to rank community members by their expertise or to sep-arate experts from non-experts. Until now proposed al-gorithms for expert identification do not perform well on all datasets. We present an analysis of the structures of topic-induced sub-communities of Question&amp;Answer com-munities. This analysis aims to provide a basis for expert identification research. The results from the analysis of the network structures explain why common expert identifica-tion algorithms are not suitable for all communities. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Human Factors, Measurement Social network analysis, online communities, expert identi-fication, network structures
Question&amp;Answer (Q&amp;A) sites (among other Yahoo! An-swers, Answerbag and LycosIQ) have established themselves as a social information retrieval instrument on the Web 2.0. Their providers aim to create knowledge bases that can com-pete in information retrieval with search engines by letting humans answer questions instead of algorithms.

Q&amp;A sites are platforms where a user community recip-rocally answers questions. In contrast to other forums these services do not facilitate discussions but encourage their users to write just one single answer to a question that con-tains all requested information. Repeated answers are often prohibited as each user should correct the original answer if it is incorrect or incomplete. However, a comment function-ality allows everyone who does not have an answer, to ask for a more detailed problem description or to contribute in some other way. These regulations are expected to generate a collection of useful and easily retrievable knowledge.
Users who seek for information by asking a question or searching in the already available collection of questions and answers have an interest to get to know how reliable the provided information is. Therefore, all major Q&amp;A sites provide a rating system where members can rate the answers of others. Unfortunately, this feedback on the quality of a particular answer is often sparse. A rating system for members avoids this problem as the general reliability of a user gets evaluated.

Several authors tried to apply statistical measures and ranking algorithms for internet search engines to identify experts in Q&amp;A communities. The ranking quality showed to depend on the community X  X  topic (e.g. [2]). Also, sim-ulations indicate that the structural characteristics of the analyzed networks influence the performance of the ranking algorithms [9].

To explain the dependence of the quality of expertise rank-ing algorithms on the topic of a community, we will an-alyze, whether the assumption that answering is an indi-cator of expertise is justified. The following analysis of topic-induced sub-communities will show differences respec-tively parallelisms of the posting networks of these sub-communities and it tries to evaluate whether common meth-ods for expert identification are applicable.

The remainder of this paper is organized as follows. In section 2, an overview of recent work done in the field of expert identification is provided. Section 3 describes how we extracted topic-induced sub-communities in this study and gives a general description of their properties. In section 4 we analyze the structures of these sub-communities. Finally, in section 5 we draw conclusions from the results of the structural analysis of communities.
Expert identification in web communities has been dis-cussed by several authors. Zhang et al. [9] evaluated several ranking algorithms on the Sun Java Forum. They compared the algorithmic rankings with manually created rankings by humans and found relatively high correlations for all tested Members 9084 9282 7644 8550 16584 4325 3142 13607 4783 7584
Registered Members 6649 6385 5912 6490 10822 3615 2824 11698 4412 6587 unregistered members 2635 2897 1732 2060 5762 710 318 1909 371 997
Questions 9709 10993 7972 11341 23770 4303 2817 21213 6058 6939 Answers 29902 32958 38010 41994 91828 10554 6856 117505 25913 25167
A/Q ratio 3.1 3.0 4.8 3.7 3.86 2.4 2.4 5.5 4.3 3.63 ranking algorithms. Furthermore, they presented two net-work models to simulate the Java forum network. From their simulations they assume, that experts do not just an-swer questions from users with medium expertise which in turn answer the question of users with low expertise. In fact, it seems that users answer all questions they are able to answer and do not distinguish between easy and compli-cated ones. The ranking performance showed a significant dependency on the network model for all algorithms.
Bouguessa et al. [1] worked on a Yahoo! Answers dataset and ranked users to separate authoritative from non-authori-tative ones. This separation needs besides a ranking a cut-off level to decide how many users could be regarded as experts. They provided only a basic evaluation of the precision of their method but omitted to take recall into account, to speak in terms of information retrieval.

Jurczyk and Agichtein [5] also used a Yahoo! Answers dataset and compared the algorithmic expertise ranking re-sults with user ratings of their peers. They experienced that the ranking performance differs by question topic.
A third work on Yahoo! Answers data was conducted by Chen and Nayak [2]. They proposed a method to iden-tify experts that takes user ratings of answers into account. Their expertise score basically uses the number of answers and the number of answers rated as best answer to a ques-tion. The ranking results were evaluated against human ratings for three topical categories. While good results were received for all categories, the ranking quality for  X  X cience &amp; math X  was significantly better than for  X  X rts &amp; humanity X  and  X  X ports X . The quality difference for the latter two was low by contrast.

While all aforementioned approaches try to identify exper-tise by analysing the structure of the posting network other approaches for expertise identification are based on the text content people generate. Reichling et al. [8] used Latent Se-mantic Indexing to identify peoples X  fields of expertise. Liu et al. [7] tried to reach the same aim by employing several state-of-the-art information retrieval techniques.
In this paper the English-language Q&amp;A forum Answerbag (http://www.answerbag.com) and the German-language com-munity LycosIQ (http://www.cosmiq.de after rebranding) will be analyzed. The dataset of Answerbag contains all questions and answers written from the forum opening on May 27th, 2003 up to November 3rd, 2008. The LycosIQ dataset contains all question and answers from December 9th, 2005 up to June 6th, 2008. Questions rejected by the platform operators for various reasons (e.g. spam) and the respective answers have been omitted from the analysis.
The two analyzed Q&amp;A-communities provide features to allow users to browse through the questions. On LycosIQ users can assign tags to questions. On Answerbag there is a hierarchical tree of topics and each question is attached to one topic. This information has been used for the division of the communities into sub-communities. We generated sub-communities from LycosIQ and Answerbag data for the five topics mathematics, physics, religion, politics and medicine as we expected different communication behavior for science topics and philosophical and social science topics. Further-more, health has been selected as a fifth topic because we assumed that it is a topic where beside professionals every-body can contribute from his own experience.
 Table 1 shows basic properties of the sub-communities. The first peculiarity is that for both Q&amp;A forums the aver-age number of answers per question is significantly lower for the topics mathematics and physics (3.1 respectively 3.0 on LycosIQ, 2.4 for both topics on Answerbag) and questions concerning religion get especially numerous answers on av-erage (4.8 on LycosIQ, 5.5 on Answerbag). Noteworthy is also the ratio of questions from users that are not part of the community (unregistered users without user account at Ly-cosIQ, respectively users who decided that their user name should not be displayed with their questions at Answerbag). The mathematics (31.9% on LycosIQ resp. 25.5% on An-swerbag), the physics (31.4% resp. 15.7%) and to a lesser extent the health community (27.1% resp. 13.8%) are espe-cially attractive for  X  X isitors X . As LycosIQ does not require signing up before posting questions, it is very convenient for information seekers to turn to this community.
Out of topic specific subsets of questions and answers we created community expert networks (CEN) [9]. A CEN is a directed, weighted graph where directed edges go from an asker to all respondents of one of his questions (see fig. 1). The edge weights reflect the number of questions from one asker a respondent has answered. The network is loop-free as answers get disregarded if asker and answerer are identical. The underlying assumption for this network design is that answering a question is an indication that the respondent has a higher level of knowledge as the asker in the corresponding domain. Therefore, the link direction goes from lower to higher expertise.

The model behind this assumption about answering be-havior is in its simplest and ideal variant as follows: (1) All members of a community have a objectively measurable ex-pertise concerning the topic the community is dealing with. Figure 1: Generation of a Community Expert Net-work (CEN) from questions (Q), answers (A) and their authoring users (U) The set of members and the relation  X  X as equal or higher expertise X  form a total order. (2) Only members who have a higher expertise than the asker of a question are able to write answers.

Testing to which extent communities comply with the as-sumptions will be analyzed later on. First, some implica-tions of strict model compliance will be discussed. The CEN of a community that matches this model would be a directed acyclic graph (DAG) (see figure 2). Note, however, that fig-ure 1 shows a CEN which is not an acyclic graph and thus does not conform to the ideal model described above. If a graph conforms to the model, only the amount of pairwise relations (observed questions and answers) limits the ability to reconstruct the hidden expertise order.

Given the model is correct in principle but real-world datasets contain random noise (e.g. a member posted an answer to different than the intended question), the com-munity expertise networks are partially cyclic and identify-ing experts gets harder. Now, the member at the head of a directed edge has not necessarily a higher expertise than the member at the tail. Heuristics need to be employed to create an order from the noisy data.

If a dataset seems to contain a high ratio of noise, the question arises whether the original assumptions were cor-rect and the assumed noise is in fact noise or whether the assumptions were wrong. A deviation from the ideal model that is not independent of the question could result for ex-ample from the fact, that users incorrectly believe that they know an answer and send incorrect replies. The correctness of the model assumptions are crucial for all link-based ex-pertise identification algorithms used by [1], [2], [5] and [9] (e.g. weighted indegree, HITS, PageRank). For example, Figure 3: Cycles with 2 and 3 vertices: reciprocal dyad and cyclic triad Table 3: Analysis results for filtered Answerbag net-works the weighted indegree measure counts how many answers a user has posted. If for a given community writing answers in no (good) indication for superior expertise, the expected ranking quality of weighted indegree is low.
The CENs generated for the five mentioned topic-induced sub-communities of the two Q&amp;A sites have been analyzed for their hierarchicalness. With hierarchicalness we denote the degree of the similarity of a graph to a DAG. This sim-ilarity could be measured e.g. by the minimum number of edges which have to be removed to transform a graph into a DAG. Finding this minimum set of edges is known as the minimum feedback arc set problem. Because this problem is NP-complete and even heuristic algorithms (see e.g. [3]) were not capable of processing our datasets in reasonable time, we assess hierarchicalness by an analysis of cycles. Ta-ble 2 shows the results of this analysis of the posting behav-ior.

The two smallest network structures of loop-free graphs that have a cycle are the reciprocal dyad and the cyclic triad (see figure 3). The measure
H 1 = observed # cyclic structures is a noise-robust indicator for hierarchicalness. The number of expected reciprocal dyads has been calculated according to Katz and Powell [6] and takes the given degree distribu-tions into account. The number of expected cyclic triads is the expected value for a network with a given number of nodes and vertices.

The comparison of the H 1 ( dyad )and H 1 ( triad ) values for the different sub-networks clearly shows the substantially lower H 1 values of the mathematics and physics networks. This shows that the communication in the religion, politics and health communities is much less hierarchical.
Harper et al. [4] showed that not all postings in a Q&amp;A forum are sent to ask for respectively to provide information. Table 4: Questions expected to be conversational by keyword list. The Lycos datatset has been already filtered manually.

A.bag 249 139 2506 1181 614
Lycos 61 38 52 107 78 They distinguish informational and conversational questions and tried to automatically classify questions to one of these categories. According to their analysis, the keywords  X  X o you X ,  X  X ould you X ,  X  X ou think X  and  X  X s your X  are the best predictors for conversational questions and amount for about 40% of all conversational questions.

As we can expect that answering those questions is not as-sociated with superior knowledge, conversational questions do not fulfill the model assumptions for a CEN. Therefore, the topic networks generated from filtered data sets without conversational questions should be more hierarchical. The phrases mentioned above have been used to filter conversa-tional questions from our data sets. The analysis results for the Answerbag networks shown in table 3 confirm the as-sumption about the influence of conversational questions on the hierarchicalness. At LycosIQ the portal guidelines ask the members not to write conversational questions. Moder-ators oversee the compliance with this rule and delete non-compliant questions. Accordingly, the LycosIQ data con-tained only very few questions with (the German language equivalents of) the keywords from above. Table 4 shows how many question contained these keywords.

The results for the Answerbag networks show that conver-sational questions significantly contribute to the non-hierar-chicalness of the CENs. A more sophisticated filtering ap-proach would probably be able to remove even more  X  X is-turbance X  from the networks. However, for LycosIQ, where conversational questions are deleted by moderators, the re-ligion, politics and health networks still show a strong devi-ation from strict hierarchicalness.
In this paper we presented an ideal model for communi-cation in a Q&amp;A community that reflects the assumptions most link-based expertise identification methods make. Fur-thermore, we presented measures to evaluate whether an observed communication network conforms to this model. Our analysis showed that filtering conversational questions reduces the deviation from the ideal model.

From the analysis of the community expert networks of five topic-induced sub-communities for each of the two Q&amp;A communities LycosIQ and Answerbag we can conclude that there is a correlation between the type of topic a sub-com-munity is dealing with and the communication behavior. General statistical figures (posting frequency etc.) as well as the distribution of answers per user and the analysis of the degree of hierarchical communication structure showed a clear separation between mathematics and physics on the one hand and religion and politics on the other hand. The fifth considered topic health showed mostly characteristics similar the latter two.

The findings about hierarchicalness are important for the design and application of expertise identification algorithms. Topics networks with a strong deviation from hierarchical-ness will hardly provide good expert retrieval results with any algorithm. A test for hierarchicalness should be con-ducted to assess if expert identification from a CEN is suit-able. A reason for a non-hierarchical structure might be a too broadly defined topic. This can be tested be determin-ing the hierarchicalness of sub-topics and, if necessary, the scope of the definition of the topic communities could be reduced.
 The research leading to these results has received funding from the European Community X  X  Seventh Framework Pro-gramme FP7/2007-2013 under grant agreement n  X  215453 -WeKnowIt. [1] M. Bouguessa, B. Dumoulin, and S. Wang. Identifying [2] L. Chen and R. Nayak. Expertise analysis in a question [3] T. Coleman and A. Wirth. Ranking tournaments: [4] F. M. Harper, D. Moy, and J. A. Konstan. Facts or [5] P. Jurczyk and E. Agichtein. Discovering authorities in [6] L. Katz and J. H. Powell. Measurement of the tendency [7] X. Liu, W. B. Croft, and M. Koll. Finding experts in [8] T. Reichling, K. Schubert, and V. Wulf. Matching [9] J. Zhang, M. S. Ackerman, and L. Adamic. Expertise
