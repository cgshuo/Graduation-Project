 Rademacher Averages and the Vapnik-Chervonenkis dimen-sion are fundamental concepts from statistical learning the-ory. They allow to study simultaneous deviation bounds of empirical averages from their expectations for classes of functions, by considering properties of the functions, of their domain (the dataset), and of the sampling process. In this tutorial, we survey the use of Rademacher Averages and the VC-dimension in sampling-based algorithms for graph anal-ysis and pattern mining. We start from their theoretical foundations at the core of machine learning, then show a generic recipe for formulating data mining problems in a way that allows to use these concepts in efficient randomized al-gorithms for those problems. Finally, we show examples of the application of the recipe to graph problems (connectiv-ity, shortest paths, betweenness centrality) and pattern min-ing. Our goal is to expose the usefulness of these techniques for the data mining researcher, and to encourage research in the area.
 H.2.8 [ Database Management ]: Database Applications X  Data mining ; G.3 [ Probability and Statistics ]: [Proba-bilistic algorithms (including Monte Carlo)] Betweenness Centrality; Frequent Itemsets; Graph Mining; Pattern Mining; Randomized Algorithms; Tutorial
Random sampling is a natural technique to speed up the execution of algorithms on very large datasets [6]. The re-sults obtained by analyzing only a random sample of the dataset are an approximation of the exact solution. When only a single value must be computed, the trade-off between dations are presented in the first part, where we introduce the Rademacher Averages [3, 10] and the VC-dimension [19], showing how they allow to answer the questions posed in the introduction, and how they are related to each other. In particular, we show the key theorems that allow to com-pute an upper bound to the sample size sufficient to obtain a high-quality approximation of the quantities of interest, uni-formly. The bounds depend linearly on the Rademacher av-erages and on the VC-dimension. We then focus on comput-ing, estimating, and bounding the Rademacher averages and the VC-dimension, which is a key step in the process of us-ing them to develop algorithms. We show a number of basic examples of classes of functions with finite and infinite VC-dimension and discuss different techniques for developing analytical bounds and empirical estimations. The examples will range from toy examples (e.g., axis-aligned rectangles, half-spaces, and sinusoidal functions) to much more complex instances that are presented in research papers (e.g., graph neighborhood functions, neural networks [2], and shortest paths [1]).
 showing how to use Rademacher averages and VC-dimension to develop sampling-based algorithms for data and graph mining problems. We start by presenting a generic recipe for developing such algorithms, which eases the application of the techniques and the analysis of the algorithms. We then show a number of examples of application of this technique for different graph and data analysis problems, including network connectivity [9], shortest paths algorithms [1], be-tweenness centrality computation [13], and frequent pattern mining [14 X 16], and set covering [5].
 will focus on more advanced material, to encourage the au-dience to further explore the field of statistical learning the-ory, and to stimulate discussion and research on using the results from this field to develop data mining algorithms. Specifically, we will discuss: PAC-Bayesian bounds [3, 17], which show a connection between the typical frequentist ap-proach followed in statistical learning theory to the Bayesian probabilistic approach and may be useful for data mining al-gorithms on uncertain or probabilistic data, and a selection of the extensions of VC-dimension to real-valued or non-binary functions, including pseudodimension [12], Natarajan dimension, and fat-shattering dimension [8].
We set up a mini-website ( http://bigdata.cs.brown. edu/vctutorial ) with links to the slides that we use for the presentation, and a bibliography of theoretical and application-oriented works about VC-dimension and Rademacher Aver-ages. This work was supported by NSF grant IIS-1247581 and NIH grant R01-CA180776. [1] I. Abraham, D. Delling, A. Fiat, A. V. Goldberg, and
