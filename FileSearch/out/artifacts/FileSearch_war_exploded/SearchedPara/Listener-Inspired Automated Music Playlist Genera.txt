 The objective of this PhD research is to deepen the un-derstanding of how people listen to music and construct playlists. We believe that further insights into such mech-anisms can lead to enhanced music recommendations. We research on the exploitation of user-generated data in the context of on-line music services, since it constitutes a rich and increasing source of information of user behavior. The research carried out so far has centered on the scenario of producing a single artist recommendation. Concretely, in this paper we show how to mitigate the cold-start problem for new artists, elaborating on our findings on the combined effect of users X  listening histories and users X  tagging activity. As future research, we will investigate how improved tech-niques to exploit user-generated data can also be applied to the task of producing sequential recommendations, like playlists. We are particulary interested in creating music playlists similarly as users would do, and in finding mecha-nisms to make such music streams adapt to users X  feedback on-line.
 H.3.3 [ Information Search and Retrieval ]: Information filtering; H.5.5 [ Sound and Music Computing ]: Model-ing Music recommendation, playlist generation, recommender systems, cold-start problem, collaborative filtering
Given the amount of digitized music available, music rec-ommendations and automatically generated playlists assist users in navigating, organizing and exploring music collec-tions.

The interaction of users with on-line music streaming ser-vices provides an increasing amount of data, mostly in the form of implicit feedback (e.g., the number of times a user listened to an artist). Such feedback is easy to collect in comparison to elicited user ratings (i.e., explicit feedback), and has been proven powerful in collaborative filtering ap-plications [12, 24]. Part of this PhD research is devoted to understanding and improving the techniques required to ex-ploit implicit feedback datasets for the task of music recom-mendation (including automated playlist generation). For the task of recommending a single artist, we show how to mitigate the cold-start problem for new artists by extend-ing state-of-the-art collaborative filtering models for implicit feedback with additional tagging activity.

As future work, we want to investigate the automated generation of music playlists. As already suggested in [2, 7, 21, 29], user-generated data can be successfully exploited for this task. We will explore how our extensions to state-of-the-art collaborative filtering models can be translated into sequential recommendations as well.
 The remainder of this paper is organized as follows. In Section 2 we review the relevant work related to (1) the cold-start problem in collaborative filtering, and (2) the auto-mated generation of playlists. We elaborate on our findings to mitigate the cold-start problem in Section 3. We outline future directions for music playlist generation in Section 4.
In the context of music streaming services, users X  listen-ing activity represents an abundant source of data. This is commonly referred to as implicit feedback , because the users do not provide it actively. In contrast, the extensively stud-ied explicit feedback is actively provided by users (e.g. rat-ings, or  X  X ikes X ). Collaborative filtering methods for explicit feedback, based on the factorization of the user-item-rating matrix [16], can be applied to implicit feedback datasets as well. In order to do this, the factorization models need to be adapted, weighting the observations according to their relative importance in the dataset [12, 24].

Hybrid models can be used to mitigate cold-start situa-tions in collaborative filtering [1]. Models based on matrix factorization can fuse together the user-item feedback ma-trix and additional sources of data [9, 19]. The decompo-sition into latent factors is then reinforced, and the lack of user-item feedback is compensated with external data.
Evaluation has often focused on optimizing the accuracy achieved for predicting ratings. However, accuracy may not be enough to satisfy users [22]. Alternatively, [15] evaluates the performance of different recommender systems on the basis of ranked lists of recommendations. This evaluation methodology is adapted in [12] to deal with implicit feedback and it is further applied in [17].
Automated playlist generation can be based on different sources of data, e.g., musical features extracted from the au-dio signal [23], social web data [11], or usage data, like users X  listening histories [6] or manually created playlists [21].
The context and application of playlists is an important aspect too, because ultimately, the music recommendations have to fit the users X  needs. We find examples of application-specific automated playlist generators for background music while doing sports [20], while driving [3], or for discovery and exploration of new music [28, 29].

Manually generated playlists are organized following an internal logic and coherence [8]. In automated music playlist generation, we find three main approaches to favor the gen-eration of coherent sequences. First, by imposing smooth transitions between tracks [10, 14, 25]. Second, by learning from manually generated playlists [2, 7]. Third, by using exploration-exploitation techniques to build playlists that adapt to users X  feedback on-line [18, 28, 29].

For a complete survey on automated generation of music playlists please refer to [5].
We provide a concise description of our collaborative fil-tering model hybridized with tagging activity and show how to evaluate the quality of its recommendations. For a com-plete derivation of the model, please refer to our previous work [27]. For the task of artist recommendation, we show how the proposed model mitigates the cold-start problem for new artists and provide a qualitative example. Our model naturally extends the ones described in [9, 12]. The main contribution is that we identify tags as a source of positive-only feedback (even if the meaning of a tag is se-mantically negative). Therefore, we use a weighting scheme for the tags similar to the one used for implicit feedback, which proves to be beneficial.

Given a system with N users and M artists, the counts for each user-artist pair are stored in a matrix R  X  N N  X  M Each row of R corresponds to a user, and each column to an artist. Assuming that users have tagged artists using T different tags, two matrices are defined. The counts for each user-tag pair are stored in a matrix T U  X  N N  X  T , with rows relating to users and columns relating to tags. The counts for each artist-tag pair are stored in a matrix T A  X  N M  X  T with rows relating to artists and columns relating to tags.
We define binary matrices e R , e T U and e T A , such that all non-zero entries in the matrices become 1, indicating that a user-artist, user-tag or artist-tag pair has been observed. All zero entries remain as zeros. A weight function is defined: We jointly factorize e R, e T U , e T A into three D -rank matrices features for users, artists and tags respectively) such that the following cost function is minimized: Matrices e R , e T U and e T A are reconstructed using combina-tions of P , Q and X . In the first term of equation (2), is the entry of e R corresponding to user u and artist a . P the row of P corresponding to user u , and Q a is the row of Q corresponding to artist a . The squared reconstruction error is weighted using a function of the actual number of counts in R ua according to equation (1), and summed over all the user-artist pairs in the training matrix R tr . 1 The second and third terms can be described analogously, with the dif-ference that the squared reconstruction errors are summed over all user-tag pairs and artist-tag pairs respectively. Spe-cific parameters  X  ,  X  and  X  are used for each term, and are determined by grid search. A regularization term involving the Frobenius norm of P , Q and X is added to prevent the model from over-fitting. The regularization parameter  X  is also determined by grid search.

The cost function in equation (2) is minimized using Al-ternating Least Squares, similarly as in [9, 12]. The details can be found in [27].

Once the model is trained, the expected binary preference for all the user-artist pairs is predicted as Z = PQ T . Note that the tags X  factor matrix X is not directly involved in this product, although it is important to learn the users X  and artists X  factor matrices P and Q .
We follow the evaluation methodology proposed in [15] and extended in [12] to deal with implicit feedback datasets.
The observed user-artist pairs are split into training and test sets to perform 5-fold cross validation. Each user has approximately 80% of the listened artists in the training set and 20% in the test set. For each observed user-artist pair u,a in the test set, a random list of artists (not including a ) is drawn. The list is then ranked according to the learned preferences of user u . Finally, a is inserted in the sorted list, and its percentile rank within the list is stored as rank If a is ranked among the top positions of the list, then its percentile rank is close to 0%. If it is ranked in last positions, then its percentile rank is close to 100%.

This process is performed over the 5 splits, so that rank is available for all the observed user-artist pairs in the dataset. The expected percentile rank is defined as a weighted average of rank ua :
Including also the zero entries of R tr , as described in [12]. Figure 1: Performance of CF and Hybrid as a func-tion of the number of observed occurrences of the artists in the dataset. The dots correspond to the expected percentile rank and the error bars display 95% basic bootstrap confidence intervals. The dif-ferent models are dodged to avoid overlapping. The top line correspond to the baseline model. The lower corresponds to the proposed model.
We compare the proposed model that uses implicit feed-back and tags (i.e., Hybrid ), with its simpler version, where plain collaborative filtering ( CF ) on implicit feedback data is considered.

We use a dataset of Last.fm listening events, top tags used by users and top tags applied to artists, collected through the Last.fm API. 2 The dataset includes 2 , 902 users, 71 , 223 artists and 687 , 833 observed user-artist pairs. We addition-ally obtain 630 unique tags for 600 users (20% of all users) and 12 , 902 unique tags for 67 , 332 artists (95% of all artists). We will focus on showing how the proposed model mitigates the cold-start problem for new artists.

We explore the effect of the number of observed occur-rences of an artist in the dataset on the performance of the recommender systems. If an artist has only been listened by few users, plain collaborative filtering has little information on the relations between this artist and the users. Then, hybridizing with tags will prove advantageous.

Each user-artist pair in the dataset is assigned to a sub-set, defined by the number of occurrences of the artist in the dataset. Equation (3) is computed for each subset, sum-ming only over the corresponding user-artist pairs. Figure 1 shows the expected percentile rank for CF and Hybrid for each subset. Artists with a single occurrence in the dataset are poorly recommended by CF . Thanks to the tags, Hybrid has more information about them and produces better rec-ommendations. The more often an artist is observed in the dataset, the smaller the difference of performance becomes.
We provide an example to give a qualitative explanation of the effect of using tags. We train CF and Hybrid using 80% of the data. Table 1 shows the predicted preference http://www.last.fm/api Table 1: Prediction of CF and Hybrid for the pref-erence Z ua of a selected user for artists known to be relevant for the user (but hidden from the training). Z ua of a selected user u for four different artists he or she listened to, but belong to the 20% of data withheld for test. CF is not able to identify that the first and the fourth artist were interesting for the user. The reason is that these artists were not observed at all in the training set. For the second and third artists, CF predicts low values. Hybrid is able to identify the artists without listening examples and predicts higher values for the other two. Comparing the artist tags in the user X  X  training data with the artist tags of these four artists, we find several coincidences, like catalan , indie pop or pop surrealista (surreal pop). Therefore, Hybrid is able to identify the relations.
As next steps within this PhD research, we will explore how the described techniques to exploit user-generated data can be translated into automated playlist generation. In this line, the modeling of radio streams presented in [2] and the adaptive learning approach taken in [29] are interesting stepping stones. However, since both are based on plain collaborative filtering, they can not provide sensible esti-mates for artists in the long tail or for which little listening information is available. We will explore how our hybrid model can enhance their performance, probably mitigating the cold-start problem.

We are particularly interested in the adaptive learning capabilities of the reinforcement learning models described in [18, 28, 29]. Although implicit feedback can be used to estimate the expected user preferences, these models up-date their knowledge of the current user needs on the ba-sis of elicited explicit user feedback. We consider that this does not reflect a realistic use case, where ideally, the model should learn with minimal effort from the user. We will in-vestigate whether similar reinforcement learning approaches can be developed, updating the user X  X  current needs on the basis of skipping behavior only (see e.g., [13, 23]).
The evaluation of the automated generation of playlists is a crucial aspect per se. We aim at generating playlists in a similar way as listeners would do. For this reason, we need to evaluate our models by means of user studies and, if possible, by means of actual large-scale on-line experiments. However, user studies also have limitations, and large-scale on-line experiments may be too complex in the context of a PhD research. Therefore, we will also need to investigate on improved off-line evaluation methods, probably in line with the approaches described in [7, 21], where actual manually generated playlists are used as a reference.

Websites, movies or videos are usually not organized in playlists, but for example, in the case of book recommenda-tions, framing the recommendation as a sequential problem has been proven beneficial in [26]. We will evaluate whether our model is also valid to produce recommendations in fields other than music. If so, the planned work for automated mu-sic playlist generation will also be explored for those fields.
Manually preparing playlists has been found to be a com-plex task. It involves the engagement of the creator and may require their creativity [8]. The development of algo-rithms able to imitate creative processes, is probably one of the most interesting challenges in automated playlist gener-ation. We will research the field of computational creativity (see e.g., [4]) to explore how we can make computers gener-ate playlists as listeners would do.
This research is supported by the Austrian Science Fund (FWF): P25655. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] N. Aizenberg, Y. Koren, and O. Somekh. Build your [3] L. Baltrunas, M. Kaminskas, B. Ludwig, O. Moling, [4] M. A. Boden. The creative mind: Myths and [5] G. Bonnin and D. Jannach. Automated generation of [6] K. Bosteels, E. Pampalk, and E. E. Kerre. Evaluating [7] S. Chen, J. L. Moore, D. Turnbull, and T. Joachims. [8] S. J. Cunningham, D. Bainbridge, and A. Falconer. [9] Y. Fang and L. Si. Matrix co-factorization for [10] A. Flexer, D. Schnitzer, M. Gasser, and G. Widmer. [11] N. Hariri, B. Mobasher, and R. Burke. Context-aware [12] Y. Hu, Y. Koren, and C. Volinsky. Collaborative [13] Y. Hu and M. Ogihara. NextOne player: a music [14] P. Knees, T. Pohle, M. Schedl, and G. Widmer. [15] Y. Koren. Factorization meets the neighborhood: a [16] Y. Koren, R. Bell, and C. Volinsky. Matrix [17] Y. Li, J. Hu, C. Zhai, and Y. Chen. Improving [18] E. Liebman and P. Stone. DJ-MC: a [19] H. Ma, T. C. Zhou, M. R. Lyu, and I. King. Improving [20] N. Masahiro, H. Takaesu, H. Demachi, M. Oono, and [21] B. McFee and G. R. Lanckriet. The natural language [22] S. M. McNee, J. Riedl, and J. A. Konstan. Being [23] E. Pampalk, T. Pohle, and G. Widmer. Dynamic [24] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. Lukose, [25] T. Pohle, E. Pampalk, and G. Widmer. Generating [26] G. Shani, R. Brafman, and D. Heckerman. An [27] A. Vall, M. Skowron, P. Knees, and M. Schedl. [28] X. Wang, Y. Wang, D. Hsu, and Y. Wang.
 [29] Z. Xing, X. Wang, and Y. Wang. Enhancing
