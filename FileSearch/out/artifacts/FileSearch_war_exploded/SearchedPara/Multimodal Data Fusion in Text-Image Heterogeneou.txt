 The explosion of the web data has witnessed multi-modality. Every day, millions of people over the world discuss hot topics on Twitter, share travel photos on Flickr or life videos on YouTube, add new friends on Facebook, etc. These multimodal data are potential good resources for valuable information and provide new applications for us, one of which is the recommendation. For multimodal recommendation, if a user browsed a flower image, the recommended results to him (her) should contain both similar flower images and texts about the flower, such as species, habitat, habit of the flower. However, in traditional single modal recommendation, such task implements hardly when there is not explicit links between above images and texts. 
It is useful to fuse multimodal data. By fusing multimodal data, we can obtain a global and vivid view of events or a better recommended result. Besides, the fusion makes the similar query of the recommendation more effectively and efficiently. However, it is difficult to fuse multimodal data. Firstly, computing the similarity of objects from different modalities is very hard. Different modalities have different features, and most feature spaces are not compatible. Secondly, multimodal data is often in big size, and finding an efficient fusion method is also tough. 
In this paper, we propose an approach of multimodal data fusion, which (1) creates a text-image heterogeneous graph to represent the semantic relationship of texts and images, (2) fuses the graph based on graph clustering, and (3) applies the fusing results to a new social media recommendation strategy. Fig.1 shows our framework. 
The rest of the paper is organized as follows. Section 2 describes the heterogeneous graph for multimodal data. The method of multimodal data fusion is proposed in Section 3. Section 4 shows the experiments. We conclude our work in Section 5. A heterogeneous graph G =( V , E , W ) is an undirected weighted graph. Each vertex v  X 
V is an entity of multimodal data. Each link e  X  E is a relationship between entities, denoted by its weight w  X  W . Our heterogeneous graph consists of two modalities: text (white) and image (black), is shown in Fig.1. Although the heterogeneous graph can effectively model the relationship of texts and images, it is very difficult. First, it is a tough job to calculate the similarity of entities in a large data set. Second, due to the incompatible feature spaces of texts and images, it is hard to calculate their semantic relevance. The following details the creating of each part of our heterogeneous graph. Text Modal-Graph. Text modal-graph keeps the semantic relationship of texts. For words detected from it. Different datasets have different characteristics and require different similarity measurements. As previous work [1] confirmed the effectiveness of Boolean model for microblog texts, words in the bag only have boolean value. Thus, the text similarity is the Jaccard coefficient of word bags. To speed up, an in-verted index of words is created to point out which texts share same words. Image Modal-Graph. Image modal-graph keeps the content-based relationship of images. First of all, we extract the SIFT (Scale-Invariant Feature Transform) features [2] as image features. To compute image similarity, many works use bag of visual Here, the image similarity is the Jaccard coefficient of SIFT features. To speed up, images with similar color histograms are grouped together, using p -stable LSH [4]. Text-Image Similarity Graph. Text-Image similarity graph keeps the semantic rela-tionship between text and image entities. Generally, if entities belong to the same document, they must be semantically relevant. Due to the incompatible feature spaces of different modalities, the cross-modality similarity is based on the Boolean model of co-occurrence. Thus, if two entities from different modalities are semantically rele-vant, their weight w is 1; otherwise w is 0. Multimodal data fusion can provide a wider and more accurate description of the object. During the fusion, each modality exchanges information with others. The ex-change is based on the semantic relationship among entities of multimodal data. Graph Clustering. Although entities are connected in the modal-graph, they may not be semantically relevant, due to the imperfect similarity model. Intuitively, if entities are in the same cluster, they are semantically relevant. Hence, we cluster entities in each modal-graph, based on a modified SCAN [5]. Data Fusion. After clustering, the fusion exchanges information among modal-graphs, according to the semantic relevance. If two texts (images) as well as their corresponding images (texts) are semantically relevant, their relationship will be enhanced. If two texts (images) as well as their corresponding images (texts) are semantically irrelevant, their relationship will be weakened. Social Media Recommendation. Based on the clustering and fusing result, the recommendation is treated as a process of walking on the heterogeneous graph. Gen-semantically relevant to the query. To obtain candidate targets, the method starts with the query entities, and walks along the link in the graph. The candidate targets with large weights are returned as the recommended results for the query. We download microblogs from Tencent website ( t.qq.com ). The microblogs con-tains 1.2 million texts and 0.4 million images, over April 7  X  13, 2012. As people pre-fer up-to-date news, we create the heterogeneous graph on a per day basis. 
First, we evaluate the efficiency of heterogeneous graph construction and multi-modal data fusion. The heterogeneous graph consists of text modal-graph, image modal-graph and text-image similarity graph. To create modal-graphs, it takes 6h on 170k texts and 8h on 57k images. Creating text-image similarity graph takes only 40s. Though it is time-consuming to create the graph, it is an offline job. Multimodal data fusion includes graph clustering and data fusion. It costs 1.5s to clustering images and 200s to clustering texts. The time cost of data fusion is 150s. 
Second, we evaluate the effectiveness of the recommendation for texts and images, based on AP (average precision). The recommendation based on text modal-graph, image modal-graph, and heterogeneous graph are denoted as T-Re, I-Re and Fu-Re respectively. The baseline for text recommendation is the Lucene and for image rec-ommendation is the SMH (Sim-Min-Hash) [6]. Fig.2 shows that the Fu-Re is suited Fig.3 shows that the Fu-Re is good at image recommendation. We also compare the Fu-Re with the QRCs (Query-Relative Classifiers) on five groups in [7], based on MAP. The advantage on ALL groups in Fig.4 shows the effectiveness of the Fu-Re. To provide more effective social media recommendation results, in this paper we represent the semantic relationship among entities of multimodal data by a heteroge-neous graph, and propose a multimodal data fusion algorithm to refine the recom-mendation result based on graph clustering. As the heterogeneous graph is based on entities of different modalities, our algorithm is insensitive to the modality. The pro-posed multimodal data fusion based on graph clustering in the heterogeneous graph can improve the recommendation efficiently and effectively. The experimental results validate the advantages of the proposed approach over the baseline. 
