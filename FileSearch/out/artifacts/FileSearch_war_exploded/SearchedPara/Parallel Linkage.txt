 We study the parallelization of the (record) linkage prob-lem  X  i.e., to identify matching records between two collec-tions of records, A and B . One of main idiosyncrasies of the linkage problem, compared to Database join, is the fact that once two records a in A and b in B are matched and merged to c , c needs to be compared to the rest of records in A and B again since it may incur new matching. This re-feeding stage of the linkage problem requires its solution to be iterative, and complicates the problem significantly. Toward this problem, we first discuss three plausible sce-narios of inputs  X  when both collections are clean, only one is clean, and both are dirty. Then, we show that the in-tricate interplay between match and merge can exploit the characteristics of each scenario to achieve good paralleliza-tion. Our parallel algorithms achieve 6.55 X 7.49 times faster in speedup compared to sequential ones with 8 processors, and 11.15 X 18.56% improvement in efficiency compared to P-Swoosh.
 Categories and Subject Descriptors: H.2.4 [Database Management]: Systems  X  concurrency, distributed databases, parallel databases General Terms: Algorithms, Performance Keywords: Parallel Linkage, Record Linkage
Poor quality data is prevalent in databases due to a va-riety of reasons, including transcription errors, data entry mistakes, lack of standards for recording database fields, etc. To fix such errors, considerable recent work has focused on the (record) linkage problem , i.e., identify all matching records between two collections of records. Such a problem is also known as the de-duplication or entity resolution prob-lem. The linkage problem frequently occurs in data appli-cations (e.g., digital libraries, search engines, customer rela-tionship management) and gets exacerbated when data are integrated from heterogeneous sources. For instance, a cus-Copyright 2007 ACM 978-1-59593-803-9/07/0011 ... $ 5.00. tomer address table in a data warehouse may contain multi-ple address records that are all from the same residence, and thus need to be consolidated. For another example, imagine integrating two digital libraries, DBLP and CiteSeer. Since citations in two systems tend to have different formats, iden-tifying all matching pairs is not straightforward.
Although the linkage problem has been studied exten-sively in various disciplines, by and large, the contempo-rary approaches have focused on how to identify  X  matching  X  records  X  faster  X  using a  X  better  X  distance function. For instance, a nested-loop style algorithm, A , with O ( mn ) run-ning time can solve the linkage problem as follows (assuming a distance function, dist (), and a preset threshold,  X  ): Due to its quadratic nature, however, when both inputs A and B have a large number of records, the naive approach becomes prohibitively expensive. To remedy this problem, people have proposed many improvements  X  e.g., faster link-age approaches such as blocking [21] or computationally effi-cient distance functions such as upper-bound matching [15]. Toward this scalability problem, however, we take a differ-ent approach, and study how to make a linkage algorithm  X  X arallel. X  Therefore, in this paper, we do not consider is-sues like which distance function, dist (), to use or how to add blocking/indexing to A (as in blocked nested-loop or indexed join) and leave that as future work.

Another important aspect of the linkage problem  X  how to merge records that are found to be matching  X  has been largely ignored until recently when SERF project studied it explicitly [14]. This novel perspective emphasized that two main differences between the linkage problem and Database join problem are: (1) the linkage problem has both match-ing and merging steps tangled while the join problem has only the matching step, and (2) the linkage problem often involves the entire set of (long string) attributes in a record in the matching step while the join problem often focuses on a few (numeric or short string) attributes.

To take these points into consideration, the linkage prob-lem that we consider in this paper can be defined as follows: Note that neither A nor B itself is assumed to be clean (to be defined in Definition 2)  X  i.e., there may be two matching records in it. Therefore, we investigate three scenarios  X  when both collections are clean, when only one is clean, and when both are dirty. Furthermore, we show that the intricate interplay between matching and merging steps can exploit the characteristics of each scenario to achieve good parallelization. The intuition of our algorithms is that if: (1) a i is deemed to be a duplicate of b j , and (2) an input collection B is a set, not a bag (i.e., clean), then one does not need to check if a i is a duplicate of b j +1 , ..., b algorithm A . Depending on the relationship between a i and b (i.e., a i contains b j , b j contains a i , a i is identical to b or a i is overlapping with b j ), this intuition can be exploited differently.

Our contributions are as follows: (1) We formally intro-duce the linkage problem with separate match and merge steps, and exploit them to have better sequential linkage framework for three scenarios; (2) We extend sequential link-age algorithms to parallel ones under three scenarios such that redundant computation and overhead among multiple processors are minimized; (3) Our proposals are evaluated using citation data sets with a variety of characteristics. Our parallel algorithms achieve 6.55 X 7.49 times faster in speedup compared to sequential ones with 8 processors, and 11.15 X  18.56% improvement in efficiency compared to P-Swoosh.
Consider two records, r and s , with | r | columns each, where r [ i ] (1  X  i  X | r | ) refers to the i -th column of the record r . Further, let us assume that corresponding columns of r and s have compatible domain types: dom ( r [ i ])  X  dom ( s [ i ]). Definition 1 (Record Matching) When two records, r and s , are deemed to refer to the same real-world entity, both are said matching , and written as r  X  s (otherwise r 6 X  s ). 2 Note that how one determines if two records refer to the same real-world entity or not is not the concern of this pa-per. Assuming the existence of such oracle, we focus on the parallelization of the linkage problem instead. In practice, however, the matching of two records can be often deter-mined by distance or similarity functions. For instance, one may use the cosine angle of token sets of r and s (i.e., cosine similarity) to determine the match of r and s . Or, one may use the ratio of intersected vs. unioned q -gram tokens of two records (i.e., jaccard similarity) for the same purpose.
When two records, r and s , are matching (i.e., r  X  s ), four relationships, as illustrated in Figure 1, can occur: (1) r w s : all information of s appears in r , (2) r v s : all information of r appears in s , (3) r  X  s : information of r and s is identical (i.e., r w s  X  r v s ), and (4) r  X  s : nei-ther (1) nor (2), but the overlap of information of r and s is beyond a threshold  X  . Note that to be a flexible frame-work we do not tie the definitions of the four relationships to a particular notion of containment or overlap . Instead, we assume that the containment or overlap of two records can be further specified by users or applications. Let us as-sume the existence of two such functions: (1) contain(r,s) returns True if r contains s , and False otherwise, and (2) match(r,s) returns True (i.e., one of the four inter-record relationships) or False for non-matching. We assume that match(r,s) is implemented using contain(r,s) function inter-nally (e.g., if both contain(r,s) and contain(s,r) return True, then match(r,s) returns r  X  s ).
 Example 1. For a table with five columns, consider the r :( X  X  X , X  X  X , X  X  X ,  X  ,  X  ), r 4 :(  X  , X  X  X , X  X  X , X  X  X ,  X  ), and r  X  , X  X  X , X  X  X ). Further, let us assume two function: (1) the containment of two records is determined by the contain-ment of token sets of two records, and (2) the overlap of two records is measured by the average jaccard similarity of two corresponding columns of two records with  X  = 0 . 3. Then, r 1 v r 3 holds since { a }  X  { a, b, c } , r 2 v r since { b }  X  { a, b, c } , and r 2 v r 4 since { b }  X  { b, c, d } . In addition, jaccard( r 3 , r 4 )= 0+1+1+0+0 5 = 0 . 4 &gt;  X  and jaccard( r 4 , r 5 )= 0+0+0+1+0 5 = 0 . 2 &lt;  X  . Therefore, both r  X  r 4 and r 4 6 X  r 5 hold. 2
When two records r and s are matching (i.e., r v s , r w s , r  X  s , or r  X  s ), one can merge them to get a record with more (or better) information. Again, how exactly the merge is implemented is not the concern of this paper. We simply refer to a function that merges r and s to get a new record w as merge(r,s) .
 Example 2. For instance, like [14], if one uses the set union operator,  X  , as the merge function for Example 1, then merge( r 3 , r 4 ) would generate a new record r 34 : ( X  X  X , X  X  X , X  X  X ,  X  X  X ,  X  ), while merge( r 1 , r 3 ) would generate r 13 = r  X  X  X ,  X  X  X ,  X  ,  X  ) since r 1 v r 3 . 2 Definition 2 (Clean vs. Dirty) When a collection A has no matching records in it, it is called clean , and dirty oth-erwise. That is, (1) A is clean iff  X  r, s  X  A , r 6 X  s , and (2) A is dirty iff  X  r, s  X  A , r  X  s . 2 Table 1 summarizes the notations.
 Related Work. The general linkage problem has been known as various names  X  record linkage (e.g., [8, 3]), iden-tity uncertainty (e.g., [17]), merge-purge (e.g., [11]), citation matching (e.g., [16]), object matching (e.g., [4]), entity res-olution (e.g., [18]), and approximate string join (e.g., [10]) etc. Since the focus of our parallel linkage is orthogonal to many of these works, in this section, we survey a few recent representative works only.
Unlike the traditional methods exploiting textual similar-ity, Constraint-Based Entity Matching (CME) [20] examines  X  X emantic constraints X  in an unsupervised way. They use two popular data mining techniques, Expectation-Maximization (EM) and relaxation labeling for exploiting the constraints. [1] presents a generic framework, Swoosh algorithms, for the entity resolution problem. The recent work by [7] pro-poses an iterative de-duplication solution for complex per-sonal information management. Their work reports good performance for its unique framework where different de-duplication results mutually reinforce each other (e.g., the resolved co-author names are used in resolving venue names).
Another recent trend in linkage problem is to exploit ad-ditional information beyond simple string comparison. For instance, [12] presents a relationship-based data cleaning (RelDC) which exploits context information for entity res-olution, or [6] proposes a generic semantic distance metric between two terms using the page counts from the Web. Parallel database join has been well studied (e.g., [19]). However, as mentioned in Section 1, parallel linkage has dis-tinct characteristics, making the application of parallel join solutions non-trivial. In recent years, parallel linkage has been studied in P-Febrl [5], D-Swoosh [2], and P-Swoosh [13]. P-Febrl is the parallelization model by Python mod-ule Pypar but no detailed algorithms are shown. Both D-Swoosh and P-Swoosh, parallel versions of Swoosh [1], are implemented by Java emulator, and runs in dual core proces-sors. In parallel structures, D-Swoosh uses the task graph model while P-Swoosh uses the master-slave model . Our algorithm, implemented in distributed MATLAB, runs in real parallel environment (while P-Swoosh runs only in sim-ulated environment). Our parallel solutions use the task graph model to keep simple control of load balancing. All of works can adapt any ER algorithm as a matching function.
We investigate three scenarios depending on types of in-put sets, as shown in Table 2: (1) clean vs. clean : This scenario is relevant when two already-clean data sources are integrated, (2) dirty vs. clean : Consider a search engine that has a clean data set A , but its crawler fetches new dirty data set B every day. In this case, not only B may have matching records in it, there can be new matching pairs between A and B , (3) dirty vs. dirty : When one has two dirty sets, one can clean each dirty set independently and apply clean vs. clean scenario. However, as we will present, one may be able to improve the linkage by matching two dirty sets directly. The scenario of cleaning single dirty set A (i.e., self cleaning) will be shown to be covered by dirty-clean or dirty-dirty cases easily.
Recall that unlike database join, in the linkage problem, if two records a i and b j match, then a merged record c ij (= merge( a i , b j )) is created and re-feeded into A and B . However, depending on the type of matching, one can do further saving. Suppose we use the naive algorithm A in Section 1 for its simplicity. Consider four records: a i , a A ( i &lt; l  X  m ), b j , b k in B ( j &lt; k  X  n ), and two sets E (to hold an instance of two identical records) and C (to hold merged record c ij of a i and b j ). Then, if:
The iterative sequential linkage algorithm for clean-clean case, referred to as s-CC , is shown in Algorithm 2 that terminates when no more merge() occurs (line 1). The main functionality of s-CC using the five cases of inter-record relationships is captured in s-CC-single of Algorithm 1. At line 1 of s-CC-single , both m = | A | and n = | B | continue to shrink as records in A or B are removed. The function (a) a i v b j : (left) a i 6 X  b k (middle) a i  X  b k (right) a (b) a i w b j : (left) a i 6 X  b k (middle) a i  X  b k (right) a Figure 2: Six possible relationships for a i , b j , and b when b j 6 X  b k (i.e., B is clean).
 s-merge( c ij , C ), whose details are omitted, merges a record c ij into a clean list C , ensuring that resulting list C be still clean by comparing c ij to all records in C .
The detailed procedure for dirty-clean case, s-DC , is shown in Algorithm 4 that uses Algorithm 3 as a sub-step. When only one collection, A , is dirty , one can use the other clean collection, B , as the final merged clean set C to minimize space cost. Therefore, when either v or w relationship oc-curs, the record can be simply removed from one collection (lines 1 and 2 of s-DC-single ). Similarly, when  X  rela-tionship occurs (line 3), both original records, a i and b are removed and the new matched record c ij is added to the dirty collection C . After the iteration, when a i is not matched to any records b j from B (line 3), it becomes safe to move a i to the clean collection, B . From the next iter-ation, this newly-moved record a i will be compared to the rest of records A . This step is necessary since A was dirty. When no more merge occurs in the line 3 of Algorithm 3 (i.e. A is empty in the line 1 of Algorithm 4) the algorithm terminates.

By using s-DC , note that one can clean a single dirty collection A . That is, by moving the first record from A to B , one can turn the problem into the sequential linkage of dirty-clean case. As a syntactic sugar, let us call this algorithm as s-self , shown in Algorithm 5. Then, another way to implement the sequential linkage for dirty-clean case to use s-self and s-CC  X  i.e., clean the dirty collection A using s-self first and apply the sequential linkage for clean-clean case. To distinguish from the s-DC of Algorithm 4, we denote this implementation as s-DC self . Algebraically, the following holds: s-DC self ( A , B )  X  s-CC ( s-self ( A ), B ).
Note that algorithms s-DC and s-DC self behave differ-ently depending on the level of  X  X irty-ness X  within A or between A and B . For instance, consider three records, a a k  X  A and b j  X  B . Suppose the following relationship occurs: a i  X  a k &lt; b j . Then, using s-DC self , a i  X  a be compared again with other records in A . However, us-ing s-DC , a i and a k will be removed, saving | A | number of comparisons. On the other hand, for instance, assume that a i  X  a k , a i 6 X  b j , and a k 6 X  b j . Using s-DC self , there is only one comparison after a i  X  a k is made. However, using s-DC , both a i and a k are compared to b j before a i  X  a occurs, increasing the number of comparisons. In general, if the number of matches in A is significantly higher than that between A and B , then s-DC self is expected to perform better. Figure 3: Parallel linkage model with 2 processors.
Since neither collection A or B is clean, more comparisons are needed for dirty-dirty case. By using sequential linkage algorithms for clean-clean or dirty-clean cases, we propose three variations, referred to as s-DD1 , s-DD2 , and s-DD3 , as follows: 1. s-DD1 ( A , B )  X  s-DC ( A , s-self ( B )) 2. s-DD2 ( A , B )  X  s-CC ( s-self ( A ), s-self ( B )) 3. s-DD3 ( A , B )  X  s-self ( A  X  B )
The different behaviors of variations will be evaluated ex-perimentally.
The high-level overview of parallel linkage using 2 pro-cessors is illustrated in Figure 3. Parallel linkage is a dis-tributed algorithm to perform matching and merging con-currently. To achieve this, either data or task needs to be partitioned and distributed to multiple processors. In an ideal parallel model, tasks are evenly distributed among all processors. However, in reality, such an even partition is not trivial since a task cannot be measured easily before it actually executes. In our setting, therefore, we estimate the number of record comparisons by the size of data, and use the data partition model to simulate the task partition model.

Given two inputs, A and B with | A |  X  | B | , the gist of our parallel algorithms is that each processor P i has a replicated A and a partition of B , called B i . At each P then, an appropriate sequential linkage is done separately (e.g., s-CC-single ( A , B i ) for clean-clean case). Once intra-processor cleanness is ensured using sequential linkage, next, inter-processor cleanness needs to be addressed. Therefore, the outputs of sequential linkage at P i are then properly shipped and compared to the rest of data at the other pro-cessors. This process repeats until no more merge() occurs at any processors.

To make the presentation simpler, we assume that each processor P i has a local queue, Q i , while there is a sin-gle global queue Q G . With an efficient implementation us-ing the linked list or priority queue, we assume that oper-ations such as enqueue ( a , Q ), enqueue ( { a 1 , a 2 } , Q ) (= en-queue ( a 2 , enqueue ( a 1 , Q ))), and dequeue ( Q ) are efficiently supported for all queue data structures. Furthermore, we assume the following functions:
The parallel linkage for two clean inputs, referred to as p-CC in Algorithm 7, is the parallelization of the sequen-tial linkage s-CC . Suppose there are k processors, P 1 , ..., P Once the data set B is partitioned to B i and shipped to each processor (lines 1 and 2), at each processor P i , a single iter-ation of sequential linkage for clean-clean, s-CC-single , is applied to generate a clean set C i and two intermediate sets of A 0 i and B 0 i . Note that the initial input data set A was replicated to all processors. However, each intermediate set of A 0 i may be different since A is compared to different piece of B . Therefore, to avoid redundant comparison, we needs to synchronize all intermediate A 0 i from all processors (line 3). Similarly, intermediate B 0 i at each processor may have dif-ferent values after s-CC-single . To increase the efficiency of parallel linkage, therefore, one needs to re-distribute B across all processors (line 4). Finally, all clean sets C ated from s-CC-single are gathered and re-feeded into the local queue Q i (line 5). This step is necessary since a clean set of P 1 still needs to be compared to intermediate sets in P 1 as well as in another processor P 2 .
 The algorithm to clean n clean input sets, termed as p-CC-multi , can be straightforwardly made by extending the p-CC that links  X  X wo X  clean input sets (i.e., at line 1 of p-CC , all n input clean sets need to be enqueued to Q G ). Example 3. Using Figure 4, let us illustrate how merged sets can have inter-and intra-comparisons to intermediate sets iteratively in p-CC . Consider two clean input sets, A = { a 1 , a 2 , a 3 } and B = { b 1 , b 2 , b 3 , b 4 , b cessors, P 1 and P 2 . Furthermore, let us assume that only two merge() occur: c 12 = a 1  X  b 2 and c 34 = a 3  X  b 4 . Since | B | &gt; | A | , B is partitioned to B 1 = { b 1 , b 2 , b B 2 = { b 4 , b 5 } in P 2 and A is replicated to Q 1 and Q box in Figure 4). Now, after dequeue( Q i ) is done (second box) and s-CC-single runs at each processor (third box), we have: A 0 1 = { a 2 , a 3 } , B 0 1 = { b 1 , b 3 } , and C at P 1 , and A 0 2 = { a 1 , a 2 } , B 0 2 = { b 5 } , and C P 2 (third box). Then, sync( { A 0 1 , A 0 2 } ,  X  ) of line 3 in p-CC yields A 0 1 = A 0 2 = { a 2 } (fourth box). Since B 1 and B have different left-over and | B 2 | &lt; | B 1 | , by re-partition on line 4 in p-CC , a 1 is added to B 2 to make B 1 = { b 1 , b and B 2 = { b 5 , a 2 } (fifth box). On line 5 in p-CC , if C not empty, algorithm enqueues C i to all local queues. Then, both Q 1 and Q 2 contain both C 1 and C 2 by synchronizing C 1 and C 2 among other processors (sixth box). This steps repeat until a queue Q i is empty. 2 We propose two different parallel schemes, named as p-DC self and p-DC , similar to two sequential schemes of s-DC self and s-DC , respectively.

In p-DC self of Algorithm 8, first dirty input set A is partitioned to A i and distributed to each processor. Then, each A i is separately cleaned by applying s-self at each processor. At this point, k clean sub-lists and one clean input list, B , remain. Then, all these clean sub-lists can be gathered and cleaned, including B , by p-CC-multi .
In p-DC (algorithm not shown for space), like s-DC self , the dirty list A is partitioned to k sub-lists, A 1 , ..., A each processor. However, unlike s-DC self , the clean list B is also shipped to each processor. Then, dirty-clean case of sequential linkage algorithm, s-DC-single ( A i , B ) is ex-ecuted at each processor. Once data at each processor is cleaned, multiple clean lists from all processors can be gath-ered and cleaned by p-CC-multi .
We propose two parallel linkage schemes to handle two dirty lists: (1) p-DD1 , parallelization of s-DD1 , cleans one dirty set first, then apply p-DC , while p-DD2 , paral-lelization of s-DD2 , attempts to clean both sets at the same time and apply p-CC .

In p-DD1 of Algorithm 9, first, data set B is partitioned to k pieces and cleaned by s-self at each processor. When k clean sub-lists of B are created, they are merged back via a queue and cleaned by the parallel linkage solution p-CC-multi . After B is cleaned and stored in B clean , then, we apply p-DC to get a merged clean list of C . In the p-DD2 scheme (algorithm not shown for space), each input set A and B are separately partitioned and cleaned by s-self , generating 2 k clean sub-lists of A and B . At the end, all these sub-lists are cleaned by p-CC-multi .
In this section, we present a few important properties of the representative sequential and parallel linkage algorithms. First, we show the termination of s-self algorithm since it represents the worst case of input data.
 Lemma 1. The s-self ( A ) algorithm terminates.
 Proof. The algorithm will stop when | A | = 0. Note that the output of match() has only five types as shown in Fig-ure 1. For each output, we have all possibilities before per-forming next comparison, match ( a i +1 , c 1 ): Whenever | A | remains, | C | decreases by 1. If | C | reaches to 0, then | A | decreases by 1 in next iteration. Therefore, | A | decreases monotonically, ensuring the termination of the algorithm. (q.e.d) Lemma 2. The upper bound of number of comparison in s-self ( A ) algorithm is O ( m 2 ) , where m = | A | . Figure 5: Structure of | C | with respect to iteration path in s-self.
 Proof. The lower bound of s-self ( A ) is simply O ( m ) when all records are merged into a single record. In order to calcu-late the upper bound, since three match() relationships (i.e., v , w , and  X  ) always decrease the total number of records without the  X  X e-feed X , we will focus only on the other two relationships:  X  and 6 X  . Note that the maximum number of possible comparison (when a i is compared to C ) is bounded by | C | .

Let us draw a flow with respect to | C | in Figure 5, where each number indicates | C | . The left-most node starts from 0, i.e. C =  X  , and | C | increases by 1 without comparison, taking  X   X   X , i.e., a 1 is added to C and becomes c 1 . For the next record, a 2 , if c 1  X  a 2 , | C | becomes 0, we draw  X   X   X . If a 2 6 X  c 1 , taking  X   X   X , | C | becomes 2 and a 2 becomes c this manner, for a 3 , the maximum number of comparisons is 2 (= | C | ). We already prove in Lemma 1 that | A | decreases when C =  X  or a i 6 X   X  c j , and | A | remains the same when a  X  X  X  c j . Therefore, | A | = | A | X  1 for  X   X   X , and | A | remains the same for  X   X   X . At any moment, the number of  X  from the left-most node via one path is the same as | A | . Note that when | A | = 0, the algorithm stops. At any node of Figure 5, the number of  X   X   X  is the same for any pathes from the left-most node. In addition, in the same column, the number of  X   X   X  through any pathes is the same at any nodes. It is also true that the last node should be  X   X   X . If the last node is  X   X   X , then it indicates that the algorithm stopped at previous node. At any nodes, the number of comparison is the sum of nodes up to previous node through one path. Thus, the worst case is to stop at | C | = 1 or 2. For example, if the algorithm stops at node value 3 on the first row, the number of comparisons is 1+2=3. However, if it stops at node value 1 or 2 below, then we can add one more comparison with the same | A | . In addition, initial size of A is 3 since there are 3  X   X   X  when algorithm stops. In this manner, when | A | = m , the total number of comparisons in the worst case becomes (1+2+ ... + m  X  2)+( m  X  1)+( m  X  2+ ... +1) = 2 P m  X  2 i =1 1 = O ( m 2 ). (q.e.d) Lemma 3. Space and time complexities of all six sequential algorithms are O ( m + n ) and O (( m + n ) 2 ) , respectively. Lemma 4. All parallel linkage algorithms have polynomial upper bounds in time and space complexities.
 Proof. See Table 3. Details are omitted due to space con-straint. (q.e.d)
In this section, under various settings, we evaluate the per-formance of six sequential linkages ( s-CC , s-DC , s-DC self s-DD1 , s-DD2 , and s-DD3 ) and five parallel linkages ( p-CC , p-DC , s-DC self , p-DD1 , and p-DD2 ).
 Table 3: Summary of complexities ( m = | A | , n = | B | , and p = # of processors).

Two main metrics are used as baseline: wall-clock running time , denoted as RT and number of comparison for match(), denoted as NC . Then, the speedup and efficiency for parallel algorithms are defined in terms of RT and NC.
Errors are synthetically introduced to real citation data from DBLP according to two matching rates: (1) Internal (2) Cross Matching Rate of A against B : CMR (A , B) = and CMR, we control the  X  X irty-ness X  of data sets. When errors are introduced, four types of matching errors (e.g.,  X  , v , w , and  X  ) are uniformly distributed. For instance, to have an IMR of 0.4 for A , we synthetically generate 40% of A as matching (i.e., dirty) records, with 10% each for  X  , v , w , and  X  types. To compare directly against P-Swoosh and P-Febrl and keep the experimentation manageable, data sets of 100  X  50,000 records in size are used. Despite their relatively small sizes, consistent performance patterns emerge (to be shown) and one can easily extrapolate the performance for very large data sets.

Distance metric between two citation records used Jaccard similarity with the threshold  X  = 0 . 5 by default. All pro-posed algorithms are implemented in the Distributed MAT-LAB, and executed in the LION-XO PC Cluster at Penn State 2 , which includes 133 nodes, each with dual 2.4 X 2.6GHz AMD Opteron processor and 8GB X 32GB memory. Since it is a multi-user multi-tasking machine, RT is measured as the average of multiple runs (i.e., 5-10).
First, RT and NC among sequential algorithms are com-pared. Figure 6 shows both RT and NC of six sequential algorithms using IMR=0.0 and CMR=0.3 and 100 to 5,000 Figure 6: The RT and NC of six sequential algo-rithms (IMR=0.0 &amp; CMR=0.3). Figure 7: The total NC of four sequential algorithms with different IMR and CMR. records. Although data is a clean-clean case (i.e., IMR=0.0), sequential algorithms for dirty-clean or dirty-dirty cases pre-tend not to know that they are clean so that comparison among all six algorithms is possible. Note that both graphs for RT and NC show consistent patterns of O ( N 2 ), where N is size of input. Since one does not need to compare records internally, s-CC shows the best RT and NT among sequen-tial algorithms. For dirty-clean case, s-DC outperforms s-DC self . Since s-DC compares records across A and B , due to high CMR of 0.3, after one iteration, s-DC matches and merge many records, reducing NC at subsequent iterations. For dirty-dirty case, RT of s-DD1 and s-DD3 are similar while both outperform s-DD2 . Because of direct compar-isons between A and B , records in A will be removed before being compared with records in the same set. Therefore, RT of s-DD1 or s-DD3 is faster than that of s-DD2 .

Second, we compared how IMR or CMR affects the perfor-mance of sequential algorithms. We used five variations of IMR (0.0, 0.1, 0.2, 0.3, and 0.4) and eight variations of CMR (0.0, 0.1, 0.2, 0.3, 0.4, 0.6, 0.8, and 1) with 2,000 records. Both IMR and CMR are applied on A in dirty-clean case. For dirty-dirty case, both A and B take the same IMR, and CMR is applied only to A . Since results of both RT and NC are similar, here, we present only results of NC. In Figure 7, s-DC / s-DD1 and s-DC self / s-DD2 are shown as solid and dotted lines, respectively. For dirty-clean case, NC decreases as both IMR and CMR increase in both s-DC Figure 9: The RT and NC of p-CC (# in legend is # of processors). and s-DC self . Therefore, the data set with IMR=0.4 and CMR=1 (i.e., dirtiest data set) gives the best performance. The effect of IMR is more significant in s-DC self while the effect of CMR is more significant in s-DC .

This happens because, in s-DC self , all redundant data are first merged during s-self stage, reducing CMR when s-CC is applied later. On the other hand, in s-DC , since the dirty set A is first compared to the clean set B , if CMR is high, then more records are merged at the first iteration. In conclusion, CMR (resp. IMR) is the dominant factor in s-DC (resp. s-DC self ). This conclusion can be interpreted as: the M R at the first iteration plays a major role on NC. Because of this, with IMR=0.0, s-DC always performs bet-ter. NC is significantly reduced by the increase of CMR on s-DC , i.e., s-DC performs better when it gets a higher CMR. As an example, with IMR=0.1, there is a cross-over point between s-DC and s-DC self at CMR=0.2. In gen-eral, s-DC is better with a higher CMR, and s-DC self is better with a higher IMR. The patterns of dirty-dirty case is analogous to those of dirty-clean case. With a large CMR and a small IMR, in general, s-DD1 outperforms s-DD2 .
Finally, Figure 8 illustrates the iterative nature of sequen-tial linkage algorithm, s-self . Matched records at each iter-ation are merged into a new record, and re-compared to the rest of records at subsequent iteration. Therefore, sequen-tial linkage algorithms continue until no more new merged record occur. With the data set of 2,000 records, depend-ing on the default threshold  X  for distance metric, Figure 8 shows that 3-8 iterations are needed to do complete self-clean.
Now, using speedup RT and speedup NC , we compare sequential and parallel solutions. Up to 8 processors and 5,000 records are used. Since relative performance of all parallel algorithms, compared to sequential ones, are simi-lar, we show only detailed behavior of p-CC under various characteristics.

In Figure 9, intuitively, both RT and NC decrease as # of ) (a) Total NC
Figure 10: Details of p-CC (legend shown in (a)). processors increases. Despite overhead cost of parallel exe-cution, both RT and NC show the same pattern. However, speedup on RT is more affected by parallel overhead and it is shown in Figure 10. Figure 10(a) shows that although total # of NC increases as the input size increases, it gets little affected by # of processors used. That is, as more # of processors are used, it may increase involved overhead among processors (as communication cost shown in Figure 10(b)), but it does not increase # of comparison since p-CC has few redundant computations among processors  X  an ideal property for parallel algorithm. In our experimenta-tion, communication overhead such as ones in Figure 10(b) typically consume 10% of total RT.

Therefore, in general, speedup RT is less than speedup NC Because it takes about 4-7 seconds to submit a parallel job and recollect final clean sets from processors, speedup RT is very low when input data is less than 100 in Figure 10(c). With larger input data, however, RT is less affected by com-munication cost, and speedup with more processors is higher than that with less processors. When 8 processors are used, speedup RT is close to 6.55. Finally, Figure 10(d) shows how scalable p-CC is. When data is sufficiently large and enough # of processors is used, p-CC shows linear increase of speedup RT  X  another ideal property of parallel algo-rithm.
Figure 11 shows the comparison of five parallel algorithms with respect to their efficiency. Also, both RT and NC are shown after being normalized (i.e., re-scaled to 0-1 range). Overall, efficiency NC is better than efficiency RT due to various overhead negatively affecting RT of parallel algo-rithms. Among parallel algorithms, p-DD2 shows the best efficiency in both RT and NC. Because of the characteristic of input data (IMR=0.0 and CMR=0.3), using clean prop-erty gives better performance on both RT and NC. Thus, even though p-DD2 gives the best efficiency overall, its RT and NC are also the highest. Specifically, note that RT of Figure 11: The RT and NC of five parallel algo-rithms (w. IMR=0.0, CMR=0.3, 8 processors, and 5,000 records). s-DC is 1.11 times faster than that of s-DC self and RT of s-DD1 is also 1.11 times faster than that of s-DD2 . In general, in terms of RT, parallel algorithms are in order of p-CC (fastest) &lt; p-DC &lt; p-DC self &lt; p-DD1 &lt; p-DD2 (slow-est).

We also tried larger data set with more processors X  50,000 records with 16 processors and 32 processors. Since RT fol-lows O ( N 2 ), an approximate equation for RT of a sequential linkage, say s-CC , can be obtained by polyfit() in MATLAB as: RT= 0 . 000173 x 2  X  0 . 01035 x +3 . 645, where x is the size of input data. That is, for 50,000 records, a sequential linkage algorithm such as s-CC would take about 120 hours to finish the job. However, when parallel linkage algorithms are used with up to 16 processors, RT can be reduced to about 9.17 hours with 16 processors and 4.7h with 32 processors. That is, we achieve speedup RT =13.08/ efficiency RT =0.8175 and speedup RT =25.53/ efficiency RT =0.7979 with 16 and 32 processors, respectively.
To our best knowledge, there are two parallel linkage so-lutions comparable to our proposals: P-Swoosh [13] from Stanford SERF project and parallel Febrl (P-Febrl) [5] from ANU record linkage project. Since space complexities of all three proposals are similar, let us focus on the comparison of speedup and efficiency. It is important to emphasize that in parallel experimentation, it is not straightforward to com-pare RT or NC directly. This is because RT may change depending on parallel execution model, choice of data char-acteristics, environment of execution system. However, the  X  X atio X  of how much parallel solutions improve upon sequen-tial solutions is meaningful. That is, if the ratio such as speedup in environment X is higher than that in Y , regard-less of algorithmic details, one can argue that the parallel solution of X be superior to that of Y 3 .

Since both P-Febrl and P-Swoosh studied only dirty-dirty case, here, we compare using s-DD2 for sequential and p-DD2 for parallel case. In addition, in [13], P-Swoosh re-ports only speedup NC while in [5], P-Febrl reports only speedup RT . Therefore, we compare each against our so-lution separately. # of records and processors used in the experimentation are: Note that P-Febrl reports only results using blocking in link-age while both P-Swoosh and ours use nested-loop style link-age (thus no blocking). Therefore, the speedup of P-Febrl should be much higher than those of P-Swoosh and ours. Figure 12: Comparison with other parallel schemes. As shown in Figure 12(a), however, our algorithms performs only 0.16% worse than P-Febrl (0.9360 vs. 0.9375). In [5], P-Febrl reports that their use of blocking and indexing on both sequential and parallel algorithms reduces substantial communication cost so that at the end only 0.35% of over-all RT is due to the communication. In our experimenta-tion, however, communication cost consumes about 7 X 13% of overall RT, leaving much room for improvement if block-ing was used. Therefore, despite seemingly lower efficiency of our parallel algorithms in Figure 12(a) than that of P-Febrl, we believe that our parallel solutions is more efficient compared to P-Febrl. We plan to verify this claim in future work when blocking/indexing is combined with our parallel solutions.

As shown in Figure 12(b), against P-Swoosh, our algo-rithm, P-DD2 , with efficiency NC =0.9781 is 11.15% better than P-Swoosh (shown as P-Swoosh(2)) that has efficiency NC of 0.88. However, the result of P-Swoosh in [13] only consid-ered the number of slave nodes in computing the efficiency, without including the master node. If the master node is also counted as # of processors (as it should be), then their efficiency NC drops to 0.825 (shown as P-Swoosh(1) in Fig-ure 12(b)). Therefore, our proposals shows 11.15 X 18.56% improvement on efficiency NC against P-Swoosh 4 .
With our input data set, RT and NC of sequential algo-rithms are ordered by s-CC (best) &lt; s-DC &lt; s-DC self DD1  X  s-DD3 &lt; s-DD2 (worst). Study on IMR and CMR shows that different algorithms can be used for better per-formance. Specially, for the one dirty set, s-DD2 is always better than s-DD1 . In parallel linkage, RT and NC of par-allel algorithms are in the order of p-CC (best) &lt; p-DC &lt; p-DC self &lt; p-DD1 &lt; p-DD2 (worst). However, the order of efficiency NC/RT among parallel algorithms is p-CC &lt; p-DC  X  p-DC self  X  p-DD1 &lt; p-DD2 . Efficiency of our algo-rithm performs comparably against P-Febrl (but ours do not use blocking and indexing while P-Febrl does) and performs better than P-Swoosh when the same comparison schemes are used on both parallel and sequential algorithms.
Parallel version of record linkage problem is studied in detail. For three input cases of clean-clean, dirty-clean, and dirty-dirty, we presented six sequential solutions and five parallel solutions. Our proposed parallel algorithms are shown to exhibit consistent improvement in speedup and efficiency when compared to sequential ones. In addition, compared to two other competing parallel solutions, ours show 11.15 X 18.56% improvement in efficiency.
Many directions are ahead for future work. First, we plan to extend the current nested-loop style linkage solu-tions to support blocking as in blocked nested-loop style. This enables linkage solutions to quickly filter out unneces-sary records so that only small number of candidate records are further examined. Second, we plan to do a large-scale validation using DBLP, CiteSeer, and ACM data sets (with 0.7-1 million citation records each).

Parallel linkage implementations and data sets that we used in this paper are available at: http://pike.psu.edu/download/cikm07/
