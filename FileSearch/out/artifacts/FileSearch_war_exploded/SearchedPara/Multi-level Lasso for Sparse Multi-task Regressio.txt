 Aur  X elie C. Lozano and Grzegorz  X  Swirszcz { aclozano,swirszcz We address the problem of variable selection in the settings of multiple-output and multi-task regression. Multiple-output regression extends the basic single-output regression model to one involving multiple output variables, while multi-task regression further generalizes the classical regression model to enable joint estimation of regression models for multiple tasks (each model involving one or multiple outputs). Vari-able selection in such settings is of significant inter-est due to many relevant applications in fields rang-ing from econometrics to computational biology. In computational biology, for instance, the fundamen-tal problem of understanding genome associations be-tween expression data (predictors) and phenotypic data (response) is crucial in order to identify poten-tial biomarkers for diseases. Since many diseases such as cancer involve a variety of related phenotypes, it is desirable to perform variable selection in multiple-output regression across the related phenotypes, as in-formation can then be shared among them. Also gene association data is often available for multiple genes within the same pathway, and it may thus be advanta-geous to study the associations jointly on the multiple genes, rather than performing separate studies. A widely used approach to tackle the variable selec-tion problem in multiple-output and multi-task regres-sion models, is based on extending the Lasso formu-lation (Tibshirani, 1994) to impose block-structured regularization via the l 1 -l q norm with q &gt; 1 (Turlach et al., 2005; Obozinski et al., 2006; Negahban &amp; Wain-wright, 2009; Lounici et al., 2009; Tropp et al., 2006). Specifically, the Multi-task lasso (Obozinski et al., 2006) encourages group-wise sparsity across multiple tasks. The Multi-task lasso formulation can also be applied to multiple-output regression. In this context it is often referred to as simultaneous Lasso (Turlach et al., 2005), as simultaneous feature selection is en-couraged. Namely a given feature is either selected as relevant for all the outputs simultaneously, or is ex-cluded all-together for all the outputs. These methods have been frequently used in genome-wide association studies (Puniyani et al., 2010; Zhang et al., 2010) to identify common mechanism of response.
 The main limitation of this multi-task lasso formal-ism is that a common structure is imposed across the multiple regressions/tasks, as the features are selected in an  X  X ll-in-all-out X  manner. Namely the set of se-lected features is identical across the multiple out-puts/tasks, albeit allowing for different amplitude for the selected regression coefficients. However, many important problems require more flexibility. To effi-ciently address the above issue, this paper proposes a novel penalized regression framework, called Multi-level Lasso , to allow for discrepancies in support be-tween the multiple models, while preserving the com-mon structure among them (and thus avoiding the loss of robustness if one were to estimate the models sep-arately). Our approach is based on an intuitive de-composition of the regression coefficients into a prod-uct between a global component that is common to all tasks and another component that captures task-specificity. Such a decomposition is very natural from the standpoint of variable selection, as it is  X  X parsity-preserving X . Namely, a specific regression coefficient is equal to zero if either of its two components is zero; furthermore the global components control the global sparsity pattern common to all tasks. We present an efficient procedure to solve the resulting optimization problem, and derive closed-form shrinkage formulae for the Multi-level Lasso in the case of orthonormal design. We also examine the shrinkage of another de-composition method recently proposed to tackle the same problem, the dirty model of (Jalali et al., 2010), which re-parameterizes the regression coefficients as a sum between two components (rather than a prod-uct). This reveals some interesting insights on the dif-ferences between the two methods. Another relevant model of two-level sparsity was proposed in (Dhillon et al., 2011) for an l 0 setting. As future work it would certainly be interesting to compare our method with such greedy approach. We demonstrate the strength of the Multi-level Lasso on simulated data comparing it against multitask-lasso, the dirty model, individual lasso for each task, and lasso on an aggregated dataset. Experiments on real microarray data further illustrate the usefulness of our approach.
 The Multi-level Lasso objective we propose is con-vex with respect to each of its parameters but is not jointly convex. We show, however, that the solutions still enjoy attractive theoretical properties. In partic-ular we characterize the asymptotic distribution of the Multi-level Lasso estimator. Our theoretical analysis together with our empirical findings are in line with the recent body of work on non-convex penalties in demonstrating their benefits as an alternative to tra-ditional convex formulations. For instance (Fan &amp; Li, 2001) proposed the Smoothly Clipped Absolute Devi-ation (SCAD) penalty to circumvent the drawbacks of the Lasso penalty, in particular with respect to bias. Another pertinent case is that of the l q pseudo-norm where 0 &lt; q &lt; 1 which has been shown to provide sparser solutions than the Lasso and for which several theoretical guarantees exit on the accuracy of variable selection (e.g. (Fu &amp; Knight, 2000)).
 Note that our work considers general cases where there is no prior knowledge on the relatedness of the out-puts. In cases where some knowledge is available, the works (Kim &amp; Xing, 2010; Lu et al., 2009) are rele-vant approaches capable of accounting for discrepan-cies in structure across datasets. Specifically (Kim &amp; Xing, 2010) addresses cases where outputs are related by a known tree structure so the problem can then be cast as a group lasso with groups induced by the tree. The formulation of (Lu et al., 2009) is based on (penalized) Hidden Markov Random Fields. In this setting, each dataset corresponds to a node in a re-lational graph, which embodies prior information on the relatedness between tasks, and is assigned a hid-den state by leveraging the relational graph. Another noteworthy approach considers an adaptive multiple-output Lasso formulation, where mixture weight are introduced over the features (Lee et al., 2010) that are estimated via a Bayesian framework. Note that the formulation is presented for the multiple-output case, not the full multi-task setting. We formulate the problem for the multi-task setting. The formulation for multiple-output regression follows in a straightforward manner, as it corresponds to the special case where all tasks share the same predictor matrix. Assume that there are K tasks. Let X ( k )  X  R n k  X  p denote the predictor matrix for the k th task, whose rows are p-dimensional feature vectors for n k training examples. Denote by X ( k ) ij the j th observation response vector for the k th task. For simplicity assume that data has been standardized so that we need not consider intercept terms. Consider the K -task linear regression model: where  X   X  ( k )  X  R p are formed by the true regression co-efficients one wishes to estimate, and ( k )  X  R n k is the formed by the concatenation of all the coefficients of the i th feature across all tasks. The multi-task Lasso estimate is the solution to the following penalized re- X  P p i =1 k  X  i k  X  . Typical choices for  X  are 2 (Obozinski et al., 2006) and  X  (Zhang, 2006). Both encourage a common sparsity pattern across the multiple tasks. As we have stated earlier, such an  X  X ll-in-all-out X  for-mulation is too rigid in many situations. 2.1. The Multi-Level Lasso Objective We now motivate our multi-level approach that is based on decomposing the regression coefficients into two components: one component reflects the part that is common across tasks, the second component ac-counts for the part that is task-specific.
 The traditional approach in Bayesian statistics is to employ a linear mixed effects model , where the vector of regression coefficients for each task is rewritten as a sum between a fixed effect vector that is constant across tasks, and a random effect vector that is task-specific. Formally, for the coefficients corresponding to the i th feature, i  X  { 1 ,...,p } ,  X  k i is rewritten as  X  i =  X  i +  X  linear mixed effects model is not very natural from the standpoint of variable selection. For instance if the i th feature is irrelevant for all tasks, we would need  X  i = 0 and  X  ( k ) i = 0 for all k.
 A more natural setup would consist of having the  X  X ain effect X  variables control the  X  X lobal X  sparsity. Namely it would be desirable to have the property that  X  i = 0  X   X  tive decomposition that satisfies the desired property by rewriting  X  k i as: and consider  X  i  X  0 , to remove ambiguity (i.e. for model identifiability). As desired, for the i th feature, the  X  i  X  X  will induce the sparsity pattern common across tasks, while the  X  ( k ) i  X  X  will reflect task specificity. We then propose to address multi-task variable selec-tion via the following optimization problem: min above objective can be rewritten as: min Aside from the constraint that  X  should have non-negative entries, the model penalty is the sum of two l penalties, one at a global level, one a task-specific level, hence we call our formulation Multi-level Lasso . 2.2. Algorithm for the Multi-level Lasso In this section we present a procedure to efficiently solve the Multi-level Lasso problem. We adopt an al-ternating optimization approach, where we iteratively Algorithm 1 Alternate Optimization Algorithm for the Multi-level Lasso
Input: Standardized training data X ( k ) ,Y ( k ) ,k = 1 ,...,K , parameters  X  1 , X  2 , &gt; 0.

Initialize m = 0,  X  i (0) = 1 , i = 1 ,...,p and tial estimate (e.g. the ordinary least estimate for task k or the estimate from a ridge regression). Set for m = 1 ... do end for solve for either  X  or  X  , while fixing the other. Min-imizing the Multi-level Lasso objective with respect to  X  while fixing  X  boils down to solving a classical Lasso problem, which can be efficiently solved (e.g. using (Efron et al., 2004) or (Friedman et al., 2007)). Minimizing the objective with respect to  X  while fixing  X  reduces to solving a classical non-negative Garrote objective (Breiman, 1995), which can also be solved ef-ficiently (e.g. via (Yuan &amp; Lin, 2007), (Cantoni et al., 2006)). The alternating optimization procedure is stated as Algorithm 1. Note that each step of the algorithm decreases the original objective (1). Hence the procedure necessarily converges. 2.3. Orthonormal Design Case and As advocated in (Tibshirani, 1994), inspecting the spe-cial case of an orthonormal design sheds light on the nature of the shrinkage.
 Shrinkage for the Multi-Level Lasso: In the or-thonormal design case, closed-form solutions are read-ily available for each step of the alternating optimiza-tion algorithm 1, as it is well know that both lasso and non-negative garrote estimators are equivalent to soft-thresholding operators in the orthonormal case. nary least square estimate. Similarly as in (Tibshirani, 1994) we obtain for the step ( P 1 ) of Algorithm 1: ( P 0 1 )  X  ( k ) i ( m ) = I [  X  i ( m  X  1) &gt; 0]sign( and for the step ( P 2 ) of Algorithm 1: ( P 2 )  X  i ( m ) = I [ k  X  i ( m ) k 1 &gt; 0]  X   X   X  i (  X  ( m ) ,  X   X  i )  X  , Focusing on ( P 0 1 ), first note that if  X  i ( m  X  1) = 0 for the i th feature, then all the task-specific coeffi-which is desirable. Noting that  X  ( k ) i is  X  X omparable X  ing  X  ( k ) i towards zero, by a quantity proportional to since a large value for the estimated  X  i indicates that the i th feature is considered to be significant, the task-specific coefficients for that feature should not be shrunk by a large amount.
 Focusing on ( P 0 2 ), note that if all the task-specific co-efficients for the i th feature are zero, then so is  X  as desired. Also, noting that  X  i is  X  X omparable X  to  X   X  (  X  ( m ) ,  X   X  i ) (since the latter is a weighted average of fect of ( P 0 2 ) is to shrink the estimate for  X  i towards 0 by a quantity whose dependence on  X  ( k ) i is of the order that the larger the task-specific coefficients for the i th feature, the lesser shrinkage should be applied to  X  i . Shrinkage for the dirty model: We now examine the nature of the shrinkage for the dirty block sparse method of (Jalali et al., 2010), which re-parameterizes the regression coefficients as a sum of two coefficients:  X  i = s to the resulting vectors s ( k ) and b ( k ) . While the former are encouraged to have task-specific sparsity patterns, the latter are encouraged to exhibit identical sparsity patterns. Notice that our model involves ( p + 1) K parameters, whereas the dirty model involves 2 pK pa-rameters, as both sets of coefficients depend on task and feature. Formally the dirty block sparse method solves min where S and B are the matrices formed by the coef-solved by alternating minimization fixing B and S re-spectively. We now examine the method X  X  behavior under orthonormal design. Unfortunately there is no closed-form shrinkage formula for the l 1 ,  X  norm. To shed light on the nature of the shrinkage we use the l 1 , 2 norm as a proxy and consider instead min At iteration m , fixing B , we get: Then, fixing S we get: ( D 2 ) b i ( m ) = 1  X   X  2 that ( D 1 ) is comparable to s ( k ) i ( m ) = sign( s shrinkage applied to s ( k ) i is similar across features and tasks. Similarly noting that b ( k ) is comparable to set-ting  X   X  ( k )  X  s ( k ) , we can see that ( D 2 ) is comparable to setting b i ( m ) = 1  X   X  2 k b hence the amount of shrinkage applied to b ( k ) i is similar across tasks.
 Comparing ( P 0 1 ) and ( P 0 2 ) to ( D 1 ) and ( D that the shrinkage of the global and task-specific co-efficients are more tightly coupled for our multi-level lasso model than for the dirty model. The product decomposition used in our model is more natural from the standpoint of variable selection, whereas the dirty model employs an additive decomposition, which sim-ilarly to the linear mixed effects model is not spar-sity preserving: under the dirty model the i th fea-ture is excluded from task k , if both of its compo-s i =  X  b level Lasso model is that its global coefficients have a useful interpretation with respect to  X  X ure screening X  as the sparsity pattern of the global coefficients induces the removal of the features irrelevant to all tasks. 2.4. Extensions for Variable Grouping and Our formulation can be readily extended to incor-porate input variable grouping. Consider G groups and denote by X ( k ) g the columns of X ( k ) corre-sponding to the g th group. Consider the de-are coefficient vectors for the g th group, and  X  g is a scalar controlling the group-sparsity across tasks. Then the multi-level group lasso objective is: min  X  Algorithm 1 can be extended in a straightforward man-ner: Solving for  X  ( k ) g boils down to a classical group-lasso problem, while solving for  X  g can still be reduced to a non-negative garrote problem by considering Z g = dition, note that out Multi-level Lasso objective can also be generalized in a straightforward manner to han-dle the case where each task involves a multiple-output regression. Namely for each k , Y ( k ) is an n k  X  q ma-trix and  X  ( k ) is a p  X  q regression coefficient matrix. A similar re-parametrization is used in (Guo et al., 2011) for learning multiple graphical models, but the proce-dure and algorithm depart significantly from ours, as the objective cannot be solved directly and local linear approximation of the penalty is performed. The Multi-level objective of (1) is convex with respect to each of its parameters  X  and  X  individually but it is not jointly convex. However the local solutions still enjoy attractive theoretical properties. In this sec-tion we characterize the asymptotic distribution of the Multi-level Lasso estimator. Details of the proofs are skipped due to space constraint, but will be provided in a longer version of this manuscript. The first step is to show that the Multi-level Lasso objective can be reformulated as an alternate optimization problem as follows.
 Proposition 1 Solving the Multi-level Lasso problem of (1) is equivalent to solving with  X  = 2 The proof of the proposition follows a reasoning similar to to (Lin &amp; Zhang, 2006).
 We will state the convergence theorem in terms of the equivalent objective (2). Before we do so we need to introduce the following relevant quantities. risk corresponding to the (unpenalized) squared loss imizer of the risk:  X   X  = arg min  X  R (  X  ) . Let  X   X  (  X  ) be a solution of (2). Let
J (  X   X  ) = E [  X   X  L (  X   X  )  X   X  L (  X   X  ) T ] = E Let H be the Hessian of the risk R . Since we are dealing with the squared loss, the Hessian is constant and does not depend on  X . We have in (3). Denote by I the set of indices i for which  X   X  i not all zero. We obtain G  X  ( u ) = The following theorem characterizes the asymptotic distribution of the Multi-level Lasso estimator and shows that it is Theorem 1 Consider a sequence  X  n = 2 with n = P K k =1 n k such that  X  n n  X  1 / 2  X   X   X  0 as n  X  X  X  . Let V  X   X  ( w,u ) = u T Hu + w T u +  X G  X   X  ( u ) , where u  X  R pK and v  X  R pK , H and G  X   X  ( u ) are defined in (5) and (6) respectively. There exists a random vector W  X  X  (0 ,J (  X   X  )) , where J is defined in (4), such that In particular if  X  n  X  0 and the error terms are i.i.d. with mean zero and variance (  X  ( k ) ) 2 , k = 1 ,...,K we get where  X  = diag (  X  (1) I p ,..., X  ( K ) I p ) . The proof follows easily from Theorem 4 in (Rocha et al., 2009). As remarked in (Rocha et al., 2009), lo-cal minima may exist for finite sample, yet asymptoti-cally the penalty is negligible compared to the squared loss and the minimizer is unique. Though the penalty in (3) is concave, computation of the Hessian of the full objective (risk + penalty) reveals that there are regions where the objective is locally convex. We plan to characterize these regions as future work (in the spirit of (Breheny &amp; Huang, 2011)). 4.1. Synthetic Data We evaluate the performance of Multi-level Lasso against Multi-task Lasso (abbreviated as  X  X ulti-task X ), the dirty model estimator, running Lasso in-dependently for each task (referred to as  X  X ndLasso X ), and running Lasso on the aggregated dataset formed by combining data for all task (referred to as  X  X l-lLasso X ). As a measure of variable selection accu-racy, we use the F 1 measure, which is the harmonic mean of precision and recall (The F 1 measure is be-tween 0 and 1; the larger F 1 , the higher the accu-racy). For all methods, we consider the  X  X oldout val-idated X  estimates, namely we select the penalty pa-rameters that minimize the average squared error on a validation set. We remark that for the multi-level Lasso, the coefficient vectors  X  and  X  for the multi-level Lasso are combined multiplicatively. Therefore only one of the two regularization parameter is nec-essary, since one can always multiply  X  by a constant and divide  X  by the same constant. Thus, one does not need to search over a 2-D grid of regularization parameters, while such a search cannot be avoided for the dirty model. We consider the K -tasks regres-each task we generate a n  X  p predictor matrix X ( k ) , where the rows are generated independently according each task is generated according to N (0 , 1) . The true regression coefficients for each task are generated as a p  X  K matrix  X  B, where  X  B i,k =  X   X  ( k ) i . We consider two setups. The first setup is as follows. All the entries of  X  B are first set to zero. Next we generate a row-wise sparsity pattern that will determine which features are irrelevant to all tasks. Specifically, the row-wise spar-sity is determined by selecting b p  X   X  p c rows at random to contain non-zero coefficients (the remaining rows are 0 for all tasks), where  X  p is a simulation parame-ter. For each of the selected rows, we introduce some amount of disagreement between tasks with respect to the within-row sparsity pattern. We do so by ran-domly selecting b K  X   X  K c entries to be set to 0, where  X 
K is a simulation parameter. Then for each non-zero entry of  X  B, independently, we set its value according to N (0 , 1) . Note that  X  K =0 corresponds to the case where the relevant predictors are common to all tasks, a setting that should be favorable to Multitask Lasso. Our second setup is for the extreme case where the sparsity pattern is arbitrary (no task sharing), which should be favorable to Lasso. For that setup, we ran-domly select  X  arb pK entries of  X  B to contain non-zero coefficients, where  X  arb is a simulation parameter. For each setting, we ran 50 runs. We set training and evaluation sample sizes to n train = n eval = 50 , feature size to p = 20 . We considered various combinations for the values of ( K, X  p , X  K , X  arb ) , so as to enforce more or less discrepancies in the sparsity pattern across tasks, so as to consider models of varying sparsity, and vary-ing ratios between feature dimensionality and number of tasks. The results are presented in Table 1. Overall, Multi-level Lasso performs better than all the comparison methods in multitask settings with some amount of discrepancy across tasks. In addi-tion, our method is remarkably competitive with Lasso and Multitask Lasso for the extreme cases of  X  X o task-sharing X  (red rows in Table 1) and  X  X ull-sharing X  (blue rows in Table 1) respectively. 4.2. Application to Microarray Data Analysis We apply our method to the analysis of gene ex-pression data using a microarray dataset pertaining to isoprenoid biosynthesis in Arabidopsis thaliana (A. thaliana) provided by (Wille et al., 2004). A. thaliana is a small flowering plant widely used as a model organism for studies in genetics and molecular biol-ogy. Isoprenoids play a key role in major plant pro-cesses including photosynthesis, respiration and de-fense against pathogens. They are also important com-ponents in a variety of drugs (e.g. against cancer and malaria), fragrances (e.g. menthol) and food colorants (carotenoids). Understanding the mechasnisms of iso-prenoid synthesis is thus highly relevant to a large spectrum of applications. Of particular relevance is to develop an understanding of the crosstalks between the two isoprenoid pathways: the mevalonate pathway an the plastidial pathway. In the dataset considered the predictors are the expression levels of 21 genes in the mevalonate pathway, the responses are the expres-sion levels of 19 genes in the plastidial pathway. There are 131 samples. All variables are log transformed. The predictors are centered and standardized to unit variance.
 We first evaluated the predictive accuracy of our method and the comparison methods by randomly partitioning the data into training and test sets, using 90 observations for training and the remainder for test-ing. The tuning parameters were selected via 5-fold cross-validation. We computed the prediction MSE for the testing set. The average MSEs based on 100 random partitions are presented in Table 2. We can see that overall the predictive performance of the Mul-tilevel Lasso is superior to the other methods. We now proceed with an analysis of the associations identified by our method between genes from the mevalonate pathways (predictors) and those from the plastidial pathway (responses), using the full dataset. We apply bootstrap resampling to determine the sta-tistical confidence of the associations identified. The associations identified using the full dataset that also appear more than 70 percent of the time in the boot-strap datasets are depicted in Figure 1. We note that several of our findings are consistent with find-ings from the biological literature. For instance, con-nections between genes MK and GGPPS 6 &amp; 12, be-tween genes FPPS2 and IPPL1 , and between genes DPPS2 and PPDS1 have also been reported in (Wille et al., 2004). The absence of connections stemming from genes GGPPS 1,3,4,5,8,9 is also consistent with findings in (Wille et al., 2004). The above insights il-lustrate the value of our approach for gene association discovery. We plan to apply our method on a variety of datasets in this domain, and hope to shed light on important aspects of the regulatory mechanisms. Breheny, P. and Huang, J. Coordinate descent al-gorithms for nonconvex penalized regression, with applications to biological feature selection. ArXiv e-prints , 2011.
 Breiman, L. Better subset regression using the nonneg-ative garrote. Technometrics , 37(4):373 X 384, 1995. Cantoni, E., Mills Flemming, J., and Ronchetti,
E. Variable selection in additive models by non-negative garrote. Technical report, D  X epartement d X  X conom  X etrie, Universit  X e de Gen`eve, March 2006. Dhillon, Paramveer S., Foster, Dean, and Ungar, Lyle.
Minimum description length penalization for group and multi-task sparse learning. Journal of Machine
Learning Research (JMLR) , 12:525 X 564, February 2011. ISSN 1532-4435.
 Efron, B., Hastie, T., Johnstone, I., and Tibshirani,
R. Least angle regression. Annals of Statistics , 32: 407 X 499, 2004.
 Fan, J. and Li, R. Variable selection via nonconcave penalized likelihood and its oracle properties. Jour-nal of American Statistical Association , pp. 1348 X  1360, 2001.
 Friedman, J., Hastie, T., H  X ofling, H., and Tibshirani,
R. Pathwise coordinate optimization. Technical re-port, Annals of Applied Statistics, 2007.
 Fu, W. and Knight, K. Asymptotics for lasso-type estimators. Ann. Statist. , 28:1356 X 1378, 2000. Guo, J., Levina, E., Michailidis, G., and Zhu, J. Joint estimation of multiple graphical models. Biometrika , 98(1):1 X 15, 2011.
 Jalali, A., Ravikumar, P., Sanghavi, A., and Ruan, C.
A dirty model for multi-task learning. In Advances in Neural Information Processing Systems 23 , pp. 964 X 972. 2010.
 Kim, S. and Xing, E.P. Tree-guided group lasso for multi-task regression with structured sparsity. In
International Conference on Machine Learning , pp. 543 X 550, 2010.
 Lee, S., Zhu, J., and Xing, E. Adaptive multi-task lasso: with application to eqtl detection. In Lafferty,
J., Williams, C. K. I., Shawe-Taylor, J., Zemel, R.S., and Culotta, A. (eds.), Advances in Neural Informa-tion Processing Systems 23 , pp. 1306 X 1314. 2010. Lin, Y and Zhang, H. Component selection and smoothing in multivariate nonparametric regression. Ann. Statist. , 34:2272 X 2297, 2006.
 Lounici, K., Tsybakov, A.B., Pontil, M., and Van
De Geer, S. A. Taking advantage of sparsity in multi-task learning. In 22nd Conferecnce on Learn-ing Theory (COLT) . 2009.
 Lu, Y., Rosenfeld, R., Nau, G. J., and Bar-Joseph, Z.
Cross species expression analysis of innate immune response. In Proc. of the 13th Annual International
Conference on Research in Computational Molecu-lar Biology , 2009.
 Negahban, S. and Wainwright, M. Phase transitions for high-dimensional joint support recovery. In Ad-vances in Neural Information Processing Systems 21 , pp. 1161 X 1168. 2009.
 Obozinski, G., Taskar, B., and Jordan, M. Multi-task feature selection. Technical report, Department of Statistics, University of California, Berkeley, 2006. Puniyani, S., Kim, S., and Xing, E.P. Multi-population gwa mapping via multi-task regularized regression. Bioinformatics [ISMB] , 26(12):208 X 216, 2010.
 Rocha, G. V., Wang, X., and Yu, B. Asymptotic distri-bution and sparsistency for l1-penalized parametric
M-estimators with applications to linear SVM and logistic regression. ArXiv e-prints , 2009.
 Tibshirani, R. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B , 58:267 X 288, 1994.
 Tropp, J.A., Gilbert, A.C., and Strauss, M.J. Algo-rithms for simultaneous sparse approximation. In
Signal Processing, Special issue on  X  X parse approxi-mations in signal and image processing , volume 86, 2006.
 Turlach, B.A., Venables, W. N., and Wright, S. J. Si-multaneous variable selection. Technometrics , 47, 2005.
 Wille, A., Zimmermann, P., Vranova, E., Furholz,
A., Laule, O., Bleuler, S., Hennig, L., Prelic, A., von Rohr, P., Thiele, L., Zitzler, E., Gruissem,
W., and Buhlmann, P. Sparse graphical gaussian modeling of the isoprenoid gene network in ara-bidopsis thaliana. Genome Biology , 5, 2004. doi: 10.1186/gb-2004-5-11-r92.
 Yuan, M. and Lin, Y. On the non-negative garrotte estimator. Journal Of The Royal Statistical Society Series B , 69(2):143 X 161, 2007.
 Zhang, J. A probabilistic framework for multi-task learning . PhD thesis, Carnegie Mellon University, 2006.
 Zhang, K., Gray, J.W., and Parvin, B. Sparse multi-task regression for identifying common mechanism of response to therapeutic targets. Bioinformatics ,
