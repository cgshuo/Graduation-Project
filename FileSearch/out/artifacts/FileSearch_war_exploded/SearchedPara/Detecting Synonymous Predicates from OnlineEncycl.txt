 1.1 Motivation Recently, much effort has been devoted to automatically building structured KBs from English version of Wikipedia, such as Freebase [ 1 ], DBpedia [ 2 ]and YAGO [ 3 ]. These KBs, unlike Wikipedia itself, extract facts in structured form of subject-predicate-object (s-o-p) triples from infoboxes in Wikipedia. However, Wikipedia pages are maintained by volunteers around the world. People from different backgrounds may use different expressions to convery the same idea, causing different surface forms of predicates which are semantically identical. The incorrect use of predicates causes either low recall of extractor or redundant relations in structured KBs.
 Many predicates are in fact synonymous. For example, there are 17 predicates containing ( postcode ) in Chinese Wikipedia infoboxes, shown in Table 1 . Most of them represent  X  X ostcode X . When an editor is submitting a new attribute of an entity, the system should provide the editor with candidate predicates. Besides, query expansion also requires synonymous predicates recommendation. The tremendous predicate list makes it impossible to get rid of duplicate pred-icates manually. It X  X  urgent to put forward an auto-detect method to find syn-onymous predicates in online encyclopedias, which helps improve the quality of structured KB X  X  extractions. 1.2 Challenges Predicate detection in online encyclopedias differs from that on Linked Open Data(LOD), such as DBpedia and Freebase. Objects in online encyclopedias are often non-standard, making it difficult to use. Moreover, predicates are various due to different backgrounds of editors. Usage of global synonym databases is not sufficient as predicates are used in various KBs for various purpose by various editors. There are also different interpretations of predicates. Some predicates are too concrete while others are too general. Besides, many Chinese charac-ters share a similar pronunciation, causing typos (mistaking characters with the same pronunciation) and transliteration differences (different characters chosen to represent the same pronunciation). As for Chinese predicates, there are fewer external resources like WordNet. The long tail [ 4 ] causes little information could be extracted from low frequent predicates. 1.3 Contribution Earlier studies on structured KBs are not appropriate in the case for online encyclopedias. Our method is exactly designed for semi-structured online ency-clopedias where objects are seldom linked entities. The contribution of this paper is fourfold. First, we leverage Wikipedia X  X  wikitext for the first time to describe predicates. Besides, we extract many detailed information in Wikipedia and joint dumps and web pages together for the first time. Second, we propose various word-embeddings, varying from predicate types to predicate semantics. Third, we use linking information between Freebase schema and Wikipedia schema and use the better organized Freebase schema to describe predicates. Finally, we understand the predicate from these features.
 The rest of this paper is organized as follows: In the next section we present related work with regard to synonymous predicates discovery. Next in Sects. 3 and 4 , we introduce the resources and features used in our experiment. We Although it is a fundamental step in building structured KB, rare work has been done on this intractable problem. Many released KBs avoid predicate unification by using a predefined and limited predicate list, such as YAGO. There are less than 200 predicates in YAGO. You cannot find the screenwriter of any movies in YAGO because this relation has not be defined yet. Freebase indeed detect synonyms based on user domain expertise and co-occurrence of objects and subjects [ 5 ]. However, this method calls for user logs and well-structured KB, which can not be utilized by other KBs.
 mining in the Semantic Web, associated with query expansion and synonym discovery. Others are based on different language processing and information retrieval techniques.
 analysis [ 6 ], are not suitable because no two different nodes in an RDF graph have the same URI. Instead, we consider the corresponding type of each URI as different URI may belong to the same type. Cafarella [ 7 ] presents a approach to detect synonyms among table attributes. However, the authors restrict attributes and ignore instance-based method because they concentrate only on extracted table schemata. So far, Abedjan [ 8 ] treats synonymous predicates detection as a association rule mining problem. Note that he works on structured DBpedia using linking information of objects and does not understand the predicates. This method is not appropriate for encyclopedias.
 synonym candidates in web documents, based on the idea of synonymous word co-occur [ 11 ]. Naumann [ 12 ] proves the effectiveness of aggregate features and Li X  X  work [ 13 ] shows that the performance using dictionaries only in real data is poor. In this case, we use multi features to capture predicate semantics. NLP tools to discover synonyms, leveraging both the benefit of schema matching and semantic understanding. Various resources have been used as features to present predicates, from inside and outside the KB. We leverage both web pages and dumps of Wikipedia in the experiment. Besides, bilingual dictionary and Freebase schema are used to repre-sent each predicate. Our main dataset is a semi-structured KB (See footnote 1) (only subject is defined as an entity) with 3.5 millions s-p-o triples extracted from 33.8 thousands of infoboxes in Chinese Wikipedia [ 14 ], which contains 14,000 dif-ferent predicates. The KB is open-domain and predicates in the KB that have same surface forms are considered the same in our experiment. 3.1 Wikipedia In Wikipedia, there are mainly three parts of data to help evaluate the simi-larity between predicates, including section names in Wikipedia web pages and wikitext-predicates , infobox names in Wikipedia dumps 2 . Figure 1 shows all the information in Wikipedia. wikiText. Wikitext, also known as wikicode, is a lightweight markup language used to write pages in Wikipedia. Infobox [ 15 ] is a template used to collect and present a subset of information about its subject. All the wikitexts and infoboxes [ 16 ] mentioned in this paper are referred to Wikipedia X  X  wikitexts and infoboxes. wikiSection. Attributes are often divided into different sections in Wikipedia infoboxes. As shown in Fig. 1 , ( personal information ) is the section name of predicate ( birth )and ( listed hight ) while predicate ( college )and ( NBA draft ) are in section ( career information ). wikiInfobox. Actually, each infobox of an entity has a name, which can be extracted from wikitext. For example, predicate ( nationality ) usually appears in infoboxes concerning people while predicate ( writer ) appears in infoboxes concerning drama. 3.2 Freebase Different predicates are usually associated with different kinds of entities [ 17 ]. Predicate categories can be represented by their corresponding subject cate-gories. On the one hand, Wikipedia X  X  build-in categories are too detailed to use. There are more than 190,000 categories in Chinese Wikipedia. In addition, there exist many confusing but frequent categories, such as ( good articles ) and ( articles containing Hebrew-language text ). On the other hand, Freebase provides find-grained category information for most entities and fortunately many Freebase entities have been linked to Wikipedia entities example, the category information of Hongkong film actor 4 , 5 ung ) in Freebase is shown in Table 2 . We collect all the categories of Freebase entities that correspond to Chinese Wikipedia entities 6 . Features are of great importance in our experiment. We not only use the surface form of predicate, but also extract many latent features inside Wikipedia and Freebase. Table 3 presents all the features used in the experiment.
 4.1 SurfaceForm The most straightforward features would be those extracted from surface forms of predicates. This kind of features express the character level similar-ity between two predicates. We first consider unigram overlap and explore two metrics, unigram (0 , 1) (feature 1) and unigram (1 , 0) (feature 2). Unigram unigram (0 , 1) scores between a predicate pair ( pred 1 ,pred lows, while other features containing subscript (1 , 0) or same way as Eqs. ( 1 ) and ( 2 ): We also compute edit distances of each pair of predicates (feature 3 and 4) in Eq. ( 3 ). Synonymous predicates usually have similar length in characters, which is taken into account by length(shorter predicate)/length(longer predicate) as character length ratio (feature 5). 4.2 Pinyin Pinyin is the official phonetic system (and ISO standard) for transcribing Mandarin pronunciations into the Latin alphabets. There are many words in Chinese with different writing forms, conveying the same meaning. For example, ( coordinate )and ( coordinate ) are different predicate forms but actu-ally the same. We use the most frequent pinyin string of each Chinese character to construct the pinyin representation for a predicate. Features in Pinyin (fea-ture 6 X 10) are similar to features in surfaceForm . Compared to features in sur-faceForm , characters are replaced with their corresponding pinyin strings while calculating the similarity scores. 4.3 WikiText Wikipedia uses a large amount of rules to translate particular wikitext templates to the infoboxes we see on web pages. In our case, predicates in wikitext are aligned to predicates in web pages. The alignment is based on manual rules calculating the similarities between objects in web page triples and the attribute values of dumps X  wikitexts. Accordingly, given a predicate, we can collect a set of aligned wikitext-predicates, with their alignment frequencies to this predicate. The alignment is not a one-to-one mapping, causing noise in the alignment. For example, the wikitext-predicates and their frequency aligned to predicate ( area ) are shown in Table 4 . We defined it the wikitext-predicate distribution of predicate Let freq ( p i ,wp j ) be the frequency of predicate p predicate wp j .Let WL ( p i ) be the wikitext-predicate set that has aligned to p . The wikiText (0 , 1) (feature 12) and wikiText (1 , 0) (feature 13) further charac-terize the overlap between the two predicates in an asymmetric way, defined in Eqs. ( 4 ) and ( 5 ): ( v is the normalized frequency between predicate p i and wikitext-predicate wp representing their co-occurence, defined in Eq. ( 6 ). Feature 11 of each predicate pair is the cosine similarity of the two wikitext-embedding vectors. 4.4 WikiSection and WikiInfobox The predicate synonyms should have similar sections and infobox names. We collect all the wikiSections and wikiInfoboxes of predicates and convert them to distribution vectors. For example, The wikiSection and wikiInfobox distrib-ution of predicate ( country ) is shown in Tables 5 and 6 . wikiSection and wikiInfobox features are calculated in a similar way as wikiText features. 4.5 Bilingual Dictionary Some synonymous predicates are in different surface forms mainly because of translation differences. Thus we translate the original Chinese predicates to their corresponding English expressions. This kind of features works well when one or both predicates is low frequent and less information could be extracted by other kind of features.
 4.6 Freebase Category For each predicate, we average the category vectors of entities that appear as subject of this predicate to generate category vectors of predicate. Since Freebase has a large mount of different categories, the raw category information will be very sparse. Therefore, we use two kinds of Freebase category embeddings. One uses the original category distribution vector while the other uses singular value decomposition (SVD) to transform each entity X  X  category information to a 100-dimension unit vector.
 Let S i = { e i 1 ,e i 2 , ..., e i N i } be the set of entities that has predicate p be the set of types of entity e i j in Freebase. The original category embedding of p is F i =( f i 1 ,f i 1 , ..., f i N ). N i is size of S i categories in Freebase. f j i is the normalized frequency between predicate p Freebase category cate j , defined in Eq. ( 7 ). The Freebase-embedding (Feature 21) of predicate pair ( p i ,p j )is F i  X  F j . So does feature 20. We treat this task as a binary classification problem, that is, given a pair of predicates pred 1 , pred 2 , predicting whether these two predicates are synonyms. The dataset is validated by three experts in computer science major. The first expert randomly selects predicate pairs and tag 0 or 1 to represent whether they are synonyms. Since the class distribution is highly skewed with most predicate pairs being negative, we select a balanced set of 1500 pairs with 1000 positive and 500 negative to avoid failures in training. Then the second expert tags on this balanced pairs and the last person only tags the inconsistent pairs. The result training set contains 1000 pairs (464 pairs are tagged 1 ) of predicates while the test set contains 500 pairs (240 pairs are tagged 1 ).
 To evaluate features X  validity, we present three experiments: In Sect. 5.1 we evaluate the classification performance using only one kind of features. In Sect. 5.2 we evaluate the classification performance using all except one kind of features at one time. In Sect. 5.3 we evaluate all the feature combinations and seek the feature combination that outperforms others. We use LibSVM (with kernel type of LINEAR and RBF), decision tree (C4.5), voted perceptron and AdaBoost as classifiers in each experiment. Compared to Abedjan X  X  work [ 8 ], we deal with different resources (linked open data and online encyclopedia) using different methods, thus, we use different evaluation methods.
 5.1 Single Feature Experiment First we explore the effectiveness of each kind of features. For each classifier, We report the accuracy using only one kind of features, shown in Table 7 . frequent predicates do not have enough wikiSections and wikiInfoboxes to rep-resent predicates properly. Pinyin feature is of great importance as expected. It takes spell mistakes and different forms of expression into consideration. Sur-faceForm and bilingual dictionary are also reported as good single features. 5.2 Minus One Feature Experiment In the second experiment we have evaluated the redundancy of each kind of predicate comparing other features. We first remove one kind of features and then evaluate the utility of remaining features. The detached kind of features is more likely to be redundant if the remaining features have higher accuracy. The results are shown in Table 8 .
 wikiText feature is only inferior to surfaceForm feature while bilingual dic-tionary performs poor. It shows the importance of wikiText. wikiText includes the bilingual information for its cross-linguistic property. It also indicates that the wikiText defined by Wikipedia.org is valid and irreplaceable in representing predicate. No matter what kind of classifier we use, surfaceForm and wikiText appear the top 2 features in this experiment. What X  X  more, bilingual dictionary is usually the most useless kind of features. It is because bilingual dictionary and Pinyin features can be seen as a coarse combination of surfaceForm and wiki-Text features. Comparing to the first experiment, wikiInfobox and wikiSection take effect in complex feature combinations. 5.3 Best Feature Combination In our last experiment, we want to find the best feature combination. We use LibSVM with RBF kernel as classifier example. The other classifiers output similar results. The result is shown in Table 9 . Our best accuracy is achieved with features: [ pinyin, surfaceForm, wikiText, wikiSection, wikiInfobox, Free-base category ]. It corresponds to the previous two experiments: surfaceForm and wikiText are fundamentally useful while wikiInfobox and wikiSection show their efficacy in complex feature combinations.
 In this paper, we propose a full-fledged method on detecting predicate synonyms, including features extraction and comparison. It is groundwork for building Chinese structured KB. We exploit a mount of features, including linking infor-mation between Freebase and Wikipedia. Thorough study has been done on wikitext. Our experiment shows that the wikitext provides unique information comparing to normal features. Subject category information and section informa-tion are also essential features, which can be used by other online encyclopedias. In online encyclopedias, only few predicates will be inserted or changed by editors to entities pages during a short time. Synonymous predicates can be cal-culated offline and we can only calculate the similarity between recently modified predicates and other predicates, which reduces computation resources. Another way to speedup our system is using part of distribution data. We find that the top three wikitext-predicates in Sect. 4.3 already account for most correct align-ment. Hence, the time complexity of feature calculating can be approximately linear.
 wikitext-predicate alignment. They depict the predicates directly and may con-tribute much in predicting the predicate synonyms. Future work will explore the use of objects in predicate comparison. Predicate unification between different Chinese encyclopedias, such as baidu baike and Chinese Wikipedia will also be conducted.

