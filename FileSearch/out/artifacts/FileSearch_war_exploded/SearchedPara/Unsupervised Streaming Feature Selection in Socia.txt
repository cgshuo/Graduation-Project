 The explosive growth of social media sites brings about mas-sive amounts of high-dimensional data. Feature selection is effective in preparing high-dimensional data for data an-alytics. The characteristics of social media present novel challenges for feature selection. First, social media data is not fully structured and its features are usually not prede-fined, but are generated dynamically. For example, in Twit-ter, slang words (features) are created everyday and quickly become popular within a short period of time. It is hard to directly apply traditional batch-mode feature selection methods to find such features. Second, given the nature of social media, label information is costly to collect. It ex-acerbates the problem of feature selection without knowing feature relevance. On the other hand, opportunities are also unequivocally present with additional data sources; for ex-ample, link information is ubiquitous in social media and could be helpful in selecting relevant features. In this paper, we study a novel problem to conduct unsupervised streaming feature selection for social media data. We investigate how to exploit link information in streaming feature selection, re-sulting in a novel unsupervised streaming feature selection framework USFS. Experimental results on two real-world so-cial media datasets show the effectiveness and efficiency of the proposed framework comparing with the state-of-the-art unsupervised feature selection algorithms.
 D.2.8 [ DATABASE MANAGEMENT ]: Database Ap-plications X  Data Mining Unsupervised Feature Selection; Streaming Features; Social Media Data c  X 
The rapid growth and popularity of social media services such as Twitter 1 , Facebook 2 provide a platform for people to perform online social activities by sharing information and communicating with others. Massive amounts of high-dimensional data (blogs, posts, images, etc.) are user gener-ated and quickly disseminated. It is desirable and of great importance to reduce the dimensionality of social media data for many learning tasks due to the curse of dimensionality. One way to resolve this problem is feature selection [17, 20], which aims to select a subset of relevant features for a com-pact and accurate representation.

Traditional feature selection assumes that all features are static and known in advance. However, this assumption is invalid in many real-world applications especially in so-cial media which is imbued with high-velocity streaming features. In social media, features are generated dynami-cally, new features are sequentially added and the size of features is unknown in most cases. For example, Twitter produces more than 320 millions of tweets everyday and a large amount of slang words (features) are continuously be-ing user generated. These slang words promptly grab users X  attention and become popular in a short time. It is not prac-tical to wait until all features are available before performing feature selection. Another example is that after earthquakes, topics (features) like  X  X epal X  emerge as hot topics in social media shortly, traditional batch-mode feature selection can hardly capture and select such features timely. Therefore, it could be more appealing to perform streaming feature selec-tion (SFS) [37] to rapidly adapt to the changes.

In SFS, the number of instances is considered to be con-stant while candidate features arrive one at a time, the task is to timely select a subset of relevant features from all fea-tures seen so far [37]. Instead of searching for the whole feature space which is costly, SFS processes a new feature upon its arrival. A general framework of streaming feature selection is presented in Figure 1. At each time step, a typical SFS algorithm first determines whether to accept the most recently arrived feature; if the feature is added to the selected feature set, it then determines whether to discard some existing features from the selected feature set. The process repeats until no new features show up anymore. The vast majority of existing streaming feature selection al-gorithms are supervised which utilize label information to guide feature selection process [27, 33, 37]. However, in so-cial media, it is easy to amass vast quantities of unlabeled https://twitter.com/ https://www.facebook.com/ data, while it is time and labor consuming to obtain labels. To deal with large-scale unlabeled data in social media, we propose to study unsupervised streaming feature selection. Unsupervised streaming feature selection is particularly dif-ficult and challenging: (1) without any label information, it is difficult to assess the importance of features; and (2) fea-tures are usually not predefined but are generated dynam-ically, hence it cannot be carried out by directly applying traditional unsupervised feature selection algorithms. Figure 1: A framework of streaming feature selection. It consists of two phases: testing newly arrived features and testing existing features.

On the other hand, link information is abundant in so-cial media. As observed by homophily [21] from social sci-ences, linked instances are likely to share similar features (or attributes). Therefore, as label information for supervised streaming feature selection, link information could provide helpful constraints to enable unsupervised streaming feature selection. However, linked social media data is inherently not independent and identically distributed ( i.i.d. ) while ex-isting streaming feature selection are based on the data i.i.d. assumption, it is challenging to exploit link information for streaming feature selection.

In this work, we investigate: (1) how to exploit link infor-mation for feature selection; and (2) how to perform stream-ing feature selection in unsupervised scenarios. Our solu-tions to these two questions lead to a novel unsupervised streaming feature selection framework USFS. The main con-tributions of this paper are outlined as follows: The rest of this paper is organized as follows. We formally define the problem of unsupervised streaming feature selec-tion in Section 2. In Section 3, we introduce the proposed framework of unsupervised streaming feature selection. Em-pirical evaluation on real-world social media datasets is pre-sented in Section 4 with discussion. In Section 5, we briefly review related work. The conclusion and future work are presented in Section 6.
We first summarize some notations used in this paper. We use bold uppercase characters to denote matrices, bold low-ercase characters to denote vectors, normal lowercase char-acters to denote scalars. For an arbitrary matrix A  X  R n  X  d a and a j mean the i -th row and j -the column of matrix A , respectively. A ij or a j i denotes the ( i,j )-th entry of matrix A . A ( t ) denotes the matrix of A at time step t , ( a ( t ) ( a ( t ) ) j represent i -th row and j -th column of matrix A respectively. Tr ( A ) is the trace of matrix A if it is square, the Frobenius norm of the matrix A  X  R n  X  d is defined as
Let U = { u 1 ,u 2 ,...,u n } denote a set of n linked data in-stances. We assume features are dynamically generated and one feature arrives at each time step, thus, at time step t , each linked data instance is associated with a set of t fea-each linked instance is tied with a new feature set F ( t +1) { f 1 ,f 2 ,...,f t ,f t +1 } . The data representation at time step t and t + 1 can be represented as X ( t ) = [ f 1 , f 2 ,..., f X ture vectors corresponding to features f 1 ,...,f t ,f denote the link information between instances in a matrix M  X  R n  X  n , where M ij = 1 if u i and u j are linked, other-wise M ij = 0. The link information can either be a directed or an undirected graph. Note that in this paper, we do not consider the dynamics of link information and the reason is that link information does not change as fast as feature in-formation; for example, the friend circles of most users are often stable once they are established.

With these notations, the task of unsupervised streaming feature selection in social media focuses on finding a subset of most relevant features S ( t )  X  F ( t ) at each time step t to facilitate clustering by utilizing both the feature information F ( t ) and the link information M .
The work flow of the proposed framework USFS is shown in Figure 2. We can observe that it consists of three compo-nents. The first component shows the representation of data. We have a set of linked instances (for example u 1 ,u 2 ,...,u for each linked instance, its features arrive through a stream-ing fashion, for example, u 1 ,u 2 ,...,u 5 are associated with features f 1 ,f 2 ...,f t at time step t ; are associated with fea-tures f 1 ,f 2 ,...,f t + i at time step t + i . The second component shows the process of the algorithm, we will first talk about how to model link information via extracting social latent factors and how to use them as a constraint through a re-gression model in Section 3.1. Then we will introduce how to model feature information to make it consistent with social latent factors in Section 3.2. At last, we will show how to efficiently test new feature and existing features in Section 3.3. After that, as shown in the third component, we obtain a subset of relevant features at each time step (for example, S ( t ) at time step t ).
Social media users connect to each other due to different factors such as movie fans, football enthusiasts, colleagues and each factor should be related to certain features (or at-tributes) of users [19]. Therefore, extracting these factors from link information should be very useful to steer the un-supervised streaming feature selection. However, in most cases, these hidden factors are not explicitly available in so-cial media data.

Uncovering hidden social factors has been extensively stud-ied [2, 22, 30]. In this work, we extract the social latent factors for each instance based on the mixed membership stochastic blockmodel [2]. In the blockmodel, it assumes that there exists a number of latent factors, and these latent factors interact with each other with certain probabilities to form social relationships. More specially, each instance is as-sociated with a k -dimensional latent factor vector  X  i  X  where  X  ig denotes the probability of u i in factor g . This means that each instance can simultaneously be sided with multiple latent factors with different affiliation strength. For each instance, the indicator vector z i  X  j denotes the latent factor membership of u i when it links to u j and z i  X  j notes the latent factor membership of u i when it is linked from u j . The interaction strength between different latent factors is encoded in a k  X  k stochastic matrix B , in which each element is between 0 and 1. Then the observed link information is generated according to the following process: 1. For each linked instance u i , 2. For each pair of linked instance ( i,j )  X  X   X U , Motivated by [12], we use a scalable inference algorithm to obtain the social latent factors  X  = [  X  1 ,  X  2 ,...,  X  n for all n instances efficiently.

As we obtain the social latent factors for each linked in-stances, we take advantage of them as a constraint to per-form feature selection through a regression model. We mea-sure the importance of each feature by its ability to differ-entiate different social latent factors. At time step t , given each social latent factor  X  i (a column of  X  ) for all instances, we are able to find a subset of most relevant features by the following minimization problem: min assigns each instance a k -dimensional social latent vector at time step t . Each column of W ( t ) , i.e., ( w ( t ) ) coefficients of t different features in approximating the i -th social latent vector of  X  .  X  is a parameter which controls the trade-off between the loss function and the ` 1 -norm. One main advantage of ` 1 -norm regression (Lasso) [31] is that it property makes it to be suitable for feature selection, as we can select features with corresponding non-zero coefficients.
In Lasso, the number of selected features is usually bounded by the number of data instances, which is unrealistic in many applications. Besides, features in social media usually have strong pairwise correlations, such as synonyms or antonyms words in text data. Lasso tends to randomly select features from a group and discards the others. Therefore, we employ the elastic net [38] on the basis of Eq. (1): min bustness of the learned model.
In Twitter, if two users post similar contents (features), they are more likely to share similar social latent factors, like hobbies, education background, etc. The similarity of social latent factors reflects the correlation of two linked in-stances in the feature space. In other words, social latent factors of two instances are more likely to be consistent when their feature similarity (like textual similarity) is high. To model the feature information, we first construct a graph G to represent the feature similarity between different data instances. The adjacency matrix A  X  R n  X  n of the graph G at time step t is defined as: A where ( x ( t ) ) i indicates the feature information of u represents p -nearest neighbors of ( x ( t ) ) i . Then feature infor-mation can be modeled by minimizing the following term: L ( t ) = D ( t )  X  A ( t ) is the Laplacian matrix. Since the Lapla-cian matrix in Eq. (3) is positive-semidefinite, Eq. (3) can also be written as:
The optimization formulation, which integrates feature in-formation, is defined as: min where  X  is the regularization parameter to balance link in-formation and feature information.
The objective function in Eq. (5) at time step t is pa-rameterized by a transformation matrix W ( t ) . It can be further decomposed into a series of k sub-problems which correspond to k social latent factors: min where i = 1 ,...,k . By solving each sub-problem in Eq. (6), we can select a subset of features at time t . Next we in-troduce how to efficiently perform feature selection when a new feature f t +1 is generated at a new time step t + 1. Following common steps of supervised streaming feature se-lection [33], the proposed framework will test: (1) whether we should select the new feature; and (2) whether we should discard some existing features.
It can be observed from Eq. (6) that at time step t + 1, incorporating a new feature feature f t +1 involves adding a new non-zero weight value ( w ( t +1) ) i t +1 to the model, which incurs a penalty increasing  X  k ( w ( t +1) ) i t +1 k 1 on the ` ularization term. The addition of the new feature f t +1 duces the overall objective function value in Eq. (6) only when the overall reduction from the first, third, and forth term outweighs the increase of ` 1 penalty  X  k ( w ( t ) )
Motivated by [26, 27], we adopt a stagewise way to check newly arrived features. Let J (( w ( t +1) ) i ) denotes the objec-tive function of Eq. (6) at time step t + 1: then the derivative of J (( w ( t +1) ) i ) with respect to ( w is as follows: In Eq. (8), the derivative of ` 1 -norm term  X  k ( w w.r.t ( w ( t +1) ) i t +1 is not smooth. Here we discuss the sign of the derivative, i.e., sign( w ( t +1) ) i t +1 . When the new feature f t +1 arrives, we first set its feature coefficient ( w ( t +1) be zero and add it to the model, if: it is easy to verify that:
In order to reduce the objective function value J (( w ( t +1) we need to slightly reduce the value of ( w ( t +1) ) i t +1 For the same reason, if: then: the sign of ( w ( t +1) ) i t +1 will be positive. If both of previous conditions are not satisfied, it is impossible to reduce the objective function value J (( w ( t +1) ) i ) by making ( w as a small disturbance around 0. In other words, for the new feature f t +1 , we need to check:
As the condition in Eq. (13) is satisfied, it indicates that the addition of the new feature f t +1 will reduce the objec-tive function value J (( w ( t +1) ) i ), therefore the new feature is included in the model described in Eq. (7).
In social media, when new features are continuously be-ing generated, they may take place of some existing features since new features can better reflect the interests of users, etc. Old features become outdated as a result, therefore, in the proposed unsupervised streaming feature selection framework USFS, we also investigate if it is necessary to remove any existing selected features.

After a new feature is accepted and added to the model, we optimize Eq. (7) with respect to existing feature weights, the optimization may force some feature weights to be zero. If the feature weight obtains a zero value, it indicates that the existence of the feature is not likely to reduce the ob-jective function value and the feature can be removed. Here we discuss how to solve the optimization problem in Eq. (7). The objective function in Eq. (7) is convex and the gradient with respect to ( w ( t +1) ) i t +1 can be easily obtained as Eq. (8), then a global optimum solution can be achieved. We choose to use a Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-newton method [3] to solve the optimization problem. Un-like traditional Newton X  X  method which requires the calcu-lation of second derivatives (the Hessian), BFGS only needs the gradient of the objective function to be computed at each iteration. Therefore, it is more efficient than Newton X  X  methods especially when Hessian evaluation is slow.
The minimization problem in Eq. (7) can be generalized to the following form:
At each iteration, the optimal solution x is updated as: where H m = B  X  1 m , B m is an approximation to the Hessian matrix ( B m  X  [  X  2 f ( x m )]), g m =  X  f ( x m ) is the gradient and  X  m is the step size that can be determined by line search. Let the vectors s m and c m be: the next Hessian approximation has to meet the secant equa-tion:
By pre-multiplying the secant equation s T m at both sides, we obtain the curvature condition:
If the curvature condition is satisfied, B m +1 in the secant equation has at least one solution, which can be updated by the following way: Its inverse, i.e., H m +1 , can be updated efficiently by Sherman-Morrison formula [10]: H
With these, the BFGS algorithm to solve Eq. (7) is illus-trated in Algorithm 1.
 Algorithm 1 BFGS to optimize Eq. (7) Input: Starting point x 0 , convergence threshold , initial Output: Optimal solution x  X  1: m  X  0 2: g m =  X  f ( x m ) 3: while || g m || &gt; do 4: Obtain a direction p m =  X  H m g m 5: Compute x m +1 = x m +  X  m p m , where  X  m is chosen 10: m  X  m + 1 11: end while 12: return x m
By solving all k sub-problems at time step t + 1, we obtain the sparse coefficient matrix W = [( w ( t +1) ) 1 ,..., ( w Since we solve each sub-problem separately, the number of non-zero weights in each ( w ( t +1) ) i ( i = 1 ,...,k ) is not neces-sarily to be the same. For each feature f j , if any of the k corresponding feature weight coefficients ( w ( t +1) model, otherwise the feature is not selected. If f j is selected, its feature score at time step t + 1 is defined as: The selected features are then sorted according to their fea-ture scores in a descending order, the higher the feature score, the more important the feature is.

The pseudo code of the proposed unsupervised stream-ing feature selection algorithm for social media data is illus-trated in Algorithm 2. It efficiently performs unsupervised feature selection when a new feature f t +1 arrives. In line 1, we obtain the social latent factor matrix  X  using the link information M . The algorithm to check new feature and existing features is illustrated in lines 2-8. More specifically, for each sub-problem, we first check the gradient condition, this step decides whether we accept the new feature (line 3). If the condition is satisfied (line 4), the new feature is included in the model (line 5) and the model is re-optimized with respect to all existing feature weights (line 6). At last, when the new feature is included in the model, it updates the Laplacian matrix (line 10), calculates the feature scores, and updates the selected feature set (lines 11-12).
The mixed membership stochastic model to extract social latent factors has a time complexity of O ( n 2 k 2 ). Assuming the total number of streaming features is t and the number of obtained features is s , the time complexity of updating Algorithm 2 Unsupervised streaming feature selection framework (USFS) Input: New feature f t +1 at time t + 1, feature weight ma-Output: Selected feature subset S ( t +1) at time step t + 1 1: Obtain social latent factors  X  from M 2: for each social latent factor  X  l ( l = 1 ,...,k ) do 3: compute gradient g for f t +1 according to Eq. (8) 4: if abs( g ) &gt;  X  then 5: add feature f t +1 to the model 6: optimize the model via BFGS in Algorithm 1 7: end if 8: end for 9: if feature f t +1 is accepted then 10: update Laplacian matrix L ( t +1) 11: obtain feature scores according to Eq. (21) 12: sort features by scores and update S ( t +1) 13: end if Laplacian matrix is bounded by O ( n 2 st ). At each time step, we check the gradient condition in Eq. (13). The time com-plexity upper bound of the gradient checking over all t time steps is O ( n 2 kst ). Since we optimize the model in Eq. (7) when the new feature is accepted, the total time of opti-mization with BGFS is O ( n 2 s 2 t ) in the worst case when the selected s features are the latest arrived s features.
Overall, the total time complexity of the proposed USFS is O ( n 2 k 2 ) + O ( n 2 st ) + O ( n 2 kst ) + O ( n 2 and s t , the upper bound of the overall time complexity is O ( n 2 s 2 t ). However, it only provides an upper bound, in real-world applications, the time complexity could be much lower than this upper bound. We will empirically show the efficiency of the proposed framework in the experiments.
For the newly generated feature, suppose there are already s features in the model, if its previous feature is added in the model, the time complexity of gradient test is O ( n 2 otherwise the time complexity is only O ( n ). To test existing features via BFGS, the time complexity is O ( n 2 s 2 ).
In this section, we conduct experiments to evaluate the performance of the proposed framework USFS. In particu-lar, we evaluate the following: 1) How is the quality of se-lected features by USFS compared with the state-of-the-art unsupervised feature selection algorithms? 2) How efficient is the proposed USFS framework? Before introducing the details of experiments, we first introduce the datasets and experimental settings.
We use two real-world social media datasets BlogCatalog and Flickr for experimental evaluation. Some statistics of the datasets are listed in Table 1.

BlogCatalog: BlogCatalog 3 is a social blog directory which manages bloggers and their blogs. Bloggers are asso-http://www.blogcatalog.com/ ciated with sets of tags, which provide feature information. Users in blogcatalog follow each other which form the social link information. Bloggers can also register their blogs un-der predefined categories, which are used as ground truth for validation in our work.

Flickr: Flickr 4 is an image hosting and sharing website, the key features in flickr are tags, user can specify the list of tags to reflect their interests. Similar to blogcatalog, users in flickr can interact with others. Photos are organized under prespecified categories, which are used as the ground truth.
Following a standard way to assess unsupervised feature selection [4, 18, 29, 35], we use the clustering performance to evaluate the quality of selected features. Two commonly used clustering performance evaluation metrics, i.e., accu-racy (ACC) and normalized mutual information (NMI) [4] are used in this paper.

To the best of our knowledge, we are the first to study streaming feature selection in social media. To investigate the effectiveness and the efficiency of the proposed frame-work, we choose the following state-of-the-art unsupervised feature selection algorithms as baseline methods: For LapScore, NDFS and USFS, we follow previous work [4, 16] to specify the number of neighborhood size to be 5 to construct the Laplacian matrix on the data instances. NDFS and LUFS have different regularization parameters, we set these regularization parameters according to the suggestions from the original papers. For USFS, we set the number of so-cial latent factors as the number of clusters. There are three important regularization parameters  X  ,  X  and  X  in USFS.  X  controls the sparsity of the model,  X  is the parameter for elastic net which controls the robustness of the model, and  X  balances the contribution of the link information and feature information. In the experiments, we empirically set  X  = 10, https://www.flickr.com/  X  = 0 . 1,  X  = 0 . 1 and more details about the effects of these parameters on the proposed framework will be discussed in Section 4.5.
 All experiments are conducted on a machine with 16GB RAM and Intel Core i7-4770 CPU.
Following streaming feature selection settings that assume features arrive one at a time [37], we divide all features into 9 groups where we choose the first { 20% ,..., 90% , 100% } as streaming features. In each group, we perform feature se-lection with traditional unsupervised feature selection algo-rithm as well as the proposed USFS algorithm. We record how many features USFS selects and specify the same num-ber as that of selected features by traditional unsupervised feature selection algorithms for a fair comparison.
After obtaining the feature selection results, K-means clus-tering is performed based on the chosen features. We repeat the K-means algorithm 20 times and report average results because K-means may converge to local minima. The clus-tering results are evaluated by both accuracy (ACC) and normalized mutual information (NMI). The higher the ACC and NMI values are, the better feature selection performance is. The comparison results are shown in Table 2 and Ta-ble 3 for BlogCatalog and Flickr, respectively. Note that the number in parentheses in the table indicates the num-ber of selected features determined by USFS. We make the following observations:
To evaluate the efficiency of the proposed USFS algorithm, we compare the running time of different methods in Fig-ure 3. As LapScore, SPEC, NDFS and LUFS are not de-signed for dealing with streaming features, we rerun the fea-ture selection process at each time step. For both datasets, we set the cumulative running time threshold to be around 10 4 seconds since all methods except USFS take more than 50 hours. As can be observed, the proposed USFS algorithm is significantly faster than other baseline methods, the av-erage processing time for each feature in BlogCatalog and Flickr is only 0.62 seconds and 1.37 seconds, respectively.
We also record the cumulative running time of USFS when the cumulative running time of other methods arrive the threshold (10 4 seconds), the results show that in BlogCat-alog, USFS is 7  X  , 20  X  , 29  X  , 76  X  faster than LapScore, LUFS, NDFS, SPEC, respectively; in Flickr, USFS is 5  X  , 11  X  , 20  X  , 75  X  faster than LapScore, LUFS, NDFS, SPEC, respectively. The difference is becoming larger as the curve of USFS in Figure 3 is getting more smooth when streaming features continuously arrive.
As discussed in Section 3, USFS has four important pa-rameters: the number of social latent factors k , and param-eters  X  ,  X  , and  X  in Eq. (6). To investigate the effects of these parameters, we vary one parameter each time and fix the other three to see how the parameter affects the feature selection performance in terms of clustering with different number of selected features. We only perform the param-eter study on BlogCatalog dataset to save space since we have similar observations on the Flickr dataset.

First, we vary the number of social latent factors k from 5 to 10 while fix the other three parameters (  X  = 10,  X  = 0 . 1,  X  = 0 . 1). The clustering performance in terms of Accuracy and NMI is illustrated in Figure 4. The clustering perfor-mance is the best when the number of social latent factors is close to the number of clusters, which is 6 in BlogCatalog.
To assess the effect of parameter  X  which controls the model sparseness, we vary  X  as { 0.001, 0.01, 0.1, 1, 10, 100, 1000 } while fix k = 6,  X  = 0 . 1,  X  = 0 . 1, performance vari-ance between  X  and number of selected features is presented in Figure 5. With the increase of  X  , the clustering perfor-mance rises rapidly and then keeps stable between the range of 10 to 1000. A high value of  X  indicates that it is not easy for new features to pass the gradient test in Eq. (13), thus the accepted features are more relevant and meaningful.
We study the effect of parameter  X  which makes the model more robust. Similar to the setting of  X  ,  X  is also in the range of { 0.001, 0.01, 0.1, 1, 10, 100, 1000 } and k = 6,  X  = 10,  X  = 0 . 1. The results are shown in Figure 6. We can see that clustering performance is much more sensitive to the number of selected features than to  X  . The performance is relatively higher when  X  is between 0.1 and 10.

At last, we evaluate the trade-off between link information and feature information by varying  X  in { 0.001, 0.01, 0.1, 1, 10, 100, 1000 } while fix k = 6,  X  = 10,  X  = 0 . 1. The results are presented in Figure 7. As shown in the figure, in most cases, the clustering performance first increases, reaches its peak and then it gradually decreases. The best performance achieves when  X  is around 0.1. These observations suggest the importance of both link information and feature infor-mation in unsupervised streaming feature selection. Cumulative Runtime (seconds)
In this section, we first briefly review some related work in traditional feature selection, then we introduce existing work on supervised streaming feature selection.
Feature selection algorithms can be broadly grouped into three categories: filter methods, wrapper methods as well as embedded methods. Filter methods are independent of any learning algorithms and are thereby very efficient, they rely on some data characteristics such as distance, consis-tency, dependency, information, and correlation to measure the strength of each feature individually [13, 16, 25, 28]. Wrapper methods use the prediction power of a predefined learning algorithm to evaluate the quality of selected fea-tures. They are inevitably computational expensive since the search space grows exponentially with the number of features [9]. Embedded methods is a tradeoff between these two models which combines feature selection and model con-struction [4, 15, 23]. Therefore, they are usually comparably efficient to filters and are comparably accurate to wrappers.
According to the availability of labels, feature selection methods consist of supervised methods and unsupervised methods. Supervised methods take advantage of the dis-criminative information encoded in class labels to select the subset of features that are able to distinguish instances from different classes [8, 23, 24]. Since real-world data is usually unlabeled and collecting labeled data is particular expensive requiring both time and effort, unsupervised feature selec-tion receives more and more attention recently. Due to the lack of label information, unsupervised feature selection al-gorithms exploit different criteria to define the relevance of features such as data similarity [4, 16, 36] and local discrim-inative information [18, 35].
Previous mentioned methods can only handle static dataset without considering the dynamic properties of features. Some-times, it is difficult to obtain the whole feature space. Stream-ing feature selection, provides a solution to this problem. The first attempt to perform streaming feature selection is credited to Perkins and Theiler [27]. They adopted a stage-wise gradient descent technique to dynamically update se-lected feature set. Alpha-investing calculates the p -value of new feature in a regression model to determine if it should be included. In Alpha-investing [37], once a feature is added, it will never be discarded in the future. OSFS [33] finds an op-timal subset of features based on online feature relevance and feature redundancy analysis. At each time step, new candi-date feature is accepted if it is strongly or weakly relevant to existing features; then existing but redundant features will be removed by redundancy analysis. In contrast to tradi-tional streaming feature selection, Wang et al. [32] studied the problem of group streaming feature selection in which features continuously arrive in groups, the setting is more realistic in real-world applications. Guo et al. [14] proposed a node classification algorithm for streaming network. They consider the changes of network structure and node contents for node classification via feature selection techniques. All previous mentioned methods, focus on supervised scenarios, unsupervised streaming feature selection, however, is not well studied yet.

It should be noted that streaming feature selection dif-fers from the task of feature selection/extraction on data streams, such as [1, 5, 6, 11, 34], in which feature sets are predefined while data are generated dynamically.
The prevalence of unlabeled, high-dimensional and high-velocity social media data presents new challenges for fea-ture selection. Meanwhile, the unique characteristics of link information in social media bring about new opportunities as we might be able to leverage them to dynamically select relevant features efficiently. In this work, we study a novel problem, unsupervised streaming feature selection for social media data. In particular, we investigate how to take advan-tage of link information to perform unsupervised streaming feature selection. Also, we propose a stagewise algorithm to solve the optimization problem at each time step. There-fore, the model has the power to decide whether to add a newly arrived feature and whether to remove existing fea-tures promptly. Theoretical time complexity analysis and empirical experimental results on real-world social media datasets demonstrate that the proposed USFS framework works effectively and efficiently.

Future work can be focused on two aspects: First, in this work, we assume that the link information is relative sta-ble compared with dynamic feature information. However, sometimes, the network structure is also continuously chang-ing, how to perform streaming feature selection on dynamic network is a challenging problem and is unsolved yet. Sec-ond, in social media, we have features from multiple sources, like image features, text features, video features, etc. We will investigate how to fuse heterogeneous feature sources in the streaming feature selection task in the future. This material is, in part, supported by National Science Foundation (NSF) under grant number IIS-1217466. [1] C. C. Aggarwal, J. Han, J. Wang, and P. S. Yu. A [2] E. M. Airoldi, D. M. Blei, S. E. Fienberg, and E. P. [3] S. Boyd and L. Vandenberghe. Convex Optimization . [4] D. Cai, C. Zhang, and X. He. Unsupervised feature [5] X. Chen and K. S. Candan. Gi-nmf: Group [6] X. Chen and K. S. Candan. Lwi-svd: low-rank, [7] J. Dem X sar. Statistical comparisons of classifiers over [8] C. Ding and H. Peng. Minimum redundancy feature [9] J. G. Dy and C. E. Brodley. Feature subset selection [10] G. H. Golub and C. F. Van Loan. Matrix [11] L. Gong, J. Zeng, and S. Zhang. Text stream [12] P. K. Gopalan, S. Gerrish, M. Freedman, D. M. Blei, [13] Q. Gu, Z. Li, and J. Han. Generalized fisher score for [14] T. Guo, X. Zhu, J. Pei, and C. Zhang. Snoc: [15] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene [16] X. He, D. Cai, and P. Niyogi. Laplacian score for [17] G. H. John, R. Kohavi, K. Pfleger, et al. Irrelevant [18] Z. Li, Y. Yang, J. Liu, X. Zhou, and H. Lu.
 [19] H. Liu, E.-P. Lim, H. W. Lauw, M.-T. Le, A. Sun, [20] H. Liu and H. Motoda. Computational Methods of [21] M. McPherson, L. Smith-Lovin, and J. M. Cook. [22] M. E. Newman and M. Girvan. Finding and [23] F. Nie, H. Huang, X. Cai, and C. H. Ding. Efficient [24] F. Nie, S. Xiang, Y. Jia, C. Zhang, and S. Yan. Trace [25] H. Peng, F. Long, and C. Ding. Feature selection [26] S. Perkins, K. Lacker, and J. Theiler. Grafting: Fast, [27] S. Perkins and J. Theiler. Online feature selection [28] M. Robnik- X  Sikonja and I. Kononenko. Theoretical and [29] J. Tang and H. Liu. Unsupervised feature selection for [30] L. Tang and H. Liu. Relational learning via latent [31] R. Tibshirani. Regression shrinkage and selection via [32] J. Wang, Z.-Q. Zhao, X. Hu, Y.-M. Cheung, M. Wang, [33] X. Wu, K. Yu, H. Wang, and W. Ding. Online [34] W. Yang, H. Xu, and A. Theorem. Streaming sparse [35] Y. Yang, H. T. Shen, Z. Ma, Z. Huang, and X. Zhou. [36] Z. Zhao and H. Liu. Spectral feature selection for [37] J. Zhou, D. Foster, R. Stine, and L. Ungar. Streaming [38] H. Zou and T. Hastie. Regularization and variable
