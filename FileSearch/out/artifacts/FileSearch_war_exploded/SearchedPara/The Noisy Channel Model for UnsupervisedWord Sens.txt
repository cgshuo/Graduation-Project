 Koc  X  University Koc  X  University sense disambiguation. In our model, each context C is modeled as a distinct channel through in the given context P ( S | C ) and possible words that can express each meaning P ( W
ThemainproblemofunsupervisedWSDisestimatingcontext-dependentP ( S best supervised systems. 1. Introduction
Word sense disambiguation (WSD) is the task o fidenti fying the correct sense o fan ambiguous word in a given context. An accurate WSD system would benefit appli-cations such as machine translation and information retrieval. The most successful
WSD systems to date are based on supervised learning and trained on sense-tagged corpora. In this article we present an unsupervised WSD algorithm that can leverage untagged text and can perform at the level of the best supervised systems for the all-nouns disambiguation task.
 considerable amounts o ftraining data, also known as the knowledge acquisition bottleneck . Yarowsky and Florian (2002) report that each successive doubling o fthe training data for WSD only leads to a 3 X 4% error reduction within their experimental range. Banko and Brill (2001) experiment with the problem o fselection among confusable words and show that the learning curves do not converge even after a billion words o ftraining data. They suggest unsupervised, semi-supervised, or active learning to take advantage o flarge data sets when labeling is expensive. Yuret (2004) observes that in a supervised naive Bayes WSD system trained on SemCor, approximately hal fo fthe test instances do not contain any o fthe contextual features (e.g., neighboring content words or local collocation patterns) observed in the training data. SemCor is the largest publicly available corpus o fsense-tagged text, and has only about a quarter million sense-tagged words. In contrast, our unsupervised system uses the Web1T data set (Brants and Franz 2006) for unlabeled examples, which contains counts from a 10 12 word corpus derived from publicly-available Web pages.  X  X nsupervised X  is typically used to describe systems that do not directly use sense-tagged corpora for training. However, many of these unsupervised systems, including ours, use sense ordering or sense frequencies from WordNet (Fellbaum 1998) or other dictionaries. Thus it might be more appropriate to call them weakly supervised or semi-supervised. More specifically, context X  X ense pairs or context X  X ord X  X ense triples are not observed in the training data, but context-word frequencies (from untagged text) and word-sense frequencies (from dictionaries or other sources) are used in model building. One o fthe main problems we explore in this study is the estimation o fcontext-dependent sense probabilities when no context X  X ense pairs have been observed in the training data.
 sense disambiguation that seamlessly integrates unlabeled text data into the model building process. Our approach is based on the noisy channel model (Shannon 1948), which has been an essential ingredient in fields such as speech recognition and machine translation. In this study we demonstrate that the noisy channel model can also be the key component for unsupervised word sense disambiguation, provided we can solve the context-dependent sense distribution problem. In Section 2.1 we show one way to estimate the context-dependent sense distribution without using any sense-tagged data. Section 2.2 outlines the complete unsupervised WSD algorithm using this model.
We estimate the distribution o fcoarse-grained semantic classes rather than fine-grained senses. The solution uses the two distributions for which we do have data: the distribu-tion o fwords used to express a given sense, and the distribution o fwords that appear in a given context. The first can be estimated using WordNet sense frequencies, and the second can be estimated using an n -gram language model as described in Section 2.3. ent levels of granularity for word sense disambiguation. Using fine-grained senses for model building is inefficient both computationally and from a learning perspective. The noisy channel model can take advantage o fthe close distribution o fsimilar senses i fthey are grouped into semantic classes. We take semantic classes to be groups o fWordNet synsets defined using the hypernym hierarchy. In each experiment we designate a number o fsynsets high in the WordNet hypernym hierarchy as  X  X ead synsets X  and use their descendants to partition the senses into separate semantic classes. In Section 3 we present performance bounds for such class-based WSD and describe our method of exploring the different levels of granularity.
 the best supervised and unsupervised systems from SensEval-2 (Cotton et al. 2001), SensEval-3 (Mihalcea and Edmonds 2004), and SemEval-2007 (Agirre, M ` arquez, and
Wicentowski 2007). Section 5 discusses these results and the idiosyncrasies o fthe data sets, baselines, and evaluation metrics used. Section 6 presents related work, and
Section 7 summarizes our contributions. 112 2. The Noisy Channel Model for WSD 2.1 Model
The noisy channel model has been the foundation of standard models in speech recog-nition (Bahl, Jelinek, and Mercer 1983) and machine translation (Brown et al. 1990).
In this article we explore its application to WSD. The noisy channel model can be used whenever a signal received does not uniquely identify the message being sent.
Bayes X  Law is used to interpret the ambiguous signal and identify the most probable intended message. In WSD, we model each context as a distinct channel where the intended message is a word sense (or semantic class) S , and the signal received is an ambiguous word W . In this section we will describe how to model a given context C as a noisy channel, and in particular how to estimate the context-specific sense distribution without using any sense-tagged data.
 This is the well-known Bayes X  formula with an extra P( . | dependence on the context.

To perform WSD we need to find the sense S that maximizes the probability P( S
This is equivalent to the maximization o fthe product P( W denominator P( W | C ) does not depend on S . To perform the maximization, the two distributions P( W | S , C )andP( S | C ) need to be estimated for each context C . be expressed in the given context. In unsupervised WSD we do not have access to any sense-tagged data, thus we do not know what senses are likely to be expressed in any given context. Therefore it is not possible to estimate P( S frequencies for the given context P( W | C ). We use the WordNet sense frequencies to estimate P( W | S ) and a statistical language model to estimate P( W
Section 2.3. We make the independence assumption P( W | S , C ) = P( W distribution o fwords used to express a particular sense is the same for all contexts.
Finally, the relationship between the three distributions, P( S is given by the total probability theorem:
We can solve for P( S | C ) using linear algebra. Let WS be a matrix, s and w two vectors such that:
Using this new form, we can see that Equation (2) is equivalent to the linear equation w = WS  X  s and s can be solved using a linear solver. Typically WS is a tall matrix and the system has no exact solutions. We use the Moore X  X enrose pseudoinverse WS compute an approximate solution: solutions. We use the pseudoinverse solution in all our experiments because it can be computed fast and none of the alternatives we tried made a significant difference in
WSD performance. 2.2 Algorithm Section 2.1 described how to apply the noisy channel model for WSD in a single context.
In this section we present the steps we follow in our experiments to simultaneously apply the noisy channel model to all the contexts in a given word sense disambiguation task.

Algorithm 1 1. Let W be the vocabulary. In this study we took the vocabulary to be the 2. Let S be the set o fsenses or semantic classes to be used. In this study we 3. Let C be the set o fcontexts (nine-word windows for a 5-gram model) 4. Compute the matrix WC where WC ik = P( W = i | C = k ). Here i ranges 5. Compute the matrix WS where WS ij = P( W = i | S = j ). Here i ranges over 6. Compute the matrix SC = WS +  X  WC where SC jk = P( S = j 7. Compute the best semantic class for each WSD instance by using 114 8. Compute the fine-grained answer for each WSD instance by taking the 9. Apply the one sense per discourse heuristic: I fa word is found to have 2.3 Estimation Procedure
In Section 2.1, we showed how the unsupervised WSD problem expressed as a noisy channel model can be decomposed into the estimation o ftwo distributions: P( W P( W | C ). In this section we detail our estimation procedure for these two distributions. meaning, we used the WordNet sense frequencies. 1 We did not perform any smoothing for the zero counts and used the maximum likelihood estimate: count( W , S ) / count( S ).
As described in later sections, we also experimented with grouping similar WordNet senses into semantic classes. In this case S stands for the semantic class, and the counts from various senses of a word in the same semantic class are added together to estimate P( W | S ).
 language model. We define the context as the nine-word window centered on the target expressed as:
Equation (5) indicates that P( W | C ) is proportional to P( w words in the context are fixed for a given WSD instance. Equation (6) is the standard decomposition o fthe probability o fa word sequence into conditional probabilities.
The first four terms do not include the target word w 5 , and have been dropped in Equa-tion (7). We also truncate the remaining conditionals to four words reflecting the Markov assumption o fthe 5-gram model. Finally, using an expression that is proportional to theargmaxinEquation(1).
 language model. To get accurate domain-independent probability estimates we used the
Web 1T data set (Brants and Franz 2006), which contains the counts o fword sequences up to length five in a 10 12 word corpus derived from publicly-accessible Web pages.
Estimation o fP( W | C ) is the most computationally expensive step o fthe algorithm, and some implementation details are given in Appendix B. 3. Semantic Classes Our algorithm internally differentiates semantic classes rather than fine-grained senses.
Using fine-grained senses in the noisy channel model would be computationally ex-pensive because the word X  X ense matrix needs to be inverted (see Equation [4]). It is also unclear whether using fine-grained senses for model building will lead to better learning performance: The similarity between the distributions of related senses is ignored and the data becomes unnecessarily fragmented.
 fine-grained senses for evaluation. During evaluation, the coarse-grained semantic classes predicted by the model are mapped to fine-grained senses by picking the lowest numbered WordNet sense in the chosen semantic class. 2 This is necessary to perform a meaningful comparison with published results.
 nym hierarchy (see Section 6 for alternative definitions). Section 4 presents three WSD experiments using different sets of semantic classes at different levels of granularity.
In each experiment we designate a number o fsynsets high in the WordNet hypernym hierarchy as  X  X ead synsets X  and use their descendants to form the separate semantic classes.
 collectively exhaustive descendants. To assign every synset to a unique semantic class, we impose an ordering on the semantic classes. Each synset is assigned only to the first semantic class whose head it is a descendant o faccording to this ordering. I fthere are synsets that are not descendants o fany o fthe heads, they are collected into a separate semantic class created for that purpose.
 to return the correct fine-grained sense when this is not the lowest numbered sense in a semantic class. To quantify the restrictive effect of working with a small number of semantic classes, Figure 1 plots the number o fsemantic classes versus the best possible 116 oracle accuracy for the nouns in the SemCor corpus. To compute the oracle accuracy, we assume that the program can find the correct semantic class for each instance, but has to pick the first sense in that class as the answer. To construct a given number o fsemantic classes, we used the following algorithm:
Algorithm 2 1. Initialize all synsets to be in a single  X  X efault X  semantic class. 2. For each synset, compute the following score: the oracle accuracy achieved 3. Take the synset with the highest score and split that synset and its 4. Repeat steps 2 and 3 until the desired number o fsemantic classes is classes is surprisingly high. In particular, the best reported noun WSD accuracy (78%) is achievable i fwe could per fectly distinguish between five semantic classes. 4. Three Experiments
We ran three experiments with the noisy channel model using different sets of semantic classes. The first experiment uses the 25 WordNet semantic categories for nouns, the second experiment looks at what happens when we group all the senses to just two or three semantic classes, and the final experiment optimizes the number o fsemantic classes using one data set (which gives 135 classes) and reports the out-of-sample result using another data set.
 are used for evaluation. We focus on the disambiguation of nouns for several reasons. Nouns constitute the largest portion o fcontent words (48% o fthe content words in the
Brown corpus [Kucera and Francis 1967] are nouns). For many tasks and applications (e.g., Web queries [Jansen, Spink, and Pfaff 2000]) nouns are the most frequently encoun-tered and important part o fspeech. Finally, WordNet has a more complete coverage o fnoun semantic relations than other parts o fspeech, which is important for our experiments with semantic classes.
 most likely semantic class in all the experiments. The lowest numbered sense in that class is taken as the fine-grained answer. Finally we apply the one sense per discourse heuristic: I fthe same word has been assigned more than one sense within the same document, we take a majority vote and use sense numbers to break the ties. vised and unsupervised systems on noun disambiguation for each data set are given.
The first-sense baseline (FSB) is obtained by always picking the lowest numbered sense for the word in the appropriate WordNet version. We prefer the FSB baseline over the commonly used most-frequent-sense baseline because the tie breaking is unambiguous.
All the results reported are for fine-grained sense disambiguation. The top three systems given in the table for each task are all supervised systems; the result for the best unsupervised system is given in the last column. The reported unsupervised systems do use the sense ordering and frequency information from WordNet. 4.1 First Experiment: The 25 WordNet Categories
In previous work, descendants o f25 special WordNet synsets (known as the unique beginners ) have been used as the coarse-grained semantic classes for nouns (Crestan,
El-B ` eze, and De Loupy 2001; Kohomban and Lee 2005). These unique beginners were used to organize the nouns into 25 lexicographer files based on their semantic category during WordNet development. Figure 2 shows the synsets at the top o fthe noun hierarchy in WordNet. The 25 unique beginners have been shaded, and the two graphics show how the hierarchy evolved between the two WordNet versions used in this study. 118 classes. The distribution o fwords for each semantic class, P( W on WordNet sense frequencies. The distribution of words for each context, P( W estimated using a 5-gram model based on the Web 1T corpus. The system first finds the most likely semantic class based on the noisy channel model, then picks the first sense in than the previously reported unsupervised results.
 gives the confusion matrix for the Senseval2 data set. We can see that frequently occur-ring concrete classes like person and body are disambiguated well. The largest source 25 classes may not be the ideal candidates for word sense disambiguation. Even though they allow a sufficient degree of fine-grained distinction (Table 2 shows that we can get 85 X 90% i fwe could pick the right class every time), they seem too easy to con fuse. In the next few experiments we will use these observations to design better sets of semantic classes. 4.2 Second Experiment: Distinguishing Mental and Physical Concepts
Figure 1 shows that the upper bound for fine-grained disambiguation is relatively high even for a very small number of semantic classes. In our next experiment we look at how well our approach can perform differentiating only two or three semantic classes. synsets used to define the semantic classes. Figure 2 shows that the top level o fthe hypernym hierarchy has changed significantly between the WordNet versions. Thus, different head synsets are chosen for different data sets. However, the main distinction captured by our semantic classes seems to be between mental and physical concepts.
Table 4 gives the results. The performance with a few semantic classes is comparable to the top supervised algorithms in each o fthe three data sets. 4.3 Third Experiment: Tuning the Number of Classes
Increasing the number of semantic classes has two opposite effects on WSD perfor-mance. The higher the number, the finer distinctions we can make, and the maximum possible fine-grained accuracy goes up. However, the more semantic classes we define, the more difficult it becomes to distinguish them from one another. For an empirical analysis o fthe e f fect o fsemantic class granularity on the fine-grained WSD accuracy, we generated different sets of semantic classes using the following algorithm.
Algorithm 3 1. Sort all the synsets according to their  X  X ubtree frequency X : i.e., the total 2. Take the desired number o fsynsets with the highest subtree frequency and with up to 600 semantic classes defined based on Algorithm 3. Note the differences: (i) 120
Figure 1 gives the best possible oracle accuracy, Figure 3 gives the actual WSD accuracy; (ii) Algorithm 2 chooses the head synsets based on their oracle score, Algorithm 3 chooses them based on their subtree frequency.
 identify distinct peaks at 3, 25, and 100 X 150 semantic classes. One hypothesis is that these peaks correspond to  X  X atural classes X  at different levels of granularity. Here are some example semantic classes from each peak: o fFigure 3, we used the SemEval-2007 data set as our test sample. When the 135 semantic classes from the highest peak are used for the disambiguation of the nouns in the SemEval-2007 data set, an accuracy o f69.8% was achieved. This is higher than the accuracy o fthe best supervised system on this task (68.6%), although the di f ference is not statistically significant. 5. Discussion
In this section we will address several questions raised by the results o fthe experi-ments. Why do we get different results from different data sets? Are the best results significantly different than the first-sense baseline? Can we improve our results using better semantic classes? 5.1 Why Do We Get Different Results from Different Data Sets?
Table 5 summarizes our results from the three experiments of Section 4. There are some significant differences between the data sets. with its generally lower baseline and scores. The difference in accuracy is probably due to the difference in data preparation. In the two Senseval data sets all content words were targeted for disambiguation. In the SemEval-2007 data set only verbs and their noun arguments were selected, targeting only about 465 lemmas from about 3,500 words o ftext. For the Senseval-3 data set none o four results, or any published result we know of, is significantly above the baseline for noun disambiguation. This may be due to extra noise in the data X  X he inter-annotator agreement for nouns in this data set was 74.9%. 5.2 Are the Best Results Significantly Different Than the FSB? Among all the published results for these three data sets, our two results for the
Senseval-2 data set and the top supervised result for the Senseval-2 data set are the only ones statistically significantly above the FSB for noun disambiguation at the 95%
SemEval-2007 data set has only 159 nouns; and a result o f71.8% would be needed to demonstrate a difference from the baseline of 64.2% at the 95% confidence interval.  X  X ignificance X  in general. A statistically significant difference may not be necessary or sufficient for a significant impact on an application. Even a WSD system that is statis-tically indistinguishable from the baseline according to the  X  X otal accuracy X  metric is most probably providing significantly different answers compared to always guessing the first sense. There are metrics that can reveal these differences, such as  X  X alanced detecting the use o fa non-dominant sense. X  combined with the relatively low inter-annotator agreement (e.g., 74.9% for Senseval-3 nouns) makes progress in the traditional WSD task difficult. Annotators who are perfectly proficient in comprehending language nevertheless find it difficult to distin-guish between artificially-created dictionary senses. I four long term goal is to model human competence in language comprehension, it would make sense to focus on tasks at which humans are naturally competent. Dictionary-independent tasks such as lexical substitution or textual entailment may be the right steps in this direction. 5.3 Can We Improve Our Results Using Better Semantic Classes?
In order to get an upper bound for our approach, we searched for the best set of semantic classes specific to each data set using the following greedy algorithm. 122
Algorithm 4 1. Initialize all synsets to be in a single  X  X efault X  semantic class. 2. For each synset, compute the following score: the WSD accuracy achieved 3. Take the synset with the highest score and split that synset and its 4. Repeat steps 2 and 3 until the WSD accuracy can no longer be improved. sets o fsemantic classes. The noisy channel model was applied with the best set o f semantic classes for each data set. Table 6 summarizes the results. Note that these results are not predictive o fout-o f-sample accuracy because Algorithm 4 picks a specific set o f semantic classes optimal for a given data set. But the results do indicate that a better set o fsemantic classes may lead to significantly better WSD accuracy. In particular each result in Table 6 is significantly higher than previously reported supervised or unsupervised results.
 fiability is a topic o fongoing research. See Kohomban and Lee (2007) for a supervised solution using feature-based clustering that tries to maintain feature X  X lass coherence.
Non-parametric Bayesian approaches such as Teh et al. (2006) applied to context distri-butions could reveal latent senses in an unsupervised setting. 6. Related Work
Stevenson (2003). The Senseval and SemEval workshops (Cotton et al. 2001; Mihalcea and Edmonds 2004; Agirre, M ` arquez, and Wicentowski 2007) are good sources o frecent work, and have been used in this article to benchmark our results.
 used for speech recognition (Bahl, Jelinek, and Mercer 1983), machine translation (Brown et al. 1990), question answering (Echihabi and Marcu 2003), spelling correction (Brill and Moore 2000), and document compression (Daume III and Marcu 2002) among others. To our knowledge our work is the first application o fthe noisy channel model to unsupervised word sense disambiguation. plored in Yuret (2007) and Hawker (2007). For specific modeling techniques used in this article see Yuret (2008); for a more general review of statistical language modeling see Chen and Goodman (1999), Rosenfeld (2000), and Goodman (2001).
 ous work. Senses that are similar have been identified using WordNet relations (Peters,
Peters, and Vossen 1998; Crestan, El-B ` eze, and De Loupy 2001; Kohomban and Lee 2005), discourse domains (Magnini et al. 2003), annotator disagreements (Chklovski and Mihalcea 2003), and other lexical resources such as Roget (Yarowsky 1992), LDOCE (Dolan 1994), and ODE (Navigli 2006).
 essentially the 25 WordNet noun categories we have used in our first experiment in addition to 15 verb categories similarly defined. They report a supersense precision of 67.60 for nouns and verbs of Senseval-3. Table 2 gives our supersense score as 78% for
Senseval-3 nouns. However, the results are not directly comparable because they do not report the noun and verb scores separately or calculate the corresponding fine-grained score to compare with other Senseval-3 results.
 pher files and experiment with clustering techniques to construct their semantic classes.
Their classes are based on local features from sense-labeled data and optimize feature X  class coherence rather than adhering to the WordNet hierarchy. Their supervised system achieves an accuracy o f74.7% on Senseval-2 nouns and 73.6% on Senseval-3 nouns. (2004) explore the large-scale acquisition o fsense-tagged examples from the Web and train supervised, minimally supervised (requiring sense bias information from hand-tagged corpora, similar to our system), and fully unsupervised WSD algorithms using this corpus. They report good results on the Senseval-2 lexical sample data compared to other unsupervised systems. Martinez, de Lacalle, and Agirre (2008) test a similar set of systems trained using automatically acquired corpora on Senseval-3 nouns. Their mini-mally supervised system obtains 63.9% accuracy on polysemous nouns from Senseval-3 (corresponding to 71.86% on all nouns). 7. Contributions
We have introduced a new generative probabilistic model based on the noisy channel framework for unsupervised word sense disambiguation. The main contribution of this model is the reduction o fthe word sense disambiguation problem to the estimation o f two distributions: the distribution o fwords used to express a given sense, and the dis-tribution o fwords that appear in a given context. In this framework, context similarity is determined by the distribution o fwords that can be placed in the given context. This replaces the ad hoc contextual feature design process by a statistical language model, allowing the advances in language modeling and the availability o flarge unlabeled corpora to have a direct impact on WSD performance.
 fine-grained WSD. The noisy channel model is a good fit for class-based WSD, where the model decides on a coarse-grained semantic class instead o fa fine-grained sense.
The chosen semantic class is then mapped to a specific sense based on the WordNet ordering during evaluation. We show that the potential loss from using coarse-grained classes is limited, and state-of-the-art performance is possible using only a few semantic classes. We explore semantic classes at various levels o fgranularity and show that 124 the relationship between granularity and fine-grained accuracy is complex, thus more work is needed to determine an ideal set o fsemantic classes.
 system with the best systems from previous Senseval and SemEval workshops. We consistently outperform any previously reported unsupervised results and achieve comparable performance to the best supervised results.
 Appendix A: Solutions for P ( S | C )
P( S = j | C = k )and w i = P( W = i | C = k ), and the last one as a matrix: WS i |
S = j ). Our problem becomes finding a solution to the linear equation w = WS
Using the Moore X  X enrose pseudoinverse, WS + ,wefindasolution s = WS solution minimizes the distance | WS  X  s  X  w | . There are two potential problems with this pseudoinverse solution. First, it may violate the non-negativity and normalization constraints o fa probability distribution. Second, a maximum likelihood estimate should minimize the cross entropy between WS  X  s and w , not the Euclidean distance. We addressed the normalization problem using a constrained linear solver and the cross-entropy problem using numerical optimization. However, our experiments showed the difference in WSD performance to be less than 1% in each case. The pseudoinverse solution, s = WS +  X  w , can be computed quickly and works well in practice, so this is the solution that is used in all our experiments.
 Appendix B: Estimating P ( W | C )
Estimating P( W | C ) for each context is expensive because the number of words that need to be considered is large. The Web 1T data set contains 13.5 million unique words, and
WordNet defines about 150,000 lemmas. To make the computation feasible we needed
WordNet lemmas with the same part o fspeech as the target word. We further required the word to have a non-zero count in WordNet sense frequencies. The inflection and capitalization o feach word W was automatically matched to the target word. As a result, we estimated P( W | C ) for about 10,000 words for each noun context and assumed the other words had zero probability. The n -grams required for all the contexts were listed, and their counts were extracted from the Web 1T data set in one pass. The P( W was estimated for all the words and contexts based on these counts. In the end, we only used the 100 most likely words in each context for efficiency, as the difference in results using the whole distribution was not significant. For more details on smoothing with a large language model see Yuret (2008), although we did not see a significant difference in WSD performance based on the smoothing method used.
 Acknowledgments 126
