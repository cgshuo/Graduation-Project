 Link prediction is a fundamental problem in social network analysis. The key technique in unsupervised link prediction is to find an appropriate similarity measure between nodes of a network. A class of wildly used similarity measures are based on random walk on graph. The traditional ran-dom walk (TRW) considers the link structures by treating all nodes in a network equivalently, and ignores the cen-trality of nodes of a network. However, in many real net-works, nodes of a network not only prefer to link to the similar node, but also prefer to link to the central nodes of the network. To address this issue, we use maximal en-tropy random walk (MERW) for link prediction, which in-corporates the centrality of nodes of the network. First, we study certain important properties of MERW on graph G by constructing an eigen-weighted graph G . We show that the transition matrix and stationary distribution of MERW on G are identical to the ones of TRW on G . Based on G , we further give the maximal entropy graph Laplacians, and show how to fast compute the hitting time and commute time of MERW. Second, we propose four new graph ker-nels and two similarity measures based on MERW for link prediction. Finally, to exhibit the power of MERW in link prediction, we compare 27 various link prediction methods over 3 synthetic and 8 real networks. The results show that our newly proposed MERW based methods outperform the state-of-the-art method on most datasets.
 H.2.8 [ Database management ]: Database applications X  Data mining ; G.2.2 [ Discrete mathematics ]: Graph the-ory X  Graph algorithms Algorithm, Theory, Experimentation Maximal entropy random walk, graph kernels, similarity measures, link prediction
Link prediction has been recognized as a fundamental problem in social network analysis that aims to infer which unobserved links will appear in the near future by a given snapshot of a network [24]. Many problems in social com-puting and data mining can be modeled as a link prediction problem, such as the friends suggestion problem [32] in so-cial network and the product recommendation problem [22] in online shopping system.

Due to the broad applications, the link prediction problem has been attracted much attention in research communities [24, 11, 25]. The key challenges of the link prediction prob-lem are owing to the sparsity and huge size of the networks. Recently, a large number of approaches have been proposed to address this issue. The methods can be classified into two categories: supervised methods and unsupervised methods. The supervised method for link prediction is identified as the state-of-the-art method, which predict the unobserved links by a binary classifier. However, the supervised methods, such as the method proposed in [25], typically suffer from the so-called class imbalance and feature selection problem [2]. Moreover, most classifiers are based on the class distri-bution of the training data, thus they could perform poorly in some datasets that do not meet the prior assumptions. Instead, the unsupervised methods work in an agnostic way, thus they can naturally avoid this problem. In addition, unsupervised methods do not need to decide which node features and edge features to use for link prediction, thus they also avoid the feature selection problem. In this paper, we focus on unsupervised methods for link prediction.
The key point of the unsupervised methods for link pre-diction is to find an appropriate similarity measure between nodes of a graph. The widely applied methods to measure similarities between nodes of a graph are based on random walk on graph. We call this random walk the traditional random walk (TRW) in this paper. In TRW, the transi-tion probabilities from a start node to any of its neighbors are equivalent, i.e., the reciprocal of the out-degree of the start node. TRW considers the link structures by treating all nodes of a graph as equivalent entities. In other words, TRW ignores the centrality of nodes of a graph. However, in the context of link prediction, we argue that the centrality of the nodes play an important role. The reason is because the nodes in a network not only prefer to link to the sim-ilar nodes, but also prefer to link to the central nodes of the network. Consequently, it is highly desirable to take the centrality of nodes into consideration in the random walk for link prediction task.
To address the issue of node centrality, we resort to maxi-mal entropy random walk (MERW), which incorporates the centrality of nodes of a graph. This is because the transition probabilities of MERW are proportional to the eigenvector centrality of the nodes [5], which is wildly applied to mea-sure the importance of the nodes over a graph. Unlike other centrality measures based on degree, betweenness, and close-ness [30], the eigenvector centrality measure is based on the idea that the important of a node in a graph is larger if it has many important neighbors. In other words, the eigenvector centrality measure captures the structural context of a node. In addition to capture the centrality of the nodes, MERW has two nice properties. First, the same length paths be-tween two given nodes have the same probabilities. Second, the longer path between two given nodes has lower prob-ability, which consists with the intuition for measuring the similarity between nodes. As a result, we believe MERW can probably perform much better than TRW in link prediction. MERW has been studied recently. Burda et al. study the MERW design [6]. Sinatra et al. present the evidence of constructing approximate MERW to make it more practical [34]. Devenne and Libert propose centrality measures for complex networks based on MERW, but they do not sys-tematically analyze the properties of MERW [9]. To the best of our knowledge, we are the first group to investigate a series of important properties of MERW in depth, and systematically design a set of similarity measures based on MERW for link prediction task. Our extensive experiments exhibit the power of MERW in link prediction.

The main contributions of this paper are summarized as follows.
The rest of the paper is organized as follows. We introduce maximal entropy random walk (MERW) in comparison with traditional random walk (TRW) in Section 2. We give a new construction of eigen-weighted graph as basis to study the important properties of MERW in Section 3. And then we propose new graph kernels and similarity measures based on MERW for link prediction in Section 4. We show the extensive performance results on link prediction in Section 5, and discuss related work in Section 6. Finally, We conclude this work in Section 7.
In this section, we introduce traditional random walk fol-lowed by discussions on maximal entropy random walk on graphs. As mentioned in [9], maximal entropy random walk cannot be applied to weighted graphs. We concentrate our-selves on unweighted and undirected graphs in this paper.
Consider an unweighted and undirected graph G ( V, E ), with a set of nodes V and a set of edges E , where the size of nodes is n = | V | . Below, we use V i to denote a node in a graph and v to denote a vector representation. The graph G can be represented as a symmetric adjacency matrix, A , where A ij = 1 if nodes ( V i , V j )  X  E otherwise A ij = 0. The degree of a node V i  X  G is denoted as d i = Let D = diag ( d 1 , d 2 ,  X  X  X  , d n ) be a diagonal matrix of node degrees, a random walk on G can be defined using a tran-sition matrix P = D  X  1 A , with entries p ij = A ij d traditional random walk (TRW) in order to distinguish from the maximal entropy random walk discussed below. It is well known that TRW on an undirected graph forms a reversible Markov chain and reaches a unique stationary distribution  X  [26]. The stationary distribution  X  satisfies the so-called detailed balance equation  X  i p ij =  X  j p ji , where  X  i for 1  X  i  X  n .
 Maximal entropy random walk : We review the maxi-mal entropy random walk (MERW) on graphs [6, 31, 33]. First, we introduce the entropy rate of random walk on graph which is well known in information theory [8]. Con-sider a path  X  t ij generated by the random walk with length t from V i to V j . Suppose the path  X  t ij passes through the nodes V i , V i 1 ,  X  X  X  , V i t  X  1 , V j , then the probability p (  X  path  X  t ij is defined as p (  X  t ij ) = p ii 1 p i 1 i 2 The Shannon entropy [8] of all paths with length t gener-ated by the walker is E t =  X  entropy rate of random walk is defined as  X  = lim t  X  X  X  E t
A well known result in information theory [8] indicates that the maximal entropy rate of random walk on a graph can be computed from the transition matrix P and the sta-tionary distribution  X  as follows. On the other hand, the maximal entropy rate of random walk on a graph is bounded by ln  X  [31] (In [9, 33], this quantity is called topological entropy of a graph.), where  X  is the largest eigenvalue (or called dominant eigenvalue) of the adjacency matrix A . As shown in [6], this quantity can be obtained by the following asymptotic value. Unlike TRW, MERW aims to maximize the entropy rate of a walk by carefully constructing a probabilistic transition ma-trix as follows. Let v = ( v 1 , v 2 ,  X  X  X  , v n ) be the normalized eigenvector ( va lue  X  of the adjacency matrix A . Obviously, v i is positive as guaranteed by Forbenius-Perron theorem [20]. Then, the transition probability of MERW becomes W e can reformulate it into a matrix form as follows. wh ere D v = diag ( v 1 , v 2 ,  X  X  X  , v n ) denotes the diagonal ma-trix with respect to v . The stationary distribution of MERW becomes  X   X  = ( v 2 1 , v 2 2 ,  X  X  X  , v 2 n ) [6]. Together with  X  p , it can be easily confirmed that MERW maximizes the entropy rate. In addition, all paths  X  t ij with length t between nodes V i and V j have the same probability p (  X  t ij ) = whic h is independent of the intermediate nodes in the path. Obviously, the longer path has the smaller probability. In this section, we identify certain useful properties of MERW which are crucial to construct MERW based similar-ity measures on graph. First, we propose an eigen-weighted graph as a basic tool for studying important properties of MERW. Then, we define three new graph Laplacians based on MERW, and show that the hitting time and commute time of MERW can be efficiently computed by the new graph Laplacian.
TRW has been well studied, but MERW is not yet. In order to investigate the properties of MERW, we propose a new method to construct a family of eigenvector-weighted graphs G from the original unweighted and undirected graph G . With this construction, we show that the transition ma-trix and stationary distribution of MERW on G are identi-cal to those of TRW on G , respectively. We further define a special eigenvector-weighted graph, called an eigen-weighted graph G . Based on the special graph, we simplify our investi-gation of MERW on G by analyzing and deriving from TRW on G .
 Definition 3.1: ( Eigenvector-weighted graph ) Given an unweighted and undirected graph G ( V, E ), its adjacency matrix A , the largest eigenvalue  X  of A , and the normalized eigenvector v = ( v 1 , v 2 ,  X  X  X  , v n ) w.r.t.  X  , an eigenvector-weighted graph is G ( V, E, W ), where W is an eigenvector-weighting set, and is constructed by W ij =  X v i v j , if edge ( V i , V j )  X  E , where  X  is a real parameter. 2 We give two theorems regarding relationships between MERW on G and TRW on G .
 Theorem 3.1: The transition matrix of MERW on graph G is identical to the transition matrix of TRW on the eigenvector-weighted graph G . 2 Proof Sketch: Consider TRW on eigenvector-weighted graph G . Deriving from the transition probability on an edge ( V i , V j ), we have tha t is identical to Eq. (3). Hence, the transition matrix of MERW on G is the same as that of TRW on G . 2 Theorem 3.2: The stationary distribution of MERW on graph G is identical to the stationary distribution of TRW on the eigenvetor-weighted graph G . 2 Proof Sketch: Consider TRW on a n -node eigenvetor-weighted graph G . Let the stationary distribution of TRW be  X  = (  X  1 ,  X  2 ,  X  X  X  ,  X  n ), we have where i = 1 , 2 ,  X  X  X  , n . That is identical to the stationary distribution of MERW on graph G . 2
In order to define maximal entropy graph Laplacians and to deduce important properties of MERW, we define an eigen-weighted graph which uses the dominant eigenvalue and eigenvector as the weighting set, by letting  X  = 1 / X  . Definition 3.2: ( Eigen-weighted graph ) Given an un-weighted and undirected graph G ( V, E ), its adjacency ma-trix A , the largest eigenvalue  X  of A , and the normalized eigenvector v = ( v 1 , v 2 ,  X  X  X  , v n ) w.r.t.  X  , an eigen-weighted graph is defined as G ( V, E, W ), where W is an eigen-weighting set, and is constructed by W ij = v i v j  X  , if edge ( V
It is worth noting that for any connected, non-bipartite, undirected and unweighted graph G , there exists only one eigen-weighted graph G corresponding to G , based on the uniqueness properties of the dominant eigenvalue and eigen-vector of the adjacency matrix of G . On the other hand, TRW on a general weighted graph typically cannot be con-verted to MERW on the corresponding unweighted graph. This is because it is almost impossible to decompose the weight of a given edge of the weighted graph into the product form of two corresponding elements of the dominant eigen-vector. Note that both Theorem 3.1 and Theorem 3.2 hold for eigen-weighted graphs, because the eigen-weighted graph is a special eigenvector-weighted graph.

Fig. 1 shows an example. For the given original graph G (left), the largest value (  X  ) of A and the normalized eigen-vector v w.r.t.  X  can be obtained using the power iteration 0 . 5651 , 0 . 2454). The resulting eigen-weighted graph G is shown on the right.
Graph Laplacian in spectral graph theory [7] is widely used to analyze important parameters of random walk on graph. Here we first study the graph Laplacians on the eigen-weighted graph, and then we propose three new graph Laplacians on the original un-weighted and undirected graph based on MERW.
 Laplacian of eigen(vector)-weighted graph : We in-troduce two graph Laplacians. The first graph Laplacian, which we call general graph Laplacian, is characterized by the transition matrix and stationary distribution of the walk [7, 4]. Let  X  be the stationary distribution of the walk, P be the transition matrix, and  X  = diag (  X  ), the general graph Laplacian L can be defined as where I is an identity matrix. In eigenvector-weighted graph, we have wh ere D v = diag ( v 1 , v 2 ,  X  X  X  , v n ) and A is the adjacency matrix of G . Together with Theorem 3.1 and Theorem 3.2, we can conclude that the Laplacian matrix L defined on the eigenvector-weighted graph G is identical to the general graph Laplacian defined on the original G based on MERW. The second is the combinatorial graph Laplacian [7]. For any weighted graph, the combinatorial graph Laplacian L is defined by where W is the adjacency matrix of the weighted graph and D is a diagonal matrix of row sums of W .

It is easy to verify that, for eigen-weighted graphs, the combinatorial graph Laplacian L is identical to the general graph Laplacian L . In effect, the eigen-weighted graph is only the case that satisfies L = L .
 Maximal entropy graph Laplacians : Based on the gen-eral graph Laplacian (Eq. (6)) and the combinatorial graph Laplacian on the eigen-weighted graph, we introduce three new graph Laplacians based on MERW.

Given an unweighted and undirected graph G , its adja-cency matrix A , the largest eigenvalue  X  , and the normal-ized eigenvector v = ( v 1 , v 2 ,  X  X  X  , v n ) w.r.t  X  . Let D diag ( v 1 , v 2 ,  X  X  X  , v n ). The maximal entropy combinatorial Laplacian (MECL) of G is defined as follows.

L is equal to the combinatorial graph Laplacian defined on the eigen-weighted graph, thus we refer L to maximal entropy combinatorial Laplacian (MECL). Similar to the normalized Laplacians based on TRW, we can define the normalized maximal entropy Laplacians based on MERW, namely, the symmetric normalized maximal entropy Lapla-cian L sym , and the asymmetric normalized maximal entropy Laplacian L rw as follows.

Hitting time and commute time are two important param-eters of TRW. The hitting time h ( i, j ) defines the average number of steps that start from node V i and first arrive at node V j in TRW [26]. It can be computed in an iterative fashion as follows [10, 26]. The commute time c ( i, j ) is defined as the average number of steps that the walker starts at node V i , reaching node V ( i 6 = j ) for the first time, and then goes back to node V commute time of TRW on an undirected graph can be used as a distance measure.

It is well known that the hitting time and commute time of TRW can be computed by the pseudo-inverse of the graph Laplacian. We show that the hitting and commute time of MERW can also be computed by the pseudo-inverse of the MECL. Assume the Moore-Penrose pseudo-inverse [10] of MECL is denoted by L + , with entries L + ij . We have Theo-rem 3.3.
 Theorem 3.3:
We give the proof in Appendix A. It is not hard to verify that the hitting and commute time of MERW on an un-weighted and undirected graph is equal to that of TRW on the corresponding eigen-weighted graph. Based on the construction of eigen-weighted graph, the commute time of MERW on graph G is also a distance measure.
The key technique in unsupervised link prediction is to define a similarity measure between nodes of a graph. In this section, we give a class of new graph kernels and similarity measures based on MERW for link prediction.
In TRW on graph, the pseudo-inverse of the Laplacian ma-trix is called commute time kernel [10]. We showed that the commute time of MERW can be computed by the pseudo-inverse of the MECL. Likewise, we can define a new maximal entropy commute time kernel based on MERW. The maxi-mal entropy commute time kernel ( CK ) and its normalized kernel ( CK N ) are given below.
 Since the pseudo-inverse of L (Eq. (7)) and L sym (Eq. (8)) are positive semidefinite, CK and CK N are valid kernels.
As shown in [17], the heat diffusion kernel is closely related to TRW on graph. The heat diffusion kernel is constructed based on heat equation and can be defined as matrix ex-ponentiation. Motivated by this, we define new maximal entropy heat diffusion kernels. The maximal entropy heat diffusion kernel DK and the normalized maximal entropy heat diffusion kernel DK N are given below.
 where  X  is a positive real parameter.

The regularized Laplacian kernel of TRW on graph was first presented in [35] based on the regularization operators. By regularization on MECL, we define new maximal entropy regularized Laplacian kernels of MERW on graph. The max-imal entropy regularized Laplacian kernel RK and the nor-malized counterpart RK N are given below.
 where  X  is a positive real parameter, and I is an identity matrix.

With the help of the eigen-weighted graph, we can also define new maximal entropy Neumann kernels on the eigen-weighted graph G . Firstly, we give a theorem.
 Theorem 4.1: Given a graph G , and its adjacency matrix A , let D v be a diagonal matrix defined in Eq. (7), then the matrix I  X   X  D v AD v / X  is a positive definite matrix, for  X   X  (0 , 1) . 2 Proof Sketch: Obviously, the matrix I  X   X  D v AD v / X  is symmetric. Let  X  be the spectral radius of matrix D v AD v then we have  X   X || D v AD v First, because v is a normalized vector, the last inequality holds. Second, because  X   X  (0 , 1), all eigenvalues of matrix I  X   X  D v AD v / X  is positive. This completes the proof. 2
Based on the theorem, a maximal entropy Neumann ker-nel NK and the corresponding normalized version NK N can be defined as follows.
 where  X   X  (0 , 1) is a real parameter.

The normalized maximal entropy Neumann kernel NK N is closely related to the Katz index when it is applied for link prediction [15]. In addition, it is not hard to show that the normalized maximal entropy regularized Laplacian kernel is equivalent to the maximal entropy Neumann kernel with a different real parameter, i.e., RK N = (1  X   X  )( I  X   X  A / X  ) where  X  =  X / (1 +  X  ).
In [12], inverse P-distance ( P D ) is defined to compute the proximity between nodes, V i and V j , on a graph. Here, the summation is taken over all paths  X  ij that start at node V i and ends at node V j , where l (  X  ij ) denotes the length of path  X  ij . P D ( i, j ) measures distances inversely: it is larger for nodes V i  X  X loser X  to V j . In MERW, all paths with equal length have the same probabilities. The maximal entropy inverse P-distance can be written in a more compact form. Here, Eq. (22) can be put in a matrix form as follows. By replacing  X  l by  X  l l ! in Eq. (22), we have a matrix expo-nentiation, denoted as P D 0 as follows.

The original SimRank is based on the idea that two nodes are similar if they are joined to similar neighbors [13]. The SimRank is closely related to the random walk on a product graph. Motivated by this, we propose a new SimRank based maximal entropy random walk on graph, denoted as S ( x, y ) for two nodes V x and V y in Eq. (25).

S ( x, y ) = where  X   X  (0 , 1) and N ( x ) denotes to the neighbor node set of node V x . We call Eq. (25) the maximal entropy SimRank equation. The existence and uniqueness of the solution to the maximal entropy SimRank equation is guaranteed by the following theorem.
 Theorem 4.2: The maximal entropy SimRank equation de-fined in Eq. (25) has a unique solution. 2 Proof Sketch: This can be proved in the similar as to prove the original SimRank equation in [13]. 2
The maximal entropy SimRank can be computed in an iterative fashion as follows.
 where the initial point is defined as R 0 ( x, y ) = 0 if x 6 = y , otherwise R 0 ( x, y ) = 1. Since the maximal entropy Sim-Rank equation has an unique solution, Eq. (26) has the same form as Eq. (25), the iterative computation can be reached by a fixed-point. More formally, S ( x, y ) = lim
Note that SimRank can be computed by the so-called  X  X xpected-f meeting distance X  of two surfers in a random surfer-pairs model [13]. The random surfer pairs model is identical to a random surfer on a so-called product graph induced by the original graph. Each node in the product graph is a node pair of the original graph. Thus, two random surfers starting at nodes V x and V y , respectively, meeting at V , is equal to the random surfer starting from node ( V x , V and ending at node ( V u , V u ) in the product graph. The max-imal entropy SimRank can also be modeled by a maximal entropy random surfer-pairs model. Below, we first give a maximal entropy expected-f meeting distance and then es-tablish the equivalent relationship between it and maximal entropy SimRank. The maximal entropy expected-f meeting distance dist ( x , y ) between two nodes V x and V y is defined as where  X  denotes a path generated by the maximal entropy random surfer starting at node ( x, y ) and ending at node ( u, u ) in the product graph. p (  X  ) denotes the probability of path  X  , and l (  X  ) is the length of the  X  . Based on Eq. (27), we have the following theorem.
 Theorem 4.3: The maximal entropy SimRank between a node pair ( x, y ) is their maximal entropy expected-f meeting distance traveling back-edges. 2 Proof Sketch: We prove it by splitting the path  X  into two parts: the first step ( a, b ) ; ( x, y ), where a  X  N ( x ) , b  X  N ( y ); an d the remaining path  X  0 . dist ( x, y ) = Note that l (  X  ) = l (  X  0 ) + 1 and p (  X  ) = v x v y  X  2 v dist ( x, y ) is equal to the maximal entropy SimRank (Eq. (25)), the maximal entropy expected-f meeting distance between two nodes is equivalent to their maximal entropy SimRank.
In this section, we evaluate the effectiveness of MERW based similarity measures for link prediction. In the fol-lowing, we first introduce the experimental setup, and then report our results. Datasets: We conduct our experiments on 3 synthetic and 8 real networks. Specifically, for synthetic networks, we generate three networks with 1,000 nodes using three clas-sic random graph generators: ER (Erods-Renyi random graph [30]), BA (Barabasi-Albert random graph (scale free graph) [3]), and SW (small word random graph [30]). For real networks, we test the proposed methods on 8 represen-tative datasets, which are widely used for link prediction both in computer science community and physics commu-nity. Specifically, the first five are USAir (network of US air transportation system), C.elegans (neural network of the nematode worm [36]), Yeast (protein-protein interac-tion network [14]), Power (network of power grid of the western US [36]), and NetScience (collaboration network of researchers [29], who work on complex network theory). The second three are collaboration networks collected form Arxiv e-print archive [21], including three different areas of physics. In particular, they are Gr-Qc (General Relativity and Quantum Cosmology), Hep-ph (High Energy Physics-Phenomenology), and Hep-Th (High Energy Physics-Theory). Table 1 shows the numbers of nodes and edges of the net-works as well as the numbers of nodes and edges of the giant components (GC) of the networks.
 Evaluation metrics: We employ two widely used metrics to evaluate the link prediction methods: the Area under the ROC curve (AUC)[25, 27] and precision [27, 2]. The first metric evaluates the overall ranking yielded by the al-gorithms, while the second metric focuses on top-K predic-tive results. In our experiments, the AUC is computed by a standard method described in [27]. A larger AUC value indicates a better link prediction performance. The preci-sion is defined as the ratio of relevant number of items over all selected items using k K . Here, we set K = 30, and k is the number of links that successfully predicted by the al-gorithms. Obviously, the larger precision means the higher predictive accuracy.
 Baselines: We compare 27 various link prediction algo-rithms involving 11 MERW based methods, 10 TRW based methods, common neighbor, Adamic/Ada [1], supervised link prediction method [25], and supervised random walk [2]. In particular, the similarity measures we tested in-clude: Commute time of TRW (CTT), Commute time of MERW (CTME), Commute time kernel (CK), Maximal en-tropy commute time kernel (MECK), Normalized commute time kernel (NCK), Normalized maximal entropy commute time kernel (NMECK), Heat diffusion kernel (DK), Maximal entropy heat diffusion kernel (MEDK), Normalized heat dif-fusion kernel (NDK), Normalized maximal entropy heat dif-fusion kernel (NMEDK), Regularized Laplacian kernel (RK), Maximal entropy regularized Laplacian kernel (MERK), Nor-malized regularized Laplacian kernel (NRK), Normalized maximal entropy regularized Laplacian kernel (NMERK), Neumann kernel (NK), Maximal entropy Neumann kernel (MENK), Normalized Neumann kernel (NNK), Normalized maximal entropy Neumann kernel (NMENK), Inverse P-distance (PD), Maximal entropy inverse P-distance (MEPD), Inverse P-distance with matrix exponentiation (PDM), Max-imal entropy inverse P-distance with matrix exponentiation (MEPDM), SimRank (SR), Maximal entropy SimRank (MESR), and Common Neighbor (CN), Adamic/Adar (AA), super-vised link prediction method (HPLP+), and supervised ran-dom walk (SRW).
 Link prediction methodology: For each dataset given in Table 1, first, we extract the giant component of the graph, and randomly split the edges into a training set and a test set. The test set contains 10% of all edges in the giant component. In particular, we conduct 10 times ran-dom partitions of training and test sets on the datasets, and the link prediction results are the average over this parti-tions. Second, we perform the link prediction algorithms on the datasets. Specifically, we consider two cases: the unsu-pervised methods and supervised methods. For the unsu-pervised algorithms, which involve MERW and TRW based method, CN, and AA, we compute the similarity matrix on the training set using the similarity measures described in the previous sections. Then, we use the similarity matrix to calculate the predictions for the edges in the test set. For the supervised algorithms, including HPLP+ and SRW, we perform the corresponding learning algorithms on the train-ing set, and then compute predictions on the test set. It is worth mentioning that we implement a naive Bayes classifer using the full feature set (HPLP+) defined in [25] for super-vised link prediction, as it obtains the best performance in [25]. Finally, we compare the link prediction performance using the evaluation metrics described above.
 Parameter settings: In MERW or TRW based similarity measures, there is only one parameter: the damping factor  X  . W e set  X  = 0 . 5, as it is not very sensitive in our experi-ments. In addition, we set all parameters of other baselines as the same as their original papers respectively. Experimental environment: All experiments are con-ducted on the Linux workstation with 2xQuad-Core Intel Xeon 3.06 GHz CPU, 48 Gb memory, and running CentOS 5.5. All the algorithms are implemented by MATLAB 2009 and Visual C++ 6.0.
Table 2 and Table 3 show the results of 27 various link prediction algorithms on 11 datasets under AUC and preci-sion metric respectively. We can clearly see that the MERW based link prediction methods achieve better predictions than all other unsupervised methods under both AUC and precision metric. Moreover, on most datsets, the MERW based methods perform better than supervised methods. Generally, the normalized maximal entropy graph kernels outperform the non-normalized ones. And the normalized maximal entropy diffusion kernel achieves the best perfor-mance among all the MERW based methods. We show the detail analysis as follows.
 Commute time kernels : By comparing commute time of TRW (CTT), commute time of MERW (CTME, Eq. (12)), commute time kernel (CK), maximal entropy commute time kernel (MECK, Eq. (13)) and their corresponding normal-ized kernels (Eq. (14)), we observe that the normalized max-imal entropy commute time kernel outperforms the others on most datasets. We also see that both CTT and CTME perform poorly. This consists with a recent result reported in [28], in which the authors claim that the commute time distance converges to a meaningless distance measure, thus it results in poor precision for link prediction. Heat diffusion/regularized Laplacian kernels/Neumann kernels : Comparing among heat diffusion kernel (DK), max-imal entropy heat diffusion kernel (MEDK, Eq. (15)), and their normalized counterparts (NMEDK, Eq. (16)), the re-sults show that NMEDK achieves significant improvement over DK, MEDK, and NDK. For instance, in Yeast dataset (column 7, row 8-11), NMEDK achieves near-optimal AUC, and obtain 67.6%, 517.4%, and 135.9% relative improvement on DK, MEDK, and NDK in terms of precision metric, re-spectively. The similar comparisons can also be observed between the regularized Laplacian kernels and maximal en-tropy regularized Laplacian kernels (Eq. (17) and Eq. (18)), and also between the Neumann kernels and maximal en-tropy Neumann kernels. In addition, it is worth mention-ing that the normalized maximal entropy heat diffusion, normalized maximal entropy regularized Laplacian kernel, and the normalized maximal entropy Neumann kernel get the similar performance on most datasets, and they outper-form the corresponding non-normalized kernels. The AUC of the maximal entropy graph kernels achieve near-optimal value on most datasets. Moreover, the precision of these graph kernels is close to 1 on USAir, C.elegances, Yeast, and NetScience, as well as the three synthetic graphs. This results indicate that the MERW based similarity measures can probably capture the nature of link formation process of these networks, thus the link predictive precision is close to 1.
 Inverse P-distance and SimRank : Among inverse P-distance (PD), inverse P-distance with matrix exponenti-ation (PDM), SimRank (SR), and their maximal entropy counterparts (Eq. (23), Eq. (24), and Eq. (25)), the maximal entropy P-distances with matrix exponentiation (MEPDM) outperforms others in terms of both AUC and precision. Over all the datasets, the maximal entropy P-distances with matrix exponentiation (MEPDM) improves AUC over PD, MEPD, and PDM by 11.8%, 1%, 16.6% on average, respec-tively. And also it boosts precision on PD, MEPD, and PDM by 52.8%, 0.7%, 362.6% on average respectively. Sim-Rank measures perform poorly than the graph kernels. More worse, the time complexity of SimRank measures is O ( n 4 thus we cannot obtain all the experimental results. Comparison with supervised methods : Here we com-pare the MEPDM with supervised link prediction (HPLP+), and supervised random walk (SRW), as MEPDM achieves the best performance over the unsupervised methods in our experiments. We can clearly see that the performance of MEPDM is better than supervised methods (HPLP+ and SRW) over USAir, C.ele, Yeast, NetScience, GrQc, HepTh, and three synthetic networks under both AUC and pre-cision metric. Moreover, on the rest datasets, MEPDM achieves competitive performance with supervised methods. It is worth mentioning that the SRW slightly outperforms HPLP+ under both AUC and precision metric on most datasets. The AUC of MEPDM, HPLP+, and SRW falls into the range (0.72, 0.99), and the precision of this algo-rithms is roughly between (0.12, 0.96). This results suggest that the MEPDM, as supervised methods do, exhibits very good performance in link prediction.

To summarize, the experimental results highlight the power of MERW in link prediction. Also, the results empirically confirm that the centrality of nodes is very important in link prediction. Since the MERW based methods inherently capture the centrality of nodes, they can yield much better performance than those methods that do not consider the centrality of nodes in link prediction. Indeed, as observed in many real networks [3, 30], the link formation typically con-sists with a preferential attachment process. That is to say, the nodes in the network tend to link to the central nodes, which results in a so-called X  X ich-get-richer X  X henomenon. In effect, the MERW implicitly incorporates this phenomenon into the random walk process. This is because the transition probability in each step of MERW refers to the eigenvec-tor centrality of the nodes, thus it makes the walk greedily moves to the important nodes. Besides, as opposed to the supervised methods, the MERW based methods work in a agnostic manner, thus they can naturally avoid the class distribution and feature selection problem [2]. Link prediction and similarity measure on graph : Af-ter the seminal work by Liben-Nowell and Kleinberg [24], the link prediction problem has attracted considerable attention in recent years both from computer science and physics com-munity [27, 11, 19, 25, 18, 2]. The existing link prediction approaches can be classified into two categories: unsuper-vised and supervised methods. Most unsupervised link pre-diction algorithms are based on the similarity measure be-tween the nodes of a graph. A recent survey can be found in [27]. Below, we focus on the random walk based similar-ity measures. In [13], SimRank is proposed, based on the idea that two nodes are similar if they are joined to sim-ilar neighbor nodes. The complexity of this algorithm is O ( n 4 ). Pei, et al. [23] propose an efficient single-pair Sim-Rank algorithm. In [38], a family of dissimilarity measures are developed based on a biased random walk on graph, which generalize both the commute-time and the shortest-path distances. In [12], a unified distance function, namely inverse P-distance, is proposed. There exists a strong con-nection between the personalized pagerank and the inverse P-distance. In [11], it uses graph kernels for link prediction task. In [19], a learning framework is proposed for link pre-diction, which generalizes several graph kernels based meth-ods. Later, in [18], the same authors proposes a spectral evolution model and develops a spectral extrapolation algo-rithm for link prediction, which is based on the observation that the large networks change over time result in a change of a the graph X  X  spectrum and keeping the eigenvectors un-changed. More recently, the supervised link prediction has attracted much attention [25, 2]. For instance, in [25], the authors propose a supervised method using classic classifier such as naive Bayes, decision tree, and bagging for link pre-diction. In [2], a supervised random walk is designed for link prediction, which performs random walk on a weighted graph with the weights learned by a supervised learning al-gorithms. The supervised methods typically suffer from the class imbalance and feature selection problems. However, our MERW based methods are unsupervised approaches, thus they can naturally avoid the feature selection problem. Maximal entropy random walk on graph : The maxi-mal entropy random walk was first proposed by Ruelle and Bowens in [33] and it was also called Ruelle-Bowens random walk [9]. This random walk on an unweighted graph chooses the transition probabilities proportional to the importance of the nodes measured by its eigenvector centrality, which is a well known centrality measure in sociology [5]. As shown in [31, 6, 9, 34], this process makes all paths between two given nodes with same length have the same probabilities. That is to say, the transition probabilities of this random walk are chosen to maximize the entropy rate of the walk.
In [6], a maximal entropy random walk is designed on an unweighted and undirected graph according to the dominant eigenvector of the adjacency matrix. In [31], it shows that there is one and only one random walk that can achieve the maximal entropy rate. The main problem of designing max-imal entropy random walk is that it requires to know the dominant eigenvector of the adjacency matrix, which means one should have global knowledge of the graph. However, such global knowledge is unavailable. To make MERW more practical, [34] shows that one can construct approximate maximal entropy random walk with the degree of the nodes on graph. The most related to our work is [9], where a cen-trality measure, namely entropy rank, is proposed based on the maximal entropy random walk on an unweighted and di-rected graph. The entropy rank uses the stationary distribu-tion of the maximal entropy random walk on an unweighted and directed graph as the measure of centrality. As shown in [9], the stationary distribution of MERW on a directed graph is identical to the element-wise product of the domi-nant left and right singular vector of the adjacency matrix. The major drawback of this measure is that it can make the centrality of the nodes with zero in-degree or out-degree equal to zero. In this work, we explore the power of MERW on undirected graphs in the context of link prediction. Graph kernel : Graph kernel is a powerful tool. We focus on graph kernels defined as a similarity measure between nodes of a given graph. Kondor and Lafferty [17] first pro-pose to construct kernel on graph. They propose an expo-nential kernel, namely diffusion kernel, based on the heat equation. This kernel can be naturally constructed by ma-trix exponentiation. Smola and Kondor extend it based on the regularization operators, which produce several regular-ized graph kernels [35]. Recently, Fouss, et al. [10] show that the Moore-Penrose pseudo-inverse of the graph Lapla-cian is a kernel, also called commute time kernel. This kernel is wildly used as a similarities measure between nodes of a graph [10, 37]. However, very recently, Luxburg, et al. [28] show that the commute time distance does not consider the structure of the graph and it will converge to a meaningless distance measure on a graph. This results indicate that the commute time kernel is not very well for measure the sim-ilarity between nodes of a graph. The other graph kernel is Neumann kernel [11], which can be expressed as infinite series of matrix powers. The Neumann kernel is closely re-lated to the random walk with restart, which is well known for measure the importance of nodes on a graph. It is im-portant to note that all the graph kernels mentioned above are closely related to the transitional random walk on graph. We study new graph kernels based on MERW.
In this paper, we propose a set of unsupervised link pre-diction methods that incorporate the centrality of nodes of the graph based on MERW. We first study certain im-portant properties of MERW by constructing a new eigen-weighted graph. Specifically, based on the eigen-weighted graph, we give a class of new graph Laplacians, namely maximal entropy graph Laplacians, and show that the hit-ting and commute time of MERW can be computed using the pseudo-inverse of the maximal entropy combinatorial Laplacian. Then, we define four types of graph kernels and two similarity measures on graph based on MERW for link prediction. Finally, we compare 27 various link prediction algorithms over 11 diverse datasets, and show our newly proposed MERW based method (NMEDK) outperforms all the other unsupervised approaches as well as the supervised methods on most datasets. Future work includes generaliz-ing the MERW based methods to directed graphs and ex-ploring MERW for other data mining and machine learning applications.
 We claim some properties of pseudo-inverse of MECL as follows: (1) L + is a positive semidefinite matrix. (2) L rank n  X  1. (3) L + is doubly centered. Since MECL is equal to the graph Laplacian of the corresponding eigen-weighted graph, MECL shares the properties of the ordinary weighted graph Laplacian. Hence, the claims hold. In addition, we give a lemma [20] in order to prove the Theorem 3.3. Lemma 7.1: For any irreducible matrix A  X  R n  X  n with spectral radius  X  ( A ) = r , if A i is a principal submatrix of A obtained by removing the i-th row and column of blocks, then matrix r I  X  A i is nonsingular. 2
With the properties and Lemma 7.1, we can prove Theo-rem 3.3. First, we reformulate Eq. (10) into a vector form as h = e + P h , where e is a vector with elements all one, h is the hitting time vector, and P is the transition matrix. Without loss of generality, we remove the n -th row of vec-tor h and e , and remove the n -th row and n -th column of matrix A , and denote them by  X  h ,  X  e , and  X  A respectively. We can obtain  X  h =  X  e +  X  P  X  h , where  X  P =  X  D thermore, let  X  L =  X  D 2 v  X   X  D v  X  A  X  D v / X  , we have According to Lemma 7.1,  X  L is nonsingular. We thus get  X  [10], we can efficiently solve the pseudo-inverse of  X  L ( computing the pseudo-inverse of L ( L + ). We have More generally, we have We can further compute the commute time as follows: c ( i, k ) = h ( i, k ) + h ( k, i ) = ( L + ii + L + kk Since we normalize the dominate eigenvector v , the last equality holds.
 The work was supported by grants of the Research Grants Council of the Hong Kong SAR, China No. 419008 and 419109. [1] L. A. Adamic and E. Adar. Friends and neighbors on [2] L. Backstrom and J. Leskovec. Supervised random [3] A.-L. Barabasi and R. Albert. Emergence of scaling in [4] D. Boley, G. Ranjan, and Z.-L. Zhang. Commute [5] P. Bonacich. Power and centrality: A family of [6] Z. Burda, J. Duda, J. M. Luck, and B. Waclaw. [7] F. R. K. Chuang. Spectral Graph Theory . Regional [8] T. M. Cover and J. A. Thomas. Elements of [9] J.-C. Delvenne and A.-S. Libert. Centrality measures [10] F. Fouss, A. Pirotte, J.-M. Renders, and M. Saerens. [11] T. Ito, M. Shimbo, T. Kudo, and Y. Matsumoto. [12] G. Jeh and J. Widom. Scaling personalized web [13] G. Jeh and J. Widom. Simrank: a measure of [14] H. Jeong, S. P. Mason, A.-L. Barab  X  l  X csi1, and Z. N. [15] L. Katz. A new status index derived from sociometric [16] D. J. Klein and M. Randic. Resistance distance. [17] R. I. Kondor and J. Lafferty. Diffusion kernels on [18] J. Kunegis, D. Fay, and C. Bauckhage. Network [19] J. Kunegis and A. Lommatzsch. Learning spectral [20] A. N. Langville and C. D. Meyer. Google X  X  PageRank [21] J. Leskovec. Standford network analysis project. [22] M. Li, B. M. Dias, I. Jarman, W. El-Deredy, and P. J. [23] P. Li, H. Liu, J. X. Yu, J. He, and X. Du. Fast [24] D. Liben-Nowell and J. M. Kleinberg. The [25] R. N. Lichtenwalter, J. T. Lussier, and N. V. Chawla. [26] L. Lovasz. Random walks on graphs: A survey. [27] L. Lu and T. Zhou. Link prediction in complex [28] U. v. Luxburg, A. Radl, and M. Hein. Getting lost in [29] M. E. J. Newman. Finding community structure in [30] M. E. J. Newman. Networks: An Introduction . [31] W. PARRY. Intrinsic markov chains. Transactions of [32] M. Roth, A. Ben-David, D. Deutscher, G. Flysher, [33] D. Ruelle. Thermodynamic Formalism .
 [34] R. Sinatra, J. Gomez-Gardenes, R. Lambiotte, [35] A. J. Smola and R. I. Kondor. Kernels and [36] D. J. Watts and S. Strogatz. Collective dynamics of [37] L. Yen, F. Fouss, C. Decaestecker, P. Francq, and [38] L. Yen, A. Mantrach, and M. Shimbo. A family of
