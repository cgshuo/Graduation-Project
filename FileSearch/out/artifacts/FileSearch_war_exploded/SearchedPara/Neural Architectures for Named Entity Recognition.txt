 Named entity recognition (NER) is a challenging learning problem. One the one hand, in most lan-guages and domains, there is only a very small amount of supervised training data available. On the other, there are few constraints on the kinds of words that can be names, so generalizing from this small sample of data is difficult. As a result, carefully con-structed orthographic features and language-specific knowledge resources, such as gazetteers, are widely used for solving this task. Unfortunately, language-specific resources and features are costly to de-velop in new languages and new domains, making NER a challenge to adapt. Unsupervised learning from unannotated corpora offers an alternative strat-egy for obtaining better generalization from small amounts of supervision. However, even systems that have relied extensively on unsupervised fea-tures (Collobert et al., 2011; Turian et al., 2010; Lin and Wu, 2009; Ando and Zhang, 2005b, in-ter alia ) have used these to augment, rather than replace, hand-engineered features (e.g., knowledge about capitalization patterns and character classes in a particular language) and specialized knowledge re-sources (e.g., gazetteers).

In this paper, we present neural architectures for NER that use no language-specific resources or features beyond a small amount of supervised training data and unlabeled corpora. Our mod-els are designed to capture two intuitions. First, since names often consist of multiple tokens, rea-soning jointly over tagging decisions for each to-ken is important. We compare two models here, (i) a bidirectional LSTM with a sequential condi-tional random layer above it (LSTM-CRF;  X  2), and (ii) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition-based parsing with states represented by stack LSTMs (S-LSTM;  X  3). Second, token-level evidence for  X  X eing a name X  includes both ortho-graphic evidence (what does the word being tagged as a name look like?) and distributional evidence (where does the word being tagged tend to oc-cur in a corpus?). To capture orthographic sen-sitivity, we use character-based word representa-tion model (Ling et al., 2015b) to capture distribu-tional sensitivity, we combine these representations with distributional representations (Mikolov et al., 2013b). Our word representations combine both of these, and dropout training is used to encourage the model to learn to trust both sources of evidence (  X  4). Experiments in English, Dutch, German, and Spanish show that we are able to obtain state-of-the-art NER performance with the LSTM-CRF model in Dutch, German, and Spanish, and very near the state-of-the-art in English without any hand-engineered features or gazetteers (  X  5). The transition-based algorithm likewise surpasses the best previously published results in several lan-guages, although it performs less well than the LSTM-CRF model. We provide a brief description of LSTMs and CRFs, and present a hybrid tagging architecture. This ar-chitecture is similar to the ones presented by Col-lobert et al. (2011) and Huang et al. (2015). 2.1 LSTM Recurrent neural networks (RNNs) are a family of neural networks that operate on sequential data. They take as input a sequence of vectors ( x 1 , x 2 ,..., x n ) and return another sequence ( h 1 , h 2 ,..., h n ) that represents some information about the sequence at every step in the input. Although RNNs can, in theory, learn long depen-dencies, in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence (Bengio et al., 1994). Long Short-term Memory Networks (LSTMs) have been designed to combat this issue by incorporating a memory-cell and have been shown to capture long-range depen-dencies. They do so using several gates that control the proportion of the input to give to the memory cell, and the proportion from the previous state to forget (Hochreiter and Schmidhuber, 1997). We use the following implementation: h t = o t tanh( c t ) , where  X  is the element-wise sigmoid function, and is the element-wise product.

For a given sentence ( x 1 , x 2 ,..., x n ) containing n words, each represented as a d -dimensional vector, an LSTM computes a representation context of the sentence at every word t . Naturally, generating a representation of the right context as well should add useful information. This can be achieved using a second LSTM that reads the same sequence in reverse. We will refer to the former as the forward LSTM and the latter as the backward LSTM. These are two distinct networks with differ-ent parameters. This forward and backward LSTM pair is referred to as a bidirectional LSTM (Graves and Schmidhuber, 2005).

The representation of a word using this model is obtained by concatenating its left and right context representations, h t = [ tions effectively include a representation of a word in context, which is useful for numerous tagging ap-plications. 2.2 CRF Tagging Models A very simple X  X ut surprisingly effective X  X agging model is to use the h t  X  X  as features to make indepen-dent tagging decisions for each output y t (Ling et al., 2015b). Despite this model X  X  success in simple problems like POS tagging, its independent classifi-cation decisions are limiting when there are strong dependencies across output labels. NER is one such task, since the  X  X rammar X  that characterizes inter-pretable sequences of tags imposes several hard con-straints (e.g., I-PER cannot follow B-LOC; see  X  2.4 for details) that would be impossible to model with independence assumptions.

Therefore, instead of modeling tagging decisions independently, we model them jointly using a con-ditional random field (Lafferty et al., 2001). For an input sentence we consider P to be the matrix of scores output by the bidirectional LSTM network. P is of size n  X  k , where k is the number of distinct tags, and P i,j cor-responds to the score of the j th tag of the i th word in a sentence. For a sequence of predictions we define its score to be where A is a matrix of transition scores such that A i,j represents the score of a transition from the tag i to tag j . y 0 and y n are the start and end tags of a sentence, that we add to the set of possi-ble tags. A is therefore a square matrix of size k +2 . A softmax over all possible tag sequences yields a probability for the sequence y : During training, we maximize the log-probability of the correct tag sequence: log( p ( y | X )) = s ( X , y )  X  log where Y X represents all possible tag sequences (even those that do not verify the IOB format) for a sentence X . From the formulation above, it is ev-ident that we encourage our network to produce a valid sequence of output labels. While decoding, we predict the output sequence that obtains the maxi-mum score given by:
Since we are only modeling bigram interactions between outputs, both the summation in Eq. 1 and the maximum a posteriori sequence y  X  in Eq. 2 can be computed using dynamic programming. 2.3 Parameterization and Training The scores associated with each tagging decision for each token (i.e., the P i,y  X  X ) are defined to be the dot product between the embedding of a word-in-context computed with a bidirectional LSTM X  exactly the same as the POS tagging model of Ling et al. (2015b) and these are combined with bigram compatibility scores (i.e., the A y,y 0  X  X ). This archi-tecture is shown in figure 1. Circles represent ob-served variables, diamonds are deterministic func-tions of their parents, and double circles are random variables.

The parameters of this model are thus the matrix of bigram compatibility scores A , and the parame-ters that give rise to the matrix P , namely the param-eters of the bidirectional LSTM, the linear feature weights, and the word embeddings. As in part 2.2, let x i denote the sequence of word embeddings for every word in a sentence, and y i be their associated tags. We return to a discussion of how the embed-dings x i are modeled in Section 4. The sequence of word embeddings is given as input to a bidirectional LSTM, which returns a representation of the left and right context for each word as explained in 2.1.
These representations are concatenated ( c i ) and linearly projected onto a layer whose size is equal to the number of distinct tags. Instead of using the softmax output from this layer, we use a CRF as pre-viously described to take into account neighboring tags, yielding the final predictions for every word y . Additionally, we observed that adding a hidden layer between c i and the CRF layer marginally im-proved our results. All results reported with this model incorporate this extra-layer. The parameters are trained to maximize Eq. 1 of observed sequences of NER tags in an annotated corpus, given the ob-served words. 2.4 Tagging Schemes The task of named entity recognition is to assign a named entity label to every word in a sentence. A single named entity could span several tokens within a sentence. Sentences are usually represented in the IOB format (Inside, Outside, Beginning) where ev-ery token is labeled as B-label if the token is the beginning of a named entity, I-label if it is inside a named entity but not the first token within the named entity, or O otherwise. However, we de-cided to use the IOBES tagging scheme, a variant of IOB commonly used for named entity recognition, which encodes information about singleton entities (S) and explicitly marks the end of named entities (E). Using this scheme, tagging a word as I-label with high-confidence narrows down the choices for the subsequent word to I-label or E-label , however, the IOB scheme is only capable of determining that the subsequent word cannot be the interior of an-other label. Ratinov and Roth (2009) and Dai et al. (2015) showed that using a more expressive tagging scheme like IOBES improves model performance marginally. However, we did not observe a signif-icant improvement over the IOB tagging scheme. As an alternative to the LSTM-CRF discussed in the previous section, we explore a new architecture that chunks and labels a sequence of inputs using an algorithm similar to transition-based dependency parsing. This model directly constructs representa-tions of the multi-token names (e.g., the name Mark Watney is composed into a single representation).
This model relies on a stack data structure to in-crementally construct chunks of the input. To ob-tain representations of this stack used for predict-ing subsequent actions, we use the Stack-LSTM pre-sented by Dyer et al. (2015), in which the LSTM is augmented with a  X  X tack pointer. X  While sequen-tial LSTMs model sequences from left to right, stack LSTMs permit embedding of a stack of objects that are both added to (using a push operation) and re-moved from (using a pop operation). This allows the Stack-LSTM to work like a stack that maintains a  X  X ummary embedding X  of its contents. We refer to this model as Stack-LSTM or S-LSTM model for simplicity.

Finally, we refer interested readers to the original paper (Dyer et al., 2015) for details about the Stack-LSTM model since in this paper we merely use the same architecture through a new transition-based al-gorithm presented in the following Section. 3.1 Chunking Algorithm We designed a transition inventory which is given in Figure 2 that is inspired by transition-based parsers, in particular the arc-standard parser of Nivre (2004). In this algorithm, we make use of two stacks (des-ignated output and stack representing, respectively, completed chunks and scratch space) and a buffer that contains the words that have yet to be processed. The transition inventory contains the following tran-sitions: The SHIFT transition moves a word from the buffer to the stack, the OUT transition moves a word from the buffer directly into the output stack while the REDUCE ( y ) transition pops all items from the top of the stack creating a  X  X hunk, X  labels this with label y , and pushes a representation of this chunk onto the output stack. The algorithm com-pletes when the stack and buffer are both empty. The algorithm is depicted in Figure 2, which shows the sequence of operations required to process the sen-tence Mark Watney visited Mars .

The model is parameterized by defining a prob-ability distribution over actions at each time step, given the current contents of the stack, buffer, and output, as well as the history of actions taken. Fol-lowing Dyer et al. (2015), we use stack LSTMs to compute a fixed dimensional embedding of each of these, and take a concatenation of these to ob-tain the full algorithm state. This representation is used to define a distribution over the possible ac-tions that can be taken at each time step. The model is trained to maximize the conditional probability of sequences of reference actions (extracted from a la-beled training corpus) given the input sentences. To label a new input sequence at test time, the maxi-mum probability action is chosen greedily until the algorithm reaches a termination state. Although this is not guaranteed to find a global optimum, it is ef-fective in practice. Since each token is either moved directly to the output (1 action) or first to the stack and then the output (2 actions), the total number of actions for a sequence of length n is maximally 2 n .
It is worth noting that the nature of this algorithm model makes it agnostic to the tagging scheme used since it directly predicts labeled chunks. 3.2 Representing Labeled Chunks When the REDUCE ( y ) operation is executed, the al-gorithm shifts a sequence of tokens (together with their vector embeddings) from the stack to the out-put buffer as a single completed chunk. To compute an embedding of this sequence, we run a bidirec-tional LSTM over the embeddings of its constituent tokens together with a token representing the type of the chunk being identified (i.e., y ). This function is given as g ( u ,..., v , r y ) , where r y is a learned em-bedding of a label type. Thus, the output buffer con-tains a single vector representation for each labeled chunk that is generated, regardless of its length. The input layers to both of our models are vector representations of individual words. Learning inde-pendent representations for word types from the lim-ited NER training data is a difficult problem: there are simply too many parameters to reliably estimate. Since many languages have orthographic or mor-phological evidence that something is a name (or not a name), we want representations that are sen-sitive to the spelling of words. We therefore use a model that constructs representations of words from representations of the characters they are composed of (4.1). Our second intuition is that names, which may individually be quite varied, appear in regular contexts in large corpora. Therefore we use embed-dings learned from a large corpus that are sensitive to word order (4.2). Finally, to prevent the models from depending on one representation or the other too strongly, we use dropout training and find this is crucial for good generalization performance (4.3). 4.1 Character-based models of words An important distinction of our work from most previous approaches is that we learn character-level features while training instead of hand-engineering prefix and suffix information about words. Learn-ing character-level embeddings has the advantage of learning representations specific to the task and do-main at hand. They have been found useful for mor-phologically rich languages and to handle the out-of-vocabulary problem for tasks like part-of-speech tagging and language modeling (Ling et al., 2015b) or dependency parsing (Ballesteros et al., 2015).
Figure 4 describes our architecture to generate a word embedding for a word from its characters. A character lookup table initialized at random contains an embedding for every character. The character embeddings corresponding to every character in a word are given in direct and reverse order to a for-ward and a backward LSTM. The embedding for a word derived from its characters is the concatenation of its forward and backward representations from the bidirectional LSTM. This character-level repre-sentation is then concatenated with a word-level rep-resentation from a word lookup-table. During test-ing, words that do not have an embedding in the lookup table are mapped to a UNK embedding. To train the UNK embedding, we replace singletons with the UNK embedding with a probability 0 . 5 . In all our experiments, the hidden dimension of the for-ward and backward character LSTMs are 25 each, which results in our character-based representation of words being of dimension 50 .

Recurrent models like RNNs and LSTMs are ca-pable of encoding very long sequences, however, they have a representation biased towards their most recent inputs. As a result, we expect the final rep-resentation of the forward LSTM to be an accurate representation of the suffix of the word, and the fi-nal state of the backward LSTM to be a better rep-resentation of its prefix. Alternative approaches X  most notably like convolutional networks X  X ave been proposed to learn representations of words from their characters (Zhang et al., 2015; Kim et al., 2015). However, convnets are designed to discover position-invariant features of their inputs. While this is appropriate for many problems, e.g., image recog-nition (a cat can appear anywhere in a picture), we argue that important information is position depen-dent (e.g., prefixes and suffixes encode different in-formation than stems), making LSTMs an a priori better function class for modeling the relationship between words and their characters. 4.2 Pretrained embeddings As in Collobert et al. (2011), we use pretrained word embeddings to initialize our lookup table. We observe significant improvements using pretrained word embeddings over randomly initialized ones. Embeddings are pretrained using skip-n-gram (Ling et al., 2015a), a variation of word2vec (Mikolov et al., 2013a) that accounts for word order. These em-beddings are fine-tuned during training.

Word embeddings for Spanish, Dutch, German and English are trained using the Spanish Gigaword version 3, the Leipzig corpora collection, the Ger-man monolingual training data from the 2010 Ma-chine Translation Workshop and the English Giga-word version 4 (with the LA Times and NY Times ding dimension of 100 for English, 64 for other lan-guages, a minimum word frequency cutoff of 4 , and a window size of 8 . 4.3 Dropout training Initial experiments showed that character-level em-beddings did not improve our overall performance when used in conjunction with pretrained word rep-resentations. To encourage the model to depend on both representations, we use dropout training (Hin-ton et al., 2012), applying a dropout mask to the final embedding layer just before the input to the bidirec-tional LSTM in Figure 1. We observe a significant improvement in our model X  X  performance after us-ing dropout (see table 5). This section presents the methods we use to train our models, the results we obtained on various tasks and the impact of our networks X  configuration on model performance. 5.1 Training For both models presented, we train our networks using the back-propagation algorithm updating our parameters on every training example, one at a time, using stochastic gradient descent (SGD) with a learning rate of 0 . 01 and a gradient clipping of 5 . 0 . Several methods have been proposed to enhance the performance of SGD, such as Adadelta (Zeiler, 2012) or Adam (Kingma and Ba, 2014). Although we observe faster convergence using these methods, none of them perform as well as SGD with gradient clipping.

Our LSTM-CRF model uses a single layer for the forward and backward LSTMs whose dimen-sions are set to 100 . Tuning this dimension did not significantly impact model performance. We set the dropout rate to 0 . 5 . Using higher rates nega-tively impacted our results, while smaller rates led to longer training time.

The stack-LSTM model uses two layers each of dimension 100 for each stack. The embeddings of the actions used in the composition functions have 16 dimensions each, and the output embedding is of dimension 20 . We experimented with different dropout rates and reported the scores using the best that apply locally optimal actions until the entire sentence is processed, further improvements might be obtained with beam search (Zhang and Clark, 2011) or training with exploration (Ballesteros et al., 2016). 5.2 Data Sets We test our model on different datasets for named entity recognition. To demonstrate our model X  X  ability to generalize to different languages, we present results on the CoNLL-2002 and CoNLL-2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) that contain in-dependent named entity labels for English, Span-ish, German and Dutch. All datasets contain four different types of named entities: locations, per-sons, organizations, and miscellaneous entities that do not belong in any of the three previous cate-gories. Although POS tags were made available for all datasets, we did not include them in our models. We did not perform any dataset preprocessing, apart from replacing every digit with a zero in the English NER dataset. 5.3 Results Table 1 presents our comparisons with other mod-els for named entity recognition in English. To make the comparison between our model and oth-ers fair, we report the scores of other models with and without the use of external labeled data such as gazetteers and knowledge bases. Our models do not use gazetteers or any external labeled resources. The best score reported on this task is by Luo et al. (2015). They obtained a F 1 of 91.2 by jointly model-ing the NER and entity linking tasks (Hoffart et al., 2011). Their model uses a lot of hand-engineered features including spelling features, WordNet clus-ters, Brown clusters, POS tags, chunks tags, as well as stemming and external knowledge bases like Freebase and Wikipedia. Our LSTM-CRF model outperforms all other systems, including the ones us-ing external labeled data like gazetteers. Our Stack-LSTM model also outperforms all previous models that do not incorporate external features, apart from the one presented by Chiu and Nichols (2015). Tables 2, 3 and 4 present our results on NER for German, Dutch and Spanish respectively in compar-ison to other models. On these three languages, the LSTM-CRF model significantly outperforms all pre-vious methods, including the ones using external la-beled data. The only exception is Dutch, where the model of Gillick et al. (2015) can perform better by leveraging the information from other NER datasets. The Stack-LSTM also consistently presents state-the-art (or close to) results compared to systems that do not use external data.

As we can see in the tables, the Stack-LSTM model is more dependent on character-based repre-sentations to achieve competitive performance; we hypothesize that the LSTM-CRF model requires less orthographic information since it gets more contex-tual information out of the bidirectional LSTMs; however, the Stack-LSTM model consumes the words one by one and it just relies on the word rep-resentations when it chunks words. 5.4 Network architectures Our models had several components that we could tweak to understand their impact on the overall per-formance. We explored the impact that the CRF, the character-level representations, pretraining of our word embeddings and dropout had on our LSTM-CRF model. We observed that pretraining our word embeddings gave us the biggest improvement in overall performance of +7 . 31 in F 1 . The CRF layer gave us an increase of +1 . 79 , while using dropout resulted in a difference of +1 . 17 and finally learn-ing character-level word embeddings resulted in an increase of about +0 . 74 . For the Stack-LSTM we performed a similar set of experiments. Results with different architectures are given in table 5. In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL-2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later im-proved on this with a neural network by doing unsu-pervised learning on a massive unlabeled corpus.
Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word em-beddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM be-ing replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Zhou and Xu (2015) also used a similar model and adapted it to the semantic role labeling task. Lin and Wu (2009) used a linear chain CRF with L 2 regular-ization, they added phrase cluster features extracted from the web data and spelling features. Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers.

Language independent NER models like ours have also been proposed in the past. Cucerzan and Yarowsky (1999; 2002) present semi-supervised bootstrapping algorithms for named entity recogni-tion by co-training character-level (word-internal) and token-level (context) features. Eisenstein et al. (2011) use Bayesian nonparametrics to construct a database of named entities in an almost unsu-pervised setting. Ratinov and Roth (2009) quanti-tatively compare several approaches for NER and build their own supervised model using a regular-ized average perceptron and aggregating context in-formation.

Finally, there is currently a lot of interest in mod-els for NER that use letter-based representations. Gillick et al. (2015) model the task of sequence-labeling as a sequence to sequence learning prob-lem and incorporate character-based representations into their encoder model. Chiu and Nichols (2015) employ an architecture similar to ours, but instead use CNNs to learn character-level features, in a way similar to the work by Santos and Guimar  X  aes (2015). This paper presents two neural architectures for se-quence labeling that provide the best NER results ever reported in standard evaluation settings, even compared with models that use external resources, such as gazetteers.

A key aspect of our models are that they model output label dependencies, either via a simple CRF architecture, or using a transition-based algorithm to explicitly construct and label chunks of the in-put. Word representations are also crucially impor-tant for success: we use both pre-trained word rep-resentations and  X  X haracter-based X  representations that capture morphological and orthographic infor-mation. To prevent the learner from depending too heavily on one representation class, dropout is used. This work was sponsored in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under Contract No. HR0011-15-C-0114. Miguel Balles-teros is supported by the European Commission un-der the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA).

