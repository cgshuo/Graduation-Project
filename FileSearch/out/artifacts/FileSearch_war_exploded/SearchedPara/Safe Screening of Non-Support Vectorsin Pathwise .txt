 Kohei Ogawa ogawa.mllab.nit@gmail.com Yoshiki Suzuki suzuki.mllab.nit@gmail.com Ichiro Takeuchi takeuchi.ichiro@nitech.ac.jp Nagoya Institute of Technology, Nagoya, Japan The support vector machine (SVM) is one of the most successful classification algorithms. An advantage of the SVM is that, once we identify non-support vectors (non-SVs) that do not have any influence on the classi-fier, they can be thrown away in the future test phase. In this paper, we show that, in a certain scenario, some of the non-SVs can be screened out and they can be thrown away prior to the training phase, which often leads to substantial reduction in the training cost. The main contribution of this paper is to introduce an idea of recently proposed safe screening rule for the purpose of identifying non-SVs in SVM prior to train-ing phase. The safe screening rule was first introduced in Ghaoui et al. (2010) in the context of L 1 sparse reg-ularization, and some extensions have been recently reported (Xiang et al., 2012; Xiang &amp; Ramadge, 2012; Wang et al., 2012; Dai &amp; Pelckmans, 2012). Those rules allow us to safely identify the features whose coef-ficients would be zero at the optimal solution before ac-tually solving it 1 . Our contribution is in non-trivially adapting the idea of safe feature screening to non-SV screening in SVM. To the best of our knowledge, there are no other screening rules that can  X  X afely X  screen-out non-SVs of standard kernel-based SVM before ac-tually training it.
 In this paper, we argue that the advantage of our non-SV screening rule can be fully exploited in a pathwise computation scenario where a sequence (or path) of SVM classifiers for different regularization parameters must be trained. Since pathwise computation is in-dispensable for model selection, our approach, that can often substantially reduce the total cost, would be practically useful. Suppose we have a training set D N := { ( x i ; y i ) } i where x i  X  X   X  R d , y i  X  X  X  1 ; 1 } and N := { 1 ; : : : ; n We consider classifiers in the form 2 : where w is a vector in a feature space that is often defined implicitly by a kernel function K : X X X  X  R . Our task is to train a support vector machine (SVM) by solving the following optimization problem where [ z ] + indicates the positive part of z . Here, C &gt; 0 is a regularization parameter that controls the balance between the regularization term and the loss term. The dual of (2) is the following maximization problem where is an n -dimensional vector of Lagrange mul-tipliers, and Q is an n  X  n matrix whose element is defined as Q ij := y i y j K ( x i ; x j ). With the Lagrange multipliers, the classifier (1) is rewritten as If we categorize the n training instances into the optimality conditions of the problems (2) or (3) are summarized as where E stands for the points at the E lbow of the hinge loss function, while R and L are for R ight and L eft of the elbow.
 The optimality condition (6a) suggests that, if some of the training instances are known to be the members of R in advance, we can throw away those instances prior to the training stage. Similarly, if we know that some instances are the members of L , we can fix the corresponding i = C at the following training phase. Namely, if some knowledge on these three index sets are known a priori , our training task would be ex-tremely simple and easy. In fact, existing SVM solvers usually spend most of their computational resources for identifying these index sets (Joachims, 1999; Platt, 1999; Cauwenberghs &amp; Poggio, 2001; Hastie et al., 2004; Fan et al., 2008; Chang &amp; Lin, 2011). The instances in R are often called non-support vec-tors (non-SVs) because they have no influence on the resulting classifier. In this paper, we show that, in a certain situation, some of the non-SVs and some of the instances in L can be screened out prior to the train-ing stage. In what follows, we call the screening rule as non-SV screening rule . Note, however, that this ter-minology refers to the rule for screening out not only non-SVs (the instances in R ) but also those in L . In order to derive a non-SV screening rule, let us write the primal SVM problem (2) as where s &gt; 0 is an upper bound of the sum of the hinge loss. The formulations in (2) and (7) are equivalent in the sense that there is always a correspondence be-tween the regularization parameter C and the upper bound s (see section 4). We denote the optimal solu-tion of (7) at s as w ( s ) and the optimal classifier at s as f ( x | s ) := w ( s ) &gt; x .
 Properties of the optimal solutions Let us con-sider two upper bounds s a &gt; s b . Then, it is easy to see that because the constraint in the latter problem is uni-formly tighter than the former. The relation (8) can be rewritten with  X  J ( w ) | w = w ( s at w = w ( s a ). Since (8) indicates that the vector w ( s b )  X  w ( s a ) must be an ascent direction of J ( w ), we have Next, let  X  w ( s b ) be an arbitrary primal feasible solution to (7) at s = s b . Then, it is clear that
J ( w ( s b ))  X  J (  X  w ( s b ))  X  || w ( s b ) || 2  X || because w ( s b ) minimizes J ( w ) under the constraint H ( w )  X  s b .
 In fact, the relations (9) and (10) can be applied not only to w ( s b ), but also to the entire range of the op-timal solutions for all s  X  [ s b ; s a ]. This simple fact is summarized in the following lemma: Lemma 1 Consider two positive scalars s a &gt; s b , and assume that there exists at least a (primal) feasible so-lutions of (7) at s = s b . Let w ( s a ) be the optimal solution of the problem (7) at s = s a , and  X  w ( s b ) be an arbitrary feasible solution of (7) at s = s b , i.e., H (  X  w ( s b ))  X  s b . Then, and are satis ed for all s  X  [ s b ; s a ] .
 We omit the proof of this lemma because it is clear from the discussion in (9) and (10). Figure 1 provides a geometric interpretation of Lemma 1.
 Basic idea According to the optimality conditions (5) and (6), the i th training instance is categorized into either of R , E or L based on the value of y i f ( x i ). The basic idea for constructing our non-SV screening rule is to bound y i f ( x i ) by using the properties discussed in Lemma 1. Namely, we consider a subset of the solu-tion space  X  [ s w ( s ) for any s  X  [ s b ; s a ] is guaranteed to exist, and compute the minimum and the maximum values of y f ( x i ) within the set  X  [ s We can ensure that i th training instance would be in R (i.e, non-SV) if and, it would be in L if for the entire range optimal solutions at s  X  [ s b ; s a the idea behind this.
 Computing the bounds A nice thing about (13) and (14) is that the solutions of these minimization and maximization problems can be explicitly com-puted. We obtain the following screening rule: Theorem 2 Consider two scalars s a &lt; s b , and let w ( s a ) and  X  w ( s b ) be the optimal solution at s = s a feasible solution at s = s b , respectively, as Lemma 1. Also, let us write for notational simplicity. Then, for all s  X  [ s b ; s a ] , y i f ( x i | s a ) &gt; 1 and ` i &gt; 1  X  i  X  X   X  i = 0 ; where ` i := y i f ( x i | s a ) Similarly, where u i := Algorithm 1 SafeScreening 1: Input: D N = { ( x i ; y i ) } i 2 N , w ( s a ),  X  w ( s 2: Output:  X  L ,  X  R ,  X  Z ; 3: Initialize:  X  L  X   X  ,  X  R  X   X  ,  X  Z  X  N and i  X  1; 4: while i  X  n do 5: if ( x i ; y i ) satisfies the conditions in (15) then 6:  X  R  X   X  R X  X  i } ,  X  Z  X   X  Z \{ i } ; 7: end if 8: if ( x i ; y i ) satisfies the condition in (17) then 9:  X  L  X   X  L X  X  i } ,  X  Z  X   X  Z \{ i } ; 10: end if 11: i  X  i + 1; 12: end while The proof of Theorem 2 is given in the supplementary, where we just solved the minimization and the max-imization problems (13) and (14) by using standard Lagrange multiplier method. The main cost for eval-uating the rule is in the computation of f ( x i | s a ) = ever, in pathwise computation scenario that we discuss in section 4, it is likely that this value would have been already computed in earlier steps and stored in cache. In this case, the rule can be evaluated with O (1) cost. From (16), we see that ` i is not larger than y i f ( x i It suggests that the screening rule (15) should be applied only to the instances with y i f ( x i | s a ) &gt; 1. On the other hand, we might get a chance to have y f ( x i | s ) &lt; 1 for some i and s  X  [ s b ; s a ] even if y f ( x i | s a )  X  1 at s a . Thus, the rule (17) is also ap-plied to such an instance.
 Algorithm 1 describes the non-SV screening rule. Here, {  X  L ;  X  R ;  X  Z} are the output of the rule. Each of them represents the subset of the training instances guaranteed to be in L (i.e., i = C ), those guaranteed to be in R (i.e., i = 0), and those we are not sure where it belongs to, respectively. When we compute the solution at an s  X  [ s b ; s a ], we can only work with the subset of the training instances D ~ Z (note that, the variable i ; i  X   X  L can be fixed to C during the opti-mization process).
 Kernelization It is easy to note that the lower bound ` i and the upper bound u i can be computed only through kernels. The dual of (7) has similar form as (3) (see (20) later), and f ( x ) is written as in (4) using the Lagrange multipliers  X  R n . Not-and (18) can be evaluated by using the kernel K and the dual variables . In this case, the computation of y for a vector v represents the i th element, and ( s a ) is the optimal Lagrange multipliers at s a . In pathwise computation scenario, this value has been often com-puted in earlier steps. Then, the rule can be evaluated with O (1) cost for each instance.
 Interpretation The non-SV screening rule can be interpreted in the following way. The lower bound ` i in (16) indicates that y f ( x i | s a )  X  y i f ( x i | s )  X  i.e., the difference of y i f ( x i ) values between s a and s are bounded by the quantity in the r.h.s. Roughly speaking, (  X  ) is the relative difference of the objec-tive function values between s a and s b , while (  X  ) rep-resents how close the i th instance is to the decision boundary at s a (note that, the quantity in (  X  ) is max-imized when cos( x i ; w ( s a )) = 0 and minimized when cos( x i ; w ( s a )) =  X  1 if the sizes of the two vectors are fixed). It suggests that, if there is no much difference between two objective function values and the i th in-stance is away from the decision boundary at s a then y f ( x i | s ) would not be so different from y i f ( x i | i th instance is likely to be screened out as a non-SV. For evaluating a non-SV screening rule, we need to know two things: the optimal solution at a smaller C , and a feasible solution at a larger C (note that a small s in (7) corresponds to a large C in (2), and vice-versa). If we do not have these two, they must be computed prior to the rule evaluation. Then, the computational gain brought by the screening rule might be small or even vanish with these additional costs.
 However, in a situation that a series of pathwise op-timal solutions w.r.t. the regularization parameter C must be computed, we can exploit the advantage of the screening rule. In this section, we describe how we can efficiently compute pathwise solutions by us-ing the non-SV screening rule we have developed in the previous section. Computing pathwise solutions is practically important because model selection (e.g., by cross-validation) is indispensable for selecting a classi-fier that generalizes well.
 Regularization Path These pathwise solutions are often referred to as regularization path s. In the SVM, the regularization path was first studied in (Hastie et al., 2004), where parametric programming (Allgo-wer &amp; George, 1993; Gal, 1995; Best, 1996; Takeuchi et al., 2009; Karasuyama et al., 2012) is introduced for computing the exact path of the SVM solutions. This algorithm is quite efficient for small or medium-size problems. For larger problems, approximate path obtained by computing a fine grid sequence of path-wise solutions can be used as an alternative (Friedman et al., 2007; 2010; Yuan et al., 2010; 2012). In what follows, we present an algorithm for comput-ing a sequence of the optimal solutions of (3) at T that will be explained later, we first solve two opti-mization problems at the smallest C (1) (the largest s (1) ) and at the largest C ( T ) (the smallest s ( T ) ). Al-though these costs might be rather expensive, once we obtain these two optimal solutions, the rest of the pathwise solutions at C (2) ; : : : ; C ( T 1) can be fairly cheaply computed by exploiting the advantage of the non-SV screening rule. In the algorithm, many non-SV screening rules are made, and each rule is used for computing a sub-sequence of the pathwise solutions. Let M  X  2 be the length of each sub-sequence. Then, roughly speaking, each non-SV screening rule covers the sub-sequence of the solutions in the following way: The details of the algorithm are described in Algo-rithms 2 at the end of this section.
 Our algorithm is a meta -algorithm in the sense that any SVM solvers that can solve the dual optimization problem (3) can be used as a component.
 Relation between C -form and s -form Before presenting how to construct the rule for each sub-sequence, we need to clarify the relation between the two formulations (2) and (7) because the algorithm uses both formulations interchangeably. We call the former as C -form, and the latter as s -form. First, consider a situation that we have the optimal solution of C -form at a certain C . Then, the corre-sponding s in the s -form is simply computed as where f (  X | C ) is the optimal classifier at C . On the other hand, when we have an s -form problem at a certain s , the dual of (7) is written as max and the corresponding C in C -form is obtained by solv-ing this dual problem.
 When we interchangeably use these two formulations in the algorithm, we use the following simple fact: Lemma 3 For C a &lt; C b in C -form, the corresponding s a and s b in s -form satisfy s a s a &gt; s b in s -form, the corresponding C a and C b satisfy C The proof is in the supplementary.
 Using the correspondence between C and s , we de-note the sequence of s corresponding to C (1) &lt; C (2) &lt; In addition, with a slight abuse of notation, we write w ( C ) and w ( s ) to represent the optimal solution at C in C -form, and at s in s -form, respectively. 4.1. Constructing a non-SV screening rule Consider a situation that the pathwise solutions at C  X  C ( t ) have already been computed. Our next task is to construct a non-SV screening rule that is used for computing the solutions at C ( t +1) ; : : : ; C ( t + M ) to know the optimal solution at an s a  X  s ( t +1) and a feasible solution at an s b  X  s ( t + M ) . For the former, we can just set s a = s ( t ) and use the optimal solution at s ( t ) because it has already been computed.
 For the later, we can set s b = s ( T ) since s ( T )  X  s (see Lemma 4). However, if s ( T ) is much smaller than s ( t + M ) , then the rule would not be able to screen out many non-SVs. In order to make a better rule, we need to find a tighter s b  X  s ( t + M ) . Unfortunately, this is a non-trivial challenging task from the following two aspects. First, we do not know s ( t + M ) since it is avail-able only after finding the optimal solution at C ( t + M ) Second, even if we could have an s b  X  s ( t + M ) , finding a feasible solution at the s b is often as difficult as find-ing the optimal solution. We address the first issue in a trial-and-error manner, and its detail is discussed in section 4.2 and supplementary. In the sequel, we describe how to handle the second issue.
 Computing a feasible solution To address the difficulty in finding a feasible solution at s b , we first compute the optimal solution w ( s ( T ) ) at the largest C = C ( T ) (corresponding to the smallest s = s ( T ) ) us-ing any algorithms that suits to this task. Although this cost would be rather expensive, once we have w ( s ( T ) ), we can utilize it for finding all the other feasible solutions in later steps in a trivial amount of computation.
 Consider a case that we make a rule defined on [ s b ; s a Given w ( s a ) and w ( s ( T ) ), we can easily find a feasible solution at s b by solving the following one-dimensional convex optimization problem: This one-dimensional convex problem can be easily solved by a line-search algorithm such as bisection method. Note that we can always find a feasible v  X  [0 ; 1] for any s set H ( w )  X  s b is convex. This line-search can be ker-nelized since both the objective function values and the constraints can be evaluated only through kernel computations. Figure 3 depicts how to find a primal feasible solution.
 4.2. Pathwise computation algorithm Algorithm 2 describes how we compute pathwise SVM solutions with the help of non-SV screening rules. In this algorithm, we first compute two SVM solutions w ( C (1) ) and w ( C ( T ) ) using the entire training set. Once we obtain these two optimal solutions, the rest of the pathwise solutions at C (2) ; : : : ; C ( T 1) can be computed efficiently because non-SV screening rules can reduce the training set size. The algorithm is a bit complicated because we need to use C -form and s -form interchangeably. Due to space limitation, FindSb and ComputeSubPath functions in Algorithm 2 and some additional information about the algorithm is provided in the supplementary.
 Algorithm 2 SafeScreeningPath (see section 4.2) 3: Initialization: 5: Compute s (1) by (19); 7: Compute s ( T ) by (19); 8: t  X  1; 9: while t &lt; T  X  M do 12: {  X  L ;  X  R ;  X  Z}  X  Screen ( D N ; w ( s a ) ; w ( s 14: t  X  t + M ; 15: end while In this section, we report experimental results. Toy example First, we applied our non-SV screen-ing to a simple 2-dimensional toy data set plot-ted in Figure 4. In each class, 300 data points were generated from N ( { 1 : 5 ; 1 : 5 } &gt; ; 0 : 75 identity matrix. We constructed a non-SV screening rule for the range of C  X  [1 ; 10]. Among 600 data points, the rule screened out 530 points. Among them, only one instance was screened out as a member of  X  L , and the rest were screened out as  X  R (non-SVs). It means that, if we want to train an SVM for any C  X  [1 ; 10], we have only to solve the optimization problem with the remaining 70 data points. Figure 4 shows which data points are screened out. Interest-ingly, some data points close to the boundaries were also screened out, while some points distant away from the boundaries are kept remained.
 Pathwise computation cost We investigate the performance of our non-SV screening in the pathwise computation scenario discussed in section 4. Here, we only report the experiments with linear kernel. As the plug-in SVM solver, we used a state-of-the-art linear SVM solver LIBLINEAR (Hsieh et al., 2008).
 We applied the pathwise computation algorithm to four data sets as described in Table 1. For each data set, our task is to compute the SVMs for 100 different C  X  [10 2 ; 10 2 ] evenly distributed in logarithmic scale. As explained in the previous section, we set M = 5, where M is the number of solutions each screening rule covers. It means that, we constructed 20 screen-ing rules for each data set.
 We compared our non-SV screening with  X  X aive X  and  X  X euristic screening X  approaches. In the naive ap-proach, we just computed each pathwise SVM solu-tions one by one. In the heuristic screening approach, we predicted the active sets R and L as those at the previously computed SVM solution with smaller C , and their corresponding i were fixed as 0 and C , re-spectively. If some of those predictions were found to be wrong (by cheking the optimality condition), we trained the SVM again after adding those violating instnaces to the training set, and this process was re-peated until all the instances satisfy the optimality. Table 1 summarizes the computation time for obtain-ing the pathwise solutions on these four data sets. Figures 5  X  8 show the screening results of the four data sets in Table 1. In each figure, the top plot in-dicates the rate of screened out instances at each C . Since we made 20 rules in this experiment, there are 20 segments in these plots. The bottom plot indicates the computation time spent for computing an SVM solution at each C . The blue lines and dots indicate the time when we solve an SVM with the entire train-ing set in the naive approach. The green lines and dots indicate the total computation time of the heuris-tic screening. The red lines and points indicate SVM training time for reduced training set after applying our non-SV screening rule. In these, red circles are plotted at every five C s (since M = 5). These points correspond to the cost at line 8 of Algorithm 3 (see supplementary). The costs at these points are usually higher than others because we used a rough estimate of s b when making the rule for this solution. From Table 1 and Figures 5  X  8, we see that our non-SV screening rule could substantially reduce the total pathwise computation cost. Compared with the naive approach, our non-SV screening rule reduced the to-tal cost by 65%, 36%, 32%, and 41% in each data set. Note that the cost for finding feasible solutions  X  X ine Search X  and that for constructing rules  X  X ule X  are neg-ligible. In three of the four data sets, the costs of the heuristic screening rule were larger than those of our non-SV screening rule. Note that, in PCMAC data set, it was even worse than the naive approach. This is because the heuristic screening rule is NOT safe, meaning that, when it makes false positive screenings, one needs to repeat the training after correcting those mistakes ( X  X asted X  in Table 1 indicates the training cost with false positive screenings). Overall, the ex-periments illustrate that our non-SV screening rule is useful in pathwise computation scenario. In this paper, we introduced the idea of safe screen-ing rule for the task of identifying non-SVs prior to SVM training. We show that the advantage of non-SV screening rule can be fully exploited in pathwise computation scenario. Some experiments illustrate ad-vantages of our approach.
 In our experiments, our non-SV screening rule could not screen-out many instances when we used Gaussian kernel. It would be important to clarify what types of kernel could be used with our non-SV screening rule. In addition, we would also need to consider how we can extend the current non-SV screening rules when the decision function has constant bias term. We thank Kohei Hatano for his insightful comments and suggestions. IT was supported in part by MEXT KAKENHI 23700165 and the Hori sciences and arts foundation.
 Allgower, E. L. and George, K. Continuation and path following. Acta Numerica , 2:1 X 63, 1993.
 Best, M. J. An algorithm for the solution of the parametric quadratic programming problem. Ap-plied Mathemetics and Parallel Computing , pp. 57 X  76, 1996.
 Boyd, S. and Vandenberghe, L. Convex Optimization . Cambridge University Press, 2004.
 Cauwenberghs, G. and Poggio, T. Incremental and decremental support vector machine learning. Ad-vances in Neural Information Processing Systems , 13, 2001.
 Chang, C. C. and Lin, C. J. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology , 2:27:1 X 27:27, 2011.
 Dai, L. and Pelckmans, K. An ellipsoid based two-stage sreening test for bpdn. In Proceedings of the 20th European Signal Processing Conference , 2012. Fan, J. and Lv, J. Sure independence screening for ultrahigh dimensional feature space. Journal of The Royal Statistical Society B , 70:849 X 911, 2008. Fan, R. R., Chang, K. W., Hsieh, C. J., Wang, X. R., and Lin, C. J. Liblinear: A library for large lin-ear classification. Journal of Machine Learning Re-search , 9:1871 X 1874, 2008.
 Friedman, J., Hastie, T., Ho X  X ling, H., and Tibshirani,
R. Pathwise coordinate optimization. 1(2):302 X 332, 2007.
 Friedman, J., Hastie, T., and Tibshirani, R. Regular-ization paths for generalized linear models via coor-dinate descent. Journal of Statistical Software , 33 (1):1 X 22, 2010.
 Gal, T. Postoptimal Analysis, Parametric Program-ming, and Related Topics . Walter de Gruyter, 1995. Ghaoui, L. E., Viallon, V., and Rabbani, T. Safe feature elimination in sparse supervised learning. eprint arXiv:1009.3515 , 2010.
 Hastie, T., Rosset, S., Tibshirani, R., and Zhu, J. The entire regularization path for the support vector ma-chine. Journal of Machine Learning Research , 5: 1391 X 415, 2004.
 Hsieh, C. J., Chang, K. W., and Lin, C. J. A dual co-ordinate descent method for large-scale linear svm. Proceedings of the 25th International Conference on Machine Learning , 2008.
 Joachims, T. Making large-scale svm learning practi-cal. In Scholkopf, B., Burges, C. J. C., and Smola, A. J. (eds.), Advances in Kernel Methods -Support Vector Learning , pp. 169 X 184. MIT Press, 1999. Karasuyama, M., Harada, N., Sugiyama, M., and
Takeuchi, I. Multi-parametric solution-path al-gorithm for instance-weighted support vector ma-chines. Machine Learning , 88(3):297 X 330, 2012. Platt, J. Fast training of support vector machines us-ing sequential minimal optimization. In Scholkopf,
B., Burges, C. J. C., and Smola, A. J. (eds.), Ad-vances in Kernel Methods -Support Vector Learn-ing , pp. 185 X 208. MIT Press, 1999.
 Takeuchi, I., Nomura, K., and Kanamori, T. Non-parametric conditional density estimation using piecewise-linear solution path of kernel quantile re-gression. Neural Computation , 21(2):539 X 559, 2009. Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Si-mon, N., Taylor, J., and Tibshirani, R. J. Strong rules for discarding predictors in lasso-type prob-lems. Journal of the Royal Statistical Society, Series B , 74:245 X 266, 2011.
 Vats, D. Parameter-free high-dimensional screen-ing using multiple grouping of variables. eprint arXiv:1208.2043 , 2012.
 Wang, J., Lin, B., Gong, P., Wonka, P., and Ye, J.
Lasso screening rules via dual polytope projection. eprint arXiv:1211.3966 , 2012.
 Xiang, Z. J. and Ramadge, P. J. Fast lasso screen-ing test based on correlatins. In Proceedings of the 37th IEEE International Conference on Acoustics, Speech and Signal Processing , 2012.
 Xiang, Z. J., Xu, H., and Ramadge, P. J. Learning sparse representations of high dimensional data on large scale dictionaries. In Advances in Neural Infor-mation Processing Systems 24 , pp. 900 X 908. 2012. Yuan, G. X., Chang, K. W., Hsieh, C. J., and Lin,
C. J. A comparison of optimization methods and software for large-scale l1-regularized linear classifi-cation. Journal of Machine Learning Research , 11: 3183 X 3234, 2010.
 Yuan, G. X., Ho, C. H., and Lin, C. J. An improved glmnet for l1-regularized logistic regression. Journal
