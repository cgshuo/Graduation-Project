 1. Introduction
Given the demand of massive computing power in modern video game applications, GPUs (as well as the Cell/BE) are de-pose tasks. For some problems, such as Web information retrieval, the results obtained, in term of computational latency, gramming models like Map-Reduce. For example Bingsheng, Wenbin, Qiong, Naga, and Tuyong (2008) designed Mars, a Map-Reduce framework, on graphics processors, and Kruijf and Sankaralingam (2007) presented an implementation of Map-Reduce for the Cell architecture.

The main idea in modern processors like GPUs, Sony X  X  Cell/BE, and multi-core CPUs, is stuffing several computing cores the chip-area to cache mechanism and memory management, in general. Differently from standard CPUs, the majority of the chip-area in modern manycores is devoted to implement computational units. For this reason, they are able to perform spe-cialized computations over massive streams of data in a fraction of the time needed by traditional CPUs. The counterpart, support.

In this paper we digress from the motivation of sorting efficiently a large amount of data on modern GPUs to propose a  X  sorting network can be seen as a viable alternative for sorting large amounts of data on GPUs.
During our research we studied a new function to map bitonic sorting network (BSN) on GPU exploiting its high band-width memory interface. We also present this novel data partitioning schema that improves GPU exploitation and maxi-mizes the bandwidth with which the data is transferred between on-chip and off-chip memories. It is worth noticing that being an in-place sorting based on bitonic networks our solution uses less memory than non in-place ones (e.g. ( Cederman trieval (LSDS-IR).

To design our sorting algorithm in the stream programming model, we started from the popular BSN, and we extend it to in many applications. Examples are the divide-and-conquer strategy used in the computation of the Fast Fourier Transform multicasting network ( Al-Hajery &amp; Batcher, 1993 ).
 The main contributions of this paper are the following:
We perform a detailed experimental evaluation of state-of-the-art techniques on GPU sorting and we compare them on different datasets of different size and we show the benefits of adopting in-place sorting solutions on large datasets.
By using the performance constraints of a novel computational model we introduce in Capannini, Silvestri, and Baraglia (2010) , we design a method to improve the performance (both theoretical and empirical) of sorting using butterfly net-works (like bitonic sorting). Our theoretical evaluation, and the experiments conducted, show that following the guide-lines of the method proposed improve the performance of bitonic sorting also outperforming other algorithms.
This paper is organized as follows. Section 2 discusses related works. Section 3 introduces some relevant characteristics programming model and the single-instruction multiple-data (SIMD) architecture. Section 5 describes the new function to map BSN on GPU we propose. Sections 6 and 7 presents the results obtained in testing the different solutions on synthetic and real dataset. Section 8 presents the conclusions and discusses how to evolve in this research activity. 2. Related work but the hardware they use belongs to previous generations of GPUs, which does not offer the same level of programmability of the current ones.
 Since most sorting algorithms are memory-bound, it is still a challenge to design efficient sorting methods on GPUs.
Purcell, Donner, Cammarano, Jensen, and Hanrahan (2003) present an implementation of bitonic merge sort on GPUs structure providing an efficient search mechanism for GPU-based photon mapping. Comparator stages are entirely realized pute-bound rather than bandwidth-bound, and they achieve a throughput far below the theoretical optimum of the target architecture.
 merge sort. They present an improved bitonic sort routine that achieves a performance gain by minimizing both the number of instructions executed in the fragment program and the number of texture operations.

Gre X  and Zachmann (2006) present an approach to parallel sort on stream processing architectures based on an adaptive ware showing that they approach is competitive with common sequential sorting algorithms not only from a theoretical viewpoint, but also from a practical one. Good results are achieved by using efficient linear stream memory accesses, and by combining the optimal time approach with algorithms.

Govindaraju, Raghuvanshi, and Manocha (2005) implement sorting as the main computational component for histogram approximation. This solution is based on the periodic balanced sorting network method by Dowd, Perl, Rudolph, and Saks (1989) . In order to achieve high computational performance on the GPUs, they used a sorting network based algorithm, databases using a GPU. This approach uses the data and task parallelism to perform memory-intensive and compute-inten-sive tasks on GPU, while the CPU is used to perform I/O and resource management.

Cederman and Tsigas (2008) show that GPU X  X uicksort is a viable sorting alternative. The algorithm recursively partition can be sorted independently by each thread block. The conducted experimental evaluation point out the superiority of GPU X  Quicksort over other GPU-based sorting algorithms.
 Recently, Sengupta et al. (2007) present a Radixsort and a Quicksort implementation based on segmented scan primitives. itives are an excellent match for a broad set of problems on parallel hardware. 3. Application to data indexing exploit the computational power of novel computing architectures to keep up with the exponential growth in Web content. where indexing of a large amount of data has to be performed with limited resources. A sort-based approach first makes term and document frequency). For small collections, all this can be done in memory. When memory is not sufficient, we is the minimization of the number of random disk seeks during sorting. A possible approach is Blocked Sort-Based Indexing tions of any size as long as there is enough disk space available. The algorithm parses documents and turns them into a stream of term X  X ocID pairs, called tokens. Tokens are then processed one by one. For each token, SPIMI adds a posting di-ings lists were written in unsorted order, merging blocks could not be accomplished by a simple linear scan through each the final merging step ( Manning et al., 2008 ). 4. Key aspects of manycore program design
GPUs have been originally designed to execute geometric transformations that generate a data stream of pixels to be displayed.

In general, a program is processed by a GPU by taking as input a stream of data to be distributed among different threads them back to the main memory. 4.1. The SIMD architecture
SIMD machines, also knows as processor-array machines, basically consists of an array of execution units (EUs) connected some classes of problems. 4.2. The stream programming model kernels. A stream is a sequence of homogeneous data elements, that are defined by a regular access pattern. A kernel typ-ically loops through all the input stream elements, performing a sequence of operations on each element, and appending results to an output stream.

These operations should usually be arranged in a way to increase the amount of parallel instructions executed by the operation itself. Moreover, they should not access arbitrary memory locations. To avoid expensive concurrent memory ac-ment of the input stream can be processed simultaneously allowing kernels to exhibit a large amount of data parallelism.
Task parallelism, instead, can be obtained by allowing kernels to simultaneously access elements from the stream, this can only be accomplished if elements are opportunely arranged in main memory to disallow concurrent accesses to overlap-ping memory areas. Furthermore, other important features shared by all efficient stream-based applications are: elements are read and computed once from memory, and applications perform a high number of arithmetic operations per memory reference, i.e. applications are compute-intensive. 4.3. K-model: a many-core stream-based computational model
K -model has been designed to model all the main peculiarities of the novel generation of stream multiprocessors ( Capan-liarities we need in order to understand, discuss, and compare our approaches.

K -model consists of a computer with an array of k scalar execution units linked to a single instruction unit. The memory hierarchy consists of an external memory and a local memory made of a set of private registers, and a shared memory of r locations equally divided into k parallel modules.
 multiplying the complexity of a stream element by the total number of elements in a stream.
 instruction corresponds to the highest number of requests involving one of the k memory banks. Regarding arithmetic instructions, their latency has unitary cost.

The computational complexity is defined as the classical sequential complexity assuming we are simulating the execution rithm is efficient.

To evaluate the number of memory transactions , we need to take into account the data-transfers from/to the off-chip mem-5. K -Model-based bitonic sorting network
A sorting network is a mathematical model of a sorting algorithm that is made up of a network of wires and comparator elements in the stream programming fashion, as K -model requires.

In particular, BSN is based on repeatedly merging two bitonic sequences sequences. Fig. 1 shows graphically the various stages described above.
 16 BSN.

Algorithm 1. BitonicSort ( A ) 1: n j A j 2: for s =1 to log n do 3: for c = s 1 to 0 step 1 do 4: for r =0 to n 1 do 5: If r 2 c r 2 s  X  mod 2  X ^ A  X  r &gt; A  X  r 2 c then SWAP  X 
To design our sorting algorithm in the stream programming model, we start from the original parallel BSN formulation efficient schema for mapping items into stream elements. Such mapping should be done in order to perform all the compar-isons involved in the BSN within a kernel. The structure of the network, and the constraint of the programming model, in-order, the stream programming model requires the stream elements to be  X  X  X ndependently computable X  X . In other words, each is fixed by the r parameter, i.e. the amount of memory available for each stream element.

In our solution we define different partition depending on which step of the BSN we are. Each partitioning induces a dif-the stream processor.

Since each kernel invocation implies a communication phase, such mapping should be done in order to reduce the com-munication overhead. Specifically, this overhead is generated whenever a processor begins or ends the execution of a new up to one order of magnitude when measured on the real architecture.

Resuming, in order to maintain the communication overhead as small as possible, our goals are: (i) to minimize the num-ber of communications between the on-chip memory and the off-chip one, (ii) to maximize the bandwidth with which such communications are done. Interestingly, the sequential version of the bitonic network algorithm exposes a pattern made up two goals above described.

Let us describe how a generic bitonic network sorting designed for an array A of n =2 in K -model.

In order to avoid any synchronization, we segment the n items in such a way each part contains all the items to perform porarily stored in the on-chip memory, the number of items per part is bounded by the size of such memory. In the follow, we show the relation between the number of items per part, and the number of steps each kernel can perform. This relation emerges from the analysis of Algorithm 1 .

Briefly, to know how many steps can be included in the run of a partition, we have to count how many distinct values the
Furthermore, let c and s be the variables specified in Algorithm 1 , the notation step the variable c .
 the cth bit.
 representation. h The claim above gives a condition on the elements of the array A involved in each comparison of a step. Given an element form the comparisons done within a generic number of consecutive steps, namely k , called k-superstep . straightforwardly follows from Algorithm 1 , and it is divided in two cases, specifically for k
Definition 1. ( C -sequence ) Within the k -superstep starting at step Algorithm 1 flips when it performs the comparisons is defined as follows: worth being noted that the values assigned to c in the k steps are distinct because of the initial condition k the comparisons within the 1 2 k  X  k  X  1  X  steps starting from step definition of the C -sequence, we can retrieve the following claim.
 and only if " i R C r [i] =q [i] , where the notation r [i] From the previous claims, we can also retrieve the size of each partition as function of C .
 Lemma 1. Each part is composed by 2 j C j items.
 forms a new pair to compare, the number of items to include in the part doubles, namely it is 2 2 From the above lemma, and because each partition covers all the elements of A , it follows directly that
Corollary 1. The number of parts for covering all the comparisons in the superstep is 2 The previous claim can be extended to define the C -partition procedure.

Definition 3 ( C -partition ). Given a k -superstep, the relative C -partition is the set of parts P  X f p each part is constructed by means of Algorithm 2 .
Algorithm 2. BuildPartition ( A , n , k , C ) 1: / create a bit-mask corresponding to the fixed log n k bits whose positions are not in C / 2: j =0, m =0; 3: for b =0 to log n 1 do 4: if b R C then m [ b ] = i [ j ] , j = j +1; 5: / populate the partition using the bit-mask m defined in the previous step / 6: for e =0 to 2 k 1 do 7: j =0, r = m ; 8: for b =0 to d log n e 1 do 9: if b 2 C then r [ b ] = e [ i ] , i = i +1; 10: p i = p i [ A [ r ];
Now, let us make some consideration about the communication overhead discussed above. Each time we perform a them back. In order to minimize this overhead, we need to minimize the number of streams needed to cover all the network, i.e. to maximize the number of steps performed within each partition. Because each C -sequence is made up of 2 by balancing the contention the latency of the accesses is reduced because the maximum contention is lower.
The pseudo-code in Algorithm 3 discards some side aspects, to focus the main technique. In particular it takes a part ( A forms the comparisons between the argument elements and, if needed, swaps them. Note that, each divergent execution paths that are serialized by the (single) instruction unit of the processor.
Algorithm 3. RunStreamElement ( A p , C ) 1: for each id 2 [0, k 1] parallel do 2: n = log 2 ( r ) 3: for i =0 to n 1 do 4: c = C [ i ] 5: for j = id to n /2 1 step k do 6: p = InsAt ( j ,0, c ) 7: q = InsAt ( j ,1, c ) 8: Compare &amp; Swap ( A p [ p ], A p [ q ]) bandwidth is fully exploited when simultaneous memory accesses can be coalesced into a single memory transaction. This chip memory.
 the range from 0 to log k 1.

Let us analyze the k -coalesced condition in the K -model. By definition of C -sequence, when we fall into a C c &gt; log k , the k -coalesced condition is verified because the C -partition accesses to 2 c lesced condition. On the other hand when we fall into a C partitions, because the value 0 cannot be included in such type of sequence, for Definition 1 . Eventually, the C composed of a unique sequence of contiguous addresses.

To satisfy the k -coalesced condition for all the generated C -partitions, we move some pairs of items from a part of the current partition to another part. The aim is to group in the same memory transaction items having consecutive addresses, whenever we need longer sequences of consecutive memory addresses. To do that, each C -sequence is initialized with the it contains log r distinct values.
 supersteps. This means to perform more supersteps to cover the whole bitonic network. 6. K -Model-based sorting network evaluation
The solution we propose is evaluated theoretically and experimentally by comparing its complexity and performance the popularly known divided-and-conquer principle, whereas Radixsort exploits the processing of key digits. 6.1. Theoretical evaluation means to know how many stream elements are computed, then the number of fetch/flush phases, and the number of mem-ory transactions.

From Definition 2 , it follows that the first partition covers the first (log ering log( r / k ) steps. Resuming, the number of partitions needed to cover all the network is Since, each element fetches and flushes only coalesced subset of elements, the number of transactions is The time complexity is as it is obtained by Algorithm 3 which equally spreads the contentions among the k memory banks and maintains active all elements.
 Regarding the computational complexity it is known and it is input and counts the number of elements greater than the pivot, and the number of the elements smaller than the pivot. In writes the elements to the two opposite heads of an auxiliary array beginning at the positions calculated in the previous kernel.
 within a unique stream element. Consequently, for each prefix sum we need n / k element.
 of the whole algorithm, the sum of such formulas have to be multiplied by log n : kernel #1 n / k +2 n / kn kernel #2 4 n / k 2 4 log n k 2 n / k kernel #3 n / k + nn / kn OVERALL O ( n log n ) O  X  n k log n  X  O ( n log n ) reduces the data transfer overhead exploiting the on-chip memory to locally sort data by the current radix-2 blocks write their 2 b -entry digit histogram to global memory, and perform a prefix sum over the p 2 to compute the correct output position of the sorted data. However, consecutive elements in the subset may be stored into can be as high as a factor of 10.

In their experiments, the authors obtained the best performance by empirically fixing b = 4 and h = 1024. That means each to formalize the total number of memory transactions performed as follows: linear with the number of input-elements, i.e. b n . 6.2. Experimental evaluation The experimental evaluation is conducted by considering the execution time and amount of memory required by running BSN, Quicksort and Radixsort on different problem size. The different solutions have been implemented and tested on an
Ubuntu Linux Desktop with an Nvidia 8800GT, that is a device equipped with 14 SIMD processors, and 511 MB of external memory. The compiler used is the one provided with the Compute Unified Device Architecture (CUDA) SDK 2.1 ( NVIDIA, distribution, 20 different arrays were generated.
 consider the special case of sorting an all-zero array. 4 zero results.

The experiments confirm our theoretical ones. Fig. 5 shows the means, the standard deviation, and the maximum and the in terms of the number of memory transactions that it needs, see Table 1 .

This confirms our assumption that the number of memory transactions is dominant w.r.t the other two complexity mea-number of memory access operations (like in the case of data-intensive algorithms).

Radixsort, in fact, has a O ( n ) number of memory transactions, that is smaller than O ( n log O ( n log n / k ) of the Quicksort.
 method analyzed, see Fig. 6 .

On the other hand, our BSN approach is comparable to Radixsort and it is always faster than Quicksort, mainly because the mapping function proposed allows the full exploitation of the available memory bandwidth.
A last word has to be spent regarding the memory consumption of the methods. Quicksort and Radixsort are not able to formed on manycores. Otherwise, CPU can perform the merging step, but exploiting a bandwidth lower than the GPU X  X  one.
Table 1 measures the memory contention, and the number of divergent paths. The first value measures the overhead due to the contention on the on-chip memory banks as K -model expects. The second value measures how many times threads of the multiprocessors can not work simultaneously. These two last metrics together show the efficiency of the algorithms tested. Keeping low both values corresponds to a better exploitation of the inner parallelism of the SIMD processor. All the memory banks and all the computational, in fact, are allowed to work simultaneously.
 better analysis of the comparison-based sorting algorithms tested. Due to the in-place feature and due to the best Elapsed Time (milliseconds) Elapsed Time (milliseconds) performance resulting from the test conducted, BSN seems more preferable than Quicksort. Furthermore, BSN exposes lower result increases the overhead of synchronization among the set of available devices. 7. Indexing a real dataset
We present experiments showing the performance of a Blocked Sort-Based Indexer ( Manning et al., 2008 ) (BSBI), modified ing a portion of the web. The biggest collection contains 26 million documents and each document has an average size of 200 KB. The first column of Table 2 reports the different sizes of each collection tested.
 Our goal is to show that using a GPU-based solution we can speed-up the indexing process due to, mainly, two reasons: (i) mID X  X ocID pairs and then, the generated run is sorted by the GPU while the CPU carries out a new parsing step.
Fig. 7 shows the three different indexing algorithms we have tested. Index that takes as input a dataset D and generate the index f out the CPU-based sorting algorithm with our BSN. Index gpu  X  in more details this last algorithm in the remaining part of the section.

We have already pointed out in the introduction of this paper that GPUs are considerably more powerful from a compu-the number of intermediate indexes to merge. This number affects the merge algorithm complexity of a logarithmic factor (see below). However, the complexity of merging is dominated by the latency of data transferring from/to disk. Since the amount of data to transfer (namely the termId X  X ocId pairs) is the same in both the CPU-based and the GPU-based solution, we expect the time needed by the merging phase to be similar as well.

Table 2 shows how indexing time varies with the size of the collection processed. We consider the two main components sorted files. Instead,  X  X  X erge X  X  represents the time spent in merging sorted files. Concerning the two GPU-based methods,  X  X  X erge X  X  column of Table 2 is shown only once since the solutions perform the same merge procedure on exactly the same input.
 not represent a bottleneck anymore of the indexing process. In contrast, in GPU-based indexing, the computational time is dominated by the latency of the merging phase.
 Seconds than the complexity of the parsing step.

Fig. 8 B shows the execution times obtained by different versions of Index more steps, then to merge the partial results. 8. Conclusion and future work This paper focuses on using GPUs as co-processors for sorting. We propose a new mapping of bitonic sorting network on and experimentally, by comparing its complexity and performance with those obtained by two others state-of-the-art solu-tions (Quicksort and Radixsort).

The theoretical algorithms complexity was evaluated by using K -model a novel computational model to specifically de-signed to capture important aspects in stream processing architectures.

The experimentally evaluation was conducted using input streams generated according to different distributions. This mance obtained for different data distributions. Regarding the execution time, our solution is outperformed by Radixsort for sorting large amounts of data.

Accordingly the results of the experiments, we have chosen bitonic sorting network and Radixsort to develop an indexer prototype to evaluate the possibility of using an hybrid CPU X  X PU indexer in the real world. The time results obtained by indexing tests are promising and suggest to move also others computational intensive procedure on the GPUs. References
