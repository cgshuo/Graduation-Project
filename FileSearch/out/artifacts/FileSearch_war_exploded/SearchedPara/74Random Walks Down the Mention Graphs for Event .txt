 Coreference resolution , the task of resolving and linking different mentions of the same object/event in a text, is important for an intelligent text processing system. The re-solved coreferent mentions form a coreference chain representing an object/event. Fol-lowing the natural appearing order in the text, any two consecutive mentions in a coreference chain form an anaphoric pair, with the latter one referring back to the prior one. The latter expression is called the anaphor, while the prior expression is called the antecedent.

Most previous works on coreference resolution ([Soon et al. 2001; Yang et al. 2006] and others) aimed at object coreference 1 , of which both the anaphor and its antecedent are, are mentions of the same real-world object, such as person, location, organization, and etc.

In contrast, an event coreference as defined in Asher [1993] is an anaphoric reference to an event, fact, and proposition which is representative of eventuality and abstract entities, as in the following example.
 The four mentions here, [fired] , [it], [fired] and [the attack] , are referring to the same event (an Israel attack in the Gaza Strip on the Palestinian Authority). The pronouns, noun phrases, and action verbs are taken as the representations of events. This is also in line with OntoNotes 2.0 annotation practices. Finding the scope of event expressions is another challenging task beyond the scope of this article.

Event coreference resolution is an important task in natural language processing (NLP) research. According to our corpus study, 69% of articles in OntoNotes 2.0 corpus contain at least one event coreference chain, while 16% of all the coreference chains are event coreference chains. In addition to the significant proportion, event coreference resolution helps, event extraction system to acquire important details related to the same event-crossing sentence boundaries. Considering the previous example, resolving the event chain [fired] -[it] -[fired] -[the attack] will provide us all the necessary details about the  X  X ir strike X  event mentioned in different sentences, such as  X  X srael/Israel helicopter gunships X  being the actuator,  X  X ffices of Palestinian Authority X  being the target,  X 7 deaths and many injuries X  being the consequence,  X  X aza Strip X  being the location, and  X  X ore than two hours X  being the duration. Without a successful event coreference resolution, such separated pieces of information mentioned at different sentences cannot be assembled correctly to facilitate higher-level NLP tasks, such as multi-slot event template extraction and understanding with several even tens of slots covering different aspects of events.

Event coreference resolution incurs more difficulties compared to traditional object coreference resolution from two aspects. In a semantic view, an object (such as a person, location, organization, etc.) is uniquely defined by its name (e.g., Barrack Obama), while an event requires its role 2 information to distinguish it from other events. For example,  X  X he crash yesterday X   X   X  X rash in 1968 X  share the same event type, an airplane crash, but they are different events due to their different time arguments. Similarly,  X  X urder of Joe X   X   X  X urder of John X  and  X  X onfliction in Mid-East X   X   X  X onfliction in Afghanistan X  also share same event types but are distinguished by the patient and location ar-guments, respectively. In a syntactic view, object coreference resolution only involves mentions from noun category, while event coreference involves mentions from verbs as well. The syntactic differences will dysfunction the tradition coreference features, as reported by our previous work [Chen et al. 2010a, 2010b] for verb-pronoun/verb-NP resolution. The dysfunctional features include mention characteristics features, such as  X  X f NP is a proper name X , semantic features such as number/gender agreement and grammatical features, such as appositive structure. In addition to those findings, we further find that even the event NP-Pronoun/NP-NP resolution requires very different linguistic features from traditional ones. For example, previous semantic compatibility features only focus on measuring the compatibility between objects, such as person, location, etc. Event cases generally fall in the  X  X ther X  category, which provides us, no useful information in distinguishing different events. These extra syntactic and seman-tic difficulties make event coreference resolution a more complicated task compared to the conventional object coreference resolution.

In this article, we address the various different event coreference phenomena with seven distinct mention-pair resolvers. Three new linguistic features are proposed to capture the distinct characteristics of event coreferences. We then propose various enhancement techniques to boost up performance at both the mention-pair level and chain-formation level. At mention-pair level, we have proposed two techniques. First, we have proposed the technique or utilizing competitive classifiers X  results to boost mention-pair resolvers X  performances. Second, a new instance selection strategy is proposed to prevent mention-pair resolvers from being misguided by locally optimized instances used previously.

Moving up to the chain-formation process, we proposed a global resolution model using random walk partitioning on mention graphs. The conventional random walk model is modified with new terminating criteria dedicated to the sophisticated event coreference scenarios through sampling. These amendments greatly improve the effec-tiveness of random walk model in event coreference resolution task. In addition, these amendments make the random walk a self-interacting walk that is capable of incor-porating more powerful linguistic constraints and preferences. To our knowledge, it is the first attempt to use random walk for coreference resolution. On top of the modified random walk model, we further propose three techniques for effective chain formation. First, the incorporation of linguistic constraints and preferences prunes inappropriate candidates and selects preferable ones. Second, the incorporation of object mention graphs provides additional pruning ability. Last, the incorporation of pronoun coref-erence information provides additional knowledge, which previous models for object coreference resolution failed to use. All of them show significant improvements in our experiments.

The rest of this article will be organized in the following way. The next section introduces related works. Section 3 revisit the traditional object coreference resolution framework. After that, we will move on to our proposed model in Section 4. In Section 5, we present experiment results to justify our proposed method with discussions. The last section wraps up with a conclusion and future research directions. Despite its importance, event coreference resolution had not attracted much atten-tion. There is only a limited number of previous works related to this task. Asher [1993] discusses a method for resolving references to abstract entities using discourse representation theory. However, no computational system was proposed.

Besides Asher X  X  linguistic study, there were only a few previous works attempting to tackle some of the subproblems of event coreference resolution. [Byron 2002; M  X  uller 2007; Chen et al. 2010a] attempted event pronoun resolution. [Chen et al. 2010b] attempted resolving noun phrases to verb mentions. All these works only focused on identifying specific types of pairs of event mentions such as verb-pronoun pairs and verb-NP pairs. The ultimate goal, which is extracting the whole event chain (including all possible pairs among NP, verb and pronoun), lacks attention.
 Pradhan et al. [2007] applied a conventional coreference resolution system to the OntoNotes1.0 corpus using the same set of features for object coreference resolution. However, there was no specific performance reported on event coreference. As Chen et al. [2010b] pointed out, the conventional features did not function properly on the event coreference problem.

Bejan and Harabagiu [2010] proposed an unsupervised Bayesian-model-based at-tempt at event coreferences. The corpus statistics gathered on their Event Coref Bank V1.0 corpus 3 had shown a more focused corpus on the cross-document verb corefer-ences resolution. There are only 20.9% (272 out of 1302) of intra-document chains having more than one mention. The intra-document event coreferences appear not well captured due to its corpus design. At the same time, 89.7% (1,564 out of 1,744) 4 of event mentions were verb mentions, while none of the mentions annotated were pronouns. These observations were fundamentally different from the OntoNotes2.0 corpus, where only intra-document coreferences were annotated and all the NPs, pro-nouns, and verbs were annotated. All of these factors made the findings in Bejan and Harabagiu [2010] more suitable for cross-document verb coreference resolution instead of the intra-document event coreference according to Asher X  X , [1999] definition. Before we introduce our proposed system for event coreference, we would like to revisit the widely used two-step resolution framework to understand some of its weaknesses. Most previous coreference resolution systems employ a two-step approach (e.g., [Soon et al. 2001; Nicolae and Nicolae 2006] and many others). The first step identifies all the pairs of coreferent mentions. The second step forms coreference chains using the coreferent pairs identified from the first step.
 Although a handful of single-step frameworks were proposed recently (e.g., [Cai and Strube 2010]), a two-step framework is still widely in use because it has been well-studied. Conceptually, the two-step framework adopts a divide-and-conquer strategy which in turn, allows us to focus on different subproblems at different stages. The mention-pair detection step allows us to employ many features associated with strong linguistic intuitions which have been proven useful in the previous linguistic study. The later on-chain formation step allows us to leverage on efficient and robust graph parti-tioning algorithms, such random walk partitioning used in this article. Practically, the two-step framework is also more mature for practical uses and has been implemented as a number of standard coreference resolution toolkits widely available, such as REC-ONCILE [Stoyanov et al. 2010] and BART [Versley et al. 2008]. Performance wise, two-step approaches also show comparable performance to single-step approaches on some benchmark datasets. 5
In this article, we are exploiting a brand new type of coreference phenomenon with merely no previous attempts. Therefore, we employed the much more matured two-step framework with innovative extensions to accommodate complicated event coreference phenomena. Such a divide-and-conquer strategy will provide us more insight for further advancements. Most mention-pair models adopted the well-known machine learning framework for object coreference resolution, as proposed in Soon et al. [2001].

Instances Generation. During training, many systems employed the widely used in-stance selection strategy [Ng and Cardie 2002]. In brief, only the closest antecedent of a given anaphor is used as positive instance while only candidates between the anaphor and its closest antecedent are used as negative instances. Intuitively, a set of training instances generated using such a strategy will represent the preference among the locally selected candidates. During testing, an instance is generated by pairing can-didates within n sentences 6 from the anaphor. However, according to our study, such an instance selection strategy will misguide the chain formation process using graph partitioning approaches.

Support Vector Machine with Tree Kernel. In such a learning framework, many well-known learning models, such as support vector machine (SVM), can be applied to the coreference resolution task. In addition, syntactic structures are incorporated through a convolution tree kernel. Effectiveness of various structures has been investigated [Yang et al. 2006; Chen et al. 2010a, 2010b]. Based on their findings, minimum-expansion is chosen for this article. In brief, it contains only the path in the syntactic parse tree connecting an anaphor and its antecedent. After the coreferent mention pairs are identified, coreference chains are formed based on those coreferent pairs. There are two major ways to form coreference chains in the literature, namely, best-link heuristic and graph partitioning.

Best-Link Heuristic Approach. The best-link heuristic selects the candidate with the highest confidence for each anaphor and forms a best-link between them. After that, it simply joins all the mentions connected by best-links into the same coreference chain. The best-link heuristic approach is widely used (e.g., [Soon et al. 2001; Yang et al. 2006]) because of its simplicity and reasonably good performance.

The major critics of best-link heuristic point out its lack of global consideration when forming the coreference chains. The mentions are only joined through locally selected best-links. Thus, the chain consistency is not enforced. Remedies to such a critique are proposed, such as Best-Cut in the next section and our proposed method of going through a mention graph partitioning process.

Graph Partitioning Approach. Graph partitioning approaches are proposed by sev-eral researchers to form coreference chains with global consideration. Here we take Best-Cut [Nicolae and Nicolae 2006] as a representative work of graph partitioning ap-proaches. Best-Cut is a variant from the well-known minimum-cut algorithm. A graph is formed using all the mentions as vertices. An edge is added between two mentions of a positive output from the mention-pair model. Then the set of edges is iteratively cut to form the final coreference chains.

Nicolae and Nicolae [2006] assume that the incorporation of coreferent pairs in-volving pronouns will bring the extra sophistication which will cause difficulties in the chain formation, thus the pronoun-related information is not used in Best-Cut. However, event coreference chains contain a significant proportion of pronouns (18.8% of event coreference mentions in the OntoNotes2.0 corpus). Leaving them untouched would be problematic. In the next section, we propose an alternative chain forma-tion method for incorporating coreferenting pronouns into the graph partitioning to accommodate its intensive occurrences in event chains. Our proposed resolution framework follows a similar system flow as the two-step frame-work illustrated in Figure 1 (for an overview of our resolution system). Each document is passed to our system as input texts . At the beginning, the mention extractor will extract the event/object mentions from the input texts. Then, at the mention-pair step, different types of mention pairs will be handled their corresponding resolvers. After that, the mention graph is constructed using the resolved mention pairs. Moving to the chain formation step, our proposed random walk model is applied. At last, the coreference results are output from the random walk clustering process.

A brief discussion on various types of event coreference is given in Section 4.1. Each type corresponds to a distinct mention-pair resolver. New features are proposed to cap-ture three newly encountered phenomena. The object NP-pronoun and NP-NP resolvers are trained to help the event resolvers. The details are given in Section 4.1. After that, we proposed two techniques for improving the mention-pair performance, namely, a new instance selection strategy and the utilization of the competing classifiers X  results. After coreferent pairs are identified, the mention graph is formed using these corefer-ent mentions. At chain formation step, we proposed the alternative method, random walk partitioning on the mention graphs to utilize linguistic preferences, enforce lin-guistic constraints, incorporate the object mention graph information and make use of the coreferent pronouns which are not used in the previous work. Details on mention graph and random walks will be elaborated upon in Section 4.2.

Event Mention Extraction. We use the system mentions generated by our mention ex-tractor. The mentions are extracted in the following way. First, all the verbs (excluding model verbs) in an article are treated as event mentions. Since pronouns have too little information to classify them as event-pronoun versus object-pronoun, all the pronouns will be resolved by both event resolvers and object resolvers. Last, all the noun phrases (NP) are subjected to a categorization as event NP, object NP, and ambiguous NP. This categorization is done automatically using its hypernymy information from the Word-Net. 7 A list of event hypernyms and another list of object hypernyms are collected from the training corpus. If an NP X  X  hypernym matches the event/object hypernym list, it will be classified as event/object NP. If an NP X  X  hypernym matches none or both of the event and object hypernym lists, it is classified as ambiguous NP. Since the ambiguous NPs may be either event or object, we present them to both event resolvers and object resolvers.

Seven Distinct Mention-Pair Resolvers. One major difficulty of event coreference res-olution lies in the gap between different syntactic types of mentions (e.g., nouns, verbs, and pronouns). As discussed in Chen et al. [2010a, 2010b], different syntactic types of coreferent mentions show characteristics very differently, which requires distinct features to resolve them. Following this insight, we have built five distinct resolution models for event coreference resoution involving noun phrases, pronouns, and verbs: Verb-Pronoun, Verb-NP, Verb-Verb, NP-NP, and NP-Pronoun resolver. Conventionally, pronouns can only appear as anaphor but not antecedent. Therefore, we do not train Pronoun-Pronoun, Pronoun-Verb, and Pronoun-NP resolvers. In addition, we find that the effective feature sets for Verb-NP and NP-Verb resolvers are the same. There-fore, the Verb-NP resolver will handle the Verb-NP mention pairs in both forward and backward directions.

In addition to the syntactic difference, we find that event NPs have different char-acteristics from the object NPs. Event NPs require the event roles to distinguish one event from the others while the object NPs are quite self explaining. The conventional features, such as string-matching and head-matching, will not work properly when handling cases like  X  X onfliction in Mid-East X  versus  X  X onfliction in Afghanistan X . In our approach, a sophisticated argument-matching feature is proposed to capture such infor-mation. The argument X  X  information is extracted automatically from the premodifiers and prepositional phrase attachments.

Similarly, conventional features try to match mentions into semantic categories, like person, location, etc. It then evaluates the semantic-matching features to pair up men-tions from the same semantic type. However, event NPs exhibit very different type re-lations in WordNet from the object NPs. A set of dedicated event hypernymy-matching features is proposed to match events of the same type. Such rules will match from Word-Net hypernyms to several surface words or sub-hypernyms in the hypernymy hierarchy tree. For example, the Hypernymy class  X  X ommunication X  will be matched to surface word  X  X ay X ,  X  X nnounce X  and  X  X ell X  or sub-hypernymy class  X  X ransmission X ,  X  X ail X , and  X  X erbal communication X . These rules are generated from linguistic intuitions and error analysis from the training corpus. With respect to the differences between object NPs and event NPs, we train two distinct models to handle object NP-NP and event NP-NP resolution separately with distinct features. Similarly, we train separate resolvers with distinct features for event/object NP-Pronoun. In total, we have seven distinct mention-pair resolvers for different syntactic and semantic types of mentions. Five of them focus on event coreference, while the other two aim at object coreference. Object coreference results are used to enhance event coreference resolution performance by ruling out inappropriate event anaphors. All the features we incorporated are tabulated in Table I.

Besides the new features (Event-Semantic, Argument-Matching and etc.), the other features we used in the seven mention pair resolvers are employed from a number of previous works such as (Soon et al. 2001; Yang et al. 2008) for object coreference feature, (Chen et al. 2010a,b) for features involving verbs.

Utilizing Competing Classifiers X  Results. For the same mention, different mention-pair resolvers will resolve it to different antecedents. Some of these resolution results contradict each other. In the following example, for the anaphor [it] , event NP-Pronoun resolver may pick [the attack] , as the antecedent while object NP-Pronoun resolver may pick { some evidence } as the antecedent. Instead of choosing one from these contradicting outputs as the final resolution result, we feed the object resolver results into the event resolvers as a feature and retrain the event resolvers. The idea behind this is to provide the event resolvers X  learning models with a confidence on how likely the anaphor refers to an object. This type of information is par-ticularly useful when the anaphor is a pronoun, which conveys much less information than verbs and NPs.

New Training Instances Selection Strategy. As we mentioned previously, the tra-ditional training instance selection strategy [Ng and Cardie 2002] has some inap-propriateness. The original purpose of mention-pair resolvers is to identify any two coreferent mentions (not restricted to the closest one). By using the previous training instance selection strategy, the selected training instances actually represent a sample space of locally closest preferable mention versus locally nonpreferable mentions. Most previous works show a reasonably good performance when using it with  X  X est-link X  chain formation technique.

However, our investigation shows it actually misguided the graph partitioning meth-ods. Therefore, we propose a new training instance selection strategy which reflects the true sample space of the original coreferent/non-coreferent status between mentions. In brief, our revised strategy exhaustively selects all the coreferent mention-pairs as positive instances and non-coreferent pairs as negative instances, regardless of their closeness to the anaphor. Consider the following example. The traditional instance selection scheme will only select [the attack] X  X it] as positive instance and { top assistants }  X  X it] as negative instance. Our new instance selection scheme will select an additional positive instance [the bombing] X  X it] and additional negative instance as { Bin Laden }  X  X it] , { USA Today }  X  X it] , { some evidence }  X  X it] , { Saudi terrorist Osama Bin Laden }  X  X it] and other candidates from the previous two sen-tences. Thus the full sample space is represented using our training instances selection strategy. After obtaining the mention-pair model results, we can construct the weighted mention graph for chain formation process. The set of nodes of the mention graph includes all the mentions of an article. Two nodes are linked by an edge if one of the SVM mention-pair resolvers has a positive output. The weights of edges are set to the SVM confidence 9 outputs.

As we mentioned previously, traditional chain formation technique suffers from ei-ther a local decision (as in best-link approaches) or failure to incorporate pronoun in-formation (as in best-cut approaches). Our approach based on random walk shows its advantages as a global solution over the best-link approaches. Compare with best-cut approaches, we can incorporate coreferent pronouns, linguistic constraints/preferences, and object graph information to effectively form chains for event coreference resolution.
Random walk is formally defined and introduced in Barber and Ninham [1970]. It has made its success in several NLP applications, such as polarity classification [Hassan and Radev 2010], semantic similarity [Ramage et al. 2009], and semantic relatedness [Hughes and Ramage 2007; Yeh et al. 2009]. This is the first attempt in the literature to use random walk partitioning for coreference resolution.

In the following paragraphs, we will first introduce the traditional random walk model. After that, we will elaborate our modified version of random walk model in detail and explain why such modifications are preferable for applying random walks to coref-erence resolution. Then we will explain our proposed techniques (linguistic/preferences and object graph information) to further improve the chain formation process.
Conventional Random Walk Model. Given a weighted graph G with a vertices (nodes) set V and an edges set E , a random walk W starting from a node n 0 will move from a node n to another node n j with a probability P ij . This probability is calculated by normaliz-ing the edge weights of node ni. ( P ij = w ij / w ik , where w ij is the weight of edge between node n i and n j , w ik is the sum of weights on all the edges connected to node n i . )With-out any terminating condition, if we repeat the random walk process a sufficiently large number of times, the random walk W will eventually become stationary to be trapped in densely connected subgraphs. 10 Therefore, a stationary transition matrix can be de-rived in conventional random walk models to identify the most probable final nodes of a random walk. This traditional stationary transition probability based random walk was used in previous works [Ramage et al. 2009; Hughes and Ramage 2007; Yeh et al. 2009].
Modified Random Walk for Event Coreference. Although the conventional random walk model can be applied to coreference resolution as a graph partitioning algorithm, it fails to care for certain special characteristics of event coreferences. First, for the event coreference resolution task, we are more interested in all the nodes visited by the random walks instead of the final node of the random walks. All the nodes visited by a random walk are considered as mentions of the same entity. Second, conventional random walk model assumes an infinite length of the walk where event coreference chains are, in general, very short. In addition, a conventional model fails to incorporate the constraints and preferences at all. Therefore, we have made three meaningful modifications to the conventional random walk model to make it more suitable to the event coreference resolution task. (1) Self-Interacting Random Walk through Sampling. As the list of nodes visited by a (2) Terminating Criteria. As random walks traverse in the mention graph, we introduce (3) Starting Nodes Selection. Since our focus is on event coreference resolution, only
Utilizing Pronoun Coreference Information. In addition to the preceding advantages of the random walk model, one particular reason to employ it is that the previous best-cut approaches failed to incorporate pronoun information in their similarity graph. This may not be an issue in object coreference scenario, as pronouns are only a relatively small proportion (9.78% of object mentions in OntoNotes); however, in event cases, pronouns contribute 18.8% of the event mentions. As we further demonstrated in our corpus study, event chains are relatively more sparse and shorter than object chains. In fact, a significant proportion of the event chains consist only two mentions, the pro-noun and its verb/NP antecedent. Removing pronouns from the similarity graph would break a significant proportion of the event chains. Thus we propose this random walk partitioning approach to overcome this inappropriateness from the previous models.
Incorporating Linguistic Constraints and Preferences. Random walks can be con-veniently equipped with linguistic constraints and preferences to guide the walking process when selecting the next-step node to move on. We have crafted a set of pruning rules 14 to eliminate next-step nodes which may cause inconsistence in the event chain. A neighboring node of the current node will be disqualified for the random walk if it triggered one of the pruning rules. To enforce the chain consistency, a next-step node is tested against all the nodes currently in the walk. Such pruning rules are crafted based on linguistic intuition and error analysis on the training corpus. The pruning rules include five major types. (1) Conflicting Event Hypernymy. This rule is fired if a next-step node and one of the (2) Conflicting Event Arguments. This rule is to filter out cases as  X  X onfliction in Mid-(3) Conflicting Number Agreement. This rule is to prune improper links between  X  X on-(4) Conflicting Text-Span. This rule will prune out cases when one next-step node is (5) Conflicting Governing-Node. This rule wil eliminate cases where two mentions are
Some of these constraints are utilized in the mention-pair SVM models introduced previously to calculate the similarity between mentions. However, SVM models only considere linguistics constraints between two mentions as soft feature, which means that two mentions may still obtain a high score even if they violate one of the hard lin-guistic constraints. In contrast, by using these linguistics-constraints-guided random walks, we can enforce the nodes X  consistency at the set level.

In addition to the pruning rules, we also derive a list of preference rules to favor that the next-step nodes satisfie linguistic preferences. 17 To maintain the randomness of the walk, instead of picking the node preferred by the rules, we increase 18 the probability for selecting that node for the walk. Similarly as with the pruning rules, the preference rules are tested against all the nodes in the current walk. These preference rules are derived from both linguistic knowledge and error analysis on the training corpus. The set of preference rules includes three major types. (1) Shared/Compatible Event Hypernymy. A neighboring node is more preferred if it (2) Shared/Compatible Event Arguments. A next-step node is more preferred if it (3) Fixed Pairing of Headwords. A list of fixed pairing of headwords is collected from
Incorporating Object Graph Information. In our proposed model, only event and am-biguous noun phrases are used for final event chain formation. However, the ambiguous NPs may also consist of object NPs which may introduce noises to the event corefer-ence chains. Therefore, we propose to incorporate a portion of object graph nodes, which helps to rule out the object NPs. Object mention graphs in general are much larger and denser than event mention graphs. Including the whole object graph would increase the computation complexity unnecessarily. Therefore, we only expand the mention graph by adding those object NP nodes having links with any of the ambiguous NP nodes. Those object nodes linked only to other object nodes will not be added to keep a smaller graph for random walk partitioning.

After adding in the object nodes, we impose one more terminating criterion into the random walk model. Any random walks visiting an object node will be immediately terminated and discarded. Since we use a sampling approach for random walks, in order to maintain the size of the samples, a new walk from the same starting node is conducted. In this section, we present the experiment results to verify the effectiveness of our pro-posed methods. Before the numerical results are presented, we would like to introduce the corpus used for this study X  X he OntoNotes version 2.0 X  X nd the evaluation metric for event coreference resolution. The corpus we used is OntoNotes2.0, which contains 300K English news wire data from the Wall Street Journal and 200K English broadcasting news data from various sources, data including ABC, CNN, etc. OntoNotes2.0 provides gold annotation for parsing, named entity, and coreference. The distribution of event coreference is tabulated in Table II.

The distribution of event chains is quite sparse. On average, an article contains only 2.6 event chains comparing to 9.7 object chains. Furthermore, event chains are generally shorter than object chains. Each event chain contains 2.72 mentions com-pared to 3.74 mentions in object chains. In this work, we employ two performance metrics for evaluation purposes. At the mention-pair level, we used the standard pair-wise precision/recall/F-score to evaluate the seven mention-pair resolvers. At the coreference-chain level, we use the B-Cube (B 3 ) measure [Bagga and Baldwin 1998]. B 3 provides an overall evaluation of coreference chains instead of coreferent links. Thus it is widely used in previous works.
For each experiment conducted, we use the following data splitting. Four hundred articles are reserved to train the object NP-Pronoun and object NP-NP resolvers. 20 Among the remaining 1,118 articles, we randomly selected 894 (80%) for training the five event resolvers, while the other 224 articles are used for testing.

In order to separate the propagated errors from the preprocessing procedures, such as parsing and named entity recognition, we used OntoNotes 2.0 gold annotation for Parsing and Named Entities only. Coreferent mentions are generated by our system instead of using the gold annotations.

In order to test the significance in performance differences 21 , we perform paired stu-dents X  t-tests at 5% level of significance. To make the paired t-test statistics sufficient, we conduct the experiments 20 times through a random sampling method in order to gather the performance data. In the tables of this section, a line ending with an  X   X   X  indicates it has statistically significant different results compared the line preceding it. In this section, we will present the experiment results to verify each of the improve-ments we proposed in previous sections. The experiment results will be grouped into three major sections, namely, the mention-pair improvements (competing classifiers and new instance selection), the modified random walk model (with terminating cri-teria and probability), and the improvements made on top of the modified random walk model (linguistic constraints/preferences, object mention graph, and coreferent pronouns).

Mention-Pair Models Performances. The first set of experiment results presented here is the seven mention-pair resolvers using all conventional settings without any proposed methods. They are tabulated in Table III. The Verb-Verb resolver performance is particularly low due to lack of training instances, where only 48 positive instances are available from the corpus. Our mention-pair models are not directly comparable with those of Chen et al. [2010a, 2010b] which used gold annotation for object coreference information while we resolve such coreferent pairs using our trained resolvers. There are also a number of differences in the preprocessing stage, which makes the direct comparison impractical.

The coreference chains formed using the conventional random walk model (the sta-tionary transition probability method) without any proposed improvements yields a B 3 F-score of 31.67%, which serves as our initial baseline (BL) for further comparisons. Af-ter presenting the baseline, we will demonstrate the effectiveness of the two proposed techniques at the mention-pair level, namely, the utilization of competing classifiers X  results and the new instance selection strategy. Mention-Pair Models X  Performances after Utilizing Competing Classifiers X  Results. The first improvement we proposed to the mention-pair models is the utilization of competing classifiers X  results. Since object resolver results are in general better than those of event resolvers, we propose to utilize competing object classifiers X  results to improve event resolvers X  performance. The experiment results are tabulated in Table IV. The BL + CC row presents the performance when utilized competing classifiers X  results into the baseline system.

By incorporating the object coreference information, we manage to improve the event coreference resolution significantly X  X ore than 9% F-score for Verb-Pronoun resolver and about 7% F-score for event NP-Pronoun resolver. Object coreference information improves pronoun resolution more than NP resolution, mainly because pronouns con-tain much less information than NPs. Such additional information helps greatly in preventing object pronouns from being resolved by event resolvers mistakenly. Al-though object coreference is incorporated at the mention-pair level, we still measures its contribution to B 3 score at the chain level. It improves the B 3 F-score from 31.67% to 37.23%, which is a 5.56% improvement. This observation also shows the importance of collective decision of multiple classifiers.

Mention-Pair Models X  Performances after Using New Instance Selection. The sec-ond technique we proposed for improving the mention-pair models is a new training instances selection strategy. Table V shows improvement using the new instance se-lection strategy. We refer to the traditional instance selection strategy as BL + CC and to our proposed instance selection strategy as BL + CC + NIS (New Instance Selection). At the mention-pair level, we take the event NP-Pronoun resolver for demonstration. Similar behaviors are observed in all the mention-pair models. In order to demon-strate the power of the new instance selection scheme, we evaluate the mention-pair results in two different ways. The best-candidate evaluation follows the traditional mention pair evaluation. It first groups mention-pair predictions by anaphor. Then an anaphor is correctly resolved as long as the candidate-anaphora pair with the highest resolver X  X  score is the true antecedent-anaphor pair. The correct/incorrect resolution of other candidates X  outputs are not counted at all. The coreferent link evaluation counts each candidate-anaphor pair resolution separately. Intuitively, best-candidate evalua-tion measures how well a resolver can rank the candidates, while the coreferent link evaluation measures how well a resolver identifies coreferent pairs.

An interesting phenomenon here is that the performance evaluation using the best candidate actually drops 3.26% in f-measure when employing our new instance selec-tion scheme. But when we look at the coreferent link results, our new instance selection scheme improves the performance by 2.84% f-measure. As a result, our new instance selection scheme trains better classifiers with higher coreferent link prediction results, since this coreferent link information is further used in the final chain formation step. Our new scheme contributes an improvement on the final event chain formation by 3.59% F-Score in B 3 measure.

This observation shows that the traditional mention-pair model should be revised to maximize the coreferent link performance instead of the traditional best-candidate performance, because the coreferent link performance is more influential on the final chain formation process using a graph partitioning approach.

Modified Random Walk Model Chain Formation Performances. After presenting the improvements at the mention-pair level, we will move up to the chain formation level to show experiments X  results on various techniques we proposed to the chain forma-tion process. First, we present the empirical support for our modified version of the random walk model. Table VI shows the performance differences in two settings. The first setting uses none of the proposed mention-pair level techniques. The conventional random walk model is denoted by BL, as introduced previously. Our proposed modified version of random walk is denoted by MRW (short for Modified Random Walk). In the second setting, we have applied both competing classifiers X  results and the new in-stance selection strategy to the MRW model. The BL + CC + NIS line is the conventional random walk model plus competing classifiers and new instances selection, while the MRW + CC + NIS line is our proposed modified random walk model using competing classifiers and new instance selection.

In both settings, our proposed modified random walk model significantly outperforms the conventional random walk model. This shows that the proposed modification to the random walk model is necessary and effective for applying the random walk model to the event coreference resolution task. Therefore, we will use the MRW + CC + NIS from this point onwards. All the proposed techniques for the chain formation process will be applied to and tested on the MRW + CC + NIS model collectively.

Modified Random Walk Model Utilizing Pronoun Information. The first improve-ment we proposed on top of the modified random walk model is the incorporation of coreferent pronoun information. The improvement is demonstrated in Table VII be-low. The MRW + CC system corresponds to the fair comparison model consisting of the modified random walk model combined with competing classifiers X  results. These MRW + CC + Pron system corresponds to the MRW + CC model with additional corefer-ent pronouns information.

By incorporating the coreferent pronoun information, the performance is signifi-cantly improved by 1.77% in F-score. This observation is in line with our intuition that coreferent pronouns in event coreference chains are very important in the chain formation process. Incorporation of the coreferent pronoun is an effective technique for boosting the chain formation performance for event coreference resolution. By further incorporating the new instance selection strategy into the MRW + CC + Pron model, we can further improve the B 3 -F-score to 46.49%.

Modified Random Walk Model Incorporating Linguistic Constraints. In Table VIII, we present the performance comparison before and after enforcing the linguistic con-straints and incorporating linguistic preferences in the random walk process.
The MRW + CC + NIS + Pron system corresponds to the best model in the previous section for comparison purposes. The MRW + CC + NIS + Pron + Const + Perf system cor-responds to the MRW + CC + NIS + Pron system further extended with enforcement of linguistic constraints and incorporation of linguistic preferences.

As the results show, linguistic constraints and preferences incorporation brings us a 1.88% improvement in B 3 -F-score. Especially, the precision score is greatly im-proved. It shows that the incorporation of linguistic constraints helps to accurately identify the event coreference chains. In balanced overview between precision and re-call, the improvement is roughly a trade-off between precision and recall, as precision improves about 4% while recall decreases a similar amount. The final F-score improves as MRW + CC + NIS + Pron + Const + Perf provides a more balanced precision and recall than the system without linguistic constraints and preferences.

Incorporating Object Graph Information. Table IX below demonstrates the per-formance improvement by further incorporating object graph information. The MRW + CC + NIS + Pron + Const + Perf system corresponds to the system without the ob-ject mention graph. The MRW + CC + NIS + Pron + Const + Perf + Obj system corresponds to the previous best-performing system with further extension of object mention graph information.

As the results show, by utilizing the object graph information, we can further enhance the overall resolution performance by 1.92% in B 3 F-score. This is mainly from the improvement in precision with only a small drop in recall. By incorporating the object mention graph, we can identify the event coreference chains more precisely. 50.29% B 3 F-score 22 is the best performance we achieved in this work. In this article, we have conducted a systematic study for the important and complicated event coreference resolution. We followed the two-step framework to gain more insights into complex event coreference phenomenon. At the same time, we also proposed var-ious techniques and a new model for improving the event resolution performances at both the mention-pair level and the chain-formation level.

Two techniques (i.e., utilization of competing classifiers X  results and a new instance selection strategy) are proposed to enhance the mention-pair performances. The ran-dom walk model is introduced to the coreference resolution task with effective and necessary modifications for the first time. The modifications include sampling method, terminating criteria, and stop probability. The modified version of the random walk shows significantly better results than the conventional ones, which is a success of our model adaptation. On top of the modified random walk model, we further pro-posed three methods to boost the chain formation results. Incorporation of coreference pronouns handles an important source of information which is left out by previous attempts. Enforcement of linguistic constraints and preferences integrates linguistic knowledge into the random walk model to enhance its chain formation capability. In-corporation of an object mention graph utilizes additional object information in the event coreference chain formation process.
 All these techniques contribute to a significant improvement over the initial system. We managed to achieve a 50.29% B 3 -F-score as the highest in this work. In the future, we plan to incorporate more semantic knowledge for mention-pair models, such as semantic roles and word senses. For chain formation, we plan to incorporate more linguistic knowledge through constraints.

