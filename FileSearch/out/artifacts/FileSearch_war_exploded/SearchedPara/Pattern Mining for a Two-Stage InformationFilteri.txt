
Xujuan Zhou 1 , Yuefeng Li 1 , Peter Bruza 1 ,YueXu 1 , and Raymond Y.K. Lau 2 An Information Filtering (IF) [1] system monitors an incoming document stream to find the documents that match information needs of users. With information filtering, the representation of the user information needs is variously referred to as user profiles or a topic profile where the filters are applied to the dynamic streams of incoming data. Unlike the traditional search query, user profiles are persistent, and tend to reflect a long-term information need [1].

The traditional IF systems make the d ecision of rejection or reception for a document when it arrives in the stream. The relevant document is displayed to its users without further scrutiny. This decision-making is completed in one step. Such systems often have difficulties in dealing with issues, such as the feature selection on how to remove noisy and non-relevant features, and threshold setting (how to learn the optimal threshold). To improve the robustness of the IF systems, this paper will propose a novel two-stage framework for information filtering.

To illustrate the two-stage IF model, consider an example that may occur in a TV series. Louisa is a girl from a big city looking for a partner. There are three strategies Louisa may adopt, (i) set herself criteria, check out the information of as many people as possible. When Mr Right meets the criteria, she chooses him and lives happily ever after, (ii) date with everyone that is available, rank them according to suitability, then choose the highest-ranked person and live happily ever after, (iii) set herself criteria for rejection, date with a small number of people then choose the best fit.
 The first approach has some obvious setbacks. If the criteria are set too high, Louisa may never find Mr Right. If the criteria are too low, she would have difficulties choosing Mr Right. The second strategy is also not practical. What is the point in wasting the energy with every one and ranking the obviously not suitable people? The third approach is a two-stage method. It is the only sensible and efficient way in the three approaches.

Within the new two-stage IF framework, the first filtering stage is supported by a novel rough analysis model which efficiently removes a large number of irrelevant documents. The intention after the first stage is that only a relatively small amount of potentially highly relevant documents remain as the input to the second stage. The second filtering stage is empowered by a semantically rich pattern taxonomy mining model which effectively rank incoming documents ac-cording to the specific information need s of a user and fetches the top ranking documents for a user. The initial idea already proposed in the paper [2]. The main contribution of this research work is that a novel rough sets based optimal filtering threshold calibration method has been developed. It was found that a good  X  X alance X  must be found between reducing the  X  X oise X  at the first stage, and at the same time, effectively matching incoming documents with a semanti-stage, pattern mining and matching can be conducted efficiently and applied to realistic IF settings.

The remainder of the paper is organized as follows. Section 2 highlights pre-vious research in related areas. Sect ion 3 introduces the Rough Set Decision Rule-based Topic Filtering. Section 4 pr esents filtering model based on the pat-tern taxonomy mining. The empirical res ults are reported in Section 5. Section 6 describes the findings of the experiments and discusses the results. Concluding remarks are sketched in Section 7. The term-based approaches for IF have been proposed to address the problems of overload and mismatch over the past decades. The term-based IF systems used terms to represent the user profiles. Such profiles are the most simplest and common representation of the profiles. For examples: the probabilistic models [3], BM25 [4], rough set-base models [5], and ranking SVM [6] based filtering models used the term-based user profiles. The advantage of term-based model is efficient computational performance as well as ma ture theories for term weighting, which have emerged over the last couple of decades from the IR and machine learning communities. However, ter m-based models suffer from the problems, such as, the relationship among the words can not be reflected and also, only considering single words as features is the semantic ambiguity.

In the presence of these setbacks, sequential closed patterns used in data min-ing community have turned out to be a promising alternative to phrases [7]. Pattern mining techniques can be used to find various text patterns, such as co-occurring terms and multiple grams, maximal frequent patterns, and closed patterns, for building up a representation with these new types of features. In [8], data mining techniques have been used for text analysis by extracting co-occurring terms as descriptive phrases from document collections. However, the effectiveness of the text mining syst ems using phrases as text representation showed no significant improvement. Mini ng maximal frequent patterns [9] was also proposed to reduce the time complexity of mining all frequent patterns, where an itemset (or a pattern) was maximal frequent if it had no superset that was frequent.

To consider the very impor tant semantic relationsh ips between the terms, a pattern taxonomy model (PTM) for IF has been proposed in [10]. Pattern taxonomy is a tree-like hierarchy that re serves the sub-sequence (that is,  X  X s-a X ) relationship between the discovered sequential patterns. These pattern based approaches have shown encouraging impr ovements on effectiveness, but at the expense of computational efficiency. Another challenging issue for PTM is to deal with low frequency patterns because the measures used in data mining to learn profiles turn out to be not suitable in the filtering tasks. To deal with the uncertainty issues, a Ro ugh Set-based IF model(RSIF) has been developed in [11]. There are two key tasks in developing a RSIF model. The first one is using discovered rough patterns to represent the topic profiles. The second task is deciding an optimal threshold based on the obtained topic profiles. In this paper, only the positive documents will be used to represent the user profile as a rough set. It is less efficient to use the features of the non-relevant documents for an information filter since coverage of the feature descriptions for negative documents can be very large. 3.1 Discovery of R-Patterns A set of terms is referred to as a termset . Given a positive document d i and a term t , tf ( d i ,t ) is defined as the number of occurrences of t in d i .Asetofterm frequency pairs is referred to as an initial r-pattern (rough pattern) in this paper.
Let termset ( p )= { t | ( t, f )  X  p } be the termset of r-pattern p .Inthispaper, r-pattern p 1 equals to r-pattern p 2 if and only if termset ( p 1 )= termset ( p 2 ). Two initial r-patterns can be composed if they have the same termset .Inthis paper, we use the composition operation,  X  , that defined in [11] to compose r-patterns. For example, (Notice:  X  is also suitable for patterns with different termsets, e.g., { ( t 1 , 2) , ( t , 5) } X  X  ( t 1 , 1) } = { ( t 1 , 3) , ( t 2 , 5) } ).

Based on the above definitions, for a given set of positive documents D + = { d 1 ,d 2 ,...,d n can also group the initial r-patterns that have the same termset into clusters and use their composition, a r-pattern, to represent the cluster. Therefore, the training set of positive documents, D + , is described as a set of r-patterns, RP = { p 1 ,p 2 ,...,p r
Let cluster ( p i ) be the set of documents (initial patterns) that are composed to generate p i . We can define the support of a r-pattern p i as follows: Theorem 3.11 Let RP = { p 1 ,p 2 ,...,p r } be the set of r-patterns discovered in D + . We have Proof. For any two r-patterns p i and p j ,wehave cluster ( p i )  X  cluster ( p j )=  X  since the documents in the different r-patterns have the different termset. Therefore, we have BasedonthisequationandEq.1,wealsohave 3.2 Rough Threshold Model Up to now, the positive documents in th e training set have been represented as r-patterns. In the topic filtering stage, discovered r-patterns are employed to filter out most irrelevant documents rather than to identify relevant documents.
Formally the relationship between r-patterns and terms can be described as the following association mapping if we consider term frequencies: such that where p i  X  RP is a r-pattern; and w i = f i k We call  X  ( p i )the normal form of r-pattern p i in this paper. The association mapping  X  can derive a function for the weight distribution of terms on T in order to show the importance of terms in the positive documents, which satisfies: for all t  X  T .
 Theorem 3.21 Let RP be the set of discovered r-patterns, then pr  X  is a proba-bility function on T if  X  ( p i ) be the normal form of all r-pattern p i  X  RP . Proof. BasedonEq.3andTheorem3.11,wehave Based on the above discussion, a positive document d i can be described as an event that represents what users want with the probability value. Therefore, the weight of a positive document d i is
To work out the suitable thresholds, it is assumed that document d is irrelevant if it is not closed to the common feature of the topic profiles in the training set. For a given topic, it consists of a set of the positive document, D + .Each document d i has a weight W d i . To capture the common feature of the topic from the training data, the distributions of the document weights for a given topic must be first understood.

Many simplistic models assume normal distribution, that is, the data is sym-metric about the mean. The normal distribution has a skewness of zero. It is reasonable to assume that the scores of the document follow a normally dis-tributed pattern. Using the mean of the Rough Set weights as a threshold would be a good initial choice, because the me an represents the  X  X ommon feature X . According to the statistical approach, if the distributions of the weights of the documents is assumed as a normal distribution then the common feature,  X  j for a topic can be modelled as: where n is the number of the positive documents, n = | D + | .Infact,  X  j is the mean, m , of the probabilities of the positive documents in D + . The thresholds, therefore, can be simply determined as threshold =  X  j .

However, real data points are not always perfectly symmetric. Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable. By observations from the experiments conducted in this study, the distributions of the weights of the documents have exhibited a high degree of skewness. To obtain the  X  X eal X  common feature, both the standard derivation and the skewness must be taken into consideration for modeling the document weights. The following features have been used to characterize a histogram in this paper.  X  is the standard deviation of the probabilities of positive documents. It is given by:  X  is the skewness of the probabilities. The skewness is given by: A linear discriminated function is us ed to make a decision based on features obtained from the above analysis. Therefore, the threshold can be determined as follows: where  X  is an experimental coefficient obtained from specific data sets. It is an empirical value. When the user profiles are specific, a lower value of  X  can be used and allow more documents into th e second stage filtering. Likewise, a higher value of  X  will potentially limit the irrelevant documents into next stage. If adaptive filtering will be the study,  X  can be a dynamic parameter (reducing) as the user profiles b ecome more certain. In the second stage, all the documents (training and testing documents) are split into paragraphs and each paragraph is treated as an individual transaction (a sequence). Each transaction consists of a set of words (terms). So, a given doc-a set of all items -an item is a term which can be a word or keyword in a text document. An itemset is a termset. A sequence S is an ordered list of terms, such pattern of d if there is a S i  X  d such that S + S i .
 covering set of X , which includes all sequences S  X  d such that X  X  S , i.e.,  X  X = {
S | S  X  d, X  X  S } . The absolute support of X is used to indicate the number of occurrences of X in d . It is denoted as supp a ( X )= |{ S | S  X  d, X + S }| .The relative support of X is the fraction of the sequences that contain the termset termset X is to properly estimate the significance of the pattern. The absolute support of X only measures the frequency of X in d .Atermset X with the same frequency will acquire the same support in documents of varying lengths. However, with the same frequency, a termset X is more significant in a short document than in a long one, because we decompose a document into a set of transactions and discover the frequent patterns in them by using data mining methods; the relative support is estimated by dividing the absolute support by the number transactions in a document. Therefore, a termset X can have an adequate support in various document lengths with the same frequency.
A termset X is called a frequent sequential pattern if its relative support supp r ( X ) is greater than or equal to a predefined minimum support, that is, supp r ( X )  X  min sup . The purpose of using min sup in our approach is to reduce the number of patterns discovered in a large document. Otherwise, these patterns with a lower relative support will increase the burden of the training. Removing the less significant patterns will save much computational time without affecting the performance very much.

A frequent sequential pattern X is called a closed sequential pattern if there exists no frequent sequential pattern X such that X  X  X and supp a ( X )= supp a ( X ). The relation  X  represents the strict part of the subsequence relation + . The closed pattern mining can substantially reduce the redundant patterns and increase both efficiency and effectiveness.

The closed sequential patterns discovered from a document collection (a train-ing set) then can be structured into a taxonomy by using the  X  X s-a X  relationship between the patterns. Pattern taxonomy is a tree-like hierarchy that reserves the sub-sequence relationship between disc overed sequential patterns [10]. Pattern taxonomy model (PTM) is a rich semantic representation of users information needs. On the other hand, documents are also represented by frequent sequen-tial patterns. The monotonic reasoning framework is then used to infer if certain documents satisfy the information n eeds of a specific information seeker.
After the pattern taxonomies are obtained, the next step is to develop a method to measure how important patterns are to a given topic, that is, develop a pattern weighting function.

If each mined sequential pattern p in a pattern taxonomy is viewed as a rule p  X  positive , then a value can be assigned to each pattern by exploiting the support or confidence of the patterns within the relevant documents. One of existing pattern weighting functions usi ng the confidence of pattern in [12] is as follow: where D is the training set of documents, and D + is the set of positive document in D .

However, using support or confidence as a significant measurement of the patterns suffers from time-consuming a nd ineffective problems. The measures used by data mining ( X  X upport X  and  X  X onfidences X ) to learn the profile lead to the low frequency patterns problem; thus, it is not suitable in the filtering stage. Patterns have different lengths. Intuitively, the long patterns have more specificity and a lower frequency; the short ones are more general and have a higher frequency. By way of illustration, given a specified topic, a highly frequent pattern (normally a short pattern with a large support) is usually a general pattern or a specific pattern having a low frequency.

Instead of evaluating the pattern X  X  support, we evaluate a term X  X  support (weights) within a discovered pattern and then calculate a specificity value for each pattern. The difference between this term weight method and the IR term-based approaches is that a component of a given term X  X  weighting of the term-based method is based on its appearance in the documents.

Formally, for all positive document d i  X  D + , we first deploy its closed patterns on a common set of terms T in order to obtain the following r-patterns: is the number of closed patterns that contain t i j .
 These r-patterns are composed by usin g an association mapping (see Eq 2). The supports of terms can be calculated by using Eq 3.

After the supports of terms have been computed from the training set, the specific value of a pattern p for the given topic is defined as follows: of pattern p 2 . This property shows that a document should be assigned a large weight if it contains many large patterns. Based on this observation, we will assign the following weight to a document d for ranking documents in the second stage: The Reuters Corpus Volume 1 (RCV1) has b een selected to test the effectiveness of the new two-stage information filtering model. All 100 TREC-11 topics have been used in our experiments. F 1 = 2 PR ( P + R ) matrix is used and the paired two-tailed t-test is applied over all 100 topics on F 1 scores.

The proposed two-stage filtering model (T-SM) integrates two types of filter-ing models: a term-based and a pattern mining-based model. In the first stage, a threshold setting method is developed based on the rough set analysis. In the second stage, a document ranking model is developed based on the pattern tax-onomy model. The major objectives of the experiments are to show how the rough threshold model of the first stage can affect the performance of the two-stage filtering system and how the pattern mining method can help improve the performance in the second stage. Hence, to give a comprehensive investigation for the proposed model, our experiments involve comparing the filtering perfor-mance of the different threshold setting methods and the different combinations of term-based and pattern-based filtering models.
 The Different Threshold Setting Methods. The two-stage models use threshold min developed in [11] and threshold (see Eq. 6)newly developed in this study at the first stage, r espectively. The results on F 1 matrix and p  X  value of t-test display in Table 1. As can be seen from Table 1, the performance of the two-stage model using threshold is better than the two-stage model using threshold min on F 1 over F 1 matrix. For the t-test, the p  X  value is less than 0.0001. This is considered to be extremely statistically significant. With regard-ing to the threshold setting methods, the newly developed threshold setting method significantly outperforms than the threshold setting method developed in [11].
 The Different Types of Two-stage Models. Table 2 illustrates the possible combinations of a term-based model integrated with the pattern-based model or a term-based model integrated with another term-based model, where for the efficiency issue, the model that used fo r the first stage should be a term-based model. In this section, the topic filtering model developed in this study is called a Topic Filtering Model(TFM). This term-based model will be integrated with other term-based model to form a term-based + term-based two stage models. The results shown in Table 3 are the comparisons of T-SM with other possible two-stage models. The BM25 and SVM based term weight methods are used in the first stage for the BM25 + PTM and SVM + PTM models. The results show that T-SM significantly outperforms all other two-stage models. The purpose of the topic filtering stage is to move the  X  X oisy X  and prepare more  X  X lean X  data for pattern-mining stage. As can be seen from the above results TFM can achieve this goal because the design objectives of TFM are different from the traditional filtering models. The TFM has an excellent performance for determining non-relevant information comparing with the traditional models that focus on the performance for determining relevant information. As was mentioned previously, the pattern taxonomy mining is sensitive to the data noise. To deal with this phenomenon, a two-stage theory was put forward. In the proposed two-stage model, the rough threshold model were used to remove the majority of the irrelevant documents in the first stage. The goal of the first stage is to produce a relatively small set of mostly relevant documents as the input for the second stage, pattern taxonomy mining.

In theoretical perspective, any incoming document with a larger probability value than the minimum score of positive documents in the training set should be considered as possibly relevant. However, in real life, user profiles can be very uncertain. Using the minimum positive score as the threshold will allow too many irrelevant documents into the s econd stage. In certain cases, when the user profiles are most specific, using the minimum score as the threshold would be appropriate. However, in most cases, the user profiles are not well defined; therefore, the higher score should be used.

Observe the typical document score distributions shown in Figure 1, where the distributions of the positive documents for Topic 155 in both the training set and positive documents in the training set can only filter out a few testing documents. In the rough threshold model, the mean score of the positive documents in the training set was used as the threshold instead of the minimum score because it describes the common feature of the positive documents in the training set. Also, a significant improvement was ac hieved using the common feature as the threshold.

We also observed that many of the positive training sets X  scores do not strictly follow the normal distributions. Therefore, the mean plus a fraction of the stan-dard deviation and the skew was used for the rough threshold model. Compared with some other possible types of  X  X wo-stage X  models, the experimen-tal results confirm that the proposed two-stage IF model (T-SM) significantly outperforms the other models. The substantial improvement is mainly due to the rough sets based threshold optimization method applied to the first stage and the  X  X emantic X  based patterns mining and matching applied to the second stage. This research work has delivered a very p romising methodology for developing effective and efficient filtering systems ba sed on positive relevance feedback.
