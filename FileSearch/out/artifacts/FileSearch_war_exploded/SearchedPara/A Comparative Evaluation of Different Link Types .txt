 With a growing number of works utilizing link information in enhancing document clustering, it becomes necessary to make a comparative evaluation of the impacts of different link types on document clustering. Various ty pes of links between text documents, including explicit links such as citation links and hyperlinks, implicit links such as co-authorship links, and pseudo links such as content similarity links, convey topic similarity or topic transferring patterns, whic h is very useful for document clustering. In this study, we adopt a Relaxation Labeling (RL)-based clustering algorithm, wh ich employs both content and linkage information, to evaluate the effectiveness of the aforementioned types of links for document clustering on eight datasets. The experimental results show that linkage is quite effective in improving conten t-based document clustering. Furthermore, a series of interes ting findings regarding the impacts of different link types on docum ent clustering are discovered through our experiments.
 H.3.3 [ Information Search and Retrieval ]: Clustering Algorithms, Experimentation, Performance Link-based clustering, Markov Random Field The links between documents are considered as very useful information for text processing because they usually encode external human knowledge bey ond document contents. PageRank [18] and HITS [8] are two very successful models using such linkage information for docum ent importance ranking. Exploiting link information to enhance text classification has been studied extensively in the research community [1] [3] [4] [7] [17]. Most of these studies fall into two frameworks. One is referred to as relaxation labeling (RL) in which the label of a document is determined by both local content and its neighbors X  labels [3]. The other improves classification accuracy by incorporating neighbors X  content information text into the local content. However, Ghani [7] et al . discovered that neighbors X  text content information could be useful only when the neighbor link structure exhibits encyclopedia regularity. Moreover, a growing number of works [10, 15, 16, 20, 21 and 22] used hyperlink information in the clustering of web search results. Whereas these approaches provide valuable insights on employing link information, they all rely on heuristic similarity measures, which linearly combine te xt similarity information with link similarity or co-citation sim ilarity information. However, how to set up the parameter for a linear combination is really data dependent and requires a great deal of tuning. Furthermore, the findings from only clustering web s earch results may not work for other datasets. We adopt a relaxation labeling (RL) based clustering algorithm to evaluate the effects of various link types in document clustering. Relaxation labeling is initially designed to handle link-based text classification. It incorporates both text and link information into a unified probabilistic framework, and has been proved very effective in text classification [1, 3 and 17]. Relaxation labeling requi res some seed documen ts, i.e. documents with labels. In the setting of text classification, training documents can serve as seeds. For document clustering purpose, we use a content-based clustering tool to initialize labels of all documents. We argue that rela xation labeling would iteratively utilize the linkage information to improve the initial clustering. Angelova and Siersdorfer [2] a pplied this method to enhance traditional text clustering and achieved very positive results. However, they only studied the undirected linkage, and they did not consider many other factor s such as different neighborhoods settings, pure-link effects, etc. Moreover, the existing findings should be evaluated on more type s of links. All these reasons encourage us to deeply study the behavior of linkages in text clustering problems within the RL framework. and pseudo links. Explicit links su ch as hypertext and citations usually encode topic transition patterns. Implicit links often indicate the similarity of the corresponding documents. For instance, two documents by the same author should have an implicit link denoting the topic similarity of these two documents. A pseudo link is constructed as long as the content similarity between two documents is over a th reshold. Similarity links as pseudo links have proven to be us eful for text summarization [6], but their effectiveness in the setting of clustering is still unclear. three different types of link in formation including one set from DBLP[1], one in WebKB(http ://www.cs.cmu.edu/~webkb/), two from CORA[14], and four others using pseudo link information (TDT2_10, LATimes10, 20NG, a nd Reuters10). We make the following comparative studies: (1) link-based clustering using explicit, implicit or pseudo links vs. content-based clustering; (2) pure link-based clustering vs. pure content-based clustering; (3) uniform priors vs. empirical prio rs; (4) the effects of different neighborhood; (5) the effects of th resholding and scaling. Our main findings are: (1) link-based clustering performance is significantly better than content-based clustering except for pseudo links; (2) different link type s affect clustering differently; (3) uniform priors is better than em pirical priors for clustering; (4) out-neighbors of a document have more impacts on clustering than in-neighbors; (5) thresholding and scaling have negative or neutral effects on clustering. describes related work. Section 3 shows the proposed method. In section 4, we present and discu ss experimental results. Section 5 concludes the paper. In recent years, link-enhanced text classification and clustering has received more and more attentions from the text mining community. We make a brief review of the methods that integrate linkage between documents. A group of researchers have examined the hyperlink effects in text classification [1, 3 and 17] and text clustering [2] using MRF-RL-based methods. Chakrabati et al [3] found that the method is very effective in improving text classification without using neighbors X  text. Ghani et al. [7] discovered that neighbors X  text is helpful when a document is similar to most of documents it connects to, namely, an  X  X ncyclopedia X  scenario. Later on, MRF-RL-based methods have been applied in [17, 1] and [2] for link-based document classification and cl ustering, respectively. All of them used some heuristics such as thresholding to refine link graphs, making the link pattern closer to the  X  X ncyclopedia X  scenario. However, improperly omitting links between documents may cause serious information loss and thus distort the clustering results. hyperlinks from different angles with regard to text classification. Slattery and Mitchell [19] employ ed FOIL (First Order Inductive Learner), a relational learner to exploit the relational structure of the web, and a Hubs &amp; Authorities style algorithms [9] to exploit the hyperlink topology. Lu and Getoor [13] used aspect models for link-based classification. Cohan and Hoffman [4] applied a factorized model to combine the link model a nd the content model. However, these generative linear models require optimizing the parameters that determine how much links s hould affect clustering, which is very challenging. Moreover, a group of st udies developed some heuristic similarity metrics that linearly combined link information with text information for clustering web search results [10, 15, 16, 20, 21, and 22]. Modha and Spangler [16] proposed an algorithm called TORIC k-means that clusters hypertext doc uments using words, in-links and out-links. Similarly, [25] represents each document using the combination of three vectors: in-link vector, out-link vector and text vector. Each cluster is annotat ed using six information nuggets: summary, breakthrough, review, keyw ords, citation, and reference. In [10], the similarity measure includes three types of information: hyperlink structure, textual in formation and co-citation pattern. Works [22] and [15] combined shortest path and content similarity information to enhance text classifi cation. Moreover, Halkidi et al. [9] claimed that a page X  X  classification is enriched by the detection of its incoming links X  semantics. All these approaches rely on heuristic similarity metrics using both text and link information. As discussed earlier, these approaches are limited to web search results, more data dependent, and requiring a lot of tuning. Therefore, it becomes necessary to make a comprehensive study on how different link types affect te xt clustering within an objective framework. That is why we conduct this study. Pelkovwitz [12] developed an algorithm for labeling a Markov Random Field defined on an arbitrar y finite graph. Later on, it was applied to text classification by Chakrabarti et al [3]. We use the generative model from [3] to de scribe the link-based document e  X  be the directed links from i d to j d ; let } { the entire collection of text corresponding to D . Each sequence of { } k i w i ,.., 2 , 1 | = tokens; let } { assignments for the entire collection D . Assuming that there is a probability distribution for collection D , we choose a class assignment C such that ( ) T G C , | Pr is the maximum. As maximize ( ) ( ) C C T G Pr | , Pr . We believe that the class labe ls of all documents neighboring document i d can form an adequate representation of neighborhood. A usual Bayes classifier is then employed to update a document X  class label based on its immediate neighbors X  class labels such that a i c is chosen to maximize () i i represents all the known class labe ls of the neighbor documents of d . Similarly, since ( ) i N Pr is not a function of Thus, when we assume there is no direct coupling between the texts of a document and the classes of its neighbors, equation (1) can be rewritten as: In equation (2), i N can be further decomposed into in-neighbors 
I and out-neighbors i O . A class prior ( ) i c Pr is the frequency of class i c from content-based clustering results. Notice that not a true prior as there is no true class label for text clustering. So using empirical ( ) i c Pr from the content-based clustering process may distort the clustering results . This problem will be further discussed in the experimental sec tion. Given the class label of the current document, based on Markov network X  X  assumption, all the neighbor class labels (including in-neighbors and out neighbors) are independent of each other (see equation (4)). = From the discussions above, a cla ss label that maximizes equation (5) is chosen for a document: The initial class label assignment is simply the output of a content-based clustering. However, a one step re-estimation that uses equation (5) may not achieve the global optimization. Therefore, each document is iteratively labele d based on its neighbors X  class labels of the previous iteration until there is no change of label assignments or a pre-defined iteration number is reached. Content-based clustering (based on documents X  text content only) is important to link-based clustering since: (1) the initialization of link-based clustering is based on a cont ent-based clustering; (2) using content information to label a document is also part of the probability framework (see term () i i c | Pr  X  in equation (5)). 
Theoretically, any content-based clustering algorithms can be used for the initialization of a link-based document clustering. However, when estimating the probability of term () complete probabilistic approach, it is natural to employ a multinomial model instead of other non-probabilistic methods like spherical k-means.

Recent studies [23, 25] showed that model-based k-means clustering performs slightly wo rse than spherical k-means clustering. In this paper, we compare both schemes in our experiments. As indicated in [ 23, 25], smoothing techniques have a big impact on model-based doc ument clustering algorithms; background collection smoothing out performs Laplacian smoothing because Laplacian is only used for anti zero probability whereas background smoothing considers a wo rd either generated from one of K cluster models ) | ( i ml c w p or a background collection model 
Therefore, we apply backgr ound smoothing techniques to both model-based k-means clustering and link-based k-means clustering. The proposed link-based clustering algorithm is described in figure 2. The entire clustering procedure is as follows. First, we run spherical k-means or model-based k-means until it converges. Then, we take the output class assignments of step 1 as the input of the relaxation labeling process. Ne xt, the class model based on document content is re-estimated. Later on, the class label of each document is re-estimated based on its neighbors X  class labels and its own content using equation (5). Last, the algorithm stops if it reaches a fix point or a pre-defined iteration number, otherwise it repeats model re-estimation and re laxation labeling (step 3 and 4). Neighborhood Definition 
As shown in equation (4), by de fault, the neighborhood of a given document is defined as its immediate in-neighbors and out-neighbors. In practice, the ne ighborhood definition can be more flexible. We may consider a radi us-2 neighborhood, which can also include the neighbors of neighbors of a document. For instance, if we only consider the immediat e out-neighbors of a document, equation (4) will be replaced by equation (8). But if we also include out-neighbors of out-neighbors of a document, then equation (8) can be rewritten as equation (9). We claim that the study the effects of the neighborhood-ranges can give us a global picture of different link structures X  impacts on document clustering. where document i d  X  X  neighbors includes its immediate out-neighbor k d = where document i d  X  X  neighbors includes not only its each immediate out-neighbor k d , but also each immediate out-neighbor d of k d . Algorithm: Link-based K-Means 
Input : dataset } ,..., { 1 n T  X   X  = , and the desired number of clusters k . 
Output : trained cluster models {} k  X   X  ,..., 1 =  X  and the document 
Steps : 1. Repeat content-based cluste ring such as spherical K-Means or model-based K-Means clustering until it reaches a fix point. 2. Initialize document assignment C using output class label assignment from step1. 3. Model re-estimation:  X  = 4. Iteration Labeling using equation (5): where ( ) ) | ( log | Pr 5. Stop until a pre-defined iteration number is reached or if C does not change, otherwise go to step 3 Chakrabarti et al [3] explained that the RL algorithms can find a fix point as long as the initialization confirms the link structure. However, the optimization of an initialization itself should be an active research topic, especially for clustering. In our experiments, our algorithm converged for most of the runs. For some runs, the algorithm did not converge, but the number of changing labels was decreased to a very small digit X  X sually less than 10, after 10 iterations. For clustering docum ents of over 1000, this is acceptable. The WebKB4 dataset contains web pages about university computer science departments. There are around 8,300 documents and they are divided into seven cat egories: student, faculty, staff, course, project, department a nd other. There are around 11,000 hyperlinks between these documents. Among these seven categories, student, faculty, course and project are the four most populous entity-representing categories. The associated subset is typically called WebKB4 (Table 1). For each document, the HTML tags are removed because these tags have negative effects on content-based clustering. Cora [14] is an online archive of computer science research papers. The archive was built automatically using a combination of smart spidering, information extraction, and statistical text classification from online papers in postscript format. These papers are then categorized into a Yahoo-like topic hierarchy with approximately 30,000 papers and over 1 million links to roughly 200,000 distinct documents. We sel ected two subsets of the Cora database: all 7 classes under the machine learning category (CORA7) and all 18 classes under the artificial intelligence category (CORA18) which includes CORA7 (Table 1). Pseudo link-based clustering expe riments are conducted on four datasets: TDT2, LA Times (from TREC), Reuters-21578 and 20-newsgroups (20NG). The TDT2 corpus has 100 document classes, each of which reports a major news event. LA Times news is labeled with 21 unique secti on names, e.g., Financial, Entertainment, Sports, et c. Reuters-21578 contains 21587 documents covering 135 economic subcategories. The 20-Newsgroups dataset is collect ed from 20 different Usenet newsgroups, 1,000 articles from each. We selected 7,094 documents in TDT2 that have a unique class label, 18,547 documents from the top ten sections of the LA Times, 9467 documents from the top 20 categories of Reuters-21578 by excluding documents with multiple labels and all 19,997 documents in 20-newsgroups. Th e ten classes selected from TDT2 are 20001, 20015, 20002, 20013, 20070, 20044, 20076, 20071, 20012, and 20023 . The ten sections selected from LA Times are Entertainment, Financial, Fo reign, Late Final, Letters, Metro, National, Sports, Calendar, and View . The twenty classes from Reuters are the top twenty categories: earn, acq, crude, trade, money-fx, interest, money-supply, ship, sugar, coffee gold, gnp, cpi, cocoa, jobs, coppe r, reserves, grain, alum, and ipi . All 20 classes of 20NG are selected for testing. The DBLP3 [1] dataset includes approximately 16000 scientific publications chosen from the DBLP database including three categories:  X  X atabase X  (DB),  X  X achine Learning X  (ML), and  X  X heory X . These papers are labeled based on the conference where they were published. We use a paper title as document content and co-authorship information as an undirected link between documents (Table 1). Cluster quality is evaluated by three extrinsic measures: F-score [24], purity [24], and normalized mutual information (NMI) [25]. However, because of the space limitation, in some experiments, we only publish the result of the NMI, an increasingly popular measure of cluster quality. NMI is defined as the mutual information between the cluster assignments and a pre-existing labeling of the dataset normalized by the arithmetic mean of the maximum possible entropies of the empirical marginals, i.e. where X is a random variable fo r cluster assignments, Y is a random variable for the pre-existi ng labels on the same data, k is the number of clusters, and c is the number of pre-existing classes. NMI ranges from 0 to 1. The bigger the NMI, the higher the quality of the clustering is. In the following sections, we evaluate the performance of the spherical k-means, model based k-means and link-based k-means. As discussed in [23] [25], s pherical k-means using the TFIDF (Term frequency * inverse documen t frequency) scheme always has the best performance, compared to TF (Term Frequency) and NormTF (normalized term frequency) schemes. It is also slightly better than model-based k-means. Similarly, we find link-based k-means using the outputs of sphe rical k-means TFIDF scheme (spherical link-based k-means) ha s the best performance on most of the datasets. Therefore, if not explicitly mentioned, we use it as the basic scheme. Table 2 shows the explanati ons of these scheme symbols. For example, mk_bkg stands for model-based k-means using multinomial model with backgr ound smoothing. The coefficient of background smoothing is set to 0.5, which means the probability of a word of a given document being generated from one of K class models is 50 % and from a background collection model is also 50 %. In our e xperiments, the coefficient of background smoothing is set to 0. 5, which achieves the best performance for most of the datasets. Since the result of k-Means clustering varies with the initiali zation, we run it ten times with random initializations and take the average as the result. During the comparative experiment, each run has the same initialization values. In the following experimental result tables and figures, the symbols ** and * indicates the change is significant according to the paired-sample T-test at th e level of p&lt;0.01 and p&lt;0.05, respectively. In terms of ne ighborhood settings, unless explicitly mentioned, a document X  X  neighbor hood is its immediate in-and out-neighbors. We use the dragon toolkit [26] to implement the corresponding algorithms and experiments. We choose WebKB4 dataset to exam ine the effects of hyperlinks because it is in fact a vivid web community (computer science departments) where web page s from different classes are interwoven together. As shown in Table 3, the improvement of link-based k-means over spherical k-means is evaluated as significant by three evaluation measures X  X score, Purity and NMI, whereas the improveme nt of mk_bkg_l over mk_bkg is trivial. But, we also notice that the performance of mk_bkg is much better than that of spk_tfidf, and very close to that of spk_tfidf_l. This indicates that the improvement of hyperlink-based k-means over content-based clustering is dependent on the performance of content-based clus tering. Furthermore, this can also be arisen from the fact that the link graph of WebKB4 is sparse (see table 1) and therefore the influence of linkage on clustering is limited. However, the limited links still improves clustering significantly for one clustering method. Hyperlinks contain the most complicated patterns among the three compared link types because there are no strict requirements on the links. Here is a helpful example to explain this. The content of a student X  X  homepage can be very clos e to a professor X  X  if they have similar interests. Based only on c ontent-based clustering, they are in the same cluster such as f aculty cluster. However if the student X  X  homepage also connect s with many more students X  homepages, it can be assigned to student cluster. 
Table 3. WebKB4 Link-based vs. content-based clustering In table 4 and 5, citation li nk-based k-means significantly outperforms spherical k-means clustering and model-based k-means clustering (mk_bkg). For ex ample, spk_tfidf_l has a 15% performance increase over spk_t fidf on CORA7 and a 17% on CORA18. For both datasets, s pherical citation link-based kmeans(spk_tfidf_L) has the best performance. Moreover, note that the performance of mk_bkg is worse than spk_tfidf on both datasets and the same pattern holds for their corresponding link-based k-means. This infers that the improvement is dependent on not only the citation links, but also the output of the corresponding content-based clusteri ng. Compared to hyperlinks, citation links are more helpful in improving content-based clustering performance. This can be resulted from that citation links usually have a stronger indication of relatedness than hyperlinks since a scientific pape r is usually serious about their choice of references and these references are often related to each other. Table 5. CORA18 link-based vs. content-based clustering Table 6 shows the experimental re sults of implicit link-based (co-authorship) clustering. Compared to other link types, co-authorship link-based k-means clustering achieves a very significant improvement over its corresponding content-based clustering. Compared to that of hyperlinks and citation links, the improvement is the biggest. For example, the NMI score dramatically grows from 0.310 (spk_tfidf) to 0.677, a 54% increase. The main reason is that author tends to write papers on related topics and therefore the linkage can be a strong indication of similarity. Similarity links as Pseudo links between sentences was proved to be effective in text summarization [6]. It is interesting to evaluate their impacts on text clustering application. More importantly, the findings can be very indicative for clustering documents without explicit (citation linkage) and im plicit (co-authorship) linkage information. In this experiment, we build similarity links between the text contents (vector of words) of two documents. The threshold is set to 0.4. Although we have tried other thresholds, the results are more or less the same, which indicates only very small amounts of links have effects on clustering. In table 7, we compare si milarity link-based clustering with content-based clustering. Note th at similarity link-based k-means clustering performs very similarly to content-based k-means with slight improvements on the LATimes. This is consistent with our observation that only a small numbe r of documents X  labels were changed for each run on four datasets. Therefore, these label changes do not have a big enough impact on affecting the clustering results. Moreover, pseudo links are based only on content-based similarity measures that do not contain external human knowledge that found in implicit and explicit links. Originally, MRF theory is built on undirected graphs. However, if a directed link is taken as an undir ected link, then this link will be double-counted in the iteration la beling process. Moreover, a document X  X  out-neighbors may ha ve different impacts on clustering than its in-neighbors. Take a citation network as an example, a document X  X  out-neighbor s should be considered more important than its in-neighbors because an author of a scientific theory paper usually cites related theory papers while his or her paper can be cited by other app lied science papers on different topics. Therefore, we differe ntiate a document X  X  out-neighbors from its in-neighbors. Furthermor e, we explore the radius-2 neighborhoods X  effects. We argue that this comparison of different neighborhoods can be ve ry indicative to both text clustering and other related applications. As shown in table 8, we present fifteen different neighborhood settings with I_O as the default setting (See equation (2)). Among these settings, there are three radius-1 neighborhood settings: I, O, I_O; the remaining are radius-2 neighborhood settings. In figure 3, neighborhood settings including the immediate out-neighbors and their out-neighbor expansions (O, OO and I_OO) have a very slightly better perform ance than that of other settings. We also observe that all neighborhood settings are significantly better than spherical k-means using TFIDF scheme (spk_tfidf) and no one setting is significantly better than the others. This can be due to the trade-off between in-and out-neighbors and the relatively sparse connectivity of WEBKB4 dataset. Figure 3: WebKB4: neighborhood effects on link-based clustering. Please refer to table 8 for the neighborhood symbols. Here, we only present the experime ntal results on CORA7 as they are very similar to that of CORA18. In figure 4, observe that with immediate in-neighbors (I) only or immediate out-neighbors (O) only, link-based k-means cluste ring achieves very comparable results to that of I_O. Furthermor e, with O only is better than with I only. One main reason can be that a paper usually cites related papers while it can be cited by many other papers from various topics. We also find that OO, OI, I_OO and I_OI have much better performance than II, IO, II_ O and IO_O. This shows that expanding a document X  X  immediate out-neighbors is more helpful than expanding its in-neighbors for document clustering. Especially, the inclusion of out-neighbors of its immediate in-neighbors (IO, IO_O and IO_OO] is the worst scheme. For other radius-2 settings including II_OO, IO_OO and IO_OI, note that there are compensations between in-neighbors and out-neighbors and the results are comparable to the baseline settings (I_O). Figure 4: CORA7 : neighborhood effects on link-based clustering. Please refer to table 8 for the neighborhood symbols. Since co-authorship graph is an undirected graph, all immediate neighbors of a document are treated as its immediate out-neighbors (O). Similarly, OO is used to represent a document X  X  immediate neighbors and its immediate expansions. In table 9, OO performs worse than O in terms of all three metrics. This indicates that including the radius -2 expansion on a co-authorship graph has no positive impacts. Figure 5 . Uniform priors vs. empirical priors. The dataset name ending with  X  X L_U X  mean s link-based clustering using uniform priors while the dataset name ending with  X  X L_E X  indicates link k-means us ing empirical priors. Theoretically, according to the basic model in section 3, we should use empirical priors ( () i c Pr ) for RL, which are re-calculated iteratively. However, there are no true priors to rely on for text clustering. The initial document labels of link-based k-means are based on content-base d clustering algorithms, which may contain much noise. Thus, empirical priors may hurt the performance of link-based clusteri ng and uniform priors may be a better choice. Therefore, we compare the performance of both priors. In figure 5, for link-based k-means with uniform priors and with empirical priors, there are no significant difference discovered on CORA7 and CORA18. However, with empirical prior, link-based k-means perform s significantly worse than that with uniform priors on WebKB4 (-6 %*) and DBLP3 (-6 %*). This is consistent with our fi ndings during the experiments. For instance, there are some clusters containing no documents for 7 out of 10 runs on WebKB4 datase t. All these findings confirm that uniform priors provide a more stable performance than empirical priors for MRF plus RL in text clustering. Therefore, we employ link-based k-means clus tering using uniform priors for all the other experiments. Departing from equation (5), it shoul d be interesting to study link clustering based on only pure links. In this case, the text factor labeling process. In table 10, th e symbol  X  X pk_tfidf_pl X  means that it uses pure link-based k-means clustering. clustering performs very poorly on the WebKB4 dataset, which indicates that the complicated hyperlinked structure is not good itself for the relaxation labeling pr ocess; the content of web pages appear to be more crucial for hy perlink-based k-means clustering. Pure link-based clustering on CO RA7 and CORA18 achieves very similar performance to content-based clustering. This shows that the more  X  X areful X  citation links, compared to the more  X  X oisy X  hyper links, are good for the global labeling optimization. Pure link-based cluste ring on DBLP3 does have a big improvement over content-based cl ustering (+10% and +35). This implies that co-authorship links ha ve a very strong indication of the similarity of two documents. In this subsection, we evaluate how heuristics such as thresholding and scaling affect link-based docum ent clustering. Thresholding is filtering out links between two docum ents whose similarity value is below a pre-defined threshold. Th e scaling strategy is to scale equation (5) using the similarity score between two documents if there is a link between them: where immediate neighbor document j. The motivation behind these heuristics is to filter out  X  X rrelevant X  links and to emphasize the effects of  X  X seful X  links [1, 2, and 17]. However, we argue that ignor ing certain link information improperly can cause informa tion loss and therefore impose negative impacts to link-based clus tering. Moreover, using heuristic scores such as the weights between two documents to scale the basic model may adversely affect the entire probabilistic model. Figure 6 . Thresholding effects. We set cosine similarity thresholds the link-based clustering will not us e links below the threshold. If then the cosine similarity score will be used to scale the basic model. In Figure 6, we observe a sharp performance decrease when increasing the similarity threshol ds on two citation datasets. The clustering performance drops about 10 percent on both datasets from thresholds 0.0 to 0.4. An even more apparent pattern is observed on DBLP3 dataset. Increas ing the threshold from 0.0 to 0.4 causes a 40% huge drop. This shows thresholding is a bad scheme for citation link and co-aut horship link structures. One main reason is that the citation and co-a uthorship link structures contain very indicative reference relati onships between documents, and therefore thresholding easily leads to a serious information loss. Moreover, observe that thresholdi ng has no significant influence on WebKB4 dataset. These findings strongly indicate that ignoring links has negative or neutral im pacts on document clustering. Figure 7 . Scaling Effects. We set cosine similarity thresholds to 0, 0.1, 0.2, 0.3, 0.4, 0.5 and 0.8 respectively.  X 0 X  means scaling the basic spherical link-based k-means (spk_tfidf_l) without thresholding. If the threshold is set to 0.5, links below 0.5 is filtered out; then the cosine similarity scaling is applied to the remaining links during the re-estimation process (see equation 11). To evaluate the scaling effects (see Figure 7), experiments are conducted both with thresholding (0.1, 0.2, 0.3, 0.4, 0.5, and 0.8) and without thresholding (0). As s hown in table 11, scaling without thresholding has significant negativ e impacts on DBLP3 and neutral effects on the other three datasets. The result of scaling with thresholding is similar with using thresholding only (the performance curves of four datasets are almost the same as those of thresholding experiments (Figure 6)). These results show that scaling is not a good strategy fo r improving link-based clustering. Moreover, thresholding and scaling te nd to exaggerate the impact of the text similarity between docum ents which inevitably hurts the influence of link patterns. Howeve r, thresholding and scaling could be helpful for a not-well-structured  X  X raph X . For example, using co-actor as a link between two movies gives very little indication of the connection between two movies. In f act, this is not an issue of how links affect document clustering, but an issue of how to construct a graph. In this paper, we adopt a RL -based algorithm to examine the impacts of different linkage type s on link-based document clustering. We conduct extensive comparative studies in link-based clustering datasets. In detail, we have th e following interesting findings. First, using explicit or implicit link information, link-based k-means exhibits significant improve ment over spherical k-means and model-based k-means; by the NMI measure, implicit (co-authorship) link-based k-means achieves the best performance with a 54% increase; the performance of those us ing citation links stands in the middle with a 17% increase; the wo rst are those using hyperlinks with an 11% increase. These findings are consistent with those from the comparison between pure link-based k-means and spherical k-means: the performance of the k-means using pure hyperlink, citation link and co-authorship link is inferior, similar and superior to that of pure content-based clus tering, respectively. We also find that link-based k-means using c ontent similarity links performs slightly better, but not significantly better than spherical k-means. These results indicate that the pos itive impacts of links on clustering are affected by the degree of complication of the link patterns. Moreover, it infers that explicit and implicit links are more helpful for clustering documents than similarity links because they encode human knowledge. Another important finding is that using uniform priors is better than using empiri cal priors in improving clustering performance. Furthermore, we disc over that: (1) for citation link and hyperlink structures, immediate out-neighbors of a document are more important than its imme diate in-neighbors in improving clustering performance; (2) expa nding in-neighbors of a document, especially the out-neighbors of its immediate in-neighbors, cause the worse result. Last, thresholding and scaling have very neutral or negative impacts on improving clustering performance. means on more hyperlink datasets and on other types of implicit links such as co-citation, co-conference and so on. This work is supported in part by NSF Career grant (NSF IIS 0448023), NSF CCF 0514679, PA Dept of Health Tobacco Settlement Formula Grant (No. 240205 and No. 240196), and PA Dept of Health Grant (No. 239667). [1] Angelova, R. and Weikum, G. Gra ph-based text classification: [2] Angelova, R. and Siersdorfer, S. A neighborhood-based approach [3] Chakrabarti,S., Dom, B. E. , and Indyk, P. Enhanced hypertext [4] Cohn, D. and Hofmann,T. The missing link -a probabilistic model [5] Eppstein, D. Finding the k shortest paths. In IEEE Symp. On [6] Erkan, G., Radev, D.R.: LexRank: Graph-based Lexical Centrality [7] Ghani, R., Slattery, S. and Ya ng, Y. Hypertext Categorization [8] Kleinberg. J. Authoritative sources in a hyperlinked environment. [9] Halkidi, Ml, Nguyen, B., Varl amis, I., and Vazirgiannis M. [10] He, X., Zha, H, Ding, C. and S imon, H. Web document clustering [11] Lafferty, J., McCallum, A. and Pereira, F. Conditional random [12] Pelkowitz, L. A continuous relaxation labeling algorithm for [13] Lu,Q. and Getoor, L. Li nk-based classification. ICML , 2003. [14] McCallum, A., Nigam, K., Rennie, J. and Seymore, K. A machine [15] Menczer, F. Lexical and Sema ntic Clustering by Web links. [16] Modha, D. S. and Spangler, W. S. 2000. Clustering hypertext with [17] Oh, H.-J., Myaeng, S. H. and L ee, M.-H. A practical hypertext [18] Page, L., Brin,S., Motwani, R ., and Winograd,T. The PageRank [19] Slattery, S. and Mitchell, T. Discovering text set regularities in [20] Strehl, A., Ghosh, J. andMooney, R. J. Impact of similarity [21] Wang, Y. and Kitsuregawa, M. 2002. Evaluating contents-link [22] Weiss, R., Velez, B., Sheldon, M. A. et al. HyPursuit: a [23] Zhang X., Zhou X., and Hu X., Semantic Smoothing for Model-[24] Zhao, Y. and Karypis, G. Cr iterion functions for document [25] Zhou X., Zhang X. and Hu X., Se mantic Smoothing of Document [26] Zhou, X., Zhang, X., and Hu, X., Dragon Toolkit: Incorporating 
