 Personalized PageRank is a standard tool for finding ver-tices in a graph that are most relevant to a query or user. To personalize PageRank, one adjusts node weights or edge weights that determine teleport probabilities and transition probabilities in a random surfer model. There are many fast methods to approximate PageRank when the node weights are personalized; however, personalization based on edge weights has been an open problem since the dawn of per-sonalized PageRank over a decade ago. In this paper, we describe the first fast algorithm for computing PageRank on general graphs when the edge weights are personalized. Our method, which is based on model reduction, outper-forms existing methods by nearly five orders of magnitude . This huge performance gain over previous work allows us  X  for the very first time  X  to solve learning-to-rank prob-lems for edge weight personalization at interactive speeds , a goal that had not previously been achievable for this class of problems.
 H.2.8 [ Database applications ]: Data mining Personalized PageRank; Model Reduction
PageRank was first proposed to rank web pages [13], but the method is now used in a wide variety of applications such as object databases, social networks, and recommendation systems [9, 7, 20, 22]. In the PageRank model, a random walker moves through the nodes in a graph, at each step moving to a new node by transitioning along an edge (with probability  X  ) or by  X  X eleporting X  to a position independent of the previous location (with probability (1  X   X  )). That is, the vector x ( t ) representing the distribution of walker c  X  locations at time t satisfies the discrete-time Markov process where the column-stochastic matrix P represents edge tran-sition probabilities and the vector v represents teleportation probabilities. The PageRank vector x is the stationary vec-tor for this process, i.e. the solution to The entries of x represent the importance of nodes.
Standard PageRank uses just graph topology, but many graphs come with weights on either nodes or edges. For example, we may weight nodes in a web graph by how rel-evant we think they are to a topic [26], or we might assign different weights to different types of relationships between objects in a database [9]. Weighted PageRank methods use node weights to bias the teleport vector v [29, 26, 8] or edge weights to bias the transition matrix P [44, 30, 9, 43]. Through these edge and node weights, PageRank can be personalized to particular users or queries. Concretely, in node-weighted personalized PageRank , we solve where w is a vector of personalization parameters. In edge-weighted personalized PageRank , we solve For example, the personalization vector can specify the topic preference for the query, so the random walker will teleport to nodes associated with preferred topics (node-weighted personalized PageRank) or move through edges associated with preferred topics more frequently (edge-weighted per-sonalized PageRank). The parameters are determined by users, queries, or domain experts [26, 9, 43, 42], or tuned by machine learning methods [34, 3, 7, 20, 21].

Despite the importance of both edge-weighted and node-weighted personalized PageRank, the literature shows an unfortunate dichotomy between the two problems. There exists a plethora of papers and efficient methods for node weight personalization (as we will discuss in Section 2), but not for the much harder problem of edge weight personal-ization! Yet many real applications are in dire need of edge-weighted personalized PageRank. For example, together with machine learning methods, edge-weighted personalized PageRank improves ranking results by incorporating user feedback [34, 7, 20, 21]; but training with existing meth-ods takes hours. In TwitterRank [43], weighting edges by the similarity of user X  X  topical interests gives better results than just weighting the nodes according to those same inter-ests. However, because of their inability to compute edge-weighted personalized PageRank online, the TwitterRank authors recommend simply taking a linear combination of topic-specific PageRank vectors instead of solving the  X  X or-rect X  problem of edge-weighted personalized PageRank.
In early work on the ObjectRank system, the authors mention the flexibility of personalizing edge weights as an advantage of their approach, but they do not describe a method for doing this fast. In subsequent work, the com-putation was accelerated by heuristics [41, 42], but it was still too slow. The later ScaleRank work [28] tried to ad-dress this issue, but only applies to a limited type of graph. Thus, despite its importance and serious attempts to address performance concerns, efficiently computing edge-weighted personalized PageRank remains an open problem.

In this paper, we introduce the first truly fast method to compute x ( w ) in the edge-weighted personalized PageR-ank case. Our method is based on a novel model reduction strategy, where a reduced model is constructed offline, but it allows fast evaluation online. The speed of our new ap-proach enables exciting interactive applications that have previously been impossible; for example, in one application described in Section 5, the reduced model is nearly five or-ders of magnitude faster than state-of-the-art algorithms. This is very important for standard algorithms with cost proportional to the graph size, but reduced models also speed up sublinear approximation algorithms for localized PageRank problems. Thus a variety of applications can ben-efit from the techniques that we describe.

We discuss some preliminaries in Section 2, then turn to the three main contributions of the paper in the following sections. In Section 3, we describe the first scalable method for edge-weighted personalized PageRank by applying the idea of model reduction . We also show how to use common, simple parameterizations to make our methods even faster, and we reason about cost/accuracy tradeoffs in model reduc-tion for PageRank. In Section 4, we show how fast reduced models enable new applications, such as interactive learning to rank, that were not previously possible. And in a thor-ough experimental evaluation in Section 5, we show that our method outperforms existing work by nearly five orders of magnitude . Finally, we discuss related work in Section 6, and make some concluding remarks in Section 7.
In standard (weighted) PageRank, we consider a random walk on a graph with weighted adjacency matrix A  X  R n  X  n in which a nonzero entry A ji &gt; 0 represents the weight on an edge from node j to node i . The weighted out-degree of node j is d i = P j A ji , and we define the degree matrix D to be a diagonal matrix with diagonal entries d j . Assuming no sinks (nodes with zero outgoing weight), the transition matrix for a random walk on this weighted graph is P = AD  X  1 the graph has sinks, one must define an alternative for what happens after a random walker enters a sink. For example, the walker might either stay put or teleport to some node in the graph chosen from an appropriate distribution.

When v is a dense vector (the global PageRank problem), the standard PageRank algorithm is the iteration (1), which can be interpreted as a random walk, a power iteration for an eigenvalue problem, or a Richardson or Jacobi iteration for (2) [22]. One can use more sophisticated iterative meth-ods [32, 10], but (1) converges fast enough for many offline applications [22]. All these repeatedly multiply the matrix P by vectors, and therefore have arithmetic cost (and memory traffic) proportional to the number of edges in the graph. Hence, these methods are ill-suited to interactive applica-tions for large graphs [8].

There are two standard methods to approximate PageR-ank online in time sublinear in the number of edges. The first case is when the teleportation vector v can be well ap-proximated as a linear combination of a fixed set of reference distributions, i.e. v  X  V w where V  X  R n  X  d and w  X  R d see [29, 26]. For example, in web ranking, the j th column of V may indicate the importance of different nodes as author-ities to a reference topic j , and the weight vector w indicates how interesting each topic is to a particular ranking task. In this case, the solution to the PageRank problem is If M  X  1 V is precomputed offline by solving d ordinary PageR-ank problems, then any element of x ( w ) can be reconstructed in O ( d ) time, while the entire PageRank vector can be com-puted in O ( nd ) time. Because some nodes are unimportant to almost any topic, it may be unnecessary to compute every element of the PageRank vector.

We also can estimate PageRank in sublinear time when the restart vector v is sparse. In a common case that we re-fer to as localized PageRank , the vector v is one for an entry corresponding to a particular vertex and zero in all other components. Since the random surfer often returns to this vertex, the localized PageRank vector reveals properties of the region around this vertex. The Monte Carlo method [8] and Bookmark-Coloring Algorithm (BCA) [11, 4] estimate the PageRank values for the important nodes close to the target vertex, which are often the most important compo-nents of the PageRank vector in recommendation systems. Though these PageRank values can be computed with sub-second latency for a specific set of parameters, in an appli-cation requiring frequent recomputation for different users and queries, the total latency may still be significant.
Unfortunately, none of these methods apply to fast com-putation of edge-weighted personalized PageRank. We will discuss how to quickly approximate edge-weighted personal-ized PageRank via model reduction in the following section.
Model reduction is a general approach to approximating solutions of large linear or nonlinear systems. Applied to PageRank, the key steps in model reduction are: 1. Observe that x ( w ) often lies close to a low-dimensional 2. To pick an approximation in a k -dimensional reduced 3. After we solve the reduced system, we reconstruct the In the experiments in Section 5, we achieved good accuracy with reduced spaces of dimension k  X  100.
The basic assumption in our methods is that for each pa-rameter vector w  X  R d , the PageRank vector x ( w ) is well approximated by some element in a low-dimensional linear space (the trial space). We construct the trial space in two steps. First, we compute PageRank vectors { x ( j ) a sample set of parameters { w ( j ) } r j =1 . Then we find a ba-sis for a k -dimensional reduced space U (typically k &lt; r ) containing good approximations of each sample x ( j ) .
In our work, we choose the sample parameters w ( j ) uni-formly at random from the parameter space. This works well for many examples, though a low-discrepancy sequence [33] might be preferable if the sample size r is small. Sparse grids [14] or adaptive sampling [35] might yield even better results, and we leave these to future work.

Once we have computed the { x ( j ) } , we construct the re-duced space U by the proper orthogonal decomposition (POD) method, also known as principal components analysis (PCA) or a truncated Karhunen-Lo`eve (K-L) expansion [37, 36]. We form the data matrix X = [ x (1) ,x (2) ,  X  X  X  x ( r ) and compute the  X  X conomy X  SVD where U X  X  R n  X  r and V X  X  R r  X  r are matrices with or-thonormal columns and  X  is a diagonal matrix with en-tries  X  1  X   X  X  X   X   X  r  X  0. The best k -dimensional space for approximating X under the 2-norm error is the range of U = u X 1 ... u X k . The singular value  X  k +1 bounds space is probably adequate; if  X  k +1 is large, the trial space may not yield good approximations.
Given a trial space and an online query parameter w , we want an element of the trial space that approximates x ( w ). That is, if U = [ u 1 ,u 2 ,  X  X  X  ,u k ]  X  R n  X  k , we want an approx-imate PageRank vector of the form  X  x ( w ) = Uy ( w ) for some y ( w )  X  R k . All of the methods we describe can be expressed in the framework of Petrov-Galerkin methods; that is, we choose y ( w ) to satisfy for some set of test functions W . We can also choose y ( w ) to force the entries of  X  x ( w ) to sum to one, i.e. minimize k W T [ M ( w ) Uy ( w )  X  b ] k 2 s.t. X we solve this constrained linear least squares problem by standard methods [23, Chapter 6.2]. We consider two meth-ods of choosing an approximate solution: Bubnov-Galerkin and the Discrete Empirical Interpolation Method (DEIM) .
In the Bubnov-Galerkin approach, we choose W = U ; that is, the trial space and the test space are the same. Because U is a fixed matrix, we can precompute the system matrices that arise from the Bubnov-Galerkin approach for simple edge weight parameterizations. However, this approach is expensive for more complicated problems.

In the DEIM approach, we enforce a subset of the equa-tions. That is, we choose y to mimimize the norm of a projected residual error: where  X  is a diagonal matrix that selects entries from some index set I ; that is, Equivalently, we write the objective in the minimization problem as where M I , : ( w ) is the submatrix of M ( w ) involving only those rows in the index set I , and similarly with b |I| = k , we enforce k of the equations in the PageRank sys-tem; otherwise, for |I| &gt; k , we enforce equations in a least squares sense.

Minimizing this objective is equivalent to Petrov-Galerkin projection with W =  X  M ( w ) U . Because W has few nonzero rows, only a few rows of M ( w ) need ever be materialized, even when the parameterization is complicated. The key to this approach is a proper choice of the index set I of equations to be enforced.

Once the reduced system is solved and the coordinates in the reduced space y ( w ) are located, we can reconstruct the approximate PageRank vector in the original space by  X  x ( w ) = Uy ( w ). This can be slow when the graph has mil-lions of nodes, as it requires O ( kn ) work. In practice, we usually do not require the whole PageRank vector, but only the PageRank values for a subset of the vertices, such as the nodes with high PageRank values, or the nodes associated with some keywords. Assuming the subset of vertices we are interested in is S , we can compute the PageRank values for the vertices in S by  X  x S ( w ) = U S , :  X  y ( w ). The computation can be done with only O ( k |S| ) work.
How P ( w ) depends on w matters to how fast we can make different model reduction methods. For example, suppose P ( w ) = A ( w ) D ( w )  X  1 , where the weighted adjacency matrix A ( w ) has edge weights that are general nonlinear functions in w and D ( w ) is the diagonal matrix of out-degree weights, as before. The DEIM approach would require that we com-pute the weight for all edges in E 0 = { ( i,j )  X  E : j  X  X } ; and also all the edges E 00 = { ( i,j 0 )  X  E :  X  ( i,j )  X  E to normalize. The expense of the Bubnov-Galerkin method in this case would be even worse: to form U T M ( w ) U we would need to multiply P ( w ) by each of the k columns of U , at a cost comparable to running the PageRank iteration to convergence. We therefore consider two simple parameteri-zations for which the Bubnov-Galerkin and DEIM methods can be made more efficient.
 In a linear parameterization, we assume where each P ( i ) is column stochastic and the weights w non-negative and sum to one. For a linear parameterization, the Bubnov-Galerkin approach is fast, since and the (small) matrices U T U and U T P ( s ) U can be precom-puted. The linear parameterization has been used several times in settings in which each edge has one of m different types [9, 42, 34]. In such settings, the parameterization cor-responds to a generalized random walker model in which the walker first decides what type of edge to follow with proba-bilities given by the w s , then decides between edges of that type. However, the model is limited in that it does not al-low the probability of picking a particular edge type to vary across nodes.

In a scaled linear parameterization, P ( w ) = A ( w ) D ( w ) where the weighted adjacency A ( w ) is and D ( w ) is the diagonal matrix of outgoing weights The scaled linear parameterization corresponds to choosing each edge weight A ji ( w ) as a linear combination of edge features A ( s ) ji . For example, in a social network, the edge weights might depend on features like communication fre-quency, profile similarity, and the time since last communi-cation [7]. Post topic similarity between users has also been adopted as an edge feature for sites such as Twitter [43]. Be-cause of the normalization, M ( w ) in the scaled linear case is not a linear function of w , and so the Bubnov-Galerkin system cannot be written as a linear combination of pre-computed matrices. For DEIM in the scaled linear case, however, we can materialize only the subset of rows of A ( w ) with indices in I , since the out-degree weight vector needed for normalization is a linear combination of the weight vec-tors d ( i ) , which we can precompute.
As shown in Appendix A, if the columns of U are normal-ized to unit 1-norm, the key quantity in the error bound for DEIM is k ( M I , : U )  X  k 1 . We do not want interpolation nodes that are always unimportant, since in this case M I , : U will have small elements, and this might suggest that we should choose  X  X mportant X  nodes according to the average of some randomly sampled PageRank vectors. However, this heuris-tic sometimes fails, as M I , : U can be nearly singular even if the elements of M I , : U are not nearly zero.

In the case |I| = k , we want rows M I , : U that are maxi-mally linearly independent. While an optimal choice is hard in general, this type of subset selection problem is stan-dard in linear algebra, and a standard approach to select-ing the most important rows is to apply the pivoted QR algorithm and use the pivot order as a ranking [23, Chap-ter 5]. However, if we were to apply pivoted QR to the rows of M ( w ) U in order to choose the interpolation set, we would first have to compute M ( w ) U , which is again comparable in cost to computing PageRank directly. As an alternative, we evaluate M (  X  w ( j ) ) U for one or more test parameters  X  w ( j ) , then apply pivoted QR to the rows of Z = M (  X  w (1) ) U ...M (  X  w ( q ) ) U . By using more than one test parameter in the selection process, not only do we en-sure that we are taking into account the behavior of M at more than one point, but we can choose as many as kq in-dependent rows of the matrix Z .
 Algorithm 1 Row subset selection via QR In: Z : n  X  m matrix Out: I : list of m indices def Find-I ( Z )
Initialize I  X   X  , Q  X  0 n  X  m , r  X  0 m norm2  X  squared norms of rows of Z for k = 1 to m do end for return I
For scaled linear and nonlinear parameterizations, the cost of forming the Bubnov-Galerkin matrix U T M ( w ) U is linear in the size of the graph, and may even exceed the cost of an ordinary PageRank computation. In contrast, for DEIM methods, the cost of forming the reduced system is propor-tional to the number of incoming edges for the nodes with indices in I (in the scaled linear case) or those nodes plus all outgoing edges from those nodes (in the fully nonlinear case). Hence, there is a performance advantage to choosing low-degree nodes for the interpolation set I . At the same time, choosing nodes with high in-degree for the set I may improve the accuracy of the reconstruction.

Note that the pivoted QR algorithm defines  X  X tility X  for the nodes, and it chooses the nodes with the highest util-ities. The utility for vertex i is p norm2( i ) described in Algorithm 1. We write the utility of vertex i as util( i ) and the incoming degree as cost( i ). We propose two methods to manage the trade-off between util( i ) and cost( i ): the cost-bounded pivot method and the threshold pivot method.
In the cost-bounded pivot method, a parameter C indi-cates the average cost we are willing to pay for each node. A straightforward heuristic would be choosing the nodes with highest util( i ) with cost( i )  X  C . However, this heuristic ig-nores all the nodes that exceed the cost budget. We can soften the budget constraint by choosing the nodes with highest util( i ) / max(cost( i ) ,C ). Alternatively, in the thresh-old pivot method, we require the node selected in each step to have utility higher than max i util( i ) /F . Among these nodes, the one with smallest cost is selected. The param-eter F indicates the maximum ratio allowed between the highest utility and the utility of the selected node in each step. We will discuss the trade-offs of the two methods in the experiment section.
We now describe an application enabled by fast PageR-ank approximations: learning to rank. In learning to rank, the goal is to learn the best values of the parameters that determine edge weights, based on training data such as user feedback or historic activities, as discussed in [7, 3]. Using surrogates, learning to rank becomes not only an interesting offline computation, but an online computation that could be done on a per-query basis.

As a concrete example, consider training data T = { ( i q where each pair ( i,j )  X  T indicates that i should be ranked higher than j . The optimization problem is where x i ( w ) and x j ( w ) indicate the PageRank of nodes i and j for a parameter vector w . The loss l (  X  ) penalizes violations of the ranking preference; popular choices include squared loss and Huber loss [7]. To minimize L by gradient-based methods, one needs both L and the derivatives To compute the derivatives of x , one uses the relation that is, each partial derivative requires solving a linear sys-tem involving the PageRank matrix M ( w ).

We replace the PageRank vector x ( w ) with an approxi-mation  X  x ( w ) = Uy ( w ), and seek to minimize the modified objective We write the component differences  X  x i ( w )  X   X  x j ( w ) as z where z ij = U i, :  X  U j, : is the difference between two rows of U . The derivatives of  X  L are then Depending on the size of the training set, one might form the z ij vectors at the start of the optimization.

Once y ( w ) has been evaluated for a given parameter vec-tor w , the subsequent derivative computations can re-use factorizations, and so require less computation. For exam-ple, in the case of a linear parameterization and the Bubnov-Galerkin method, the derivatives of y satisfy where  X  M ( w ) = U T M ( w ) U is the same matrix that appears in the linear system for y ( w ), while  X  P ( s ) = U T P of the matrices that was precomputed in the offline phase. Though computing y ( w ) requires O ( k 3 ) work to factor after it has been formed, the derivative computations can re-use this work, and involve only O ( k 2 ) additional work to form the right hand side and solve the resulting system.
For a scaled linear parameterization or nonlinear param-eterization with the DEIM method, y satisfies the normal equations where  X  M = M I , : U . Differentiating (10) gives the formula where  X  r =  X  My  X  b I is the residual in the reduced least squares problem. Once we have computed  X  M and its deriva-tives, computing y takes O ( k 2 |I| ) time, while additional solves to evaluate partial derivatives take O ( k |I| ) time. In many graphs, though, the dominant cost is the time to form  X  M and its derivatives.
Our experimental evaluation has three goals: First, we want to show that global edge-weighted personalized PageR-ank can be computed interactively with model reduction . As reported in Section 5.3, our model reduction methods can answer such queries in half a second, while the standard power method takes 20 seconds (DBLP graph) to 1 minute (Weibo graph). Second, we want to verify that model reduc-tion improves the performance of localized PageRank . As reported in Section 5.4, our methods are usually 1-2 orders of magnitudes faster than BCA. Third, we want to demon-strate that model reduction enables learning-to-rank at in-teractive speeds . As we report in Section 5.5, our model reduction methods are up to nearly five orders of magni-tudes faster than the full computation on a learning-to-rank application. We discuss our experimental setup and prepro-cessing in Sections 5.1 and 5.2. Environment. We run all experiments on a computer with two Intel Xeon X5650 2.67GHz processors and 48GB RAM. We have implemented most of our methods in C++ using Armadillo [38] for matrix computations. For the LP solver required by the ScaleRank algorithm, we follow the origi-nal implementation and use the GLPK package [1]. A few routines are currently prototyped with SciPy [2].
 Datasets. We run experiments on two graphs: the DBLP citation graph and the Weibo social graph. Table 1 shows their basic statistics. The DBLP graph, provided by Arnet-Miner [40], has four types of vertices (paper, author, con-ference and venue) and seven edge types. We adopt the linear parameterization used in ObjectRank to assign edge weights [9, 28]. This graph is used for global PageRank and parameter learning.

The Weibo graph, released as part of the KDD Cup 2012 competition, 1 is a microblog dataset with subscription edges between users and text. In prior work [43], personalized PageRank with edge weights derived from topic analysis over user posts was used to rank topical authorities. In our ex-periments, we apply LDA to analyze the topics represented in user posts, following the approach in [12], with the num-ber of topics set to 5, 10, and 20. We use both scaled-linear and linear parameterizations based on the user topic distri-butions. For the scaled-linear parameterization, we adopt a weighted cosine as the edge weight: where  X  i denotes the topic distribution for user i and A (  X  i ) s  X  (  X  j ) s . We can also normalize the weighted adjacency matrix A ( s ) for a linear parameterization: We use the Weibo graph in experiments for both global and localized PageRank. For localized PageRank, we run the experiments on 1000 randomly selected nodes, as different nodes can have slightly different running time and accuracy for both our methods and the baseline.
 Baselines. We compare our methods to ScaleRank [28] for global PageRank and the Bookmark Coloring Algorithm https://www.kddcup2012.org/c/kddcup2012-track1 (BCA) [11, 4] for localized PageRank. We discuss these methods in more detail in Section 6. For global PageRank, ScaleRank [28] is the only previous work we know to effi-ciently compute edge-weighted personalized PageRank. Like our approach, ScaleRank has an offline phase that samples the parameter space, and an online phase in which queries are processed interactively. We only report the performance of ScaleRank on the DBLP graph, as it is ill-suited for gen-eral graphs. We set the parameters according to the original paper. For localized PageRank, prior methods approximate the PageRank of the top-k nodes in sub-second time without preprocessing. We compare our model reduction methods with a variant of BCA proposed in [4]. For the BCA ter-mination threshold , which controls the trade-off between running time and accuracy, we use = 10  X  8 ; this leads to Kendall  X  values of around 0 . 01.
 Metrics. We evaluate both how well our approximate PageR-ank values match the true PageRank values and the distance between the induced rankings. We denote the exact and ap-proximate PageRank vectors as x and  X  x , respectively. We measure accuracy of the approximate PageRank values by the normalized L1 distance. The normalized L1 distance on the evaluation set S is defined as For global PageRank, S contains all the nodes, while for lo-calized PageRank, S is the exact top-100 set. To evaluate ranking, we adopt Kendall X  X   X  , the de facto standard for ranking measurement [31]. Denote the number of concor-dant pairs and discordant pairs as n C and n D , respectively; the normalized Kendall distance is defined as the percentage of pairs that are out of order, i.e. More specifically, we compute  X  0 over the union of the exact and the approximated top-100 sets.

For both metrics, we report average results over 100 ran-dom test parameters. Reduced Space Construction. For all experiments, we construct the reduced space from PageRank vectors com-puted at 1000 uniformly sampled parameters.

We report the singular values of the global PageRank data matrix (see Section 3) for both DBLP and the Weibo graph in Figure 1(a). We examine the singular values of the local-ized PageRank data matrix for 10 randomly selected nodes and report them in Figures 1(b) and 1(c). For the scaled-linear parameterization, we found that for a given number of parameters, the singular values for most of our sample nodes decay at a similar rate, though one or two nodes show singular values with slower decay. We show the singular val-ues for two representative nodes, denoted as v 4 and v 7 Figure 1(b). For the linear parameterization, the singular values for all the nodes decay at a similar speed; thus we pick one node to report in Figure 1(c). Unsurprisingly, the singular values decay more slowly when there are more pa-Table 2: Preprocessing Time on Different Datasets Procedure DBLP Weibo-G Weibo-L Prepare Samples 6 hours 17 hours 3-7 min Get Basis U 0.8 hours 0.4 hours 1-2 min
Choose I 4 11 min 12-18min &lt; 1 min rameters on the Weibo graph. With the same number of parameters, the singular values decay more rapidly for the scaled-linear than for the linear parameterization.
For most experiments, we set the reduced dimension to k = 100, as the singular values either decay slowly after this point, or they are already small enough. The only exception is localized PageRank with the scaled-linear parameteriza-tion, where we set k = 50 for 5 and 10 parameters, as the singular values are already quite small there.
 Interpolation Set Selection. We use the pivoted QR method discussed in Section 3 in our experiments. For graphs with skewed incoming degree distributions, we need to restrict the total number of incoming edges for the nodes in I for performance. For both cost-bounded pivot and threshold pivot methods, we compute the interpolation set with different values for the parameter C or F , and select the I that works best on a small validation set of personalized PageRank parameters 2 . As the incoming degree distribution for DBLP is not heavy-tailed, we use the unconstrained piv-oted QR method. For the Weibo graph, we use cost-bounded pivot with C = 100 and threshold pivot with F = 10 for lo-calized PageRank, and cost-bounded pivot with C = 1000 for global PageRank.

With |I| = k , the reduced system is nearly singular for some parameters. We can avoid this issue by slightly in-creasing the size of |I| (i.e. 1 . 2 k ). This suggests that there is no single best interpolation set with size k for all param-eters, and the more robust choice for DEIM is to choose |I| &gt; k . Further increasing the size of the interpolation set in DEIM produces more accurate results, with some increase in running time. We set |I| = 2 k for all the experiments; the extra cost over choosing k nodes is modest, and we usually see little improvement after this point.
 Preprocessing Time. We show preprocessing times in Ta-ble 2. For global PageRank, we compute sample PageRank vectors using the standard power method with stopping cri-we use BCA with = 10  X  9 ; in this case, the preprocessing time slightly varies with different numbers of parameters and parameterization forms. In general, global PageRank takes hours of CPU time for preparation while localized PageR-ank takes minutes. In either case, the computation can be easily parallelized across samples.
In this section, we discuss the results for global PageR-ank. The standard power iteration is ill-suited for interac-tive global PageRank  X  it takes about 20 seconds on the DBLP graph and a minute on the Weibo graph.
 DBLP Graph. We conduct experiments on the DBLP graph to demonstrate the performance of model reduction
We try C = 10 0 , 10 1 ,..., 10 5 and F = 5 , 10 , 20 , 40 , 80.
Used by Bubnov-Galerkin.
Used by DEIM. methods for linearly parameterized problems. Figure 2 and Figure 3 show the running time and accuracy of different methods. The running time has two parts: finding the co-efficients in the reduced space, and constructing the PageR-ank vector as a linear combination of reduced basis vectors. Both model reduction approaches are accurate. For DEIM with |I| = 200, the Kendall distance for the top 100 results is around 3  X  10  X  5 , while the normalized L1 distance is around 0 . 0005. The Bubnov-Galerkin method has even lower error. In contrast, ScaleRank has both Kendall distance and nor-malized L1 distance around 0 . 07.

All the methods produce results with interactive latency, and the model reduction methods run slightly faster than ScaleRank. Most of the time for model reduction is spent on PageRank vector construction. As discussed in Section 3.2, this time can be largely reduced by materializing the PageR-ank values only for a subset of the nodes. In contrast, the ScaleRank method spends much more time in obtaining the coefficients, and this cost cannot be easily reduced. This is because ScaleRank requires solving several linear programs and is much slower than solving a linear system.
 Weibo Graph. We run experiments on the Weibo graph to see how model reduction methods work for social graphs with scaled-linear parameterization. We use DEIM in this case, since the Bubnov-Galerkin method is slow for scaled-linear parameterization. Because the DEIM approximation vector usually does not sum to one for this problem, we add the constraint discussed in Section 3. In Figure 5(a) and Figure 5(b), we report the running time and accuracy for different numbers of parameters. The reduced model can an-swer queries with interactive latency, and the running time would be much less if we only reconstructed the PageRank values for a small subset of vertices. The error increases with the number of parameters, as increasing the number of parameters increases the error intrinsic in approximating so-lutions from a low-dimensional space. The Kendall distance is about 0 . 005 with 10 parameters, but it increases to about 0 . 013 when there are 20 parameters.
In this section, we discuss localized PageRank on the Weibo graph. Due to space limitations, we only report the Kendall distance for accuracy, but the approximations computed by model reduction always have much lower L1 distance than BCA. With fewer parameters, we see better performance for our methods. Thus we only report the result with 10 and 20 parameters, and not the easier case with 5 parameters.
In most applications of localized PageRank, we are only interested in a relatively small number of top-ranked nodes. To find the top 100 nodes, it generally suffices to compute PageRank values for the most promising 10000 nodes as in-dicated by average PageRank values over the sampled pa-rameters. To construct the part of the PageRank vector associated with these 10000 nodes requires less than a mil-lisecond. The running time for model reduction method reported in this section assumes constructing the PageRank values only for top vertices.
 Linear Parameterization. For a linear parameterization, the only work in forming the Bubnov-Galerkin system is adding m matrices of size k  X  k . In our experiments, with k = 100 and m = 20, we are able to form and solve the reduced system within a millisecond.
 For the problem to retrieve top 100 nodes, the Bubnov-Galerkin approach is nearly two orders of magnitudes faster than BCA, with better accuracy. We report the running time and accuracy for m = 20 in Figure 6(a) and 7(a). The Bubnov-Galerkin method has much better accuracy with m = 5 , 10 than for m = 20. For m = 20, the error in-creases as the 100-dimensional trial space is too small to contain highly accurate approximations to the PageRank vector. Still, the Bubnov-Galerkin method produces more accurate results compared with BCA.
 Scaled-Linear Parameterization. For the scaled-linear parameterization, the Bubnov-Galerkin approach is expen-sive, and we turn instead to DEIM. The running time and accuracy for m = 10 are reported in Figures 6(b) and 7(b). Overall, DEIM answers the query nearly one order of mag-nitude faster than BCA with better accuracy.
 We show the running time and accuracy for m = 20 in Figures 6(c) and 7(c). DEIM is much slower for m = 20 than for m = 10 for two reasons. First, because the edge weight computation is proportional to the number of parameters, it Figure 2: Running Time on DBLP Graph takes more time to form M I , : ( w ). Second, we need a larger reduced space ( k = 100) to achieve acceptable accuracy.
In all cases, while the interpolation sets selected by the cost-bounded pivot method result in shorter running time, it produces less accurate results for a tiny fraction of outlier nodes. The threshold pivot method selects the interpolation sets in a more robust way, but it also requires longer time for each PageRank computation.
In this section, we demonstrate that parameter learning for edge-weighted personalized PageRank can be done at interactive rates via model reduction. For our experiments, we considered the DBLP graph with linear parameterization over the edge types, similar to the setting of PopRank [34]. A partial ranking list with eight papers was used in our ex-periments to represent the feedback received from the user. Although the original PopRank model is based on a non-differentiable objective function, we use a differentiable ob-jective suggested in later work [3, 7] as it achieves the same goal with much less training time. Following the parameter learning framework described in Section 4, we choose the squared loss with margin b = 0 . 2 and set  X  = 1000 for the regularizer. That is, we minimize the loss function Here T is the set of pairwise ranking preferences based on the partial ranking list, and w 0 is the default parameter (0 . 34 , 0 . 33 , 1 . 0 , 0 . 33 , 0 . 75 , 0 . 25 , 1 . 0).
Figure 4 shows the value of the objective function after each iteration with different methods, and all the methods result in almost the same objective function value. As ex-pected, the parameters after each iteration with different methods are also very similar.
 The average running time for each iteration is reported in Table 3. For applications involving interactive learning, user feedback usually just slightly adjusts the parameters, and 5-10 optimization iterations should be enough to produce the adjusted parameters to reflect the user X  X  personal preference. In fact, as shown in Figure 4, the objective function value does not change much after eight iterations. Assuming ten iterations are required to produce the new ranking result af-ter taking user feedback, for linear parameterization, we are able to finish the learning procedure in 0.02 seconds by the Bubnov-Galerkin method. Otherwise, DEIM is required to give the adjusted ranking within a second. In comparison, the original method takes minutes for each optimization it-eration, as a separate PageRank computation is required for the objective and for each component of the gradient. Table 3: Avg. Running Time per Opt. Iteration
Time(sec) 159.3 0.002 0.033 Personalized PageRank Computation. There has been a lot of work on fast personalized PageRank computation. A recent review can be found in [8]. However, as discussed in Section 2, most previous work focuses on node-weighted personalized PageRank and does not apply to edge-weighted personalized PageRank.

To the best of our knowledge, ScaleRank [28] is the only other work that answers edge-weighted personalized PageR-ank queries interactively. Like our approach, this method computes sample PageRank vectors { x ( j ) } r j =1 in the offline phase, and the approximation to the online query is a linear combination of these samples. However, extracting the lin-ear combination in ScaleRank is slower, as a series of linear programming problems must be solved. It is also unclear how to support learning-to-rank application in this frame-work. Furthermore, it can only find approximate solutions efficiently on typed graphs, as general graphs would intro-duce too many constraints in the linear programs.

For localized PageRank, the Monte Carlo method [8] and the Bookmark Coloring Algorithm (BCA) [4] have been pro-posed to efficiently find the approximate PageRank vector without preprocessing. These methods only explore a local-ized region close to the target vertex, as the nodes far from the target node usually have negligible PageRank value. The Monte Carlo method simply simulates random walks start-ing from the target node. BCA maintains a running estimate of the PageRank vector and the residual vector, and repeat-edly picks a node and  X  X ushes X  its residual to the outgoing nodes until all the residuals are smaller than a predefined threshold.
 Surrogate Model. Another approach is to approximate x ( w ) = M ( w )  X  1 b by a fast surrogate model  X  x ( w )  X  x ( w ). One could build surrogates by interpolation or regression, whether via polynomials, radial basis functions, Gaussian processes, or neural networks. In these methods, PageRank is a black box: the methods sample the PageRank vector during model construction, but do not use the structure of the underlying linear system during model evaluation. An example of this approach is [16, 17, 18, 19], in which high-degree polynomial interpolation is used to approximate the dependence of the PageRank vector on the teleportation pa-rameter  X  . In contrast, we pursue a model reduction strategy that uses the PageRank equations.
 Model Reduction in Simulation. In physical simula-tions, one uses model reduction for tasks that involve re-peated model evaluation, such as design, optimization, con-trol, and uncertainty quantification. Our work is inspired by the work of Patera and co-workers on the use of reduced ba-sis methods for elliptic PDEs with uncertain (or stochastic) coefficients [24], and by work on model reduction of dynam-ical systems in the time or frequency domain [5, 39, 15, 6].
In this paper, we present the first general scalable method for edge-weighted personalized PageRank based on model re-duction. We discuss optimizations for common parameter-izations and cost/accuracy tradeoffs when applying model reduction to power-law graphs. For applications such as learning to rank, our model reduction methods are nearly five orders of magnitudes faster than the standard approach.
In future work, we plan to investigate whether the PageR-ank computations can be pushed to the client side by sending the reduced model. We would also like to investigate how to update the reduced model efficiently as the graph evolves over time.
 Acknowledgements. This work was funded by NSF Grants IIS-0911036 and IIS-1012593, and by the iAd Project from the National Research Council of Norway. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.
Petrov-Galerkin methods are quasi-optimal : the error in the Petrov-Galerkin approximation is within some factor of the best error possible in the space. We summarize with the following theorem.

Theorem 1. Suppose k e  X  k = min y  X  k Uy  X   X  x k is the er-ror in the best approximation from R ( U ) in some norm. The error k e k = k Uy  X  x k for the Petrov-Galerkin approximation is bounded by k e k X  (1+  X  ) k e  X  k for  X  = k U kk ( W T For the DEIM method, the error is bounded by k e k  X  (1 +  X  indicates the Moore-Penrose pseudoinverse, i.e. the least-squares solution operator.

Proof. Suppose Uy  X  is the best approximation in the space to x = M  X  1 b under some norm. Let e = x  X  Uy and e  X  = x  X  Uy  X  be the error in the chosen approximation and the best approximation, respectively. Substituting b = M ( Uy  X  + e  X  ) into the Petrov-Galerkin ansatz yields and therefore Taking norms, we have In the DEIM case, a similar argument yields and taking norms as before yields the final result.
The most significant term in this bound is the norm of the inverse of the projected system, i.e. k ( W T MU ) k ( M I , : U )  X  1 k . In particular, if the columns of U are normal-ized to have absolute sums equal to zero, then k U k and k M I , : k 1  X  1 +  X  , so that for the one-norm the quasi-optimality constant is bounded by That is, the key quantity in this case is k ( M I , : U ) measures how close the projected system is to being sin-gular. When evaluating the reduced model, we can bound this quantity with little extra cost via Hager X  X  algorithm or variants [25, 27].
