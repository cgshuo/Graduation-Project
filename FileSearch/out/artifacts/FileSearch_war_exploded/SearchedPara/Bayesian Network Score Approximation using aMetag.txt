 The problem we address in this paper is, broadly speaking, fu nction approximation. Specif-ically, the application we present here is that of estimatin g scores on the space of Bayesian networks as a first step toward a quick way to obtain a network w hich is optimal given a set of data. Usually, the search process requires a full recompu tation of the posterior likelihood of the graph at every step, and is therefore slow. We present a new approach to the problem of approximating functions such as this one, where the mappi ng is of an object (the graph, in this particular case) to a real number (its BDe score). In o ther words, we have a function f :  X  n  X  R (where  X  n is the set of all directed graphs on n nodes) from which we have a small number of samples, and we would like to interpolate the rest. The technique hinges on the set  X  n having a structure which can be factored into a Cartesian pro duct, as well as on the function we approximate being smooth over this struct ure.
 Although Bayesian networks are by definition acyclic, our ap proximation technique applies to the general directed-graph case. Because a given directe d graph has n 2 possible edges, we can imagine the set of all graphs as itself being a Hamming c ube of degree n 2  X  a  X  X etagraph X  with 2 n 2 nodes, since each edge can be independently present or absen t. We say that two graphs are connected with an edge in our metagrap h if they differ in one and only one edge. We can similarly identify each graph with a bit string by  X  X nraveling X  the adjacency matrix into a long string of zeros and ones. Howeve r, if we know beforehand an ordering on the nodes of our graph to which all directed graph s must stay consistent (to enforce acyclicness), then there are only n drops to 2 ( n 2 ) . The same correspondence can then be made between these grap hs and bit strings of length n Since the eigenvectors of the Laplacian of a graph form a basi s for all smooth functions on the graph, then we can use our known sampled values (which cor respond to a mapping from a subset of nodes on our metagraph to the real numbers) to inte rpolate the others. Despite the incredible size of the metagraph, we show that this probl em is by no means intractable, and functions can in fact be approximated in polynomial time . We also demonstrate this technique both on a small network for which we can exhaustive ly compute the score of every possible directed acyclic graph, as well as on a larger real-world network. The results show that the method is accurate, and additionally suggest that t he BDe scoring metric used is quadratic over the metagraph. 2.1 The Kronecker Product and Kronecker Sum The matrix operators known as the Kronecker product and Kron ecker sum, denoted  X  and  X  respectively, play a key role in the derivation of the spectr al properties of the hypercube. Given matrices A  X  R i  X  j and B  X  R k  X  l , A  X  B is the matrix in R ik  X  jl such that: The Kronecker sum is defined over a pair of square matrices A  X  R m  X  m and B  X  R n  X  n as A  X  B = A  X  I n + I m  X  B , where I n denotes an n  X  n identity matrix[8]. 2.2 Cartesian Products of Graphs The Cartesian product of two graphs G 1 and G 2 , denoted G 1  X  G 2 , is intuitively defined as the result of replacing every node in G 1 with a copy of G 2 and connecting corresponding edges together. More formally, if the product is the graph G = G 1  X  G 2 , then the vertex set of G is the Cartesian product of the vertex sets of G 1 and G 2 . In other words, for any u 2  X  v 2 is an edge in G 2 , or u 2 = v 2 and u 1  X  v 1 is an edge in G 1 .[7] In particular, the set of hypercube graphs (or, identically , the set of Hamming cubes) can be derived using the Cartesian product operator. If we denote t he graph of an n -dimensional hypercube as Q n , then Q n +1 = Q n  X  Q 1 , where the graph Q 1 is a two-node graph with a single bidirectional edge. 2.3 Spectral Properties of Cartesian Products The Cartesian product has the property that, if we denote the adjacency matrix of a graph  X  k and corresponding eigenvalues  X  k (with k = 1 ...m ) while A ( G 2 ) has n eigenvectors  X  l with corresponding eigenvalues  X  l (with l = 1 ...n ), then the full spectral decomposition of A ( G 1  X  G 2 ) is simple to obtain by the properties of the Kronecker sum; A ( G 1  X  G 2 ) will have mn eigenvectors, each of them of the form  X  k  X   X  l for every possible  X  k and  X  l in the original spectra, and each of them having the corresponding eigenvalue  X  k +  X  l [2]. It should also be noted that, because hypercubes are all k -regular graphs (in particular, the hypercube Q n is n -regular), the form of the normalized Laplacian becomes sim ple. The usual formula for the normalized Laplacian is: However, since the graph is regular, we have D = kI , and so Also note that, because the formula for the combinatorial La placian is L = D  X  A , we also have  X  L = 1 k L .
 The Laplacian also distributes over graph products, as show n in the following theorem. Theorem 1 Given two simple, undirected graphs G 1 = ( V 1 ,E 1 ) and G 2 = ( V 2 ,E 2 ) , with combinatorial Laplacians L G graph G 1  X  G 2 is then given by: Proof.
 Here, D G denotes the degree diagonal matrix of the graph G . Now, by the definition of the Laplacian, However, the degree of any vertex uv in the Cartesian product is deg( u ) + deg( v ) , because all edges incident to a vertex will either be derived from one of the original graphs or the other, leading to corresponding nodes in the product graph. So, we have Substituting this in, we obtain Because the Kronecker product is distributive over additio n[8], Additionally, if G 1  X  G 2 is k -regular, Therefore, since the combinatorial Laplacian operator dis tributes across a Kronecker sum, we can easily find the spectra of the Laplacian of an arbitrary hypercube through a recursive process if we just find the spectrum of the Laplacian of Q 1 . 2.4 The Spectrum of the Hypercube Q n First, consider that This is a k -regular graph with k = 1 . So, Its eigenvectors and eigenvalues can be easily computed; it has the eigenvector 1 eigenvalue 0 and the eigenvector h 1  X  1 i with eigenvalue 2. We can use these to compute the four eigenvectors of L Q L and [1  X  1  X  1 1] T , with corresponding eigenvalues 0, 1, 1, and 2 (renormalize d by a factor the combinatorial Laplacian would require no normalizatio n). It should be noted here that an n -dimensional hypercube graph will have 2 n eigenvalues with only n + 1 distinct values; they will be the values 2 k n for k = 0 ...n , each of which will have multiplicity n k [4]. If we arrange these columns in the proper order as a matrix, a f amiliar shape emerges: This is, in fact, the Hadamard matrix of order 4, just as placi ng our original two eigenvectors side-by-side creates the order-2 Hadamard matrix. In fact, the eigenvectors of the Laplacian on a hypercube are simply the columns of a Hadamard matrix of t he appropriate size; this can be seen by the recursive definition of the Hadamard matrix in terms of the Kronecker product: Recall that the eigenvectors of the Kronecker sum of two matr ices are themselves all possible Kronecker products of eigenvectors of those matrices. Sinc e hypercubes can be recursively constructed using Kronecker sums, the basis for smooth func tions on hypercubes (i.e. the set of eigenvectors of their graph Laplacian) is the Hadamar d basis. Consequently, there is no need to ever compute a full eigenvector explicitly; there is an explicit formula for a given entry of any Hadamard matrix: The notation b x here means  X  X he n -bit binary expansion of x interpreted as a vector of 0s and 1s X . This is the key to computing our kernel efficiently, no t only because it takes very little time to compute arbitrary elements of eigenvectors, but because we are free to compute only the elements we need instead of entire eigenvectors at o nce. 3.1 The Optimization Framework Given the above, we now formulate the regression problem tha t will allow us to approximate to nodes x i in the metagraph, we wish to find the  X  f which minimizes the squared error between our estimate and all observed points and also which i s a sufficiently smooth function on the graph to avoid overfitting. In other words, The variable m in this expression controls the type of smoothing; if m = 1 , then we are penalizing first-differences (i.e. the gradient of the funct ion). We will take m = 2 in our ex-periments, to penalize second-differences (the usual case w hen using spline interpolation)[6]. This problem can be formulated and solved within the Reprodu cing Kernel Hilbert Space framework[9]; consider the space of functions on our metagr aph as the sum of two orthogonal spaces, one (called  X  0 ) consisting of functions which are not penalized by our regu larization the case of our hypercube graph,  X  0 turns out to be particularly simple; it consists only of the space  X  1 is formulated under the RKHS framework as a set of columns of t he kernel matrix (denoted K 1 ). Consequently, we can write  X  f = 1 T d + K 1 e , and so our formulation becomes: on y , our vector of observed values. In other words, there exist m atrices  X  d ( c,m ) and  X  e ( c,m ) , dependent on our smoothing coefficient c and our exponent m , such that: estimate over the entire graph. Because  X ( c,m ) is entirely dependent on the two matrices  X  d and  X  e as well as our kernel matrix, we can calculate an estimate for any set of nodes in the graph by explicitly calculating only those rows of  X  which correspond to those nodes and then simply multiplying that sub-matrix by the vector y . Therefore, if we have an efficient way to compute arbitrary entries of the kernel matri x K 1 , we can estimate functions anywhere in the graph. 3.2 Calculating entries of K 1 polynomial used to perform standard interpolation on the hy percube. The effect that r will have on our problem will be to select the set of basis function s we consider; the eigenvectors corresponding to a given eigenvalue 2 k n are the n k eigenvectors which divide the space into identically-valued regions which are themselves ( n  X  k ) -dimensional hypercubes. For example, the 3 eigenfunctions on the 3-dimensional hypercu be which correspond to the negative plane along each of the three axes. Because these ei genfunctions are all equivalent apart from rotation, there is no reason to choose one to be in o ur basis over another, and so we can say that the total number of eigenfunctions we use in our approximation is equal to P r k =0 n k for our chosen value of r .
 All eigenvectors can be identified with a number l corresponding to its position in the natural-ordered Hadamard matrix; the columns where l is an exact power of 2 are ones that alternate in identically-sized blocks of +1 and -1, while th e others are element-wise products of the columns correponsing to the ones in l  X  X  binary expansion. Therefore, if we use the notation | x | 1 to mean  X  X he number of ones in the binary expansion of x  X , then choosing the to r. Therefore, we have: Because k is equal to | l | 1 , and because we already have an explicit form for any H xy , we can write The  X   X  symbol here denotes exclusive-or, which is equivalent to ad dition mod 2 in this domain. The justification for this is that only the parity of t he exponent (odd or even) matters, and locations in the bit strings b i and b j which are both zero or both one contribute no change to the overall parity. Notably, this shows that the value of the kernel between compute these values quickly. If we let S k ( b i ,b j ) = P | l | used in the experiments due to its speed and feasability of co mputation. 4.1 The 4-node Bayesian Network The first set of experiments we performed were on a four-node B ayesian Network. We generated a random  X  X ase truth X  network and sampled it 1000 t imes, creating a data set. We then created an exhaustive set of 2 6 = 64 directed graphs; there are six possible edges in a four-node graph, assuming we already have some sort of no de ordering that allows us to orient edges, and so this represented all possibilities. Because we chose the node ordering to be consistent with our base network, one of these graphs wa s in fact the correct network. We then gave each of the set of 64 graphs a log-marginal-likel ihood score (i.e. the BDe score) based on the generated data. As expected, the correct network came out to have the greatest likelihood. Additionally, computation of the Rayleigh quotient shows that the function is a globally smooth one over the graph topology. We then performed a set of experiments using the metagraph kernel. 4.1.1 Randomly Drawn Observations First, we partitioned the set of 64 observations randomly in to two groups. The training group ranged in size from 3 to 63 samples, with the rest used as the testing group. We then used the training group as the set of observations, and q ueried the metagraph kernel to predict the values of the networks in the testing group. We re peated this process 50 times for each of the different sizes of the training group, and the resu lts averaged to obtain Figure 1. Note that order 3 performs the best overall for large numbers of observations, overtaking the order-2 approximation at 41 values observed and staying the best until the end. However, order 1 performs the best for small numbers of observations ( perhaps due to overfitting errors caused by the higher orders) and order 2 performs the b est in the middle. The data suggests that the proper order to which to compute the kernel in order to obtain the best approximations is a function of both the size of the space and the number of observations made within that space. 4.1.2 Best/worst-case Observations Secondly, we performed experiments where the observations were obtained from networks which were in the neighborhood around the known true maximum , as well as ones from networks which were as far from it as possible. These results are Figures 2 and 3. Despite small differences in shape, the results are largely identica l, indicating that the distribution of the samples throughout  X  n matters very little. 4.2 The Alarm Network The Alarm Bayesian network[1] contains 37 nodes, and has been used in m uch Bayes-net-related research[3]. We first generated data according to th e true network, sampling it 1000 times, then generated random directed graphs over the 37 nod es to see if their scores could be predicted as well as in the smaller four-node case. We gene rated two sets of these graphs: a set of 100, and a set of 1000. We made no attempt to enforce an o rdering; although the graphs were all acyclic, we placed no assumption on the gr aphs being consistent with the same node-ordering as the original. The scores of these s ets, calculated using the data drawn from the true network, served as our observed data. We t hen used the kernel to approximate, given the observed scores, the score of an addi tional 100 randomly-generated graphs, with the order of the kernel varying from 1 to 20. The r esults, with root-mean-squared error plotted against the order of the kernel, are sh own in Figure 4. Additionally, we calculated a baseline by taking the mean of the 1000 sample d scores and calling that the estimated score for every graph in our testing set.
 The results show that the metagraph approximation method pe rforms significantly better than the baseline for low orders of approximation with highe r amounts of sampled data. This makes intuitive sense; the more data there is, the bette r the approximation should be. Additionally, the spike at order 2 suggests that the BDe scor e itself varies quadratically over the metagraph. To our knowledge, we are the first to make this o bservation. In current work, we are analyzing the BDe in an attempt to analytically valida te this empirical observation. If true, this observation may lead to improved optimization techniques for finding the BDe-maximizing Bayesian network. Note, however, that, even if t rue, exact optimization is still unlikely to be polynomial-time because the quadratic form i s almost certainly indefinite and, therefore, NP-hard to optimize. Functions of graphs to real numbers, such as the posterior li kelihood of a Bayesian network given a set of data, can be approximated to a high degree of acc uracy by taking advantage of a hybercubic  X  X etagraph X  structure. Because the metagraph is regular, standard techniques of interpolation can be used in a straightforward way to obta in predictions for the values at unknown points. Although this technique allows for quick and accurate predi ction of function values on the metagraph, it offers no hints (as of yet) as to where the maximu m of the function might be. This could, for instance, allow one to generate a Bayesia n network which is likely to be close to optimal, and if true optimality is required, that ap proximate graph could be used as a starting point for a stepwise method such as MCMC. Even wi thout a direct way to find such an optimum, though, it may be worth using this approx imation technique inside an MCMC search instead of the usual exact-score computation in order to quickly converge on a something close to the desired optimum.
 Also, many other problems have a similar flavor. In fact, this technique should be able to be used unchanged on any problem which involves the comput ation of a real-valued function over bit strings. For other objects, however, the s tructure is not necessarily a hypercube. For example, one may desire an approximation to a function of permutations of some number of elements to real numbers. The set of permuta tions of a given number of elements, denoted S n , has a similarly regular structure (which can be seen as a gra ph in which two permutations are connected if a single swap leads f rom one to the other), but not a hypercubic one. The structure-search problem on Bayes Net s can also be cast as a search over orderings of nodes alone[5], so a way to approximate a fu nction over permutations would be useful there as well.
 Other domains have this ability to be turned into regular gra phs  X  the integers mod n with edges between numbers that differ by 1 form a loop, for example . It should be possible to apply a similar trick to obtain function approximations not only on these domains, but on arbitrary Cartesian products of them. So, for instance, rem embering that the directions of the edges of Bayesian network are completely specified given an ordering on the nodes, the network structure search problem on n nodes can be recast as a function approximation over the set S n  X  Q ( n only just scratched the surface here.
 The authors would like to thank Curtis Storlie and Joshua Nei l from the UNM Department of Mathematics and Statistics, as well as everyone in the Machi ne Learning Reading Group at UNM. This work was supported by NSF grant #IIS-0705681 under the Robust Intelligence program.
 [1] I. Beinlich, H.J. Suermondt, R. Chavez, G. Cooper, et al. The ALARM monitoring [2] D.S. Bernstein. Matrix Mathematics: Theory, Facts, and Formulas with Applicat ion to [3] D.M. Chickering, D. Heckerman, and C. Meek. A Bayesian ap proach to learning Bayesian [4] Fan R. K. Chung. Spectral Graph Theory . Conference Board of the Mathematical [5] N. Friedman and D. Koller. Being Bayesian about network s tructure. Machine Learning , [6] Chong Gu. Smoothing Splines ANOVA Models . Springer Verlag, 2002. [7] G. Sabidussi. Graph multiplication. Mathematische Zeitschrift , 72(1):446 X 457, 1959. [8] Kathrin Schacke. On the kronecker product. Master X  X  the sis, University of Waterloo, [9] Grace Wahba. Spline Models for Observational Data . CBMS-NSF Regional Conference
