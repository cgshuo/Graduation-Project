
H.3.3 [ Information Search and Retrieval ]: Clustering; In-formation filtering Knowledge Extraction; Summarization; Cluster Visualization
Corresp ondence author. This work was partially support-ed by the National NSFC(No.61472085, 61171132, 61033010), by National Key Basic Research Program of China under No.2015CB358800, by Shanghai STCF under No.13511505302, by NSF of Jiangsu Prov. under No. BK2010280, by the NSF under Grant No. IIS1017415, by the Army Research Laboratory under Cooperative Agreement No. W911NF-09-2-0053, by Defense Ad-vanced Research Projects Agency (DARPA) under Contract No. W911NF-11-C-0200 and W911NF-12-C-0028, and by Region II University Transportation Center under the project No. 49997-33 25.

Wikipedia 1 has become one of the best sources for creating and sharing a massive volume of human knowledge. Among others, an important reason that makes it extremely valuable is that part of its data is structured , and hence machine processible. Usually, a Wikipedia article is about an entity. Many Wikipedia arti-cles contain structured information such as table , text , hyper link , etc., all of which are the targets of information extraction. More importantly, many entities are associated with an infobox which consists of a set of ( property , value ) pairs about the entities. Such structured information is the core building block behind many ap-plications, including search engines, for answering user questions about these entities, etc. But the current infobox in Wikipedia is often incomplete [3].

In this paper, we present WiiCluster , a scalable platform for automatically generating structural information to supply knowl-edge for infobox in Wikipedia. Instead of performing information extraction over unstructured natural language text directly, we focus on a rich set of semi-structured data in Wikipedia articles: linked entities . A Wikipedia article typically consists of many links to other Wikipedia articles. Intuitively, the author of the article, in describing a Wikipedia entity, refers the reader to many other entities that are important or related to the entity. The key idea of this paper is the following: If we can summarize the rela-tionship between the entity and its linked entities, we immediately harvest some of the most important information about the entity.
In order to convert such semi-structured data (i.e., linked enti-ties) to the structure infobox, we propose an effective cluster-then-label algorithm to map the (cluster-label, cluster) to (property, value) pairs. For example, the article  X  X hanghai X  in Wikipedia has the linked entity like  X  X he Bund X , X  X riental Pearl Tower X , X  X udan University X , X  X hanghai Jiao Tong University X  . We group  X  X he Bund X , X  X riental Pearl Tower X  together by clustering and assign a label (such as  X  X isitor Attractions in Shanghai X  ) to the cluster. The cluster label explains how the entities in the cluster are relat-ed to  X  X hanghai X  and can be considered as a property of  X  X hang-hai X  . Similarly we can group  X  X udan University X , X  X hanghai Jiao Tong University X  together and generate a corresponding label  X  X -niversities in Shanghai X  .

Recently, extensive effort has focused on expanding and en-riching the structured data by automatic information extraction from unstructured text in Wikipedia [3, 2] to complete infobox. Although remarkable progresses have been made, its effectiveness and scalability are still somewhat limited (related work for detail).
Our approach outperforms above methods in scale and efficien-cy, because we harvest knowledge from all linked entities instead of limiting  X  X roperty X  in the infobox template (scale guaranteed), and knowledge are harvested by summarizing instead of pairwise relationship extracting (efficiency guaranteed). Even if the qual-ity of our method might not be as good as those by dedicated human labeling, since our  X  X luster-label X  is not constrained as  X  X roperty X  in infobox template, the infobox we generate could still represent knowledge in human sense and provide a starting point for further manual editing by human (which might save part of their labeling/editing cost).
Our Wi iCluster enjoys three key advantages. First, it is ef-fective in generating semantically meaningful summarization for Wikipedia articles. Second, it is totally unsupervised and thus does not require any human label. Third, it is scalable by adopt-ing an efficient cluster-reuse algorithm. Overall, our WiiCluster is able to generate nearly 10 million new facts. We also develop a web-based platform to demonstrate WiiCluster , which enables the users to access and browse the generated knowledge. In this section, we demonstrate the main functionalities of iCluster . We implement the WiiCluster in Java. The knowl-edge is stored in MySql database and can be accessed on our web platform http://gdm.fudan.edu.cn/WiiCluster . The platform receives a user X  X  search request and retrieves its infobox gener-ated by WiiCluster from the database. It further supports the browsing and visualization of the intrinsic knowledge of a given infobox.

Scale of WiiCluster . We run the experiments on Wikipedia (released in January 1, 2013), which has 3.2 million English arti-cles in total. After removing the noisy, unrelated linked entities and filtering the linked entities without feature for clustering, we have 1.95 million valid articles in total. Our WiiCluster 9.8M clusters for these 1.95M articles. On average, we find 5 clus-ters for each article; and 3.3 entities for each cluster. If we treat the (article entity, property, an entity in a cluster) as a single fact, WiiCluster generates 32M such facts in total.

Display Platform . Figure 1 shows the main interface of Wi-iCluster . It has a search box on the top. If the user types in an entity name (e.g., Shanghai ), WiiCluster retrieves its infobox from the database and displays it in the main screen. There are three main parts/views to browse a given infobox. Let us use the example of  X  X hanghai X  to illustrate these views. In Figure 1, part A lists the primary property index in the alphabetical order (e.g.,  X  X irports X ,  X  X ttractions X  ). A user can be further navigated to browse the corresponding dependent clusters in part B. Part B shows all the primary properties and their dependent clusters, each of which is composed of a secondary property and one or more linked entities. For example, the two primary properties  X  X ttractions X  and  X  X niversities X  , each of which has a dependent cluster. Furthermore, secondary property of  X  X ttractions X  is  X  X is-itor attractions in Shanghai X  and it contains 18 entities such as  X  X he Bund X ,  X  X riental Pearl Tower X  etc. famous tourist attrac-tions in Shanghai . The secondary property represents the label of a group of the linked entities in the same cluster. Each entity can be used to navigate the user to browse the information of the corresponding article. In part C, we use prefuse (an open source graph view package) 2 to visualize the intrinsic knowledge of a giv-en infobox. For example, if the user clicks the primary property  X  X istricts X  , its dependent cluster will be shown in an aggregated area.

We present two different models to visualize the clusters: (a) aggregate graph model and (b) tree model . For example, If the user clicks the entry Graph in part C, the aggregate graph viewer will show. This visualization model displays the properties and values in the form of a graph, where all the properties and entities are represented as nodes and their dependency are represented as the directed edges. The nodes in one cluster are wrapped within an aggregated area. In the display panel, the central node is the article (  X  X hanghai X  ), the red node represents the primary property of the article, the linked aggregated areas represent their inner clusters. Each aggregated area composes of a green node and a set of white nodes. The green node represents the secondary property, and it is dependent on the primary property by which it is linked. And the linked white nodes represent the linked entities, which depend on the secondary property. By default, we display the article and its primary properties in the panel only, and all the inner clusters are hidden. A user can click the primary property to display or hide its inner clusters.

The generated summarization WiiCluster is meaningful and sensible. In the  X  X hanghai X  example, we generate a number of clusters of entities with the primary and secondary properties. Each cluster and the corresponding properties are good supple-ment for the infobox of the article.
In this section, we present the algorithmic details. Figure 2 shows the overall flowchart of WiiCluster . From algorithmic perspective, there are two challenges, including C1. how to ac-curately summarize linked entities? , and C2. how to efficiently extract knowledge for all articles? . Next, we describe an effective cluster-then-label algorithm and a cluster reuse strategy to ad-dress these two challenges, respectively. Details of the algorithms and quantitative comparison can be found in [4].
In order to convert the semi-structured linked entities into structured (property, value) pairs as infobox in Wikipedia, we need to group the similar linked entities (i.e., values) together as well as assign a label (i.e., property) for each group.
We thus propose a  X  X luster-then-label X  approach: we cluster linked entities into different semantic groups, and then assign each group a semantic label (a property). More specifically, we use a G-means based clustering algorithm to cluster the linked entities into different semantic groups. And further propose a LCA (Least Common Ancestor) based label generating algorithm to assign a label for each group. Each labeled group is eventually a candidate ( property , value ) pair for the infobox.

A. Clustering . In Wikipedia, an entity is typically associated with one or more categories by editors. A category is widely used to characterize the concept of an entity. Hence, we use the categories to construct the feature vector for the entity.
However, the clustering performance is poor if we use the cat-egories directly as the feature vector, mainly due to the following two limitations. First, some categories are not hypernyms of the corresponding entities, which may lead to mis-clustering. Second, direct categories of article are usually too specific, which leads to the small clusters with the limited number of linked entities. To address these two limitations, we propose a feature expansion-and-weighting procedure to construct the feature vectors for the entities. To be specific, for a given entity e , we recursively extend its feature set from its direct categories to the higher level cate-gories. For each expanded category c , we define its weight p ( c as the probability of category c being a hypernym of the entity e . The higher the weight p ( c j e ) is, the more the clustering algorithm will reply on this expanded feature c . Once we have constructed the feature vectors for all the linked entities, many off-the-shelf clustering algorithms can be plugged in. In our current imple-mentation, we use G-means [1] algorithm to cluster the linked entities with the cosine distance that is defined on the expanded and weighted feature vectors.
 B. Labeling . Next, we assign a semantic label for each group/cluster. In this way, we could explain why the group of entities are linked by the article entity. The semantic label as well as the group of entities thus becomes a property of the article entity and its corresponding value. This information might provide a good sup-plement of the current infobox. In WiiCluster , we design the following two labeling methods, including Category as Label and Word as Label .
Category as Label . Generally speaking, a good cluster label should capture the common theme of entities within each cluster completeness and in the meanwhile differentiate itself from other clusters informativeness . The completeness and the informative-ness could be contradicting to each other. In general, the more abstract a label is, the more entities it can cover, but the less informative it might be.

Since we use the (directed and indirected) categories as the fea-ture vectors of the entities, we can select an appropriate category as the cluster label. In order to carefully balance the completeness and informativeness, we propose a LCA (Least Common Ances-tor) model. The LCA model is defined on the taxonomy graph G (Wikipedia category system). Given a cluster of entities, we search the LCA of these entities in the taxonomy G . The LCA is the nearest category which is reachable from all entities in the cluster via the hypernym edges in the taxonomy graph G . In this way, the label we get is specific enough (i.e., informativeness) yet still covers the entire cluster (i.e., completeness).

Word as Label . Using the above labeling strategy, we assign an appropriate category for each cluster. However, the categories in Wikipedia are manually edited, which are often represented as a phrase and organized in a specific Wiki style. Thus, some cate-gories might be hard for the machine to understand (e.g.,  X  X eople by status X  instead of  X  X eople X  ). Therefore, our WiiCluster additional strategies to generate a more structural and machine processable label.

To distinguish these two labeling methods, we refer to the cat-egory label as the secondary property , and the word label as pri-mary property since the word label might be upper concept of the category label.
If we apply the above cluster-then-label procedure to each of the million of articles in wikipedia independently, the computa-tion quickly becomes the bottleneck. In order to address the scal-ability issue, we propose a novel cluster reuse strategy to speedup the computation. Here, the key observation is that different ar-ticles might share many common linked entities. Thus, once the knowledge extraction for one article is done, the other article might reuse/inherit its summarization result instead of recom-puting from scratch. For example, the two Wikipedia articles,  X  X hanghai X  and  X  X udong X  , have some common linked entities such as  X  X uangpu District X ,  X  X angpu District X  etc. district in Shang-hai. When we process  X  X hanghai X  , suppose that we have grouped  X  X uangpu District X ,  X  X angpu District X  together with the label  X  X istricts of Shanghai X  . Thus, when we process  X  X udong X  , we might be able to directly inherit/reuse this result. Apparently, this strategy would be much more efficient in terms of computa-tion. To do this, we construct a maximal spanning tree from the article graph (article as node, hyper link between articles as edge) in Wikipedia, and serve weight of edge as percent of their com-mon linked entities. It X  X  obvious that the more common linked entities exist, the larger probability reuse working and the less time cost on direct clustering and labeling.
Extensive efforts have focused on extracting structural informa-tion from unstructural text from Wikipedia to supply knowledge for infobox, such as [3, 2]. Limitation for these methods are: First, these methods, such as [2], rely on several natural language understanding tasks (e.g., named entity recognition, dependency parsing, and relationship extraction), which themselves are ex-tremely challenging and error prone. [2] can harvests knowledge only from the sentences that contain both the article or its varia-tion and the linked entity, it X  X  obvious time consuming and is not scale guaranteed. Second, many of the existing approaches are costly, such as [3], since they are essentially supervised learning methods, and hence require a large amount of labeled training examples. In our approach, we harvest structural information though summarizing linked entities instead of pairwise relation-ship extraction or template constrained extraction, then we will harvest more knowledge in an efficient manner.
Discovering and enriching structural information in online en-cyclopedia is a valuable yet challenging task. We present iCluster , a scalable platform for automatically generating in-fobox for articles in Wikipedia. Unlike the existing  X  X nformation Extraction+Supervised Learning X  paradigm, WiiCluster offers a radically different, yet promising avenue for automatic infobox generation. The heart of our system is an effective cluster-then-label algorithm over a rich set of semi-structured data in Wikipedi-a articles: linked entities . It has three key features, including (a) requiring no human labels; (b) effective in generating meaningful knowledge and (c) scalable to millions of new facts. We demon-strate its main functionalities through web-based platform. [1] G. Hamerly and C. Elkan. Learning the k in k-means. In [2] D. P. Nguyen, Y. Matsuo, and M. Ishizuka. Exploiting [3] A. Sultana, Q. M. Hasan, A. K. Biswas, S. Das, H. Rahman, [4] K. Zhang, Y. Xiao, H. Tong, H. Wang, and W. Wang. The
