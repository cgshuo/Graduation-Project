 FULL PAPER Sabine Barrat  X  Salvatore Tabbone Abstract Inthispaper,weproposeadescriptorcombination method, which enables to improve significantly the recogni-tion rate compared to the recognition rates obtained by each descriptor. This approach is based on a probabilistic graph-ical model. This model also enables to handle both discrete and continuous-valued variables. In fact, in order to improve the recognition rate, we have combined two kinds of fea-tures: discrete features (corresponding to shape measures) and continuous features (corresponding to shape descrip-tors). In order to solve the dimensionality problem due to the large dimension of visual features, we have adapted a variable selection method. Experimental results, obtained in a supervised learning context, on noisy and occluded sym-bols, show the feasibility of the approach.
 Keywords Symbol recognition  X  Descriptor combination  X  Variable selection  X  Probabilistic graphical models  X  Bayesian networks 1 Introduction Pattern recognition applications have to face the problem of describing a large number of different objects for recog-nition. A recognition system should be robust to variability (geometrictransformations,noise,occlusions,...)andtosca-lability, when a large number of classes and images should be recognized. Symbol recognition is a field within pattern recognition for which a lot of efforts have already been made [ 22 , 37 , 39 , 44 ]. Symbol recognition is usually decomposed into two steps: symbol description and classification [ 1 , 6 , 12 ]. In order to describe symbols, a lot of different shape descriptors have been proposed (see surveys [ 15 , 33 , 40 ]) but one descriptor is usually not enough to describe all kinds of shapes properly and therefore to give satisfactory shape rec-ognition rates. One solution is to combine several descriptors in a classification task [ 38 ] or to use several classifiers and to combine their outputs [ 32 , 34 ]. Classification is a basic task in data analysis and pattern recognition. This task requires a classifier, i.e., a function that assigns a class label to instances described by a set of features. The induction of classifiers from training sets (sets of labeled data) is a central problem in machine learning: it is a problem of supervised learning. In fact, in numerous applications, the aim is to assign a feature vector f ={ f 1 , f 2 ,..., f n } to a class c i among k classes, designed by a vector c ={ c 1 , c 2 ,..., c k } . Some approaches to this problem are based on various functional representa-tions such as decision trees, neural networks, decision graphs [ 2 , 23 , 29 , 42 ], associated with decision rules.
Probabilistic approaches also play a central role in clas-sification [ 21 , 43 , 24 ]. A way to reach the previous goal, by using probabilities, is to compute the conditional probabil-ity distribution P ( c i | f ),  X  i  X  X  1 , 2 ,..., k } and assign the instance f to the class c i for which this probability is max-imal. In order to represent probability distributions over a large set of variables, we introduce several conditional inde-pendence assumptions that will help to reduce the complexity of the model and provide a tractable model. Within the frame-work of the graphical models [ 16 ], a class of models called Bayesian networks allows an efficient representation of any probability distribution that can be factorized according to a set of independence assumptions. This factorization will help to reduce the computational complexity of the model. Moreover, this framework comes with many algorithms for performing inference (i.e., the computation of posteriors) and learning (factorization of parameters fitting, computation of probability distributions ...). We propose, in this paper, an original method of descriptor combination applied to symbol classification. Thus, we have adapted the proba-bilistic graphical model theory to the symbol recognition problem. In this model, continuous and discrete variables are combined. Continuous variables correspond to shape descriptors, and the discrete ones correspond to shape mea-sures. Thanks to this combination, the proposed classifier is more robust to deformations and to the size of database, when the number of symbols increases. The originality of our approach also relies on the use of a variable selection method [ 35 ], to overcome the dimensionality problem related to the size of feature vectors and the inherent network complexity.
The organization of the paper is as follows. In Sect. 2 ,the main properties of a Bayesian network-based classifier are introduced and lead to the presentation of our probabilistic model for symbol recognition. The visual features used to represent the symbols are described in Sect. 3 . The feature selection algorithm which allows us to increase the recogni-tion rate by focusing only on the main features while reducing the dimensionality problem is also explained in Sect. 4 .Our method is evaluated on a database of noisy and occluded symbols (Sect. 5 ). Finally, Sect. 6 brings conclusions and opens new perspectives to our work. 2 Representation and classification of images 2.1 Context and objectives Our work is focused on symbol recognition by combining descriptors. Given an image database, where each image con-tains one symbol, we try to recognize the  X  X erfect X  symbol (the model) represented in each image. In fact, the symbols contained in the images are not perfect: they can be noisy, deformed and can have occlusions. This recognition problem can be view as a classification problem: our aim is to assign each image to the class corresponding to the perfect sym-bol (the model) of this image. However, no perfect symbol is available. Therefore, we cannot just minimize a distance between each image of the database and each perfect sym-bol. On the other hand, this classification task can be resolved by using a supervised learning method, from a subset of the database where the class label (the perfect symbol) is known for each image.

Moreover, in order to describe all kinds of shapes prop-erly, even deformed or noisy shapes, and thus increase the recognition rate, our proposition is to combine several shape descriptors. Now, shape descriptors can provide vectors of continuous or discrete values: let f j be a query image characterized by a set of features F composed of:  X  m continuous visual features, denoted v 1 ,...,v m ,  X  n discrete visual features, denoted DF 1 ,..., DF n .
The chosen visual features are issued from 3 shape descriptors and 3 shape measures. Shape descriptors pro-vide vectors of continuous values, and each shape measure provides a single discrete value.

Consequently, it seems appropriate to propose a classifier that enables to manage both discrete and continuous features. Although most classification methods handle only discrete dataandthus requireapre-processingstepof discretizationin order to transform each continuous-valued variable into a dis-crete one, few classification methods can handle both discrete and continuous-valued variables. It is the case of Support Vector Machines [ 4 ], Random Forests [ 3 ], and Bayesian clas-sifiers [ 13 ]. Support Vector Machines (SVM) and Random Forests (RF) are well known for their ability to handle high-dimensional data. On the contrary, Bayesian classifiers are sensitive to the dimensionality of the data, but they often per-form well in many domains. Therefore, we have chosen to construct a Bayesian classifier for its ability to combine dis-crete and continuous-valued variables. Moreover, we show that this Bayesian classifier, associated with a variable selec-tion method, is competitive with SVM, even on high-dimen-sional data. 2.2 Bayesian classifiers Let I be a new image designed by a particular instance f = { f aim is to assign I toaclass c i among k classes. Each c i particular instance of the variable C . The Na X ve Bayes ( NB ) is a simple probabilistic classification algorithm that often performs well in many domains. This classifier encodes a distribution P NB ( F 1 ,..., F n , C ) , from a given training set (composedoflabeleddata).Theresultingprobabilisticmodel can be used to classify the new instance I . In fact, the Bayes rule is applied to compute the probability of c i given the par-ticular instance f . Then the classifier based on NB returns the label c i , i  X  X  1 ,..., k } , that maximizes the posterior probability P i = P NB ( c i | f 1 ,..., f n ) , where: P and P NB ( f 1 ,..., f n ) = k j = 1 P NB ( f 1 ,..., f n P
However, we are interested in the probability distribu-tions of discrete and continuous features and their conditional dependence relations. Let us consider each component of continuous vectors (issued from shape descriptors) as a con-tinuous random variable and the discrete values (provided by shape measures) as discrete variables. This model is too big to be represented as a unique joint probability distribu-tion. Therefore, it is required to introduce some sparse and structural apriori knowledge: the Na X ve Bayes has to be extended to take into account continuous and discrete variables. In this perspective, the probabilistic graphical models, and especially Bayesian networks, are a good way to solve this kind of problem. In fact, within Bayesian networks, the joint probability distribution is replaced by a sparse rep-resentation only among the variables directly influencing one another. Interactions among indirectly related variables are then computed by propagating inference through a graph of these direct connections. Consequently, Bayesian networks are a simple way to represent a joint probability distribution over a set of random variables, to visualize the conditional properties and to compute complex operations like probabil-ity learning and inference, according to graph-based compu-tations. 2.3 Bayesian networks 2.3.1 Definitions Formally, a Bayesian network for a set of random variables V (continuous or/and discrete) is a pair B = G , .The first component, G , is a directed acyclic graph whose verti-ces correspond to random variables V 1 ,..., V n , and whose edges represent direct dependencies between variables. The graph G encodes independence assumptions: each variable V is independent of its non-descendants given its parents in G . The second component of the pair, , represents the set of parameters that quantifies the network. It contains a parame-ter  X  v i | Pa (v i ) = P B (v i | Pa (v i )) for each possible value V , and Pa (v i ) of Pa ( V i ) , where Pa ( V i ) denotes the set of parents of V i in G . That is, the Bayesian network, in its initial state, contains the initial apriori probabilities of each node of the network: P B (v i | Pa (v i )) . Thanks to the conditional inde-pendence assumption of each variable given its parents, the joint probability distribution P B ( V 1 ,..., V n ) can be reduced to this formula: P
The framework of Bayesian networks comes with many algorithms for performing inference (i.e., the computation of posteriorsprobabilities)andlearning(factorizationofparam-eters fitting, computation of probability distributions,...). The algorithms we used in this work are briefly described below. 2.3.2 Parameter learning Only one has a description of a model, knowing the structure of the graph and probabilistic forms for each variables, one wants to estimate the numerical values of each parameter. Let assume we have either discrete or continuous variables (or a mix of them), and, for the simple case, a set of data describing many possible cases for each variables. The data set can either be complete or have missing data. In each case, a different solution will be used. In the case, where the data set has no missing values, an approach is to consider the parameters having the highest probabilities to generate the most similar data set if the Bayesian network was used to draw random values according to the probability distribu-tion it describes (hence the name  X  X enerative model X ). This method is known as the Maximum Likelihood. Let call D the data set, then P ( d | M ) is the probability of a data d to be generated by the model M and is called the likelihood of M given d . Therefore, the likelihood of M given the full data set D is: L (
M | D ) = P ( D | M ) = For the sake of computational simplicity (or to help deriving an analytic form), the log-likelihood is often used: L (
M | D ) = Therefore, the principle of maximum likelihood prefers to choose parameters with the highest likelihood:  X   X  = argmax  X  L ( M  X  | D ) In general, a maximum likelihood is obtained by counting the frequencies over the total count. Whenever you do not have enough data for each case (missing data) one of the most pop-ular algorithm is the Expectation-Maximization (EM) algo-rithm. The general purpose of this algorithm is explained in detail in [ 7 ]. During the Expectation phase, the data set is locally completed, then a Maximization step is performed to find the current maximum likelihood estimate (as seen above) using the completed data. In a Bayesian network, the first step of the EM algorithm can be easily done using a map a posteriori algorithm, i.e., computing the most proba-ble values of the missing data variables given other known variables. The second step is then executed and can either be done with an optimization algorithm if no analytical form of the maximum likelihood is known, or with the previous approach. These two steps are repeated until convergence. The algorithm is initialized with random probability distri-bution parameters.

The EM algorithm is also used to learn the parameters of Gaussian distributions, which are considered as missing data. 2.3.3 Inference An inference algorithm is necessary to compute the posterior probability distributions of unobserved nodes. According to the Bayesian network topology, the inference process propa-gates the values from the leaf level to the inferred node. Many algorithms can be used [ 17 ]. The most popular is the message passing algorithm [ 20 ]. In this technique, each node is asso-ciated with a processor, which can send some messages to its neighbors, in an asynchronous way, until it reaches stability. 2.3.4 Bayesian network classifiers Bayesian networks can be used as classifiers. For example, the Na X ve Bayes can be represented by the structure in Fig. 1 , where:  X  C refers to the class variable,  X  F 1 ,..., F n are the feature variables.

The Na X ve Bayes is a simple and efficient model, but it requires discrete variables. Since, we have to manage con-tinuous (values provided by shape descriptors) and discrete (values provided by shape measures) variables, this model has to be extended to take into account continuous and dis-crete variables. 2.4 A Gaussian-Mixtures and Bernoulli Mixture model A Bayesian network classifier, which handles both discrete and continuous-valued variables, is proposed. We present a hierarchical probabilistic model, the Gaussian-Mixtures and Bernoulli Mixture model, in order to classify large databases of symbols. In fact, the observation of some peaks on the different histograms of the vector components provided by shape descriptors has led us to consider that the continuous visual features can be estimated by mixtures of Gaussian den-sities. The discrete variables have a Bernoulli distribution. In fact, these variables can take two values: 1 if the correspond-ing shape measure provides a value smaller than 0 . 5, or else 2. Finally, the proposed model is inspired by the Na X ve Bayes. Indeed, the class variable is connected to each other.
Now let F be the training set composed of m instances f where n is the dimension of the signatures provided by the concatenation of the feature vectors issued from the compu-tation of all the descriptors for each image on the training set. Each instance f j ,  X  j  X  X  1 ,..., m } is then character-ized by n continuous variables. A supervised classification is considered; then, F instances are divided into k classes c ,..., c Gaussian density with a mean  X  l ,  X  l  X  X  1 ,..., g } and a covariance matrix l . Besides, let  X  1 ,..., X  g be the propor-tions of the different groups,  X  l = ( X  l , l ) be the parame-ter of each Gaussian and = ( X  1 ,..., X  g , X  1 ,..., X  g ) the global mixture parameter. The probability density of F con-ditionally to the class c i ,  X  i  X  X  1 ,..., k } can be defined by P ( f ,) = where p ( f , X  l ) is the multivariate Gaussian defined by the parameter  X  l .

We have one Gaussian Mixture model per class, which can be represented by the probabilistic graphical model in Fig. 2 , where:  X  The  X  X lass X  node is a discrete node, which can take k val- X  The  X  X omponent X  node is a discrete node which corre- X  The  X  X aussian X  node is a continuous variable which rep- X  Finally, the edges represent the effect of the class on each
The model can be completed by the discrete variables, denoted DF 1 ,..., DF n , where n is the number of shape measures, and DF i represents the value of each shape mea-sure. Dirichlet priors [ 27 ] have been used for the probability estimation of the variables DF 1 ,..., DF n . That is we introduce additional pseudo counts at every instance in order to ensure that they are all  X  X irtually X  represented in the train-ing set. Therefore every instance, even if it is not represented in the training set, will have a not null probability. Like the continuous variables, the discrete variables corresponding to the discrete measures are included in the graphical model by connecting them to the class variable.

Now our classifier can be depicted by the Fig. 3 . The hid-den variable  X   X   X  shows that a Dirichlet prior is used. The box around the variable DF denotes n repetitions of DF for each shape measure.

This Bayesian classifier means that continuous and shape features, representing images, are assumed to have been generated conditional on the same class. Therefore, the resulting Bernoulli and Gaussian mixture parameters should correspond: concretely if an image, represented by continu-ous visual descriptors, has an high probability under a certain class, then its discrete shape measures should have an high probability under the same class.

In order to classify a query image f j , the class node C is inferredthankstothemassagepassingalgorithm.Thisimage, characterized by its continuous shape features v j 1 ,...,v and its discrete shape features DF 1 j ,..., DF k j is consid-ered as an  X  X vidence X  represented by: P ( f when the network is evaluated. Thanks to the inference algo-rithm, the probabilities of each node are updated in func-tion of this evidence. After the belief propagation, we know,  X  i  X  X  1 ,..., k } , the posterior probability: P ( c i | f j ) = P ( c i | v j 1 ,...,v j m , DF 1 j ,..., DF n The query f j is assigned to the class c i which maximizes this probability. 3 Symbol description This section explains how we have adapted the theoretical method before mentioned to a symbol recognition problem. We present the visual features we used. The set of chosen fea-tures is composed of 3 different off-the-shelf shape descrip-tors and 3 shape measures. The choice of these features is not really important because, the aim of this paper is to show that combining shape features improves the symbol classifi-cation, whatever the used features. The distinction between shape descriptors and shape measures is determined by the sizeofthefeatures:weconsidersinglevaluefeaturesasshape measures and feature vectors as shape descriptors. Moreover, shape measures are discretized with a discretization thresh-old fixed at 0 . 5. This discretization has sense with this kind of features, because each of them is composed of a single value normalized between 0 and 1. Thus, a shape measure has an intrinsic meaning. Finally, this discretization enables to con-sider shape measures as discrete variables and thus to show the interest of discrete and continuous features combination for shape recognition.

In this perspective, we have chosen three pixel shape descriptors: the Generic Fourier Descriptor (GFD), the Zernike descriptor, and the R -signature 1 D and three shape measures: compactness, rectangularity and ellipticity. We briefly present each descriptor and measures below. 3.1 Shape descriptors Generic Fourier descriptor: Generic Fourier descriptor is based on Fourier transform [ 41 ]. The rotation invariance is achieved by using the modified polar Fourier transform (MPFT), and the scaling invariance is achieved after nor-malization.
 Zernike descriptor: Zernike descriptor [ 14 ] is a descriptor based on Zernike moments. Zernike moments of a given shape are calculated as correlation values of the shape with Zernike basis functions, in that all the pixels of the shape, independently of their position, contribute with the same weight to the Zernike moments. These moments are rotation invariant. To make the Zernike moments of the shape descriptor invariant also to translation and scaling, a given shape is normalized, by obtaining the smallest circle centered at the center of mass, covering all the shape pixels. Then the obtained circle is adjusted to match the radius of Zernike moment basis functions. The Zernike shape descrip-tor consists of low-order magnitudes of Zernike moments. R -signature 1 D : The R -signature 1 D [ 30 ] uses Radon transform to represent an image. The Radon transform is the projection of an image in a particular plan. This projection has interesting properties. According to these geometrical properties, a 1 D signature of the transform is created. This signature checks the properties of invariance to some geo-metrical transformations, such as the translation and the scal-ing (after normalization). The rotation invariance is achieved by a cyclic permutation of the signature, or directly from its Fourier transform. 3.2 Shape measures Compactness: The compactness measure C represents the ratio of the shape area to the area of a circle (the most com-pact shape) having the same perimeter:
C = where P is the perimeter and A , the area.
 This measure is invariant to translation, rotation, and scaling. Rectangularity: The rectangularity degree [ 28 ] R is equal to the ratio of the shape area to the area of its minimal bounding box: R = where A is the shape area and L (respectively l ) is the length (respectively the width) of the minimal bounding box. Ellipticity: The ellipticity degree is obtained from the ratio of the major axis to the minor axis [ 31 ]: where a is the major axis and b the minor axis. This measure is invariant to rotation, translation, and homothety. 4 Dimensionality reduction Only the n , n  X  X  1 , 2 , 3 } continuous descriptors we want to combine are computed on each image, we dispose of n signa-tures per image. The concatenation of these signatures pro-videsusanewfeaturevectorperimage.Thelargedimensions of initial visual signatures and their concatenation imply a dimensionalityproblem.Infact,atoolargefeaturedimension increases the computation time and causes a wrong Gauss-ian mixture learning, because of the Small Sample Size (SSS) problem: there is a disproportion between the training set size and the feature vector dimension. To overcome this problem, we have used a dimensionality reduction method. A lot of methods have been proposed in the literature to reduce the dimension of vectors [ 8 , 25 ]. Among dimensionality reduc-tion methods, we consider especially feature selection meth-ods, because they enable to reduce dimension by selecting a subset of initial features, on the contrary to the methods which reduce dimension by providing new variables issued from initial variable combinations [ 10 ]. Methods of variable selection are then more suitable to our problem, because our aim is to reduce the number of features, in order to reduce the size of our Bayesian network and our method complexity. The most popular methods of variable selection are heuristics based on sequential runs, which consist in iteratively adding or removing variables [ 9 ]. In these approaches, it is possible to begin with an empty set of variables and to add variables to the variables which are already selected (it is the Sequential Forward Selection (SFS) [ 26 ]), or to begin with the set of all variables et to remove variables in this set (it is the Sequential Backward Selection (SBS)). These methods are known for their simplicity and their rapidity. However, they are known for their instability too. Moreover, since they do not explore all possible subsets of variables and they do not enable to come back during the process, they are not optimal. Thus, we have chosen a feature selection method since it enables to extract from the feature vectors, just the most rele-vant and discriminating features, with a minimal information loss. The regression method LASSO (Least Absolute Shrink-age and Selection Operator) [ 35 ] has been used for its sta-bility and implementation efficiency. Moreover, the LASSO method especially enables to select variables and takes into account the class variable values to select a subset of vari-ables. In Sect. 5 , we compare the results obtained on our database, by the LASSO and the SFS method which stay one of the most popular.
 The principle of LASSO is to shrink the regression coeffi-cientsbyimposingapenaltyontheirsize(wespeakofshrink-age methods too). These coefficients minimize a penalized residual sum of squares:  X  lasso = arg min subject to p j = 1 |  X  j | X  s .

The linear form of the LASSO has been applied in a pre-processing stage, totally independent of our Bayesian classifier, on our visual features. For each training set, y represents the sum of the mean vector features of the class c Then, just the subset of the selected variables is used in our model.

The LASSO uses a L 1 penalty: p j = 1 |  X  j | . This con-straint implies that for small values of s ( s  X  0), some of the coefficients  X  will be null. So choosing s is like choosing the number of predictors in a regression model. Therefore, the variables corresponding to the coefficients different from zero are selected.
 The LASSO solutions have been computed by the Least Angle Regression (LAR) procedure [ 11 ]. This algorithm exploits the special structure of the LASSO problem and provides an efficient way to compute the solutions simulta-neously for all values of s .

Thus, we can distinguish two independent phases in our method: first, the LASSO is used to select the most rele-vant visual features. Secondly, our Gaussian-Mixtures and Bernoulli Mixture model is used to classify new images rep-resented by the pre-selected continuous visual features and the three discrete shape measures. 5 Experimental results We have used the symbols of GREC database [ 36 ] for our tests (see Fig. 4 ), especially created for the symbol recogni-tion contest GREC X 2005.

This database is mainly defined from two application domains, architecture and electronic, because these symbols are most largely used by graphic recognition teams and rep-resent a great number of different forms. We have 50 differ-ent symbol models for which we have applied some noises based on Kanungo [ 18 ] model. These noises are similar to noise obtained when a document is scanned, printed, or pho-tocopied. Moreover, we have applied to these symbols some rotations of different degrees and different zooms, in order to obtain a database of 3 , 600 images, constituted of 72 different images per model.

We have evaluated our method by performing a cross-validation by using 75% of the database for the training and the remaining 25% for the tests. The tests are repeated four times in order to use each database instance for the training and the tests. The recognition rate is obtained by taking the mean recognition rate of the 4 tests.

Since, we want to improve the recognition rate by com-bining descriptors and selecting variables, we limit ourselves to the experiments comparing:  X  the classification after variable selection with the LASSO  X  the classification by combining 2 or 3 continuous descrip- X  the classification by combining discrete and continuous  X  the classification by combining 3 continuous descriptors
First of all, we can remark that the reducing the size of the descriptors improves the recognition rate for all the clas-sifiers. Moreover, the variable selection with the LASSO method has enabled us to significantly reduce the number of variables. Table 1 shows the mean number of variables selected for each descriptor with the LASSO method com-pared to the well-known SFS method [ 26 ]. We can see that the LASSO method enables to select fewer variables than the SFS method (see Table 1 ). Table 2 shows the mean recogni-tion rates in function of different variable selection methods, by combining the 3 available descriptors with our Gauss-ian-Mixtures and Bernoulli Mixture model (GM-B) and two state-of-art classifiers: a classical SVM classifier [ 5 ] and the fuzzy k -nearest neighbor (FKNN) [ 19 ]. The recognition rates for these three classifiers without variable selection and after a variable subset selection with the SFS and LASSO meth-ods, or with some random selections, are compared. For the random selection, the number of variables chosen randomly is set to the one obtained with the LASSO. The FKNN has been computed with k = 1 and with k = m where m is the mean number of images per class in the training set. The results in Table 2 show that the variable selection with the LASSO method improves the recognition rate by 8 . 7% on average compared to the classification without variable selection, by 5 . 3% on average with random selection, and by 1 . 8% on average compared to the SFS selection. Thus the LASSO method is more robust, experimentally, on this database, than the SFS method. In fact, the shrinkage meth-ods like the LASSO are well known to be more stable than iterative methods, to select variables among a large set of variables but with few examples. Thus, the variables selected with the LASSO method have been used for our following experiments.
 Let us consider Table 3 . The notation G (respectively Z and R ) means that the GFD descriptor (respectively the Zernike descriptor and the R -signature 1 D ) has been used. Finally, the  X  +  X  operator indicates that we have combined the descriptors. The recognition rates confirm that combin-ing 2 or 3 descriptors performs always better than any of them alone. In fact, we observe that the combination of 2 descriptors increases the recognition rate by 18% on average compared to the use of only one descriptor. Besides we can notice that the combination of the 3 descriptors is better, by 18 . 3% on average, to use just one of them. Moreover, even if we obtain a high recognition rate with Zernike descriptor, this rate will not decrease when we combine this descriptor with one or two other descriptors, whatever these descriptors and even if the added descriptors have a low rate (it is the case with the R -signature 1 D ). The bad behavior of a descriptor does not impede the other descriptor behaviors.

Finally, the last line of the Table 2 shows the effec-tiveness of our approach compared to the SVM classifier and the FKNN. The results have been obtained by combin-ing the 3 descriptors and after using the variable selection method LASSO. It appears that the proposed GM-B results are always better than the ones of SVM and FKNN.

The initial database of 3 , 600 instances has been extended to a database of 5 , 400 instances by randomly generating occlusions on the half image set of each class from the initial database. In fact, we can meet this kind of degradation when we have to segment a graphical document where symbols are embedded into the graphic or are partially occluded by text for example. The generated occlusions are from differ-ent sizes, and their locations in the images have been chosen randomly. Now we have a larger and more distorted data-base, composed of 108 instances per class. For example, the Fig. 5 presents4symbolmodels(firstcolumn)and5occluded images disturbed by some noises derived from these models. Our classifier has been applied after variable subset selection with the LASSO. This time, our method has been evaluated by performing 3 cross-validations whose each proportion of the training set is 25,50 and 75% of the database, the remain-ing, respectively, 75,50, and 25% are hold for testing set. In each case, the tests are repeated 10 times in order that each database instance would be used for the training and the test. For each training set size, the recognition rate is obtained by taking the mean recognition rate of the 10 tests. On this database the LASSO method has enabled to select quite the same number of variables than with the initial database: 12 variablesonaveragefromGFDfeatures,13fromZernikefea-tures and 13 from R -signature 1 D features. Let us consider Table 4 . The used notations are the same as the ones previ-ously used in Table 3 . Moreover, the notation DF means that the three discrete shape measures have been used. The recog-nition rates show the descriptor combination interest. Indeed, even if the classification is less efficient on this database than on the initial one, the results show the combination of con-tinuous descriptors improves the recognition rate. Moreover, the addition of the 3 discrete shape measures outperforms these results. In fact, the integration of the discrete measures improves the recognition rate by 3 . 8% on average compared to the recognition rate obtained by the combination of the 3 continuous descriptors. Finally, Table 5 shows that the pro-posed GM-B classifier performs always better than the SVM and the FKNN classifiers. In the same way, Table 6 shows the maximal and minimal values and the mean and the standard deviation of the recognition rates obtained by the 3 compared classifiers, during the 10 tests for a training on 50% of the database. The standard deviation is small whatever the classi-fier and shows a low variability of recognition rate following the different training and testing sets.
 Table 7 shows CPU times of the SVM classifier, the FKNN, and the proposed GM-B classifier, for training and test stages, with the same experimental conditions like in the Table 5 . All the experiments have been performed with a processor Intel Core 2 Duo 2.40GHz, 2 Go RAM, Windows OS. The three classifiers have been run with Matlab X . If we consider only the test stages (training has been made off line for the SVM and the GM-B classifier), the SVM classifier is faster than the two others. The CPU time is higher for the GM-B model because it depends on the number of Gaussians (in this case, 2) and the pre-defined precision of the EM algo-rithm. However, the processing time remains weak since it takes less than 0 , 03s per image. Following the discussion in Sect. 4 , we can remark that without variable selection the CPU time raises drastically for the GM-B classifier. 6 Conclusion and future works In this paper, we have proposed an original adaptation of the Bayesian theory to combine descriptors. We have shown that a Bayesian network has good properties for symbol recogni-tion. In the proposed model, the bad behavior of a descriptor does not impede the behavior of the others. Moreover, we can take into account different types of descriptors. Indeed, we have combined discrete shape measures with continu-ous shape descriptors. This combination provides a classifier more robust to variability and scalability. Moreover, we have adapted the LASSO method, which solves our dimensional-ity problem and thus decreases our method complexity and especially increases the recognition rate. The experimental results are very promising and show the efficiency of our method.

In our future works, we want to use our approach in the caseofverycomplexsymbolslikeelectricalwiringdiagrams. In this case, to recover the maximum amount of information, it is useful to add more descriptors in our combination frame-work. Even if the LASSO has shown good results, it does not take into account correlation between variables and after selection, variables are always correlated. However, it is well known that the descriptors are often partially redundant since they address the same task. In this case, a more appropriate reduction method should be investigated.

Moreover, it can be interesting to annotate some symbols and add the information given by possible keywords associ-ated with a subset of training data. Our motivation is based on the property of Bayesian networks to enable to manage, in a same network, different kinds of information (in this case different media), and to their ability to handle missing values.
 References
