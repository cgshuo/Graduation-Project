 The ability to learn from numeric attributes is very useful because many at-tributes needed to describe real-world problems are most naturally expressed by continuous numeric values. The decisi on tree learners C4.5 and CART success-fully handle numeric attributes. Doing so is relatively straightforward, because in the batch learning setting every numeric value is present in memory and available for inspection.

For stream classification algorithms such as the Hoeffding tree [5] the situa-tion is more complicated, although Domingos and Hulten claim that handling numeric attributes is immediate . While this statement is true, the practical impli-cations warrant serious investigation. The storage of sufficient statistics needed to exactly determine every potential nume ric threshold, and the result of split-ting on each threshold, grows linearly with the number of unique numeric values. A high speed data stream potentially has an infinite number of numeric values, and it is possible that every value in the stream is unique. Essentially this means that the storage required to precisely track numeric attributes is unbounded and can grow rapidly.

For a Hoeffding tree learner to handle numeric attributes, it must track them in every leaf it intends to split. This is e xtremely expensive and necessitates the development of an effective memory management strategy that will deactivate some leaves in favour of more promising ones when facing memory shortages. This may reduce the impact of leaves with heavy storage requirements but may also significantly hinder growth.

Several approaches to handling numeric attributes during Hoeffding tree in-duction have been suggested before, and are discussed in Section 2. Prior to this study the methods have not been compar ed, so Section 3 explores the tradeoff of accuracy versus model size by empirical comparison. All the methods described in this sectio n attempt to handle numeric attributes at each node of the Hoeffding tree, in a similar fashion to C4.5. Each approach represents an alternative approximation of the C4.5 method. 2.1 VFML Domingos and Hulten released working source code for a numeric handling method in their VFML package [10]. Numeric attribute values are summarized by a set of ordered bins. The range of values covered by each bin is fixed at cre-ation and does not change as more examples are seen. A hidden parameter serves as a limit on the total number of bins allowed X  X n the VFML implementation this is hard-coded to allow a maximum of one thousand bins. Initially, for every new unique numeric value seen, a new bin is created. Once the fixed number of bins have been allocated, each subsequent value in the stream updates the counter of the nearest bin.

Essentially the algorithm summarizes the numeric distribution with a his-togram, made up of a maximum of one thousand bins. The boundaries of the bins are determined by the first one thousand unique values seen in the stream, and after that the counts of the static bins are incrementally updated.
There are two potential issues with the approach. Clearly, the method is sensitive to data order. If the first one thousand examples seen in a stream happen to be skewed to one side of the total range of values, then the final summary will be incapable of accurately re presenting the full range of values. The other issue is estimating the optimal number of bins. Too few bins will mean the summary is small but inaccurate, whereas too many bins will increase accuracy at the cost of space. In the exp erimental compari son the maximum number of bins is varied to test this effect. 2.2 Exhaustive Binary Tree This method represents the extreme case of achieving perfect accuracy at the necessary expense of storage space. The d ecisions made are the same that a batch method would make, because essentially it is a batch method X  X o information is discarded other than the observed order of values.

Gama et al. present this method in their VFDTc system [7]. It works by incrementally constr ucting a binary tree structure as values are observed. The path a value follows down the tree depends on whether it is less than, equal to or greater than the value at a particular node in the tree. The values are implicitly sorted as the tree is constructed.

The only way that this structure saves space over remembering the entire sequence of values is if a value that has a lready been recorded reappears in the stream. In this case the counter in the binary tree node responsible for tracking that value can be incremented. In every o ther case a new node will be introduced to the tree. Even then, the overhead of the tree structure will mean that space can only be saved if there are many repeated values. If the number of unique values were limited, as is the case in some data sets, then the storage requirements will be less intensive. In all of the synthetic data sets used for this study the numeric values are generated randomly across a continuous range, so the chance of repeated values is almost zero. The impact of the space cost is measured in the experimental comparison.

Beside memory cost, this method has ot her potential issues. Because every value is remembered, every possible thres hold is also tested when the information gain of split points is evaluated. This makes the evaluation process more costly than more approximate methods. This method is also prone to data order issues. The layout of the tree is established as the values arrive, such that the value at the root of the tree is the first value seen. There is no attempt to balance the tree, so data order is able to affect the effic iency of the tree. In the worst case, an ordered sequence of values will cause the binary tree algorithm to construct a list. 2.3 Quantile Summaries Researchers in the field of database syst ems are concerned wit h accuracy guaran-tees associated with quantile estimates, helping to improve the quality of query optimizations. Random sampling is often considered as a solution to this prob-lem. Vitter [15] shows how to randomly sample from a data stream, but the non-deterministic nature of random sampling and lack of accuracy guarantees motivate search for other solutions. Munro and Paterson [13] show how an ex-act quantile can be deterministically co mputed from a single scan of the data, but this procedure requires memory proportional to the number of elements in the data. Using less memory means that quantiles must be approximated. Early work in quantile approximation includes the P 2 algorithm proposed by Jain and Chlamtac [11], which tracks five markers and updates them as values are ob-served via piece-wise fitting to a parabo lic curve. The method does not provide guarantees on the accuracy of the estimates. Agrawal and Swami [2] propose a method that adaptively adjusts the boundaries of a histogram, but it too fails to provide strong accuracy guarantees . More recently, the method of Alsabti et al. [3] provides guaranteed error bounds, continued by Manku et al. [12] who demonstrate an improved method with tighter bounds.

The quantile estimation algorithm of Manku et al. [12] was the best known method until Greenwald and Khanna [8] proposed a quantile summary method with even stronger accuracy guarant ees. The method works by maintaining an ordered set of tuples, each of which records a value from the input stream, along with implicit bounds for the range of each value X  X  true rank. An operation for compressing the quantile summary is defined, guaranteeing that the error of the summary is kept within a desired bound. The quantile summary is said to be -approximate, after seeing N elements of a sequence any quantile estimate returned will not differ from the exact value by more than N .Theworst-case space requirement is shown by the authors to be O ( 1 log( N )), with empirical evidence showing it to be even better in practice.

Greenwald and Khanna mention two variants of the algorithm. The adaptive variant is the basic form of the algorithm, that allocates more space only as error is about to exceed the desired . The other form, used here, is referred to as the pre-allocated variant, which imposes a fixed limit on the amount of memory used. The pre-allocated method was chosen because it guarantees stable approximation sizes throughout the tree, and is consistent with the majority of other methods by placing upper bounds on the memory used per leaf.

When used to select numeric split points in Hoeffding trees, a per-class ap-proach is used where a separate quantile summary is maintained per class label. When evaluating split decisions, all values stored in the tuples are tested as potential split points. Different limits on the maximum number of tuples per summary are examined in the experimental comparison. 2.4 Gaussian Approximation This method approximates the numeric distribution on a per-class basis in small constant space, using a Gaussian (i.e. normal) distribution. Such a distribution can be incrementally maintained by storing only a few numbers in memory (such as the mean and variance), and is completely insensitive to data order.
Algorithm 1. is a method for incrementally computing the mean and variance of a stream of values. The method only requires three numbers to be remembered. It was derived from the work of Welford [16], and its advantages are studied in [4]. A method similar to this is described by Gama et al. in their UFFT system [6]. When evaluating split points, a single optimal point is computed as derived from the crossover point of two distributions. It is possible to extend their approach to search for split points allowing any number of classes: a set of points spread equally across the range between the minimum and maximum values observed, are evaluated as potential split points. The number of points is determined by a parameter, so the search for split points is parametric, even though the under-lying Gaussian approximations are not. For each candidate point the weight of values to either side of the split can be approximated for each class, using their respective Gaussian curves, and the information gain is computed from these weights. This procedure can also cope with extreme cases like distribution with very similar means, but different standard deviations. Algorithm 1. Numerically robust incremental Gaussian
The process is illustrated in Figure 1. At the top of each figure are Gaussian curves, each approximating the distribution of values seen for a numeric attribute and labeled with a particular class. The curves can be described using three values; the mean, variance, and the total weight of examples. For example, in the leftmost figure the class shown to the left has a lower mean, higher variance and higher example weight (larger area under the curve) than the other class. Below the curves the range of values has been divided into ten split points, labeled A to J. The horizontal bars show the proportion of values that are estimated to lie on either side of each split, and the vertical bar at the bottom displays the relative amount of information gain calculated for each split. For the two-class example (the left figure), the split point that would be chosen as the best is point E, which according to the evaluation has the highest information gain. In the four-class example (the right figure) the split point C is chosen which nicely separates the first class from the others.

A refinement of this method, found to increase precision at low cost, is used in the final implementation. It involves additionally tracking the minimum and maximum values of each class (the distribution cutoff points in Figure 1 depict these values). This requires storing an extra two counts per class, but precisely maintaining these values is simple and fast. When evaluating split points the per-class minimum and maximum information is exploited to determine when class values lie completely to one side of a split, eliminating the small uncer-tainty otherwise present in the tails of the Gaussian curves. From the per-class minimum and maximum, the minimum and maximum of the entire range of values can be established, which helps to determine the position of split points to evaluate.

This simplified view of numeric distributions is not necessarily harmful to the accuracy of the trees it produces beca use there will be further opportunities during training to refine split decisions on a particular attribute by splitting again further down the tree. The method does not have only one chance of getting the optimal value but can have multiple atte mpts, where each subsequent attempt will be in a more focused range of values based on increasingly more confident information. In addition, the approximation may prove more robust and resistant to noise than more complicated methods, which concentrate on finer details. Each method is tested to see how efficiently it produces Hoeffding trees. The methods compared are all based on the basic htmc (Hoeffding Tree Majority Class) algorithm described in [5], with only the method for handling numeric attributes varied. Predictions are m ade using majority class at each leaf 1 .
The methods compared are listed in Table 1, including the memory limits im-posed per numeric attribute per leaf, and with reference to the text explaining each method. Three realistic application scenarios are envisaged where memory for learning is limited to a pre-specified maximum. A sensor node environment (memory limit 100K), a handheld comput er environment (32MB) and a server environment (400MB). Eighteen datasets are used (see Table 3) in the evalua-tion. They are all generated in order to provide a proper evaluation and have all appeared previously in the data stream literature (see for example [9] and [7]). The experimental methodology used is co nsistent with other studies, particu-larly [5], but on a larger scale. In all cases, training takes place over a period of ten hours and testing is accomplished with a holdout set of one million examples.
Table 2 lists the final results averaged over all 18 data sources, sorted by scenario. For the sensor environment the figures for the number of training ex-amples are low because learning was stopped when the last active leaf in a tree had been deactivated. In the handheld case these figures are much higher than in the server case because the former gene rates smaller trees with fewer active nodes and therefore processes examples f aster. The speeds achievable are quoted as percentages of the maximum speed at which these streams can be generated by the experimental software and hardware.

In terms of average accuracy, the four different approaches are easily ranked from best to worst. In all three memory environments, vfml10 is the most accurate on average over all data sour ces. The second most accurate method in every environment is gauss10 .The gk x methods are generally third, and bintree is consistently the least accu rate of the methods on average.
The default number of 1000 bins hard-coded in the original vfml implemen-tation turns out to be the worst performer of the three vfml configurations. The general trend is that smaller numbers of bins, sacrificing accuracy for space per leaf, leads to more accurate trees overall. Requesting more space for numeric ap-proximation reduces the numbers of active tree nodes that can reside in memory, slowing tree growth in a way that impacts final tree accuracy.

The Gaussian method follows this trend, in that it is the smallest approxi-mation tested, permitting the most tree growth and correspondingly accurate trees. Comparing the number of split evaluations tested, it is apparent that the finer grained exploration of gauss100 can be harmful. The gauss100 trees are on average much deeper than any of the other methods, suggesting that splits on certain numeric attributes are being repeated more often, because in many cases the tree depth exceeds the number of attributes availa ble for splitting. These additional splits are probably very small and unnecessary refinements of previous split choices, and they may be very skewed. This is a symptom of try-ing to divide the range too finely based on approximation by a single smooth curve. 2 The gauss10 method uses a suitably matched coarse division of only 10 possibilities, which is far less susceptible to this problem.

Comparing the quantile summary methods gk100 and gk1000 , having 1000 tuples is helpful in the higher memory environments but harmful in 100KB of memory. Lower numbers of tuples can severely hinder the quantile sum-mary method X  X  parameter setting of 10 was tested but found to be much worse than any other method, so was omitted from the final results. Figure 2 shows some examples of how much worse the 10-tuple summary can perform. In particular, the graph on rts (left figure) shows other settings getting very close to 100% accuracy in contrast to the 10-tuple variant achieving less than 65%. Like gauss100 , gk10 results in excessively deep trees which strongly indicates poor split decisions. More fine grained quantile summaries perform well but the tradeoff between space and accura cy is not as effective as for the gauss x and vfml x methods. The performance of gk1000 is similar to bintree in several situations, suggesting that it is highly accurate. At the same time, it manages to build larger trees, suggestin g that it is more space efficient.
The poor performance of bintree shows that in limited memory situations, striving for perfect accuracy at the l ocal level can result in lower accuracy globally. The problem is most pronounced in the 100KB environment, where tree growth for every data source was halted before the first evaluation took place, some time before 1 million training examples. Similar behaviour is evi-dent in the other two most memory-intensive methods vfml1000 and gk1000 , but bintree has the highest memory requirements of all, thus suffers the most in tree growth and accuracy. The method is simply too greedy to support rea-sonable tree induction in this environment. In the other environments it fares better, but is not as successful on average as the more approximate methods.
Table 3 compares the individual final a ccuracies of the best two methods, vfml10 and gauss10 . Bold figures indicate a better result, in this case both methods win 20 times each. gauss10 loses to vfml10 by a fair margin on rtcn in 400MB, although on this dataset some of the other methods are not much better than gauss10 and some are worse still. Some of the worst losses for gauss10 occur on genF2 and genF5 in 100KB, where it is outperformed by all other methods. These functions are very similar (see [1]). The function genF2 relies on two numeric attributes salary and age ,and genF5 includes further dependency on a third numeric attribute, loan . The trees induced by the Gaussian method were inspected to find the cause of the problem. The trees make the mistake of choosing a discrete attribute car with many possible values that is completely irrelevant. After making this mistake the example space is highly segmented, so a lot of extra effort is required to make co rrections further down the tree. The Gaussian methods slo wly recover to come within reasonably close accuracy, except for the 100KB environment where the lack of space limits any opportunity of recovering. This demonstrates a limitation of the Gaussian method, where the high level of approximation causes the best attributes to be underrated, although the true underlying cause of the issue is unknown. It might relate to an unintentional bias towards certain split types that could potentially be corrected in a style similar t o Quinlan X  X  correction in [14].

Conversely, there are situations where the high level of approximation gives the Gaussian method an advantage over all others. The clearest cases of this are on the data sources rrbfs , rrbfc , wave21 and wave40 . Such a bias is perhaps not surprising since the generators responsible for these streams use numeric values drawn from random Gaussian distributions.

Analysing space complexity, the amount of memory required per leaf to track n numeric attributes and c classes is 10 n +10 nc for vfml10 and 5 nc for gauss10 . For vfml10 the 10 n term accounts for storage of the boundary positions, while the 10 nc term accounts for the frequency counts. This simplified analysis un-derestimates the true cost of the vfml implementation, which also retains in-formation about the class and frequency of values that lie exactly on the lower boundary of each bin, increasing the precision of decisions. For gauss10 the mul-tiplying constant is 5 values per attribute and class because there are 3 values tracking the Gaussian curve and additional 2 numbers tracking the minimum and maximum values.

In theory, at the local level vfml10 should be very sensitive to data order, whereas gauss10 should not be sensitive at all. Whether this translates into poorer global decisions during tree induction is not tested by the benchmark generators because all examples are randomly drawn uniformly from the space of possible examples. The right hand side of Figure 3 shows a constructed ex-ample where data order has been manipulated to expose vfml10  X  X  weakness. genF2 has been modified so that every sequence of one million examples drawn from the stream has been sorted by the salary attribute. In this case the accu-racy of gauss10 has improved while the early accuracy of vfml10 has dropped markedly. On average gauss10 trees reach much larger sizes than the other numericmethodsinthesametimeandsp ace, with many more active leaves. The ability of vfml10 to slowly recover may be partly due to additional tree structure increasing the dispersion of examples down different paths of the tree, reducing the degree to which values encountered at leaves are sorted. We have presented an extension to Gama X  X  method of using Gaussian distribu-tions to approximate numeric attributes encountered in tree-based classification of data streams. In order to evaluate its efficacy we have designed an experiment involving three realistic memory-limiting data stream environments and eighteen datasets from previous studies. Five main approaches from the literature were implemented, and eight final configurations of algorithm were tested, ranging from perfectly accurate and memory inten sive to highly approximate. In experi-mental comparison, the most approximate methods produced the most accurate trees, by virtue of allowing the most tree growth. The two methods gauss10 and vfml10 are highly competitive on most datasets. Of these, gauss10 uses less memory and is less susceptible to data order, but is prone to choosing irrel-evant attributes in some cases. Adding a bias to correct for this behaviour will be explored in future work.

