 Learning kernel functions is an ongoing research topic in machine learning that focuses on learning an appropriate kernel function for a given task. While several methods have been proposed, many of the existing techniques can only be applied transductively [1 X 3]; i.e., they cannot be applied inductively to new data points. Of the methods that can be applied inductively, several are either too computationally expensive for large-scale data (e.g. hyperkernels [4]) or are limited to small classes of possible learned kernels (e.g. multiple kernel learning [5]).
 In this paper, we propose and analyze a general kernel matrix learning problem using provided side-information over the training data. Our learning problem regularizes the desired kernel matrix via a convex regularizer chosen from a broad class, subject to convex constraints on the kernel. While the learned kernel matrix should be able to capture the provided side-information well, it is not clear how the information can be propagated to new data points. Our first main result demonstrates that our kernel matrix learning problem is equivalent to learning a linear transformation (LT) kernel function (a kernel of the form  X  ( x ) T W  X  ( y ) for some matrix W  X  0 ) with a specific regularizer. With the appropriate representation of W , this result implies that the learned LT kernel function can be naturally applied to new data. Additionally, we demonstrate that a large class of Mahalanobis metric learning methods can be seen as learning an LT kernel function and so our result provides a constructive method for kernelizing these methods. Our analysis recovers some recent kernelization results for metric learning, but also implies several new results.
 As our proposed kernel learning formulation learns a kernel matrix over the training points, the memory requirements scale quadratically in the number of training points, a common issue arising in kernel methods. To alleviate such issues, we propose an additional constraint to the learning formulation to reduce the number of parameters. We prove that the equivalence to LT kernel function learning still holds with the addition of this constraint, and that the resulting formulation can be scaled to very large data sets.
 We then focus on a novel application of our framework to the problem of inductive semi-supervised kernel dimensionality reduction. Our method is a special case of our kernel function learning framework with trace-norm as the regularization function. As a result, we learn low-rank linear transformations, which correspond to low-dimensional embeddings of high-or infinite-dimensional kernel embeddings; unlike previous kernel dimensionality methods, which are either unsupervised (kernel-PCA) or cannot easily be applied inductively to new data (spectral kernels [6]), our method intrinsically possesses both desirable properties. Furthermore, our method can handle a variety of side-information, e.g., class labels, click-through rates, etc. Finally, we validate the effectiveness of our proposed framework. We quantitatively compare several regularizers, including the trace-norm regularizer for dimensionality reduction, over standard data sets. We also apply the methods to an object recognition task in computer vision and qualitatively show results of dimensionality reduction on a handwritten digits data set.
 Related Work: Most of the existing kernel learning methods can be classified into two broad cat-egories. The first category includes parametric approaches, where the learned kernel function is restricted to be of a specific form and then the relevant parameters are learned according to the pro-vided data. Prominent methods include multiple kernel learning [5], hyperkernels [4], infinite kernel learning [7], and hyper-parameter cross-validation [8]. Most of these methods either lack modeling flexibility, require non-convex optimization, or are restricted to a supervised learning scenario. The second category includes non-parametric methods, which explicitly model geometric structure in the data. Examples include spectral kernel learning [6], manifold-based kernel learning [9], and kernel target alignment [3]. However, most of these approaches are limited to the transductive setting and cannot be used to naturally generalize to new points. In comparison, our method combines both of the above approaches. We propose a general non-parametric kernel matrix learning framework, sim-ilar to methods of the second category. However, we show that our learned kernel matrix corresponds to a linear transformation kernel function parameterized by a PSD matrix. Hence, our method can be applied to inductive settings also without sacrificing significant modeling power. Furthermore, our methods can be applied to a variety of domains and with a variety of forms of side-information. Existing work on learning linear transformations has largely focused on learning Mahalanobis dis-tances; examples include [10 X 15], among others. POLA [13] and ITML [12] provide specialized kernelization techniques for their respective metric learning formulations. Kernelization of LMNN was discussed in [16], though it relied on a convex perturbation based formulation that can lead to suboptimal solutions. Recently, [17] showed kernelization for a class of metric learning algo-rithms including LMNN and NCA [15]; as we will see, our result is more general and we can prove kernelization over a larger class of problems and can also reduce the number of parameters to be learned. Independent of our work, [18] recently proved a representer type of theorem for spectral regularization functions. However, the framework they consider is different than ours in that they are interested in sensing the underlying high-dimensional matrix using given measurements. Kernel dimensionality reduction methods can generally be divided into two categories: 1) semi-supervised dimensionality reduction in the transductive setting, 2) supervised dimensionality reduc-tion in the inductive setting. Methods in the first category include the incomplete Cholesky de-composition [19], colored maximum variance unfolding [20], manifold preserving semi-supervised dimensionality reduction [21]. Methods in the second category include the kernel dimensionality re-duction method [22] and Gaussian Process latent variable models [23]. Kernel PCA [24] reduces the dimensionality in the inductive unsupervised setting, while various manifold learning methods can reduce the dimensionality but only in the unsupervised transductive setting. In contrast, our dimen-sionality reduction method, which is an instantiation of our general kernel learning framework, can perform kernel dimensionality reduction simultaneously in both the semi-supervised as well as the inductive setting. Additionally, it can capture the manifold structure using an appropriate baseline kernel function such as the one proposed by [25]. Given an input kernel function  X  : R d  X  R d  X  R , and some side-information over a set of points  X  but incorporates the provided side-information (the use of the subscript W will become clear later). The initial kernel function  X  is of the form  X  ( x , y ) =  X  ( x ) T  X  ( y ) for some mapping  X  . applying the mapping  X  . We will also assume that the data vectors in X have been mapped via  X  , is an ill-posed problem since infinitely many such kernels can satisfy the provided supervision. A common approach is to formulate a transductive learning problem to learn a new kernel matrix over the training data. Denoting the input kernel matrix K as K = T , we aim to learn a new kernel matrix K W that is regularized against K while satisfying the available side-information. In this work, we study the following optimization problem: where f and g i are functions from R n  X  n  X  R . We call f the regularizer and the g i the constraints . Note that if f and constraints g i  X  X  are all convex functions, then the above problem can be solved optimally using standard convex optimization algorithms. Note that our results will also hold for unconstrained variants of the above problem, as well as variants that incorporate slack variables. In general, such learning formulations are limited in that the learned kernel cannot readily be applied to new data points. However, we will show that the above proposed problem is equivalent to learning linear transformation (LT) kernel functions. Formally, an LT kernel function  X  W is a kernel function think of the LT kernel as describing the linear transformation  X  i  X  W 1 / 2  X  i . A natural way to learn an LT kernel function would be to learn the parameterization matrix W using the provided side-information. To this end, we consider the following problem: where, as before, the function f is the regularizer and the functions g i are the constraints that encode the side information. The constraints g i are assumed to be a function of the matrix T W of learned kernel values over the training data. We make two observations about this problem: first, for data mapped to high-dimensional spaces via kernel functions, this problem is seemingly impossible to optimize since the size of W grows quadratically with the dimensionality. We will show that (2) need not explicitly be solved for learning an LT kernel function. Second, most Mahalanobis metric learning methods may be viewed as a special case of the above framework, and we will discuss some of them throughout the paper. 2.1 Examples of Regularizers and Constraints To make the kernel learning optimization problem concrete, we discuss a few examples of possible regularizers and constraints.
 subject to the constraints given by g i . For linear g i , this problem was studied in [12, 26]. In terms of constraints, pairwise squared Euclidean distance constraint between a pair of points (  X  i ,  X  j ) in feature space can be formulated as K W ( i, i ) + K W ( j, j ) K
W ( i, i ) + K W ( j, j ) Similarity constraints can be represented as K W ( i, j )  X  b or K W ( i, j )  X  b and are also linear in K than  X  k , and are often used in metric learning formulations and ranking problems; such constraints can be easily formulated within our framework. Finally, non-parametric probability estimation con-straints can be used to constrain the conditional probability of a class c given a data point  X  i , where C is the number of classes. This constraint can be written as a linear constraint over K W after appropriate manipulation. We are now ready to analyze the connection between problems (1) and (2). We will show that the solutions to the two problems are equivalent, in the sense that by optimally solving one of the problems, the solution to the other can be computed in closed form. More importantly, this result will yield insight into the type of kernel that is learned by the kernel learning problem. We begin by defining the class of regularizers considered in our analysis. Note that each of the example regularizers discussed earlier satisfy the following definition of spectral functions. Definition 3.1. We say that f : R n  X  n  X  R is a spectral function if f ( A ) =  X  , ...,  X  n are the eigenvalues of A and f s : R  X  R is a real-valued function over the reals. Note that if f s is a convex function over the reals, then f is also convex. 3.1 Learning Linear Transformation Kernels Now we present our main result, i.e., for a spectral function f , problems (1) and (2) are equivalent. Theorem 1. Let K  X  0 be an invertible matrix, f be a spectral function and denote the global minima of the corresponding scalar function f s as  X  . Let W  X  be an optimal solution to (2) and K  X  W be an optimal solution to (1) . Then, where S  X  = K  X  1 ( K  X  W  X   X K ) K  X  1 . Furthermore, K  X  W = T W  X  .
 The first part of the theorem demonstrates that, given an optimal solution K  X  W to (1), one can con-struct the corresponding solution W  X  to (2), while the second part shows the reverse (this also demonstrates why W is used in the subscript of the learned kernel). The proof of this theorem appears in the supplementary material. The main idea behind the proof is to first show that the op-timal solution to (2) is always of the form W =  X I + S T , and then we obtain the closed form expression for S using algebraic manipulations.
 As a first consequence of this result, we can achieve induction over the learned kernels. Given that K W = T W , we can see that the learned kernel function is a linear transformation kernel; that the learned kernel is a linear transformation kernel, along with the first result of the theorem ( W =  X I + S T ) to compute the learned kernel as: As mentioned in Section 2, many Mahalanobis metric learning methods can be viewed as a special case of (2). Therefore, a corollary of Theorem 1 is that we can constructively apply these metric learning methods in kernel space by solving their corresponding kernel learning problem, and then compute the learned metrics via (3). Thus, W need not explicitly be constructed to learn the LT ker-nel. Kernelization of Mahalanobis metric learning has previously been established for some special cases; our results generalize and extend previous methods, as well as provide simpler techniques in some cases. Below, we elaborate with some special cases.
 Example 1 [Information Theoretic Metric Learning (ITML)]: [12] proposed the following Ma-halanobis metric learning problem formulation: min where S and D specify pairs of similar and dissimilar points, respectively, and d W (  X  i ,  X  j ) = (  X  of our framework with regularizer f ( A ) = tr ( A )  X  log det ( A ) and pairwise distance constraints encoded as the g i functions. Furthermore, it is straightforward to show that f is a convex spectral function with global optima  X  = 1 , so the optimal W can be learned implicitly using (1). The corresponding kernel learning optimization problem simplifies to: positive definiteness of K W is satisfied automatically. This recovers the kernelized metric learning problem analyzed in [12], where kernelization for this special case was established and an iterative projection algorithm for optimization was developed. Note that, in the analysis of [12], the g i were limited to similarity and dissimilarity constraints; our result is therefore more general than the exist-ing kernelization result, even for this special case.
 Example 2 [Pseudo Online Metric Learning (POLA)]: [13] proposed the following metric learn-ing formulation: where y ij = 1 if  X  i and  X  j are similar, and y ij =  X  1 if  X  i and  X  j are dissimilar. P is a set of pairs of points with known distance constraints. POLA is an instantiation of (2) with f ( A ) =  X 
A  X  2 F and side-information available in the form of pair-wise distance constraints. Note that the regularizer f ( A ) = 1 2  X  A  X  2 was also employed in [2, 27], and these methods also fall under our general formulation. In this case, f is once again a convex spectral function, and its global minima is  X  = 0 , so we can use (1) to solve for the learned kernel K W as The constraints g i for this problem can be easily constructed by re-writing each of POLA X  X  con-straints as a function of T W . Note that the above approach for kernelization is much simpler than the method suggested in [13], which involves a kernelized Gram-Schmidt procedure at each step of the algorithm.
 Other Examples: The above two examples show that our analysis recovers two well-known ker-nelization results for Mahalanobis metric learning. However, there are several other metric learning approaches that fall into our framework as well, including the large margin nearest neighbor met-ric learning method (LMNN) [11] and maximally collapsing metric learning (MCML) [14], both of which can be seen as instantiations of our learning framework with a constant f , as well as rel-evant component analysis (RCA) [28] and Xing et al. X  X  Mahalanobis metric learning method for clustering [10]. Given lack of space, we cannot detail the kernelization of all these methods, but they follow in the same manner as in the above two examples. In particular, each of these methods may be run in kernel space, and our analysis yields new insights into these methods; for example, kernelization of LMNN [11] using Theorem 1 avoids the convex perturbation analysis in [16] that leads to suboptimal solutions in some cases. 3.2 Parameter Reduction One of the drawbacks to Theorem 1 is that the size of the matrices K W and S are n  X  n , and thus grow quadratically with the number of data points. We would like to have a way to restrict our optimization over a smaller number of parameters, so we now discuss a generalization of (2) by introducing an additional constraint to make it possible to reduce the number of parameters to learn, permitting scalability to data sets with many training points and with very high dimensionality. Theorem 1 shows that the optimal K  X  W is of the form T W  X  =  X K + KS  X  K . In order to accommodate fewer parameters to learn, a natural option is to replace the unknown S matrix with a low-rank matrix JLJ T , where J  X  R n  X  r is a pre-specified matrix, L  X  R r  X  r is unknown (we use L instead of S to emphasize that S is of size n  X  n whereas L is r  X  r ), and the rank r is a parameter of the algorithm. Then, we will explicitly enforce that the learned kernel is of this form. By plugging in K W =  X K + KSK into (1) and replacing S with JLJ T , the resulting optimization problem is given by: While the above problem involves just r  X  r variables, the functions f and g i  X  X  are applied to n  X  n matrices and therefore the problem may still be computationally expensive to optimize. Below, we problem that applies f and g i  X  X  to r  X  r matrices only, which provides significant scalability. Theorem 2. Let K = T  X  0 and J  X  R n  X  r . Also, let the regularization function f be a spectral function (see Definition 3.1) such that the corresponding scalar function f s has a global minima at  X  . Then problem (6) is equivalent to the following problem: Note that (7) is over r  X  r matrices (after initial pre-processing) and is in fact similar to the kernel learning problem (1), but with a kernel K J of smaller size r  X  r , r  X  n . A proof of the above theorem is in the supplementary material, and follows by showing that for spectral functions the objective functions of the two problems can be shown to differ by a universal constant. Similar to (1), we can show that (6) is also equivalent to linear transformation kernel function learn-ing. This enables us to naturally apply the above kernel learning problem in the inductive setting. We provide a proof of the following theorem in the supplementary material.
 Theorem 3. Consider (6) with a spectral function f so that corresponding scalar function f s has a global minima at  X  and let K  X  0 be invertible. Then, (6) and (7) are equivalent to the following linear transformation kernel learning problem (analogous to the connection between (1) and (2) ): Note that, in contrast to (2), where the last constraint over W is achieved automatically, (8) requires that constraint should be satisfied during the optimization process which leads to a reduced number of parameters for our kernel learning problem. The above theorem shows that our reduced parame-ters kernel learning method (6) also implicitly learns a linear transformation kernel function, hence we can generalize the learned kernel to unseen data points using an expression similar to (3). The parameter reduction approach presented in this section depends critically on the choice of J . A few simple heuristics for choosing J beyond choosing a subset of the points from include a randomly sampled coefficient matrix or clustering into r clusters such that J is the cluster membership indicator function. Also note that using this parameter reduction technique, we can scale the optimization to kernel learning problems with millions of points of more. For example, we have applied a special case of this scalable framework to learn kernels over data sets containing nearly half a million images, as well as the MNIST data set of 60,000 data points [29]. We now consider applying our framework to the scenario of semi-supervised kernel dimensionality reduction, which provides a novel and practical application of our framework. While there exists a variety of methods for kernel dimensionality reduction, most of these methods are unsupervised (e.g. kernel-PCA) or are restricted to the transductive setting. In contrast, we can use our kernel learning framework to learn a low-rank transformation of the feature vectors implicitly that in turn provides a low-dimensional embedding of the dataset. Furthermore, our framework permits a variety of side-information such as pair-wise or relative distance constraints, beyond the class label information allowed by existing transductive methods.
 We describe our method starting from the linear transformation problem. Our goal is to learn a low-rank linear transformation W whose corresponding low-dimensional mapped embedding of  X  i is W 1 / 2  X  i . Even when the dimensionality of  X  i is very large, if the rank of W is low enough, then the mapped embedding will have small dimensionality. With that in mind, a possible regularizer could function. Unfortunately, optimization is intractable in general with the non-convex rank function, so we use the trace-norm relaxation for the matrix rank function, i.e., we set f ( A ) = Tr ( A ) . This function has been extensively studied as a relaxation for the rank function [30], and it satisfies the definition of a spectral function (with  X  = 0 ). We also add a small Frobenius norm regularization for ease of optimization (this does not affect the spectral property of the regularization function). Then using Theorem 1, the resulting relaxed kernel learning problem is: min where  X  &gt; 0 is a parameter. The above problem can be solved using a method based on Uzawa X  X  inexact algorithm, similar to [31].
 We briefly describe the steps taken by our method at each iteration. For simplicity, denote ~ K = K Table 1: UCI Datasets: accuracy achieved by various methods. The numbers in parentheses show the rank of the corresponding learned kernels. Trace-SSIKDR achieves accuracy comparable to Frob (Frobenius norm regularization) and ITML (LogDet regularization) with a significantly smaller rank. be the step size at iteration t . The algorithm performs the following updates: The above updates require computation of K 1 / 2 which is expensive for large high-rank matrices. However, using elementary linear algebra we can show that ~ K and the learned kernel function can be computed efficiently without computing K 1 / 2 by maintaining S = K  X  1 / 2 ~ KK  X  1 / 2 from step to step. Algorithm 1 details an efficient method for optimizing (9) and returns matrices k , D k and V k all of which are contain only O ( nk ) parameters, where k is the rank of changes from iteration to iteration. Note that step 4 of the algorithm computes k singular vectors and requires O ( nk 2 ) . Since k is typically significantly smaller than n , the computational cost will be significantly smaller than computing the whole SVD. Note that the learned embedding  X  i  X  ~ K 1 / 2 K  X  1 / 2 k i , where k i is a vector of input kernel function values between  X  i and the training We defer the proof of correctness for Algorithm 1 to the supplementary material.
 Algorithm 1 Trace-SSIKDR Require: K , ( C i , b i ) , 1  X  i  X  m ,  X  ,  X  1: Initialize: z 0 i = 0 , t = 0 2: repeat 3: t = t + 1 4: Compute V k and k , the top k eigenvectors and eigenvalues of 5: D k ( i, i )  X  1 / v T i K v i , 1  X  i  X  k 7: until Convergence 8: Return k , D k , V k We now present empirical evaluation of our kernel learning framework and our semi-supervised kernel dimensionality approach when applied in conjunction with k -nearest neighbor classification. In particular, using different regularization functions, we show that our framework can be used to obtain significantly better kernels than the baseline kernels for k -NN classification. Additionally, we show that our semi-supervised kernel dimensionality reduction approach achieves comparable accuracy while significantly reducing the dimensionality of the linear mapping.
 UCI Datasets: First, we evaluate the performance of our kernel learning framework on standard UCI datasets. We measure accuracy of the learned kernels using 5 -NN classification with two-fold cross validation averaged over 10 runs. For training, we use pairwise (dis)similarity constraints as described in Section 2.1. We select parameters l and u (right-hand side of the pairwise constraints) using 5 th and 95 th percentiles of all the pairwise distances between points from the training dataset. Figure 1: (a) : Mean classification accuracy on Caltech101 dataset obtained by 1 -NN classification with learned kernels obtained by various methods. (b) : Rank of the learned kernel functions obtained by various methods. The rank of the learned kernel function is same as the reduced dimensionality of the dataset. (c) : Two-dimensional embedding of 2000 USPS digits obtained using our method Trace-SSIKDR for a training set of just 100 USPS digits. Note that we use the inductive setting here and the embedding is color coded according to the underlying digit. (d) : Embedding of the USPS digits dataset obtained using kernel-PCA.
 Table 4 shows the 5 -NN classification accuracies achieved by our kernel learning framework with different regularization functions. Gaussian represents the baseline Gaussian kernel, Frob represents an instantiation of our framework with Frobenius norm ( f ( A ) =  X  A  X  2 F ) regularization, while ITML corresponds to the LogDet regularization ( f ( A ) = Tr ( A )  X  log det( A ) ). For the latter case, our formulation is same as formulation proposed by [12]. Note that for almost all the datasets (except Iris and Diabetes), both Frob and ITML improve upon the baseline Gaussian kernel significantly. We also compare our semi-supervised dimensionality reduction method Trace-SSIKDR (see Sec-tion 4) with baseline kernel dimensionality reduction methods Frob LR , ITML LR-pre , and ITML LR-post . Frob LR reduces the rank of the learned matrix W (equivalently, it reduces the dimension-ality) using Frobenius norm regularization by taking the top eigenvectors. Similarly, ITML LR-post reduces the rank of the learned kernel matrix obtained using ITML by taking its top eigenvectors. ITML LR-pre reduces the rank of the kernel function by reducing the rank of the training kernel ma-trix. The learned linear transformation W (or equivalently, the learned kernel function) should have the same rank as that of training kernel matrix as the LogDet divergence preserves the range space of the input kernel. We fix the rank of the learned W for Frob LR, ITML LR-pre, ITML LR-post as the rank of the transformation W obtained by our Trace-SSIKDR method. Note that Trace-SSIKDR achieves accuracies similar to Frob and ITML, while decreasing the rank significantly. Furthermore, it is significantly better than the corresponding baseline dimensionality reduction methods. Caltech-101: Next, we evaluate our kernel learning framework on the Caltech-101 dataset, a bench-mark object recognition dataset containing over 3000 images. Here, we compare various methods using 1 -NN classification method and the accuracy is measured in terms of the mean recognition accuracy per class. We use a pool of 30 images per class for our experiments, out of which a vary-ing number of random images are selected for training and the remaining are used for testing the learned kernel function. The baseline kernel function is selected to be the sum of four different kernel functions: PMK [32], SPMK [33], Geoblur-1 and Geoblur-2 [34]. Figure 1 (a) shows the accuracy achieved by various methods (acronyms represent the same methods as described in the previous section). Clearly, ITML and Frob (which are specific instances of our framework) are able to learn significantly more accurate kernel functions than the baseline kernel function. Furthermore, our Trace-SSIKDR method is able to achieve reasonable accuracy while reducing the rank of the kernel function significantly (Figure 1 (b)). Also note that Trace-SSIKDR achieves significantly better accuracy than Frob LR, ITML LR-pre and ITML LR-post, although all of these methods have the same rank as Trace-SSIKDR.
 USPS Digits: Finally, we qualitatively evaluate our dimensionality reduction method on the USPS digits dataset. Here, we train our method using 100 examples to learn a linear mapping to two dimensions, i.e., a rank-2 matrix W . For the baseline kernel, we use the data-dependent kernel func-tion proposed by [25] that also takes data X  X  manifold structure into account. We then embed 2000 (unseen) test examples into two dimensions using our learned low-rank transformation. Figure 1 (c) shows the embedding obtained by our Trace-SSIKDR method, while Figure 1 (d) shows the embed-ding obtained by the kernel-PCA algorithm. Each point is color coded according to the underlying digit. Note that our method is able to separate out most of the digits even in 2D, and is significantly better than the embedding obtained using kernel-PCA.
 Acknowledgements: This research was supported in part by NSF grant CCF-0728879.
