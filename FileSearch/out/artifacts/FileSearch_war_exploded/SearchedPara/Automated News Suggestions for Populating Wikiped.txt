 Wikipedia entity pages are a valuable source of information for di-rect consumption and for knowledge-base construction, update and maintenance. Facts in these entity pages are typically supported by references. Recent studies show that as much as 20% of the refer-ences are from online news sources. However, many entity pages are incomplete even if relevant information is already available in existing news articles. Even for the already present references, there is often a delay between the news article publication time and the reference time. In this work, we therefore look at Wikipedia through the lens of news and propose a novel news-article sugges-tion task to improve news coverage in Wikipedia, and reduce the lag of newsworthy references. Our work finds direct application, as a precursor, to Wikipedia page generation and knowledge-base ac-celeration tasks that rely on relevant and high quality input sources.
We propose a two-stage supervised approach for suggesting news articles to entity pages for a given state of Wikipedia. First, we sug-gest news articles to Wikipedia entities (article-entity placement) relying on a rich set of features which take into account the sali-ence and relative authority of entities, and the novelty of news arti-cles to entity pages. Second, we determine the exact section in the entity page for the input article (article-section placement) guided by class-based section templates. We perform an extensive evalu-ation of our approach based on ground-truth data that is extracted from external references in Wikipedia. We achieve a high preci-sion value of up to 93% in the article-entity suggestion stage and upto 84% for the article-section placement . Finally, we compare our approach against competitive baselines and show significant improvements.
 H3.3 [ Information Systems ]: Information Storage and Retrieval X  Information Search and Retrieval
Wikipedia is the largest source of open and collaboratively cu-rated knowledge in the world. Introduced in 2001, it has evolved c  X  Figure 1: Comparing how cyclones are reported in Wikipedia entity pages. into a reference work with around 5m pages for the English Wiki-pedia alone. In addition, entities and event pages are updated quick-ly via collaborative editing and all edits are encouraged to include source citations, creating a knowledge base which aims at being both timely as well as authoritative. As a result, it has become the preferred source of information consumption about entities and events 1 . Moreso, this knowledge is harvested and utilized in build-ing knowledge bases like YAGO [19] and DBpedia [4], and used in applications like text categorization [24], entity disambiguation [11], entity ranking [13] and distant supervision [20, 14].

However, not all Wikipedia pages referring to entities (entity pages) are comprehensive: relevant information can either be miss-ing or added with a delay . Consider the city of New Orleans and the state of Odisha which were severely affected by cyclones Hur-ricane Katrina and Odisha Cyclone , respectively. While Katrina finds extensive mention in the entity page for New Orleans , Odisha Cyclone which has 5 times more human casualties (cf. Figure 1) is not mentioned in the page for Odisha . Arguably Katrina and New Orleans are more popular entities, but Odisha Cyclone was also re-ported extensively in national and international news outlets. This highlights the lack of important facts in trunk and long-tail entity pages, even in the presence of relevant sources. In addition, previ-
Wikipedia is one of the Top 10 viewed page sites and the top ref-erence site according to Alexa Internet ranking www.alexa.com . ous studies have shown that there is an inherent delay or lag when facts are added to entity pages [10].

To remedy these problems, it is important to identify informa-tion sources that contain novel and salient facts to a given entity page. However, not all information sources are equal. The online presence of major news outlets is an authoritative source due to ac-tive editorial control and their articles are also a timely container of facts. In addition, their use is in line with current Wikipedia editing practice, as is shown in [10] that almost 20% of current citations in all entity pages are news articles. We therefore propose news suggestion as a novel task that enhances entity pages and reduces delay while keeping its pages authoritative.

Existing efforts to populate Wikipedia [18] start from an entity page and then generate candidate documents about this entity us-ing an external search engine (and then post-process them). How-ever, such an approach lacks in (a) reproducibility since rankings vary with time with obvious bias to recent news (b) maintainabil-ity since document acquisition for each entity has to be periodi-cally performed. To this effect, our news suggestion considers a news article as input, and determines if it is valuable for Wikipedia. Specifically, given an input news article n and a state of Wikipedia, the news suggestion problem identifies the entities mentioned in n whose entity pages can improve upon suggesting n . Most of the works on knowledge base acceleration [2, 1, 8], or Wikipedia page generation [18] rely on high quality input sources which are then utilized to extract textual facts for Wikipedia page population. In this work, we do not suggest snippets or paraphrases but rather en-tire articles which have a high potential importance for entity pages. These suggested news articles could be consequently used for ex-traction, summarization or population either manually or automati-cally  X  all of which rely on high quality and relevant input sources.
We identify four properties of good news recommendations: sali-ence , relative authority , novelty and placement . First, we need to identify the most salient entities in a news article. This is done to avoid pollution of entity pages with only marginally related news. Second, we need to determine whether the news is important to the entity as only the most relevant news should be added to a precise reference work. To do this, we compute the relative authority of all entities in the news article: we call an entity more authoritative than another if it is more popular or noteworthy in the real world. Entities with very high authority have many news items associated with them and only the most relevant of these should be included in Wikipedia whereas for entities of lower authority the threshold for inclusion of a news article will be lower. Third, a good rec-ommendation should be able to identify novel news by minimizing redundancy coming from multiple news articles. Finally, addition of facts is facilitated if the recommendations are fine-grained, i.e., recommendations are made on the section level rather than the page level ( placement ).

Approach and Contributions. We propose a two-stage news suggestion approach to entity pages. In the first stage, we determine whether a news article should be suggested for an entity, based on the entity X  X  salience in the news article, its relative authority and the novelty of the article to the entity page. The second stage takes into account the class of the entity for which the news is suggested and constructs section templates from entities of the same class. The generation of such templates has the advantage of suggesting and expanding entity pages that do not have a complete section struc-ture in Wikipedia, explicitly addressing long-tail and trunk entities. Afterwards, based on the constructed template our method deter-mines the best fit for the news article with one of the sections.
We evaluate the proposed approach on a news corpus consisting of 351,982 articles crawled from the news external references in Wikipedia from 73,734 entity pages. Given the Wikipedia snap-shot at a given year (in our case [2009-2014]), we suggest news articles that might be cited in the coming years. The existing news references in the entity pages along with their reference date act as our ground-truth to evaluate our approach. In summary, we make the following contributions.
As we suggest a new problem there is no current work address-ing exactly the same task. However, our task has similarities to Wikipedia page generation and knowledge base acceleration. In addition, we take inspiration from Natural Language Processing (NLP) methods for salience detection.

Wikipedia Page Generation is the problem of populating Wiki-pedia pages with content coming from external sources. Sauper and Barzilay [18] propose an approach for automatically generat-ing whole entity pages for specific entity classes. The approach is trained on already-populated entity pages of a given class (e.g.  X  Diseases  X ) by learning templates about the entity page structure (e.g. diseases have a treatment section). For a new entity page, first, they extract documents via Web search using the entity title and the section title as a query, for example  X  Lung Cancer  X + X  Treatment  X . As already discussed in the introduction, this has problems with re-producibility and maintainability. However, their main focus is on identifying the best paragraphs extracted from the collected docu-ments. They rank the paragraphs via an optimized supervised per-ceptron model for finding the most representative paragraph that is the least similar to paragraphs in other sections. This paragraph is then included in the newly generated entity page. Taneva and Weikum [21] propose an approach that constructs short summaries for the long tail. The summaries are called  X  gems  X  and the size of a  X  gem  X  can be user defined. They focus on generating summaries that are novel and diverse. However, they do not consider any struc-ture of entities, which is present in Wikipedia.

In contrast to [18] and [21], we actually focus on suggesting en-tire documents to Wikipedia entity pages. These are authoritative documents (news), which are highly relevant for the entity, novel for the entity and in which the entity is salient. Whereas relevance in Sauper and Barzilay is implicitly computed by web page ranking we solve that problem by looking at relative authority and salience of an entity, using the news article and entity page only. As Sauper and Barzilay concentrate on empty entity pages, the problem of novelty of their content is not an issue in their work whereas it is in our case which focuses more on updating entities. Updating entities will be more and more important the bigger an existing reference work is. Both the approaches in [18] and [21] (finding paragraphs and summarization) could then be used to process the documents we suggest further. Our concentration on news is also novel.
Knowledge Base Acceleration. In this task, given specific in-formation extraction templates, a given corpus is analyzed in order to find worthwhile mentions of an entity or snippets that match the templates. Balog [2, 1] recommend news citations for an entity. Prior to that, the news articles are classified for their appropriate-ness for an entity, where as features for the classification task they use entity, document, entity-document and temporal features. The best performing features are those that measure similarity between an entity and the news document. West et al. [25] consider the problem of knowledge base completion, through question answer-ing and complete missing facts in Freebase based on templates, i.e. Frank_Zappa bornIn Baltymore, Maryland .

In contrast, we do not extract facts for pre-defined templates but rather suggest news articles based on their relevance to an entity. In cases of long-tail entities, we can suggest to add a novel sec-tion through our abstraction and generation of section templates at entity class level.

Entity Salience. Determining which entities are prominent or salient in a given text has a long history in NLP, sparked by the linguistic theory of Centering [23]. Salience has been used in pro-noun and co-reference resolution [15], or to predict which entities will be included in an abstract of an article [8]. Frequent features to measure salience include the frequency of an entity in a docu-ment, positioning of an entity, grammatical function or internal en-tity structure (POS tags, head nouns etc.). These approaches are not currently aimed at knowledge base generation or Wikipedia cover-age extension but we postulate that an entity X  X  salience in a news article is a prerequisite to the news article being relevant enough to be included in an entity page. We therefore use the salience features in [8] as part of our model. However, these features are document-internal  X  we will show that they are not sufficient to predict news inclusion into an entity page and add features of entity authority, news authority and novelty that measure the relations between sev-eral entities, between entity and news article as well as between several competing news articles.
We are interested in named entities mentioned in documents. An entity e can be identified by a canonical name, and can be men-tioned differently in text via different surface forms . We canonical-ize these mentions to entity pages in Wikipedia, a method typically known as entity linking . We denote the set of canonicalized entities extracted and linked from a news article n as  X  ( n ) . For example, in Figure 2, entities are canonicalized into Wikipedia entity pages (e.g. Odisha is canonicalized to the corresponding article http://en.wikipedia.org/wiki/Odisha collection of news articles N , we further denote the resulting set of entities by E =  X  n  X  N { e i } .

Information in an entity page is organized into sections and evolves with time as more content is added. We refer to the state of Wiki-pedia at a time t as W t and the set of sections for an entity page e as its entity profile S e ( t ) . Unlike news articles, text in Wikipedia could be explicitly linked to entity pages through anchors. The set of en-tities explicitly referred in text from section s  X  S e ( t ) is defined as  X  ( s ) . Furthermore, Wikipedia induces a category structure over its entities, which is exploited by knowledge bases like YAGO (e.g. Barack_Obama isA Person ). Consequently, each entity page be-longs to one or more entity categories or classes c . Now we can define our news suggestion problem below: of news articles N = { n 1 ,..., n k } and set of Wikipedia entity pages E = { e 1 ,..., e m } (from W t ) we intend to suggest a news article n published at time t i &gt; t to entity page e and additionally to the most relevant section for the entity page s  X  S e ( t ) .
We approach the news suggestion problem by decomposing it into two tasks: 1. AEP : Article X  X ntity placement 2. ASP : Article X  X ection placement
In this first step, for a given entity-news pair  X  n , e  X  , we determine whether the given news article n  X  N should be suggested (we will refer to this as  X  X elevant X  ) to entity e  X  E . To generate such  X  n , e  X  pairs, we perform the entity linking process,  X  ( n ) , for n .
The article X  X ntity placement task (described in detail in Sec-tion 4.1) for a pair  X  n , e  X  outputs a binary label (either  X  X on-relevant X  or  X  X elevant X  ) and is formalized in Equation 1.

In the second step, we take into account all  X  X elevant X  pairs  X  n , e  X  and find the correct section for article n in entity e , respectively its profile S e ( t ) (see Section 4.2). The article X  X ection placement task, determines the correct section for the triple  X  n , e , S formalized in Equation 2.

In the subsequent sections we describe in details how we ap-proach the two tasks for suggesting news articles to entity pages.
In this section, we provide an overview of the news suggestion approach to Wikipedia entity pages (see Figure 2). The approach is split into two tasks: (i) article-entity ( AEP ) and (ii) article-section ( ASP ) placement. For a Wikipedia snapshot W t and a news corpus N , we first determine which news articles should be suggested to an entity e . We will denote our approach for AEP by F e we determine the most appropriate section for the ASP task and we denote our approach with F s .

In the following, we describe the process of learning the func-tions F e and F s . We introduce features for the learning process, which encode information regarding the entity salience , relative authority and novelty in the case of AEP task. For the ASP task, we measure the overall fit of an article to the entity sections, with the entity being an input from AEP task. Additionally, consider-ing that the entity profiles S e ( t ) are incomplete, in the case of a missing section we suggest and expand the entity profiles based on section templates generated from entities of the same class c (see Section 4.2.1).
In this step we learn the function F e to correctly determine whether n should be suggested for e , basically a binary classification model (0=  X  X on-relevant X  and 1=  X  X elevant X  ). Note that we are mainly in-terested in finding the relevant pairs in this task. For every news article, the number of disambiguated entities is around 30 (but n is suggested for only two of them on average). Therefore, the dis-tribution of  X  X on-relevant X  and  X  X elevant X  pairs is skewed towards the earlier, and by simply choosing the  X  X on-relevant X  label we can achieve a high accuracy for F e . Finding the relevant pairs is there-fore a considerable challenge.

An article n is suggested to e by our function F e if it fulfills the following properties. The entity e is salient in n (a central con-cept), therefore ensuring that n is about e and that e is important for n . Next, given the fact there might be many articles in which e is salient , we also look at the reverse property, namely whether n is important for e . We do this by comparing the authority of e (which is a measure of popularity of an entity, such as its frequency of mention in a whole corpus) with the authority of its co-occurring entities in  X  ( n ) , leading to a feature we call relative authority . The intuition is that for an entity that has overall lower authority than its co-occurring entities, a news article is more easily of importance. Finally, if the article we are about to suggest is already covered in the entity profile S e ( t ) , we do not wish to suggest redundant infor-mation, hence the novelty . Therefore, the learning objective of should fulfill the following properties. Table 1 shows a summary of the computed features for F e . 1. Salience: entity e should be a salient entity in news article n 2. Relative Authority: the set of entities e 0  X   X  ( n ) with which 3. Novelty: news article n should provide novel information for
Baseline Features. As discussed in Section 2, a variety of fea-tures that measure salience of an entity in text are available from the NLP community. We reimplemented the ones in Dunietz and Gillick [8]. This includes a variety of features, e.g. positional fea-tures, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in. Table 2 in [8] gives details.
Relative Entity Frequency. Although frequency of mention and positional features play some role in baseline features, their inter-action is not modeled by a single feature nor do the positional fea-tures encode more than sentence position. We therefore suggest a
This is why people occurring infrequently in the news keep any press cutting mentioning them. novel feature called relative entity frequency ,  X  ( e , n ) , that has three properties.: (i) It rewards entities for occurring throughout the text instead of only in some parts of the text, measured by the number of paragraphs it occurs in (ii) it rewards entities that occur more fre-quently in the opening paragraphs of an article as we model as an exponential decay function. The decay corresponds to the po-sitional index of the news paragraph. This is inspired by the news-specific discourse structure that tends to give short summaries of the most important facts and entities in the opening paragraphs. (iii) it compares entity frequency to the frequency of its co-occurring mentions as the weight of an entity appearing in a specific para-graph, normalized by the sum of the frequencies of other entities in  X  ( n ) .
 where, p represents a news paragraph from n , and with p ( n ) we indicate the set of all paragraphs in n . The frequency of e in a indicate the number of paragraphs in which entity e occurs, and the total number of paragraphs, respectively.
Relative Authority. In this case, we consider the comparative relevance of the news article to the different entities occurring in it. As an example, let us consider the meeting of the Sudanese bishop Elias Taban 4 with Hillary Clinton 5 . Both entities are salient for the meeting. However, in Taban X  X  Wikipedia page, this meeting is discussed prominently with a corresponding news reference whereas in Hillary Clinton X  X  Wikipedia page it is not reported at all. We believe this is not just an omission in Clinton X  X  page but mirrors the fact that for the lesser known Taban the meeting is big news whereas for the more famous Clinton these kind of meetings are a regular occurrence, not all of which can be reported in what is supposed to be a selection of the most important events for her. Therefore, if two entities co-occur, the news is more relevant for the entity with the lower a priori authority.

The a priori authority of an entity (denoted by  X  ( e ) ) can be mea-sured in several ways. We opt for two approaches: (i) probability of entity e occurring in the corpus N , and (ii) authority assessed through centrality measures like PageRank [16]. For the second case we construct the graph G = ( V , E ) consisting of entities in E and news articles in N as vertices . The edges are established be-tween n and entities in  X  ( n ) , that is  X  n  X   X  ( n )  X  , and the out-links from e , that is  X  e  X   X  ( s ( t  X  1 ))  X  (arrows present the edge direction).
Starting from a priori authority, we proceed to relative author-ity by comparing the a priori authority of co-occurring entities in  X  ( n ) . We define the relative authority of e as the proportion of co-occurring entities e 0  X   X  ( n ) that have a higher a priori authority than e (see Equation 4. As we might run the danger of not suggesting any news articles for entities with very high a priori authority (such as Clinton) due to the strict inequality constraint, we can relax the constraint such that the authority of co-occurring entities is above a certain threshold. http://en.wikipedia.org/wiki/Elias_Taban http://en.wikipedia.org/wiki/Hillary_Clinton http://tinyurl.com/mshf7j2
News Domain Authority. The news domain authority addresses two main aspects. Firstly, if bundled together with the relative au-thority feature, we can ensure that dependent on the entity author-ity, we suggest news from authoritative sources, hence ensuring the quality of suggested articles. The second aspect is in a news stream-ing scenario where multiple news domains report the same event  X  ideally only articles coming from authoritative sources would fulfill the conditions for the news suggestion task.

The news domain authority is computed based on the number of news references in Wikipedia coming from a particular news domain D . This represents a simple prior that a news article n is from domain D in corpus N . We extract the domains by taking the base URLs from the news article URLs.
An important feature when suggesting an article n to an en-tity e is the novelty of n w.r.t the already existing entity profile S ( t  X  1 ) . Studies [3] have shown that on comparable collections to ours (TREC GOV2) the number of duplicates can go up to 17%. This figure is likely higher for major events concerning highly au-thoritative entities on which all news media will report.
Given an entity e and the already added news references N { n 1 ,..., n k } up to year t  X  1, the novelty of n k + 1 at year t is mea-sured by the KL divergence between the language model of n and articles in N t  X  1 . We combine this measure with the entity over-lap of n k + 1 and n 0  X  N t  X  1 . The novelty value of n the minimal divergence value. Low scores indicate low novelty for the entity profile S e ( t ) .
 where D KL is the KL divergence of the language models (  X   X  ( n 0 ) ), whereas  X  is the mixing weight (  X  = { 0 ,..., 1 } ) between the language models D KL and the entity overlap in n and n
We model the ASP placement task as a successor of the AEP task. For all the  X  X elevant X  news entity pairs, the task is to determine the correct entity section. Each section in a Wikipedia entity page represents a different topic. For example, Barack Obama has the sections  X  X arly Life X ,  X  X residency X ,  X  X amily and Personal Life X  etc. However, many entity pages have an incomplete section structure. Incomplete or missing sections are due to two Wikipedia proper-ties. First, long-tail entities miss information and sections due to their lack of popularity. Second, for all entities whether popular or not, certain sections might occur for the first time due to real world developments. As an example, the entity Germanwings did not have an  X  X ccidents X  section before this year X  X  disaster, which was the first in the history of the airline.

Even if sections are missing for certain entities, similar sections usually occur in other entities of the same class (e.g. other airlines had disasters and therefore their pages have an accidents section). We exploit such homogeneity of section structure and construct templates that we use to expand entity profiles. The learning ob-jective for F s takes into account the following properties: 1. Section-templates: account for incomplete section structure 2. Overall fit: measures the overall fit of a news article to sec-
Given the fact that entity profiles are often incomplete, we con-struct section templates for every entity class . We group entities based on their class c and construct section templates b S ferent entity classes, e.g. Person and Location , the section struc-ture and the information represented in those section varies heavily. Therefore, the section templates are with respect to the individual classes in our experimental setup (see Figure 3).
Generating section templates has two main advantages. Firstly, by considering class-based profiles, we can overcome the problem of incomplete individual entity profiles and thereby are able to sug-gest news articles to sections that do not yet exist in a specific entity S ( t ) . The second advantage is that we are able to canonicalize the sections, i.e.  X  X arly Life X  and  X  X arly Life and Childhood X  would be treated similarly.

To generate the section template b S c , we extract all sections from entities of a given type c at year t . Next, we cluster the entity sec-tions, based on an extended version of k X  X eans clustering [12], namely x X  X eans clustering introduced in Pelleg et al. which esti-mates the number of clusters efficiently [17]. As a similarity metric we use the cosine similarity computed based on the tf X  X df models of the sections. Using the x X  X eans algorithm we overcome the re-quirement to provide the number of clusters k beforehand. x X  X eans extends the k X  X eans algorithm, such that a user only specifies a range [ K min , K max ] that the number of clusters may reasonably lie in.
The learning objective of F s is to determine the overall fit of a news article n to one of the sections in a given section template b S . The template is pre-determined by the class of the entity for which the news is suggested as relevant by F e . In all cases, we measure how well n fits each of the sections s  X  b S c ( t  X  1 ) as well as the specific entity section s 0  X  S e ( t  X  1 ) . The section profiles in b S ( t  X  1 ) represent the aggregated entity profiles from all entities of class c at year t  X  1.

To learn F s we rely on a variety of features that consider several similarity aspects as shown in Table 2. For the sake of simplicity we do not make the distinction in Table 2 between the individual entity section and class-based section similarities, s e ( t  X  1 ) and s ( t  X  1 ) , respectively. Bear in mind that an entity section s e might be present at year t but not at year t  X  1 (see for more details the discussion on entity profile expansion in Section 6.2.4).

Topic. We use topic similarities to ensure (i) that the content of n fits topic-wise with a specific section text and (ii) that it has a similar topic to previously referred news articles in that section. In a pre-processing stage we compute the topic models for the news articles, entity sections S e ( t  X  1 ) and the aggregated class-based sections in b S c . The topic models are computed using LDA [5]. We only computed a single topic per article/section as we are only interested in topic term overlaps between article and sections. We distinguish two main features: the first feature measures the overlap of topic terms between n and the entity section s e ( t  X  1 ) and s ( t  X  1 )  X  b S c , and the second feature measures the overlap of the topic model of n against referred news articles in N t  X  1 at time t  X  1.
Syntactic. These features represent a mechanism for conveying the importance of a specific text snippet, solely based on the fre-quency of specific POS tags (i.e. NNP, CD etc.), as commonly used in text summarization tasks. Following the same intuition as in [18], we weigh the importance of articles by the count of specific POS tags. We expect that for different sections, the importance of POS tags will vary. We measure the similarity of POS tags in a news article against the section text. Additionally, we consider bi-gram and tri-gram POS tag overlap. This exploits similarity in syntactical patterns between the news and section text.

Lexical. As lexical features, we measure the similarity of n against the entity section text s e ( t  X  1 ) and the aggregate section text s ( t  X  1 ) . Further, we distinguish between the overall similarity of n and that of the different news paragraphs ( p ( n ) which denotes the paragraphs of n up to the 5th paragraph). A higher similarity on the first paragraphs represents a more confident indicator that n should be suggested to a specific section s . We measure the sim-ilarity based on two metrics: (i) the KL-divergence between the computed language models and (ii) cosine similarity of the corre-sponding paragraph text p ( n ) and section text.

Entity-based. Another feature set we consider is the overlap of named entities and their corresponding entity classes . For different entity sections, we expect to find a particular set of entity classes that will correlate with the section, e.g.  X  Early Life  X  contains mostly entities related to family, school, universities etc.

Frequency. Finally, we gather statistics about the number of entities, paragraphs, news article length, top X  k entities and entity classes, and the frequency of different POS tags. Here we try to capture patterns of articles that are usually cited in specific sections.
In this section we outline the evaluation plan to verify the effec-tiveness of our learning approaches. To evaluate the news sugges-tion problem we are faced with two challenges.
Consider the ground truth challenge. Evaluating if an arbitrary news article should be included in Wikipedia is both subjective and difficult for a human if she is not an expert. An invasive approach, which was proposed by Barzilay and Sauper [18], adds content di-rectly to Wikipedia and expects the editors or other users to redact irrelevant content over a period of time. The limitations of such an evaluation technique is that content added to long-tail entities might not be evaluated by informed users or editors in the experi-ment time frame. It is hard to estimate how much time the added content should be left on the entity page. A more non-invasive approach could involve crowdsourcing of entity and news article pairs in an IR style relevance assessment setup. The problem of such an approach is again finding knowledgeable users or experts for long-tail entities. Thus the notion of relevance of a news rec-ommendation is challenging to evaluate in a crowd setup.

We take a slightly different approach by making an assumption that the news articles already present in Wikipedia entity pages are relevant. To this extent, we extract a dataset comprising of all news articles referenced in entity pages (details in Section 5.2). At the expense of not evaluating the space comprising of news articles ab-sent in Wikipedia, we succeed in (i) avoiding restrictive assump-tions about the quality of human judgments, (ii) being invasive and polluting Wikipedia, and (iii) deriving a reusable test bed for quicker experimentation.

The second challenge of construction of training and test set sep-aration is slightly easier and is addressed in Section 5.4.
The datasets we use for our experimental evaluation are directly extracted from the Wikipedia entity pages and their revision his-tory. The generated data represents one of the contributions of our paper. 7 The datasets are the following:
Entity Classes. We focus on a manually predetermined set of entity classes for which we expect to have news coverage. The number of analyzed entity classes is 27, including 73 , 734 entities with at least one news reference. The entity classes were selected from the DBpedia class ontology. Figure 3 shows the number of entities per class for the years (2009-2014). Figure 3: Number of entities with at least one news reference for different
News Articles. We extract all news references from the col-lected Wikipedia entity pages. 8 The extracted news references are associated with the sections in which they appear. In total there were 411 , 673 news references, and after crawling we end up with 351 , 982 successfully crawled news articles. The details of the news http://l3s.de/~fetahu/cikm2015/data/
A news reference in Wikipedia is denoted by the template {cite type= X  X ews X  | url= X  X  X  article distribution, and the number of entities and sections from which they are referred are shown in Table 3. Table 3: News articles, entities and sections distribution across years.
Article-Entity Ground-truth. The dataset comprises of the news and entity pairs  X  n , e  X  X  X { 0 , 1 } . News-entity pairs are relevant if the news article is referenced in the entity page. Non-relevant pairs (i.e. negative training examples) consist of news articles that contain an entity but are not referenced in that entity X  X  page. If a news article n is referred from e at year t , the features are computed taking into account the entity profiles at year S e ( t  X  1 ) .

Article-Section Ground-truth. The dataset consists of the triple determined as relevant. We therefore have a multi-class classifica-tion problem where we need to determine the section of e where n is cited. Similar to the article-entity ground truth, here too the features compute the similarity between n , S e ( t  X  1 ) and We POS-tag the news articles and entity profiles S e ( t ) with the Stanford tagger [22]. For entity linking the news articles, we use TagMe![9] with a confidence score of 0.3. On a manual inspection of a random sample of 1000 disambiguated entities, the accuracy is above 0.9. On average, the number of entities per news article is approximately 30. For entity linking the entity profiles, we simply follow the anchor text that refers to Wikipedia entities. We evaluate the generated supervised models for the two tasks, AEP and ASP , by splitting the train and testing instances. It is im-portant to note that for the pairs  X  n , e  X  and the triple  X  n , e , news article n is referenced at time t by entity e , while the features take into account the entity profile at time t  X  1. This avoids any  X  X verlapping X  content between the news article and the entity page, which could affect the learning task of the functions F e Table 4 shows the statistics of train and test instances. We learn the functions at year t and test on instances for the years greater than t . Please note that we do not show the performance for year 2014 as we do not have data for 2015 for evaluation.
 Table 4: Number of instances for train and test in the AEP and ASP tasks.
Here we introduce the evaluation setup and analyze the results for the article X  X ntity (AEP) placement task. We only report the evaluation metrics for the  X  X elevant X  news-entity pairs. A detailed explanation on why we focus on the  X  X elevant X  pairs is provided in Section 4.1.
Baselines. We consider the following baselines for this task.
Learning Models. We use Random Forests (RF) [6]. 9 We learn the RF on all computed features in Table 1. The optimization on RF is done by splitting the feature space into multiple trees that are considered as ensemble classifiers. Consequently, for each classi-fier it computes the margin function as a measure of the average count of predicting the correct class in contrast to any other class. The higher the margin score the more robust the model.

Metrics. We compute precision P, recall R and F1 score for the relevant class. For example, precision is the number of news-entity pairs we correctly labeled as relevant compared to our ground truth divided by the number of all news-entity pairs we labeled as relevant.
The following results measure the effectiveness of our approach in three main aspects: (i) overall performance of F e and compar-ison to baselines, (ii) robustness across the years, and (iii) optimal model for the AEP placement task.

Performance. Figure 4 shows the results for the years 2009 and 2013, where we optimized the learning objective with instances from year t and evaluate on the years t i &gt; t (see Section 5.4). The results show the precision X  X ecall curve. The red curve shows baseline B1 [8], and the blue one shows the performance of The curve shows for varying confidence scores (high to low) the precision on labeling the pair  X  e , n  X  as  X  X elevant X  . In addition, at each confidence score we can compute the corresponding recall for the  X  X elevant X  label. For high confidence scores on labeling the news-entity pairs, the baseline B1 achieves on average a precision score of P=0.50, while F e has P=0.93. We note that with the drop in the confidence score the corresponding precision and recall val-ues drop too, and the overall F1 score for B1 is around F1=0.2, in contrast we achieve an average score of F1=0.67.
 It is evident from Figure 4 that for the years 2009 and 2013, significantly outperforms the baseline B1 . We measure the signifi-cance through the t-test statistic and get a p-value of 2 . 2 e  X  16. The improvement we achieve over B1 in absolute numbers,  X  P=+0.5 in terms of precision for the years between 2009 and 2014, and a similar improvement in terms of F1 score. The improvement for recall is  X  R=+0.4. The relative improvement over B1 for P and F1 is almost 1.8 times better, while for recall we are 3.5 times better. In Table 5 we show the overall scores for the evaluation metrics for B1 and F e . Finally, for B2 we achieve much poorer performance, with average scores of P=0.21, R=0.20 and F1=0.21.

Robustness. In Table 5, we show the overall performance for the years between 2009 and 2013. An interesting observation we make is that we have a very robust performance and the results are stable across the years. If we consider the experimental setup,
Our emphasis in this paper is not a comparison of learning models but of course other classifiers can be used for this task.
We only show the first year 2009 and the last year 2013, since the difference to the other years is marginal. where for year t = 2009 we optimize the learning objective with only 74k training instances and evaluate on the rest of the instances, it achieves a very good performance. We predict with F1=0.68 the remaining 469k instances for the years t  X  ( 2009 , 2014 ] .
The results are particularly promising considering the fact that the distribution between our two classes is highly skewed. On aver-age the number of  X  X elevant X  pairs account for only around 4  X  6% of all pairs. A good indicator to support such a statement is the kappa (denoted by  X  ) statistic.  X  measures agreement between the algorithm and the gold standard on both labels while correcting for chance agreement (often expected due to extreme distributions). The  X  scores for B1 across the years is on average 0 . 19, while for F e we achieve a score of 0 . 65 (the maximum score for  X  is 1).
In Figure 5 we show the impact of the individual feature groups that contribute to the superior performance in comparison to the baselines. Relative entity frequency from the salience feature, mod-els the entity salience as an exponentially decaying function based on the positional index of the paragraph where the entity appears. The performance of F e with relative entity frequency from the sali-ence feature group is close to that of all the features combined. The authority and novelty features account to a further improvement in terms of precision, by adding roughly a 7%-10% increase. How-ever, if both feature groups are considered separately, they signifi-cantly outperform the baseline B1 .
Here we show the evaluation setup for ASP task and discuss the results with a focus on three main aspects, (i) the overall perfor-mance across the years, (ii) the entity class specific performance, and (iii) the impact on entity profile expansion by suggesting miss-ing sections to entities based on the pre-computed templates.
Baselines. To the best of our knowledge, we are not aware of any comparable approach for this task. Therefore, the baselines we consider are the following:
Figure 5: Feature analysis for the AEP placement task for t = 2009. Learning Models. We use Random Forests (RF) [6] and Support Vector Machines (SVM) [7]. The models are optimized taking into account the features in Table 2. In contrast to the AEP task, here the scale of the number of instances allows us to learn the SVM mod-els. The SVM model is optimized using the  X   X  SV R loss function and uses the Gaussian kernels.

Metrics. We compute precision P as the ratio of news for which we pick a section s from b S c and s conforms to the one in our ground-truth (see Section 5.2). The definition of recall R and F1 score follows from that of precision.
Figure 6 shows the overall performance and a comparison of our approach (when F s is optimized using SVM) against the best per-forming baseline S2 . With the increase in the number of training instances for the ASP task the performance is a monotonically non-decreasing function. For the year 2009, we optimize the learning objective of F s with around 8% of the total instances, and evaluate on the rest. The performance on average is around P=0.66 across all classes. Even though for many classes the performance is al-ready stable (as we will see in the next section), for some classes we improve further. If we take into account the years between 2010 and 2012, we have an increase of  X  P=0.17, with around 70% of in-stances used for training and the remainder for evaluation. For the remaining years the total improvement is  X  P=0.18 in contrast to the performance at year 2009.
 On the other hand, the baseline S1 has an average precision of P=0.12. The performance across the years varies slightly, with the year 2011 having the highest average precision of P=0.13. Always picking the most frequent section as in S2 , as shown in Figure 6, re-sults in an average precision of P=0.17, with a uniform distribution across the years. Figure 6: Article-Section performance averaged for all entity classes for
Here we show the performance of F s decomposed for the dif-ferent entity classes. Specifically we analyze the 27 classes in Fig-ure 3. In Table 6, we show the results for a range of years (we omit showing all years due to space constraints). For illustration purposes only, we group them into four main classes ( { Person, Organization, Location, Event } ) and into the specific sub-classes shown in the second column in Table 6. For instance, the entity classes OfficeHolder and Politician are aggregated into Person  X  Politics .

It is evident that in the first year the performance is lower in con-trast to the later years. This is due to the fact that as we proceed, we can better generalize and accurately determine the correct fit of an article n into one of the sections from the pre-computed templates b S . The results are already stable for the year range ( 2009 , 2012 ] . For a few Person sub-classes, e.g. Politics , Entertainment , we achieve an F1 score above 0.9. These additionally represent classes with a sufficient number of training instances for the years [ 2009 , 2012 ] . The lowest F1 score is for the Criminal and Tele-vision classes. However, this is directly correlated with the insuf-ficient number of instances.

The baseline approaches for the ASP task perform poorly. S1 , based on lexical similarity , has a varying performance for different entity classes. The best performance is achieved for the class Per-son -Politics , with P=0.43. This highlights the importance of our feature choice and that the ASP cannot be considered as a lin-ear function , where the maximum similarity yields the best results. For different entity classes different features and combination of features is necessary. Considering that S2 is the overall best per-forming baseline, through our approach F s we have a significant improvement of over  X  P=+0.64.

The models we learn are very robust and obtain high accuracy, fulfilling our pre-condition for accurate news suggestions into the entity sections. We measure the robustness of F s through the statistic. In this case, we have a model with roughly 10 labels (corresponding to the number of sections in a template b S score we achieve shows that our model predicts with high confi-dence with  X  = 0 . 64.
The last analysis is the impact we have on expanding entity pro-files S e ( t ) with new sections. Figure 7 shows the ratio of sections for which we correctly suggest an article n to the right section in the section template b S c ( t ) . The ratio here corresponds to sections that are not present in the entity profile at year t  X  1, that is s /  X  S However, given the generated templates b S c ( t  X  1 ) , we can expand the entity profile S e ( t  X  1 ) with a new section at time t . In details, in the absence of a section at time t , our model trains well on similar sections from the section template b S c ( t  X  1 ) , hence we can predict accurately the section and in this case suggest its addition to the entity profile. With time, it is obvious that the expansion rate de-creases at later years as the entity profiles become more  X  X omplete X .
This is particularly interesting for expanding the entity profiles of long-tail entities as well as updating entities with real-world emerg-ing events that are added constantly. In many cases such missing sections are present at one of the entities of the respective entity class c . An obvious case is the example taken in Section 4.1, where the  X  X ccidents X  is rather common for entities of type Airline . However, it is non-existent for some specific entity instances, i.e Germanwings airline.

Through our ASP approach F s , we are able to expand both long-tail and trunk entities. We distinguish between the two types of entities by simply measuring their section text length. The real dis-tribution in the ground truth (see Section 5.2) is 27% and 73% are long-tail and trunk entities, respectively. We are able to expand the entity profiles for both cases and all entity classes without a signif-icant difference, with the only exception being the class Creative Work , where we expand significantly more trunk entities.
Figure 7: Correctly suggested news articles for s  X  S e ( t )  X  s /  X  S
In this work, we have proposed an automated approach for the novel task of suggesting news articles to Wikipedia entity pages to facilitate Wikipedia updating. The process consists of two stages. In the first stage, article X  X ntity placement, we suggest news articles to entity pages by considering three main factors, such as entity salience in a news article, relative authority and novelty of news articles for an entity page. In the second stage, article X  X ection placement, we determine the best fitting section in an entity page. Here, we remedy the problem of incomplete entity section pro-files by constructing section templates for specific entity classes. This allows us to add missing sections to entity pages. We carry out an extensive experimental evaluation on 351,983 news articles and 73,734 entities coming from 27 distinct entity classes. For the first stage, we achieve an overall performance with P=0.93, R=0.514 and F1=0.676, outperforming our baseline competitors significantly. For the second stage, we show that we can learn incre-mentally to determine the correct section for a news article based on section templates. The overall performance across different classes is P=0.844, R=0.885 and F1=0.860.

In the future, we will enhance our work by extracting facts from the suggested news articles. Results suggest that the news content cited in entity pages comes from the first paragraphs. However, challenging task such as the canonicalization and chronological or-dering of facts, still remain.
 Acknowledgements . This work is funded by the ERC Advanced Grant ALEXANDRIA (grant no. 339233). [1] K. Balog and H. Ramampiaro. Cumulative citation [2] K. Balog, H. Ramampiaro, N. Takhirov, and K. N X rv X g. [3] Y. Bernstein and J. Zobel. Redundant documents and search [4] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [6] L. Breiman. Random forests. Machine Learning , 45(1):5 X 32, [7] C.-C. Chang and C.-J. Lin. Libsvm: a library for support [8] J. Dunietz and D. Gillick. A new entity salience task with [9] P. Ferragina and U. Scaiella. Fast and accurate annotation of [10] B. Fetahu, A. Anand, and A. Anand. How much is wikipedia [11] J. Hoffart, M. A. Yosef, I. Bordino, H. F X rstenau, M. Pinkal, [12] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, [13] R. Kaptein, P. Serdyukov, A. De Vries, and J. Kamps. Entity [14] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant [15] V. Ng. Supervised noun phrase coreference research: The [16] L. Page, S. Brin, R. Motwani, and T. Winograd. The [17] D. Pelleg, A. W. Moore, et al. X-means: Extending k-means [18] C. Sauper and R. Barzilay. Automatically generating [19] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: A core [20] M. Surdeanu, D. McClosky, J. Tibshirani, J. Bauer, A. X. [21] B. Taneva and G. Weikum. Gem-based entity-knowledge [22] K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. [23] M. A. Walker, A. K. Joshi, and E. F. Prince. Centering theory [24] P. Wang and C. Domeniconi. Building semantic kernels for [25] R. West, E. Gabrilovich, K. Murphy, S. Sun, R. Gupta, and
