 We introduce a novel sentence ranking problem called explanatory sentence extraction (ESE) which aims to rank sentences in opinion-ated text based on their usefulness for helping users understand the detailed reasons of sentiments (i.e.,  X  X xplanatoriness"). We propose and study several general methods for scoring the explanatoriness of a sentence. We create new data sets and propose a new measure for evaluation. Experiment results show that the proposed meth-ods are effective, outperforming a state of the art sentence ranking method for standard text summarization.
 I.2.7 [ Artificial Intelligence ]: Natural Language Processing X  Text analysis ; H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  Information filtering Explanatory sentence ranking, Explanatoriness scoring, Opinion summarization
Most previous studies on opinion mining and summarization have focused on predicting sentiments of entities and aspect-based rat-ing for the entities [4, 5, 6]. Although these existing techniques can show the general opinion distribution (e.g. 70% positive and 30% negative opinions about battery life), they cannot provide the underlying reasons why people have positive or negative opinions about the product. Therefore, even if such an opinion summariza-tion technique is available, people would still need to read through the classified opinionated text collection to find out why people ex-pressed those opinions.

General automatic summarization techniques [3, 8, 9] can be used to shrink the size of text to read, but they generally extract sen-tences based on  X  X opularity X , and as a result, the output summary tends to cover already known information. For example, to sum-marize  X  X ositive opinions about iPhone screen X , a pure popularity-based summary may be  X  X creen is good X . Given that the sentences to be summarized are already known to be about  X  X ositive opin-ions about iPhone screen X , such a summary is obviously redundant and does not give any additional information to explain the reason why the positive opinion about iPhone screen is held. A more ex-planatory summary, such as  X  X etina display is very clear X , would be much more useful to the users.

In this paper, we introduce and study a novel sentence ranking problem called explanatory sentence extraction (ESE) which aims to rank sentences in opinionated text based on their usefulness for helping users understand the reasons of sentiments. As can be seen in the previous example, explanatory sentences should not only be relevant to the target topic we are interested in, but also include de-tails explaining reasons of sentiments; generic positive or negative sentences are generally not explanatory.

The main technical challenge in solving this problem is to assess the explanatoriness of a sentence in explaining sentiment. We fo-cus on studying how to solve this problem in an unsupervised way. Compared with a supervised approach which requires manually la-beled training data, an unsupervised approach has the practical ad-vantage of not requiring any manual effort, and being applicable to many different domains. Moreover, in case when we have labeled data available, we can always plug in an unsupervised approach into any supervised learning approach as a feature.

We propose three heuristics for scoring explanatoriness of a sen-tence (i.e., length, popularity, and discriminativeness). In addition to the representativeness of information which is a main criterion used in the existing summarization work, we also consider discrim-inativeness with respect to background information and lengths of sentences. We propose two general new methods for scoring ex-planatoriness of a sentence based on these heuristics, including a method adapted from TF-IDF weighting and a probabilistic model based on word-level likelihood ratios.
 We created two new data sets and proposed a novel weighted Mean Average Precision measure to evaluate the proposed explana-toriness scoring methods. Experiment results show that all the pro-posed methods are effective in selecting explanatory sentences, out-performing a state of the art sentence ranking method taken from a regular text summarization method.
Our problem formulation is based on the assumption that exist-ing techniques can be used to (1) classify review sentences into different aspects (i.e., subtopics); and (2) identify the sentiment polarity of an opinionated sentence (i.e., either positive or nega-tive). Thus, as a computational problem, the assumed input is (1) a topic T as described by a phrase (e.g., iPhone), (2) an aspect A as described by a phrase (e.g.,  X  X creen X  of iPhone), (3) a polarity of sentiment P (on the specified aspect A of topic T ), which is ei-ther  X  X ositive X  or  X  X egative X , and (4) a set of opinionated se ntences O = { S 1 , ..., S n } of the sentiment polarity P .

Given T , A , P , and O as input, the desired output is a ranked list of all sentences in O by their explanatoriness, L = ( S  X  where S  X  i  X  O and explanatory sentences would be ranked on top of non-explanatory ones. Such a ranked list can be directly useful to help users digest opinions or fed into any existing summarization algorithm to generate an explanatory opinion summary.
We first propose three heuristics that may be potentially helpful for designing an explanatoriness scoring function. 1. Sentence length: A longer sentence is more likely explanatory than a shorter one since a longer sentence in general conveys more information. 2. Popularity and representativeness: A sentence is more likely explanatory if it contains more terms that occur frequently in all the sentences in O . This intuition is essentially the main idea used in the current standard extractive summarization techniques. We thus can reuse an existing summarization scoring function such as LexRank for scoring explanatoriness. However, as we will show later, there are more effective ways to capture popularity than an existing standard summarization method; probabilistic models are especially effective. 3. Discriminativeness relative to background: A sentence with more discriminative terms that can distinguish O from background information is more likely explanatory. As illustrated by the exam-ple in Section 1, over emphasis on representativeness would give us redundant information. Explanatory sentences should provide us more specific information about the given topic. Therefore, in-tuitively, an explanatory sentence would more likely contain terms that can help distinguish the set of sentences to be summarized O from more general background sets which contain opinions that are not as specific as those in O . That is, we can reward a sentence that has more discriminative terms, i.e., terms that are frequent in O , but not well covered in a background set.

There may be different ways to construct the background set, which in general would be a superset of O . In our problem setting, the set O consists of sentences satisfying the constraints that they cover aspect A of topic T with sentiment polarity P . We can con-struct a background set by relaxing any of these constraints. For example, we may drop the requirement on polarity P to construct a background set consisting of sentences about aspect A of topic T (regardless sentiment polarity), or drop the requirement on aspect A to obtain a background set with sentences covering broadly topic T with sentiment polarity P . The most general background set would consist of all sentences about topic T , which can be easily obtained and has been used in our experiments.

While all the three heuristics make sense, how to combine them optimally remains a challenging question. Over empahsis on any single factor would likely lead to unsatisfactory results. For ex-ample, if we only focus on popularity, we would likely rank non-informative sentences high. On the contrary, if we only focus on discriminativeness, the rare opinions which are mentioned only a few times by eccentric users may be ranked very high. Below we present two general methods to combine these heuristics for ex-planatoriness scoring.
The first method is to adapt an existing ranking function of infor-mation retrieval such as BM25 [7], which is one of the most effec-tive basic information retrieval functions. Indeed, our popularity heuristic can be captured through Term Frequency (TF) weighting, while the discriminativeness can be captured through Inverse Doc-ument Frequency (IDF) weighting. We thus propose the following modified BM25 for explanatoriness scoring (BM25 E ).

For explanatoriness ranking, we can consider a sentence as a query and measure explanatoriness of each word of the sentence based on how frequent the word is in the input data set ( O ) and the background data set ( B ). Specifically, given a sentence S = w , w 2 , ..., w n , the modified BM25 E would be defined as: where c ( w, O ) is the count of w in data set O , | O | and | B | is the total number of term occurrences in data set O and B respectively, and avgdl is the average number of total term occurrences of sub-clusters in T that O is extracted from. k 1 and b are parameters to be empirically set. The length heuristic is implicitly captured by this scoring function since the sum is taken over all the words in S .
The basic idea of our second method is to define the explana-toriness of a sentence as the sum of the explanatoriness of each word in the sentence, and model the explanatoriness of each word with a probabilistic model. Indeed, if we treat each word as a unit for modeling explanatoriness, it would be natural to define the ex-planatoriness of a sentence based on how explanatory each word in the sentence is. Thus if we use ES ( S ) and ES ( w ) to denote ex-planatoriness score of sentence S and that of word w respectively, we can define ES ( S ) as the sum of ES ( w ) over all the words in S , i.e. ES ( S ) = P w  X  S ES ( w ) . Note that with this strategy, we explicitly encode the length heuristic, and a longer sentence would be generally favored.

Now the question is how to model and estimate ES ( w ) . For this purpose, we assume that each word w can be either explanatory or not, denoted by E  X  X  0 , 1 } and score the explanatoriness of word w based on the conditional probability p ( E = 1 | w ) , which can be interpreted as the posterior probability that word w is explanatory. That is, ES ( w ) = p ( E = 1 | w ) . Since ranking sentences based on p ( E = 1 | w ) is equivalent to ranking them based on p ( E =1 | w ) will use the latter. According to Bayes rule, the score is With these estimates, the scoring function would be: We call this function Sum of Word Likelihood Ratio ( SumWordLR ). The explanatoriness score of a word is now seen as the ratio of the probability that word w is observed from an explanatory source (i.e., E = 1 ) to that it is observed from a non-explanatory source (i.e., E = 0 ). We have two sets of parameters that have to be estimated, i.e., p ( w | E = 1) and p ( w | E = 0) .

In general, our estimate of p ( w | E = 1) and p ( w | E = 0) would depend on what kind of words we would regard as explanatory. For estimating p ( w | E = 1) , without additional knowledge, a reason-able assumption is that the set of sentences to be summarized O can be used as an approximate of sample of words that are explanatory, which is to say that we will use O to approximate the explanatory source. With maximum likelihood estimate, we have W ith this estimate, a word that is popular in O would tend to have a higher explanatoriness score, capturing the popularity heuristic.
For estimating p ( w | E = 0) , we use all the text available to us about this topic T (denoted by B topic for background of all text about the topic). In this case, the maximum likelihood estimator would give Though we did not explore in this paper, these estimates may be further extended using various smoothing techniques.
Because there is no data set for evaluation of explanatory opin-ion extraction, we created two new data sets. The first is based on a publicly available product review data set used in previous works [1, 4], which is one of the most popular data sets for opin-ion mining. The sentences in this data set are already tagged with sentiment polarities and classified into different aspects, thus we can take them as the input data for evaluating our algorithm. The second data set is in a different domain, i.e., hotel. We crawled the hotel reviews from trip advisor 1 and generated aspect and sen-timent labels for each sentence in the data set with three human labelers. We filtered out disagreed labels.

Our key evaluation questions are whether the proposed methods can really find explanatory sentences, and which of the two pro-posed methods is more effective. To answer these questions, we need to further create gold standard labels about the explanatori-ness of all the sentences. To this end, we first clustered sentences in each data set based on their aspect and sentiment labels. Each cluster thus consists of a set of opinionated sentences (i.e., O ) cor-responding to a unique ( T, A, P ) . We discarded clusters with fewer than 10 sentences because such a small cluster does not really need summarization. We then provided the topic label of each cluster, ( T, A, P ) , to two human labelers and asked them to judge whether each sentence is explanatory or not in each cluster. A sentence is explanatory if it helps explain why the reviewer holds a particular polarity ( P ) of opinions about aspect A of topic T . Human labelers generated a binary label for each sentence in this way. The detailed labeling instructions, labeling examples provided to labelers, and the generated labels for the product review data set are available at a public website 2 .

The generated test data sets include totally 89 topic clusters with 3799 sentences. The label agreement, defined as the ratio of num-ber of sentences having the same label to the total number of sen-tences, is 0.69. To ensure reliable evaluation, we discarded sen-tences which have no agreed labels, as well as topics that have no explanatory sentences. The input data set is pre-processed by a ba-sic stemmer to alleviate the problem of data sparseness.
Our task is ranking sentences based on the predicted explana-toriness of a sentence. Thus we can use the standard information retrieval measure, Mean Average Precision ( MAP ), to quantita-tively evaluate the ranking accuracy.
Intuitively, it is more interesting to see the performance of a method on a topic where the percentage of explanatory sentences is low (i.e.,  X  X ard X  topics) than that on a topic where most sentences in O are explanatory (i.e.,  X  X asy X  topics) since even a random ranking of sentences may perform well for an easy topic. Thus, when we take the average of performance on individual topics, it would be reasonable to place more weight on a hard topic than on an easy topic. To implement this idea, we propose a general way to quan-tify the  X  X oom for improvement X  of a topic based on the gap be-tween the expected performance of a random ranking and the best performance achieved by an ideal ranking.
 Formally, suppose p is a performance measure. Given a topic Q , let R ( Q ) be the ranking results for Q . Let p random expected performance of a random ranking for Q , and p ideal be the performance of an ideal ranking for Q . We can measure the  X  X oom for improvement X  by: Intuitively, gap ( Q ) would be large if the performance of a random ranking is much lower than that of the performance of the ideal ranking, while if the random ranking has perfect performance (as, e.g., in the case that all the sentences in O are explanatory), gap ( Q ) would be zero.

We can then use gap ( Q ) as a weight on each topic to combine the performance on different topics. Formally, the aggregated per-formance on a set of queries { Q 1 , ..., Q m } is given by: where R ( Q i ) is the ranking results on topic Q i and p ( R ( Q the performance value of R ( Q i ) as measured using measure p . It is easy to see that we put more weight on a query with more room for improvement, and if the random ranking gives ideal performance, we would essentially eliminate the topic from the evaluation. We apply this weighting strategy to MAP and denote it by wMAP (for weighted MAP ).

The MAP score of an ideal ranking is always 1 . 0 . To compute the MAP score of a random ranking, we note that the probabil-ity that an explanatory sentence would be retrieved at each rank is equal to the ratio of explanatory sentences in the input sentence set. Thus, for given topics { Q 1 , ..., Q m } , suppose n i is the number of sentences, and k i is the number of explanatory sentences of topic Q , we will have
T hus according to Equation 1 and 2, we obtain wMAP as
We hypothesize that the proposed sentence ranking methods would work better than that of standard text summarization method which mostly implements the popularity heuristic. As a baseline to test this hypothesis, we use LexRank [2] sentence ranking method, which is popularly used in a general summarization algorithm and can generate ranking score of sentences.
In Table 1 we compare the wMAP values of the LexRank base-line and the proposed methods on both data sets. LexRank and SumWordLR do not have any parameter to tune, so their perfor-mance figures are on all the test topics in the two data sets. BM 25 has two parameters, and we used two-fold cross validation to s et the parameters and evaluate its performance. The wMAP values shown in the table are the average over all the test instances in the two data sets, respectively.

As a reference point, the wMAP score of random ranking is 0.4494 for the product data set, and 0.4764 for the hotel data set. It is clear that all methods perform better than the random ranking as we should expect. We further observe that all our methods out-perform the LexRank baseline. This confirms our hypothesis that a combination of both representativeness and discriminativeness in the proposed methods is more effective than mostly relying on the representativeness (i.e., popularity) as in the case of LexRank. T-test shows that the improvement of our methods over LexRank is statistically significant with 95% confidence level (marked with a ). For the hotel data set, although the performance values of our methods are higher than those from LexRank, the difference is not statistically significant. We suspect that this is because hotel data set has only 23 topics, not big enough to show statistically signifi-cant differences. Another possible explanation is that the centrality-based method LexRank suffers less when there are more sentences in one input sentence set where the redundancy may have helped the popularity heuristic to pick up explanatory sentences. Table 2: Sample summaries ( T :MP3player1, A:sound, P:+) .
In Table 2, we show an example of explanatory summary gen-e rated by a proposed method and a baseline summary generated by LexRank about  X  X ositive opinion about MP3 player sound X . We generated a summary by picking sentences one by one from the top of the ranked list generated by the explanatory ranking algorithm until the length limit (500 characters) is reached. To simulate actual usage of the scoring function in summarization setup, we also ap-ply simple redundancy removal technique. That is, when we pick sentences from the top of the explanatoriness ordered list, if more than 50% of the content of the next candidate sentence is covered by already selected sentences, we would skip, and do not include the candidate sentence in the summary.

We can easily see the benefits of the explanatory summary over the general summary from the results in Table 2. Specifically, the explanatory summary shows useful details about  X  X ound X  of the target product, such as  X  X eated dead center in front of the or-chestra X ,  X &gt; 98db signal-to-noise ratio X ,  X  X eats ipod X ,  X  X llow large head-phones as well as external speakers X . On the other hand, in the baseline summary, despite of redundancy removal, we can see many repetitions of  X  X ood sound X  because it is popular in the in-put data. In general, the baseline summary seems to have many generic sentiment words, which are not informative, and useless for explaining specific reasons why a reviewer has held a particular opinion.
In this paper, we introduced a novel sentence ranking problem called explanatory sentence extraction (ESE), which can extend the capabilities of the current opinion summarization methods. We proposed two general methods, modified TF-IDF weighting and the probabilistic model for scoring explanatoriness. Experiment results showed that proposed methods are effective in ranking sen-tences by explanatoriness outperforming a state of the art sentence ranking method for a standard text summarization method.
Our work is the first step toward studying the new problem of measuring explanatoriness. With the two data sets and appropriate measures, we have paved the way for further studying this prob-lem. As a future work, we can further explore different ways of estimating the proposed probabilistic models, especially through exploiting pseudo feedback to refine the estimate of the models. In the current paper, we chose to focus on language-independent methods, which have the advantage of being applicable to any lan-guage without requiring any manual effort. For the future work, we can further improve these methods by doing deeper linguistic and semantic analysis on characteristics of explanatory sentences.
Thanks to human labelers for data set labeling in our experi-ments. This material is based upon work supported in part by the National Science Foundation under Grant Number CNS-1027965 and by an HP Innovation Research Award.
