 The traditional approach to data representation utilizes a matrix structure, with obser-existence of a single target variable and lacks a means of modeling dependencies between other features. Additionally, such a structure assumes that observed variables mains, such as diffusion tensor imaging, where higher-order features predominate. 
Traditionally, these problems have been solved by reducing the features to scalars information, this strategy also employs a questionable approach from a philosophical standpoint: attempting to fit the data to an imprecise model rather than attempting to accurately model the existing structure of the data. Finally, while it may be possible to model dependencies between features by making many runs, each with a different target variable, this yields suboptimal performance and may not be computationally feasible when real-time performance is required or when the dataset is very large. 
To address these issues, we propose to model such datasets using tensors , which known as the order of the tensor. Using a combination of wavelet and tensor analysis tools, we propose a framework for summarization, classification, clustering, concept discovery, and compression, which we call TWave. Applying our technique to analy-dataset, we compare the performance of TWave against voxelwise, SVD-based, wavelet-only, and tensor-only techniques and demonstrate that TWave achieves supe-rior results and reduces computation time vs. competing methodologies. 2.1 Tensor Tools Tensors are defined within the context of data mining as multidimensional arrays. The number of indices required to index the tensor is referred to as the rank or order of the tensor, while each individual dimension is referred to as a mode . The number of ele-ments defined on each mode is referred to as the mode X  X  dimensionality . The dimen-sionality of a tensor is written in the same manner as the dimensionality of a matrix; for example, 20x50x10. Tensors represent generalizations of scalars, vectors, and matrices, which are respectively orders 0, 1, and 2. another tensor rather than a block matrix. Given order r and s tensors  X  and  X  , their tensor product  X  X  X  is a tensor of order  X  X  X  :: 
Singular value decomposition (SVD) is a unique factorization by which an  X  X  X  matrix is decomposed into two projection matrices and a core matrix, as follows: where  X  is an  X  X  X  matrix,  X  is an  X  X  X  column-orthonormal projection matrix,  X  is an  X  X  X  column-orthonormal projection matrix, and  X  is a diagonal  X  X  X  core matrix , where  X  is the (matrix) rank of matrix  X  . 
SVD is used in Latent Semantic Analysis (LSA), an unsupervised summarization value decomposition automatically derives a user-specified number of latent concepts from the given terms, each representing a li near combination. The projection matrices  X  and  X  contain term-to-concept and document-to-concept similarities, respectively. Thus, SVD can be used to provide simple yet powerful automatic data summarization. 
The natural extensions of singular value decomposition to tensors are the Tucker and PARAFAC decompositions [2,3]. Let  X  be an order-r tensor. Tucker decomposi-tion is a factorization into a core tensor  X  and projection matrices  X   X  : 
Though the Tucker decomposition provides SVD-like data summarization, evaluat-ing it requires computing  X   X  X  covariance matrix. This can come at a memory cost of RAFAC avoids this problem. PARAFAC is a generalization of PCA [2] and forms the PARAFAC decomposes an order-r tensor  X  into a columnwise sum of the tensor product of  X  projection matrices, denoted  X   X  X  X  X   X ...  X  X  X  X  , as follows: 
Where the  X  matrices represent projection matr ices containing mode-to-concept represents the strength of a concept. The notation  X  :, X  refers to the i th column of  X  . 
Both the Tucker and PARAFAC decompositions may be computed using alternat-ing least squares (ALS) [ 4 ], as shown below: 
Penrose pseudoinverse, and  X   X   X   X  represents  X  matricized [4] on mode i . The resulting PARAFAC decomposition is illustrated in Figure 1 below: 3.1 Overview Our methodology makes use of both wavelets and tensors. Because spatiotemporal data tends to exhibit a high degree of spatial locality, the spatiotemporal modes of the dataset are first preprocessed using an m -dimensional discrete wavelet transform (obtained through cascading), where m is the number of spatiotemporal modes. For applications other than clustering, we utilize the Daubechies-4 wavelet; clustering suppressing outliers. We then linearize the wavelet coefficients to form a vector representing all spatiotemporal voxels in the dataset, reducing the order of the tensor by d -1; this overcomes many of the performance issues associated with a high-order storing the results in a sparse matrix to achieve a significant compression rate. 
PARAFAC is then performed using alternating least squares and the resulting pro-analysis, including concept discovery, compression, clustering, and classification. 3.2 Other Methods It is also possible to analyze data using wavelets and tensors alone, or by using neither preprocessing method (the voxelwise approach). Singular value decomposition run on the dataset in matrix representation additionally provides a benchmark for comparison of the tensor model and techniques. 
We performed voxelwise classification by linearizing each image in the dataset and using the normalized values of an image X  X  voxels as a feature vector in classification. Similarly, we performed wavelet classification by using each image X  X  linearized 3-level wavelet coefficient vector (using the Daubechies-4 wavelet) as a feature vector representing that image. Both approximation and detail coefficients at each resolution were included in this analysis. 3.3 Classification To perform classification using TWave, we wavelet-transform the dataset, run a tensor decomposition such as PARAFAC or Tucker, and directly use each wavelet on the majority class of that image X  X  k nearest neighbors (using Euclidean distance). 
When classifying on a variable other than the principal variable of the dataset, we subtract the mean of the principal variable from the dataset. We have empirically observed this to boost accuracy. 3.4 TWaveCluster We extended the WaveCluster algorithm to use the PARAFAC decomposition rather than a connected component algorithm to grow the clusters, calling our algorithm TWaveCluster. Our approach exhibits a number of advantages, including the ability to create a fuzzy clustering (where each voxel X  X  degree of membership in cluster c is its similarity to concept c in the decomposed tensor), the ability to cluster noncontiguous represent cluster variance. The first few steps of our algorith m are identical to WaveCluster: However, the remaining steps in our algorithm differ: 4.1 Dataset We analyzed each approach on a high-order motor task fMRI dataset consisting of 11 subjects performing 4 simple motor tasks: left finger-to-thumb, left squeeze, right finger-to-thumb, and right squeeze. Classification was also performed on 10,000 ran-domly-sampled observations from the low-order MNIST digit recognition dataset, split into 5,000 element training and test sets [6]. Acquisition of the fMRI dataset took place using one scanner and one common set of acquisition parameters. Data was acquired from each subject over 120 time points, each 3 seconds long. The period of each task was 30 seconds. Each acquired volume consisted of 79 X 95 X 69 voxels. 79 X 95 X 69 X 120 X 4 X 11 , of which the first four modes were spatiotemporal and the remaining two were categorical. 4.2 Discovered Concepts When summarizing the data using a 2-concept TWave analysis, we noticed two out-liers among the subject-to-concept similarities, which we found corresponded exactly to the 2 left-handed subjects in the dataset. This pattern was made even more explicit when subtracting the subject means from each subject X  X  set of images, suggesting that the task residuals discriminate better between left and right handed subjects than when task activations are biased by subjects X  means. The results of TWave using the Daubechies-4 wavelet and mean subtraction are shown in Figure 2. These results suggest that PARAFAC may be employed as a powerful concept-discovery and fea-ture extraction tool on complex datasets, though we caution that a larger dataset may be necessary to adequately confirm these findings. 4.3 Classification Use of wavelets in particular greatly improved subject classification accuracies, given a complex enough wavelet (to 98% using the Daubechies-4 wavelet but only to 82% proving time and space costs while preserving the discriminative power of the clas-sifier. Further compression is possible in the decomposed tensor through truncation of weak concepts (though computation of these concepts is expensive). 
Task classification was more difficult because the intra-subject between-task va-riance (  X   X   X 179.29 ) was less than the between-subject variance (  X   X   X 9066.85 ). Initial results yielded only 2% accuracy for voxelwise analysis and 27% accuracy for wavelet-based analysis. However, by subtracting the voxelwise mean of each subject MPCA+LDA [7] as a preprocessing step further improved accuracy. As the sampled between low and high-order approaches than in the fMRI dataset, though wavelet preprocessing still did significantly boost accuracy. 4.4 Clustering We analyzed two subjects on all four spatiotemporal modes of the fMRI tensor using approaches. Average running times for each method were 53 seconds and 23 seconds, also yield a clustering in-line with domain expectations. 4.5 Speed and Summary of Results Runtime was assessed for the voxelwise, wavelet-based, and TWAVE approaches on a dual-processor 2.2 GHz Opteron system with 4 GB of memory. The SVD and pure tensor approaches were measured on an 8 processor (16 core) 2.6 GHz Opteron su-percomputer with 128 GB of memory. Despite running on a much more powerful system, the tensor and SVD approaches still took significantly longer to complete than other approaches, as shown in Tables 1 and 2: From these results, we may conclude that the combination of wavelets and tensor tools in the analysis of fMRI motor task datasets yields better performance in space, time, and accuracy than the voxelwise approach or either technique alone, achieving costs of using only tensors. Additionally, such an approach provides powerful auto-matic data summarization techniques, as demonstrated through discovery of left-different wavelet functions, extension of our methods to streaming and sparse tensor data, and applications to high-order datasets in other fields. Acknowledgments. We would like to thank Charalampos Tsourakakis for assistance National Science Foundation under grants IIS-0237921, IIS-0705215, IIS-0705359, and the National Institutes of Health under grant R01 MH68066-05 funded by the responsibility for any analyses, interpretations, or conclusions . 
