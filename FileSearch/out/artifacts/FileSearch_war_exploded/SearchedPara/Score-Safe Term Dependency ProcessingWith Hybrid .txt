 Score-safe index processing has received a great deal of attention over the last two decades. By pre-calculating maximum term im-pacts during indexing, the number of scoring operations can be minimized, and the top-k documents for a query can be located efficiently. However, these methods often ignore the importance of the effectiveness gains possible when using sequential dependency models. We present a hybrid approach which leverages score-safe processing and suffix-based self-indexing structures in order to pro-vide efficient and effective top-k document retrieval.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  indexing methods ; H.3.2 [ Information Storage and Retrieval ]: Information Storage X  file organization ; H.3.3 [ Inform-ation Storage and Retrieval ]: Information Search and Retrieval X  query formulation, retrieval models, search process ; I.7.3 [ Docu-ment and Text Processing ]: Text Processing X  index generation Text indexing; text compression; experimentation; performance Search engines rely on fast evaluation of ranking computations. Formulations based on bag-of-word queries and TF  X  IDF -type scor-ing mechanisms have been in use for several decades, and users are now accustomed to expressing their information needs via short keyword-based queries. One promising approach to improving the effectiveness of bag-of-words querying is term dependency mod-eling [12], which include statistics based on phrase combinations into the retrieval mechanism, to favor documents in which query terms appear consecutively, or near each other in unordered win-dows. The objective is to create a more favorable ordering than is achieved by a bag-of-words computation.

To process term-dependency queries, standard inverted index-based techniques can be augmented in two different ways. The first pre-defines queryable phrases at index construction time and includes information about them in the index, so that those phrases are automatically supported during query evaluation. The second approach includes word positions within the inverted index, so that arbitrary phrases can be identified during query evaluation. Com-pared to a document-level inverted index suited for bag-of-words retrieval, the first option has the disadvantage of either requiring a substantially enlarged inverted index or limiting the set of phrases that are supported; while the second option uses a smaller index, but needs additional processing when phrases are part of the query. Compromises between these two implementation options are also possible. Zobel and Moffat [20] give an overview of indexing and searching using inverted files.

The last decade has seen the emergence of other techniques, in-cluding self-indexing methods derived from suffix arrays and the Burrows-Wheeler transformation (BWT), following initial work by Muthukrishnan [13]. Until recently, these techniques were  X  X ingle term X  and based solely on occurrence frequency, reporting a ranked list of documents in decreasing frequency order in which any sin-gle fixed string of characters or words appears. That is, they report  X  X op-k  X  for one-term queries, with  X  X op X  based on frequency alone. More recent work has explored the use of bag-of-word similarity computations, and stepped closer to the functionality that can be supported using document-level inverted indexes [3].
 Our contribution . We implement term-dependency similarity mod-els using suffix-based indexes, including combining single terms with multi-term phrases, and present an efficient safe-to-k approach for processing bag-of-word and phrase-based queries. Our method combines both pre-calculation and on-the-fly processing in order to achieve attractive time/space trade-offs, and demonstrates that multi-term queries can be processed using a suffix-based index more quickly than is possible using an inverted index alone.
Metzler and Croft [12] describe the use of query term depen-dencies to increase the effectiveness of similarity-based retrieval techniques. They make use of two operators,  X  X rdered window X  phrases, in which two or more of the query terms appear in a doc-ument in the sequence in which they appear in the query, and  X  X n-ordered windows X  in which two or more of the query terms appear within some specified distance of each other, but not necessarily adjacent, and not necessarily in the order in which they appear in the query. Metzler and Croft assign 80% of the final score of the document to a bag of words computation, 10% to ordered phrases, and 10% to unordered windows.

In the approach used here, scores are based on terms and a com-plete set of ordered phrases from the query. For example, the query  X  arthur conan doyle  X  is processed as a total of six components: the three terms, plus two 2-grams  X  arthur conan  X ; and  X  conan doyle  X  plus the 3-gram  X  arthur conan doyle  X . Including the t singletons, a query of t terms has t ( t + 1) / 2 different components, each of which is scored across the collection as if it were a separate term. Intersection-based phrase discovery . The simplest way of query-ing term m -grams is to store and process position offsets as part of an inverted index [20]. At query time, the various terms X  postings lists are combined, and when terms co-occur in a document, a more detailed intersection operation is carried out to check for positional adjacencies. We call this the intersect method; for example, to create a query-time postings list for a phrase of m terms t the postings lists for the phrases t 1 ...t m  X  1 and t 2 tersected, with the base case supplied by the stored postings lists in the term-level inverted index.

Adding position offsets to the term-level inverted index of a typ-ical document collection approximately doubles its size, a reason-able overhead; and when terms are infrequent, intersection of list components is also reasonably fast. But when long lists for com-mon terms are involved, for example, in  X  the president of the united states  X , query processing can be costly.
 Score-safe processing . A technique is score-safe if it guarantees that the top-k documents are the same as an exhaustive processing regime would generate. Score-safe methods include MaxScore [17], WAND [1], and BlockMax WAND (BMW) [4, 5]. All rely on a key insight: if the ranking metric is summative over terms, and if the maximum  X  X ontribution to any document X  score of each term across the collection (or in the case of BMW, over a part of the collection) is known, then an evolving subset of the terms can be identified such that any document that might be added to the top k must contain at least one term in that set. Hence, attention can be restricted to documents containing one or more of those terms.
The FM-INDEX , introduced by Ferragina and Manzini [6], is a data structure used for pattern matching. Building on the suffix ar-ray , it also incorporates ideas embedded in the Burrows-Wheeler transform. A character-level FM-INDEX for a text can be stored in a fraction of the space occupied by the text itself, and provides pattern search and (with small overhead) random-access decoding from any location in the text. To build a word-level FM-the input T is parsed into case-folded stemmed words exactly as for any other type of index, and retained as a sequence of word identifiers, W , relative to a vocabulary. An end-of-document sen-tinel is inserted after each document in W .

The sequence of integers is provided as input to a suffix-sorting algorithm, after which the FM-INDEX is constructed and stored. The FM-INDEX stores the symbols of W in a suffix-permuted or-dering, W BWT , which allows efficient access, search and extrac-tion of W using space equivalent to the compressed representa-tion of the input. A third structure used during querying is the n -element document array , or D -array , which notes, for each el-ement of W BWT [ i ] , the document number from which that word originated [13]. Each value D [ i ] requires log d bits, where d is the number of documents in the collection; D takes n log d bits in total. Single term query processing . Single term queries are processed as follows. First, the query is mapped to a sequence of integers, Q , using the vocabulary. Using the FM-INDEX , a range W BWT is determined which corresponds to all suffixes in W prefixed by Q . The corresponding range in the D -array , D [ sp .. ep ] now contains all document identifiers containing Q .

The interval D [ sp .. ep ] corresponds to the total number of oc-currences of Q in W , which can be large; and hence processing D [ sp .. ep ] efficiently is critical to overall query performance. Var-ious schemes have been proposed to identify the top-k most fre-quent document identifiers in D [ sp .. ep ] . Culpepper et al. [2] show that if D is stored as a wavelet tree, the top-k documents can be found quickly in practice, but without a worst-case guarantee. Hon et al. [8] present a data structure that augments the D -array in or-der to provide worst-case bounds on both execution time and space. Their HSV mechanism pre-computes top-k result lists for a set of pre-determined [ sp .. ep ] intervals, guaranteeing that only relatively small ranges are processed exhaustively at query time. Recently, Konow and Navarro [10] presented a compact data structure which efficiently retrieves the top-k most frequent documents indepen-dently of the size of the range [ sp .. ep ] .
 Beyond single term top-k frequency retrieval . In the methods sum-marized so far,  X  X op-k  X  has the semantics  X  X eturn the k documents which contain the most occurrences of the pattern X . These ap-proaches cannot be easily adapted to process more complex queries. For example, the formulation of the LMDS similarity computation includes a range of factors, meaning that the top-k answer set to a query over two or more terms is not constrained to lie within the union of the terms X  independent top-k answer sets.

Sadakane [15] described a support data structure which com-putes the number of unique document identifiers in D [ sp .. ep ] in constant time. Sadakane used this structure to support exhaus-tive processing over [ sp .. ep ] , and hence calculate a simple based similarity metric. Culpepper et al. [3] adopt the HSV data structure [8] in combination with a wavelet tree-based representa-tion [2] to compute similarity scores for multi-term queries. Their scheme retrieves a fixed number of most frequent document iden-tifiers for each query term, and combines them to retrieve a ranked document listing, an approach that gives competitive runtime per-formance for k  X  100 , but is not score-safe.
We build on the previous work, and explore ways of implement-ing more complex score-safe query semantics  X  in particular, the sequential dependency model outlined earlier. The approach de-scribed shortly is a hybrid, consisting of a pruned suffix tree of document-level postings lists to efficiently handle large intervals, and fast sequential exhaustive processing of smaller sections to cre-ate document-level postings lists on-the-fly.
 Pruned suffix tree . The set of intervals in the D -array that can be generated by the FM-INDEX correspond to the internal nodes of a suffix tree over T . The HSV mechanism [8] attaches partial pre-computed postings lists to nodes within that suffix tree, in order to compute the top-k (by frequency) values at any suffix tree node using a controlled amount of time. The length of each posting list is determined by the size of the corresponding [ sp .. ep ] interval, the number of sampled points within it, and its location in the suffix tree over W ; and is strategically balanced so as to achieve attractive time and space bounds.

We return to a simpler scheme which embeds several observa-tions that apply to similarity metrics. Instead of the differing-length lists of the HSV structure, we store full postings lists every node in the suffix tree that corresponds to an interval wider than T and compute postings lists on-the-fly for [ sp .. ep ] intervals that are smaller. That is, we store a pruned suffix tree ( PST ) of full post-ings lists, for a subset of the intervals. A second threshold T also defined  X  postings lists are not stored for intervals greater than T max = d (the number of documents in the collection), since such terms have negligible bearing on the LMDS similarity computation. Query evaluation . When a query identifies a small [ sp .. ep ] inter-val, a posting list is computed from the D -array via a sequential cache-friendly process in O ( ep  X  sp ) time. The longer the phrase and more precise the interval, the faster this computation is.
The thresholds T min and T max allow index space and execution cost to be traded  X  the broader the band between the thresholds, the larger the index, and the faster that query evaluation can be carried out. Our query processing scheme is summarized as follows: (1) determine for each query component (including all phrase compo-nents) the corresponding [ sp .. ep ] interval using the vocabulary and the FM-INDEX ; then either (2a) retrieve the posting list from the PST if T min  X  ep  X  sp  X  T max , or (2b) create the postings list on the fly by processing D [ sp .. ep ] if ep  X  sp &lt; T ply DAAT or MaxScore or WAND or BMW to the complete set of postings lists to identify the top-k documents.
We took the TREC GOV2 collection, containing 426 GB of web pages crawled from the .gov domain, and applied the Boilerpipe software package 1 to generate a plain text version occupying 90 GB. A total of 150 test topics and corresponding relevance judgments are available for this collection (topics 701  X  850 ). Algorithms were implemented using C++ and compiled with gcc 4.8.1 with  X  X 3 op-timizations; and experiments were run on a 24 core Intel Xeon E5-2630 running at 2.3 GHz using 256 GB of RAM. All efficiency runs are reported as the mean or median of 5 consecutive runs of a sequence of at least 100 queries for a given query length, and cor-respond to fully in-memory evaluation, under optimal conditions. All results shown in this short paper are for identification of the top k = 100 documents, and with T min = 64 , 000 . Other combinations of parameters will be explored in a full paper.

Each postings list is stored and compressed in blocks of 128 en-tries using the FastPFOR library [11]. All methods can take advan-tage of block representatives to allow faster skipping of list entries. Position offsets are stored using the succinct storage scheme of Vi-gna [18]. The FM-INDEX , the position offset representation, and the PST are implemented using the sdsl library [7].
 Baseline performance . Table 1 shows the effectiveness of NewSys relative to Indri 2 and Atire [16]. The FDM model uses both phrase expansion and unordered-window proximity. Our approach uses only the phrase expansion component, and provides a compromise between the lesser effectiveness achievable using the efficient BOW approach, and the more complex FDM mechanism.

Figure 1 shows the effect that length has on query execution time for score-safe processing methods, using queries drawn randomly from the TREC million query set. The times reported in Figure 1 do not include phrase postings list creation cost, and cover only raw posting list evaluation cost after all required lists have been generated. The sequence of improvements arising from score-safe heuristics  X  WAND and BlockMax WAND (BMW)  X  is clear.
 Improvements . The bars in Figure 2 show the relative cost of dif-ferent approaches to on-the-fly construction for index lists that are https://code.google.com/p/boilerpipe/ http://www.lemurproject.org/indri.php Table 1: Effectiveness (MAP) on GOV2 for topics 701  X  850 . The  X  X est Known X  system is uwmtFadTPFB from the TREC 2006 Ter-abyte Track. All systems use Krovetz stemming, the LMDS simi-larity formulation with  X  = 2500 , and Boilerpipe preprocessing. A  X  represents p &lt; 0 . 05 using a paired t -test against the BOW runs. Figure 1: Query time distribution for different processing ap-proaches using GOV2 and query lengths from 1 to 10 . Note the logarithmic vertical scale. not in the pruned index, normalized (at each query length) to the cost of processing the query using the exhaustive DAAT approach. The horizontal lines show the relative costs of different process-ing strategies, as already shown (in absolute terms) in Figure 1, normalized against the same reference costs. That is, the bars cor-respond to different ways of generating a full set of component postings lists, and the horizontal lines to different ways of then computing (the same top-k ) document scores from those postings lists. A retrieval mechanism requires both steps. In Figure 2 the method labeled intersect is the baseline intersection-based ap-proach; darray is our hybrid approach with phrases below the threshold T min generated on the fly from the stored D -array; and intersect-ppst is described shortly.
 Figure 2 shows very clearly that sequential processing of the D -array is far more efficient than generating postings lists from an inverted index via iterated intersections. The longer the query, the greater the advantage. Also worth noting is that iterative inter-section dominates the similarity computation: intersect +BMW is only marginally faster than intersect +DAAT. Using the pro-posed darray approach the full benefits of BMW can be realized.
Table 2 lists timings for the queries shown in Figure 2. These times should be regarded as being indicative rather than precise: In-dri and Atire were run with standard out-of-the-box configurations and each query was measured after a pre-execution to bring the re-quired data in to memory; but the timings might still not be exactly like-for-like. Even so, the new approach provides clear benefits. Figure 2: Relative query costs, normalized so that the list process-ing time using the DAAT strategy is rated at 100 . 0 %.
 Table 2: Average query processing time (milliseconds) on GOV2 . The NewSys implementation uses the darray +BMW combination.
The performance advantages of the darray method come at a cost. It uses a total of 97 GB for the FM-INDEX , the PST , and the D -array; whereas the intersect method requires only 33 GB for a positional inverted index over terms. An obvious question is: can this space difference be used instead to index a subset of important phrases? Several authors have suggested just such a trade-off ap-proach in which a subset of important phrases is added to the index (see Section 6). The intersect-ppst method shown in Figure 2 offers a pragmatic compromise between using position offsets for each term to generate the phrase components, and indexing all pos-sible phrases. Since the PST already captures all possible phrases of frequency between T max and T min , we can add positional offsets to those inverted lists, covering the intermediate lists needed to con-struct postings lists for long phrases. Any remaining lists required are still generated on demand using iterated intersection. With the same value of T min , this approach requires 101 GB, and t = 5 term ( 15 component) queries take an average of 1 , 700 millisec, compared to 400 millisec for the darray , and 5 , 800 millisec for intersect . That is, for our chosen query semantics the darray offers superior performance even when compared to a similar-sized inverted index that includes positional postings lists for all phrases (of any length) that occur more than T min times.
Williams et al. [19] propose a modified inverted index, in which each posting list includes a record of what the next terms in the doc-ument are. Williams et al. also index common phrases as a single term within an inverted index in order to increase the efficiency of n -gram processing. Other researchers have also explored indexing common phrases, for example, Huston et al. [9] and Ozcan et al. [14]. These approaches provide efficient querying, but also have disadvantages: the phrase length is usually fixed at indexing time; index sizes can grow quickly; and infrequent phrases might not be recorded in the index at all.
We have explored algorithmic components that can be used to support efficient phrase querying as a contributing factor in docu-ment similarity ranking using term dependency models. While the complete system is still not as effective as the best available sys-tems (which make use of non-consecutive term pairs, and/or query expansion), the efficiency advantages of the BWT-based index are appealing. The BWT-based systems can also support arbitrarily long exact match phrase queries with no additional space costs be-yond the fixed D array overhead. Indeed, as the phrases get longer, the processing time gets faster, since the corresponding [ sp .. ep ] in-terval gets smaller.
 Acknowledgments . This work was supported in part by the Aus-tralian Research Council (DP110101743). Shane Culpepper is the recipient of an ARC DECRA Research Fellowship (DE140100275). Simon Gog designed and implemented the pruned suffix tree used in the experiments and integrated it into the IR framework.
