 Term proximity retrieval rewards a document where the matched query terms occur close to each other. Although term proximity is known to be effective in many Informa-tion Retrieval (IR) applications, the within-document dis-tribution of each individual query term and how the query terms associate with each other, are not fully considered. In this paper, we introduce a pseudo term, namely Cross Term, to model term proximity for boosting retrieval performance. An occurrence of a query term is assumed to have an impact towards its neighboring text, which gradually weakens with the increase of the distance to the place of occurrence. We use a shape function to characterize such an impact. A Cross Term occurs when two query terms appear close to each other and their impact shape functions have an intersection. We propose a CRoss TErm Retrieval (CRTER) model that combines the Cross Terms X  information with basic proba-bilistic weighting models to rank the retrieved documents. Extensive experiments on standard TREC collections illus-trate the effectiveness of our proposed CRTER model. H.3.3 [ Information Storage &amp; Retrieval ]: Information Search &amp; Retrieval Performance, Experimentation Cross Term, Kernel, BM25, Proximity, Probabilistic IR
Most of the traditional Information Retrieval models are based on the assumption that query terms are independent of each other, where a document is represented as a bag of words. Nevertheless this assumption may not hold in practice. There might be some implied associations among them. For example, given a query  X  X ancouver Olympics X , there exists an association between the query terms. Users are not looking for either other events in Vancouver or the Olympic games in other cities. Given above two documents with both  X  X ancouver X  and  X  X lympics X  occurring once each, a traditional IR model will give them the same weights. Obviously the former docu-ment contains more valuable information for users than the later. The association between  X  X ancouver X  and  X  X lympics X  reflects the distance between them. Therefore, it is neces-sary to reward the document where the matched query terms appear together/close.

Many researchers have been working on proximity met-rics in information retrieval [4, 11, 12]. Whereas, the na-ture of the associations among query terms still awaits fur-ther study. Some proximity approaches only considers ad-jacency [26, 27], while non-adjacent terms may also have associations. N-gram models [1, 17] consider n word se-quences, which expand the radius of matching. It is yet hard to determine N, and complexity grows exponentially with the growth of N. Also, redundant proximity informa-tion may be led into the model, which may on the other hand decreases the performance of the probabilistic weight-ing models. Other proximity based probabilistic weighting models, such as [5, 6], add proximity information into their weighting functions in a heuristic manner. However, their experiments are not conclusive and their retrieval functions are not shown to be effective and robust enough [28].
In this paper, we present a Cross Term Retrieval model, denoted as CRTER, to model the associations among query terms in probabilistic retrieval models. A pseudo term, Cross Term, is introduced for boosting retrieval performance. A Cross Term is generated by two closely occurred query terms.

We assume that an occurrence of a query term has an im-pact towards its neighbouring text. This impact attenuates when a position is farther away. If we try to characterize this impact with a mathematic function, intuitively the function should satisfy the following properties: Non-negative, Con-tinuous, Symmetric, Monotonic, Identity (See detail in Sec-tion 3). We use kernel functions that have been brought to proximity retrieval [10, 16] to estimate the query term oc-currence X  X  impact. In this paper, we investigated three more kernel functions that satisfy the above properties: Quartic Kernel, Epanechnikov Kernel, and Triweight Kernel. The kernel functions are normalized with a minimum value of 0 and a maximum value of 1. When two query terms, q i and q , occur closely in a document, their impact shape functions wil l have an intersection. We say q i and q j are generating t erms of a Cross Term q i,j , which occurs when q i and q im pact shape functions intersect.

The corresponding impact shape functions X  value at this intersection is Cross Term q i,j  X  X  occurrence value, which rang es from 0 to 1. The closer two query terms X  occurrences are, their generated Cross Term has higher occurrence value, according to the impact shape function X  X  properties. There-fore, the Cross Term X  X  occurrence value indicates to which extent two query terms are correlated.
 The challenge we are facing is how to estimate a Cross Term from a document collection, and integrate the Cross Term into a probabilistic weighting model. In a probabilis-tic weighting model, some variants change from term to term, namely the within-document Cross Term frequency ( tf ), the number of documents containing the Cross Term ( n ), and the within-query Cross Term frequency ( qtf ), re-spectively. We define the corresponding variants for Cross Terms: tf ( q i,j ), n ( q i,j ), and q tf ( q i,j ). For tf ( q pseudo term, the traditional counting method of the occur-rences in a document does not make much sense, especially when we aim to reward more on the Cross Terms that the generating query terms are closer. Instead of accumulating the number of occurrences, we accumulate a Cross Term X  X  occurrence value in a document as its tf ( q i,j ), which is small when generating query terms are far away, and large when generating query terms are close to each other. Please note that tf ( q i,j ) is always smaller than the number of occur-rences of a Cross Term in the document. In order to balance with tf ( q i,j ), we define n ( q i,j ) and q tf ( q i,j n ( q i,j ) is the accumulated Cross Term X  X  average value on eac h document over the collection. In a query, since it is normally short and only contains query terms, we assume that terms in a query are densely distributed. If two terms exist in a query, they are regarded as being adjacent. Then qtf ( q i,j ) is a simplified case of within-document frequency b y treating the query as a document. With the defined Cross Term variants, we integrate Cross Terms into tradi-tional BM25 weighting model [25], by treating them as spe-cial terms. Cross Terms X  weights are computed and linearly combined with query terms X  weights.
 The remainder of this paper is organized as follows. In Section 2, we discuss the prior related work. In Section 3, we introduce the concept of Cross Term and define its variants. In Section 4, we propose Cross Term Retrieval (CRTER) model utilizing BM25 as basic weighting function. In Section 5, we set up our experimental environment on six TREC collections, and test the proposed CRTER model and compare it with some existing models. In Section 6, we conclude the paper with a discussion of our findings and future work.
In the 1990s, some early researchers started to investi-gate the effectiveness of term proximity in Information Re-trieval. Allan and Ballesteros [2] indexed phases instead of terms with InQuery [7], and obtained improvements in TREC campaigns. Clarke and Cormack [9] introduced a  X  X EAR X  operator to quantify the proximity of query terms. Hawking and Thistlewaite [12] evaluated  X  X pan X  proximity approaches on TREC data sets, which is the text segments containing all query term instances.

Some studies heuristically integrated word proximity into probabilistic weighting models, such as [5, 6, 24, 28]. How-ever, the nature of query term proximity still awaits further study.

N-gram IR models have been investigated as a proxim-ity approach by researchers for years [1, 17, 18]. They are recognized as having high complexity and may lead to re-dundant information. Bigram models make a better balance between effectiveness and complexity. Song and Croft [26] proposed a general language model that combines bigram language models with several smoothing techniques includ-ing a Good-Turing estimate and corpus-based smoothing of unigram probabilities. The relative contributions of the dif-ferent models to the query generation probability are deter-mined empirically. Srikanth and Rohini [27] approximated the biterm probabilities using the frequency of occurrence of terms. Biterm language models are similar to bigram lan-guage models except that the constraint of order in terms is relaxed. In [3], authors proposed a language modeling ap-proach that incorporates word pairs, without a constraint on adjacency or word order, where word pairs are determined statistical relationships, or lexical affinities, between words. Pickens [23] introduced an approach that uses non-adjacent biterms, but the particular domain, musical documents, re-quires an emphasis on the order of  X  X ords X . We also only investigate the proximity between a pair of query terms, whereas, our approach differs from the previous studies that we propose the concept of a pseudo term, Cross Term, gen-erated by two query terms to investigate how two query terms X  occurrence change together. Cross Terms are nat-urally integrated into basic retrieval models as new terms, and therefore incorporate proximity into retrieval process.
Various proximity approaches integrating knowledge from other realms were investigated. [11] introduced the linkage of a query as a hidden variable, which expressed the term dependencies within the query as an acyclic, planar, undi-rected graph. A method of incorporating term dependence in probabilistic retrieval model was proposed in [8] by adapt-ing Bahadur-Lazarsfeld expansion (BLE), which was origi-nally used in the pattern recognition field. In [4], authors proposed a mathematical model based on a fuzzy proximity degree of term occurrences particularly for boolean queries. A retrieval model based on Markov random field [20] was presented for developing a general framework for modeling term dependencies.

Density functions based on proximity have been adopted to characterize term influence propagation [10, 14, 16, 19, 22]. [10] is early work that proposed to propagate the tf idf score of each query term to other positions, where triangle, cosine, circle, and arc contribution functions were discussed. The highest accumulated tf idf score on all the positions is adopted as the document X  X  score. [14] uses hanning (co-sine) window function to characterize the density of terms. Lv and Zhai [16] proposed a positional language model that incorporates the term proximity in a model based approach using four term propagation functions: gaussian, triangle, cosine, and circle. We utilize the above term influence prop-agation functions in measuring Cross Terms, and test more potentially functions.
In this section, formally define the notion of Cross Term and the method of computing a Cross Term. Suppose we pos is one of the positions of query term q i in document D . The term q i will influence the positions between p os + k and pos  X  k .

We assume that the impact of a matching term q i at posi-t ion pos can be captured by the value of an impact function f ( p os, k ).

Property 1. Let f i ( p os, k ) be the impact function of a following 5 properties. (1) Non-negative: f i ( p os, k ) &gt; 0 , the impact of a term to-wards its neighborhood is always non-negative(We only focus on the position influence between query terms). there is a slight difference between two neighboring positions. same impact towards two equal-distance positions. The influence would decrease with the increasing of | k | . (5) Identity: f i ( p os, 0) = 1 , set one as standard influence.
Gaussian Kernel, Circle Kernel, and Triangle Kernel are widely used kernel function and satisfy all of above proper-ties [16]. We will discuss more about various kernel function in Section 3.4. When two query terms are close enough, their impact shape functions will join. Their point of intersection reflects the association between these two query terms.
Definition 1. Given two query terms q i and q j , if there ex-is ts points of intersections for impact functions f i ( p os and f j ( p os 2 , k 2 ), where p os 1 + k 1 equals to p os t hat a Cross Term occurs, denoted as q i,j . We call q i q Generating Terms o f q i,j .
 Definitio n 2. When a Cross Term q i,j occurs, the C ross Term X  X  value is the impact function X  X  value at the inter-section.

Without previous domain knowledge of a query and a col-lection, we assume that the query terms are identically dis-tributed, i.e., the query terms have the same impact shape functions. A Cross Term always locates in the middle of its generating query terms, and has higher value when the query terms occur closer.

Figure 1 shows a Cross Term X  X  example. Two query terms, q 1 and q 2 , located at the 8th position and the 12th posi-t ion in the document. More intuitively, We adopt Gaus-sian Kernel as impact shape function. Their impact X  X  shape functions are f 1 ( p os 1 , 2 ) and f 2 ( p os 2 can see that two shape functions cross over each other, and there exists an intersection at the 10th position. There is no threshold incorporated into the definitions of Cross Term and its value. For a continuous kernel function, there would always be an intersection if two terms occur in one docu-ment.
Here we want to define the counting method for the fre-quency. The within-document frequency is the rate at which a term occurs in a document. For a single query term, its frequency in document D equals how many times it occurs in D. But for a Cross Term, to simply count its occurrence might overestimate the frequency. We introduce a new es-timation of a Cross Term X  X  within-document frequency. As we can see from Figure 1 that q 1 . 2  X  X  value becomes higher if q and q 2 are close and lower if there is a farther distance b etween q 1 and q 2 . Naturally we adopt the Cross Term X  X  v alue in estimating its within-document frequency.
Suppose the positions of q i in a document are { p os 1 , i ..., pos tf i ,i } , where tf i is the term frequency of q sp ondingly, the positions of q j in the document are { p os Then the within-document term frequency of q i,j is defined a s follows.

Definition 3. The frequency of q i,j in D is the accumula-tion of q i,j  X  X  value. where tf is the term frequency of q i,j in D , K ernel ( ) is the kernel function adopted in query term X  X  impact function, and dist ( ) is the distance between two positions
Pl ease note that the frequency of a Cross Term might not be an integer. Meanwhile, various of kernel functions will be studied in 3.4. To evaluate the number of documents containing a Cross Term q i,j , it is not reasonable to simply count how many d ocuments in which q i,j occurs. The contribution from a query term and a Cross Term are different. For a query term q i , an occurrence means its frequency accumulates 1, a nd the number of documents containing q i is where 1 { q i  X  D } is an indicator function, which equals to 1 if q  X  D oc i and equals to 0 otherwise. On the other hand, a n occurrence of a Cross Term adds a value less than 1 to its frequency. A Cross Term X  X  value could be various, and ranges from 0 to 1. Thus the difference of Cross Terms should be shown in its variants. Definition 4. The number of documents containing a Cross Term q i,j , is the sum of q i,j  X  X  average value on each docu-men t, shown as follows where Occur is the number of occurrence of q i,j , which is
O ccur ( q i,j , D ) =
T o evaluate a Cross Term X  X  within-query term frequency, we can track each positions of q i and q j the same way as withi n-document frequency by the sum of all possible in-tersections. Moreover, different from in documents, query terms distribute densely in a query, so we can assume that query terms are adjacent to each other.

Definition 5. The within-query frequency of q i,j is where q tf ( q i ) and q tf ( q j ) are within query term frequencies of q i and q j , and Kernel is the same kernel function utilized in tf ( q i,j , D ) .
We first apply four kernel functions that have been brought into IR applications. Among them, the Gaussian kernel is widely used in statistics and machine learning algorithms such as the Support Vector Machines. Moreover, the Tri-angle kernel, Circle Kernel, and Cosine Kernel come from basic genomic graphics, which are applied to estimate the proximity-based density distribution for the positional lan-guage model [16]. below: where  X  is a normalization parameter, and 1 { u  X   X  } is indi-c ator function, which equals to 1 if u  X   X  and equals to 0 otherwise.

Moreover, some other kernel functions satisfy the query term impact function X  X  properties (Property 1). We also in-troduce them into IR since the kernel functions are not ma-turely applied in Information Retrieval. They are: Quartic Kernel, Epanechnikov Kernel and Triweight Kernel, shown as the following. The shape of the included kernel functions are shown in Figure 2. The performance of using all the kernel functions will be investigated in the experiments.
We propose a Cross Term Retrieval (CRTER) model, uti-lizing the Cross Term as a special term. A new combined weighting model for a document is CRT ER ( D ) = (1  X   X  ) X where w is the weighting function of query terms in Q, w  X  t he Cross Term q i,j  X  X  weight, and  X  is a parameter balancing the query terms and Cross Terms. When  X  equals to 0, the retrieval model uses query terms only, which is the standard weighting model. When  X  equals to 1, the retrieval model uses Cross Terms only. Since the weights of query terms and Cross Terms are normalized independently, the value of  X  reflects the influence of using Cross Terms. In this paper, we use BM25 as the basic weighting model in CRTER. BM25 is a classical weighting function employed by the Okapi system [25]. As shown by previous TREC experimentation, BM25 usually provides very effective retrieval performance on the TREC collections that are used in [29]
In BM25, the weight of a search term is assigned based on its within-document term frequency and query term fre-quency. The corresponding weighting function is as follows. where w is the weight of a query term q i in a document. The variants in the above formula can be grouped into two categories as follows:
Finally, a document X  X  weight for a query is given by the sum of its weight for each terms in the query, where w is the term weight obtained from Equation 12.
In this section, we present the CRTER Algorithm with an analysis on tis time complexity. We show that the time complexity of our proposed algorithm is at the same level of the basic weighting model. Figure 3 is the algorithm for Cross Term Retrieval (CRTER) model. In the rest of this section, we will analyze its time complexity.

Suppose the number of terms in a query Q is | Q | , the number of documents in an index is | Index | , the average document length is | D | . The first 6 steps are query pro-cessing that the within-query term frequency is computed for both query terms and Cross Terms, and the time com-plexity of this part is O ( | Q | 2 ). The remaining steps are do cument processing through the index. Also both query terms and Cross Terms are computed separately. For steps 8 to 12, this is standard document processing except storing term positions as well, and the time complexity of this part is 1: for all q i  X  Q do 2 : Compute qtf ( q i ) 3 : end for 4: for all q i , q j  X  Q do 5 : Compute qtf ( q i,j ) 6 : end for 7: for all D  X  Index do 8: for all q i  X  Q do 9 : Compute tf ( q i , D ) 1 0: Store q i  X  X  positions on D in an array 11: Compute n ( q i ) = n ( q i ) + 1 1 2: end for 13: for all q i , q j  X  Q do 1 4: for k 1 &lt; tf ( q i , D ) &amp; k 2 &lt; tf ( q j , D ) 1 5: Kernel temp = K ernel ( 1 2 | p os k 1 ,i  X  p os k 1 6: if Kernel temp 6 = 0 1 7: tf ( q i,j , D ) + = Kernel temp 18: O ccur ( q i,j , D ) + = 1 19: end if 20: end for 2 2: end for 23: end for 24: Compute w ( q i , D ) 2 5: Compute w  X  ( q i,j , D ) ; 26: Compute CRT ER ( D ) 27: Rank documents according to CRTER(D) a re accumulated, and it takes O ( tf 2 ) time, where tf is av-erage within-document term frequency. Thus computing all Cross Terms X  variants on one document takes O ( | Q | 2 tf F rom step 24 to step 27, the weights for query terms and Cross Terms are computed and the documents are ranked according to CRTER. This algorithm for CRTER model has the time complexity of the follows General ly the number of query terms | Q | in a submitted query is far smaller than the number of documents in index, i.e., | Q | &lt;&lt; | Index | , therefore the first | Q | c ould be eliminated. The use of Cross Terms is represented as | Q | 2 tf 2 in Formula 14. The percentage of query terms X  within-do cument frequency over the documents X  length is normally very low. Formula 14 will become when | Q | tf 2  X  | D | . This means the time complexity of CRTER is the same as a basic probabilistic weighting model.
We conduct a series of experiments on six standard TREC collections shown in Table 1. These collections are diverse in both sizes and content, which facilitate a thorough evalu-ation of our proposed CRTER model. The TREC8 contains newswire articles from various sources, such as Financial Times (FT), the Federal Register (FR) etc., which are usu-ally considered as high-quality text data with little noise. AP88-89 contains articles published by Association Press from the year of 1988 to 1989. The WT2G collection is a 2G size crawl of Web documents. The WT10G collection is a medium size crawl of Web documents, which was used in the TREC 9 and 10 Web tracks. It contains 10 Giga-bytes of uncompressed data. The .GOV2 collection, which has 426 Gigabytes of uncompressed data, is a crawl from the .gov domain. This collection has been employed in the TREC 14 (2004), 15 (2005) and 16 (2006) Terabyte tracks. The Blog06 collection includes 100,649 blog feeds collected over an 11 week period from December 2005 to February 2006. Following the official TREC settings [21], we index only the permalinks, which are the blog posts and their as-sociated comments. For all test collections used, each term is stemmed using Porter X  X  English stemmer, and standard English stopwords are removed.
 Collection Name # of Docs Topics # of Topics Table 1: Overview of the TREC collections used.

Each topic contains three topic fields, namely title, de-scription and narrative. We only use the title topic field that contains very few keywords related to the topic. The title-only queries are usually short which is a realistic snap-shot of real user queries in practice. On each collection, we evaluate our proposed model by a 10-fold cross-validation. The test topics associated to each collection are randomly split into ten equal subsets. In each fold, 9 subsets of the test topics are used for training, and the remaining subset is used for testing. The overall retrieval performance is av-eraged over all 10 test subsets of topics.

We use the TREC official evaluation measures in our ex-periments, namely the topical MAP on Blog06 [21], and the Mean Average Precision (MAP) on the other five collec-tions [29]. To emphasize on the top retrieved documents, we also include P@5 and P@20 in the evaluation measures. All statistical tests are based on Wilcoxon Matched-pairs Signed-rank test. We first investigate how much our proposed Cross Term Retrieval (CRTER) model can boost BM25. We use opti-mal BM25 as our baseline. The parameter b is set to be 0 . 35, which is shown to be optimal in our preliminary ex-periments (See Figure 7). The related experimental results are presented in Table 2. Seven different kernel functions are applied to instantiate the CRTER model, including: Gaus-sian, Triangle, Circle, Cosine, Quartic, Epanechnikov, and Triweight kernels. All the results are evaluated by MAP, P@5, and P@20. The percentage of how much CRTER out-performs BM25 is also listed. The best result obtained on each collection is marked bold. As shown by the results, our proposed CRTER model outperforms BM25 on all six collections used. The advantage of CRTER over BM25 is es-pecially evident on the .GOV2 and Blog06 Web collections, where statistically significant improvement is observed with all 7 kernel functions used. Moreover, according to the re-sults in Table 2, each kernel function has its advantage on some aspects. There is no single kernel function can outper-form others on all the datasets. An important issue that may affect the robustness of the CRTER model is the sensitivity of its parameters  X  (in Equation 11) and  X  (in Equation 4-10) to retrieval perfor-mance. The parameter  X  balances the query terms and the Cross Terms. When  X  equals to 0, the retrieval model uses query terms only, which is the standard BM25 weighting model. When  X  equals to 1, the retrieval model uses Cross Terms only. Since the weights of query terms and Cross Terms are normalized independently, the value of  X  reflects the influence of using Cross Terms. The kernel parameter  X  controls the range of a query term X  X  impact. When  X  is small, a Cross Term occurs only if its generating term are very close. When  X  is large, query terms far away from each other can generate a Cross Term. But the Cross Term X  X  value will be different according to the distance between query terms.
 Figure 4 plots the evaluation metrics MAP, P@5, and P@20 obtained by CRTER over  X  values ranging from 0 to 1 on WT2G. In addition, a group of different settings of  X  are applied, namely  X  = 2 , 5 , 10 , 20 , 50 , 75 , 100. The general tendency on each evaluation metric is similar. As we can see from Figure 4, CRTER X  X  retrieval performance decreases with large  X  values. CRTER generally performs well over different datasets when  X  locates between 0 and 0.2. Overall, a  X  value between 0 and 0.2 is recommended as it is shown to be reliable. In addition, CRTER X  X  retrieval performance increases with large  X  values, and tends to be stable when  X  is larger than 20.

Figure 5 plots the CRTER model X  X  performance with all kernel functions against kernel parameter  X  on TREC8 dataset. It illustrates the effect of kernel parameters in detail. The increment of  X  is 1 at the beginning and becomes larger after 10, because the model is more sensitive when  X  is smaller. For a given  X  , the range of CRTER X  X  performance under dif-ferent kernel functions is shown as a segment. The CRTER model overall appears to be effective over a wide range of  X  values. When  X  becomes larger, CRTER X  X  performance nor-mally increases at the beginning, due to the incorporation of term proximity. However, a larger  X  value brings noise to the model and therefore decreases CRTER X  X  performance. A  X  value between 20 and 25 is recommended to be a reliable setting.
The proposed CRTER model X  X  robustness is important to its applications in practice. Ideally, we would like to have a reliable retrieval performance using CRTER on various datasets with its parameters within a stable safe range. This issue is particularly crucial for a given new dataset without training data.

We first fix BM25 X  X  parameter b = 0.35, and evaluate the robustness of CRTER provided by an empirical setting, ob-CRTER Epanechnikov BM25 on all collections. tained from previous observations, namely Triangle Kernel,  X  = 25, and  X  = 0 . 2. We compare this empirical setting with the following optimization strategies: first, optimize each of kernel function,  X  or  X  while setting the other parameter to the empirical value; second, optimize all of kernel functions,  X  and  X  . Figure 6 presents the related results on 4 datasets: TREC8, AP88-89, WT2G and WT10G. From this figure, we can see that the performance obtained by the empirical setting is comparable to the retrieval performance obtained by the optimized parameter settings on all datasets used.
We also test CRTER X  X  performance under the same em-pirical setting with BM25 over different BM25 parameters on all the six collections. In BM25, b ranges from 0.15 to 0.95. In Figure 7, MAP of BM25 changes over different b , and CRTER can boost BM25 under all b X  X  settings and over all the collections. More specifically, for .GOV2 dataset, we can see that BM25 X  X  performance is very sensitive to b , its MAP gradually increases with the incrementment of b at the beginning, and sharply decreases later. CRTER, on the other hand, boosts basic BM25 over different b, and tends to stabilize the retrieval performance.

In general, CRTER performs robustly and has strong gen-eralized performance. Without much knowledge of a new dataset, a group of parameters are recommended for CRTER: Triangle Kernel,  X  = 25, and  X  = 0 . 2.
We study how the proposed CRTER model performs com-pared to the state-of-the-art approaches. In particular, its effectiveness is evaluated over the positional language model proposed by Lv et al. [16]. The positional language model (PLM) estimates a language model for every single position in a document. Among various kernel functions tested, the Gaussian kernel is shown to provide the best retrieval per-formance [16].

We compare CRTER X  X  results with PLM on the datasets used in [16], including AP88-89, WT2G and TREC 8. As illustrated in Table 3, PLM improves the language model baseline (LM) by 1.3%, 2.0%, and 1.1% on TREC8, AP88-89 and WT2G, respectively, while the corresponding improve-ment over BM25 by CRTER is 1.8%, 2.9% and 6.4%, respec-tively. Overall, CRTER X  X  retrieval performance is at least comparable to, if not better than PLM in our experiments. CRTER 0.2606(1.8%) 0.2789(2.9%) 0.3359(6.4%)
What is more, another advantage of CRTER model is that it can be fitted into more basic probabilistic weighting mod-els other than BM25, using defined variants of Cross Term.
To further analyze the effectiveness of the proposed CRTER model, out of the 550 test topics used in our experiments, we conduct a case study on topic 867 on the Blog06 collection. Using the judged documents, namely the golden standard provided by TREC, we explore the factors that could dif-ferentiate relevant documents from non-relevant ones. The statistics about the distribution of the query terms in the kernel functions,  X  and  X  relevant and non-relevant documents is shown in Table 4. The relevant and non-relevant documents cannot be distin-guished from each other by neither their average minimum distance nor the average distance (less than 10). Therefore, traditional proximity-based approaches may not be able to boost the relevant documents of this specific topic. However, the average Cross Term X  X  within document frequency (in CRTER model) of the relevant documents is clearly higher than the non-relevant ones, indicating that our proposed CRTER model can effectively identify the relevant docu-ments from the non-relevant ones for this topic.
In this paper, we introduce the concept of Cross Term to integrate the context of the query term proximity into IR models. The Cross Term measures the association that two terms have in the textual proximity context. A query term X  X  influence to their neighboring text is approximated by a kernel function, which gradually decreases with the distance to the term. The Cross Term is then defined as the overlapped influence of the two terms. Based on this idea, we propose a Cross Term Retrieval (CRTER) model, where within-document, within-query frequency of a Cross Term and the number of documents that contain the Cross Term are well defined. Through extensive experiments on standard TREC collections with various kernel functions, we show that the proposed model outperforms the BM25 baseline, and is at least comparable to the state-of-the-art Positional Language model. Furthermore, how the setting of the balancing parameter in the CRTER model and the shape parameter of kernel function affect CRTER X  X  effectiveness are discussed. Meanwhile, a group of optimal parameters shows the robustness of the CRTER model on all collections used.

Our proposed concept of Cross Term has various promis-ing future research directions. For example, we can apply the CRTER model to other classical IR models, such as lan-guage modeling, and the divergence from randomness mod-els. We can also conduct an in-depth study on the Cross Term X  X  distribution in documents collections, and examine which of the kernel functions fits the best to the actual dis-tribution. Blog06 Collection) This research is supported by the research grant from the Natural Sciences &amp; Engineering Research Council (NSERC) of Canada and the Early Researcher Award/ Premier X  X  Re-search Excellence Award. We thank four anonymous review-ers for their thorough review comments on this paper. [1] F. Ahmed and A. Nurnberger. Evaluation of n-gram [2] J. Allan, L. Ballesteros, J.P. Callan, W.B. Croft, and [3] C. Alvarez, P. Langlais, and J. Nie. Word pairs in [4] M. Beigbeder and A. Mercier. An information retrieval [5] A. Broschart and R. Schenkel. Proximity-aware [6] S. Buttcher, C. Clarke, and B. Lushman. Term [7] J.P. Callan, W.B. Croft, and S.M. Harding. The [8] B. Cho, C. Lee, and G. Lee. Exploring term [9] C. Clarke, G. Cormack, and F. Burkowski. Shortest [10] O. de Kretser and A. Moffat. Effective document [11] J. Gao, J. Nie, G. Wu, and G. Cao. Dependence [12] D. Hawking and P. Thistlewaite. Proximity operators [13] S. Kirkpatric, C. Gelatt, and M. Vecchi. Optimization [14] K. Kise, M. Junker, A. Dengel, and K. Matsumoto. [15] E. Krause. Taxicab Geometry. Mathematics Teacher , [16] Y. Lv and C. Zhai. Positional language models for [17] J. Mayfield and P. McNamee. Single n-gram [18] P. McNamee and J. Mayfield. Character n-gram [19] A. Mercier and M. Beigbeder. Fuzzy proximity [20] D. Metzler and W. Croft. A Markov random field [21] I. Ounis, M. De Rijke, C. Macdonald, G. Mishne, and [22] D. Petkova and W. Croft. Proximity-based document [23] J. Pickens. A comparison of language modeling and [24] Y. Rasolofo and J. Savoy. Term proximity scoring for [25] S. Robertson, S. Walker, S. Jones, [26] F. Song and W. Croft. A general language model for [27] M. Srikanth and R. Srihari. Biterm language models [28] T. Tao and C. Zhai. An exploration of proximity [29] E. Voorhees and D. Harman. TREC: Experiment and
