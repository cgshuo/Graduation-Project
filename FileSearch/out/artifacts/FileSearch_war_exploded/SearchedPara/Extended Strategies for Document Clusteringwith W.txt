 Representing documents as fixed-length vectors with the bag-of-words (BOW) model [8] is a common approach in document clustering for its simplicity, effi-ciency and acceptable accuracy. The dime nsions of the BOW feature space are simply generated by the distinct words in a document collection. However, the sparse data weakens the effect of BOW on identifying the similarities between documents composed of different words.

Our intuitive idea is to address the sparse data problem by enriching a doc-ument with the relatedness of all the words in a corpus to the document, which is inspired by the semantic representations for words [4], [9], [13]. Since the re-latedness between words cannot be obtained directly from the dimensionality reduction methods of word representation [2], [5] or the distributed representa-tion for words [13], the distributional representation in the feature space of BOW [4], [9] is used as the statistical foundation to measure the relatedness of all the words to the original words in a document, where the relatedness between words is measured by their co-occurrence times in the whole corpus. The relatedness of words to a document can be obtained through the combination of the generated word vectors.

Regarding each word vector as an abstract concept, the idea of combining word vectors to represent documents is similar to those mapping documents into lower feature spaces [2], [6], [7], [11] based on the assumption that docu-ments with similar meanings are supposed to share similar concepts. Significant improvements have been achieved with th ose methods. Nevertheless, the param-eters, especially dimension of the sp ace, are often difficult to be decided.
In fact, representing a document with word relatedness has been long studied [1], [10], [15]. In the previous studies, researchers mainly focus on the construct-ing of word vectors. The combination scheme is confined to the weighted sum of word vectors. The relatedness of a word to a document is the average of all the relatedness of the word to the original words in the document. However, such combination scheme has already been proved to hinder its usage on representing the semantic meanings of phrases [14]. By introducing three criterions to restrict the document enriching process, we deduce the theoretic degree of relatedness of a word to a document through the combination of word vectors. We further demonstrate that, the weighted sum of word vectors gives the upper bound of the theoretic degree of relatedness, and the lower bound has also been established. Unfortunately, the theoretic degree of relatedness cannot be obtained directly. Instead, we define the specificity of a word to re-weight the weights of all the word vectors. As a result, several strategie s have been proposed to extend the current weighted sum strategy. The main contr ibution of the newly proposed schemes is to approach the theoretic degree of relatedness by eliminating the overlaps between word co-occurrences. The experime ntal results agree with our analysis that significant improvements have been achieved with the proposed strategies. Furthermore, the guideline of how to se lect the proper strategy according to different datasets has been discussed in this paper. 2.1 Word Vector The idea that word usage can reveal sem antics was claimed by Harris [8], that words occur in similar contexts tend to have similar meanings. Following this instinct, the co-occurrence times of th e context words near a target word have been used to reveal the semantic meaning s of the target word [3], [16]. Formally, if there is a corpus D with n documents and m distinct words in it, the set of the m words constitute the vocabulary V ,andeachwordin V is denoted as v . Then the context of a target word v i is defined as: where c i,j is the co-occurrence times between v i and v j in D ,and c i is the total times v i occurs in D . Generally, the meanings of words should be independent of the corpus size, so c i is introduced here to give the basic context of each word [4]. The defined context is called word vector . The values in the word vector measure the relatedness of the dimensions to the target word.
 The co-occurrence times can be estimated with several methods [4], [9], [12]. We choose the one used in the weighted sum of the word vectors for document representation [1]: where c i,j | D and c i | D emphasize that the values are generated from the whole corpus D ,and c i | d d . Together with all the word vectors produced by (2), an m  X  m matrix is obtained, which is called context matrix : 2.2 Context Vector Model Given a target document d , the document vector produced by BOW is: where  X  i | d is the weight of the word v i in d . The popular weighting scheme is the Term Frequency and Inverse Document Frequency (tf  X  idf): where n i is the number of documents in which v i occurs.

Since BOW cannot figure out similar documents composed of different words, the Context Vector Model (CVM) tries to reveal the meanings of documents with a set of weighted word vectors [1], [ 10], [15]. Given the generated context matrix, the new document vector could be obtained by:
As a result, the value of any word in d is defined as the weighted sum of the relatedness of the word to the original words in d . Billhardt [1] has compared the effects of using the tf and tf  X  idf schemes to estimate the weights of the word vectors, and further exploited the deviations of the co-occurrence times in the word vectors to estimate their weights:
Experimental results demonstrate that tf  X  idf and tf  X  dev improve the perfor-mance of CVM. By revealing the improvement is achieved through eliminating the overlaps between word co-occurrences , we extend the two weighting schemes to re-weight the values for each dimensi on in the document vector generated by CVM directly. Nine subversions of the traditional CVM are hence generated in total.

The other symbols used for statistics throughout this paper include:  X  The symbol v i  X  v j | d refers to the event that the words v i and v j co-occur in  X  V d stands for the vocabulary composed of the distinct words in the document 3.1 Document Enriching Inspired by the success of word vectors t hat word usage can reveal semantics [4], [9], [12], our intuitive idea is to find the relatedness of all the words in a corpus to a document in terms of their relatedness to the original words in the document. Since the values of th e word vectors generated from a corpus have been proved effective on evaluating the relatedness between words, they are adopted as standards to specify whether the relatedness of a word to a document is well defined. As a result, the enriched representation of d , denoted as d , must satisfy the criterion that the word vectors of the words in d generated from d are equivalent to the corpus-lev el word vectors generated from D . Criterion 1  X  v i  X  V d , v i | d = v i | D .
 Considering the definition of a word vector in (1), we can get the following result with Criterion 1:  X  v the words in V to the original words in d should be considered, it X  X l make sense to force c i | d equal to c i | d , and to omit the other co-occurrences between the words in V  X  V d , which correspond to the following two criterions: Criterion 2  X  v i  X  V d ,c i | d = c i | d .
 Criterion 3  X  v i ,v j  X  V  X  V d ,c i,j | d =0 .

With Criterion 2, c i,j | d will depend merely on c i,j | D c is the inherent relatedness between v i and v j ,and c i | d makes the degree of the relatedness between v i and v j decided by the occurrence times of v i in d ; According to Criterion 3, the final relatedness of v j to d will not be affected by the words not in d .Asaresult,thetwoconstraintsmake d connected to d only, which distinguishes d from the representations for other documents.

Criterion 3 makes the words in V  X  V d merely show up with the words in V d :
Similarly, the occurrence times of the words in V d could be represented as:  X 
In (10), for each of the words in V d , its occurrences are separated into two parts. In the first part, it co-occurs with the words in V d ; in the second part, it co-occurs with the words not in V d . According to the definition of word vectors, the co-occurrences in the first part act as descriptors to describe the meanings of the words in V d ; while those in the second part are the same as the co-occurrences in (9). Only the first part is useful to estimate the relatedness between the words both in V d . Together with (9) and the first part in (10), we get the occurrence times of all the words used to measure their relatedness to the words in V d .These occurrence times are used to const ruct the new document vector:
We stress again that d implies the total occurrence times of the words in V d isthesameasthosein d , for the values of the words in V  X  V d can only be valid when the second part in (10) is established. To distinguish the values of each word in d and the real occurrence times of the words in d , the former are called semantic occurrence times in the following.

Compared (11) with (4), the significan t difference between BOW and the pro-posed method is that the values of the document vector generated by BOW are mainly depend on the local occurrences of the words in d , while the relatedness of all the words to the original words in d with the solid corpus -level statistics are used to fill the new document vector. 3.2 Calculation of the Enriched Document Substitute Criterion 2 into (8), the required co-occurrence times between the words in d could be obtained: Then the issue becomes how to get the se mantic occurrence times through the co-occurrence times. As shown in Example 1, the occurrence times of v 2 could be obtained by eliminating the repeat accumulations for v 2 during the sum of accumulations for a word during the su m of its co-occurrence times with other words are named as overlaps between co-occurrences for the word.
 Example 1. For a word segment v 1  X  v 2  X  v 3 , the sum of the co-occurrence accumulations exist during the sum of the co-occurrence times.

Without loss of generality, assume the first k words in V are specified as those in V d . The general formula to get the semantic occurrence times of each word in d by eliminating the overlaps between co-occurrences could be obtained inductively as:  X  v
According to (13), the semantic occurrenc e times will not be calculable unless the co-occurrence times among more than two words are given. Unfortunately, this information cannot be obtained under current conditions. Alternatively, several approximate approaches are proposed in the following.
 Boundaries of the Semantic Occurrence Times. Ideally, if there is no co-occurrence among more than two words, the sum of the co-occurrence times between two words will contain no rep eat accumulations. Namely, assume, Then all the items in (13) will equal zero except the first one: This is the maximum value the semantic occurrence times can approach. Con-versely, by assuming every word in d co-occurs with all the other words, the minimum value of the semantic occurrence times can be achieved. Namely,  X 
V =( v 1 ,v 2 , ...v a )  X  V d ,where v i  X  V is randomly selected from V d , assume, c ( v 1  X  v 2  X  X  X  v a  X  v j | d )=min { c 1 | d ,c 2 | d ,  X  X  X  ,c a | d ,c j | d } Taken (15) into (13), the items in (13) will offset each other and leave the maximum one:
Example 1 demonstrates that for any word segment whose length is longer than two, repeat accumulations will exist with the simple sum of co-occurrence times. Therefore, the semantic occurrenc e times of all the words will be overesti-mated by (14) in practice. On the other hand, the assumption required by (16) seems too farfetched to force all the words co-occur in such a long enriched text. Pragmatically, the upper and lower bounds of the semantic occurrence times are just the approximate results of th e real semantic occurrence times. Approximation with Specificities. Since the deviations between the upper bounds and the real semantic occurrence times for words are caused by the overlaps between co-occu rrences, the parameter  X  is introduced to eliminate the overlaps:
Theoretically, with proper  X  parameters, the real semantic occurrence times can be obtained. We define the specificities of words to estimate the  X  parameters. If a word mainly co-occurs with several specific words along the whole corpus, it is regarded as high specificity; otherwise, it is regarded as low specificity. The popular Inverse Document Frequency (idf) defined in (5) and the deviation (dev) defined in (7) can be used to estimate the specificities of each word.
Usually, words with high idf or dev values must mainly co-occur with several specific words, and therefore is regarded as high specificity. Such words are more possible to appear in different places. If not, these words tend to co-occur with all their specific words repeatedly in the same place, which has little chance to happen. Therefore, the overlaps between co-occurrences for the words with high specificities tend to be low, and they will be given a large value of  X  . Since the overlaps between the co-occurrence v i  X  v j and other co-occurrences are affected by the specificities of both v i and v j ,the  X  parameter should be decided by both of the two factors. Hence we have: where  X  i and  X  j evaluate the specificities of v i and v j respectively.
We can define the parameters  X  and  X  to either idf or dev optionally. With either of them,  X  and  X  will stay unchanged all over the corpus. For all the words in V ,their  X  and  X  parameters are organized as the vector  X  and  X  respectively. Then (17) could be rewritten in the vector form:
Generally,  X  and  X  should be normalized to make the parameters within the range defined by (18). However, the normalization will be non-necessary when the cosine similarity is used to evaluate the similarities between document vectors. In this case, normalization or not will produce the same result, for an overall normalization is performed during the calculation of the cosine similarity.
All the proposed strategies are regarded as the extended versions of CVM, for they all based on the context matrix. These versions could be divided in two branches. The first branch is the lower bound version of CVM defined by (16), CVM-LowerBound, which eliminates the overlaps between co-occurrences under a unified assumption that all the words in d co-occur with each other. The other branch is the versions incorporated with the specificities of words, which are listed in Table 1. Specifically, by setting both  X  and  X  to one, CVM-UpperBound is the upper bound version of CVM defined by (14). By setting  X  to idf and  X  to one, CVM-IDF-1 corresponds to the classical CVM where the values in the document vectors gener ated by BOW are weighted with the tf  X  idf scheme. By setting  X  to dev and  X  to one, CVM-DEV-1 corresponds to the one proposed in [1]. By setting  X  to idf or dev, the weighting schemes used in CVM are extended to consider the specificities of all the dimensions. 4.1 Experimental Setup In this section, we evaluate our algorithms on document clustering problem. The algorithms to be compared include: 1) the traditional BOW method with the tf  X  idf weighting scheme; 2) the Non-negative Matrix Factorization based clustering (NMF in short) [6]; 3) the existing versions of CVM, including, CVM-UpperBound, CVM-IDF-1, and CVM-DEV-1; 4) the other versions of CVM proposed in this paper.

Document vectors generated by the al gorithms listed above are used as the input of the k -means clustering algorithm. Two metrics are adopted in our ex-periments to evaluate the performance of document clustering, i.e., the accuracy (AC) and the normalized mutual information (NMI) [17]. The average similarity (avg. sim) and the average standard deviation (avg. std) of all the document vec-tors generated by the versions of CVM are also calculated to further demonstrate the differences between each version, wher e the cosine similarity is used to mea-sure the similarity between two document vectors; and the standard deviation of a document vector is calculated as follows: where  X  i is the value in d ,and  X   X  is the average of all the values in d . We conduct the performance evaluation using the TDT2 and Reuters datasets 1 . TDT2 consists of data extracted from six sources, including two newswires (APW, NYT), two radio programs(VOA, PRI) and two television programs(CNN, ABC). Those documents appearing in two or more categories are removed, and the cat-egories with more than 10 documents are kept, thus leaving us with 10,021 doc-uments in total. Reuters contains 21,578 documents which are grouped into 135 clusters. Those documents with multiple category labels are discarded, and the categories with more than 10 documents are selected. This leaves us with 8,213 documents in total. Reuters is more difficult for clustering than TDT2. In TDT2, the content of each cluster is narrowly d efined, whereas in Reuters, documents in each cluster have a broader variety of conte nt [17]. Table 2 provides the statistics of the datasets. 4.2 Performance Evaluations The clustering results are shown in Table 3. Besides the existing versions of CVM, the results of the proposed versions with the best performances are also shown in Table 3, specifically, CVM-IDF-DEV for TDT2 and CVM-LowerBound for Reuters. The evaluations were conducted with the cluster numbers ranging from two to ten. For each given cluster number, 50 test runs were performed on different randomly chosen clusters and the average performance is reported in the table. These experiments reveal a number of interesting points: 1) The versions of CVM achieve the best performances for most cases on both datasets. This demonstrates that by introducing word co-occurrences, they can generate a better representation by e nriching the target document with the relatedness between words. One possible reason for more improvement of CVM on the TDT2 dataset than on the Reuters dataset is that the average word frequency on TDT2 is much higher than t hat on Reuters. The high frequencies of words on the TDT2 dataset implies that the co-occurrence data is sufficient to produce more effective word vectors on the TDT2 dataset; 2) The proposed versions of CVM have competitive performance on both datasets, indicating the effectiveness on eliminating the overla ps between co-occurrences; and 3) None of the versions of CVM can perform the b est on all the sub-tasks with different cluster numbers, wherefore it X  X  more sensible to select appropriate methods in different situations rather than to find the best method for all cases. 4.3 Method Selection According to the above discussion, it X  X  e ssential to find the rule of selecting the proper version of CVM on different situations. Figure 1 shows how the perfor-mances of different versions of CVM vary r espectively. The results of different evaluation metrics are normalized into the same scale with the following function: where e 1 ,e 2 ,  X  X  X  ,e 10 are the evaluation results of all the ten versions of CVM forAvg.sim,Avg.std,Avg.AC,orAvg.NMI,  X  e is the average result of the evaluation, and e 1 is the one-norm of e .

As we can see, the rankings of the ten v ersions of CVM according to Avg. sim are the same on both datasets, and along with the rising of Avg. sim, Avg. std decreases. Since the standard deviation measures the specificities of each document vector, the opposite rankings agree with the common sense that high specificity corresponds to low similarity. By eliminating the additional overlaps between co-occurrences, the proposed version of CVM make the document vec-tors more specific. The rankings of the ten versions of CVM demonstrate the degrees of the overlaps between co-o ccurrences have been eliminated.
Figure 1 also shows that the branch incorporated with the specificities of words perform relatively stable on the two datasets, while CVM-LowerBound achieves contrary performances comp ared with the other versions on the two datasets. Since the semantic occurrenc e times of each word produced by CVM-LowerBound is decided by a single value, the maximum co-occurrence times with the original words in a document, CVM-LowerBound is more sensitive to the variance of statistics of dataset s. Therefore, its performance compared with the other versions will be unstable. CVM-LowerBound has a relatively bad performance on TDT2 may attribute to the long lengths of documents as shown in Table 2. Keeping the maximum value only will lose too much information contained by the other values; on the contrary, the short lengths of documents on Reuters makes the assumption that all words co-occur with each other in the same document more sensible. So with an abundant corpus, the versions incorporated with the specificities of w ords are expected to perform better; with a small corpus, especially in the case as R euters, where the lengths of documents are short, CVM-LowerBound is worth trying. In this paper, we present a novel document representation framework to enrich a document with the relatedness of words to the document. The framework demon-strates that the overlaps between word co-occurrences hinder the performance of the traditional Context Vector Model (CVM). By defining the specificities of words, several extended strategies based on CVM are proposed to eliminate the overlaps between co-occurrences. I n addition, a novel method which keeps the maximum values of word vectors on the same dimensions is proposed. The substantial experiments demonstrate that, with an abundant corpus, the branch incorporated with the specificities of w ords are expected to perform better; while with a small corpus, it X  X  worth trying the maximum-value-keeping strategy. Acknowledgments. This work was supported by the National Natural Science Foundation of China unde r grant 61070089, the Science Foundation of TianJin under grant 14JCYBJC15700.

