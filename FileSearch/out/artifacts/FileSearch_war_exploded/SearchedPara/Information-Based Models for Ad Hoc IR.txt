 We introduce in this paper the family of information-based models for ad hoc information retrieval. These models draw their inspiration from a long-standing hypothesis in IR, na me-ly the fact that the difference in the behaviors of a word at the document and collection levels brings information on th e significance of the word for the document. This hypothesis has been exploited in the 2-Poisson mixture models, in the notion of eliteness in BM25, and more recently in DFR mod-els. We show here that, combined with notions related to burstiness, it can lead to simpler and better models. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Theory, Algorithms, Experimentation IR Theory, Probabilistic Models, Burstiness The purpose of this paper is to introduce the family of in-formation based model for ad hoc information retrieval (IR). By information, we refer to Shannon information when ob-serving a statistical event. The informativeness of a word in a document has a rich tradition in information retrieval since the influential indexing methods developed by Harter ([11]). The idea that the respective behaviors of words in documents and in the collection bring information on word type is, de facto , not a novel idea in IR. It has inspired the 2-Poisson mixture model, the concept of eliteness in BM25 models and is at the heart of DFR models. In this paper, we come back to this idea in order to present a new family of IR models: information models . To do so, we first present, in section 2, the conditions a retrieval function should sat -isfy, on the basis of the heuristic retrieval constraints pr o-posed by Fang et al. [9]. Section 3 is then devoted to the presentation of information models, and their link with the retrieval conditions and the phenomenon known as bursti-ness . We present two instances of information models, based on two power law distributions, and show how to perform pseudo-relevance feedback for information models. Sectio n 4 provides an experimental validation of our models. Our ex-periments show that the information models we introduce significantly outperform language models and Okapi BM25. They are on par with DFR models, while being conceptually simpler, when pseudo-relevance feedback is not used. When using pseudo-relevance feedback, they significantly outpe r-form all models, including DFR ones. The notations we use throughout the paper are summarized in table 2 ( w represents a term). They slightly differ from standard notations for convenience reasons, i.e. their eas i-ness of use in the mathematical framework we deploy. We
Notation Description consider here retrieval functions, denoted RSV , of the form: where  X  is a set of parameters and where h , the form of which depends on the IR model considered, is assumed to be of class 1 C 2 and defined over R +  X   X  R +  X   X  R +  X   X   X , where
A function of class C 2 is a function for which second deriva-tives exist and are continuous.  X  represents the domain of the parameters in  X  and a is often the identity function. Language models [21], Okapi [15] and Divergence from Randomness [3] models as well as vector space models [16] all fit within the above form. For ex-ample, for the pivoted normalization retrieval formula [17 ],  X  = ( s, m, N ) and: h ( x, y, z,  X  ) = I ( x &gt; 0) where I is an indicator function which equals 1 when its argument is true and 0 otherwise. A certain number of hy-potheses, experimentally validated, sustain the developm ent of IR models. In particular, it is important that documents with more occurrences of query terms get higher scores than documents with less occurrences. However, the increase in the retrieval score should be smaller for larger term freque n-cies, inasmuch as the difference between say 110 and 111 is not as important as the one between 1 and 2 (the number of occurrences has doubled in the second case, whereas the increase is relatively marginal in the first case). In additi on, longer documents, when compared to shorter ones with ex-actly the same number of occurrences of query terms, should be penalized as they are likely to cover additional topics th an the ones present in the query. Lastly, it is important, when evaluating the retrieval score of a document, to weigh down terms occurring in many documents, i.e. which have a high document/collection frequency, as these terms have a lower discrimination power. These different considerations can b e analytically formalized as a set of simple conditions the re -trieval function h should satisfy: Conditions 1, 3 and 4 directly state that h should be increas-ing with the term frequency, and decreasing with the doc-ument length and the document/collection frequency. Con-ditions 1 and 2, already mentioned in this form by Fang et al. [9], state that h should be an increasing, concave func-tion of the term frequency, the concavity ensuring that the increase in the retrieval score will be smaller for larger te rm frequencies. We will refer to the above conditions as the form conditions inasmuch as they define the general shape the function h should have. They respectively correspond to the heuristic retrieval constraints TFC1, TFC2, LNC1 and TDC 2 introduced by Fang et al. [9]. In addition to this form conditions, Fang et al. [9] used two additional constraints to regulate the interaction between frequency and document length, i.e. between the derivatives wrt to x and y . These conditions, which we will refer to as adjustment condi-tions , allow to adjust the functions h satisfying the form conditions 1, 2, 3 and 4. They correspond to:
Condition 4 is in fact a special case of TDC, but this is beyond the scope of the current paper.
 Condition 5 LNC2: Let q a query.  X  k &gt; 1, if d 1 and d 2 are two documents such that y d 1 = k  X  y d 2 and for all words w , x d 1 w = k  X  x d 2 w , then RSV ( d 1 , q )  X  RSV ( d 2 , q ) Condition 6 TF-LNC: Let q = w a query with only word RSV ( d 2 , q ).
 We are now ready to proceed to the presentation of infor-mation models. In order to take into account the fact that one is comparing documents of different length, most IR models do not rely directly on the raw number of occurrences of words in doc-uments, but rather on normalized versions of it. Language models for example use the relative frequency of words in the document and the collection. Other classical term nor-malization schemes include the well know Okapi normaliza-tion, as well as the pivoted length normalization [17]. More recently, [14] propose another formulation for the languag e model using the notion of verbosity. DFR models usually adopt one of the two following term frequency normaliza-tions ( c is a multiplying factor): The concept of the information brought by a term in a doc-ument has been considered in several IR models. Harter [11] observed that  X  X ignificant X ,  X  X pecialty X  words of a documen t do not behave as  X  X unctional X  words. Indeed, the more a word deviates in a document from its average behavior in the collection, the more likely it is  X  X ignificant X  for this p ar-ticular document. This can be easily captured in terms of information: If a word behaves in the document as expected on the collection, then it has a high probability of occurren ce in the document p , according to the distribution collection, and the information it brings to the document,  X  log( p ), is small. On the contrary, if it has a low probability of oc-currence in the document, according to the distribution col -lection, then the amount of information it conveys is more important. Because of the above consideration, this idea, a t the basis of DFR models, has to be applied to the normal-ized form of the term frequency. This leads to the general and simple retrieval function: where t d w is the normalized form of x d w and  X  w is a param-eter for the probability distribution of w in the collection. We simply consider here that  X  w is set to either the aver-age number of occurrences of w in the collection, or to the average number of documents in which w occurs, that is: It is interesting to note that the retrieval function defined by equation 2, which is rank invariant by the change of the logarithmic base, satisfies the heuristic retrieval condit ions 1 and 3. Indeed, P rob ( X w  X  t d w |  X  w ) is a decreasing function of t d w . So, as long as t d w is an increasing function of x a decreasing function of y d , which is the case for all the normalization functions we are aware of, conditions 1 and 3 are satisfied for this family of models. Church and Gale [6] were the first to study, to our knowl-edge, the phenomenon of burstiness in texts. The term  X  X urstiness X  describes the behavior of words which tend to appear in bursts, i.e., once they appear in a document, they are much more likely to appear again. The notion of bursti-ness is similar to the one of aftereffect of future sampling ([10]), which describes the fact that the more we find a word in a document, the higher the expectation to find new oc-currences. Burstiness has recently received a lot of attent ion from different communities. Madsen [13], for example, pro-posed to use the Dirichlet Compound Multinomial (DCM) distribution in order to model burstiness in the context of text categorization and clustering. Elkan [8] then approx-imated the DCM distribution by the EDCM distribution, which learning time is faster, and showed the good behavior of the model obtained on different text clustering experi-ments. A related notion is the one of preferential attach-ment ([4] and [5]) often used in large networks, such as the web or social networks. It conveys the same idea: the more we have, the more we will get . In the context of IR, Xu and Akella [19] studied the use of a DCM model within the Probability Ranking Principle for modeling the dependency of word repetitive occurrences (a notion directly related t o burstiness), and argue that multinomial distributions alo ne are not appropriate for IR within this principle. More for-mally, Clinchant and Gaussier [7] introduced the following definition (slightly simplified here for clarity X  X  sake) in o rder to characterize discrete distributions which can account f or burstiness:
Definition 1. [Discrete case] A discrete distribution P is bursty iff for all integers ( n  X  , n ) , n  X   X  n : We generalize this definition to the continuous case as fol-lows:
Definition 2. [General case] A distribution P is bursty iff the function g  X  defined by: is a strictly increasing function of x . A distribution which verifies this condition is said to be bursty. which translates the fact that, with a bursty distribution, it is easier to generate higher values of X once lower values have been observed. We now show that this notion is directly related to the heuristic retrieval condition 2.

In the retrieval function defined by equation 2, the func-tion h we have considered so far corresponds to: In this case, condition 2 can be re-expressed as: negative as f is log( P rob ( X  X  t d w )). So, as long as (which is the case for all the normalization functions we are aware of, in particular the ones provided by equation 1), a sufficient condition for condition 2 is: The following theorem (the proof of which is given in the appendix) shows that bursty distributions satisfy this con -dition.

Theorem 3. Let P be a  X  X ursty X  probability distribution of class C 2 . Then: We thus see that under certain assumptions, IR models de-fined by equation 2 satisfy the form conditions 1, 2 and 3. We now summarize these assumptions which characterize information models. We characterize information models by the following three elements: 1. Normalization function The normalization function 2. Probability distribution The probability distribu-3. Retrieval function The retrieval function satisfies The general form of the retrieval function and the first two inequalities on the normalization function ensure that the model satisfies conditions 1 and 3. Theorem 3, in conjunc-tion with the last condition on the normalization function, additionally ensures that it satisfies condition 2. Hence, i n-formation models satisfy three (out of four) form condition s. The choice of the particular bursty distribution to be used has to be made in such a way that the last form condition and the two adjustment conditions are satisfied. We present here two power law distributions which are bursty and lead to information models satisfying all form and ad-justment conditions. The use of power law distributions to model burstiness is not entirely novel, as other studies ([4 , 5]) have used similar distributions to model preferential a t-tachment, a notion equivalent to burstiness.
 Log-Logistic Distribution The log-logistic (LL) distribution is defined by, for X  X  0: We consider here a restricted form of the log-logistic distr i-bution where  X  = 1, so that the the log-logistic information model takes the form: The log-logistic motivation resorts to previous work on tex t modeling. Following Church and Gale [6] and Airoldi [1], Clinchant and Gaussier [7] studied the negative binomial distribution in the context of text modeling. They then assumed a uniform Beta prior distribution over one of the parameters, leading to a distribution they refer to as the Beta negative binomial distribution, or BNB for short. One problem with the BNB distribution is that it is a discrete distribution and cannot be used for modeling t d w . However, the log-logistic distribution, with its  X  parameter set to 1, is a continuous counterpart of the BNB distribution since P LL ( x  X  X &lt; x + 1; r ) = P BNB ( x ).
 A Smoothed Power-Law (SPL) Distribution We consider here the distribution, which we will refer to as SPL , defined, for x &gt; 0, by: where f denotes the probability density function. Based on this distribution, the SPL information model thus takes the form: From equations 4 or 5, and using the normalization functions defined by equation 1, one can verify (a) that the log-logisti c and SPL distributions are bursty, and (b) that their corre-sponding information models additionally satisfy conditi ons 4, 5 and 6 (the demonstration is purely technical, and is skipped here). The log-logistic and SPL information models thus satisfy all the form and adjustment conditions.
Figure 1 illustrates the behavior of the log-logistic model , the SPL model and the InL2 DFR model (referred to as INL for short). To compare these models, we used a value of 0.005 for  X  and computed the term weight obtained for term frequencies varying from 0 to 15. For information models, the weight corresponds to the quantity  X  log P rob , whereas in the case of DFR models, this quantity is corrected by the Inf 2 part, leading to, with the underlying distributions retained: As one can note, the weight values obtained with the two information models are always above the ones obtained with the DFR model, the log-logistic model having a sharper in-crease than the other ones for low frequency terms. Pseudo-relevance feedback (PRF) in information models can be performed following the same approach as the one used in other models: The weight of a term in the original query is updated on the basis of the information brought by the top retrieved documents on the term. Denoting by R the set of top n documents retrieved for a given query, R = ( d 1 , . . . , d n ), the average information this set brings on a given term w can directly be computed as: where the mean is taken over all the documents in R . This is a major difference with the approach in [2] where all docu-ments in R are merged into a single document. Considering the documents in R as different documents allows one to take into account the differences in document lengths and number of occurrences. The original query is then modified, following standard approaches to PRF, to take into account the words appearing in R as: where  X  is a parameter controlling the modification brought by R to the original query. x q 2 w denotes the updated weight of w in the query. To assess the validity of our models, we used standard IR col-lections, from two evaluation campaigns: TREC (trec.nist. gov) and CLEF (www.clef-campaign.org). Table 2 gives the num-ber of documents ( N ), number of unique terms ( M ), aver-age document length and number of test queries for the col-lections we retained: ROBUST (TREC), TREC3, CLEF03 AdHoc Task, GIRT (CLEF Domain Specific Task, from the years 2004 to 2006). For the ROBUST and TREC3 collec-tions, we used standard Porter stemming. For the CLEF03 and GIRT collections, we used lemmatization, and an addi-tional decompounding step for the GIRT collection which is written in German.
 Table 2: Characteristics of the different collections ROBUST 490 779 992 462 289 250
We evaluated the log-logistic and the SPL model against language models, with both Jelinek-Mercer and Dirichlet Prior smoothing, as well as against the standard DFR mod-els and Okapi BM25. For each dataset, we randomly split queries in train and test (half of the queries are used for training, the other half for testing). We performed 10 such splits on each collection. The results we provide for the Mean Average Precision (MAP) and the precision at 10 doc-uments (P10) is the average of the values obtained over the 10 splits. The parameters of the different models are opti-mized (respectively for the MAP and the precision at 10) on the training set. The performance is then measured on the test set. To compare the different methods, a two-sided t-test (at the 0.05 level) is performed to assess the significan ce of the difference measured between the methods. All our experiments were carried out thanks to the Lemur Toolkit (www.lemurproject.org). In all the following tables, ROB-t represents the robust collection with query titles only, ROB-d the robust collection with query titles and description fields, CL-t represent titles for the CLEF collection, CL-d queries with title and descriptions and T3-t query titles fo r TREC-3 collection. The GIRT queries are just made up of a single sentence.

The version of the log-logistic model used in all our ex-periments is based on  X  w = n w N and the second length nor-malization in equation 1 (called L2 in DFR). We refer to this model as the LGD model. The same settings are cho-sen for the SPL model. As the parameter c in equation 1 is not bounded, we have to define a set of possible values from which to select the best value on the training set. We make use of the typical range proposed in works on DFR models, which also rely on equation 1 for document length normalization. The set of values we retained is: { 0.5, 0.75, 1, 2, 3, 4, 5, 6, 7, 8, 9 } .
 Comparison with Jelinek-Mercer and Dirichlet lan-guage models As the smoothing parameter of the Jelinek-Mercer language model is comprised between 0 and 1, we use a regular grid on [0 , 1] with a step size of 0 . 05 in order to select, on the train-ing set, the best value for this parameter. Table 3 shows the comparison of our models, LGD and SPL, with the Jelinek-Mercer language model (LM). On all collections, on both short and long queries, the LGD model significantly out-performs the Jelinek-Mercer language model. This is an in-teresting finding as the complexity of the two models is the same (in a way, they are both conceptually simple). Further-more, as the results displayed are averaged over 10 different splits, this shows that the LGD model consistently outper-forms the Jelinek-Mercer language model and thus yields a more robust approach to IR. Lastly, the SPL model is better than the Jelinek-Mercer model for most collections for MAP and P10.
 Table 3: LGD and SPL versus LM-Jelinek-Mercer after 10 splits; bold indicates significant difference MAP ROB-d ROB-t GIR T3-t CL-d CL-t JM 26.0 20.7 40.7 22.5 49.2 36.5 LGD 27.2 22.5 43.1 25.9 50.0 37.5 P10 ROB-d ROB-t GIR T3-t CL-d CL-t JM 43.8 35.5 67.5 40.7 33.0 26.2 LGD 46.0 38.9 69.4 52.4 33.6 26.6 MAP ROB-d ROB-t GIR T3-t CL-d CL-t JM 26.6 23.1 39.2 22.3 47.2 37.2 SPL 26.7 25.2 41.7 26.6 44.1 37.7 P10 ROB-d ROB-t GIR T3-t CL-d CL-t JM 44.4 39.8 66.0 43.9 34.0 25.6 SPL 47.6 45.3 69.8 56.0 34.0 25.6
For the Dirichlet prior language model, we optimized the smoothing parameter from a set of typical values, defined by: { 10, 50, 100, 200, 500, 800, 1000, 1500, 2000, 5000, 10000 } . Table 4 shows the results of the comparison between our models and the Dirichlet prior language model (DIR). These results parallel the ones obtained with the Jelinek-Mercer language model on most collections, even though the difference is less marked. For the ROB collection with short queries, the Dirichlet prior language model outperforms in average the log-logistic model (the difference being signifi -cant for the precision at 10 only). On the other collections, with both short and long queries and on both the MAP and the precision at 10, the log-logistic model outperforms in average the Dirichlet prior language model, the difference being significant in most cases. The Dirichlet model has a slight advantage in MAP over the SPL model, but SPL is better for precision. Overall, the information-based mode ls outperform in average language models.
 Table 4: LGD and SPL versus LM-Dirichlet after 10 splits; bold indicates significant difference MAP ROB-d ROB-t GIR T3-t CL-t CL-d DIR 27.1 25.1 41.1 25.6 36.2 48.5 LGD 27.4 25.0 42.1 24.8 36.8 49.7 P10 ROB-d ROB-t GIR T3-t CL-t CLF-d DIR 45.6 43.3 68.6 54.0 28.4 33.8 LGD 46.2 43.5 69.0 54.3 28.6 34.5 MAP ROB-d ROB-t GIR T3-t CL-t CL-d DIR 26.7 25.0 40.9 27.1 36.2 50.2 SPL 25.6 24.9 42.1 26.8 36.4 46.9 P10 ROB-d ROB-t GIR T3-t CL-t CL-d DIR 45.2 43.8 68.2 52.8 27.3 32.8 SPL 46.6 44.7 70.8 55.3 27.1 32.9 Comparison with BM25 We adopt the same methodology to compare information models with BM25. We choose only to optimize the k 1 pa-rameter of BM25 among the following values: { 0.3, 0.5, 0.8, 1.0, 1.2, 1.5, 1.8, 2, 2.2, 2.5 } . The others parameters b and k take their default values implemented in Lemur (0 . 75 and 7). Table 5 shows the comparison of the log-logistic and SPL models with Okapi BM25. The log-logistic is either better (4 collections out of 6 for mean average precision, 3 collec-tions out of 6 for P10) or on par with Okapi BM25. The same thing holds for the SPL model, which is 3 times bet-ter and 3 times on par for the MAP, and 4 times better, 1 time worse and 1 time on a par for the precision at 10 doc-uments. Overall, information models outperform in average Okapi BM25.
 Table 5: LGD and SPL versus BM25 after 10 splits; bold indicates best performance significant differ-ence MAP ROB-d ROB-t GIR T3-t CL-t CL-d BM25 26.8 22.4 39.8 25.4 34.9 46.8 LGD 28.2 23.5 41.4 26.1 34.8 48.0 P10 ROB-d ROB-t GIR T3-t CL-t CL-d BM25 45.9 42.6 62.6 50.6 28.5 33.7 LGD 46.5 44.3 66.6 53.8 28.7 34.4 MAP ROB-d ROB-t GIR T3-t CL-t CL-d BM25 26.9 24.2 38.5 25.3 35.1 47.3 SPL 27.1 25.4 40.5 26.8 34.5 47.0 P10 ROB-d ROB-t GIR T3-t CL-t CL-d BM25 45.7 41.4 62.8 51.0 28.5 36.1 SPL 47.6 44.1 67.9 57.0 28.0 35.4 Comparison with DFR models To compare our model with DFR ones, we chose, in this latter family, the InL2 model, based on the Geometric dis-tribution and Laplace law of succession, and the PL2 model based on the Poisson distribution and Laplace law. These models have been used with success in different works ([3, 7, 18] for example). All the models considered here make use of the same set of possible values for c , namely: { 0.5, 0.75, 1, 2, 3, 4, 5, 6, 7, 8, 9 } . It is however interesting to note that both PL2 and InL2 make use of discrete distributions (Geometric and Poisson) over continuous variables ( t d w are thus theoretically flawed. This is not the case of the information models which rely on a continuous distribution .
The results obtained, presented in tables 6 and 7 are more contrasted than the ones obtained with language models and Okapi BM25. In particular, for the precision at 10, LGD and InL2 perform similarly (LGD being significantly better on GIRT whereas InL2 is significantly better on ROB with long queries, the models being on a par in the other cases). For the MAP, the LGD model outperforms the InL2 model as it is significantly better on ROB (for both sort and long queries) and GIRT, and on a par on CLEF. SPL is better than InL2 for precision but on a par for MAP. Moreover, LGD and PL2 are on a par for MAP, while PL2 is better for P10. Lastly, PL2 is better than SPL for MAP but not for the precision at 10 documents. Overall, DFR models and information models yield similar results. This is all th e more so interesting that information models are simpler tha n DFR ones: They rely on a single information measure (see equation 2) without the re-normalization ( Inf 2 part) used in DFR models.
 Table 6: LGD and SPL versus INL after 10 splits; bold indicates significant difference MAP ROB-d ROB-t GIR T3-t CL-t CL-d INL2 27.7 24.8 42.5 27.3 37.5 47.7 LGD 28.5 25.0 43.1 27.3 37.4 48.0 P10 ROB-d ROB-t GIR T3-t CL-t CL-d INL2 47.7 43.3 67.0 52.4 27.3 33.4 LGD 47.0 43.5 69.4 53.2 27.2 33.3 MAP ROB-d ROB-t GIR T3-t CL-t CL-d INL 26.9 24.3 40.4 24.8 35.5 49.4 SPL 26.6 24.6 40.7 25.4 34.6 48.1 P10 ROB-d ROB-t GIR T3-t CL-t CL-d INL 47.6 42.8 63.4 52.5 28.8 33.8 SPL 47.8 44.1 68.0 53.9 28.7 33.6 Table 7: LGD and SPL versus PL2 after 10 splits; bold indicates significant difference MAP ROB-d ROB-t GIR T3-t CL-t CL-d LGD 27.3 24.7 40.5 24.0 36.2 47.5 LGD 46.6 43.2 66.7 53.9 28.5 33.7 MAP ROB-d ROB-t GIR T3-t CL-t CL-d PL2 26.3 25.2 42.8 25.8 37.3 45.7 SPL 26.3 25.2 42.7 25.3 37.4 44.1 PL2 46.0 45.2 69.3 54.8 26.2 32.7 SPL 47.0 45.2 69.8 55.4 25.9 32.9 Pseudo-relevance feedback There are many parameters for pseudo-relevance feedback algorithms: The number of document to consider ( N ), the number of terms to add the query ( T C ) and the weight to give to those new query terms (parameter  X  in equa-tion 7). Optimizing all these parameters and smoothing ones at the same time would be very costly. We thus mod-ify here our methodology. For each collection, we choose the optimal smoothing parameters for each model ( c , , k on all queries. The results obtained in this case are given in table 8, where LM+MIX corresponds here to the Dirichlet language model. They show, for example, that on the RO-BUST collection there is no difference between the baseline systems we will use for pseudo-relevance feedback in terms of MAP. Overall, the precision at 10 is very similar for the different systems, so that there is no bias, with the setting chosen, towards a particular system. We compare here the results obtained with the information models to two state-o f-the-art pseudo-relevance feedback models: Bo2, associate d with DFR models ([2]), and the mixture model associated with language models ([20]). For each collection, we aver-age the results obtained over 10 random splits, the variatio n of N and T C being made on each split so as to be able to compare the results of the different settings. For each set-ting, we optimize the weight to give to new terms:  X  (within { 0.1, 0.25, 0.5, 0.75, 1, 1.5, 2 } ) in information and Bo2 mod-feedback in language models. In this latter case, we set the feedback mixture noise to its default value (0 . 5). As before, we used Lemur to carry our experiments and optimize here only the mean average precision. Table 9 displays the result s for the different models (as before, a two-sided t-test at the 0.05 level is used to assess whether the difference is statis-tically significant, which is indicated by a  X  ). As one can note, the information models significantly outperform the pseudo-relevance feedback versions of both language mod-els and DFR models. The SPL model is the best one for N = 5 and T C = 5, while the LGD model yields the best performance in most other cases. Altough DFR and infor-mation models perform similarly when no feedback is used, their pseudo-relevance feedback versions do present differ -ences, information models outperforming significantly bot h language and DFR models in this latter case.
 Table 8: Performances of baseline setting for PRF ( N = 0 , T C = 0 ): bold indicates significant difference Table 9: Mean average precision of PRF experi-ments; bold indicates best performance,  X  significant difference over LM and Bo2 models LM+MIX 5 5 27.5 44.4 30.7 36.6 INL+Bo2 5 5 26.5 42.0 30.6 37.6 LM+MIX 5 10 28.3 45.7  X  33.6 37.4 INL+Bo2 5 10 27.5 42.7 32.6 37.5 LM+MIX 10 10 28.4 45.5 31.8 37.6 INL+Bo2 10 10 27.2 43.0 32.3 37.4 LM+MIX 10 20 29.0 46.2 33.7 38.2 INL+Bo2 10 20 27.7 43.5 33.8 37.7 LM+MIX 20 20 28.6 47.9 32.9 37.8 INL+Bo2 20 20 27.4 44.3 33.5 36.8 The Divergence from Randomness (DFR) framework pro-posed by Amati and van Rijsbergen [3] is based on the informative content provided by the occurrences of terms in documents, a quantity which is then corrected by the risk of accepting a term as a descriptor in a document ( first normalization principle ) and by normalizing the raw occur-rences by the length of a document ( second normalization principle ). The informative content Inf 1 ( t d w ) is based on a first probability distribution and is defined as: Inf 1 ( t  X  log P rob 1 ( t d w ). The first normalization principle is associ-ated with a second information defined from a second prob-ability distribution through: Inf 2 ( t d w ) = 1  X  P rob overall IR model is then defined as a combination of Inf 1 and Inf 2 : The above form shows that DFR models can be seen as information models, as defined by equation 2, with a cor-rection brought by the Inf 2 term. If Inf 2 ( t d w ) was not used in DFR models, the models with Poisson, Geometric, Bino-mial distributions would not respect condition 2, i.e would not be concave. In contrast, the use of bursty distributions in information models, together with the conditions on the normalization functions, ensure that condition 2 is satisfi ed. Another important difference between the two models is that DFR models make use of discrete distributions for real-valued variables, a conceptual flaw that information models do not have. Lastly, if the log-logistic, SPL and INL models have very simple forms (see for example the formulas given above for the weight they generate), the PL2 DFR model, one of the top performing DFR models, has a much more complex form ([18]). Information models are thus not only conceptually simpler, they also lead to simpler formulas. We have presented in this paper the family of information models. These models draw their inspiration from a long standing idea in information retrieval, namely the one that a word in a document may not behave statistically as ex-pected on the collection. Shannon information can be used to capture whenever a word deviates from its average be-havior, and we showed how to design IR models based on this information. In particular, we showed that the choice of the distribution to be used in such models was crucial for obtaining good retrieval models, the notion of good retriev al models being formalized here on the basis of the heuristic retrieval constraints developed in [9]. Our theoretical de vel-opment also emphasized the notion of  X  X urstiness X , which has been central to several studies. We showed how this no-tion relates to heuristic retrieval constraints, and how it can be captured through, e.g., power-law distributions. From these two distributions, we have proposed two effective IR models. The experiments we have conducted on four differ-ent collections illustrate the good behavior of these model s. They outperform in average the Jelinek-Mercer and Dirich-let prior language models as well as the Okapi BM25 model. They yield results similar to state-of-the-art DFR models (InL2 and PL2) when no pseudo-relevance feedback is used. When using pseudo-relevance feedback, however, the infor-mation models we have considered significantly outperform all the other models. This research was partly supported by the Pascal-2 Network of Excellence ICT-216886-NOE and the French project Fra-grances ANR-08-CORD-008. [1] E. M. Airoldi, W. W. Cohen, and S. E. Fienberg. [2] G. Amati, C. Carpineto, G. Romano, and F. U.
 [3] G. Amati and C. J. V. Rijsbergen. Probabilistic [4] A. L. Barabasi and R. Albert. Emergence of scaling in [5] D. Chakrabarti and C. Faloutsos. Graph mining: [6] K. W. Church and W. A. Gale. Poisson mixtures. [7] S. Clinchant and  X  E. Gaussier. The BNB distribution [8] C. Elkan. Clustering documents with an [9] H. Fang, T. Tao, and C. Zhai. A formal study of [10] W. Feller. An Introduction to Probability Theory and [11] S. P. Harter. A probabilistic approach to automatic [12] C. Macdonald, I. Ounis, V. Plachouras, I. Ruthven, [13] R. E. Madsen, D. Kauchak, and C. Elkan. Modeling [14] S.-H. Na, I.-S. Kang, and J.-H. Lee. Improving term [15] S. E. Robertson and S. Walker. Some simple effective [16] G. Salton and M. J. McGill. Introduction to Modern [17] A. Singhal, C. Buckley, and M. Mitra. Pivoted [18] I. O. V. Plachouras, B. He. University of Glasgow at [19] Z. Xu and R. Akella. A new probabilistic retrieval [20] C. Zhai and J. Lafferty. Model-based feedback in the [21] C. Zhai and J. Lafferty. A study of smoothing Let us recall what property 3 states: Let P be a probability distribution of class C 2 . A necessary condition for P to be bursty is: Proof Let P be a continuous probability distribution of class C .  X  y &gt; 0, the function g y defined by:  X  y &gt; 0 , g y ( x ) = P ( X  X  x + y | X  X  x ) = P ( X  X  x + y ) is increasing in x (by definition of a bursty distribution). Let F be the cumulative function of P . Then: g y ( x ) =
F ( x )  X  1 . For y sufficiently small, using a Taylor expansion of F ( x + y ), we have: where F  X  denotes  X  X   X  X  . Then, derivating g wrt x and consid-ering only the sign of g  X  , we get: which establishes the property.
