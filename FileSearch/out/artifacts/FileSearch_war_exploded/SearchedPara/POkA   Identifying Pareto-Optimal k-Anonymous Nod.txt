 Data generalization is widely used to protect identities and prevent inference of sensitive information during the pub-lic release of microdata. The k -anonymity model has been extensively applied in this context. The model seeks a gen-eralization scheme such that every individual becomes indis-tinguishable from at least k  X  1 other individuals and the loss in information while doing so is kept at a minimum. The search is performed on a domain hierarchy lattice where ev-ery node is a vector signifying the level of generalization for each attribute. An effort to understand privacy and data utility trade-offs will require knowing the minimum possible information losses of every possible value of k . However, this can easily lead to an exhaustive evaluation of all nodes in the hierarchy lattice. In this paper, we propose using the concept of Pareto-optimality to obtain the desired trade-off information. A Pareto-optimal generalization is one in which no other generalization can provide a higher value of k without increasing the information loss. We introduce the Pareto-Optimal k-Anonymization (POkA) algorithm to tra-verse the hierarchy lattice and show that the number of node evaluations required to find the Pareto-optimal generaliza-tions can be significantly reduced. Results on a benchmark data set show that the algorithm is capable of identifying all Pareto-optimal nodes by evaluating only 20% of nodes in the lattice.
 H.2.7 [ Database Management ]: Database Administra-tion X  security, integrity, and protection Algorithms k X  X nonymity, Microdata disclosure control, Pareto-optimality
Privacy violations emanating from the sharing of per-sonal information have raised an important concern in recent years. A study on the year 2000 census data of the U.S. pop-ulation reveals that 53% of the individuals can be uniquely identified by their gender, city and date of birth; 63% if the ZIP code is known in addition [4]. Such attributes, called quasi-identifiers, can be linked with other publicly available information to enable the re-identification. A classic experi-ment demonstrating the possibility is presented by Sweeney [15] where she managed to obtain the medical records of the Governor of Massachusetts from a medical insurance data set, containing no explicit identifying information, and a voter X  X  registration list. Much research in information assur-ance has therefore delved into the protection of respondent X  X  identity. The question to answer is how such information be modified so that the data remains useful for statistical stud-ies while protecting the respondents X  identities.

To attend to such privacy concerns, Samarati and Sweeney proposed the concept of data generalization to be used to satisfy a property called k -anonymity [13, 14]. Generaliza-tion of data is performed by grouping together data attribute values into a more general one, for example, replacing the age by an age range. A transformed data set of this nature is then said to be k -anonymous if each record in it is same as at least k  X  1 other records. This property implies that any record can be related to at least k underlying individuals.
Data generalization directly affects the utility of the data set. Enforcing higher k values during an anonymization makes more records indistinguishable from each other, thus reducing the statistical utility of the data set. Therefore, the amount of generalization to perform is chosen such that the k -anonymity property is satisfied for a given k while the information loss resulting from it is minimized [1, 3, 6, 7, 8, 9, 10, 12, 16]. Most algorithms identify such a so-lution from the set of all possible generalizations arranged in the form of a graph, called the domain hierarchy lattice, where every node is a vector signifying the amount of gener-alization for each quasi-identifier. Efficient traversal of the lattice has been particularly explored so that the number of nodes that undergo evaluation (determining equivalence classes and computing loss) can be reduced.

Although optimal k -anonymization is an important re-search problem, the applicability of available techniques is severely restricted. This is because the choice of a specific k remains dependent on the data publisher. More often than not, the choice is made in an arbitrary manner. Thus, it is often difficult to justify the choice of a particular value for k . Moreover, finding an optimal generalization with a fixed value of k provides no information on how the loss in information changes if a comparatively higher value of k is chosen instead. Fixating on a specific k value prevents one from determining if a higher k is possible with the same amount of information loss, or even with an information loss that is a tad more. Using existing algorithms for optimal k -anonymization to explore this trade-off would imply trying out all possible values of k . This can become expensive ow-ing to the number of nodes in the domain hierarchy lattice that need to be evaluated in the process. Algorithms meant for optimal k -anonymization with a fixed k , cannot correlate nodes in terms of trade-offs.

In this paper, we propose using the concept of Pareto-optimality to identify generalizations that cannot be simul-taneously improved both along the privacy and data util-ity dimensions. Our principle contribution is the Pareto-Optimal k -Anonymization (POkA) algorithm for identifying Pareto-optimal nodes in a domain hierarchy lattice. POkA utilizes a combination of depth first traversals of the lat-tice to efficiently move from one Pareto-optimal node to an-other. Two key properties, namely height boundary prop-erty and ground node property , have been proposed to guide the search in a manner that requires minimal node evalua-tions while assuring that all Pareto-optimal nodes are iden-tified. Performance analysis on a benchmark data set shows that POkA can prune a large number of sub-optimal nodes from being evaluated and can still obtain all Pareto-optimal nodes.

The rest of the paper is organized as follows. Section 2 presents some related work in k -anonymization. Background concepts are introduced in Section 3. Theoretical ground-work for the algorithm is presented in Section 4. Section 5 describes our algorithm. Performance analysis on a stan-dard benchmark data set is presented in Section 6. Finally, Section 7 concludes the paper.
Several algorithms have been proposed to find effective k -anonymization. The  X  -argus algorithm is based on the greedy generalization of infrequently occurring combinations of quasi-identifiers and suppresses outliers to meet the k -anonymity requirement [6]. The Datafly approach uses a heuristic method to first generalize the quasi-identifier con-taining the most number of distinct values [14]. Sequences of quasi-identifier values occurring less than k times are sup-pressed.
 On the more theoretical side, Sweeney propose the Min-Gen algorithm [14] that exhaustively examines all poten-tial generalizations to identify the optimal generalization that minimally satisfies the anonymity requirement. How-ever, the approach is impractical even on modest sized data sets. Meyerson and Williams have recently proposed an ap-proximation algorithm that achieves an anonymization with O( k log k ) of the optimal solution [11].

Samarati proposed an algorithm [12] that identifies all generalizations satisfying k -anonymity. Choice of an optimal generalization can then be made based on certain preference information provided by the data recipient. The approach in Incognito [8] is also aimed towards finding all generaliza-tions that satisfy k -anonymity for a given value of k . The basic Incognito algorithm starts with the generalization lat-tice of a single attribute and performs a modified bottom-up breadth-first search to determine the possible generalized domains of the attribute that satisfy k -anonymity. There-after, the generalization lattice is updated to include more and more number of attributes.

A genetic algorithm based formulation is proposed by Iyen-gar to perform k -anonymization [7]. Bayardo and Agrawal propose a complete search method that iteratively constructs less generalized solutions starting from a completely gener-alized data set [1]. The algorithm starts with a fully gen-eralized data set and systematically specializes it into one that is minimally k -anonymous. The idea of a solution cut is presented by Fung et al. in their approach to top down specialization [3]. A generalization is visualized as a  X  X ut X  through the taxonomy tree of each attribute. A cut of a tree is a subset of values in the tree that contains exactly one value on each root-to-leaf path. A solution cut is a cut that satisfies the anonymity requirement.

LeFevre et al. extend the notion of generalizations on at-tributes to generalization on tuples in the data set [9]. The authors argue that such multidimensional partitioning of the generalization domain show better performance in capturing the underlying multivariate distribution of the attributes, often advantageous in answering queries with predicates on more than just one attribute.

The first known attempt of exploring the privacy and util-ity trade-offs is undertaken by Dewri et al. [2]. The work focus on multi-objective optimization formulations involving a privacy parameter and an utility metric. A similar concept is presented by Huang and Du in the problem of optimizing randomized response schemes for privacy protection [5].
Our work significantly differs from earlier approaches ei-ther in terms of the cardinality of the solutions reported or in terms of the solution methodology where attempts have been made to find trade-off information.
A data set is conceptually arranged as a table of rows (or tuples ) and columns (or attributes ). Each attribute de-notes a semantic category of information that is a set of possible values. Attributes are unique within a table. Each row is a tuple of s values h v 1 , . . . , v s i , s being the num-ber of attributes in the data set, such that the value v j is in the domain of the j th attribute A j , for j = 1 , . . . , s . The domain of attribute A j is denoted by the singleton sets A the attribute.

A generalization of attribute A j is a union of its domain into supersets. Hence the generalized domain of A j can be written as H 1 j = A j 1 , . . . , A jm where  X  A jp  X  A jq =  X  for p 6 = q . We then say H 1 j is a generalized domain of A j , denoted as H 1 j &lt; G A j . The domain H 1 Generalization of an attribute X  X  domain in this manner gives rise to a domain generalization hierarchy (DGH) H N j j &lt; the attribute X  X  DGH. Refer to Fig. 1a for an example DGH. The DGH is a specification of how an attribute X  X  values can be combined progressively to bigger sets. H 0 j is called a full specialization of attribute A j , meaning that no two values belong to a single set. The other extreme of this is a full generalization H N j j where all values of the attribute belong to a single set. The generalization level of the attribute is lattice. signified by an integer between 0 and N j . A generalization level of 0 signifies that all values are distinguishable from each other, while a level of N j signifies that no two values can be distinguished from each other.

Given a DGH for each quasi-identifier in the data set, a tuple is said to be in an anonymized form when a generaliza-tion is applied on the attribute values. The anonymized form is represented as follows. Let us assume a tuple h v 1 , . . . , v in the data set. Let ( n 1 , . . . , n s ); 0  X  n i  X  N i representing the generalization level for each attribute; n is the level to use in the DGH for attribute A i . To map the value v 1 to its generalized form we replace it by the index of the set to which it belongs in the generalized do-main at level n 1 . For example, if H n 1 1 = A 11 , . . . , A v  X  A 1 p 1 , then v 1 is replaced by p 1 . After performing sim-ilar operations for the other attribute values, the tuple is anonymized to the form h p 1 , . . . , p s i , p i being the set index for value v i in H n i i . Transforming all tuples in the data set in this manner results in an anonymized data set.
The anonymized tuples of a data set can be grouped to-gether into equivalence classes. Two anonymized tuples h p class if p i = q i ; 1  X  i  X  s . The k -anonymity property re-quires that every such equivalence class should be of size at least k . Table 1 shows an example of this property. With reference to Fig. 1a, ZIP is generalized at level 1, SEX at level 1, and SALARY at level 0. Note that as higher k values are desired, higher generalization levels need to be used for the attributes. In principle, the anonymized tuples have categorical labels instead of the set index. Thus, two anonymized tuples in the same equivalence class have the same labels making them indistinguishable from each other. Higher k values therefore signify higher preservation of pri-vacy for the individuals whose information is represented in the tuples.
A domain hierarchy lattice DHL is a graph with Q i ( N i +1) nodes. Every node ( n 1 , . . . , n s ); 0  X  n i  X  N i is a vector of s dimensions where the i th element n i specifies the gener-alization level for attribute A i . The 3-anonymous table in Table 1 corresponds to the node (1 , 1 , 0). An edge exists be-tween two nodes ( n 1 , . . . , n s ) and ( m 1 , . . . , m
ZIP SEX SALARY 12345 M &lt; 50K 12346 M &lt; 50K 12345 F &lt; 50K 12355 F  X  50K 12355 M  X  50K 12356 M  X  50K
Table 1: 3 -anonymous version (right) of a table. if the vectors differ in exactly one element and the differ-ence is one. To put it formally, P i | n i  X  m i | = 1 . The node (0 , . . . , 0) (s times) is the fully specialized node of the lat-tice and corresponds to the un-anonymized data set. The node ( N 1 , . . . , N s ) is the fully generalized node and corre-sponds to no disclosure of the data. Fig. 1b illustrates these terms. In a typical k -anonymization algorithm, a node is sought in this lattice such that it satisfies k -anonymity and results in minimum information loss for the specified value of k . An exhaustive search is often not desired since evalua-tion of equivalence class sizes on a moderately sized data set can be computationally intensive. Most algorithms there-fore perform some form of pruning of the lattice. Note that the algorithms we refer to here are meant to find optimal generalization levels for a given value of k . Pruning of the lattice when no k value is specified is an unresolved problem until now.

Given a domain hierarchy lattice and a node ( n 1 , . . . , n we can also define a specialization graph which contains the nodes ( p 1 , . . . , p s ) such that p i  X  n i ; 1  X  i  X  s . Edges be-tween nodes in this graph are drawn similar to as in a DHL. The node ( n 1 , . . . , n s ) is called the root node of the special-ization graph. Along similar lines, we can also define a gen-eralization graph if nodes instead satisfy p i  X  n i ; 1  X  i  X  s . Fig. 1c highlights the specialization and generalization graph of the node (1 , 0 , 1). Intuitively, the specialization graph of a node contains all other nodes which are more specialized in one or more attributes, and in effect induce comparatively smaller k and lower loss. Contrary to that, a generalization graph of a node contains all other nodes which are more generalized in one or more attributes, and in effect induce comparatively larger k and higher loss. Hence, we shall often use the term  X  X ove down X  and  X  X ove up X  while traversing a specialization and generalization graph respectively. These two graphs will be used later while performing the search for optimal nodes.
Let k N and L N signify the k and information loss asso-ciated with a node N in the domain hierarchy lattice. The node N is a Pareto-optimal generalization if there is no other node M in the lattice such that one of the following two con-ditions hold. If one of the conditions is true, then M is said to domi-nate N . Therefore, a Pareto-optimal node is one whose k value cannot be improved upon by another node without in-creasing the information loss, and the information loss at the induced k value is minimal. Note that Pareto-optimal nodes need not always exist at every possible value of k . For ex-ample, in Fig. 2, no Pareto-optimal node appears with k = 4 since the node with k = 5 offers a higher value of k but with lower information loss. As is evident from the figure, trade-off behavior becomes clear when all Pareto-optimal solutions are known. The advantage of finding Pareto-optimal nodes is two fold. First, the minimal information loss at relevant k values is computed. Second, the choice of a particular so-lution can be based on the change of information loss rather than on arbitrary selection of k . In the figure, the choice of k = 5 can perhaps be made under the light that there is not much difference in information loss from the k = 3 node. Therefore, our objective is to search the DHL in an efficient manner and identify the Pareto-optimal nodes. Note that this process does not require the specification of a k value by the data publisher. Instead, optimal k values are reported as part of the set of Pareto-optimal generalizations.
The basic search strategy we adopt is to start from an already known Pareto-optimal node and prune nodes that cannot be Pareto-optimal. We shall start with the Pareto-optimal node with the highest possible k value and then use it as a starting step to find the next Pareto-optimal node. The next Pareto-optimal node is the one with a k value closest to that of the previous one but with lower loss. Hence, given a Pareto-optimal node N = ( n 1 , . . . , n s next Pareto-optimal node M is the one with minimum ( k N  X  k
M ); k M &lt; k N and L M &lt; L N . We shall call the node N a base node. In Fig. 2, the next Pareto-optimal node for the node with k = 5 is the node with k = 3. The node M is found by combining a DFS traversal of the specialization graph of N with a DFS traversal of the generalization graph of another node.

The first step in this process is to have a known Pareto-optimal node to begin with  X  the first base node. This is not difficult since the node ( N 1 , . . . , N s ) is bound to be Pareto-optimal. This node induces a k value equal to the size of the data set since all tuples get transformed to the same anonymized form. No other node can produce a k value higher than this. Once the next Pareto-optimal node is found, the process is repeated using this newly found node as the base node.

The next step is to assure that the node M can be reached from the base node N . Note that k M &lt; k N . Hence, M will not be present in the generalization graph of N . Recall that all nodes in the generalization graph of N will have more generalization in one or more attributes and will result in a higher k value. This observation results in the first level of pruning of the DHL. Given the Pareto-optimal node N , the number of nodes pruned by not searching the general-ization graph of N is Q i ( N i  X  n i +1). At first glance it may seem that the node M should be somewhere in the special-ization graph of N and is hence directly reachable from N . In fact, if the node M is in the specialization graph of N , then it would be the one with the highest k value in the set { ( n 1 , . . . , n j  X  1 , n j  X  1 , n j +1 , . . . , n s mediate neighbors of N in the graph. Since other nodes in the specialization graph are indeed specializations of a node in this set, they would have k values lower than the highest possible k in these immediate neighbors.

However, there is still a set of nodes that are not present in the specialization graph of N but can potentially include M . These nodes are the ones that can be generated from N by performing generalization in some attributes, specialization in some and no change in others. A positive observation in this context is that the node M can still be reached from N through a node common in the specialization graph of M and N , called a ground node .
A ground node is a node common in the specialization graphs of two nodes in the domain hierarchy lattice. A trivial ground node for any two nodes is the fully special-ized node (0 , . . . . , 0). Other non-trivial nodes also do ex-ist. Given the nodes N and M , any node in the set G = and M . M can then be reached from N by first moving down the specialization graph of N to a ground node and then moving up in the generalization graph of the ground node. The first phase of this process, i.e. moving down to the ground node, is called a depth search rooted at N . The phase of moving up from the ground node is called a height search rooted at the ground node. Fig. 3 illustrates this process.

Note that although a depth search is essential to find a ground node, nodes traversed in the process need not be Figure 3: Use of depth search and height search to reach node M from node N through a ground node. evaluated if they are not immediate neighbors of the base node. This follows from the earlier observation that if M resides in the specialization graph of N , then it will be one of the immediate neighbors. This observation leads to the second level of pruning in node evaluation. However, nodes in the height search are to be evaluated since k values will increase progressively. The only exception are nodes that are also part of the depth search but not immediate neigh-bors of the base node. A brute force method to perform the searches would mean traversing the specialization graph of N all the way down to the fully specialized node and then traversing the generalization graph of the trivial ground node. Clearly, this is an exhaustive search. The following two sections discuss the theoretical properties that bound the extent of search to be performed in both phases. Effi-ciently determining the minimum extent to search will fur-ther reduce the number of nodes to be evaluated.
Height search from a ground node is a DFS traversal of the generalization graph with the ground node as the root. To clarify any ambiguity in language, we say that height search is a height first traversal of the graph. The search is said to be at height h when the current node H is h steps away from the ground node G , i.e P i | h i  X  g i | = h . The height h is a dynamically chosen parameter in our approach. The assumption we make is that both k and information loss are non-decreasing quantities when performing more generaliza-tion in an attribute. Based on this assumption, the following property states how  X  X igh X  should the height search proceed before coming back to its parent node.

Height Boundary Property: Let A = { A 1 , . . . , A s } be a set of attributes. Given a Pareto-optimal node N = ( n 1 , . . . , n s ), the next Pareto-optimal node M can be found by a height search of a node in the specialization graph of N , further search up a node H being terminated whenever k H  X  k N or L H  X  L N .

Proof: The node to start the height search is a ground node G present in the specialization graph of N . Since M is the next Pareto-optimal node, we have k M &lt; k N and L
M &lt; L N . Based on the aforementioned assumption, if M is in the generalization graph of G , then it must be reached in a height search before a node H with k H  X  k N or L H  X  L is reached. Hence, whenever such a node is encountered, we have reached the maximum height along that path.

The height to search is therefore bounded by the k and loss values of the base node. We shall start with the ground node and choose a child for subsequent search only if it has k and loss lower than that of N . Child nodes that are common to the specialization graph of N (and not immediate neigh-bors of N ) are not evaluated and are always selected. All selected child nodes are subjected to the same evaluation for further search. Exact specification of how the next Pareto-optimal node is identified from height searches is presented in Section 5. The further down the ground node is from the base node, the more will be the number of nodes that will require evaluation in the height search. We therefore need a good estimate of the ground node closest to the next Pareto-optimal node. This is achieved by the depth search.
Depth search from a base node N is a DFS traversal of the specialization graph with N as the root. The depth d of the search is the maximum total difference in generalization levels from the base node to an internal node. Therefore, a depth search of depth d implies the traversal of nodes D such that P i | n i  X  d i |  X  d . Under this specification, an internal node is searched further only if its maximum total difference in generalization levels from the base node is less than d . Each node at depth d in the specialization graph of N is considered a candidate ground node and is subjected to a height search. Note that the DHL is a graph (not a tree) and hence the general intuition that the number of nodes subjected to height search will increase exponentially with larger d , is not true. In the following, we deduce the depth at which a ground node (not necessarily the closest one) for the next Pareto-optimal node is bound to exist and derive an estimate of the depth to actually search.

Lemma 1: The minimum equivalence class size of a data set with s attributes A = { A 1 , . . . , A s } is the same as the minimum equivalence class size of a data set with two at-tributes A 1 and A 2 , where A 1 = A j and A 2 = A 1  X  . . .  X  A
Proof: The proof follows from the fact that A 2 is sim-ply a concatenated version of the values of the attributes A ass sizes in a data set with s attributes require finding the frequency of occurrence of every possible combination of val-ues for s  X  1 attributes. In the case with two attributes, this step is performed while finding the equivalence class sizes for the attribute A 2 . Hence in both cases, a tuple with a particular sequence of values for A 1 , . . . , A s will belong to an equivalence class of the same size. This means that the minimum equivalence class size will also be same.

Lemma 2: Let A 1 and A 2 be two attributes with DGH lengths of N 1 and N 2 respectively. If N = ( n 1 , n 2 ), such that 0  X  n 1  X  N 1 and 0  X  n 2  X  N 2 , is a Pareto-optimal node, then one of G 1 = ( n 1 , 0) and G 2 = (0 , n 2 ) is a ground node for the next Pareto-optimal node.

Proof: The next Pareto-optimal node M is the one with k M closest to k N satisfying the constraints k M &lt; k N and L
M &lt; L N , i.e. there is no other node M  X  such that k M k
M  X  &lt; k N and L M  X  &lt; L N . The first set of possibilities are the immediate descendants of N , i.e. D 1 = ( n 1  X  1 , n D 2 = ( n 1 , n 2  X  1). Other descendant nodes of N , i.e. nodes max( k D 1 , k D 2 ) and hence do not satisfy the requirements for the next Pareto-optimal node. If one of D 1 or D 2 is Pareto-optimal then G 2 or G 1 respectively is a ground node for it.

If none of D 1 and D 2 is the next Pareto-optimal node then nodes of the form ( l 1 , h 2 ) or ( h 1 , l 2 ), where n N , must dominate them (after satisfying the required con-straints) and are likely candidates. Since G 1 is a ground node for any node of the form ( h 1 , l 2 ) and G 2 for nodes of the form ( l 1 , h 2 ), the result still holds.

Ground Node Property: Let A = { A 1 , . . . , A s } be a set of attributes with N i as the DGH length of A i . If N = ( n 1 , . . . , n s ); 0  X  n i  X  N i is a Pareto-optimal node, then one or more nodes of the set G best  X  G worst , where G best . . . , n j , . . . , 0) | 1  X  j  X  s } , are ground nodes of the next Pareto-optimal node.

Proof: The proof follows from the observation that the problem of finding Pareto-optimal nodes for s properties can be transformed to the case of finding Pareto-optimal nodes for two properties. To do so, we map the attribute set A to two attributes A 1 and A 2 such that A 1 = A j and A 2 = A 1  X  . . .  X  A j  X  1  X  A j +1  X  . . .  X  A s . A tuple h v 1 , . . . , v s i in the original data set is transformed to the form h v j , v 1 ; . . . ; v j  X  1 ; v j +1 ; . . . ; v s sion of a tuple is mapped in a similar manner. By Lemma 1, both data sets (generalized or not) will induce the same value of k . The loss metric can be modified so that the loss associated with a tuple in the data set with s attributes is proportional to that of the tuple in the data set with two attributes.
 The step that remains is a specification for the DGH of A . Nodes in the DGH of A 2 are formed by taking ev-ery possible combination of nodes from the DGHs of the s  X  1 attributes. A node is therefore of the form N j = ( n 1 ; . . . ; n j  X  1 ; n j +1 ; . . . ; n s ); 0  X  n i  X  N dering of these nodes is obtained by first sorting them in ascending order of the k value they induce and then in as-cending order of the loss (if k is same for two nodes). This follows the general notion of a DGH where k (and then loss) increases as we step from one node to the next. The do-main hierarchy lattice for two attributes contains nodes of the form ( n j , N j ).

Therefore, there is a bijective mapping between the set of nodes in the domain hierarchy lattice with s attributes and the set of nodes in the domain hierarchy lattice with two attributes. Since, k and loss of two corresponding nodes are also same, any Pareto-optimal node in the lattice with s attributes will also be Pareto-optimal in the lattice with two attributes, and vice versa.

Hence by Lemma 2, given N and a particular value for j , the ground nodes for the next Pareto-optimal node are one or both of (0 , N j ) or ( n j , 0). This translates to nodes of the the lattice for s attributes. Since the transformation into the case with two attributes can be performed in s different ways (1  X  j  X  s ), the set of such possible ground nodes is given as G best = { ( n 1 , . . . , n j  X  1 , 0 , n j +1 , . . . , n and G worst = { (0 , . . . , n j , . . . , 0) | 1  X  j  X  s } .
Assuring that nodes in G worst are covered while perform-ing a depth search, i.e. d = max sible Pareto-optimal nodes before N (ones with k and loss lower than that of N ) will be discovered. This is achieved by allowing the discovery of nodes that require specialization in all (but one) attributes. However, this is often not required if only the immediately next Pareto-optimal node has to be found. Covering nodes in G best , i.e. d = max ficient for this purpose. Depth search that covers nodes in G best allows the discovery of nodes that may require full spe-cialization in at most one attribute. To be precise, it allows finding nodes that have full specialization in the attribute with the longest DGH length. A longer DGH is typically specified for an attribute with a bigger domain size. Hence, full specialization in such an attribute has the tendency to induce small equivalence classes, thereby a small value of k . The immediately next Pareto-optimal node is more likely to have a k value closer to k N .

A good strategy to adopt here is to ensure that attributes which are close to full specialization in N , get a chance to become so while attributes that are far away from being fully specialized are explored in less depth. Consider the nature of nodes that get covered in a depth search of d avg =  X  P i depth, d avg being the average number of steps required for an attribute to become fully specialized from a fully gener-alized state. First, depth search to d avg depth ensures that half of the number of attributes will have a chance to be-come fully specialized. Second, in d avg number of steps, the attributes which are closer to being fully specialized stand a higher chance of becoming so. Third, taking d avg steps allows combination of different levels of specialization for different attributes without making any of them fully spe-cialized (if not already). Performance analysis in Section 6 corroborates that this strategy indeed provides the best bal-ance between exploration of nodes and discovery of Pareto-optimal ones.
The Pareto-Optimal k -Anonymization (POkA) algorithm is an iterative search method to identify the Pareto-optimal generalizations for a given data set. The attributes in the data set that are subjected to generalization (convention-ally called quasi-identifiers) are specified and a DGH is de-fined for every such attribute. The algorithm is iterative because one combination of depth search and height search, and a base node, is required to identify one Pareto-optimal node. The process is applied repeatedly to identify subse-quent nodes.
Before looking into the implementation details of the al-gorithm, we must define a strategy to handle outliers in the data set. Outliers in a data set are uncommon combina-tions of attribute values in a tuple. The existence of such tuples often make the process of k -anonymization difficult. Enforcing a k -anonymity property in the presence of out-liers may lead to excessive generalization in the attributes, thereby reducing the utility of the data set. A typical ap-proach to handle outliers is tuple suppression . Given a value of k , the tuples which belong to equivalence classes of size less than k are removed from the data set. Impact of the re-moval is then captured in the information loss measurement. The method is not directly applicable while finding Pareto-optimal generalizations since a k value is not pre-specified. The approach applied here is to use an upper bound on the number of suppressed tuples.

Let  X  be the maximum number of tuples that is allowed for suppression and R be the total number of tuples in the data set. Consider the sets E 1 , . . . , E R where E i contains tuples that are indistinguishable from i  X  1 other tuples. In other words, all tuples in the set E i are i -anonymous. Note Function 1 HeightSearch(Node P , boolean useNode) Input: A node P and a boolean value ( true or false ) useN-ode . N is the base node.
 Output: ( M, k M , L M ): M is a node in GG ( P ) with the highest k value such that k M &lt; k N and L M &lt; L N , or NULL if no such node exists. if [ useNode = true ] N o = ( P, k P , L P ) else N o = ( NULL , 0 , 0) for every child node C of P in GG ( P ) { if [ C  X  SG ( N ) and P i | n i  X  c i | &gt; 1] else { } if [ k Q &gt; k N o or ( k Q = k N o and L Q &lt; L N o )] } return N o that some E i s may be empty sets. If the anonymized data set is to be made k -anonymous, then all tuples in the sets E , . . . , E k  X  1 must be suppressed. Given the hard limit on suppression, this will be possible only if the number of tuples in the union of these sets is less than or equal to  X  . The same strategy can be applied in a reverse manner. Tuples in all sets E 1 , . . . , E j are suppressed such that j satisfies the with k = j + 1. The number of tuples suppressed is | E 1  X  . . .  X  E j | and can be accounted for in the loss measurement. This provides us a method to make maximum possible usage of the suppression limit without specifying a k value.
Height search and depth search are the two crucial com-ponents of POkA. We use the notation SG ( P ) and GG ( P ) to signify the set of nodes in the specialization graph and generalization graph of a node P respectively. Let N be the base node. k P and L P signify the k and information loss value associated with node P . Further, we assume the exis-tence of a function Evaluate which takes as input a node P in the DHL and returns k P and L P . These returned values are computed after using the suppression strategy mentioned earlier.

Function 1 presents the pseudo-code for a height search implementation. Height search initiated at a node therefore returns the node in its generalization graph with the highest k and one which satisfies the constraints on the k and loss. Any node that belongs to SG ( N ) and is not an immediate neighbor of N is not evaluated and height search proceeds without considering the k and loss of such a node. Other-wise, the node is evaluated to determine if further search is required as determined by the height boundary property. The method is initiated at a candidate ground node decided in the depth search. d signifies the depth to search in the following.
 Function 2 shows the pseudo-code for a depth search im-Function 2 DepthSearch(Node P ) Input: A node P in SG ( N ), N being the base node. Output: ( M, k M , L M ): M is a node reachable by height search of some node in SG ( P ) and with the highest k value such that k M &lt; k N and L M &lt; L N .
 N o = ( NULL , 0 , 0) for every child node C of P in SG ( P ) if [ P i | n i  X  c i | = d ] { } else Q = DepthSearch ( C ) if [ k Q &gt; k N o or ( k Q = k N o and L Q &lt; L N o )] } return N o Function 3 POkA() Output: The set P of Pareto-optimal nodes in the DHL N = ( N 1 , . . . , N s ) P = { N } while [ k N &gt; 2] { ( N, k N , L N ) = DepthSearch ( N )
P = P  X  { N } } return P plementation. The implementation is a simple DFS traversal with a height search being initiated when nodes at depth d are encountered. The best M found in these height searches is translated upwards towards the root of the specialization graph. Hence, the node M returned from a call to Depth-Search ( N ) is the next identified Pareto-optimal node.
POkA starts by a call to DepthSearch with the fully gener-alized node. For every new Pareto-optimal node identified, DepthSearch is iteratively called until the Pareto-optimal node with k  X  2 is found. Function 3 shows the pseudo-code of this process.
Node traversal in DepthSearch and HeightSearch can be further reduced by taking into account the structure of the DHL. Since the structure is that of a graph, nodes in the spe-cialization graph and generalization graph will share nodes as children. This structure results in repeated visits to a node during a depth/height search initiated by multiple par-ent nodes that share the node as a child. Although repeated visits to the same node do not increase the number of unique node evaluations required, there is redundancy involved as the results from searching the node further have already been taken into account. We therefore perform some book-keeping at every visited node to prevent repeated visits.
The output from every node visited during a height search, i.e. the return values, is separately stored in a list HBest . Whenever a height search is to be initiated at a node, the list HBest is first checked to find if an entry corresponding to the node exists. If it does, then the node (and subsequent nodes) has already been searched and the stored values are returned. If not then the height search is done as usual. Similar to HBest , a list DBest is created for every node visited during a depth search. Depth search at a node is not performed if results for the node already exist in the list. Both lists are emptied before calling DepthSearch in Function 3. Functions 1 and 2 can be easily modified to maintain and use these lists.
We applied our methodology to the  X  X dult.data X  bench-bureau database and has been extensively used in studies related to k -anonymization. We prepared the data set as described in [1, 7]. All rows with missing values are re-moved to finally have a total of 30162 rows. The attributes used in this study along with their DGH lengths are listed in Table 2. Attributes with larger domains have been assigned a longer DGH. The DGHs used are not shown here due to space constraints. The total number of nodes in the lattice is 17920. The suppression limit  X  is set at 1% of the data set size, i.e.  X  = 301. Experiments are performed with three different loss metrics  X  namely general loss metric (GLM) [7], discernibility (DCN) [1] and classification error (CE) [7]. The attribute  X  X alary Class X  is used as the class label while performing experiments with the CE metric. The lattice size in this case is 8960. Solutions reported by POkA are com-pared with those obtained by an exhaustive search of the entire DHL. Note that the number of nodes evaluated in the exhaustive search is equal to the size of the DHL, while that used by POkA is much less. Nonetheless, the exhaustive search provides us a definitive platform to judge the effi-ciency of POkA in finding true Pareto-optimal nodes. The depth d used in the experiment is set at d avg =  X  P i Ni unless otherwise stated.
 Table 2: Attributes and DGH lengths used from the adult census data set.
Pareto-optimal nodes identified by POkA for the three different loss metrics are shown in Fig. 4. The top row high-lights the nature of the search space while using different loss metrics and the true Pareto-optimal nodes. All plots are in log scale. An interesting observation is that, for all three loss metrics, the search space is more dense towards lower val-ues of k . This means as POkA proceeds towards finding the Pareto-optimal nodes in these regions, the number of nodes in the specialization graph of the base node decreases. Fur-ther, the Pareto-optimal nodes follow varied trends in the three metrics -concavity, convexity and disconnectedness.
The bottom row in Fig. 4 compares the nodes identified by POkA with those obtained from an exhaustive search. POkA demonstrates noteworthy convergence to the true Par-eto-optimal nodes across the different loss metrics. It man-ages to overcome the limitations that may be posed due to the arrangement of the Pareto-optimal nodes in the search space. While all solutions identified with GLM and CE are true Pareto-optimal nodes, one or two cases of sub-optimal or no identification is observed for DCN (notice the center of the plot). Nonetheless, identification of a sub-optimal node did not affect any subsequent searches. The requirement that the base node is a Pareto-optimal one is therefore not a strict one. POkA can very well be started from a sub-optimal node in the lattice and Pareto-optimal nodes with a k value lower than the starting node can still be discovered.
The depth d used in a depth search plays a crucial role in the identification of Pareto-optimal nodes. Table 3 shows the number of nodes evaluated in the lattice when using the GLM metric and varying depths. The maximum depth experimented with is 6, which is the equal to max( N i ), and ensures that nodes in G best will be reached for any base node. However, using such a value results in the evaluation of more than 60% of the nodes. As discussed in Section 4.3, coverage of all nodes in G best should not be required. The guiding principle derived is to search a depth of at least d avg =  X  Fig. 5 shows the Pareto-optimal nodes identified by using a depth of d avg or less. All Pareto-optimal nodes have been identified by using a depth of d avg (= 3). Using a depth of 2 resulted in some misidentification while a depth of 1 missed a number of the Pareto-optimal nodes. Using a depth higher than 3 did not prove to be of any advantage, less the number of nodes evaluated increased without necessity.
We found that the node pruning efficiency of POkA is much better in domain generalization lattices of bigger sizes. Bigger lattices may be formed either when the DGH lengths of the attributes considered are sufficiently long or when the number of attributes to anonymize is large. We ex-perimented with the latter possibility and found that the number of nodes evaluated dropped exponentially with in-creasing lattice size. Fig. 6 shows the percentage of nodes evaluated when using 2 , 3 , . . . , 8 attributes for anonymiza-tion in the census data set. The depth to search is set to d avg in each case. The higher number of evaluations for Table 3: Number of nodes evaluated when using dif-ferent depth limits. Results are generated by using the GLM metric. The total number of nodes is 17920 . Number of true Pareto-optimal nodes is 45 . smaller lattices can be attributed to the fact that the ob-served high concentration of solutions in certain regions of the search space no longer holds. As nodes are spread out in the search space, potential number of Pareto-optimal nodes are also high, thereby resulting in the evaluation of a higher fraction of the nodes. On an average, node evaluations are observed to be around 20% across the three metrics when anonymizing for all attributes.
To summarize the results, POkA can identify true Pareto-optimal nodes for a wide range of loss metrics that structure the search space in different ways. The experimental results corroborate the theoretical motivation behind using the av-erage number of steps for full specialization of an attribute as the depth to search. The performance of POkA does not deteriorate even if certain nodes identified by it are not Pareto-optimal and used as base nodes. Finally, the number of nodes evaluated is a small percentage of the total num-ber of nodes when the lattice is significantly bigger than the number of Pareto-optimal nodes it contain.
Privacy preserving data dissemination has to minimize the information loss in the anonymized data set while protect-ing the identity of underlying individuals to the maximum extent possible. In the context of k -anonymity, existing ap-proaches address these aspects only partially by concentrat-ing only on the issue of minimum information loss. Specif-ically, these approaches do not provide any information on the trade-off behavior between privacy and data utility.
In this paper, we proposed the POkA algorithm to find generalization schemes that are Pareto-optimal with respect to k -anonymity and a loss metric. By identifying Pareto-Figure 6: Percentage of nodes evaluated for vary-ing lattice sizes. Results are generated by using the DCN metric. Varying lattice sizes are gener-ated by considering varying number of attributes to anonymize. optimal nodes in a domain hierarchy lattice we can guaran-tee that no other generalization can improve on the privacy aspect without deteriorating data utility. POkA uses a com-bination of depth first traversals of the lattice to efficiently find the Pareto-optimal nodes. Theoretical groundwork be-hind efficiently performing these traversals is presented. Re-sults on a benchmark data set show that POkA has the potential to identify all Pareto-optimal nodes with a small percentage of node evaluations. They also demonstrate that the algorithm is applicable for a number of commonly used loss metrics.

Node evaluation can be further reduced if a better heuris-tic to stop the depth search can be found. An initial step in this direction is to investigate more stringent properties for the ground node. Another research direction is to extend the algorithm to other models of privacy. Pareto-optimality is used here as a two dimensional concept between privacy and data utility, while there exists privacy models that require the specification of more than a single parameter. Investi-gating Pareto-optimal anonymization with such models is a challenging area as well. This work was partially supported by the U.S. Air Force Office of Scientific Research under contract FA9550-07-1-0042. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing official policies, either expressed or implied, of the U.S. Air Force or other federal government agencies. [1] Bayardo, R. J., and Agrawal, R. Data Privacy [2] Dewri, R., Ray, I., Ray, I., and Whitley, D. On [3] Fung, B. C. M., Wang, K., and Yu, P. S.
 [4] Golle, P. Revisiting the Uniqueness of Simple [5] Huang, Z., and Du, W. OptRR: Optimizing [6] Hundepool, A., and Willenborg, L. Mu and Tau [7] Iyengar, V. S. Transforming Data to Satisfy Privacy [8] LeFevre, K., DeWitt, D. J., and Ramakrishnan, [9] LeFevre, K., DeWitt, D. J., and Ramakrishnan, [10] Loukides, G., and Shao, J. Capturing Data [11] Meyerson, A., and Williams, R. On the [12] Samarati, P. Protecting Respondents X  Identities in [13] Samarati, P., and Sweeney, L. Generalizing Data [14] Sweeney, L. Achieving k X  X nonymity Privacy [15] Sweeney, L. k X  X nonymity: A Model for Protecting [16] Wang, K., Yu, P., and Chakraborty, S.

