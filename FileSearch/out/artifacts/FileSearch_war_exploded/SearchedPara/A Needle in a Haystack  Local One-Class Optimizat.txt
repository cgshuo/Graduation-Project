 The goal of unsupervised learning is to extract concise de-scriptions of data given empirical samples. However, one is often only interested in modeling small parts of the data and ignore its remaining irrelevant parts. This is for ex-ample the case when the data consists of small groups of coherently structured data points, and the rest of the data are irrelevant or mere noise. It is a common scenario in a variety of applications from detecting regions of inter-est in images to finding subsets of co-expressed genes in genome-wide experiments.
 A related problem is demonstrated by the task of informa-tion retrieval by search engines. In this application, given a query, a handful of documents is sought out of billions of potential web pages. Unlike classification problems, re-trieval systems are often trained only with word phrases or documents marked as relevant, because it is difficult to sample well the space of irrelevant documents. In practice, users tend to consider only the first few pages retrieved by search engines. As a result, a system that retrieves few doc-uments that are all highly relevant, is preferred over a sys-tem that retrieves a large collection of documents, many of which are irrelevant, even if the larger collection suc-ceeds to cover more relevant documents. From the point of view of the trade-off between recall (retrieving most rel-evant documents) vs. precision (retrieving only relevant ones), this application favors high precision values. In these two general scenarios, one looks for a small but coherent subsets of data items, which can be achieved by finding a small-radius ball that covers as many data points as possible. This problem is known as one-class classifica-tion (Tax &amp; Duin, 1999), and has two opposite approaches for its solution: Most previous approaches formulate the task as a problem of outlier and novelty detection, in which most of the samples are identified as relevant. Here we take the opposite approach and try to identify a small subset of relevant samples, rather than keep all but few outliers. As we will see below, this  X  X eedle in a haystack X  approach re-quires a different formulation of the detection problem. Current approaches to one-class classification use convex cost functions that focus on large-scale structures in the data: the cost is constant within the ball and grows linearly on its outside (Sch  X  olkopf et al. (1995), Tax and Duin (1999) and Ben-Hur et al. (2001) used the Euclidean distance). In a related problem, introduced by Sch  X  olkopf et al. (2001), the goal is to separate most of the samples from the origin using a single hyper-plane. Recently, Crammer and Singer (2003) generalized these approaches to the more general family of Bregman Divergences. In all these formulations, due to the convexity of the cost function, the solution con-verges to the center of mass when the radius of the ball goes to zero, thus ignoring any explicit local structures. In order to model the fact that the distribution of the points outside the cluster is not of interest, the current paper takes the opposite approach: we use a cost function that grows linearly inside the ball but is kept constant outside it. This cost function is indifferent to the values of the  X  X on-interesting X  samples. Flat cost outside the ball is therefore expected to be better than linear cost when the interesting samples are localized in a small region, or when there are relatively few interesting samples. Unfortunately, this cost function leads to a non-convex optimization problem, that is harder to solve exactly. It is therefore necessary to de-velop approximation algorithms for this problem.
 This paper sets out to formalize this problem in the con-text of an unsupervised learning problem: searching for a compact, yet informative description of the data. We use the Information Bottleneck approach (Tishby et al., 1999) (IB) to formalize the learning task as an optimization prob-lem. IB is a general framework that we use here mainly as a tool. It allows us to formalize a stochastic description of the solutions, and to adapt previously studied IB algorithms for the current problem. We use here a previously stud-ied generalization of IB to a rather wide family of distance measures known as Bregman divergences. Since the Eu-clidean distance is a special case of a Bregman divergence our formulation can also be naturally combined with Mer-cer kernels, in a manner similar to Sch  X  olkopf et al. (1995; 2001). Assume we are given a set of m samples, { v x } m x =1  X   X  where the identity of a sample is indexed by the random variable X (that is, x  X  X  1 ,...,m } ).  X  can be a d di-mensional space or simplex and p ( x ) def = p ( X = x ) is the prior distribution over the samples. Our goal is to identify a subset of meaningful samples from the large set of data points, and we identify the meaningful points by the fact that they are clustered together. The learning task is there-fore to find a small body that covers as many samples as possible. To keep the shape of this body simple, we focus here on covering the points with a ball.
 We take a probabilistic approach and define C to be the event of being assigned to the ball. This binary event C  X  X  TRUE , FALSE } , has a joint distribution with the samples p ( C, x ) , and we denote by q ( C | x ) the probabil-ity that the sample indexed by x is assigned to the ball. This probability can be thought to reflect our belief that the specific sample v x is  X  X nteresting X . We also define the marginal q ( C )= x q ( C | x ) p ( x ) . Note that we allow soft assignment of a sample to the ball, when q ( C | x ) is neither strictly zero or one. As will be discussed below, our formu-lation allows to control the level of assignment X  X  softness and to reduce it to the hard limit where the assignment of each sample is dichotomous.
 The goal is to find a ball with radius R that is centered at a representative vector w  X   X  such that it covers as many samples as possible. The distance measures that we con-sider are Bregman divergences, which for the sake of clar-ity will be defined and discussed in the next section. At this point we treat them as generic distance measures denoted by B F ( v x w ) . Clearly, the value of R strongly effects the solution: The smaller the value of R is, less samples will be assigned to the ball. We first treat the case where R is constant and known, and discuss the more general case be-low. For any value of R , a solution to the problem is a set of probabilities q ( C | x ) for all x and a center of a ball To formalize the learning task as an optimization prob-lem we use the Information Bottleneck framework (IB) (Tishby et al., 1999). IB is an elegant formulation for reg-ularized unsupervised learning, that aims to extract a sim-ple, yet meaningful, representation C of a given data X . It is usually formalized as an optimization problem that trades off between two terms: One that measures how com-pact the representation is I ( C ; X ) , and another, I ( C ; Y ) , that measures how informative it is about an additional given variable Y . Both terms are formally quantified us-ing the same functional, the Shannon mutual information The current paper uses an alternative, yet mathematically equivalent, formulation of the IB principle. The accuracy of representation is measured here using the average distor-tion between the data and its compact representation. In the context of the one-class problem, the learning task is writ-ten as a minimization of the tradeoff between two terms where I ( C ; X ) is the mutual information between X and C , and  X  is a free parameter which determines the tradeoff between the model X  X  accuracy and simplicity.
 The first term D is an average distortion, D ( C | x, w ; v x )= q ( C | x ) B F ( v x w )+(1  X  q ( C | For each sample v x , the distortion term D ( C | x, w ; v erages terms that correspond to hard assignments of the sample. When q ( C | x )=1 ( v x is certain to be assigned to the ball) the assignment is penalized with a loss equal to the divergence between v x and w . When q ( C | x )=0 ( v is certain not to be assigned to the ball) the assignment is penalized with a constant loss R .
 The second term of Eq. (1) provides a measure of how strongly the model C compress the data. To compress the structures in X to a simpler representation, we wish to choose a ball that removes information about the specific identity of the points. This becomes clear when rewriting I ( C ; X )= h ( X )  X  h ( X | C ) where h (  X  ) is the differential entropy. Since h ( X ) is a constant, minimizing the informa-tion is equivalent to maximizing the entropy h ( X | C ) which is the fundamental measure of uncertainty about X . To simplify the form of the optimization problem we can rewrite the distortion term of Eq. (2) as x p ( x )[ q ( C | x ) B F ( v x w )+(1  X  q ( C | x )) R ]= x p ( x ) q ( C | x )( B F ( v x w )  X  R )+ R x p ( x ) . The second term is constant because both R and p ( x ) are given and thus can be omitted without affecting the mathematical properties of the distortion term.
 We therefore consider the optimization problem of the combined terms subject to q ( C | x )  X  [0 , 1]  X  x Since the objective function contains a product of two vari-ables ( q ( C | x ) B F ( v x w ) , where both q ( C | x ) and parameters) it is not convex, hence the optimization prob-lem is not convex either. We now turn to describe the properties of the optimal solu-tion of Eq. (3). We follow the derivation of (Tishby et al., 1999), use some algebraic manipulation, and obtain the fol-lowing set of self-consistent equations which describe the optimal solution. The first equation describes the marginal over C The second equation describes the location of the centroid w in terms of the input samples v x and the probabilities q ( C | x ) , w is therefore a weighted average of the input samples v x weighted by the likelihood probabilities p ( x | C ) . The third equation connects the value of the probabilities q ( C | x ) to the distance between the centroid w and each of the points v , When  X  =0 , the information term in the objective func-tion is dominant, yielding the simplest solution: all the (a) (b) points are assigned to the cluster with the same probabil-ity q ( C | x )= q ( C ) regardless of the specific sample value v . The specific value of q ( C ) is determined by ranging over the value of R . When  X   X  X  X  , q ( C | x ) attains one of three values as follows Thus if v x is inside (outside) the ball then q ( C | x )=1 ( q ( C | x )=0 ). If v x lies on the ball boundary then q ( C | x )= q ( C ) , the a-priori probability of being assigned to the ball. In other words, for a given w , the best assign-ment for x is to minimize the loss function As discussed above, most previous work on one-class prob-lems (Sch  X  olkopf et al., 1995; Tax &amp; Duin, 1999; Sch  X  olkopf et al., 2001; Crammer &amp; Singer, 2003) used a convex and unbounded loss functions such as max { 0 ,B F ( v x w )  X  R } . This is (up to a constant R ) the opposite of L . Inside the ball, L grows linearly, while the other loss function is constant. On the other hand, outside the ball L is constant while the other loss function grows linearly. An illustration of these two loss functions is given in Fig. 1.
 To demonstrate the effect of this difference between the two loss functions, we created a simple synthetic example in which 300 points were normally distributed in two Gaus-sians centered at [0 . 5 , 0 . 9] and [0 . 9 , 0 . 5] and 700 points were uniformly distributed over the 2-d unit square. Ap-plying one-class SVM to this problem tends to find clusters that are centered somewhere between the two Gaussians, because its cost function penalizes for centers that are dis-tant from the center of mass. This result is demonstrated in Fig. 2, where dot-dashed circles correspond to the results of a one-class SVM, each covering a different fraction of the data. One-class IB circles successfully identify one of the Gaussians for a large range of cluster sizes. To simplify the demonstration we consider the points in input space rather than in feature space and did not use any kernel with the one-class IB. The same problem is expected to occur in feature space if kernels are used.
 Until this point, we discussed the case where the radius R was assumed to be known to the algorithm. The effect of R on the solution is crucial: smaller R lead to smaller sub-sets assigned to the ball. The natural question arises: can this R be set in advance to a  X  X ight X  value? As in other model-complexity meta parameters (the number of clusters in clustering problems or number of components in PCA) this question is not well defined in an unsupervised learning setting. It is often the case that the data can be described at several resolutions: each revealing different aspects of the data. Which of them is the relevant one depends on the task at hand and is not dictated by the problem. A cor-rect characterization of the data therefore requires to obtain solutions for a spectrum of R values.
 The parameter R can in principle be replaced by other global parameters such as the  X  X eight X  of the ball. How-ever, we found that such formulations yielded more cum-bersome solutions whose properties were more difficult to analyse. In some cases however, it is possible to formally relate R to other global parameters. For example, in the case of an L 2 norm, the problem discussed here can be shown to be equivalent to a mixture model of a single Gaus-sian and a uniform background distribution. In this case R can be explicitly related to the prior weight of the mixture X  X  components. This derivation will be published elsewhere. In the original formulation of the IB (Tishby et al., 1999) the input samples represent multinomial distributions over a finite set. As a natural consequence, the definition of the mutual information over the joint distribution gives rise to the Kullback-Leibler (KL) divergence that emerges as a measure of discrepancy between any single sample v x and the centroid of a cluster w . In many interesting problems however, representing the data as distributions is not nat-ural. For example, in various experimental measurements the data are the sum of a signal and some near-Gaussian noise, and are well represented as vectors in a Euclidean space. In such cases the Euclidean distance is a more nat-ural divergence. In this paper we follow Crammer and Slonim (2003) which extended the IB to a richer set of pos-sible divergences, called Bregman divergences. A Breg-man divergence is defined via a strictly convex function F : X   X  R defined on a closed, convex set  X   X  R n . F has to satisfy a set of constraints, whose description we omit and refer the reader to Censor and Zenios (1997). All the functions we discuss in this paper obey these constraints and are hence Bregman functions. Assume that F is contin-uously differentiable at all points of  X  int , the interior of  X  , which we assume is nonempty. The Bregman divergence associated with F is defined for v  X   X  and w  X   X  int to be
B F ( v w ) def = F ( v )  X  [ F ( w )+  X  F ( w )  X  ( v  X  w Thus, B F measures the difference between F and its first-order Taylor expansion about w , evaluated at v . The di-vergences we employ are defined via a single scalar con-vex function f such that F ( v )= n l =1 f ( v l ) , where v l is the l -th coordinate of v . As a consequence, the Bregman divergences we use are sums of Bregman Diver-gences per coordinate of the input vectors, B F ( v w )= Although Bregman divergences are quite general they share many interesting properties. A property relevant to this pa-per is stated in the following Lemma.
 Lemma 1 (Convexity of a Bregmanian Ball) The set of points { v : B F ( v w )  X  R } is convex .
 The proof of the lemma is straightforward and uses the fact that Bregman divergences are convex in their first argu-ment. Thus, although we use a rich family of divergences we are still guaranteed to have only convex bodies. A straightforward consequence of the lemma is that if some point v belongs to the ball around w then all the points constituting the line connecting v and w also belong to it. Bregman distances provide a generalization over several commonly used distance measures. In this paper we demonstrate our algorithms and their analysis with two commonly used divergences. The first is the square dis-tance between v and w , B F ( v w )= 1 2 v  X  w 2 , ob-tained by setting f ( x )=(1 / 2) x 2 . In this case  X   X  R The second divergence can be obtained when  X  is the n -dimensional simplex. Setting f ( v )= v log( v ) yields the KL divergence B F ( v w )= n l =1 v l log v l w other Bregman divergences are Itakura-Saito (Censor &amp; Input: Set of Points { v x } m x =1 ; Divergence B F ; Radius R&gt; 0 Initialize:  X  Pick a point v  X  Set w = v Loop: For t =1 , 2 ,...,T  X  Draw a random permutation  X  of 1 ...m  X  For i =1 , 2 ,...m Return: Centroid w ; Assignment X  X  indicators q ( C | x ) Zenios, 1997) ( f ( x )=  X  log( x ) ,  X   X  R n + ) designed for speech analysis and Unnormalized Relative Entropy (Cen-sor &amp; Zenios, 1997) ( f ( x )= x log( x )  X  x ,  X   X  R used with data represented by counts (positive integers). The Itakura-Saito is not convex in its second argument. In this section we describe an algorithm that finds a local optimum for the problem defined in Eq. (3). Its output is the center of the ball w and the probabilistic assignments q ( C | x ) of points to the ball.
 Several algorithms were developed for the original IB prob-lem (see Slonim (2002) for review and comparison). Some of these algorithms can be adapted to the problem discussed in this paper, but some may not be easily extended to gen-eral Bregman divergences. For example, the iterative al-gorithm by Tishby et al. (1999) is based on iterations be-tween the three self-consistent equations of the bottleneck solution Eqs. (4,5,6). Unfortunately, for general Bregman divergences, it is no longer guaranteed that B F is convex with respect to its second argument, therefore the optimiza-tion over Eq. (5) may not find its minimum.
 Among the algorithms developed for IB, we chose to adapt the sequential algorithm (Slonim, 2002) because it is fast, easy to implement, and usually finds good local minima. It was designed for the hard clustering case, in which the as-signment of samples to clusters is deterministic rather than stochastic. While hard solutions may be inferior to soft so-lutions, they are often more easily interpretable, since the set of samples that belong to a cluster is clearly defined. An additional advantage of hard assignments is that the first term of the objective function can be further simpli-fied, I ( C ; X )= h ( C )  X  h ( C | X )= h ( C ) , since for hard assignment q ( C | x )  X  X  0 , 1 } and h ( C | x )=0 . The sequential algorithm operates in iterative steps. At each step, the algorithm picks a sample v x and tests if modifying its status and updating the centroid accordingly would decrease the value of the objective function. That is, for a sample that is not assigned to the ball, the algo-rithm checks if assigning it to the ball decreases the objec-tive function. Similarly, the algorithm checks if excluding a sample that is already assigned to the ball decreases the ob-jective function. We now describe in details the first case, the derivation of the second case is similar.
 Assume we have a set of parameters q ( C | x )= x p ( x ) q ( C | x ) , w = x on some specific  X  x for which q ( C |  X  x )=0 . Let  X  q ( C be the probability of assignment after making the change, and check how the objective function F changes by set-ting  X  q ( C |  X  x )=1 . We thus set  X  q ( C | x )= q ( C x =  X  x and  X  q ( C |  X  x )=1 =0= q ( C |  X  x ) . Let us denote by sample  X  x into the set of interesting samples. It is straight-forward to verify that  X  q ( C )= q ( C )+  X  p ( x ) , and Let us now compute the difference in the objective func-tion for both assignments of parameters. From the equality I ( C ; X )= h ( C ) above we obtain that the difference be-tween the two compression term is a difference between two values of the entropy functional,  X  h = h (  X  C )  X  h ( C ) . For the simplicity of presentation we omit this term, and discuss only the case for which  X   X  1 =0 . It is straightfor-ward to compute these terms for finite  X  as well :
F{  X  q ( C | x ) ,  X  w } X  X { q ( C | x ) , w } (10) where the equality stems from 0= q ( C | x ) =  X  q ( C | x )= 1 . We now use the definition of Bregman divergences (Eq. (8)) together with Eq. (9) and have the first term, Finally, plugging Eq. (11) into Eq. (10) we obtain F{  X  q ( C | x ) ,  X  w } X  X { q ( C | x ) , w } = q ( C ) B p (  X  x )( B F ( v  X  x  X  w )  X  R ) . By defining the following cluster of interesting points if, Plugging Eq. (9) into the left hand side of Eq. (12),  X  This term equals to the weighted average of two distor-tion terms, between w and v  X  x to their common average, where the two distortions are weighted in according to  X  c and  X  v  X  x . We thus call Eq. (13) the Average Bregman Di-vergence (ABD) 1 . Finally, using again the special form of Bregman divergences we can write Eq. (13) as,  X  Eq. (14) enables us to compute the Average Bregman Di-vergence in a very simple manner. To conclude this part of the algorithm, given a new sample which is not part of the cluster, we evaluate Eq. (14). If it is smaller than  X  v  X  x we merge it into the cluster, otherwise we ignore it. The case where we need to decide if to exclude a point out of the cluster is similar. In fact, we can always remove the given point v x out of the cluster and then apply the above procedure to determine if we like to include it back or not. We also like to note in passing that in the case of the Eu-clidean distance the condition of Eq. (12) can be written in case of the Euclidean distance we check whether the sam-ple v  X  x lies in a ball around the current center w . Where the radius of the ball depends also on the weight ( q ( C ) ) of the current cluster and the weight of the specific sample ( p (  X  x ) ). Finally, for the general case  X &lt;  X  , the following condition determines if the current example v  X  x should be merged into the cluster or not,  X  c B F ( w  X  c w +  X  v  X  x  X  outline of the algorithm is given in Fig. 3. We evaluate the performance of our approach on two real-life and high dimensional problems: identifying predictive genes, and document retrieval. 6.1. Gene expression in B-Cell Lymphoma We applied our approach to the problem of finding a small sets of interesting genes in micro arrays gene expression (a) (b) experiments. In genome-wide gene expression experi-ments, a large set of genes is pre-defined and their level of expression in various tissues is measured. The goal is often to identify subsets of genes that can be related to a biological process or function. In some cases labeled sets of tissues are available, and the relevant genes can be found using supervised techniques. In other cases, labeling is un-known. An important example is the case of identifying subtypes of disease, where gene expression are used to un-cover different biological mechanisms that lead to similar clinical symptoms. Within this framework we have cho-sen to use the data of (Alizadeh &amp; et al., 2000), that con-tains gene expression levels in tissues of B-cells lymphoma patients. It was previously used in numerous (mostly su-pervised) machine learning studies. Importantly, there also exists data on the survival of 39 patients, thus predicting this survival rate provides an external independent mea-sure of performance for any automated approach. Using their expertise, Alizadeh and colleagues were able to iden-tify genes that are believed to separate B cell lymphoma into two subtypes, which also differ considerably in the ex-pected survival rate. In the context of the current paper, Our goal with this data is to identify a single cluster that con-tains genes that are good predictors of survival in a fully automated unsupervised manner.
 The data consists of the expression profiles of 4 , 026 genes over 46 tissues. To reduce the di-mensionality of the data to a level that can be han-dled by the SVM package we used (OSU SVM, www.eleceng.ohio-state.edu/  X  maj/osu svm), we first chose the 500 genes with the highest single-gene information I ( x )= D KL [ p ( y | x ) || p ( y )] (where p ( x, y )=1 / [1 + exp( data ( x, y ))] is the probability assigned with a gene x and a tissue y ). We then applied both one-class-IB and one-class-SVM to the genes, each gene being a vector in R 46 . For one-class-SVM, we enumerated over  X  (the fraction of outliers) thus obtaining an optimal cluster for each cluster size. For one-class IB, we enumerated over the cost R , yielding optimal clusters of different sizes. For each cost, optimization was repeated 100 times with different random seeds, and the result with best target function value was used. All the genes of the chosen clusters, were then used in a linear regression to predict the survival rate. The log of the p values of this prediction are plot in Fig. 4 as a function of the cluster size. Since for one-class-IB several cost values may lead to the same cluster size, we plot the mean and standard deviation of log( p value) over all costs that yield the same cluster size.
 The performance of one-class-IB in predicting survival rates largely outperforms that of one-class-SVM for almost all cluster sizes, and is in particular better than one-class-SVM for smaller clusters. We believe that the reason is that small SVM clusters are strongly biased to the center of the whole data, thus are not sensitive enough to local structures. 6.2. Document Retrieval We further examined the usefulness of our approach on another real life complex data, by evaluating its performance in a documents retrieval problem. We used the Reuters-21578 corpus (available at www.daviddlewis.com/resources/testcollections), which contains 10 , 789 documents, each associated with cate-gories taken from a set of about 90 topics. We used the ModApte pre-processing of the corpus and the feature selection schema described in Slonim (2002), and picked a dictionary of size 2 , 000 .
 In our experiments we used a subset of the data that con-tained the five most frequent categories: earn (3964 docu-ments), acq (2369), money-fx (717), grain (582) and crude (578). Finally, we represented each document as a multino-mial distribution over term counts. For each of the above five topics we repeated the following experimental setup. The data was split into a training set that contained half of the documents from the chosen topic, and a test set that contained all other documents. For example, in the case of the earn category, the training contained 1 , 982 documents (all from the earn category), and the remaining 8 , 807 doc-uments constituted the test set ( 1 , 982 document from the earn category and 6 , 825 documents from other categories). We implemented two algorithms. First, one-class-IB, as described in Sec. 5 (OC-IB for short) and second, the one-class algorithm described in (Crammer &amp; Singer, 2003), called here OC-Convex. Both algorithms used the Kullback-Leibler divergence. OC-Convex uses a single pa-rameter  X  which controls the number of outliers, that was set here to range between 0 . 04% and 99 . 999% . OC-IB is tuned by setting the constant-cost parameter R . To cover the range of parameters values we first sampled pairs of samples from the training set, and used it to estimated the maximal and minimal divergence. We generated a list of possible costs approximately between this two estimates. For every possible cost value we trained the sequential al-gorithm 10 times and picked the most populated ball. We evaluated both learning algorithms as following. For both algorithms we set the value of the controlling param-eter and ran the algorithm which generated a ball parame-terized by a center and a radius. For each of the documents in the test set, we tested if it falls inside or outside this ball, and computed the two following measures (using the test set only). The recall which equals to the fraction of docu-ments labeled earn that are contained in the ball from the total number of documents labeled earn . The second mea-sure is the precision which equals to the fraction of docu-ments labeled earn that are contained in the ball from the total number of documents which are contained in the ball. In both algorithms the control parameter enables the user to trade between recall and precision. Recall-Precision curves are given in Fig. 5.
 The result over the five topics share a common behavior. For large values of recall the one-class combined with the convex loss achieves slightly better precision than OC-IB. As we decrease the recall value, the gap between the value of the precision decreases until the recall attains some value (around 20% ), in which the performance of the one-class IB is better than the one-class with convex loss. For very low values of recall the one-class IB clearly outperforms the one-class with convex loss. In fact, in three differ-ent categories, the ball obtained by the one-class algorithm with the convex loss did not contain even a single docu-ment from the test set. This result is in accordance with our intuition stated above: For very low values of recall the one-class with convex loss convergence to the center of mass of the input examples, regardless of any other prop-erty of the input data. On the other hand, the one-class IB locates areas of the input data which is dense related to its neighborhood. This intuition is further supported by the bottom-right panel in Fig. 5. For each of the categories and algorithms we computed the divergence of the centroid from the center of mass of examples. The height of the bar designates the maximal divergence over the possible set of control parameters for each of the algorithms. We addressed the problem of finding a small coherent sub-sets of data points in a high dimensional complex data. We demonstrated why current approaches to the problem are less sensitive to local structures of the data when search-ing for small subsets, and described a localized cost func-tion that improves this sensitivity. Building on the elegant Information-Bottleneck approach we cast the learning task as an optimization problem which trades-off between the two opposite demands of simplicity and accuracy. We de-rived a simple algorithm that is guaranteed to converge to a local minima that also works well in practice. In two real-world domains, when searching for small clusters, our approach provides an improvement over current one-class methods which are based on the principle of large mar-gin. Our approach also provides a well-defined probability measure which indicates for each of the input examples the probability that it is belonging to the single cluster or not. Similarly to previous approaches (Crammer &amp; Singer, 2003) our framework can be combined with a general no-tion of divergence measures -the Bregman divergences. Furthermore, Since the Euclidean norm is a Bregman di-vergence, we can also combine Mercer kernels (Sch  X  olkopf et al., 1995) with our method.
 The current paper focused on training a single one-class model for the input data. However, real world complex data, often contains several distinct regions that can be learned separately. It is therefore an interesting question how current algorithms for combining several one-class classifiers can be combined with our approach.
 Several extensions of the current work are of interest. First, it will be interesting to explore alternative macroscopic pa-rameters that control the problems solutions. Specifically, the constant cost R may be replaced with another parameter which will control the size of the cluster q ( C ) similar to the parameter  X  in previous formulations. Another extension is to use our approach to solve problems of set covering in the context of information theory and rate distortion. Acknowledgments: We thank Amir Globerson and Yoram Singer for fruitful discussions and careful reading of the manuscript.

