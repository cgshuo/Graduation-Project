 1. Introduction
Efficiently computing statistical models on large data sets remains an important problem. Data mining research has intro-
DBMS with a systems language (e.g. C++) allows flexibility in developing optimizations (reducing disk access and pushing most processing into main memory). Unfortunately, this approach eliminates the DBMS extensive functionality to the end lem that has received scant attention due to the mathematical nature of their computations, DBMS software complexity and the comprehensive set of techniques available in statistical packages [21]. Thus in a modern database environment, users generally export data sets to a data mining tool and perform most or all of the analysis outside the DBMS. SQL has been used as a mechanism to integrate data mining algorithms [34] since it is the standard language in relational DBMSs, but unfor-tunately it is generally slow and it has limitations to perform complex matrix computations. In this work, we show aggregate UDFs combined with caching are an outstanding alternative for high-performance data mining.

UDFs are a standard Application Programming Interface (API) available in modern DBMSs [13,14] . In general, UDFs are developed in the C language (or similar language), compiled to object code and efficiently executed inside the DBMS like any other SQL function. Thus UDFs represent an outstanding alternative to extend a DBMS with statistical models, exploiting the C language flexibility and speed. Therefore, there is no need to change SQL syntax with new data mining primitives or clauses, making UDF implementation and usage easier [39]. The UDF studied in this article can be programmed on any DBMS supporting aggregate UDFs. Since our research is based on exploiting UDFs as available in a modern DBMS, we assume the
DBMS provides basic SQL syntax and programming features to compile and run UDFs. Also, we assume the DBMS source code is not available and thus it cannot be extended. We provide sufficient technical details to program UDFs on any DBMS. Most research on exploiting hardware has focused on faster processors (e.g. multi-core, graphics processor [6]), which is a minor aspect considered in this work. We believe main memory is as aspect that needs more study, especially today. Performing data mining in main memory is important due to the following reasons, among others. Data mining is an iterative process: the same data set can be analyzed multiple times. Data mining algorithms generally require table scans: they do not need random access. Memory access based on 32 bit addressing can store up to 4 GB, which can hold fairly large data sets (mil-sampling introduces error in estimations. Therefore, it is crucial to understand the tradeoff between speed and accuracy. To pling and caching.

The article is organized as follows. Section 2 provides definitions and an overview of UDFs. Section 3 explains how aggre-the UDF and C++, evaluating UDF optimizations and profiling the UDF at run-time. Section 5 discusses related work. Conclu-sions are discussed in Section 6. 2. Preliminaries 2.1. Definitions The article focuses on the computation of multidimensional (multivariate) statistical models on a d -dimensional data set. x represents a column vector. To avoid confusion we use i  X  1 ... n as a subscript for points and h , a , b as dimension sub-scripts. The T superscript indicates matrix transposition, which is generally used to make matrices compatible for multipli-cation. To simplify notation, R without subscript means the sum is computed over all rows i  X  1 ... n . used for statistical purposes. We consider a standard horizontal layout for X , represented by table X X
X with a vertical layout represented by table X V  X  i ; h ; 2.2. User-defined functions
UDFs are programmed in a high-level programming language (like C or C++) and can be called in a  X  X  X ELECT X  statement, like any other SQL function. There are two classes of UDFs: (1) Scalar, that take one or more parameter values and return a single value, producing one value for each input row. (2) Aggregate, which work like standard SQL aggregate functions. They return one row for each distinct grouping of column value combinations and a column with some aggregation (e.g.  X  X  X um() X ). If there are no grouping columns then they return one row.

UDFs provide several important advantages. There is no need to modify internal DBMS code, which allows end-users to extend the DBMS with data mining functionality. UDFs are programmed in a traditional programming language and once compiled they can be used in any  X  X  X ELECT X  statement, like other SQL functions. The UDF source code can exploit the flex-ibility and speed of the programming language. Most importantly, UDFs work in main memory; this is a fundamental feature to reduce disk I/O and reduce run-time. Notice a UDF gets its input from a physical table scan, but such scan comes from evaluating the query calling the UDF. UDFs are automatically executed in parallel exploiting multi-threaded capabilities of the DBMS. This is an advantage, but also a constraint because code must be developed accordingly. On the other hand,
UDFs have important constraints and limitations. UDFs cannot perform any I/O operation, which is a constraint to protect internal storage. UDFs generally return one value of a simple data type. In other words, they cannot return a set of values or a matrix. Currently, UDF parameters in most DBMSs can be only of simple types (e.g. numbers or strings); array support is limited or not available. Scalar functions cannot keep values in main memory from row to row, which means the function can only keep temporary variables in stack memory. In contrast, aggregate functions can keep aggregated values in heap memory from row to row. UDFs cannot access memory outside their allocated heap memory or stack memory. The amount of memory that can be allocated is low, compared to available memory. Finally, UDFs are not portable since their code de-pends on the internal DBMS architecture. 3. Fast computation of sufficient statistics with UDFs 3.1. Overview of models
Past research has shown sufficient statistics summarize properties of a large data set [5,17,30] and can reduce the number is beyond the scope of this article explaining the mathematical details of how sufficient statistics apply to each model. We focus instead on their efficient computation inside a modern DBMS. 3.2. Multidimensional sufficient statistics
Let L be the linear sum of points, in the sense that each point is taken at power 1. L is a d 1 matrix, shown below with sum and column-vector notation.
 which is equivalent to Let Q be the quadratic sum of points, in the sense that each point is squared with a cross-product. Q is d d Matrix Q has sums of squares in the diagonal and sums of cross-products off the diagonal.
 The most important property about L and Q is that they are much smaller than X , when n is large (i.e. d n ). However, L and is that we can substitute every sum P x i for L and every matrix product XX general is the performance challenge). Our goal will be to study how to compute such sufficient statistics as efficiently as possible with a UDF scanning the data set once. 3.2.1. Computing models based on sufficient statistics
All statistical models considered in this article are based on computing the mean vector l , the covariance matrix R and the correlation matrix q on data set X as follows: The covariance matrix R is given by
Depending on the model, R can be assumed a diagonal matrix ( K -means clustering, EM clustering, Naive Bayes) or a full ma-trix (linear regression, PCA). Diagonal matrices are used when dimensions are assumed independent (i.e. covariances are as-sumed zero). Ref. [21] has an excellent overview of all models. Consequently, we consider a diagonal Q or non-diagonal Q , depending if the model assumes independence or not. Finally, the correlation matrix is derived by standardizing covariances, by dividing by standard deviations. Each entry of the correlation matrix q is given by: where R ab is the covariance between variables X a ; X b and r 3.2.2. Properties of sufficient statistics rather than computing models. 1. Storage space for n , L , Q is O  X  d 2  X  . Therefore, it is independent from n . n  X  n S  X  n T ; L  X  L S  X  L T ; Q  X  Q S  X  Q T . Notice this result states that cross-products are also distributive. gations accept GROUP BY. 4. Matrix Q is symmetric and can be diagonal when dimensions are assumed independent.

These properties have a practical application as follows: 1. Assuming d is low, storage space is low was well, despite being O  X  d statistics n , L , Q in main memory at all times. 2. Assuming S and T are analyzed by separate threads (or processing nodes) the distributive property enables full parallel
This aspect will be explained in detail below. 3. The sufficient statistics from several sample sets S 1 feasible to derive incremental algorithms [5]. 3.3. Alternatives to integrate statistical models with a DBMS
We discuss four alternatives to evaluate matrix equations taking into account X is stored inside a DBMS: (1) performing no computations only with SQL queries, manipulating matrices as relational tables; (4) computing matrix equations involving the data set inside the DBMS with UDFs.
 records), lack of functionality to manage data sets and models, the potentially lower processing power of a workstation com-pared to a fast database server and compromising data security. Alternative (2) represents the  X  X  X deal X  scenario, where all to the internal DBMS source code, the need to understand the internal DBMS architecture, the possibility of introducing memory leaks with array computations and the availability of many statistical and machine learning libraries and tools that can easily work outside the DBMS (e.g. in files outside the DBMS). Alternative (3) requires generating SQL code and exploits
DBMS functionality. However, since SQL does not provide advanced manipulation of multidimensional arrays, matrix oper-gramming matrix computations with UDFs exploiting arrays and the C control statements, enabling great efficiency and flexibility. For instance, inverting a matrix, evaluating a long expression involving many matrices, implementing a New-ton X  X aphson method and computing singular value decomposition (SVD) are difficult to program in SQL. Instead, matrices matrix expressions can be analyzed by a software system different from the DBMS, which can reside in the database server and UDFs. The remaining matrix equations, explained above, can be easily and efficiently computed with a mathematical library inside or outside the DBMS. 3.3.1. Computing sufficient statistics with SQL queries We assume X is stored on a vertical layout represented by table X thatsomedimensionvaluesarezero.When Q isadiagonalmatrix(e.g.forclustering)thequeryfor L canalsocompute Q bysquar-SELECT cast(count(distinct i ) AS double) /* n */ FROM X V ; SELECT h ; sum  X  v al  X  /* L h */ FROM X V GROUP BY h ;
SELECT T1.h AS a, T2.h AS b FROM X V T1 JOIN X V T2 ON T1.i=T2.i
WHERE a P b GROUP BY a , b ; 3.4. Aggregate UDF for a horizontal layout of data set
We now explain how to efficiently compute sufficient statistics n , L , Q with an aggregate UDF assuming a horizontal lay-out for X . We assume points in X appear in a random order and computations are performed in double precision variables to avoid numerical issues. 3.4.1. Aggregate UDF definition in SQL
The SQL definition specifies the call interface with parameters being passed at run-time and the value being returned. The aggregate UDF takes as parameter the type of Q matrix being computed: diagonal or triangular, to perform the minimum number of operations required. UDFs in cannot directly accept arrays as parameters or return arrays. To solve the array parameter limitation, x i is passed as a list of values to the UDF (currently d DBMSs provide limited array support through user-defined types (UDTs); our ideas can be easily extended with UDTs.
REPLACE FUNCTION udf _ nLQ _ triang( d INTEGER,
X _ 1 FLOAT, ... ,X _ d FLOAT ) RETURNS CHAR(36000) CLASS AGGREGATE (52000) The amount of maximum heap memory allocated is specified in the UDF definition with the  X  X  X LASS AGGREGATE X  clause.
The amount of memory required by the  X  X  X ig X  output value is also specified here. 3.4.2. Aggregate UDF variable storage
Following the single table scan approach for the SQL query, the aggregate UDF also computes n , L and Q in one pass. A C onal a one-dimensional array is sufficient and more efficient, whereas when Q is triangular a two-dimensional array is re-quired. Therefore, we propose to create two versions for the UDF depending on Q ; this saves memory and time. X is horizontally partitioned so that each thread can work in parallel. Notice n is double precision and the UDF has a fixed max-imum d because the UDF memory allocation is static (the UDF needs to be compiled before being called). typedef struct { } UDF _ nLQ _ storage; 3.4.3. Aggregate UDF run-time execution
We omit discussion on code for handling errors (e.g. empty tables), invalid arguments (e.g. data type), nulls and memory allocation. The aggregate UDF has four main steps: that are executed at different run-time stages: (1) Initialization, where memory is allocated and UDF arrays for L and Q are initialized in each thread. (2) Aggregation, where each x and passed to the UDF, x i entries are unpacked and assigned to array entries, n is incremented and L and Q entries are most time-consuming. (3) merging partial results, which is required to compute totals, by adding sub-totals obtained by by the master thread. (4) Returning sufficient statistics, where n , L , Q are packed and returned to the user.
Dimensionality d of X cannot be known at compile time. Therefore, the UDF  X  X  X truct X  record has a maximum dimension-ality, wasting some space. This is because the UDF allocates memory in the heap before the table scan starts. Otherwise, an alternative solution is to create d UDFs which accept a k -dimensional vector, where k  X  1 ; ... ; d .

Step (2) is the most intensive because it gets executed n times. The most intensive step is (2) because it gets executed n times, whereas the rest only once or as many threads the DBMS uses. Hence optimizations are incorporated in step (2). We the parameter list to the UDF internal array entries. Given the UDF parameter compile-time definition, d must also be passed as a parameter. The UDF updates n , L , Q as follows: n in incremented, L L  X  x trix: diagonal, triangular or full (default = triangular).
 dates n , L and Q reading each row once. /* Step 2: aggregate rows */ thread _ storage-&gt; n+=1.0; for(a=1;a &lt; =d;a++) { }
The partial result aggregation code in step (3), is somewhat similar to the aggregation step code, with the fundamental differences that we aggregate matrices instead of arithmetic expressions and all partial results are summarized into global totals for all threads. /* Step 3: aggregate partial results */ thread _ storage-&gt; n+= distributed-&gt; n; for(a=1;a &lt; =d;a++) for(a=1;a &lt; =d;a++)
Step (4) is the inverse of getting vector values in step 2, where we need to pack n , L and Q into a long string. This is a constraint imposed by SQL. Such string has the same memory limitation as the UDF, but on the stack. Therefore, both the impact on performance since it is executed only once. 3.4.4. Calling the aggregate UDF We first discuss models where Q is diagonal and then models where Q is triangular. For K -means, EM, Na X ve Bayes and the rows (GROUP BY clause), except for K -means or EM clustering in which case the closest cluster subscripts is used to compute k sets of sufficient statistics. 3.5. Sampling large data sets
If X is larger than memory available then we propose to collect samples from X to accelerate n , L , Q computation. However, sampling introduces error in estimations and such error has to be controlled and minimized. Sampling theory states error in estimation decreases as sample size s increases and sampling becomes more accurate with sampling with replacement [10]. Therefore, we exploit sampling mechanisms with replacement given their desirable statistical properties.
We propose two mechanisms to sample X to quickly compute and approximation of n , L , Q : geometric sampling and boot-strapping [21], adapting sampling techniques to work with the UDF. Each mechanism provides different advantages and dis-advantages with respect to speed and accuracy. The key issue is being able to get accurate approximations.
Geometric sampling starts with some initial sample size s and then sample size s grows geometrically until error comes down to an acceptable level. In our case, we consider sample size s growing proportional to powers of 2 (e.g. erable s n , when n is large, but not so small as to introduce significant error in approximations. On the other hand, boot-fore, we can incrementally update them based on each new sample. The number of times X is sampled ( B ) grows until an acceptable error level is reached. To keep a uniform statistical framework, we also consider a geometric growth rate for B , terms is more important than the fraction it represents from n . We argue this is a great property about sampling because to develop an efficient UDF on very large n it is preferable s is independent from n .

We now analyze some relevant DBMS aspects about sampling. SQL has a sampling clause (available in most DBMSs). In the case of the Teradata DBMS the sample clause just needs to specify s , the number of rows (e.g.  X  X  X ELECT * FROM X SAMPLE s  X ); Sampling in SQL is more efficient if sampling is done with replacement (SWR) because there is no need to track which points have already been sampled. The DBMS is efficient to collect sample set of small size s , using a block-based access up to s blocks from disk, and in many cases less when blocking factor is large. There exists an interaction between sampling and caching since future samples can be collected from blocks cached in main memory. If s is large then the DBMS needs to of n . Notice sampling can be a slow mechanism if X needs to be scanned, but in such case many blocks can be cached. If the
DBMS does not provide a sampling clause in SQL then to sample k random rows it may be necessary to generate k random cussion on how the DBMS collects samples can be found in [7].
 sion. Therefore, we will study how accurate nL and Q are to approximate correlation based on a sample set, considering com-plementary Q cases: diagonal or triangular Q . Our experiments will analyze error behavior with both sampling methods to approximate mean, variance and correlation, going from simplest to hardest case. 3.6. Time complexity and I/O cost based on sampling is analogous.

The aggregate UDFs for the horizontal layout to compute n , L , Q has time complexity O  X  d X assuming one I/O operation per row. Clearly SQL is the worst alternative for the vertical layout. 3.6.1. Time complexity for each technique
Once matrices are computed statistical techniques take the following times to compute models (with C code or a math-ematical or statistical library): linear correlation takes O  X  d to compute models becomes independent from n . The UDF approach will be efficient as long as d n . The CPU cost is the same for SQL and UDFs: for diagonal Q there are 2 dn floating point operations (flops), whereas for triangular Q there are  X  d  X  d 2 = 2  X  n flops. 3.7. Summary at a conceptual level
All linear statistical models can exploit the same sufficient statistics (summary matrices) and fortunately such matrices based on each model. Both scalar and aggregate UDFs perform as many calculations as possible when processing each data point in order to build models and score data sets in a single table scan. The remaining complex equations (rewritten based ing large matrices are performed inside the DBMS. UDFs represent an alternative to extend the DBMS with advanced statis-tical functionality without modifying the internal DBMS source code, which is a difficult task. UDFs can easily manipulate matrices using the C language data types and flow control statements. The query calling the UDF is automatically executed processed scanning table rows in any order, which makes fast parallel processing possible. That is, each data point updates summary matrices and gets scored in an independent manner from the other points. Finally, since our UDFs have wide appli-as a back-end server for intensive numeric processing tasks. 4. Experimental evaluation
Our goal is to make a fair comparison between UDFs and C++ running on the same hardware, analyzing the tradeoff be-tween sample size and accuracy for sampling mechanisms, assessing the impact of developing numeric optimizations (i.e. reducing FLOPS) and identifying bottlenecks in UDF execution. 4.1. Experimental setup 4.1.1. Hardware
We present an experimental evaluation on the Teradata DBMS, which supports UDFs. We used two  X  X  X verage X  DBMS serv-ers. Our first server hardware configuration had one Intel Dual Core CPU with each core running at 3.2 GHz, 4 GB of RAM memory and 750 GB on disk. The second server had identical hardware, except the CPU was an Intel Quad-core with each core running at 2.13 GHz. The operating system was Microsoft Windows XP. We emphasize the DBMS can cache (load) large tables into main memory (especially with 4 GB). Caching is automatically performed when the table is scanned and the table size is smaller than available memory. We carefully controlled when the DBMS cached the data set to measure performance.
We also consider the case where the data set is large enough so that it cannot fit in main memory. Our workstation had a 2.4 GHz CPU, 2 GB of main memory and 160 GB on disk. 4.1.2. Software: DBMS and C++
The UDFs were programmed in the C language variant provided the DBMS turning on optimizations for best performance (e.g. enabling unprotected execution; also called unfenced). It is recommended that a UDF is thoroughly tested in protected mode foe memory leaks (e.g. accessing an array out of bounds) before deployment. We also compare UDF performance with
C++. In order to conduct a fair comparison, the C++ implementation ran on another computer with identical hardware exploiting the same optimizations: caching and multi-threading. For caching we forced the operating system to cache the data set into main memory. On the other hand, we used the OpenMP multi-thread C library. For completeness, we also show
C++ time measurements without caching and multi-threading. In general, we computed a non-diagonal Q matrix to assume a worst case time complexity.
 We explain DBMS settings to enhance performance. UDFs were executed in unprotected (unfenced) mode. This means UDFs take full advantage of the DBMS multi-threading capabilities and run in the same address space as queries). During
UDF development and testing UDFs were executed in protected mode to detect any memory leaks. The DBMS was configured with four threads to improve I/O performance. The recovery log was disabled. All queries were run on temporary (spool) space, instead of creating temporary tables. We managed caching of large data sets, making sure the data set was not cached for disk-based measurements. UDFs in our DBMS were constrained to a 16-bit memory address space (i.e. 64 KB). The DBMS and the OS took about 800 KB of main memory.

The C++ implementation analyzed data sets stored on flat files exported out from the DBMS with the standard ODBC interface. The C++ program was optimized to scan X once, keeping L and Q in main memory at all times.

Average time is calculated from five runs of each experiment and in general it is reported in seconds. Times reported on processing in main memory exclude the time to initially cache the data set. When times are a fraction of a second we report 1 s (this happens only a few times; we avoided reporting milliseconds to make presentation clear). In general, UDF execution was stopped after 30 minutes indicated by *. 4.1.3. Data sets
We used both real and synthetic data sets to test performance and accuracy. To test accuracy we used real data with dif-compare with C++ and to analyze optimizations we generated large synthetic data sets with a mixture of normal distribu-tions varying n and d . Such data set were stored as tables in the DBMS. To test sampling efficiency we generated large data sets that could not fit in main memory ( n = 100M). 4.2. Disk versus memory processing
We start by comparing running time of our UDF and C++, with real data sets. Table 2 provides a summary. For C++ we use four threads (same as the DBMS), with and without caching. Given the speed of the server these data sets are processed in a few seconds. The UDF is 50% slower than C++ in main memory and about twice as slow on disk for the largest data set. For practical purposes, the UDF in main memory slightly than C++ running with the same optimizations (caching and multi-threading). We will study scalability with larger data sets in depth below.

We now compare how time grows to compute n , L , Q with a triangular (most demanding) Q . The first plot in Fig. 1 com-pares the UDF scanning the table from disk and reading the data set in main memory as n grows. The UDF achieves 6 speed-
Fig. 1 compares the UDF scanning the table from disk and reading the data set from main memory varying d (i.e. number of columns). Notice the speedup decreases as d grows. It becomes just 2 at the highest d . Our explanation is that time com-plexity O  X  d 2  X  to get Q becomes important and it outweighs caching.

We now compare C++ and the UDF. The UDF works on four threads in the DBMS and the data set is cached. Such four threads are configured when the DBMS is installed (they cannot be changed later). To make a fair comparison we also use multi-threading and caching with C++. Fig. 2 compares the UDF and C++. The first plot analyzes data set size n (in mil-lions), whereas the second one considers dimensionality. As expected, C++ is the fastest, but not much faster than the UDF. and multi-threading in more depth we now compare two C++ versions: one with one thread reading from disk (worst case) and a second one with four threads reading from main memory (best case, same advantages as UDF). Table 3 summarizes this detailed comparison varying n , including the time to export the data with ODBC. We want to point out there are much faster than ODBC. We preferred to show ODBC times because it is a standard interface, but as can be seen even one fifth of consider our experimental findings significant because C++ enjoys the same optimizations as the UDF, but the UDF executes
For completeness, we show times to export a table with the standard ODBC interface: C++ becomes the worst alternative considering ODBC. 4.3. Sampling large data sets
We divide these experiments in two parts. The first set of experiments compares time efficiency of sampling compared to full table scans, with data sets that can be cached in main memory and larger data sets that do not fit in main memory. The second set of experiments analyzes accuracy of estimation from samples. We focus on estimating mean, variance and cor-relation as they are the common statistics used in most models. To understand the tradeoff between accuracy and sample size, we study time efficiency with synthetic data sets (especially with data sets that do not fit in main memory) and we analyze accuracy with real data sets (to measure approximation error under stringent conditions).
 Table 4 compares evaluation time for both sampling mechanisms, against a full table scan being a baseline (worst case). We use two data sets with significantly different size: n = 1M can be cached in main memory and n = 100M cannot be cached.
We also consider dimensionality d  X  8 and d  X  16 to understand its impact on time. For geometric sampling s starts at sam-for bootstrapping we also allow X to be cached when n = 1M.

Our findings for geometric sampling are as follows (look at left part of Table 4 ). When n is small ( n = 1M) caching is not used the first time (column 1 below  X  X  X eometric X ), but it is used thereafter (columns labeled 2 and 3). We can see for n = 1M the second and third samples are computed much faster (the increment in time is small), indicating sampling does benefit from caching the entire data set. Geometric sampling is efficient only when s is small and n = 1M. Other-wise, geometric sampling time with s = 8000 approaches the time to scan the entire table indicating a high I/O cost to collect a sample data set. When n = 100M caching does not accelerate processing. The explanation is that the sample must be truly random and therefore many new blocks must be read from disk at each iteration. Geometric sampling acceleration becomes important when n is large  X  n  X  100M  X  . But for large n caching produces a marginal acceleration.
We can see that when X is cached geometric sampling performs much better than a table scan. In fact, when s is large and n = 1M it turns out that the entire data set X is cached and the second and their geometric samples take a fraction of 1 s. Last, we can see from the last two rows d plays a minor role in sampling: times are practically the same despite doubling d .

We now discuss bootstrapping, which is shown on the right part of Table 4 . Comparatively, bootstrapping is much slower than geometric sampling. The explanation is of course, the high number of iterations ( B ). If both s and B are large  X  s  X  8000 ; B  X  40  X  bootstrapping time approaches and even surpasses the time to perform a table scan. In such case, boot-strapping is inefficient. Another interesting aspect is that the difference between s = 500 and s = 1000 is small when n = 100M, indicating that collecting smaller sample sizes does not necessarily improve performance. Bootstrapping does seem a reasonable alternative for s = 1000, which is a decent sample size [10]. The last rows of Table 4 show times for d  X  16 for s = 1000, 2000 (notice we do not show all sample sizes s for n = 1M and n = 100M since the trend is the same).
For both geometric sampling and bootstrapping when d is doubled the change in time is marginal, indicating d plays a minor following paragraphs, we will carefully analyze accuracy with s = 1000 by default.
 be used to compute the exact value of l ; R and q (with double precision variables, as explained above). We then compute the relative of the approximation with respect to the exact value. Table 5 shows the maximum relative error to estimate l and R dimensions. To make the table intuitive and concise errors between 0.001 and 0.01 are shown as 0.01 and errors below 0.001 (1.0E 3) are shown as 0.00; the rationale is scientific notation takes too much space and errors below 1% prove high accu-of l approaches zero rapidly (such error is always smaller than the error for R ) and hence we do not show it. The general ually decreases as s doubles. We must notice that bootstrapping starts off with good accuracy, even at B = 10 and thus error decreases slower. At s = 8000 there are two data sets whose error has gone down to 1%. On the other hand, bootstrapping error decreases as B grows, being 0.01 or better for four data sets at B  X  100. Overall, both sampling mechanisms are good, but bootstrapping tends to be more accurate and more stable, although slower (as shown in Table 4 ).

Table 6 shows error behavior for the most difficult case: the correlation matrix, with s = 1000 by default. As noted above, to make the table intuitive and concise errors between 0.001 and 0.01 are shown as 0.01, errors below 0.001 (1.0E 3) are shown as 0.00 (scientific notation takes too much space). Both sampling techniques become fairly accurate when s = 8000 and B = 100, respectively, but bootstrapping is clearly superior.

Given the longer time bootstrapping requires geometric sampling is a faster, yet reasonably accurate alternative (except for data set Corel, which warrants larger samples). In summary, based on our experimental results we recommend using s = 1000 or larger and geometric sampling by default. If high accuracy is needed bootstrapping represents a more accurate, but slower, alternative, Another observation is that there is no universal setting for B . In two data sets B accuracy, whereas two data sets required going up to B = 100. It may well be the case that some data set may require going up to B = 500 or even B = 1000 to make relative error come down, although we must emphasize that was not necessary with the diverse real data sets we used. A similar trend holds for geometric sampling. Such trends indicate that the amount of sampling required depends on the statistical properties of the data set rather than its size. 4.4. Optimizations
Table 7 analyzes the impact of performing O  X  d  X  computations instead of O  X  d the amount is memory allocated is O  X  d  X  (a one-dimensional array), but O  X  d
Therefore, the amount of allocated main memory and time complexity are related. When the UDF reads the data set from disk there is practically no difference: the speedup achieved by this numerical optimization is marginal. On the other hand, when the data set is read from main memory this numerical optimization does make a difference. For the highest d the
In the second optimization we compare an SQL query and the UDF to get the summary matrices. The SQL query requires a self-join on a vertical layout of X to compute the cross-products in Q and thus it has time complexity O  X  d pare a more efficient SQL query based solution that requires O  X  d maximum number of columns allowed by the DBMS (e.g. d P 32 makes this query useless). As we can see from Table 8 the speedup provided by the UDF is two orders of magnitude bigger, compared to the SQL query. We attempted to exploit cach-data mining should be avoided.

We now compare UDF running time between two high-performance CPUs: one with two cores (dual-core) and another one with four cores (quadcore). Table 9 shows a great benefit about running UDFs with cached data sets: the UDF running on the 4-core CPU takes half the time it takes on the 2-core CPU. The reason behind such acceleration is that the threads eval-other hand, there is no acceleration when the UDF reads X from disk: time is about the same since I/O time dominates. We out developing any complex parallel algorithm.

We conclude our experiments with a breakdown of execution time of the UDF at run-time. Table 10 compares the UDF with the data set retrieved from disk and the data set cached in main memory. We wanted to profile the UDF with a data large enough to eat up all RAM. After trial and error we found that n = 4 M and d  X  32 was a fairly large data set that took almost all main memory (over 3.2 GB). This experiment was conducted with several versions of the UDF, going from an empty UDF to a UDF performing all calculations and returning matrices. The initial query was a straight table scan. The second version was a query calling an empty UDF doing no computations. The next UDF version allocated arrays, but per-formed no computations. The next one passed the input vector as parameter. We then created a UDF that computed all was benchmarked to understand its contribution to overall time. To our surprise, the UDF contributed almost no time when plays a little role. On the other hand, when the UDF works with the cached data set in main memory I/O contributes less than arrays in main memory was the bottleneck. This is surprising because these results indicate important DBMS overhead man-aging the UDF execution in main memory. Returning results had a measurable time contribution, explained by the size of Q and ODBC. 5. Related work
There has been considerable work in data mining to develop efficient techniques, but most work has concentrated on pro-techniques with a DBMS has received little attention. Most research work has concentrated on association rules, followed by clustering and decision trees. [15]. The importance of the linear sum of points and the quadratic sum of points (without are simpler than those for continuous dimensions, accelerating and simplifying data mining algorithms. Even further, suffi-accelerate data mining computations or query evaluation. Some references include [5] to speedup K -means clustering, [4,37]
Association rules are mined in one pass with a sample and refined on a second pass over the entire data set [37]. Biased sampling with skewed distributions is used to accelerate data mining algorithms, which generally require multiple passes over the data set [4]. Samples are collected from multiple nodes in a hierarchy over a data set to quickly answer different aggregate queries [2]. Maintaining a materialized view indexed for efficient random sampling is proposed in [24]. On a com-plementary approach, aggregation queries are efficiently answered with histograms instead of samples in [26], where such histograms are optimally computed with splines. In contrast, in our proposal we directly exploit the DBMS random sampling mechanism without developing a new sampling algorithm (which would require modifying the DBMS source code). Our ap-sufficient statistics on subsets of data sets with skewed distributions in one pass, but cannot exploit existing random sam-pling mechanisms in the DBMS.

There exists work to integrate data mining with SQL and UDFs. Most proposals add syntax to SQL and optimize queries using the proposed extensions. The importance of pushing statistical and data mining computations into a DBMS is recog-
MapReduce can be two complementary mechanisms to analyze large data sets, but favors MapReduce as an external mech-anism to analyze large data sets with data mining functionality instead of extending the DBMS; this work falls short recog-nizing UDFs as a fundamental and feasible extensibility mechanism. Developing data mining algorithms, rather than statistical techniques, using SQL has received moderate attention. Some important approaches include [28,34,36] to mine association rules, and [35] to define primitives for decision trees. SQL syntax is extended to allow spreadsheet-like compu-tations in [41], letting an end-user express complex equations in SQL. Data mining primitive operators are proposed in [9], and deploy data mining models are proposed in [29]; this proposal focuses on managing models rather than computing them and therefore such extensions are complementary to aggregate UDFs to compute sufficient statistics. User-Defined Aggre-gates have been used to develop other data mining algorithms [39,40,30] , but not studying performance aspects on a modern
DBMS. ATLaS [40] extends SQL syntax with object-oriented constructs to define aggregate and table functions (with initial-ize, iterate and terminate clauses), providing a user-friendly interface to the SQL standard. UDFs have been used as a mech-anism to implement primitive vector and matrix operators in mathematical equations used by data mining algorithms [33,30] . We must mention UDFs require different strategies for cost-based optimization [22] since they are different from
SPJ queries. Our UDF automatically benefits from computing SPJ queries to produce the data set. To the best of our knowl-edge, we are the first to consider accelerating computation of sufficient statistics combining UDFs, sampling and caching.
We close this section by discussing some related work on database systems and exploiting new hardware. Exploiting main memory to accelerate data mining is not new [1,16,27] . Data mining algorithms under-utilize a modern CPU because their memory access patterns has low temporal locality [16]; the solution is based on maintaining a separate context per thread. On the other hand, [1] shows that incorporating small processors into memory chips enables fast parallel processing for a graph clustering problem. 6. Conclusions
We focused on efficiently computing sufficient statistics with an aggregate UDF in a DBMS exploiting caching and sam-and linear regression. The UDF enables fast memory-based processing of demanding matrix computations. The DBMS auto-by accelerating UDF processing. We introduced two sampling mechanisms (geometric sampling and bootstrapping) to accelerate the UDF when data sets cannot fit in main memory. Experiments with large real and synthetic data sets show promising results. The UDF is significantly faster when analyzing cached data sets, compared to reading from disk. The
UDF has slightly worse performance than C++ on a direct comparison, with both the UDF and C++ exploiting caching and multi-threading. We consider such performance result encouraging, given the perception that DBMSs are slow for data min-ing processing, given DBMS overhead. Also, users can enjoy the extensive DBMS functionality. Not surprisingly, the UDF is significantly faster than C++ when C++ reads from disk or when it works in a single thread. On the other hand, geometric sampling and bootstrapping are generally faster than performing a table scan on large data sets that cannot fit in main mem-ory. Both techniques achieve high accuracy (around 99%) to approximate the data set statistics (mean vector, covariance ma-to return sufficient statistics in a few seconds. We studied the relative merit of three optimizations: computing a diagonal matrix instead of a full matrix to reduce CPU time, illustrating how the UDF avoids joins in SQL and taking advantage of a
I/O when the data set is not cached. On the other hand, managing matrix arrays in main memory on each thread is the main performance bottleneck. Contrary to intuition, the quadratic number of floating point operations was the second major bot-tleneck. In short, our UDF exploiting caching and sampling can efficiently compute sufficient statistics on large data sets, even if they cannot fit in main memory.

There exist several issues for future work. Other statistical techniques can follow the same approach: finding matrices that summarize large data sets to build a model, efficiently computing such matrices with aggregate UDFs and exploiting
DBMS features and hardware. We want to investigate how the DBMS can control storage of a small array corresponding to a matrix in L1 or L2 cache memory; such optimization is beyond the UDF API. We need to develop accurate cost models for
UDF evaluation considering caching, multi-threaded execution and sampling. It is feasible our sampling techniques can en-able online models computation with increasing accuracy as more samples are collected.
 Acknowledgment This research work was partially supported by US National Science Foundation Grants CCF 0937562 and IIS 0914861.
References
