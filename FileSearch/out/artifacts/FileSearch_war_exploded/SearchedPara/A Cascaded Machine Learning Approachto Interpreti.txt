 } @science.uva.nl In order to fully understand a piece of text, we must understand its temporal structure. The first step toward such an understanding is identifying ex-plicit references to time. We focus on the task of automatically annotating temporal expressions (or timexes ) X  X oth identifying them in text and inter-preting them to determine what times they refer to. Timex annotation is more than normalizing date ex-pressions. First, time consists of more than calen-dar dates and clock times X  X t also includes points of finer and coarser granularity, durations, and sets of times. Second, the expressions that refer to time are not just full date and time expressions X  X hey may be underspecified, ambiguous, and anaphoric.

Building a system for the full timex identifica-tion and interpretation task can be tedious, requiring a great deal of manual effort. The 2004 Temporal Expression Recognition and Normalization (TERN) evaluation 1 evaluated systems on two tasks: timex recognition (identification) alone and recognition and normalization (interpretation) together. All the full-task systems were rule-based systems; the top performing full-task system uses in excess of one thousand hand-crafted rules, which probe words and their contexts in order to both identify timexes and to assemble information necessary to interpret them (Negri and Marseglia, 2004). By contrast, machine learned systems dominated the recognition-only task and even achieved slightly better recognition scores than their rule-based counterparts.

We seek to demonstrate that a timex annotation system that performs both recognition and normal-ization need not be a tangle of rules that serve dou-ble duty for identification and interpretation and that mix up context-dependent and context-independent processing. We propose a novel architecture that clearly separates syntactic, semantic, and prag-matic processing and factors out context-dependent from context-independent processing. Factoring out context-dependent disambiguation into separate classification tasks introduces the opportunity for using machine learning, which supports our main goal: building a portable, trainable timex annota-tion system in which the role of hand-crafted rules is minimized. The system we present here (avail-able from http://ilps.science.uva.nl/ Resources/timextag/ ) achieves the goal of making use of only a small set of hand-crafted, context-independent rules to achieve state-of-the-art normalization performance.

In the following section, we define what a timex is. We give an overview of our system architecture in  X  3 and describe the components in  X  4 X 7.  X  8 pro-vides an evaluation of our system on the full timex annotation task, and we conclude in  X  9. Temporal semantics receives a great deal of attention in the semantics literature (cf. (Mani et al., 2005)), but the focus is generally on verbal semantics (i.e., tense and aspect). In determining what a timex is and how one should be normalized, we simply fol-low the TIDES TIMEX2 standard for timex annota-tion (Ferro et al., 2004). According to this standard, timexes are phrases or words that refer to times, where times may be points or durations, or sets of points or durations. Points are more than just in-stanteous moments in time X  X  point may also be a time with some duration, as long as it spans a single unit of some temporal granularity. Whether a timex refers to a point or a duration is a question of per-spective rather than of ontology. A point-referring timex such as October 18, 2006 refers to an interval of one day as an atom at the granularity of a day. A duration-referring timex such as the whole day may refer to the same temporal interval, but it focuses on the durative nature of this interval.

In addition to specifying which phrases are timexes, the TIMEX2 standard also provides a set of attributes for normalizing these timexes. We fo-cus on the VAL attribute, which takes values that are an extension of the ISO-8601 standard for represent-ing time (ISO, 1997). TIMEX2 VAL attributes can take one of three basic types of values: Points are expressed as a string matching the pat-tern dddd-dd-ddTdd:dd:dd.d+ , where d in-dicates a digit. Such a string is to be interpreted as year -month -date T hour : minute : seconds , and may be truncated from the right, indicating points of coarser granularity. Any place may be filled with a place-holder X , which indicates an unknown or vague value, and there are also a handful of token values (character strings) for seasons and parts of the day which may substitute for months and times. There is also an alternate week-based format dddd-Wdd-d , interpreted as year -W week number -day of the week . Durations are expressed as a string matching the pattern Pd+u or PTd+u , where d+ indicates one or more digits and u indicates a unit token (such as Y for years). A placeholder X may be used instead of a number to indicate vagueness.
 Vague points : past ref , present ref , future ref .
 Figure 1: Timex annotation architecture (letters for ease of reference).

The other attribute which we address in this paper is the boolean-valued SET attribute; a SET timex is one that refers to a recurring time. The remain-ing attributes are MOD, ANCHOR VAL, and AN-CHOR DIR; our system produces values for these attributes, but we do not address them in this paper.
The TIMEX2 annotation standard has been used to create several manually annotated corpora. For the experiments we present in this paper, we use the corpora annotated for the TERN 2004 evalua-tion (Ferro, 2004). These consist of a training set of 511 documents of newswire and broadcast news transcripts, with 5326 TIMEX2s, and a test set of 192 similar documents, with 1828 TIMEX2s. The architecture of our timex annotation system is depicted in Fig. 1. Our system begins with parsed documents as input. Our recognition module is a machine learned classifier (A); it is described in  X  4.
Phrases that have been classified as timexes are then sent to the semantic class classifier (B). Seman-tic class disambiguation is the first point at which context dependence enters into timex interpretation. While some timexes are unambiguous with respect to whether they refer to a point, a duration, or a set, many timexes are semantically ambiguous and can only be disambiguated in context. The machine learned classifier for this task is described in  X  5.
Based on the class assigned by the semantic class classifier, the semantic composition component (C) generates (underspecified) semantic representations using class-specific, context-independent rules. The rules we use are simple pattern-matching rules that map lexical items or sequences of lexical items within a timex to semantic representations. We de-scribe the semantic composition component in  X  6.
For most classes of timexes, the semantic compo-sition component generates a semantic representa-tion that can be directly translated into a normalized value. Timexes that refer to specific points are the only exception. While some point timexes are fully qualified, and thus also directly normalizable, many need to be anchored to another time in context in order to be fully normalized. Thus, context depen-dence again enters the timex interpretation process, and now in two ways. One is obvious: these refer-ential timexes, which need a temporal anchor, have to find it in context. This task requires a reference resolution process (E), which is described in  X  7.1.
The second ambiguity regards the relation be-tween a referential timex and its anchor. Referen-tial timexes, like anaphoric definites, relate to their anchors through a bridging relation, which is deter-mined primarily by the content of the timex X  X .g., two years later refers to a point two years after its anchor. For some referential timexes, though, the direction of the relation (before or after the anchor) is not specified. The machine learned classifier (D) resolves this ambiguity; see  X  7.2.

For referential timexes, final normalization (F) is a straightforward combination of semantic represen-tation, temporal anchor, and direction class.
Not pictured in Fig. 1 is a module that recognizes and normalizes timexes in document metadata using a set of simple regular expressions (REs; 14 in total). This module also determines the document time-stamp for referential timexes by using a few heuris-tics to choose from among multiple timestamps or a date from the document text, if necessary.

While our architecture is novel, we are not the first to modularize timex annotation systems. Even thor-oughly rule-based systems (Negri and Marseglia, 2004; Saquete et al., 2002), separate temporal an-chor tracking from the rest of the normalization pro-cess. The system of Mani and Wilson (2000) goes further in using separate sets of hand-crafted rules for recognition and normalization and in separating out several disambiguation tasks. Ahn et al. (2005b) decouple recognition from normalization X  X ven us-ing machine learning for recognition X  X nd handle several disambiguation tasks separately. In none of these systems, though, are context-independent and context-dependent processing thoroughly separated, as here, and in all these systems, it is the rules that drive the processing X  X n both Mani et al. and Ahn et al. X  X  systems, sets of rules are used to determine which timexes need to be disambiguated. Systems that perform both recognition and nor-malization tend to take a rule-based approach to recognition (Mani and Wilson, 2000; Saquete et al., 2002; Schilder, 2004; Negri and Marseglia, 2004). Recognition-only systems are often based on machine learned classifiers (Hacioglu et al., 2005; Bethard and Martin, 2006), although some do use finite-state methods (Boguraev and Ando, 2005). Ahn et al. (2005a) find a benefit to decoupling recog-nition from normalization, and since our goal is to build a modular, trainable system, we take a machine-learning approach to recognition that is in-dependent of our normalization components.

Generally, machine learned timex recognition systems reduce the task of identifying a timex phrase to one of classifying individual words by us-ing (some variant of) B-I-O tagging, in which each word is tagged as (B)eginning, (I)nside, or (O)utside a timex phrase. Such a tagging scheme is not in-herently sensitive to syntactic constituency and not well-suited to identifying nested timexes (but cf. (Hacioglu et al., 2005)). Considering that syntactic parsers are readily available, we have explored sev-eral ways of leveraging parse information in recog-nition, although we describe here only the method we use for experiments later in this paper.
We treat timex recognition as a binary phrase classification task: syntactic constituents are clas-sified as timexes or non-timexes. We restrict clas-sification to the following phrase types and lexical categories (based on (Ferro et al., 2004,  X  5)): NP, ADVP, ADJP, NN, NNP, JJ, CD, RB, and PP. 2 In order to identify candidate phrases and to extract parse-based features, we parse the TEXT elements of our documents with the Charniak parser (Char-niak, 2000). Because of both parser and annotator errors, only 90.2% of the timexes in the training data align exactly with a parse, which gives an estimated upper-bound on recall using this method.

We use support vector machines for classification, in particular, the LIBSVM linear kernel implemen-tation (Chang and Lin, 2001). The features we ex-tract include character type patterns, lexical features such as weekday name and numeric year, a context window of two words to the left, and several parse-based features: the phrase type, the phrase head and initial word (and POS tag), and the dependency par-ent (and corresponding relation) of the head.
As with all our experiments in this paper, we train on the TERN training corpus and test on the test corpus. Our scores (precision, recall and F-measure for both identification (i.e., overlap) and exact-match) are given in Table 1, along with the scores of the best recognition-only ( BRO ) and full-task ( BFT ) TERN 2004 systems. Since our phrase classification method is only applied within docu-ment TEXT elements, we also present results using both our RE-based document metadata tagger and our phrase classifier for full documents (D OC ). Only these scores can be compared with the TERN scores.
Our scores using this method approach those of the best systems, but there is still a gap, which, as we see in  X  8, affects our overall task performance. Timexes may refer to points, durations, or recur-rences. While some timexes refer unambiguously to one of these, many timexes are ambiguous between two or even three of these (see (Hitzeman, 1993) for a theoretical semantic perspective on this ambigu-ity). Timexes may also refer generically or vaguely, which is another source of ambiguity.

While the TIMEX2 standard does not explicitly specify semantic classes in its annotations, the se-mantic classes we distinguish for our normalization system can be easily inferred from the form of the values of the attributes that are annotated, as follows: Recurrence (recur) : SET attribute set to true Generic or vague duration (gendur) : VAL begins with PX or PTX Duration : VAL begins with P[0-9] or PT[0-9] Generic or vague point (genpoint) : Three possi-bilities: time-of-day w/o associated date expression (VAL begins with T[0-9]); general reference to past, present, or future (VAL is one of the vague tokens); date expression with unspecified high-order position (i.e., millennium position is X) Point : Date expression with specified high-order position (may be precise or not X  X .e., may include X at other positions X  X lso may be of any granularity, from millennium down to hundredths of a second).
Resolving semantic class ambiguities is a context-dependent task that can be easily factored out of se-mantic interpretation, reducing the burden on the se-mantic interpretation rules. The classification task is straightforward: each timex must be classified into one of the five classes described above or into the null class (for timexes that have no VAL). Since the TERN data is not explicitly annotated for semantic class, we use the class definitions above to derive the semantic class of a timex from its VAL attribute.
We again use the LIBSVM linear kernel for clas-sification, with the same features as for recogni-tion. Even though some timexes are unambiguous with respect to semantic class, we train the classi-fier over all timexes, in the expectation that the con-texts of unambiguous timexes will be similar enough to those of ambiguous timexes of the same class to help in classification. We compare the performance of our machine learned classifier to a heuristic base-line classifier that uses the head of the timex and the presence of numbers, names, and certain modifiers within the timex to decide how to classify it.
Table 2 gives the error rates, per class and overall, for the baseline and learned classifiers over phrase-aligned gold-standard timexes. The machine learned classifier halves the error rate of the baseline, mostly as a result of better performance on the duration and point classes. In  X  8, we see how this improvement in classification affects end-to-end performance.
Mani and Wilson (2000) and Ahn et al. (2005b) also perform limited semantic class disambiguation. Both use machine learned classifiers to distinguish specific and generic uses of today , and Ahn et al. also use a machine learned classifier to disambiguate timexes between a point and a duration reading. Their error rate for this task is 27%, but since a set of heuristics is first used to select just ambiguous timexes, this score cannot be compared to ours. The semantic composition module uses context-independent, class-specific rules to compute for each timex an underspecified representation X  X  typed feature structure that depends on the timex X  X  seman-tic class (features include unit and value for dura-tions, year, month, date, and referential class for points; cf. (Dale and Mazur, 2006)). As the rules are not responsible for identification or class or direc-tion disambiguation, they are fewer in number and simpler than in other systems (cf. 1000+ in (Negri and Marseglia, 2004)). Each rule consists of an RE-pattern, which may refer to a small lexicon of names, units, and numeric words, and is applied using a cus-tom transducer. In total, there are 89 rules; Table 3 gives the distribution of rules and an example rule for each class. Tokens in ALLCAPS indicate lexical classes; tokens in MixedCase indicate other rules; and tokens in lowercase indicate lexical items. Some point timexes are fully qualified, while others require a reference time, or temporal anchor, to be fully normalized. 3 There are three ways in which a temporal anchor is chosen for a timex. Some timexes, such as today , three years ago , and next week , are deictic and anchored to the time of speech Table 3: Distribution of semantic composition rules. (for us, the document timestamp). Others, such as two months earlier and the next week , are anaphoric and anchored to a salient time in discourse, just like an anaphoric pronoun or definite. The distinction between deictic and anaphoric timexes is not always clear-cut, since many anaphoric timexes, in the ab-sence of an appropriate antecedent, are anchored de-ictically. A timex may also contain its own anchor: e.g., two days after May 3 , whose anchor is the em-bedded anaphoric timex May 3 .

Once a referential timex X  X  temporal anchor has been determined, the value of the anchor must be combined with the timex, which may be either an offset or a name-like timex. Offsets, such as two months earlier , provide a unit u , a magnitude m , and optionally, a direction (before or after); the value of an offset is the point (of granularity u ) that is m u units from its anchor in the indicated direction. Name-like timexes provide a position in a cycle, such as a day name within a week, and optionally, a direction. The value of a name-like timex is the time point bearing the name within the correspond-ing cycle of its anchor (or the immediately preceding or succeeding cycle, depending on the direction).
For both offsets and name-like timexes, the direc-tion indication is optional. When no direction in-dication is given, the appropriate direction must be determined from context, as in this initial sentence from an article from 1998-11-28: (1) A fundamentalist Muslim lawmaker has vowed The first timex, February , clearly refers to the Febru-ary following its anchor (the timestamp), while the second timex, Saturday , seems to refer to a point preceding its anchor (also the timestamp).

The next two sections describe our methods for temporal anchoring and direction classification. 7.1 Component E: Temporal anchor tracking Since temporal anchors are not annotated in the TIMEX2 standard, our system uses a simple heuris-tic method for temporal anchoring (cf. (Wiebe et al., 1997), who use a more complex rule-based system for timex anchoring in scheduling dialogues). Since we distinguish deictic and anaphoric timexes during semantic composition, we use a combination of two methods: for deictic timexes, the document time-stamp is used, and for (some) anaphoric timexes, the most recent point timex, if it is fine-grained enough, is used as the temporal anchor (otherwise, the docu-ment timestamp is used). Because the documents in our corpora are short news texts, we actually treat anaphoric name-like points as deictic and use the most recent timex only for anaphoric offsets. 7.2 Component D: Direction classification The idea of separating direction classification from the remainder of the normalization task is not new. (Mani and Wilson, 2000) use a heuristic method for this task, while (Ahn et al., 2005b) use a ma-chine learned classifier. In contrast to Ahn et al., who use a set of heuristics to identify ambiguous timexes and train and test only on those, we train our classifier on all point and genpoint timexes and apply it to all point timexes. Genpoint timexes and many point timexes are not ambiguous w.r.t. direc-tion, but we expect that the contexts of unambiguous timexes will be similar enough to those of ambigu-ous timexes of the same class to help classification. Direction class is not annotated as part of the TIMEX2 standard. Given a temporal anchor track-ing method, though, it is possible to derive imperfect direction class information from the VAL attribute. We use our anchor tracking method to associate each point and genpoint timex with an anchor and then compare the VAL of the timex with that of its an-chor to decide what its direction class should be.
We again use the LIBSVM linear kernel for clas-sification. We add two sets of features to those used for recognition and semantic classification. The first is inspired by Mani et al., who rely on the tense of neighboring verbs to decide direction class. Since verb tense alone is inherently deictic, it is not suffi-cient to decide the direction, but we do add both the closest verb (w.r.t. dependency paths) and its POS tag (as well as any verbs directly related to this verb) as features. The second set of features compares day names, month names, and years to the document timestamp. The comparison determines whether, within a single cycle of the appropriate granularity (week for day-names and year for month-names), the point named by the timex would be before, after, or the same as the point referred to by the timestamp.
We compare our learned classifier with a heuristic baseline classifier which first checks for the presence of a year or certain modifiers such as ago or next in the timex; if that fails, it computes the date features described above for each word in the timex and re-turns same if any word compares to the timestamp as same; if that fails, it uses the tense of the nearest verb; and finally, it defaults to same .

Table 4 shows the results of applying our clas-sifiers to all phrase-aligned gold-standard point timexes. BL is the baseline; SVM , SVM VERB , and SVM ALL are the classifiers learned using our basic feature set, the basic feature set plus the verb fea-tures, and all the features, respectively. The learned classifier using all the features reduces the error rate of the baseline classifier by about a third. Note, though, that the learned classifiers without the date comparison features ( SVM and SVM VERB ) perform substantially worse than even the baseline. One rea-son for this becomes clear from Table 5, which gives the error rates for the classifiers restricted to timexes consisting solely of a month or a day name. Unlike points in general, these timexes are all ambiguous with respect to direction and are, in fact, the primary motivation for both Mani et al. and Ahn et al. to con-sider direction classification as a separate task.
These results demonstrate that the date compari-son feature is responsible for a substantial reduction in error rate (over 85% from SVM to SVM ALL ) and that for the same class, performance is perfect. This is largely due to the writing style of the documents, in which the current day is often referred to by name instead of as today , as in example (1).

Although both Mani et al. and Ahn et al. build direction classifiers, neither provide comparable re-sults. Mani et al. do not evaluate their direction heuristics at all, and Ahn et al. train and test their machine learned classifier only on timexes deter-mined to be ambiguous by their heuristics. In any case, their error rate is significantly higher, at 38%. We now consider the performance of the entire sys-tem and the contributions of the components. First, though, we discuss our evaluation metrics. 8.1 Scoring The official TERN scoring script computes precision and recall for VAL only with respect to correctly rec-ognized TIMEX2s with a non-null VAL. While this may be useful in determining how far behind nor-malization is from recognition for a given system, it does not provide an accurate picture of end-to-end system performance, since the recall base does not include all possible timexes and the precision base does not include incorrectly recognized timexes.
The scoring script provides several raw counts that can be used to compute measures that are more indicative of end-to-end performance: actTIMEX2 (# of actually recognized TIMEX2s); corrTIMEX2 (# of correctly recognized TIMEX2s); posVAL (# of correctly recognized TIMEX2s with a non-null gold VAL); corrVAL (# of correctly recog-nized TIMEX2s with a non-null gold VAL for which the system assigns the correct VAL); and spurVAL (# of correctly recognized TIMEX2s with null gold VAL for which the system assigns a VAL). With these counts, we can define corrNOVAL (# of correctly recognized TIMEX2s with a null gold VAL for which the system assigns a null VAL), as corrTIMEX2  X  posVAL  X  spurVAL . We then define end-to-end precision ( absP ) and recall ( absR ) as ( corrVAL + corrNOVAL ) / actTIMEX2 and ( corrVAL + corrNOVAL ) / possTIMEX2 , respec-tively. Official precision and recall for VAL are com-puted as corrVAL / actVAL and corrVAL / possVAL . 8.2 Results Our first set of results (Table 6(Top)), which are restricted to timexes in document TEXT elements, compares our system ( LLL ) to a version of our sys-tem ( BL ) that uses the baseline classifiers for seman-tic and direction class. It also presents a series of or-acle results that demonstrate the effect of swapping in perfect classification for each of the learned clas-sifiers. The oracle runs are labeled with a three-letter code in which the first letter (( P )erfect or ( L )earned) refers to phrase classification; the second, to seman-tic classification; and the third, to direction classi-fication. Note: perfect phrase classification is not the same as perfect recognition, since it excludes timexes that fail to align with parsed phrases.
Using the learned classifiers ( LLL ), which reduce error rates by about one-half for semantic class and one-third for direction class over the baseline clas-sifiers, results in a five-point improvement in abso-lute F-measure over the baseline system ( BL ). We also see from runs LLP , LPL , and LPP that further improvement of these classifiers would substantially improve end-to-end performance. Finally, we see from runs PLL and PPP that recognition performance is a major limiting factor in our end-to-end scores.
In Table 6(Bottom), we present results over full documents, including metadata and text. LLL and PLL are the same as before; ITC-IRST is the sys-tem of (Negri and Marseglia, 2004), which achieved the highest official F-measure in the TERN 2004 evaluation. The results of our system ( LLL ) are comparable to those of ITC-irst: because we recog-nize fewer timexes, our official F-measure is higher (0.899 vs. 0.872) while our absolute F-measure is lower (0.769 vs. 0.806). We see from run PLL that our recognition module is largely to blame X  X ith perfect phrase classification for recognition, our nor-malization modules produce substantially better re-sults. With a system such as ITC-irst X  X , it is not pos-sible to separate recognition performance from nor-malization performance, since there is a single rule base that jointly performs the two tasks X  X ll normal-izable timexes are presumably already recognized. We have described a novel architecture for a timex annotation system that eschews the complex set of hand-crafted rules that is a hallmark of other systems. Instead, we decouple recognition from normalization and factor out context-dependent se-mantic and pragmatic processing from context-independent semantic composition. Our architec-ture allows us to use machine learned classifiers to make context-dependent disambiguation decisions, which in turn allows us to use a small set of sim-ple, context-independent rules for semantic compo-sition. The normalization performance of this sys-tem is competitive with the state of the art and our overall performance is limited primarily by recog-nition performance. Improvement in semantic and direction classification will yield further improve-ments in overall performance. Our other plans for the future include experimenting with dependency relations for semantic composition instead of lexi-cal patterns, evaluating our temporal anchor tracking method, and training the full system on other cor-pora and adapting it for other languages.
 Acknowledgement This research was supported by the Netherlands Organization for Scientific Re-search (NWO) under project numbers 017.001.190, 220-80-001, 264-70-050, 354-20-005, 600.065.120, 612-13-001, 612.000.106, 612.066.302, 612.069.006, 640.001.501, 640.002.501, and by the E.U. IST programme of the 6th FP for RTD under project MultiMATCH contract IST-033104.
