 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval; H.3.5 [ Online Information Ser-vices ]: Web-based services Experimentation, Measurement
This paper addresses the problem of estimating the size of a deep web data source that is accessible by queries only. Since most deep webs are non-cooperative, a data source size can only be estimated by sending queries and analyzing the returning results.

Depending on the returning results, there are two ap-proaches to size estimation. One is based on the docu-ment ids returned, the other is based on both the ids and the downloaded document contents which is computation-ally more expensive.

We propose an efficient estimator based on Capture Re-capture method that relies on the document ids only. First we derive an equation between the overlapping rate and the percentage of the data examined when random samples are obtained from a uniform distribution.

In reality, documents are selected from a power law dis-tribution, where large documents are easier to be captured. Based on the equation derived from uniform distribution, we empirically derived an equation for real data sources.
In this simple model documents are assumed to have the equal probability of being retrieved. It can be formalized as below, which is a variation of the urn model with replace-ment:
Let the data source has n documents, each is labeled with a unique number from 1 to n . Suppose that each time we randomly select k ( k  X  n ) documents with replacement from the data source, and record the document numbers that are drawn. We repeat the process for i times. In this stage total of t ( i ) = k  X  i documents are captured, among them u ( i ) documents are unique. u ( i ) constitutes P ( i ) percent of all the documents in the data source, and the overlapping rate OR ( i ) = t ( i ) /u ( i ) . The task is to estimate n using OR and u .

P ( i ) , the percentage of the data obtained up to i -th it-eration, can be also interpreted as the probability of one particular document that is captured in all i iterations. We start with: Solving the equations (1) and (2) we obtain the following: Which can be approximated by When P &lt; 0 . 5 , Equation 3 can be approximated by Equa-tion 4: The details of the derivation is available at http://cs.uwindsor.ca/  X  jlu/estimate. Although the deriva-tion above assumes that k has a fixed value, we conducted simulation studies and found that Equation 4 holds when k varies in each capture.
In real data source size estimation, documents will have varying probabilities of being captured. Based on Equation 4, we conjecture that there is also a fixed relation between P and OR in real data sources, with a modified equation as below: or By running regression using four collections of newsgroups data, we obtained  X   X  = 1 . 001 ,  X   X  =  X  1 . 132 . Here the R square is 0.875, which means that the regression fits the data well. Hence we derived Equation 5 and the estimator for the size of data source: Figure 1: Reuters. n=806,790, k=1000. Estimate n using three vocabularies.
We test on six data collections, including Reuters (806,790 news stories in English), Reuters33K ( a subset of Reuters corpus that consists of 32,930 files), 20NG ( 20,417 posts in 20 news groups), NG240K (241,932 newsgroup posts down-loaded from the web), Ohsumed (a subset of the Medline database and consists of 56,984 documents), and Amazon Ecommerce web service ( a web service provided by Ama-zon which contains 1,149,968 items for book category).
One estimation process uses 100  X  300 queries, each con-sists of one single word randomly chosen from various vo-cabularies. Each query returns at most k results. If a query overflows, i.e., it matches more than k documents, k number of documents are randomly selected from the returning set.
In order to evaluate the impact of vocabularies to the estimation, we tested on vocabularies collected from three different corpora, i.e., newsgroups, ohsumed, and WSDL (Web Service Description Language) files. Figure 1 show that these different vocabularies produce similar results on Reuters corpus. We tested on other corpora and produced similar results.

Figure 1 also shows that the estimation oscillates in the beginning, but converges to a constant quickly around 100 queries. The number of queries needed to reach a satis-factory estimation depends on the total number document ids retrieved. If each query returns a small number of re-sults, more queries are needed to reach a stable estimation. According to the birthday paradox, documents are needed in order to have collisions. Data sources have various limit for the returning size k . We tested the cases for k=100, 1000, 2000. Other step sizes are also tested and exhibit similar behavior. Figure 2 shows the impact of step size on our estimator. Since each query may return different number of results, we compare three estimations in terms of the percentage of the data that have been probed. When we set larger limits on the number of returns, the estimator will tend to over estimate the size at the beginning. This can be explained that larger step size will give more fresh document ids in the first a few probes. But this difference will diminish when more queries are sent and the large gains will be neutralized gradually. Figure 2: The impact of step size when estimat-ing Reuters corpus using WSDL dictionary. Return limits are set as 100, 1000, and 2000.
Our method demonstrates very small bias and deviation on a variety of data sources, including unranked data sources, which returns the results in a random order, or data sources without return size limit. When data sources rank the re-sults, and the number of results is larger than the limit, we are always sampling the shallow water of the deep web, hence producing negative bias for the estimation. By re-moving the overflowing queries, i.e., the queries that have more matches than the limit, our method also perform well in ranked data sources.

Shokouhi et al also used random queries and document ids to estimate the data source size. Compared with their work, this paper distinguishes biases introduced by two sources. One is introduced by non-uniform sampling of documents selected by random queries. Our method solved this kind of query bias very well for the corpora experimented.
Another bias is introduced by the sorting of the matches combined with limiting of the return results. By ignoring overflowing queries, our estimator works fine for this type of rank bias. It is better than Shokouhi X  X  method when the sample size increases.

When the density of overflowing queries is high, filter-ing out those queries will induce another bias. Hence our method is only applicable to ranked data sources with low overflowing density. From this perspective, our method is suitable for relatively small text databases. To estimate the size of large search engines such as Google, throwing away overflowing queries is no longer an option since most of the random queries will match more pages than the 1000 limit.
Although for the data sources in our experiment the value of  X  in Equation Hyp is -1.1,  X  can take other values between 0 and -2.1 depending on the magnitude of the heterogeneity of the data source. The future work will be the estimation of the value  X  for different types of corpora so that the esti-mation of the corpus size can be more accurate. [1] Broder, A., Fontura, M., Josifovski, V., Kumar, R., [2] M Shokouhi, J Zobel, F Scholer, SMM Tahaghoghi,
