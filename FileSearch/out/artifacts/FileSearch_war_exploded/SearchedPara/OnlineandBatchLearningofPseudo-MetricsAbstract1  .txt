 Shai Shale v-Shwartz SHAIS @ CS . HUJI . AC . IL Computer Science Department, Stanford Uni versity Man y problems in machine learning and statistics require the access to a metric over instances. For example, the per -formance of the nearest neighbor algorithm (Co ver &amp; Hart, 1967), multi-dimensional scaling (Cox &amp; Cox, 1994) and clustering algorithms such as K-means (MacQueen, 1965), all depend critically on whether the metric the y are given truly reflects the underlying relationships between the in-put instances. Several recent papers have focused on the problem of automatically learning a distance function from examples (Xing et al., 2003; Shental et al., 2002). These papers have focused on batch learning algorithms. A batch algorithm for learning a distance function is pro vided with a predefined set of examples. Each example consists of two instances and a binary label indicating whether the two instances are similar or dissimilar . The work of (Xing et al., 2003; Shental et al., 2002) used various techniques that are effecti ve in batch settings, but do not have nat-ural, computationally-ef ficient online versions. Further -more, these algorithms did not come with any theoretical error guarantees. In this paper , we discuss, analyze, and experiment with an online algorithm for learning pseudo-metrics. As in a batch setting, we recei ve pairs of instances which may be similar or dissimilar . But in contrast to batch learning, in the online setting we need to extend a prediction on each pair as it is recei ved. After predicting whether the current pair of instances is similar , we recei ve the correct feedback on the instances X  similarity or dissim-ilarity . Informally , the goal of the online algorithm is to minimize the number of prediction errors. Online learning algorithms enjo y several practical and theoretical adv an-tages: The y are often simple to implement; the y are typ-ically both memory and run-time efficient; the y often come with formal guarantees in the form of worst case bounds on their performance; there exist several methods for con-verting from online to batch learning, which come with for -mal guarantees on the batch algorithm obtained through the con version. Moreo ver, there are applications such as text filtering in which the set of examples is indeed not given all at once, but instead revealed in a sequential manner while predictions are requested on-the-fly .
 The online algorithm we suggest incrementally learns a pseudo-metric and a threshold. As in (Xing et al., 2003), the pseudo-metrics we use are quadratic forms parametrized by positi ve semi-definite (PSD) matrices. At each time step, we get a pair of instances and calculate the distance between them according to our current pseudo-metric. We decide that the instances are similar if this dis-tance is less than the current threshold and otherwise we say that the instances are dissimilar . After extending our prediction, we get the true similarity label of the pair of in-stances and update our pseudo-metric and threshold. Our update rule is based on the projection operation. Intuiti vely , we look for a new pseudo-metric and threshold that on the one hand will predict correctly the last example we have just recei ved and on the other hand will be as close as pos-sible to the pre vious pseudo-metric and threshold. The idea of using the projection operation for online algorithms was first introduced by (Herbster , 2001), and was further de-veloped by (Crammer et al., 2003). The resulting update rule enjo ys some nice properties. First, the PSD matrix we learn is a linear combination of rank-one matrices defined by vectors in the span of the instances. This allo ws us to de-velop a dual version of the algorithm that emplo ys kernels. Further , we sho w that all the PSD matrices obtained by the online algorithm are norm bounded. We use this property to pro ve an online error bound, and to design a lar ge-mar gin batch algorithm based on the online algorithm.
 This paper is organized as follo ws. Sec. 2 formally intro-duces the problem of online learning of pseudo-metrics and sets the notation used throughout the paper . In Sec. 3, we describe our pseudo-metric learning algorithm for the sep-arable case and sho w that the resulting online algorithm can be implicitly implemented using kernel operators. In Sec. 4, we deri ve a worst-case loss bound for the online algorithm. A modification of the online algorithm to the inseparable case and a corresponding loss bound is briefly discussed in Sec. 5. In Sec. 6, we describe a simple online to batch learning con version, and discuss the generaliza-tion properties of resulting batch algorithm. Experimental results are pro vided in Sec. 7. The experiments apply our algorithm to the tasks of digit recognition and online doc-ument filtering. We compare the performance of our algo-rithm to both other batch similarity learning algorithms and online algorithms for classification. Let X denote our feature space. For concreteness we as-sume that X = R n . Our goal is to learn a pseudo-metric over X . A pseudo-metric is a function d : X X ! R , which needs to satisfy three requirements, (i) d ( x ; x 0 ) 0 (ii) d ( x ; x 0 ) = d ( x 0 ; x ) , and (iii) d ( x 1 ; x 2 d ( x 1 ; x 3 ) . While the instances may belong to a well de-fined partition of X into classes, we do not recei ve direct supervision in the form of class labels. Instead, we get sim-ilarity and dissimilarity feedback. Therefore, we assume that we recei ve examples of the form z = ( x ; x 0 ; y ) 2 ( X X f +1 ; 1 g ) . Each example is composed of an instance pair ( x ; x 0 ) and a label y which equals +1 if x 0 are considered similar and 1 otherwise. As in (Xing et al., 2003), we restrict ourselv es to pseudo-metrics of the form where A 0 is a symmetric positi ve semi-definite matrix. It is easy to verify that if A 0 then d metric. Furthermore, there exists a matrix W such that Therefore, d image of x and x 0 due to a linear transformation W . The mar gin of a sample S , denoted , is defined to be the minimum separation between all pairs of similar and dis-similar examples. Let ( x 1 ; x 0 such a pair . Then, the mar gin requirement translates to Note that we can scale A and by any positi ve constant factor without essentially modifying the properties of the solution (as in the case of man y classification problems). We therefore set to be 2 and later on look for a matrix A which has a small norm. If we get a sample S of m tuples of of the form described by Eq. (1). Thus, we introduce a threshold b 2 R and replace the abo ve constraints with the follo wing set of constraints, which can be written as a single linear constraint as follo ws, Given a set of examples we can now define a constrained optimization problem to find A . Note that in addition to the constraint defined in Eq. (2), we also need to impose the constraint that A must be positi ve semi-definite (PSD). Solving this constrained optimization problem can be per -formed by standard methods, such as interior -point algo-rithms for solving semi-definite programs. In this paper we focus instead on a simple and efficient online approach, and later use well-studied techniques for con verting from online to batch learning algorithms. We thus obtain the best of both worlds: a loss bound for an efficient online al-gorithm, and a generalization bound for the resulting batch algorithm.
 In the online setting we observ e tuples ( x ; x 0 ; y ) in a se-quential manner . On time step we first observ e ( x ; x 0 ) and calculate d greater than the threshold b we predict that the pair is dis-similar . Otherwise, we say that the pair is similar . After ex-tending the prediction, we recei ve the true label y and may suf fer a loss if there is a discrepanc y between our prediction and y . The loss we discuss in this paper is an adaptation of the hinge loss, ` ( A; b ) : = max 0 ; y ( d A ( x ; x 0 )) 2 b + 1 : Thus, if we satisfy the inequality in Eq. (2) we suf fer no loss. Otherwise, we pay a cost that gro ws linearly with the amount the inequality is violated. The goal of the online algorithm is to minimize the cumulative loss it suf fers. As in other online algorithms the matrix A and the threshold b are updated after recei ving the feedback y . Therefore, we denote by ( A ; b ) the matrix-threshold pair used for prediction on round . We now present our first algorithm, which assumes that there exists a matrix A ? 0 and a scalar b ? 1 that perfectly separates the data. Namely , we assume that ` ( A ? ; b ? ) = 0 for all . A modification of the algorithm for the inseparable case is given in Sec. 5.
 The general method we use for deri ving our on-line up-date rule is based on the orthogonal projection operation. Formally , given a vector x 2 R k and a closed con vex set C R k , the orthogonal projection of x onto C is defined by In words, P For simplicity of presentation, we refer to ( A; b ) both as a matrix-scalar pair and as a vector in R n 2 +1 where the first n 2 elements of the vector are the elements of A (listed column-wise) and the last entry of the vector is b . For each time step , we define the set C R n 2 +1 as Thus, C is the set of all matrix-threshold pairs which attain zero loss on the example ( x ; x 0 ; y ) . Recall that a neces-sary condition imposed on a matrix A used as a pseudo-metric is that A 0 . In addition, the threshold must be at least 1 (otherwise the loss on any similar points will be non-zero). Thus, we denote by C matrix-threshold pairs, Equipped with the abo ve definitions, we now describe the update step of the online algorithm. The update is com-prised of two projections. First we project the current matrix-threshold pair ( A ; b ) onto C . Let ( A P
C ( A ; b ) words, we attempt to keep ( A possible, while forcing ( A the most recent example. We then define the new matrix-threshold pair ( A onto the set C sible for deciding whether two instances x ; x 0 are similar or dissimilar . In summary , the update rule of our online algorithm is composed of two successi ve projections, In the follo wing, we sho w how to efficiently perform these projections. 3.1. Pr ojecting onto C Recall that we refer to ( A; b ) both as a matrix-scalar pair and as a vector in R n 2 +1 . For the simplicity of represen-tation, we denote by w 2 R n 2 +1 the vector representation of ( A; b ) . Analogously , w ; w ^ ; w +1 denote the vectors corresponding to ( A ; b ) ; ( A dition, let 2 R n 2 +1 be the vector corresponding to the matrix-scalar pair ( y v v t ; y ) , where v = x x 0 . Using the abo ve terminology along with simple algebraic manipulations, we can rewrite the definition of C from Eq. (3) as C = f w 2 R n 2 +1 : w 1 g . It is easy to verify that the projection of w onto C is given by P
C ( w ) = w + otherwise = (1 w ) = k k 2 We now use the fact that w and are the vec-tors corresponding to the matrix-scalar pairs ( A ; b ) and ( y v v t ; y ) . Therefore, we get that, and the update becomes 3.2. Pr ojecting onto C We now describe an efficient method for projecting ( A P set of all PSD matrices and b onto the set f b 2 R : b 1 g . The projection of b abo ve set is max f 1 ; b A We start with the case y = 1 . In this case A the projection of A A semi-definite. Since A as A ^ = P n i =1 i u i u t i , where i is the i  X  X h eigen value of A of generality , we assume that the eigen vectors f u 1 ; : : : ; u n g form an orthonormal basis of R n . The matrix A +1 is the projection of A ^ onto the positi ve semi-definite cone. Given the set of eigen vectors and eigen values of A be written as, (See for instance Golub &amp; Van Loan, 1989.) In addition, from the (eigen value) Interlacing Theorem we have that A son, 1965, pp. 94-97 and Golub and Van Loan, 1989, page 412). Therefore, we get that A zos method (see, e.g., Golub and Van Loan, 1989). We name the resulting algorithm POLA as an abbre viation for Pseudo-metric Online Learning Algorithm. The pseudo-code of POLA is given in Fig. 1. 3.3. Kernel-based Implementation The pseudo-metrics we have used so far tak e the form ( d where W = p A exists since A 0 . Therefore, d is the Euclidean distance between the image of x and x 0 due to a linear transformation W . In real-w orld applica-tions, similarity and dissimilarity constraints over instances might not be satisfied by such simple distance functions. A common preprocessing strate gy is to use a non-linear map-ping function : X ! F that maps the data into some high dimensional feature space F and then learn in F (Vapnik, 1998). Since F is high-dimensional, we need an efficient way to access the data in F . In this section we present a dual version of the algorithm in Fig. 1, where interf ace to the data is limited to inner products. Thus, if we have a kernel function K : X X ! R that efficiently computes the inner products in F , K ( x ; x 0 ) = ( x ) ( x 0 ) , we can efficiently learn a pseudo-metric over F .
 To deri ve a dual version for the online algorithm, we first sho w that for any time , the matrix A can be written as where m 2 and all the vectors r i are in the span of the vectors f v 1 = ( ( x 1 ) ( x 0 ( x 0 )) g , namely , The abo ve representation of A enables us to efficiently calculate the distance between a new pair of instances using the kernel function, because using Eq. (5), we have: ( d In addition, we have that v j v +1 = K ( x j ; x +1 ) We now use an inducti ve argument to sho w that A can in-deed be written as in Eq. (5). The initial matrix A zero matrix and clearly fits the form of Eq. (5). Assume that A is of the form in Eq. (5). The first step of the online update rule is to define A can also be written as in Eq. (5). If the resulting matrix is positi ve semi-definite we do not have to do anything. If it does have a (single) negative eigen value, we find ( ; the minimal eigen value of A vector . We then set A sho w that u is also in the span of f v 1 ; : : : ; v g . Since an eigen vector of A inducti ve assumption, we rewrite A that, ( P m Therefore, u is in the subspace spanned by f r 1 ; : : : ; cludes the deri vation.
 In the rest of this section we explain how to find, via inner -products, the minimal eigen value, , and its corresponding eigen vector , u , of the matrix A matrix whose columns form an orthonormal basis for the subspace spanned by f v 1 ; : : : ; v m g and let q i denote the i  X  X h column of Q . From Eq. (6) we get that each eigen vec-tor u of A columns of Q and thus there exists a vector 2 R d such that u = Q . Since u is an eigen vector of A Multiplying both sides by Q t we get that Q t A The reverse direction is also correct. Namely , if is an eigen vector of Q t A the same eigen value. We have thus sho wn that u is an eigen vector of A is an eigen vector of Q t A The matrix Q t AQ can be computed using inner -products since ing the kernel Gram-Schmidt procedure. In summary , we have sho wn that all the steps of the online algorithm in Fig. 1 can be implemented via a kernel function. The follo wing theorem pro vides a loss bound for the algo-rithm in Fig. 1. After pro ving the theorem we discuss a few of its implications.
 Theor em 1 Let ( x 1 ; x 0 quence of examples and let R be an upper bound suc h that 8 : R k x x 0 k 4 2 + 1 . Assume that ther e exist A ? 0 and b ? 1 for whic h 8 1 , ` ( A ? ; b ? ) = 0 . Then the following bound holds for any T 1
X The proof of the theorem is based on the follo wing lemma Lemma 2 Let w 2 R n be any vector and let C R n be a closed con vex set. Then for any w ? 2 C we have k w w ? k 2 2 k P C ( w ) w ? k 2 2 k w P C ( w ) k 2 2 : (8) For a proof see for instance (Censor &amp; Zenios, 1997), Thm. 2.4.1.
 Pr oof of Theor em 1: For simplicity , we use the definitions of w ; w ; w +1 ; and from Sec. 3.1. We also denote by w ? 2 R n 2 +1 the vector corresponding to the matrix-scalar pair ( A ? ; b ? ) Define = k w w ? k 2 the theorem by bounding P T First note that P T This pro vides an upper bound on P . In the follo wing we pro ve the lower bound ( ` ( A ; b )) 2 =R . We can subtract and add the term k w ^ w ? k 2 Recall that w ^ is the projection of w onto C and that w +1 is the projection of w ^ onto C a . By assumption, w ? 2 C a and w ? 2 C . Therefore, we get from Lemma 2 that We now use the fact that w ^ = w + where = ` ( A ; b ) = k k 2 2 to get that where the last inequality is due to the fact that k k 2 k x x 0 k 4 2 + 1 R . Combining Eq. (11) with Eq. (10) we get ( ` ( A ; b )) 2 =R . Comparing the abo ve lower bound with the upper bound in Eq. (9) we get P bound in Eq. (7) since k w ? w 1 k 2 Note that the loss bound of Thm. 1 does not depend on the dimension of the instance space. Therefore, the bound does not change if we emplo y kernels which map the instances to high dimensional spaces. The sole dif ference in the bound when using kernels is that the norm of A ? and the norm of the instances are assumed to be small in the mapped space. Note also that we mak e a similarity prediction mistak e iff y b ( d A ( x ; x 0 )) 2 0 . Thus, if on round the predicted similarity is incorrect, then ( ` ( A ; b )) 2 1 Therefore, the number of prediction mistak es cannot ex-ceed R k A ? k 2 note that while Thm. 1 pro vides a loss bound on the sum of squar es of hinge losses, it is possible to deri ve a bound which is a mere sum of losses. The proof howe ver is more complicated, and is omitted due to lack of space. So far, we have assumed that there exists a pseudo-metric that perfectly matches the similarity and dissimilarity re-lations between instances. In this section, we relax this assumption and describe a modification for the algorithm in Fig. 1 for the inseparable case. Since there is no per -fect pseudo-metric that explains the data even from hind-sight, we do not expect our online algorithm to attain a fix ed amount of loss. Instead, we measure the loss of the online algorithm relative to the loss of any other fix ed pseudo-metric parametrized by ( A ? ; b ? ) . The algorithm emplo ys a relaxation parameter , denoted by &gt; 0 . The only modifi-cation to the algorithm in Fig. 1 is to define It is possible to deri ve a loss bound for POLA with the abo ve modification relative to the loss of any other fix ed pseudo-metric using the same techniques as in (Crammer et al., 2003). The full details are omitted due to the lack of space. We have focused thus far on online algorithms. Ho we ver, in man y machine learning tasks the entire training data is given to the learning algorithm in adv ance. Such settings are typically referred to as batch learning. In batch learn-ing the goal is to find an hypothesis which exhibits small empirical error or loss on the training data and generalizes well by obtaining similar low loss on unseen examples. In this section we build upon the fact that we have devised an online learning algorithm with a loss bound on its per -formance. Specifically , we use POLA as a building block to devise a batch procedure which returns a pseudo-metric that is guaranteed to generalize well.
 There are various techniques to con vert from online to batch learning which come with some formal guarantees, quite a few can be used in our setting. We present here one of the simplest con version techniques. The con version procedure uses a con vergence parameter , denoted . It in-vokes POLA multiple times so long as there exists an ex-ample in the training set whose hinge loss exceeds . If no such example exists the procedure stops and returns the final matrix obtained by POLA. The loss bound of Sec. 3 guarantees that at most d R k A ? k 2 vocations of POLA will be required. By construction, the loss of the final pseudo-metric on any example from the training set is at most . Furthermore, combining the in-equality of Eq. (9) with the fact that is non-ne gative we obtain Lemma 3 belo w (see also Crammer et al., 2003). This lemma assures that the norm of the result is bounded. Lemma 3 Under the same conditions of Thm. 1, the fol-lowing bound holds for any 1 Combining the bound on the norm with the fact that its em-pirical loss on the training set is small implies that the re-sulting pseudo-metric has good generalization properties. That is, assuming that the training set and the test set are i.i.d samples from the same source then with high probabil-ity the loss on the test set is also small. The formal deri va-tion uses standard learning theoretic tools and is omitted due to the lack of space. In this section we present experimental results with syn-thetic and natural data that demonstrate dif ferent merits of POLA. In the first experiment we created two synthetic im-ages of the digits  X 5 X  and  X 2 X . Each image is composed of 12 12 pix els. We then created 64 noisy versions of the two original images by adding biased noise as follo ws. We defined two noise patterns: the first pattern was gener -ated by adding a zero mean Gaussian noise to all the odd columns of the digit image; the second pattern was gener -ated in a similar manner by adding noise to the odd rows. The variance of the noise was set such that the signal to noise ratio is 1 ( 0 dB SNR). The noisy images are depicted on the top left part of Fig. 2. The noise degraded the orig-inal images up to the point where it is almost impossible to recognize whether the original digit is  X 2 X  or  X 5 X . The plot on the bottom left of Fig. 2 is a color coded represen-tation of the distance between each pair of original noisy images. It is also clear that the distances are rather random and do not reflect the identity of the underlying prototypes. We next performed principal component analysis (PCA) on the images and reconstructed the images using the lar gest eigen vector . The corresponding reconstructed images are sho wn on the middle of the top row of Fig. 2. The distances between each two reconstructed images are given on the middle of the bottom row. While some of the images be-come more intelligible, it is still not possible in most cases to reveal whether an image represents the digit  X 2 X  or  X 5 X . Similarly , the corresponding distances do not clearly sho w the underlying two-class structure. Last, we applied POLA to all pairs of noisy images, and reconstructed each image using the lar gest eigen vector of the learned matrix A . The reconstructed images and the distances are depicted on the right hand side of Fig. 2. Most, if not all, of the recon-structed images look intelligible and we can easily reveal the original digit prototype for each image. Indeed, the dis-tances matrix depicted on the right exhibits a clear block structure corresponding to the partition of the data into two classes. This experiment demonstrates the power of super -vised learning of pseudo-metrics for extracting rele vant in-formation.
 Our next set of experiments compares the performance of the k Nearest Neighbor (kNN) classifier with dif ferent (pseudo) metrics on the MNIST dataset. MNIST contains images of the 10 digits each of which is represented by 28 28 pix els. We randomly pick ed 10 ; 000 examples from the training set and used all the 10 ; 000 examples of the test set. Ne xt, 10 lems were generated by comparing all pairs of digits. In the first experiment we compared the performance of kNN using the Euclidean distance to its performance when using a pseudo-metric obtained by running POLA on the training set. To train POLA we randomly chose 1 ; 000 pairs of in-stances and used the last hypothesis generated by POLA for evaluation on the test set (see Sec. 6). A comparison of the error on all 45 binary classification problems is given on the top left scatter plot of Fig. 3. Each point in the plot corresponds to a binary classification problem. The x -axis designates the error of kNN with Euclidean distance while the y -axis is the error of kNN using POLA  X  X  pseudo-metric. It is clear that using the learned pseudo-metric greatly re-duces the error rate. In fact, the error when using POLA as a pre-processing step is lower than the vanilla kNN in all of the 45 binary problems. Ne xt, we compared the RCA algo-rithm for learning distances (Shental et al., 2002) to POLA. RCA follo ws the same learning setting as POLA in a batch mode. We compared the performance of kNN using a dis-tance function learned by RCA to its performance using a pseudo-metric learned by POLA. RCA uses PCA as a pre-processing step in order to reduce dimensionality . We thus applied PCA independently to each binary problem and re-duced the dimension of each 28 2 image to a 40 dimensional vector . This value of the dimension was chosen by exper -imentation on the test set. The results, comparing POLA with RCA, are given on the middle right plot of Fig. 3. POLA outperforms RCA on all but one of the 45 binary problems. We also applied RCA without the dimensional-ity reduction step. The results are given on the middle left plot of Fig. 3. Here, the results of RCA are much worse and POLA outperforms RCA on all of the 45 binary prob-lems. The fact that POLA does not require dimensionality reduction is in accordance with our formal analysis. In-deed, the loss bound of Thm. 1 depends on the Frobenius norm of A ? and does not depend on the actual dimension of the instances.
 We also compared POLA to Fisher Discriminant Analy-sis (FD A) (Duda et al., 2001). FD A can be vie wed as a dimensionality reduction method in the presence of super -vision. The simplest form of FD A for binary classification problems projects the instances onto a single dimension. Thus, to mak e a fair comparison with FD A, we projected the data of each binary problem onto the lar gest eigen vec-tor of the matrix found by POLA. We then compared the performance of kNN using the projected data obtained by POLA and FD A. The results are depicted on top right part of Fig. 3. Here again kNN with POLA clearly outperforms kNN with FD A on all of the problems.
 In the final experiment with the MNIST dataset we ran-domly selected 100 images corresponding to the digits  X 0 X  and  X 8 X . We then projected each image onto the two lar gest eigen vectors obtained by PCA and the two lar gest eigen-vectors of the matrix learned by POLA. The two projec-tions are sho wn in the bottom row of Fig. 3 The projected points using the eigen vectors obtained by POLA generated two perfectly separable clusters each of which corresponds to a dif ferent digit. In contrast the analogous clusters when using PCA are interlea ved. This demonstrates the potential power of POLA for dimensionality reduction.
 We also compared POLA with other online algorithms on the problem of document filtering. We used the Reuters-21578 dataset. This dataset contains about 10 ; 000 docu-ments. Each document in the corpus is labeled by zero or more topics from a predefined set. We represented the doc-uments using the standard vector -space model with length-normalized tfidf after selecting 500 words (Singhal et al., 1996). Of the entire set of topics we chose 13 topics to use in our experiments. The topics were chosen such that the number of rele vant documents is much smaller than the number of irrele vant documents for the topic. (The ratio between the number of rele vant and irrele vant documents was in the range [0 : 1 ; 0 : 01] ). We then evaluated POLA and various online algorithms on the task of online docu-ment filtering as follo ws. On each time step, we calculated the distance between a new instance to all of the rele vant documents observ ed thus far. If the distance to the closest rele vant document was less than the current threshold we predicted that the document is rele vant. Otherwise, we pre-dicted that it is irrele vant. To evaluate the performance of the algorithms we calculated both the average number of false positi ves (rele vant) and the average number of false negatives (irrele vant). We then took the average of these two errors. The end results is an equalized error estimate that does not depend on the density of rele vant document. We compared the results of POLA to the results obtained by the Perceptron algorithm (Rosenblatt, 1958), the PAUM algorithm (Li et al., 2002) (a variant of the Perceptron), and a simple 1NN classifier that uses on each round the documents observ ed so far. The results are given in Fig. 4. As can be seen from the scatter plot in the right, POLA clearly outperforms the simple 1NN algorithm. Ho we ver, the performance of PAUM and the standard Perceptron al-gorithm are often comparable to POLA. Since the PAUM algorithm depends on parameters that drastically effect its performance, it is possible that finer tuning of these param-eters will impro ve its performance.
 Ackno wledgments Thanks to G. Elidan, M. Fink, E. Egozi,
