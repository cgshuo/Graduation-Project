
Applications built on top of search engines such as question an-swering and data mining are gradually leveraging rich language resources such as microformats (e.g. X  X eo X  and  X  X Atom X ) and RDF/RDFa markup which can be integrated with content-based resources [14]. Using these resources effectively requires com-plex structural representations of information needs [2]. Addition-ally, there may be retrieval environments where users are expert searchers willing to use structured query languages. Librarians, application developers, patent searchers and users of legal infor-mation systems often form complex queries.

Both search applications and expert searchers rely on informa-tion retrieval (IR) approaches that can support constraint-checking and ranking with respect to document structure and annotations as well as keywords. Such approaches enable direct expression of higher-level constraints on answer structures which can be checked at retrieval time. One approach is to use SQL, which is still the most widely used standard for formulating structured and semantic-expressive queries, and extend it with similarity predicates to per-form the ranking, e.g. [3]. However, most of these approaches use physical SQL math and aggregation operators (e.g. log, sum) to express the ranking function. In other words, they specify the in-formation need and the ranking function in the same query. From an abstraction point of view, this mix is physical , since bare SQL is used to implement the ranking. Other approaches, such as [4] implement the ranking function directly on top of a physical doc-ument representation. Such engineering approaches can also be problematic because they result in implementations that are diffi-cult to maintain and re-use [9, 6, 10].

A general-purpose framework for ranking structured queries can address these issues by providing transparent and effective ranking. We propose such a framework that interprets any SQL query and then assigns a probabilistic retrieval model to rank the results of that query. The interpretation is part of a processing strategy that fo-cuses on satisfying the information need while concurrently adding the model as part of the processing. To implement these models we use Probabilistic SQL (PSQL, [20]), which helps to express them in an abstract and logical way. The abstraction takes control of tu-ple (the results from the structured query) probability estimation and aggregation, which replaces the physical approaches that use SQL aggregation operators. The resulting implementation is easy to understand, debug and customise.
To achieve the ranking-based processing of SQL, retrieval mod-els (due to space constraints, we mainly focus on the TF-IDF and language modelling) which are designed primarily for document retrieval are generalised for tuple-based retrieval. The main contri-bution of this paper are the SQL-to-PSQL algorithms which trans-form the SQL queries to PSQL programs. These programs deliver an IR-model-based ranking of tuples retrieved for the genuine SQL query and are evaluated using several benchmarks.

The remainder of the paper is organised as follows: Section 2 provides the essential background on probabilistic DB technology, retrieval models and PSQL. Sections 3 and 4 cover the core contri-butions of this paper: the generalisation from document retrieval to tuple retrieval, and the evaluation. Section 5 summarises the paper and concludes the discussion.
The research in this paper draws upon advances in probabilis-tic data models. Early work that discusses the notion of quality and its estimation in databases using a probabilistic approach can be found in [15]; the relational model was extended and a quality specification was associated with each relation instance. [13] intro-duced RankSQL, which integrates ranking support into SQL. An-other significant approach is the framework for probabilistic rank-ing of tuples based on a variant of the BIR retrieval model [4]. This framework, which measures the correlation between the specified and unspecified attributes to produce the ranking, differs from the framework proposed here. We support any probabilistic retrieval model and its variations, and the probability estimation need not be modelled outside the probabilistic framework. Efficient query evaluation in probabilistic databases is discussed by [7], which pro-poses a system that efficiently ranks the top-k answers to an SQL query on a probabilistic database. This system provides an opti-misation technique referred to as  X  X afe-plan X  that facilitates the ef-ficient computation of probabilistic results for queries. This tech-nique also ensures that the correctness of the probabilistic seman-tics is maintained. Another approach for improving query process-ing times, proposed by [17], is based on materialising probabilistic views in probabilistic databases. Other approaches concerned with the efficient ranking of tuples, but geared towards specific types of data such as semi-structured, have been initiated. [21] compute ap-proximate top-k results using probabilistic score estimators. As a result, retrieval efficiency improves without a major loss in effec-tiveness.

The probabilistic relational modelling concepts employed herein build upon the aforementioned research and the theoretical foun-dations by [8], [7] and [17]. The contribution of this paper is to leverage such concepts for the probabilistic ranking of tuples.
This section reviews the core retrieval models used in this pa-per. To define this models, let N D ( c ) denote the number of Doc-uments in collection  X  c ", and let n D ( t,c ) denote the number of Documents with term  X  t " in collection  X  c ", where df t is the document frequency . Similarly, let N L ( c ) denote the number of Locations in collection  X  c ", and let n L ( t,c ) denote the number of Locations with term  X  t ". N L ( d ) and n L ( t,d ) are the Location-based counts for document  X  d ", where tf d := n L ( t,d ) is the within-document term-frequency . The dual notation helps to achieve the unifying definitions of TF-IDF and LM shown next.
 D EFINITION 1. TF-IDF RSV:
IDF( t,c ) :=
The TF-IDF term weight is a combination of TF and IDF values. The RSV is the sum over the TF-IDF weights.

The definition captures traditional TF-IDF and some of the prob-abilistic and information-theoretic interpretations proposed in [5, 18]. TF( t,d ) is the within-document term frequency component. TF( t,d ) can be set to one of the following: the BM25-motivated quantification tf d / ( tf d + K d ) , K d is a normalisation factor reflect-ing the document length and is usually proportional to the piv-oted document length ( pivdl := dl / avgdl ); and the maximum-likelihood estimate P L ( t | d ) := n L ( t,d ) /N L ( d ) , which estimates the within-document term probability ( dl := N L ( d ) is the docu-ment length, i.e. the number of locations in document d ).
IDF( t,c ) is the inverse document frequency component. This can be the negative logarithm of the term probability P D n
D ( t,c ) /N D ( c ) . IDF( t,c ) can also be normalised, for example, idf ( t,c ) / maxidf. This corresponds to the logarithm to base N since the maximum idf value is maxidf :=  X  log 1 /N D ( c ) [20].
The setting of the TF( t,d ) to the BM25-motivated quantifica-tion and the IDF( t,c ) to the normalised (probabilistic) one are the settings used for the experiments in Section 4.

D EFINITION 2. LM RSV: Language modelling (LM) [16] con-sists of two term probabilities: the within-document term probabil-ity P ( t | d ) (foreground model) and the collection-wide term proba-bility P ( t | c ) (background model).

The LM term weight is defined as follows:
The RSV is derived from the conditional probability P ( q | d ) , which is based on P ( q | d ) / ( P ( q )  X  Q t  X  q (1  X   X  ) query-based but document-independent normalisation of the query probability P ( q | d ) .The 0 &lt;  X  &lt; 1 is the mixture parameter. The query probabilities are decomposed as follows: P ( q | d,c ) = Q t  X  q P ( t | d,c ) P ( t | d,c ) is the mixed term probability.
The syntax of PSQL is like SQL apart from a few additional syntactic elements. For example, in the PSQL SELECT statement, one can specify an aggregation assumption in front of the target list, an estimation assumption per source, and an estimation assumption for the SELECT statement. One feature of PSQL is, hence, the esti-mation assumption, which locates probability estimation within the probabilistic relational paradigm. This differs from probabilistic relational algebra [8] and approaches such as [1] where probability estimation is modelled outside the relational paradigm, a feature perceived as disadvantageous by both developers and designers.
Below we illustrate the most commonly used syntactic elements particular to PSQL and their definitions. Their theoretical under-pinnings are described in [20], and to improve readability, a sum-marised version of the syntax of PSQL is illustrated in Figure 1. D EFINITION 3. Aggregation assumption: DISJOINT, INDE-PENDENT and SUBSUMED are the main aggregation assump-tions. For DISJOINT, the aggregation corresponds to the sum of probabilities. For INDEPENDENT, the aggregation is based on the usual set overlap: P ( A  X  B ) = P ( A ) + P ( B )  X  P ( A )  X  P ( B ) . For SUBSUMED, the aggregation yields the maximal probability.
D EFINITION 4. Evidence key: An evidence key is a set of at-tributes. The notion  X  X vidence X  is borrowed from the conditional probability P ( h | e ) where h is the hypothesis and e is the evidence. In the relational world, P ( Term | Doc ) is a probability that can be defined for a relation with attributes  X  X erm" and  X  X oc". The set {Doc} is the evidence key. Evidence keys can be specified for the targets (SELECT targets) or for the sources (FROM source1, source2, ...) of an SQL query.

D EFINITION 5. Estimation assumption: The estimation as-sumption is related to the evidence key and describes the way P ( h | e ) is estimated. The main assumptions are: Dis-joint, Independent, Subsumed and the so-called complex assump-tions (e.g. VALUE_FREQ (VF), INVERSE_VALUE_FREQ (IVF), MAX_IVF, and similar for TUPLE_FREQ (TF)).

One important feature of the estimation is whether the assump-tion is based on value frequency , or tuple frequency . Value fre-quency corresponds to the number of distinct relations in which a term occurs. For document-based retrieval (and IR in general), the value in a (Key, Value) pair corresponds to a document , i.e. (Term, Doc). Value-frequency-based estimation, for instance, is funda-mental for estimating the IDF. An example of an assumption that facilitates such an estimation is the MAX_IVF assumption, which computes a probability according to the maximal inverse value fre-quency. For IR, this is the maximal IDF, i.e. MAX_IDF.

Tuple frequency, on the other hand, corresponds to the number of tuples (locations) in which a term occurs. For example, tuple-based probability corresponds to the so-called within-document term fre-quency (TF) of a term. This probability can be estimated using the TUPLE_FREQ (TF) assumption.
In [20] the application of PSQL to document retrieval was demonstrated. This lays the groundwork for this section where the generalisation from document retrieval to tuple retrieval is pre-sented. For the demonstration, assume a term-based representa-tion of queries and documents. The schema consists of two ex-tensional relations,  X  X Idx(Term, QueryId)" and  X  X ocIdx(Term, Do-cId)". Given such a schema, document retrieval is:
A typical SQL query, as the one shown above, can be decom-posed into two parts: an indexing part and a retrieval part. This de-composition, which facilitates the discussion about the ranking of tuples, is illustrated using the following SQL query on table  X  X rop-erties(Type, Area, Price)": find the areas with flats or studios in the price range 200-250k .
 Step 1: Create a view to index the source  X  X roperties".

The view illustrates how an index ( X  X reaIndex(Type, Area) X ) can be obtained for one attribute (here, the property type) and then used to retrieve areas with flats or studios in the price range 200-250k. Step 2: Retrieve in document-retrieval fashion.

Figure 2 illustrates that this general decomposition helps to trans-form any SQL query into a form aligned with that of document re-trieval. The significance of this alignment is that the ranking func-tions for document retrieval, such as the ones illustrated in [20], become applicable to tuple retrieval.

Having attained this aligned, or in other words, generalised SQL structure, a translation process takes place through which a well de-fined ranking is attached to this SQL query. This translation process is the transformation of the generalised SQL structure into PSQL programs that implement ranking functions, such as TF-IDF or LM. Note that the original information need ( X  X ind from table proper-ties the areas ... X ) is intact; yet, instead of a set-based processing of the SQL query, a rank-based processing takes place.

The following illustrates the SQL-to-PSQL (SQL2PSQL) trans-lation process. To define the algorithms that automate the SQL2PSQL translation, we use the following notation. Each SQL query has a set of conditioned attributes (referred to as X ), a set of target attributes (referred to as Y ) and a source relation(s) (re-ferred to as Z ). For example, in document retrieval X = { Term } , Y = { DocId } and Z = docIdx. Figure 3 presents the generalised structure of an SQL query.

The first algorithm decomposes the SQL expression and gener-ates probabilistic views (spaces) that are later combined accord-ing to the ranking rationales of retrieval models, such as TF-IDF and LM. With respect to the generalisation of document retrieval to tuple retrieval, the location frequency corresponds to the tuple frequency, and the document frequency corresponds to the value frequency.
 A LGORITHM 1. SQL2PSQL: Basic Views For each conditioned attribute in X in SQL query  X  X ELECT DISTINCT Y FROM Z WHERE X X  create the following views: 1. Tuple-based (Location-based) probabilities: p_Z_X. Proba-2. Value-based (Document-based) probabilities: p_Y_Z_X. 3. Conditional probabilities in sources: p _ Z _ X _ Y . Probabil-4. Information-based probabilities: p _ inv _ Y _ Z _ X . Probabil-The SQL2PSQL algorithm generates probabilistic views indepen-dent of a particular retrieval model. The algorithms illustrated in the following sections show how and which of these views are used to construct retrieval models such as TF-IDF and LM.
Algorithm 2 shows how a TF-IDF PSQL program is automati-cally generated for the genuine SQL query in Figure 3.
 A LGORITHM 2. SQL2PSQL(TF-IDF)
Step 1: Create for each conditioned attribute X in SQL query  X  X ELECT DISTINCT Y FROM Z WHERE cond(X) X  one view to reflect tf and another one to reflect idf.
Step 2: In the genuine query, replace each source (Z) by the tf (variable D) and idf (variable Q) relations of the spec-ified attributes. Replace each ordinary condition Z.X= X  X alue X  by Q.X= X  X alue X  AND Q.X=D.X. Replace each join condition Z1.X=Z2.X by D1.X=D2.X AND Q1.X=D1.X AND Q2.X=D2.X.
 For a genuine query SELECT Y FROM Z WHERE Z.X= X  X alue1 X  OR Z.X= X  X alue2 X , the output is: To improve readability, we omit the output for the join conditions. The algorithm generates a PSQL program for TF-IDF retrieval. T HEOREM 1. Algorithm SQL2PSQL(TF-IDF) is correct.

P ROOF . The view Z_X_Y_tf (for document retrieval, p_docIdx_Term_DocId, i.e. the  X  X F" component) has the probabilistic semantics P Z ( X | Y ) ( P docIdx ( Term | DocId ) ). The view pidf_Z_X (for document retrieval, pidf_docIdx_Term) has the semantics P ( X is informative ) = idf ( X ) / maxidf ( c ) . The probabilities in the SQL query  X  X ELECT SUM D.Y" correspond to P t  X  d  X  q TF( t,d )  X  IDF( t,c ) / maxidf ( c ) .
Algorithm 3 shows how an LM PSQL program is automatically generated for the genuine SQL query in Figure 3.
 A LGORITHM 3. SQL2PSQL(LM) Step 1: Create for each conditioned attribute X in query  X  X E-LECT DISTINCT Y FROM Z WHERE cond(X) X  one view for the foreground and one view for the background model. These corre-spond to the document and the collection model, respectively. The models are weighted with the respective mixture parameters. Next, the foreground is united with the background model. This yields the LM mixture in view  X  X _X_mix".
Step 2: In the genuine SQL query, replace each source (Z) by Z_X_mix. For a genuine query SELECT Y FROM Z WHERE Z.X= X  X alue1 X  OR Z.X= X  X alue2 X , the output is: T HEOREM 2. Algorithm SQL2PSQL(LM) is correct.
 P ROOF . The probabilistic views Z_X_Y_fg ( P ( t | d ) ) and Z_X_bg ( P ( t | c ) ) correspond to the docModel and collModel views in the reference implementation. The view Z_X_mix has the se-mantics P ( t | d,c ) =  X   X  P ( t | d ) + (1  X   X  )  X  P ( t | c ) . The prob-abilities in the SQL query  X  X ELECT PROD D.Y" correspond to P ( q | d,c ) = Q t  X  q P ( t | d,c ) .
 This concludes the ranking-based processing of SQL queries. The SQL2PSQL translation has been given for a simple SQL query (one source relation, one specified attribute, one target attribute). The translation, however, applies to multiple sources, ordinary and joined conditions and several attributes in the target list, all of which have been demonstrated in the algorithms and proofs.
The purpose of the evaluation is twofold. Firstly, it demonstrates the feasibility of the proposed technology, namely the PSQL frame-work and the SQL2PSQL functionality. Secondly, using the TREC benchmark [20] demonstrated how the PSQL framework is capable of solving traditional IR tasks (e.g. TREC experiments). The eval-uation is now extended to more structured/semantic benchmarks. The evaluation employs the IMDB and Monster benchmarks. The former consists of 437,281 documents with 40 topics, and the latter consists of 1,040,000 documents with 60 topics. The experi-ments were run on a single CPU using a DB+IR prototype [20] that supports the retrieval of text, structured and semi-structured data. The prototype provides customisable concepts to represent the data and supports PSQL to implement the ranking models.

To demonstrate how the modelling framework performs, we evaluate the generated retrieval models, TF-IDF and LM (settings in Section 2.2), with respect to a restricted evidence size. In stan-dard IR this corresponds to processing the most selective terms first and choosing the top-k document Ids from the posting list. The aim is to investigate how much quality can be achieved by the imple-mented retrieval models and at what cost . This cost-based evalua-tion is with respect to the retrieval model itself and how favourably or poorly it utilises the underlying evidence. One achievement is to support nested expressions whereby the required efficiency can be achieved via top-k and evidence-based processing. Nowadays, relational approaches to IR often rely on materialisation and are based on stepwise processing pipelines. The views used to imple-ment the models, however, have not been materialised, and, hence, the cost includes the time to generate and process them.

We report the retrieval quality of the generated programs and the average retrieval time (total retrieval time divided by the total number of queries) of the generated programs against the number of evidence tuples. The number of considered evidence tuples ranges from 10,000 to 200,000. We use Mean Average Precision (MAP) and Reciprocal Rank (RecipRank) as the evaluation metrics. The baselines are based on the document-oriented TF-IDF and LM retrieval models with no top-k processing. In other words, the structure of the data is not taken into consideration by the retrieval model and a bag-of-words representation is utilised. In the case of tuple retrieval, however, the document structure (see Section 4.2 for the list of element types) is considered when constructing the SQL queries. Note that the purpose of the evaluation is primarily to present the feasibility of the modelling framework and translation algorithms. Other task-specific retrieval models can be generated.
To formulate the SQL queries , the keyword queries and the ideal mapped attribute for each query term are used. This ideal attribute is extracted from the set of relevant documents for each query term.
The intuition behind the mapping is that if a term occurs fre-quently within a certain element type then it is more likely to be  X  X haracterised X  by that type [12]. This method offers an easy way to automatically transfer a keyword-based query to a structured one. The correctness of the extracted attribute is verified manually. After the SQL query is formulated it is automatically augmented with the desired ranking strategy which results in a relevance-based processing of that query. Below is an SQL-based formulation of IMDB query number 28  X  gladiator action maximus scott  X .

As noted above, when creating the SQL queries, only the top-mapped attribute for each query term is considered. For exam-ple, the top-mapped attribute for query term  X  X ladiator X  is  X  X i-tle X . This is particularly important when evaluating a larger and noisier dataset such as Monster because the significant term over-lap among the different elements makes estimating the correct at-tributes a challenge. However, the focus of our experimental study is not on the mapping process, and the SQL2PSQL facility is flex-ible enough to process queries with multiple attributes in the target list and any other customisations.
The IMDB dataset is formatted in XML and was constructed from text data (http://www.imdb.com/interfaces#plain). Each doc-ument corresponds to a movie.
 Table 1: Quality w.r.t number of evidence tuples considered; the best overall result is in bold, and the best result out of the baseline and its tuple-based variant is italicised; results that are statistically significant (p &lt; 0 . 05 ) above the baseline are denoted by  X  , as determined by a signed t-test.

There are several processing steps that present the data in the form that is best situated for achieving effective retrieval. For ex-ample, we have opted to propagate the keywords occurring within elements such as  X  X ctor" and  X  X irector" upwards to their corre-sponding part. Having a coarser schema helps to improve the accu-racy of the derived attributes for each query term. Another exam-ple is that the terms that occur in a specific context have also been propagated upwards to the root element. This propagation helps to model document-based retrieval as opposed to element-based one. [12] provided the test-bed which included 40 queries. These queries were created assuming a situation in which a user wants to find a movie using partial information spanning over many ele-ments. Relevant documents were found manually.

As discussed in Section 3, the SQL queries are decomposed into two parts: an indexing part and a retrieval part. In the retrieval part, for each query the SQL2PSQL algorithm (Algorithm 1) gen-erates a set of probabilistic views independently of a particular re-trieval model. These views are then used to construct the desired retrieval model. For the evaluation, the generated relevance-based ranking of the SQL queries is based on SQL2PSQL(TF-IDF) and SQL2PSQL(LM) algorithms (Algorithm 2 and Algorithm 3).

In Table 1 the values in parenthesis represent the relative (per-centage) difference in performance from the baseline results. If evi-dence beyond the 200,000 tuples for the generated TF-IDF program is considered, retrieval quality continues to monotonically increase. However, considering further evidence tuples for the generated LM program (optimal performance  X   X  0 . 7 ) does not significantly in-crease the quality; in fact, after a certain amount of evidence is con-sidered, it starts to decrease. This degradation is due to the top-k operators, which are optimised for the TF-IDF-based models.
The benchmark contains longer documents with more com-plex structure and full text content than the IMDB. Moreover, the queries, job requests created by users of the Monster service, are longer and more complex. An example of a keyword-based query is  X  aircraft airplane mechanic technician tech fort riley kansas ". This query seeks an Aircraft Structural Mechanic to determine the meth-ods to repair malfunctioning or damaged components of aircrafts in Fort Riley, Kansas. Similar to the IMDB we have adopted a coarse schema to improve the accuracy of the derived SQL queries. Table 2: Quality w.r.t Number of Evidence Tuples Considered
Table 2 illustrates the quality results of the TF-IDF and LM gen-erated programs. As previously observed, considering evidence tu-ples beyond 200,000 does not significantly increase retrieval qual-ity. The average retrieval times demonstrate the processing cost for the ranking-based processing of the SQL queries. The retrieval quality is reasonable at the one second ceiling, but further precision requires longer retrieval times. Although the retrieval times appear to be scalable, the processing costs can present a further challenge if lager text-based or semi-structured datasets are used. One solu-tion is [11], which demonstrates how the probabilistic descriptive approach employed herein can be parallelised.
This paper advances the feasibility of using IR models for pro-cessing SQL queries and develops the application of PSQL for tu-ple retrieval. The SQL2PSQL translation algorithms were shown to generate PSQL programs that assign an IR-model-based, proba-bilistic ranking to the tuples retrieved for an SQL query. The evaluation showed that the PSQL framework and the SQL2PSQL facility deliver good retrieval performance. This out-come is a direct consequence of the IR models. The main contribu-tions are customisable rankings and the expressiveness to model tu-ple retrieval while incorporating keyword-based document retrieval models. A quality-oriented evaluation is required. Ideally, how-ever, this evaluation should measure how DB+IR technology im-proves productivity , and specifically, how it adapts to customer-tailored and changing relevance criteria. Developing such an eval-uation measure is part of future research.

The ranking-aware processing of SQL queries is particularly rel-evant at a time when SQL queries, and more generally, structured queries, are increasingly being generated by applications built on top of search engines. The SQL2PSQL facility is also applicable to other structured query languages such as SPARQL. For exam-ple, a SPARQL2SQL translation, which acts as a query processing front-end, or a more direct one, such as SPARQL2PSQL, can be implemented. This re-use of the proposed SQL2PSQL technology can lead to general-purpose DB+IR solutions.

The automatic generation of PSQL from SQL has a twofold im-pact. The generation of  X  X eamless X  DB+IR technology becomes more accessible since applications/users are not required to learn PSQL to achieve customisable and re-usable retrieval strategies. Moreover, well-defined rankings are available as modules ready to be tailored to specific search tasks and to other structured query languages. In summary, the presented DB+IR approach supports the use of SQL and extends the well-defined IR models for the ef-fective ranking of documents and tuples.
We are grateful for the support of Yahoo! Labs Barcelona. We would also like to thank Prof. Bruce Croft for providing us with the two benchmarks, and the reviewers for their excellent suggestions.
