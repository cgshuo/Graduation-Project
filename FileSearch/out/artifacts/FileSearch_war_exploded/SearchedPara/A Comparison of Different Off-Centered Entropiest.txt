 In supervised learning, the data set is said to be imbalanced if the class prior probabilities are highly unequal. In the case of two-class problems, the larger class is called the majority class and the smaller the minority class. Real-life two-class problems have often minority class prior under 0 . 10 (e.g. fraud detection, medical diagnostic or credit scoring). In such a case the performances of data mining algorithms are lowered, especially the error rate corresponding to the minority class, even though the minorit y class corresponds to positive cases and the cost of misclassifying the positive examples is higher than the cost of misclassifying the negative examples. This problem gave rise to many papers, from which one can cite papers from [1], [2] and [3]. Dealing with imbalanced and cost-sensitive data has been recognized as one of the 10 most challenging problems in data mining [4]. As summarized by the review papers of [5], [6] and [7] or by the very comprehensive papers of [8] and [9], solutions to the class imbalance problems were proposed both at the data and algorithmic level.
At the data level, these solutions change the class distribution. They include different forms of re-sampling, such that over-sampling [3] [10] or under-sampling [11], on a random or a directed way. A comparative study using C4.5 [12] decision tree show that under-sampling beat over-sampling [13]. At the algorithmic level, a first solution is to re-balance the error rate by weighting each type of error with the corresponding cost [14]. A study of the consistency of re-balancing costs, for misclassification costs and class imbalance, is presented in [15]. For a comparison of a cost sensitive approach and a sampling approach one can see for example [16]. In decision trees learning, other algorithmic solutions consist in adjusting the probabilistic estimates at the tree leaf or adjusting the decision thresholds. [17] propose to use a criterion of minimal cost, while [18] explore efficient pre-pruning strategies for the co st-sensitive decisi on tree algorithm to avoid overfitting. At both levels, [19] studied three issues (quality of probabilistic estimates, pruning, and effect of preprocessing the imbalanced data set), usually considered separately, con cerning C4.5 decision trees and imbalanced data sets.
Our contribution belongs to the second category. We propose to replace the entropy used in tree induction algorithms by an off-centered entropy. That is to say that we work at the split level of decision trees learning taking into account an entropy criter ion. The rest of the paper is organized as follows. In Section 2, we first review splitting criteria based on Shannon X  X  entropy. We first recall basic considerations on Shannon X  X  entropy and then briefly present our off-centered entropy and the asymmetric entr opy. Then, we compare the entropies X  performance on 20 imbalanced data sets in Section 3. Finally, Section 4 draws conclusions and suggests future work. In this section we first recall basic considerations on Shannon X  X  entropy and then present the two families of non-centered entropies. For both of them we mainly present the boolean case and mention the results in the general case. Experiments presented in Section 3 are done in the boolean case. 2.1 Usual Measures Based on Shannon X  X  Entropy In supervised learning of induction tree on categorical variables, many learning algorithms use predictive association measures based on the entropy pro-posed by Shannon [20]. Let us consider a class variable Y having q modalities, p =( p 1 ,...,p q ) be the vector of frequencies of Y , and a categorial predic-tor X having k modalities. The joint relative frequency of the couple ( x i ,y j ) is denoted p ij ,i =1 ,...k ; j =1 ,...q . What is more, we denote by h ( Y )= E ( h ( Y/X = x i )) the conditional expectation of the entropy of Y with respect to X .

Shannon X  X  entropy, is a real positive function of p =( p 1 ,...,p q )to[0 .. 1], verifying notably interesting properties for machine learning purposes: 1. Invariance by permutation of modalities: h ( p ) does not change when 2. Maximality: the value of h ( p ) reaches its maximum log 2 ( q ) when the dis-3. Minimality: the value of h ( p ) reaches its minimum 0 when the distribution 4. Strict concavity: the entropy h ( p ) is a strictly concave function.
Amongst the measures based on Shannon X  X  entropy, particularly studied in by [21] and [22], we especially wish to point out:  X  the entropic gain [23], which values h ( Y )  X  h ( Y/X );  X  the u coefficient [24] is the relative gain of Shannon X  X  entropy i.e. the entropic  X  the gain-ratio [12] which relates the entropic gain of X to the entropy of X ,  X  the Kvalseth coefficient [25], which normalizes the entropic gain by the mean
The peculiarity of these coefficients is that Shannon X  X  entropy of a distribution reaches its maximum when this distribution is uniform. Even though it is the entropic gain with respect to the a priori entropy of Y whichisusedinthe numerator part of the previously mentioned coefficients, the entropies of Y and Y/X = x i used in this gain are evaluated on a scale for which the reference value (maximal entropy) corresponds to the uniform distribution of classes. The behavior of Shannon X  X  entropy is illustrated in Fig. 1 in the boolean case.
It would seem more logical to evaluate directly the entropic gain through the use of a scale for which the reference value would correspond to the a priori distribution of classes. The above-mentioned characteristic of the coefficients based on the entropy is particularly questionable when the classes to be learned are highly imbalanced in the data, or when the classification costs differ largely. 2.2 Off-Centered Entropy The construction of an off-centered entropy principle is sketched out in the case of a boolean class variable in [26] and [27]. In these previous works we proposed a parameterized version of s everal statistical measures assessing the interest of association rules and constructed an off-centered entropy.

Let us consider a class variable Y made of q = 2 modalities. The frequencies distribution of Y for the values 0 and 1 is noted (1  X  p, p ). We wish to define an off-centered entropy associated with (1  X  p, p ), noted  X   X  ( p ), which is maximal when p =  X  ,  X  being fixed by the user and not necessarily equal to 0 . 5(inthecaseofa uniform distribution). In order to define the off-centered entropy, following the proposition described in [26], we propose that the (1  X  p, p ) distribution should be transformed into a (1  X   X ,  X  ) distribution such that:  X  increases from 0 to 1 / 2 when p increases from 0 to  X  ,and  X  increases from 1 / 2to1when p increases from  X  to 1. By looking for an expression of  X  as  X  = p  X  b a ,onbothintervals0  X  p  X   X 
To be precise, the thus transformed frequencies should be denoted as 1  X   X   X  and  X   X  . We will simply use 1  X   X  and  X  for clarity reasons. They do correspond to frequencies, since 0  X   X   X  1. The off-centered entropy  X   X  ( p ) is then defined as the entropy of (1  X   X ,  X  ):  X   X  ( p )=  X   X  log 2  X   X  (1  X   X  )log 2 (1  X   X  ).
With respect to the distribution (1  X  p, p ), clearly  X   X  ( p ) is not an entropy strictly speaking. Its properties mu st be studied considering the fact that  X   X  ( p ) is the entropy of the transformed distribution (1  X   X ,  X  ), i.e.  X   X  ( p )= h (  X  ). The behavior of this entropy is illustrated in Fig. 1 for  X  =0 . 2.

The off-centered entropy preserves var ious properties of the entropy, among those studied in particular by [28] in a data mining context. Those properties are easy to prove since  X   X  ( p )isdefinedasanentropyon  X  and thus possess such characteristics. It can be noticed that  X   X  ( p ) is maximal for p =  X  i.e. for  X  =0 . 5. Invariance by permutation of modalities property is of course voluntarily abandoned. Proofs are given in detail in [29].

Following a similar way as in the boolean case we then extended the definition of the off-centered entropy to the case of a variable Y having q modalities, q&gt; 2 [29,30]. The off-centered entropy for a variable with q&gt; 2 modalities is the defined by  X   X  ( p )= h (  X   X  )where:  X   X  j =  X  j q normalization property), 0  X   X  j  X  1, q j =1  X  j =1(  X  j should be analogous to frequencies),  X  j = p j q X  2.3 Off-Centered Generalized Entropies Shannon X  X  entropy is not the only diversity or uncertainty function usable to build coefficients of predictive associat ion. [31] already proposed a unified view of the three usual coefficients (the  X  of Guttman, the u of Theil and the  X  of Goodman and Kruskal), under the name of Proportional Reduction in Error co-efficient. In a more general way we built the Proportional Reduction in Diversity coefficients, which are the analogue of the standardized gain when Shannon X  X  entropy is replaced by whichever con cave function of uncertainty [32].
One of the particularities of the off-centering we here propose, compared to the approach proposed by [33] is that rather than defining a single off-centered entropy, it adapts to whichever kind of entropy. We thus propose a decentring framework that one can apply to any measure of predictive association based on a gain of uncertainty [30]. 2.4 Asymmetric Entropy With an alternative goal, directly related to the construction of a predictive association measure, especially in the c ontext of decision trees, [34] proposed a consistent and asymmetric entropy for a boolean class variable. This measure is asymmetric in the sense that one may choose the distribution for which it will reach its maximum; and consistent since it takes into account n ,thesize of the sampling scheme. They preserve the strict concavity property but alter the maximality one in order to let the entropy reach its maximal value for a distribution chosen by the user ( i.e. maximal for p =  X  ,where  X  is fixed by the user). This implies revoking the invariance by permutation of modalities . asymmetric entropy corresponds to the quadratic entropy of Gini. The behavior of this entropy is illustrated in Fig. 1 for  X  =0 . 2.

In [33], the same authors extend their approach to the situation where the class variable has q&gt; 2 modalities. What is more, since one may only make ( f j ) j =1 ,...,q , they wish that for same values of the empirical distribution, the value of the entropy should decrease as n rises (property 5, a new property called consistency ). They thus are led to modify the third property ( minimality )ina new property 3 ( asymptotic minimality ): the entropy of a sure variable is only required to tend towards 0 as n  X  X  X  . In order to comply with these new prop-erties, they suggest to estimate the theoretical frequencies p j by their Laplace estimator, p j = nf j +1 n + q . They thus propose a consistent asymmetric entropy as: h In our experiments, we compare the behaviors of decision tree algorithms to classify imbalanced data sets using our proposed off-centered entropy oce ,the Shannons entropy se and the asymmetric entropy ae . To achieve the evaluation we added oce and ae to the decision tree algorithm C4.5 [12]. In these experi-ments, in each node the distribution for which oce and ae are maximal is the a priori distribution of the class variable in the considered node.

The experimental setup used the 20 dat a sets described in Table 1 (column 1 indicates the data set name, the numbers of cases and of attributes), where the first twelve ones are from the UCI repository [35], the next six are from the Statlog repository [36], the following data set is from the DELVE repository (http://www.cs.toronto.edu/  X  delve/), while the last one is from [37].
In order to evaluate the performance of the considered entropies for classify-ing imbalanced data sets, we pre-processe d multi-class (more than two classes, denoted by an asterisk) data sets as two-class problems. The columns 2 and 3 of Table 1 show how we convert multi-class to minority and majority classes. For example, with the OpticDigits data set, the digit 0 is mapped to the minority class (10%) and the remaining data are considered as the majority class (90%). For the 20-newsgroup collection, we pre-processed the data set by representing each document as a vector of words. With a feature selection method which uses mutual information, we get a binary data set of 500 dimensions (words).
The test protocols are presented in the column 4 of Table 1. Some data sets are already divided in training set (trn) and testing set (tst). If the training set and testing set are not available then we used cross-validation protocols to evaluate the performance, else k-fold cross valid ation is used. With a data set having less than 300 data points, the test protocol is leave-one-out cross-validation (loo). It involves using a single data point of the data set as the testing data and the remaining data points as the training data. This is repeated such that each data point in the data set is used once as testing data. With a data set having more than 300 data points, k-fold cross-validation is used to evaluate the performance. In k-fold cross-validation, the data set is partitioned into k folds. A single fold is retained as the validation set for testing, and the remaining k-1 folds are used as training data. The cross-validation process is then repeated k times. The k results are then averaged. The columns 5 to 9 of Table 1 present the results according to each entropy in terms of tree size, global error rate, error rate on the minority class and on the majority class (best results are in bold). The synthetic comparisons two by two are presented in Table 2.

For these first comparisons, we recall that the rule of prediction is the majority rule. The definition of another rule of prediction, better adapted to non-centered entropies, is one of the enhancem ents which we intend to accomplish.
We can conclude that the non-centered ent ropies, particularly the off-centered entropy, outperform the Shannon X  X  entropy. These both entropies significantly improve the MinClass accuracy, without penalizing the MajClass accuracy, where MinClass (MajClass) accuracy is the proportion of true results in the minority (majority) class.
Indeed, compared to Shannon X  X  entropy se , the off-centered entropy oce im-proves the MinClass accuracy 18 times out of 20, with 1 defeat and 1 equality, which corresponds to a p-value of 0.0000. The corresponding average gain in accuracy is close to 0.02 (p-value = 0.0016 according to a paired t-test). The accuracy of the MajClass is not signifi cantly modified, but the global accuracy is improved 16 times out of 20, with 3 defeats and 1 equality (p-value = 0.0044), while the average corresponding gain is close to 0.008. Moreover, the trees pro-vided by oce have often a more reduced size, but this reduction is not significant.
The asymmetric entropy ae gives slightly less significant results when com-pared to Shannon X  X  entropy se .Itimproves15timesoutof20theMinClassac-curacy (p-value = 0.0192), with an average gain close to 0.01 (p-value = 0.0112). However, the improvement of the global accuracy is not significant: ae wins 14 times out of 20, with 1 equality and 5 defeats, while the increase of the global ac-curacy is only 0.002. In the same way, the performance for the MajClass accuracy is comparable ( ae wins 8 times, se wins 8 times, and 4 equalities). Furthermore, for the size of the tree, the per formance is also comparable ( ae wins 8 times, se wins 10 times, and 2 equalities).

When comparing the two non-centered entropies oce and ae , one can observe a slight but not significant superiority of the off-centered entropy oce for each criterion. Particularly, a gain of 1 point on the MinClass error rate and 0.5 point on the total error rate must be noticed. In order to deal with imbalanced classes, we proposed an off-centered split func-tion for learning induction trees. It has the characteristic to be maximum for the distribution a priori of the class in the node considered. We then compare, in the boolean case on 20 imbalanced data bases, the performances of our entropy with the entropy of Shannon and an asymmetric entropy. All our experiments are founded on C4.5 decision trees algorithm, in which only the entropy is mod-ified. Compared to Shannon X  X  entropy both non-centered entropies, significantly improve the minority class accuracy, without penalizing the majority one. Our off-centered entropy is slightly better than the asymmetric one, but this is not statistically significant. However one major advantage of our proposal is that it can be applied to any kind of entropy, for example to the quadratic entropy of Gini used in the CART algorithm [38]. We plan to improve the pruning scheme and the criterion to affect a class to a lea f. Indeed, these two criteria such as defined in C4.5, do not well support the recognition of the minority class. We then can hope for an improvement of our already good results. It could be also valuable to take into account a cost-sensitive matrix.

