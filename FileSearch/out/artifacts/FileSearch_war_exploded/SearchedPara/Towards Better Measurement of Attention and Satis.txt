 Web Search has seen two big changes recently: rapid growth in mobile search traffic, and an increasing trend towards providing answer-like results for relatively simple information needs (e.g., [weather today]). Such results display the answer or relevant infor-mation on the search page itself without requiring a user to click. While clicks on organic search results have been used extensively to infer result relevance and search satisfaction, clicks on answer-like results are often rare (or meaningless), making it challenging to evaluate answer quality. Together, these call for better measure-ment and understanding of search satisfaction on mobile devices. In this paper, we studied whether tracking the browser viewport (visible portion of a web page) on mobile phones could enable ac-curate measurement of user attention at scale, and provide good measurement of search satisfaction in the absence of clicks. Fo-cusing on answer-like results in web search, we designed a lab study to systematically vary answer presence and relevance (to the user X  X  information need), obtained satisfaction ratings from users, and simultaneously recorded eye gaze and viewport data as users performed search tasks. Using this ground truth, we identified increased scrolling past answer and increased time below answer as clear, measurable signals of user dissatisfaction with answers. While the viewport may contain three to four results at any given time, we found strong correlations between gaze duration and view-port duration on a per result basis, and that the average user atten-tion is focused on the top half of the phone screen, suggesting that we may be able to scalably and reliably identify which specific re-sult the user is looking at, from viewport data alone.
 Search on mobile phone; user attention and satisfaction; viewport logging.
Recent years have witnessed a rapid explosion in the usage of mobile devices on the web. According to recent surveys, web browsing on mobile devices increased five fold from 5.2% three years ago to 25% in April 2014[26]; and a significant amount of Figure 1: An example of the search results page showing Knowl-edge Graph result. The yellow area indicates current position of the browser X  X  viewport (visible portion of the page). search engines X  traffic (about one in every five searches) is gen-erated by mobile devices[25]. Another recent change in search is the increasing trend towards providing answer-like results for sim-ple information needs that are popular on mobile (e.g., [weather today], [pizza hut hours]). Such results display the answer or rel-evant information on the search page itself without requiring the user to click. Instant information is desirable on mobile devices, but poses a challenge  X  while clicks on organic search results have been extensively used to infer result relevance and search satisfac-tion [5, 6], answer-like results often do not receive clicks, which makes it difficult to evaluate answer quality and search satisfac-tion. Together, the rapid growth in mobile traffic and answer-like results in Search warrants better understanding of user attention and satisfaction in search on mobile devices.

Search behavior on mobile devices can be different than on desk-top for several reasons. Unlike traditional desktop computers with large displays and mouse-keyboard interactions, touch enabled mo-bile devices have small displays and offer a variety of touch inter-actions, including touching, swiping and zooming. As a result, user experience and search behavior on mobile devices is different  X  for example, due to the lack of a physical keyboard, users tend to is-sue shorter queries than on the desktops [19]. Compared to large desktop displays (13-30" displays or bigger), the displays on mo-bile phones are small (4-5" or smaller), and limit the amount of information that the user can view simultaneously.

We introduce viewport as the portion of the web page that is visible on the phone screen at a given point in time. Viewport coor-dinates are recorded in the web page coordinate system, (i.e., upon scrolling, viewport moves towards the bottom of the web page). Since the small displays on mobile phones limit the number of vis-ible search results to 3-4, viewport tracking could be used to better measure users X  attention on a web page, as was recently recognized by some researchers [21, 13]. To the best of our knowledge, there is no quantitative evaluation or validation of viewport data in how well it can approximate user attention on mobile devices, or be used to detect search satisfaction. In this paper we test the utility of view-port signals. To approximate attention from viewport tracking, we measure the result view time -the duration for which a search result appeared within the viewport .

In desktop settings, the amount of time user spent gazing (or hovering with mouse cursor) on a particular result was shown to be useful for inferring result relevance [24], predicting future clicks [15], improving ranking, estimating snippet attractiveness[21] and whole page quality [22]. While cursor hovers do not exist on mo-bile devices, these findings suggest that measurement of viewing time of results on mobile could lead to several useful applications in relevance estimation and whole page optimization.

In this paper we demonstrate how viewport metrics can be used to measure user attention (eye gaze), and detect search satisfaction. Specifically, our paper makes the following contributions:
We begin by surveying related work in eye tracking for search on desktops and user behavior for search on mobile devices. We then describe our experiment and user study, followed by the anal-ysis of searcher X  X  attention and satisfaction on mobile phones. We conclude with a discussion reviewing the findings and limitations of this study, along with suggestions for future work.
Eye tracking technology has been extensively used in studies of web search result examination behavior in desktop settings. Granka et al. [9] studied how users browse search results and select links. They showed that users spend most of the time inspecting the first and the second result before their first click. Based on insights gained from eye tracking, Joachims et al. [17] compiled the most common examination strategies and demonstrated their utility in inferring user-perceived relevance of result ranking. Lorigo et al. [23] used eye tracking to study gaze trajectories on a search re-sults page in more detail. They found that only 25% of users ex-amine search results in the order they are presented by the search engine. A similar study was conducted by Guan and Cutrell [10], who showed the effect of target result position on searcher X  X  exam-ination behavior.

Apart from organic search results, previous work explored user attention and search behavior and their relation to ads and rich in-formational panels in the desktop settings. Buscher et al.[4] inves-tigated the effect of ad quality on searcher X  X  receptiveness to the advertisements. They found that when ad quality varied randomly, users paid very little attention to the ads. Navalpakkam et al.[24] conducted a controlled study where they varied the presence and relevance of a rich informational panel placed to the right of organic search results. They found that the information panels containing information relevant to the user X  X  task attract more attention and longer mouse cursor hovers. Our work is similar to Navalpakkam et al. in that we both study user behavior in the presence of informa-tional panels among the search results (results based on Knowledge Graph 1 ). However, there are important differences: 1) we study at-tention and satisfaction on mobile search, while the previous study was conducted in desktop settings; 2) unlike desktop where the in-formation panel appears on the right hand side of the page (and hence may be ignored), on mobile phones, the information panel is interleaved between organic search results. In addition to informa-tional panels, we also study Instant Answer results, such as related to current weather information, price of currency exchange, etc.
User factor and individual differences strongly affect the way searchers examine the results and interact with the search engine. Aula et al.[1] reported two types of search result examination pat-terns  X  economic and exhaustive . Economic users inspect results sequentially from the top to bottom and click on the first relevant link they notice. In contrast, exhaustive searchers thoroughly exam-ine the search result page and consider every result before deciding what to click. Dumais et al. [8] extended this work by clustering users based on their examination behavior of whole search page. In addition to user examination pattern on organic search results they considered user attention on advertisements.

Despite the abundance of research about searcher X  X  attention on desktops, attention on mobile devices remained relatively unex-plored. Huang and Diriye [13] discussed the potential utility of viewport logging on touch-enabled mobile devices. In this pa-per, we use client based viewport logging (similar to [13]) to track user interactions on the search result page. Recent study of Guo et al. [12] investigated the value of user interaction data on mo-bile phones for predicting search success. Continuing this line of research Guo et al. [11] demonstrated the utility of tracking touch-based interactions to improve relevance estimation of destination web pages (a web page linked by a search result). Among many user activity metrics, they found the inactivity time on a web page to be highly correlated with page relevance. While their work fo-cused on user interactions and behavior on the destination pages, this paper considers viewport behavior and in addition, eye track-ing, on the search results page.

Kim et al. [20] investigated result examination strategies on dif-ferent screen sizes. Similarly to [23] they adopted taxonomy of three examination strategies: Depth-First, Mixed, Breadth-First . Surprisingly, they did not find any significant variation in the way users examine search results on large and small displays. It is worth noting that they used a simulation of the mobile phone screen, and it is possible that behavior on simulated phone screens (shown on a desktop monitor) and an actual mobile device can vary substan-tially for reasons mentioned in the introduction (e.g., actual phones can be held in the hand, and allow several touch interactions in-cluding zooming in and scrolling that simulated phone setting may not offer). To the best of our knowledge, study of Biedert et al.[2] remains the only quantitative eye tracking study of reading behav-ior performed on an actual mobile device. While our study uses a similar technical setup, we focus on analyzing search behavior on a mobile phone (search attention and satisfaction). In addition, we demonstrate the utility of viewport based metrics and their high correlation with user attention.
In order to evaluate our ideas, we designed and conducted a user study with answer-like search results. We split the user study into two parts: first, to study how a rich information panel with Knowl-edge Graph results (KG) affects user search and attention behavior, and second, to study how Instant Answers (IA) influence search and attention behavior. Knowledge Graph results are often shown for queries related to some entity, e.g. famous person, place, etc. Examples of such queries are [angelina jolie] or [louvre] (shown in Figure 1). Examples of queries that trigger Instant Answers in-clude [weather today], [twitter stock price], [define amazing], [gi-ants schedule]).

Our choice of dividing the study into two parts is motivated by the fact that KG and IA have quite different user interfaces which may potentially affect results of the study. Indeed, both re-sult types (KG and IA) provide users with answer-like information (i.e., the information is visible on the search page, no need to click through), but they have different user interfaces. Instant Answer re-sult type has a diverse UI, sometimes interactive, such as in weather and  X  X alculator X  related queries; sometimes containing charts and graphs, such as in weather and finance, and sometimes containing text only, such as in dictionary lookup queries. On the other hand, KG results have a consistent user interface and appearance  X  an image block on top, followed by textual facts, and some links.
Both parts of the study used the following protocol. Participants were presented with a web page containing a list of 20 search tasks. Each entry in the list consisted of the task description, followed by 2 hyperlinks  X  one pointing to the search results page (with a predefined query related to the task), and the second pointing to the post-task questionnaire. Participants were instructed to read the task description, (attempt to) find the answer to the task, and complete the post-task questionnaire.

To ensure that the tasks had similar levels of difficulty, two au-thors of the paper verified that for each task, the corresponding search results page (SERP) contained the answer in one of the search result snippets, and the task could be solved by simply in-specting the results. Thus, the tasks were fairly easy (required less than a minute) and participants were instructed to spend not more than three minutes per task. Upon finding the answer, participants were asked to navigate back to the study home page by using the  X  X ack X  button on the phone, and follow the second hyperlink to complete the post-task questionnaire. On the post-task question-naire page, participants were asked to rate their satisfaction with the search results as a whole (single rating) on a 7 point likert scale  X  1 being completely dissatisfied and 7 being completely satisfied. Note that the queries were predefined per task, and query reformu-lation was not allowed.

For the first part of the study, we used a 2 x 2 within subject de-sign with two factors: Relevance of the Knowledge Graph result to the user X  X  information need, and Presence of the Knowledge Graph result on the search page. Both factors have two levels: Relevance -Figure 2: Top panel shows Tobii mobile stand including scene cam-era, the eye tracker and a mobile phone placed at the device holder. We used this setup to perform eye tracking in our user study. Bot-tom panel illustrates post-processing step of mapping gaze from scene camera coordinates to phone screen coordinates. relevant or irrelevant, Presence -present or absent. Each participant performed 20 search tasks (5 tasks per condition). The task presen-tation order was randomized to eliminate any learning or task order effects. In order to familiarize participants with the mobile device and the study flow, each participant completed 4 practice tasks prior to starting the study. After completing 20 tasks in the first study, participants were given a 5 minute break before proceeding to the second part of the study, which was similar, except that it focused on Instant Answer results instead of Knowledge Graph results. In second the part, IA was always present and we only varied the sin-gle factor: IA Relevance . This enabled us to double the number of tasks per condition (from 5 in KG to 10 in IA).
We recruited 30 participants with informed consent (12 male and 18 female) aged 18-65, with various occupations and self-reported mobile search experience. Data from 6 participants was excluded due to calibration problems with the eye tracker (missing fixations, poor calibration accuracy). Most of the participants had normal or corrected vision (e.g. contact lenses) and were able to read from the mobile phone without wearing glasses.
We used the Tobii X60 eye tracker to record participant X  X  eye gaze movements on the mobile phone. The eye tracker allowed us to record eye gaze with a frequency of 60 Hz and accuracy of 0.5  X  of visual angle [27]. We used a Nexus 4 mobile phone run-ning Android operating system as the mobile device. The Chrome Query Task Description university of cambridge What was the enrollment of the University of Cam-sfo to atl price Find the ticket price of the Delta flight from San browser was used to display the task description page and search result pages. The phone was attached to Tobii X  X  mobile stand as shown in the top panel of Figure 2. As part of the Tobii mobile stand setup, the scene camera was configured to capture the video of the mobile device during the study (sample screenshot shown in bottom panel of Figure 2). The experiment began by calibrating eye gaze of each participant using a five point calibration (four points were shown in the corners of the phone screen and one point was shown in the center). Unfortunately, Tobii X60 does not record eye gaze in the phone X  X  coordinate system, which is required for de-termining the exact result seen by the user, hence gaze data was processed using the procedure described in Section 3.4.
To record the exact information that was displayed on the phone screen at any given time, we instrumented custom viewport log-ging . This allowed us to record the portion of the web page cur-rently visible on the screen, as well as bounding boxes of all search results shown on the page. Viewport logging was instrumented with JavaScript and inserted into every SERP shown to the users. Our script recorded bounding boxes of the search results, shortly after the page was rendered in the browser, and logged viewport change events such as scrolling and zooming. All the viewport events were buffered and subsequently sent with an HTTP request to a user study server where they were stored for subsequent analysis. Such instrumentation allowed us to reconstruct what the user saw on the screen at any point of time.
As mentioned earlier, Tobii X60 captures gaze position in the scene camera coordinate system instead of the phone coordinate system 2 , which poses a challenge as quantitative analysis of atten-tion on results requires gaze data to be in the phone coordinate system. To this end, we designed a custom software to annotate bounding boxes around the phone screen in Tobii X  X  scene video of each participant, and to accurately map gaze from the scene to phone coordinate system. The bottom panel in Figure 2 illustrates the difference between scene and phone coordinate systems.
To perform the mapping, we chose two vectors along the phone X  X  vertical and horizontal axes: v horiz = v 3  X  v 0 and v vert v , where v i corresponds to a vertex of the phone screen bounding box, as shown in Figure 2. The eye gaze position in the phone coordinate system is given by v phone = ( v  X  v 0 ) A  X  1 [ v vert , v horiz ] is the coordinate change matrix. Finally, to get the
A Tobii technical support specialist confirmed that Tobii x60 can-not record gaze coordinates in the phone coordinate system. actual eye gaze coordinates on the phone in pixels one needs to scale v phone with phone X  X  screen size (378 x 567 px).

To associate eye gaze data with a particular page view recorded in the viewport logs, we synchronized the eye tracker X  X  clock with the clock used by the viewport logging on the phone. This allowed us to map each gaze position to the corresponding search result on the SERP by using the bounding boxes of all results on page recorded in the viewport logs. The resulting mapping was accurate enough to distinguish gaze position between two adjacent lines of text, allowing even more fine grained analysis at sub-result level.
The raw eye gaze data was parsed to obtain a sequence of fixa-tions (brief pauses in eye position for around 100-500ms) and sac-cades (sudden jumps in eye position) using standard algorithms [7]. Eye fixations and their duration are thought to represent meaningful information processing and can approximate attention [7]. Thus, our subsequent analysis was performed using eye fixations.
We begin by analyzing the relationship between user behavior metrics, derived from gaze, viewport and user actions, and the ex-perimental conditions of our user study. Then, we present our find-ings about user attention during search on mobile, including the effect of result rank position and strong preference for the top half of the screen. We conclude with presenting correlation analysis of result viewing time measured with eye tracking and result display time measured using viewport.
As search engines strive to provide answer-like results to users to satisfy their information need instantly (without the need to click), it becomes challenging to evaluate the effect of disturbing the origi-nal ranked list (of clickable results) with a novel type of result (that is often not clickable). In this section, we attempt to quantify how user behavior and satisfaction are affected by injecting Knowledge Graph (KG) (described in section 3) to the search results page. We formulated the following hypothesis:
To test this hypothesis, we performed a 2-way repeated mea-sures ANOVA (within subjects design) and examined the effect of KG presence on user X  X  satisfaction ratings. Consistent with H1 , the mean satisfaction ratings increased from 5 . 28  X  0 . 09 when KG is absent to 5 . 69  X  0 . 09 when KG is present (F(1,23)=13.35,p=0.001), revealing a significant effect of KG presence on user satisfaction. This shows that users are more satisfied when the answer-like result is present. As expected, when KG is absent, we did not find any sta-tistically significant differences in satisfaction between questions used for KG relevant and KG irrelevant tasks, since they had simi-lar difficulty levels(F(1,23)=3.578, p=0.07)).
Prior research in the desktop domain identified the relevance and position of results as two major factors influencing user behavior in search. In this section we investigate the effect of answer relevance on user behavior, and Section 4.3 describes the effect of result po-sition on the attention distribution on mobile phones. We focus on KG-and IA-present conditions in order to identify useful behavior metrics that can signal the relevance of answer-like results. To this end we formulated four hypotheses:
Table 2 summarizes gaze, viewport and page metrics computed for the data collected in the KG part of the study. These metrics are defined below.
 Gaze metrics : TimeOnKG and TimeBelowKG report total fixa-tion time spent (in seconds) viewing Knowledge Graph results; %TimeOnKG and %TimeBelowKG report corresponding quantities divided by time spent on all search result elements.
 Viewport metrics : TimeOnKG and TimeBelowKG report the total duration (in seconds) for which the Knowledge Graph result was inside the user X  X  viewport (visible to the user); %TimeOnKG and %TimeBelowKG viewport metrics report the corresponding quanti-ties divided by the sum of viewport time of all result elements. Page metrics : NumberOfScrolls reports number of times the user scrolled down; TimeOnPage reports total time the user spent on the search result page; TimeOnTask reports the time user spent on completing the task (task end is determined by submission of the task satisfaction rating); SatisfactionScore reports the user X  X  satis-faction rating regarding search engine X  X  performance in the task. For each of the metrics we performed a two-way with-subject de-sign ANOVA for two factors related to Knowledge Graph result  X  KG presence and KG relevance .

We start by analyzing the effect of answer relevance on these metrics. Since answer relevance makes sense only when KG is present, we focus on that condition (blue lines in Figures 3a-d). Consistent with H2 , we found that the users are more satisfied when KG is relevant than irrelevant (6.03  X  0.13 for relevant vs. 5.39  X  0.13 for irrelevant, F(1,23)=14.47, p&lt;0.001), suggesting that rele-vant Knowledge Graph results significantly enhance user satisfac-tion. Consistent with H3 , when KG was relevant, users quickly found the answer and completed the task faster, while when KG was irrelevant, they spent more time on the page looking for the an-swer. Thus, time on task increased significantly from 48 . 30  X  30 . 06 Figure 4: Attention heatmaps for KG Relevant and KG Not Rele-vant conditions. This figure shows that on average, across all users in the study, there is increased gaze activity below KG when it is irrelevant than relevant. to 163 . 33  X  33 . 12 seconds (p &lt; 0.05), and time on page also in-creased significantly from 5 . 37  X  0 . 65 to 7 . 98  X  0 . 47 seconds (p &lt; 0.05). Consistent with H5 , relevant KG results were associated with less scrolling down the page ( 1 . 77  X  0 . 28 vs. 3 . 32  X  0 . 25 ; p &lt; 0.05). Thus, in all cases we observe positive effect of KG results on user experience.
 As expected and as shown in the red lines in Figures 3a-d, when KG is absent, there were no statistically significant differences be-tween the KG relevant and irrelevant conditions.

To aid more in-depth analysis of metrics in our study (2x2 de-sign), we performed post-hoc tests on pairwise comparisons be-tween the conditions using a 1-way ANOVA with Bonferroni cor-rection and annotated corresponding plots of Figure 3 with signif-icance markers. Comparisons that are significantly different are denoted by a  X * X  in Figure 3, and the rest are denoted by  X  X S X  for not significant.

We focus on Figure 3d here. As seen in Section 4.1 and con-sistent with H1 , users are significantly more satisfied when KG is present than absent (F(1,23)=13.35, p=0.001). Interestingly, for KG-irrelevant tasks, KG presence had no effect (F(1,23)=0.349, p=0.56). This suggests that Knowledge Graph results do not harm user satisfaction, even when they are not directly answering user X  X  information need. We found a significant interaction effect between KG presence and relevance (F(1,23)=12.41, p=0.001), as seen by the intersecting lines on Figure 3d. Figures 3a-3c are similar to Figure 3d, and show the number of scrolls, time on task and time on page as a function of KG presence and relevance.
 Metric IA Relevant IA Not Relevant p-value Gaze
TimeOnIA (s) 0.55  X  0.09 0.74  X  0.11 p=0.812 % TimeOnIA 45  X  5 38  X  3 p=0.237
TimeBelowIA (s) 1.21  X  0.23 1.41  X  0.17 p=0.298 % TimeBelowIA 55  X  5 62  X  3 p=0.343 Viewport
TimeOnIA (s) 1.96  X  0.24 3.64  X  0.26 p &lt; 0.001 % TimeOnIA 11  X  1 16  X  1 p &lt; 0.001
TimeBelowIA (s) 11.74  X  1.59 19.02  X  1.30 p &lt; 0.001 % TimeBelowIA 32  X  3 56  X  2 p &lt; 0.001 NumberOfScrolls 1.33  X  0.17 2.96  X  0.20 p &lt; 0.001 NumberOfEvents 6.12  X  0.39 9.93  X  0.38 p &lt; 0.001 TimeOnPage (s) 3.89  X  0.43 7.17  X  0.41 p &lt; 0.001 TimeOnTask (s) 90.7  X  1.65 102.82  X  1.73 p &lt; 0.001 SatisfactionScore 6.25  X  0.09 5.08  X  0.11 p &lt; 0.001 Table 3: Summary of Gaze, Viewport and Page (M  X  SE) for  X  X A Relevant X  and  X  X A Not Relevant X  experiment conditions. Time re-lated metrics are measured in seconds.

Unlike H4 , we found that users spend more viewport time and gaze time on KG results when they are irrelevant compared to the relevant KG results. Viewport time increased from 3 . 96  X  0 . 42 on relevant KG results to 5 . 38  X  0 . 34 seconds on irrelevant KG results (p &lt; 0.001); similar increase in % time on KG as reported in Table 2. The latter fact seems counter-intuitive, as we would expect irrele-vant answers to get rejected sooner, and relevant answers to receive more attention. A plausible explanation is: since both relevant and irrelevant KG results display information on entities related to the query, users do not know that it is irrelevant to their task until they have read it fully (e.g., for the task [find watchable cartoons of the Simpsons], the user would see a KG result on the Simpsons entity, showing images of the Simspons cartoon and names of characters). Thus, expecting to find the answer in KG, users may read through the entire KG result, and upon not finding the answer, they continue to examine the remaining results below. We suspect for this reason, irrelevant KG results in our study get more viewport and gaze time.
It is worth noting that increased attention on a result does not necessarily mean it is more relevant (it may also indicate user diffi-culty). This ambiguity was also found by Just and Carpenter [18]. We believe a stronger test of relevance is the user X  X  next action  X  did the user scroll past the answer and spend more time examin-ing results below? If so, it suggests that users were probably not satisfied with the answer.

Consistent with H5 , we found that irrelevant KG results were in-deed associated with increased scrolling down the page ( 3 . 32  X  0 . 25 vs. 1 . 77  X  0 . 28 ; p &lt; 0.05) and more time below KG in seconds ( 12 . 83  X  1 . 26 vs. 11 . 28  X  2 . 18 seconds, p=0.01) and as a % of page time (% time below KG for irrelevant KG results is 26  X  2 vs. 16  X  2 for relevant KG results, p &lt; 0.001). Similar patterns were ob-served with eye gaze. Figure 4 illustrates this by showing heatmaps of gaze activity comparing the KG relevant vs. KG irrelevant across all users in the study. The red hotspots that received high atten-tion are positioned near the Knowledge Graph results. Note the increased gaze activity below irrelevant KG results on Figure 4b as compared to relevant KG results on Figure 4a, suggesting that upon looking at irrelevant KG results, since users did not find the answer, they continued to inspect results below KG (unlike in the relevant condition where upon looking at relevant KG results, users found the answer and completed the task).
Table 3 summarizes gaze, viewport and page metrics for the sec-ond part of the study on Instant Answers (IA), which was designed with a single factor IA Relevance making data analysis significantly simpler. Most of the findings we discovered by comparing relevant vs. irrelevant conditions in the KG part of study apply to IA results as well. For example, consistent with H2, H3, H5 for KP, when IA was relevant (vs. not), we found that users were significantly more satisfied, completed the task sooner (less time on task and page), scrolled less and spent less time below the answer.

Although gaze metrics lacked statistical significance, likely due to the large variance in the data, they exhibited similar behavior as the viewport metrics providing additional evidence for the validity of viewport data for evaluation answer like results.

Finally, we verify that viewport metrics can be related to searcher X  X  satisfaction. We restrict our analysis to the KG Present where Knowledge Graph results were shown to the user (eliminating Ab-sent condition where answer like viewport metrics are undefined). The linear regression analysis reveals statistically significant ef-fect of gaze metrics on the user satisfaction scores (gaze TimeBe-lowKG F(2,138) = 7.55, p-value &lt; 0.001 and viewport TimeBe-lowKG F(2,138) = 42.18, p-value &lt; 0.001). The results are almost identical for Instant Answer data. This finding allows us to estab-lish the relationship between viewport metrics and the user satis-faction with answer-like results.

To summarize, we established that the time spent below an an-swer result, measured using viewport data, can signal result rele-vance and user satisfaction with the search. We confirm that amount of scrolling is negatively correlated with user satisfaction, which is consistent with previous findings in the desktop and mobile set-tings. Finally, we found that relevance of Knowledge Graph results and Instant Answers have similar effect on user behavior, as mea-sured using gaze and viewport data.
It is well known in search on desktops that the first result re-ceives much higher portion of user attention and clickthrough rate (CTR) than the second result, and in general, attention and CTR decrease from top to bottom on the SERP. We tested whether a similar phenomenon exists on mobile phones. We considered data from the KG absent condition, so that the SERP consists of 10 or-ganic clickable web results, which is the most commonly studied scenario prior work on search on desktops. The left panels in figure 5 show viewport time on result in milliseconds (top-left panel) and in % (bottom-left panel) as a function of result position (x axis). A 1-way ANOVA shows a main effect of result position or rank on time on result (F(9, 2660) = 64.57, p &lt; 0.001) suggesting that position bias affects user attention on mobile phones too. While for most positions the viewport time on result (in ms, %) decreases with result position, we find a surprising bump at positions 2 and 3 (significantly higher % time on the second result than the first: t(528)=-2.2, p=0.02; and higher % time on the third result than the first: t(504)=-3.7, p &lt; 0.001). Authors verified that this is not a bug and is indeed feature of the mobile data. One possible explanation for the bump at position 2 and 3 is the presence of short scrolls on mobile phones. Figure 6 illustrates this with an example  X  unlike desktop where the page up down keys allow users to move from one page fold to another non-overlapping page fold, in mobile phones, users often tend to perform short scrolls that may render the second or third result visible across more viewports and for longer time than the first result. It is possible that for navigational tasks where
For Page measures the p-values are computed using the repeated measures ANOVA; for Viewport and Gaze measures Wilcoxon rank sum test is used.
 Figure 5: This figure shows how viewport time (left panels) and gaze time (right panels) vary with result position. Top-left panel shows viewport time in ms, and the bottom-left panel shows view-port time as a fraction of time on all results on that page. The right panels show similar plots for eye gaze. users mostly click the first result (e.g., twitter), since scrolling is unlikely, we may observe that viewport time decreases with posi-tion. This remains to be tested in a future study.

An obvious question is whether the bump at position 2 or 3 is an artifact of viewport data, or is a real attention phenomenon that oc-curs with eye gaze too. The right panels in figure 5 show gaze time on each result in milliseconds (top-right panel) and in % (bottom-right panel) as a function of result position (x axis). Similar to viewport, we find a main effect of result position or rank on time on result (F(9, 1720) = 15.1, p &lt; 0.001) and a bump at position 2 (% time on result is significantly higher for second result than the first: t(343)=-2.3, p=0.02). We believe this may be a function of scrolling too  X  due to the small screen size in phones, the second result may only be partially visible; in order to bring it fully into view, the user has to adjust the scroll distance by continuing to look at the second result until it its bottom portion comes into view.

This finding of non-monotonic attention decay with rank posi-tion may have implications for results ranking and design of a novel discount function (as opposed to MAP or NDCG[16]) that better re-flects user experience in mobile search. We plan to investigate this question in the future work.
Figure 7 shows the attention distribution across all users and con-ditions in our study. The left panel shows a heatmap of gaze activity (note that the red hotspots of increased attention are clearly shifted to the top half of the screen). The right panel shows a distribution of eye fixations as a function of y position. The median fixated y position was 224 pixels which is above the screen center (290 pix-els). Thus, we found that on average, almost 70 % of the users X  attention was focused on the top half of the phone screen, with lit-tle or no attention paid to the bottom 1/3 portion of the screen (only 14%). This trend was consistent on a per user basis (20/24 users showed the preference for top half of the screen). We hypothesize that weighting viewport measurements by this attention distribution may further improve gaze viewport correlations.
We have already shown in the previous section that viewport metrics can signal relevance of answer like results and reflect user X  X  Figure 6: This figure shows that in the presence of short scrolls, the second/third result may be visible across several viewports and for longer compared to the first result.
 Figure 7: Heatmap of where users spent more time looking on the phone is shown on the left (hotspots of attention in red, cool spots indicate lack of attention). Note that the attention distribution ap-pears shifted to the top half of the phone screen. Distribution of fixations along the vertical is shown on the right panel. satisfaction with the search. In this section we investigate whether viewport data can serve for an additional benefit  X  tracking user attention. To this end, we attempt to correlate result viewing time measured with the eye tracking and viewport data. If a reason-ably strong correlation between gaze and viewport time exists, it implies that we can measure user attention at scale from viewport data alone.

We analyze viewing time on per-result basis. We gather all the data collected in the user study independent of experimental con-dition, relevance, result position and result type (traditional web results vs. answer-like results). We hypothesize that viewport time alone might provide a poor proxy for the user X  X  attention, thus, in order to refine our measurements we account for two factors: result coverage and exposure defined below. Let v denote the viewport. We explore different ways of computing viewport time on result as a combination of the time the result was visible on the viewport ( t and two factors: how much of the result area was visible to a user (result exposure , e v ) and how much of the viewport real estate did the result occupy (viewport coverage , c v ). Total viewport time on result using all factors is computed as P n v =1 ( t v  X  c v can take values from [1 , n ] ( n is the number of viewports). Figure 8: This figure shows fairly strong gaze viewport correla-tions. In each panel, the x axis is a gaze measure, and the y axis is the corresponding viewport measure. Left panels show time mea-sures in milliseconds, while right panels show time measures as a % of time on all results on that page.

Table 4 reports the gaze-viewport correlations for combinations of the above factors. We denote the baseline approach computing viewport time = P n v =1 t v as C1. We find that the best combina-tion among C1-C4 is C4 (C2 is close), which is weighted by result exposure and viewport coverage. The scatter plots in Figure 8 are generated using C4.

Figure 8 (top-left panel) shows the scatter plot of viewport time on result vs. gaze time on result, both measured in milliseconds. Each data point in the scatter plot is a (user, query, condition, result) tuple. The correlations are reasonably strong (Pearson X  X  correlation r=0.57; the blue line shows the metric values obtained by binning the data into deciles, binned r = 0.77). Figure 8 (top-right panel) is similar, but shows a scatter plot of percent time on result (time on result / time on page) as measured by gaze (x axis) and viewport (y axis). Interestingly, we found higher correlations using % time on result than absolute time on result in milliseconds (raw correlation: r = 0.69 vs. 0.57; binned correlation: r = 0.97 vs. 0.77). We suspect that the normalization (by time on all results on the page) helps adjust for the pace at which users read the page. For example, some users may quickly glance and skim through the results, while others may read carefully. In such cases, the absolute time measure will vary a lot while the percent time measure may be relatively stable.

Since 3-4 results may be shown on the viewport simultaneously, the observed gaze-viewport correlation on a per-result basis (raw correlation of 0.69, binned correlation in deciles of 0.97) is high and suggests that we may reliably infer how long a specific result was seen by the eye, from the viewport data alone.

The middle and bottom panels in Figure 8 are similar to the top panel, and show gaze viewport correlations for other measures, such as time spent below KG (mid-left panel) and percent time spent below KG (i.e., time below KG / time on all results on the
Formula weight by time below
KG (ms) C1 no no 0.55 0.6 0.71 0.84 0.58 0.79 C2 yes no 0.57 0.69 0.71 0.86 0.59 0.79 C3 no yes 0.53 0.63 0.56 0.82 0.57 0.79 C4 yes yes 0.57 0.69 0.71 0.86 0.59 0.81 page, mid-right panel) measured using gaze (x axis) and viewport (y axis). Here too, we find strong gaze viewport correlations, and again, the % time measures show higher correlations than time in millisecond measures (time below KG: r = 0.71, %time below KG: r = 0.86). The bottom panel in Figure 8 shows correlations for time below IA (r = 0.59) and % time below IA (r = 0.81). In all three figures, we find that the percent time measures, that are normalized by time on page, show higher gaze-viewport correlations than time in millisecond measures, for reasons discussed earlier.
To our knowledge, this is the first quantitative mobile eye track-ing study in search. As more traffic goes mobile, there is a need to better understand user attention and satisfaction on mobile devices. Prior work has focused on search behavior in desktops. These stud-ies report a Golden Triangle [23], where searcher attention is fo-cused near the top-left of the page and decreases as we go down or to the right on the SERP. It is not clear whether attention behavior on desktop will generalize to mobile phones, as they differ from desktops in several ways  X  small real estate, variety of touch inter-actions (touch, scroll, swipe, zoom) and tendency to perform short queries. In this study, we found that indeed, user attention behavior on mobile phones is very different from that on desktops.
First, unlike desktop where engagement (both clicks and atten-tion) has been widely reported to decrease from top to bottom po-sitions [9, 10], on mobile phones, we found surprisingly, that the second result gets more viewport and gaze time than the first. The most likely explanation for this is short scrolls. Unlike desktop where searchers can use the page up/down keys on the keyboard to move from one page fold to the next (no overlap between the results in different page folds), on mobile phones, users tend to perform short and continuous scrolls that render the second and third results visible across more viewports and hence longer than the first. Fig-ure 6 illustrates this with an example. This bias towards the second position occurs in eye gaze too. We think this is because the second result is often partially hidden, and to bring it fully into view, the user has to carefully scroll (to avoid scrolling too much or too little) by continuously looking at the result until it is fully visible, leading to longer gaze time on the second result than the first.

It is possible that in the absence of scrolling, viewport and gaze time on results (in mobile phones) may decrease with position, sim-ilar to desktop. For example, navigational tasks ("BBC") where the user often clicks the first result, may not require scrolling, and may show higher viewport time on the first than second result. In our study, however, all tasks were non-navigational, and often in-volved scrolling. An intriguing question that immediately follows is, whether there is a more appropriate evaluation metric or rank po-sition discount that better reflects user experience on mobile phones than current evaluation metrics, such as mean average precision or discounted cumulative gain.

The second finding which is different on mobile phones than desktop is that, unlike the Golden Triangle in desktop, where at-tention is focused on the top-left and decreases towards the bottom and right of the search result page, in our study on mobile phones, we found that on average, user attention is focused on the center and top half of the screen. This, together with the already strong gaze viewport correlations (r=0.7 for %time on a page element as shown in Fig 8) suggests that by using the appropriate weighting functions on viewport data, we may identify which result the user is looking at, with high confidence. In other words, this offers an opportunity, for the first time, to scalably and reliably measure user attention on mobile phones. Another possible direction for improv-ing accuracy of user attention measurements is to follow the work Huang et al. [14] and Navalpakkam et al. [24] that advocate to directly predict user attention on the screen from user interactions. While the absence of cursor movements in mobile phones makes attention prediction more difficult, we hypothesize that features of smaller screen size and the time user spends in the viewport with-out scrolling can be used to improve the accuracy of the  X  X anilla X  approach that uses viewport time information only.

In addition to understanding searcher attention on mobile phones, we examined search satisfaction and its effect on viewport data. We systematically varied task-relevance (whether the KG/IA con-tained the answer to the user X  X  task), and found that users reported less satisfied when the KG/IA was task-irrelevant than when it was relevant. We also identified viewport metrics that signal user dis-satisfaction with answers  X  increased scrolling down the SERP and increased % time below the answer. We found that when the KG/IA is task-irrelevant, users read through it (expecting to find the an-swer) and upon not finding the answer, they continued to examine results below, leading to increased scrolling down the page, and increased time below KG/IA (in milliseconds, and as a % of page time). These results suggest that we may auto-detect answer satis-faction at scale by using viewport signals.
We acknowledge several limitations of this study. First, we fo-cused on tasks with information seeking search intent and have not explored navigational search intent [3]. In our data we observed 2.51 viewport scrolls performed on average. We expect the amount scrolling activity to be smaller for navigational searches, as often the first result is the destination site (e.g. queries like  X  X BC X  or "Twitter"). In the absence of scrolling, we may find that attention strictly decreases with result position (unlike the bump at position 2 observed in this study).

Second, in this study, we fixed the mobile phone X  X  position by mounting it to the eye tracker X  X  stand. In real life, user X  X  atten-tion on the phone can vary depending on whether s/he is moving or not; whether the user is right handed or left handed or, perhaps interacting with the phone with both hands. Other factors such as demographics can also influence user behavior. For example, de-pending on the user X  X  language s/he may read information on the phone from left to right or vice-versa. Age and experience with touch interfaces is widely recognized throughout research commu-nity as an important factor in touch interactions, thus can affect user attention/search behavior.
Third, examination habits on the mobile device may vary across users, as noted by [2]. While Figure 7 already shows a clear pattern -most users prefer focusing on the top half of the phone screen, it is possible that a few users may prefer the center or bottom of the screen. In our future work we plan to address this limitation by exploring the possibility of adaptively weighting user attention based on current user actions, e.g. direction of page examination (upward or downward).

Despite these limitations, our study offers the hope of accurately measuring user eye-fixated result at scale on mobile phones. Future work will consider tablets (this study focused on mobile phones) and other devices, satisfaction with clickable results (including ads), and explore diverse user settings such as users who are moving or multi-tasking.
We demonstrated, for the first time, that by tracking the browser viewport (visible portion of the page), one can develop viewport metrics that are strongly correlated with user attention (eye gaze) and search satisfaction on mobile phones. Focusing on answer-like results, in a controlled lab study, we found that increased scrolling past answer and increased time below answer can signal user dis-satisfaction with answer results. We demonstrated strong gaze-viewport correlations on a per-result basis, and found that attention (on average) is focused on the top half of the phone, suggesting that we may infer the amount of attention received by a specific result (of 3-4 results shown in the viewport) scalably and reliably using viewport data alone. Potential applications of this work include better estimation of result relevance and satisfaction in search, and could benefit other areas including advertising, web page design and optimization, and measuring engagement in social networking sites. [1] A. Aula, P. Majaranta, and K.-J. R X ih X . Eye-tracking reveals [2] R. Biedert, A. Dengel, G. Buscher, and A. Vartan. Reading [3] A. Broder. A taxonomy of web search. In ACM SIGIR [4] G. Buscher, S. T. Dumais, and E. Cutrell. The good, the bad, [5] B. Carterette and R. Jones. Evaluating search engines by [6] O. Chapelle and Y. Zhang. A dynamic bayesian network [7] A. T. Duchowski. Eye tracking methodology: Theory and [8] S. T. Dumais, G. Buscher, and E. Cutrell. Individual [9] L. A. Granka, T. Joachims, and G. Gay. Eye-tracking [10] Z. Guan and E. Cutrell. An eye tracking study of the effect of [11] Q. Guo, H. Jin, D. Lagun, S. Yuan, and E. Agichtein. Mining [12] Q. Guo, S. Yuan, and E. Agichtein. Detecting success in [13] J. Huang and A. Diriye. Web user interaction mining from [14] J. Huang, R. White, and G. Buscher. User see, user point: [15] J. Huang, R. W. White, G. Buscher, and K. Wang. Improving [16] K. J X rvelin and J. Kek X l X inen. Cumulated gain-based [17] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. [18] M. A. Just and P. A. Carpenter. A theory of reading: From [19] M. Kamvar and S. Baluja. A large scale study of wireless [20] J. Kim, P. Thomas, R. Sankaranarayana, and T. Gedeon. [21] D. Lagun and E. Agichtein. Viewser: enabling large-scale [22] D. Lagun and E. Agichtein. Re-examining search result [23] L. Lorigo, M. Haridasan, H. Brynjarsd X ttir, L. Xia, [24] V. Navalpakkam, L. Jentzsch, R. Sayres, S. Ravi, A. Ahmed, [25] RKG Digital Marketing Report, Q2 2013. Retrieved from [26] Statcounter Global Stats. Retrieved from [27] Tobii X Series Eye Trackers Product Description. Retrieved
