
The mission of DOLAP is being the premier forum where both researchers and practitioners can share their findings in theoretical foundations, current methodologies, practical experiences, and new research directions in the areas of data ware-housing and on-line analytical processing. This special issue presents a selection of the best papers DOLAP X 08, the 11th ACM International Workshop on Data Warehousing and OLAP, held on October 30, 2008 in Napa, California (USA) in conjunction with CIKM X 08, the ACM 17th Conference on Information and Knowledge Management.

Continuing its tradition, in its sessions, papers were presented that explored novel research directions and emerging application domains in the areas of data warehousing and OLAP, and identified new directions for future research and devel-opment. DOLAP gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of data warehousing.

In this edition, the call for papers attracted 29 submissions from 16 different countries. The program committee accepted 12 papers yielding an acceptance rate of 41.4%. These papers were grouped into four different sessions: performance opti-mization, multidimensional queries, multidimensional design and ETL, and tools for data warehousing and OLAP. Among those 12 papers, 5 were accepted for this special issue after two rounds or reviews: two from the performance and optimi-zation session, another one from the querying session and the other two from the session on design and ETL.

The first paper,  X  X  X orting improves word-aligned bitmap indexes X , by Daniel Lemire, Owen Kaser and Kamel Aouiche, pre-sents a new approach to improve compression techniques for bitmap indexes by reducing I/O costs and minimizing CPU usage. To accelerate logical operations (AND, OR, XOR) over bitmaps, techniques based on run-length encoding (RLE), such as word-aligned hybrid (WAH) compression, are used. These techniques are sensitive to the order of the rows. They pre-sented reordering heuristics based on computed attribute-value histograms. Simply permuting the columns of the table based on these histograms can increase the sorting efficiency by 40%.

The second paper,  X  X  X ransaction reordering X , by Gang Luo, Jeffrey F. Naughton, Curt J. Ellmann and Michael W. Watzke, shows how to extend the existing transaction reordering method into a general transaction reordering framework that can incorporate various factors as the reordering criteria. Transaction reordering method reorders the transaction sequence submitted to the RDBMS and improves the transaction throughput by considering both the current system status and infor-mation about the interaction between queued and running transactions. The existing transaction reordering method only considers the reordering opportunities provided by analyzing the lock conflict information among multiple transactions. In this work, it was shown that by analyzing the resource utilization information of transactions, the transaction reordering method can also improve the system throughput by increasing the resource sharing opportunities among multiple transac-tions. A concrete example on synchronized scans was provided and demonstrated the advantages of this method through experiments with a commercial parallel RDBMS.
 The third paper,  X  X  X mproving estimation accuracy of aggregate queries on data cubes X , by Elaheh Pourabbas and Arie Shoshani, investigates the problem of estimation of a target database from summary databases derived from a base data cube. They showed that such estimates can be derived by choosing a primary database which uses a proxy database to esti-mate the results. This technique is common in statistics, but an important issue they addressed is the accuracy of these esti-mates. Specifically, given multiple primary and multiple proxy databases sharing the same summary measure, the problem is how to select the primary and proxy databases that will generate the most accurate target database estimation possible. They proposed an algorithmic approach for determining the steps to select or compute the source databases from multiple summary databases, which made use of the principles of information entropy. They showed that the source databases with the largest number of cells in common provide the more accurate estimates. They proved that this is consistent with max-imizing the entropy and provided some experimental results on the accuracy of the target database estimation in order to verify their results.

The fourth paper,  X  X  X xtended aggregations for databases with referential integrity issues X , by Javier Garc X   X  a-Garc X   X  a and Car-los Ordonez, extends aggregate functions computed over tables with referential integrity errors on OLAP databases to return complete answer sets in the sense that no tuple is excluded. The probability that an invalid reference may actually be a cer-tain correct reference was associated to each valid reference. In a data warehouse environment, even though the ETL pro-cesses take care of the referential integrity errors, in many scenarios, referential integrity is generally guaranteed by includ-ing  X  X ummy X  records in the dimension tables used to relate to the fact tables with referential errors. When two tables are joined and aggregations are computed, the tuples with an undefined foreign key value are aggregated in a group marked as undefined, effectively discarding potentially valuable information. The main idea of this work was that in certain contexts, it is possible to use tuples with invalid references by taking into account the probability that an invalid reference actually be a certain correct reference. This method obtained improved answer sets from aggregate queries in settings where a database violates referential integrity constraints.

Simitsis, Dimitrios Skoutas and Mal X  Castellanos, builds upon their previous results (on the automation of the conceptual design of ETL leveraging Semantic Web technology) and tackles the need of technical knowledge from the business people to understand the ETL processes by investigating the application of natural language generation techniques to the ETL envi-ronment. In particular, they provided a method for the representation of a conceptual ETL design as a narrative, which is the most natural means of communication and does not require knowledge of any specific model. They discussed how linguistic techniques can be used for the establishment of a common application vocabulary. Finally, they presented a flexible and cus-tomizable template-based mechanism for generating natural language representations for the ETL process requirements and operations.
 the reviewers for their detailed and valuable comments on the extra review process that helped the authors to significantly improve their papers.

