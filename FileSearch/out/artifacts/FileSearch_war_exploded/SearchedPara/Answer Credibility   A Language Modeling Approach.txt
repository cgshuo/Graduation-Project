 In recent years, Answer Validation has become a topic of significant interest within the Question Answering community. In the general case, one can describe Answer Validation as the process that decides whether a Question is correctly answered by an Answer according to a given segment of sup-porting Text. Magnini et al. (Magnini, 2002) pre-sents an approach to Answer Validation that uses redundant information sources on the Web; they propose that the number of Web documents in which the question and the answer co-occurred can serve as an indicator of answer validity. Other re -cent approaches to Answer Validation Exercise in the Cross-Language Evaluation Forum (CLEF) (Peters, 2008) make use of textual entailment methodologies for the purposes of Answer Valida-tion. In this paper, we propose the use of language mod-eling methodologies for Answer Validation, using corpus-based methods that do not require the use of external sources. Specifically, we propose the development of an Answer Credibility score which quantifies reliability of a source document that contains a candidate answer with respect to the Question X  X  Context Model. Unlike many textual entailment methods, our methodology has the ad-vantage of being applicable to question types for which hypothesis generation is not easily accom-plished. The remainder of this paper describes our work in progress, including our model for Answer Credi-bility, our experiments and results to date, and fu -ture work. Credibility has been extensively studied in the fie ld of information science (Metzger, 2002). Credibil-ity in the computational sciences has been charac-terized as being synonymous with believability, and has been broken down into the dimensions of trustworthiness and expertise. Our mathematical model of Answer Credibility attempts to quantify the reliability of a source us -ing the semantic Question Context. The semantic Question Context is built using the Aspect-Based Relevance Language Model that was presented in (Banerjee, 2008) and (Banerjee, 2009). This model builds upon the Relevance Based Language Model (Lavrenko, 2001) and Probabilisitic Latent Seman-tic Analysis (PLSA) (Hofmann, 1999) to provide a mechanism for relating sense disambiguated Con-cept Terms (CT) to a query by their likelihood of relevance. The Aspect-Based Relevance Language Model assumes that for every question there exists an un-derlying relevance model R, which is assigned probabilities P(z|R) where z is a latent aspect of the information need, as defined by PLSA. Thus, we can obtain a distribution of aspects according to their likelihood of relevancy to the user X  X  informa -tion need. By considering terms from the aspects that have the highest likelihood of relevance (eg. highest P(z|R) values), we can build a distribution that models a semantic Question Context. We define Answer Credibility to be a similarity measure between the Question Context (QC) and the source document from which the answer was derived. We consider the Question Context to be a document, which has a corresponding document language model. We then use the well-known Kullback-Leibler divergence method (Lafferty, 2001) to compute the similarity between the Ques-tion Context document model and the document model for a document containing a candidate an-swer: Here, P(w|QC) is the language model of the Ques-tion Context, P(w|d) is the language model o the document containing the candidate answer. To insert this model into the Answer Validation proc-ess, we propose an interpolation technique that modulates the answer score during the process us-ing Answer Credibility. The experimental methodology we used is shown as a block diagram in Figure 1. To validate our approach, we used the set of all factoid questions from the Text Retrieval Conference (TREC) 2006 Question Answering Track (Voorhees, 2006). The OpenEphyra Question Answering testbed (Schlaefer, 2006) was then used as the framework for our Answer Credibility implementation. OpenEphyra uses a baseline Answer Validation mechanism which uses documents retrieved using Yahoo! search to support candidate answers found in retrieved passages. In our experiments, we con-structed the Question Context according to the methodology described in (Banerjee, 2008). Our experiments used the Lemur Language Modeling toolkit (Strohman, 2005) and the Indri search en-gine (Ogilvie, 2001) to construct the Question Context and document language models. We then inserted an Answer Credibility filter into the OpenEphyra processing pipeline which modu-lates the OpenEphyra answer score according to the following formula: Here score is the original OpenEphyra answer score and score' is the modulated answer score. In this model,  X  is an interpolation constant which we set using the average of the P(z|R) values for those aspects that are included in the Question Context. For the purposes of evaluating the effectiveness of our theoretical model, we use the accuracy and Mean Reciprocal Rank (MRR) metrics (Voorhees, 2005). We compare the results of the baseline OpenEphyra Answer Validation approach against the results after our Answer Credibility processing has been included as a part of the OpenEphyra pipeline. Our results are presented in Table 1 and Table 2. To facilitate interpretation of our results, we sub -divided the set of factoid questions into categorie s by their question words, following the example of (Murdock, 2006). The light grey shaded cells in both tables indicate categories for which improve-ments were observed after our Answer Credibility model was applied. The dark grey shaded cells in both tables indicate categories for which no change was observed. The paired Wilcoxon signed rank test was used to measure significance in improve-ments for MRR; the shaded cells in Table 2 indi-cate results for which the results were significant (p&lt;0.05). Due to the binary results for accuracy a t the question level (eg. a question is either correc t or incorrect), the Wilcoxon test was found to be inappropriate for measuring statistical significanc e in accuracy. Our results show the following:  X  A 5% improvement in accuracy over the base- X  An overall improvement of 13% in accuracy  X  A 9% improvements in MRR for  X  X hat X  type  X  An overall improvement of 25% in MRR for  X  Overall, 7 out of 13 categories (58%) per-In this section, we examine some examples of questions that showed improvement to better un-derstand and interpret our results. First, we examine a  X  X ho X  type question which was not correctly answered by the baseline system, but which was correctly answered after including Answer Credibility. For the question  X  X ho is the host of the Daily Show? X  the baseline system cor-rectly determined the answer was  X  X on Stewart X  but incorrectly identified the document that this answer was derived from. For this question, the Question Context included the terms  X  X tewart, X   X  X omedy, X   X  X elevision, X   X  X ews, X  and  X  X ilborn. X  (Craig Kilborn was the host of Daily Show until 1999, which makes his name a logical candidate for inclusion in the Question Context since the AQUAINT corpus spans 1996-2000). In this case, the correct document that the answer was derived from was actually ranked third in the list. The An -swer Credibility filter was able to correctly in-crease the answer score of that document so that it was ranked as the most reliable source for the an-swer and chosen as the correct final result. Next, we consider a case where the correct answer was ranked at a lower position in the answer list i n the baseline results and correctly raised higher, though not to the top rank, after the application o f our Answer Credibility filter. For the question  X  X hat position did Janet Reno assume in 1993? X  the correct answer ( X  X ttorney general X ) was ranked 5 in the list in the baseline results. However, in this case the score associated with the answer was lower than the top-ranked answer by an order of magnitude. The Question Context for this question included the terms  X  X iami, X   X  X lian, X   X  X onzales, X   X  X oy, X   X  X ttorney X  and  X  X ustice. X  After the applica -tion of our Answer Credibility filter, the score an d rank of the correct answer did increase (which con-tributed to an increase in MRR), but the increase was not enough to overshoot the original top-ranked answer. Categories for which the Answer Credibility had negative effect included  X  X ow much X  and  X  X ow many X  questions. For these question types, the correct answer or correct document was frequently not present in the answer list. In this case, the An-swer Credibility filter had no opportunity to in-crease the rank of correct answers or correct documents in the answer list. This same reasoning also limits our applicability to questions that re-quire a date in response. Finally, it is important to note here that the very nature of news data makes our methodology appli-cable to some categories of questions more than others. Since our methodology relies on the abilit y to derive semantic relationships via a statistical examination of text, it performs best on those ques -tions for which some amount of supporting infor-mation is available. In conclusion, we have presented a work in pro-gress that uses statistical language modeling meth-ods to create a novel measure called Answer Credibility for the purpose of Answer Validation. Our results show performance increases in both accuracy and MRR for  X  X hat X  and  X  X ho X  type questions when Answer Credibility is included as a part of the Answer Validation process. Our goals for the future include further development of the Answer Credibility model to include not only terms from a Question Context, but terms that can be deduced to be in an Answer Context. 
