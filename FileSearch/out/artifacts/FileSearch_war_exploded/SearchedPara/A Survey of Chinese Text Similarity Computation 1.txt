 Text similarity computation plays important roles in the fields of duplicate detection, contexts [1]. 
Chinese is more difficult to understand by computers than Western language, such as English. English is a merplotactic language focusing on syntax, while Chinese is a semotactic language focusing on semantics. Since Chinese texts do not have a natural delimiter between words, Chinese word segmentation and feature vector spaces in high-level dimensions are expensive. Moreover, difficulties of Chinese text similarity computation also lie in extraction of keywords, processing of synonymies, polysemies and combination among concepts, distingui shing commendatory and derogatory re-marks, etc. For example, sentence 1  X   X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X   X  and sentence 2  X   X   X   X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  12  X   X  are very different in expression, but much related in content. All these characteristics of Chinese language cause problems in efficiency and effectiveness, which are essential for evaluating a similarity algorithm of Chinese text similarity computation. 
In this paper, we provide a survey of Chinese text similarity computation, and the advantages and disadvantages of both measures based on statistics and semantics understanding are evaluated. 2.1 Measures Based on VSM The Vector Space Model (VSM) [2] [3] is an algebraic model widely used not only for Western languages but also for Chinese document similarity computation. It real-valued component, which is computed using the TF-IDF (Term Frequency-Inverted Document Frequency) weighting scheme for each term (keywords) in a multi-dimensional linear space. The set of documents in a collection then turns into a vector space, with one axis for each term. The set of terms is a predefined collection of terms, occurring in the document corpus. To compensate for the effect of different document/query length, the standard way of quantifying the similarity between a query/document vector and a document vector is to compute the cosine similarity coefficient between them, which reflects the degree of similarity in the corresponding terms and term weights. 
The IF-IDF algorithm based on VSM has some disadvantages. On the one hand, it can achieve good effectiveness only if enough number of words are included. On the other hand, only the statistic of words in text is considered and all the structural and pendent (orthogonal) to each other, which is incorrect regarding natural language that causes problems with synonyms or strong related terms. Meanwhile, messages have to pass through a stopword-list, stemming and thesaurus-algorithms before they are forwarded to the VSM.

Generalized Vector Space Model (GVSM) [3 ] [4], an improved VSM, assigns a document vector to each one without the assumption of orthogonal terms. For the GVSM, term-angles are based on the computation of co-occurrence of terms. Jorg Becker [5] explored a topic-based vector space model (TVSM), which does not as-sume independence between terms and is flexible regarding the specification of term-similarities. Stemming and thesaurus can be fully integrated into the model. Cheng [6] established a component frequency model (CFM) based on components, in which Chinese texts were expressed in the aspect of component granularities. The text at-frequency. This measure has higher precision and recall than algorithms based on keywords. The most advantage of CFM is that it avoids difficulties in Chinese word segmentation. 2.2 Measures Based on Attribute Theory and on Hamming Distance Pan [7] analyzed the relationship between textual attributes and the attribute barycen-ter coordinate model, and established a text attribute barycenter coordinate model, in three-dimension Cartesian coordinates, and based on attribute theory. A text vector texts and the queries was elicited. This measure could express more semantic infor-mation than VSM and improve precision and recall. However, difficulties in auto-matic Chinese word segmentation cannot be avoided.
Hamming distance, named after Richard Hamming, is the number of positions in other words, it measures the number of substitutions required to change one into the other. For example, the Hamming distance between 1011101 and 1001001 is 2. Zhang [8] presented a Chinese text similarity algorithm based on the theory of Hamming computation, not like many modern genomics algorithms taking gaps into considera-tion. We cannot simply see how many characters are different because some differ-ences are more significant than others. With more effort, inter-document similarity can be measured in more sophisticated ways, in which semantics are considered. 3.1 Words Similarity Measures Ontology, which usually uses a thesaurus, is one of approaches to compute the dis-tance between words. Early in 1995, Agirre E. [9] gave a. a proposal for word sense disambiguation using conceptual distance based on WordNet. Wang [10] computed similarity algorithm based on HowNet. Suppose there are two Chinese words and
Primitive is a minimal meaning unit of describing a concept. HowNet describes Here d is the distance between 1 P and 2 P , and  X  is a smoothing parameter, which is the distance when the similarity equals to 0.5. 
Xia [12] proposed a method based on HowNet, geared to semantic and could be expanded. The proposed method defined a similarity computation formula among HowNet X  X  sememes according to information theory, finding a way out of the diffi-culty that out-of-vocabulary (OOV) words cannot participate in semantic computation by implementing concept segmentation and automatic semantic production to OOV words, and realized the similarity computation on the semantic level among arbitrary words. Although semantic lexicon based on structure relations between conceptions is simple and effective, building a Chinese semantic lexicon is a large-scale system project.

Individual Chinese characters, bi-grams, n-grams (n  X  2), and words are the most widely used indexing units. Kwok [13] concluded that single character indexing is good but not sufficiently competitive, while bi-gram indexing works surprisingly well and it is as precise as short-word indexing. 
Zhao [14] proposed a measure based on Chinese Character Association Measure-ment (CCAM) matrix. To reduce the higher complexity, using the association of Chi-nese characters for text similarity analysis was probed with feature words. CCAM is a better solution than bi-gram and keywords indexing. Since word segmentation is not needed, the algorithm is useful in massive Chinese data corpus. 3.2 Sentence Similarity Measures Che [15] improved the original edit-distance approach by computing Chinese sen-tence semantic similarity with more information in structure. Both HowNet and  X  X ilin X  thesauruses were used as the semantic resource to compute the semantic simi-larity between two words. Jing [16] proposed a model of semantic-based text formal-three demensions: domain, situation and background. Based on the context frame-object, computed synonymies, polysemies, combinations of concepts, and distinguish-ing commendatory and derogatory remarks. The algorithm can improve the efficiency of text filtering practically. 3.3 Paragraphs and Documents Similarity Measures Jin [17] proposed a set of textural similarity algorithm to study paragraphs similarity. HowNet was used to compute words similarity. Only substantives such as noun, verb, adjective, numeral, measure word and pronoun, were extracted, it could avoid the complicated Chinese syntax analysis. Suppose there are two paragraph texts: The similarity between paragraphs Afterward, Jin [18] proposed a set of document-structure-based algorithms to detect plagiarism of Chinese academic articles, with the help of document-structure analysis, fingerprinting and word-frequency techniques. 
Compared with measures based on statistics, measures based on semantic under-usually much more expensive. For Chinese information retrieval, Chinese natural language processing, and many other tasks, a measure for text similarity is a key element of it. evaluate inter-document similarity. All those measures can be divided into two cate-gories: statistics-based and semantics-based. Compared with each other, they have corpus to achieve reasonably good performance since contextual information is not 596 X. Wang, S. Ju, and S. Wu using semantics-based measures is more expensive on computational cost than statis-tics-based measures, though the former can often lead to more accurate judgment than the latter. 
Up to now, how to find a Chinese text similarity measure which can be performed effectively and efficiently remains to be a challenging issue. In our opinion, language direction. In order to achieve this, some measures such as information-theorectic measures [19, 20] and corresponging algorithms are desirable. Acknowledgments. This work was financially supported by the Natural Science Foundation of Jiangsu Province (grant No. BK2006073). 1. McGill, M., Koll, M., Norrreault, T.: An Evaluation of Factors Affecting Document Rank-2. Lesk, M.E.: Computer Evaluation of Indexing and Text Processing. Journal of the ACM 1, 3. Beaza-Yates, R., Ribeiro-Neto, B.: Modern Information Retrieval. ACM Press, New York 4. Wong, S.: On Modeling of Information Retrieval Concepts in Vector Spaces. ACM Trans-5. Becker, J., Kuropka, D.: Topic-based Vector Space Model. Business Information Systems. 6. Cheng, Y., Wu, S.: Text Similarity Computing Based on Components. Computer Engi-7. Pan, Q., Wang, J., Shi, Z.: Text Similarity Computing Based on Attribute Theory. Chinese 8. Zhang, H., Wang, G., Zhong, Y.: Text Similarity Computing Based on Hamming Distance. 9. Agirre, E., Rigau, G.: A Proposal for Word Sense Disambiguation Using Conceptual Dis-10. Wang, B.: Study on Chinese-English Bi-language Corpus Automatic Ordering. Institute of 14. Zhao, Y., Li, Q.: Chinese Character Association Measurement Method and Its Application 16. Jin, Y.: Text Similarity Computing Based on Context Framework Model. Computer Engi-17. Jin, B., Shi, Y., Teng, H.: Similarity Algorithm of Text Based on Semantic Understanding. 18. Jin, B., Shi, Y., Teng, H.: Document-structure-based Copy Detection Algorithm. Journal of 19. Javed, A., Aslam, M.F.: An Information-theoretic Measure for Document Similarity. ACM 
