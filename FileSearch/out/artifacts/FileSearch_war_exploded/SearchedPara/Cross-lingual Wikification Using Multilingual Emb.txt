 Wikipedia has become an indispensable resource in knowledge acquisition and text understanding for both human beings and computers. The task of Wikification or Entity Linking aims at disambiguat-ing mentions (sub-strings) in text to the correspond-ing titles (entries) in Wikipedia or other Knowledge Bases, such as FreeBase. For English text, this problem has been studied extensively (Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Ratinov et al., 2011; Cheng and Roth, 2013). It also has been shown to be a valuable component of several natural language processing and information extraction tasks across different domains.

Recently, there has also been interest in the cross-lingual setting of Wikification: given a mention from a document written in a foreign language, the goal is to find the corresponding title in the English Wikipedia. This task is driven partly by the fact that a lot of information around the world may be written in a foreign language for which there are limited lin-guistic resources and, specifically, no English trans-lation technology. Instead of translating the whole document to English, grounding the important entity mentions in the English Wikipedia may be a good solution that could better capture the key message of the text, especially if it can be reliably achieved with fewer resources than those needed to develop a translation system. This task is mainly driven by the Text Analysis Conference (TAC) Knowledge Base Population (KBP) Entity Linking Tracks (Ji et al., 2012; Ji et al., 2015; Ji et al., 2016), where the target languages are Spanish and Chinese.

In this paper, we develop a general technique which can be applied to all languages in Wikipedia even when no machine translation technology is available for them.

The challenges in Wikification are due both to ambiguity and variability in expressing entities and concepts: a given mention in text, e.g., Chicago, may refer to different titles in Wikipedia (Chicago Bulls, the City, Chicago Bears, the band, etc.), and a title can be expressed in the text in multiple ways, such as synonyms and nicknames. These challenges are usually resolved by calculating some similarity between the representation of the mention and can-didate titles. For instance, the mention could be rep-resented using its neighboring words, whereas a ti-tle is usually represented by the words and entities in the document which introduces the title. In the cross-lingual setting, an additional challenge arises from the need to match words in a foreign language to an English title.

In this paper, we address this problem by using multilingual title and word embeddings. We repre-sent words and Wikipedia titles in both the foreign language and in English in the same continuous vec-tor space, which allows us to compute meaningful similarity between mentions in the foreign language and titles in English. We show that learning these embeddings only requires Wikipedia documents and language links between the titles across different languages, which are quite common in Wikipedia. Therefore, we can learn embeddings for all lan-guages in Wikipedia without any additional anno-tation or supervision.

Another notable challenge for the cross-lingual setting that we do not address in this paper is that of generating English candidate titles given a foreign mention when there is no corresponding title in the foreign language Wikipedia. If a title exists in both the English and the foreign language Wikipedia, there could be examples of using this title in the foreign language Wikipedia text, and this informa-tion could help us determine the possible English ti-tles. For example, Vladimir N. Vapnik exists in both the English Wikipedia ( en/Vladimir Vapnik ) 1 and the Chinese Wikipedia ( zh/  X  ...  X  s  X   X  n &lt; ). In the Chinese Wikipedia, we may see the use of the mention , n &lt; K as a reference, that is, , n &lt; K is linked to the title zh/  X  ...  X  s  X   X  n &lt; K . Fol-lowing the inter-language links in Wikipedia, we can reach the English title en/Vladimir Vapnik . On the other hand, Dan Roth does not have a page in the Chinese Wikipedia, it would have been harder to get to en/Dan Roth from the Chinese mention. In this case, a transliteration model may be needed. Note that the difference between these two cases is only in generating English title candidates from the given foreign mention. The disambiguation method which identifies the most probable title is conceptu-ally the same, so our method could generalize as is to this case.
For evaluation purposes, we focus in this paper on mentions that have corresponding titles in both the English and the foreign language Wikipedia, and concentrate on disambiguating titles across lan-guages. This allows us to evaluate on a large number of Wikipedia documents. Note that under this set-ting, a natural approach is to do wikification on the foreign language and then follow the language links to obtain the corresponding English titles. However, this approach requires developing a separate wiki-fier for each foreign language if it uses language-specific features, while our approach is generic and only requires using the appropriate embeddings. Im-portantly, the aforementioned approach will also not generalize to the cases where the target titles only exist in the English Wikipedia while ours does.
We create a challenging Wikipedia dataset for 12 foreign languages and show that the proposed ap-proach, WikiME ( Wiki fication using M ultilingual E mbeddings), consistently outperforms various baselines. Moreover, the results on the TAC KBP2015 Entity Linking dataset show that our ap-proach compares favorably with the best Spanish system and the best Chinese system despite using significantly weaker resources (no need for transla-tion). We note that the need for translation would have prevented the wikification of 12 languages used in this paper. We formalize the problem as follows. We are given a document d in a foreign language, a set of men-tions M d = { m 1 ,  X  X  X  ,m n } in d , and the English Wikipedia. For each mention in the document, the goal is to retrieve the English Wikipedia title that the mention refers to. If the corresponding entity or con-cept does not exist in the English Wikipedia,  X  X IL X  should be the answer.

Given a mention m 2 M d , the first step is to gen-erate a set of title candidates C m . The goal of this step is to quickly produce a short list of titles which includes the correct answer. We only look at the sur-face form of the mention in this step, that is, no con-textual information is used.

The second and the key is the ranking step where we calculate a score for each title candidate c 2 C m , which indicates how relevant it is to the given men-tion. We represent the mention using various con-textual clues and compute several similarity scores between the mention and the English title candidates based on multilingual word and title embeddings. A ranking model learnt from Wikipedia documents is used to combine these similarity scores and output the final score for each title candidate. We then se-lect the candidate with the highest score as the an-swer, or output NIL if there is no appropriate candi-date.

The rest of paper is structured as follows. Sec-tion 3 introduces our approach of generating multi-lingual word and title embeddings for all languages in Wikipedia. Section 4 presents the proposed cross-lingual wikification model which is based on multi-lingual embeddings. Evaluations and analyses are presented in Section 5. Section 6 discusses related work. Finally, Section 7 concludes the paper. In this section, we describe how we generate a vector representation for each word and Wikipedia title in any language. 3.1 Monolingual Embeddings The first step is to train monolingual embeddings for each language separately. We adopt the  X  X lignment by Wikipedia Anchors X  model proposed in Wang et al. (2014). For each language, we take all docu-ments in Wikipedia and replace the hyperlinked text with the corresponding Wikipedia title. For exam-ple, consider the following Wikipedia sentence:  X  X t is led by and mainly composed of Sunni Arabs from Iraq and Syria . X , where the three bold faced men-tions are linked to some Wikipedia titles. We re-place those mentions and the sentence becomes  X  X t is led by and mainly composed of en/Sunni Islam Arabs from en/Iraq and en/Syria . X  We then learn the skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b) on this newly generated text. Since a title appears as a token in the transformed text, we will obtain an embedding for each word and title from the model.

The skip-gram model maximizes the following objective:
X where w is the target token (word or title), c is a con-text token within a window of w , v w is the target embedding represents w , v 0 c is the embedding of c in context, D is the set of training documents, and D 0 contains the sampled token pairs which serve as neg-ative examples. This objective is maximized with respect to variables v w  X  X  and v 0 w  X  X . In this model, tokens in the context are used to predict the target token. The token pairs in the training documents are positive examples, and the randomly sampled pairs are negative examples. 3.2 Multilingual Embeddings After getting monolingual embeddings, we adopt the model proposed in Faruqui and Dyer (2014) to project the embeddings of a foreign language and English to the same space. The requirement of this model is a dictionary which maps the words in En-glish to the words in the foreign language. Note that there is no need to have this mapping for every word. The aligned words are used to learn the projection matrices, and the matrices can later be applied to the embeddings of each word to obtain the enhanced new embeddings. Faruqui and Dyer (2014) obtain this dictionary by picking the most frequent trans-lated word from a parallel corpus. However, there is a limited or no parallel corpus for many languages. Since our monolingual embedding model consists also of title embeddings, we can use the Wikipedia title alignments between two languages as the dic-tionary.
 trices containing the embeddings of the aligned En-glish and foreign language titles, where a is the num-ber of aligned titles and k 1 and k 2 are the dimen-sionality of English embeddings and foreign lan-guage embeddings respectively (i.e., each row is a title embedding). Canonical correlation analysis (CCA) (Hotelling, 1936) is applied to these two ma-trices: jection matrices for English and foreign language embeddings, and d is the dimensionality of the pro-jected vectors, which is a parameter in CCA.
Let E en 2 R n 1  X  k 1 be the matrix containing the monolingual embeddings for all words and titles in English, where the number of words and titles is n 1 , We obtain the multilingual embeddings of English words and titles by Similarly, the multilingual embeddings of the for-eign words and titles are stored in the rows of where there are n 2 words and titles in the foreign sentations of words and titles that we use to create the similarity features in the ranker.

Faruqui and Dyer (2014) show that the multi-lingual embeddings perform better than monolin-gual embeddings on various English word similarity datasets. Since synonyms in English may be trans-lated into the same word in a foreign language, the CCA model could bring the synonyms in English closer in the embedding space. In this paper, we further show that projecting the embeddings of the two languages into the same space helps us com-puting better similarity between the words and titles across languages and that a bilingual dictionary con-sisting of pairs of Wikipedia titles is sufficient to in-duce these embeddings. We now describe the algorithm for finding the En-glish title given a foreign mention. 4.1 Candidate Generation Given a mention m , the first step is to select a set of English title candidates C m , a subset of all ti-tles in the English Wikipedia. Ideally the correct ti-tle is included in this set. The goal is to produce a manageable number of candidates so that a more so-phisticated algorithm can be applied to disambiguate them.
 Since we focus on the titles in the intersection of English and the foreign language Wikipedia, we can build indices from the anchor texts in the foreign lan-guage Wikipedia. More specifically, we create two dictionaries and apply a two-step approach. The first dictionary maps each hyperlinked mention string in the text to the corresponding English titles. We sim-ply lookup this dictionary by using the query men-tion m to retrieve all possible titles. The title candi-dates are initially sorted by Pr ( title | mention ) , the fraction of times the title is the target page of the given mention. This probability is estimated from all Wikipedia documents. The top k title candidates are then returned.

If the first high-precision dictionary fails to gen-erate any candidate, we then lookup the second dic-tionary. We break each hyperlinked mention string into tokens, and create a dictionary which maps to-kens to English titles. The tokens of m are used to query this dictionary. Similarly, the candidates are sorted by Pr ( title | token ) and the top k candidates are returned. 4.2 Candidate Ranking Given a mention m and a set of title candidates C m , we compute a score for each title in C m which indi-cates how relevant the title is to m . For a candidate c 2 C m , we define the relevance as: a weighted sum of the features, i , which are based on multilingual title and word embeddings. We rep-resent the mention m by the following contextual clues and use these representation to compute fea-ture values:  X  context j ( m ) : use the tokens within j charac- X  other -mentions ( m ) : a set of vectors that rep- X  previous -titles ( m ) : a set of vectors that rep-Let e ( c ) be the English embedding of the title can-didate c . The features used in Eq. (1) are shown in Table 1. We train a linear ranking SVM model with the proposed features to obtain the weights, w i , in Eq. (1). Finally, the title which has the highest relevant score is chosen as the answer to m . We evaluate the proposed method on the Wikipedia dataset of 12 langugaes and the TAC X 15 Entity Link-ing dataset.

For all experiments, we use the Word2Vec imple-with dimensionality 500 for each language. The CCA code for projecting mono-lingual embeddings parameter is set to 0.5 (i.e., the resulting multilingual embeddings have dimensionality 250).

We use Stanford Word Segmenter (Chang et al., 2008) for tokenizing Chinese, and use the Java built-in BreakIterator for Thai. For all other languages, tokenization is based on whitespaces. The number of tokens we use to learn the skip-gram model and the number of title alignments used by the CCA are given in Table 2. For learning the weights in Eq. (1), we use the implementation of linear ranking SVM in Lee and Lin (2014). Parameter selection and feature engineering are done by conducting cross-validation on the training data of Spanish Wikipedia dataset. 5.1 Wikipedia Dataset We create this dataset from the documents in Wikipedia by taking the anchors (hyperlinked texts) as the query mentions and the corresponding English Wikipedia titles as the answers. Note that we only keep the mentions for which we can get the corre-sponding English Wikipedia titles by the language links. As observed in previous work (Ratinov et al., 2011), most of the mentions in Wikipedia docu-ments are easy, that is, the baseline of simply choos-ing the title that maximizes Pr ( title | mention ) , the most frequent title given the mention surface string, performs quite well. In order to create a more chal-lenging dataset, we randomly select mentions such that the number of easy mentions is about twice the number of hard mentions (those mentions for which the most common title is not the correct title). This generation process is inspired by (and close to) the distribution generated in the TAC KBP2015 Entity Linking Track. Another problem that occurs when creating a dataset from Wikipedia documents is that even though training documents are different from test documents, many mentions and titles actually overlap. To test that the algorithms really general-ize from training examples, we ensure that no (men-tion, title) pair in the test set appear in the training set. Table 3 shows the number of training men-tions, test mentions, and hard mentions in the test set of each language. This dataset is publicly avail-able at http://bilbo.cs.illinois.edu/  X  ctsai12/xlwikifier-wikidata.zip .

The performance of the proposed method (WikiME) is shown in Table 4 along with the fol-lowing approaches:
MonoEmb : In this method, we use the mono-lingual embeddings before applying CCA while all the other settings are the same as in WikiME. Since the monolingual embeddings are learnt separately for each language, calculating the cosine similarity of the word embedding in the foreign language and an English title embedding does not produce a good similarity function. The ranker, though, learns that the most important feature is Pr ( title | mention ) , and, consequently, performs well on easy mentions but has poor performance on hard mentions.
 WordAlign : Instead of using the aligned Wikipedia titles in generating multilingual embed-dings, the CCA model operates on the word align-ments as originally proposed in Faruqui and Dyer (2014). We use the word alignments provided by Faruqui and Dyer (2014), which are obtained from the parallel news commentary corpora com-bined with the Europarl corpus for English to Ger-man, France, and Spanish. The number of aligned words for German, France, and Spanish are 37,484, 37,582, and 37,554 respectively. WikiME performs statistically significantly better than WordAlign on all three languages.

EsWikifier : We use Illinois Wikifier (Ratinov et al., 2011; Cheng and Roth, 2013) on a Spanish Wikipedia dump and train its ranker on the same set of documents that are used in WikiME.

Ceiling : These rows show the performance of ti-tle candidate generation. That is, the numbers indi-cate the percentage of mentions that have the gold title in its candidate set, therefore upper-bounds the ranking performance.

In sum, WikiME can disambiguate the hard men-tions much better than other methods without sacri-ficing the performance on the easy mentions much. Comparing across different languages, it is impor-tant to note that languages which have a smaller size Wikipedia tend to have better performance, despite the degradation in the quality of the embeddings (see below). This is due to the difficulty of the datasets. That is, there is less ambiguity because the number of articles in the corresponding Wikipedia is small. Figure 1 shows the feature ablation study of WikiME. For each language, we show results on hard mentions (the left bar) and all mentions (the right bar). We do not show the performance on easy mentions since it always stays high and does not change much. We can see that Local Context and Other Mentions are very effective for most of the languages. In particular, on hard mentions, the per-formance gain of the three feature groups is from almost 0 to around 50. For the easier dataset such as Urdu, Basic features alone work quite well.
Figure 2 shows the performance of WikiME when we vary the number of aligned titles in generating multilingual embeddings. The performance drops a lot when there are only few aligned titles, especially for Spanish and French, where the results are even worse than MonoEmb when only 2000 titles are aligned. This indicates that the CCA method needs enough aligned pairs in order to produce good em-beddings. The performance does not change much when there are more than 16,000 aligned titles. 5.2 TAC KBP2015 Entity Linking To evaluate our system on documents outside Wikipedia, we conduct an experiment on the evalu-ation documents in TAC KBP2015 Tri-Lingual En-tity Linking Track. In this dataset, there are 166 Chinese documents (84 news and 82 discussion fo-rum articles) and 167 Spanish documents (84 news and 83 discussion forum articles). The mentions in this dataset are all named entities of five types: Per-son, Geo-political Entity, Organization, Location, and Facility.

Table 5 shows the results. Besides the Span-ish Wikifier (EsWikifier) that we used in the previ-ous experiment, we implemented another baseline for Spanish Wikification. In this method, we use Google Translate to translate the whole documents from Spanish to English, and then the English Illi-nois Wikifier is applied to disambiguate the English gold mentions. Note that the target Knowledge Base of this dataset is FreeBase, therefore we use the FreeBase API to map the resulting English or Span-ish Wikipedia titles to the corresponding FreeBase ID. If this conversion fails to find the corresponding FreeBase ID,  X  X IL X  is returned instead.

The ranker models used in all three systems are trained on Wikipedia documents. We can see that WikiME outperforms both baselines significantly on Spanish. It is interesting to see that the translation-based baseline performs slightly better than the Spanish Wikifier, which indicates that the machine translation between Spanish and English is quite re-liable. Note that this translation-based baseline got the highest score in this shared task when the men-tion boundaries were not given.

The row  X  X op TAC X 15 System X  lists the best scores of the diagnostic setting in which mention boundaries are given (Ji et al., 2016). Since the offi-cial evaluation metric considers not only the linked FreeBase IDs but also the entity types, namely, an answer is counted as correct only if the FreeBase ID and the entity type are both correct, we built two simple 5-class classifiers to classify each mention into the five entity types so that we can compare with the state of the art. One classifier uses Free-Base types of the linked FreeBase ID as features, and this classifier is only applied to mentions that are linked to some entry in FreeBase. For NIL men-tions, another classifier which uses word form fea-tures (words in the mention, previous word, and next word) is applied. Both classifiers are trained on the training data of this task. From the last two rows of Table 5, we can see that WikiME achieves better results than the best TAC participants. Wikification on English documents has been stud-ied extensively. Earlier works (Bunescu and Pasca, 2006; Mihalcea and Csomai, 2007) focus on local features which compare context words with the con-tent of candidate Wikipedia pages. Later, several works (Cucerzan, 2007; Milne and Witten, 2008; Han and Zhao, 2009; Ferragina and Scaiella, 2010; Ratinov et al., 2011) proposed to explore global fea-tures, trying to capture coherence among titles that appear in the text. In our method, we compute lo-cal and global features based on multilingual embed-dings, which allow us to capture better similarity be-tween words and Wikipedia titles across languages.
The annual TAC KBP Entity Linking Track has used the cross-lingual setting since 2011 (Ji et al., 2012; Ji et al., 2015; Ji et al., 2016), where the target foreign languages are Spanish and Chinese. To our best knowledge, most of the participants use one of the following two approaches: (1) Do en-tity linking in the foreign language, and then find the corresponding English titles from the resulting foreign language titles; and (2) Translate the query documents to English and do English entity linking. The first approach relies on a large enough Knowl-edge Base in the foreign language, whereas the sec-ond depends on a good machine translation system. The approach developed in this paper makes sig-nificantly simpler assumptions on the availability of such resources, and therefore can scale also to lower-resource languages, while doing very well also on high-resource languages.

Wang et al. (2015) proposed an unsupervised method which matches a knowledge graph with a graph constructed from mentions and the corre-sponding candidates of the query document. This approach performs well on the Chinese dataset of TAC X 13, but falls into the category (1). Moro et al. (2014) proposed another graph-based approach which uses Wikipedia and WordNet in multiple lan-guages as lexical resources. However, they only fo-cus on English Wikification.

McNamee et al. (2011) aims at the same cross-lingual Wikification setting as we do, where the challenge is in comparing foreign language words with English titles. They treat this problem as a cross-lingual information retrieval problem. That is, given the context words of the target mention in the foreign language, retrieve the most relevant English Wikipedia page. However, their approach requires parallel text to estimate word translation probabili-ties. In contrast, our method only needs Wikipedia documents and the inter-language links.

Besides the CCA-based multilingual word em-beddings (Faruqui and Dyer, 2014) that we ex-tend in Section 3, several other methods also try to embed words in different languages into the same space. Hermann and Blunsom (2014) use a sen-tence aligned corpus to learn bilingual word vectors. The intuition behind the model is that representa-tions of aligned sentences should be similar. Unlike the CCA-based method which learns monolingual word embeddings first, this model directly learns the cross-lingual embeddings. Luong et al. (2015) pro-pose Bilingual Skip-Gram which extends the mono-lingual skip-gram model and learns bilingual em-beddings using a parallel copora and word align-ments. The model jointly considers within language co-occurrence and meaning equivalence across lan-guages. That is, the monolingual objective for each language is also included in their learning objec-tive. Several recent approaches (Gouws et al., 2014; Coulmance et al., 2015; Shi et al., 2015; Soyer et al., 2015) also require a sentence aligned parallel corpus to learn multilingual embeddings. Unlike other ap-proaches, Vuli  X  c and Moens (2015) propose a model that only requires comparable corpora in two lan-guages to induce cross-lingual vectors. Similar to our proposed approach, this model can also be ap-plied to all languages in Wikipedia if we treat docu-ments across two Wikipedia languages as a compa-rable corpus. However, the quality and quantity of this comparable corpus for low-resource languages will be low, we believe.

We choose the CCA-based model because we can obtain multilingual word and title embeddings for all languages in Wikipedia without any additional data beyond Wikipedia. In addition, by decoupling the training of the monolingual embeddings from the cross-lingual alignment we make it easier to im-prove the quality of the embeddings by getting more text in the target language or a better dictionary be-tween English and the target language. Neverthe-less, as cross-lingul wikification provides another testbed for multilingual embeddings, it would be very interesting to compare these recent models on Wikipedia languages. We propose a new, low-resource, approach to Wik-ification across multiple languages. Our first step is to train multilingual word and title embeddings jointly using title alignments across Wikipedia col-lections in different languages. We then show that using features based on these multilingual embed-dings, our wikification ranking model performs very well on a newly constructed dataset in 12 languages, and achieves state of the art also on the TAC X 15 En-tity Linking dataset.

An immediate future direction following our work is to improve the title candidate generation process so that it can handle the case where the correspond-ing titles only exist in the English Wikipedia. This only requires augmenting our method with a translit-eration tool and, together with the proposed disam-biguation approach across languages, this will be a very useful tool for low-resource languages which have a small number of articles in Wikipedia. This research is supported by NIH grant U54-GM114838, a grant from the Allen Institute for Artificial Intelligence (allenai.org), and Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.)
