 Roland Memisevic memisevr@iro.umontreal.ca University of Montreal Georgios Exarchakis exarchakis@berkeley.edu University of California, Berkeley The learning of invariant representations of objects has been a long-standing research goal in machine learn-ing and vision over many years. Among the multitude of methods that have been proposed, the most com-mon approach is to use transformation sequences (e.g. Foldiak , 1991 ; Wiskott &amp; Sejnowski , 2002 ) in order to learn to discount variations in nearby frames. Since object class typically does not change on a fast time scale in natural videos, this leads to representations that retain object identity and that discard within-class variations (see, e.g., Wiskott , 2006 , for a discus-sion). Learning of complex cell models, such as inde-pendent subspace analysis ( Hyv  X arinen &amp; Hoyer , 2000 ) and related models, can be viewed as an alternative ap-proach to achieving invariance. Complex cell models typically learn invariances with respect to small , local-ized translations by utilizing the invariance of subspace energies, computed from quadrature Gabor features, to such translations (e.g. Hyv  X arinen et al. , 2009 ). Common to practically all approaches to learning in-variance is that they utilize multiple views, typically videos, of objects at training time, and still images at test time. In this work, we explore an alternative ap-proach to learning invariant representations by learn-ing features that encode videos not images to represent objects. Our approach thus utilizes videos for training and for testing. We show how this makes it possible to derive a very simple, biologically plausible, deep learn-ing approach to extracting invariant features based on the learning of subspace energies from videos. In particular, we show that learning a 3-layer network containing subspace energies at the second layer will naturally learn both the transformations inherent in the training data and a representation that is invari-ant to those transformations. In contrast to classical complex cell models, our approach is not invariant to small, but also to large transformations, and it is in-variant to transformations other than translation, such as rotation, or changes in 3-D orientation of objects. To explore the learning of invariant 3-D features of ob-jects from data, we introduce a derivative of the NORB data set ( LeCun et al. , 2004 ) consisting of videos of objects transforming in 3-D. 1.1. Related work Our work is related to the approach described in ( Hoyer &amp; Hyv  X arinen , 2002 ) which is also based on adding a third feature learning layer on top of a layer of complex cell energy units. In contrast to that work, our approach involves learning to encode transforma-tions not still images. Furthermore, we show how the second layer changes its semantics as a result of adding the third layer, and how this allows us to learn invari-ant features. Because of this, learning of all layers has to be performed jointly in our model, whereas related complex cell models, such as ( Hoyer &amp; Hyv  X arinen , 2002 ), are trained greedily, layer-by-layer. A prerequisite to a representation which is invari-ant to 3-D transformations is some degree of under-standing of the 3-D structure of objects. Our ap-proach may therefore be viewed also as a way to learn mid-level features of objects. Learning mid-level features using complex cell like models is also discussed in ( Cadieu &amp; Olshausen , 2011 ; Zou et al. , 2012 ). In contrast to that work, our method is some-what simpler, because it does not utilize an explicit polar decomposition of features. On a more techni-cal level, ( Cadieu &amp; Olshausen , 2011 ) and ( Zou et al. , 2012 ) learn mid-level features by utilizing the transfor-mation invariance of subspace norms , whereas our ap-proach makes use of the invariance of phase-derivatives instead. We therefore utilize videos not still images to extract representations, even at test-time.
 Our model makes use of the close relationship be-tween the cross-correlation (or  X  X ating X ) view (e.g., Arndt et al. , 1995 ; Fleet et al. , 1996 ) and the oriented energy view ( Adelson &amp; Bergen , 1985 ) of motion anal-ysis. In this work, we show that the gating perspective can be viewed as a special case of the energy model, when using a denoising autoencoder ( Vincent et al. , 2008 ) with a particular noise-process for learning. Consider a short video sequence ~ X  X  R N consisting M = N/T ) which are related by the repeated applica-tion of a transformation L , so that ~x t = L t ~x 1 . Most transformations between frames in a natural video can be represented by such a linear transformation in pixel space. L could, for example, take the form of a per-mutation matrix which moves any pixel x i in x t to any other position x j in x t +1 . 2.1. Translation and Fourier components Videos showing only spatial translations in time can be represented naturally using the phase components of the Fourier spectra of the images across time: the Fourier components will show phase shifts, which to-gether uniquely identify the translation direction and amount. The amplitude spectra, in contrast, will be constant, and so they constitute a representation that is invariant with respect to translation. The reason why the temporal evolution of the Fourier spectrum is a natural way to represent translations is that Fourier components (sines and cosines) are the eigenvectors of translations (e.g., Gray , 2005 ). For the same reason, the Fourier amplitude spectrum is invariant to trans-lation.
 The well-known energy model of complex cells ( Adelson &amp; Bergen , 1985 ) is an example of an applica-tion of this principle to the extraction of motion from videos: the energy model computes projections onto Fourier components in the form of simple cell responses (or localized Fourier components if one uses Gabor features), and subsequently measures phase changes in the subspaces spanned by these components. The whole set of phase changes across multiple spectral components constitutes a population code of motion. The same line of thought applies to estimation of binocular disparity (e.g. Fleet et al. , 1996 ), because it also amounts to the estimation of local translations. In real world data, however, not all images are repre-sented equally well by all Fourier frequencies and ori-entations. If some Fourier components are not present in an image sequence, then it is not possible to ex-tract any motion from that component. The absence of Fourier components of a given orientation makes it impossible to recover motion in that direction. Hori-zontal shifts, for example, are undetectable in images containing no horizontal Fourier components. The in-ability to detect motion as a result of missing Fourier components is typically referred to as the aperture problem in vision. In this work, we discuss how the aperture problem is intimately related to the task of learning representations , and how this relationship al-lows us learn invariant features from data.
 To this end, first note that the partial absence of spectral components typically has an adverse affect on the analysis of motion, because it entails content-dependence : a population code of phase deltas en-coding some observed motion will be different for any two images that contain different spectral components. That is, the same transformation applied to two differ-ent images will give rise to different code vectors to the degree that the images have different spatial frequency content.
 In this work we shall show how using multi-layer pool-ing allows us to utilize this content-dependence, by learning features that are both invariant to learned transformations and selective to other properties of the inputs. 2.2. Complex cell video encodings To learn about motion from videos, it is common to use complex cell models, which sum over two squared linear ( X  X imple cell X ) responses (cf., Figure 1 (a)) as-sumed to be in quadrature (for example, sines and cosines). The reason is that the transformations may be decomposed into rotations in two-dimensional sub-spaces (e.g. Memisevic , 2012 ).
 When the input data is a video sequence of T frames ( ~
X = [ ~x 1 , . . . , ~x T ]), then the response of a vector ~a of complex cell responses can be written: where W  X  R F  X  N denotes F feature vectors ~ W  X  R
N stacked side-by-side. Each feature vector spans a whole video, so it is composed of frame features ~w s , each of which spans a single frame. The ma-trix Q  X  R ( F/ 2)  X  F is a band-diagonal pooling ma-trix with entries Q i, 2 i = 1 and Q i, 2 i +1 = 1 and zero elsewhere. It pools over the elementwise products of ( W ~x )  X  ( W ~x ) = ( W ~x ) 2 and remains constant during learning. When trained on natural images, the two features added together by Q tend to turn into Ga-bor features that are approximately 90 degree out-of-phase, so we refer to Q as quadrature pooling matrix. A single a q may also be written a where ~w fs  X  R M , for s = 1 , . . . , T , are the components of ~ W f , which span the individual frames.
 The decomposition in Eq. 2 (rhs) follows from the binomial identity (see, e.g., Fleet et al. , 1996 ; Memisevic , 2012 ). It shows that the complex cell response consists of two parts: The computation of relative 2-D phase-angles , that take the form of 2-D inner products between all pairs of projected frames (first sum); the computation of the norms of the projected frames in the 2-D-subspaces (second sum). When both frames and frame features are contrast-normalized, then the complex cell response is an en-coding of motion. For example, if frame features ~w fs , ~w ft are phase-shifted Fourier components, then the set of a q encode translations in the form of phase-deltas between the projections of multiple frames (e.g., Fleet et al. , 1996 ; Qian , 1994 ; Memisevic , 2012 ). Because of the aperture problem, the representation of motion depends on the Fourier amplitudes of the input: a frequency component that is not present in the images turns off the response a q , even if the mo-tion is present in the video. As a result, the a q show a dependency on image content through the Fourier am-plitudes of the individual images. A solution is to let complex cells pool not only over quadrature features but over multiple subspaces to reduce this dependency (e.g., Fleet et al. , 1996 ).
 Formally, this amounts to replacing the quadrature pooling matrix Q in Eq. 1 by a full pooling matrix P that can be learned from data along with the fea-tures W . See Figure 1 (b) for an illustration. Pool-ing across multiple components a q yields motion fea-tures, ~m , that are robust against the aperture problem because any motion affects multiple frequency com-ponents. Translation, for example, is visible in all Fourier frequencies whose orientation is the same as the direction of translation, and it is also visible in other, nearby orientations. P reduces the dependen-cies on image content by pooling across multiple 2-d subspaces. The pooled motion features ~m are also re-ferred to as  X  X apping units X  in the literature. Various parameter estimation schemes have been used to learn complex cell models to encode videos (both with pooling across subspaces and without). They include maximizing sparsity ( Hyv  X arinen &amp; Hoyer , 2000 ; Le et al. , 2011 ), maxi-mizing likelihood ( Memisevic &amp; Hinton , 2010 ), sub-space clustering ( Bethge et al. , 2007 ), predictive sparse coding ( Memisevic , 2011 ), minimizing recon-struction error while enforcing amplitude constancy across frames ( Cadieu &amp; Olshausen , 2011 ; Zou et al. , 2012 ), or training the closely related bilinear models (e.g., Rao &amp; Ruderman , 1999 ; Olshausen et al. , 2007 ; Miao &amp; Rao , 2007 ; Grimes &amp; Rao , 2005 ). 2.3. Generalizing the aperture problem Learning features from data instead of hard-coding them was shown to extend the applicability of com-plex cell models beyond Fourier and Gabor compo-nents and beyond modeling translations. In particu-lar, there exist features, W , which can represent any orthogonal image transformations that form a com-mutative group (e.g., Memisevic , 2012 ; Lee &amp; Soatto , 2011 ). Since orthogonal transformations are uniquely determined by a set of rotations in their eigenspaces and since the eigenspaces are shared among commut-ing transformations, extracting transformations from images amounts to extracting rotation angles in the eigenspaces. This makes it possible to learn video feature that encode rotations, local shifts or other, more complex motions. When training complex cell models on natural videos, it can be shown that the models decompose these into approximately group-structured components such as Gabor features (e.g. Cadieu &amp; Olshausen , 2011 ; Memisevic , 2012 ). This al-lows them to learn complex motion patterns for tasks like activity recognition ( Le et al. , 2011 ; Taylor et al. , 2010 ). It is important to note that, even in these more general settings, the aperture problem holds. In other words, motion estimates in the form of a population code of complex cell features, a q , will depend on how well individual images are aligned with each subspace, unless across-subspace pooling is used to remove (or at least alleviate) that dependency. 2.4. Subspace segregation Although the population code ~a = a q F/ 2 optimal representation of motion, it can be a good rep-resentation of object identity : Since it is affected by the aperture problem, it is sensitive to any transformation that the model was not trained on. By definition, these transformations change the magnitude of all subspace projections and thus change both the inner products and the norms in the subspaces.
 More importantly, ~a will nevertheless tend to be pose-invariant , because a transformation applied to all frames of a video changes only the phase-angles of the subspace projections. In other words, the learned transformations, by definition, leave subspace norms and the relative phase-deltas between the projections constant.
 Here, we suggest extracting these pose-invariant (but otherwise object sensitive) features using two-layer pooling as follows (cf., Figure 1 (c)): We use two-dimensional subspaces, as prescribed by the geometry of orthogonal transformations, and we use a separate pooling layer to overcome the aperture problem. For-mally, we introduce a across-subspace pooling matrix S  X  R D  X  F/ 2 of dimension F/ 2  X  D , and define a D -dimensional video code The components of ~a are the same as in Eq. 1 (cf., Fig-ure 1 ). This allows the learning of content-independent motion components, while leaving subspace features accessible to other pathways, such as those involved in object recognition.
 As the lower-level features ~a encode image content via the aperture problem, we shall call them  X  X perture features X  in the following. In the special case of trans-lations, aperture features are similar to the amplitude spectrum. But they can learn other, more complex types of transformation as we shall show. For robust-ness, it can make sense to apply an elementwise non-linearity to ~m in classification tasks, which we do in our experiments below.
 It is interesting to note that performing invariant recognition by using a linear classifier on the aperture features amounts to computing a linear combination, thus it is a form of pooling, too. Computing a repre-sentation of motion (Eq. 3 ) amounts to a different kind of pooling over the same features. That way, aperture features provide a multi-view  X  X ubstrate X  for objects, which encodes information about both, transforma-tions and invariances. Multiple higher-level pathways can selectively extract this information through differ-ent forms of pooling. 2.5. Learning Training the model (Figure 1 (c)) seems hard at first sight because of the presence of squaring non-linearities. However, there is a wide variety of complex cell models as discussed is Section 2.2 , which can learn in the presence of these. Here, we use the approach described by ( Memisevic , 2011 ), who show that a de-noising autoencoder ( Vincent et al. , 2008 ) can learn in the presence of such squaring non-linearities by train-ing to reconstruct multiple noisy copies of the input from each other.
 More specifically, consider a decoder corresponding to the encoder network shown in Figure 1 (c). We use  X  X ied weights X , so the decoder parameters are the transpose of the encoder parameters. By combining Eqs. 1 and 3 , and writing the square in Eq. 1 as a product, we may write the encoder activation in the form where  X  denotes elementwise multiplication. This shows that we can interpret the encoder as a three-layer linear autoencoder SQW ~ X whose second layer is gated (multiplied elementwise) by another projec-tion of the input, W ~ X . Likewise, we can define the decoder as which is another three-layer linear network, whose next-to-last layer is again gated by W ~ X . In con-trast to a standard denoising autoencoder, the input occurs twice in both the encoder and decoder. Using independent (factorial) noise as the corruption process ( Vincent et al. , 2008 ) thus implies that the two copies of ~
X have to be corrupted independently for training the model (see also, Memisevic , 2011 ).
 Formally, we thus generate two noisy copies ~ X noise1 , ~ X noise2 for each training case minimizing the reconstruction error that results from reconstructing one noisy copy of the input from the other: with Since gating is symmetric, one may also switch the roles of ~ X noise1 and ~ X noise2 for each training example in Eqs. 6 and 7 , add up the two resulting costs. There are many kinds of noise-process one can use for training denoising autoencoders ( Vincent et al. , 2008 ). In our experiments we use  X  X ero-mask X  noise which amounts to independently setting to zero a certain fraction (typically 50% in our experiments) of the com-ponents of the inputs ( Vincent et al. , 2008 ). Rather than reconstruction one noisy copy of the im-age from another noisy copy in Eq. 6 , one may also reconstruct the original input. We experimented with both approaches in practice and did not observe a sig-nificant difference across any of these approaches. 2.6. Relationship with factored bilinear models Computing a weighted sum of squared filter re-sponses is closely related to factored bilinear mod-els, such as the factored gated Boltzmann machine ( Memisevic &amp; Hinton , 2010 ). One can show that both squaring non-linearities and factored bilinear models encode relationships between images by recovering ro-tation angles in the invariant subspaces of the trans-formation class ( Memisevic , 2012 ; Fleet et al. , 1996 ). It is interesting to note that we can recover the gated autoencoder ( Memisevic , 2011 ) as a special case from Eqs. 6 and 7 using a particular corruption process: consider a video consisting of only two frames. Now define ~ X noise1 as the input video with the first frame blanked out (set to zero), and ~ X noise2 as the input video with the second frame blanked out. Inference during learning (Eq. 7 ) now amounts to multiplying the filter responses from two frames, and reconstruc-tion amounts to multiplying filter responses from one frame to reconstruct the other. Both these operations are exactly the same as as in a factored bilinear model (e.g. Memisevic &amp; Hinton , 2010 ; Memisevic , 2011 ). An illustration of features learned using the gating ap-proach is shown in the two right-most panels in Fig-ure 1 (c). They show subsets of the mapping units ~m computed for test-data after training the model on videos showing 3-D rotations of objects. The figure shows that top-level units are more structured and less noisy than the intermediate level units, which is con-sistent with the fact that the data set consists of many different objects which are transformed in a small num-ber of ways (rotations in 3-D) For details, see Section 3 . 3.1. Learning rotation invariant features We performed a quantitative comparison to evaluate the utility and the degree of invariance of the aperture features. We trained the three-layer model (Figure 1 (c)) using 2000 video features that are pooled into 1000 aperture features and subsequently into 200 mapping units. For training, we used videos of length 2 show-ing rotations of random dot images to learn rotation-invariant aperture features.
 To test the model used the mnistrot data set described in ( Larochelle et al. , 2007 ). The data set consists of 60000 training images and 12000 test images showing rotated MNIST digits of size 28  X  28 pixels. Some ex-ample images are shown in Figure 2 (left). The two right-most plots in the figure show the cosine-similarity between aperture features trained on rotations and raw images for a random subset of 100 images (10 from each class). They demonstrate that aperture features, although not trained on digits, are better at capturing the similarity structure inherent in the data, as rep-resentations of digits from the same class tend to be more similar than representations of digits from differ-ent classes.
 Figure 3 shows classification error rates for several training set sizes, obtain using raw images (blue), sub-space features trained with a gating model ( Memisevic , 2012 ) (dark blue), and subspace features trained with an energy model (black). We used logistic regres-sion to predict digit class from the aperture features. On the images, we tried logistic regression and k-nearest neighbors, and we found that k-nearest neigh-bors works best. The number of neighbors, as well as weight-decay for logistic regression are determined us-ing cross-validation. The figure shows that aperture features based on subspace norms perform best, and clearly outperform raw digits. Note that aperture fea-tures were trained on rotating random dot images, not digits. 3.2. The  X  X ORBvideos X  data set To evaluate our approach, and to facilitate ex-perimentation with learning-based 3-D invari-ance, we created a data set of objects that rotate in 3-D. The data set is available at www.iro.umontreal.ca/ ~ memisevr/aperture The data set is derived from the NORB data set ( LeCun et al. , 2004 ), which consists of images of ob-jects shown from various viewpoints (by changing az-imuth and camera elevation) and under various light-ing conditions. Each object belongs to one of 5 classes ( four-legged animals , human figures , airplanes , trucks , and cars ), and to one of 9 instances per class. From the NORB data, we generate video sequences that show  X  X ly-overs X  of each object by incrementally changing viewpoints, while maintaining the same ob-ject class, instance and lighting. We generate 109350 videos for training, using only NORB object instances 1 through 8 and 12150 videos for testing, using in-stance 9. This ensures that the sets of objects shown in the training and test sets are disjoint. Some exam-ples are shown in Figure 4 (left).
 All images are PCA-whitened, retaining 95% of the variance, and subsequently projected onto a basis learned using a contractive autoencoder ( Rifai et al. , 2011 ) with a complete basis. We abbreviate the result-ing  X  X utoencoder sparse coding X  representation ASC in the following. To learn mid-level features of 3-D structure, we concatenate the preprocessed images to obtain videos of 5 frames. Note that a subset of the transformations in this data set are 2-D rotations, since for some views the rotation angle will be parallel to the image plane. 3.3. Learning a 3-D invariant representation To test complex cell video features, we used the au-toencoder described in Subsection 2.4 with 2 frames, 1000 filters, 500, intermediate-level units and 100 mapping units. We trained the model using  X  X at-ing X  noise (cf., Section 2.6 ). Figure 4 (right) shows that some features resemble a circular Fourier basis ( Memisevic &amp; Hinton , 2010 ; Bethge et al. , 2007 ), that can encode 2-D rotations, while others seem to be re-sponsible for encoding out-of-plane rotations. Many filters look distinctly different than 2-D rotation fil-ters.
 We use analogy making as a way to assess qualita-tively if the model can extract 3-D structure from the data: given two source images ~x source and ~y source , we infer the top-level poolings. Clamping a different in-put image ~x target and using the decoder of the model then allows us to infer a prediction ~y target based on the transformation seen in the source image pair (see, e.g. Memisevic &amp; Hinton , 2010 ).
 Figure 5 (bottom right) shows output images from a random subset of target plane images taken from the test data , not seen during training. Source in-put and output images, as well as target input im-ages, are shown on the top left, top right, and bottom left, respectively. The figure shows that the model is able to infer the correct transformation in most (not all) cases. More importantly, the correctly inferred transformations indicate that the model correctly in-fers some aspects of the 3-D structure of the test data to produce output images of the right shape (cf., for example, third from the right, top row). 3.4. Quantitative evaluation We compared the representation from our model with the autoencoder (ASC) features on single frames by training a logistic regression classifier on the inferred representations. Since aperture features should tend to be invariant to 3-D transformations by design, we expect them to perform better than image features, in particular on small training set sizes. We use as training, validation and test data sets using the NORB object instances 0  X  4, 5  X  8, and 9, respectively (cf., LeCun et al. , 2004 ). This makes it possible to obtain a sufficiently large training set for learning features, and still a sufficiently large test set of about 12000 videos to obtain significant results. Only the training subset is used for learning features. We learn features on the full 5-frame videos using the independent noise across all frames (cf., Section 2.5 ) Figure 6 shows classifi-cation results using regularized logistic regression on various training set sizes. We cross-validate the regu-larization parameter on the training subset. The figure shows that aperture features significantly outperform the ASC features, as well as classification based on mapping units. More importantly, classification per-formance is much more consistent as the training set size gets very small. Both ASC and aperture features perform much better than random, which is 20% cor-rect in this data. The plot also shows that representing objects using videos improves recognition performance both over images in isolation and over videos consist-ing just of pairs of frames.
 One of the defining properties of a 3-D model is that it is a representation that is invariant to 3-D rotations. Both theory and experiments suggest that subspace energies computed from short videos are capable of providing a representation from which such a model can be easily derived, for example through pooling. Our work shows that one way to learn suitable sub-space energies is by making use of a twist on deep learning, where adding a layer is not used to learn more abstract features but to change the meaning of a lower layer of the model.
 Our work parallels some recent work in computer vi-sion on learning features for recognition using videos (e.g., Lee &amp; Soatto , 2011 ). A common finding in this line of work is that current object recognition bench-marks are somewhat unrealistic, because they provide training data in the form of still images not videos. Unlike practically all common object recognition sys-tems that are based on local descriptors, humans have no problem distinguishing a picture of a chair from a real chair, and one of the reasons for this is probably that humans learn about objects not from still images but from multiple views of the objects (as well as from interacting with the objects).
 Acknowledgments This work was supported in part by the German Fed-eral Ministry of Education and Research (BMBF) in the project 01GQ0841 (BFNT Frankfurt). This work was done in part while the authors were at the Uni-versity of Frankfurt. The authors would also like to thank the reviewers for several useful suggestions.
