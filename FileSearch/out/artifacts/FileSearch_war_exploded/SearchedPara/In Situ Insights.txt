 When consuming content in applications such as e-readers, word processors, and Web browsers, users often see mentions to topics (or concepts) that attract their attention. In a scenario of significant practical interest, topics are explored in situ , without leaving the context of the application: The user selects a mention of a topic (in the form of continuous text), and the system subsequently recommends references (e.g., Wikipedia concepts) that are relevant in the context of the application. In order to realize this experience, it is necessary to tackle challenges that include: users may select any continuous text, even potentially noisy text for which there is no corresponding reference in the knowledge base; references must be relevant to both the user selection and the text around it; and the real estate available on the application may be constrained, thus limiting the number of results that can be shown.

In this paper, we study this novel recommendation task, that we call in situ insights : recommending reference con-cepts in response to a text selection and its context in-situ of a document consumption application. We first propose a selection-centric context language model and a selection-centric context semantic model to capture user interest. Based on these models, we then measure the quality of a reference concept across three aspects: selection clarity , context coher-ence , and concept relevance . By leveraging all these aspects, we put forward a machine learning approach to simultane-ously decide if a selection is noisy, and filter out low-quality candidate references. In order to quantitatively evaluate our proposed techniques, we construct a test collection based on the simulation of the in situ insights scenario using crowd-sourcing in the context of a real-word e-reader application. Our experimental evaluation demonstrates the effectiveness of the proposed techniques.
 H.3.3 [ Information Search and Retrieval ]: Information Filtering  X 
Thi s work was done while the author was at Microsoft Re-search.
 c Algorithms In situ insights, entity recommendation
When consuming content in applications such as e-readers, word processors, and Web browsers, users often see mentions to topics (or concepts) that attract their attention. In a scenario of significant practical interest, topics are explored in situ , without leaving the context of the application: The user selects a mention of a topic (in the form of continuous text), and the system subsequently recommends references (e.g., Wikipedia concepts) that are relevant in the context of the application. 1 We call this capability in situ insights .
As an example, consider a user who is reading an arti-cle on Health Hazard. At some point, the user may select the term  X  X ntibiotics X . Figure 1 shows a screenshot of the resulting in situ insights experience. Recommended refer-ences include the Wikipedia concepts  X  X ntibacterial X  2 and  X  X ntibiotic misuse X  3 .

In order to realize the in situ insights experience, a number of challenges must be faced. First, users may select any continuous text, even potentially noisy text for which there is no corresponding reference in the knowledge base. (See [20] for a discussion on users selecting text  X  X nconsciously X ). In such cases, the in situ insights system should generally avoid recommending any result. Back to the screenshot in Figure 1, suppose the user unconsciously selects  X  X onjunction X  in the third line. It is unclear what result would be relevant to the selection, and it is probably best not to show any result.
Second, the in situ insights system should understand not only the text selection but also the context of the document that the user is reading. For instance, consider a user who is reading an article on History of Antibiotics and selects the term  X  X ntibiotics X . It would now make sense to recommend the reference  X  X lexander Fleming X  4 instead of the  X  X ntibi-otic misuse X  reference recommended in Figure 1. However, context may not always be useful, and may even be harmful if it is incoherent with the text selection [30]. Moreover, al-though the reading context is critical in the in situ insights experience, revealing it to an external recommender system may raise privacy concerns or even legal problems, as the doc ument may be a private email or enterprise confidential documentation. In this paper, we address both issues.
Finally, since the problem is defined in situ of a doc-ument consumption application, the  X  X eal estate X  (display space) available for the recommendation results may be con-strained. As a result, it is acceptable, and occasionally even desirable, to limit the number of results shown for a given text selection. This is in contrast to conventional informa-tion retrieval systems which always return results if the cor-pus contains documents lexically related to the query (e.g., some query terms appear in the document).

In this paper, we study this novel recommendation task, that we call in situ insights : recommending reference con-cepts in response to a text selection and its context in-situ of a document consumption application. Our contributions include: Recommendation has been studied extensively in the past. Content-based filtering and collaborative filtering are the two main types of recommendation techniques that have been considered [1]. In situ insights is a new instance of content-based filtering. Content-based filtering, e.g., [34, 42], typically focuses on recommending documents that re-flect users X  long-term interests (e.g., a user might generally like sports). In contrast, our work recommends content re-lated to users X  ad hoc interests implied by the text selection when reading a document.

Related content recommendation (e.g., [28]), cumulative citation recommendation (e.g., [4, 44]), contextual citation recommendation(e.g., [19]), and contextual advertising (e.g., [33]) are also in the direction of ad hoc content-based filter-ing. They recommend related content, such as news articles, Wikipedia articles, academic papers, Web documents or ads, to a target document. Of particular interest is entity-centric document filtering [44], where, given an entity represented by its Wikipedia page, the task is to find relevant documents with respect to the input entity. These works are based on a problem formulation that is insufficient for in situ insights, as they do not allow for a text selection as part of its input.
There are other applications that also attempt to enhance the reading experience by bringing to bear other content, e.g., enriching Web tables with entities [26] and enriching textbooks with images [2]. They also differ with in situ in-sights in that they do not take user interaction into account.
Inline citation recommendation proposed by Livne et al. [27] is arguably the most related work to ours. Their setting can be regarded as a special case of in-situ recommendation that focuses on recommending reference papers to an academic paper being authored by a user. In contrast, in situ insights is an open domain problem, in which there are no restrictions on the type of user selections or references to be retrieved.
Our recent poster paper [15] gives a high-level overview of a recommendation system, called Leibniz, that was powered by the situ insights algorithm that we present and evaluate in this work. In another recent work, we have further explored in situ insights by exploiting the Wikipedia graphs [25]. Our work is related to context-sensitive search, such as IntelliZap [12], Y!Q [24, 23], UCAIR [38], and the recent work by Sun and Lou [40]. All these context-sensitive search sys tems and our system leverage general-purpose search en-gines, and exploit contextual information to postprocess the search results. However, a significant difference is that the problem at hand involves dealing with entity recommen-dation, while these related works are in the scope of Web search. Moreover, conventional information retrieval in gen-eral, and context-sensitive search [12, 24, 23, 38] in particu-lar, always return results if the corpus contains documents lexically related to the query. In contrast, our problem is defined in-situ of a document consumption application, and thus it is acceptable, and occasionally even desirable, not to show any results (or to limit the number of results) for a given text selection. This need to decide  X  X hether to swing X  has been observed in many other application domains; an eloquent argument in the online advertising domain can be found in [6]. In any case, as a point of comparison through-out our paper, we use as a baseline a context-sensitive search (re-ranking) system as done in [12, 24, 23, 38], in order to highlight the need for our proposed methods.

Entity search [32, 5] and related entity finding [3] are also related to our work. Entity search aims to answer a query with direct entity answers extracted from documents or en-tity records in a knowledge base; related entity finding takes as input a query entity, the type of the target entity, and the nature of their relation, and outputs a ranked list of related entities. Besides, another two related problems are named entity linking (e.g., [11]) and Wikipedia cross-reference (e.g., [31]), where the goal is to disambiguate the mention of an entity in unstructured text to the entity X  X  record in some knowledge base. However the task at hand is principally dif-ferent: recommending related entities (often more than one) that are related to (beyond the notion of  X  X inking X ) any text selection (which can be any continuous text, including but not limited to entity mentions) in the context of the docu-ment being consumed. We next define the problem of in situ insights.

Selection or Text Selection. A selection s is a mention to a topic or concept that attracts a user X  X  attention, and for which the user would like to explore more and gain insights about. It can be a word, phrase or any span of continuous text in the article that the user is reading. In our example in Figure 1, the selection is  X  X ntibiotics X . Table 1 shows more examples of selections chosen by crowd annotators.
When reading an article, a user can focus his/her attention in various ways, such as text tracing, link pointing, text selection, and text highlighting [20, 8]. In situ insights can potentially take any of these actions as input. Without loss of generality, we mainly employ  X  X ext selection X  to describe the scenarios, as it is one of the most precise indicators of user interest [20].

Context. The context c represents the content that the user is consuming. In Figure 1, the context consists of the article on Health Hazard. In this paper, we will evaluate an ad hoc context  X  the text around the text selection. We leave long-term context, such as all the text that the user has read so far, as future work.

Reference, Concept, or Document. A reference d can be any concept, entity, or topic. In this paper, we use Wikipedia concepts/documents. In Figure 1, there are two references recommended: the Wikipedia concepts  X  X ntibac-terial X  and  X  X ntibiotic misuse X .

In situ insights. Given a selection s and a context c , the problem of in situ insights consists of the following 3 steps:
First, identify a set of candidate references D ( s, c ). This is a typical information retrieval task; and the retrieval sys-tem must be scalable and fast. Moreover, our goal is to recommend relevant reference concepts for a selection from a knowledge source, in a way similar to recommending ci-tations for a research paper, so we choose Wikipedia as our target corpus, which provides a reasonably large number of concepts with high quality. We thus leverage a major com-mercial search engine to obtain candidate results, where each query is appropriately site-restricted by adding an operator site:en.wikipedia.org.

Second, design a prediction function to score each candi-of this paper, and will be discussed in detail later.
Third, recommend d , if h ( d | s, c, D ( s, c ))  X   X  , where  X  is a pre-defined threshold; discard d otherwise. By setting the threshold  X  very high, we will only show references in a small number of cases where the reference quality is very high. On the other hand, if the threshold is set very low, then refer-ences will be shown for many text selections, although the quality of the references would not be as good. Therefore, the threshold should be considered a parameter that can be used in conjunction with a well-accepted evaluation metric to determine whether or not to show a reference.

There is another important issue regarding the in situ in-sights experience: privacy . When retrieving candidate refer-ences, if we send much context to an external search engine, it may raise privacy concerns or even legal problems, since the context may be a private email or enterprise confidential documentation. To relax this issue in our insights system, inspired by the idea of client-side personalized search [38], we only send to the search engine a query purely based on the text selection, while leaving all the context-related com-putation at the client-side to postprocess the search results.
In this section, we first present a crowdsourcing exercise that we conducted in order to collect text selections and con-texts by simulating the environment of a reader consuming content in an application that has in situ insights capabil-ities. We then describe the construction of our evaluation dataset.
We employed a corpus consisting of all English textbooks from the Wikibooks site (2600 books in total). The text-books cover a broad spectrum of topics, such as engineering, humanities, health sciences, and social sciences. We ran-domly sampled 500 books from the corpus, and one para-graph from each book. The 500 paragraphs were then shown to each of the 100 annotators via crowdsourcing, using a UI that shows the paragraph in the context of the page of the book where it appears. To acquire the text selections, the annotators were asked to select from the paragraph any mentions of topics or concepts for which they would like to see additional references from an external knowledge source (such as Wikipedia), and they could choose not to make any selection if they did not find anything interesting. Table 1 shows some representative examples of the user selections we collected.

As a result of the crowdsourcing exercise, each paragraph was assigned 13.3 selections on average (we kept the selec-tions made by at least two annotators); 2 . 2 out of these
Figure 2: Power-Law distribution of selections. 13 . 3 s elections were chosen by at least 10 annotators. For 78 . 8% of the paragraphs, there is at least one selection that was chosen by at least 10 annotators. On the one hand, this indicates a reasonable level of agreement, given that the annotators can choose any continuous text from the entire paragraph (and they can also choose not to select anything). On the other hand, there is still a large number of long-tail selections, which is evidenced by the power-law distribution of the selection frequency in Figure 2.

We sampled 500 distinct selections proportional to fre-quency. We refer to them as  X  X egular selections X . In addi-tion, we create another set to account for the fact that in actual reading environments users may select text  X  X ncon-sciously X  X 20] (i.e., random text selections for which they may not necessarily expect a reasonable result). When selections are made by clicking with a mouse or tapping with a finger, it is reasonable to assume that these selections will tend to be single words. To simulate these selections, we randomly sampled another 300 words as X  X elections X  X rom the 500 para-graphs proportional to word frequency. We refer to this set as  X  X andom selections X . Finally, we combined the two sets and constructed a dataset with 800 selections in total.
After running each selection as a query through a commer-cial search engine, we obtained a set of (selection, document) pairs. A document is comprised of three components: title, url and snippet, as shown in Figure 3.
 Figure 3: A sample document (i.e., search result).
Ove rall, we retrieved Wikipedia documents for 490/500 regular selections and 299/300 random selections. There were a few selections for which the search engine did not return any result. In the following sections, we only consid-ered these 490 regular selections and 299 random mentions.
For each regular selection, we sent the top-25 documents, if any, to crowd labelers for relevance labeling. An analy-Figure 4: Relevance label distributions for regular (le ft) and random (right) selections respectively. sis of the crowsourced results showed that there on average 7.2 perfect/good results in the top-25 documents. For each random selection, we only sent the top-8 documents for rel-evance labeling, since based on our analysis in Figure 4, we can safely assume any result after the eighth position has a negligible probability of being relevant.

We had each &lt; selection, document &gt; pair labeled by ten crowd labelers using a user interface that we designed for that purpose: on the left hand side, a page with the se-lected text; and on the right hand side, the correspond-ing Wikipedia document. The labelers were provided with appropriate guidelines and asked to answer a succession of questions, such as: The questions were then mapped to three relevance labels:  X  X erfect X ;  X  X ood X  and  X  X ad X . If the result was what the labeler expected, then it was labeled as  X  X erfect X ; if the la-beler could find a connection, and she was willing to explore the result, then it was labeled as  X  X ood X ; otherwise, it was labeled as  X  X ad X .

We followed the majority voting method [22] to aggregate labels from multiple crowd labelers, but adjusted it empir-ically to the characteristics of our task: (1) if the majority of the labels is  X  X ad X  we consider the result to be  X  X ad X ; (2) otherwise, if the number of  X  X erfect X  labels is larger than the number of  X  X ood X  labels, we consider the result to be  X  X erfect X ; (3) otherwise, we consider the result to be X  X ood X . We plot the label distribution in Figure 4, which shows that, although the search engine has done a good job to rank doc-uments, it is not satisfactory for in situ insights due to the existence of many  X  X ad X  results on top.

We summarize the dataset characteristics in Table 2, in-cluding the number of selections with relevance judgments, the average and standard deviation of the length of selec-tions, and the total numbers of  X  X erfect X ,  X  X ood X  and  X  X ad X  judgments in each dataset. Figure 5: Relevance of a context term against its pro ximity to the text selection.
Context may not be always useful, and at times may even be harmful if it is incoherent to the text selection [30]. Thus, one key to the in situ insights problem is properly mod-eling the context around a text selection. In this section, we present a selection-centric context language model and a selection-centric context semantic model, which will be used to compute several important features in Section 6.
It has been shown in the information retrieval literature (e.g., [30]) that terms closer to the occurrences of the query (in analogy to the text selection) in a document are more likely to be related to the query. To examine this heuristic in the in situ insights problem, we compute the probability of every context term occurring in Perfect/Good documents 6 , and plot the average probability of terms with the same dis-tance to their corresponding text selections with respect to the distance in Figure 5. As we can see, this verifies the proximity heuristic that the relevance of a context term de-creases with the distance to the text selection.

This motivates us to factor in the selection s in the es-timation of the context language model. Specifically, we estimate the conditional probability p ( w | s, c ) in terms of the joint probability of observing a term w with the selection s at every position given the context c . Formally, where i is a random variable which indicates a  X  X erm X  posi-tion in context c , and | c | is the total number of positions in c . We then factor the probability p ( w, s, i | c ) in a way similar to assume that every position is equally likely, i.e., p ( i | c ) = The n, we obtain the following estimate: where p ( w | c, i ) is the probability of sampling term w at posi-tion i in context c . For efficiency reasons, p ( w | c, i ) is simply calculated using an indicator function 1 { c [ i ] = w } , which is 1 if w occurs at position i in c , and 0 otherwise.
The other term p ( s | c, i ) is the key component to model the term proximity and position information, which can be es-timated using the positional language model [29]. However, different from the conventional keyword queries, the selection s as a whole often represents a single semantic unit in which there is strong dependency among the words . Considering this, when computing p ( s | c, i ), we take the selection s (which may consist of multiple words) as a single X  X erm X , rather than assuming word independency and computing Q w  X  s p ( w | c, i ) instead. This was also shown to work better than the bag-of-words representation of s in our task.

We choose the Gaussian kernel to implement the context language model, as it is consistent to the curves observed in Figure 5 and was also shown to be effective in [29, 30]: where the standard deviation  X  in the Gaussian kernel is set to 60 empirically.
One potential problem to apply this context language model to score each reference document is that a document is very short (see the snippet in Figure 3), which may make the score sub-optimal due to the word mis-matching problem. To address this problem, we adopt a state-of-the-art seman-tic analysis technique, namely Explicit Semantic Analysis (ESA) [16, 17], to map the context (as well as the text se-lection and the document) into the semantic space. ESA represents a word w as a weighted vector of actual (explicit) Wikipedia concepts V ( w ). That is, the meaning of a word is given by a vector of concepts paired with the relevance score of each concept to the word. And then, a text fragment can be represented as the centroid of the vectors represent-ing its words. So the relatedness of any pair of texts can be computed as the cosine similarity between their concept vectors.

We build an ESA index using a Wikipedia dump (En-glish) from March 2014, which contains 700 , 514 Wikipedia concepts after preprocessing and pruning the noise from the total 10 , 836 , 201 concepts using the heuristic rules proposed in [17]. Note that the ESA index can be regarded as a highly compressed inverted index, but it is not appropriate for directly computing the relevance score of every candi-date reference (which can be any Wikipedia article), due to the low coverage (6 . 5%) of the index, in particular for fresh Wikipedia articles. To assess the quality of our ESA in-dex, we apply it to compute word relatedness on the widely-accepted WS-353 benchmark dataset [12], which contains 353 word pairs, and our experiments show a Spearman X  X  rank correlation of 0 . 735, which is consistent to the previously re-ported numbers [16, 17].

With both the ESA index and the proposed selection-centric context language model p ( w | s, c ), we can compute a selection-centric context semantic vector V ( s, c ) based on the centroid of the semantic vector of each term. Formally, Although using ESA for semantic matching is not entirely novel, we are the first to leverage the term proximity evi-dence when computing the ESA vector.
We model reference quality from three aspects: the co-herence of the context, the clarity of the selection, and the relevance of the reference with respect to the selection and the context.
Int uitively, if the context is more coherent, it tends to be more consistent with the text selection. Therefore, a more coherent context may be trusted more than a less coherent one.

One classic method to measure text coherence is using the entropy of its language model. If context c is about a clear and coherent topic, the language model is usually charac-terized by large probabilities for a small number of topical terms, while if c is non-coherent (e.g., only consisting of un-correlated words), the language model would be smoother. Since the classic entropy may be dominated by common terms (e.g.,  X  X he X ,  X  X nd X , ...), we instead compute a relative entropy [10], which essentially measures the language usage associated with c as compared to the background language model p ( w | B ) of the whole collection. With the selection-centric context language model p ( w | s, c ) proposed in Sec-tion 5.1 (Equation 2), we can now compute the relative en-tropy of context, which we denote as ContextClarity : w here generally the higher the coherence, the larger the Con-textClarity score.

Arguably, if the selected text s repeats in many sentences in the context, the context is more likely to be coherent with the selected text. We thus propose another feature using the number of the sentences that contain the selected text.
Yet in some cases, even if the selected text s occurs in many sentences, it is still unclear whether or not the remain-ing sentences that do not contain s are also coherent with s , since it might be the case that the first half of the context contains s and is more coherent, whereas the second half does not contain s and is about a different topic. To capture this intuition, we propose another feature, namely Topic-DominanceInContext , as the cosine similarity (Equation 8) between the set of sentences that contain s and the set of sentences that do not, where the tf  X  idf weight of each term w in both vectors is calculated in the same way as D in Section 6.3.1.
The motivation of modeling the clarity of the selection s is that, if s is unclear/ambiguous/noisy, we may rely more on the context, or decide not to show any results for this s . We present one feature by exploiting the anchor texts. Specifically, we calculate the entropy of the distribution of the links for which s is the anchor text, and denote it as SelectionEntropy : where L ( s, p ) denotes the frequency that a web page p is linked with an anchor text s . A smaller entropy means that s is used as an anchor text to mostly link to a single web page, suggesting that s is more likely to be a specific topic/concept; a larger entropy indicates that s is used as the anchor text to link to different web pages, and thus s might be a too general or ambiguous topic/concept.

SelectionEntropy measures the selection clarity at the X  X opic X  level. We also introduce a set of features to measure the clarity at the term level based on the IDF distribution  X  if s contains high-IDF terms, it is more likely to be a clear topic. Indeed, Scholer et al. [37] have shown that the query term IDF is a useful signal for predicting query performance. We use maximum IDF, average IDF and the standard deviation of IDF.

The above features are all pre-retrieval features without using the results from the search engine. After receiving the search results, we can also explore post-retrieval features. One well-known post-retrieval query performance predictor is the relative entropy of the language model estimated on the search results to the background language model p ( w | B ), namely query clarity [10]. We employ this idea to mea-sure the clarity of the document set D . Specifically, we first estimate a standard multinomial language model based on the maximum likelihood estimator and Formula 9, for-mentSetClarity can be computed as follows:
We first present several features that use a standard re-trieval function, i.e., cosine similarity, to compute how a doc-ument is syntactically relevant to the text selection and the context. The cosine similarity sim ( Q, D ) between a  X  X uery X  Q and a  X  X ocument X  D is computed as follows: w here Q w and D w are the tf  X  idf weights of term w in Q and in D , respectively. Following the term frequency nor-malization formulas in BM25 [35, 39], the tf  X  idf weight in w here N is the total number of (Wikipedia) documents, DF ( w ) is the number of documents that contain w , and k is a free parameter in BM25 that is empirically set to 1 . 5.
Our query consists of two parts: the text selection s and the context c . Note that the term frequency f ( w, c ) for the context is special, as we would like to assign more weights to terms closer to the text selection based on the proximity heuristic [30]. To this end, we use the selection-centric con-text language model developed in Section 5.1 as the context term frequency, i.e., f ( w, c ) = p ( w | s, c ).
Our  X  X ocument X  consists of two fields, as shown in Fig-ure 3: (1) the title field d t ; (2) the snippet field d s we represent the whole document using a weighted linear combination of the term frequencies from both fields, similar to the idea of BM25F [36], which is denoted as X  X ocument X  d , with empirically tuned weights 3 and 1 for title and snippet
Therefore, we generate six features by combining two types of X  X ueries X  X nd three types of X  X ocuments X  X n different ways.
In addition, we introduce another feature that attempts to capture the topical consistency of the entire set of docu-ments as a whole to the context. Intuitively, the relevance of an individual document should be promoted if all the docu-ments in the set are likely to share the same topic with the context. We denote this feature as DocumentSetRelat-edness , which measures the similarity between the context c and the document set D . The key step is to calculate the term frequency for D , i.e., f ( w, D ). We follow the idea of BM25F [36], and use a weighted linear combination of the term frequencies from all documents d  X  D wit h an appro-priate weight w ( d ) for each document. We borrow the idea the ranking position of d . Then, the frequency of w in D is computed as: where d t and d s are the title and snippet fields of d , respec-tively. Finally, we can compute DocumentSetRelatedness using the cosine similarity between the context vector and the document set vector.
The syntactic similarity would often suffer from the word-mismatching problem, since we are dealing with short  X  X oc-uments X . To relax this problem, as discussed in Section 5.2, we map each text fragment into the semantic space using ESA [16]. We have presented how to build a selection-centric context semantic vector. Similarly, following the notation in Section 5.2, we can also build a semantic vector for the mantic vector for each of the three types of  X  X ocument X  and
F inally, we generate a set of 7 semantic-similarity based features by comparing the corresponding pairs of semantic vectors.
Inspired by the entity linking works (e.g., [43]), we also include several features to measure the surface/string simi-larity between the selection s and the document title d t cluding the edit distance between s and d t , and abbreviation matching that indicates if s is an abbreviation of d t or d an abbreviation of s . Moreover, the length of the selections often varies a lot as shown in Table 2, which motivates us to propose another two normalized matching features, e.g., the normalized edit distance, which divides the edit distance by the length of the longer one between s and d t (to avoid overly penalizing long selections).
We have proposed 27 features from three aspects. Given the proposed features and the constructed dataset, we use a state-of-the-art regression algorithm, namely MART (Mul-tiple Additive Regression Trees) [41], to develop a predic-tion function. MART is based on the stochastic gradient boosting approach described in [13, 14] which performs gra-dient descent optimization in the functional space. In our experiments, we used the log-likelihood as the loss function, steepest-descent (gradient descent) as the optimization tech-nique, and binary decision trees as the fitting function.
We construct a training instance for each (selection, con-text, reference, reference set) quadruple that consists of a set of features and a relevance label. The training data is fed into MART to build a regression model, which we use to score each document in the test data. In Situ Insights is in the line of content-based filtering. It makes a set of binary decisions to accept or reject each candidate reference concept. We first evaluated our system based on the metrics widely used in the TREC filtering task [34], including precision, recall and F  X  .

Precision, recall and F  X  are all binary metrics. Hence, we employed two strategies to map the trinary label (i.e.,  X  X erfect X ,  X  X ood X  and  X  X ad X ) to a binary label:
Precision is then computed by averaging the precision of all selections for which the system shows results. Recall is computed by averaging the recall of all selections that have at least one correct label in the judgment set. With precision and recall ready, F  X  is computed as follows: wh ere we follow the TREC filtering task [34] to set  X  = 0 . 5.
Next, in order to account for the multiple-graded relevance labels, we also report DCG [21] at different positions: w here k is the cutoff position.

The in situ insights problem differs from conventional in-formation retrieval in that it is acceptable, and occasionally even desirable, not to show any results for a given selection. As a result, the  X  X dditive X  nature of DCG (and other Web search metrics) is not suitable in this application. To see why, look at two scenarios: (1) algorithm A returns exclu-sively a  X  X erfect X  result, while algorithm B returns a  X  X er-fect X  result followed by a  X  X ad X  one; (2) algorithm A returns nothing, while algorithm B returns a set of  X  X ad X  results. In both scenarios, DCG will produce the same score for both algorithms, although algorithm A is more desirable in both cases. This is mainly caused by the always non-negative rele-vance labels in DCG, which does not penalize false positives results appropriately. To overcome this limitation, in our work, we map  X  X ad X  to a negative value  X  1, and  X  X erfect X  and  X  X ood X  to 2 and 1 respectively. DCG is then computed as the average DCG of all selections. 7
Our algorithm is labeled as Insights . We employed a 3-fold cross-validation for training and evaluating different methods. We randomly split the dataset (including 490 reg-ular selections and 299 random selections) into 3 folds, and then fixed the folds in the experiments. Each time we trained the algorithm on two folds, and then ran the algorithms on the remaining test fold. Since there was a threshold  X  that required tuning, we ran another process of cross-validation, in which we tuned  X  to optimize the F  X  score on two folds, and evaluated it on the remaining fold.

In situ insights is a novel problem, and there is no state-of-the-art system readily available to be used as a baseline. For this reason, we resorted to the two following baselines:
We first compare Insights with Baseline for aggressive rec-ommendation. Note that the problem at hand is defined in-situ of a document consumption application, in which the display space for the recommendation results is constrained and only allows to show a few results. In this work, the com-parison is done at the fourth position, by assuming that at most four references can be shown.

The results are summarized in Table 3. We can see that, for all selections, Insights outperforms the Baseline signif-icantly by 16 . 3% and 27 . 9% in terms of F  X  and precision, respectively. Furthermore, we inspect the regular and ran-dom selections separately. The results show that Insights works effectively on both types of selections. In particular, Insights even improves precision significantly (+12 . 4%) with only a slight decrease (-2 . 5%) in recall on the regular selec-tions, while on the random selections, Insights achieves an F  X  improvement as high as 55 . 0%. In addition, Insights can produce reference recommendation for over 93 . 3% selections. Figure 7: Comparison of F  X  w.r .t. the maximum number of references to show.

Next, we compare the conservative recommendation per-formance in Table 3. Conservative recommendation is harder than aggressive recommendation, because there are often no  X  X erfect X  references for many selections; this is also reflected by the numbers: we can see that the F  X  of Baseline drops dramatically for conservative recommendation as compared with aggressive recommendation. In contrast, the F  X  of In-sights only decreases slightly, or even increases for random selections. As a result, over all selections, Insights outper-forms Baseline by 82 . 0% in terms of F  X  . Here, we would like to highlight the X  X overage X  X umbers: Insights can cover more regular selections (65 . 9%) than random selections (27 . 1%), and automatically discards 72 . 9% random selections; this is highly valuable for improving user experience.
 We also compare the precision-recall curves of Insights and Baseline in Figure 6. We can see that Insights can improve the precision significantly with a reasonable degradation of recall for both aggressive and conservative recommendation.
We fixed the maximum number of references to four in the previous experiments. And now we examine if our Insights algorithm can still do well when the system allows to display more or fewer references. We plot the F  X  w.r.t. the max-imum number of references allowed in Figure 7. It clearly shows that Insights consistently outperforms Baseline in all cases, and the improvement of Insights becomes even larger when we show more results.

So far, the experiments are mainly based on binary eval-uation metrics. Next, we evaluate Insights using a graded-relevance metric, i.e., DCG@ k , with different length cutoff k . Besides the  X  X aseline X , we also compare Insights with a contextual ranking model, CtxRank, as described in Sec-tion 7.1. CtxRank serves as a strong representative of the contextual search systems [12, 24, 23, 38]. The comparison is presented in Table 4. We can see that: (1) Insights im-proves over Baseline significantly, especially for larger k ; (2) CtxRank also performs better than Baseline, confirming the observation in previous work [12, 24, 23, 38]; (3) Insights outperforms CtxRank significantly, suggesting that the pro-posed filtering technique is desirable, and that filtering works better than ranking for in situ insights; (4) the DCG scores Table 4: DCG Comparison. Superscripts 1, 2 and 3 in dicate significant improvements over Baseline, CtxRank and Insights at the 0.01 level using the Wilcoxon non-directional test.
Figure 8: Sensitivity of F  X  /DC G to the threshold. of Baseline and CtxRank decrease when we increase k to some value, but in contrast, Insights increases steadily.
Can we combine Insights and CtxRank to further improve the performance? To answer this question, we introduce X  X n-sights+ X , which does both filtering and ranking. We report its DCG numbers in Table 4. We observe that Insights+ only improves over Insights slightly, i.e., re-ranking does not help much after result filtering. This is probably because the search engine already does a good job at ranking  X  X er-fect X  results relatively higher than  X  X ood X  results, though it might not work well for recognizing X  X ad X  X esults without the contextual information. This suggests that Insights alone al-ready works very well, and that it may not be necessary to maintain an additional ranking model.

The setting of the threshold is important for a filtering task. We now examine the sensitivity of the F  X  and DCG scores against the threshold in Figure 8. We can see that in both aggressive and conservative recommendation scenarios, F  X  is sensitive to the threshold value, and the optimal thresh-old value for the aggressive and conservative recommenda-tion is within [0 . 5 , 0 . 7] and [1 . 0 , 1 . 3], respectively. DCG is also sensitive to the threshold value, but DCG at differ-ent positions appears to largely share the optimal threshold, which is usually in [0 . 5 , 0 . 7].
We go in depth to analyze what the relative contributions of each category of features are (see Section 6). We remove each category of features one by one, and examine how much the F  X  s core decreases after excluding one category. 8 The results are presented in Table 5.

The proposed features all work well. Features based on syntactic and semantic similarity contribute the most. And for regular selections, these similarity based features play a more significant role in aggressive recommendation than in conservative recommendation, suggesting that they are bet-ter at excluding  X  X ad X  results rather than identifying  X  X er-fect X  results, probably because  X  X erfect X  and  X  X ood X  results are easily mixed up for regular selections. For random se-lections, however, these similarity features work more effec-tively in conservative recommendation (i.e., identifying X  X er-fect X  results) than in aggressive recommendation (i.e., ex-cluding  X  X ad X  results), probably because  X  X ood X  and  X  X ad X  results are easily mixed up for random selections.
The other three categories of features, i.e., surface match-ing, selection clarity, and context coherence, also contribute to Insights, especially for random selections. In this paper, we proposed a novel recommendation task, In Situ Insights, which adaptively recommends reference con-cepts in response to a text selection in-situ of a document consumption application. We developed a selection-centric context language model and a selection-centric context se-mantic model to measure user interest. We then proposed different aspects for characterizing reference quality, includ-ing context coherence, selection clarity, and reference rel-evance with respect to the selection and the context. We also constructed a dataset using crowdsourcing by simulat-ing the environment of a reader consuming content in an application that has in situ insights capabilities. Finally, we developed a prediction model based on the proposed features using machine learning, and conducted a thorough experi-mental evaluation to show the effectiveness of the proposed techniques.

As a new class of content recommendation, in situ insights opens up many interesting future directions. One of the most interesting directions is to conduct online evaluation for measuring insights experience in real applications. An-oth er interesting direction is to leverage rich user reading his-tory (such as other documents a user has consumed/created, history insights experience, etc.) to improve the recommen-dation performance. We thank Ashok Chandra, Bernhard Kohlmeier, Bo Zhao, Dhyanesh Narayanan, and Pradeep Chilakamarri for their invaluable comments in many discussions about this project. We also thank the anonymous reviewers for their helpful feedback.
