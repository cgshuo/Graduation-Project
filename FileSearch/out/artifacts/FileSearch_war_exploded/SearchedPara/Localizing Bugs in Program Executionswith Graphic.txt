 In today X  X  software projects, two types of source code are developed: product and test code. Product code, also referred to as the program , contains all functionality and will be shipped to the customer. The program and its subroutines are supposed to behave according to a specification. The example program in Figure 1 (left), is supposed to always return the value 10. It contains a defect in line number 20, which lets it return a wrong value if the input variable equals five.
 In addition to product code, developers write test code that consists of small test programs, each testing a single procedure or module for compliance with the specification. For instance, Figure 1 (right) shows three test cases, the second of which reveals the defect. Development environments provide support for running test cases automatically and would report failure of the second test case. confirms only the existence of a defect, not its location.
 When a program is executed, its trace through the source code can be recorded. An executed line of source code is identified by a code position s  X  S . The stream of code positions forms the trace t of a test case execution. The data that our model analyses consists of a set T of passing test cases t . In addition to the passing tests we are given a single trace  X  t of a failing test case. The passing test traces and the trace of the failing case refer to the same code revision; hence, the semantics of each code position remain constant. For the failing test case, the developer is to be provided with a ranking of code positions according to their likelihood of being defective.
 The semantics of code positions may change across revisions, and modifications of code may impact the distribution of execution patterns in the modified as well as other locations of the code. We focus on the problem of localizing defects within a current code revision. After each defect is localized, the code is typically revised and the semantics of code positions changes. Hence, in this setting, we cannot assume that any negative training data X  X hat is, previous failing test cases of the same code revision X  X re available. For that reason, discriminative models do not lend themselves to our task. Instead of representing the results as a ranked list of positions, we envision a tight integration in development environments. For instance, on failure of a test case, the developer could navigate between predicted locations of the defect, starting with top ranked positions.
 So far, Tarantula [1] is the standard reference model for localizing defects in execution traces. The authors propose an interface widget for test case results in which a pixel represents a code position. The hue value of the pixel is determined by the number of failing and passing traces that execute this position and correlates with the likelihood that s is faulty [1]. Another approach [2] includes return values and flags for executed code blocks and builds on sensitivity and increase of failure probability. This approach was continued in project Holmes [3] to include information about executed control flow paths. Andrzejewski et al. [4] extend latent Dirichlet allocation (LDA) [5] to find bug patterns in recorded execution events. Their probabilistic model captures low-signal bug patterns by explaining passing executions from a set of usage topics and failing executions from a mix of usage and bug topics. Since a vast amount of data is to be processed, our approach is designed to not require estimating latent variables during prediction as is necessary with LDA-based approaches [4]. Outline. Section 2 presents the Bernoulli graph model, a graphical, generative model that explains program executions. This section X  X  main result is the closed-form solution for Bayesian inference of the likelihood of a transitional pattern in a test trace given example execution traces. Furthermore, we discuss how to learn hyperparameters and smoothing coefficients from other revisions, despite the fragile semantics of code positions. In Section 3, reference methods and simpler probabilistic models are detailed. Section 4 reports on the prediction performance of the studied models for the AspectJ and Rhino development projects. Section 5 concludes. The Bernoulli graph model is a probabilistic model that generates program execution graphs. In contrast to an execution trace, the graph is a representation of an execution that abstracts from the number of iterations over code fragments. The model allows for Bayesian inference of the likelihood of a transition between code positions within an execution, given previously seen executions. The n -gram execution graph G t = ( V t ,E t ,L t ) of an execution t connects vertices V t by edges E t  X  V t  X  V t . Labeling function L t : V t  X  S ( n  X  1) injectively maps vertices to n  X  1 -grams of code positions, where S is the alphabet of code positions.
 indicates that code position L t ( v ) has been executed directly after code position L t ( u ) at least once during the program execution. In n -gram execution graphs, each vertex v represents a fragment L ( v ) = s 1 ...s n  X  1 of consecutively executed statements. Vertices u and v can only be connected by an arc if the fragments are overlapping in all but the first code position of u and the last code Figure 2: Expanding vertex  X 22 18 X  in the generation of a tri-gram execution graph corresponding to the trace at the bottom. Graph before expansion is drawn in black, new parts are drawn in solid red. connected by an arc if code positions s 1 ...s n are executed consecutively at least once during the execution. For the example program in Figure 1 the tri-gram execution graph is given in Figure 2. Generative process. The Bernoulli graph model generates one graph G m,t = ( V m,t ,E m,t ,L m,t ) per execution t and procedure m . The model starts the graph generation with an initial vertex representing a fragment of virtual code positions  X  .
 In each step, it expands a vertex u labeled L m,t ( u ) = s 1 ...s n  X  1 that has not yet been expanded; e.g., vertex  X 22 18 X  in Figure 2. Expansion proceeds by tossing a coin with parameter  X  m,s 1 ...s n for each appended code position s n  X  S . If the coin toss outcome is positive, an edge to vertex v labeled L m,t ( v ) = s 2 ...s n is introduced. If V m,t does not yet include a vertex v with this labeling, it is added at this point. Each vertex is expanded only once. The process terminates if no vertex is left that has been introduced but not yet expanded. Parameters  X  m,s 1 ...s n are governed by a Beta distribution with fixed hyperparameters  X   X  and  X   X  . In the following we focus on the generation of edges, treating the vertices as observed. Figure 3a) shows a factor graph representation of the generative process and Algorithm 1 defines the generative process in detail.
 Inference. Given a collection G m of previously seen execution graphs for method m and a new execution G m = ( V m ,E m ,L m ) , Bayesian inference determines the likelihood p (( u,v )  X  E m | V m , G m , X   X  , X   X  ) of each of the edges ( u,v ) , thus indicating unlikely transitions in the new execution of m represented by execution graph G m . Since we employ independent models for all Algorithm 1 Generative process of the Bernoulli graph model. for all procedures m do Figure 3: Generative models in directed factor graph notation with dashed rectangles indicating gates [6]. methods m , inference can be carried out for each method separately. Since vertices V m are ob-served, coin parameters  X  are d-separated from each other (cf. Figure 3a). We yield independent Beta-Bernoulli models conditioned on the presence of start vertices u . Thus, predictive distributions for presence of edges in future graphs can be derived in closed form (Equation 1) where # G u denotes the number of training graphs containing vertices labeled L ( u ) and # G ( u,v ) denotes the number of training graphs containing edges between vertices labeled L ( u ) and L ( v ) . See the appendix for a detailed derivation of Equation 1. By definition, an execution graph G for an execution contains a vertex if its label is a substring of the execution X  X  trace t . Likewise, an edge is contained if an aggregation of the vertex labels is a substring of t . It follows 1 that the predictive distribution can be reformulated as in Equation 2 to predict the probability of seeing the code position  X  s = s n after a fragment of preceding statements  X  f = s 1 ...s n  X  1 using the trace representation of an execution. Thus, it is not neccessary to represent execution graphs G explicitly. Estimating interpolation coefficients and hyperparameters. For given hyperparameters and fixed context length n , Equation 2 predicts the likelihood for  X  s i following a fragment  X  f =  X  s i  X  1 ...  X  s i  X  n +1 . To avoid sparsity issues while maintaining good expressiveness, we smooth various context lengths up to N by interpolation. We can learn from different revisions by integrating multiple Bernoulli graphs models in a generative process, in which coin parameters are not shared across revisions and context lengths n . This process generates a stream of statements with defect flags. We learn hyperparameters  X   X  and  X   X  jointly with  X  using an automatically derived Gibbs sampling algorithm [7].
 Predicting defective code positions. Having learned point estimates for  X   X   X  ,  X   X   X  , and  X   X  from other revisions in a leave-one-out fashion, statements  X  s are scored by the complementary event of being normal for any preceding fragment  X  f . The maximum is justified because an erroneous code line may show its defective behavior only in combination with some preceding code fragments, and even a single erroneous combination is enough to lead to defective behavior of the software. The Tarantula model is a popular scoring heuristic for defect localization in software engineering. We will prove a connection between Tarantula and the unigram variant of a Bernoulli graph model. Furthermore, we will discuss other reference models which we will consider in the experiments. 3.1 Tarantula Tarantula [1] scores the likelihood of a code position s being defective according to the proportions of failing F and passing traces T that execute this position (Equation 4). For the case that only one test case fails, we can show an interesting relationship between Tarantula, the unigram Bernoulli graph model, and multivariate Bernoulli models (referred to in [8]). In the unigram case, the Bernoulli graph model generates a graph in which all statements in an execution are directly linked to an empty start vertex. In this case, the Bernoulli graph model is equal to a multi-variate Bernoulli model generating a set of statements for each execution.
 Using an improper prior  X   X  =  X   X  = 0 , the unigram Bernoulli graph model scores a statement by positions s 1 , s 2 is determined by 1  X  g ( s 1 ) &gt; 1  X  g ( s 2 ) or equivalently 1 1+ g ( s Tarantula X  X  ranking criterion if # F is 1. 3.2 Bernoulli Fragment Model Inspired by this equivalence, we study a naive n -gram extension to multi-variate Bernoulli models which we call Bernoulli fragment model. Instead of generating a set of statements, the Bernoulli model may generate a set of fragments for each execution.
 Given a fixed order n , the Bernoulli fragment model draws a coin parameter for each possible fragment f = s 1 ...s n over the alphabet S m . For each execution the fragment set is generated by tossing a fragment X  X  coin and including all fragments with outcome b = 1 (cf. Figure 3b). The The model deviates from reality in that it may generate fragments that may not be aggregateable into a consistent sequence of code positions. Thus, non-zero probability mass is given to impossible events, which is a potential source of inaccuracy. 3.3 Multinomial Models The multinomial model is popular in the text domain X  e.g., [8]. In contrast to the Bernoulli graph model, the multinomial model takes the number of occurrences of a pattern within an execution into account. It consists of a hierarchical process in which first a procedure m is drawn from multinomial distribution  X  , then a code position s is drawn from the multinomial distribution  X  m ranging over all code positions S m in the procedure.
 The n -gram model is a well-known extension of the unigram multinomial model, where the dis-tributions  X  are conditioned on the preceding fragment of code positions f = s 1 ...s n  X  1 to draw a follow-up statement s n  X   X  m,f . Using fixed symmetric Dirichlet distributions with parameter  X   X  and  X   X  as priors for the multinomial distributions, the probability for unseen code positions  X  s following on fragment  X  f is given in Equation 5. Shorthand # T s  X  m denotes how often statements in prodecure m are executed (summing over all traces t  X  T in the training set); and # T m,s the number times statements s 1 ...s n are executed subsequently by procedure m . 3.4 Holmes Chilimbi et al. [3] propose an approach that relies on a stream of sampled boolean predicates P , each corresponding to an executed control flow branch starting at code position s . The approach evaluates whether P being true increases the probability of failure in contrast to reaching the code position by chance. Each code position is scored according to the importance of its predicate P which is the harmonic mean of sensitivity and increase in failure probability. Shorthands F e ( P ) and S ( P ) refer to the failing/passing traces that executed the path P , where F o ( P ) and S o ( P ) refer to failing/passing traces that executed the start point of P .
 This scoring procedure is not applicable to cases where a path is executed in only one failing trace, as a division by zero occurs in the first term when F e ( P ) = 1 . This issue renders Holmes inapplicable to our case study where typically only one test case fails. 3.5 Delta LDA Andrzejewski et al. [4] use a variant of latent Dirichlet Allocation (LDA) [5] to identify topics of co-occurring statements. Most topics may be used to explain passing and failing traces, where some topics are reserved to explain statements in the failing traces only. This is obtained by running LDA with different Dirichlet priors on passing and failing traces. After inference, the topic specific statement distributions  X  = p ( s | z ) are converted to p ( z | s ) via Bayes X  rule. Then statements j are about a bug topic i than any other topic k . In this section we study empirically how accurately the Bernoulli graph model and the reference models discussed in Section 3 localize defects that occurred in two large-scale development projects. We find that data used for previous studies is not appropriate for our investigation. The SIR repos-itory [9] provides traces of small programs into which defects have been injected. However, as pointed out in [10], there is no strong argument as to why results obtained on specifically designed programs with artificial defects should necessarily transfer to realistic software development projects with actual defects. The Cooperative Bug Isolation project [11], on the other hand, collects execution data from real applications, but records only a random sample of 1% of the executed code positions; complete execution traces cannot be reconstructed. Therefore, we use the development history of two large-scale open source development projects, AspectJ and Rhino, as gathered in [12]. Data set. From Rhino X  X  and AspectJ X  X  bug database, we select defects which are reproducable by a test case and identify corresponding revisions in the source code repository. For such revisions, the test code contains a test case that fails in one revision, but passes in the following revision. We use the code positions that were modified between the two revisions as ground truth for the defective code positions D . For AspectJ, these are one or two lines of code; the Rhino project contains larger code changes. For each such revision, traces T of passing test cases are recorded on a line number basis. In the same manner, the failing trace t (in which the defective code is to be identified) is recorded.
 The AspectJ data set consists of 41 defective revisions and a total of 45 failing traces. Each failing trace has a length of up to 2,000,000 executed statements covering approx. 10,000 different code procedures. For each revision, we recorded 100 randomly selected valid test cases (drawn out of approx. 1000).
 have an average length of 3,500,000 executed statements, covering approx. 2,000 of 38,000 Figure 4: Recall of defective code positions within the 1% highest scored statements for AspectJ (top) and Rhino (bottom), for windows of h = 0 , h = 1 , and h = 10 code lines. code positions, spread across 70 files and 650 procedures. We randomly selected 100 of the 1500 valid traces for each revision as training data. Both data sets are available at http://www.mpi-inf.mpg.de/~dietz/debugging.html.
 Evaluation criterion. Following the evaluation in [1], we evaluate how well the models are able to guide the user into the vicinity of a defective code position. The models return a ranked list of code positions. Envisioning that the developer can navigate from the ranking into the source code to inspect a code line within its context, we evaluate the rank k at which a line of code occurs that lies within a window of  X  h lines of code of a defective line. We plot relative ranks; that is, absolute ranks divided by the number of covered code lines, corresponding to the fraction of code that the developer has to walk through in order to find the defect. We examine the recall@ k %, that is the fraction of successfully localized defects over the fraction of code the user has to inspect. We expect a typical developer to inspect the top 0.25% of the ranking, corresponding to approximately 25 ranks for AspectJ.
 Neither the AUC nor the Normalized Discounted Cummulative Gain (NDCG) appropriately measure performance in our application. AUC does not allow for a cut-off rank; NDCG will inappropriately reward cases in which many statements in a defect X  X  vicinity are ranked highly.
 Reference methods. In order to study the helpfulness of each generative model, we evaluate smoothed models with maximum length N = 5 for each the multinomial, Bernoulli fragment and Bernoulli graph model. We compare those to the unigram multinomial model and Tarantula. Tuning and prediction of reference methods follow in accordance to Section 2. In addition, we compare to the latent variable model Delta LDA with nine usage and one bug topics,  X  = 0 . 5 ,  X  = 0 . 1 , and 50 sampling iterations.
 Results. The results are presented in Figure 4. The Bernoulli graph model is always ahead of the reference methods that have a closed form solution in the top 0.25% and top 0.5% of the ranking. This improvement is significant with level 0.05 in comparison to Tarantula for h = 1 and h = 10 . It is significantly better than the n -gram multinomial model for h = 1 . Although increasing h makes the prediction problem generally easier, only Bernoulli graph and the multinomial n -gram model play to their strength.
 A comparison by Area under the Curve in top 0.25% and top 0.5% indicates that the Bernoulli graph is more than twice as effective as Tarantula for the data sets for h = 1 and h = 10 . Using the Bernoulli graph model, a developer finds nearly every second bug in the top 1% in both data sets, where ranking a failing trace takes between 10 and 20 seconds.
 According to a pair-t-test with 0.05-level, Bernoulli graph X  X  prediction performance is significantly better than Delta LDA for the Rhino data set. No significant diffference is found for the AspectJ data set, but Delta LDA takes much longer to compute (approx. one hour versus 20 seconds) since parameters can not be obtained in closed form but require iterative sampling.
 Analysis. Most revisions in our data sets had bugs that were equally difficult for most of the models. From revisions where one model drastically outperformed the others we identified different categories of suspicious code areas. In some cases, the defective procedures were executed in very few or no passing trace; we refer such code as being insufficiently covered. Another category refers to defective code lines in the vicinity of branching points such as if-statements. If code before the branch point is executed in many passing traces, but code in one of the branches only rarely, we call this a suspicious branch point.
 The Bernoulli fragment model treats both kinds of suspicious code areas in a similar way. They have a different effect on the predictive Beta-posteriors in the Bernoulli graph model: insufficient coverage decreases the confidence; suspicious branch points will decrease the mean. The Beta-priors  X   X  and  X   X  play a crucial role in weighting these two types of potential bugs in the ranking and encode prior beliefs on expecting one or the other. Our hyperparameter estimation procedure usually selects  X   X  = 1 . 25 and  X   X  = 1 . 03 for all context lengths.
 Revisions in which Bernoulli fragment outperformed Bernoulli graph contained defects in insuffi-ciently covered areas. Presumably, Bernoulli graph identified many suspicious branching points, and assigned them a higher score. Revisions in which Bernoulli graph outperformed Bernoulli fragment contained bugs at suspicious branching points.
 In contrast to the Bernoulli-style models, the multinomial models take the number of occurrences of a code position within a trace into account. Presumably, multiple occurrences of code lines within a trace do not indicate their defectiveness. We introduced the Bernoulli graph model, a generative model that implements a distribution over program executions. The Bernoulli graph model generates n -gram execution graphs. Compared to execution traces, execution graphs abstract from the number of iterations that sequences of code positions have been executed for. The model allows for Bayesian inference of the likelihood of transitional patterns in a new trace, given execution traces of passing test cases. We evaluated the model and several less complex reference methods with respect to their ability to localize defects that occurred in the development history of AspectJ and Rhino. Our evaluation does not rely on artificially injected defects.
 We find that the Bernoulli graph model outperforms Delta LDA on Rhino and performs as good as Delta LDA on the AspectJ project, but in substantially less time. Delta LDA is based on a multinomial unigram model, which performs worst in our study. This gives raise to the conjecture that Delta LDA might benefit from replacing the multinomial model with a Bernoulli graph model. this conjecture would need to be studied empirically.
 The Bernoulli graph model outperforms the reference models with closed-form solution with respect to giving a high rank to code positions that lie in close vicinity of the actual defect. In order to find every second defect in the release history of Rhino, the Bernoulli graph model walks the developer through approximately 0.5% of the code positions and 1% in the AspectJ project.
 Acknowledgements Laura Dietz is supported by a scholarship of Microsoft Research Cambridge. Andreas Zeller and Tobias Scheffer are supported by a Jazz Faculty Grant. [1] James A. Jones and Mary J. Harrold. Empirical evaluation of the tarantula automatic fault-[2] Ben Liblit, Mayur Naik, Alice X. Zheng, Alex Aiken, and Michael I. Jordan. Scalable statis-[3] Trishul Chilimbi, Ben Liblit, Krishna Mehra, Aditya Nori, and Kapil Vaswani. Holmes: Ef-[4] David Andrzejewski, Anne Mulhern, Ben Liblit, and Xiaojin Zhu. Statistical debugging using [5] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet allocation. Journal of [6] Tom Minka and John Winn. Gates. In Advances in Neural Information Processing Systems , [7] Hal Daume III. Hbc: Hierarchical Bayes Compiler. http://hal3.name/HBC, 2007. [8] Andrew McCallum and Kamal Nigam. A comparison of event models for Naive Bayes text [9] Hyunsook Do, Sebastian Elbaum, and Gregg Rothermel. Supporting controlled experimenta-[10] Lionel C. Briand. A critical analysis of empirical research in software testing. In Proceedings [11] Ben Liblit, Mayur Naik, Alice X. Zheng, Alex Aiken, and Michael I. Jordan. Public deploy-[12] Valentin Dallmeier and Thomas Zimmermann. Extraction of bug localization benchmarks from
