 Text categorization is an important research area in many Information Retrieval (IR) applications. To save the storage space and computation time in text categorization, efficient and effective algorithms for reducing the data before analysis are highly desired. Traditional techniques for this purpose can generally be classified into feature extraction and feature selection . Because of efficiency, the latter is more suitable for text data such as web documents. However, many popular feature selection techniques such as Information Gain (IG) and 2  X  -test (CHI) are all greedy in nature and thus may not be optimal according to some criterion. Moreover, the performance of these greedy methods may be deteriorated when the reserved data dimension is extremely low. In this paper, we propose an efficient optimal feature selection algorithm by optimizing the objective function of Orthogonal Centroid (OC) subspace learning algorithm in a discrete solution space, called Orthogonal Centroid Feature Selection (OCFS). Experiments on 20 Newsgroups (20NG), Reuters Corpus Volume 1 (RCV1) and Open Directory Project (ODP) data show that OCFS is consistently better than IG and CHI with smaller computation time especially when the reduced dimension is extremely small. I.5.4 [ Pattern Recognition ]: Applications X  X ext processing. Algorithms, Performance. Feature Selection (FS), Feature Extraction (FE). Many information retrieval problems [1, 20] such as filtering, routing or searching for relevant information benefit from the text categorization research. For instance, building a news directory needs one to identify a modest number of training examples to train a classifier, and then classify the unknown data to populate the directory. However, with the explosive growth of the web data set, algorithms that can improve the classification efficiency while maintaining accuracy are highly desired [3, 23]. Dimension Reduction techniques have attracted much attention recently since effective dimension reduction make the learning task such as categorization more efficient and save more storage space [25]. Moreover, the lower dimension the reduced data is, the faster IR systems should be. The complexity of many learning algorithms [3] increase nonlinearly with increased data dimension. Thus efficient algorithms that can reduce the original data into a small dimensional space effectively are highly desired. Dimension reduction techniques can generally be classified into Feature Extraction (FE) approaches [13] and Feature Selection (FS) approaches [11, 25]. The traditional FE algorithms reduce the dimension of data by linear algebra transformations (such as Principal Component Analysis (PCA) [8], Linear Discriminant Analysis (LDA) [16] and Maximum Margin Criterion (MMC) [12, 24], etc.) or nonlinear transformations (Locally Linear Embedding (LLE) [21], ISOMAP [22], etc.). On the other hand, FS algorithms reduce the dimension of data by select features from the original vectors directly. Though the FE algorithms have been proved to be very effective for dimension reduction, the high dimension of data sets in the text domain often fails many FE algorithms due to their high computational cost. Thus FS algorithms are more popular for real life text data dimension reduction problems. In contrast to FE approaches, FS techniques aim to remove non-informative features according to corpus statistics. Many novel FS approaches, such as PCA based algorithm [15], Margin based algorithm [19], SVM-based algorithm [2], etc. were proposed in the past decades. In the text domain, the most p opular used FS algorithms are still the traditional ones such as Information Gain (IG), 2  X  -test (CHI), Document Frequency (DF) and Mutual This work is done at Microsoft Research Asia. Information (MI) [17, 25], etc. Information gain measures the number of bits of information obtained for category prediction by knowing the presence or absence of a term in a document. Given a corpus of training text, we compute the information gain of each term, and then remove those features whose information gain was less than some pre-determined threshold. The computation of CHI, DF, and MI are similar to that of IG. The differences are the approaches to rank features. However, MI are not comparable with IG, DF, and CHI on text categorization [25]. We use two of them, IG and CHI, as our baselines in this paper. Though IG and CHI are popular in text categorization, they are greedy in nature and thus their solutions are not optimal according to some criterion. In this paper, we propose a novel feature selection algorithm based on the Orthogonal Centroid algorithm [6, 14]. We call this algorithm as Orthogonal Centroid Feature Selection (OCFS). The main advantages of this algorithm are: 1), it is optimal according to the objective function implied by the original Orthogonal Centroid algorithm and thus it can get superior performance in sparse data sets of an extremely small dimension; 2), it is more efficient than the popular IG and CHI; 3), and it is very easy to be implemented with simple theoretical background. Experiments on 20 Newsgroups data, Reuters Corpus Volume 1 and ODP data show the efficiency and effectiveness of our proposed approach. The Orthogonal Centroid (OC) algorithm is a traditional FE algorithm by QR matrix decomposition. It has been proved to be very effective on text data [14]. However, the main drawback of it is the high computational complexity of QR matrix decomposition. This makes the OC algorithm not suitable for real life large scale web document categorization problems. In this paper, we first define the objective function implied by the orthogonal centroid computation. Then we give our OCFS algorithm by optimize the objective function in a discrete solution space. This rest of the paper is organized as follows. In Section 2, we give the mathematical notations used in this paper and our problem definition. In Section 3, we introduce some related work on text data feature selection. In Section 4, we describe our proposed OCFS algorithm. In Section 5, the experimental results on large scale data sets are given. Section 6 concludes our paper. In this paper, a corpus of documents are mathematically represented by a dn  X  term by document matrix dn XR is generated by the traditional TFIDF indexing in Vector Space Model (VSM) [5], where n is the number of documents, and d is the number of features (terms). Each document is denoted by a column vector , 1,2, , i xi n = L , and the th by , 1,2, , xk d = L . T X is used to denote the transpose of matrix X . Assume that these feature vectors are belonging to c different classes and the class size of the th j class is j n . Using represent class j , 1, 2 , , jc = L , the mean vector of the th j class is (1 / ) mnx mnx n nm == ==  X  X  X  . The dimension reduction problem could be defined as the finding of a function : dp f RR  X  , where p is the dimension of data after dimension reduction ( p&lt;&lt;d ), so that a document transformed into () p ii yfx R = X  . From the FE point of view, the dimension reduction problem aims to find an optimal transformation matrix dp WR  X   X  according to some criterion such that ( ) Tp ii i yfxWxR == X  , 1, 2, , in = L are the p -dimensional representation of original data. The solution space continuous and is consisted of all the real dp  X  matrices. On the other hand, from FS point of view, the purpose of dimension reduction is to find a subset of features indexed by , 1,2, , l kl p = L such that the low dimensional representation of Notice that FS could be formulated under the same model with FE to make the selection optimal according to some criterion. In other words, the goal of the FS problem is to find an optimal transformation matrix dp WR  X   X  % according to some criterion entries equal to zero or one and each column of W only one non-zero element. Then the low dimensional yfxWx xx x === % L . The solution space of the FS problem is discrete and consists of all matrices satisfy the constraint given above. We define this space as Following the notations and discussions above, we define the optimal feature selection problem for text data categorization as: Then we can transform the unlabeled d -dimensional data into a low p -dimensional space by applying T ii yWx = % and classify these unlabeled data in the p -dimensional space. An essential technique to improve the efficiency and effectiveness of the web document categorization problem is dimension reduction. Among them, feature selection approaches have attracted much attention due to their efficiency. In this paper, we involve two popular used feature selection algorithms, Information Gain (IG) and 2  X  -test (CHI), which have been proved to be effective [17, 25] in the text domain as our baselines. We next give a brief introduction on IG and CHI in this section. Following the notations above, information gain of a selected I GT Pc Pc PT Pc T Pc T where t is used to denote a unique term, ( ) I GT is the information gain of a term group, ( ) rj P c is the probability of class () P T is the probability of term group T and (|) ri corresponding conditional probability. Following the problem definition in Section 2, we define ( ) ( ) J WIGT = IG aims to find an optimal dp WH  X   X  % so that each document is problem and a greedy approach is typically used. Given a corpus of training text, we compute the information gain of each term by: ( ) ( ) log ( ) ( ) ( | ) log ( | ) I Gt Pc Pc Pt Pc t Pc t Then we remove those features whose information gain is less than some predetermined threshold. Obviously, the greedy IG is not optimal according to ( ) ( ) J WIGT = % . CHI is also aiming at maximizing a criterion ( ) J W details in this paper to save space. It is also a greedy algorithm to save the computation cost and thus is not optimal either. To a given term t and a category j c , suppose A is the number of times t and j c co-occur, B is the number of times the t occurs without C is the number of times j c occurs without t, D is the number of times neither j c nor t occurs. The 2  X  statistics is: each category in a training corpus, and then combine the category specific scores of each term into: The computational complexity of IG and CHI are very similar. The main computation time of them are spent on the evaluation of respectively with complexity O(cd) . The Orthogonal Centroid Feature Selection (OCFS) selects features optimally according to the objective function implied by the Orthogonal Centroid algorithm, which is the foundation of our approach. Thus in this section we introduce the original OC algorithm firstly. After that, we transform the OC algorithm into an optimization problem by giving the objective function implied by the OC algorithm. After that, we optimize this objective function in the discrete solution space dp H proposed OCFS algorithm. At the end of this section, we analyze the complexity and the choice of optimal dimension by OCFS. Orthogonal Centroid (OC) algorithm is a recent pr oposed supervised feature extraction algorithm which utilizes orthogonal transformation on centroid [6, 14]. It has been proved very effective for classification problems on text data [6, 14] and is based on the Vector Space Computation in linear algebra by QR matrix decomposition [7]. The OC algorithm also aims to find the transformation matrix dp WR  X   X  that maps each column xR  X  of dn XR  X   X  to a vector p i yR  X  . However, the time and space cost of QR decomposition can not meet the requirements of web documents since the scale of web documents are growing rapidly nowadays. To address this issue, we formulate the OC algorithm as a feature selection problem and find the solution in a corresponding discrete sp ace. Theorem 1 intr oduces the objective function implied by OC. And then we propose the OCFS algorithm to optimize this objective function.
 Theorem 1 The solution of Orthogonal Centroid algorithm equals to the solution of the following optimization problem, arg max ( ) arg max ( ) T b J WtraceWSW = , subject to where function of LDA [16], is called inter-class scatter matrix. The detailed proof of this theorem could be found in [6, 14]. This criterion defined by inter-class scatter matrix intuitively aims at making the data of different classes as far as possible in the transformed low dimensional space through the optimal projection matrix W . Based on the feature selection problem defined in Section 2, we use the criterion ( ) J W implied by the OC algorithm optimize ( ) J W in dp H  X  in the next subsection. According to the discussion in Section 2, the feature selection problem according to criterion ( ) J W % is an optimization problem: arg max ( ) arg max ( ) T b J WtraceWSW = %%% subject to Suppose {, 1 , 1,2, ,} ii K kkdi p = X  X  X = L is a group of indices of features. Since W % belongs to space dp H  X  , it must be a binary matrix with its elements of zero or one, and there are one and only one non-zero element in each column. Following this constraint, let { } k i
Ww = % % and let: Then, indices {, 1 , 1,2, ,} ii K kkdi p = X  X  X = L can maximize ==  X   X  X  X  , the binary matrix W % generated by K following (1) should maximize, ( ) ( ) T b J WtraceWSW = index set K should be the optimal solution of the feature selection problem according to the criterion ( ) J W % subject to The problem now is to find an index set K such that could be solved simply by finding the p largest ones from optimal feature selection algorithm according to ( ) J W details of the OCFS algorithm are given in Table 1. From Table 1, the selected index set K can define a matrix W (1). This matrix is the solution of the optimization problem ( ) arg max ( ) T b J WtraceWSW = %%% in the space dp WH  X   X  % . We demonstrate our algorithm with a simple example. The UCI machine learning dataset is a repository of databases, domain theories and data generators that are used by the machine learning community for the empirical analysis of machine learning algorithms 2 . We use the IRIS data set of UCI as our sample data to show how our algorithm works. The documentation of this data set is complete, and there are 3 classes, 4 numeric attributes and 150 samples. There are 50 samples in each class. Class 1 is linearly separable from the other two, but the other two are not linearly separable from each other. Wit hout loss of generality and for intuition, we do not split the IRIS into training and testing data in this example. Suppose P=2 , Following our proposed OCFS: Step 1, computing the class mean of each class respectively; Step 2, computing the mean of all the 150 samples; http://www.ics.uci.edu/~mlearn/MLRepository.html Step 3, computing the feature scores of all the features; Step 4, selecting the features corresponding to the indices of the 2 largest ones among { ( ) | 1 4} Ssi i = X  X  X  . Then represent the original data with these 2 features. It is obvious that we should preserve the third and the first features here. The OCFS aims at finding out a group of features from all the features of original data such that this group of features could maximize the ( ) ( ) T b J WtraceWSW = %%% in space Intuitively, OCFS aims at finding out a subset of features that can make the sum of distance between all the class means maximized in the selected subspace. Step 1 is used to compute all the class means and then use the class means to represent different classes. Step 2 is used to calculate the mean of all the samples and then the sum of distance between all the class means can be computed by 
Step 1, compute the centroid i m i=1,2,...,c of each class for Step 2, compute the centroid m of all training samples; 
Step 3, compute feature score 2 1 () ( )
Step 4, find the corresponding index set K consisted of the p computing the distance between each class mean and the mean of all samples. In step 3, the score of a feature is in fact the weighted sum of distance among all the class means along the direction of this feature. Step 4 is used to select the directions with maximum sum of distance. Our theoretical analysis above could prove that the features selected this way are optimal according to our proposed criterion. Figure 1 shows the 2-dimensional visualization of IRIS by select different 2 features.
 The main computation cost of OCFS is spent on the calculation of each feature score. The algorithm tells us that its time complexity is O(cd) which is the same as its counterparts: IG and CHI. However OCFS only need to compute the simple square function instead of some functional computation such as logarithm of IG. Thus though the time complexity are the same, OCFS should be much more efficient than IG and CHI. Experiments tell us that OCFS can process a dataset with about half time in contrast to IG and CHI. OCFS is also robust since OCFS focuses only on the mean of each class and all samples (see Table 1). That means that a little amount of mislabeled data could not affect the final solution, i.e. the robustness is determined by the algorithm itself. A question regarding OCFS is how to determine the optimal number of features to be selected. We use the energy function approach to solve this problem just like what the Principal Component Analysis [8] has done to select the subsp ace dimension. Without loss of generality, suppose the feature score of function is defined as: Giving a threshold such as T =80%, i.e. the proportion of energy to be preserved after the feature selection procedure, we can get the optimal number of features * p by: Note that the larger the threshold T is, the more features will be selected and vice visa. This paper is not focusing on the optimal subspace dimension. We only compare the performance of different approaches on a given extremely low dimensional space. In this section, we conduct our experiments on three real large scale text data sets to show the performance of OCFS. We first describe the experiments setup, then give the experimental results, and finally discuss the results. To demonstrate the efficacy of OCFS, we performed experiments on three data sets: 20 Newsgroups [9], Reuters Corpus Volume 1 (RCV1) [10], and Open Directory Project (ODP) 3 .  X  20 Newsgroups. The 20 Newsgroups data consists of Usenet articles Lang collected from 20 different newsgroups.  X  X ver a period of time 1000 articles were taken from each of the newsgr oups, which make an overall number of 20000 documents in this collection. Except for a small fraction of the articles, each document belongs to exactly one newsgroup. X  4 In this paper, we select the five classes of comp.os.mswindows.misc, (3), comp.sys.ibm.pc.hardware, (4), comp.sys.mac.hardware, and (5), comp.windows.x as our data set. 1000 samples for each class. The dimension of data is 131,072 by the TFIDF indexing. We use the "bydate" version of data whose training and testing data are split previously by the data provider.  X  Reuters Corpus Volume 1 Reuters Corpus Volume 1 (RCV1) data set which contains over 800,000 documents and the data dimension is about 500,000. We choose the data samples with the highest four topic codes (CCAT, ECAT, GCAT, and MCAT) in the  X  X opic Codes X  hierarchy, which contains 789,670 documents. Then we split them into 5 equal-sized subsets, and each time 4 of them are used as the training set and the remaining ones are left as the test set. The experimental results reported in this paper are the average of the five runs. Moreover, we use this dataset as a single label problem, i.e. we only keep the first label if a sample is multi-labeled.  X  Open Directory Project Open Directory Project (ODP) consists of web documents crawled from the Internet. In this paper, we use the first layer ODP and other non-English documents thus involve 13 classes: Arts, Business, Computers, Games, Health, Home, Kids and Teens, News, Recreation, Science, Shopping, Society, Sports. preprocessing of classification problems. Among them, Information Gain (IG) and 2  X  -test (CHI) are dominant in the area of text categorization since they have been proved to be very effective and efficient. Moreover, they are two of the most widely used dimension reduction algorithms for real web document categorization problems. Thus in this paper, we select the IG and CHI as our baseline algorithms. IG and CHI are the state-of-the-art text feature selection approaches. In this paper, we applied them on all training data to generate 10, 100, 1000 and 10000 dimensional sp aces. The original IG and CHI selects a given number of features for each class and can not select a given number of features globally; To control the global number of features selected by IG and CHI, in http://rdf.dmoz.org/ http://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups this paper we select the given number of features by compute their average score (weighted summation [4]) in different classes and select the largest ones to meet the given number. Precision, Recall and F1 are the most widely used performance measurements for text categorization problems nowadays. Precision could be computed by the number of correctly categorized data over the number of all testing data. Recall could be computed by the number of correctly categorized data over the number of all the assigned data. F1 is a common measure in text categorization that combines recall and precision. In this paper, we use Micro F1 measure as our effectiveness measurement which combines recall and precision into a single score according to the following formula: where P is the Precision and R is the Recall. In the figures of this section, we use F1 to denote Micro F1. The efficiency is evaluated by the real CPU runtime. We ignore the I/O time and record only the time of the feature selection procedure. We apply the OCFS on all the training data to select 10, 100, 1000 and 10000 features to compare the effectiveness with baselines. In all our experiments, we use a single computer with Pentium(R) 4 2.80GHz CPU, and 2GB of RAM, to conduct the experiments. The experiment consists of the following steps: z Apply the feature selection algorithm on a specific size of the z Transform all the training data into the low dimensional space; z Train the SVM classifier by SMO [18] (linear kernel is used z Transform all the testing data to the selected low dimensional z Evaluate the classification performance, using Micro F1, on Besides the Micro F1 and CPU runtime, we also give the number of overlap among the features selected by different feature selection approaches in Table 2. Moreover the selected different terms by IG, CHI and OCFS are given in Table 3 while their overlaps are ignored. Each line of Table 2 shows the number of overlap features selected by two algorithms in all dimensions involved. It can be seen that most features selected by IG and CHI are the same. On the other hand, about half of the features selected by OCFS are different with its counterparts of IG and CHI. It is very interesting that they have comparable performance while the features used are very different in 100, 1000 and 10000 dimensional spaces. The OCFS is outstanding in 10 dimensional case. There are four features (terms) selected by OCFS, IG and CHI are different in the 10 dimensional space, we list their corresponding terms in Table 3 to feed the reader for intuition. OCFS vs IG 7 62 532 6494 OCFS vs CHI 6 55 509 6530 IG vs CHI 9 85 831 9096 OCFS drive file edu card CHI Lcs 3d1 x11r5 motif The classification performance on RCV1 data is summarized in Figure 4. From this figure, we can infer that the low dimensional space selected by OCFS have better performance than its counterpart selected by the popular used IG and CHI. The efficiency is showed in Figure 5. These indicate that in practice on web scale data, the performance of OCFS is outstanding. Though the efficiency improvement is only about half, to a real large scale problem, save half time is significant improvement. The performance of different feature selection algorithms on ODP data are reported in Figure 6. To save space, we do not report the time used since the time of OCFS is still about half of its counterpart of IG and CHI. Since the ODP is very large scale data, too low dimension such as 10 could make most of its sample to be zero vectors no matter which feature selection approach used by our experiments. Thus we ignore 10 on ODP. To show the results in low dimensional space, we add the performance in 200 and 500 dimensional space in these experiments. From the experiments we can see that the proposed OCFS is consistently better than IG and CHI especially when the reduced dimension is extremely small for text categorization problems. On the other hand, it is more efficient than the others by using only about half of the time used by baselines to select good features. To very large scale data such as the rapid growth web data, saving about half of the computation time is valuable and exciting. From the dimension by Micro F1 figures (Figure 2, Figure 4, Figure 6) we can draw the conclusion that OCFS can get significant improvements than baselines when the selected subspace dimension is extremely small while get a little better performance when the selected subspace dimension is relative large. This phenomenon occurs due to the reason that when the selected feature dimension is small, the proposed OCFS, which is an optimal feature selection approach, can outperform the greedy ones. With the increasing number of selected features, the saturation of features makes additional features of less value. Then when the number of selected features is large enough, all feature selection algorithms involved can achieve comparable performance no matter they are optimal or greedy. From the tables we can see that our proposed optimal OCFS selects many different features from those by IG and CHI and these different features improved the performance of text categorization. In this paper, we proposed a novel efficient and effective feature selection algorithm, Orthogonal Centroid Feature Selection (OCFS), for text categorization. With the growing number of text documents on the Web, many traditional text categorization techniques fail to produce a satisfactory result in handling this scale of data due to their time complexity and storage requirements. The OCFS can help save both data storage space and computation time by feature selection. The OCFS is designed by optimizing the objective function of the effective Orthogonal Centroid algorithm. The main advantages of OCFS are: 1) it is optimal according to the objective function of the Orthogonal Centroid algorithm and thus it can get better performance when the features are not saturated in a extremely small dimensional space; 2) it is more efficient than the popular IG and CHI methods when handling the data at the web scale and has better performance; 3) it is easy to compute. In the future, we plan to extend our work to unbalanced data through revising the objective function. The other is to use our proposed approach as the first step of feature selection to generate an optimal group of candidate features, and then use other effective techniques such as IG to select features from the candidates to improve the performance. [1] Belkin, N.J. and Croft, W.B. Retrieval Techniques. Annual [2] Douglas H., Ioannis T. and Aliferis, C.F., A theoretical [3] Duda, R.O., Hart, P.E. and Stork, D.G. Pattern Classification [4] Franca D., Fabrizio S., Supervised term Weighting for [5] Greengrass, E. Information Retrieval: A Survey , 30 [6] Howland, P. and Park, H. Generalizing discriminant analysis [7] James E. Gentle, J. Chambers, W. Eddy, W. Haerdle, S. [8] Jolliffe, I.T. Principal Component Analysis . New York: [9] Lang, K. and NewsWeeder, Learning to Filter Netnews. In [10] Lewis, D., Yang, Y., Rose, T. and Li, F. RCV1: A new [11] Lewis, D., Feature Selection and Feature Extraction for Text [12] Li, H., Jiang, T., and Zhang, K., Efficient and Robust Feature [13] Liu, H. and Motoda, H. Feature Extraction, Construction [14] Jeon M., Park, H. and Rosen, J.B. Dimension Reduction [15] Malhi, A. and Gao, R.X. PCA-Based Feature Selection [16] Martinez, A.M. and Kak, A.C. PCA versus LDA. IEEE [17] Mitchell, T.M. Machine Learning . McGraw Hill, 1997. [18] Platt, J. Fast training of support vector machines using [19] Ran G.-B., Amir N. and Tishby, N., Margin based feature [20] Ricardo B.-Y. and Ribeiro N. B. Modern Information [21] Roweis, S.T. and Saul, L.K. Nonlinear Dimensionality [22] Tenenbaum J.B., Silva, V.d. and Langford, J.C. A global [23] Wang, G., Lochovsky, F.H. and Yang, Q., Feature Selection [24] Yan, J., Zhang, B., Yan, S., Chen, Z., Fan, W., Xi, W., Yang, [25] Yang, Y. and Pedersen, J.O., A comparative Study on 
