 Information retrieval systems are compared using evaluati on met-rics, with researchers commonly reporting results for simp le met-rics such as precision-at-10 or reciprocal rank together wi th more complex ones such as average precision or discounted cumula tive gain. In this paper, we demonstrate that complex metrics are as good as or better than simple metrics at predicting the perfo rmance of the simple metrics on other topics. Therefore, reporting of re-sults from simple metrics alongside complex ones is redunda nt. H.3.4 [Information Storage and Retrieval]: Systems and sof tware  X  performance evaluation .
 Measurement, performance, experimentation
The performance of information retrieval systems is assess ed using effectiveness metrics. Many metrics have been propos ed, amongst which a distinction can be made between relatively s imple metrics such as precision at 10 documents retrieved (P@10) a nd re-ciprocal rank (RR), and more complex metrics such as average pre-cision (AP), normalized discounted cumulative gain (nDCG) , and rank-biased precision (RBP) [Buckley and Voorhees, 2005, J  X rvelin and Kek X l X inen, 2002, Moffat and Zobel, to appear]. The simp le metrics, it is sometimes argued, capture some elementary us er be-havior such as  X  X ooks at the first page X  or  X  X tops at the first re le-vant document X  [Turpin and Scholer, 2006], while the other m et-rics make richer use of an arbitrarily long ranking containi ng (it is presumed) many relevant documents.

While the more complex metrics are recognized as being more stable and discriminative [Buckley and Voorhees, 2000], re searchers frequently report several different metrics when giving pe rformance results, and may even base inferences about system properti es on inconsistencies between the metrics. However, even if one a ccepts the premise that a given, simple metric is perfectly represe ntative of user experience (a premise that we do not support), in retr ieval experiments what is of interest is not how well a system perfo rms on the particular set of topics included in the test collecti on, but rather, how reliably these results predict system performa nce on other topics and other collections.

Here we demonstrate that the more complex metrics are in fact as good as or better than the simple metrics at predicting the per-formance of the same simple metrics on new topics, presumabl y because the former incorporate more information about over all sys-tem performance than do the latter. Therefore, provided eno ugh in-formation  X  in particular a sufficient depth of judgments  X  is avail-able, only the complex metrics should be reported, and confli cting results from the simpler metrics should be either discounte d, or in-terpreted as an indication that no conclusions can be drawn. We use the submitted runs and relevance judgments from the AdHoc Track of TREC 8 and the Terabyte Track of TREC 2004. The TREC 8 experimental set has 50 topics and 129 systems; the TREC 2004 experimental set has 49 topics and 70 systems.

In a retrieval experiment, systems are ranked by the mean of t heir per-topic performance measures. Predictive power can be un der-stood as measuring how reliably the ranking based on the expe r-imental topics predicts system ranking on other, untested t opics. To estimate this from the TREC experimental collections, we ran-domly partition the topic set in half, and calculate the Kend all X  X   X  correlation on the system rankings based on one partition an d the system rankings based on the other. This random partitio ning is performed 2 , 000 times for each data point. The mean of these Kendall X  X   X  values is then the statistic of predictive power, which we denote as  X  . (In separate experiments we have explored halv-ing the topic set as an approximation of sampling from a large population, following up the tests of Voorhees and Buckley [ 2002], Sanderson and Zobel [2005], and Sakai [2005]. Our conclusio ns are that it is a reliable approach.) Being based on correlati on, is reflexive:  X  A,B =  X  B,A . A metric X  X  predictivity, therefore, must be assessed in relation to all other metrics, including its self-predictivity.

In any TREC experiment, there are poor runs that may have some programming bug or use an unsuccessful experimental algori thm. Since such systems are consistently lowly ranked, no matter what the topic, they inflate  X  and hence  X  . To prevent this effect, only the top 75% of systems by AP are included in our experiments. T he trends reported below are also observable with the full syst em sets.
Table 1 gives the predictive power of the simple and complex metrics for the TREC 8 experimental set. The diagonal values indi-cate that, as expected, the simple metrics are poorer self-p redictors, Table 1: Predictive power  X  of different metrics on the top 75% of TREC 8 AdHoc Track systems, calculated from 2 , 000 random repartitionings of the topic set.
 Table 2: Predictive power  X  of different metrics on the top 75% of TREC 2004 Terabyte Track systems. and therefore less reliable, than the more complex measures , with RR being particularly poor. More significantly, the results show that the complex metrics are as good at predicting the simple mea-sures as the simple measures are at predicting themselves. T he re-sults for TREC 2004, in Table 2, tell a similar story for P@10. They also show that the complex measures and even P@10 are better a t predicting RR than RR is at predicting itself; despite the em phasis that has been given to RR in some experiments, we infer that it has poor predictivity. Note that on TREC 2004 the RBP metric (wit h p = 0.95) performs relatively poorly, compared to TREC 8, possi -bly due to the large number of relevant documents in the TREC 2 004 Terabyte collection flooding the higher ranks of each run. Ot her experiments not reported here suggest that p = 0.98 yields results consistent with AP and nDCG.

Figure 1 explores the effect on predictive power of increasi ng the size of the topic sets. When there are only 5 topics in each subset, predictive power in all combinations is low, but the self-predictive powers of nDCG and P@10 are similar, while P@10 is better at predicting itself than nDCG is at predicting P@10. As topic set size increases, nDCG X  X  self-predictive power inc reases more rapidly than that of P@10, and gradually nDCG becomes a (marginally) better predictor of P@10 than P@10 is of itself . Other complex metrics also demonstrate similar increases in rela tive pre-dictive power against P@10 as topic subset size increases on the TREC 8 dataset. In contrast, on the TREC 2004 data set, the com -plex metrics have greater outperformance over P@10 at small er topic subset sizes, with the gap narrowing as more topics are added. The reasons for this differing behavior merit further inves tigation.
Given that the aim of reporting results is to demonstrate tha t a new system is expected to be better, on unseen data, than a bas eline system is according to some measure, and that nDCG and AP are as good at predicting ordering by P@10 as P@10 is, we conclude that reporting P@10 is redundant. For the same reasons, but w ith even more compelling evidence, we conclude that reporting R R is also unnecessary.

In a comparison of a small number of systems, as is typical of the results section of a research paper, the different metri cs can give inconsistent results. A tempting interpretation is that th e improve-ment is good for precision but poor for recall, or similar. Ou r inves-Figure 1: Predictive power  X  of nDCG and P@10 of themselves, and nDCG of P@10, with different topic subset sizes, on the TREC 8 runs. tigation shows that such inferences are probably wrong, and that a better interpretation is that inconsistency in results mea ns that they cannot be used to draw any substantial conclusions. That is, since experiments show that even significant results are not neces sarily predictive, a conflict may mean that the number of topics was i nsuf-ficient to infer true system behavior. Where such a conflict oc curs, the greater overall predictivity of the complex measures ob served in Tables 1 and 2 means that they are more likely to be correct t han the simple measures that are often given equal prominence.
We note, as a final remark, that we have not addressed the more complex issue of how best to design an experiment so as to obtain the maximum amount of predictivity given a resource cost in t erms of judgments to be performed. When all costs are taken into ac -count, it may well be that computing P@10 over a large number o f queries is both more economical and more predictive than com put-ing a more intricate measure over a smaller number of queries .
