 Goal-oriented Requirements Engineering (G ORE) has a great impact and importance in the Requirements Engineering community, helping in identifying, organizing, and structuring requirements, as well as in exploring and evaluating alternative solutions to a problem [1]. There are several GORE approaches, such as i* [2], KAOS [3], and GRL [4]. When modelling real-world systems with a GORE approach, the models can quickly become very complex. A common challenge for the GORE approaches is to manage the complexity of their models. While real-world problems have an unavoid-able essential complexity, we need to mini mize, as much as possible, the accidental complexity introduced by the way we model those problems [5]. 
A possible way of minimizing the accidental complexity of a model is to improve its modularity. In particular, this can be achieved by identifying model refactoring opportunities. In this paper, we focus on the i* framework, and how we can manage the accidental complexity of i* models. In order to identify refactoring opportunities for these models, we define a metrics suite for assessing their complexity and the complexity of the elements defined in them. By collecting such metrics on several different models, we are able to establish a typical usage profile of the modelling mechanisms. collected from different model elements. Fo r example, the number of goals and tasks for a system agent may indicate whether this agent holds too many responsibilities in the system. This can hint the modeler for a refactoring opportunity where this agent should in fact be decomposed into several sub-agents. 
The objective of this paper is to provide a metrics suite, along with the correspond-ing tool support, targeted to the measurement and analysis of the complexity of i* models, with the goal of identifying refactoring opportunities to improve the modular-ity of those models. The identification of such opportunities can be useful during the development of the system, where a better modularization can lead to a sounder dis-tribution of responsibilities among the system components. If performed in a timely the model X  X  accidental complexity. Refactorin g opportunities identifi cation is also an asset in the context of preventive maintenance, as a facilitator for future requirements changes. 
Our metrics suite is integrated in an eclipse-based i* editor, so that metrics can be computed during the requirements modelling process, whenever the requirements engineer requests them. The metrics are defined using the Object Constraint Lan-guage (OCL) [6] upon the i* metamodel. This makes our metrics set easily extensible, as improving the metrics set can be done by adding new OCL metrics definitions to the ones presented in this paper. 
In [7], we proposed and validated a metrics suite for evaluating the completeness and complexity of KAOS goal models, formally specified (using OCL) and incorpo-rated in a KAOS modelling tool. The metrics suite was evaluated with several real-world case studies. The work described in this paper shares a common approach to metrics definition and tool implementation. However, the goals and structure of the KAOS approach are significantly different from those of the i* framework. In particu-lar, i* has a modularity mechanism  X  the actor X  X  boundaries  X  which is not present in KAOS, that paves the way for a significantly different approach to modularity, by encapsulating model elements within the actor s boundaries. This is reflected in the choice of relevant complexity metrics. Actor X  X  boundaries are a key mechanism in the modularity of i* models. 
This paper is organized as follows. Section 2 describes background information on the i* framework. Section 3 describes the metrics set, defined using the Goal-Question-Metrics approach, and a concrete ex ample of its application to a real-world model. Section 4 reports the evaluation pro cess, including a presentation of the case studies used, the results obtained by applying the metrics on those case studies, and a discussion on the results. Section 5 discusses the related work. Section 6 draws some conclusions and points out directions for future work. While the paper is self-contained, additional information such as the complete i* metamodel, the detailed specification of auxiliary metrics, and the fully detailed statistical analysis of the case studies presented in this paper can be found in this paper X  X  companion site 1 . i* [2] was developed for modelling and reasoning about organizational environments and their information systems. It focuses on the concept of intentional actor . Actors in their organizational environmen t are viewed as having intentional properties such as goals , beliefs , abilities and commitments . i* has two main modelling components: the Strategic Dependency (SD) model and the Strategic Rationale (SR) model. The SD model describes the dependency relationships among the actors in an organizational context. In this model, an actor (called depender ) depends on another actor (called The SR model provides a more detailed level of modelling than the SD model, since it focuses on the modelling of intentional elements and relationships internal to actors. Intentional elements ( goals , softgoals , tasks and resources ) are related by means-end or decomposition links. Means-end links are used to link goals (ends) to tasks (means) in order to specify alternative ways to achieve goals. Decomposition links are used to decompose tasks. A task can be decomposed into four types of elements: a subgoal , a subtask , a resource , and/or a softgoal . Apart from these two links, there are the con-tribution links , which can be positive or negative . In this work we are particularly interested in assessing the complexity of i* models. To support this, we needed a flexible platform upon which we could define our me-trics set. To the best of our knowledge, none of the existing i* tools provides adequate support for a flexible definition of such me trics (detailed comparison of the existing i* tool support can be found in [8, 9]). One of the important requirements of the tool was that it should be extensible, so that new metrics (which can potentially target different quality attributes) can be easily added. To fill this gap, we implemented an Eclipse-based i* editor using Epsilon [10], EMF/GMF [11, 12] and Ecore Tools [13]. 
Figure 1 presents a fragment of the i* metamodel implemented in our tool, show-ing only the concepts which will be used in the metrics definitions proposed in this paper. This metamodel is the basis for the tool support for the specification of i* models, and their evaluation with model metrics. The root of the metamodel is the metaclass ISTAR , which contains all the nodes and relationships of an i* model. This top-level metaclass serves as a basis for model analysis. The remaining metaclasses can be easily mapped into some of the concepts described earlier in this section. metrics-based analysis framework for i* models, using the Goal-Question-Metric (GQM) approach [14]. Table 1 summarizes the GQM-based proposal for a set of me-trics that will allow satisfying the goal of complexity evaluation . The first column presents questions that will allow evaluating whether the overall goal is being achieved. The second column shows a set of metrics that provide quantitative infor-mation to answer the corresponding question. Q1 concerns complexity, as perceived when regarding the model as a whole. In particular, we are interested in the number of actors, elements, and their ratio, within a model. The remaining questions are targeted to assessing the complexity of model elements, namely the amount of responsibilities supported by an actor ( Q2 ), and the number of decompositions of actor X  X  goals ( Q3 ), softgoals ( Q4 ) and tasks ( Q5 ). For each of these elements-centered questions, we define a basic metric (e.g. NEA , for Q2 ) and three additional distribution metrics pre-senting the minimum , maximum and average values for the basic metric. 3.1 Metrics Definition In Table 2 we present the metrics outlined in the previous section. For each question definition using OCL upon the metamodel fragment presented in figure 1. When required, we include pre-conditions in the formal definition. For example, when de-typical pre-condition is to ensure that there are goals , sofgoals , or tasks , to be decom-posed. Elements without decompositions may have been modeled in order to be final elements. It would not make sense analyzing the extent to which they are decom-posed. For the sake of brevity, we omit trivial auxiliary metrics definitions with basic metrics, can be found in the paper X  X  companion site. 
Regarding question Q1 , the values of NAct (number of actors) and NElem (number of elements) are measures for the SD/SR model size. Size can be used as a surrogate for overall model complexity, and used to compare the complexity among different models. For example, different candidate models for the same system can be com-pared, using these metrics, with respect to their overall complexity. 
Concerning question Q2 , a high value for NEA (number of elements of an actor) can be an indicator that a particular actor has too much responsibility. The minimum , maximum and average values help the requirements engineer recognizing cases where the responsibility is higher than expected. Complexity can also be used for supporting project estimation efforts. 
Questions Q3 , Q4 and Q5 , provide different perspectives on the complexity asso-ciated with a particular actor. The value of NDG (number of decompositions of an actor X  X  goal), presented in Q3 , measures the complexity of the goal decompositions associated with an actor . The value of NDS (number of decompositions of an actor X  X  softgoal), presented in Q4 , measures the complexity of the softgoal decompositions associated with an actor . Finally, the value of NDT (number of decompositions of an actor X  X  task), presented in Q5 , measures the complexity of the task decompositions associated with an actor . The minimum , maximum and average values for NDG , NDS and NDT help the requirements engineer identifying out of the ordinary goal , soft-goal , or task decomposition complexities, respectively. Note that the minimum value is computed only for goals, softgoals, or tasks, which are decomposed. As such, it excludes leaf elements in its computation. 3.2 Example Figure 2 shows a fragment of the Media Shop (MS) case study, whose main objective is to allow an online customer to examine the items in the Medi@ internet catalogue (books, newspapers, magazines, audio CD, videotapes, and the like) and place orders. The figure, taken from our tool, shows the actor Media Shop and some of its elements, as well as the model metrics. 
The tool allows to create i* models using a visual language and provides metrics values for the model. These values can be updated at any time of the construction process. As such, they can be valuable to detect potential problems early in the process, such as a high accidental complexity caused by a modelling option. 4.1 Case Studies To evaluate the presented metrics, we modelled i* case studies, namely Media Shop (MS) [15], Newspaper Office (NO) [16], Health Care (HC) [2], Health Protection Agency (HPA) [17] and National Air Traffic Services (NATS) [17] with our tool, and then collected the corresponding metrics. The case studies MS, NO, and HC have been extensively used in the literature, while HPA and NATS are real-world case studies, also available in the literature. They target different domains and have differ-ent essential complexities. A common characteristic of these models is that they are available with full details, making them good candidates for evaluation. 4.2 Results and Discussion In this section we present the main findings from our statistics analysis of the col-lected metrics. The statistics data files and scripts for performing the statistics analysis outlined here can be found in the paper X  X  companion site. Concerning model size ( Q1, Fig. 3a and Fig. 3b), the NATS (National Air Traffic Services) system has, approximately, twice the size of the second largest system (HPA, Health Protection Agency). The HC (Health Care) system has less actors, but more elements than the MS (Media Shop) and NO (Newspaper Office) systems, which have a very similar size. In fact, if we compute the elements to actors ratio very similar ratios. This may suggest that HC could be an interesting candidate for the lowest element/actor density, suggesting a good overall modularity. 
This overview on complexity is but a first impression. We need to analyze more detailed features to get a clearer picture of the modularity profile of these systems. For each of the counting metrics NDG, NDS and NDT (number of decompositions of an actor X  X  goal, softgoal and task, respectively), we present here a boxplot chart with their distributions on the actors of their corresponding systems, where we can identify the outliers (denoted with O) and extremes (denoted with *), in Fig. 3d-f. 
A closer inspection on the boxplot graphs (Fig. 3d-f) shows that there are two ac-tors which present outlier, or even extreme values, in NDS and NDT. These should be our most likely candidates for further scrutiny. For example, the actor Civil ATCO (Civilian Air Traffic Controller), from the National Air Traffic Services system, has an outlier value for the softgoal decomposition (NDS) and an extreme value for the task decomposition (NDT) metrics. Civil ATCO is a crucial actor in that system, whose specification is much more complex than that of most other actors in the same system. There are at least two possible problems th at should be checked, concerning the Civil ATCO actor X  X  decomposition. A first potential problem is that this actor may have too many respons ibilities. A typical refactoring would be to decompose the actor to god classes [18] and their refactoring, in object-oriented design. Note that, some-one. In such case, this analysis is still useful, in the sense that it highlights an actor in the system which has an extremely high essential complexity associated with it. This may hint project managers to assign more resources to quality assurance activities (e.g. inspections and testing) to artifacts related to the implementation of the require-ments associated with this actor. 
It may also be the case that the requirements engineer may over-decompose these goals, softgoals, or tasks, by following a functional decomposition strategy, leading to poor modularity. This is similar to the functional decomposition anti-pattern [18], where the encapsulation principle is neglect ed. Another consequence is that the ab-straction level of the model lowers: including too many (design) details may obfus-cate the requirements model, making it harder to understand and evolve. Abstracting away the unnecessary detailed decompositions can improve the overall modularity of the requirements model. Horkoff and Yu [19] evaluate seven goal sa tisfaction analysis procedures using avail-able tools that implement those procedures. They evaluate three sample goal models. The results help to understand the ways in which procedural design choices affect analysis results, and how differences in analysis results could lead to different rec-ommendations over alternatives in the model. Compared to our work, they study a different aspect of goal modelling, i.e. goal satisfaction analysis, not complexity. DKL) framework. This framework provides an approach for extracting, coding and storing relational excerpts of design knowledge from academic publications. This framework was designed for knowledge reuse purposes. Our work could extend that framework by providing information about the complexity of those existing models. and unclear descriptions, duplicated information) and the removal of their causes can improve the quality of use case models. They describe the AIRDoc approach, which aims to facilitate the identification of potential problems in requirements documents using refactoring and patterns. To evaluate use case models, the AIRDoc process uses the GQM approach to elaborate goals and define questions to be addressed by me-trics. Their target quality attributes are reusability and maintainability, different from ours. Their metrics were neither formally defined nor implemented in a tool. requirements of a software process maturity model in order to support the application of GORE methodologies in industry scenarios. The proposed approach, called GO-MDD, describes a six-stage process that integrates the i* framework into a concrete MDD process (OO-Method), applying the CMMi perspective. The fourth stage of this process concerns the verification, analysis and evaluation of the models defined in the previous stages; and uses a set of measurements, specified with OCL rules, that eva-luate the completeness of the MDD model generation with respect to the requirements using GQM. Compared to ours, their approach focuses on a different set of metrics as their goal was to support the evaluation of i* models to generate MDD models. 
Franch and Grau [23] propose a framework for defining metrics in i* models, to analyze the quality of individual models, and to compare alternative models over certain properties. This framework uses a catalogue of patterns for defining metrics, and OCL to formulate these metrics. In a follow up work, Franch proposes a generic method to better guide the analyst throughout the metrics definition process, over i* models [24]. The method is applied to evaluate business process performance. Their approach is more focused on the process, and more generic, while we focus on mod-ularity assessment of i* models. models, formally specified (using OCL), implemented and incorporated in an eclipse based modelling tool. The metrics were proposed using the GQM approach. The se-lected questions allow evaluating the complexity of the model as a whole concerning and tasks). The set of metrics provide quantitative information to answer the corres-ponding questions. The contribution of this paper is that evaluating complexity at early stages to identify modularity problems of the models allows avoiding eventual extra costs in during the later stages of software development and also during soft-ware maintenance and evolution. The realization that the modularity of a require-ments model can be improved can trigger requirements refactoring opportunities, like decomposing a system X  X  actor using an is-part-of relationship between sub-actors, or abstracting over-decomposed goals, softgoals, or tasks. These metrics were validated by applying them to well-known industrial and academic case studies. The results of these metrics also reveal a pattern of usage in goal modelling concerning modularity of those models. 
For future work, we intend to replicate this evaluation with other i* models and ex-final aim is to provide a metrics-based modelling support in GORE tools. In particu-lar, with an increased number of evaluated models, we will be able to, for example, identify thresholds for suggesting merging and/or decomposing model elements to reduce complexity of an i* model. As a cross-validation for those thresholds, we plan to conduct an experiment with requirements engineers to assess the extent to which those thresholds are correlated with an increased difficulty in i* model comprehen-sion. We also plan to define and apply refactoring patterns for GORE models. Acknowledgments. The authors would like to thank FCT/UNL and CITI  X  PEst-OE/EEI/UI0527/2011, for the financial support for this work. 
