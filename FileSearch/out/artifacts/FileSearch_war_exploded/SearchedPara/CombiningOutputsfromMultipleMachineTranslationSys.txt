 In recent years, machine translation systems based on new paradigms have emer ged. These systems emplo y more than just the surf ace-le vel information used by the state-of-the-art phrase-based translation systems. For example, hierarchical (Chiang, 2005) and syntax-based (Galle y et al., 2006) systems have recently impro ved in both accurac y and scalability . Combined with the latest adv ances in phrase-based translation systems, it has become more attracti ve to tak e adv antage of the various outputs in forming consensus translations (Frederking and Nirenb urg, 1994; Bangalore et al., 2001; Jayaraman and La vie, 2005; Matuso v et al., 2006).

System combination has been successfully ap-plied in state-of-the-art speech recognition evalua-tion systems for several years (Fiscus, 1997). Ev en though the underlying modeling techniques are sim-ilar , man y systems produce very dif ferent outputs with approximately the same accurac y. One of the most successful approaches is consensus netw ork decoding (Mangu et al., 2000) which assumes that the confidence of a word in a certain position is based on the sum of confidences from each system output having the word in that position. This re-quires aligning the system outputs to form a con-sensus netw ork and  X  during decoding  X  simply finding the highest scoring path through this net-work. The alignment of speech recognition outputs is fairly straightforw ard due to the strict constraint in word order . Ho we ver, machine translation outputs do not have this constraint as the word order may be dif ferent between the source and tar get languages. MT systems emplo y various re-ordering (distortion) models to tak e this into account.

Three MT system combination methods are pre-sented in this paper . The y operate on the sentence, phrase and word level. The sentence-le vel combi-nation is based on selecting the best hypothesis out of the mer ged N-best lists. This method does not generate new hypotheses  X  unlik e the phrase and word-le vel methods. The phrase-le vel combination is based on extracting sentence-specific phrase trans-lation tables from system outputs with alignments to source and running a phrasal decoder with this new translation table. This approach is similar to the multi-engine MT frame work proposed in (Fred-erking and Nirenb urg, 1994) which is not capable of re-ordering. The word-le vel combination is based on consensus netw ork decoding. Translation edit rate (TER) (Sno ver et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis. All combination methods use weights which may be tuned using Po well X  s method (Brent, 1973) on -best lists. Both sentence and phrase-le vel combination methods can generate -best lists which may also be used as new system out-puts in the word-le vel combination.

Experiments on combining six machine transla-tion system outputs were performed. Three sys-tems were phrasal, two hierarchical and one syntax-based. The systems were evaluated on NIST MT05 and the newsgroup portion of the GALE 2006 dry-run sets. The outputs were evaluated on both TER and BLEU. As the tar get evaluation metric in the GALE program was human-mediated TER (HTER) (Sno ver et al., 2006), it was found important to im-pro ve both of these automatic metrics.

This paper is organized as follo ws. Section 2 describes the evaluation metrics and a generic dis-criminati ve optimization technique used in tuning of the various system combination weights. Sentence, phrase and word-le vel system combination methods are presented in Sections 3, 4 and 5. Experimental results on Arabic and Chinese to English newswire and newsgroup test data are presented in Section 6. The official metric of the 2006 DARP A GALE evaluation was human-mediated translation edit rate (HTER). HTER is computed as the minimum trans-lation edit rate (TER) between a system output and a tar geted reference which preserv es the meaning and fluenc y of the sentence (Sno ver et al., 2006). The tar geted reference is generated by human post-editors who mak e edits to a reference translation so as to minimize the TER between the reference and the MT output without changing the meaning of the reference. Computing the HTER is very time con-suming due to the human post-editing. It is desir -able to have an automatic evaluation metric that cor -relates well with the HTER to allo w fast evaluation of the MT systems during development. Correla-tions of dif ferent evaluation metrics have been stud-ied (Sno ver et al., 2006) but according to various internal HTER experiments it is not clear whether TER or BLEU correlates better . Therefore it is prob-ably safest to try and not degrade either .
The TER of a translation is computed as where erence translation . In the case of multiple ref-erences, the edits are counted against all references, translations and the final TER is computed using the minimum number of edits. The NIST BLEU-4 is a variant of BLEU (Papineni et al., 2002) and is com-puted as where the hypothesis given the reference and counts from multiple references are accumulated in estimating the precisions.

All system combination methods presented in this paper may be tuned to directly optimize either one of these automatic evaluation metrics. The tuning uses -best lists of hypotheses with various fea-ture scores. The feature scores may be combined with tunable weights forming an arbitrary scoring function. As the deri vatives of this function are not usually available, Brent X  s modification of Po well X  s method (Brent, 1973) may be used to find weights that optimize the appropriate evaluation metric in the re-scored -best list. The optimization starts at a random initial point in the rameter space, first searching through an initial set of basis vectors. As searching repeatedly through the set of basis vectors is inef ficient, the direction of the vectors is gradually mo ved toward a lar ger posi-tive change in the evaluation metric. To impro ve the chances of finding a global optimum, the algorithm is repeated with varying initial values. The modified Po well X  s method has been pre viously used in opti-mizing the weights of a standard feature-based MT decoder in (Och, 2003) where a more efficient algo-rithm for log-linear models was proposed. Ho we ver, this is specific to log-linear models and cannot be easily extended for more complicated functions. The first combination method is based on re-ranking a mer ged -best list. A confidence score from each system is assigned to each unique hypothesis in the mer ged list. The confidence scores for each hypoth-esis are used to produce a single score which, com-bined with a 5-gram language model score, deter -mines a new ranking of the hypotheses. 3.1 Hypothesis Confidence Estimation Generalized linear models (GLMs) have been ap-plied for confidence estimation in speech recogni-tion (Siu and Gish, 1999). The logit model, which models the log odds of an event as a linear function of the features, can be used in confidence estima-tion. The confidence for a system generating a where each system has weights , and is the th feature for system and hypothesis . The features used in this work were: 1. Rank in the system X  s -best list; 2. Sentence posterior with system-specific total 3. System X  s total score; 4. Number of words in the hypothesis; 5. System-specific bias.
 If the system did not generate the hypothesis , the confidence is set to zero. To pre vent overflo w in ex-ponentiating the summation in Equation 3, the fea-tures have to be scaled. In the experiments, feature scaling factors were estimated from the tuning data to limit the feature values between scaling factors have to be applied to the features ob-tained from the test data.

The total confidence score of hypothesis is ob-where is the number of systems generating the hypothesis (i.e., the number of non-zero for ) and is the number of systems. The weights through are three free parameters. These weights can balance the total confidence between the number of systems generating the hypothesis (votes), and the sum, max-imum and average of the system confidences. 3.2 Sentence Posterior Estimation The second feature in the GLM is the sentence pos-terior estimated from the -best list. A sentence posterior may simply be estimated from an -best list by scaling the system scores for all hypotheses to sum to one. When combining several systems based on dif ferent translation paradigms and feature sets, the system scores may not be comparable. The to-tal scores may be scaled to obtain more consistent sentence posteriors. The scaled posterior estimated from an -best list may be written as where the log-score system assigns to hypothesis . The scaling factors may be tuned to optimize the evalua-tion metric in the same fashion as the logit model weights in Section 3.1. Equation 4 may be used to assign total posteriors for each unique hypothesis and the weights may be tuned using Po well X  s method on -best lists as described in Section 2. 3.3 Hypothesis Re-ranking The hypothesis confidence may be log-linearly com-bined with a 5-gram language model (LM) score to yield the final score as follo ws where The number of words is commonly used in LM re-scoring to balance the LM scores between hypothe-ses of dif ferent lengths. The number of free pa-rameters in the sentence-le vel combination method is given by where is the num-ber of systems and is the number of features; i.e., polation weights (Equation 4) for the scaling factor estimation, GLM weights ( ), three free in-terpolation weights (Equation 4) for the hypothesis confidence estimation and two free LM re-scoring weights (Equation 6). All parameters may be tuned using Po well X  s method on -best lists as described in Section 2.

The tuning of the sentence-le vel combination method may be summarized as follo ws: 1. Mer ge indi vidual -best lists to form a lar ge 2. Estimate total score scaling factors as described 3. Collect GLM feature scores for each unique hy-4. Estimate GLM feature scaling factors as de-5. Scale the GLM features; 6. Estimate GLM weights, combination weights 7. Re-rank the mer ged -best list using the new Testing the sentence-le vel combination has the same steps as the tuning apart from all estimation steps; i.e., steps 1, 3, 5 and 7. The phrase-le vel combination is based on extracting a new phrase translation table from each system X  s tar get-to-source phrase alignments and re-decoding the source sentence using this new translation table and a language model. In this work, the tar get-to-source phrase alignments were available from the indi vidual systems. If the alignments are not avail-able, the y can be automatically generated; e.g., us-ing GIZA++ (Och and Ne y, 2003). The phrase trans-lation table is generated for each source sentence us-ing confidence scores deri ved from sentence poste-riors with system-specific total score scaling factors and similarity scores based on the agreement among the phrases from all systems. 4.1 Phrase Confidence Estimation Each phrase has an initial confidence based on the sentence posterior estimated from an -best list in the same fashion as in Section 3.2. The confi-dence of the phrase table entry is increased if several systems agree on the tar get words. The agreement is measured by four levels of similarity: 1. Same source interv al, same tar get words, and 2. Same source interv al, same tar get words, with 3. Ov erlapping source interv als with the same tar -4. Ov erlapping tar get words. to all the hypotheses in the system at the similar -ity level . Basically , if there is a similar phrase in a given hypothesis in the system to the phrase , the similarity score is increased by . Note that each phrase in one hypothesis is similar to an-other hypothesis at only one similarity level, so one hypothesis can contrib ute to at only one simi-larity level. The final confidence of the phrase table where are system weights and are similarity score weights. The parameters through late between the sum, average and maximum of the similarity scores. These interpolation weights and the system weights are constrained to sum to one. The number of tunable combination weights, in ad-dition to normal decoder weights, is where is the number of systems and is the number of similarity levels; i.e., weights, similarity score weights and two free in-terpolation weights. 4.2 Phrase-Based Decoding The phrasal decoder used in the phrase-le vel com-bination is based on standard beam search (K oehn, 2004). The decoder features are: a trigram lan-guage model score, number of tar get phrases, num-ber of tar get words, phrase distortion, phrase dis-tortion computed over the original translations and phrase translation confidences estimated in Section 4.1. The total score for a hypothesis is computed as a log-linear combination of these features. The fea-ture weights and combination weights (system and similarity) may be tuned using Po well X  s method on -best lists as described in Section 2.

The phrase-le vel combination tuning can be sum-marized as follo ws: 1. Estimate sentence posteriors given the total 2. Collect all unique phrase table entries from 3. Combine the similarity scores to form phrase 4. Decode the source sentences using the current 5. Estimate new decoder and combination Testing the phrase-le vel combination is performed by follo wing steps 1 through 4. The third combination method is based on confusion netw ork decoding. In confusion netw ork decoding, the words in all hypotheses are aligned against each other to form a graph with word alternati ves (in-cluding nulls) for each alignment position. Each aligned word is assigned a score relati ve to the votes or word confidence scores (Fiscus, 1997; Mangu et al., 2000) deri ved from the hypotheses. The decod-ing is carried out by picking the words with the high-est scores along the graph. In speech recognition, this results in minimum expected word error rate (WER) hypothesis (Mangu et al., 2000) or equi va-lently minimum Bayes risk (MBR) under WER with uniform tar get sentence posterior distrib ution (Sim et al., 2007).

In machine translation, aligning hypotheses is more complicated compared to speech recognition since the tar get words do not necessarily appear in the same order . So far, confusion netw orks have been applied in MT system combination using three dif ferent alignment procedures: WER (Bangalore et al., 2001), GIZA++ alignments (Matuso v et al., 2006) and TER (Sim et al., 2007). WER align-ments do not allo w shifts, GIZA++ alignments re-quire careful training and are not always reliable. TER alignments do not guarantee that similar but lexically dif ferent words are aligned correctly but TER does not require training new models and al-lows shifts (Sno ver et al., 2006). This work extends the approach proposed in (Sim et al., 2007). 5.1 Confusion Netw ork Generation Due to the varying word order in the MT hypotheses, the decision of confusion netw ork skeleton is essen-tial. The skeleton determines the general word order of the combined hypothesis. One option would be to use the output from the system with the best perfor -mance on some development set. Ho we ver, it was found that this approach did not always yield bet-ter combination output compared to the best single system on all evaluation metrics. Instead of using a single system output as the skeleton, the hypothesis that best agrees with the other hypotheses on aver-age may be used. In this paper , the minimum av-erage TER score of one hypothesis against all other hypotheses was used as follo ws This may be vie wed as the MBR hypothesis under TER given uniform tar get sentence posterior distri-bution (Sim et al., 2007). It is also possible to com-pute the MBR hypothesis under BLEU.
Finding the MBR hypothesis requires computing the TER against all hypotheses to be aligned. It was found that aligning more than one hypothesis ( pro ves the combination outputs. Ho we ver, only the rank-1 hypotheses were considered as skeletons due to the comple xity of the TER alignment. The con-fidence score assigned to each word was chosen to be rank of the aligned hypothesis in the system X  s -best. This was found to yield better scores than sim-ple votes. 5.2 Tunable System Weights The word-le vel combination method described so far does not require any tuning. To allo w a variety of outputs with dif ferent degrees of confidence to be combined, system weights may be used. A confu-sion netw ork may be represented as a standard word lattice with all paths tra veling via all nodes. The links in this lattice represent the alternati ve words (including nulls) at the corresponding position in the string. Confusion netw ork decoding may be vie wed as finding the highest scoring path through this lat-tice with summing all word scores along the path. The standard lattice decoding algorithms may also be used to generate -best lists from the confu-sion netw ork. The simplest way to introduce sys-tem weights is to accumulate system-specific scores along the paths and combine these scores linearly with the weights. The system weights may be tuned using Po well X  s method on -best lists as described in Section 2.

The word-le vel combination tuning can be sum-marized as follo ws: 1. Extract 10-best lists from the MT outputs; 2. Align each 10-best against each rank-1 hypoth-3. Choose the skeleton (Equation 8); 4. Generate a confusion netw ork lattice with the 5. Generate -best list hypothesis and score files 6. Estimate system weights as described abo ve; Arabic system A 42.98 49.58 59.73 20.36 system B 43.79 47.06 61.55 18.08 system C 43.92 47.87 60.81 18.08 system D 40.75 52.09 59.25 20.28 system E 42.19 50.86 59.85 19.73 system F 44.30 50.15 61.74 20.61 phrcomb 40.45 53.70 59.90 21.49 sentcomb 41.56 52.18 60.21 19.77 no weights 6 39.33 53.66 58.15 20.61 TER 6 39.41 54.37 58.21 20.85 TER 8 39.43 54.40 57.96 21.44 Table 1: Mix ed-case TER and BLEU scores on Arabic NIST MT05 (ne wswire) and the newsgroups portion of the GALE 2006 dry-run data. 7. Re-rank the -best list using the new weights. Testing the word-le vel combination has the same steps as the tuning apart from steps 6 and 7. Six systems trained on all data available for GALE 2006 evaluation were used in the experiments to demonstrate the performance of all three system combination methods on Arabic and Chinese to En-glish MT tasks. Three systems were phrase-based ( A , C and E ), two hierarchical ( B and D ) and one syntax-based ( F ). The phrase-based systems used dif ferent sets of features and re-ordering approaches. The hierarchical systems used dif ferent rule sets. All systems were tuned on NIST MT02 evaluation sets with four references. Systems A and B were tuned to minimize TER, the other systems were tuned to maximize BLEU.

As discussed in Section 2, the system combina-tion tuning metric was chosen so that gains were ob-serv ed in both TER and BLEU on development test sets. NIST MT05 comprising only newswire data (1056 Arabic and 1082 Chinese sentences) with four reference translations and the newsgroup portion of the GALE 2006 dry-run (203 Arabic and 126 Chi-nese sentences) with one reference translation were used as the test sets. It was found that minimiz-ing TER on Arabic also resulted in higher BLEU scores compared to the best single system. Ho we ver, Chinese system A 56.57 29.63 68.61 13.20 system B 56.30 29.62 69.87 12.33 system C 59.48 31.32 69.37 13.91 system D 58.32 33.77 67.61 16.86 system E 58.46 32.40 69.08 15.08 system F 56.79 35.30 68.08 16.31 phrcomb 56.50 35.33 68.48 15.88 sentcomb 56.71 36.24 69.50 16.11 no weights 6 53.80 36.17 66.87 15.90 BLEU 6 54.34 36.44 66.50 16.44 BLEU 8 54.86 36.90 66.45 17.32 Table 2: Mix ed-case TER and BLEU scores on Chi-nese NIST MT05 (ne wswire) and the newsgroups portion of the GALE 2006 dry-run data. minimizing TER on Chinese resulted in significantly lower BLEU. So, TER was used in tuning the com-bination weights on Arabic and BLEU on Chinese.
The sentence and phrase-le vel combination weights were tuned on NIST MT03 evaluation sets. On the tuning sets, both methods yield about 0.5%-1.0% gain in TER and BLEU. The mix ed-case TER and BLEU scores on both test sets are sho wn in Ta-ble 1 for Arabic and Table 2 for Chinese ( phrcomb represents phrase and sentcomb sentence-le vel combination). The phrase-le vel combination seems to outperform the sentence-le vel combination in terms of both metrics on Arabic although gains over the best single system are modest, if any. On Chi-nese, the sentence-le vel combination yields higher BLEU scores than the phrase-le vel combination. The combination BLEU scores on the newsgroup data are not higher than the best system, though.
The word-le vel combination was evaluated in three settings. First, simple confusion netw ork de-coding with six systems without system weights was performed ( no weights 6 in the tables). Second, system weights were trained for combin-ing six systems ( TER/BLEU 6 in the tables). Fi-nally , all six system outputs as well as the sen-tence and phrase-le vel combination outputs were combined with system weights ( TER/BLEU 8 in the tables). The 6-w ay combination weights were tuned on mer ged NIST MT03 and MT04 evaluation sets and the 8-w ay combination weights were tuned only on NIST MT04 since the sentence and phrase-level combination methods were already tuned on NIST MT03. The word-le vel combination yields about 2.0%-3.0% gain in TER and 2.0%-4.0% gain in BLEU on the tuning sets. The test set results sho w that the simple confusion netw ork decoding with-out system weights yields very good scores, mostly better than either sentence or phrase-le vel combina-tion. The system weights seem to yield even higher BLEU scores but not always lower TER scores on both languages. Despite slightly hurting the TER score on Arabic, the TER 8 combination result was considered the best due to the highest BLEU and sig-nificantly lower TER compared to any single sys-tem. Similarly , the BLEU 8 was considered the best combination result on Chinese. Internal HTER experiments sho wed that BLEU 8 yielded lower scores after post-editing even though no weights 6 had lower automatic TER score. Three methods for machine translation system com-bination were presented in this paper . The sentence-level combination was based on re-ranking a mer ged -best list using generalized linear models with fea-tures deri ved from each system X  s output. The com-bination yields slight gains on the tuning set. Ho w-ever, the gains were very small, if any, on the test sets. The re-rank ed -best lists were used success-fully in the word-le vel combination method as new system outputs. Various other features may be ex-plored in this frame work although the tuning may be limited by the chosen optimization method in the higher dimensional parameter space.

The phrase-le vel combination was based on de-riving a new phrase translation table from the align-ments to source pro vided in all system outputs. The phrase translation scores were based on the level of agreement between the system outputs and sentence posterior estimates. A standard phrasal decoder with the new phrase table was used to produce the fi-nal combination output. The handling of the align-ments from non-phrasal decoders may not be opti-mal, though. The phrase-le vel combination yields fairly good gains on the tuning sets. Ho we ver, the performance does not seem to generalize to the test sets used in this work. As usual, the phrasal decoder can generate -best lists which were used success-fully in the word-le vel combination method as new system outputs.

The word-le vel combination method based on consensus netw ork decoding seems to be very ro-bust and yield good gains over the best single sys-tem even without any tunable weights. The decision of the skeleton is crucial. Minimum Bayes Risk de-coding under translation edit rate was used to select the skeleton. Compared to the best possible skeleton decision  X  according to an oracle experiment  X  fur -ther gains might be obtained by using better decision approach. Also, the alignment may be impro ved by taking the tar get-to-source alignments into account and allo wing synon yms to align. The confusion net-work decoding at the word level does not necessarily retain coherent phrases as no language model con-straints are tak en into account. LM re-scoring might alle viate this problem.

This paper has pro vided evidence that outputs from six very dif ferent MT systems, tuned for two dif ferent evaluation metrics, may be combined to yield better outputs in terms of dif ferent evaluation metrics. The focus of the future work will be to ad-dress the indi vidual issues in the combination meth-ods mentioned abo ve. It would also be interesting to investigate how much dif ferent systems contrib ute to the overall gain achie ved via system combination. This work was supported by DARP A/IPT O Contract No. HR0011-06-C-0022 under the GALE program (appro ved for public release, distrib ution unlimited). The authors would lik e to thank ISI and Uni versity of Edinb urgh for sharing their MT system outputs.
