 1. Introduction
Data fusion has been used as an effective tool for improving information retrieval performance. Several IR researchers  X 
The Genetic Algorithm (GA) has been used in finding useful solutions to optimization and search problems. GAs generate optimum fusion weights for a weighted linear combination of retrieval scores of different runs.
Next, we study how the use of the top ranked documents on linear combination of scores can be used to get an optimal mum weights thus learned are tested on the runs. The idea of using the top k documents is modeled on Multi-armed Bandits problem ( Auer et al., 1995 ) where the gambler has no initial knowledge about the levers and tries to maximize the gain based on existing knowledge of each lever. Here we attempt to use partial knowledge about the IR performance of each on evaluation. We draw our motivation from this observation also.

We also study how the correlation between the scores of runs can help in removing the redundant runs in data fusion. We scores have similar contribution in fusion performance.

Our contributions made in the present paper are summarized below: 1. Given a set of runs, a GA based approach to finding the optimal weights for an efficient fusion of these runs on the 3. A new approach to determination of the runs that make insignificant contributions in fusion, and hence to determi-The rest of the paper is organized as follows:
We provide a discussion on the related works in Section 2 . In Section 3 , we describe the GA and discuss how this algo-imental results and a comparative study in Section 5 and conclude in Section 6 . 2. Related work
Work of different genres has been done on data fusion. Many new data fusion techniques have been proposed. On the other hand, approaches that focussed on improving the existing methods were also reported. Fox and Shaw proposed Comb-
MIN, CombMAX, CombSUM, CombANZ and CombMNZ algorithms based on linear combinations of scores ( Fox and Shaw, 1993 ). CombSUM and CombMNZ have emerged as effective methods based on linear combinations of scores. Lee (1997) per-formed an experiment on six submitted runs of TREC 3 and concluded that CombMNZ was slightly better than CombSUM. and no clear inference could be drawn about the supremacy of any single approach. Popular voting algorithms were also used effectively in data fusion. Montague and Aslam (2002) used popular voting method called Condorset method (named after French mathematician and philosopher Marquis de Condorset) to good effect in data fusion. The fusion algorithm was called Condorset fusion. Another voting algorithm, viz., Borda count (named after another French mathematician Jean-
Charles de Borda) was used by Aslam and Montague (2001) and was referred to hitherto as Borda fusion. Cormack et al. (2009) showed that Reciprocal Rank Fusion paired with learning to rank outperforms Condorset fusion and individual rank vance of documents based on the position in the ranked list. Khudyak Kozorovitsky and Kurland (2011) used document clus-ter-based approach (named ClustFuse) to find retrieval scores in the fused list. But there is a fundamental factor differentiating the fusion algorithms. CombSUM (and other Comb variants), ProbFuse and ClustFuse make use of the rele-use the rank of the documents in the ranked lists.

In the case of the algorithms that use the relevance scores, Lee (1997) introduced a score normalization scheme to the algorithms proposed by Fox and Shaw and showed that score normalization is important in data fusion. Similarly, Savoy (2004) used a new score normalization formula on linear combination fusion algorithms. He called the normalized score Z-Score. Montague and Aslam (2001) also studied different score normalization schemes.

Many researchers have tried to improve the fusion performance by optimizing the weights used in the linear combination the runs. Also, we learn the weights using the performance information from the top few documents per query. Previous we do a twofold training and testing on the queries.

Correlation among the runs has been used effectively by researchers in data fusion. Wu and McClean (2005) determined chose at most one run from any particular group for fusion and there were no marked difference in performance from the case when no discrimination was done based on groups. In this paper, we have studied how the redundant runs can be identified and removed from the fusion exercise without sacrificing retrieval performance in any significant way. 3. Genetic Algorithm 3.1. Introduction
Suppose f is a real-valued function defined on a bounded set A  X  X  a point y 0  X  X  y 0 1 ; y 0 2 ; ; y 0 p  X  in S such that f  X  y 0  X  X  max
For example, let us consider a function f defined on A  X  X  0 ; 10  X  0 ; 10 ( Fig. 1 )as f  X  x
Note that the maximum value of f occurs at x 0  X  X  5 ; 5  X  . Now, let the interval corresponding to each x the basis of binary strings of length 3 where the binary strings 000 and 111 represent the values 0 and 10 respectively.
In general, suppose M  X  X  b 1 b 2 b 3 ; b 4 b 5 b 6  X  is a grid point in S where the binary strings b z and z 2 be the two decimal values corresponding to the binary numbers b to f 0 ; 1 ; ; 7 g . Then  X  y 1 ; y 2  X  X  X  10 z 1 = 7 ; 10 z the grid point M  X  X  b 1 b 2 b 3 ; b 4 b 5 b 6  X  is in fact the value of f at  X  y x in A whenever needed. The details are provided in the sub-sections below.

An optimization approach is based either on exploration or on exploitation or on both. The GA is based on both these aspects and hence is capable of avoiding getting stuck at a local optimum. Mutation and crossover operations described are discussed later in this section.

The basic steps of the GA are: 3.1.1. Initialization
A initial population P 1  X f y 1 ; y 2 ; y m g of m grid points (or members) selected at random from S is formed where m ability 0.5 for all the positions in the binary string corresponding to each of the m members in P selection, crossover, mutation and elitism operations that are described below. The value of m is taken as 30 in our experiments. 3.1.2. Selection
The fitness value of a member y i of the population P 1 is defined as f  X  y member from P 1 is made in such a way that the probability of y fitness value is more likely to be selected. This process is repeated m times so that a new set P from P 1 . The two sets P 1 and P 2 are not necessarily the same. A member having a smaller fitness value in P disappear from P 2 . On the other hand, a member having a higher fitness value in P in P 3.1.3. Crossover site, the two members are split into heads and tails as ( head ( head 1 ; tail 2 ) and ( head 2 ; tail 1 ). Each pair of members undergoes the crossover operation with probability p their offspring (that is, the two corresponding binary strings remain unchanged). Let P selected site is 5. 3.1.4. Mutation over operation undergoes the mutation operation with probability p that the algorithm has made, unnecessary time is spent on exploration. In other words, we deliberately reduce the search space while making sure that the reduced search space contains the globally maximum value. 3.1.5. Elitism
In elitism operation, the member having the highest fitness value in the population in one generation is carried over to the population in the next generation replacing the member having the lowest fitness value. Thus, in any generation, the ues. Let P 5 be the population obtained after elitism operation. 3.1.6. Convergence
The role of mutation in GA has been that of restoring lost or unexplored genetic material into the population to prevent ability transform the GA into a purely random search algorithm, while some mutation is required to prevent the premature converges to the global maximum of the fitness value irrespective of the initial population.
We will now illustrate how the above operations in the GA described above, are actually implemented. For this, we con-size 4. Let us assume that these 4 randomly selected grid points representing population P the first entry in the fourth column representing population P having the highest probability is selected twice in population P survive in the selection process.

Now, the members in the fourth column are paired at random. An example of such pairing is shown in the fifth column. A tion 3.1.3 ). Suppose the two sites for the two pairs are 4 and 2 respectively. The population P bits of the last two entries in the sixth column are selected for mutation. The resulting population P in the seventh column. Now, note that the highest fitness value in the population P member having the lowest fitness value in the population P  X  X 100111 X  X  in P 4 . Thus, the final population of the current iteration is P values of the members of P 5 are shown in the last column.
 (eighth column). In the next generation, the new population P whole process is repeated.

Now we will discuss how the GA is implemented for our fusion data task. The value of L depends on the number of runs to we run the algorithm for a finitely many generations and the output of the algorithm is the member of the population for which the value f is the maximum. The number of generations needed for convergence of the GA depends largely on the number of runs to be fused. In our experiments, the number of generations varies from 25 to 1000. 3.2. Optimization
Let r 1 ; r 2 ; ; r N be N runs on a query set Q with scores s here is to find a linear combination (in fact, a weighted average) of s possible combinations. In other words, we would like to determine the weights w these optimal weights w i . However, for the GA, the search space needs to be unconstrained while the search space A of  X  w space A in N dimensions to an unconstrained search space B in  X  N 1  X  dimensions such that there is one-to-one and onto there is one-to-one and onto mapping between them. Formally, B and the corresponding mapping are defined as follows:
B  X  X  0 ; p = 2  X  0 ; p = 2  X  0 ; p = 2 (  X  N 1  X  times), and where h i 2 X  0 ; p = 2 for all i .

Let  X  h 1 ; h 2 ; ; h N 1  X  be a point in B and let  X  w g  X  w 1 s 1  X  w 2 s 2  X  X  w N s N denotes the score of the run r produced by the linear combination (fusion) of the runs r ; r 2 ; ; r N with respective weights as w 1 ; w 2 ; ; w N . Note that g is a function of  X  h documents for an information need q j 2 Q be m j . Suppose, for the fused run r ; R retrieved results. Now let a function f  X  h 1 ; h 2 ; ; h of retrieved documents that are relevant for q j at the point in the ranked list when we get to the document d to determine the values of h 1 ; h 2 ; ; h N 1 (equivalently, the values of w
We use the GA described above to maximize the function f  X  h ues of h 1 ; h 2 ; ; h N 1 maximizing f are obtained, the values of the optimal weights w (1) above. These optimal weights define the best linear combination of the runs r 4. Experimental setup 4.1. Using the top-ranked documents in fusion depths can be used in learning the fusion weights by the GA.

Table 4 shows the correlation coefficients between the MAPs of the fused runs at depth 1000 and the MAPs of the same runs at lower depths, viz., 100, 50 and 25. The MAPs even at lower depths show high correlation with the MAP values at depth 1000. This led us to believe that using only the documents at shallower depths may be sufficient in achieving good fusion runs by the GA.
 the fused run. That is, we want to maximize the MAP of the fused run. For each query q run will contain a ranked union of all the documents in the runs participating in fusion. This fused run f ranked pool of the top d documents (per query) in r 1 ; r learned by the GA be w d opt 1 ; w d opt 2 ; ; w d opt N . These optimal weights are then tested on the runs r retrieval performance.
 4.2. Score normalization different ranges of score are assigned to the retrieved documents in different runs. For example, in run r scores may range from 43.085 to 447.169 whereas in run r 2 be combined unless they are mapped to a common range, say, [0,1]. So, we use the following normalization scheme pro-posed by Lee (1997) :
So, a score s 1 , for a document d and run r 1 , is normalized to, say s norm max score are the minimum and maximum scores respectively among all the documents in the run for the given query q . score is mapped to 1 and the minimum score is mapped to 0. The intermediate values are adjusted accordingly. We do this normalization for each run and get a set of normalized scores for document d (for query q ): s norm form a weighted combination (say, g ) of the scores as g  X  w run for a given query q . Now, we determine the weights w for a given query, their scores in the runs where they do not appear, are set to zero. 4.3. Correlation between retrieval scores of a pair of runs
Let r 1 and r 2 be two runs and Q  X  q 1 ; q 2 ; ; q M be a query set. Let D  X  d at least one of the two runs for at least one query. Let us now consider a query q from Q . Then for each document d an ordered pair of scores ( s 1 i ; s 2 i ), where s 1 i is the score of d to the document in the run where it does not appear. For example, if document d appears in run r the rest of the pairs, we compute the correlation coefficient between the scores obtained by r 5. Experimental results decreasing order of MAP values. The total number of submitted runs for TREC 3, 5, 6 and 7 are 40, 61, 74 and 103 respec-tively. We used simple GA code implemented at Kanpur Genetic Algorithms laboratory found at http://www.iitk.ac.in/kan-gal/codes.shtml. We compared GA with the following baselines:
CombSUM ; CombMNZ and Z Score : These are unsupervised parameter-free methods ( Fox and Shaw, 1993 )( Savoy, 2004 ).

Linear combination method with performance level ( LC ) and performance level squared ( LC 2): These are supervised parameter-free methods ( Wu et al., 2009 ).
 for all the queries in Section 5.2 and on disjoint query sets in Section 5.3 .
 groups, as 3.
 Lambda Merge ( LambdaMerge ): This is another supervised method ( Sheldon et al., 2011 ). For LambdaMerge , we optimized MAP. Here, we used all the  X  X  X uery-document features X  X  and  X  X  X ating Features X  X . However, Gating features like IsRewrite,
RewriteScore, RewriteRank, Overlap@N and RewriteLen did not have much significance since our original query and the reformulated query were the same. In other words, we did not do any query reformulation and used the Lambda-Merge scheme as a baseline for data fusion method.

ClustFuse variants ClustFuseCombSUM and ClustFuseCombMNZ : These are unsupervised baselines ( Khudyak Kozorovitsky siderations were done to cope with the high resource complexity of their algorithm. So, we did our experiment with the used in ClustFuse, as mentioned in the paper, we performed K Nearest Neighbour algorithm where, the value of K was chosen as 10. The ClustFuse method incorporates a single free parameter, k . The value of k is chosen from the set f 0 ; 0 : 1 ; ; 1 g . 5.1. Genetic algorithm on all documents
First we consider d to be 1000 and the top 30 runs in terms of MAP. The results are shown in Table 5 which shows that GA produces numerical improvements in MAP, P@5 and P@10 over the best component run as well as the unweighted linear combination methods ( CombSUM ; CombMNZ and Z Score ), the linear combination method with performance level ( LC ) signed-rank test ( Siegel, 1956 ). In other words, the performance improvement (in terms of MAP, P@5 and P@10) achieved by GA over all the other runs shown in Table 5 is statistically significant.

However, for comparison with ClustFuse variants ClustFuseCombSUM and ClustFuseCombMNZ, we had to make some considerations. These have been already discussed when we described the baselines. Table 6 shows the comparisons of
GA with two variants of ClustFuse, viz., ClustFuseCombSUM and ClustFuseCombMNZ . Note that all the reported values are obtained on the basis of the top 50 documents in a run. We can see that GA outperforms both ClustFuseCombSUM and Clus-value &lt; 0.05) by Wilcoxon signed-rank test. 5.2. Genetic algorithm using top-ranked documents
Here the training was done at depths d  X  100, 50 and 25 while the testing was done at depth d = 1000. Tables 7 and 8 show the results. For all the depths and top 30 runs, GA produced numerical improvements in MAP, P@5 and P@10 over the best component run as well as CombSUM ; CombMNZ ; Z Score ; LC ; LC 2 ; RegFuse ; MixModel and LambdaMerge and all
For the top 10 runs and d = 50, this experiment was done for comparison with the ClustFuse variants. But note that here the experiment was done for d = 50 and 25. Table 8 shows that GA outperforms both ClustFuseCombSUM and ( p -value &lt; 0.05) by Wilcoxon signed-rank test.
 out of the top documents contains a rich collection of useful documents that consistently produces encouraging retrieval performance.

Table 9 shows that the average number of documents per query in the fused run highly varies over datasets and also over (TREC 7, top 25). We now consider a more resource constrained situation. We take only the top d (=100, 50, 25) documents
Nevertheless, for TREC 3, 6 and 7, the performance in MAP, P@5 and P@10 using the top d relevance assessments remains significantly better than the best component run as well as CombSUM ; CombMNZ ; Z Score ; LC ; LC 2 ; RegFuse ; MixModel ponent run as well as CombSUM ; CombMNZ ; Z Score ; LC ; LC 2 ; MixModel and LambdaMerge and these pairwise differences signed-rank test. 5.3. Learning GA weights using disjoint query sets
In the previous sections, we have learned the optimal weights using either all the relevant judgements or the relevant judgements of the top-ranked documents. However, training was done using all the queries. In this sub-section, we divide the queries into two disjoint groups: even-numbered queries and odd-numbered queries. First, we perform the training on the even-numbered queries only and test the learned optimal weights on the odd-numbered queries. Let the optimal MAP thus obtained be MAPTrainOnEvenTestOnOdd . Then, we swap the train-test sets. In other words, we train on the odd-num-bered queries and test on the even-numbered queries to get the optimal MAP MAPTrainOnOddTestOnEven . Finally, we take an average of MAPTrainOnEvenTestOnOdd and MAPTrainOnOddTestOnEven values to get MAPTrainTestAverage . We report MAP-and LambdaMerge . The remaining methods are unsupervised and so the same values are reported for them as found in the for all the remaining methods. We see that GA outperforms, in MAP, P@5 and P@10, the best component run as well as
CombSUM ; CombMNZ ; Z Score ; LC ; LC 2 ; RegFuse ; MixModel ; LambdaMerge ; ClustFuseCombSUM and ClustFuseCombMNZ , test.
 5.4. Learning GA weights on randomly chosen runs In the previous subsections, we have obtained the evaluation measures for the top performing Ad Hoc runs submitted in in the last subsection. Then, we take an average of all the 12 values for each evaluation measure.
Figs. 5 and 6 show the MAP and P@5 values for TREC 3 at different values of k (number of runs) of GA against the best component, CombSUM ; CombMNZ ; Z Score ; LC ; LC 2 ; RegFuse ; MixModel and LambdaMerge . Figs. 7 X 12 depict the MAP and P@5 values for TREC 5, 6 and 7. In MAP, for each randomly chosen set, GA outperforms the best component, CombSUM, runs, for all values of k and all the four years, in P@5 GA outperforms the best component, CombSUM ; CombMNZ ; 95% confidence level ( p -value &lt; 0.05) by Wilcoxon signed-rank test.
 Figs. 13 and 14 show the MAP and P@5 comparisons of GA with ClustFuseCombSUM and ClustFuseCombMNZ for TREC 3. 5.5. Time requirements of GA
The GA is a time consuming method. The high time requirements can be ascribed to the extensive search exercise per-formed by GA over the solution space to find the optimal performance. However, the time requirements can be reduced of generations varies from 200 to 150, 100, 75, 50 and 25. For each year, we found the time required when the maximum number of runs are considered for fusion. In our experiments, this number is 30. These runs are chosen randomly and the
MAPs are obtained based on the train-test procedure. We see that there is no notable drop in performance when the number baselines at 95% confidence level ( p -value &lt; 0.05) by Wilcoxon signed-rank test.

However, reduction in the number of generations reduces the time requirements considerably. Table 14 shows the time requirements of GA for different numbers of generation along with the time requirements of
CombSUM ; CombMNZ ; Z Score ; LC ; LC 2 ; RegFuse ; MixModel and LambdaMerge . The time was calculated on a Intel Core i5 processor machine with 5 GB RAM. The values are averages over TREC 3, 5, 6 and 7. We see that although the time taken of GA with ClustFuseCombSUM and ClustFuseCombMNZ in terms of time requirement. Here, 10 runs are fused and top 50 FuseCombSUM and ClustFuseCombMNZ in terms of time requirement even when the number of generations is not reduced. So, we see that GA is not the most expensive algorithm in terms of computation time. It betters MixModel and LambdaMerge at a lower number of generations and also the ClustFuse variants even when the number of generations is
GA in terms of standard evaluation measures. 5.6. Removal of redundant runs on the basis of MAP values consider only the 10 runs having the highest 10 MAP values. We then separately run GA on these 10 runs and get the opti-for an efficient fusion. Next we will see if the number of runs to be fused can be further reduced. 5.7. Removal of redundant runs using correlation between retrieval scores
In the previous sections, we have fused the runs and determined their optimal weights irrespective of how similar the superiority of a document common to them. Here we explore if we can further remove the redundant information by con-
Our algorithm is as follows: results when a run is barred from participating in a fusion. We chose x as 3%.
 component run and all the baselines at 95% confidence level ( p -value &lt; 0.05) by Wilcoxon signed-rank test. 5.8. GA as a natural selector of the superior runs
Given a set of a high number of runs, GA naturally selects the highly contributing runs in a fusion setup. High fusion weights are automatically assigned to the runs which have higher number of relevant documents at the top and hence con-datasets of TREC. 6. Conclusion show that considerable improvement can be obtained in fusion performance even if partial relevance assessments are used for determining the optimal weights. The proposed approach also determines the redundant runs that may be removed from Genetic Algorithm.
 References
