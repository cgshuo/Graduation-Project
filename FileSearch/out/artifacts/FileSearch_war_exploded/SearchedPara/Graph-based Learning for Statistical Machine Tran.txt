 Current phrase-based statistical machine translation (SMT) systems commonly operate at the sentence level X  X ach sentence is translated in isolation, even when the test data consists of internally coherent paragraphs or stories, such as news articles. For each sentence, SMT systems choose the translation hypothesis that maximizes a combined log-linear model score, which is computed independently of all other sentences, using globally optimized com-bination weights. Thus, similar input strings may be translated in very different ways, depending on which component model happens to dominate the combined score for that sentence. This is illustrated by the following example (from the IWSLT 2007 Arabic-English translation task): The phrase lA ymknk ( you may not/you cannot ) is translated differently (and wrongly in the sec-ond case) due to different segmentations and phrase translations chosen by the decoder. Though differ-ent choices may be sometimes appropriate, the lack of constraints enforcing translation consistency of-ten leads to suboptimal translation performance. It would be desirable to counter this effect by encour-aging similar outputs for similar inputs (under a suit-ably defined notion of similarity, which may include e.g. a context specification for the phrase/sentence).
In machine learning, the idea of forcing the out-puts of a statistical learner to vary smoothly with the underlying structure of the inputs has been formal-ized in the graph-based learning (GBL) framework. In GBL, both labeled (train) and unlabeled (test) data samples are jointly represented as vertices in a graph whose edges encode pairwise similarities be-tween samples. Various learning algorithms can be applied to assign labels to the test samples while en-suring that the classification output varies smoothly along the manifold defined by the graph. GBL has been successfully applied to a range of problems in computer vision, computational biology, and natu-ral language processing. However, in most cases, the learning tasks consisted of unstructured classi-fication, where the input was represented by fixed-length feature vectors and the output was one of a finite set of discrete labels. In machine translation, by contrast, both inputs and outputs consist of word strings of variable length, and the number of possi-ble outputs is not fixed and practically unlimited.
In this paper we propose a new graph-based learn-ing algorithm with structured inputs and outputs to improve consistency in phrase-based statistical ma-chine translation. We define a joint similarity graph over training and test data and use an iterative label propagation procedure to regress a scoring function over the graph. The resulting scores for unlabeled samples (translation hypotheses) are then combined with standard model scores in a log-linear transla-tion model for the purpose of reranking. Our con-tributions are twofold. First, from a machine trans-lation perspective, we design and evaluate a global consistency model enforcing that similar inputs re-ceive similar translations. Second, from a machine learning perspective, we apply graph-based learning to a task with structured inputs and outputs, which is a novel contribution in itself since previous ap-plications of GBL have focused on predicting cat-egorical labels. We evaluate our approach on two machine translation tasks, the IWSLT 2007 Italian-to-English and Arabic-to-English tasks, and demon-strate significant improvements over the baseline. GBL algorithms rely on a similarity graph consisting of a set of nodes representing data samples x i ranges over 1 , . . . , l labeled points and l +1 , . . . , n unlabeled points), and a set of weighted edges en-coding pairwise similarities between samples. The graph is characterized by a weight matrix W whose elements W between vertices x bels for the first l points. If there is no edge linking nodes x able freedom in choosing the weights. The similar-ity measure used to compute the edge weights de-termines the graph structure and is the most impor-tant factor in successfully applying GBL. In most applications of GBL, data samples are represented by fixed-length feature vectors, and cosine similar-ity or Euclidean distance-based measures are used for edge weights.

Learning algorithms on similarity graphs include e.g. min-cut (Blum and Chawla, 2001), spectral graph transducer (Joachims, 2003), random walk-based approaches (Szummer and Jaakkola, 2001), and label propagation (Zhu and Ghahramani, 2002). The algorithm proposed herein is based on the latter. 2.1 Label Propagation Given a graph defined by a weight matrix W and a label set Y , the basic label propagation algorithm proceeds as follows: 1. Initialize the matrix P as P 2. Initialize a n  X  C matrix f with binary vectors 3. f  X   X  P  X  f 4. Clamp already-labeled data rows: f  X  5. If f  X   X  = f , stop. 6. f  X  f  X  7. Repeat from step 3.
 After convergence, f contains the solution in rows l + 1 to n in the form of unnormalized label proba-bility distributions. Hard labels can be obtained by The algorithm minimizes the following cost func-tion (Zhu, 2005): S measures the smoothness of the learned function, i.e., the extent to which the labeling allows large-weight edges to link nodes of different labels. By minimizing S , label propagation finds a labeling that, to the extent possible, assigns similar soft labels (identical hard labels) to nodes linked by edges with large weights (i.e., highly similar samples). The labeling decision takes into account not only sim-ilarities between labeled and unlabeled nodes (as in nearest-neighbor approaches) but also similarities among unlabeled nodes. Label propagation has been used successfully for various classification tasks, e.g. image classification and handwriting recogni-tion (Zhu, 2005). In natural language processing, la-bel propagation has been used for document classifi-cation (Zhu, 2005), word sense disambiguation (Niu et al., 2005; Alexandrescu and Kirchhoff, 2007), and sentiment categorization (Goldberg and Zhu, 2006). Our goal is to exploit graph-based learning for im-proving consistency in statistical phrase-based ma-chine translation. Intuitively, a set of similar source sentences should receive similar target-language translations. This means that similarities between training and test sentences should be taken into ac-count, but also similarities between different test sentences , which is a source of information currently not exploited by standard SMT systems. To this end we define a graph over the training and test sets with edges between test and training sentences as well as between different test sentences. In cases where a test sentence does not have any connections to training sentences but is connected to other test sentences, helpful information about preferred trans-lations can be propagated via these edges.

As mentioned above, the problem of machine translation does not neatly fit into the standard GBL framework. Given that our samples consist of variable-length word strings instead of feature vectors, the standard cosine or Euclidean-distance based similarity measures cannot be used mean-ingfully, and the number of possible  X  X abels X  X  correct translations X  X s unbounded and practically very large. We thus need to modify both the graph construction and the label propagation algorithms.
First, we handle the problem of unlimited out-puts by applying GBL to rescoring only. In most SMT systems, an N -best list (generated by a first de-coding pass) approximates the search space of good hypotheses reasonably well, provided N is large enough. For all hypotheses of all sentences in the test set (set we denote with H ), the system learns a ranking function r : H  X  [0 , 1] . Larger values of r indicate better hypotheses. The corresponding loss functional is L ( r ) measures the smoothness of r over the graph by penalizing highly similar clusters of nodes that have a high variance of r (in other words, simi-lar input sentences that have very different transla-tions). The smaller L ( r ) , the  X  X moother X  r is over the graph. Thus, instead of directly learning a clas-sification function, we learn a regression function X  similar to (Goldberg and Zhu, 2006) X  X hat is then used for ranking the hypotheses. 3.1 Graph Construction Each graph node represents a sentence pair (consist-ing of source and target strings), and edge weights represent the combined similarity scores computed from comparing both the source sides and target sides of a pair of nodes. Given a training set with l source and target language sentence pairs ( s source sentences, s the similarity graph proceeds as follows: 1. For each test sentence s 2. For each hypothesis h 3. For each hypothesis h 4. Finally,for each node x
The synthetic nodes and edges need to be added to prevent the label propagation algorithm from con-verging to the trivial solution that assigns r = 1 to all points in the graph. This choice is theoretically motivated X  X  similarity graph for regression should have not only  X  X ources X  (good nodes with high value of r ) but also  X  X inks X  (counterparts for the sources). Figure 1 illustrates the connections of a test node. Similarity Measure The similarity measure used for comparing source and target sides is of prime importance, as it determines the structure of the graph. This has consequences for both computa-tional efficiency (denser graphs require more com-putation and memory) and the accuracy of the out-come. A low similarity threshold results in a rich graph with a large number of edges but possibly in-troduces noise. A higher threshold leads to a small graph emphasizing highly similar samples but with too many disconnected components. The similarity measure is also the means by which domain knowl-edge can be incorporated into the graph construc-tion process. Similarity may be defined at the level of surface word strings, but may also include lin-guistic information such as morphological features, part-of-speech tags, or syntactic structures. Here, we compare two similarity measures: the famil-iar BLEU score (Papineni et al., 2002) and a score based on string kernels. In using BLEU we treat each sentence as a complete document. BLEU is not symmetric X  X hen comparing two sentences, differ-ent results are obtained depending on which one is considered the reference and which one is the hy-pothesis. For computing similarities between train and test translations, we use the train translation as the reference. For computing similarity between two test hypotheses, we compute BLEU in both direc-tions and take the average. We note that more ap-propriate distance measures are certainly possible. Many previous studies, such as (Callison-Burch et al., 2006), have pointed out drawbacks of BLEU, and any other similarity measure could be utilized instead. In particular, similarity measures that model aspects of sentences that are ill handled by standard phrase-based decoders (such as syntactic structure or semantic information) could be useful here.
A more general way of computing similarity be-tween strings is provided by string kernels (Lodhi et al., 2002; Rousu and Shawe-Taylor, 2005), which have been extensively used in bioinformatics and email spam detection. String kernels map strings into a feature space defined by all possible sub-strings of the string up a fixed length k , and com-puting the dot product between the resulting feature vectors. Several variants of basic string kernels ex-ist, notably those allowing gaps or mismatches, and efficient implementations have been devised even for large scale applications. Formally, we define a sentence s as a concatenation of symbols from a fi-nite alphabet  X  (the vocabulary of the language) and an embedding function from strings to feature vec-tors,  X  :  X   X   X  H . A kernel function K ( s, t ) com-putes the distance between the resulting vectors for two sentences s and t . In our case, the embedding function is defined as where k is the maximum length of substrings, | i | is the length of i , and  X  is a penalty parameter for each gap encountered in the substring. K is defined as where w is a weight dependent on the length of the substring u . Finally, the kernel score is normalized by from being favored. Thus, our similarity measure is a gapped, normalized string kernel, which is a more general measure than BLEU in that is considers non-contiguous substrings. We use a dynamic program-ming implementation of string kernels (Rousu and Shawe-Taylor, 2005).
For the combination of source-side and target-side similarity scores (the function we denoted as  X  ) we test two simple schemes, using either the ge-ometric or the arithmetic mean of the individual scores. In the first case, large edge weights only re-sult when both source and target are close to each other; the latter may produce high edge weights when only one of them (typically the source score) is high. More sophisticated combination schemes, using e.g. weighted combination, could be used but were not investigated in this study.
 Scalability Poor scalability is often mentioned as a drawback of graph-based learning. Straightfor-ward implementations of GBL algorithms often rep-resent the joint training and test data in working memory and therefore do not scale well to large data sets. However, we have developed several tech-niques to improve scalability without impeding ac-curacy. First, we construct separate graphs for each test sentence without losing global connectivity in-formation. The graph for a test sentence is com-puted as the transitive closure of the edge set E over the nodes containing all hypotheses for that test sen-tence. This smaller graph does not affect the out-come of the learning process for the chosen sentence because in label propagation the learned value r ( x can be influenced by that of another node x only if x oretical case, the transitive closure could compre-hend the entire graph, but in practice the edge set is never that dense and can be easily pruned based on the heuristic that faraway nodes connected through low-weight edges have less influence on the result. We use a simple embodiment of this heuristic in a work-list approach: starting from the nodes of inter-est (hypotheses for the focal sentence), we expand the closure starting with the direct neighbors, which have the largest influence; then add their neighbors, which have less influence, and so forth. A thresh-old on the number of added vertices limits undue expansion while capturing either the entire closure or a good approximation of it. Another practical computational advantage of portioning work is that graphs for different hypothesis sets can be trivially created and used in parallel, whereas distributing large matrix-vector multiplication is much more dif-ficult (Choi, 1998). The disadvantage is that overall redundant computations are being made: incomplete estimates of r are computed for the ancillary nodes in the transitive closure and then discarded.
Second, we obtain a reduction in graph size of or-ders of magnitude by collapsing all training vertices of the same r that are connected to the same test vertex into one and sum the edge weights. This is equivalent to the full graph for learning purposes. 3.2 Propagation Label propagation proceeds as follows: 1. Compute the transitive closure over the edges 2. On the resulting graph, collapse all test-train 3. Normalize test-to-train weights such that 4. Initialize the matrix P as P 5. Initialize a column vector f of height n with 6. f  X   X  P  X  f 7. Clamp f  X  8. If f  X   X  = f , continue with step 11. 9. f  X  f  X  10. Repeat from step 6. 11. The result r is in the slots of f that correspond Convergence Our algorithm X  X  convergence proof is similar to that for standard label propagation (Zhu, 2005, p. 6). We split P as follows: where P larities of test hypotheses with train sentences, P is a horizontal vector holding the same similarities (though P P of test hypotheses. We also separate f : where we distinguish the first entry because it repre-sents the training part of the data. With these nota-tions, the iteration formula becomes: Unrolling the iteration yields: f It can be easily shown that the first term converges to zero because of normalization in step 4 (Zhu, 2005). The sum in the second term converges to ( Our system uses the iterative form. On the data sets used, convergence took 61.07 steps on average.
At the end of the label propagation algorithm, nor-malized scores are obtained for each N-best list (sen-tences without any connections whatsoever are as-signed zero scores). These are then used together with the other component models in log-linear com-bination. Combination weights are optimized on a held-out data set. We evaluate our approach on the IWSLT 2007 Italian-to-English (IE) and Arabic-to-English (AE) travel tasks. The first is a challenge task, where the training set consists of read sentences but the de-velopment and test data consist of spontaneous di-alogues. The second is a standard travel expres-sion translation task consisting entirely of read in-put. For our experiments we chose the text input (correct transcription) condition only. The data set sizes are shown in Table 1. We split the IE develop-ment set into two subsets of 500 and 496 sentences each. The first set (dev-1) is used to train the system parameters of the baseline system and as a training set for GBL. The second is used to tune the GBL pa-rameters. For each language pair, the baseline sys-tem was trained with additional out-of-domain text data: the Italian-English Europarl corpus (Koehn, 2005) in the case of the IE system, and 5.5M words of newswire data (LDC Arabic Newswire, Multiple-Translation Corpus and ISI automatically extracted parallel data) in the case of the AE system.
Our baseline is a standard phrase-based SMT system based on a log-linear model with the fol-lowing feature functions: two phrase-based trans-lation scores, two lexical translation scores, word count and phrase count penalty, distortion score, and language model score. We use the Moses de-coder (Koehn et al., 2007) with a reordering limit of 4 for both languages, which generates N -best lists of up to 2000 hypotheses per sentence in a first pass. The second pass uses a part-of-speech (POS) based trigram model, trained on POS sequences generated by a MaxEnt tagger (Ratnaparkhi, 1996). The lan-guage models are trained on the English side using SRILM (Stolcke, 2002) and modified Kneser-Ney discounting for the first-pass models and Witten-Bell discounting for the POS models. The baseline system yields state-of-the-art performance. We started with the IE system and initially inves-tigated the effect of only including edges between labeled and unlabeled samples in the graph. This is equivalent to using a weighted k -nearest neighbor reranker that, for each hypothesis, computes average similarity with its neighborhood of labeled points, and uses the resulting score for reranking.

Starting with the IE task and the BLEU-based similarity metric, we ran optimization experiments that varied the similarity threshold and compared sum vs. product combination of source and target similarity scores, settling for  X  = 0 . 7 and prod-uct combination. We experimented with three dif-ferent ways of weighting the contributions from labeled-unlabeled vs. unlabeled-unlabeled edges: (a) no weighting, (b) labeled-to-unlabeled edges were weighted 4 times stronger than unlabeled-unlabeled ones; and (c) labeled-to-unlabeled edges were weighted 2 times stronger. The weighting schemes do not lead to significantly different results. The best result obtained shows a gain of 1.2 BLEU points on the dev set and 1 point on the eval set, re-flecting PER gains of 2% and 1.2%, respectively.
We next tested the string kernel based similarity measure. The parameter values were 0.5 for the gap penalty, a maximum substring length of k = 4 , and weights of 0, 0.1, 0.2, 0.7. These values were chosen heuristically and were not tuned extensively due to time constraints. Results (Table 3) show significant improvements in PER and BLEU.

In the context of the BTEC challenge task it is interesting to compare this approach to adding the development set directly to the training set. Part of the improvements may be due to utilizing k NN in-formation from a data set that is matched to the test set in terms of style. If this data were also used for training the initial phrase table, the improvements might disappear. We first optimized the log-linear model combination weights on the entire dev07 set (dev-1 and dev-2 in Table 1) before retraining the phrase table using the combined train and dev07 data. The new baseline performance (shown in Ta-ble 4) is much better than before, due to the im-proved training data. We then added GBL to this system by keeping the model combination weights trained for the previous system, using the N-best lists generated by the new system, and using the combined train+dev07 set as a train set for select-ing similar sentences. We used the GBL parameters that yielded the best performance in the experiments described above. As can be seen from Table 4, GBL again yields an improvement of up to 1.2% absolute in both BLEU and PER.

For the AE task we used  X  = 0 . 5 ; however, this threshold was not tuned extensively. Results using BLEU similarity are shown in Table 5. The best result on the eval set yields an improvement of 1.2 BLEU points though only 0.2% reduction in PER. Overall, results seem to vary with parameter settings and nature of the test set (e.g. on dev5, used as a test set, not for optimization, a surprisingly larger im-provement in BLEU of 2.7 points is obtained!).
Overall, sentence similarities were observed to be lower for this task. One reason may be that the AE system includes statistical tokenization of the source side, which is itself error-prone in that it can split the same word in different ways depending on the con-text. Since our similarity measure is word-based, this may cause similar sentences to fall below the threshold. The string kernel does not yield any im-provement over the BLEU-based similarity measure on this task. One possible improvement would be to use an extended string kernel that can take morpho-logical similarity into account.
 Example Below we give an actual example of a translation improvement, showing the source sen-tence, the 1-best hypotheses of the baseline system and GBL system, respectively, the references, and the translations of similar sentences in the graph neighborhood of the current sentence.
 GBL is an instance of semi-supervised learning, specifically transductive learning. A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al., 2007). Ours is the first study to explore a graph-based learning ap-proach. In the machine learning community, work on applying GBL to structured outputs is beginning to emerge. Transductive graph-based regularization has been applied to large-margin learning on struc-tured data (Altun et al., 2005). However, scalability quickly becomes a problem with these approaches; we solve that issue by working on transitive closures as opposed to entire graphs. String kernel represen-tations have been used in MT (Szedmak, 2007) in a kernel regression based framework, which, how-ever, was an entirely supervised framework. Finally, our approach can be likened to a probabilistic imple-mentation of translation memories (Maruyana and Watanabe, 1992; Veale and Way, 1997). Translation memories are (usually commercial) databases of segment translations extracted from a large database of translation examples. They are typically used by human translators to retrieve translation candidates for subsequences of a new input text. Matches can be exact or fuzzy; the latter is similar to the iden-tification of graph neighborhoods in our approach. However, our GBL scheme propagates similarity scores not just from known to unknown sentences but also indirectly, via connections through other un-known sentences. The combination of a translation memory and statistical translation was reported in (Marcu, 2001); however, this is a combination of word-based and phrase-based translation predating the current phrase-based approach to SMT. We have presented a graph-based learning scheme to implement a consistency model for SMT that encourages similar inputs to receive similar out-puts. Evaluation on two small-scale translation tasks showed significant improvements of up to 2.6 points in BLEU and 2.8% PER. Future work will include testing different graph construction schemes, in par-ticular better parameter optimization approaches and better string similarity measures. More gains can be expected when using better domain knowledge in constructing the string kernels. This may include e.g. similarity measures that accommodate POS tags or morphological features, or comparisons of the syntax trees of parsed sentence. The latter could be quite easily incorporated into a string kernel or the related tree kernel similarity measure. Additionally, we will investigate the effectiveness of this approach on larger translation tasks.
 Acknowledgments This work was funded by NSF grant IIS-032676 and DARPA under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of these agencies.
