 In this work, we attempt to tackle domain-transfer problem by combining old-domain labeled examples with new-domain classifier to label some informative unlabeled examples in new examples. The experimental resu lts demonstrate that proposed scheme can significantly boost the accuracy of the base sentiment classifier on new domain. H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; I.2.7 [ Artificial Intelligence ]: Natural language processing Algorithms; Performance; Experimentation Sentiment Classification; Opini on Mining; Information Retrieval 
With the rapid growth of sema ntic web page (such as product more attention in the community of information retrieval and sentiment classification, in which a document is labeled as a positive or negative evaluation of a target object (book, product, etc.). In most cases, the use of statistical or machine learning techniques has proven to be successf ul in this context, such as Naive Bayes (NB), Maximum Entr opy Classification (ME), and Support Vector Machines (SVM)) [5]. 
Due to highly domain-specific nature, however, supervised sentiment classifier [1][2][4][5] typically requires a large amount of new labeled training data when moving from one domain to another (e.g. from  X  X ouse reviews X  to  X  X omputer reviews X ). As a result, when transferred to a new domain without any labeled examples, a sentiment classifier often performs extremely bad. This is so-called domain-tr ansfer problem [1][7][8]. 
A simple solution to this problem is to manually label a large number of examples for each new domain. Unfortunately, acquisition of these labeled da ta can be time-consuming and expensive. Consequently, it is an important and urgent job to investigate an ideal and practicable method for domain-transfer problem. 
To the best of our knowledge, no previous work has been conducted on exactly this kind of problem, where there are a large amount of labeled data in old domain but scarcely any labeled data in new domain. The most re lated research is conducted by Aue and Gamon [1]. But their work still needs to label a small amount of training data for a new domain. 
In this work, we attempt to tackle domain-transfer problem by combining old-domain labeled examples with new-domain informative unlabeled examples in new domain and learn a new classifier based on these selected examples ( n is a pre-defined number indicating how many exam ples in new domain shall be picked out as informative ones). Without loss of generality, we employ centroid classifier [3] as the base classifier. 
In this work, the documents are represented using vector space model. In this model, each document d is considered to be a vector in the term-space. For term weight we employ TFIDF [6]. First we compute a centroid C i using formula (1) for each class c where | z | indicates the cardinality of set z . 
Then we count the similarity of one document d to each centroid by cosine measure, where product of the two vectors. corresponding to the most similar centroid. 
Let X  X  take a simple example (s ee Figure 1(A)). The old-domain examples are represented by two circles: negative example is denoted by grey and positive example is denoted by white; the new-domain examples are represented by two ellipses: negative example is denoted by grey and positive example is denoted by white. C ON and C OP are the centroids of negative and positive class in old domain respectively. Middle Line is the perpendicular bisector of the line between C ON and C OP perspective, Middle Line serves as a decision hyper-plane that separate negative class and positive class. 
As a result, according the Middle Line, we can observe that all examples of old domain can be correctly classified. However, this Middle Line does not work well in new domain: from the figure, these new-domain negative examples under the Middle Line will be misclassified into positive cla ss. This is the mechanism why domain-transfer degrades the performance of old classifier. 
Figure 1: Performance of old classifier when transferred to 
An intuitive method to address this issue is to pick out some informative examples for new-domain and to train the base classifier again (see Figure 1(B)). We use  X - X  to represent selected examples from new-domain negative class, and use  X  selected ones from new-domain positiv e class. Then we calculate such, New Middle Line can be drawn. In this time, we can observe that most examples in new domain can be correctly categorized. This is the rationale why propos ed scheme can address domain-transfer problem. 
Detailed algorithm for proposed scheme is presented in Figure is to label some informative ones for a new domain, because the old classifier performs poorly in new domain. In the following subsection we attempt to solve this problem. Figure 2: Outline of Proposed Scheme for Domain-transfer 
Assuming X 1 denotes the word space of old domain, X 2 denotes the word space of new domain, and X=X 1 UX 2 denotes the total word space. We make following presumptions: (i): Words in X are independent each other; (ii): X  X  =X 1  X  X 2  X  X ; (iii): X  X  in different domains accords with the same probability distribution. 
According to Bayesian formula, with respect to one examples d in old domain, the Bayesian discriminant function is where c denotes the class label, i.e., positive or negative, and D indicates a constant. As such, with respect to one examples e in new domain, the Bayesian discriminant function is where D 2 indicates a constant. 
Consequently, if we directly appl y the discriminant rule trained on old domain to new domain, the Bayesian discriminant function is 
Obviously, there is a noticeab le difference between above discriminant function and new-dom ain discriminant function. In discriminant function (3) to cl assify new-domain examples is unfeasible. This is the mech anism why old-domain-trained classifier often performs extr emely badly in new domain (as illustrated in Figure 1(A)). However, X  X  and X 2 \X  X  are independent each other. According to formulas (4) and (5), for every example e on new domain, we can make a conclusion that the larger g larger g 2 (e) is. 
Assuming the data in X 2 \X  X  is in accord with a kind of distribution. As a result, with respect to examples e domain, we obtain, 
Above formula indicates that, if g 1 (e 1 )&gt;g g (e 1 )&lt;g 2 (e 2 ). 
With respect to centroid classifier, we can calculate its positive Under this scenario, formula (6) leads to a conclusion: for one example, the larger the S N , the more likely it is drawn from positive class. Based on this conc lusion, we propose Similarity Ranking method (SR): we first rank S N of all examples, and assign top n /2 largest examples as negative; then rank S n /2 largest ones as positive. 
However, this method doesn X  X  hold when the length difference among different reviews is very large, because it is often the case that the larger the length of one review, the larger the S What X  X  worse, when transfer the old classifier to another domain, space difference between old domain and new domain can make a large difference on S N or S P . 
To tackle this problem, we normalize (or divide) the original similarity so that the adverse effect of length difference and word-idea of relative similarity. Formally, we define Negative Relative Similarity ( S RN ) and Positive Relative Similarity ( S following, 
Up to this point, we can make a refined conclusion that, for one example, the larger the S RN , the more likely it is drawn from positive class. According to this supposition, we propose Relative Similarity Ranking method (RSR): we first rank S examples, and assign top n /2 largest examples as negative; then rank S RP , and label top n /2 largest ones as positive. In conclusion, we present the detailed algorithm for Relative Similarity Ranking. In this figure, Sizeof(NU) indicates the cardinality of unlabeled examples set in new domain. 
Figure 3: The Outline of Relative Similarity Ranking Method 
Given Computer review (Com p) as old domain and House review (Hou) as new domain. For the sake of being easy to explain, we take out six examples (d 15 , d 20 , d randomly (refer to Table 1) from H ou. The former three examples (d (d 63 , d 76 , d 114 ) are coming from positive class. 
For each example, we use formula (2) to calculate negative decision rule doesn X  X  work at all: it classifies all of the six examples into negative class. 
In accordance with SR, we first rank S N and label examples (d d , d 63 ) as negative; then rank S P and assign (d positive (assuming n =6). Obviously, d 63 and d 49 are misclassified. This observation indicates that SR suffers from word-space difference incurred by domain-transfer. 
To overcome this shortcoming of SR, we proposed RSR method in preceding subsection. Let X  X  turn to Table 1 again. First we calculate relative similarity for si x examples using formulas (7-8), then rank S RN and label examples (d 15 , d 20 , d S and label examples (d 63 , d 76 , d 114 ) as positive (assuming n =6). In this time, all examples are correctly classified. 
To validate the effectiveness and robustness of proposed method, we collected three datase ts from three different domains: Computer Reviews (Comp), Edu cation Reviews (Edu) and House Reviews (Hou). The details are listed in Table 2. Comp 390 544 120 4725 Edu 1012 254 600 19150 
Hou 445 555 300 12674 
We use 50% of one dataset as trai ning set when it is used as old domain. There is no doubt that feat ure selection may remove some important features in new domain, so we don X  X  delete any features when training the base classifier in old domain. 
Joachims X  X  SVM-light package can be used for TSVM classification. (http://svmlight.joachims.org/). We use a linear kernel and leave all parameters as default. We use 50% of old-domain data as labeled training examples and use 50% of new-domain data as unlabeled ones. 
Under the proposed scheme, we use two methods to pick out new-domain informative examples : Similarity Ranking (SR), and Relative Similarity Ranking (RSR). Both SR and RSR pick out some informative examples rather than label all examples in new domain. We split the new-domain data evenly into unlabeled set and test set; the Ratio is set to 0.4 for SR and RSR. 
As we can observe from table 3, RSR dramatically improves the performance of base classifier in new domain. The wide margin improvement indicates that proposed scheme combined with Relative Similarity Ranking method performs very effectively and robustly. 
Despite of simplicity and strai ghtforwardness, SR performs quite well. Its average accuracy is about 5% lower than RSR but about 18% higher than base classifier. For problems  X  X du-&gt;Hou X , SR even achieves better result than RSR. This performance provides convincing proof for the effectiv eness of ranking based method. 
In contrast to Centroid classi fier, TSVM performs very well for domain-transfer problems. Apart from  X  X omp-&gt;Edu X , TSVM beats Centroid classifier by a wide margin. The average accuracy of TSVM is about 12 percents higher than Centroid classifier. On other hand, however, TSVM is still outperformed by proposed scheme. For examples, apart from  X  X omp-&gt;Hou X , TSVM is outperformed by SR or RSR scheme with wide margin. This observation indicates that proposed straightforward scheme can produce much better results than theoretically-more-sound TSVM. 
Figure 4 shows the performance cu rves of proposed method vs. informative examples. The parameter  X  X atio X  indicates what percentage of new-domain unlabeled data shall be picked out as informative examples. We split the new-domain data evenly into unlabeled set and test set. Figure 4: Accuracy curves of proposed method vs. the Ratio 
We can clearly observe that increasing the Ratio increases the classification accuracy in new domain. However, the increase in As the Ratio gets larger, the accuracies start leveling off as we can observe from this figure apart from the problem  X  X du-&gt;Hou X . 
The second observation is that, when the Ratio exceeds 0.15, proposed method achieves persiste nt results except  X  X du-&gt;Hou X  problem. This fact validates that the relative similarity method can pick out informative ex amples in new domain.
There are a few limitations with this work. First, although RSR can pick out  X  X nformative X  examples in new domain, we still cannot guarantee that the selected "informative" ones are representative to the new domain. The second problem is that the chosen old domain may be far di fferent from the new one, which makes it difficult to select really  X  X nformative X  examples. Thirdly, when the negative examples are severely overlapped with the positive ones, RSR may label one same example into negative class as well as into positive class. 
In this work, we proposed an effective scheme for domain-transfer problem. It works by usi ng old classifier to label some informative unlabeled examples in new domain, and training the base classifier again. To effectively pick out informative examples, we proposed Relative Similarity Ranking method. The main idea is to counteract the aff ect of domain-transfer by altering the original similarities. An empirical evaluation conducted on three domains indicates that pr oposed method dramatically boost the accuracy of the base sentiment classifier on new domain. This work was mainly supported by special fund of Chinese Academy of Sciences,  X  X esearch on Opinion Mining of Web Text X , under grant number 0704021000 and two projects, i.e., 2007CB311100 and 2007AA01Z441. [1] Aue, A. and Gamon, M. Cust omizing Sentiment Classifiers to [2] Finn, A., and Kushmerick, N. Learning to classify documents [3] Han, E. and Karypis, G. Centroid-Based Document [4] Mullen, T. and Collier, N. Sentiment analysis using support [5] Pang, B., Lee, L., and Vaithyanathan, S. Thumbs up? [6] Rijsbergen, C. Information Retrieval. Butterworths, London, [7] Jing Jiang and ChengXiang Zhai. Instance weighting for [8] Jing Jiang and ChengXiang Zh ai. Exploiting domain structure 
