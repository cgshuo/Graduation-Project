 ORIGINAL ARTICLE Qiang Sun  X  Li-Lun Wang  X  Shiau Hong Lim  X  Gerald DeJong Abstract Handwritten Chinese character recognition is difficult due to the unstructured and noisy nature of its train-ing examples. There are often too few training examples for a statistical learner like SVM to overcome the noise and extract useful information reliably. Existing prior domain knowledge represents a valuable source of information for classifying handwritten characters. Explanation-based learning (EBL) provides a way to incorporating prior domain knowledge into the learner. The dynamic bias formed by the interaction of domain knowledge with training examples can yield solution knowledge of potential higher quality. Two EBL approaches, one that uses a special feature kernel function in SVM, the other uses a conventional kernel for the SVM but provides additional preference in choosing the classification hyper-plane, are reported.
 Keywords Explanation-based learning  X  Support vector machine  X  Domain knowledge  X  Character recognition  X  Machine learning 1 Introduction Handwritten Chinese character classification plays a special role in the field of optical character recognition. It differs from digit classification and western character classification in that it has an extremely large number of different classes, and it is often not possible to obtain too many labeled train-ing examples for each class. The small number of training examples per class makes it difficult for the learner to reli-ably overcome noise in the data, which results in the effect known as overfitting. Use of available prior knowledge is an attractive way to compensate the lack of enough training examples. 1.1 Support vector machines The support vector machine (SVM) [ 25 ] is a generic means to classify data by learning from training examples. The SVM learns a linear hyperplane in the high-dimensional dot prod-uct space that separates the training data points.

SVMs reduce overfitting by preferring a separating hyper-plane that maximizes the margin between the hyperplane and the most constraining data points of the two classes. Let w be the separating hyperplane, x i and y i be the feature vector and class label of the i th training example, and finding the sepa-rating hyperplane can be formulated as the following convex optimization problem subject to y i ( w  X  x i + b )  X  1  X  0 ,  X  i . The separating hyperplane w can be represented as a linear combination of a subset of the training examples called the support vectors. Soft margin SVM SVMs can handle non-separable data and mislabeled data points by introducing slack variables. Instead of requiring every data point to be classified correctly, the soft margin SVM penalizes the incorrectly classified examples. The soft margin SVM is formated as follows: subject to y i ( w  X  x i + b )  X  ( 1  X   X  i )  X  0 ,  X  i , where C controls the penalty on misclassification, and  X  i are the slack variables.
 Kernel methods To classify more difficult data, SVMs add expressiveness by introducing nonlinearity with kernel functions. Kernel functions map data points into high-dimensional spaces and computes inner products efficiently. Since only the inner products of training examples are needed, we can replace the dot products in the SVM formulation with kernel function evaluations. The SVM optimization problem with kernel is formulated as: subject to y i ( k ( w , x i ) + b )  X  1  X  0 ,  X  i , where k is the kernel function. Choosing a good kernel func-tion that captures the intrinsic distance metric of the problem is essential in kernel methods.

Kernels can also be used in soft margin SVMs, and we get subject to y i ( k ( w , x i ) + b )  X  ( 1  X   X  i )  X  0 ,  X 
SVMs have been successful in many domains, includ-ing handwritten digit classification [ 9 ]. However, its success comes at a price of thousands of training examples, which is not generally available for handwritten Chinese characters. Furthermore, Chinese characters are much more complicated than digits. Even though SVMs can achieve very good overall accuracy in Chinese character classification, there exist sets of characters that SVMs can easily confuse, whereas humans can distinguish them without difficulty (see Fig. 1 ). 1.2 Explanation-based learning Pre-existing domain knowledge represents an attractive source of classification information. When examples are expensive or limited the information in a training set may be insufficient to confidently learn a classifier. Incorporating available prior knowledge may result in a more accurate and more confident classifier.

Figure 2 illustrates the opportunity for additional domain information in SVM classification of handwritten digits. The top two lines represent one learning problem, and the bot-tom two lines represent another. In both the task is to say which images correspond to the numeral  X 3 X  and which to the numeral  X 6 X . The second problem is derived from the first by applying a fixed random permutation to all of the image pixels. Since the SVM kernel function operates on the dot product of feature vectors, these two learning problems are indistinguishable to the SVM. It performs equally well on both [ 10 ]. But clearly, the first affords additional informa-tion to humans. Indeed, the second is impossible for humans to learn, although they achieve nearly perfect performance with very little training on tasks like the first one using sim-ple foreign shapes.

We consider two sorts of prior knowledge. The first, solu-tion knowledge , concerns the target of learning itself and is specific to the learning task at hand. Examples of solution knowledge include the structure of a Bayes net, the kernel function of an SVM, the topology of a neural network, etc. The other sort, domain knowledge , describes objects of the world. For example, in handwritten character recognition one may believe that the pixels in the input images arise from strokes of a writing implement.
One possible way to exploit prior knowledge is through inductive bias. Inductive bias is an unavoidable part of any learning system [ 13 ]. Solution knowledge can be easily inte-grated into the machine learning process as inductive bias. Domain knowledge, while generally more reliable and more easily articulated by a human expert, cannot often be expressed as an inductive bias. For example, although we know that the pixels of a handwritten character are manifes-tations of the srokes from a writing implement and that these strokes are intended to portray some idealized figure, it is difficult to translate such knowledge into an effective induc-tive bias (e.g., a better kernel function or a more appropriate neural network topology). This knowledge alone cannot help to classify any particular image as a  X 3 X  or a  X 6 X  or anything else.

Explanation-based learning (EBL) dynamically integrates domain knowledge into the learning process by allowing its interaction with training examples. An explanation justifies, in terms of the domain knowledge, why a particular training example might merit its assigned training label. If the expla-nation is correct, then other examples that satisfy its con-ditions should form a conceptual equivalence class; they all should be given the same classification label for the same rea-son. Through conjecturing and confirming such equivalence classes, additional guidance can be given to the machine learner: a learner should prefer classifiers whose labels better respect the partitionings induced by confirmed explanations.
We describe two methods for incorporating domain knowledge into SVMs. The first approach learns specialized Feature Kernel Functions . A feature kernel function is a spe-cially constructed distance metric that is automatically tai-lored to the learning task at hand. The feature kernel approach estimates which image regions in the characters will likely be more helpful in distinguishing between the classes. The specialized distance metric assesses a greater distance contri-bution from pixels in regions of high expected discriminative information.

The other approach, the explanation-augmented support vector machine (EA-SVM), does not incorporate prior knowledge by crafting the kernel function. It uses a conven-tional kernel but imposes an additional preference over the space of possible classifiers. EA-SVMs use the fact that the equivalence class on examples that is induced by an expla-nation forms a lower dimensional surface in the hypothesis space: since all of the examples that satisfy the explanation are (ideally) labeled the same way for the same reason, they should have the same margin. If the domain knowledge were perfect, in the SVMs high dimensional space, an explanation would be a lower dimensional hyperplane to which the cor-rect classifying hyperplane must be parallel. Due to imperfect prior knowledge, noise, etc., a soft loss function is used to asses a penalty on classifiers not parallel to explanation sur-faces. Thus, the explanation surfaces impose an additional preference over the space of high-dimensional linear clas-sifiers. Through kernelization, the explanation preferences can be handled efficiently in the dimensionality of the input space. 2 Feature kernel functions 2.1 Overview EBL requires a domain theory of prior knowledge to drive the explanation process. While images of characters are com-posed of pixels, it is natural to break the span from pixels to characters into two subdomains. The first relates handwrit-ten characters to the strokes that are used to form them. The notion of a stroke is not intrinsic to this classification prob-lem. A conventional SVM has no place for such a notion. Rather, strokes are introduced as  X  X idden X  features to orga-nize prior knowledge. In addition, combinations of strokes form yet another level of derivable hidden features, called stroke-level features . For example, the two prototype Chi-nese characters shown in Fig. 3 are quite similar. However, some stroke-level features are quite informative for this dis-crimination task. We have circled these in Fig. 3 . Described with these derived stroke-level features the characters are quite different.

The second sub-domain explains the correspondence between pixels and strokes. Strokes are modeled as straight lines of a particular width with a particular starting and ending location. The corresponding pixels are those that fall within the boundaries of a long thin rectangle which is the stroke. The correspondence is determined by applying a Hough transformation [ 5 ] to detect lines in the training images. Note that using a Hough transform on training images is far more reliable than to find lines in an unknown image. The label of the training image provides access to its prototype stroke rep-resentation. Thus, the procedure is reduced to finding image lines that best match the known stroke lines.

Given a pair of Chinese character labels and a set of train-ing examples for each, the following procedure is performed to produce a feature kernel function: 1. Conjecture stroke-level features: the prototypes of 2. Determine the best pixel representation for each stroke: 3. Given evidence pixel sets for strokes, construct the set 4. Build the feature kernel function: component kernel func-2.2 Component kernel function Stroke-level features represent interactions between strokes. A component kernel is a specialized polynomial kernel to to assemble the final feature kernel function. Conceptually, component kernels operate over monomials . A monomial represents the conjunction of pixels, as evidence for a stroke-level feature.

Given the connections between pixels and strokes, it is straightforward to determine evidence monomials for differ-ent stroke-level features. This is illustrated in Fig. 4 . When the pixels in the region A serve as evidence for a vertical stroke, and the pixels in the region B serve as evidence for a horizontal stroke, then those second-degree monomials with one pixel from region A and another from region B can be evidence for the corner feature composed with the vertical and horizontal strokes.

Specifying a function to compute the dot product between two examples using those evidence monomials gives us a component kernel function for SVMs to detect the corre-sponding stroke-level features. The corner feature shown in Fig. 4 employs a component kernel function like the following: k It is easy to see that this kernel function computes the dot product between example x 1 and x 2 using the monomials of pixels from region A and region B k Therefore, a component kernel function is similar to a con-ventional kernel function in that it is easy to compute, and it gives the value of dot product between two examples in high-dimensional space. The only difference is that a component kernel function is designed for discovering the presence of certain highlevel feature in the example, not for determining the label. We can also build higher order component kernel functions for the same corner feature, such as k This kernel uses monomials with three pixels, two from region A , and the other one from region B . We can also define a similar function that uses monomials with one pixel from region A , and two from region B . 2.3 Combining component kernel functions The component kernel functions are designed to detect stroke-level features, but not all features are equally useful to distinguish characters. We now examine how to linearly com-bine the component kernel functions to obtain a feature kernel function to classify future character examples. Using differ-ent weights, our feature kernel function is able to emphasize diagnostic stroke-level features, while deemphasizing less informative or redundant ones.

Weighting kernel functions is a question addressed by many other researchers [ 3 , 8 , 14 ]. In our study, we employ the approach proposed in [ 8 ]. Here, we briefly summarize the approach. It uses a simple computable notion, called align-ment [ 3 ], to estimate the goodness of a kernel function. The (empirical) alignment of a kernel k 1 with another kernel k with respect to the sample S is defined as A where K i is the kernel matrix for the sample S using kernel k , and  X  ,  X  F is the Frobenius product 1 .Let y be the target function that gives the label, then the target kernel k  X  nel target alignment can be defined as A S ( k ) = A S ( k Higher kernel target alignment implies better performance of the resulting SVM classifier [ 3 ]. Given a set of kernels k , the problem of combining them can be formulated as to choose  X  in k ( X ) = i  X  i k i so that the alignment of k ( X ) to the given target vector y is optimized. Kandola et al. [ 8 ] showed that this optimization problem can be solved by the standard quadratic programming. 2.4 Analysis Our algorithm uses the notion of stroke-level features to orga-nize pixel level features, and enables SVMs to emphasize the distinctive stroke-level features by weighting component kernel functions. Intuitively, emphasizing small amount of useful features can greatly reduce learning complexity, there-fore allowing SVM learning algorithm to perform well even when small number of training examples are available. In this section, we use the notion of kernel alignment to provide insights about how our algorithm can produce an improved kernel function.
 Decomposing a kernel function First, observe that weighting multiple kernel functions can usually produce a better kernel function than simply combining them with equal weights. Specifically, given two kernel functions k 1 and k the target alignment of the combined kernel function with coefficients 1 and w is A With some algebra, we see that the optimal w to maximize A (w) is w get alignment for k 1 and k 2 , A 12 denotes alignment between k and k 2 , and | K i |= optimally weighted kernel is A (w opt ) = A while the target alignment of equally weighted kernel is A ( are normalized, i.e. | K 1 |=| K 2 | . The difference between the two alignments is therefore A (w opt ) 2  X  A ( 1 ) 2 = ( A This indicates that the difference between optimally weighted kernel and equally weighted kernel becomes large when the ratio of their target alignment increases.

The above arguments suggest that a better kernel func-tion could result from a re-composition of the original kernel function as an appropriately weighted set of kernels. In our approach, specifying component kernels according to stroke-level features serves as a mechanism to decompose the con-ventional polynomial kernel function.
 High-level feature alignment Without prior knowledge to guide the processes, a decomposition is unlikely to produce kernel functions with significantly different alignments. On the other hand, knowledge of high-level features can help to group input features according to their information content. In particular, if we assume the high-level features have binary values, then we can define the alignment between a high-level feature label and a given kernel matrix. We call this its high-level feature alignment. We u s e A F (  X  ) to denote alignment of a kernel to high-level feature, and A t (  X  ) to denote align-ment of a kernel to the target label. For a high-level feature that is unique for its class, its label has perfect alignment with the class label. Therefore, the corresponding high-level feature alignment equals the target alignment, which means A non-informative high-level feature results in a low alignment to the class label.

Now we examine those input features X that exhibit high alignment with some high-level feature F . Assume that F takes values from { 1 ,  X  1 } and we have binary input features X ={ 0 , 1 } .Let x f be a column vector representing the values of F for each example, then A where N (  X  ) represents the counting function and n is the total number of examples. In the above equation, Pr ( X = 1 , F 1 )  X  Pr ( X = 1 , F = X  1 ) represents the correlation of X with high-level feature F . Those input features that exhibit high correlation with F can be used to reliably detect high-level feature F . In our experimental section, we use such correlation to determine evidence pixel set for each stroke, and specify component kernel functions. 2.5 Experiments To demonstrate the improvement in accuracy achieved by incorporating stroke-level knowledge into kernel functions for SVM learning algorithms, we pick ten Chinese charac-ters (see Fig. 1 ) from the ETL9B database [ 15 ], forming a total of 45 pairwise classification problems. The ten charac-ters can be further divided into several groups, where pairs within each group are fairly challenging for the computer to classify, while pairs between groups are easier. The database contains 200 handwritten samples as binary images of size 64 by 64 for each character.

We used the LIBSVM package [ 2 ] as our SVM learning algorithm. To simplify the comparison of kernel functions, we used the default value for all the SVM parameters, except those for kernel functions, during the learning. The reported error rates are based on leave-one-out cross-validations.
We want to compare our specialized kernel function with a conventional polynomial kernel function. Third degree poly-nomial kernel functions have previously been employed for handwritten digit recognition. We conducted pilot experi-ments using polynomial kernels from second through fifth degree. The differences are often small but the optimal degree for a conventional polynomial kernel for the ETL9B dataset seems also to be three. To make a fair comparison, we also used third degree polynomials to define component kernel functions and, therefore, the derived feature kernel functions. Experiment 1 : Does a feature kernel outperform a similar conventional kernel? Figure 5 compares the classification error rate classifying using different kernels with different number of training examples. It can be seen that the feature kernel approach achieves similar performance as an SVM using polynomial kernel with nearly an order of magnitude less training examples. Furthermore, all 45 classification problems benefits from the feature kernel approach.
Figure 6 compares the number of support vectors result-ing from the training using different kernels with different number of training examples. The feature kernel approach always produces fewer support vectors than cubic kernel SVM, which implies less overfitting and more computational efficiency.
 Experiment 2 : How does task difficulty affect the improve-ment achieved by specialized feature kernel functions? To further analyze which problems benefit more from the additional information in the form of prior domain knowl-edge, we evenly divide the 45 classification problems into three groups according to number of errors made by a con-ventional SVM. Figure 7 shows that the feature kernel func-tion helps more in more difficult problems.
 Experiment 3 : How important is the EBL interaction? Finally, we test the importance of interaction between prior knowledge and the training examples by constructing a fea-ture kernel by weighting the component kernels equally with-out interacting with the training examples. Figure 8 shows that the interaction between prior knowledge and the training examples is crucial. 3 Explanation-augmented SVM 3.1 Overview EA-SVM uses generalized or explained examples and let SVMs treat them much as it treats conventional examples. In explained examples, only the important features identified by the explanations are allowed to contribute to the kernel computation. Given an original example x , and a subset of important features e  X  x from an explanation, the explained example v is constructed thus: v v The special symbol  X   X   X  indicates that this feature does not participate in the inner product evaluation. With numerical features one can simply use the value zero.

An explained example can be viewed as a generalization of an original example, in the sense that examples that satisfy the same explanations merit the same label for the same rea-sons and thus should be treated equivalently by the learner. Consider an SVMs linear separator in its high-dimensional lower dimensional linear surface to which the correct classi-fier should be parallel . To see why, consider the extensional definition of the explanation which is the set of all exam-ples that satisfy the explanation X  X  requirements. Assuming the ideal case, the SVMs feature space and linear separa-and relations. All of the examples that merit this label for the same reasons should be treated identically. In the high dimensional feature space they should have the same mar-gin from the correct classifier. This means that they fall on a parallel linear surface. This surface will be of a lower dimen-sion if there are any redundancies or irrelevancies in the high dimensional feature space with respect to this explana-tion. Such explanations constrain the correct classifier, and therefore, once discovered, can guide the learner. A sim-plified example in three dimensional space is illustrated in Fig. 9 , where the explanation specifies that one feature as rel-evant, therefore the constraint surface is a two dimensional plane.

In EA-SVM the constructed explanations are treated as preferences or soft constraints rather than hard constraints on the correct classifier. Their effect are blended on the SVM classifier with the conventionally-treated training set. 3.2 The EA-SVM optimization problem Perfect knowledge The EA-SVM is formulated in a way analogous to the hard margin SVM (Eq. 1 ). In the case of per-fect explanations, the learned classifier evaluates the original example and the generalized example to the same value: w  X  x + b = w  X  v
Geometrically, this requires the classifier hyperplane to be parallel to the direction x i  X  v i . These are called parallel constraints . The SVM quadratic problem becomes min 1 2 w 2 , subject to y i ( w  X  x i + b )  X  1  X  0 ,  X  i , Imperfect knowledge If the domain knowledge is imper-fect, the constraints cannot all be met. The explained exam-ples should then be treated as a bias to be respected as much as possible. This is similar to the standard SVM algorithm for the non-separable case (Eq. 2 )[ 25 ]. New slack variables (  X  ) measure the difference between the evaluations of the original examples and the generalized examples:  X  i , w  X  x
To penalize violations of the constraints, the objective function is changed from w 2 / 2to w 2 / 2 + K i  X  i ; K is called the confidence parameter . It reflects confidence in (or assessed quality of) the domain knowledge. It will be set automatically by cross validation. A larger K corre-sponds to better knowledge and a greater penalty for dis-agreeing with the explanations. Now the primal problem becomes subject to y i ( w  X  x i + b )  X  1  X  0 ,  X  i , 3.3 Solutions for EA-SVM With perfect knowledge The optimization problem can be solved with Lagrange multipliers. The primal Lagrangian of ( 18 )is L Setting the derivatives w.r.t. the primal variables to zero, we have w = Substituting into L P , the dual problem becomes maximizing (w.r.t.  X  i , X  i ): subject to  X  i  X  0 , i  X  i y i = 0. This is a standard form of the quadratic programming (QP) problem w.r.t. the variable vector composed by  X  i , and  X  The quadratic term is no longer the original n  X  n kernel matrix but a 2 n  X  2 n matrix in which we employ a single explanation for each training example. After computing this quadratic matrix, we can use a standard QP solver for the above optimization problem.

Notice that, if we fail to build an explanation for a particu-lar example, all input features are treated as important, there-variable in the QP problem can be simply eliminated. With no explanations the problem reduces to a standard SVM. With imperfect knowledge The Lagrangian of ( 20 )is subject to  X  i  X  0 , i  X  i y i = 0 , Requiring that the gradient of L P w.r.t. w , b , and  X  i yields  X   X  w  X   X  b  X   X  X  Substituting into the Lagrangian formulation L P with  X  =  X   X   X  we obtain the dual subject to  X  i  X  0 , i  X  i y i = 0, This is a QP problem with the same solution as the perfect explanation case, w = i  X  i y i x i + i  X  i ( x i  X  v i )  X  s are now bounded by K . If we have perfect explanations, then K and the  X  i are unbounded, and the problem reduces to the ideal case. Conversely, if K and the  X  i are 0, the problem ignores the explanations and reduces to a standard SVM.
EA-SVMs can be solved by the standard SVM methods as the QP problem is convex. 3.4 Analysis EA-SVM with hard constsraints The fat-shattering dimension measures the expressiveness of a hypothesis space. The parallel constraints from our explanations should further restrict the expressiveness yielding an easier learning prob-lem. Consider a class of linear functions F of norm less than or equal to B restricted to the examples in the sphere of radius R about the origin. The fat-shattering dimension of F is bounded by fat F ( X  )  X  ( BR / X  ) 2 (see [ 1 ]). To this we add parallel constraints: Theorem 1 Fat-shattering of linear functions with parallel constraints.
 Consider a Hilbert space and one class of linear functions F of norm less than or equal to B satisfying the following example of x i ,let R V denote the radius of the ball that con-tains all v i , then the fat shattering dimension of F can be bounded by fat F ( X  )  X  ( BR V / X  ) 2 .
 Proof For every f  X  F , where f = w  X  x + b , we define a linear function g  X  G : V  X  X  0 , 1 } where g (v) = w  X  v + The norm of functions g  X  G is the same as f  X  F ,but they are restricted to the examples in the sphere of radius R
V . Bartlett and Shawe-Taylor [ 1 ] tell us the fat-shattering dimension of G is bounded by fat G ( X  )  X  ( BR V / X  ) 2 .
Next we show that F has the same fat-shattering dimen-sion as G . First observe that V  X  X , therefore if a set of points V m ={ v 1 ,v 2 ,...,v m } is shattered by G ,theyare also shattered by F . Therefore fat F ( X  )  X  fat G ( X  ) consider the explanations as a many-to-one mapping m X  X  V , then f ( x ) = g ( m ( x )) . Therefore if a set of points X m ={ x 1 , x 2 ,..., x m } is shattered by F , then the set V { m ( x fat fat
Applying Theorem 3.12 in [ 18 ] to linear classifiers with parallel constraints immediately yields the following theorem: Theorem 2 Generalization error bound on linear classifiers with parallel constraints.

Let S ={ x 1 , x 2 ,..., x m } be a training set of size m drawn from a fixed but unknown distribution over the input space X . Let v i be the explained example of x i , and let R V the radius of the sphere containing all v i . Then with prob-ability 1  X   X  , the generalization error of a linear classi-fier ( u , b ) on X with u = 1 that correctly classifies all examples in S with margin  X &gt; 0 , and satisfies the parallel constraints  X  i , f ( x i ) = f (v i ) , is bounded by: ( This bound is of the same form as the bound for standard SVM in [ 18 ], with R V playing the role of R . R measures the radius of the example sphere in the original space, while R measure the radius of the explained example sphere. There-fore, the explanations have most to offer when the ratio R is small. This is the case when the learning problem is diffi-cult but the domain knowledge is informative.

This yields our first two testable qualitative predictions: 1. Holding the domain knowledge constant, explanation-2. Over the same learning problems, better knowledge EA-SVM with soft constraints When the constraints cannot all be satisfied, we want a classifier that is as consistent with the constraints as possible. We follow the analysis of [ 19 ] for soft margins. The input space X is mapped to a higher dimensional space so that the parallel constraints can be sat-isfied.

Following [ 19 ], we use the notion of L f ( X ) to represent the set of real valued functions f on X with inner product of f , g  X  L f ( X ) as f , g = x  X  supp ( f ) f ( x ) g ( supp ( f ) is the support of f .Weuse f 0 to denote a special function such that f 0 , g  X  0.
 Now we define a new inner product space X  X  L f ( X ) . For any fixed &gt; 0, we embed X into X  X  L f ( X ) with  X  : x  X  ( x , f v  X  (v,  X  v ) , where  X  v  X  L if z = v , and  X  v ( z ) = 0, otherwise.

We augment a linear classifier ( u , b ) on X to a (  X  u , X  X  L some algebra, we observe that the augmented classifier (  X  on X  X  L f ( X ) classifies  X  ( x ) the same as ( u , b ) x :  X  straints defined by  X  ( x i ) and  X  (v i ) :  X  u ( X  ( x i 0. Therefore, Theorem 1 provides a bound on its fat-shatter-ing dimension, and Theorem 2 yields the bound on its error rate, which is the same as the error rate of the original clas-sifier ( u , b ) .

To examine the fat-shattering dimension of the augmented nent in  X  u increases the square of the norm of the classifier to  X  u 2 = u 2 + D 2 / 2 , where D = i ( u  X  ( x i  X  v i )) 2 . Also the explained examples are embedded in X  X  L f ( X ) the mapping  X  : v  X  (v,  X  v ) , which makes  X  (v) 2 =
Taking these adjustments into account, Theorems 1 and 2 yield the following result: Theorem 3 Generalization error bound of linear classifiers with soft constraints.

Fix &gt; 0 , b  X  R. Randomly draw training set S = x , x ity distribution on the input space X . Let v i be the explained example of x i , and R V denote the radius of the sphere con-taining all v i . Then with probability 1  X   X  , the generalization error of a linear classifier ( u , b ) on X with u = 1 that correctly classifies all examples in S with margin  X &gt; 0 is bounded by: ( m , h , X ) = 2 where h = 64 D  X  3.5 Experiments We devised several empirical investigations to validate the EA-SVM method and test the predicted qualitative behaviors.
The SVM and EA-SVM are implemented with Matlab using PR_LOQO as the QP solver. The confidence parameter K in the algorithm is set automatically by five fold cross-validation.

The primary domain concerns classifying pairs of hand-written Chinese characters. We used the ten characters as in Fig. 1 from the ETL9B database. The ten characters can be further divided into groups, in which characters from differ-ent groups are easier to distinguish, while characters within the same group are more difficult to distinguish. We followed the same experiment settings as in Sect. 2.5 , with the same 45 pair-wise classification problems for handwritten Chinese characters.

Domain knowledge is specified at the level of stroke descriptions. For each character, we provide a prototype that describes how many strokes are in the character, whether the strokes are horizontal, vertical or slanted, from this we derive whether strokes will be connected, crossed, etc. We approx-imate each stroke with a line segment, and use Hough trans-formation to determine the significant lines in the images. This provides us with some approximate information on asso-ciating pixels with strokes. With such association, building explanations is simply to select those pixels that realize the set of strokes that are essential to distinguish two training char-acters. The explained examples are also images with some pixels (automatically) removed. In this work, human experts provide the knowledge of which strokes are important for classification.
 Experiment 1 : does explanation-augmentation help? In our first experiment, we compared EA-SVM with three con-ventional SVMs, which are trained on original examples, explained example and original + explained examples respec-tively. The last two control conditions can viewed as na X ve approaches to use explained example. The main thrust of EBL generally, and this research particularly, concerns the advantage of interaction between training examples and knowledge. A better control would be one that directly inputs knowledge instead of explanations to SVM training exam-ples. Yet, it is not clear how to design such control condition; domain knowledge is the kind of knowledge that cannot be directly used by a statistical learner. The control conditions used in this experiments, nevertheless, illustrates the advan-tage of EA-SVM.

Figure 10 shows the comparison of the average error rates in all 45 tasks over a randomly selected training set of 320 examples; the remaining examples are used as the test set. The results are shown as a scatter plot where the horizontal axis is the conventional SVM trained on original examples, so that points falling below the 45  X  line correspond to learning problems for which EA-SVM or control SVMs outperforms the standard SVM.

The results in Fig. 10 show that the success of EA-SVM is due to the appropriate use of explained examples. Using only explained examples is similar to setting confidence parameter to infinity, while using both equally corresponds roughly to setting the confidence parameter to 1. EA-SVM uses cross-validation to automatically set confidence parameter using training examples. This gives us the robustness.
 Experiment 2 : Do difficult problems benefit more? We f u r -conditions. In the easy condition, characters to be distin-guished are drawn from different groups (33 tasks). In the difficult condition, characters from the same group must be distinguished (12 tasks). Figure 11 shows the average learning curves of these two conditions. The improvement on difficult problems is greater than easy ones at all training levels. EA-SVM and SVM performance on the easy tasks is almost indistinguishable.
 Experiment 3: The effect of knowledge quality Good but imperfect knowledge improves EA-SVM behavior over con-ventional SVMs. But does behavior degrade gracefully with poorer knowledge? To answer this question, we built two additional kinds of explanations. In the first one, random sets of pixels are set to 0 with no regard to the original domain knowledge. In the second one, the complement of the original explanation is used so that important pixels are now unimpor-tant and vice versa. Figure 12 shows the scatter plots of errors made by standard SVM and EA-SVM in these conditions. Random explanations almost never harm the performance, with some marginal improvement in certain tasks probably due to chance. It is clear that even opposite explanations do not significantly harm performance. 4 Conclusion We have shown that statistical learners like SVMs can benefit from incorporating prior knowledge via special kernel func-tions, as well as via other means like additional preference over the space of possible classifiers.

There have been other approaches to incorporate knowl-edge into SVMs and other statistical learners as well. Virtual jittering [ 4 ], and tangent propagation [ 21 ] incorporate trans-formation invariance into SVMs. Locally-improved kernel function [ 17 ] explores spatial locality property. Convolu-tional networks [ 9 , 22 ] incorporate knowledge in their spe-cially designed architecture to recognize visual patterns with robustness to distortions and simple geometric transforma-tions. Knowledge-based SVMs and kernels [ 6 , 7 , 12 ] incorpo-rate prior rules. There are also many approaches that extract character high-level features from pixel representations [ 20 , 24 ].

Combining EBL into a statistical learner is a novel direc-tion of using knowledge. With the ability to specify prior knowledge as domain knowledge rather than solution knowl-edge, the domain expert need not also be an expert in the learning algorithms. The interaction of domain knowledge with training examples introduced by EBL yields solution knowledge of higher quality and helps to obtain good clas-sifiers effectively, especially in difficult learning problems even if the domain knowledge is not perfect. EA-SVMs have been applied to other domains including categorizing pro-teins into super-families and classification of Reuters news articles [ 23 ].

The role of domain knowledge rather than solution knowl-edge demands further study; this is an important and little-explored direction. The approaches described above are extremely simple, and are just two of the many possible EBL approaches to exploiting existing domain knowledge to benefit statistical learning. One of our next steps is to use EBL to automatically construct informative high-level fea-tures guided by the interaction of domain knowledge and training examples [ 11 ]. Other very different approaches may well be found that possess more interesting tradeoffs and more attractive performance.
 References
