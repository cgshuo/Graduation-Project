 Interleaving is an online evaluation method to compare two alternative ranking functions based on the users X  implicit feedback. In an interleaving experiment, the results from two ranking functions are merged in a single result list and presented to the users. The users X  click feedback on the merged result list is analysed to derive preferences over the ranking functions. An important property of interleaving methods is their sensitivity, i.e. their ability to reliably derive the comparison outcome with a relatively small amount of user behaviour data. This allows testing of changes in the search engine ranking functions frequently and, as a result, rapid iterations in developing search quality improvements can be achieved.

In this paper we propose a novel approach to further im-prove interleaving sensitivity by using pre-experimental user behaviour data. In particular, the click history is used to train a click model, which is then used to predict which interleaved result pages are likely to contribute to the ex-periment outcome. The probabilities of presenting these in-terleaved result pages to the users are then optimised, such that the sensitivity of interleaving is maximised. In order to evaluate the proposed approach, we re-use data from six actual interleaving experiments, previously performed by a commercial search engine. Our results demonstrate that the proposed approach outperforms a state-of-the-art baseline, achieving up to a median of 48% reduction in the number of impressions for the same level of confidence.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval Keywords: interleaving, online evaluation
The evaluation of retrieval systems is vital to ensure progress in information retrieval (IR). Historically, the system-based evaluation with manual judgements [18] motivated by Cran-field X  X  experiments has proven popular. However, this ap-proach has several limitations, as discussed by Voorhees [18]. Firstly, often the manual judgements are expensive to col-lect, since labelling requires labour from the trained profes-sionals. Secondly, document relevance can be hard to define, especially when the user X  X  intent can be ambiguous or the result set is personalised [1]. On the other hand, the online user-based experimental methods, such as A/B testing (e.g., [17]) and interleaving [9, 10, 11, 14], can overcome these lim-itations. As these methods leverage the live query stream to infer the users X  preferences, they are believed to represent the actual preferences of the users.

Interleaving was first proposed by Joachims et al. [10, 11] as an online unbiased evaluation method. The idea behind this method is the following. Given two ranking algorithms we want to compare, A and B , we run an interleaving ex-periment on a portion of the query stream. In a user session affected by the experiment, the results of A and B are mixed into an interleaved result list that is shown to the user just as a regular search result page. Next, the user X  X  click be-haviour is interpreted in order to derive which of the tested algorithms provides the users with a better results ranking.
Following Radlinski and Craswell [12, 13], we define the sensitivity of an interleaving method as its ability to derive reliable experiment outcomes with little data. High levels of sensitivity are important in web search retrieval for sev-eral reasons. Firstly, sensitive methods allow search engines to evaluate improvements and iterate fast, i.e. to make more business decisions in a unit of time and progress quickly. Sec-ondly, with a sensitive method, it is possible to evaluate even subtle improvements in a short time. Finally, since a con-siderable part of the evaluated changes may lead to a degra-dation of the user experience, sensitive methods can help to identify such changes early and reduce the user frustration.
In this work, we concentrate on improving the sensitiv-ity of the interleaving methods. As we will discuss in the next section, this problem has received a considerable at-tention from the research community. However, we address this problem from a new perspective. Our hypothesis is that by using the user behaviour data prior to the start of the experiment, it is possible to adjust how often a particular interleaved result list is shown, to achieve higher levels of sensitivity.

In order to test this hypothesis, we propose and evaluate a method to leverage historical user behaviour data observed in the click log to optimise the parameters of the interleaving experiment.

The contributions of this paper are three-fold:
The remainder of the paper is organised as follows. Af-ter discussing the related work in Section 2, we derive a problem of optimisation of the interleaving experiment pa-rameters in Section 3. Next, in Section 4 we discuss how this optimisation problem can be formulated and solved be-fore deploying the experiment online. The dataset and the evaluation methodology used in our study are discussed in Sections 5 and 6, respectively. Further, we report our re-sults in Section 7 and close the paper with conclusions and a discussion of the possible future work in Section 8.
We consider our work to be related with two areas of re-search. Firstly, the research in the area of the interleaving algorithms and, in particular, a variety of methods to im-prove sensitivity of interleaving is relevant to this work and we discuss it in Section 2.1. Secondly, in this paper, we rely on the recent progress in the user click modelling methods, discussed in Section 2.2. Three popular interleaving methods were proposed so far: Balanced Interleaving [10], Team Draft [14], and Proba-bilistic Interleaving [9]. In addition, several modifications of these methods aimed to improve their sensitivity were considered [3, 12, 19]. While Radlinski and Craswell [12] aimed to study the agreement between various metrics used in Cranfield paradigm evaluations with outcomes of the on-line interleaving experiments, they also paid considerable at-tention to the problem of weighting clicks in the Team Draft algorithm to ensure the sensitivity of the experiments. They demonstrated that weighting the user clicks by the logarithm of the clicked document rank can lead to higher sensitivity of the interleaving experiments.

A similar idea was studied by Yue et al. [19], who proposed a method to train a credit assignment scheme so that the overall sensitivity of the interleaving is increased. More pre-cisely, Yue at al. considered a supervised machine-learning problem where the outcome of the interleaving experiment known. Given the user behaviour data for these experi-ments, they learned a click weighting scheme so that the power of a statistical test on new experiments was max-imised. Chapelle et al. [3] performed a thorough experimen-tal comparative study of interleaving algorithms, concen-trating on the Balanced Interleaving and Team Draft algo-rithms. In particular, they investigated relative sensitivity of these algorithms and several of their modifications with non-uniform click weighting schemes. For instance, their re-sults suggested that ignoring clicks on the top results shared by both ranked lists considerably improved the sensitivity of the Team Draft algorithm.

As we can see from the above discussed work, a consid-erable body of previous research studied how to improve the sensitivity of the online experiments by means of ad-justing the credit assignment (click weighting) scheme used in the corresponding interleaving algorithm. In our work, we consider a somewhat complimentary approach: given a particular credit assignment scheme, we investigate how one can control the probabilities of showing the interleaved re-sult pages to the users to achieve maximum sensitivity. We expect that combining the previously proposed click weight-ing schemes with the approach considered in this work can lead to even higher levels of sensitivity. However we leave a thorough study of the possible combinations as a promising direction of future research.

Our proposed approach is based on the study of Radlinski and Craswell [13], who proposed a formal Optimised Inter-leaving framework describing how three components of an interleaving method (a set of interleaved result lists, a credit assignment scheme, and a distribution over interleaved re-sult lists) can be combined so that the resulting interleaving algorithm is unbiased. Their proposed framework was used to build a family of interleaving algorithms with different credit assignment schemes. They also optimised the distri-bution over the interleaved result sets so that the uncer-tainty in a winner of a particular impression is maximised. This uncertainty is calculated with respect to a randomly clicking user. Despite relying on the framework proposed by Radlinski and Craswell [13], our approach has the follow-ing differences. Firstly, we study a different perspective of the interleaving sensitivity. Instead of considering the win-ner uncertainty within a single interleaved result list, we are aiming to reduce the number of user impressions that con-tribute little to the experiment outcome. Secondly, instead of considering a randomly clicking user, we leverage massive click log data containing the real user feedback and use it to train a click model that is further used to predict the actual behaviour of a real user.
 Another closely related work is by Hofmann et al. [8]. In this work, the authors study the possibility to leverage the query log data to predict an interleaving experiment outcome without actually running the experiment. Their results suggests that Probabilistic Interleaving [9] can effec-tively re-use such historical data. In contrast, in our work we aim to leverage historical data to improve the sensitivity of the future interleaving experiments.
Apart from the research in the area of the interleaving methods, this paper is based on the recent progress in re-search on user behaviour modelling. The problem of inter-preting and modelling the user X  X  clicking behaviour is non-trivial, since the user X  X  actions are prone to several biases. Arguably, the position bias is the most studied one [6]. It af-fects the way users click on results: results that are ranked higher collect more clicks from users even if they are not as relevant as the results ranked lower. One of the ap-proaches to model the position bias is considered in the posi-tion click models [5, 15]. The underlying idea was formalised by Craswell et al. [5] in the examination hypothesis : a search result is clicked only if it is examined, and the user considers the result to be relevant. The position-based models assume that the examination probability depends only on the rank the result is presented at. A more sophisticated approach is considered by the cascade click model [5]. This model is based on the following assumption: a result can be exam-ined only if all of the results ranked higher were examined. An important extension of the cascade model is the Dy-namic Bayesian Network (DBN) click model later proposed by Chapelle and Zhang [4]: this model additionally accounts for the effect of the users abandoning their search, and is ca-pable of modelling sessions with several clicks, by separating the perceived and the actual relevance of the results.
In most cases, the user click modelling has been used to ex-tract new ranking features, or to substitute expensive man-ual assessment procedures [4]. In our work, the click mod-elling is used to predict the user behaviour once a new result page is shown to the user. A similar scenario was considered by Guo et al. [7] to evaluate click models by their ability to predict the position of the first and the last clicks.
A somehow related approach was used by Hofmann et al. [9] to evaluate interleaving algorithms: the parameters of the Dependent Click Model [7] were estimated from the available document relevance judgements, and the result-ing click model was used to evaluate interleaving algorithms in the absence of an actual query log by generating a syn-thetic one. In contrast, in our work, we learn a user click behaviour model from the actual, not synthetic, query log data and hence leverage it for a completely different purpose: optimising an interleaving algorithm.

As can be seen from the above discussed work, the sensi-tivity of the interleaving methods attracted a considerable attention from the IR community. However, the possibil-ity to use the historical user behaviour data to increase the interleaving sensitivity has not been studied before. After reviewing how interleaving experiments are performed, we describe the proposed interleaving approach in Section 3.
Before discussing the sensitivity of the interleaving exper-iments, we briefly review how the interleaving experiments are performed and introduce the required notation. Each interleaving algorithm essentially consists of three parts: a rule to build a set of the interleaved result lists for a query; a credit assignment scheme used to interpret the user clicks; and a distribution determining how often a particular inter-leaved result list is shown to the users. The latter is further referred to as the experiment policy . Further, assuming that a query q is fixed, let us introduce the notation used in this paper. L is a set of considered interleaved result lists, L stands for the i th interleaved result list, and L i ( r ) denotes a result ranked on the r th rank of L i . Similarly to [3, 13], we define a credit function  X  i ( r ) that describes a score assigned result ranked on position r in the result list L i .
A part of the user X  X  interaction with a search engine result page, which starts with submitting a query and ends when the user submits a new query or leaves the search engine, is further referred to as impression . Each impression v is associated with a shown result page L i and with the user X  X  clicks. The interleaving credit scheme can be used to derive which of the compared systems ( A or B ) wins in a particu-lar impression. More formally, the user X  X  clicks observed in a particular impression v with a result list L i shown are trans-formed into scores h ( v ) = P r  X  i ( r ) 1 { r clicked in v } , where 1 { X } is an indicator function. After that, an impression-level aggregation c (  X  ) can be applied. For instance, this aggrega-tion can represent which alternative ( A or B ) wins a par-ticular impression (i.e. c ( v ) = sign ( h ( v ))) or normalise the score by the number of clicks [3].

After running the experiment, a single statistic  X  is used to describe the experiment outcome [3]: where N denotes the total number of impressions.

To simplify the analysis, we follow Yue et al. [19] and con-sider that A ( B ) wins in the interleaving experiment if it receives more credit than B ( A ). This approach is equiv-alent to considering an identity credit aggregation function c ( v ). In order to represent the experiment outcome in this case, we define w  X  A to be the total credit assigned to A during the experiment. Similarly, w  X  B denotes the credit assigned to B . As a result, the experiment outcome can be defined as follows: Informally,  X   X  equates to the difference in the credits as-signed to A and the credits assigned to B , divided by the total number of impressions the users observed in the exper-iment. Again,  X   X  &gt; 0 means that A outperforms B .
We hypothesise that organising the interleaving experi-ment policy in such a way that the result pages L i that are unlikely to contribute to the difference in Equation (2) are shown as rarely as possible should improve the ability to derive reliable conclusions with less impressions. In other words, with N being fixed, the interleaved result pages that often lead to ties 1 should be shown less frequently in com-parison with those that witness a contrast between A and B .
In the remainder of this section, we study how this goal can be achieved based on the Optimised Interleaving frame-work [13], initially assuming that information about the fu-ture user behaviour is available at the start of the experi-ment. We will relax this assumption in Section 4.

At first, let us denote the credit assigned to A after show-ing the result list L i as w  X  i A . Similarly, we define w this notation, it is possible to expand w  X  A and w following way: Putting (3) into (2) and further grouping the terms in the numerator, we re-write  X   X  as follows: Next, we denote as N i the number of impressions where L i was shown and re-write (4) in the following way: Further, we refer to  X  i as the probability of showing L i the interleaving experiment, i.e. the vector  X  represents the experiment policy. Substituting  X  i , we obtain: Intuitively, with the total number of impressions N fixed, higher absolute values of  X   X  correspond to higher contrast between A and B , and lead to the ability to determine the experiment outcome with higher reliability. Thus, let us consider a simple upper bound on the absolute value of  X   X 
For instance, the result pages that do not attract clicks or often have similar credits assigned to both alternatives. Indeed, the absolute value of  X   X  is bounded by the product of the experiment policy and the statistics of the interleaved bution that the impressions with L i make to the difference (2), as we discussed earlier.

So far, we have discussed the experiment outcome  X   X  and obtained the upper bound on its absolute value. However, this upper bound also provides us with an idea how the ex-periment policy can be adjusted before starting the experi-ment. Despite the fact that a higher upper bound does not necessary imply a higher value of  X   X  , in this work we argue that controlling  X  i , so that the upper bound increases, leads to higher sensitivity of the interleaving. Intuitively, this idea can be expressed as follows: in a real world scenario, with everything else being equal, it is generally better not to show
Having discussed the motivation behind our approach, let us consider the problem of selecting the optimal experiment policy before starting the experiment. Due to the random nature of user behaviour, it is reasonable to consider the expected difference in the credits assigned to A and B for a particular interleaved result list L i , instead of | w which approaches this expectation as the number of impres-sions N i grows. Further, we denote this quantity as  X  i where C i A and C i B stand for the credits assigned to A and B after demonstrating the result list L i to a user, respectively. The values of  X  i form the vector  X  .

As discussed above, our goal is to adjust policy  X  so that the upper bound (7) increases. In order to achieve that, we leverage the Optimised Interleaving framework [13], which can be used to optimise the interleaving experiment proper-ties without introducing biases. In particular, Radlinski and Craswell [13] define a criterion of the unbiased interleaving policy: the expected credit from a randomly clicking user should be zero. Formally, for an interleaving method to be unbiased, the following constraint on its set of interleaved result lists L , its credit function  X  , and its policy  X  has to be met: Temporarily assuming that values of  X  are known and com-bining (7), (8), and (9) we formulate our optimisation prob-lem as follows:
Indeed, the solution of the optimisation problem stated by the set of Equations (10) maximises the upper bound of the experiment X  X  outcome |  X   X  | (10a), meets the fairness condition (10b) proposed by Radlinski and Craswell [13], and represents a valid distribution (10c &amp; 10d). In this work, we argue that setting the interleaving experiment policy to the solution of the linear optimisation problem (10) leads to a higher sensitivity of the experiment.

Since the values of  X  are predicted from the noisy user feedback, this can cause undesired noise to the solution. In-deed, a small variation in values of  X  might result in the linear programming problem (10) having a completely dif-ferent solution. In order to reduce this noise, we introduce a regularisation term to the optimisation objective (10a) that adds a penalty to solutions that diverge too far from the uniform policy  X  U 2 . Thus, we replace objective (10a) with the following expression: where  X  is a non-negative scalar parameter. With  X  being zero (10a*) reduces to (10a), while large values of  X  force the solution to be the uniform vector. In the latter case, the pre-experimental user behaviour is ignored and all in-terleaved result pages are demonstrated to the users with equal probabilities. If L and  X  coincide with that of the Team Draft algorithm, the uniform policy  X  U corresponds to the policy of Team Draft, hence the solution of the opti-misation problem (10) with large  X  coincides with the Team Draft algorithm. Therefore, Team Draft can be seen as a fea-sible solution of the optimisation problem (10), which does not rely on the user behaviour information and weights all possible interleaved result sets equally. On the other hand, with  X  = 0 the optimal solution of (10) is completely de-fined by the noisy estimates of  X  from the previous user click behaviour and becomes  X  X isky X . A higher level of risk may result in higher improvements in the interleaving sensitivity, but it also may lead to a decrease in sensitivity when the estimates of  X  are incorrect due to prevalent noise in the user feedback.

We argue that the parameter  X  provides a convenient mechanism to control the amount of prior user feedback in-troduced into the resulting interleaving algorithm. For in-stance,  X  can be set to zero for queries that have sufficiently large pre-experimental user click data and set to infinity for queries with little user information available. However, in this work, we restrict  X  to be uniform for all queries, and leave the study of the per-query adaptation of  X  as a direc-tion for future work.

Since the values of vector  X  are not known before starting the experiment, a question arises how  X  can be estimated. In this paper, we propose to use pre-experimental click log data to train a model of the user click behaviour and use it to predict  X  . We discuss this approach in the next section.
As discussed in the previous section, once the values of  X  for the interleaved result pages L are available, it is possible to optimise the interleaving to achieve a higher sensitivity by selecting the solution of (10) as the experiment policy. In this section, we study how  X  can be estimated by using the click log data.
In the experimental part of this paper, we work with the Team Draft-based set of interleaved result lists L and associated credit functions, so  X  U is a feasible solution of (10). For other combinations of L and  X  , other policies might be more suitable.
Input : Set of sessions Q ; Beta prior parameters:
Output : The click model parameters for each foreach session s  X  Q do end foreach u do end Algorithm 1: Training the sDBN model, as described by Chapelle et al. [4].

We argue that leveraging the click log data is a very promising approach due to the following reason. Usually, only a tiny part of the query stream is used to perform an in-terleaving experiment, thus the whole query stream provides a considerably larger amount of data reflecting user prefer-ences. Moreover, interleaving experiments are performed on a limited timescale, while commercial search engines can store query logs spanning several years of operation. As a result, one can easily obtain two orders of magnitude more impressions from the non-experimental log than what can be obtained from a whole experiment run for several weeks. We consider our goal to leverage this massive evidence to make the interleaving experiments more sensitive. The approach we propose achieves this goal in three steps. In the first step, a click model is trained on the user click behaviour history. In the next step, this pre-trained model is used to predict the future user behaviour and to estimate the parameters of the optimisation problem (10). Finally, we find the optimal policy and run the interleaving experiment on a part of the query stream.

In the remainder of this section, we discuss the click model (Section 4.1), the estimation of the optimisation objective parameters (Section 4.2), and a possible inter-query bias in-troduced (Section 4.3).
The general idea behind predicting the parameters  X  of the optimisation problem (10) is the following. Having ob-served a massive click log representing the users X  behaviour, we can train a generative model of the user click patterns. Once the click model with the pre-trained parameters is available, it can be used to  X  X xplain X  the behaviour observed in the click log and, more importantly, to model how users will behave once the result page is modified. The latter case is the most important in our task, since the interleaved result lists will often contain the same documents, but ar-ranged differently. Also, these documents are likely to be presented in the deployed retrieval system before, but at different positions and surrounded by different documents. For this reason, we rely on the ability of the underlying click model to generalise while predicting the user behaviour.
As discussed in Section 2, a variety of generative click models have been proposed. In this work, we use a sim-ple yet effective modification of the Dynamic Bayesian Net-work, simplified DBN (sDBN) 3 , proposed by Chapelle and Zhang [4]. Informally, the sDBN model assumes that a user examines the result list from top to bottom. After examin-ing a document u , the user either finds it attractive with probability a u and clicks on it, or continues to the next document. After clicking on a document, the user is satis-fied with probability s u and stops the examination process. Thus, for a fixed query, the model has two parameters per document u : attractiveness a u and the probability of sat-isfying the user s u . These parameters are learned from a click log by means of Algorithm 1, described by Chapelle et al. [4]. This learning procedure imposes Beta priors on the model parameters and following [4] we set them to 1, i.e.  X  a =  X  s =  X  a =  X  s = 1.

After training the model parameters it can be used to predict  X  as we discuss in the next section.
According to our definition (8),  X  i equates to the abso-lute value of the relative difference between credit assigned to alternatives A and B after observing infinitely many user interactions, or, alternatively to the absolute value of the ex-pected difference in the credits assigned to A and B . Under the sDBN model, this quantity can be calculated analyti-cally by means of Algorithm 2, as we prove in the following Lemma 1: Lemma 1. Algorithm 2 calculates  X  , defined in (8) .
Proof. Consider an interleaved result list L i and let P e denote the probability of the user examining the position r , P c ( r ) denote the probability of clicking on position r by P ( r ), and P s ( r ) denote the probability of the user being sat-isfied with the document in the r th position after clicking on it. Under this notation, we can re-write the expectation in (8) as follows: Since the sDBN model assumes that the user clicks on a result only after examining it, we can express P c ( r ) in the following form: In turn, under the cascade hypothesis the document in the r th position is examined only if the user is not satisfied with all the documents ranked above:
Putting (13) and (12) in (11) we note that Algorithm 2 indeed calculates  X  as defined by Equations (8) &amp; (11).
For queries for which there is no user click history avail-able, the optimal policy can be reduced to the uniform policy
We have also tried the Dependent Click Model [7] and found it to perform worse.
Input : Parameters of the click model, a u , s u ; set of
Output : Vector of the optimisation objective (10a*) // P e ( r ) denotes the probability of examining the r th position
P e (1)  X  1 foreach L i  X  L do end
Algorithm 2: Estimating  X  with the pre-trained sDBN click model.  X 
U , which is equal to Team Draft X  X  policy if the interleaved result lists and the credit function of Team Draft [14] are used. The question arises if improving sensitivity only for a part of queries (in our case, queries with the click data avail-able) can introduce a bias into the interleaving experiment. The fairness criterion we use in the optimisation problem (10b) eliminates bias only within a single query and, to the best of our knowledge, there is no formal criteria that can define if an inter-query bias is introduced to an interleaving experiment. For instance, the experiment policy is optimised in [13], but the inter-query bias is not discussed. However, once formal criteria of the absence of the inter-query bias are proposed, it might be possible to add them to the optimisa-tion problem constraints. Another possibility is to perform a long real-life study of the proposed algorithm X  X  agreement with other means of the retrieval evaluation: system-based evaluations with manual judgements, A/B testing, and other interleaving methods.

However, in some cases the absence of the introduced bias can be guaranteed. For instance, the inter-query bias is not introduced if the tested change in the search engine ranker has equal chances to improve (degrade) the ranking quality for the frequent and long-tail queries.
Interleaving is an online evaluation method. However, it is more convenient to evaluate the sensitivity of an interleaving method itself in an offline experimental setting, since this al-lows us to compare several methods on the same dataset. In this paper, we use an offline evaluation approach similar to the approach used by Radlinski and Craswell [13]. However, in our case two datasets are required. The first datasets rep-resents the pre-experimental user behaviour and is used to train the click model parameters. It is further referred to as the user modelling dataset. The second dataset represents the interleaving experiments that were conducted as a part of the every-day experimentation practice at Yandex. We refer to it as to the experimental dataset. In order to simu-late a real-life scenario, the datasets are sampled over conse-quent non-overlapping time periods, with the user modelling dataset preceding the experimental dataset.

In order to collect the user modelling dataset, we apply the following filtering: firstly, we exclude all sessions that N ame #queries #impressions #CM sessions winner E 1 1 ,311 181,981 3,682,895 A E 2 524 82,008 2,771,280 B E 3 468 119,287 3,198,372 A E 4 1,502 109,596 5,691,939 A E 5 1,255 52,314 2,045,122 A
E 6 279 9,175 1,100,874 B are affected by any online experiment, as that might result into a bias in the evaluation of the ability of the click model to predict the parameter  X  ; we remove all sessions with no documents clicked, as well as sessions with more than ten re-sults examined since those can introduce an additional noise to the sDBN model. In order to balance the dataset size, the freshness of the click model parameters and the dataset sparseness, we use the following strategy: for the top 100 most frequent queries we collect the users X  click behaviour over a period of week; for the rest of the queries we collected behaviour data from the eight previous weeks as well. All queries are normalised to lowercase. The same user mod-elling dataset is used in all experiments.

The interleaving experiments are sampled from the query log, starting from day after the click modelling dataset time span ended. The experimental data represents the users X  click behaviour recorded while performing six ( E 1 ...E 6 Draft-based interleaving experiments in April-May, 2013. In order to reduce variance in the offline evaluation, for a par-ticular interleaving experiment, we keep only those queries that have every possible combination of an interleaved result page and a credit assignment rule (under the Team Draft al-gorithm) shown to the users at least once. Next, in order to avoid sparsity, we consider only the top six results in each result list. To further reduce noise, we exclude queries with results that are examined less than two times in the user modelling dataset. Queries with equal result lists for both A and B are removed as non-informative under the deduped credit assignment. The total number of user sessions used to train the click model parameters for queries in the experi-mental dataset is referred to as click modelling sessions (CM sessions), We present the statistics on the whole dataset in Table 1. In the experiments considered, the A ranking func-tion represents the production search system, while B corre-sponds to the experimental ranking. Winners are defined as a result of Team Draft on the considered interleaving dataset (bootstrap test, p  X  0 . 05).

Having introduced the dataset used in the experimental part of our paper, we proceed to discussing the evaluation methodology in the next section.
Our experimental study has the following goals. The first goal is to ensure that solving the optimisation problem (10), stated in Section 3, indeed helps to maximise the interleav-ing sensitivity. The second goal is to study the effectiveness of the method proposed to estimate the parameter  X  , de-scribed in Algorithm 2. Finally, we aim to investigate how our approach compares to the baseline interleaving method when several credit aggregation schemes are considered.
However, before addressing these experimental goals, we firstly discuss the metrics (Section 6.1) and statistical ap-proaches (Section 6.2) used in our study. After that, we discuss the baseline (Section 6.3) and the credit aggregation schemes considered in our experiments (Section 6.4).
In order to measure how good the predicted values of  X  are, we use the following idea. Let us assume that we have a predicted value of the parameter  X  ,  X  predict and  X  predict is the corresponding optimal policy. In addition, the true value  X  true can be directly calculated using Equation (8) once the full experiment dataset is available. The optimal policy corresponding to  X  true is referred to as  X  true . From the literature we adapt the regret metric as a measure of
The regret represents the loss in the value of objective function (10a) due to using the estimated value  X  predict stead of the true value  X  true . In other words, the regret shows how good the solution of the optimisation problem (10) with the predicted parameters is in comparison with the solution of the problem with ground-truth parameters. Therefore if the parameters are predicted ideally, the cor-responding solution has zero regret. In order to emphasise the relative importance of queries, we weight each query by the number of impressions and report the average values for each experiment.

In order to measure the sensitivity of an interleaving method, we leverage two approaches. Firstly, similar to the previous work of Radlinski and Craswell [13], and Chapelle et al. [3], we study the probabilities of obtaining the correct experi-ment outcome after bootstrapping k impressions from the existing experimental dataset. This quantity can also be considered as the amount of user impressions an algorithm has to observe to reach a fixed p-value. We report the re-sults of these evaluations graphically with a pre-defined set of k . The number of bootstrap samples is set to 10,000.
Another measure used to estimate the interleaving sensi-tivity is z-score , as used in [3]. Denoting by c ( v j ) the aggre-gated credit assigned as a result of the impression v j , we de-fine a sample mean  X   X  as  X   X  = 1 N P j c ( v j ). Furthermore, as  X   X  is approximately normally distributed for large numbers of impressions N , we can calculate a method X  X  confidence by z-score as follows: where  X  c is the standard deviation of c ( v j ). As noted by Chapelle et al. [3], if an interleaving algorithm has a z-score m times higher than another, which indicates that the latter needs m 2 more data to achieve the same confidence. Thus, the confidence of the algorithm in the experiment outcome is directly connected to its sensitivity: an algorithm with higher confidence is more sensitive.
As we aim to investigate how the required number of user sessions to achieve a particular level of confidence changes for the considered interleaving algorithms, the evaluation must be performed on the impression level. For this reason, the query-level methodology used by Radlinski and Craswell [13] to measure sensitivity is not directly applicable in our work.

Since our experiments leverage datasets that have been obtained from the Team Draft-based experiments, the eval-uation of other algorithms is a challenging task. Indeed, we
Input : Dataset of user impressions D , sampled under
Output : Probability of error P ( e | S ) // calculating importance weights for j = 1 .. | D | do end // normalise so w is a distribution a  X  0 //agreement counter for k = 1 ..K do end P ( e | S ) = 1  X  a/K
Algorithm 3: Estimating the error probability P ( e | S ) given a sample of size S . need to estimate the sensitivity of an interleaving algorithm once it is deployed, based on the data obtained from running another algorithm. This task is related to the off-policy eval-uation and has previously been discussed in [8, 16]. Before discussing the method we used to approach this task, we in-troduce the required notation. Firstly, we assume that the available experiment dataset was generated from a source distribution P S from one interleaving method and we want to evaluate some statistic (e.g. sensitivity) of another inter-leaving method which, once deployed, would generate data under the target distribution, P T . In all our experiments the source distribution P S corresponds to the distribution of the result pages, generated by the Team Draft algorithm.
In order to estimate the expectation of a statistic f ( v ) of the dataset D of impressions v j , the importance sampling estimator can be used [8, 16]:
The core of the importance sampling estimator is to re-weight samples obtained under the source distribution, so that the expectation equates to the one of the target distri-bution. Since the probability of submitting a query by the users is independent of the interleaving algorithm deployed, only the probability of finding a particular combination of an interleaved results page and credit function in a query stream depends on the interleaving algorithm. Further, we assume that we are comparing two algorithms with the same space of generated interleaved result lists L . Let us denote the query submitted in impression v j as q j , and the result list demonstrated by L j . Then the experiment outcome  X  can be found using the following expression: where  X  T,j and  X  S,j denote the probabilities of showing re-sult list L j according to the policy of the evaluated (tar-get) and available (source) algorithms 4 . We apply the same
It is also required for  X  S,j to be positive for all L j occurring in the dataset, however this holds for the Team Draft policy. Table 2: Learning the optimisation problem param-eters regret. TD denotes Team Draft, the solution of (10) with the parameter  X  predicted by Algorithm 2 is denoted by UB.  X  is set to 0 . 5 , which is selected according to Table 3.
 TD,  X  10  X  2 1.80 1.23 1.20 0.88 0.48 3.36
UB,  X  10  X  2 1.53 1.12 1.13 0.82 0.45 3.23 approach to estimate the variance of c ( v j ),  X  T . In turn, both  X   X  T and the variance  X  T are used to obtain the impor-tance sampling estimate of the algorithm X  X  z-score metric, discussed in Section 6.1.

Another approach to measure an interleaving algorithm X  X  sensitivity that was discussed in Section 6.1 is to study the number of impressions required for the algorithm to define an experiment outcome with a particular certainty. Usu-ally, bootstrapping is used to estimate the probability of obtaining the correct (ground-truth) experiment outcome provided a fixed number of user impressions from the ex-periment [13, 19]. However, the direct approach is not ap-plicable in our case, since only the dataset generated under the Team Draft algorithm is available. In order to obtain these estimates, we perform a modified bootstrap sampling by means of Algorithm 3. Similarly to the importance sam-pling, Algorithm 3 re-weights the user impressions from the dataset. However these weights are further used as probabil-ities to sample from the dataset (after normalisation). This algorithm is related to the sampling-importance-resampling (SIR) [2] algorithm to obtain i.i.d. samples from the target distribution P T once a dataset under source distribution P is provided. The algorithm is intuitive: if a point v i in the dataset with probability P S ( v i ), after sampling with probability proportional to P T ( v i ) P the sample with probability P T ( v i ).
We believe it is interesting to compare our approach to the Optimised Interleaving approach proposed by Radlinski and Craswell [13]. However this is not feasible in our exper-imental setting. In contrast to [13], we base our evaluation on real-life Team Draft experiments and, as a result, the set of possible result lists L we can consider coincides with the one of Team Draft. Unfortunately, we found that for a considerable part of the dataset the optimisation problem (10) is infeasible if the result lists generated by Team Draft are combined with the credit assignment schemes from Op-timised Interleaving. 5 Similarly, the availability of the user interaction data only for Team Draft-based result lists makes it impossible to compare the proposed approach with Prob-abilistic Interleaving method [9] directly. For these reasons, we consider Team Draft as the baseline in our experiments.
Our proposed approach to improve the interleaving sen-sitivity can be used with various credit assignment and ag-gregation schemes. Note that our approach to increase the
For instance, A = { d 1 ,d 2 ,...,d 9 ,d 10 } and B = { d d 10 ,d 11 } . Then there are two possible interleaved result lists with the following inverse rank credit functions [13]:  X  1 (0 ,..., 0 , 1 9 , 1 10  X  1 9 ) and  X  2 = (0 ,..., 0 ,  X  1 credit assignment functions Eq. (10b) becomes infeasible. sensitivity of the algorithms and approaches relying on the modification of the credit function are not mutually exclu-sive, but, actually, complimentary to each other. In our experimental study, we investigate the sensitivity while ap-plying two credit aggregation schemes [3], namely: deduped binary and deduped click . Both credit aggregation schemes are closely related to the algorithm of building an inter-leaved result list by Team Draft. Firstly, if the top  X  r results for both A and B are equal, then  X  b ( r ) equates to zero for all positions above  X  r :  X  r = max r { X  j  X  rA ( j ) = B ( j ) } X  X  X  r  X   X  r  X  b ( r ) = 0 (18)
For the positions below  X  r , the credit for a click is assigned to the team ( A or B ) the result belongs to [14].

However, these schemes differ in the way credits are ag-gregated. In the case of the deduped binary scheme, for each impression v j a single winner is selected and its score is incremented: In contrast, in the deduped click scheme, the full credit is contributed:
We expect that advanced machine-learned click weighting functions, as introduced by Yue et al. [19], can be combined with the proposed approach and further benefit the inter-leaving sensitivity. However, as our goal is to demonstrate the utility of interleaving sensitivity optimisation by adjust-ing the experiment policy, we leave advanced click weighting functions for future work.
To examine the quality of the prediction of the optimi-sation problem parameters, in Table 2 we report the regret values for the considered algorithms. It can be seen that the optimisation with predicted values of  X  indeed results into lower regret (i.e. higher values of the non-regularised objec-tive function (10a)), in comparison with non-optimised uni-form solution of the problem (10), which is represented by Team Draft. We conclude that the learning of parameters  X  by Algorithm 2 succeeds in providing reliable estimates of the user behaviour. Thus we can expect that the interleaving sensitivity might be improved with the proposed approach in comparison with Team Draft.

In order to test this expectation, we perform the sensi-tivity analysis and report the results in Table 3. We report the relative z-scores of Team Draft and the proposed al-gorithms for two credit aggregation schemes: deduped click and deduped binary. The z-scores are normalised so that the z-score of Team Draft with the deduped click credit function equals 1 for each experiment (each row). This allows an in-tuitive interpretation of the results: e.g. in E 3 our proposed algorithm with  X  = 0 with the deduped binary click aggre-gation has a relative z-score of 2 . 02, thus Team Draft with deduped click credit needs 2 . 02 2 = 4 . 08 times more data to become that confident (Section 6.2). In order to get addi-tional insights in the performance of our proposed algorithm, we vary  X  in the pre-defined set  X   X  { 0 , 2  X  5 , 2  X  4 where  X  = 0 corresponds to the case with no regularisation, i.e. to the objective function (10a). Team Draft corresponds to  X   X  +  X  . The z-scores are estimated by means of im-portance sampling, as discussed in Section 6.2.

In general, higher values of  X  indeed force the solution of (10) to be similar to Team Draft: the values in the row experiments E 1 ...E 6 are reported. TD stands for Team Draft.
Ex p Deduped click credit aggregation Deduped binary credit aggregation corresponding to  X  = 2 and deduped click credit aggrega-tion scheme are close to 1. Similarly, in the case of deduped binary credit aggregation the rows corresponding to  X  = 2 and to Team Draft have close values. On the contrary, low values of  X  allow the optimised policy to diverge from Team Draft, so that the interleaving becomes  X  X isky X  and relies more on the noisy predictions of the user behaviour. As a result, in three experiments ( E 3 ,E 4 ,E 5 ), the best result is achieved with  X  = 0 and in two experiments ( E the best performance is achieved with  X  = 1 / 16. More-over, with  X  = 0 the maximal median improvements of 1 . 39 and 1 . 48 are achieved for the deduped click and deduped binary aggregation schemes, respectively. These values cor-respond to approximately 48% reduction in the number of impressions required in comparison with Team Draft with the same credit aggregation scheme, to achieve the same level of confidence. However, there are downsides for behav-ing too risky: the sensitivity in experiment E 2 falls to 0 . 70 with  X  = 0 and a deduped click credit aggregation. In turn, for values of  X  above 1 / 2, the sensitivity of the proposed al-gorithm outperforms that of Team Draft under both credit aggregation schemes in E 2 . This supports the role of  X  as a parameter that trades off the level of risk (i.e. variance) due to noisy user feedback and the level of possible improve-ment. Moreover, these observations also suggest that with a proper tuning of  X  , it is possible to achieve an appropriate trade-off between the median improvement and the largest descrease in the interleaving sensitivity. On analysing Table 3, we note that the median sensitivity values of the proposed approach are above 1 for the deduped click credit aggrega-tion and above the results of Team Draft for the deduped binary credit aggregation, demonstrating the advantage of our approach.

Another observation that can be made from Table 3 is that in five out of six experiments, Team Draft with the deduped binary click aggregation has relative scores that are not less than one. This indicates that Team Draft with the deduped binary credit aggregation has higher sensitivity than with the deduped click aggregation. This fact is inline with the results obtained by Chapelle et al. [3]. A possible explanation is that aggregating credit on the impression level reduces noise and thus increases the sensitivity. The same observation holds for our proposed algorithm: for all val-ues of  X  and all of the experiments considered, the deduped binary scheme outperforms the deduped click aggregation, except for experiment E 2 and  X  = 1 / 32. This observation also supports our decision to rely on the non-binary click aggregation scheme while deriving the optimisation prob-lem (10) in Section 3, since the obtained solution can still be improved by the impression-level aggregation of credits.
A visual representation of these results is presented in Fig-ure 1. The plots correspond to experiments from E 1 to E and each plot represents the probability of obtaining an in-correct experiment outcome after considering a particular number of impressions, as calculated by Algorithm 3. We present results that correspond to Team Draft (i.e.  X   X  X  X  ) and the solution of the optimisation problem (10) with  X   X  { 0 . 0 , 0 . 5 } 6 and the deduped credit aggregation scheme used. We notice that the results are consistent with those in Ta-ble 3: the proposed algorithm outperforms Team Draft in most the of experiments, and by varying  X  it is possible to control the optimisation risk: with  X  = 0 the proposed algo-rithm demonstrates high sensitivity gains in E 3 ,E 4 ,E 5 but underperforms in E 2 with respect to Team Draft. On the contrary, with  X  = 0 . 5 the maximum loss is almost neg-ligible ( E 1 ) while there are still significant improvements in E ,E 4 ,E 5 ,E 6 and a slight improvement in E 2 .

Overall, the results obtained suggest that the historical user behaviour information can be used to improve the sen-sitivity of the interleaving algorithms. The proposed inter-leaving algorithm, which adjusts the interleaving policy ac-cording to the solution of the optimisation problem (10), has generally a higher sensitivity than Team Draft, reaching up to a median of 48% decrease in the required number of im-pressions to achieve the same level of confidence. Moreover, the algorithm X  X  regularisation parameter  X  can be used to select the appropriate variance in the sensitivity gains.
In this paper, we studied the problem of improving the interleaving sensitivity. We proposed a novel theoretically-motivated approach to optimise the interleaving experiment parameters using the historical user behaviour data rou-tinely collected by search engines. Informally, the approach aims to predict what interleaved result pages are likely to contribute little to the experiment outcome and, after that, show them to the users as rarely as possible. In order to achieve this goal, a click model is trained on the historical click data. In turn, this model is used to predict future user behaviour on the interleaved result pages (that were possibly never shown to the users before). Next, we proposed to ad-
Since  X  = 0 . 5 is an appropriate trade-off among the values considered in Table 3 and  X  = 0 is an interesting case of the non-regularised version of the optimisation problem (10). respectively. The deduped binary credit aggregation scheme is considered. error probability error probability just the probabilities of showing interleaved result pages so that a predicted upper bound on the experiment outcome is maximised. Further, since the predicted user behaviour can be noisy, we introduced a regularisation trade-off parameter which can be used to adjust the level of risk.

In order to test the proposed approach empirically, we performed an offline empirical study, which leverages data from the Team Draft interleaving experiments previously performed by a commercial search engine. In our study, we used the data obtained over six interleaving experiments, representing 555K user impressions in total. We investigated the sensitivity of the proposed approach with the previously proposed deduped click and deduped binary credit aggrega-tion schemes. Our results suggest that with an appropriate tuning of the trade-off parameter, the proposed approach outperforms the Team Draft algorithm in ensuring the cor-rect experiment outcome with up to the median of 48% less impressions.

An interesting direction for future work is to study a per-query strategy for selecting the risk parameter  X  . In-deed, the optimisation can be performed more aggressively for queries with a sufficient amount of historical click data available. Finally, we believe it is promising to further inves-tigate other combinations of the credit assignment schemes and the proposed approach.
