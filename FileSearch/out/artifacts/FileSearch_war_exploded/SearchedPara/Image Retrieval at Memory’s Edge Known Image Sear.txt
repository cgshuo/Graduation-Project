 With the increasingly growing size of digital image collec-tions, known image search is gaining more and more impor-tance. Especially in collections where individual objects are not tagged with metadata describing their content, content-based image retrieval (CBIR) is a promising approach, but usually suffers from the unavailability of query images that are good enough to express the user X  X  information need. In this paper, we present the QbS system that provides CBIR based on user-drawn sketches. The QbS system combines angular radial partitioning for the extraction of features in the user-provided sketch, taking into account the spatial dis-tribution of edges, and the image distortion model. This combination offers several highly relevant invariances that allow the query sketch to slightly deviate from the searched image in terms of rotation, translation, relative size, and/or unknown objects in the background. To illustrate the bene-fits of the approach, we present search results from the evalu-ation of the QbS system on the basis of the MIRFLICKR col-lection with 25,000 objects and compare the retrieval results of pure metadata-driven approaches, pure content-based re-trieval using different sketches, and combinations thereof. H.3.3 [ Information Storage And Retrieval ]: Informa-tion Search and Retrieval X  query formulation, retrieval mod-els ; H.5.2 [ Information Interfaces and Presentation ]: User Interfaces X  input devices and strategies Design, Human Factors
Searching for information can be categorized in two fun-damentally different classes with respect to the knowledge a the user has about the information item(s) and also in the condition under which the search task can end successfully.
Searching for known items: a user knows that the items exist and has probably seen them before; the search task will end successfully only if the user has found all these items.
Searching for (potentially) novel items: a user is looking for items that satisfy her information need; the search task ends successfully as soon as the user is provided with some items that reflect the information she is looking for.
Depending on the interaction intentions [7] associated with these classes, different information seeking strategies have to be applied. The probably most effective strategy for finding a known image in an image collection is to remem-ber an access path to it. This of course only works for static collections in which the access path is kept stable over time.
A shift in information seeking strategies [7] to the strat-egy of browsing can be applied for known item search, but it is certainly not as effective and may also not lead to a successful result if it is not known in advance whether or not the collection contains the searched item.

For finding both, known images and novel images, content-based image retrieval (CBIR) provides powerful tools. How-ever, CBIR requires a query image to start with that is suf-ficiently close to the final result, i.e., that precisely expresses the user X  X  information need. Without such a query image, it is difficult or nearly impossible to achieve good retrieval quality, even if sophisticated relevance feedback mechanisms are available. Query by Sketching addresses this problem and takes user generated sketches as query images. So far, two main problems have significantly impacted the success-ful application of query by sketching to known item search and novel item search. First, the mouse as most widely available input device limits the user-friendliness and ex-pressiveness for drawing sketches. Second, users usually do not sketch complete images but concentrate on the parts which are most interesting for them. Thus, when compar-ing user-drawn sketches with images, there will usually be parts of the images to be queried that do not have any corre-sponding part in the sketch as the user may not remember or not be able to draw all details of image in the sketch. The user may also place the sketch not exactly at the right position, with proper scale, and/or orientation. Therefore, the corresponding parts may not be at the same coordinates in sketch and image. In order to successfully apply query by sketching to CBIR, both problems need to be solved jointly.
In this paper, we present the QbS approach to query by sketching that exploits novel user interface technologies and supports various invariances in the comparison of a user-generated sketch and the images to make the query process robust against translations, rotations, and different scales. Figure 1: Tablet PC running the QbS prototype.
 We have applied the results of our work to use cases from known image search and provide detailed evaluation results of the QbS system based on the MIRFLICKR-25000 image collection [3]. A more detailed description of the system and the evaluation can be found in [5].

The paper is organized as follows: Section 2 discusses re-lated work. In Section 3, we introduce the QbS approach and its implementation. Section 4 summarizes the results of the experimental evaluation. Section 5 concludes.
As the absence of a good example to start a query has been a problem from the very start of CBIR, query by sketching has always been of interest in this domain. However, it has not been as successful as other approaches to CBIR, mainly due to unavailability of appropriate input devices. A more detailed discussion of existing approaches is presented in [5].
Our approach is based on Angular Radial Partitioning (ARP) proposed in [1] which uses spatial distribution of edges. We extended ARP to support additional invariances.
The image distortion model has been used for optical char-acter recognition and automated classification of medical images [4] for which it uses images in reduced resolution. Deformations are allowed within the warp range for indi-vidual pixels and the area around them (local context). For our approach, we added IDM as a distance measure which is able to measure much finer deviations than ARP. However, we replaced the Sobel filter for edge detection with the same variant of the Canny edge detector used for ARP.
QbS provides an application that can be used in an inter-active setting to retrieve known images. The prototype [6] runs on a Tablet PC as shown in Figure 1, thus allowing the user to draw edges with a stylus directly on the screen.
The QbS system provides a fast query mechanism to re-trieve some of the top-ranked results to give the user a fast feedback about the retrieval result that can be expected from the final or full result. Then, the user is able to add further details to refine the search and remove misleading parts of the sketch without having to redo the complete sketch. (a) original image (b)  X  = 2 (c)  X  = 20 Figure 2: An example image (a), edge map for var-ious value of  X  (b-d), partitioning at  X  = 10 with 8 angular and 4 radial partitions (e) and 16 angular and 8 radial partitions (f ).

To generate edge maps [1] from the images, a variant of the Canny edge detector is applied. It takes a single thresh-old,  X  , to control how many details are preserved. As a single value of  X  might not be sufficient to cope with differ-ent lighting conditions or depth of field of different images in a collection as well as the level of detail in the user sketch, we extract edge maps from the images at several values between 2 and 50 as shown in Figure 2.

We incorporate two different sets of features and corre-sponding distance measures: Angular Radial Partitioning (ARP) [1] is used as a compact, fast way to retrieve im-ages, when rough sketches and spatial layout is sufficient to separate the desired known item from all other images in the collection. An adapted version of the Image Distor-tion Model (IDM) [4] on the edge maps is used as a more complex, computationally more expensive solution whenever the user needs more thorough comparison between a good-enough sketch and the other images. In addition, the user can in both cases restrict the comparison to images selected on their metadata, e.g., only consider images with certain tags which have been indexed using Apache Lucene.
We support ARP at various resolutions. By default ARP is used with 8 angular and 4 radial partitions (as depicted in Figure 2(e)) in which the number of edge pixels inside sketch and edge map are counted, which will generate a 32 dimensional feature vector. The user is given the possibility to distinguish areas that shall remain free of edges or ar-eas where the content is unknown (e.g., the user does not remember the content). Internally this is handled by com-paring the vectors using a weighted Manhattan distance: For areas which should get treated as  X  X nknown X , a weight of zero is assigned; all non-empty partitions have a weight of one  X  as well as areas which are intentionally left empty.
ARP can handle small misplacements and rotation, as long as not many edge pixels are moved from one partition to another. If the user expects even more rotation, invari-ance to rotation is provided by applying the 1D FFT on the features as proposed in [2].

For translations that are further off the position of the sketch, a heuristic can be applied: The original image is cut into several regions that still cover enough pixels to give meaningful search results. In addition to the image as a whole and a bounding box covering all the area with edge pixels, we also extract slightly smaller regions shifted to-wards each of the image corners. During search, the sketch will get compared with all regions, thus compensating to a certain extent for translation. As most of these regions are smaller than the full image, comparing the sketch or parts of the sketch can also provide some invariance to scale. Two additional regions in the center, one horizontally with a wide aspect ratio and one vertically with a narrow aspect ratio, can assist when the scaling does not maintain proportions.
Features of these multiple sub-images with and without 1D FFT are stored separately for each chosen  X  value. For simple, regular searches the query feature vector from the user X  X  sketch will be compared to exactly one feature vector for each image in the collection. Invariant searches will com-pare the distances with all corresponding representations, but only select the best-matching version of the image fea-tures for an individual image to compute its distance.
For IDM, the edge map image is scaled down to smaller sizes. As default size, we use 32 pixels for the longer side. As the user is expected to draw the edges in the sketch, edge detection is never applied to the sketch. This means that the scaling has to be performed after the edge detection  X  and it is preferable to use an interpolation for scaling down and to apply thresholding afterwards to return to a binary image (edge / non-edge) rather than not using interpolation in scaling, which may drop a lot of the edges.

The warp range can be defined at query time. Therefore the user can specify how many pixels (in the reduced resolu-tion) the edges are allowed to be misplaced by translation, scaling or rotation. Big warp ranges thus result in higher invariance. The size of the local context defines whether individual pixels (local context of 0) or patches (local con-text &gt; 0) are matched. Big local contexts result in small in-variance as bigger patches must fit to achieve low distances.
Invariance w.r.t. the value  X  used in extraction is achieved with the same strategy as for ARP. Again, unknown areas can be handled by ignoring non-edge pixels in the down-scaled sketch. They only affect the distance score as part of the local context of an edge pixel.
We use the MIRFLICKR-25000 dataset [3] for our evalua-tions, a collection with 25,000 images that were downloaded from flickr.com including the tags assigned to the images.
We have performed known item search on 4 different im-ages that have different characteristics with respect to diffi-culties in sketching of them:
Figure 3(a) shows a single object located in the center of the image with rather homogeneous background. Therefore at a value of  X  = 7, none of the clouds contribute to edges anymore while almost all aircraft details are still available and users can follow the common intuition to draw just the desired object directly in the image center. Moreover, the bounding box can compensate for translation and scaling.
Figure 3(e) is more challenging as it contains several dis-tinct objects (e.g., houses, mountains). There are plain or im1660.jpg by Alex Layzell, License: CC-BY-NC-ND im10853.jpg by Milachich, License: CC-BY-NC im18707.jpg by Petteri Sulonen, License: CC-BY im18797.jpg by Gideon, License: CC-BY
Figure 3: Sample sketches used for evaluation. homogenous areas like the sea or the sky as well as areas with strong contrasts like the rocks and vegetation on the mountains. The spatial distribution may lead some users to place the coastline too high/low and not to match the aspect ratio and relative sizes of objects.

Figure 3(i) contains again one main object, but this time, it is difficult to separate it from the image background. As can be seen in Figure 2, there is no value of  X  at which the details of the bike would still be preserved while the floor and walls would already disappear. The image perspective is also non-trivial as there is no planar view on the bike.
Finally, Figure 3(m) contains several prominent objects (hand, figure of the Eiffel tower) which are neither placed directly at any image border nor directly in the center of the image. No major visible lines are either horizontal or vertical, but slightly rotated. Significant parts of the image are blurred, but colors still differ too much to generate areas which would appear to edge detection as homogenous areas.
For the evaluation, we have collected at least 10 different sketches for each of the 4 images from a group of 7 peo-ple (54 sketches in total; some examples shown in Figure 3). The individuals were given time to familiarize with QbS sys-tem running on a Tablet PC as shown in Figure 1 and were then requested to search for the known items which were shown to them in printed form. For each image, the par-ticipants were also shown the tags associated on flickr and
Figure 4: Ranks of known items (text filter off ) asked, which they would use to search; the two tags selected most frequently were used in the evaluation. We measured the individual performance of ARP and IDM. Figure 4 shows the ranks split up by known item. The ends of the bar show the rank achieved by the best and the worst sketch for this image; the thick grey bar starts at the first quartile and ends at the third quartile and indi-cates therefore the range of ranks which half of all sketches achieved; the blue diamond indicates the mean or  X  X n other words X  the average rank of all sketches for this image. No-tice that the rank axis is in logarithmic scale.

Only a single sketch had a rank slightly worse than 1000 while the vast majority of sketches achieved a rank well be-low 100, which could already be a number of results a user might be willing to browse. For searching with IDM even the majority is below 10, a number of results that can easily fit on a single result screen and out of which the user would recognize the image almost instantaneously.

For all searches, the choice of the appropriate  X  value de-pends on both the known item and the user sketch. There-fore it is not possible to come up with a single value of  X  for a diverse collection like the MIRFLICKR benchmark that would suit all users. However, QbS defines a set of values with reasonable limits and lets the user not only pick a sin-gle value, but also a range from this set that she can refine and shift during the search process.

In Table 1 we compare the average ranks achieved by pure content-based retrieval using ARP and IDM with the ranks when combined with keyword search and also with the rank where Lucene would return the item when only the key-word search is used. The column  X  X earch Space X  contains the number of objects that pass the filter (in case of plain CBIR: all images of the collection; in case of a text filter, the number of hits). Notice that 8.51% (2,128 out of the 25,000 images) in the dataset do not have any tags associated to them, so using text-search alone or relying entirely on key-words to be available would not give satisfactory results to the users. More details including the execution times for searches can be found in [5].
We have presented the QbS approach that assists users in searching for known images on the basis of user-drawn sketches, which includes several invariances that allow a user to find the desired result despite of missing background in-formation, misplacements, and/or differences in scale or ro-tation. Furthermore, the QbS system has been evaluated on the MIRFLICKR-25000 benchmark collection and the re-sults show that query by sketching can in fact help the user to find known images.

We are currently extending the QbS system to also incor-porate color information and exploit novel interface types (digital pens and interactive paper). This work was supported by the Swiss National Science Foundation ( PAD-IR , contract No. 200020 126829 / 1). [1] A. Chalechale, A. Mertins, and G. Naghdy. Edge Image [2] A. Chalechale, G. Naghdy, and A. Mertins.
 [3] M. J. Huiskes and M. S. Lew. The MIR Flickr Retrieval [4] D. Keysers, T. Deselaers, C. Gollan, and H. Ney. [5] M. Springmann, I. Al Kabary, and H. Schuldt.
 [6] M. Springmann, D. Kopp, and H. Schuldt. Qbs -[7] H. I. Xie. Planned and Situated Aspects in Interactive
