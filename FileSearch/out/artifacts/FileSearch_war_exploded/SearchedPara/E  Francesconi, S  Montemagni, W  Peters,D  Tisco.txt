 Stephan Walter 1 The book The volume Semantic Processing of Legal Texts contains a total of thirteen papers that share the common theme of processing legal documents. One undisputable merit of the book is that of being the first collection to focus specifically on computational linguistic aspects of this task. Otherwise the papers in the collection represent a variety of topics as distinct as ontology engineering, multi-label classification, and translation quality assurance. They deal with theoretical foundations as well as commercial applications, and the authors X  affiliations range from universities to industry. The book is based on selected papers presented at the first workshop on Semantic Processing of Legal Texts (held at LREC 2008 in Marrakech) but comprises further, invited contributions. 2 Some thoughts on semantic processing of legal texts The application of state of the art IT, in particular artificial intelligence, for tasks in the legal domain has always received some interest in the legal as well as computer science research communities. Only within the last 10 or 15 years, however, legal information systems for various purposes have become widespread. There is now an increasing demand for advanced technological support in accessing and processing legal information within such systems. This demand has led to a number of research initiatives and projects, and in many of them it has been realized that language processing is a necessary ingredient when it comes to acquiring, interpreting and sorting out the information contained in legal documents.

Nevertheless the legal domain has received comparably little attention as a field of applied research in the computational linguistic community. Generally speaking no consensus has evolved so far on architectures, best practices or even the most important topics that need to be addressed to make effective linguistic tools available for legal information processing. This contrasts with the general situation in areas such as information extraction, where there is substantial common ground with regard to overall methods as well as the contributions to be made by language technology.

Some important reasons for this difference relate to the role that language plays in the legal domain. Firstly, texts within a legal system do not simply serve as a means  X  X o bring a message across X . Norms and regulations are strictly bound to their textual form, stronger than e.g. the story reported in a newspaper article is linked to that article X  X  text. Wording is not just a matter of style. For instance the choice between common language synonyms may have significant legal implications. This means that domain (i.e. legal) expertise is required for legal text processing, arguably a lot more than in many other fields (newspapers, biomedical text), making it a highly interdisciplinary research topic.

Secondly, the various national legal systems together with the different languages used by them impose a burden on the transfer of research results. Carrying over an approach to a different national setting (let alone re-using an implemented system) may require more than exchanging grammar rules or re-training of some components. Often, the issue of conceptual differences between the respective legal systems will have to be addressed, too.

Thirdly, legal language is almost invariably (without denying a difference in degree between national contexts) characterised by high syntactic complexity and a large number of idiosyncrasies. This makes the legal domain a quite special use case for language technology. It is rarely ever possible to process legal text with out-of-the-box linguistic tools at the same quality as many other text sorts. 3 Papers in the volume While this explains the somewhat  X  X re-paradigmatic X  state of research in the field, it also means that there is still a lot to be discovered. This quite nicely sums up the overall impression one gets from reading the book at hand: It presents an impressing width of perspective and innovative approaches. However one is also left with a sense of missing convergence and, at least in some of the papers, clear focus.
The book organizes its variety of topics into three thematic sections that center on different tasks: Information extraction, the construction of knowledge resources (a.k.a. ontologies), and finally, semantic indexing, summarization and translation. We will follow this structure and shortly summarize each of the papers in turn. Legal Language and Legal Knowledge Management Applications by Giulia Venturi introduces the first part of the book. She presents an investigation into the specific linguistic properties of legal texts. For this purpose she compares corpora of Italian and English legal text to reference corpora of  X  X rdinary language X  text in both languages, as well as to each other. She assesses lexical and some syntactic properties. To make the latter possible, all corpora are processed using shallow linguistic tool chains (chunking). Her results confirm a number of observations about legal language that are frequently reported but have rarely been studied systematically (e.g. a tendency towards nominal expressions and long prepositional chains). Interestingly, she reports similar findings for Italian as well as for English. The paper does not go very far though in interpreting the empirical results or drawing any particular conclusions from them. In this respect it probably has to be regarded as a report on work in progress. A methodological weakness that still needs to be addressed lies in the fact that parts of the legal corpora consist of European legislation, meaning that these texts may have been authored originally in only one EU official language (frequently this will have been English or French) and then translated into all others. Such a genesis may of course have far-reaching stylistic implications.

Named Entity Recognition and Resolution in Legal Text (Dozier et al.) discusses the use of named entity recognition techniques to extract key information (e.g. judges, courts, companies) from a database of legal cases. They use rule-and pattern-based approaches for document analysis and candidate recognition. Named entity candidates are then mapped to known entities in a knowledge base using a support vector machine (SVM)-classifier. The paper is of specific interest because it describes a system that is running as part of a product at Thomson Reuters. It gives quite some technical detail on the architecture and implementation of that system. Sadly however, this description as well as the evaluation of the system is somewhat intransparent.
 Quaresma and Gonc  X alves ( Using Linguistic Information and Machine Leaning Techniques to Identify Entities from Juridical Documents ) perform experiments on document classification. Additionally they also look at a named entity extraction. For document classification they use an SVM classifier to allocate European legislative texts (taken from the EUR-Lex-Website) 1 to top level categories from the EU X  X  multilingual thesaurus Eurovoc. 2 Named entity recognition is based on the output of a parser that assigns semantic tags to parts of the parse tree. These tags are translated directly into named entity categories.

The results in the document classification task seem promising but are hardly analysed or discussed in the paper. The quality of named entity recognition is high for generic types (dates, locations) and drops for more domain-specific ones (companies, references to other legal texts). This is not surprising since no attempts were made (or at least none are mentioned) to adapt the parser or its semantic tag set to the requirements of legal text. Together with the fact that simple bags-of-words were used as text representations for document classification, this leads to the conclusion that the results described in the paper should be taken as establishing a baseline indicating the performance of standard approaches applied to the legal domain without any specific adaptations.

Wyner et al. ( Approaches to Text Mining Arguments from Legal Cases ) aim at a much higher level of textual information. They discuss how to extract argumentative structures from legal cases, featuring a largely rule-based approach. They first survey the building blocks needed for this task (categories from argumentation theory, annotated corpora of arguments, a specialized grammar of argument structures). They then turn to some examples and show how these building blocks can be put together within a text mining framework. It would be interesting to see such a system evaluated on a larger scale, not just in view of standard accuracy measures but also from a user-oriented perspective. However, Wyner et al. are certainly right in pointing out that their approach will be confronted with a serious knowledge acquisition bottleneck when scaling up.

The second part of the book starts with an investigation into a prerequisite of text-based knowledge acquisition, namely term detection. Pala et al. look at the Automatic Identification of Legal Terms in Czech Law Texts . They pre-process a corpus of 50 000 documents using a morphological analyser and a chunk parser. They use the provided information to identify noun groups, which X  X o they state X  are mostly legal terms. They then turn to verbs, for which they perform a frequency analysis and try to identify a mapping between generic roles (e.g. agent) and the specific roles for the corresponding arguments in legal language (e.g. malefactor). The results are not evaluated quantitatively and no other tangible appraisal of their quality is presented. This is understandable as the authors clearly state that their work is a pilot study. Nevertheless the reader is left a little puzzled what to make of their findings.
 Integrating a Bottom -Up and Top -Down Methodology for Building Semantic Resources for the Multilingual Legal Domain (Francesconi et al.) is a (partly methodological) paper that discusses the potential of combining structured knowledge resources and text-based acquisition in the legal domain. They start with a discussion of the theoretical foundations of legal knowledge engineering and then turn to case studies where they combine data-driven modules (term extraction, statistical text analysis) with a top level legal ontology: Ontology refinement and legal rules learning and classification. Their study is based on work in the EU-funded DALOS project 3 and is therefore performed in a framework that is at a mature state. Their evaluation results are promising. However it remains a little unclear how much manual work was involved in the cases studies (including the preparation of the knowledge resources that were used), and how much effort the authors would expect for porting their methodology to other legal domains or further languages.

In Ontology Based Law Discovery , Bosca and Dini present a study on the use of ontology learning techniques (statistical identification and clustering of domain-relevant nouns and relation extraction) based on a corpus of web texts and messages containing reports on a specific law. They follow the interesting idea of using the extracted knowledge to identify prevailing attitudes towards that law. They do not give any quantitative performance measures, but nonetheless their approach seems promising. It would have been interesting to read some thoughts about possible applications of such  X  X egal opinion mining X .
Multilevel Legal Ontologies by Ajani et al. discusses conceptual issues that arise in the context of the implementation of European Union Directives in national law and how they were approached in the ontological framework of their Legal Taxonomy Syllabus system. A common EU level ontology structure is connected to and extended in national level ontologies. These represent the various national implementations of directives, and need not be congruent with each other. Each national ontology may  X  X verload X  European concepts with national ones. In such cases, both versions remain available for reference in texts. Within this framework Ajani et al. accommodate interpretative knowledge that is created in texts, as well as changes of the conceptual system over time. They introduce dedicated  X  X nterpre-tation X  and  X  X eplacement X  relations to represent these dynamics within the ontology itself. While the topic of the paper is clearly of great practical relevance and the authors refer to an implemented system, it remains unclear to the reader to what extent the described ideas have been validated through actual usage of that system. A position (or rather  X  X ision X )-paper by Schweighofer, Semantic Indexing of Legal Documents , introduces the third part of the book. He summarizes existing work on legal ontologies and semantic document processing. He then turns to the idea of a  X  X ynamic legal commentary X  as an integrated solution, based on a fusion of technologies and providing a maximum of added value to the practitioner on top of a legal database. He recognizes the knowledge acquisition problem that has to be solved for the creation of such a system, but he is optimistic that it will be overcome in the near future. Schweighofer is certainly right in fostering the convergence of research efforts by providing a user-focussed application scenario, and hopefully right in his optimism. His ideas however remain vague and X  X hich is really a pity for a contribution to a collection X  X e hardly relates them to the other papers in the volume.

De Maat and Winkels ( Automated Classification of Norms in Sources of Law ) address once more the topic of ontology learning from text. They start off with the presentation of a thorough and quite detailed model of legislative texts and the types of rules that they convey. They then pursue a rule-based approach: Norm types are mapped to typical realization patterns that are formalized as regular expressions. These are used to identify rules of the respective types in law texts. They report a very high accuracy (91%). However their evaluation methodology is not fully transparent. In particular they do not describe their gold standard sufficiently, and it is not clear to what extent its linguistic complexity can be regarded as representative. A logical next step, which is only mentioned in passing, would certainly be to try using machine learning techniques to automatically acquire the classifier used in the classification stage of their system.
 Mencia and Fu  X  rnkranz, in Efficient Multilabel Classification Algorithms for Large -Scale Problems in the Legal Domain (the by far most technical paper in the volume), look at an extended version of the document classification task investigated also by Quaresma and Gonc  X alves: The assignment of Eurovoc descriptors (in this case all of the approximately 4000 labels are used) to European legislative documents from the EUR-Lex repository. They compare different multi-label classification techniques and favour a pairwise perceptron approach, which trains separate classifiers for all label pairs. They devise a modified training algorithm that makes this approach feasible although a total of several million perceptrons are generated. They report promising results: Five relevant labels for each document are among the top-ranked ten on average. Moreover they identify document labelling in the EUR-Lex collection as a test bed of general interest, which, due to its size and number of labels, entails challenges not offered by other standard test cases for multi-label classification.
 An Automatic System for Summarization and Information Extraction of Legal Information (Chieze et al.) discusses a commercial information system which extracts factsheets and summaries from (French and English language) Canadian court decisions. The system extracts key facts using cue word-based rules that are compiled into finite automata, and comprises an additional statistical component to select sentences for extractive summaries. It also provides for a validation step to be performed by human reviewers. The paper presents the whole workflow involved in using and maintaining the system. It is good to see that such an integrated linguistics-aware system is used successfully on a day-to-day basis in a commercial setting. An interesting addition to this nice paper would have been further information on the effort that was involved in having the original (English-only immigration-law) system deal with French documents and further domains.
 Evaluation Metrics for Consistent Translation of Japanese Legal Sentences by Ogawa et al. closes the volume with a look at a less technology-oriented topic: The authors review and discuss quantitative translation quality measures used in the machine translation community (most prominently the BLEU metric). On these grounds they develop two metrics of their own that they use to assess the terminological consistency of translation of Japanese documents into English. They aim at establishing a quality control mechanism for newly translated documents, where no reference translation exists so far. Rather than standard machine translation evaluation measures, their metrics are designed to deliver meaningful results when comparing a translation to a number of  X  X seudo-reference X  translations of different (but still sufficiently similar) source sentences. 4 Summing up Our short summaries (and even more so our evaluations) may not have done full justice to the papers in many cases. However we hope that they have shown the impressive width of topics covered by the research going on  X  X  X here the language of law meets the law of language X  X  (as the subtitle of the book puts it). There are certain recurrent themes (such as document classification or ontology learning), but it seems that no substantial agreement has emerged so far on the pre-eminent research questions and in particular on evaluation methods.

Shared tasks and competitions such as TREC or, more recently, RTE (Recognizing Textual Entailment) have added significant momentum by focussing research efforts in comparable situations elsewhere. This approach might do less good in the case of legal language processing. A certain degree of incommensu-rability in research may be unavoidable here, in order to do justice to the diversity of languages and legal systems. Yet a common  X  X aradigm application X  (maybe not unlike the dynamic legal commentary proposed by Schweighofer in this volume) could serve a similar purpose in a somewhat gentler manner. Maybe the architecture of such a system could provide an organizing principle in a future workshop in the  X  X  X emantic Processing of Legal Text X  X  series.

