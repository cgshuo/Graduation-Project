 1 Introduction arm which seems to be the most promising.
 associated with some d -dimensional vector m unknown parameter vector  X  the reward conditionally to the action a is given by ( m  X  or moderate sample-sizes when these algorithms are tuned ba sed on finite-sample bounds. method. Section 5 reports the results of two experiments on r eal data sets. 2 Generalized Linear Bandits, Generalized Linear Models each time t , the agent chooses an arm A The prior knowledge available to the agent consists of a coll ection of vectors { m which are specific to each arm and a so-called (inverse) link function : R  X  R . the payoff R satisfies for some unknown parameter vector  X  which the rewards are associated with categorical variable s.
 with respect to a reference measure is given by matrix for the parameter  X  . The function b is thus strictly convex. of maximum likelihood estimator  X   X  the maximizer of the function a strictly concave function in  X  . 2 Upon differentiating, we obtain that  X   X  the following estimating equation using, for instance, Newton X  X  algorithm.

A semi-parametric version of the above model is obtained by a ssuming only that E design matrix P t  X  1 3 The GLM-UCB Algorithm of an optimal arm in order to maximize the received rewards. The greedy action argmax where  X  is an appropriate,  X  X lowly increasing X  function, is the design matrix corresponding to the first t  X  1 timesteps and k v k matrix norm induced by the positive semidefinite matrix M . The region k  X   X   X   X  a confidence ellipsoid around the estimated parameter  X   X  expected reward of each arm, thus choosing the action a that maximizes bandits, the second approach looks more appropriate.
 unique solution of the estimating equation where A Let g satisfies g we  X  X roject it X  to  X  , to obtain  X   X  Note that if  X   X  dealt with) then we can let  X   X  Algorithm 1 GLM-UCB 2: Play actions a 1 , . . . , a d , receive R 1 , . . . , R d . 3: for t &gt; d do 4: Estimate  X   X  t according to (6) 5: if  X   X  t  X   X  let  X   X  t =  X   X  t else compute  X   X  t according to (7) 6: Play the action A t = argmax 7: end for
At time t , for each arm a , an upper bound ( m  X  bonus X   X  a expected regret (for the actual form used, see (8)). Note tha t the leading term of  X  a
As we are mostly interested in the case when the number of arms K is much larger than the dimension d , the algorithm is simply initialized by playing actions a m follows we assume that the dimension of M is equal to d . Then, by playing a d steps the agent ensures that M M 3.1 Discussion interpretation of the role played by k m {
Then, M a  X  t =  X  ( t ) / for all a  X  A , where  X  R a within the confidence ellipsoid {  X  : k  X   X   X   X  as any arm a . Thus the exploration bonus  X  a orthogonal to m to not on K . 4 Theoretical analysis tuned based on asymptotic arguments. 4.1 Regret Bounds an optimal arm and the reward received following the algorit hm: k
For the logistic function k Assumption 2. The norm of covariates in { m that for all a  X  A , k m Finally, we make the following assumption on the rewards: Assumption 3. There exists R  X  the expected reward received playing an optimal arm and that of the best sub-optimal arm: Theorem 1 establishes a high probability bound on the regret underlying using GLM-UCB with where T is the fixed time horizon,  X  = p 3 + 2 log(1 + 2 c 2 eigenvalue of P d Theorem 1 (Problem Dependent Upper Bound) . Let s = max(1 , c 2 1 X 3, for all T  X  1 , the regret satisfies: P Regret T  X  ( d + 1) R max +
Note that the above regret bound depends on the true value of  X  theorem provides an upper-bound of the regret independentl y of the  X  Theorem 2 (Problem Independent Upper Bound) . Let s = max(1 , c 2 Assumptions 1 X 3, for all T  X  1 , the regret satisfies
P Regret T  X  ( d + 1) R max + Cd log [ s T ] the explicit form of the estimator given by (6) to show that 4.2 Asymptotic Upper Confidence Bound experiments presented in Section 5.
 this model is given by J = E [  X  ( X  X   X  t  X  1 M t a.s.  X  X  X   X  where  X  = E [ XX  X  ] . Hence, using the delta-method and Slutsky X  X  lemma The right-hand variance is smaller than k such that J and  X  are positive definite and sufficiently large t and small  X  , the usual UCB setting, is sufficient. This is the setting used in the simulations below. 5 Experiments two experiments using real world datasets. 5.1 Forest Cover Type Data of the 20 best arms draws using the UCB and GLM-UCB.
 estimated action, A of Figure 1 shows the number of times each of the 20 best arms have been played by the UCB actions, even for those that have never (or rarely) been play ed. 5.2 Internet Advertisement Data the entire data does not even correctly identify the best arm ). consider the pseudo-inverse of M use vectors of covariates in real-life applications. on the advertisement dataset. 6 Conclusions perform satisfactorily in practice.
 Acknowledgments References [13] P. McCullagh and J. A. Nelder. Generalized Linear Models . Chapman and Hall, 1989.
