 Crowdsourcing is a market of steadily-growing importance upon which both academia and industry increasingly rely. However, this market appears to be inherently infested with a significant share of malicious workers who try to maximise their profits through cheating or sloppiness. This serves to undermine the very merits crowdsourcing has come to repre-sent. Based on previous experience as well as psychological insights, we propose the use of a game in order to attract and retain a larger share of reliable workers to frequently-requested crowdsourcing tasks such as relevance assessments and clustering.
 In a large-scale comparative study conducted using recent TREC data, we investigate the performance of traditional HIT designs and a game-based alternative that is able to achieve high quality at significantly lower pay rates, facing fewer malicious submissions.
 H.1.2 [ Models and Principles ]: User / Machine Systems  X  Human Factors ; H.1.2 [ Models and Principles ]: User / Machine Systems  X  Human information processing ; H.5.2 [ Information Interfaces &amp; Presentation ]: User Inter-faces Crowdsourcing, Gamification, Serious Games, Relevance Assessments, Clustering
In the course of the past 5 years, crowdsourcing has ad-vanced from a niche phenomenon to becoming an accepted solution to a wide range of data acquisition challenges [10]. It has been used in the training and test phases of a great number of scientific projects and is firmly integrated into nu-merous evaluation and data acquisition schemes in academia and industry. One problem, however, seems to be inher-ent to the field; a significant share of the annotations cre-ated on crowdsourcing platforms are fraudsters X  attempts to cheat the HIT (Human Intelligence Task) provider into paying them without having properly worked on the HIT. As a consequence, almost every scientific publication that employs crowdsourcing for data acquisition details the au-thors X  tailor-made defense scheme against cheaters and a rising number of publications is exclusively dedicated to the task of detecting these individuals. The range of commonly-observed measures taken includes asking redundant ques-tions to rely on an aggregate of several workers rather than the decisions of just one individual [33], using held-out data to compare with, asking plausibility questions and many more sophisticated methods [16, 13].
 In many time-insensitive applications, HIT providers restrict the crowd of workers to certain nationalities (a typical ex-ample would be US workers only) who they trust will pro-vide higher quality results. Although the approach is widely accepted and has been shown to significantly reduce the number of spam submissions, we believe that this uptake may be treating symptoms rather than the actual under-lying cause. Rather than attributing the confirmed perfor-mance differences between the inhabitants of different coun-tries to their nationality, we hypothesize that there are 2 major types of workers with fundamentally different moti-vations for offering their workforce on a crowdsourcing plat-form: (1) Money-driven workers are motivated by the fi-nancial reward that the HIT promises. (2) Entertainment-driven workers primarily seek diversion but readily accept the financial incentives as an additional stimulus. We are convinced that the affiliation (or proximity) to one of those fundamental worker types can have a significant impact on the amount of attention paid to the task at hand, and, sub-sequently, on the resulting annotation quality. We realize, that money-driven workers are by no means bound to deliver bad quality; however, they appear to be frequently tempted into sloppiness by the prospect of a higher time efficiency and therefore stronger satisfaction of their main motivation. Entertainment-driven workers, on the other hand, appear to work a HIT more faithfully and thoroughly and regard the financial reward as a welcome bonus. They typically do not indulge in simple, repetitive or boring tasks. We pro-pose to more strongly focus on entertainment-driven work-ers by phrasing crowdsourcing problems in an entertaining and engaging way: As games. Csikszentmihalyi X  X  theory of flow [12], a state of maximal immersion and concentration at which optimal intrinsic motivation, enjoyment and high task performance are achieved, further encouraged our de-sign.
 We intend to increase the degree of satisfaction entertainment-driven workers experience. This can lead to (a) higher result quality, (b) quicker batch processing rates, (c) lower over-all cheater rates, (d) better cost efficiency. An additional incentive for delivering high-quality results in a game sce-nario would be the element of competition and social stand-ing among players. Taking into account recent behavioural analyses of online communities and games [24], entertain-ment seekers can be expected to put considerable dedication into producing high-quality results to earn more points in a game to progress into higher difficulty levels or a rank on the high score leaderboard.
 The novel contributions of this work are 5-fold: (1) We de-scribe a game-based approach to collecting document rele-vance assessments in both theory and design. (2) Based on NIST-created TREC data, we conduct a large-scale com-parative evaluation to determine the merit of the proposed method over state-of-the-art relevance assessment crowd-sourcing paradigms. (3) Venturing beyond  X  X ard X  quality indicators such as precision, cost-efficiency or annotation speed, we discuss a wide range of socio-economical factors such as demographics and alternative incentives to enhance a fundamental understanding of worker motivation. (4) In a separate study, we demonstrate the generalizability of the proposed game to other tasks on the example of noisy im-age classification. (5) We create a corpus of relevance as-sessments of considerable size and quality that we disclose to the research community.
 The remainder of this work is structured as follows: Sec-tion 2 describes the state of the art of crowdsourcing for document relevance assessments as well as in the domain of games with a purpose (GWAP). Section 3 introduces the theoretical considerations and design decisions that guide our annotation game. In Section 4, we describe the setup as well as the results of a large scale study conducted on several commercial crowdsourcing platforms in order to de-termine the usefulness of the proposed method. Section 5 demonstrates the generalizability of our method on the task of image classification. Section 6 is dedicated to a discussion of the key insights gained in the course of this work, before concluding in Section 7.
This section introduces related approaches from three dif-ferent research areas at the intersection of which this work is situated, namely, relevance assessment, crowdsourcing and games with a purpose.
 Document relevance assessments have been playing a cen-tral role in IR system design and evaluation since the early Cranfield experiments [42]. Explicit judgements of (the de-gree of) relevance between a document and a given topic are used as a proxy of user satisfaction. Based on such test collections, the results of retrieval systems can be compared without undergoing numerous iterations of user studies. As a leading actor in IR evaluation and benchmarking, NIST X  X  Text Retrieval Conference (TREC) [43] looks back on two decades of assessing document relevance. In order to be suitable for the evaluation of state-of-the-art Web-scale sys-tems, the requirements in terms of size, topical coverage, diversity and recency that the research community imposes on evaluation corpora have been steadily rising. As a conse-quence, the creation and curation of such resources becomes more expensive. To further ensure the scalability of test collection-based system evaluation, considerable effort has been invested into designing robust performance measures [6], selecting the right documents for evaluation [8] and in-ferring judgements from user interaction logs [19]. Crowdsourcing represents an alternative means of collecting and annotating large-scale data sets. By employing a large group of individuals, paid at transaction level, tasks of con-siderable size can be completed in a timely and affordable manner. Document relevance assessments have been shown to be a task that can reliably be fulfilled by crowd workers [4, 22, 15]. One of the fundamental challenges in crowdsourc-ing is overcoming malicious and sloppy submissions. Many effective schemes exist, ranging from aggregating the results of independent workers to the use of honey pot questions [17, 18, 14]. Marshall et al. discuss the importance of en-gaging HIT design on result quality [28]. Recently, several scientific workshops have been dedicated to pursuing how to use crowdsourcing effectively and efficiently [10, 26]. Most notably, TREC 2011 for the first time offered a dedicated crowdsourcing track [25] addressing the crowdsourced col-lection of document relevance assessments. In this work, we adopt the evaluation scheme and data set used there. The majority of crowdsourced tasks are plain surveys, rel-evance assessments or data collection assignments that re-quire human intelligence but very little creativity or skill. An advance into bringing together the communities of on-line games and crowdsourcing is being made by the platform Gambit [1], that lets players complete HITs in exchange for virtual currency in their online gaming world. This combi-nation, however, does not change the nature of the actual HIT carried out, beyond the fact that the plain HIT form is embedded into a game environment. Instead, we propose using an actual game to leverage worker judgements. A number of techniques have been designed to make par-ticipation in human computation efforts as engaging as pos-sible. Perhaps the most effective technique among these is a genre of serious games called games with a purpose [38] which have been developed with the focus of efficient and entertaining transformation of research data collection into game mechanics. By equating player success in the game with providing quality inputs, the idea is to extract higher-quality data than is currently done with dull repetitive tasks such as surveys. More than half a billion people worldwide play online games for at least an hour a day  X  and 183 million in the US alone [29]. The average American, for example, has played 10,000 hours of video games by the age of 21 [31]. Channeling some of this human effort to gather data has shown considerable promise. People engage in these GWAPs for the enjoyment factor, not with the objective of performing work. Successful GWAPs include the ESP Game [37], which solicits meaningful, accurate image labels as the underlying objective; Peekaboom [41], which locates objects within images; Phetch [39], which annotates images with descriptive paragraphs; and Verbosity [40], which collects common-sense facts in order to train reasoning algorithms. The typical research objective of these GWAPs is to have two randomly-selected players individually assign mutually-agreed document labels, with the game mechanics designed to reward uncommon labels. In contrast, the game mechan-ics of our proposed game is to encourage and reward con-sensus labeling. The Pagehunt game presented players with a web page and asked them to formulate a query that would retrieve the given page in the top ranks on a popular search engine to investigate the finda bility of web p ages [27]. While task-specific games have been shown to be engaging means of harnessing the players X  intelligence for a certain research goal, there has not been a formal investigation of the merits of game-based HITs over conventional ones. Additionally, current GWAPs are typically highly tailored towards a cer-tain (often niche) problem at hand and do not lend them-selves for application across domains. We will demonstrate the generalizability of our approach in Section 5.
In this section, we will introduce the annotation game as well as the necessary pre-and post-processing steps in order to acquire standard topic/document relevance assess-ments from it. Careful attention will be paid to highlighting motivational aspects that aim to replace HIT payment by entertainment as a central incentive.
The central concept of our proposed game is to require players to relate items to each other. In order to preserve its general applicability, we tried to make as few as possi-ble assumptions about the nature of those items. The game shows n = 4 concept buckets b 1 ...b n at the bottom of the screen. From the top, a single item i slides to the bottom, and has to be directed into one of the buckets by the player. Doing so expresses a relationship between i and b j . Addi-tional information about i can be found in an info box in the top left corner.
 In the case of document relevance assessments, the concept buckets b j display topic titles, item i is a keyword from a document and the info box displays the context in which the keyword appears in that document. Figure 3.1 shows a screenshot of our game. A live version can be found online For each assigned relation between i and b j the player is awarded a number of points. The score in points is based on the degree of agreement with other players. In addition to this scheme, the game shows a tree that grows a leaf for every judgement that consents with the majority decision. A full tree awards bonus points. In this way, we reward continuous attention to the game and the task. As a final element, the game is divided into rounds of 10 judgements each. After each round, the speed with which item i moves is increased, making the task more challenging to create ad-ditional motivation for paying close attention.
 After up to 5 rounds (the player can leave the game at any point in time before that) the game ends and the achieved points as well as the player X  X  position in our highscore leader-board are shown. Together with the log-in concept, this aims to encourage replaying as people want to advance into the higher ranks of the leaderboard.
Previously, we described the mechanics and underlying assumptions of the game. Now, we will detail how to set http://www.geann.org Figure 1: A screenshot of the annotation game in which the keyword  X  X iffel Tower X  has to be related to one of a number of concepts. it up with TREC data for relevance assessments. All rel-evant game information is stored in a relational database from which items, concepts and additional information are drawn and into which user judgements are stored, subse-quently. We assume a list of pairs p consisting of a query q and a document d for which we will collect user judgements. For every d , we extract the textual web page content and break it up into a set S of sentences s d, 1 ...s d, | S | LingPipe library [5]. In the following step, S is reordered by a ranking function r ( s ),basedondecreasingmeaninverse document frequency ( idf )ofsentences s with a number of | s | constituent terms t n . | C | denotes the number of docu-ments in the collection and df ( t ) is the number of documents containing term t . In this way, we promote the most salient and informative sentences in the document. The underlying idf ( t ) statistics for this step are computed collection-wide.
Finally, the k highest-ranking sentences (those with the highest scores of r ( s )) are selected and used in the game. The concrete setting of k depends on the size of document d and was set to 0 . 1 | S | in order to account for different document lengths. Higher settings of k result in a higher judgement density per document. For each of the selected sentences, we extract the single highest-idf term t n as sliding keyword i , and the full sentence as context information to be shown in the top left corner. The concept buckets b include the original query q , two randomly selected topics from the database, as well as an  X  X ther X  option to account for none of the offered concepts being related to i . The buckets are shown in random order to prevent selection biases.
As a final step, we have to transform the players X  concep-tual associations into document-wide relevance judgements. Each player annotation a can be understood as a quintuple a =( p a ,i a ,s a ,c a ,r a ) in which player p associated item i oc-curring in context sentence s with concept c in round r of the game. In a first step, we map all associations a to relevance votes. We interpret associations of any s  X  d to the concept of the original query q as a player X  X  binary relevance vote v p,s,q,r between sentence s and query q as described in Equation 1.
In order to account for wrong associations and diversity in personal preference, we aggregate a global sentence-level vote v s,q across all players p . As the game speeds up in higher rounds, players have less time available for making the relevance decision. In a preliminary inspection of anno-tation results, we noticed significant drops of accuracy across subsequent rounds of the game. In order to account for this effect, we introduce a weighting parameter  X  r representing the confidence that we put into judgements originating from round r of the game being correct. For simplicity X  X  sake, we reduce the confidence by 0.05 per round after the first one. Alternative strategies could for example include learn-ing this parameter as a maximum likelihood estimate across previous observations. Equation 2 details the aggregation to a global sentence-level vote v s,q across the set of players P s,q that had encountered the combination of sentence s and query q .
Finally, we aggregate across all sentence-level votes v s,q of a document d in order to get one global page-wide judge-ment that is comparable to well-known (e.g., NIST-created) annotations. Equation 3 outlines this process formally. It should be noted, that omission of this third step may, given the application at hand, be beneficial for the evaluation of tasks such as passage-level retrieval or automatic document summarization.
In this section, we describe the setup and results of a large-scale experiment conducted on several major commer-cial crowdsourcing platforms. Our performance comparison of traditional and game-based HITs will be guided by the following 7 fundamental directions: Quality. How does the result quality of game-based crowd-Efficiency. Are game-based HITs more popular, resulting Incentives. How much is fun worth? We investigate the Consistency. Does our game encourage a stronger task fo-Robustness. Does the share of (alleged) cheaters attracted Population. Does the use of games lead to a different crowd Location. State-of-the-art crowdsourcing approaches fre-
In this comparative study, we replicate the setting that was proposed in the TREC 2011 Crowdsourcing Track as-sessment task [25]. A total of 3200 topic/document pairs (30 distinct topics, 3195 unique documents) were judged for relevance. The documents are part of the ClueWeb09 col-lection [7], and the topics originate from the TREC 2009 Million Query Track [9]. A comprehensive list of all top-ics and document identifiers are available from the TREC 2011 Crowdsourcing Track home page 2 . We contrasted the performance and characteristics of our proposed gamified HIT (Section 4.2.2) with those of a traditional one (Section 4.2.1). To attribute for assessment mistakes and personal preference, we collected judgements from at least 3 individ-ual workers per topic/document pair in both settings. All HITs were run in temporal isolation (No more than 1 batch at any given time) to limit mutual effects between the tasks. In the following, we describe the respective task designs in detail.
As a performance baseline, we designed a state-of-the-art relevance assessment HIT. Its design follows accepted in-sights from previous work as detailed in the following. In order to limit the number of context changes, the document is shown in-line on the platform X  X  HIT form as proposed by Kazai [20]. In this way, no distracting opening and closing of windows or browser tabs is required. To further enhance the task, we highlight every occurrence of query terms in https://sites.google.com/site/treccrowd2011/ the document. This technique was reported to be benefi-cial by several previous approaches, e.g., [36]. Finally, in order to deal with malicious submissions, we measure agree-ment with NIST gold standard pairs of known relevance. Workers who disagree on more than 50% of the gold labels are rejected from the judgement pool. In the HIT instruc-tions, we briefly introduce the available relevance categories. The definition of relevance was introduced according to the TREC guidelines [25]. The HIT form contains 2 questions: 1. Please indicate the relevance of the shown document towards the topic "&lt;T&gt;". 2. Do you have any remarks, ideas or general feedback regarding this HIT that you would like to share?
For each HIT, the place holder &lt; T &gt; is replaced by the current topic. Offering the possibility for worker feedback has been frequently reported to improve task quality and track down bugs or design flaws quickly [3]. The HIT was offered at a pay rate of 2 US cents per topic/document pair assessments; a reward level previously found adequate given the task [2].
The central piece of our proposed gamified version of the relevance assessment HIT is the annotation game that was described in Section 3.1. Instead of having the workers com-plete tasks locally on the crowdsourcing platform, the tech-nical requirements of our game demanded running it off-site on a dedicated server. In order to verify task completion, workers are provided with a confirmation token after play-ing one round of the game (10 term associations). Back on the crowdsourcing platform, they enter this token in order to get paid. As a consequence, the actual HIT contained only a brief instruction to the off-site process and two input fields: 1. Please enter the confirmation token you obtained after completing one round of the game. 2. Do you have any remarks, ideas or general feedback regarding this HIT that you would like to share?
Again, we solicit worker feedback. The HIT was offered at a pay rate of 2 US cents for one round (10 term associations) of the game.
All experiments described in this section were conducted between December 2011 and February 2012 on two crowd-sourcing platforms: Amazon Mechanical Turk [35] as well as all available channels on CrowdFlower [11]. Initial eval-uation did not show any significant differences in the work delivered by workers from different platforms. We will there-fore not split the pool of submissions along this dimension. In total, 795 unique workers created 105,221 relevance judge-ments via our game. Additionally, 3000 traditional rele-vance judgements were collected for comparison. In total, we invested $90 to collect a volume of 108,221 annotations across the two compared experimental conditions. Together with the TREC 2011 Crowdsourcing Track annotations and Conventional 0.73 0.74 Game (plain) 0.65 0.75 Game (sent) 0.77 0.87 Game (doc) 0.82 0.93 Table 2: Annotation quality as a function of the game round in which judgements were issued.
 the original NIST labels, this makes the T11Crowd subset of ClueWeb09 one of the most densely-annotated Web re-sources known to us. To enable reproducibility of our in-sights and to further general crowdsourcing research, the complete set of our judgements and the game itself are avail-able to the research community 3 .
As a starting point to our performance evaluation of game-based crowdsourcing of relevance assessments, we investigate the quality of the collected labels. Table 1 details the per-formance of our game in terms of overlap with gold standard NIST labels as well as the global consensus across all TREC 2011 Crowdsourcing Track participants (TREC-CS). We can note that already the conventional HIT delivers high result quality. Ratios between 65% and 75% are often considered good rules-of-thumb for the expected agreement of faithful human judges given a relevance assessment task [44]. TREC consensus labels show a high overlap with NIST annotator decisions. The third row in Table 1 shows the performance of direct unaggregated sentence-level votes from our game as described in Equation 1. While agreement with the TREC crowd is already substantial, the overlap with high-quality NIST labels lags behind. As we aggregate across multi-ple workers X  annotations of the same sentence (Equation 2) and, finally, across all sentences extracted from the same document (Equation 3), the performance rises significantly, outperforming all compared methods. We used a Wilcoxon signed rank test at  X &lt; 0 . 05-level to test significance of re-sults.
 In order to confirm the usefulness of our assumption from Section 3 concerning the decline of label quality as the game speeds up and the player has less time to make decisions, we evaluated annotation performance of raw labels according to the round in which they were issued. Table 2 shows a near-linear decline in agreement of plain game scores with TREC consensus as the game progresses. Agreement with NIST scores also consistently shrinks from round to round.
Finally, previous work on prediction in crowdsourcing sys-tems demonstrates that reliability of the average predicted scores by the crowd improves as the size of the crowd in-http://sourceforge.net/projects/geann/
Figure 2: Quality as a function of votes per pair. creases [34, 30]. The benefit of our game-based HIT is its popularity that allows us to collect more judgements per topic / document pair than traditional HITs. On aver-age, each topic / document pair in the collection received 32 unique user judgements at the sentence level (some of which may originate from the same user as she or he rates different passages of the same document). Figure 2 shows how annotation quality develops as we add more judgements per pair. After initial fluctuation, as single votes have great influence on the overall decision, accuracy consistently im-proves as we add more votes per pair. The effect levels out as we approach the upper performance limit.
The second official performance indicator besides label quality in the TREC 2011 Crowdsourcing Track was the time necessary to collect the required judgements. For many use cases in human computation, low latencies are essential. The particular nature of the game imposed a time limit on players within which they had to make their decisions. As we detailed in the previous section, aggregation across users, weighting votes according to the difficulty level under which they were created, ensured competitive result quality. At the same time, however, a sequence of, individually quick, concept matchings enables workers to be more efficient and motivated than in conventional settings. Table 3 shows how conventional HITs take slightly longer to judge documents even when aggregating the duration of all passage-level votes in the game-based setting. Taking into account the signifi-cantly higher uptake rate (number of judgements issued per hour) of the game HITs, this serves for a considerably more efficient batch processing.
 Especially in conjunction with the previous section X  X  find-ings of high degrees of redundancy serving for better result quality, high uptake rates become crucial as they allow for timely, yet accurate decisions.
 Uptake (votes per hour) 95.2 352.1
The third and final evaluation criterion employed for TREC 2011 was the cost involved in the collection of relevance la-bels. With our game-based approach, we aim to, at least partially, replace the financial reward of the HIT with en-tertainment as an alternative motivation. In this section, we will investigate to which degree this change in incentives canbeobservedinworkerbehaviour.
 In order to be paid via the crowdsourcing platform, workers had to complete at least one round (10 concept matchings) of our game. At that point the required confirmation token was displayed to them and they could return to the plat-form in order to claim their payment. However, the game offered an additional 4 levels to be played. From a purely monetary-driven perspective there would be no reason for continuing to play at that point. As we can see in Table 4, however, over 70% of games are played beyond the first round. This essentially results in crowdsourcing workers cre-ating judgements free of charge because they enjoy the game experience. Additionally, we can observe players to return to the game after a number of hours to play again and im-prove their score and their resulting position on the leader board. Subsequent visits often happen directly to the game page, without being redirected from (and paid through) the crowdsourcing platform. Almost 80% of all players (633 out of 795) return after their first round played, with an average time gap of 7.4 hours between games. For regular HITs, we observed a return rate of only 23%.

When inspecting the concrete distribution of judgements across workers, as shown in Figure 3, we see this trend con-tinued. Crowdsourcing tasks often tend to exhibit Power-law distributions of work over unique workers with some strong performers and a long tail of casual workers who only submit single HITs. Here, however, we notice a strong center group of medium-frequency players. We hypothesise that replac-ing the workers X  extrinsic motivation ( X  do the HIT to earn money  X ) by an intrinsic one ( X  let X  X  have some fun  X ), causes these tendencies.

This has a number of noteworthy consequences: (1) We can attract workers to a HIT at a comparatively low pay rate. Even without playing beyond the first round, 2 US cents for 10 concept associations would roughly result in a prospective hourly pay of $1.20. (2) Furthermore, as most workers continue playing, additional annotations are created with no expectation of financial compensation. (3) Drawn by the competitive aspect of the game, workers re-Figure 3: Distribution of judgements across users. turn and create even more unpaid assessments. As a con-sequence, the overall amount of money invested into the game-based collection of more than 100,000 sentence-level relevance judgements was $27.74. This includes all adminis-trative fees charged by the crowdsourcing platforms. In com-parison, the participants to the TREC 2011 Crowdsourcing Track reported overall costs of $50 -$100 for the collection of significantly fewer labels.

Table 5 shows a final cost comparison of the conventional and game-based versions of the inspected relevance assess-ment HIT. While all indicators of cost efficiency from a worker X  X  perspective clearly speak for choosing the conven-tional, better-paying HIT, the previously described figures of HIT uptake rates as well as the high number of alterna-tive HITs available at all times on large-scale platforms such as AMT, indicate, that we reach workers who consider the entertainment potential of a HIT before choosing it. If we consider all judgements made in rounds after the first one and all judgements from revisits that were not paid for on the crowdsourcing platform as free-of-charge judgements, we arrive at a share of 83.7% of all labels having been created free of charge. Additionally, a number of players (39 out of 795) accessed the game without being prompted (and paid) by a crowdsourcing platform. These players were recruited from the authors X  professional and private networks or word of mouth of other players. We could not find significant differences in the judgement quality or volume created by this group of players. The invested amount of money can be seen as advertisement costs rather than actual payments. In a traditional setting, collecting the same annotation density would have cost $2104.
Following Csikszentmihalyi X  X  theory of Flow [12], a state of deep immersion is a good foundation for high perfor-mance independent of the concrete task at hand. With our game-based HIT, we aimed to exploit this observation in order to create greater task focus than workers typically achieve on conventional HIT types. The previously shown result quality figures support this hypothesis. As an ad-ditional performance indicator, we will measure the work-ers judgement consistency. Faced with the same passage of text and choice of concepts multiple times, a situation-aware worker is expected to display a high degree of intra-annotator agreement. In the course of our judgement collec-tion, we showed identical assignments to workers 837 times and observed an intra-annotator agreement of 69.8%. We set up a dedicated crowdsourcing experiment in which a por-tion of the offered topic / document pairs re-occurred. The HIT was set up following the scheme described in Section 4.2.1. Across 500 redundantly issued assignments, we ob-served an intra-annotator agreement of only 61.3%, a signif-icantly lower ratio (determined using Wilcoxon signed rank test at  X &lt; 0 . 05) than in the game-based setting. While the game setting resulted in higher consistency than usual crowdsourcing schemes, we could not match the consistency Scholer et al. [32] report for professional assessors as for ex-ample employed by NIST.
Cheating, spamming and low-quality submissions are well-known and frequently-observed incidents on commercial crowd-sourcing platforms. Previously, we demonstrated convinc-ing result quality of gamified document relevance assess-ments when labels are aggregated across a sufficiently large number of workers. Since our approach appeals more to the entertainment-seeking rather than money-driven work-ers, we did not include a dedicated cheat detection scheme as would often be considered necessary in state-of-the-art HITs. However, we realise that the observed cheat rate in an assignment can serve as a s urrogate for th e confidence and reliability of the overall results. To this end, we mea-sure the observed proportion of cheat submissions to our game as well as to the conventional HIT version. Eickhoff et al. [14] suggest categorizing workers who disagree with the majority decision in more than half of all cases as cheaters. In order to deliver a conservative estimate of the rate of cheat submissions, we tighten their definition and consider a worker as cheating if at least 67% of their submissions dis-agree with the majority vote. This scheme was applied to both, the conventional HIT as well as the gamified version. In the game-based case, we additionally flagged all submis-sions as cheat that tried using forged confirmation tokens. Overall, this resulted in a share of 13.5% of the conventional HIT X  X  judgements being considered cheated. For the game-based version, the percentage was a significantly lower 2.3%. This finding conforms with [13], who observed innovative, creative tasks being less likely to be cheated on.
In this work, we did not make use of any form of apriori filtering the pool of workers eligible to access our HITs. We hypothesise, however, that HIT type, financial reward and task phrasing influence the underlying crowd that decides to work on a given assignment. To better understand the com-
English Native Speaker 24% 25% position of the group of commercial crowdsourcing workers that are interested in games, we accompanied parts of our HITs by surveys in which we asked for high-level participant demographics and their preference for either the conven-tional or the game-based HIT. Table 6 shows an overview of several salient outcomes of the survey. The split in decisions was roughly equal, with 24% of workers not indicating clear preferences. The entertainment-seeking worker is on average several years younger, more likely to hold a university de-gree and will typically earn a higher salary. Finally, women were found to be significantly less interested in games than their male co-workers. This conforms with general observa-tions about gender differences made for example by [23]. A worker X  X  language background did not influence his or her likelihood to prefer games.
Many commercial crowdsourcing schemes report perfor-mance gains when filtering the crowd by nationality. Due to different expected levels of education, language skills or cultural properties, such steps may influence result qual-ity. As a final dimension of our investigation of games for use on commercial crowdsourcing platforms, we will inspect whether worker origin has an influence on result quality. From our survey, we found Indian workers, with a share of 60%, to be the dominant group in both settings. US work-ers were consistently the runners-up with a proportion of approximately 25%. There was no significant difference in the likelihood to prefer games over conventional HITs be-tween countries.
 Finally, when inspecting result quality from our game, again, no difference in performance or likelihood to cheat could be found. This suggests that filtering workers by nationality may not be ideal. In fact, the underlying worker motiva-tion and HIT type preference may be assumed to have a far greater impact on observed uptake, performance and trust-worthiness.
In the previous sections, we described and evaluated the performance of the proposed crowdsourcing-powered anno-tation game for the task of TREC-style document relevance assessments. To demonstrate the generalization potential of the described concept-matching method, we applied the same game in an image classification pilot.
 In the course of the Fish4Knowledge project ( http://www. fish4knowledge.eu/ ), several underwater cameras have been placed in selected locations in south-east Asian coral reefs. The continuous recordings are supposed to further knowl-edge about behaviour, frequency and migration patterns of the resident tropical fish species. A key step to coping with the large amounts of image data produced by these cam-eras is a reliable automatic species classification. In order to train such systems, numerous training examples are re-quired. While the project employs a team of marine biolo-gists, their greater expertise is costly. Using our annotation game, we crowdsource the task of classifying the encoun-tered species. Instead of relating keywords to TREC topics, the objective is now to match a shot of the underwater cam-era (often low quality) to high-quality examples of resident fish species. By initializing the underlying database with images rather than textual items, no changes to the actual game were necessary. Figure 4 shows a screenshot of this alternative game setting.

Our feasibility study encomp assed 190 unique underwater camera shots for which known gold standard labels created by the marine biologists existed. Each biologist had clas-sified all images, allowing us to contrast crowd agreement with expert agreement. The HIT was offered in January and February 2012 at the same pay rate (2 US cents per round of 10 associations) as the text-based version. Ta-ble 7 shows the results of the experiment in which the de-gree of agreement with the majority of experts as well as the crowd X  X  inter-annotator agreement are detailed. We can see high agreement with expert labels as well as substantial agreement among workers. The popularity (qualitatively perceived through worker feedback) and uptake rate of this HIT even slightly exceeded those of the game-based one for document relevance assessments. Several workers had men-tioned difficulties reading the moving text fragments in the short time available. With images, this does not appear to be an issue.
 Methods like this could play an essential role either in the creation of training and evaluation data necessary for the assessment of automatic classification quality, or as part of a hybrid human-computer classification in which automatic methods narrow down the range of potential species before human annotators select the most likely species from the pre-selection. It should be noted, however, that the domain experts are by no means obsolete in this setting. While they annotated fish images simply based on their knowledge of the resident species, players of our game only had to select one out of a range of 4 species by similarity.
In the previous section, we found convincing results across all inspected performance dimensions, supporting the bene-Experts 0.82 -fit of offering alternative incentives besides the pure financial reward. In this section, we discuss a number of observations and insights that were not yet fully covered by the evalua-tion.

Firstly, considering the fact that the round concept of the game appears to invite workers to create assessments with-out payment (by playing on after having received the con-firmation token), it is not obvious why we should limit the game to a fixed number of rounds. In the present setting, a game inevitably ends after the fifth round. One might argue that a higher number of rounds or even an open-ended con-cept would result in even greater cost efficiency. In fact, the opposite seems to be the case. In an initial version of the game, there was no upper limit to the number of rounds per game. As a consequence, some players were frustrated, as the only way to  X  X inish X  the game would be to either lose or give up. This resulted in fewer returning players. Addition-ally, the quality of annotations resulting from higher rounds was highly arguable as the objective of the game became mainly surviving through as many rounds of fast-dropping items as possible, rather than making sensible assessments. In the new, limited, setting, the clear objective is to do as well as possible in 5 rounds. Players who want to improve their score beyond this point have to return and start a new game.

A second key observation to be made is the fact that while we evaluate against the performance of NIST assessors and TREC participants, the tasks our workers face is a signif-icantly different one. In the game, no worker gets to see full textual documents or is even told that the objective is to determine topical relevance of web resources towards given information needs. We deliberately aimed for such a loose coupling between game and task as we wanted to keep the game experience entertaining without the  X  X ftertaste X  of working. It is interesting that mere conceptual matching correlates well with actual relevance assessments. Also, in the data pre-processing phase, we do not extract sentences based on query terms but rather focus on pure idf figures as described in Section 3.2. In this way, we manage to capture the general gist of a document without artificially biasing it towards the topic.

Finally, the key insight gained from this work was the substantial benefit achieved by offering an alternative in-centive to workers. Most of the interesting properties ob-served in the gamified system, such as workers producing free labels, would not have happened otherwise. This is, however, not necessarily limited to gamified tasks. In this paper we used games as one possible means of showing how a particular incentive (money) can be replaced with another one (entertainment). By doing so, we focus on a certain type of worker, entertainment-seekers, the existence of which we hypothesised based on previous experience with crowd-sourcing. We are convinced that a better understanding of worker types and their specific intrinsic motivations is essen-tial in driving the boundaries of current crowdsourcing qual-ity. Kazai et al. [21] proposed an interesting classification of workers into several categories. In their work, a number of performance-based worker types, including e.g., spammers, sloppy and competent workers are described. We believe, that more general worker models which also encompass as-pects such as worker motivation capability and interest in certain HIT types, etc. can be of significant benefit for the field. Very similar to the task of advertisement placement, a worker whose motivations we understand, can be targeted with better-suited precisely-tailored HIT types.

The common example of worker filtering by nationality illustrates the practical nee d for a better understanding of worker motivation. This practice is not only of dubious ethi-cal value, it may additionally address symptoms rather than causes. The original objective of identifying and rejecting such workers that try to game the task and get paid without actually working is often hard to fulfil. Filtering by nation-ality is straightforward to achieve, but also (at best) only correlated with reliability. This bears the significant risk of artificially thinning the pool of available workers. This work (e.g., Tables 4 and 6), demonstrates that in an entirely unfiltered environment no significant national differences in quality, cheat rates, etc. could be found when focussing on the desired worker type. In this way, we retain a large work force but, by task design, discourage undesired worker types from taking up our work in the first place.

Looking out towards future changes to the game based on lessons learned in this work, we aim for including yet an-other incentive besides entertainment. The leaderboard con-cept of the current game tries to spark competition between players and has a moderate success at doing so. However, the workers do not know each other. In a reputation-aware environment, such as a social network, this effect can be ex-pected to have a far greater impact. Having the ability to compare scores and to compete in a direct multi-player game with their friends will create much more compelling incen-tives for (a) performing well in each game, (b) continuing to play, (c) returning for subsequent matches and (d) recom-mending the game to their circle of friends. We believe that exploiting these aspects by integrating social reputation into crowdsourcing will create many interesting applications.
In this work, we demonstrated the benefits of a game-based approach for collecting relevance assessments, exploit-ing insights from the field of serious games for application in commercial large-scale crowdsourcing. After a description of key design criteria, we evaluated the proposed scheme fol-lowing the setup of the TREC 2011 Crowdsourcing Track. We achieve confirm high result quality, matching the per-formance levels of the best TREC participants for the same task at a fraction of the invested cost, while attracting fewer cheaters. In a dedicated experiment, we showed the gener-alizability of the proposed game, that, without any changes, was applied for the task of image classification and clus-tering. In summary, we are convinced that alternative in-centives besides the actual financial HIT reward can posi-tively influence the outcome of crowdsourced data collection and annotation campaigns. As a tangible outcome of this work, a large-scale set of relevance judgements towards the T11Crowd subset of Clueweb was created and is available to the research community. Furthermore, the described game can be accessed and deployed as an open source project 4 . Future work will further exploit the community aspect to increase the motivation for playing. This could be done by, e.g., introducing a multiplayer mode in which several play-ers are in direct competition or by integrating the game into an identity and reputation-aware environment such as so-cial networks or virtual worlds. More fundamentally, this work has demonstrated the benefit of addressing the spe-cific preferences of entertainment-seeking workers. In the future, however, we should investigate formal worker mod-els of worker motivation and capability to enable an optimal work distribution and representation for arbitrary worker types.
 We would like to thank Jiyin He and the Fish4Knowledge project for providing us with the fish images and expert judgements. http://sourceforge.net/projects/geann/
