 1. Introduction documents for those relevant to a novel, unanticipated topic. Comparative evaluation of IR systems in their performance of this task is one of the purposes of the Text Retrieval Conference (TREC) (Voorhees and Harman 2000). The basis of this evaluation is a document collection, a group of topics (information needs), and associated expert judgements of the retrieved documents as relevant or not relevant. In the TREC Ad Hoc task, each system provides a response for each topic in the group. A response is a search result, i.e., a list of 1000 documents that the system orders by pertinence to the input request, the query. The query may be the topic itself or a restatement. Assessment of the document lists in terms of the relevance judgements provides a gauge for system evaluation.

Consider different queries that are all intended to be natural-language expressions of the same topic. As the definition of a topic, one might point to the statement of the topic that the TREC assessor prepared originally or to the documents in the collection that the 102 LIGGETT AND BUCKLEY Nevertheless, a set of different queries for each topic can be used to obtain a set of document lists from a system and thus a more extensive output with which to characterize system behavior. The TREC evaluation in 2000 contains a task called the Query Track (Voorhees and Harman 2001) in which for each topic, a set of queries is input to each system. Multiple queries help capture the variety of different ways users with the same information need may address a system. In this paper, we show how to analyze the document lists produced by multiple queries and illustrate our methods with results from the Query Track.

The simplest way to compare two document lists for the same topic is to compute for each list a single-list performance measure such as average precision and take the differ-ence. Clearly, however, the differences between lists are much more involved. Not only are positions in each list occupied by documents marked as relevant or not, but also some doc-uments are common to both lists, and these have nearly the same or quite different ranks in each list. In the case of multiple queries for a single topic, more involved ways of analyzing list-to-list differences reveal more about system differences. For example, from the Query T rack results, one can see the relation between the words available to express a topic and the relative behavior of different systems. In this paper, we show how further insight can be obtained by viewing the system responses not only in terms of a single-list performance measure but also in terms of a list-to-list dissimilarity measure (Banks et al. 1999).
A more complete understanding of the relation between topic expression and system behavior may be interesting but does not completely satisfy the goal of system evaluation. The goal is insight into the relative system performance for topics that the systems might have to process in the future. For example, Liggett and Buckley (2001) portray system behavior for four topics. For one topic, they show that the systems fail to one degree or another to equate  X  X eather-related deaths X  with  X  X torm-related deaths X  as is appropriate in the context of the topic. The question is,  X  X ow can one generalize the system behavior that this example suggests? X  In Section 2, we compare three topics and show that such generalization is possible. We hypothesize an abstract topic factor that is present at various f actor is a latent variable in statistical terminology (Bartholomew and Knott 1999).
In addition to the topic factor discussed in Section 2, other topic factors influence system behavior. In Section 3, we describe a method for finding additional factors. Such a method seems necessary in light of the effort that would be needed to compare all topics in the manner of Section 2. The method is based on the idea that topic factors can be most easily seen by contrasting topics that are very different. To gauge topic difference, we introduce and compute for each topic four indicators of the system responses to the query set for the topic. One of these is the average of a single-list performance measure. The others are based on a list-to-list dissimilarity measure.

Some of the topics indicated in Section 3 we analyze in Section 4. We identify additional topic factors although we are not able to characterize them as well as in Section 2. We also find a few topics with corresponding system behavior that we cannot generalize.
In Section 5, we draw some conclusions about what progress we have made in under-standing topic factors and how this understanding might be applied. We also comment on what might be gained from data sets collected in the future.
 SYSTEM PERFORMANCE 103 2. Topic factors systems. The term  X  X opic factor X  we borrow from engineering experiments, which are usually conceptualized in terms of an observed quantity that is a function of independent v ariables called factors. The observed quantity consists of indicators of system behavior such as those discussed in the next section. A topic factor is something measurable about each topic of interest. At this stage in their development, topic factors are not as concretely defined as factors in physical sciences experiments. Further work on topic-factor definitions is necessary as part of the use of topic factors in conceptualizing IR systems.
In this paper, we focus on topic factors useful in the comparison of two systems and carry along another four systems to provide context. Please see the disclaimer at the end of the paper. We use the Query Track designations for the systems (Buckley 2001). The systems we compare are  X  X abe X  and  X  X k9u. X  The other four systems that appear in our figures are  X  X aba, X   X  X umA X , X  X N7a, X  and  X  X N7e. X  These systems are described in the TREC-9 proceedings (Allan et al. 2001, Buckley and Walz 2001, Robertson and Walker 2001, Tomlinson and Blackwell 2001). Note that because these systems were configured specially for the Query T rack, one cannot infer from their performance the performance of a commercially available system.

We focus on  X  X abe X  and  X  X k9u X  because their difference provides an interesting illus-tration of the methods discussed in this paper. One system  X  X abe X  has the feature query e xpansion whereas the other  X  X k9u X  does not. As implemented in a well-regarded class of information retrieval systems, the document list is computed by first deriving a set of terms (key words and phrases) with associated weights from the input query and then using this set with its weights to select and rank documents from the collection. The first step may involve a procedure based on examining documents judged particularly relevant in an initial search of the document collection. User selection of these documents is called relevance feedback (Berry and Browne 1999), and automatic selection of these documents as in the case of  X  X abe X  is called blind feedback. This procedure is intended to uncover additional terms pertinent to the need for information and to revise the weighting for the set of terms finally used. Selecting additional terms may be done by the user or as in the case of  X  X abe X  automatically by the system in which case the procedure is called (automatic) query expansion. 2.1. Topic 57 We begin our discussion of topic 57 with the topic statement, which is shown in Table 1. The topic statement, which an assessor creates as the first step in the Ad Hoc task, has three roles. First, as part of TREC-1 where the topics considered here were first introduced, the topic statement was distributed to the participants who returned document lists from which were extracted the documents assessed for relevance. Second, the topic statement is the TREC-1 assessor X  X  understanding of what constitutes a relevant document. Third, as part of the Query Track in TREC-9, the topic statement was used to create a set of queries, alternative natural language expressions of the topic. 104 LIGGETT AND BUCKLEY
Thought of as the truth against which systems are evaluated, the topic can be characterized by the topic statement or by the documents in the collection designated as relevant. In the collection used for the Query Track, there are 280 relevant documents for topic 57. As a caveat to results reported below, we note that there may be documents in the collection that should be designated as relevant but were missed by the systems used in TREC-1 and therefore not assessed for relevance. In this regard, note the absence in the description and summary for topic 57 (Table 1) of the word  X  X inancial, X  which we mention below in our discussion of topic 57. Because some TREC-1 systems did not use the entire topic statement as input, this absence might have resulted in documents that should have been judged relevant but that were never presented to the assessors.
 documents are excluded from the list. We compare this measure to average precision in the Appendix. Our measure is essentially the number of irrelevant documents in the list that are SYSTEM PERFORMANCE 105 returned before 25 percent of the relevant documents are returned. We denote this number by N 25 . Our measure is a transformation of N 25 , namely  X  log 10 ( N 25 + 1). Note that our measure gives a higher value with better performance. It gives the value 0 if there are no irrelevant documents in the list before 25 percent of the relevant documents are returned, One might expect that the variation of N 25 in absolute terms is less meaningful when N 25 is larger. The logarithm compensates for this.

To define our measure of exclusion of irrelevant documents as well as indicators discussed in Section 3, we introduce some mathematical notation. For each topic, our measure is computed from the 1000-document lists and from the designations of documents as relevant or not relevant to the topic. Let the relevant documents for a topic be indexed by i , where i = 1 ,..., n documents. For a particular list, either relevant document i is returned and its position in the list is given by r i or it is not returned and we can give its position as r i = 1001. Let the number of relevant documents returned be n ( n  X  n R ). As the first step, we reduce each 1000-document list to a sequence { r 1 ,..., r n 1000-document list occupied by a relevant document. Sorting { r 1 ,..., r n order gives { r (1) ,..., r ( n the basis for our irrelevant exclusion measure.

There are some details that are important only in programming our irrelevant exclusion measure. The number of irrelevant documents that occur before one quarter of relevant other cases is shown below. On the basis of these considerations, our count of irrelevant documents is In the data considered here, the fourth case, the case n = 0, does not actually occur.
Performance in terms of our irrelevant-exclusion measure is shown in figure 1 for twenty ones that are near duplicates of others and selecting, in order, the ones that give the best performance. To gauge query performance for this purpose, we compute for each query and 1000, and then average over the six systems. We use six systems so that a single system has less effect on whether or not a query is included.
 Reading the queries in figure 1 shows that  X  X CI X  is the primary key word for this topic. Comparing systems, we see that the performance difference between  X  X abe X  and  X  X k9u X  is positive for all queries shown. Looking at the relation between performance difference 106 LIGGETT AND BUCKLEY and query wording suggests that the words  X  X ell X  and  X  X inancial X  influence performance. The word  X  X ell X  occurs four times, the word  X  X inancial X  occurs ten times, and they occur together once. Generally speaking,  X  X ell X  decreases the performance difference even when  X  X inancial X  is present, and  X  X inancial X  increases the difference largely by improving the performance of  X  X abe. X  That only a few words have predictable effects on the  X  X abe X  - X  X k9u X  performance difference seems to be a predominant characteristic of this topic and query set.

We now consider list-to-list differences from the perspective of the order in which the relevant documents are returned. Recall that each value of i corresponds to a particular i is not returned. Let R i be the position of relevant document i in the sub-list consisting of Thus, R i is the rank of r i in the set of numbers r 1 ,..., r n with ranks { R 1 ,..., R n a single-list performance measure.
 The return order of relevant documents { R 1 ,..., R n responses for the same topic through computation of dissimilarities. Our choice of dissim-ilarity is obtained from Spearman X  X  coefficient of rank correlation (Gibbons 1985). This SYSTEM PERFORMANCE 107 choice is effective for the Query Track data. Banks et al. (1999) mention other possible choices. Consider two 1000-document lists with n ( p ) relevant documents returned in the first and with { R ( q ) 1 ,..., R n adjusted for the relevant documents not returned is given by where Converting s pq , which is a similarity measure, to our dissimilarity measure, we obtain Note that in our choice of 20 queries from the 43, our purpose in eliminating duplicate queries is to avoid different queries with  X  pq = 0 for some system.

When all the relevant documents are returned ( n = n R ), Spearman X  X  coefficient of rank not returned as all the same, that is, as tied. We include the necessary adjustment for tied ranks (Gibbons 1985).

Figure 2 displays the dissimilarities for topic 57 by use of multidimensional scaling (Cox and Cox 2001, Kruskal and Wish 1978, Rorvig 1999). An introduction to multidimensional scaling for the Query Track data is given by Liggett and Buckley (2001). The algorithm we use is Kruskal X  X  isotonic multidimensional scaling, which is named  X  X soMDS X  by Venables and Ripley (1999). In our application, multidimensional scaling gives a plot with a point for each query-system combination. The Euclidean distances between points on the plot portray as faithfully as possible the corresponding dissimilarities.

Figure 2 shows that the variation in the return order of relevant documents is much smaller for  X  X abe X  than  X  X k9u. X  We see that  X  X ell, X  one of the words that stands out in figure 1, 108 LIGGETT AND BUCKLEY influences the return order of both systems but more for  X  X k9u X  than  X  X abe. X  For both systems, we see that  X  X inancial X  has little influence on the return order. Apparently, other w ords influence the return order for  X  X k9u. X 
Summarizing topic 57, we note that compared to  X  X k9u, X   X  X abe X  excludes irrelevant documents better and is less return-order sensitive to  X  X ell X  and other query words (except  X  X inancial X ). Apparently, the system  X  X abe X  achieves this by weighting  X  X CI X  more heavily in formation of document lists. This observation is, of course, an oversimplified description of the way  X  X abe X  functions. Nevertheless, it seems that for topics like 57,  X  X abe X  has a greater ability to grasp the topic despite variations in query wording.
 T opic 76 As the next step in discovery of a topic factor, we consider topic 76. The topic statement is shown in Table 2. For this topic, the number of relevant documents in the collection is 168.
Figure 3, which is comparable to figure 1, shows the 20 best queries and our irrelevant-e xclusion measure for each system and query. Reading the queries in figure 3 suggests of the queries shown. Looking at the relation between performance difference and query SYSTEM PERFORMANCE 109 w ording suggests that the word  X  X ourt, X  which refers to the Supreme Court, and the phrase  X  X onstitutional amendment X  degrade  X  X abe X  performance. The word  X  X ourt X  occurs two times and the phrase  X  X onstitutional amendment X  occurs three times. Apparently,  X  X abe X  better performance would be obtained if this phrase were treated as two separate words as is apparently the case for query E. Figure 3 also suggests that the word  X  X ntent, X  which occurs nine times, improves the performance of  X  X abe X  when  X  X onstitutional amendment X  is not present. The performance of  X  X abe X  for Query H is not as good as one might expect. That  X  X ourt X  and  X  X onstitutional amendment X  stand out from the other query words implies that T opic 76 is like Topic 57 in that only a few words govern the  X  X abe X  - X  X k9u X  performance difference.

Figure 4, which is comparable to figure 2, is the multidimensional scaling plot for Topic 76. Figure 4 shows that the variation in the return order of relevant documents is much smaller for  X  X abe X  than  X  X k9u. X  We see that  X  X ourt, X  one of the words that stands out in figure 3, influences the return order for  X  X abe. X  This word also influences the return order for  X  X k9u X  but so do many other query words. Looking at  X  X abe X  points B, C, and G, we see that  X  X onstitutional amendment X  also influences the return order for  X  X abe. X  That query H is close to these three points suggests that  X  X abe X  treats this query as though it contained the phrase  X  X onstitutional amendment. X 
Summarizing topic 76, we note that compared to  X  X k9u, X   X  X abe X  excludes irrelevant documents better and is less return-order sensitive unless the query contains  X  X ourt X  or 110 LIGGETT AND BUCKLEY  X  X onstitutional amendment. X  Apparently, the system  X  X abe X  achieves this by weighting  X  X onstitution X  more heavily in formation of document lists. This observation suggests that as in the case of topic 57,  X  X abe X  has a greater ability to grasp the topic despite variations in query wording.
 T opic 85 As the third step in discovery of a topic factor, we consider topic 85. The topic statement is shown in Table 3. For this topic, the number of relevant documents in the collection is 581.
Figure 5, which is comparable to figures 1 and 3, shows the 20 best queries and our irrelevant-exclusion measure for each system and query. Reading the queries in figure 5 suggests that  X  X orruption X  (and its variant  X  X orrupt X ) is the primary key word for this topic. Comparing systems, we see that the performance difference between  X  X abe X  and  X  X k9u X  is positive for all but five of the queries shown. However, on average, the performance difference is smaller for topic 85 than for topics 57 and 76. Looking at the relation between performance difference and query wording suggests that the word  X  X ribery, X  which occurs three times if one includes the variant  X  X ribe, X  improves the performance of both systems. The phrases  X  X ublic official X  and  X  X fficial corruption, X  the separate words in these phrases, SYSTEM PERFORMANCE 111 112 LIGGETT AND BUCKLEY and the words  X  X overnment X  and  X  X olitical X  might influence performance, but the effects are not clear. That the effect of query words on the performance difference is hard to discern implies that topic 85 is unlike topics 57 and 76.

Figure 6, which is comparable to figures 2 and 4, is the multidimensional scaling plot for Topic 85. Figure 6 shows that the variation in the return order of relevant documents is about the same for  X  X abe X  as it is for  X  X k9u. X  We see that  X  X ribery, X  one of the words that stands out in figure 5, influences the return order for both  X  X abe X  and  X  X k9u. X  Looking figure 5.

Summarizing topic 85, we note that the secondary words affect the behavior of both  X  X abe X  and  X  X k9u. X  Apparently, in comparison with  X  X k9u, X  the influence of these secondary w ords is not reduced very much by the  X  X abe X  query expansion. In other words, the way  X  X abe X  treats the key word  X  X orruption X  has little effect. In this way, topic 85 differs from topics 57 and 72.

One might hypothesize a topic factor that governs the differences among topics 57, 76, and 85. When there is a highly specific primary key word, one that effectively, if not SYSTEM PERFORMANCE 113 perfectly, separates relevant and irrelevant documents,  X  X abe X  takes advantage of this word to outperform  X  X k9u. X  The primary key words are  X  X CI, X   X  X onstitution, X  and  X  X orruption. X  The topic factor that governs differences among these three topics might be called the key-57 and lowest for topic 85. The difference in performance between  X  X abe X  and  X  X k9u X  is a function of the level of this topic factor. Although we do not provide the details, we note that topic 62 with primary key word  X  X oup X  fits into this sequence with level of key word specificity between that of topic and 85 and that of topic 76. Apparently, for  X  X abe X  relative to  X  X k9u, X  an ideal topic is one with a single key word that distinguishes relevant from not relevant, and a more difficult topic is one that must be delimited by secondary words. There are, of course, other topic factors that affect the performance difference. 3. Indicators for topic comparison Because system behavior varies widely from topic to topic, one cannot be satisfied with results from three topics when there are fifty available. But extension of the analysis in Section 2 to fifty topics cannot entail a person viewing and comprehending one hundred figures. This difficulty leads us to propose an approach based on a low-dimensional summary computed for each of the fifty topics. 114 LIGGETT AND BUCKLEY
Our overview is based on four indicators of system behavior that can be computed for ev ery topic. Plots of system-to-system differences in the values of these indicators provide the overview. One indicator is the average over the twenty best queries of our measure of irrelevant-document exclusion, which is defined in Section 2. The best twenty queries are chosen as in Section 2. The use of average precision in a similar way would suggest itself to those familiar with the information retrieval literature (see the Appendix). We add to this indicator three others that summarize what is observed in figures like figures 2, 4, and 6. These three figures can be thought of as depicting a point cloud for each system, where the point cloud is derived from a dissimilarity matrix. We can describe the three indicators in terms of these point clouds. The uniformity of return order corresponds to how tightly the points are clustered. The centering of best query corresponds to the difference between middle of the point cloud and the point for the query with best irrelevant rejection. The sphericity of return order corresponds to how circular the point cloud is. As discussed next, we actually define our indicators not in terms of the two-dimensional view given by figures 2, 4, and 6 but in terms of the dissimilarity matrix. Our indicators are summarized in Table 4.

The system-to-system differences for these indicators provide a gauge of topic-to-topic differences. These topic-to-topic differences can be used for finding small groups of topics for which the variation in system behavior is interesting. Indicators that fulfill this purpose must be properly chosen. In particular, they should not reflect properties of the document collection but only properties of the topics, the information needs. An indicator that depends strongly on n R , the number of relevant documents, would not be satisfactory because n R is a property of the document collection, not of the topic. If one were to choose topics for comparison on the basis of an indicator that depended strongly on n R , one might miss topics that could lead to important insights. 3.1. Exclusion of irrelevant documents Figures 1, 3, and 5 display, for each query and each system, the value of  X  log 10 ( N 25 + 1), where N 25 is the number of irrelevant documents returned before 25 percent of the relevant documents. These figures show, for n Q = 20, the n Q best queries with duplicate queries e xcluded. Let j index the n Q best queries (1  X  j  X  n Q ); and let m index the systems (1  X  m  X  6). Let N ( m ) SYSTEM PERFORMANCE 115 indicator for system m is given by Fo r topics 57, 76, or 85, this indicator is just the average over the queries of what is shown for a system in figures 1, 3, or 5, respectively.

As discussed in the Appendix, we choose this indicator instead of one based on average precision to reduce topic-to-topic dependence on n R . Another dependence that we would like to avoid arises from the topic-to-topic variation in the query set available for a topic. Because the queries are natural language expressions, we can try to reduce this dependence bu t cannot hope to eliminate it. We choose n Q = 20 and choose n / n R av eraged over the systems as a gauge for selecting the best queries. Other choices might reasonably be considered. 3.2. Uniformity of return order We now define three indicators related to the patterns observed in multidimensional scaling plots such as figures 2, 4, and 6. We define these indicators in terms the dissimilarities  X  pq given in Section 2.1. We motivate their choice by considering the case in which the system returns every relevant document in response to every query. In other words, we consider the case in which the number of relevant documents returned n j in response to query j equals n
R for all queries. As in Section 2.1, let R ji denote the rank of relevant document i among those returned in response to query j . Let where Fo r the case n j = n k = n R , our dissimilarity measure for system m and queries j and k is given by where 116 LIGGETT AND BUCKLEY To define our indicators, we only need query-to-query dissimilarities for the same system. Fo r the case n j = n R , there is a one-to-one correspondence between the points given by the vectors x j and the dissimilarities (Cox and Cox 2001). This correspondence is the basis for the type of multidimensional scaling called classical scaling. We consider the vectors x j in justifying our choices of indicators.
 Our indicator of uniformity is given by n This equation follows from and The quantity x T j x j can be thought of as the squared distance from query j to the average of the queries. Thus, our uniformity indicator is the negative logarithm of the average of the squared distances of the queries from their center. Defining this indicator without the logarithm would not have a dramatic effect on our results. Using the expression giving the indicator in terms of the dissimilarities, we can compute values for the indicator even when not all the relevant documents are returned. 3.3. Centering of the best query Another aspect of the patterns seen in figures 2, 4, and 6 is the location of the best query, the one that heads the list in our choice of the best n Q queries. We compare the square of the distance between this query and the center with the average for all the queries. Let the index for the best query be j = 1. In terms of the dissimilarities, our indicator of best query centering is SYSTEM PERFORMANCE 117 In terms of the vectors x j for the case n j = n R ,wehave 3.4. Sphericity of return order It is sometimes observed in multidimensional scaling results such as those shown in figures 2, 4, and 6 that the queries for a system are spread out much more in one direction than the direction to the spread of the queries. Consider as the response from a composite query defined by the coefficients  X  j . The distance from this composite query to the center of the queries is given by The maximum of maximum by  X  1 .Wehave by Note that subtracting U m amounts to dividing by the average squared distance before taking the logarithm. Thus, this indicator is normalized by overall query-to-query variation. 118 LIGGETT AND BUCKLEY 3.5. Indicator overview Figure 7 shows, for the exclusion indicator E m and the uniformity indicator U m , the differ-ence between  X  X abe X  and  X  X k9u. X  Only 43 of the 50 topics are shown because we judged the other 7 topics to be less informative. Topics 51, 70, and 78 are so easy in terms of the e xclusion indicator that the  X  X abe X  - X  X k9u X  difference shows little of interest. Topics 74, 75, 84, and 96 are so difficult in terms of the exclusion indicator that they also show little of interest.

Figure 7 shows that in the progression from topic 57 to topic 76 to topic 85, the difference in exclusion of irrelevant documents becomes smaller and the difference in uniformity of relevant-document return order becomes smaller. This is what we observed from figures 1 X 6 in Section 2. Note that topic 62 fits between topics 76 and 85 in this progression. Figure 7 also shows that these three topics provide only a partial picture of the variation in system difference with topic.

Figure 8 shows for the centering indicator C m and the sphericity indicator S m , the dif-ference between  X  X abe X  and  X  X k9u. X  This figure shows that for topics 57, 76, and 85, the system difference varies little in best-query centering and varies appreciably in sphericity. SYSTEM PERFORMANCE 119 these three topics, the system difference in best-query centering varies appreciably. As with figure 7, figure 8 shows that the three topics discussed in Section 2 do not provide a complete picture. 4. Finding topic factors The indicators introduced in Section 3 help us expand on the perceptions formed in Section 2. Through the analysis of topics 57, 76, and 85 in Section 2, we formulated the idea of a this are needed in forming a useful model of the way relative system behavior depends on topic properties. Finding additional topic factors is the purpose of the indicators formulated in Section 3. These indicators provide a guide to further topics the analysis of which is likely to lead to additional topic factors.

The topics that seem likely to provide insight are those on the periphery of the four-dimensional scatter of indicator values shown by the two-dimensional projections in figures 7 120 LIGGETT AND BUCKLEY and 8. Although more topics could be considered, we confine our attention to topics 63, 66, 77, 81, and 98. One reason for this choice is coverage of some parts of the space of indicator v alues by topics discussed in Section 2 or in Liggett and Buckley (2001). We further narrow our attention to topics 77 and 98. Topics 63, 66, and 81 have characteristics that make generalization difficult. For four queries of topic 63, the system  X  X k9u X  has inexplicably poor performance. For topic 66, the assessor made a mistake in assigning relevance judge-ments to documents. For topic 81, system behavior is governed by the names of television ev angelists. Study of system behavior when some queries involve the names of individuals w ould be interesting, but we are not going to pursue such a study in this paper.
The topic statement for topic 77 is shown in Table 5. We note that the actions of the poacher rather than the impact of poaching on the larger community are of interest. The number of relevant documents in the collection is 75.

Query by query exclusion of irrelevant documents for topic 77 is shown in figure 9. The primary key word for this topic is  X  X oaching, X  which has variants  X  X oach X  and  X  X oacher. X  We see that although the difference between  X  X abe X  and  X  X k9u X  is positive for 18 of the 20 queries, the difference is generally small. Moreover, there are no secondary key words with effect that is obvious in this figure.

The multidimensional scaling plot for topic 77 is shown in figure 10. The return order of relevant documents is more uniform for  X  X abe X  than  X  X k9u. X  The effect of the secondary ke yw ord  X  X ildlife X  is evident in this figure, especially in the points for  X  X k9u. X  Topic 77 stands out in figure 8 because of the response of  X  X k9u X  to queries R and T. These two queries contain the word  X  X oacher X  rather than  X  X oaching. X  If one were to say that the behavior of  X  X k9u X  should not change much when  X  X oacher X  is substituted for  X  X oaching, X  SYSTEM PERFORMANCE 121 then one might say  X  X k9u X  has a stemming problem that  X  X abe X  does not have. Stemming is removing affixes to reduce a word to its root form (Berry and Browne 1999). Overall, we see that the response of  X  X abe X  is less sensitive to the secondary words in the query but that this results in only a small improvement in system performance as gauged by exclusion of relevant documents.

T opic 77 suggests that failures in stemming or similar failures in equating equivalent w ords constitute a topic factor not observed in topics 57, 76, or 85. Interestingly, system behavior for topic 86 discussed by Liggett and Buckley (2001) shows a similar failure. In this case, the failure is in equating FDIC with Federal Deposit Insurance Corporation. Topic 86 is close to topic 77 in figure 8.

The topic statement for topic 98 is shown in Table 6. We note that this topic involves the production aspects of fiber optics, not the application aspects. The number of relevant documents is 363.

Fo r topic 98, figure 11 shows the queries and irrelevant-document performance. The  X  X abe X  and  X  X k9u X  is positive for only 9 of the 20 queries and that the average of this difference is small. Secondary query words  X  X ables X  and  X  X quipment X  have some effect. 122 LIGGETT AND BUCKLEY SYSTEM PERFORMANCE 123 Fo r the three queries with  X  X ables, X   X  X k9u X  performs better, and for the six queries with  X  X quipment, X   X  X abe X  performs better.

The multidimensional scaling plot for topic 98 is shown in figure 12. We see that with the exception of queries A, C, and E, the return orders for  X  X abe X  are very tightly clustered. Queries A, C, and E are themselves tightly clustered. In comparison, the points for  X  X k9u X  are spread out over the plot with queries A, C, and E separated from the others as with  X  X abe. X  Queries A, C, and E contain the word  X  X ables. X  For  X  X k9u, X  one can also see that queries B, D, F, G, J, and K, which contain the word  X  X quipment, X  are separated from the others. Apparently,  X  X abe X  achieves considerable return order uniformity but with no effect on its exclusion of irrelevant documents relative to  X  X k9u. X  Seemingly,  X  X abe X  weights  X  X iber optic X  heavily, but this is ineffective because the primary key word does not faithfully separate relevant documents from irrelevant ones.
 topic 85,  X  X abe X  does not heavily weight the primary key word  X  X orruption, X  and therefore its irrelevant document performance is not much better than that of  X  X k9u. X  For topic 98,  X  X abe X  heavily weights the primary key word  X  X iber optic, X  but this word does not distinguish relevant and irrelevant documents well enough for this weighting to be effective in irrelevant document performance relative to  X  X k9u. X  Thus, topics 85 and 98 illustrate different ways that other topics can contrast with topic 57. 124 LIGGETT AND BUCKLEY 5. Conclusions This paper shows that distinguishing topics in terms of topic factor levels has an empirical basis. Thus, this paper supports using topic factors in thinking about the effect of topic-to-topic variation on relative system behavior. However, the empirical basis provided is limited. More data and more analysis might be required before topic factors can be applied in a particular situation.

Our primary example of a topic factor is key word specificity. As discussed, the key w ords  X  X orruption, X   X  X oup, X   X  X onstitution, X  and  X  X CI X  are ordered by increasing specificity. Another of our examples is the topic factor stemming failure. Our analysis suggests further e xamples. It seems clear that there are other topic factors. These additional factors might appear if a similar comparison of  X  X abe X  and  X  X k9u X  were performed on another set of topics. Even more likely, additional factors might appear if other pairs of systems were compared.

Consider IR system evaluation, that is, decisions about what system will serve the cus-cost limits the number of topics. Evaluation requires not only a clear description of each topic but also assessment of each document in the collection as relevant or irrelevant to each SYSTEM PERFORMANCE 125 topic. Professionals experienced in organizing information develop the topic descriptions and assess the relevance of documents. Assessment is performed without looking at every document in the collection. Nevertheless, substantial increase from the 50 topics for which assessments have been done each year in TREC would be prohibitively expensive. Thus, e xpanding the size of the topic sample is not an option. The third reason is that probability sampling in selection of topics seems impractical because the selection of topic descriptions is subject to constraints. The descriptions must be unambiguous regarding the relevance or irrelevance of documents. Documents appropriate to the topics must be present in the collection. Moreover, topics must be chosen in a way that leads to efficient use of the infor-mation professionals X  time. Because of these constraints, a practical approach to defining a topic frame and performing probability sampling from this frame seems difficult.
T ypically, IR system evaluation is based on the system performance averaged over the av ailable topics. Information on topic factors provides an alternative. Say that the topic av ailable topics can be considered as representative. In this case, adjustment can be based on topic factors if topic factors descriptive of the difference between the two populations are av ailable. Adjustment would consist of placing increased weight of topics of more interest to the user.

What more could we learn if we had more topics run with multiple queries for each topic? Obtaining multi-query responses for more topics is feasible because there are more sets of 50 topics in the TREC collection. For these topics, all that is necessary is generating by the number of topics available and that more topics would lead to more clarity on topic f actors.
 A ppendix: Average precision In the notation of Section 2, the value of the average precision for a 1000-document list is given by When one uses this measure to compare topics, one should be concerned with the depen-dence on n R , which is a property of the document collection rather than of the content of the topic. The dependence on n R can be shown by asking what would happen if half the relevant documents were selected at random and removed from the document collection. The precision, which is the ratio of the number of relevant documents retrieved to the total number of documents retrieved, would be approximately cut in half. This applies, then, to av erage precision as well. This dependence on n R is important in comparing system out-puts for different topics but the same document collection. This dependence may not be of ov erriding importance in other uses of average precision. A paper by Liggett (1999) can be usefully read as a comparison of average precision and the measure of irrelevant rejection defined in Section 2. 126 LIGGETT AND BUCKLEY
We have followed the usual statistical practice in choosing a measure for which depen-dence on n R can be largely ignored (Bartholomew 1996). In standard statistical practice, the choice would be the number of irrelevant documents returned before 50 percent of the relevant documents. We choose 25 percent instead because some document lists do not con-tain 50 percent of the relevant documents. Consider the dependence on n R for our measure of exclusion of irrelevant documents. The number of irrelevant documents returned before 25 percent of the relevant documents is roughly r ( H ) , where H  X  . 25 n R ,i fw e ignore the relevant documents included in r ( H ) . Say that half the relevant documents are removed at random. The value of H becomes . 25 n R / 2. The ranks of the remaining relevant documents are nearly the same but their numbering corresponds roughly to i / 2. Thus, the rank of the relevant document at which 25 percent of the relevant documents are returned does not change very much. In other words, the number of irrelevant documents returned before 25 percent of the relevant documents is largely independent of n R .

In the comparison of  X  X abe X  and  X  X k9u X  given in this paper, our substitution for average precision does not seem particularly consequential in light of the topic-to topic variabil-ity. In a comparison of more closely matched systems, this substitution might be more important. We computed query means of the average precision for the same 20 queries we use to compute our indicator of irrelevant document exclusion. Figure 13 shows the SYSTEM PERFORMANCE 127 av erage-precision means plotted versus our exclusion indicator. We see that average pre-cision does not invalidate our choice of topics for detailed study. In the four dimensional space of indicators shown in figures 7 and 8, topics 57, 76, 85, 77, and 98 would remain in the same neighborhoods were average precision used instead.
 Disclaimer order to describe an experimental procedure or concept adequately. Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the entities, materials, or equipment are necessarily the best available for the purpose.
 References 128 LIGGETT AND BUCKLEY
