 In the past five decades, linear programming, which is formulated with a single criterion, has been widely used in many real world problems. However, this great advancement has some inherent limitations. In most practices, there are generally multiple objectives which need to optimize. Further more, these objectives may even be conflicting with each other. The Bias-Variance dilemma (Wassaman 2005) in statistical inference and Fitness-Generality trade-off (Han and Kamber, 2000) in data mining are two famous examples which can not be well addressed within the scope of single criterion programming. In statistical inference, the best estimator or criteria can not be minimized in the same time, because the decrease of the bias will increases the variance. In data mining, the classifier with low training error (fitness) and good generalizability (performance on unseen data) is highly desired. However, the over-fitness will almost sure weaken the generalizability of the classifier. More-over, unlike the Bias and Variance criteria wh ich can be integrated together as Risk alizability can not be simply combined together since they have different measure-ments. Another example which will be investigated in detail throughout this paper are two commonly used criteria among them. The first one is the overlapping de-gree with respect to the discriminate boundary. The lower of this degree the better boundary. The larger the sum of these distances the better the classification is. Ac-cordingly, the objective of a classification is to minimize the sum of the overlapping degree and maximize the sum of the distances (Freed and Glover, 1981; Freed and Glover, 1986). Unfortunately, these two criteria can not be optimized simultane-ously because they are contradictory to each other. Thus, the multi-criteria pro-gramming can be used to overcome this kind of problems in a systematical way. gramming. During these years, the multi-criteria programming has been not only improved in theoretical foundations but also applied successfully in real world prob-programming have been widely adopted by the researches for classification, regres-sion, etc. On this basis, Jing et al. (2004) introduced the fuzzy approach in the multi-measure the overlapping degree and distance, Kou (2006) presented the Multiple Criteria Quadratic Programming for data mining. Kou et al (2002) proposed Multi-Group Multiple Criteria Mathematical Programming aimed to handle the multi-group (2008) developed a regressing method based on this technique. The development of this family of multi-criteria data mining technique is described in Figure 1. 
These multi-criteria data mining techniques also have yielded fruitful results in di-verse real world applications. Kou (2006) applied the Multiple Criteria Quadratic Programming to Credit Card Risk Analysis and get comparable results with some sophisticated methods. Classification of HIV-1 Mediated Neuronal Dendritic and Synaptic Damage is another successful example of the multi-criteria data mining work Surveillance and Intrusion Detection System. The multi-criteria data mining has also been applied to prediction bankruptcy by Kwak et al. (2005). Zhang et al. (2007a, 2007b) employed the Multiple-Criteria Linear and Quadratic Programming for VIP E-Mail Behavior Analysis. their applications, a survey paper will be helpful to understand and use this family of data mining techniques more easily. The particular objective of this paper is to closely conclusions and future works are presented. The variations of multi-criteria linear programming methods in data mining are briefly introduced and analyzed in this section. 2.1 Multiple Criteria Linear Programming objectives. The first one is to maximize the minimum distances of observations from the critical value. The second objective separates the observations by minimizing the sum of the deviations (the overlapping) among the observations (Freed and Glover, MMD and MSD simultaneously, the best tradeoff of two measurements is difficult to find. This shortcoming has been coped with by the technique of multiple criteria lin-ear programming (MCLP) (Shi et al, 2000; Shi, 2001). The first MCLP model could be described as follows: discriminator and n is the number of samples. The weights vector w and the bias b are the un-known variables need to be tuned to optimize the two objectives. A visual description of this model is shown in Figure 2. 
The Model 1 is formulized as the Multiple Criteria Linear Programming which is approach (Shi, 2000; Shi and Yu, 1989) can be employed to reform the above model  X  +  X  *; otherwise, it is 0. If - X   X   X  evolved as: model is shown as in Figure 3. 
In order to calculate a huge data set, the Linux-based MCLP classification algo-rithm was developed to implement the above Model 2 (Kou and Shi, 2002). 2.2 Multiple Criteria Quadratic Programming Based on MCLP, the Multiple Criteria Quadratic Programming is later developed to model: defined theoretically. Let where H and Q are predefined as identity matrices . We formulate a simple quadratic programming with 2-norm as in Model 4 :
In order to reduce the number of variables involved in our model and thus simplify classified records and i i  X   X   X  = for all correctly separated records. To obtain strong convexity to the objection function, we add 2 (Kou, 2006): 2.3 Multiple Criteria Fuzzy Linear Programming Instead of identifying a compromise solution for the separation of data in MCLP, the fuzzy linear program: 
Note that Model 6 will produce a value of  X  with 1 0 &lt;  X   X  . To avoid the trivial in the FLP approach becomes the standard of determining the classifications between Good and Bad records in the database. A graphical illustration of this approach can be seen from Figure 3, any point of hyper plane 1 0 &lt; &lt;  X  over the shadow area repre-sents the possible determination of classifications by the FLP method. 2.4 Multi-group Multiple Criteria Mathematical Programming The above models are concerned with two groups X  case. Now suppose we have k w = m T m R w w  X  ) ,..., ( 1 be a vector of real number to be determined. Thus, we can establish the following linear inequations (Kou et al., 2002): 
A mathematical function f can be used to describe the summation of total overlap-ping while another mathematical function g represents the aggregation of all dis-depend on simultaneously minimize f and maximize g . Thus, a generalized bi-criteria programming method for classification can be formulated as: process of identifying the optimal solution. As a result, the generalized model can be converted into a single-criterion mathematical programming model as: ( Model 7 ) 
Here x i is given, w and b j are unrestricted, and j i ,  X  , 0 ,  X  j i  X  , . 1 n i  X   X  (a) and (b) are defined as such due to the fact that the distances from any correctly limitation on j i  X  : 
Let p = 2, then objective function in Model 1 can now be a quadratic objective and we have: ( Model 8 ) 
Note that the constant 2 1 ) fect to the solution. 2.5 Regression Method by Multiple Criteria Linear Programming In order to apply MCLP to regression problem, it is necessary to construct the data set = , where m D  X  data sets in section 3, the + model is constructed. And with these data sets, MCLP regression model can be writ-ten as follows (Zhang et al. 2008) : ( Model 9 ) 
Aggregation of Good samples: 
Aggregation of Bad samples: The Multiple Criteria Mathematical Programming approaches have been successfully used for many real world data mining applications, such as Credit Card Risk Analysis, Classification of HIV-1 Mediated Neuronal Dendritic and Synaptic Damage, Network section, we introduce the examples of the utilization of the proposed models men-tioned above. 3.1 Credit Card Risk Analysis The credit card dataset is obtained from a major US bank. The data set consists of 65 attributes and 6000 records. There are two groups: current customer and bankrupt potential risk of bankruptcy. Each record in these data has a class label to indicate its X  financial status: either Normal or Bad. Bad indicates a bankrupt credit or firm account (Kou, 2006) is compared with four well-known classification methods: SPSS linear discriminant analysis (LDA) (SPSS 2004), Decision Tree (See5.0), SVM light (Joachims, 2004), and LibSVM (Chang and Lin, 2001). A standard 10-fold cross-validation is performed in this application. As a result, the overall classification accu-racies of LDA, See5, SVMlight, LibSVM and MCQP are 78.79%, 75.00%, 74.00%, 77.57% and 78.50% respectively. The experimental study indicates that MCQP can tion techniques. 3.2 Classification of HIV-1 Mediated Neuronal Dendritic and Synaptic Damage The ability to identify neuronal damage in the dendritic arbor during HIV-1-programming to classify the HIV data (each with nine attributes) produced by labora-tory experimentation and image analysis. The dataset includes four classes: (1) treat-ments with brain derived neurotrophic factor (BDNF), (2) glutamate, (3) gp120 or (4) non-treatment controls from the experimental systems. The HIV database contained data from 2112 neurons. Among them, 101 are from G1, 1001 from G2, 229 from G3, {G2 vs. G4} and {G3 vs. G4}. The conclusion on this comparison is that the proposed MCLP method can be competitive in neuronal injury classification since it showed stronger performance than the back propagation neural networks classifier in three of four class pairs. 3.3 Network Surveillance and Intrusion Detection System compromise computer networks (Kou et al. 2004; Kou, 2006). The early and reliable detection of network attacks is a pressing issue of today X  X  network security. Classifi-used in this application is KDDCUP-99 data set which was provided by DARPA in are four main categories of attacks: denial-of-service (DOS); unauthorized access from a remote machine (R2L); unauthorized access to local root privileges (U2R); KDDCUP-99 as a four-group classification problem. The four groups are DOS cords), and normal activity (812813 distinct records). The overall classification accu-racy, which is defined as the average of each class X  X  accuracy, of See5, MCMP, and MCMP with kernel is 93.08%, 95.71%, and 97.2%, respectively. MCMP with kernel outperforms See5 and MCMP in every class, especially for Normal class. The high classification accuracy of MCMP with kernel leads to low false alarm rates. 3.4 Bankruptcy Prediction Bankruptcy prediction is an interesting topic because many stakeholders such as bankers, investors, auditors, management, employees, and the general public are in-predictor variables perform better than Altman X  X  (1968) predictor variables using 77.70% and 70.27%, respectively, using Ohlson X  X  (1980) predictor variables. 
Kwak et al. (2005) also tested the use of multiple criteria linear programming (MCLP) model to data mining for bankruptcy prediction using U.S. data from a sam-ple period of 1992 to 1998 and concluded that the MCLP approach performed better than either the MDA approach of Altman (1968) or the logit approach of Ohlson (1980). Kwak et al. (2005) report overall prediction rates of 88.56% and 86.76% for the Altman (1968) and Ohlson (1980) models, respectively, and Type I prediction rates of 46% and 43%, respectively. 3.5 VIP E-Mail Behavior Analysis total market size of Chinese VIP E-mail service has reached 6.4 hundred million RMB by 2005. This huge market dramatically enforced market competition among all duce the customer loss rate. To this end, Zhang et al. (2006) applied the MCLP ( model 1 ) to the charged VIP E-mail service analysis. Through cross-validation process, they demonstrated that MCLP is highly accurate and stable on VIP E-mail dataset. These results from major-single excellent voter by noticeable margin. On these basses, They concluded that the application of MCLP to VIP E-Mail behavior classification is success and MCLP is accurate and dependable for categorizing VIP E-Mail dataset. 
MQLC, a variation of model 3 , has also been proposed by Zhang et al. (2007) to model is extremely stable for multiple groups of randomly generated training set and testing set. The comparison of MQLC and Decision Tree in C5.0 indicated that MQLC performs better than Decision Tree on small samples. gramming in data mining. Different variations of multi-criteria programming models are proposed to solve the many problems, su ch as classification/prediction and regres-sions. The history of these optimization based data mining methods has been revisited and illustrated with mathematical formulations and real world applications. methods. 
To extend the applications of multi-criteria data mining techniques, there are sev-eral problem are still in need of further investigation. The optimum of the parameter b is the one that demands extensive research since it is essential to improve the classifi-convex combination will lead to a better cla ssifier. Many efforts have been devoted to computational loads in this method need address. Another direction of great impor-potential method can be used to this end is the decision risk estimation based on the statistical learning theory. Acknowledgments. This work was partially supported by National Natural Science Foundation of China (Grant No.70621001, 70531040, 70501030, 10601064, 70472074), National Natural Science Foundation of Beijing (Grant No.9073020), 973 Project of Chinese Ministry of Science and Technology (Grant No.2004CB720103), and BHP Billiton Cooperation of Australia. The Web offers access to large amounts of heterogeneous information and allows this information to evolve any time and in any way. These evolutions take two general forms. The first is existence. Web pages (static as well as dynamic) and Web sites exhibit varied longevity pattern. The second is structure and content modification. Web pages replace its antecedent, usually leaving no trace of th e previous document. These rapid and often unpredictable changes to the information create a problem of detecting, monitoring, and analyzing these evolutions. This is a cha llenging problem because information sources in the Web are autonomous and typical database approaches to detect these changes based on triggering mechanisms are not usable. Moreover, these information sources are semistructured or unstructured in nature and hence traditional evolution management techniques for structured data (relational) cannot be used effectively.
 This tutorial offers an introduction to issues involving evolution management on the Web. We motivate the necessity for managing evolution and give an overview of the evolutionary features of the Web. Next, we id entify various research issues involved in evolution managemen t. Specifically, our discussion can be categorized into three main components: (a) detection of changes/evolutions (b) querying over evolutions and (c) an-alyzing or mining evolutions of data. The tutorial session also reveal various application domains of evolution management systems (such as social networks, blogs, and Web event detection). We conclude by identifying potential research directions in this area.
This tutorial is intended for both engineers and researchers. The former will learn about solutions to use and hard problems to avoid. The latter will get a snapshot of the research field and problems that are worth tackling next. The tutorial consists of the following topics.
 1. Introduction and motivation: This section includes a brief overview on the dy-2. Evolution management problem and characteristics: In this section we formally 3. Change detection and representation: The first step for evolution management 4. Querying evolutions: Next, we discuss research activities in querying evolutions. 5. Mining evolution of Web data: Finally, we explore works that propose novel data 6. The road ahead: We expose potential research issues in managing evolutions in Sourav S Bhowmick and Sanjay Madria have published several papers in the area of Web evolution management. One of Sour av X  X  Web evolution management paper received Best Interdisciplinary Paper Award at the ACM CIKM 2004. Biographies of Sourav and Sanjay can be found at www.ntu.edu.sg/home/assourav and web.mst.edu/  X  madrias/ , respectively.
 According to Gartner, human data-entry errors, and lack of proper corporate data standards result in more than 25 per cent of critical data used in large cor-porations to be flawed. While the issue of data quality is as old as data itself, it is now exposed at a much more strategic level, e.g. through business intelligence (BI) systems, increasing manifold the stakes involved. Corporations routinely operate and make strategic decisions based on remarkably inaccurate or incom-plete data. This proves a leading reason for failure of high-profile and high-cost IT projects such as Enterprise Resource Planning (ERP), Customer Relationship Management (CRM), Supply Chain Management (SCM) and others. According to an industry survey [1], the presence of data quality (DQ) problems costs U.S. business more than 600 billion dollars per annum.

As the response to the urgent call to prev ent semantic/syntactic data conta-mination, we can see a rising interest from research community and increasing presence of data quality related papers in prominent conferences and journals. At the same time the industry response has resulted in dedicated tools and tech-nologies to provide reliable data quality control methods. In this tutorial, we will deliver a comprehensive and cohesive o verview of the key issues and solutions regarding the Data Quality (DQ) problem . It will cover general aspects of the problem space as well as the most significant research achievements in key topics. Since the most recent tutorial [2], there have been several new developments.
In summary, the aims of the tutorial are to: (1) Presenting the full picture of data quality control in front of wider database research community in a concise and holistic way. (2) Discussing the unique set of challenges that the practices of data quality control are faced with, being aware of gaps in industry needs and existing solutions, and improving a better understanding of how to ideally cope with data quality problem. (3) Passing the insight knowledge and the latest advances in this domain and inspiring thinking with an interactive discussion on the major concerns and future trends.  X  Introduction: We will briefly introduce the history of the DQ problem and  X  Running Example: Prior to formal discussion, examples based on the real  X  Problems Classification: This section will present and discuss various flavours  X  Solutions Classification: We will discuss the key techniques, research achieve- X  Discussions: The current DQ control systems have provided significant ad-This tutorial is tailored to suit researchers with interest in the data quality control from both academia and industry. They are business analyst, solution architects and database experts, statisticians, data managers, and practitioners. Shazia Sadiq is currently Senior Lecturer in ITEE School of UQ. Her main research interests are innovative solutions for enterprise information integration, which include in particular Business Process Management Systems, Service Ori-ented Architectures, Wor kflow Systems, advanced messaging technologies, and deployment of large scale distributed devices. Shazia is leading a large Australian Research Council funded project on Data Quality.
 Xiaofang Zhou is a Professor of Computer Science at the University of Queensland. He is the Head of the Data an d Knowledge Engineering Research Group and the Convenor of ARC Research Network in Enterprise Information Infrastructure (EII), and a Chief Investigator of ARC Centre of Excellence in Bioinformatics.

Ke Deng is a research fellow in ITEE School of UQ. His current research areas focus on the data quality control, including record linkage, similarity text join, etc.

