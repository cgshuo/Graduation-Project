 applications, how to identify the minimal, or close to minimal, subset of variables that best predicts T is critical to the success, and this procedure is known as feature subset selection. 
A principle solution to the feature selection problem is to determine a subset of fea-[1,2,3]. Koller and Sahami (KS) [2] first showed that the Markov blanket (MB) of a given target variable T is the theoretically optimal set of features to predict T  X  X  value, although Markov blanket itself is not a new concept and can be traced back to the work of Pearl[11]. In other words, the Markov blanket of T is the minimal set of variables conditioned on which all other variables are probabilistically independent of the tar-get T , denoted as () MB T . Based on the findings that the full knowledge of () MB T is variables become superfluous, we normally can have a much smaller group of vari-simpler model, but without scarifying classification performance[2, 3, 4]. 
Since KS X  X  work in 1996, there are several attempts to make the learning procedure more efficient and effective, including GS (Grow-Shrink) [5], IAMB (Iterative Asso-ciative Markov Blanket) and its variants [1, 3,4,6], MMPC/MB (Max-Min Parents and Children/Markov Blanket)[3], HITON-PC/MB [7], PCMB(Parent-Child Markov Blanket learning) [1] and the more recent on e BFMB (Breadth-First search of Markov Blanket) [8]. To our knowledge, this list contains all the published algorithms. In this article, we will discuss these MB local learni ng algorithms in terms of theoretical and practical considerations, based on our expe rience gained from both academic research and industry implementation. In section 2, a brief introduction to these local learning algorithms is presented. Then, in section 3, we choose some of them, and go a little deeper by comparing their characteristics and pointing out relative merits. We conclude with a short conclusion and what our choice in one project is in SPSS Inc. Pearl is the first one to define the concept and study the property of Markov blanket in his early work on Bayesian network [10]. Following this work, Koller and Sahami proved that the Markov blanket of a given va riable is the theoretically optimal set of features to predict its value [2]. They al so proposed an information entropy-based searching algorithm (generally denoted as KS by their initials) which accepts two parameters: (1) the number of variables to retain, and (2) the maximum number of variables the algorithm is allowed to condition on. Obviously, it is a heuristic and approximate algorithm in its nature, and pr ovides no theoretical guarantees [1,3]. Therefore, although they two pointed out a promising direction, the algorithm itself is not guaranteed to succeed. 
The GS algorithm [5] was proposed to induce the Bayesian network (BN) via the discovery of local neighbours. The authors aim to construct a BN by first identify-ing each node X  X  Markov blankets. Like th e constraint-based learning algorithms, e.g. PC[9], GS depends on two basic assumptions, faithfulness(see Definition 1 below) and correct or reliable conditional independence (CI) test. Here, the second assumption is required in practice since only when the number of observations is enough, the result of statistical testing would be trustable. More discussion on this can be found in Section 3.3. Actually, th ese two assumptions are also the basis of the following algorithms. As its name indicates, GS proceeds in two steps, growing greedily first then shrinking by removing false positives. It is the first algorithm proved correct, but it is not efficient and can X  X  scale to large scale applications. However, the correctness of the algorithm makes it a proven subject for future research. Definition 1 (Faithfulness). A Bayesian Network G and a joint distribution P are faithful to one another, if and only if every conditional independence relationship encoded by G is also present in P [9]. IAMB [4], was proposed in 2003 for classification problems in microarray research where thousands of attributes are quite commo n. It is an algorithm based on the same two assumptions of GS, sound in theory and especially simple in implementation. IAMB actually is a variant of GS, consisting of two phases  X  grow and shrink, but it reorders the set of attributes each time a ne w attribute enters the blanket in the grow-ing phase based on updated CI testing results, which allows IAMB to perform better than GS since fewer false positives will be added during the first phase [3,6]. 
In spite of the improvement, IAMB is not data efficient since its CI tests may con-noticed by its authors, and several variants of IAMB were proposed, like interIAMB and IAMBnPC [4], for a smaller conditioning set, in its maximum, by interleaving the growing-shrinking phases or using PC for the backward phase, respectively. Empiri-cal study shows that IAMB and its variants outperform GS on average in accuracy [4]. Fast-IAMB [6] is another published known work by the author of GS, but there is only gain in speed as reported, no fundamental difference. Among all the algorithms for local learning of MB, IAMB is the most referred one among the family of MB local search algorithms, which can be explaine d by the fact that only after its success, more attention was attracted to this topic. 
Although several IAMB X  X  variants were proposed to improve IAMB X  X  limit on data efficiency, none of them are known as thorough solution with impressive per-formance. This situation was finally changed upon the introduction of MMPC/MB and HITON-PC/MB, which work differently from IAMB by including the underlying topology into consideration. As we know, given two nodes of a Bayesian network, e.g. T and someone ( ) XMBT  X  , if they are conditional independent, i.e. d-separated [9], the necessary conditioning set or separating set rarely have to be the whole even when the underlying graphical model is just a tree. MMPC/MB and HITON-PC/MB make full use of the topology by dividing the search procedure into finding T  X  X  parents/children first, and then discover the remaining variables belonging to ways correct by [1], but they do sugges t a correct approach for following work. PCMB [1] and BFMB [8] are examples of this work. 
Following the idea of MMPC/MB and HITON-PC/MB, PCMB was also proposed to conquer the data efficiency problem of IAMB, and, more importantly, it is proved correct theoretically. Like IAMB, PCMB can scale well to thousands of features [1], but it is known as much more time-consuming [8]. Along with PCMB, another de-rived IAMB algorithm is proposed by the same authors, called KIAMB. It requires no faithfulness assumption, and works in a stochastic manner by allowing users to trade off between greediness and randomness in the search procedure [1]. 
BFMB is the most recent progress reported on this topic, aiming at even better per-formance than PCMB. It has a similar framework to MMPC/MB, HITON-PC/MB and PCMB by recognizing firstly all T  X  X  parents and children; then, it repeats the search with each node found as target, l ooking for their own parents and children which contain the spouses of T . Its name comes from the nature of its search proce-dure, finding T  X  X  neighbours and then the neighbours X  neighbours. In an empirical study [8], BFMB is demonstrated to outperform PCMB, not only on data efficiency interesting progress, so we include it in this study as well. 
Traditionally, given the faithfulness assumption, a whole Bayesian network (BN) can be learned over T and all other features first, from which we can get the ( ) MB T . This has long been known as tedious procedure that is fought with a number of dec-ades [1,3,4,8], say nothing of making them to scale well in large applications. For instance, the publicly available version of the PC[9] and TPDA[12] algorithms accept datasets with only 100 and 255 variables respectively, indicating their expectations on the scalability. By contrast, these local learning algorithms, like IAMB, PCMB and BFMB, all claim to scale well to thousands of attributes. So, this is a meaningful work worthy of researchers X  effort, and it is very promising in real applications. In the previous section, we briefly cover the known published algorithms for local learning of Markov blanket in a chronological order, as an introduction to the topic. In this section, a comprehensive and detailed comparison on some of these algorithms will be presented, including their relative advantages. We leave out the algorithms that are not proved correct (KS, MMPC/MB , HITON-PC/MB). IAMB and its variants will be grouped together but still denoted as IAMB in the remaining text because they are not theoretically different and no obvious gain from IAMB X  X  variants, with the exception of KIAMB since it relies on different assumption. Therefore, our discussion will contain IAMB, PCMB, BFMB and KI AMB, and the content will cover their theoretical assumption, time efficiency, data efficiency, and scalability. 3.1 Theoretical Assumption IAMB, PCMB and BFMB are built on two assumptions: (1) faithfulness and (2) cor-rect CI test. These two assumptions also cons truct the basis for all CI test based learn-ing algorithms for BN. Different from them, KIAMB only requires correct CI test. 
Given the faithfulness assumption, there will be a unique Markov blanket corre-sponding to given variable T [1, 3, 10]. By removing this assumption, the Markov blanket of T doesn X  X  have to be unique, and KIAMB was proposed to work in such condition where higher complexity is expected due to the introduction of additional uncertainty. It works by choosing the candidate in the growing phase in a random Although its authors say that KIAMB outperforms IAMB and PCMB in their study, we feel skeptical on this result. The algorithm will return different ( ) MB T , but, as we understand, this will only happen when the size of data is not large enough to support correct CI tests with whole ( ) MB T as conditioning set. Its random choice will indeed allow it to discover a larger number of true positives, because possibly attributes with smaller number of categories can be selected first and this will reduce the number of degree of freedom of the conditioning set and therefore allow the algorithm go further problem because of enough data, KIAMB will produce the same outcome like IAMB. Actually, KIAMB may work with worse result than IAMB sometimes if, unfortu-nately, attributes with bigger number of cate gories may be selected first. Besides, we can X  X  see that KIAMB can outperform PCMB through such a stochastic manner since it is not a fundamentally different solution from IAMB to solve the data efficiency problem. 
We reach this conclusion based on our analysis on the algorithm X  X  design. Though teresting variant to IAMB, and it is expected to have better performance than IAMB averagely. Conclusion 1: KIAMB can be viewed as a stochastic variant of IAMB, and it is ex-pected to work with more accurate result, by average, than IAMB when the data is limited. Its requirement of no faithfulness assumption is valuable progress. 3.2 Time Efficiency Time efficiency is normally measured in terms of the real length of time consumed, but this measure is known as machine-and implementation-dependent. Therefore, researchers of CI-based learning algorithms often employ the number of CI tests con-sumed as a measure of time complexity [ 11]. Among those articles about local learn-passes ( X # rounds X  in the original paper) required when the authors study the relative time complexity of IAMB, PCMB and BFMB. We believe these two measures are more valuable reference to applican ts for reference considering that: 1. They are machine-independent, and are mostly determined by the design of al-2. Number of data passes, although rarely be referred, can be much more influen-3. Ideally all necessary statistics can be co llected in one data pass, and be cached 4. If it exceeds the memory limit, some have to be placed in second-level storage 
Therefore, in real implementation, we choos e to collect the statistics in successive scanning can be very time-consuming, an algorithm requiring fewer data passes should be preferred. Our comparison of IAMB, PCMB and BFMB here will based on these two indexes, number of data passes and CI tests required to finish the learning of the ( ) MB T . Among them two, the number of data passes should be given more attention based on our discussion above. 
IAMB X  X  algorithm is quite simple, not only in theory but in implementation. In its growing phase, it repeatedly adds th e most promising candidates into ( ) MB T in a for the CI tests; and only one data pass is necessary for the shrinking step because we number of data passes that IAMB needs scal es linearly with the number of true posi-tive in ( ) MB T , and how many false positives are wrongly recognized in the growing phase. 
PCMB is known more data efficient th an IAMB because the algorithm use the knowledge of underlying topology, given the faithfulness assumption. Readers can the original function names here for convenience [1]. PCMB firstly depend on some possibly are false positive. Within GetPCD , the algorithm repeatedly checks the data-pass consuming operation considering possible conditioning sets ranging from 0 to some particular size, and it is not good idea to collect all possible statistics in one data pass. For each candidate X  X  () GetPCD T , it has to be double checked if T known to be time consuming. If we continue this analysis, we find that for a specific mate the time complexity of PCMB to be at least 2 (| ( ) | 1) PC T + the time of BFMB X  X  whole framework is quite similar to MMPC/MB, HITON-PC/MB and PCMB since they follow the same strategy, finding ( ) PC T first, then iteratively look for PC for each ( ) XPCT  X  since some of them are the remaining part of final ent with T by conditioning a set including their common children with T [1, 3, 8, 9]. Different from PCMB, BFMB X  GetPC procedure, RecognizePC as named by the au-thor, works in a smart way by borrowing an idea from known the PC algorithm [9]. It realizes that those directly connected to T are T  X  X  parents/children, so it tries to filter out those nodes which are not directly connected to T by conditional independence test with conditioning set starting from size 0. Those which remain linked to T are expected when the conditioning set size is specified, so we can collect all we need in one data pass. Comparing with PCMB, BFMB filters out as many as possible true negatives given different conditioning set size with one data pass, not several data passes to recognize each single true positive separately as in PCMB. Thus, BFMB is expected to require much fewer number of data passes and CI tests, which is proved RecognizePC , like PCMB, but each RecognizePC is known to be much more time efficient than GetPC . 
Though BFMB is much more time efficient than PCMB, it still looses to IAMB on this point. However, as we will see in next section, BFMB is much more data efficient than IAMB, reaching a higher accuracy rate of learning than IAMB over the same data set [8]. Besides, low data efficiency  X  X elps X  IAMB to run  X  X aster X  in practice size and the data available is not enough for reliable CI tests, but PCMB and BFMB can postpone this happening due to their data efficiency, doing more CI tests and going further (see more discussion in section 3.3). Conclusion 2: IAMB is the most time efficient, and KIAMB is somewhat slower than IAMB due to its stochastic procedure. Comparing with PCMB, BFMB is more time efficient by requiring much fewer data passes and CI tests . Though IAMB works fast, it doesn X  X  bring as high accuracy as PCMB and IAMB. 3.3 Data Efficiency All these Markov blanket learning algorithms require correct CI test. In practice, these algorithms will perform a test if the CI test is reliable and skip it otherwise [1,3,4,8]. dataset is at least five times the number of degrees of freedom in the test. This means that the number of instances required by IAMB to identify ( ) MB T is at least exponen-tial in the size of ( ) MB T since the number of degrees of freedom in a test is exponen-flected by the fact that IAMB will stop the learning at an early time when the size of 
IAMB X  X  shortcoming is recognized by its authors, which drove more research work on IAMB X  X  variants, MMPC/MB, HITON-PC/MB, PCMB, and BFMB. [1,3,7,8]. Although MMPC/MB and HITON-PC/MB are the first efforts to propose data effi-cient algorithm by considering the topology, e.g. a tree, PCMB is the first one that is proven correct. BFMB follows this approach by putting the topology knowledge into consideration during learning, but does even better than PCMB. As we explain in Section 3.2, BFMB always conditions on the smallest set to remove false positives, so it is demonstrated more data efficient than PCMB in [8], where BFMB achieves high-est accuracy than PCMB and IAMB by recognizing more correct elements belonging to ( ) MB T , given the same amount of data. Conclusion 3: BFMB is the most data efficient; PCMB is much efficient than IAMB on this point, which is consistent with the finding reported in [1]. KIAMB, by average, will beat, or at least tie with, IAMB. 3.4 Scalability GS was proposed to learn the BN via the recognition of local neighbours of each variable, aiming at better performance than global learning. IAMB was directly pro-posed for the microarray research where thousands of attributes are quite common, so did its variants. In [4], we can see the empirical study with 1000 variables involved, among GS, KS, IAMB and its variants. PC is compared to other ones only when the number of variables is 50, which reflects that global learning really can X  X  scale to large problem. Although the cardinality of MB in their empirical studies is quite elements (or true negatives) within acceptable time itself is a success. 
In [1], PCMB is applied to a problem of KDD Cup 2001 in which there are 139351 comparative classification accuracy is reported. IAMB is covered in that comparison as well, producing poorer results than PCMB but in a shorter timing length, which matches the conclusion made in Section 3.2 and 3.3. 
BFMB, the latest proposed algorithm, is reported to have highest data efficiency comparing with all previous algorithms of this family; besides, it also runs faster than PCMB [8]. Although no large scale testing is conducted in the study [8], BFMB is expected to have, at least, same scaling ability as PCMB. Conclusion 4: IAMB, KIAMB, PCMB and BFMB are all scalable to large scale of applications. 3.5 Summary Although IAMB, PCMB, BFMB and KIAMB are all proved correct theoretically, they still demonstrate relative strength or weakness when put them together for a comparison study. 
For practical applicants, based on our experience, IAMB is strongly recommended for large data samples increases quickly (actually, exponentially) when the number of variables and/or the number of levels per variable increase. 
In most cases today, we often face the embarrassing problem of insufficient data, or we are not sure if the data available is enough for reliable analysis. When this hap-pens, PCMB or BFMB appears as the best choice. Among them, BFMB is further-more suggested because displays greater data and speed efficiency. In one project done in SPSS Inc(the first author once spent some internship time in SPSS in 2007), we finally chose BFMB as the local learning algorithm of Markov blanket after thorough prototype testing and analysis among IAMB, PCMB and BFMB. IAMB initially appeared the most pr omising, but it performed quite poorly in up finally since it requires too many data passes to collect the statistics data. When the training data is large in size, this is a quite time-consuming procedure. Its design does not leave much room for optimization. Finally, we decided to take BFMB though it is quite new. It works quite well since the testing began couples of weeks as of the writ-ing of this paper. Actually, after our optimization, BFMB requires now even much fewer number of data passes and CI tests than that mentioned in [9], without sacrific-ing accuracy. For example, with the same settings and testing data (Alarm network) like the experiments done in [8], BFMB needs only about two times the number of data pass needed by IAMB, as shown in Table 1 where we attach the refined perform-ance of BFMB through our test but directly refer the results of IAMB and PCMB from [8]. Conclusion 5: (1) Given faithfulness and correct CI tests, BFMB is most preferred to IAMB and PCMB overall because of its highest data efficiency and moderate time efficiency; (2) If abundant data is available, IAMB and KIAMB will be the first choice; (3) KIAMB can be viewed a candidate in the exploratory phase. In this paper, we reviewed those known algorithms to do local learning of Markov blanket, which is known as an optimal solution for feature subset selection. Compar-ing with traditional global learning algorithms, these local learning algorithms are known as much more efficient and able to scalable to large problems. By filtering out those without sound theory basis, we select IAMB, PCMB, BFMB and KIAMB for comparison, in terms of theory basis, time efficiency, data efficiency, and scalability. Empirical studies from published papers and our own experience tell that BFMB is an ideal choice by summarizing those four factors, which explains why it is selected by when data is not a problem any more, IAMB or KIAMB will be the first choice. 
