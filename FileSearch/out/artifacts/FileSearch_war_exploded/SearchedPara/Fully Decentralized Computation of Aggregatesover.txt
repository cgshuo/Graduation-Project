 In several emerging applications, data is collected in massive streams at several distributed points of observation. A basic and challenging task is to allow every node to monitor a neighbourhood of interest by issuing continuous aggregate queries on the streams observed in its vicinity. This class of algorithms is fully decentralized and diffusive in nature: collecting all data at few central nodes of the network is unfeasible in networks of low capability devices or in the presence of massive data sets.

The main difficulty in designing diffusive algorithms is to cope with duplicate detections. These arise both from the observation of the same event at several nodes of the network and/or receipt of the same aggregated information along multiple paths of diffusion.

In this paper, we consider fully decentralized algorithms that answer locally continuous aggregate queries on the num-ber of distinct events, total number of events and the sec-ond frequency moment in the scenario outlined above. The proposed algorithms use in the worst case or on realistic distributions sublinear space at every node.

We also propose strategies that minimize the communica-tion needed to update the aggregates when new events are observed. We experimentally evaluate for the efficiency and accuracy of our algorithms on realistic simulated scenarios.
A variety of emerging network applications are based on spreading a large number of network devices over a broad area. Examples are the continuous and distributed monitor-ing of IP traffic flows, in which data is collected at multiple points of observation and real-time analysis is performed on aggregated streams, or the use of sensing devices, con-nected in a wireless sensor network, for environmental mon-itoring. Sensing devices observe large amounts of events in their surroundings. These events are recorded and processed by nodes in the form of data streams. In sensor databases we are required to answer aggregate queries over the streams of data, while in distributed monitoring applications we are interested in monitoring the events observed by the sensing devices, for instance the movement of objects or measure-ments of environment parameters.

A number of constraints is imposed in these applications by the limited resources available at the sensing devices: 1. Communication is the most power-consuming operation. Transmission of data to neighbor nodes must be carefully optimized in order to ensure a longer life to battery-operated devices. A direct consequence is that large streams of raw data cannot be transmitted between nodes. 2. Sensing devices are limited in computational power. The data received from the nodes can often only be processed on the fly in a streaming fashion. 3. Sensing devices have limited storage capacity. Data can be stored, even locally, only in aggregate form.
 It is a basic and challenging task to provision the network with primitives that allow every node to monitor its sur-rounding by issuing continuous aggregate queries on the streams of events observed by all nodes reachable within a few hops. Several specific issues need to be addressed when querying and monitoring distributed streams of data: Continuous queries . Differently from traditional databases, these applications must answer long-running queries, noti-fying the application whenever a new answer is found. This occurs for instance in applications that raise an alert when-ever a specific event happens, or some measurement of in-terest exceeds a given threshold.
 Distributed streams . One first difficulty to handle when pro-cessing aggregate queries on distributed streams is to dis-criminate events that have been observed at multiple nodes of the network. For instance, this problem arises naturally for terrain-monitoring applications, where sensing devices need to monitor objects that move across an area, or in point-to-point communication when packets are observed along all the nodes of a path.

One second difficulty to handle is that nodes are unre-liable: they switch on and off in order to optimize power consumption, they are prone to communication faults and to power outage. In many cases it is impossible to aggregate information using hierarchical structures, e.g. trees, where faults at single nodes of the network may disrupt the whole structure. It is also impossible to rely on very few nodes for collecting the aggregates on raw data produced in the network.
 Local queries . In sensor databases applications, aggregate queries may be issued by any node of the network and the answer restricted to the observations made in a neighbor-hood of interest. Power consumption is optimized if the nodes can decide to operate only when their observations differ substantially from other nodes in their vicinity.
Decentralized algorithms addressing the issues above are inherently diffusive in nature. The problem of detecting du-plicates in aggregate information is now magnified, since a node may receive the same data as part of aggregates re-ceived through multiple paths of propagation. In this paper we show how a set of continuous aggregate queries can be processed from fully decentralized algorithms on distributed streams of data.
The design of algorithms for the computation of aggre-gates within suitable neighbourhoods of all nodes of a net-work entails two fundamental aspects: 1. The algorithm used to summarize the scenario of interest. For any node u it requires a compact summary, or sketch , of the stream of events observed from u that is composable and duplicate insensitive [8, 10]: it is possible to merge the sketches of a set of nodes U to form a sketch of the union of the streams of events observed by all nodes of U . We consider the natural case in which U is the set of neighbors within l hops from a node u , for some l  X  0. 2. How and when information is propagated. This in turn re-quires i) addressing communication costs and ii) estimating the (further) error introduced by delay in communication.
Given the massive data sets we are interested in, we im-pose constraints on the memory requirements for the compu-tation and the overall amount of data exchanged to perform the task. Namely, i) the amount of memory used at every node of the network should be polylogarithmic in the num-ber of nodes of the network and in the size of the streams. ii) The communication complexity measured in terms of the overall number of messages exchanged among nodes to per-form the task.

The main contributions of our work are the following: 1. We consider a very general distributed streaming sce-nario and we describe compact distributed data structures and a communication scheme to keep track of statistic aggre-gates over distributed data streams. We are able to answer at any node of the network continuous queries on the num-ber of distinct events , total number of events and the second frequency moment (size of Self-join) over the streams ob-served within a suitably defined neighborhood of each node. These algorithms require only polylogarithmic storage space at each node of the network for number of distinct events , and the total number of events . Observe that our methods apply to a very general, fully-decentralized setting, whereas existing works consider either a centralized scenario with a single aggregating point or simple models for decentralized computation where a direct communication is assumed to be available between any pair of nodes in the network. 2. For the second frequency moment , we provide an im-portant contribution by showing a polylogarithmic bound on the storage size at each node when the distribution of events follows a Zipf X  X  law. Observe that in [11] no poly-logarithmic upper bound is given on the amount of space needed to estimate the second frequency moment on data streams with duplicates, whereas a  X ( provided in the worst case for algorithms based on uniform sampling. 3. We show optimized implementations and tests on large scale real and synthetic datasets and realistic network topolo-gies. Experimental results show that the strategies we con-sider achieve in practice very good bounds on storage, com-putation and communication costs while providing a good approximation of the statistics of interest. 4. We characterize the trade-off between worst case com-munication costs and accuracy for our sketch schemes. We evaluate the performance of a conservative strategy to trade off communication costs and accuracy, proposed in [12] for a special case of the scenario considered in this paper. Organization of the paper. In Section 2 we introduce the model and notation we use in the rest of the paper. In Sections 3 and 4 we outline our overall approach and discuss sketch schemes for fully decentralized estimation of F , F 1 and F 2 in the presence of duplicates. For F 2 also present the analysis for the important case of skewed data. In Section 5 we address the implementation of the sketches we consider and show how to optimize the amount of communication exchanged among nodes of the network. In Section 6 we show the experimental evaluation of the algorithms. Finally, Section 7 offers our concluding remarks.
An excellent survey of distributed streaming techniques has recently been provided in [9]. They distinguish be-tween one-shot queries and continuous queries. Tree-based aggregation for approximately answering of one shot holis-tic queries use composable data synopses, as for instance CM sketches [10] for point query, range queries, quantiles, AMS sketches [2, 3] for frequency moments, FM [16] sketches for counting distinct items. Greenwald and Khanna [18] also propose decomposable synopses for computing quan-tiles that can be adapted to tree-based aggregation.
Alternative to tree-based aggregation is broadcasting to all neighbor vertices till the summaries reach the aggrega-tion node of the network. These proposals need to exploit synopses that are order and duplicate insensitive as those proposed from Considine et al. [8], FM sketches [16], CM sketches [10], and duplicate-insensitive aggregation schemes [11] that propose duplicate insensitive estimation of the sec-ond frequency moment for a single aggregating point. How-ever, even in the restricted case addressed in [11], no poly-logarithmic upper bound on the needed space is given.
Decentralized computation with every node in the graph taking part in the computation and receiving the computed value has been proposed in [21, 22] for computing max, sum, avg and distinct items. This model is similar in spirit to our proposal with two main differences: a. in our setting we cannot assume direct communication between any pair of nodes in the network; b. we are interested in holistic aggregates at the local neighbor of any node of the network.
Main issue addressed so far in processing continuous queries on distributed data streams is to reduce the communication overhead between remote nodes and the aggregating point by allowing some slack in the estimation. In [5] the problem of distributing the slack between the participating sites is addressed for Top-k computation on distributed streams. In [12] several strategies of communication between the coordi-nating site and the distributed participants are studied for duplicate-insensitive aggregates as FM sketches and distinct sampling [17]. Model. We assume an undirected graph G = ( V,A ), repre-senting entities connected over a communication network. Without loss of generality we assume all communication edges bidirectional. Every node in the network observes a stream of data over time. We denote by S ( u ) the stream at node u . Each element of S ( u ) is an ( item , attribute ) pair ( i,a ), where i is a value that we assume to be in [ n ] = { 0 , 1 ,...,n  X  1 } , and a is an attribute from a discrete do-main A . Since all statistics we consider are order insensitive, S ( u ) may be regarded as a multiset of pairs, each pair with a multiplicity equal to the number of times it appears in the stream. Considered any stream S , we also define the base set B ( S ), i.e., the set of distinct pairs appearing in S . Loosely speaking, we often use the expression event to mean the observation of an ( item , attribute ) pair at some node of the network. Given a subset U  X  V , we define S ( U ) in the natural way as  X  u  X  U S ( u ), where the multiplicity of a pair For l  X  0, N l ( u ) denotes the subset of G  X  X  vertices within distance l from u . We write B ( U ) instead of B ( S ( U )) for the sake of coinciseness.
 Statistics. The general problem we are interested in is the following: Given l  X  0, for every u  X  V , compute some statistics over B ( N l ( u )). Depending on the choice of the attribute, we X  X l be able to estimate different aggregates of interest. Considered a stream S , we denote by m i simply m i when no confusion arises) the number of distinct pairs ( i,a ) in S : m i = |B ( S ) | , for every i  X  [ n ]. The p -th frequency moment of S is F p = P i  X  S m p i . When consid-ering S ( u ) for some vertex u or S ( U ) for some subset U of vertices, abusing notation we write m i ( u ), F p ( u ), m and F p ( U ) for m i ( S ( u )), F p ( S ( u )), m i ( S ( U )) and F respectively. We consider F 0 , the number of distinct items, F , the total number of distinct pairs in the stream, and F the second frequency moment over the set of distinct pairs. Communication. We make the minimal assumption that in a round of communication a node can only broadcast a message to the set of its immediate neighbours. We make no assumptions as to the topological structure of the neigh-bourhood of a node or the way in which communication is performed. Our model has an immediate practical coun-terpart in wireless networks in which communication occurs over a broadcast channel, while in other cases a round of communication may actually involve multiple transmissions along point-to-point connections.
Each node in the network observes a local stream over time and keeps a compact summary or sketch thereof, which depends on the application of interest. Consider a streaming algorithm A . We denote by Sk A the function used by A to compute sketches and by Sk A ( S ) the sketch computed by A at the end of the observation of a stream S . In the setting we consider, every node u locally maintains two sketches Sk ( S ( u )) and Sk ( S ( N l ( u ))) and performs the following ac-tions: ii. if u receives a sketch Sk ( S ( v )) originating from a node iii. u continuously monitors the current estimate of the
Step ii. is achieved using messages that contain a TTL (Time To Live) field, which is decremented at each node reached by the message, as is customary in gossip-based net-work protocols. Clearly, not all sketches are suitable for our setting. Following previous work, we consider sketches that are composable and duplicate insensitive [23, 8, 19]. Con-sidered an order insensitive statistics of interest, a sketching algorithm A for its estimation is composable if, given two streams S 1 and S 2 , Sk A ( S 1  X  S 2 ) = merge ( Sk ( S where merge (  X  ) is a suitable sketch aggregation function that depends on A and the statistics of interest [8]. A sketching algorithm is duplicate insensitive if, considered any stream S , Sk ( S ) = Sk ( B ( S )) (where B ( S ) is regarded as a multi-set). In the next Section, we address the problem of design-ing accurate, composable and duplicate insensitive sketches for the primitives of interest in this paper.
We start the presentation of our algorithms with a brief overview of the most important characteristics of counting sketches , which we use as building blocks of our methods for computing aggregate statistics in a fully-decentralized setting. A counting sketch is a composable and duplicate insensitive counter of the number of distinct ( i,a ) pairs ap-pearing in a stream S . We consider two approaches: the first is the original approach of Flajolet and Martin, con-sidered in [12] and modified in [3]. The second is a slightly different technique proposed in [6]. Since these are well es-tablished techniques, we only give an overview to make the paper self-contained, referring the reader to [16, 3, 12, 6] for details. The use of composable and duplicate insensitive sketches has been considered previously for restricted dis-tributed settings, especially for data aggregation at the sink of a sensor network [23, 8, 19].
 FM sketches. We use the phrase  X  X M sketch X  to refer to any implementation of the original counting sketch of [16]. FM sketches [16] use a simple approach in which each sketch is a vector of m entries, each entry being a bitmap of length k = O (log M ), with M an upper bound on the size of the universe. In our setting, the universe is the set of possible ( i,a ) pairs, so that M  X  n |A| (in practice, M = 2 k k = 64). Considered the s -th bitmap of the sketch. Every pair (item, attribute) ( i,a ) is hashed onto the bitmap bits using a (independently chosen) hash function H [ n ]  X A X  X  0 ,..., log 2 M  X  1 } , such that the probability of hashing onto the h -th bit is 2  X  h . The bit under consideration is set to 1 if it was 0. After processing the stream, let r denote the position of the least significant bit that is still 0 in the s -th bitmap: r s is a good estimator for log the logarithm of the number of distinct pairs observed. To improve accuracy, we consider 1 m P m s =1 r s as an estimator of log 2 F 0 , where m = O 1 2 log 1  X  .
 Bar-Yossef et al. [6]. In this approach, the sketching al-gorithm maps every pair ( i,a ) to an integer using a pairwise independent hash function h (  X  ) and at any point in time it maintains the list of the L smallest distinct values observed so far, where L = d 96 / 2 e , being the required precision. If v is the L -th smallest distinct value maintained by the algorithm, LM/v is an estimator of F 0 , where M = n 3 (see [6] for details). Precision can be increased by the standard trick of considering m independent and parallel copies of the algorithm and taking the median of the corresponding esti-mations, where again m = O 1 2 log 1  X  . In the rest of the paper, we will use the phrase  X  X Y sketch X  to refer to any implementation of this counting sketch.
 Composability. Clearly, both sketches are composable and duplicate insensitive: Considered two streams S 1 and S over the same universe and their FM sketches Sk FM and Sk FM ( S 2 ), Sk ( S 1 ) OR Sk ( S 2 ) is the sketch corresponding to S 1  X  S 2 . As for the BY sketches, every such sketch is an array of m lists, each maintained according to the algorithm described above. Merging of two such sketches is simply achieved as follows: for every s = 1 ,...,m , merge the s -th lists of the two sketches and keep the L smallest values of the merged list. Both approaches achieve similar bounds in terms of efficiency and precision, as stated by the following
Theorem 1 ([12, 16, 6]). Given a stream S of (item, attribute) pairs, it is possible to maintain an estimate  X  of the number C of distinct pairs in S using O 1 2 log 1  X  memory words, such that:
Any of the schemes outlined above can be used to main-tain, for a stream S , an accurate and duplicate insensitive number of distinct items observed in S , so we set a = null . As to F 1 , this is the basic problem of counting the number of distinct pairs, therefore it can be maintained with the guarantees mentioned above.
We now present our algorithm for computing F 2 in a fully-decentralized setting. We adapt the method proposed by Achlioptas [1] to maintaining F 2 with duplicates in a de-centralized scenario. We show that the method achieves polylogarithmic space on skewed data.
The most effective approaches to efficiently maintain F 2 in a centralized setting are based on random projections of the frequency vectors over a space of smaller dimension. A similar approach, explicitely designed to maintain F 2 over a data stream, was proposed in [3] and extended in [15, 20]. Independently, a number of techniques have been proposed to achieve the more general goal of maintaining pairwise eu-clidean distances of a set of vectors in lower dimensional space, mostly extending or modifying a key result by John-son and Lindenstrauss (see [13] for a relatively simple proof). A substantial simplification of the original scheme that we adopt in this work was proposed by Achlioptas [1].
 Maintaining F 2 ( S ) in the absence of duplicates. In this case, ( i,a ) , ( i,b )  X  S implies a 6 = b . Then, the approach described in the above paragraphs immediately applies to m ( S ) as done in [3]: assume m is the state of the frequency vector after the first t elements in S have been observed. At any time t , we maintain  X  m = mR as follows: If the value of the t + 1-th item observed is i ,  X  m j =  X  m j + e for j = 1 ,...,d , where e i is the n -dimensional row vector whose i -th component is 1, all other components being 0. So, observation of i determines the addition or subtraction of 1 in every component of  X  m .
 Maintaining F 2 ( S ) in the presence of duplicates. When duplicates are present in S , we use the  X  X ug-of-war X  sketch considered in [2, 11]. In particular, we notice that  X  m I ( S )  X  D j ( S ), where I j ( S ) (respectively, D j ( S )) counts the number of distinct ( i,a ) pairs observed in S that deter-mine the addition of +1 (addition of  X  1) to  X  m j ( S ). Hence, for every j = 1 ,...,d , we can estimate I j ( S ) and D using the techniques described in Section 3. More in de-tail, for every j , we maintain a counting sketch to esti-mate I j ( S ) (respectively D j ( S )). The overall tug-of-war sketch obtained this way consists of 2 d counting sketches and it is clearly duplicate insensitive and composable. In particular, to compose two tug-of-war sketches, the compo-nent counting sketches are pairwise composed in the obvi-ous way. Following the above paragraph on maintaining the tion of I j ( S ) (respectively, D j ( S )), our estimator of F is Maintaining R. Maintaining R explicitely requires O ( nd ) space at every node, which is unfeasible. Furthermore, if we want to compute F 2 over the union stream of a subset of the vertices, it is necessary that all nodes use the same random projection. In practice, all nodes generate the entries of R whenever needed using the same pseudorandom generator (and thus polylogarithmic space). To generate R ij , u com-putes the j -th value of the random generator with initial seed i . This way, all vertices generate the same value for R
The best algorithm to maintain F 2 with duplicates under general distributions requires space O 1 4 the same paper, the authors prove an  X ( for algorithms based on uniform sampling and it is an open question whether this bound indeed holds in general.
In the present paper, we show that the algorithm de-scribed above provides an accurate estimation of F 2 using polylogarithmic space for skewed data, which is the case in most applications of interest. We assume that m ( S ) is dis-tributed according to a Zipf law with parameter  X  &gt; 1, i.e.: m i = M/i  X  . The analysis for  X   X  1 (showing decreasing accuracy as  X  decreases) proceeds along similar lines and will be given in the full version of the paper. Under these assumptions we are able to state the following theorem:
It is clear that the matrix generated this way no longer sat-isfies the independence assumptions of [1], but choosing the pseudorandom generators independently for every column is in practice enough and is close to the requirements of [3].
Theorem 2. If m ( S ) is distributed according to Zipf law with parameter  X  &gt; 1 , it is possible to compute an estimation  X  F ( S ) such that: memory words per node.

Proof. We consider F 2 ( S ) under the assumption that m ( S ) is distributed according to a Zipf law with parameter  X  . We consider the case  X  &gt; 1. We drop S in the rest of this subsection, and we assume without loss of generality that m 1  X  X  X  X  X  X  m n : where M is a positive, integer constant. It is clear that by this definition the components of m will be fractional. Assuming integer components does not affect the result, but it makes proofs much more involved, so we do not consider this issue here. The proof of the following simple lemma is deferred to the full paper.

Lemma 1. If m follows a Zipf law with parameter  X  &gt; 1 , for every n  X  2 :
We next characterize the error achieved by the overall al-gorithm. Note that this error has two sources: i) the random projection; ii) the propagation algorithm.

In particular, recall that  X  m = mR and  X  m j = I j  X  D j fact, I j and D j are estimated at u from the corresponding FM sketches, so that u actually only computes two estimates  X  I and  X  D j of I j and D j . In particular, consider realizations of I j , D j ,  X  I j and  X  D j and assume that  X  I j = (1 +  X  D j = (1 + 2 ( j )) D j . The estimation of F 2 using the pro-jected vector would be 1 d P d j =1  X  m 2 j . In practice, u computes a vector  X  m that is itself an estimation of  X  m . In particular,  X  m j =  X  I j  X   X  D j . This implies that our actual estimation of F 2 is  X  F 2 = 1 d P d j =1  X  m 2 j . The simple proof of the following lemma is given in the full paper for the sake of space.
Lemma 2. where | E | X  3 d P d j =1 max {| 1 ( j ) | , | 2 ( j ) |} ( I
We show that i) 1 d P d j =1 ( I j  X  D j ) 2 (i.e., the squared 2-norm of the projected vector) is a close approximation of F with high probability; ii) | E | can be compensated by forcing the 1 ( j ) X  X  and 2 ( j ) X  X  to be small enough. The first claim is just a restatement of Lemma 5.1 in [1].
 probability at least 1  X   X  2 the following holds:
Lemma 4. If d dimensions are used for the random pro-jection, using memory O ( d 2 (  X   X  1) 4 log d  X  ) it is possible to en-sure that with probability at least 1  X   X  2 the following holds: Proof. First note that, for every j = 1 ,...,d : which implies ( I j + D j ) 2 &lt;  X  +1  X   X  1 2 M 2 for every j .
Now, we represent every counter (2 d counters) using count-ing sketch (see Theorem 1) consisting of O 1 2 1  X   X  1 4 ln bitmaps. Choosing constants suitably, Theorem 1 allows to prove that: The same holds for  X  D j and D j . This implies:
As a result, with probability at least 1  X   X  2 : where the third inequality follows from ( I j + D j ) 2 &lt; and from Lemma 1.

We can now prove the theorem. Again, we drop S since clear from context. We apply Lemmas 3 and 4 with d =
Lemmas 3 and 4 imply that ( |  X  S  X  F 2 | X  F 2 )  X  ( | E | &lt; F with probability at least 1  X   X  , so that we have:
The bound of Theorem 2 depends on the value of  X  and it becomes unbounded as  X   X  1. In fact, for  X  = 1 we obtain a polylogarithmic bound independent of  X  and which is stronger than the one of Theorem 2 for values of  X  close to 1.
In this section, we describe a distributed implementation of the primitives described in Section 4. Given the compos-ability and duplicate insensitivity of the sketches we con-sider, the results on the accuracy immediately carry over, so we do not discuss this issue again.
 1: 2: 3: 4: if h &gt; 0 then 5: h = h -1 6: MS = BUILDMESSAGE(S, h) 7: Send MS to u X  X  neighbours 8: else 9: Drop MS 10: end if Generic node behaviour (Figure 1). Upon observing a pair ( i,a ), node u invokes PROCESSITEM(S, i, a, l) to update its local and global sketches LS and GS (lines 1 and 2) and the estimate of the statistics of interest over S ( u ) (line 3). If this exceeds the last value sent 2 by more than a given threshold (line 4), a message MS containing LS  X  X  up-date is built and sent, with initial TTL = l . Whenever u receives a message from some neighbour v , containing v  X  X  update of Sk ( S ( v )), PROCESSMESSAGE(S, MS) extracts the sketch and updates u  X  X  global sketch (lines 1 and 2). The TTL is decremented and, if larger than 0, the message is for-warded to u  X  X  neighbours. Note that this generalizes the  X  X o sharing X  update scheme of [12], where they consider a star network topology with a single, central coordinator main-taining a global sketch (See Subsection 5.2). It remains to show i) how sketches are updated and merged and ii) how the threshold is defined. The former issue is briefly discussed in the next paragraphs for F 0 (and F 1 ) and for F 2 , while the latter is addressed in Subsection 5.2.
 Fully decentralized distinct counting. In this case, LS and GS in Figure 1 are counting sketches for the estimation of F 0 ( u ) and F 0 ( N l ( u )) respectively. Counting sketch up-date and merge operations and count estimation have been succintly described in Subsection 4.1. The pseudo-code of the main routines will be described in the extended version of the paper for the sake of space.
 Fully decentralized estimation of F 2 . In this case, LS and GS in Figure 1 are composite sketches for the estimation of F 2 ( u ) and F 2 ( N l ( u )) respectively. Following Subsection 4.3.1, LS and GS each consist of 2 d counting sketches, where d is chosen according to Lemma 3. Considering LS ( GS has exactly the same structure), for every j = 1 ,...,d , the j -th sketch LS[j] consists of two counting sketches ILS[j] and DLS[j] , to keep track of counters I j ( S ( u )) and D as described in Subsection 4.3.1. Update, merge and F estimation routines occur as described in Subsection 4.3.1. The pseudo-code will be given in the extended version of the paper for the sake of space.
In this section, we describe in more detail how contin-uous monitoring of statistics is performed; in particular, when and how information is propagated within the net-work. The general approach has been described in Section 3. In the experiments we considered the Threshold triggered
More precisely, whose corresponding sketch was sent. updates approach investigated of [12] ( X  X o sharing X  policy). Carrying this policy over to our scenario, node u sends an update, i.e., its local sketch LS , whenever the estimate C of the statistics of interest over the local stream changes, with respect to the last propagated value C 0 , i.e., whenever C &gt; (1 +  X / | N l ( u ) | ) C 0 , for some  X  &gt; 0.
We also considered an orthogonal approach in which, when a new observation occurs, only the portion of the sketch that eventually changes is propagated. This approach al-lows a slight improvement in the overall (worst case) number of bits transmitted and brings to messages of smaller size, which can be important in some applications, e.g., sensor networks. For the sake of space and since we used the ap-proach of [12] in the experiments, this part will be presented in the extended version of the paper.
The graph used in the experiments is a real topology of (part of) the Internet at the level of the Autonomous Sys-tems collected by DIMES [24] in December 2008. The sym-metrized version of this graph consists of 65 512 nodes and 148 364 edges, and its diameter is equal to 8. Nodes were assigned real and synthetic traffic data. Real data consist of HTTP requests sent to the 1998 World Cup Web site, made available 3 by the Internet Traffic Archive [4] and spanning three months (May-July 1998). These data were also used in [12]. We considered a week of data, consisting of 10 mil-lion tuples. Each HTTP request record contains clientID, objectID, serverID and a timestamp. In our experiments, we did not consider serverID and we chose to focus on the triples (clientID, objectID, timestamp). The data tuples were assigned to nodes of the graph using a hash function to map clientIDs onto graph vertices. Stream tuples then consisted of (objectID, timestamp) pairs.

We generated synthetic data considering a universe of items of size 1 , 000 , 000. We imposed m i (number of distinct pairs with i the item) to follow a Zipf distribution with pa-rameter  X  = 2 . 0. Every node in the graph was assigned a stream of length uniformly distributed in [500 , 1000], consist-ing of tuples &lt; i,a &gt; , where i was chosen with probability proportional to m i and a  X  [ m i ]. Globally, we obtained a data stream of approximately 17 million tuples.
A first set of experiments only concerned the accuracy of the estimation provided by the sketches. We used counting sketches corresponding to a precision = 0 . 1 with proba-bility at least 1  X   X  , where  X  = 0 . 1. For each node in the network, statistics were computed on the streams observed within neighborhoods formed by i) all nodes at distance 1 and ii) all nodes within distance 2. For every statistics of in-terest we considered the average of the estimates computed over i) the 100 nodes with highest degrees and ii) a set of 100 nodes chosen uniformly at random. For each node and for each statistic, we computed the average error with respect to the exact value.

In estimating F 0 (see Figure 2), we considered the two counting sketches described in Subsection 4.1, i.e, the FM sketch originally proposal by Flajolet and Martin [16], and the BY sketch proposed by Bar-Yossef et al [6].

We observed that, when FM sketches are used, the count-ing sketch provides approximations with error lower than 5% in most cases of the number of distinct items observed by every node within its 1(2)-neighborhood. BY sketches perform extremely well: their accuracy is comparable or slightly better than that of FM sketches. The only issue to consider is that the estimator adopted in [6] (see Subsec-tion 4.1) is L  X  M/v , with L = 96 / 2 , that corresponds to a value of 9 600 in our case. This implies that when using BY sketches, the algorithm always returns an estimate no lower than this value. Since L = 96 / 2 is a constant for fixed , we simply keep the exact count as long as this remains below this value. In the rest of this section we retain FM sketches, which are simpler to implement and provide an accuracy that is totally adequate for our purposes. Extended results will appear in the full paper.

Accuracy for F 1 is always below 4%. For the sake of space we do not discuss these results here.

In the estimation of F 2 (see Figure 3), we used a value of d (number of dimensions of the projected space) equal to 200, which is much lower than the one requested by theoretical analysis (2567) for the values and  X  we consider, which may suggest that the analysis can be improved. Complete results will be reported in the full paper. In general, as predicted by our analysis, the quality of results is very good for synthetic data, which were generated according to a very skewed distribution following a Zipf X  X  law with parameter  X  = 2. On the converse, larger errors are observed on real data that exhibit a much lower skewness than that requested by our theoretical analysis: we observed very small values (in the range [0 . 6 , 0 . 8]) of the Zipf X  X  parameter  X  for the streams at all the nodes involved in the measurements.
The approach we adopt to optimize communication was described in the previous section. It basically consist of the following: every node u sends a message update for a given counter to its immediate neighbors whenever the local esti-mation computed for that counter exceeds the last update that was sent by more than a factor 1 +  X /N l ( u ).
In order to evaluate the communication costs of the devel-oped methods, we simulated a scenario in which the nodes in the network start to process their local stream simulta-neously. We computed F 0 and F 2 keeping track of the total number of message updates sent by the algorithm at ten equally spaced checkpoints, which were fixed by identifying ten intervals of uniform length within the stream of events observed at each node. We considered different values of  X  local threshold used by every node to decide whether send or not to send a new message update after processing a new event. For the sake of space, we omit results for F 2 and we present and discuss below accuracy/communication trade-offs for the critical F 0 estimation primitive for distance-2 neighbourhoods.

Figures 4 and 5 shows the results concerning the estima-tion of F 0 within distance l = 2. The general trend is clear: at the beginning, so for the first checkpoints, the number of first-time observations of items grow faster. Accordingly, we observe a steep increase in the number of messages sent. The estimations stabilize after few checkpoints and the amount of exchanged messages drop dramatically. Also, since most distinct items are observed in the first part of the stream, nodes gain very early a very good accuracy in their estima-first checkpoint is already around 3  X  4%.

We experimented with higher values of  X  (  X   X  [2 , 10 , 20]), in order to trade communication with degrading accuracy too much. While  X  = 2 does not change the picture, for higher values of  X  we observed a significant drop in the total number of message updates (by a factor in [1 . 7 , 2]). The quality of approximation still remains very good: With  X  = 10, the average error gets lower than 10% at the third checkpoint, and it is around 7% at the end of the obser-vation. For  X  = 20, the error gets lower than 15% at the third checkpoint, and it is not greater than 10% at the last checkpoint.
 Memory requirements. We implemented sketches for F 0 and F 1 using at most 575 bytes per sketch. This figure is compatible with commercial sensor networks platforms, al-though packet sizes have typically smaller values 4 case of F 2 , memory requirements are between 145 and 224 KB per sensing node. This is compatible with applications that use modern sensors such as Intel X  X  Imote2, which are powerful but are also characterized by higher energy con-sumption, and thus are not a proper choice in several scenar-ios where long-life guarantees are a crucial issue, like wireless sensor networks for environmental monitoring, which may be required to work for more than one year. In these cases, designers may prefer less powerful devices such as TelosB, MicaZ and TinyNode184, which are provided with less mem-ory (  X  10 KB RAM) but have smaller consumption.
This can be overcome by having a constant number of pack-ets per update message or by reducing the size of update messages, as we show in the full paper.
In this paper we have presented a set of algorithms for computing high-quality approximations of the frequency mo-ments on the data observed within a neighborhood of limited size. The algorithms operate in a fully-decentralized setting and use sub-linear space at every node in the network.
We have analyzed scenarios not considered in previous work. We have presented experimental evidence of the effi-ciency and accuracy of our strategies on realistic simulated scenarios. We have analyzed simple strategies that minimize the amount of communication needed to update the aggre-gate estimations when new events are observed. In future work, we plan to demonstrate the usefulness of our algo-rithms by using them to develop tools for practical applica-tions, like distributed consensus in decentralized systems or measurement of statistics in sensor networks.

We remark the fact that the distributed streaming model is a natural extension of the semi-streaming model [14] which has been used, for example, for approximating the local number of triangles which every node belonging to a net-work is involved in [7]. Hence, we believe that our results can be extended to assess the quality of single components in networks of large scale.

From a theoretical point of view, we plan to afford a deeper analysis, aimed at proving a  X ( the memory needed to estimate the second frequency mo-ment in distributed settings that have to cope with data duplication.
We thank Ugo Colesanti for several helpful discussions.
