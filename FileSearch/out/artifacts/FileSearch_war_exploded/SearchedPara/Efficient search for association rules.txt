 This pap er argues that for some applications direct searc h for asso ciation rules can b e more ecien t than the t w o stage pro cess of the Apriori algorithm whic h rst nds large item-sets whic h are then used to iden tify asso ciations. In par-ticular, it is argued, Apriori can imp ose large computa-tional o v erheads when the n um ber of frequen t itemsets is v ery large. This will often b e the case when asso ciation rule analysis is p erformed on domains other than bask et analy-sis or when it is p erformed for bask et analysis with bask et information augmen ted b y other customer information. An algorithm is presen ted that is computationally ecien t for asso ciation rule analyses during whic h the n um b er of rules to b e found can b e constrained and all data can b e main tained in memory .
 H.2.8 [ Database Managemen t ]: Database Applications| data mining ; I.2.6 [ Arti cial In telligence ]: Learning; H.3.3 [ Information Storage and Retriev al ]: Information Searc h and Retriev al Asso ciation Rule, Searc h The Apriori algorithm [2] and its deriv ativ es [15, 11, 17] ha v e b ecome the de facto standard for disco v ering asso cia-tion rules. This pap er presen ts an alternativ e approac hto asso ciation rule disco v ery that ma y b e more ecien t when all data can b e retained in memory and the n um b er of can-didate itemsets cannot be adequately constrained b y con-sidering individual itemsets in isolation. Giv en the curren t a v ailabilit yofv ery large memory mac hines, man y p oten tial applications of the new algorithm ma y satisfy the rst con-strain t. Man y data miners will consider their time more v aluable than the cost of a few extra gigab ytes of memory . The Apriori algorithm relies on constraining the n um ber of itemsets b y considering features of itemsets in isolation, most commonly ,b y placing a lo w er limit on the frequency of an itemset, b elo w whic h itemsets will not b e considered. This is often feasible for simple bask et analysis, as few com-binations of pro ducts will b e b ough t together in large quan-tities. Ho w ev er, ev en for bask et analysis, the n um b ers of frequen t itemsets ma y rapidly increase if simple bask et anal-ysis is augmen ted b y considering so cio-economic or other at-tributes of the customers. Augmen ting simple bask et anal-ysis in this w a y can add m uc h to the ric hness of the kno wl-edge gained. Ho w ev er, if a customer description attribute is common to 50% of the customer base then that attribute will o ccur frequen tly with a large n um b er of item com bina-tions. Add a n um b er of suc h attributes to the analysis and the n um b er of frequen t itemsets can rapidly expand to an exten t where application of Apriori b ecomes infeasible. The same problem o ccurs when asso ciation rule analysis is applied to domains other than bask et analysis. Asso ciation rules can b e a v ery v aluable to ol for disco v ering in teresting in ter-relationships b et w een v ariables in man y di eren tt yp es of domain, as they do not lter through a mac hine learning bias the rules that are presen ted to the user. This enables the user to iden tify the in teresting rules rather than rely-ing on a mac hine learning system to determine the rules of in terest.
 This pap er describ es ho w a searc h algorithm can tak e adv an-tage of in ter-asso ciation-rule constrain ts to nd asso ciation rules ecien tly . Early approac hes to iden tifying in teresting rules from data w ere dominated b y attempts to form small sets of rules for accurate classi cation of further previously unsigh ted data [9, 7, 13]. F or the most part, b orro wing from an elegan t characterization of mining optimized rules b yBa y ardo and Agra w al [3], this activit y can b e c haracterized as follo ws:
Permission to make digital or hard copies of part or all of this work or permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 In the nineties, this researc h program to ok t w o div ergen t branc hes. On one hand, a n um b er of researc hers explored tec hniques for iden tifying large n um b ers of classi cation rules [4, 8, 10, 12, 14, 16]. This w ork w as distinguished b y the remo v al of the ob jectiv e of using the rules for classi cation and hence of the requiremen t that a small n um b er of rules b e iden ti ed. Rather, all rules that satis ed some criterion of in terestingness w ere sough t. In terestingness w as usually ev aluated b y some measure that led to iden ti cation of rules for whic h the an teceden t iden ti ed subsets of the training set that w ere dominated b y a single v alue of the class attribute, the in ten t b eing to predict the o ccurrence of that v alue. The other branc h w as asso ciation rule disc overy [1]. As-so ciation rule disco v ery di ers in in ten t from most other rule disco v ery paradigms. While the other paradigms ha v e concen trated on nding rules that are predictiv e of a sin-gle, preselected, class v ariable, asso ciation rule disco v ery has b een motiv ated b y nding rules that predict increased fre-quency of an attribute v alue, or collection of attribute v al-ues, without limitation on the v alues that ma y app ear in the consequen t of a rule. Asso ciation rule disco v ery can b e distinguished b y the aims of Due to its emphasis on analysis of large datasets, asso ciation rule disco v ery has concen trated on algorithms that pro cess data via database access whereas the other branc hes of rule disco v ery ha v e tended to concen trate on algorithms that re-tain all data in memory . This has led to the dev elopmen tof v ery di eren t forms of algorithm. Asso ciation rule disco v-ery algorithms ha v e sough t to minimize the n um b er of passes through the data due to the v ery high time o v erheads that these imply when accessing a database. This is less of a concern when data is retained in memory .
 Recen t researc h has started to bring these t w o div ergen t branc hes of rule disco v ery researc h bac k together. Ba y ardo and Agra w al [3] presen tav arian t of the OPUS searc h al-gorithm [18], dev elop ed in the con text of classi cation rules researc h, to disco v er k ey rules of the t yp e sough t b y as-so ciation rule disco v ery . Ho w ev er, as is t ypical in classi ca-tion rules researc h, their tec hnique considers only the searc h space for a single consequen t at a time, limiting its applica-bilit y in the most common asso ciation rule activit y , mark et bask et analysis, where it is often desirable to consider ev ery pro duct as a p ossible candidate rule consequen t.
 This pap er presen ts tec hniques for emplo ying the OPUS searc h algorithm for rule disco v ery where the searc h space encompasses rules for whic h the an teceden t can con tain an y conjunction of a v ailable predicates and the consequen t can be an y single predicate. It is further distinguished b y the abilit y to ecien tly nd a presp eci ed n um b er of rules that maximize an arbitrary function measuring rule qualit y . This distinguishes the approac h from t ypical asso ciation rule al-gorithms that explore all rules that satisfy presp eci ed con-strain ts. This distinction is particularly signi can t. F or dense searc h spaces, t ypical rule constrain ts ma y result in n um b ers of itemsets that mak e the Apriori approac h infea-sible. The abilit y to restrict searc h to a prede ned n um ber of target rules can allo w the new algorithm to ecien tly pro cess suc h searc h spaces.
 A ma jor concern in dev eloping asso ciation rule algorithms has b een minimizing the n um b er of database accesses that are required. I con tend that the need to do this is reduced if the database is retained in main memory . I further con-tend that doing so is no w feasible for a large range of data mining tasks due to the increase in the a v ailabilit yofv ery large memory computers. Ho w ev er, I recognize that there will alw a ys remain some tasks for whic h it is not feasible to retain a sucien t sample of cases in memory for acceptable asso ciation rule disco v ery . The tec hniques explored in this pap er do not address that scenario. The Apriori algorithm disco v ers asso ciation rules in t w o steps, utilizing the concept of an itemset . An itemset is a conjunction of conditions 1 . A lar ge itemset is an item-set that o ccurs more frequen tly than a prede ned minim um frequency . The Apriori algorithm exploits the observ ation that man y common measures of the v alue of an asso cia-tion rule are functions of the frequency of LH S , RH S , and LH S ^ RH S , where LH S and RH S represen t, resp ectiv ely , the itemsets for the an teceden t and consequen t of the asso-ciation rule.
 The t w o top-lev el steps of the Apriori algorithm are: 1. Find all large itemsets. 2. Generate asso ciation rules from the large itemsets. The rst stage pla ys t w o roles, 1. limiting the n um b er of rules that need b e explored to 1 In bask et analysis the relev an t conditions are predicates, one for eac h of the a v ailable items, eac h of whic h is true i the corresp onding item w as purc hased b y a customer, hence the name itemset . 2. cac hing the relev an t information ab out those itemsets, This strategy can b e v ery successful at reducing the n um-b er of passes through the data base. Indeed, v arian ts of the approac h can reduce database access to t w o passes [15, 17]. Ho w ev er, where there are n umerous large itemsets, the o v er-heads of itemset main tenance and manipulation can sev erely impact up on the computational feasibilit y of the approac h. A dramatic illustration of this is pro vided in Section 3, b elo w. In this example, applying Apriori with standard settings to the Co v er T yp e data set, with just 120 items, but with man y items o ccurring v ery frequen tly , results in 14,567,892 large item sets. With so man y large itemsets, managemen t and manipulation of those itemsets creates a large computational burden. The mo v e to w ards searc h for large n um b ers of classi ca-tion rules resulted in the dev elopmen t of algorithms for e-cien t tra v ersal of the searc h spaces in v olv ed. These initially relied up on assigning an arbitrary order to the conditions whic hw as then used to structure the searc h space so that eac h com bination of conditions w as considered just once. A searc h space with four conditions ( a , b , c , and d ) structured in this manner is presen ted in Fig. 1.
 Suc h a searc h space is exp onen tial in size. If there are 10,000 conditions, a gure commonly exceeded in mark et bask et analysis, the searc h space size is 2 10 ; 000 . Clearly it will only b e p ossible to explore suc h a searc h space if it can b e pruned. Under xed structure searc h, algorithms t ypically seek branc hes that cannot con tain a solution 2 , and prune those branc hes. Fig. 2 demonstrates the e ect of pruning the branc h for condition c from the xed-structure searc h space illustrated in Fig. 1. As can be seen, this remo v es only one no de from the searc h space.
 The iden ti cation of branc hes to b e pruned requires pruning rules. These iden tify regions of the searc h space that cannot con tain a solution. In rule disco v ery searc h, man y pruning rules consider for a giv en no de N whether an y searc hnode in the space b elo w N that con tains a giv en condition C can b e a solution. Th us, the pruning illustrated in Fig. 1 ma y ha v e resulted from a pruning rule iden tifying that no no de con taining c ma y con tain a solution. In this case, the ideal outcome w ould b e the remo v al from the searc h space of all no des con taining c , as illustrated in Fig 3. As can b e seen, this appro ximately halv es the remaining searc h space 3 2 What constitutes a solution will dep end up on the searc h ob jectiv e. F or example, in asso ciation rule disco v ery , a solu-tion migh tan y set of conditions that is a frequen t itemset. 3 It do es not exactly halv e the remaining searc h space as the ro ot no de has already b een visited, as, dep ending up on the searc h tec hnique, ma yha v e the no de con taining c , and hence these no des should not b e coun ted as part of the remaining searc h space.
 An elegan t metho d of ac hieving this outcome is to reorder the searc h space so that an y condition to be pruned at a no de precedes all conditions not to b e pruned. This is the OPUS s strategy [18]. This algorithm guaran tees that ev ery pruning action appro ximately halv es the remaining searc h space. The OPUS o algorithm [18] extends OPUS s for opti-mization searc h, using a heuristic that reorders the searc h space to maximize the amoun t of the space asso ciated with the least promising searc h op erator 4 . This is illustrated in Fig. 4. The OPUS algorithms ha v e b een demonstrated to supp ort ecien t complete searc hof a n um ber of standard rule disco v ery searc h tasks[18].
 A further approac h to pruning is pro vided b y inclusive prun-ing [19]. Whereas the ( exclusive ) pruning actions illustrated ab o v ein v olv e excluding from the searc h space those no des con taining a particular condition, inclusiv e pruning results in the exclusion of all no des that do not con tain a giv en con-dition. Lik e exclusiv e pruning, eac h inclusiv e pruning action appro ximately halv es the remaining searc h space. Man y frequen t itemsets will relate to asso ciation rules that are not of in terest. This migh t b e addressed b y placing ad-ditional constrain ts up on the itemsets that are considered. It is p ossible, although computationally exp ensiv e, to tak e accoun t of the relationship b et w een the an teceden t and con-sequen t of asso ciation rules that migh t b e deriv ed from an itemset, suc h as the p oten tial lift 5 . Ho w ev er, this w ould require duplicating during the rst stage m uc h of the w ork of the second stage of the Apriori algorithm. More imp or-tan tly , it is not p ossible to imp ose constrain ts that rely on relationships bet w een asso ciation rules, suc h as only nd-ing itemsets that could participate in the 1000 asso ciation rules with the highest lift. It will often b e the case that the end users to receiv e the asso ciation rule rep orts will only be in terested in considering a limited n um b er of asso ciation rules. Selecting a presp eci ed n um b er of those that maxi-mize a particular measure will b e desirable from the user's p ersp ectiv e and can b e used to constrain a directed searc h for asso ciation rules.
 Searc h for asso ciation rules can b e tac kled as a searc h pro-cess that starts with general rules (rules with one condition on the LH S ) and searc hes through successiv e sp ecializations (rules formed b y adding additional conditions to the LH S ). Suc h searc his unor der e d . That is, the order in whic h suc-cessiv e sp ecializations are added to a LH S is not signi can t. A ^ B ^ C ! X is the same is C ^ B ^ A ! X . An imp ortan t comp onen t of ecien t searc h in this con text is minimizing the n um b er of asso ciation rules that need b e considered. A k ey tec hnique used to eliminate p oten tial asso ciation rules from consideration is optimistic pruning. Optimistic prun-ing op erates b y forming an optimistic ev aluation of the high-est rule v alue that ma y o ccur in a region of the searc h space. 4 In rule searc h eac h condition can be considered a searc h op erator. F ormally , the searc h op erator is the inclusion of the condition in the set of conditions asso ciated with a no de. 5 Lift is a frequen tly utilized measure of asso ciation rule util-where j X j is the n um ber of cases with conditions X and n is the total n um b er of cases in the data set. An optimistic ev aluation is one that cannot b e lo w er than the actual maxim um v alue. If the optimistic v alue for a re-gion is lo w er than the lo w est v alue that can b e of in terest, then that region can b e pruned. If searc h seeks the top m asso ciation rules, then it can main tain a list of the top m rules encoun tered so far during the searc h. If an optimistic ev aluation is lo w er than the lo w est v alue of a rule in the top m , then the corresp onding region of the searc h space ma y b e pruned. Other pruning rules ma y iden tify regions that can b e pruned b ecause they can con tain only rules that fail to meet presp eci ed constrain ts suc h as: I use the term cr e dible rule to denote asso ciation rules for whic h, at some giv en p oin t in a searc h, it is p ossible that the rule will b e of in terest, using whatev er criteria of in terest apply for the giv en searc h.
 If w e restrict asso ciation rules to ha v e a single condition on the RH S ,t w o searc h strategies are plausible, 1. for eac h p oten tial RH S condition explore the space of 2. for eac h p oten tial LH S com bination of conditions ex-The former strategy leads to the most straigh t-forw ard im-plemen tation as it in v olv es a simple iteration through a straigh t-forw ard searc h for eac h p oten tial RH S condition. Ho w ev er, this implies accessing the coun t of the n um ber of cases co v ered b y the LH S man y times, once for eac h RH S condition for whic han LH S is considered. A t the v ery least this en tails the computational o v erheads of cac hing informa-tion. A t the w orst it requires a pass through the data eac h time the v alue is to b e utilized. While a pass through the data has lo w er o v erheads when the data is stored in memory rather than on disk, it is still a time consuming op eration that m ust b e a v oided if computation is to b e ecien t. These considerations mitigate in fa v or of the second strat-egy . W e systematically explore the space of p ossible LH S condition com binations, searc hing from the general to the sp eci c. During this pro cess w e trac k the set of condi-tions that can app ear on the RH S of a credible rule in the searc hbey ond the curren t p oin t. W e then organize the searc h to attempt to minimize the n um ber of LH S condi-tion com binations that are explored. A single pass through the data can b e p erformed for ev ery LH S com bination dur-ing whic h all statistics are collected for b oth the LH S and eac h of the RH S conditions curren tly under consideration. W e prune from the searc h space an y regions of p oten tial LHSs for whic h optimistic ev aluation can ascertain no RHS can result in a credible rule. The relativ e eciency of this approac h against the Apriori approac h will dep end on the cost of a pass through the data (lo w er fa v oring the new di-rect searc h), the n um b er of frequen t itemsets (lo w er fa v oring Apriori), and the n um ber of LH S com binations that m ust b e explored (lo w er fa v oring direct searc h).
 T able 1 displa ys the algorithm that results from applying the OPUS searc h algorithm [18] to obtain ecien t searc h for this searc h task. The algorithm is presen ted as a recursiv e pro cedure with three argumen ts, Curren tLHS: the set of conditions in the LHS of the rule Av ailableLHS: the set of conditions that ma y b e added to Av ailableRHS: the set of conditions that ma y app ear on The initial call to the pro cedure sets Curren tLHS to fg , and Av ailableLHS and Av ailableRHS to the sets of conditions that are to b e considered on the LHS and RHS of asso ciation rules, resp ectiv ely .
 Step 2(c)iiA records eac h credible asso ciation rule as it is ev aluated. If the searc h seeks the m b est rules on some metric, once m rules ha v e b een added at this step, as new rules are added, the rule with the lo w est v alue on the metric can be remo v ed from the table of b est rules. A rule will not b e credible if it fails other constrain ts, suc h as minimal strength, or, once the table is full, has lo w er v alue on the T able 1: The OPUS searc h algorithm adjusted for searc h for asso ciation rules Algorithm: OPUS AR(CurrentLHS, AvailableLHS, AvailableRHS) com CurrentLHS is the set of conditions in the LHS com AvailableLHS is the set of conditions that may com AvailableRHS is the set of conditions that 1. SoFar := fg 2. FOR EACH P in AvailableLHS ev aluation metric than the w orst rule in the table of b est rules.
 Step 2c prunes conditions from the space of those explored on the LHS of a rule. Rather than exploring the space of p os-sible LHS sets b ey ond the curren t one, optimistic tec hniques with lo w computational o v erheads should b e emplo y ed. F or example, if j C ur r entLH S [f P gj is less than minim um sup-p ort then no rule in the relev an t space of p ossible rules can ac hiev e minim um supp ort as all are sp ecializations of j C ur r entLH S [f P gj and hence cannot ha v e higher sup-p ort.
 Step 2(c)iiB prunes conditions from the space of those ex-plored on the RHS of a rule. Optimistic rules with lo w computational o v erheads should b e emplo y ed here also. F or example, if j C ur r entLH S [f P gj = 0 then no credible rule will exist in the relev an t space of p ossible rules. F or b oth of the pruning steps, the exact pruning rules to b e emplo y ed will dep end up on the sp eci c constrain ts for the searc h.
 Without pruning this algorithm will systematically explore the en tire searc h space. The pruning step remo v es from the searc h space b elo w a no de all and only those rules con tain-ing the iden ti ed condition. It follo ws, therefore, that the algorithm is complete, alw a ys nding the target asso ciation rules, so long as the pruning rules emplo y ed are correct. This algorithm is based on OPUS s rather than OPUS o . This is b ecause the more ecien t OPUS o requires at least t w o passes through the a v ailable LHS conditions at eac hnodeof the searc h tree, one to select and sort the LHS conditions and the second to mak e the recursiv e call for eac h LHS with the appropriate second and third argumen ts. The o v erheads of doing this are excessiv e for this searc h task b ecause an ev aluation of whic h RHS conditions should b e retained for eac h LHS w ould need to be p erformed in b oth lo ops. If there are a v ery large n um b er of p oten tial RHS conditions, either calculating this eac h time or cac hing the information bet w een lo ops, will ha v ev ery high o v erheads. F or example, if there are 1,000 conditions then there migh t b e 1,000 LHSs for eac h of whic h 1,000 p oten tial RHS v alues need to b e con-sidered. Examining eac h of the resulting 1,000,000 p ossible com binations t wice w ould clearly be undesirable as w ould cac hing suc h a large n um ber of v alues. Th us, a single pass approac h is emplo y ed that sacri ces the eciencies to be gained from dynamic reordering on optimistic v alue but de-liv ers far greater eciency in pro cessing a searc h no de than w ould otherwise b e p ossible. The largest dataset in the UCI mac hine learning rep ository w as sub jected to asso ciation rule analysis using b oth the Apriori algorithm and the ab o v e OPUS searc h. The Co v er T yp e data set w as selected as the largest of the UCI mac hine learning rep ository datasets. A data set from the mac hine learning rep ository w as used instead of one from the UCI KDD rep ository due to ease of access b y the researc her. The Co v er T yp e data set w as already in a format that could b e directly emplo y ed b y b oth the Apriori and OPUS searc h soft w are without further data manipulation. The Co v er T yp e data set w as collected for the purp ose of predicting forest co v er t yp e from cartographic v ariables only [5]. Ho w ev er, it is quite conceiv able that asso ciation rule analysis migh t also detect in teresting in ter-relationships b e-t w een those cartographic v ariables in addition to bet w een them and the v ariable describing the forest co v er. 581,012 cases are describ ed b y 55 attributes. The ten con tin uous v al-ued attributes w ere discretized in to three sub-ranges with as close as p ossible to equal n um b ers of cases within eac h sub-range. The remaining 45 attributes w ere all binary . In con-sequence there w ere 120 attribute-v alues, eac h of whic hw as treated as a separate condition for asso ciation rule analysis purp oses. Note that this treatmen t results in man y frequen t items, as for eac h binary attribute at least one v alue m ust o ccur for 50% of the cases.
 The publicly a v ailable apriori system dev elop ed b y Borgelt [6] w as applied to the Co v er T yp e dataset. This implemen-tation of Apriori generates rules with a single RHS condi-tion and m ultiple LHS conditions, th us exploring the same space of rules as the OPUS based algorithm. It generated 14,567,892 itemsets when emplo y ed with its default settings (maxim um itemset size of 5; minim um co v erage of 10% of the data for the LHS of a rule; minim um strength of 80%). The c over age of a set of conditions is the prop ortion of the training set for whic h the conditions are true. The str ength of an asso ciation is the co v erage of the union of the LHS and RHS divided b y the co v erage of the LHS. F rom the mini-m um LHS co v erage and strength apriori can determine that only itemsets with co v erage of 8% or higher need b e gener-ated. This required 96 hours and 44 min utes CPU time on a 350MHz PI I I lin ux computer. It w as not p ossible to com-plete the generation of all asso ciation rules as the le size limit w as exhausted after 30,677,279 rules w ere generated. The OPUS AR algorithm w as applied to the same data on the same computer. The same searc h space w as explored to nd the top 1000 asso ciations on lift.
 F our pruning rules w ere emplo y ed. T o describ e these w e use the follo wing abbreviations. The rst pruning rule, used at step 2c, prunes an y condi-tion P for whic h cov er ( NewLHS ) &lt; minLH S cov er , where minLH S cov er is the minim um allo w ed LHS co v erage. No sup erset of suc h a LHS can exceed the minim um LHS co v-erage as the co v erage of a sup erset of conditions m ust b e no larger than the co v erage of the original set of conditions. The second pruning rule is used at step 2(c)iiB. It prunes an y RHS condition Q for whic h cov er ( NewLHS [f Q g ) &lt; minR H S cov er , where minR H S cov er = minLH S cov er min str eng th and min str eng th is the minim um allo w ed v alue for asso ciation strength. This is the minim um allo w ed co v erage for LH S [ RH S for an y asso ciation. The justi ca-tion for this rule mirrors that for the previous.
 The next pruning rule is also used at step 2(c)iiB. This rules utilizes an optimistic assessmen t of the maxim um v alue of asso ciation strength for a rule with Q as the consequen t in the searc h space b elo w the curren t no de. First w e de-termine the maxim um n um b er of sp ecialization op erations that ma y be applied to the curren t no de to reac h a no de in the searc h space b elo w the curren t no de. max spec = min ( max LH S siz e j NEW LHS j ; j SOF AR j ), where max LH S siz e is the maxim um n um ber of conditions al-lo w ed in a LHS. There ma y b e no more sp ecializations than there are conditions a v ailable to sp ecialize b y( j SOF AR j ). Nor ma y there b e more sp ecializations than allo w ed b y the constrain t on the n um b er of conditions p ermitted in a LHS. Next w e determine an upp er limit on the maxim um reduc-tion in co v erage that ma y result from the addition of an y one condition to the LHS of an asso ciation in the searc h space b elo w the curren t no de. All asso ciations in this searc h space co v er subsets of the items co v ered b y the asso cia-tion for the curren t no de. Hence, no condition ma y re-mo v e more items from the co v er of an asso ciation in that searc h space than it remo v es from the co v er of the asso ci-ation for the curren t no de. Hence max cov er r eduction = max ( cov er ( LH S ) cov er ( LH S [f c g ): c 2 SOF AR ). The next step is to determine the minim um co v erage for the LHS of a rule in the searc h space b elo w the curren t no de. It is not p ossible for the co v erage to b e reduced b y more than max spec max cov er r eduction . Nor is it p ossible for it to be reduced b elo w the minim um allo w ed LHS co v erage. Hence, min cov er = max ( minLH S cov er ; cov er ( LH S ) max spec max cov er r eduction ).
 If min cov er &lt; cov er ( LH S [f Q g ) then the optimistic as-sessmen t of the maxim um strength ( opt str eng th ) for an asso ciation with Q as consequen t that ma y lie b elo w the curren t no de is 1.0 on the basis that the sp ecializations ma y remo v e from the co v er of LHS all cases that are not co v ered b y Q .
 Otherwise, opt str eng th = cov er ( LH S [f Q g ) =min cov er , the result that w ould b e obtained if all reduction in co v erage remo v ed cases co v ered b y the LHS but not the RHS of the asso ciations.
 If opt str eng th &lt; min str eng th , where min str eng th is a constrain t on the minim um allo w ed v alue for strength, then the RHS condition Q can b e pruned.
 The nal pruning rule also applies at step 2(c)iiB. This rule determines an optimistic v alue for lift for asso ciations in the searc h space b elo w the curren t no de that ha v e Q as a conse-quen t. Lift is maximized when strength is maximized. Th us, opt lif t = opt str eng th=cov er ( f Q g ). If opt l if t &lt; min lif t , where min lif t is the minim um allo w ed lift, then the RHS condition Q can be pruned. Note that min lif t could be a global constrain t on asso ciations, but ma y also b e deter-mined dynamically . In the curren t application, min lif t w as initialized to zero. Ho w ev er, once the target n um b er of as-so ciations had b een added to the table of b est asso ciation rules, at step 2(c)iiA, min lif t w as progressiv ely up dated to equal the minim um v alue of lift for a rule in the table. Hence, as the searc h progressed and the o v erall qualit y of the asso ciations in the table impro v ed, more stringen t pruning could o ccur.
 Using these pruning rules, a total of 384,312 asso ciation rules w ere ev aluated and only 84,639 distinct an teceden ts considered. This to ok 48 min utes and 9 seconds CPU time. T o nd the top 100 asso ciations on lift required the explo-ration of 204,264 asso ciation rules in v olving 51,678 distinct an teceden ts and to ok 26 min utes and 49 seconds CPU time. The ab o v e example demonstrates that using OPUS searc h and pruning the searc h space on the basis of in ter-relationships bet w een itemsets, it can b e feasible to p erform ecien t as-so ciation rule analysis on data sets for whic h the Apriori approac h is infeasible. Whether or not this is useful de-p ends, of course, up on whether there are in ter-itemset con-strain ts that should b e applied for the giv en asso ciation rule application. It seems plausible, ho w ev er, that for man y ap-plications an upp er-limit on the n um b er of asso ciation rules to b e generated will b e appropriate, and this can b e all that is required to enable ecien t searc h.
 F urther searc h constrain ts, suc h as SC-Optimalit y [3], migh t usefully b e emplo y ed to deliv er ev en greater computational eciency within the OPUS AR framew ork.
 That OPUS AR has wider application than the single dataset examined herein is demonstrated b y the commercial asso ci-ation rule disco v ery system Magnum Opus 6 . This system, that utilizes the OPUS AR algorithm, is routinely emplo y ed for commercial asso ciation rule disco v ery from datasets con-taining millions of cases eac h describ ed b y tens of thousands of v ariables.
 Asso ciation rule disco v ery has b een rmly ro oted in the do-main of mark et bask et analysis. Ho w ev er, prior to the p op-ularization of mark et bask et analysis, a n um b er of mac hine learning researc hers w ere exploring tec hniques with man y similarities to asso ciation rule disco v ery . These researc hers w ere exploring the use of complete or extensiv e searc h to form large rulesets in the b elief that suc h rulesets could pro-vide insigh t or other utilit ybey ond that obtained from the small rulesets normally generated b y mac hine learning sys-tems [8, 12, 14, 16]. The curren t w ork can be view ed as a direct descenden t of this researc h e ort, extending it b y utilizing the ecien t OPUS searc h algorithm and b y utiliz-ing metrics of rule v alue dev elop ed within the eld of bask et analysis. I ha v e presen ted an algorithm for asso ciation rule analy-sis based on the ecien t OPUS searc h algorithm. This ap-proac h is distinguished from the widely utilized Apriori algo-rithm b y its abilit y to use in ter-relationships b et w een item-sets to constrain the n um b er of itemsets that are considered. It is distinguished from a n um b er of recen t rule mining algo-6 Magn um Opus is distributed b y Rulequest Pt y Ltd, h ttp://www.rulequest.com. rithms, that ha v e b een presen ted as alternativ es to Apriori [4, 3, 10], b y exploring asso ciations con taining all a v ailable conditions as consequen ts. Ho w ev er, the approac h has the p oten tial disadv an tage, compared with Apriori, that it re-quires man y more passes through the data. Where the data can b e main tained in main memory this need not b e a se-rious handicap. The a v ailabilit yofv ery large memory com-puters means that quite sizeable data sets can b e retained in main memory . Where the data cannot b e main tained in main memory , ho w ev er, this approac h to asso ciation rule disco v ery is unlik ely to b e feasible.
 A simple example has b een used to demonstrate the p oten-tial adv an tage of the new approac h in some applications. Analysis of the Co v er T yp e data set requires generation and analysis of 14,567,892 itemsets when the Apriori algorithm is utilized, ev en when the itemset size is restricted to v e. In con trast, nding the 1000 asso ciation rules with the highest v alues of lift within the same constrain ts required ev alua-tion of only 677,129 rules and 33,613 distinct an teceden ts. With the implemen tations emplo y ed, the OPUS searc hw as completed with all 1000 rules iden ti ed in less than 15 CPU min utes while it to ok apriori more than 96 CPU hours just to generate the itemsets. This starkly illustrates the p oten-tial adv an tages of the new approac h.
 Iamv ery grateful to Christian Borgelt for making his excel-len t implemen tation of the Apriori algorithm publicly a v ail-able. I am also grateful to Jo c k A. Blac k ard and the UCI mac hine learning rep ository librarians Catherine Blak e and Chris Mertz for pro viding access to the Co v er T yp e data. [1] R. Agra w al, T. Imielinski, and A. Sw ami. Mining [2] R. Agra w al, H. Mannila, R. Srik an t, H. T oiv onen, and [3] R. J. Ba y ardo, and R. Agra w al. Mining the most [4] R. J. Ba y ardo, R. Agra w al, and D. Gunopulos. [5] J. A. Blac k ard. Comp arison of Neur al Networks and [6] C. Borgelt. apriori. (Computer Soft w are) [7] P . Clark and T. Niblett. The CN2 induction [8] S. H. Clearw ater and F. J. Pro v ost. RL4: A to ol for [9] R. S. Mic halski. A theory and metho dology of [10] S. Morishita and A. Nak a y a. P arallel [11] J. S. P ark, M.-S. Chen, and S. Y. Philip. An e ectiv e [12] F. Pro v ost, J. Aronis, and B. Buc hanan. Rule-space [13] J. R. Quinlan. Generating pro duction rules from [14] R. Rymon. Searc h through systematic set [15] A. Sa v asere, E. Omiecinski, and S. Na v athe. An [16] R. Segal and O. Etzioni. Learning decision lists using [17] H. T oiv onen. Sample large databases for asso ciation [18] G. I. W ebb. OPUS: An ecien t admissible algorithm [19] G. I. W ebb. Inclusiv e pruning: A new class of pruning
