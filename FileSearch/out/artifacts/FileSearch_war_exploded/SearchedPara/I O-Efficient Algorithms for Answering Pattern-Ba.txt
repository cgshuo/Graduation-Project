 Many kinds of real-life data exhibit logical ordering among their data items and are thus sequential in nature. In re-cent years, the concept of Sequence OLAP (S-OLAP) has been proposed. The biggest distinguishing feature of S-OLAP from traditional OLAP is that data sequences man-aged by an S-OLAP system are characterized by the sub-sequence/substring patterns they possess. An S-OLAP sys-tem thus supports pattern-based grouping and aggregation. Conceptually, an S-OLAP system maintains a sequence data cube which is composed of sequence cuboids . Each sequence cuboid presents the answer of a pattern-based aggregate (PBA) query. This paper focuses on the I/O aspects of evaluating PBA queries. We study the problems of joining plan selection and execution planning , which are the core issues in the design of I/O-efficient cuboid materialization algorithms. Through an empirical study, we show that our algorithms lead to a very I/O-efficient strategy for sequence cuboid materialization.
 Categories and Subject Descriptors: H.2.0 [Database Management]: General General Terms: Design Sequence data is ubiquitous. Examples include workflow data, data streams and RFID logs. Techniques for pro-cessing various kinds of sequence data have been studied extensively in the literature (e.g., [13, 14, 11, 1, 2, 15]). Re-cently, issues related to warehousing and online analytical processing (OLAP) of archived sequence data (e.g., stock ticks archive, passenger traveling histories) have received growing attentions [6, 5, 10]. In particular, [10] developed a sequence OLAP system (called S-OLAP) that efficiently supports various kinds of pattern-based aggregate queries.
While traditional OLAP systems group data tuples based on their attribute values , an S-OLAP system groups sequences based on the patterns they possess. Common aggregate functions such as COUNT/SUM/AVG can then be applied to each group. The resulting aggregate values form the cells of a so-called sequence data cuboid, or s-cuboid .
Since an s-cuboid displays the aggregate values of sequences that are grouped by the patterns they possess, one can view an s-cuboid as the answer to a pattern-based aggregate ( PBA ) query . To illustrate PBA queries and s-cuboids, let us consider the sequence data set shown in Figure 1. The dataset models a collection of passenger traveling records registered by the Washington DC X  X  metro system. The records are captured electronically by SmarTrip , which is an RFID-card-based stored-value e-payment system. Each row in Fig-ure 1 shows a sequence of passenger events. An event con-sists of a number of attributes, such as Time , Station , Action and Amount . For example, the event [ t 9 ; Wheaton; exit; 1.9] of passenger 623 indicates that the passenger exited Whea-ton Station at time t 9 and paid $1.9 dollars for his trip.
Figure 2 shows a PBA query  X ( X, Y, Y, X ), COUNT  X  X nd a few cells of the resulting s-cuboid. A PBA query  X  T , F  X  consists of two components: A pattern template T (e.g., ( X, Y, Y, X )) and an aggregate function F (e.g., COUNT ). A pattern template is a sequence of pattern symbols (e.g., X , Y ) defined over an attribute A of event records. The pat-tern symbols are instantiated by the values of A to generate various patterns . Data sequences are grouped based on the patterns. Finally, the function F is applied to each sequence group to derive aggregate values.

For example, the pattern template ( X, Y, Y, X ) defined on the Station attribute specifies that passenger sequences are grouped together if they have traveled round-trip between stations X and Y (i.e., he first entered station X and exited station Y in his first trip, and then entered station Y and come back to station X in the next). The symbols X and Y are instantiated with various station names to form pat-terns, such as (Clarendon, Pentagon, Pentagon, Clarendon). Data sequences that possess a given pattern are grouped into acell 1 . Each data sequence gives a value (or measure )to be aggregated. For example, a passenger sequence could be associated with the amount of fare paid, or simply  X 1 X  if we only care about the cardinality of a cell. The aggregate function F is then applied to the values of the sequences of each cell to obtain an aggregate value of the cell. In this paper, if P represents a pattern (e.g., (Clarendon, Penta-gon, Pentagon, Clarendon)), we use C ( P )todenotethecell of pattern P (i.e., C ( P ) = the set of sequences containing pattern P ), and we use F ( C ( P )) to denote the aggregate value of the cell. An s-cuboid consists of all the aggregate
A data sequence may contain more than one pattern. A sequence can, therefore, belong to multiple cells. values of the cells derived from all possible instantiations of the pattern template. For example, Figure 2 shows that there are 16,289 sequences that possess the pattern (Claren-don, Pentagon, Pentagon, Clarendon). In our notation: COUNT ( C ((Clarendon, Pentagon, Pentagon, Clarendon))) = 16,289. Given a PBA query (e.g.,  X ( X, Y, Y, X ), COUNT  X ) , o u r goal is to evaluate the query by computing all the cells of the corresponding s-cuboid (e.g., all the cells and their COUNT listed in Figure 2).

In [10], a basic implementation of an S-OLAP system is presented. In that study, data sequences were indexed by inverted lists. Given a pattern P , its inverted list L [ P ]is a list of sequence id X  X  such that each sequence s listed in L [ P ] contains the pattern P .Ans-cuboidcell C ( P )can thus be represented by the inverted list L [ P ]. The inverted list of a pattern can be obtained by joining the lists of its sub-patterns. This process allows PBA queries to be eval-uated efficiently. For example, consider the query pattern template ( X, Y, Z, X ). To materialize an s-cuboid cell, say, C (( a, b, c, a )), one can intersect (or  X  X oin X ) the inverted lists L [( a, b, c )] and L [( c, a )] (if these lists are available). This is because a sequence that contains the pattern ( a, b, c, a )must contain the sub-patterns ( a, b, c )and( c, a ). (We will present the details of this list-joining procedure in Section 3.)
For large sequence databases, the inverted lists are long and should be stored on external storages. The I/O cost of retrieving the inverted lists for computing an s-cuboid is a dominating factor in PBA query processing. The objec-tive of this paper is to study the I/O issues in evaluating PBA queries and to propose I/O-efficient query processing algorithms. We identify the following two I/O issues.
First, in [10], a static joining plan is used for computing an inverted list. More specifically, given a pattern P ,thein-verted list L [ P ] is always materialized by joining L [ P L [ P 2 ], where P 1 and P 2 are the length-( | P | -1) prefix and length-2 suffix of P , respectively. For example, we always compute J 1 : L [( a, b, c )] L [( c, a )] to obtain L [( a, b, c, a )]. We observe that multiple joining plans are usually available for materializing an inverted list. For example, to compute J 3 : L [( c, a )] L [( a, b, c )]. Note that the advantage of the latter plan, J 3 , is that it shares the same joining lists with J . Hence, when evaluating a PBA query with the pat-tern template ( X, Y, Z, X ), only two joining lists (namely, L [( a, b, c )] and L [( c, a )]) are retrieved to materialize the in-verted lists that correspond to the two cells C (( a, b, c, a )) and C (( c, a, b, c )). On the other hand, if J 2 were picked over J , four joining lists are retrieved, doubling the I/O cost. In this paper, we show that the choice of joining plans has big impacts on the I/O cost of query processing. We study the Joining Plan Selection Problem (JPS)  X  given a pat-tern template T and the set of all instantiated patterns I of T , which joining plans shall be picked to materialize the inverted lists of the patterns in I ? As we will see in Sec-tion 4, the JPS problem is NP-hard. We propose an effective heuristic algorithm, GPS, for solving the JPS problem. We will show that GPS is comparable to the best approximation algorithm, LP, in terms of minimizing the I/O cost while at the same time GPS executes about 5 times faster than LP.
Second, we observe that the order in which cuboid cells are materialized significantly affects the size of the mem-ory buffer required in query processing. For example, con-sider the two joining plans J 1 and J 3 for materializing the cells C (( a, b, c, a )) and C (( c, a, b, c )), respectively. These two plans use the same joining lists L [( a, b, c )] and L [( c, a )]. If J is executed first, then J 3 should be executed soon after, be-fore the required joining lists get swapped out of the memory buffer. We study the Execution Planning Problem (EP)  X  given a set of joining plans, how shall we order the execution of the plans so that even a small memory buffer is enough to avoid repeated retrievals of joining lists? We propose the REP scheduling algorithm, which generates execution orders that result in a very small memory buffer requirement.
The rest of the paper is organized as follows. Section 2 discusses some related works. Section 3 reviews the basic algorithm for evaluating PBA queries using inverted indices on data sequences. Section 4 discusses the JPS problem and presents the GPS algorithm for selecting low-cost join-ing plans. Section 5 discusses the EP problem and presents the REP scheduling algorithm. Section 6 evaluates the per-formance of the algorithms through an experimental study. Finally, Section 7 concludes this paper. Sequence data processing has been studied extensively in the literature. The PREDATOR system [13] was first pro-posed to store and query sequence data based on the object-relational model. The DEVise system [11] addressed se-quence data processing based on the relational model. To query sequence data, [12] developed an extension to SQL, called SQL-TS, to express various kinds of pattern-based queries. Beyond native sequence data processing, recently, the processing of live stream data (e.g., [1, 2, 4, 16]) and the online analytical processing (OLAP) of RFID workflow data [6, 5] have attracted a lot of attention. Nonetheless, none of these works address the problem of pattern-based grouping and analysis.

The concept of Sequence OLAP was first introduced in [10]. The paper presented an S-OLAP architecture, a formu-lation of PBA queries, and the idea of s-cuboids as answers to PBA queries. It also put forward six S-OLAP operations, which transform an s-cuboid to another by manipulating the pattern template of a PBA query. For example, the Append operation extends a pattern template (e.g., ( X, Y, Y, X )) by adding a pattern symbol at the end of the template (e.g., ( X, Y, Y, X, Z )). These S-OLAP operations allow an analyst to incrementally modify his PBA query, exploring a sequence data cube in a manner that is similar to the application of traditional OLAP operations such as rollup and drill-down .
The query model proposed in [10] has served as a founda-tion to other Sequence OLAP research. For example, vari-ants of the PBA query model, such as iceberg and top-k queries [3], have been proposed. In [3], pruning algorithms that avoid the processing of non-top-k cells are studied. We remark that the I/O-efficient algorithms we propose in this paper are also applicable to those query model variants.
Another piece of work on pattern-based sequence analysis is given in [9]. Given a set of standing patterns and an event data stream, the objective is to determine the occurrences of the standing patterns in the data stream. An aggregate function is then applied to those occurrences to obtain sum-mary statistics of the patterns. In [9], an interesting al-gorithm chase is proposed which intelligently exploits the similarity of the standing patterns so that the processing of these patterns can be efficiently done. In this section we describe the inverted index structure and the basic s-cuboid materialization algorithm ( basic )using inverted indices. The basic algorithm is based on the in-verted lists proposed in [10] except that we add positional information into the lists to facilitate join result verification. (We will elaborate on this verification process shortly.) For reference, symbols that are frequently used in our discussion are shown in Table 1.

An inverted index consists of a set of inverted lists. An inverted list, L [ P ], is associated with a length-m pattern P =( v 1 ,...,v m ). Each element ( v i ) in pattern P is a value chosen from the domain of an event attribute. The inverted list L [ P ] is a list of postings which record the occurrences of P in the sequence dataset. Each posting is of the form ( s p ,...,p f i ), where s i is a sequence identifier, f i is the num-ber of occurrences of P in s i ,and p 1 ,...,p f i are the starting positions at which pattern P occurs in s i . Given a pattern template T , the inverted index I T is the set of inverted lists L [ P ] such that pattern P is an instantiation of the template T . Figure 3 shows three example inverted indices: I ( X,Y,Y ) I ( Y,X ) and I ( X,X ) . For instance, the first inverted list l I ( X,Y,Y ) indicates that sequences s 2, s 27 and s 34 all contain the pattern (Clarendon, Pentagon, Pentagon) 2 . The first posting ( s 2:1,4) indicates that there are two occurrences of the pattern in sequence s 2 at positions [1..3] and [4..6].
Symbols in a pattern template are unbound variables. So, the inverted indices I ( X,Y ) and I ( Z,U ) are the same. Multiple
An S-OLAP query can be evaluated (or equivalently an s-cuboid can be constructed) from a set of inverted indices. For example, given the set of inverted indices shown in Fig-ure 3, the PBA query  X ( Y, X ), COUNT  X  can be evaluated as follows. First the inverted index I ( Y,X ) is retrieved. For each pattern P that is an instantiation of ( Y, X ), the in-verted list L [ P ]in I ( Y,X ) gives the sequences that contain P . These sequences thus form one cell of the s-cuboid and the cardinality of L [ P ] gives the aggregate value of the cell. For example, the value of the cell C ((Pentagon, Clarendon)) is 4 because the inverted list l 8 contains four sequences.
Evaluating a PBA query with a pattern template T thus requires the index I T .If I T is not available, it can be ma-terialized by joining the indices of shorter templates. In particular, I T can be obtained by joining the inverted in-dices I T 1 and I T 2 ,where T 1 and T 2 are the length-( m -1) prefix and the length-2 suffix of T , respectively. (If I is not available, it is recursively constructed.) For exam-ple, the inverted index I ( X,Y,Y,X ) can be materialized by joining I ( X,Y,Y ) and I ( Y,X ) . We denote this index join by I the joining lists L [( v 1 ,v 2 ,v 2 )]  X  I ( X,Y,Y ) and L [( v I ( Y,X ) . More specifically, if a sequence s appears in a post-ing of L [( v 1 ,v 2 ,v 2 )] with a starting position i and in another posting of L [( v 2 ,v 1 )] with a starting position i +2, then s is recorded in a posting of L [( v 1 ,v 2 ,v 2 ,v 1 )] with a start-ing position i .Weuse L [( v 1 ,v 2 ,v 2 ,v 1 )] = L [( v L [( v 2 ,v 1 )] to denote this join operation. For example, the in-verted list L [(Clarendon, Pentagon, Pentagon, Clarendon)] is obtained by considering l 1 and l 8 . From the lists, we see occurrences of the same variable in a template, however, have the same binding. So, I ( X,X ) and I ( X,Y ) are different. Algorithm BuildIndex 5. for all pattern P instantiated from T 6. if L [ P ] is not materialized Figure 4: A recursive algorithm for materializing an inverted index that s 27 at position 3 is in l 1 and s 27 at position 5 is in l so the posting ( s 27:3) is added to L [( v 1 ,v 2 ,v 2 ,v
We note that in evaluating a PBA query, the S-OLAP system will materialize a number of inverted indexes. These materialized indexes can be stored on disks to facilitate the processing of future PBA queries. Also, the system can be pro-active in pre-computing some inverted indexes. In this paper we assume that all indexes of length-2 pattern tem-plates are materialized. This approach is similar to bigram indexing in document retrieval systems. Our assumption is thus a reasonable one. Algorithm 4 shows the recursive al-gorithm BuildIndex that computes an inverted index I T given a pattern template T .Our basic algorithm for evalu-ating a PBA query  X  T , F  X  X irst invokes BuildIndex to obtain I . Then, the aggregation function F is applied to the se-quences of each inverted list in I T to compute the aggregate values of the s-cuboid cells. The basic algorithm uses a static joining plan. More specif-ically, each inverted list L [ P ]  X  I T is materialized by joining lists L [ P 1 ]and L [ P 2 ], where P 1 and P 2 are the length-( m -1) prefix and the length-2 suffix of P , respectively. We call this the ( m -1) 2 joining plan . For example, basic materializes the list L [( a, b, b, a )] by: ( m  X  1) 2plan: L [( a, b, b, a )] = L [( a, b, b )] L [( b, a )] . If L [( a, b, b )] is not available, basic will recursively materi-alize L [( a, b, b )] first by computing L [( a, b )] L [( b, b )]. (Re-call that we assume all inverted lists of length-2 patterns are materialized. So, no further recursive calls are needed.) We note that other than the ( m -1) 2 joining plan, there are many others to choose from. For example, L [( a, b, b, a )] can be obtained by: In many cases, these alternate joining plans incur lower I/O costs, especially if their joining lists (e.g., L [( a, b )] and L [( b, b, a )]) are readily available. For instance, if a query  X ( Y, Y, X ), F  X  has been processed, then the list L [( b, b, a )] would have been materialized. Now, if the user modifies the pattern template to ( X, Y, Y, X ) (i.e., by prepending asymbol X to his previous query), then the inverted list L [( a, b, b, a )] will have to be materialized. Note that the joining list L [( b, b, a )] is available for executing Plan J obtain L [( a, b, b, a )]. However, the joining list L [( a, b, b )], re-quired by the ( m -1) 2 plan, is not. Plan J 2 is thus better than the static plan executed by basic in this case. This ad-vantage is especially significant if the list L [( b, b, a )] is cached in main memory due to the execution of the query ( Y, Y, X ).
An interesting question is which plan we shall pick to achieve fast query processing. As another motivating ex-ample, consider materializing the inverted index I ( X,Y,X,Y ) Let L [( a, b, a, b )] be an inverted list in the index. To mate-rialize this list, we can consider two plans: (if materialized) will be retrieved from disk and cached. Note that ( b, a, b, a ) is another instantiation of the template ( X, Y, X, Y ) and so the list L [( b, a, b, a )], which can be ob-tained by L [( b, a, b )] L [( a, b, a )], needs to be materialized as well. Since the two joining lists are cached, the material-ization of L [( b, a, b, a )] does not require any extra I/O. On the other hand, Plan J 4 does not offer similar benefit. J thus more I/O efficient.

Given a length-m pattern P , the number of joining plans for materializing the list L [ P ] is exponential w.r.t. m . Con-sidering all possible joining plans is thus expensive and in many cases an overkill. For efficiency, we limit our search space and consider only binary joining plans: L [ P ]= L [ P L [ P 2 ] that satisfy the following conditions: C1: P 1 is a length-x prefix of P , i.e., P 1 =( v 1 ,...,v C2: P 2 is a length-( m -y +1) suffix of P , i.e., P 2 =( v C3: L [ P 1 ] ,L [ P 2 ]  X  L MAT ,where L MAT is the set of inverted A joining plan that satisfies all three conditions is called a candidate joining plan . Conditions C1 and C2 ensure that the prefix pattern P 1 overlaps with the suffix pattern P Hence, we do not consider, e.g., L [( a, b, b, a )] = L [( a, b )] L [( b, a )] as a candidate plan. Condition C3 further restricts candidate plans to those whose joining lists are all materi-alized. When there are no candidate joining plans, we fall back to the ( m -1) 2scheme 3 .

With the restriction, there are O ( m 2 ) candidate joining plans for the materialization of each inverted list [3]. Our objective is to select a set of joining plans to materialize all the lists in a given inverted index I T such that the I/O cost of retrieving the joining lists is minimized. We call this problem the joining plan selection problem (JPS).

We tackle the JPS problem by formulating it as an in-stance of the minimum weight multi-colored subgraph prob-lem (MWMCSP) [7]. Since MWMCSP is NP-hard, we pro-pose a heuristic algorithm GPS (Greedy Plan Selection) to obtain an approximate solution. Our experiment results show that GPS is as effective as the best approximation algorithm while at the same time it has a much smaller run-ning time. We now describe the problem formulation and the GPS algorithm.

To materialize an inverted list L  X  I T , we need to select a candidate joining plan, say, L = L 1 L 2 , and retrieve the joining lists ( L 1 and L 2 ) from disk. Let  X ( I T )betheset
That is, P 1 and P 2 are the length-( m -1) prefix and the length-2 suffix of P , respectively. In this case, L [ P have to be recursively materialized first (see BuildIndex of selected joining plans for materializing all the lists in I Also, let be the set of joining lists to be retrieved from disk. We make two assumptions: (A1) there is at least one candidate joining plan (that satisfies the three conditions C1 to C3) for each inverted list in I T , and (A2) there is enough memory buffer to hold all joining lists in L . Assumption A1 ensures that the set  X ( I T ) is complete and well-defined. Assumption A2 implies that no joining list needs to be retrieved from disk twice (due to swapping) in materializing I T . The I/O cost of materializing I T using the set of joining plans  X ( I T denoted by cost ( X ( I T )), is given by: where b ( L x ) is the number of disk blocks needed to store the inverted list L x . We will relax the two assumptions (A1, A2) later and discuss how we handle cases in which the assumptions do not hold.

Given an inverted index I T , our goal is to find a  X ( I T that minimizes cost ( X ( I T )). We model this optimization problem as a graph problem. Let C ( I T )bethesetofall candidate joining plans for all the inverted lists in I T is, a joining plan ( L = L p L q )  X  X  ( I T )iff L  X  I T and the plan satisfies the conditions C1-C3. Since our desired out-put  X ( I T ) contains exactly one candidate joining plan for each inverted list in I T ,wehave X ( I T )  X  X  ( I T ). Now, we construct a graph G =( V, E ) as follows. First, for each L that is a joining list of some joining plan in C ( I T ), a vertex v x is created in V .Aweight w ( v L x )= b ( L x ) is assigned to v L x . Next, for each joining plan ( L = L p L q )  X  X  an edge e L , labeled L ,iscreatedin E connecting vertices v p and v L q .Theweight w ( e L )ofedge e L is the sum of the weights of its end vertices, i.e., w ( e L )= w ( v L p )+ w ( v Figure 5 shows part of the graph constructed for materializ-ing the inverted index I ( X,Y,Y,X ) . For illustration purpose, only the joining plans of six inverted lists (e.g., L [( a, b, b, a )]) are shown in the figure. In the graph, the edge e beled L [( a, b, b, a )]) connects v 1 and v 2 (labeled L [( a, b, b )] and L [( b, a )], respectively). This edge represents a join-w ( e 1 ) = 3+20 = 23, so the I/O cost of retrieving the joining lists to execute e 1 is 23 disk blocks. Edges e 3 and e 5 sent two other joining plans for materializing L [( a, b, b, a )].
Note that a set of selected plans  X ( I T ) is equivalent to a set of | I T | edges of distinct labels in G ,where | I T | the number of inverted lists in I T . Also, it is easy to see that cost ( X ( I T )) = L x  X  X  ( w ( v L x )). If we consider each distinct edge label as a distinct  X  X olor X  (e.g. The edges e 1 ,e 3 in Figure 5 are of the same edge label L [( a, b, b, a )].), then finding a  X ( I T ) with the smallest cost ( X ( I T )) is equivalent to finding a subset E of E such that (1) all edges in E are of distinct colors, (2) all colors are covered by E and (3) the total weight of the end vertices of the edges in E is minimized. This problem is MWMCSP [7].

In Figure 5, an optimal solution for the graph is high-lighted. For instance, e 3 is picked indicating that the joining plan L [( a, b, b )] L [( b, b, a )] should be used to materialize L [( a, b, b, a )]. The sum of the weights of the vertices in this optimal solution is 61, which means that materializing the inverted index requires 61 disk block accesses.

The MWMCSP problem is NP-hard. The state-of-the-art approximation algorithm for the problem is based on the LP-rounding technique [7]. The algorithm (let X  X  call it LP) has a  X ( d 2 k ) running time, where d is the number of inverted lists in I T and k is the number of candidate joining plans for each inverted list in I T . For efficiency, we propose a greedy algorithm, GPS, which runs in O ( dk ) time. Since in practice d k , GPS is much more efficient than LP. Our experiments show that GPS generally yields a cost ( cost ( X ( I T ))) that is comparable to the one obtained by LP (See Figure 9a). GPS is therefore as effective as LP.

Given the graph G , GPS selects the joining plans one at a time. For each L [ P ]  X  I T , GPS picks the edge e whose weight w ( e ) is the smallest among all the edges labeled L [ P ]. Intuitively, this means that the joining plan picked requires the least number of disk block accesses among all the avail-able candidate plans for L [ P ]. For example, in Figure 5, edge e 3 is picked for L [( a, b, b, a )] because w ( e 3 is the smallest among all candidate plans for L [( a, b, b, a )] ( w ( e 1 ) = 23 and w ( e 5 )=14).

Once an edge e =( u , v ) is selected, GPS performs two steps: (1) it removes all the edges labeled L [ P ]from G and (2) it sets w ( u )= w ( v ) = 0 and the weights of any edges that are incident to either u or v are updated accordingly. The second step is done to reflect that the joining lists rep-resented by vertices u and v have been retrieved. If they are cached, future executions of any other joining plans that require either u or v as a joining list do not incur any ad-ditional I/O. For example, suppose GPS picks e 7 to mate-rialize L [( a, c, c, a )]. After that, GPS sets w ( v 8 update w ( e 8 ) to 0+4 = 4. Later, when GPS compares the three edges (plans) for materializing L [( c, a, a, c )] (i.e., e and e 10 ), e 8 will be picked because its weight is the smallest. Algorithm 6 shows the GPS algorithm. Algorithm GPS 2. Add e =( u, v ) with the minimum weight among all 3. Remove all other edges labeled L [ P ]from G ; 4. Set w ( u )= w ( v )=0;
In our discussion so far, we have assumed that for each inverted list L [ P ]  X  I T , there is at least one candidate join-ing plan for L [ P ] (and so there is at least one edge labeled L [ P ]in G ). If that is not the case, we fall back to the ( m -1) 2 joining plan. Recall that all length-2 inverted lists are materialized in our system. So, to make the ( m -1) 2plan a candidate plan (that satisfies condition C3), we need to first materialize the list L [ P 1 ], where P 1 is the length-( m -1) prefix of P . We consider all inverted lists in I T and collect all the length-( m -1) prefixes of those lists that are without a candidate joining plan into a set S . All the lists in S would then have to be materialized first. We can achieve that by simply executing Algorithm 6 recursively, replacing the set I T by S .

Another assumption we have made is that the memory buffer is big enough to cache all the lists in L (i.e., all the joining lists of the selected plans). This assumption implies that each required joining list is retrieved from disk only once even if it is used for multiple joins. For example, in Figure 5 the joining list L [( c, a )] (vertex v 8 ) is retrieved once for computing both L [( a, c, c, a )] ( e 7 )and L [( c, a, a, c )] ( e With this assumption, the formula given in Equation 1 cor-rectly captures the I/O cost ( cost ( X ( I T ))) of materializing the inverted index I T . For large datasets and a small mem-ory buffer, the assumption may not hold. A joining list re-trieved may be swapped out of the buffer before it is needed again to execute another join. In that case, the joining list is retrieved again and additional I/O is paid. To avoid such re-dundant I/O, we should wisely schedule the execution order of the joining plans. For example, the execution of the plan e should be followed soon by the execution of e 8 while the common joining list v 8 is still residing in the buffer. Given  X ( I T ), the set of selected joining plans for materializing I we call the problem of scheduling the execution order of the plans so as to avoid redundant I/O as much as possible the execution planning problem (EP). Given a set of selected joining plans ( X ( I T )), we can con-struct a graph G  X  G that contains all and only the edges in  X ( I T ). (See Figure 7 for an example G .) To tackle the EP problem, we propose a scheduling algorithm REP (Rule-based Execution Planning), which determines an execution order of the edges (joining plans) in G . REP follows three scheduling rules (to be discussed shortly). Given a joining list L x , it may be required for the execution of multiple joins. (For example, in Figure 7, the joining list L [( a, b )] ( v quired by four joins ( e 4 , e 5 , e 8 and e 9 ).) When the first of such joins, say J 1 , is executed, L x is retrieved and placed in the buffer. After the last of such joins, say J 2 ,isdone, L can be released from the buffer and need not be retrieved again. Let us call the time span between the executions of J 1 to J 2 the span of L x . To avoid redundant I/O, we should avoid swapping a joining list out of the buffer during its span. To achieve that, we should shorten a list X  X  span as much as possible so that the chances of a buffer overflow occurs during the list X  X  span, which may lead to a swap-out of the list, is small. Also, a short span implies that the mem-ory occupied by the list can be released earlier and so buffer overflow is less likely to occur. The three scheduling rules we will present next are based on this short-span principle . Since our scheduling algorithm REP operates on the graph G , in the following discussion, we describe REP in terms of vertices and edges , with the understanding that a  X  X ertex X  refers to a  X  X oining list X  and an  X  X dge X  refers to a  X  X oining plan X . Let L buf be a memory buffer where joining lists are cached. Given a set of selected joining plans  X ( I T ) (equiv-alently, the graph G ), the REP algorithm prioritizes the plans (equivalently, the edges in G ) by applying the follow-ing rules in order:
R1 :Anedge e i is executed ahead of another edge e j if more joining lists (end vertices) of e i are cached in L buf those of e j . For example, a plan with both of its joining lists cached in L buf is executed before another plan that has only one of its joining lists cached. With Rule R1, REP tries to finish executing all the joins that require the lists cached in L buf early and to delay bringing in new lists into the buffer. The former aims at terminating the spans of the lists in L buf early and the latter aims at starting the spans of the not-yet-retrieved lists late. Intuitively, this strategy tends to shorten lists X  spans in general.

R2 :Iftwoedges e i and e j are tied w.r.t. Rule R1, they have the same number of joining lists in L buf . Inthiscase, REP executes e i ahead of e j if the joining lists (end ver-tices) of e i are of smaller degrees than those of e j . We call this strategy smallest-degree-first . 4 Note that a joining list (vertex), say v , of large degree implies that it is involved in many joins (edges). The span of v is thus generally large. Bringing v to the buffer early would extend its span and allow it to hog the memory for an extensive period of time, making the buffer more likely to overflow. REP thus prefers executing edges that involve  X  X mall-degree X  vertices first.
R3 :Anedge e i is executed ahead of another edge e j if the total size of the joining lists that have to be retrieved from disk for e i is smaller than that for e j . For example, consider edges e 3 and e 6 in Figure 7. If v 3 is in L buf , then executing e and e 6 requires the retrievals of v 4 and v 7 , respectively. Since
See Algorithm REP, Figure 8, for details. Algorithm REP Input:  X ( I T ) (as represented by a graph G =( E ,V )).
Output: An execution order of the joins in  X ( I T ). 2. AssignScore ( G ); 3. while E =  X  4. e ( u, v )=lowest-scorededgein G ; 6. E = E -e ; 7. if deg ( u )=0 then L buf = L buf -{ u } ; 8. if deg ( v )=0 then L buf = L buf -{ v } ; 9. update edge scores due to the removal of e ;
Algorithm AssignScore 1. for each edge e ( u, v )  X  E 2. R1 ( e ) = number of joining lists in e that is not 3. R2 ( e )= { deg ( x ), where x  X  X  u, v } X  x  X  X  buf } ; 4. R3 ( e )= {| x | ,where x  X  X  u, v } X  x  X  X  buf } ; 5. score ( e )= R1 ( e ) , R2 ( e ) , R3 ( e ) ; 6. Sort all e  X  X  in ascending order of score ( e ), with R1 , w ( v 7 ) &lt;w ( v 4 ), e 6 is executed ahead of e 3 . By preferring joining plans that request shorter joining lists, REP avoids bringing large lists into the buffer early. This lowers the chances and extent of buffer overflow.

The REP scheduling algorithm is shown in Figure 8. To illustrate, let us apply REP on the graph G shown in Fig-ure 7. Initially, L buf =  X  , so Rule R1 does not apply. Ac-cording to Rule R2, either edge e 7 or e 10 should be executed first because their end vertices are of the lowest degrees. REP applies Rule R3 to break the tie and selects e 10 first because the total weight of its end vertices is 1+3=4, which is smaller than that of e 7 (5+3=8). When e 10 is being ex-ecuted, L buf = { v 7 ,v 11 } .After e 10 is done, v 11 is released from the buffer because it is no longer needed by any other plans. By Rule R1, e 6 is executed next, since it is the only plan that has a joining list in L buf . After that, L buf Now e 2 and e 3 are tied w.r.t. Rule R1. Since e 3  X  X  end ver-tices are of lower degrees, it is executed next. Now, L buf { v 3 ,v 4 } .ByRuleR2, e 7 is executed ahead of e 2 , and so on.
Note that joining lists are moved into and out of L buf as plans are executed. To avoid redundant I/O, we need enough memory to store the largest L buf . For our toy example, this memory requirement is 12 disk blocks. As a comparison, if one executes the joins in the order e 1 ,e 2 ,...,e 10 , the buffer requirement is 21 disk blocks, which is much more than that required by the execution order generated by REP. We re-mark that one may come up with other scheduling rules. The ones we have presented (R1-R3) are examples which show that intelligently scheduling the plans X  execution order can significantly reduce the size of the memory requirement. With limited buffer space, this benefit translates to more I/O-efficient query processing. This section presents the results of the experiments we con-ducted on our prototype S-OLAP system. The prototype was implemented in JAVA 1.6 on a machine with a 2.83GHz Figure 9: Comparing JPS algorithms (real data) Intel Core 2 Duo CPU and 4GB RAM running Linux 2.6.24 kernel. In the experiments, we set the disk block size to 1KB. We use least-recently-used (LRU) as the replacement policy for the joining list buffer ( L buf ). Experiments are done on both real data and synthetic data. We focus on the two I/O-aware algorithms, namely, GPS and REP and study their efficiency and effectiveness in reducing the I/O cost of query evaluation compared with the BASIC algorithm The real sequence data is a clickstream and purchase dataset of Gazelle.com, a legwear and legcare web retailer. The data was prepared by [8] for KDD Cup 2000. Each tuple in the data file is a visitor click event (sorted by user sessions). The original data is 238.9MB large and consists of 164,364 click events. Each event is captured by 215 attributes. Three example attributes are session-id , request-time and page-category . These attributes identify a user session, its first access time, and the accessed page, respectively. We cleaned the data manually and removed click sequences that were generated from web crawlers (e.g., user sessions with thou-sands of clicks). After this step, an event database of 148,924 click events was obtained forming 50,524 sequences.
The query workload consists of 100 random queries. A pattern template T is randomly generated for each query. Pattern symbols are defined on the attribute page-category , whose domain consists of 24 distinct values. To generate atemplate T , we first pick a length m , which is uniformly distributed over the range [3 ... 6]. Next, the number of distinct pattern symbols n in T is randomly picked uni-formly over the range [1 ... min { m, 4 } ]. Finally, a pattern template is randomly selected from all possible pattern tem-plates of length m with n distinct symbols (e.g., the template ( X, Y, Y, X ) is of length 4 with 2 distinct symbols). We use COUNT as the aggregate function. Table 2 shows the query parameters and their default values.
 We first evaluate algorithms for the JPS problem. They are (1) BASIC, which uses the static ( m  X  1) 2 joining plans, (2) LP, which selects joining plans by solving the MWMCSP problem using LP-rounding technique, and (3) GPS, which solves MWMCSP using a greedy strategy.
Given an inverted index I T , a JPS algorithm selects one candidate joining plan for each inverted list in I T .Recall that a candidate joining plan is one whose joining lists are both materialized (i.e., in L MAT ). The set L MAT thus deter-mines how many candidate joining plans are available for the selection algorithms to choose from. This, in turn, affects the set of selected plans  X ( I T ) and its I/O cost cost ( X ( I
Let P be the set of all possible joining plans for materi-alizing the inverted lists in I T (i.e., those that satisfy Con-ditions C1 and C2). For example if L [( a, b, b, a )]  X  I L [( a, b, b, a )] = L [( a, b )] L [( b, b, a )] is a plan in JL be the set of all joining lists that appear in the plans of
P , and let JL i be those whose patterns are of length i (e.g., L [( a, b )]  X  JL 2 and L [( b, b, a )]  X  JL 3 ). Recall that we assume all lists of length-2 patterns are pre-computed, therefore, JL 2  X  L MAT .Further,let JL = JL  X  JL 2 be the set of all other joining lists. Define  X  = | JL  X  L MAT | i.e., the fraction of the joining lists in JL that are materi-alized. Our first experiment studies the performance of the JPS algorithms as  X  varies. Note that when  X  =0,only joining lists of length-2 patterns are materialized; when  X  = 1, all joining lists in JL are materialized and so all plans in P are candidate plans (satisfying Condition C3) for the al-gorithms to pick. In this experiment, we use a large memory buffer so that joining lists retrieved do not get swapped out of the buffer during query processing.

Figure 9(a) shows the number of disk blocks accessed by the three algorithms as  X  varies from 0.25 to 1. Although not shown in the figure, when  X  = 0, all three algorithms access the same number of disk blocks. To see why, consider any list L [ P ]  X  I T to be materialized. If m = 3, pattern P consists of three symbols and so only one candidate plan, L [ P ]= L [ P 1 ] L [ P 2 ], where P 1 and P 2 are the length-2 prefix and the length-2 suffix of P , exists. Any JPS algorithm will execute this plan. If m  X  4, then no candidate plans are available because any such plan requires some joining lists whose patterns are length-3 or longer. These lists are not materialized because  X  = 0. In this case, the JPS algorithms are recursively invoked to materialize L [ P ], where P is the length-( m -1) prefix of P , first. By induction, when  X  =0, all three JPS algorithms retrieve the same lists from disk to execute the same joins. They thus have the same I/O cost.
As  X  increases, more joining lists are materialized. This has two effects. First, it is more likely that at least one candidate plan is available for materializing L [ P ] and so no recursive calls of the JPS algorithms are needed to obtain the ( m -1)-prefix list. This explains why the I/O costs of all three algorithms go down as  X  increases. Second, there are more candidate plans available for GPS or LP to choose. It is thus more likely that these smart algorithms are able to pick some sets of low-cost plans. This explains why the gaps between BASIC and GPS or LP widen as  X  increases. This I/O cost reduction is very significant, particularly when  X  is large. For example, when  X  = 1, the I/O costs under LP and GPS are about 1/5 of BASIC X  X . We remark that in practice, the joining lists in JL are often materialized and available. This is especially true if the user is exploring the s-cube space by incrementally modifying his PBA query, for example, by adding a symbol to extend the query pattern template. In this case, inverted lists of shorter patterns are materialized due to the evaluation of previous queries.
Note that the curves for LP and GPS are very close to each other throughout the whole range of  X  in Figure 9(a). This shows that the set of joining plans selected by GPS is as good
Figure 10: Comparing EP algorithms (real data) as that of LP in terms of I/O cost. However, LP is much slower than GPS. Figure 9(b) shows the query execution times of the three algorithms, with the fraction attributable to plan selection (e.g., solving MWMCSP) shaded. From the figures, we see that GPS is much more efficient than LP. We also see that LP spends so much time in plan selection that it is counter productive. On the other hand, due to its relatively low computation overhead, GPS gives a much lower overall query processing time. Our second experiment evaluates execution planning (EP) algorithms. Given a set of selected joining plans  X ( I T )ob-tained by GPS, we consider two algorithms for determin-ing the joins X  execution order. They are (1) REP, our rule-based planner, and (2) RANDOM, which executes the joins in  X ( I T ) in some random order. Recall that the join ex-ecution order affects the amount of redundant I/O when the memory buffer is limited (see Section 5). Therefore, we compare the I/O costs of REP and RANDOM for processing our query workload under different buffer sizes ( B ). In this experiment, we set  X  =0.5.

Figure 10 shows the average number of disk blocks ac-cessed by our system per query under REP and RANDOM. When there is no joining list buffer (i.e., B = 0), a joining list that participates in k joins is retrieved from disk k times, regardless of the joins X  execution order. Therefore, REP and RANDOM have the same I/O cost. As B increases, we see that the curve for REP drops sharply from 45,600 blocks (when B = 700) to 5,600 blocks (when B =800). This shows that REP is very successful in putting joins that share common joining lists together so that even a small buffer al-lows joining lists to be effectively cached. This significantly avoids redundant I/O. On the other hand, the curve for RANDOM drops at a much lower rate  X  only when B is increased to about 4000 does its I/O cost reach the level of REP. The per-query memory requirement under RANDOM is thus 5 times bigger than that of REP. Moreover, when the per-query buffer is small (e.g., when B = 1,000), the I/O cost under REP is 1/21 of RANDOM. This experiment shows that our rule-based execution planner REP is very effective. To further evaluate our techniques, we have also carried out experiments on synthetic data and synthetic query work-loads. The synthetic sequence data generator is controlled by four parameters: L , M ,  X  ,and D . Specifically, a syn-thetic dataset consists of D sequences. To generate a data sequence s , we first determine its length l , which is randomly generated from a Poisson distribution with mean L .Sym-bols in sequence s are picked from a domain of M symbols. The first symbol of s is randomly selected according to a Zipf distribution with skew factor  X  . Subsequent events are generated according to a Markov chain of order one. Each probability vector in the transition probability matrix fol-lows a Zipf distribution. Table 3 shows the parameters and their default values. Table 3: Synthetic data parameters and default values
In the following discussion, we use  X  X /O-aware X  to refer to the applications of our two I/O-cognizant techniques, i.e., GPS and REP. We use  X  X aseline X  to refer to the I/O-oblivious alternative, which uses BASIC for the JPS prob-lem and RANDOM for the EP problem. We compare the I/O-aware approach with Baseline by varying the four pa-rameters listed in Table 3. When we vary one parameter, the other three are fixed at their default values. The query workload is generated in the same manner as in the real-data experiments with  X  = 0.5. We use  X  X umber of disk blocks ac-cessed X  as the performance metrics. In each experiment, two performance curves are displayed for each approach: one for a limited buffer scenario (2MB allocated to each query) and another for an unlimited buffer scenario. Section 6.2.1 shows the results of varying the data characteristics and Section 6.2.2 shows the results of varying the query characteristics. Varying D . Figure 11(a) shows the results of varying the dataset size ( D ) from 100K sequences to 1M sequences. When D increases, the joining lists become longer (more sequences contain a certain pattern) and the I/O cost of re-trieving them is higher. From the figure, we see that our I/O-aware approach performs much better than the Base-line approach. For example, at D = 1M sequences and with a 2MB per-query buffer, the I/O cost of I/O-aware is about 4 times smaller than that of Baseline. This is because the memory requirement of Baseline exceeds the given buffer. Joining lists are often swapped out of the buffer due to over-flow even if these lists are needed for the execution of more joins later. This leads to redundant I/O and hence a high I/O cost. The I/O-aware approach, on the other hand, has a small memory requirement, which allows it to work com-fortably over a small buffer. As a result, the performance of I/O-aware under a limited 2MB buffer is comparable to that of Baseline under unlimited buffer.
 Varying L . Figure 11(b) shows the results of varying the average sequence length. A longer sequence contains more patterns. Therefore, with longer sequences in the dataset, a pattern is expected to occur in more data sequences. This results in larger inverted lists and hence larger I/O costs. The effect is thus very similar to that of increasing D . Again, we see that I/O-aware performs much better than Baseline, especially when buffer space is limited. For example, I/O-aware is 3.3 times faster than Baseline when L =50. Varying M . Figure 11(c) shows the results of varying the domain size of the pattern symbols. Given a pattern tem-plate T that contains n distinct symbols, the number of cells in the s-cuboid (or equivalently, the number of lists in I is M n .Alarger M therefore implies more inverted lists to be materialized resulting in a higher I/O cost. This ex-plains why the curves in Figure 11(c) go up as M increases. Again, the I/O-aware approach outperforms Baseline by a wide margin.
 Varying  X  . Figure 11(d) shows the the results of vary-ing the data skewness. When data is more skewed (larger  X  ), some patterns occur much more frequently than others. As a result, the sizes of the inverted lists vary significantly from one pattern to another. Different joining plans, which involve different joining lists, are therefore likely to incur very different costs. Our I/O-aware approach, which em-ploys GPS as a smart join-plan-selection algorithm, is able to select low-cost plans in general. This explains why the curves for our I/O-aware approach drop as  X  increases. On the other hand, Baseline uses a static ( m -1) 2strategyand is not able to take advantage of the availability of lower-cost plans. Its performance is thus not very sensitive to  X  . Varying n . Figure 12(a) shows the I/O costs of Baseline and I/O-aware as the number of distinct pattern symbols ( n ) in the query workload (e.g., n = 2 for the query tem-plate ( X, Y, Y, X )) varies from 1 to 4. In this experiment, the query template length m is fixed at 4. Again, we see that I/O-aware is more I/O efficient. Figure 12(b) shows the I/O speedup defined as the I/O-cost of Baseline over that of I/O-aware. For example, when n = 4 and with a 2MB per-query buffer, I/O-aware achieves a 3.4 times speedup in I/O over Baseline. Note that n is the number of distinct pattern sym-bols so when n increases, there are more cells in the s-cuboid. Therefore, the system needs to materialize more inverted lists and hence the I/O costs generally increase as shown in Figure 12(a). When n = 4, we observe that I/O-aware signif-icantly outperforms Baseline in the limited buffer scenario. The reason is that in this experiment m = 4 and so when n = 4, there is only one query template, say, ( W, X, Y, Z ). Note that all four symbols in the template are distinct and therefore they are not restricted to the same binding in the instantiations. As a result, any joining list retrieved for one join can be heavily reused in many others. For example, af-the joining list L [( a, b, c )] retrieved can be used to execute any join of the form L [( a, b, c,  X  )] = L [( a, b, c )] L [( c, where  X  is any value of the pattern symbol domain. In this case, our REP algorithm tends to schedule the executions of these joins close together so that the span of L [( a, b, c )] is small. The Baseline approach, however, adopts a RANDOM strategy for the EP problem, which cannot effectively utilize the buffer cache to avoid redundant I/O when it is given a limited buffer. The curve of Baseline thus rises sharply when n =4 in the limited buffer scenario.
 Varying m . Figure 12(c) shows the I/O costs as the length of the pattern templates ( m ) varies from 3 to 6. In this ex-periment, the number of distinct pattern symbols is 3 and  X  =0 . 5. Note that m = 3 is a special case for which only one candidate plan, L [ P ]= L [ P 1 ] L [ P 2 ], where P are the length-2 prefix and the length-2 suffix of P , exists. Therefore both Baseline and I/O-aware select the same join-ing plan. This explains why both Baseline and I/O-aware have similar I/O costs when m =3. Wealsoobservethat I/O-aware outperforms Baseline in the entire range of m . The reason is that given an inverted list L [ P ]  X  I T , Baseline attempts to materialize it using the ( m -1) 2 joining plan. If this plan is not available (i.e., the joining list whose pat-tern is the ( m -1)-prefix of P is not materialized yet), Base-line will have to recursively materialize the prefix-pattern list first (which may require further recursive joins, e.g. to materialize the ( m -2)-prefix list of P , etc.). On the other hand, since I/O-aware considers any possible joining plans in addition to the ( m -1) 2 one, it is very likely that some of the possible plans are available (i.e., their joining lists are materialized). In this case, I/O-aware can avoid the costs of executing recursive joins. The I/O cost of I/O-aware is thus significantly smaller than that of Baseline.

Figure 12(d) shows the I/O speedup achieved by I/O-aware over Baseline. We observe that the saving increases as m increases. It is because the number of possible joining plans is proportional to m 2 . So, a larger m implies more candidate joining plans for I/O-aware to choose from. This allows I/O-aware to pick a low-cost plan. This paper studied the problem of sequence cuboid materi-alization. In particular, we studied the joining plan selection problem and the execution planning problem. We proposed the GPS algorithm and the REP algorithm to solve the two problems, respectively. Through extensive experiments, we showed that our I/O-aware algorithms significantly reduce the I/O cost of query processing.
 This work was supported by the Research Grants Council of Hong Kong (GRF Projects 513508, 711309E and 713008E). We would like to thank the anonymous reviewers for their insightful comments.
