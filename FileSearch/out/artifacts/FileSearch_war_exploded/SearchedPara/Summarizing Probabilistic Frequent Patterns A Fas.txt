 Mining probabilistic frequent patterns from uncertain data has received a great deal of attention in recent years due to the wide applications. However, probabilistic frequent pattern mining suffers from the problem that an exponen-tial number of result patterns are generated, which seriously hinders further evaluation and analysis. In this paper, we focus on the problem of mining probabilistic representa-tive frequent patterns (P-RFP), which is the minimal set of patterns with adequately high probability to represent all frequent patterns. Observing the bottleneck in checking whether a pattern can probabilistically represent another, which involves the computation of a joint probability of the supports of two patterns, we introduce a novel approxima-tion of the joint probability with both theoretical and em-pirical proofs. Based on the approximation, we propose an Approximate P-RFP Mining (APM) algorithm, which effec-tively and efficiently compresses the set of probabilistic fre-quent patterns. To our knowledge, this is the first attempt to analyze the relationship between two probabilistic fre-quent patterns through an approximate approach. Our ex-periments on both synthetic and real-world datasets demon-strate that the APM algorithm accelerates P-RFP mining dramatically, orders of magnitudes faster than an exact so-lution. Moreover, the error rate of APM is guaranteed to be very small when the database contains hundreds transac-tions, which further affirms APM is a practical solution for summarizing probabilistic frequent patterns.
 H.2.8 [ DATABASE MANAGEMENT ]: Database Ap-plications X  Data Mining Algorithms  X  Centre for Quantum Computation and Intelligent Systems Pattern Summarization, Uncertain Data
Data uncertainty is inherent in various applications such as sensor network monitoring, moving object tracking, and protein-protein interaction data [6]. It could be induced by different reasons including experimental error, artificial noise, and data incompleteness.Rather than cleaning the un-certain data using domain-specific rules, modeling the un-certainty of data is more rational in many applications, such as medical diagnosis and risk assessment. As a consequence, data mining over uncertain data has become an active re-search area recently. A survey of state-of-the-art uncertain data mining techniques may be found in [1].

As one of the most fundamental data mining tasks, fre-quent pattern mining has also been introduced into uncer-tain databases [3] and received a great deal of research atten-tion [4, 5, 6, 10, 11, 12]. Generally, there exist two different definitions of frequent patterns in the context of uncertain data: expected support-based frequent patterns [3, 11], and probabilistic frequent patterns [4, 5]. Both definitions con-sider the support of a pattern as a discrete random variable. The former uses the expectation of the support as the mea-surement, while the latter considers the probability that the support of a pattern is no less than some specified mini-mum support threshold. Despite the different frequentness metrics employed, both the expected support-based frequent patterns and the probabilistic frequent patterns enjoy the anti-monotonic property [3, 4]. That is, if a pattern is fre-quent in an uncertain database, then all of its sub-patterns are frequent as well. This property leads to the generation of an exponential number of result patterns. The large num-ber of discovered frequent patterns makes the understanding of, and further analysis of generated patterns troublesome. Therefore, similar to the counterpart of the problem in de-terministic data, it is indeed important to find a small num-ber of representative patterns to best approximate all other probabilistic frequent patterns.

Some initial research work has been undertaken to find a small set of representative patterns. For example, min-ing probabilistic frequent closed patterns over uncertain data has been studied in [7, 8, 9]. However, the number of prob-abilistic frequent closed patterns is still large because of the restrictive condition for a pattern being closed . For instance, in [9], the closed probability of a pattern is computed as the sum of the probabilities of the possible worlds of an uncer-tain database where the pattern is closed.
In the context of deterministic data, Xin et al. [20] has proposed the notion of a  X  -covered relationship between pat-terns as a generalization of the concept of frequent closed patterns to further reduce the size of closed patterns. A pattern X 1 is  X  -covered by another pattern X 2 if X 1 is a subset of X 2 and (Supp( X 1 )  X  Supp( X 2 )) / Supp( X 1 The goal is then to find a minimal set of representative pat-terns that can  X  -cover all frequent patterns.

Motivated by this idea in deterministic data, in our previ-ous work, we have proposed to relax the restrictive condition of probabilistic frequent closed patterns to mine probabilis-tic representative frequent patterns (P-RFP) [25]. In par-ticular, we extend the concept of  X  -cover to define the (  X ,  X  ) -covered relationship between probabilistic frequent patterns, addressing the fact that the support of a pattern becomes a discrete random variable in an uncertain database. Infor-mally, a pattern X 1 is (  X ,  X  )-covered by another pattern X in an uncertain database if X 1 is a subset of X 2 ,andthe probability that the support distance between X 1 and X 2 no greater than  X  is no less than  X  .

We have devised a dynamic programming-based approach to discover the minimal set of P-RFPs. Although this ap-proach can compute exactly the probability that the sup-port distance between two patterns is no greater than  X  ,it is not sufficiently efficient due to the bottleneck in examin-ing whether a pattern (  X ,  X  )-covers another, which involves the computation of a joint probability of the supports of the two patterns. In this work, we analyze that the joint support probability follows a joint Poisson binomial distri-bution with both theoretical and empirical proofs. Based on the analysis, we propose an Approximate P-RFP Mining (APM) algorithm that performs outstandingly faster than the dynamic programming-based exact approach.

To our knowledge, this is the first attempt to analyze the relationship between two probabilistic frequent patterns through an approximate approach. Our experimental results show that our approach summarizes frequent patterns effi-ciently and effectively, and restores the patterns and their original frequency probability information with a guaran-teed error bound. To summarize, our contributions are as follows.
The remainder of the paper is structured as follows. The next section reviews existing works related to this paper. We define important concepts and provide the problem state-ment in Section 3. Section 4 describes the proposed data mining approach. The theoretical proof of the approxima-tion of the joint support probability is demonstrated in Sec-tion 5. We evaluate the performance of the proposed ap-proach in Section 6 and close this paper with some conclu-sive remarks in Section 7.
In this section, we review related research from two sub-areas: frequent pattern mining over uncertain data and fre-quent pattern summarization.

Frequent pattern mining over uncertain data .Min-ing frequent patterns from uncertain databases has been studied extensively in the past years. Existing work on fre-quent pattern mining from uncertain data falls into two cat-egories: expected support-based frequent pattern mining [3, 10, 11] and probabilistic frequent pattern mining [4, 5]. The former utilizes the expectation of support as the frequentness metric. That is, a pattern is frequent only if its expected support is no less than a specified minimum expected sup-port. The latter considers the frequency probability as the measurement, which refers to the probability that a pattern appears no less than a specified minimum support times. Thus, a pattern is frequent only if its frequency probabil-ity is no less than a specified minimum probability (i.e. Pr(Supp( X )  X  minsup )  X  minprob ).

There are three representative algorithms for mining ex-pected support-based frequent patterns: UApriori [3], UFP-growth [10] and UH-Mine [11]. UApriori is the uncertain version of the well-known Apriori algorithm. Both UFP-growth and UH-Mine employ the divide-and-conquer frame-work that searches frequent patterns with depth-first strat-egy. For mining probabilistic frequent patterns, there are two representative algorithms: DP  X  dynamic programming-based Apriori algorithm [4], and DC  X  divide-and-conquer-based Apriori algorithm [5]. Recently, Tong et al. [6] verified that the two types of definitions of frequent patterns mined from uncertain data are closely related from a mathemati-cal perspective and can be unified when the size of data is sufficiently large.

Considering that the support of a pattern in an uncertain database follows a Poisson binomial distribution, some ap-proximate algorithms for mining probabilistic frequent pat-terns have been proposed as well. For example, both the Normal and Poisson distribution have been used to approx-imate the frequency probabilities of patterns [12, 13]. Com-pared with our work, existing approximate approaches focus on the approximation of the support probability of only one pattern. The approximation of joint probability of the sup-ports of two patterns is much more challenging because the dependency of two random variables needs to be taken into account.

Frequent pattern summarization . Motivated by the fact that frequent pattern mining may generate an expo-nential number of patterns due to the anti-monotonicity, numerous research work has been dedicated to frequent pat-tern summarization, which aims to obtain a much smaller set of patterns to represent the complete set of frequent pat-terns. A variety of definitions have been proposed, such as maximal patterns [14], frequent closed patterns [15] and non-derivable patterns [16]. While all frequent patterns can be recovered from maximal patterns, the loss of support infor-mation is unacceptable in some circumstances. For frequent closed patterns, although the exact support of all frequent patterns can be preserved, the number of frequent closed patterns can still be tens of thousands, or even more. There are several generalizations of closed patterns, such as the pattern profiling-based approaches [17, 18, 19] and the sup-port distance-based approaches [20, 21]. It was observed in [21] that the profile-based approaches [17, 18] have some drawbacks, such as no error guarantee on restored support. Hence, in our work, we borrow the framework of the support distance-based approaches to find probabilistic representa-tive frequent patterns.

Recently, some research work has been undertaken to sum-marize frequent patterns in the context of uncertain data. Tang and Peterson [8] proposed mining probabilistic fre-quent closed patterns, based on the concept called probabilis-tic support . Tong et al. [9] pointed out that frequent closed patterns defined on probabilistic support cannot guarantee the patterns are closed in possible worlds which contribute to their probabilistic supports. Instead, they defined the threshold-based frequent closed patterns over probabilistic data, which considers the probabilities of possible worlds where a pattern is closed. Our research relaxes the condi-tion to further reduce the size of patterns by considering the probabilities of possible worlds where a pattern can  X  -cover another one.
In this section, we review the relevant concepts introduced in previous work and formally state the problem of proba-bilistic representative frequent pattern (P-RFP) mining.
Xin et al. [20] defined a robust distance measure between patterns in deterministic data. definition 1. ( distance measure ) Given two patterns X 1 and X 2 , the distance between them, denoted as dist( X 1 is defined as 1  X  X  T ( X 1 )  X  T ( X 2 ) | / | T ( X 1 )  X  T ( X i ) is the set of transactions supporting pattern X Then, an  X  -covered relationship is defined on two patterns where one subsumes another. definition 2. (  X  -covered ) Givenarealnumber  X   X  [0 , 1] and two patterns X 1 and X 2 ,wesay X 1 is  X  -covered by X if X 1  X  X 2 and dist( X 1 ,X 2 )  X   X  .

Itcanbeprovedeasilythat,if X 2  X  -covers X 1 ,then (Supp( X 1 )  X  Supp( X 2 )) / Supp( X 1 )  X   X  . The goal of repre-sentative frequent pattern mining then becomes finding the minimal set of patterns that  X  -cover all frequent patterns. In the context of uncertain data, the support of a pattern, Supp( X i ), becomes a discrete random variable. Therefore, we cannot directly apply the  X  -cover relationship to proba-bilistic frequent patterns. Before explaining how to extend the concept of  X  -covered in the context of uncertain data, we examine an uncertain database where attributes are as-sociated with existential probabilities.

Table 1 shows an uncertain transaction database where each transaction consists of a set of probabilistic items. For example, the probability that item a appears in the first transaction T 1 is 0 . 7. Possible world semantics are com-monly used to explain the existence of data in an uncertain database. For example, the database in Table 1 has eight possible worlds, which are listed in Table 2. Each possible world is associated with an existential probability. For in-stance, the probability that the first possible world w 1 is (1  X  0 . 7)  X  (1  X  0 . 2)  X  1  X  (1  X  0 . 5) = 0 . 12.
Considering that the occurrences of items in every possi-ble world are deterministic, we can define the probabilistic distance between two probabilistic frequent patterns based on their distance in possible worlds. definition 3. ( probabilistic distance measure ) Given an uncertain database D , and two patterns X 1 and X 2 ,let PW = { w 1 ,...,w m } be the set of possible worlds derived from D , the distance between X 1 and X 2 in a possible world w j  X  X W is where T ( X i ; w j ) is the set of transactions containing pattern X i in the possible world w j . Then, the probabilistic distance between X 1 and X 2 , denoted by dist( X 1 ,X 2 ) , is a random variable. The probability mass function of dist( X 1 ,X 2 That is, the probability that the distance between two prob-abilistic frequent patterns is d can be computed by the sum of the probabilities of corresponding possible worlds. For example, consider the uncertain database in Table 1. Let X 1 = { a } and X 2 = { a, b } . The probability that the dis-tance between X 1 and X 2 is equal to 0 . 5, Pr(dist( X 1 0 . 5), can be computed by adding the probabilities of the possible worlds w 4 and w 8 . This is because only in the two possible worlds, the distance between the two patterns is 0 . 5. Therefore, Pr(dist( X 1 ,X 2 )=0 . 5) = 0 . 14.
Based on the probabilistic distance measure, we define the  X  -cover probability as follows. definition 4. (  X  -cover probability ) Given an uncertain database D , two patterns X 1 and X 2 , and a distance thresh-old  X  ,the  X  -cover probability of X 1 and X 2 is defined as Pr cover ( X 1 ,X 2 ;  X  )=Pr(dist( X 1 ,X 2 )  X   X  ) . definition 5. ((  X ,  X  ) -covered ) Given an uncertain database D , two patterns X 1 and X 2 , a distance threshold  X  and a  X  -cover probability threshold  X  , X 2 (  X ,  X  ) -covers X 1 if and only if X 1  X  X 2 and Pr cover ( X 1 ,X 2 ;  X  )  X   X  .

Our goal is then to obtain the minimal set of patterns that will (  X ,  X  )-cover all the probabilistic frequent patterns. The formal statement of the probabilistic representative frequent pattern (P-RFP) mining is as follows. definition 6. ( Problem Statement ) Given an uncertain database D , a set of probabilistic frequent patterns F , a prob-abilistic distance threshold  X  and a  X  -cover probability thresh-old  X  , the problem of probabilistic representative frequent pat-tern (P-RFP) mining is to find the minimal set of patterns R so that, for any frequent pattern X  X  X  ,thereexistsa representative pattern X  X  X  where X (  X ,  X  ) -covers X .
It is obvious that when  X  = 0, the probabilistic represen-tative pattern set is equivalent to the set of probabilistic closed patterns, and when  X  = 1, it is the same as proba-bilistic maximal pattern set.
This section first describes the framework of our proposed approach. Then, we explain the details of the main steps of the Approximate P-RFP Mining (APM) algorithm.
Before presenting the framework of our approximate ap-proach for P-RFP mining, we develop some important lem-mas between two patterns where one (  X ,  X  )-covers another.
Lemma 1. Given an uncertain database D and two pat-terns X 1 and X 2 s.t. X 2 (  X ,  X  ) -covers X 1 , the distance be-tween X 1 and X 2 in the possible world w j can be represented by the support of the patterns in w j :
Lemma 2. Given an uncertain database D and two pat-terns X 1 and X 2 s.t. X 2 (  X ,  X  ) -covers X 1 , the probabilistic distance dist( X 1 ,X 2 ) can be represented by the support dis-tribution of X 1 and X 2 :
Lemma 3. Given an uncertain database D and two pat-terns X 1 and X 2 s.t. X 2 (  X ,  X  ) -covers X 1 , we have These lemmas are obvious expansions of the concepts in de-terministic data. The detailed proofs are stated in [25]. Lemma 4. Given an uncertain database D , two patterns X 1 and X 2 , a support threshold minsup and a frequency probability threshold minprob ,if X 2 (  X ,  X  ) -covers X 1 X 1 is a probabilistic frequent pattern w.r.t. minsup and minprob ,then X 2 is a probabilistic frequent pattern w.r.t. (1  X   X  ) minsup and (  X   X  minprob ) .

Proof. Since X 1 is a probabilistic frequent pattern w.r.t. minsup and minprob , we have Pr (Supp( X 1 )  X  minsup )  X  minprob , which infers, From Lemma 3, we have,
Consider that the events in equation 6 and 7 are indepen-dent, we have Pr (Supp( X 2 )  X  (1  X   X  ) minsup )  X   X   X  minprob . That is, X 2 is a probabilistic frequent pattern w.r.t. ((1  X  ) minsup )and(  X   X  minprob ).

Denoting the set of probabilistic frequent patterns as F , lemma 4 indicates that if pattern X can (  X ,  X  )-cover another pattern Y in F ,then X must be probabilistic frequent w.r.t. (1  X   X  ) minsup and minprob . We call such a pattern pseudo probabilistic frequent and denote the set of pseudo proba-bilistic frequent patterns as  X  F . In order to achieve the mini-mal set of probabilistic representative frequent patterns, we have to find a subset of  X  F that can (  X ,  X  )-cover all patterns of F . Given the two sets F and  X  F , our approach for P-RFP mining consists of the following two steps. 1. Generate the cover set for every pattern in  X  F .Foreach 2. Find the minimal pattern set R  X   X  F to (  X ,  X  )-cover all
After finding the cover sets for patterns in  X  F in the first step, the second step is equivalent to finding a minimal num-ber of cover sets that cover all patterns in F .Thisisknown as a set-covering problem, which is NP-hard. Similar to [21] and [25], we adopt a well-known greedy set-covering algo-rithm [22], which achieves polynomial complexity. There-fore, in the following, we focus on describing the first step, which generates the cover set for each pseudo probabilistic frequent pattern in  X  F .
To generate the cover set for a pattern X 2 in  X  F ,foreach pattern X 1 in F such that X 1  X  X 2 , we need to check if X (  X ,  X  )-covers X 1 . That is, we need to examine whether the  X  -cover probability between X 1 and X 2 is no less than  X  (i.e., Pr(dist( X 1 ,X 2 )  X   X  )  X   X  ). According to Lemma 3, the  X  -cover probability Pr cover ( X 1 ,X 2 ;  X  )=Pr(dist( X 1 ,X is equivalent to Pr(Supp( X 2 )  X  (1  X   X  )Supp( X 1 )). Then, the  X  -cover probability between X 1 and X 2 is equal to the following sum. To compute the  X  -cover probability to find out whether it is no less than  X  , we introduce the joint support probability distribution as follows. definition 7. ( joint support probability ) Given an uncer-tain database D and patterns X 1 and X 2 , the joint support probability mass function is
Pr (Supp( X 1 )= l, Supp( X 2 )= k )=
Although definition 7 implies a brute-force solution, it is not feasible to implement because the number of possible worlds is exponential. Therefore, we establish the following approximation of joint support probability.

Theorem 1. Given an uncertain database D and pat-terns X 1 and X 2 , the joint support probability can be ap-proximated by a bivariate normal distribution, which means
Pr(Supp( X 1 )= l, Supp( X 2 )= k )  X   X   X   X  1 2 ( X  X   X  ) (9) where X = lk T ,  X  is the vector of mean values of Supp( X and Supp( X 2 ) ,and  X  is the covariance matrix of X 1 and X .

Theorem 1 provides a solution to compute the joint sup-port probability of a pair of patterns via normal distribution, rather than mining in the complete database. The detailed theoretical proof is elaborated in Section 5, and the empirical simulation is illustrated in Section 6. Similar to univariate normal distribution, we can optimize our approach with the well-known 3  X  property [23].
Corollary 1. Given an uncertain database D ,andpat-terns X 1 and X 2 , let the mean value and variance of Supp( X be  X  j ,  X  2 j , j =1 , 2 , l 1 =max { minsup,  X  1  X  3  X  1 min {| D | , X  1 +3  X  1 } , k 1 =max { (1  X   X  ) l , X  2  X  3  X  k =min { l,  X  2 +3  X  2 } ,then  X 
Note that for better precision, we use  X  1 to calculate the support lower bound and upper bound for both X 1 and X 2 because the contour of bivariate normal distribution is an ellipse, and  X  1 is the length of semi major axis. Based on corollary 1, we can reduce the computational complexity of accelerate the progress of cover set generation further, we also take advantage of some optimization strategies in [25]. Lemma 5. Given an uncertain database D , two patterns X 1 and X 2 s.t. X 1  X  X 2 , and a probabilistic distance thresh-old  X  , Pr cover ( X 1 ,X 2 ;  X  ) computed on D is equal to that on D ( X 1 ) ,where D ( X 1 ) is { t | P ( X 1  X  t ) &gt; 0 ,t Lemma 5 is intuitive because only the transactions support-ing at least the sub-pattern X 1 will contribute to the value of probabilistic distance, which in turn affects the  X  -cover probability. This lemma allows us to compute the  X  -cover probability on a projected sub-database, which significantly reduces the runtime of computation.

Lemma 6. Given an uncertain database D and two pat-terns X 1 and X 2 s.t. X 1  X  X 2 ,if X 2 (  X ,  X  ) -covers X  X  X s.t. X 1  X  X  X  X 2 , we have X 2 (  X ,  X  ) -covers X . According to Lemma 6, we have the following corollary.
Corollary 2. Given an uncertain database D and two patterns X 1 and X 2 , X 1  X  X 2 ,if X 2 cannot (  X ,  X  ) -cover X then  X  X  X  X 1 , X 2 cannot (  X ,  X  ) -cover X .
 Lemma 6 and corollary 2 reduce the number of pattern pairs, for which the  X  -cover probability needs to be computed. The complete proofs of lemma 5, lemma 6 and corollary 2 are stated in [25]. The overall framework of our APM algorithm is shown in Algorithm 1. From line 3 to line 9, we find the cover set for each pseudo probabilistic frequent pattern X 2 in  X  F .The most important step is to check whether X 2 covers X 1 in F (line 6). The details of the function isCover is illustrated in Algorithm 2, where lines 1  X  3 implement the optimization stated by Lemma 6, and lines 4  X  6 apply the Corollary 2. Fi-nally, from line 7 to line 12, we use the approximation-based scheme to compute the  X  -cover probability. As mentioned before, the function setCover in Algorithm 1 is solved using the greedy algorithm in [22].
In this section, we present the detailed proof of the bi-variate normal distribution-based approximation of the joint support probability of two patterns. Given an uncertain database D , two patterns X 1 and X 2 ,s.t. X 1  X  X 2 ,andthe Algorithm 1 APM Algorithm Framework Input: D , F ,  X  F ,  X  and  X  Output: Minimal P-RFP Set R 1: R  X   X  2: CoverSets  X   X  3: for all X 2  X   X  F do 4: NoCoverSet  X   X  5: for all X 1  X  F such that X 1  X  X 2 do 6: if isCover ( X 1 ,X 2 )= True then 7: CoverSets [ X 2 ] .add ( X 1 ) 8: else 9: NoCoverSet.add ( X 1 ) 10: R = setCover ( CoverSets, F ) 11: return R Algorithm 2 Function isCover Input: X 1 ,X 2 , Output: If X 2 (  X ,  X  )-covers X 1 , then return True ,else 1: for all X  X  CoverSets [ X 2 ] do 2: if X  X  X 1 then 3: return True 4: for all X  X  NoCoverSet [ X 2 ] do 5: if X  X  X 1 then 6: return False 7: l 1 =max { minsup,  X  1  X  3  X  1 } 8: l 2 =min {| D ( X 1 ) | , X  1 +3  X  1 } 9: k 1 =max { (1  X   X  ) l , X  2  X  3  X  2 } 10: k 2 =min { l,  X  2 +3  X  2 } 11: for l = l 1 to l 2 do 12: for k = k 1 to k 2 do 13: P cover + = Pr(Supp( X 1 )= l, Supp( X 2 )= k ) 14: if P cover  X   X  then 15: return True 16: return False corresponding support random variables, denoted as X n (1) and X n (2) hereafter, where n is the size of D , our goal is to prove that [ X n (1) X n (2) ] T converges to a bivariate normal distribution when n  X  X  X  .
Suppose the existence probabilities of patterns X 1 and X in the i th transaction t i are p ni (1) and p ni (2) ,then because X ni ( j ) follows Bernoulli distribution. The support of pattern X j , X n ( j ) , can be computed as X follow Poisson binomial distribution, the mean value and  X  Table 3: All possible situations of X 1 and X 2 in t i . Table 3 illustrates all possible existence situations of pat-terns X 1 and X 2 in transaction t i . Assuming for any i and j such that i = j , X ni (1) and X nj (2) are indepen-dent, we have Cov( X ni (1) ,X nj (2) ) = 0. Table 3 indicates sum of X ni over database as Then, { X n } ,n =1 , 2 ,  X  X  X  is a sequence of random vectors: {
X ni } is called a triangular array, which is manipulated com-monly in the study of sum of independent vectors.
Until now, we have laid the groundwork in preparation of the proof that { X n } holds asymptotic normality in the next subsection.
With the aforementioned concepts, we propose the follow-ing theorem, from which Theorem 1 can be induced directly.
Theorem 2. Let { X ni  X  R 2 } , n =1 , 2 ,  X  X  X  , i =1 , 2 , be a triangular array of random vectors such that: (1) for all n  X  1 , X n 1 ,  X  X  X  , X nn are independent, (2) for all 1 i  X  n , X ni follows a bivariate Bernoulli distribution, X n where  X  n and  X  n are the mean nd covariance of X n ,re-spectively.

Theorem 2 provides an important bridge between the joint support distribution of a pair of patterns and the bivariate normal distribution. Noting that suppose the cumulative density functions of X n and X are F n and F , X n d  X  X if and only if for any continuous point x of F , lim n  X  X  X  F n F ( x ). Before giving the detailed proof of theorem 2, two necessary lemmas should be presented first.

Lemma 7. Let X ni  X  R m i , i =1 ,  X  X  X  ,k n , be indepen-dent random vectors with m i  X  m (a fixed integer), n = 1 , 2 ,  X  X  X  ,k n  X  X  X  as n  X  X  X  ,and inf i,n  X  min [Cov( X ni 0 ,where  X  min [ A ] is the smallest eigenvalue of A .Let c If sup i,n E X ni 2+  X  &lt;  X  for some  X &gt; 0 ,then More details of lemma 7 are stated in [23]. Given a sequence of random vectors { X n } , Lemma 7 provides a solution to prove the convergence of all possible linear combinations of {
X n } . Nevertheless, it is not equivalent to the convergence of the random vector itself. Hence, we refer to the next lemma to bridge the gap.
 Lemma 8 ( Cram  X  er-Wold Theorem [26]). Suppose that X n and X are k -dimensional random vectors. Then X n if and only if for all vectors t  X  R k .

The Cram  X  er-Wold theorem states that the convergence of a k -dimensional random vector is closely related to the totality of its one-dimensional projections. With lemma 7 and lemma 8, the complete proof of theorem 2 is as follows.
Proof of theorem 2. Let k n = n ,and  X  i, 1  X  i  X  n, m i =2.

The determinant of covariance matrix is Considering that X 1 and X 2 are two patterns with different parameters, the correlation coefficient between their support distribution satisfies 0 &lt; X &lt; 1. Consequently, Cov( X a positive definite matrix and inf i,n  X  min [Cov( X ni )] &gt; 0.
Let  X  = 2, since all components of X ni are no greater than the size of database n ,wehave
For all i =1 , 2 ,  X  X  X  ,n , assume c ni = c 1 c 2 T ,where c ,c 2  X  R . Then,
Therefore, lemma 7 indicates that With lemma 8, finally we have
To further improve the accuracy of our approximation, we should take the continuity correction [24] into account, because we are using a continuous distribution to approxi-mate a discrete distribution. The final equation needs to be changed slightly as follows.

Pr (Supp( X 1 )= l, Supp( X 2 )= k )  X   X  X +0 . 5 where X = lk T ,  X  is the vector of mean values of Supp( X 1 ) and Supp( X 2 ), and  X  is the corresponding co-variance matrix. Since theorem 2 is equivalent to theorem 1, it is served as a solid theoretical background to support our algorithm. We will demonstrate the empirical proof and assess our approach subsequently.
In this section, we first empirically study the performance of the joint support probability approximation, then evalu-ate the effectiveness and efficiency of the APM algorithm.
We evaluate the accuracy of the approximation of joint support probability with simulation. Two probability sup-port vectors of a pattern X 1 and its super pattern X 2 are constructed from a synthetic uncertain database with N = 100 , 200 ,  X  X  X  , 1000 transactions. The uncertainty is incorpo-rated according to the standard normal distribution. Then, we perform both the exact and approximate algorithms to obtain all joint support probability on the sample space. For each setting of N , we run the experiment for 500 times. Figure 1 (a) shows the average and maximum absolute er-ror (e.g., | Pr a ( x, y )  X  Pr e ( x, y ) | ,wherePr a and Pr approximate and exact probability ) w.r.t. the variation of the database size. Figure 1 (b) demonstrates the average, minimum and maximum error (e.g., Pr a ( x, y )  X  Pr e ( x, y )) between the real and approximate value w.r.t. the variation of the database size. It is shown that the error decreases rapidly when N is increasing. When N = 500, which is much less than the size of a regular database, the average absolute error is less than 10  X  7 .

Three datasets have been used in our experiments. Two of them, the Retail dataset and the Chess dataset, are from the Frequent Itemset Mining(FIMI) Dataset Repository 1 . These are standard datasets used for frequent pattern min-ing in deterministic databases. In order to bring uncertainty into the datasets, we synthesize an existential probability for each item based on a Gaussian distribution with the mean of 0 . 9 and the variance of 0 . 125. The two datasets are uncer-tain databases with uncertainties associated with attributes.
The other one is the iceberg sighting record from 1993 to 1997 on the North Atlantic from the International Ice Patrol (IIP) Iceberg Sightings Database 2 . Each transaction in the database contains the information of date, location, size, shape, reporting source and a confidence level. There are http://fimi.cs.helsinki.fi/data/ http://nsidc.org/data/g00807.html six possible attributes of the confidence level, R/V(Radar and visual), R(Radar only), V(Visual), MEA(Measured), EST(Estimated) and GBL(Garbled), which indicate differ-ent reliabilities. We convert the confidence levels to prob-abilities 0.8, 0.7, 0.6, 0.5, 0.4 and 0.3, respectively. This dataset forms an uncertain database that associates uncer-tainties to tuples. The statistics of the datasets are shown in Table 4.
To analyze the performance of the APM algorithm, we carry out two sets of experiments. In the first set, we com-pare the effectiveness and efficiency of the APM against the dynamic programming-based exact method [25]. Due to the low efficiency of the exact method, we randomly se-lect 500 transactions respectively from two datasets, Retail and IIP. The sizes of FP -the set of probabilistic frequent patterns, DP -the set of P-RFPs mined by the dynamic programming-based approach, and APM -thesetofP-RFPs produced by the APM algorithm with respect to the variations of minsup , minprob ,  X  and  X  , on the two datasets are shown respectively in Figures 2 and 3. The default val-ues of the four parameters are set to 0 . 5%, 0 . 8, 0 . 2and0 . 5, respectively. It can be observed that the result of the APM algorithm is very close to that of the exact method, while both of them are able to reduce the size of the probabilistic frequent pattern set effectively. The runtime of two methods are demonstrated in Figures 4 and 5. It is impressive that the APM algorithm accelerates P-RFP mining significantly.
Then, we examine the performance of the APM algorithm on the complete database of IIP, Retail, and Chess datasets. The comparisons between the number of P-RFPs and the number of frequent patterns are illustrated in Figures 6, 7 and 8. These charts indicate that the APM algorithm can reduce the size of frequent pattern set effectively. Figures 9, 10 and 11 show the runtime vs. minsup , minprob ,  X  ,and  X  curves of the APM algorithm without and with the 3  X  prun-ing technique, which are called APM and APM + Pruning , on the three datasets, respectively. The default values of the four parameters for the IIP and Retail datasets are 0 . 5%, 0 . 8, 0 . 2, and 0 . 5. For the chess dataset, the default param- X  is increasing or minsup , minprob and  X  are decreasing, the runtime will increase because more pattern pairs are en-gaged in the cover probability checking. We can find that the APM algorithm can mine P-RFP set quickly, and the pruning technique accelerates it even further.
Due to the downward closure property, the number of probabilistic frequent patterns mined over uncertain data can be so large that they hinder further analysis and ex-ploitation. This paper proposes the APM algorithm, which aims to efficiently and effectively find a small set of pat-terns to represent the complete set of probabilistic frequent patterns. To address the high computational complexity in examining the joint support probability, we introduce an Figure 3: The Number of P-RFP on Retail-500.
 approximation of the joint support probability with both theoretical and empirical proofs. Our experimental results demonstrate that the devised algorithm can substantially reduce the size of probabilistic frequent patterns efficiently.
This work adopts the measure defined in deterministic databases to quantify the distance between two patterns in terms of their supporting transactions. Since the supports of patterns are random variables in the context of uncer-tain data, other distance measures, such as Kullback-Leibler divergence, might be applicable. As an ongoing work, we will study the effectiveness of probabilistic representative frequent patterns defined on different distance measures.
This work was supported, in part, by the Australian Re-search Council (ARC) Linkage Project under Grant No. LP120100566. [1] Aggarwal, C.C., Yu, P.S.: A survey of uncertain data [2] Aggarwal, C.C.: Managing and mining uncertain data. [3] Chui, C.K., Kao, B., Hung, E.: Mining frequent [4] Bernecker, T., Kriegel, H.P., Renz, M., Verhein, F., [5] Sun, L., Cheng, R., Cheung, D.W., Cheng, J.: Mining [6] Tong, Y., Chen, L., Cheng, Y., Yu, P.S.: Mining [7] Peterson, E.A., Tang, P.: Fast approximation of [8] Tang, P., Peterson, E.A.: Mining probabilistic [9] Tong, Y., Chen, L., Ding, B.: Discovering [10] Leung, C., Mateo, M., Brajczuk, D.: A tree-based [11] Aggarwal, C.C., Li, Y., Wang, J.: Frequent pattern [12] Calders, T., Garboni, C., Goethals, B.: [13] Wang, L., Cheng, R., Lee, S.D., Cheung, D.: [14] Bayardo Jr., R. J.: Efficiently mining long patterns [15] Pasquier, N., Bastide, Y., Taouil, R., Lakhal, L.: [16] Calders, T., Goethals, B.: Mining all non-derivable [17] Yan, X., Cheng, H., Han, J., Xin, D.: Summarizing [18] Jin, R., Abu-Ata, M., Xiang, Y., Ruan, N.: Effective [19] Poernomo, A.K., Gopalkrishnan, V.: Cp-summary: a [20] Xin, D., Han, J., Yan, X., Cheng, H.: Mining [21] Liu, G., Zhang, H., Wong, L.: Finding minimum [22] Chvatal, V.: A greedy heuristic for the set-covering [23] Shao, J.: Mathematical Statistics. Springer (2003) [24] Cox, D.R.: The Continuity Correction. Biometrika [25] Liu, C., Chen, L., Zhang C.: Mining Probabilistic [26] Cram  X  er, H., Wold, H.: Some theorems on distribution
