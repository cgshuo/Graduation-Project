 One of the objectives of knowledge extraction is to build user profiles by finding a set of features from feedback documents to describe user information needs. This is a particularly challenging task in modern information analysis, empirically and theoretically [11,13]. This problem has received much attention from the data mining, web intelligence, and information retrieval communities.

Information retrieval has deployed m any effective term-based methods to find popular terms [14]. The advantages of term-based methods include efficient com-putational performance and mature theories for term weighting. However, many noisy terms can be extracted from the lar ge-scale feedback documents. Words and phrases have also been used as terms in many models. Many researchers believe phrases are more useful and crucial than words for query expansion in building effective ranking functions [14,4,22]. However, there are usually many redundant and noisy phrases [18,19].

Popular terms are useful for describing documents; however, they do not focus on the interesting topics in these documents. We argue that patterns (itemsets, or sets of terms) can be a good alternative f orm of terms for descr ibing interesting topics in documents.

Data-mining techniques have been developed X  X .g., maximal, closed, and mas-ter patterns X  X or removing redundant and noisy patterns [27,25]. By using the ad-vantages of data-mining techniques, pattern taxonomy models (PTM) [24,23,12] have been proposed for using closed sequ ential patterns in text classification. These pattern mining-based approaches h ave improved the effectiveness for rel-evant (positive) feedback documents, but offer fewer significant improvements compared with term-based methods for using both relevant and irrelevant feedback.

Existing approaches focus more on extracting general or popular topics from feedback documents rather than what users really want. Several attempts have been made to determine terms specificity regarding to term distribution in docu-ments. For example, Inverse Document Frequency ( IDF ) measures terms speci-ficity in a set of documents, but much noisy information in text documents affects IDF for finding specific features.

This research proposes a specificity definition for mining specific features for user information needs. The proposed approach includes two stages. The first stage extracts high-level patterns (or to pics) from text documents to focus on interesting topics in order to reduce the noise. The second stage deploys these topics (high-level patterns) to lower level terms to address the low-frequency problem in order to find specific features. The proposed approach can determine specific terms based on both their appearances in relevance feedback and their distribution in interesting topics (or high-level patterns).

The remainder of this paper is organi zed as follows. Section 2 introduces a detailed overview of related works. Section 3 reviews concepts of patterns in text documents. Section 4 proposes a method o f mining specific features from positive feedback documents. Section 5 shows empir ical results; Sectio n 6 reports related discussions, followed by the final sections concluding remarks. Scientists have proposed many types of text representation. A well-known one is the bag-of-words model that uses keywords, or terms, in the vector of the feature space. In [9], the tf*idf weighting scheme was used for text representation in Rocchio classifiers. Enhanced from tf*idf , the global IDF and entropy weighting scheme was proposed in [5]. Various weighting schemes for the bag-of-words representation were given in [1,7].

Bag of words problem is how to select a limited number of feature terms to increase the system X  X  efficiency and avoid overfitting [19]. To reduce the number of features, many dimensionality reduction approaches have been conducted using feature selection techniques like information gain, mutual information, chi-square, and odds ratio [19].

In [2], data-mining techniques analyzed text by extracting co-occurring terms as descriptive phrases from document co llections. However, the effectiveness of the text classification systems using phrases as text representation showed no significant improvement. The likely r eason is that a phrase-based method has lower consistency of assignment and lower document frequency for terms [8].
The data-mining community has extensively studied pattern mining for many years. Usually, existing data-mining techniques discover numerous patterns (e.g., sets of terms) from a training set, but many patterns may be redundant [25]. Nevertheless, the challenge is dealing effectively with the many discovered pat-terns and terms with much noise.

Regarding to these setbacks, closed patterns present a promising alternative to phrases [23,6]. Patterns, like terms, enjoy good statistical properties. To use closed patterns effectively in text mining, patterns have been evaluated by being deployed into a vector with a set of terms and term-weight distributions. The pattern-deploying method encouragingly improves effectiveness compared with traditional probabilistic models and Rocchio-based methods [23,12].

In summary, we can group the existing methods for finding relevance features into three approaches. The first one is to revise feature terms in both positive and negative samples, like Rocchio-based models [15]. The second approach is based on how often terms appear or do not appear in positive and negative samples like probabilistic-based models [26]. The third approach is to describe specific features based on their appearances in both patterns or/and documents [23,10]. In this paper, we further develop the third approach to utilize high-level patterns (or topics) extracted from only positive samples (relevant documents) for finding specific terms. The major research issue is how to determine the topics specificity of terms according their distributions in both documents and topics. In this paper, we assume that all documents are split in paragraphs. So a given document d yields a set of paragraphs PS ( d ). Let D be a training set of docu-ments, which consists of a set of positive documents, D + ; and a set of negative documents, D  X  .Let T = { t 1 ,t 2 ,...,t m } be a set of terms (or keywords) which are extracted from the set of positive documents, D + . 3.1 Frequent and Closed Patterns Let T = { t 1 ,t 2 ,...,t m } be a set of terms which are extracted from D + . Given a termset X ,asetofterms,indocument d , coverset ( X )= { dp | dp  X  PS ( d ) ,X  X  dp } .Its absolute support sup sup r )  X  min sup , a minimum support.

Table 1 lists a set of paragraphs for a given document d ,where PS ( d )= { dp 1 ,dp 2 ,...,dp 6 } , and duplicate terms are removed. Let min sup =3giving rise to ten frequent patterns which are illustrated in Table 2. Normally not all frequent patterns are useful [24,25]. For example, pattern { t 3 ,t 4 } always occurs with term t 6 in paragraphs (see Table 1); ther efore, we want to keep the larger pattern only.

Given a set of paragraphs Y  X  PS ( d ), we can define its termset , which satisfies Let Cls ( X )= termset ( coverset ( X )) be the closure of X .Wecall X closed if and only if X = Cls ( X ).

Let X be a closed pattern. We have for all pattern X 1  X  X . 3.2 Closed Sequential Patterns A sequential pattern s = &lt;t 1 ,...,t r &gt; ( t i  X  T ) is an ordered list of terms. A sequence s 1 = &lt;x 1 ,...,x i &gt; is a sub-sequence of another sequence s 2 = &lt; y and x 1 = y j 1 ,x 2 = y j 2 ,...,x i = y j i .Given s 1 , s 2 , we usually say s 1 is a sub-pattern of s 2 ,and s 2 is a super-pattern of s 1 . In the following, we simply say patterns for sequential patterns.

Given a pattern (an ordered termset ) X in document d , X is still used to denote the covering set of X , which includes all paragraphs ps  X  PS ( d )such that X , ps , i.e., X = { ps | ps  X  PS ( d ) ,X , ps } .Its absolute support and relative support are defined as the same as for the normal patterns.
A sequential pattern X is called frequent pattern if its relative support  X  min sup , a minimum support. The property of closed patterns can be used to define closed sequential patterns. A frequent sequential pattern X is called closed if not  X  any super-pattern X 1 of X such that sup a ( X 1 )= sup a ( X ). Extracting high-level patterns from text documents would help us to focus on interesting topics in positive (relevant) feedback documents. In this paper, a fea-ture X  X  specificity describes the extent to w hich the feature focuses on interesting topics. 4.1 Two Levels of Features Term-based user profiles are considered the most mature theories for term weight-ing to emerge over the last couple decades in information retrieval. A term-based model is based on the bag of words, which uses terms as elements and evaluates term weights based on terms X  appearance or distribution in documents. This main drawback is that the relationship among words cannot be depicted [20]. Another problem in considering single words as features is semantic ambigui-ties, such as synonyms and polysemy. To overcome the limitations of term-based approaches, pattern mining-based techniques are used for information filtering systems, as patterns are less ambiguous and more discriminative than individual terms; but, pattern-based approaches suffer from low frequency problem. To improve the efficiency of pattern taxonomy mining, an algorithm, SP-Mining ( D + ,min sup ) [24], was proposed to find closed sequential patterns in paragraphes for all documents  X  D + that used the well-known Apriori property to reduce searching space. For all positive documents d  X  D + ,the SPMining algorithm discovered all closed sequential patterns based on a given min sup .
Let SP 1 , SP 2 , ..., SP n be the sets of discovered closed sequential patterns for all document d i  X  D + ( i =1 ,  X  X  X  ,n ), where n = | D + | ). All possible candidates of specific terms can be obtained from all SP i ( i =1 ,  X  X  X  ,n ) as follows: 4.2 Specificity of Low-level Features We assume a topic is a set of terms. We call a topic interesting if it is a closed sequential pattern. Usually large number of topics can be extracted from feed-back documents, and there are overlaps among many topics. It is very difficult to determine which topic are useful to describe user information needs. More-over, the user can be interested in one or many topics. Thus, in this paper we define the specificity of a term t based on the specificity of topics that contain t ( ST ); its distribution in topics (or term X  X  frequency in patterns, TFP ) and its appearance in documents (or called document frequency, DF )
The specificity of any given term t to topics ST can be measured by the topics size and the terms distribution in topics that contain t . The topic that contains more terms is unlikely used by other irrelevant documents, and then it is more likely a specific topic for user informatio n needs. Thus, a large topic that contains term t is more important than a short topic. Moreover, the relative support of topics is significant for measure the term X  s specificity. Therefore, the specificity ofatermtotopics ST can be calculated as follows: where sup r ( p,d i ) is the relative support of pattern p in document d i and n is the number of positive feedback documents.
 All positive documents in the user feedback describe user information needs. In other words, terms that appear in all positive documents are likely specific features. For example, if the user seeks i nformation about Unicef, we expect the keyword Unicef appear in all positive feedback documents; however, many feed-back documents contain noisy terms. To reduce noisy terms, the terms frequency will be calculated only according to the ir appearances in the extracted topics rather than in documents.

Based on the above analysis, in this paper, we propose the following equation to calculate the specificity of term t for all t  X  T : where, sup a ( p,d i ) is the frequency of patterns that contain term t in document d ; r ( t )= | coverage ( t,D + ) | is the number of positive documents that contain terms t ;and n is the total number of positive documents.

For a term t , the higher of its spe score is, the more useful for describing the user information needs. The relevance of an incoming document d to the user information needs can be evaluated using the following ranking function: where  X  ( t,d )=1if t  X  d ;  X  ( t,d )=0otherwise. In this paper, we conduct binary text classification to test the proposed approach. We use routing filtering to avoid the need for threshold tuning, which is beyond our research scope. The proposed model i n this paper is called Specific Feature Discovery (SFD). The SFD model uses positive relevance feedback to build user profiles. Unlike other models, it uses only positive feedback for selecting useful features.

According to Buckley and others [3], 5 0 topics are adequate to make a stable, high quality experiment. This evaluati on used the 50 expert-designed topics in Reuters Corpus Volume 1 (RCV1) [21]. RCV1 corpus consists of 806,791 documents produced by Reuter X  X  journalis ts. The document collection is divided into training sets and test sets. These t opics were developed by human assessors of the National Institute of Standards and Technology (NIST). The documents are treated as plain text documents by pr eprocessing the documents. The tasks of removing stop-words according to a given stop-words list and stemming term by applying the Porter Stemming algorithm are conducted. 5.1 Baseline Models and Setting The main baseline models were the well-known term-based methods: Rocchio, BM25 and SVM. The Rocchio algorithm [17] has been widely adopted in the areas of text categorization and information filtering. It can be used to build a profile for representing the concept of a topic which consists of a set of relevant (positive) and irrelevant (negative) documents. we set  X  =  X  =1 . 0inthispaper.
BM25 [16] is one of state-of-the-art term-based models. The values of k 1 and b are set as 1.2 and 0.75, respectively, in this paper.

Information filtering can also be regarded as a special form of text classifica-tion [19]. SVM is a statistical method that can be used to find a hyperplane that best separates two classes. SVM achieved the best performance on the Reuters-21578 data collection for document classification [28]. The decision function in SVM is defined as: where x is the input object; b -is a threshold and w = l i =1 y i  X  i x i for document x i is labelled positive (negative).  X  i -is the weight of the sample x i and satisfies the constraint: To compare with other baseline models, SVM was used to rank documents rather than to make binary decisions. For this purpose, threshold b was ignored. For the documents in a training set, we knew only what were positive (or negative), but not which one was more important. To avoid this bias, we assigned the same  X  i value (i.e., 1) to each positive document first, and then determined the same  X  i (i.e.,  X   X  ) value to each negative document based on Eq.(2). Therefore, we used the following weighting function to estimate the similarity between a testing document and a given topic: where  X  means inner product ; d is the term vector of the testing document; and For each topic, we also chose 150 terms in the positive documents, based on tf*idf values for all term-based baseline models. 5.2 Evaluation Measures Precision p and recall r are suitable because the complete classification is based on the positive class. In order to evaluate the effectiveness of the proposed SFD method, we utilized a variety of existing methods; Mean Average Preci-sion ( MAP ), breakeven points(b/p) , the precision of top-20 returned documents, F-scores and recall at 11-points ( IAP ). These methods have been widely used to evaluate the performance of information filtering system.
 A statistical method, t-test, was also used to analyse the experimental results. The t-test assesses whether the means of two groups are statistically different is evidence to reject the null hypothesis, and the difference in means across the paired observations is significant.

In summary, the effectiveness is measure d by five different means: the average precision of the top 20 documents, F 1 measure, Mean Average Precision ( MAP ), the break-even point ( b/p ), and Interpolated Average Precision (IAP) on 11-points . The larger their values are, the better the system performs. 5.3 Results We compared the proposed method, SFD, with baseline models, including Roc-chio, BM25, and SVM. The experimental results for all 50 assessing topics are reported in Table 3, with the percentage changes % chg . The percentage changes % chg of the proposed SFD model were compared with the performance of the best baseline model (Rocchio). The SFD model outperforms all the baseline models, including the deployment of sequential closed patterns without using the specificity score (Seq. Cls). The ave rage percentage of improvement over the standard measures is 8 . 98%, with a maximum of 12 . 71% and minimum 5 . 70% compared with the best results in Table 3.

The improvements are consistent and very significant on all five measures, as shown by 11-points on all 50 assessing topics in Figure 1. The t-test p values in Table 4 indicate the significance of improvements in the SFD model statistically. Therefore, we conclude the SFD model is an exciting achievement in discovering high-quality features in text documents b ecause it uses high-level patterns to get low-level terms and revises low-level terms based on specificity and distributions in positive relevance feedback. 5.4 Discussion Generally, term-based approaches extract many terms from documents without considering terms relationships. The advantage of using patterns is that they carry more semantic information than single terms X  X ut these suffer from low frequency [10]. Based on that observation, we used the patterns in this paper to consider the relationship among terms to reduce the extracted noise terms in features extracted from documents. As shown in Table 5, the number of extracted patterns is about 202 patterns with an average length of 2 terms in patterns. From that information, we expected about 404 = 202  X  2 terms in all patterns. But the actual number of terms deployed from those patterns are 156 terms, which indicated that about 61 . 37 = 404  X  156 404 of the patterns overlap. This overlapping from the closed pattern indicates some terms importance in the documents.
As shown in Table 5, the average weight of patterns for each topic is 86 . 393 distributed into 202 patterns on average, which gave about 0 . 428 = 86 . 393 202 .On the other hand, Table 6 shows a weight of about 202 . 788 distributed in 156 terms is increased according to terms from the deploying method.

Using common sense, we know that positive terms with large specificity are more interesting than general terms with less specificity for a given topic. How-ever, evaluating the specificity of a given term is challenging. The proposed model calculates the specificity of terms based on the specificity of each term to the topic ST , distribution of terms in topic TFP , and appearance of terms in the documents DF . Unlike other models, in this paper, specific terms ap-pear in most positive patterns in positive documents. As shown in Table 6, before revision, 202 . 788 was distributed to all positive terms as weights; the spe increased the weight of terms to 250 . 570. The percentage of increase is It has been proven that pattern-based approaches are useful for improving the quality of feature selection from text d ocuments, although they suffer from low frequency. To solve that problem, deploying methods have been proposed to deploy high-level patterns into low-level terms. However, these deploying meth-ods cause many low-level terms to have the same weight regardless of a terms specificity. The proposed SFD approach utilizes term distribution in high-level patterns (topics) and terms document fre quency to calculate terms specificity according to their appearances and distribution in topics. The experimental re-sults on RCV1 demonstrate that the proposed method has performed excitingly, with an average 8 . 98% improvement over the state-of-the-art benchmarks. Acknowledgments. This paper was partially supported by Grant DP0988007 from the Australian Research Council (ARC Discovery Project).

