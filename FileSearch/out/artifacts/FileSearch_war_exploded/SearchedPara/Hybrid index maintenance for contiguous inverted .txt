 Stefan Bu  X  ttcher  X  Charles L. A. Clarke Abstract Index maintenance strategies employed by dynamic text retrieval systems based on inverted files can be divided into two categories: merge-based and in-place update strategies. Within each category, individual update policies can be distinguished based on whether they store their on-disk posting lists in a contiguous or in a discontiguous fashion. Contiguous inverted lists, in general, lead to higher query performance, by minimizing the disk seek overhead at query time, while discontiguous inverted lists lead to higher update performance, requiring less effort during index maintenance operations. In this paper, we focus on retrieval systems with high query load, where the on-disk posting lists have to be stored in a contiguous fashion at all times. We discuss a combination of re-merge and in-place index update, called H YBRID I MMEDIATE M ERGE . The method performs strictly better than the re-merge baseline policy used in our experiments, as it leads to the same query performance, but substantially better update performance. The actual time savings achievable depend on the size of the text collection being indexed; a larger collection results in greater savings. In our experiments, variations of H YBRID I MMEDIATE M ERGE were able to reduce the total index update overhead by up to 73% compared to the re-merge baseline.
 Keywords Text retrieval Search engines Index maintenance 1 Introduction The fundamental data structure of most text search engines is the inverted index , also known as inverted file (Zobel and Moffat 2006 ). An inverted index consists of two prin-cipal components: the dictionary and the posting lists (also known as inverted lists ). The dictionary provides a lookup interface that can be used to quickly locate the posting list for a given term. A term X  X  posting list is a list of all its occurrences within the text collection for which the index has been created (each such occurrence is called a posting ). A search engine processes incoming keyword queries by locating the query terms X  posting lists and combining the information found in these lists, perhaps according to Boolean query constraints, or according to probabilistic retrieval methods, such as Okapi BM25 (Robertson et al. 1998 ).

The problem of constructing an inverted index from a given, static text collection has been studied extensively over the last two decades (cf. Moffat 1992 ; Moffat and Zobel 1994 ; Moffat and Bell 1995 ; Heinz and Zobel 2003 ) and is well understood. The dynamic case, however, where the search engine has to update an existing index, reacting to changes in the underlying text collection, has received some attention lately (cf. Cutting and Pedersen 1990 ; Tomasic et al. 1994 ; Shieh and Chung 2005 ; Lester et al. 2005 , 2006 ), but is not yet fully understood.

Many applications, such as digital libraries, Internet news search engines, and file system search engines, have to deal with continually changing text collections and sometimes even have to meet real-time (or near-real-time) performance requirements. A news search engine, for instance, would lose much of its appeal to the users if it only updated its index once per week. To keep the users happy, the engine must react to changes in the text collection by updating its index data structures, and must do so in a time-efficient manner.

Different text collections exhibit different update patterns. A fully dynamic text collection, in general, allows the following three types of update operations:  X  INSERT : a new document is added to the collection;  X  DELETE : an existing document is removed from the collection;  X  MODIFY : an existing document is modified (e.g., some text is appended at the end).
A search engine maintaining an inverted index for such a dynamic text collection must update its internal index structures whenever it detects (or receives notification about) a change to the collection. Updating the index may involve adding new postings to the index, removing postings from the index, or a combination of these two kinds of operations.

Within the scope of this paper, we restrict ourselves to the case of incremental text collections, where new documents may be added to the collection, but existing documents are never deleted or modified. Index update strategies that deal with deletions and modi-fications can easily be combined with the methods proposed in this paper. Possible ways of extending the techniques described here to non-incremental text collections are discussed by Chiueh and Huang ( 1998 ) and Bu  X  ttcher ( 2007 ).

Our work is based on the assumption that the index maintained by the search engine is substantially larger than the available amount of main memory and therefore, at least partially, needs to be stored on disk. Index maintenance strategies for on-disk inverted files are fundamentally different from those for in-memory indices. In both cases, it is generally desirable to store each posting list in a contiguous fashion, increasing spatial locality and cache effectiveness of the query processing routines. However, the impact of non-contiguous posting lists on the search engine X  X  query processing performance is far greater for on-disk indices than if the search engine stores all its data in main memory. With an in-memory index, a discontiguity in a query term X  X  posting list usually results in a CPU cache miss, carrying a penalty of around 100 CPU cycles (comparable to accessing a few dozen in-memory postings in a sequential fashion). With an on-disk index, each discontiguity at query time requires a disk seek X  X n operation that is roughly as expensive as reading 100,000 contiguously stored postings from disk.

As a consequence of this dichotomy, our results are applicable to desktop search engines and small enterprise search solutions with scarce memory resources, but not necessarily to large-scale search engines (e.g., Web search engines) that are composed of hundreds or thousands of machines and that might be equipped with enough main memory to hold the entire index in memory at all times.

Despite the fundamental difference between hard disks and RAM, however, there is a close connection between incremental update strategies for in-memory and for on-disk inverted files. Virtually every on-disk index update strategy operates by accumulating index data for incoming documents in RAM, building an in-memory inverted index; whenever the size of the in-memory index exceeds a certain, predefined threshold (or, alternatively, after some period of inactivity), the entire in-memory index is transferred to disk and combined with the existing on-disk index data (cf. Cutting and Pedersen 1990 ; Lester et al. 2005 , 2006 ). By following this approach, costly on-disk index updates are amortized over a larger number of postings and a larger number of inverted lists. Fur-thermore, because the in-memory index is immediately queriable by the search engine, it allows for real-time index updates.

Different index update policies usually only differ in the way in which they combine the in-memory index data with the existing on-disk index structures. The two traditional index update strategies are re-merge update and in-place update . The re-merge policy operates by sequentially reading the existing on-disk index and merging it with the in-memory index, thus creating a new on-disk index that includes the in-memory data and that supersedes the old index. In-place update strategies, in contrast, do not read the entire on-disk index when the indexing process runs out of memory. Instead, whenever a new on-disk inverted list is created, some amount of free space is pre-allocated at the end of the list. When transferring in-memory index data to disk, postings for a given term are appended at the end of the respective list. A term X  X  list is relocated whenever there is insufficient space for such an append operation, with some amount of free space pre-allocated again at the end of the list X  X  new on-disk location.

The necessity to store each on-disk posting list in a contiguous fashion, and to enforce this contiguity at all times, is the main limitation of traditional merge-based and in-place index maintenance algorithms. To overcome this limitation, Bu  X  ttcher and Clarke ( 2005 ) and Lester et al. ( 2005 ) investigated indexing time versus query time trade-offs , showing how the search engine X  X  index maintenance overhead can be reduced by allowing a small amount of fragmentation in the on-disk posting lists. At query time, this results in a slightly increased query processing cost, due to the greater number of disk seeks.

In this paper, we are not concerned with such trade-offs. Instead, we focus on the case where all on-disk posting lists have to be stored in contiguous form, so as to maximize query processing performance. We show that a combination ( hybrid ) of the two com-peting index update paradigms for contiguous inverted lists (re-merge, in-place) leads to better performance than each policy type alone. We also show that the search engine X  X  index maintenance overhead can be reduced even further by moving away from the idea that the entire in-memory index needs to be transferred from memory to disk when the indexing process runs out of memory. The resulting technique, called partial flushing , allows the search engine to only transfer some posting lists to disk, instead of all of them. As two different on-disk posting lists may have a different update cost, when measured in an amortized fashion (i.e., per incoming posting or incoming byte of compressed postings data), partial flushing leads to better overall performance than treating all lists equally.

In the next section, we give a brief overview of existing work in the area of inverted index maintenance. Section 3 contains a description of our basic hybrid index mainte-nance strategy, called H YBRID I MMEDIATE M ERGE with contiguous posting lists (HIM C ). theoretical complexity analysis of H YBRID I MMEDIATE M ERGE , based on the assumption that terms in the text collection being indexed roughly follow a Zipfian distribution (Zipf 1949 ). Section 6 presents the key results we obtained in our experiments with hybrid and non-hybrid index maintenance. It includes an experimental validation of the performance characteristics of hybrid index maintenance, derived theoretically in Sect. 5 . We con-clude the paper with a discussion of our findings and some directions for future work (Sect. 7 ). 2 Background and related work 2.1 Inverted indices The two principal components of an inverted index are: (i) the dictionary and (ii) the posting lists. The dictionary provides a mapping from the set of index terms to the posi-tions of their posting lists. Each term X  X  posting list is a sequence of all its appearances in the collection, sorted by order of occurrence in the text.

The posting lists, in general, need to be stored on disk, because they are too large to be kept in memory. The dictionary, in contrast, can usually either be stored in memory or on disk. Both solutions have their respective advantages and disadvantages. Keeping the dictionary in memory allows for very fast lookup operations, but has the disadvantage that precious memory space is consumed that could otherwise be used to buffer postings for incoming documents or to cache search results for frequent queries. Moreover, the space requirements of a complete dictionary for a large collection, such as the EDU corpus used in our experiments, may actually exceed the amount of main memory available to the search engine. Conversely, storing the dictionary on disk minimizes its memory require-ments, but increases the search engine X  X  query processing overhead, by introducing an additional disk seek per query term.

As an alternative to these two extremes, the dictionary can be interleaved with the on-disk posting lists (each posting list is preceded by the respective dictionary entry), keeping only a small number of dictionary entries in memory and the vast majority on disk. Dictionary interleaving leads to a substantial reduction of the dictionary X  X  memory requirements, albeit at the cost of a slightly higher disk overhead at query time, usually less than 1 ms per query term (Bu  X  ttcher, 2007 , ch. 4). The technique, however, can only be applied if all lists in the on-disk index are arranged in some pre-defined order (e.g., lexicographically).
Inverted indices come in a few slightly different flavors. Based on the exact type of information contained in the posting lists, we can distinguish between:  X  docid indices , where each posting is a simple integer, representing the numerical  X  frequency indices , in which each posting is of the form ( d , f t , d ), where d is a docid, and  X  document-centric positional indices , in which each posting is of the form  X  schema-independent indices , where each posting is a simple integer, denoting the
A schema-independent index, in contrast to the three other index types, has no explicit notion of a document . The text collection is treated as a  X  X  X lat X  X  sequence of tokens, without (or the application) on a per-query basis, for example by combining \ doc [ and \ = doc [ tokens into documents. An in-depth discussion of the schema-independent approach to text retrieval is given by Clarke et al. ( 1994 , 1995 ).

For the sake of simplicity, all algorithms presented in this paper (as well as all our experiments) assume that the search engine maintains a schema-independent index. However, the methods we discuss apply to all four types of inverted indices. In particular, the performance results obtained with schema-independent indices may be considered representative of document-centric positional indices, as both index representations lead to an inverted file of roughly the same size. 2.2 Index construction A text collection can be thought of as a sequence of tuples of the form ( token , position ). Initially, these tuples are arranged in collection order , sorted by their second component. The task of constructing an inverted index from a given collection can, in essence, be thought of as changing the ordering of the tuple sequence, bringing it from its original collection order into index order , that is, sorted by their first component (ties broken by second component).

High-performance index construction algorithms for static text collections usually operate in two stages. In the first stage, the collection is divided into n sub-collections C ; ... ; C n ; where the size of each part is defined by the amount of main memory available to the indexing process. For each sub-collection C k ; a separate inverted file I k is built in-memory and transferred to disk when the indexing process runs out of memory. The partitioning operation can be performed on-the-fly: sub-collection C k  X  1 starts as soon as sub-index I k reaches the memory limit imposed upon the indexing process and has to be transferred to disk. In the second stage, the n sub-indices I ; ... ; I n are combined into the final index, through an n -way merge operation. The general approach outlined above is referred to as merge-based index construction (Heinz and Zobel 2003 ).

Merge-based indexing algorithms may differ in the way in which they build the index partitions I k : Published work on this topic, however, suggests that hash-based implementations with move-to-front heuristic lead to the best overall performance (Zobel et al. 2001 ). In such implementations, an extensible in-memory posting list is maintained for each index term. A central, in-memory hash table is used to lookup the memory address of the corresponding list for each incoming token, and the new posting is appended at the end of the extensible list. The move-to-front heuristic pulls the hash table entry for a term T to the front of the respective collision chain whenever a new posting is added to T  X  X  posting list.

Hash-based indexing is very suitable for dynamic text retrieval systems, as all incoming postings can immediately be accessed by the query processing routines. For sort-based indexing methods (e.g., Moffat 1992 ), this is not the case. 2.3 Index compression compressed form, in order to save space and also to reduce the search engine X  X  disk transfer overhead. Various methods for compressing the postings in an inverted list exist. Essen-tially all of them make use of the fact that the postings within a given list are sorted in increasing order. A posting list can then be transformed into an equivalent D representa-tion, containing differences ( X  X  D values X  X ) between two consecutive postings instead of the postings themselves:
The D values tend to be rather small and can be encoded using standard integer coding techniques, such as the c code (Elias 1975 ), Golomb/Rice codes (Golomb 1966 ), or the Huffman-based LLRUN algorithm (Fraenkel and Klein 1985 ).

If overall performance (indexing performance, query processing performance, or index maintenance performance) is the optimization criterion, then encoding/decoding performance (measured in seconds per posting) is just as important as raw compression word-aligned encoding methods, such as vByte ( X  X  X ariable-byte X  X ; Scholer et al. 2002 )or Simple-9 (Anh and Moffat 2005 ), usually lead to better results than the more complicated, variable-bit encoding methods listed above.

Within the context of index maintenance, where a large portion of the overall com-plexity stems from the necessity to combine two or more list fragments for the same term into a single posting list, it is important to note that such a merge operation does not require any substantial decompression activity. The compression algorithm, however, needs to be modified slightly so that it emits compressed lists of the form: instead of what is shown in (1). Combining two list fragments P and Q with where q 1 [ p j P j ; then only requires re-encoding p j P j and q 1 , resulting in: By following this approach, the cost of an index merge operation is usually reduced by 15 X 20% compared to a na X   X  ve implementation that decompresses all posting lists (Bu  X  ttcher 2007 , ch. 4). 2.4 Index maintenance Most index maintenance strategies for incremental text collections can be classified as being either merge-based strategies or in-place strategies. Merge-based techniques are usually easier to implement than in-place methods, as they only require the search engine to be able to merge two or more on-disk index partitions X  X n ability that every merge-based indexing algorithm for static text collections already possesses by nature. The dif-ference between merge-based index maintenance and the merge-based index construction method from Sect. 2.2 is that the index maintenance strategy does not wait until the very end of the index construction process (after all, what is the end of an incremental text collection?). Instead, the current in-memory index is merged directly with the existing on-disk index, as soon as the indexing process runs out of memory. The resulting on-disk inverted file replaces the old one, the in-memory index is discarded, and the search engine continues its indexing operations.

The index update policy outlined above is known as R EMERGE (cf. Lester et al. 2004 ). In this paper, we refer to it as I MMEDIATE M ERGE , to emphasize that each merge operation takes place immediately, whenever the search engine runs out of buffer space. I MMEDIATE M ERGE is easy to implement, guarantees the contiguity of all on-disk posting lists, and usually leads to acceptable performance results. It has, however, a fundamental limitation: because the engine needs to process the entire on-disk index whenever it runs out of memory, the total index maintenance disk complexity (measured by the number of postings transferred from/to disk), is: where N is the number of tokens in the collection, and M is the memory limit of the indexing process (number of postings that fit into main memory). If N M ; then I MME-DIATE M ERGE  X  X  extremely high disk activity may render the method unattractive for practical applications.

In-place update strategies do not share this shortcoming. An in-place policy, when-ever it transfers an in-memory posting list to disk, allocates some amount of free space at the end of the newly created on-disk list. The next time, when the indexing process transfers postings for the same term to disk, the new postings may be appended at the end of the existing list, without the need to read the entire list from disk (as is the case with I MMEDIATE M ERGE ). If there is not enough free space to host the new postings, then the entire list is relocated to a new on-disk position, again with some free space pre-allocated at the end of the list. Just like I MMEDIATE M ERGE , in-place update maximizes query performance, as all on-disk posting lists are stored in contiguous form. If the pre-allocation decisions have to be made without any knowledge of the future growth of the respective posting list, then proportional pre-allocation policies, allocating k s bytes for a list of size s , usually lead to best performance results (Tomasic et al. 1994 ; Lester et al. 2006 ). If the search engine makes use of historical data on the growth of each inverted list to predict its future growth, slightly better results can be obtained (Shieh and Chung 2005 ).

We refer to the basic version of in-place index maintenance with proportional pre-allocation as I NPLACE . Because of the geometric progression defined by the proportional the total number of bytes transferred from/to disk during such relocation operations is at most (the factor 2 accounts for the fact that relocating a list of length s requires s bytes to be read and then s bytes to be written). For instance, with a pre-allocation factor k = 2, at most 4 s bytes are transferred from/to disk.
 Equation 6 implies an overall linear index maintenance disk complexity of the I NPLACE strategy, clearly better than the quadratic complexity exhibited by I MMEDIATE M
ERGE . Unfortunately, this advantage is not reflected by the experimental results obtained for I NPLACE and I MMEDIATE M ERGE . Lester et al. ( 2006 ), for instance, report that their implementation of I NPLACE is generally outperformed by I MMEDIATE M ERGE , except when the search engine X  X  in-memory buffers are very small and on-disk index updates have to be carried out frequently. Lester ( 2006 ) ascribes the low update performance of I
NPLACE to the large number of disk seek operations necessary to perform the in-place list updates. As on-disk lists, due to repeated relocation operations, are not arranged in in-place list update necessitates a random-access disk operation. The cost of such an operation is in the order of milliseconds. For short lists, comprising only a few hundred postings, it may be several thousand times greater than the cost associated with writing random order has also implications on the dictionary data structure employed by the search engine. Dictionary interleaving schemes (Bu  X  ttcher 2007 , ch. 4) are incom-patible with in-place update methods, and the engine has to maintain an explicit dictionary for all terms in the index X  X ost likely too large to be held completely in main memory.

In comparison, we can say that merge-based index maintenance seems appropriate when we are dealing with large numbers of short lists, where the overhead of a random-access disk operation is not worthwhile. Long lists, on the other hand, where the amount of new data is small compared to the existing, and unchanged, on-disk posting list, are likely to benefit from the random-access capabilities of in-place update. 2.5 Previous approaches to hybrid index maintenance Index maintenance policies that combine the advantages of merge-based and in-place update strategies are not entirely new. The existing approaches, however, all have in common that they are either guided by heuristics or motivated by details of the imple-mentation, as opposed to an analysis of the performance characteristics of the computer X  X  storage medium.

Cutting and Pedersen ( 1990 ), for instance, discuss an index maintenance mechanism that is very similar to the I MMEDIATE M ERGE policy, but operates on a B-tree as the basic data structure. Posting lists that are too large to be stored in a single node of the B-tree are transferred to an auxiliary storage area, the heap file , and are updated in place. Similarly, Shoens et al. ( 1994 ) describe a dual-structure index organization in which short lists are kept in fixed-size buckets (with multiple lists sharing the same bucket), while long lists are kept in a separate index and are updated in place. Initially, every list is considered short . Whenever a bucket overflows during an index update operation, the longest list in that bucket is removed from the bucket, is declared long , and is updated in place thereafter. However, no constraint is specified regarding the length of the longest list in any given bucket. Thus, a list may be declared to be long when it is in fact rather short. Brown et al. ( 1994 ) discuss an index structure that distinguishes between short, medium, and long lists and in which lists, under certain conditions, may be stored in a non-contiguous fashion. Finally, Lester et al. ( 2006 ) examine the effects of storing short posting lists, smaller than 512 bytes, directly in the index X  X  dictionary data structure, instead of in the postings file. Their results with this hybrid index organization are encouraging. However, as we shall see in Sect. 6.4 , the threshold chosen by Lester et al. is far too small. For a typical consumer hard drive, with a seek latency around 10 ms and a sequential read/write bandwidth of 50 MB/s, the optimal threshold value is approximately 1,000 times greater than 512 bytes. 3 Hybrid Index Maintenance As we have discussed in the previous section, the main limitation of I NPLACE is its non-sequential disk access pattern, requiring a large number of rather expensive disk seek operations. The main limitation of I MMEDIATE M ERGE , on the other hand, is its quadratic update complexity (reading and writing the entire on-disk index every time the indexing process runs out of memory). Suppose we do not apply the same update policy to the entire index, but may choose between the two methods on a list-by-list basis. Consider an cycle, respectively:  X  With I MMEDIATE M ERGE , the list-specific update cost during the ( i + 1)-st update cycle, where b is the hard drive X  X  read/write bandwidth for sequential transfer operations, mea-sured in bytes per second. For typical hard drives, the value of b is usually not a constant, but depends on the disk track(s) being accessed. For simplicity, however, we assume that b is a (device-specific) constant.  X  With I NPLACE , the list-specific update cost is: where C random is the hard drive X  X  random access latency, measured in seconds per random access operation. As with b , the value of C random depends on the exact disk access pattern travelled by the disk X  X  read/write head. To simplify the cost model, however, it will be treated as a constant.

By comparing Eqs. 7 and 8 , we see that the I NPLACE policy results in better performance than I MMEDIATE M ERGE for lists that meet the criterion s i [  X  b C random  X  = 2 : Conversely, for lists that are shorter than  X  b C random  X  = 2 ; I MMEDIATE M ERGE leads to better performance. It follows that the total disk overhead associated with updating on-disk inverted lists is not optimized by exclusively applying either I NPLACE or I MMEDIATE M ERGE to the entire index, but by splitting the index into two sections. One section contains short lists (less than  X  b C random  X  = 2 bytes) and is updated according to I MMEDIATE M ERGE . The other section contains long lists (more than  X  b C random  X  = 2 bytes) and is updated according to the I NPLACE policy.
 Algorithm 1 On-line index construction according to H YBRID I MMEDIATE M ERGE with contiguous posting lists (HIM C ). Input parameter: long list threshold 0 This idea leads to a new index maintenance policy, called H YBRID I MMEDIATE M ERGE , formalized in Algorithm 1. The algorithm takes a single parameter 0 , the threshold used to decide when a list becomes long and should be moved to the in-place index section. The algorithm maintains a set L of long lists, which is initially empty. Whenever, during a re-merge operation, the indexing process encounters a short list whose new size exceeds the threshold J ; the list is marked as long, added to the set L ; and transferred to the in-place index section (cf. lines 14 X 17 in the algorithm).

Because each on-disk posting list is completely stored in either the merge-maintained or the in-place-updated index section, it is guaranteed that all on-disk inverted lists are stored in a contiguous fashion at all times, so as to maximize query performance. We therefore refer to this method as H YBRID I MMEDIATE M ERGE with contiguous posting lists (HIM C )to distinguish it from versions of hybrid index maintenance that do not enforce the contiguity of the on-disk posting lists (cf. Bu  X  ttcher et al. 2006 ). 3.1 Finding the optimal threshold value The long-list threshold J ; in Algorithm 1, is passed to the update policy as an explicit parameter value. In an actual implementation, this is not necessary. The index maintenance process may calculate the optimal value of the parameter J automatically, by measuring the relative performance of sequential and random-access disk operations.

Combining Eqs. 7 and 8 , we know that the overall index update is minimized by choosing J  X  X  b C random  X  = 2 ; where b is the hard drive X  X  average sequential read/write performance (measured in bytes per second), and C random is its average random access latency (measured in seconds per random access operation). However, we have not yet discussed how to compute the exact value of C random . One might suspect that C random is simply the hard drive X  X  seek latency. But this is not the case. By unifying I
MMEDIATE M ERGE and I NPLACE update into a hybrid update policy, we have increased the cost of each random-access in-place list update. This is because each such update now interrupts a sequential read/write operation affecting the merge-maintained index section.

Within the context of H YBRID I MMEDIATE M ERGE , the total cost of an in-place index update consists of four components: 1. Seeking to the disk position within the in-place index section corresponding to the 2. Reading some small amount of data from the end of the list. 3. Writing the data just read, plus the new data, back to disk. 4. Seeking back to the merge-maintained index section, to proceed with the ongoing The necessity of steps 1, 3, and 4 is obvious. Step 2 is necessary for two reasons. Firstly, on-disk postings are stored in compressed form, and compression is applied to larger chunks ( posting list segments ) instead of individual postings. In order to add new postings to a given list segment, the segment header needs to be read from disk so that segment size and compression parameters can be obtained. Secondly, hard disk drives do not allow data access at the byte level, but only in larger blocks, typically 512 or 4,096 bytes. Changing only parts of a disk block requires the operating system to first load the old version into memory, apply all changes, and then write the modified version back to disk.

The overall cost of an in-place index update is the sum of all four components and can be estimated as follows. Steps 1 and 4 each carry the cost of a single disk seek operation. The cost of step 2 depends on the hard drive X  X  rotational latency: on average, the update process needs to wait half a disk rotation before it can read the desired data. The cost of step 3, finally, depends on the rotational latency (because the update process on-disk posting list again) and the disk X  X  sequential read/write performance. Hence, we have:
Assuming, for example, an average disk seek latency of C seek = 8 ms, a rotational velocity of 7,200 rpm (i.e., C rot = 8.33 ms), and an average disk read/write throughput of b = 40 million bytes per second, we obtain and thus As we shall see in Sect. 6 , this theoretical estimate is in line with our experimental results. threshold value is greater than 512 bytes (in their experiments, Lester et al. tried various threshold values and observed the general trend that a greater threshold 0 leads to better update performance; they stopped at J  X  512 bytes). 4 Partial flushing The basic approach to incremental index updates, as introduced in Sect. 1 , assumes that the entire in-memory index has to be transferred to disk when the indexing process runs out of memory. With the non-hybrid I MMEDIATE M ERGE policy, this is certainly the case. With I NPLACE or H YBRID I MMEDIATE M ERGE , however, it is no longer true. Because each inverted list in an in-place-maintained index can be updated individually, without the need to update the entire index, an indexing process employing the I NPLACE policy may release memory resources by transferring an arbitrary subset of the in-memory posting lists to disk. If it maintains its on-disk index structures according to H YBRID I MMEDIATE M
ERGE , the indexing process is not quite as flexible as in the case of I NPLACE , but it can still choose from an arbitrary set of long lists (updated in place) when transferring in-memory index data to disk in an attempt to make room for incoming postings. In particular, it may release memory resources without transferring any of the small lists to disk, thus avoiding a costly re-merge operation. The general idea of not transferring the entire in-memory index to disk when the indexing process runs out of memory, but only a small portion of all in-memory inverted lists, is referred to as partial flushing (Bu  X  ttcher and Clarke 2006 ).

In essence, partial flushing reduces the re-merge overhead, at the cost of a greater number of in-place updates. In order for the method to be effective, care has to be taken that the additional in-place list updates triggered by a partial flush do not out-weigh the savings achieved by delaying the next re-merge operation. For example, suppose the in-memory postings for a certain term account for 1% of the indexing process X  X  main memory consumption. By transferring these postings to disk, at the cost of a single random-access in-place list update (= C random ), some amount of main memory can be released, and the time between two subsequent re-merge operations (transferring the entire in-memory index to disk) can be increased by 1%. If the cost of a full on-disk index update (including the re-merge operation carried out in the merge-maintained index section) is more than 100 times the cost of a single in-place list update, then transferring the term X  X  in-memory postings to disk is worthwhile. Otherwise, it is not.

The relative cost of a single random-access in-place list update ( C random ) and a complete on-disk index update ( C complete ) implicitly defines a partial flushing threshold J PF that identifies the set of in-memory posting lists that should participate in a partial flush, and also the set of lists that should not. By instructing the indexing process to keep track of the cost of the previous complete index update, it is possible to make the system automatically determine the optimal partial flushing threshold J PF ; opt: where M is the total size of the in-memory index (in bytes), C random is the average in-place update overhead of a single list, and C complete is the total update cost measured during the last re-merge operation (lines 11 X 24 in Algorithm 1).

Because of the Zipfian term distribution (Zipf 1949 ) exhibited by most natural-language text collections, the vast majority of postings in the index belongs to a rather small number of terms. It is therefore not unusual that the size of the in-memory index can be reduced by over 50%, even if only a few 1,000 lists participate in the partial flush. The effectiveness of partial flushing, however, diminishes with every subsequent greater portion of the in-memory index is occupied by short lists X  X ists than cannot the method is no longer beneficial, and a complete update of all on-disk inverted lists has to be performed.
 Algorithm 2 HIM C with partial flushing. Building a schema-independent index for a potentially infinite text collection. Input parameters: J 1 ; the long-list threshold; C random , the cost of a single random-access in-place list update; x [ (0, 1), the effectiveness cri-terion, used to determine when partial flushing is no longer useful.

At first glance, it might seem that partial flushing is effective as long as at least one list is transferred to disk. However, this is not so. Each partial flush carries a cost by itself, without taking into account the cost of transferring one or more lists to disk. Among other participate in the partial flushing operation and then needs to reclaim the memory space occupied by those lists so that it can be used for incoming postings. Within the memory management regime employed by our search engine, which was originally designed with high-performance index construction in mind, not taking the special requirements of partial flushing into account, reclaiming even a single byte of memory space always requires the traversal of the entire in-memory index (cf. Bu  X  ttcher 2007 , ch. 4). Consequently, it may happen that the net effect of a partial flush is negative even though the condition stated in Eq. 14 is met by all lists participating in the operation.

The problem can be solved in the following way: in addition to the partial flushing threshold 0 PF , the system also employs an effectiveness criterion (cut-off threshold) x that is used to decide when to turn off partial flushing and to carry out a complete on-disk index update instead. As soon as the system reaches the point at which partial flushing reduces the relative main memory requirements of the indexing process by less than x , partial flushing is temporarily disabled, and the next update of the search engine X  X  on-disk index structures will be a complete update, affecting all on-disk posting lists.

The modified indexing procedure, employing hybrid index maintenance with partial flushing, is given by Algorithm 2. It automatically adjusts the partial-flushing threshold 0
PF , by keeping track of the duration of the last complete index update. The algorithm takes three input parameters: the partial flushing threshold 0 , the cost associated with an in-place list update C random , and the partial flushing cut-off criterion x . Note, however, that in an actual implementation of the algorithm it is possible to automatically estimate the optimal values of all three parameters, by continually monitoring the performance of all ongoing index maintenance operations. The effectiveness cut-off parameter x , for instance, can be obtained and adjusted dynamically, by measuring the duration of a partial flushing operation and by comparing it to the amount of main memory freed by the operation, in a way very similar to finding the optimal value of 0 PF : where C complete , again, is the total cost of the last full update of the search engine X  X  on-disk index structures, and C partial is the overall cost of the last partial flushing. 5 Complexity analysis We now present a complexity analysis of the HIM C index maintenance policy proposed in Sect. 3 . Our analysis is based on the assumption that the terms in the text collection for which an index is maintained follow a Zipfian distribution. This assumption is not uncommon and has, for example, also been made by (Cutting and Pedersen 1990 ), for very similar purposes.

The analysis consists of two parts. In the first part (Sect. 5.1 ), we derive the total number of long lists (lists containing more than a certain number 0 of postings) and the total number of postings contained in short lists (lists containing fewer than 0 postings). In the second part (Sect. 5.2 ), we use the results from the first part to estimate the number of disk operations (sequential and random-access) carried out by HIM C . Because disk activity is the main bottleneck of all index maintenance strategies, we obtain a rather accurate approximation of the total index maintenance cost of the system.

In our analysis, we assume that disk operations always carry the same cost, regardless of what part of the disk they affect (in reality, storing data on the outer tracks of the hard disk is more efficient than storing them on the inner tracks). We also assume that all postings consume the same amount of space, regardless of which term they belong to, which is slightly incorrect, as postings for a more frequent term can be compressed better than postings for a less frequent term. However, the difference only amounts to a logarithmic factor and may be ignored. 5.1 Generalized Zipfian distributions: long lists and short lists According to Zipf X  X  law (Zipf 1949 ), terms in a collection of text in a natural language roughly follow a distribution of the following form: number of times the term appears in the collection ( X  X  X  ... ] X  X  denotes rounding to the nearest integer); c and a are collection-specific constants. In his original work, Zipf postulated a = 1.

Equation 16 is equivalent to the assumption that the probability of an occurrence of the term T i , according to a unigram language model (that is, treating each token as an independent event), is where N is the total number of tokens in the text collection. In order for this probability distribution to be well-defined, the following equation needs to hold: Because N and c are constants, this implies the convergence of the sum and thus requires that a be greater than 1. If a [ 1, then the sum X  X  value is given by the Riemann zeta function f ( a ) and can be approximated in the following way: where is the Euler-Mascheroni constant. The quality of the approximation in (19) depends on the value of the collection-specific parameter a . For realistic values (1 \ a \ 1.4), the approximation error is less than 1%. The integral representation of the Riemann zeta therefore exclusively use the former when modeling the term distribution in a given text collection.

As a notational simplification, we refer to the term c  X  1 a 1 in Eq. 19 as c a : From this definition, it follows that the value of the constant c in Eq. 16 ff. is: where N again is the number of tokens in the text collection. 5.1.1 Long lists and short lists We can now calculate L  X  N ; J  X  ; the number of terms that occur more than J times in a text collection comprising N tokens. Their corresponding lists are referred to as long lists . From Eq. 21 , we know: Hence, we have The total number of postings found in these lists, denoted as ^ L  X  N ; J  X  ; then is
It immediately follows that the total number of postings found in short lists (lists containing no more than J entries), denoted as ^ S  X  N ; J  X  ; is 5.2 Complexity of hybrid immediate merge The total index maintenance cost of incrementally building and updating an inverted index for a text collection of size N consists of three components: 1. Reading and tokenizing the input data; building in-memory inverted files. 2. Merging in-memory posting lists with posting lists found in the merge-maintained 3. Updating posting lists found in the in-place-updated section of the on-disk index ( long
We determine the cost of each component individually. 5.2.1 Reading, tokenizing, indexing Reading and tokenization can be done in linear time. By employing a hash-based inversion algorithm (cf. Sect. 2.2 ) to incrementally build an in-memory inverted file, the indexing cost can be kept linear, too. In total, therefore, the cost of the first component is H  X  N  X  : 5.2.2 Merge operations Let M be the amount of main memory available to the indexing process, measured by the number of tokens that can be indexed before the process runs out of memory. When building and maintaining an index for a collection of N tokens, the system runs out of memory b N = M c times. Whenever this happens, a re-merge operation is carried out, reading the old merge-maintained index section from disk and replacing it by a new on-disk inverted file. The number of postings written to disk during the k -th re-merge operation is: Thus, the total number of postings written to disk, during all re-merge operations, is:
The number of postings read from disk is bounded from above by the number of postings written to disk (nothing is read twice). The number of random-access disk operations carried out is negligible, as the disk access pattern of a merge operation is essentially sequential. Therefore, the overall cost of the merge operations, measured by the search engine X  X  disk activity, is in fact: 5.2.3 In-place list updates When updating an on-disk inverted file according to the I NPLACE policy, the total number of Sect. 2 . Therefore, the only interesting aspect of the third component, the in-place list updates, is the number of random-access disk operations carried out by the index main-tenance process.

Let us assume that, whenever the search engine runs out of memory, every list in the in-place index section needs to be updated and that each in-place on-disk index update, on average, requires a constant number of random access operations. From a theoretical point of view, the first assumption is an oversimplification, as the number of lists in the in-place index is unbounded, while the size of the memory buffer is a constant. However, for a realistic long-list threshold of J 10 6 ; corresponding to roughly 500,000 postings, every index update cycle, at least for the first few thousand cycles. Thus, for the purpose of modeling the actual update performance in a realistic environment, the assumption is valid.

The number of lists in the in-place section of the on-disk index, after the indexing process runs out of memory for the k -th time, is: (cf. Eq. 22 ). Therefore, the total number of in-place list updates is: 5.2.4 Total cost Combining the results for all three components, and treating the long-list threshold J and the time complexity of a random-access disk operation as constants, we obtain the fol-lowing overall index maintenance complexity: for some constants a , b , c , ^ a ; ^ b (with ^ b  X  b  X  c  X  : Compared to the non-hybrid I MMEDIATE M
ERGE baseline policy with its H N N M update complexity, this is a substantial improvement. When run on a text collection with Zipf parameter a = 1.33, for instance, HIM C is expected to exhibit a total complexity of H N 1 : 75 M : 6 Experiments We evaluated H YBRID I MMEDIATE M ERGE with contiguous posting lists (HIM C ), with and without partial flushing, on two different computer systems and using two different text collections. The results are compared to the non-hybrid I MMEDIATE M ERGE baseline policy.
Our experiments are limited to the index maintenance performance of the individual methods and do not cover their query processing performance. However, because all methods covered in this paper store their on-disk posting lists in a contiguous manner, we strongly suspect that the difference between the individual methods would have been extremely small and would have been unlikely to provide any additional insight into the problem of updating an inverted index. 6.1 Data sets and hardware configuration The two computer systems used in our experiments are: a 2.2 GHz AMD Athlon64 with 2 GB of RAM and a 2.8 GHz AMD Opteron, also with 2 GB of RAM. An overview of these two systems is given in Table 1 . The main difference between the two systems, besides their different CPUs, is the fact that the Opteron system stores its on-disk index data on a two-disk RAID-0 partition (software RAID) instead of a single hard drive. The Opteron X  X  RAID achieves a sequential read/write performance that is roughly twice as high as the Athlon64 X  X  single hard drive. This difference allows us to examine the effect that the hard drive X  X  read/ write throughput has on the optimal value of the hybridization threshold J :
The two text collections used in the experiments are: GOV2 (Clarke et al. 2004 ), the result of a Web crawl of the : gov domain carried out in early 2004, and EDU, the result of a Web crawl of the : edu domain in early 2006. GOV2 is available from the University of Glasgow. 1 EDU is not publicly available. A summary of the two collections is given in Table 2 .

Using two different text collections allows us to experimentally validate the theoreti-cally suggested impact of the Zipf parameter a on the overall index maintenance performance of HIM C . According to Eq. 33 , a greater value of the Zipf parameter a results in a better performance of HIM C , compared to I MMEDIATE M ERGE . We computed approx-imate best-fit a values for both text collections by plotting each collection X  X  term rank/ frequency curve and choosing a in such a way that the area between the actual rank/ frequency curve and the curve suggested by modeling the collection according to the Zipfian distribution was minimized. The minimization step was performed after a log/log transformation (i.e., plotting term ranks and term frequencies in logarithmic scale). For GOV2, we obtained a best-fit parameter value of a GOV = 1.333; for EDU, we obtained a
EDU = 1.263. The rank/frequency curves for both collections are shown in Fig. 1 . 6.2 Implementation and index update sequence All experiments were conducted using the freely available Wumpus 2 search engine, run-ning as a 64-bit user process under Linux 3 2.6.18. All on-disk index data were stored as ordinary files in the file system, realized by Linux X  X  implementation of ext3 ; using one file per index section/inverted file. To keep the amount of fragmentation introduced by the file system layer low, we made sure that no more than 70% of the machine X  X  total disk space was utilized at any given time.

The inverted files used in our experiments were schema-independent (cf. Sect. 2.1 ), with every posting being a simple integer value, denoting the distance of a term occurrence from the beginning of the text collection. All postings, both in the on-disk and in the in-memory index, were stored in compressed form, using the byte-aligned vByte method (Scholer et al. 2002 ). Disk access operations were carried out on large chunks of data (we used a buffer size of 4 MB for all sequential read/write operations), cached by the oper-ating system. However, at the end of each on-disk index update (partial flush or complete index update cycle), all data currently in the operating system X  X  file system cache, but not yet applied to the on-disk index, were forced to be written to disk by executing an fsync system call.

Our implementation of the I NPLACE policy employed a simple proportional pre-alloca-tion strategy with a pre-allocation factor k = 2, allocating disk space for long posting lists in multiples of 512 KB (making it impossible to use in a stand-alone I NPLACE implementation, but good enough in the context of HIM C ). Whenever the index maintenance process ran out of space in the in-place index section, the size of the file was increased by performing an ftruncate system call, making the file big enough to find a new position for the posting list being relocated.

In our experiments, we simulated a dynamic search environment in which the search engine needs to maintain an index for a continually growing text collection. For this pur-pose, GOV2 was split up into 27,204 different files, with each file on average containing 1.6 million tokens in 926 documents. EDU was divided into 6,912 files, with each file containing 9.9 million tokens in 6,438 documents on average. The search engine was instructed to build an index for the given collection, processing all files in the collection (in random order) and enforcing the contiguity of each on-disk posting list at all times. The beginning of the actual index update sequences used in our experiments is shown in Table 3 .
Most of our experiments were repeated twice, and we did not see a single case in which the overall time difference between two iterations of the same experiment was greater than 1%. Thus, the performance figures given in the remainder of this section may be considered reliable. 6.3 Hybrid versus non-hybrid immediate merge In an initial set of experiments, we evaluated the basic HIM C policy, for two different threshold values J  X  125 ; 000 and J  X  2 ; 000 ; 000 ; using GOV2 as the underlying text collection and allowing the indexing process to hold up to 512 MB of postings data in its in-memory buffers.

Figure 2 a shows the number of posting lists in the in-place index section, as the index grows. It can be seen that the number of lists in the in-place section is very small, even for the rather low threshold value of J  X  125 ; 000 : After the complete index has been built, less than 20,000 lists have been transferred to the in-place index  X  J  X  2 ; 000 ; 000: less than 4,000 lists). The fact that the number of lists in the in-place index is so small (i.e., that there are so few terms with long lists) means that dictionary maintenance is far less challenging with HIM C than in the case of the non-hybrid I NPLACE method, where millions of dictionary entries need to be maintained and updated together with the posting lists. For example, posting lists in main memory X  X omething that is not always possible for I NPLACE (cf. Sect. 2.4 ).

In contrast to the small number of lists in the in-place section, the total number of postings in those lists is rather large, as shown in Fig. 2 b. For J  X  125 ; 000 ; after indexing only 1 billion tokens ( &amp; 2.5% of GOV2), more than 80% of all postings are found in the in-place index section. For J  X  2 ; 000 ; 000 ; the same percentage is reached after indexing 17 billion tokens ( &amp; 40% of GOV2). As a consequence, the merge-maintained index section remains relatively small in both cases, resulting in a lower re-merge cost than in the case of the non-hybrid I MMEDIATE M ERGE update policy. This is shown in Fig. 2 c. After indexing 50% of GOV2, for example, the cost of a re-merge operation is reduced by between 32% and 47% (between 467 and 600 s with HIM C , instead of 885 s with I MME-DIATE M ERGE ). This, of course, directly translates into a reduced cumulative indexing overhead, between 22 and 28 h for HIM C , compared to 44 h for I MMEDIATE M ERGE (cf. Fig. 2 d).

The relative gains achieved by HIM C , compared to the non-hybrid I MMEDIATE M ERGE strategy, increase as the index grows (cf. Fig. 3 ). For instance, HIM C with J  X  2 10 6 is about 50% faster than I MMEDIATE M ERGE when indexing the first 20 billion tokens from GOV2. When indexing the first 40 billion tokens, it is about 90% faster. This confirms our initial expectation that the relative performance of HIM C grows with the collection size (cf. Eqs. 5 and 33 ). The curves in Fig. 3 seem to be slightly convex, but do not appear to be bounded.

By comparing Fig. 3 a and b, we see that the performance gains are higher for GOV2 than for the EDU collection. After indexing 40 billion tokens, the relative performance gain on the GOV2 collection is 90% (for J  X  2 ; 000 ; 000  X  : For EDU, it is only 55%. This confirms our expectation that the index maintenance performance of HIM C is higher for collections with greater Zipf-a value than for collections with smaller a ( a GOV2 = 1.333, a
EDU = 1.263). 6.4 Optimal threshold values It is worth pointing out that the two threshold values examined in our initial set of experiments  X  J  X  125 ; 000 and J  X  2 ; 000 ; 000  X  represent a rather large spectrum of possible parameter values. Yet, the savings achieved relative to the I MMEDIATE M ERGE baseline are substantial in both cases: for GOV2, the total index maintenance overhead is reduced by between 37% and 49%; for EDU, between 22% and 41%. This implies that HIM C is not overly sensitive to the choice of the exact threshold value J :
Nonetheless, optimal update performance is, of course, achieved by carefully tuning the value of J and making it reflect the relative performance of sequential and random-access disk operations carried out by re-merge update and in-place update, respectively. Theo-retical considerations in Sect. 3.1 suggested that the index update cost is minimized by choosing By replacing b , C seek , and C rot with the actual performance values of our systems, we obtain in the case of the Athlon64 system with its single-disk setup, and for the Opteron with its two-disk RAID-0 storage system.

To validate these predictions, we had our search engine again build an index for GOV2 in an incremental fashion, using both computer systems (Athlon64 and Opteron), and employing a wide variety of threshold values between J  X  125 ; 000 and J  X  4 ; 000 ; 000 : The results of these experiments, shown in Table 4 , confirm our theoretical model. The system X  X  update performance is in fact maximized by choosing J Athlon 64 500 ; 000 bytes and J Opteron 1 ; 000 ; 000 bytes.

It is interesting to see that, although the optimal J value differs by about a factor 2 between the two systems, the relative speedup achieved with the optimal threshold value is roughly the same on both systems. On the Opteron, the total indexing overhead is reduced by 56% (from 43.98 to 19.75 h). On the Athlon64, it is reduced by 59% (from 75.29 to 30.99 h). This indicates that hybrid index maintenance can even be beneficial for indices that are stored on high-throughput disk arrays, such as multi-way RAID setups. 6.5 Partial flushing In our next series of experiments, we evaluated the performance of the partial flushing technique from Sect. 4 . As before, we had the search engine build an incremental index for both collections, GOV2 and EDU, employing HIM C with partial flushing as the index maintenance policy.

Figure 4 shows the impact that partial flushing has on the indexing process X  X  main memory consumption and on the frequency with which the search engine needs to carry out re-merge operations on the on-disk index data. The line-plots correspond to the search engine X  X  memory consumption. The small circles correspond to partial flushing events.
Because of the Zipfian term distribution exhibited by the text collections (the majority of all postings is found in a small number of lists), the amount of main memory freed by flushing all long lists to disk is quite remarkable. Transferring the in-memory postings for the 1,200 most frequent terms in GOV2 to disk, for instance, reduces the indexing pro-cess X  X  memory consumption by 53% (from 512 to 239 MB). Thus, as a result of applying the partial flushing technique, the amount of data indexed between two consecutive re-merge operations is increased noticeably. When building an index for GOV2, the engine runs out of memory for the first time after 280 million tokens. The next out-of-memory 280 + 360 = 640 million tokens. This trend continues, and after indexing 8 billion tokens, the distance between re-merge operations is roughly 650 million tokens, with four applications of the partial flushing sub-routines between each consecutive pair of re-merge operations.
 Again, we notice a difference between GOV2 and EDU. When building an index for EDU, the relative number of postings found in short lists is greater for EDU than for GOV2 (caused by EDU X  X  smaller a value; cf. Eq. 24 ). In the context of partial flushing, this results in a lower effectiveness of the method. For example, when flushing the postings for the 1,200 most frequent terms to disk, the system X  X  main memory consumption is only decreased by 40% for EDU (from 512 to 309 MB), as opposed to the 53% seen with GOV2.

As discussed in Sect. 4 , partial flushing does not necessarily achieve its optimal per-formance by flushing postings for all long lists to disk. Instead, a list should only be flushed when the expected benefit exceeds the cost of performing an in-place update operation for that list (cf. Eq. 14 ). We compared the slightly more sophisticated variant of partial flushing, taking into account both the cost of an in-place update and the expected savings obtained by performing the update, to the version in which all long lists are flushed to disk, to the basic HIM C policy without partial flushing, and to I MMEDIATE M ERGE . The results of our experiments, for both collections, are shown in Fig. 5 .

Plot 5(a) depicts the cumulative index maintenance overhead of all four update policies on the GOV2 collection. While partial flushing with J  X  1 (i.e., flushing all long lists) does in fact have a slight edge over HIM C without partial flushing, setting the threshold J PF according to a proper cost analysis of partial flushing results in even greater savings. While HIM C + PF with J PF  X  1 indexes GOV2 in 16.4 h (17% less than without partial flushing), J PF  X  auto finishes the same job in just 13 h (34% less than without partial flushing). Compared to the I MMEDIATE M ERGE baseline policy, the total indexing overhead is reduced by 70%.
Plot 5(b) shows how the optimal partial flushing threshold J PF develops over time. As the index grows, re-merge operations are getting more and more costly (cf. Fig. 2 c). Consequently, after a while, it becomes economical to flush even rather short list fragments with a relatively high amortized in-place update cost (amortized over the amount of data being written to disk).

Just like the basic HIM C without partial flushing, HIM C + PF realizes a speedup by taking advantage of the relative performance of sequential and random-access disk oper-ations. Compared to the variant without partial flushing, HIM C  X  PF  X  J  X  auto  X  increases the number of random disk access operations, by performing 631,556 instead of 539,921 in-place list updates in total. However, the associated cost is by far outweighed by the reduced amount of data transferred from/to disk: from 2.83 10 12 bytes down to 1.28 10 12 bytes.
Plots 5(c) and (d) show the same results as plots (a) and (b), but for the EDU collection instead of GOV2. For EDU, the savings achieved are quite similar to what we have seen before; partial flushing (with J  X  auto  X  reduces the total indexing time from 94.5 to 62.2 h ( -34%), for a total reduction of 63% compared to I MMEDIATE M ERGE . 6.6 Impact of memory buffer size In a final series of experiments, we examined the impact that the available amount of main memory has on the update performance of the various index maintenance policies. Equation 33 suggests that the relative performance of HIM C , compared to the I MMEDIATE M
ERGE baseline, is independent of M , the amount of main memory that may be used to buffer incoming postings.

Figure 6 shows the total index maintenance overhead when building an incremental index for GOV2. Plot (a) depicts the absolute overhead, while plot (b) shows the relative overhead, compared to I MMEDIATE M ERGE . The results are roughly in line with the prediction from Sect. 5 . When decreasing the size of the memory buffers from 1,024 to 256 MB, HIM C  X  X  relative update overhead remains almost constant (46.8% vs. 43.8%). For HIM C +PF, we see a slight drop, from 33.5% to 26.7%. This drop, however, does not contradict our complexity analysis, as the analysis did not make any predictions for partial flushing. 6.7 Validation of theoretical complexity analysis In the previous parts of this section, we have experimentally examined the performance of several variants of HIM C . We have shown that their actual performance is generally in line with what is predicted by the theoretical complexity analysis in Sect. 5 . For instance, HIM C leads to better results for GOV2 than for EDU, due to the greater Zipf-a value in GOV2; the relative performance of the method increases as the index grows.

However, the complexity analysis makes very specific predictions regarding the index maintenance complexity of our method. Under the assumption that the size of the memory buffers, M , is a constant, we can express the total index update time complexity for a collection comprised of N tokens as: where a corresponds to the raw indexing overhead (reading input files, tokenizing input, etc.), b 1 and c 1 correspond to random access disk operations (in-place list updates), and b 2 and c 2 correspond to the sequential disk transfer overhead (re-merge operations). c 1 And c 2 depend on the statistical properties of the text collection for which an index is maintained, while a , b 1 , and b 2 depend on both the collection and on the performance of the computer system. Based on the assumption that the text collection follows a Zipfian distribution with Zipf parameter a , the analysis in Sect. 5 predicted c 1 = c 2 =1+1/ a . For GOV2 ( a GOV2 = 1.333), this resulted in an overall index maintenance complexity function of the form: C 1 : 333  X  N  X  X  a N  X  b N 1 : 750 : For EDU ( a EDU = 1.263), it resulted in: C 1 : 263  X  N  X  X 
These specific predictions have not been verified yet. In order to do so, we had our system again build an index for GOV2 and EDU in an incremental fashion, employing an in-memory buffer of size 1,024 MB. We sampled the total number of bytes transferred from/to disk and the total number of in-place list updates carried out, with one sample taken after the completion of each complete on-disk index update (i.e., after each re-merge event). This gave us a set of 27 X 149 data points (the exact number depends on which collection is indexed and which update policy is used). We assumed that the linear component in Eq. 37 was the same for all update policies and approximated it by mea-suring the total amount of disk activity caused by a static, merge-based index construction method (we measured 15.0 bytes per token for GOV2 and 14.8 bytes per token for EDU). We then estimated the values of the remaining parameters in Eq. 37 by computing a non-linear least-squares fit for both sets of data points, in-place updates performed and bytes transferred from/to disk. The outcome of this computation is shown in Table 5 .
For the I MMEDIATE M ERGE strategy, the least-squares fit returned exponents c 2 = 1.992 (GOV2) and c 2 = 1.977 (EDU), respectively, fairly close to the expected value of c 2 =2. The deviation from the theoretically predicted exponent is due to the fact that the engine X  X  disk activity stems not only from postings being transferred from/to disk, but also from the dictionary entries (i.e., the index terms themselves) that are part of the index, too. Since the number of distinct terms does not grow linearly with the size of the collection, we see a slightly sub-quadratic overall complexity. The effect is stronger for EDU than for GOV2 because of the greater number of terms in EDU.

For the performance of the basic version of HIM C , without partial flushing, our theo-retical complexity analysis had predicted exponents c 1 = c 2 = 1.792 on the EDU collection, and in fact the least-squares fit ( c 1 = 1.823, c 2 = 1.761) is very close to this prediction. On the GOV2 collection, however, the outcome of the least-squares computation ( c 1 = 1.829, c = 1.666) is quite different from the prediction c 1 = c 2 = 1.750. To understand the reason for this discrepancy, it is helpful to have another look at Fig. 1 . The figure shows the term distribution in GOV2 and EDU, as well as the best-fit Zipfian distribution for both collections. By comparing plot 1(a) to plot 1(b), we see that EDU is much better repre-sented by its Zipfian distribution than GOV2. In both cases, the Zipfian approximation underestimates the collection frequency of all terms in the range 10 1 B rank B 10 5 . However, the error is greater for GOV2 than for EDU. For example, the Zipfian distri-bution for GOV2 (with a = 1.333) underestimates the collection frequency of the term at rank 10 3 by a factor 5. Unfortunately, in our experiments, it is exactly this term range (10 1 B rank B 10 5 ) that is affected the most by the hybridization (cf. Fig. 2 a). As a result, the analysis from Sect. 5 underestimates the number of in-place list updates and overestimates the number of bytes transferred from/to disk.

Apart from the basic version of HIM C , the regression analysis also returns specific complexity levels for the variants with partial flushing, which is convenient because we did not obtain any theoretical results for partial flushing. For a flushing threshold J PF  X  1 (i.e., all long lists are flushed to disk), the least-squares fit reveals that partial flushing affects the search engine X  X  index maintenance performance by more than just a constant factor. Because the distance between two re-merge operations increases as the index gets larger (cf. Fig. 4 ), the number of in-place updates per indexed token keeps growing, while the number of bytes transferred keeps getting smaller. This effect can be seen for both GOV2 and EDU (GOV2: c 1 jumps from 1.829 to 1.895; EDU: from 1.823 to 1.879). Hence, although we have not witnessed this phenomenon in our experiments, the regression analysis predicts that HIM C + PF with J PF  X  1 ; in the long run, will be outperformed by HIM C without partial flushing, as the cost of the in-place updates, asymptotically, is the dominant component of the method X  X  overall complexity.

For partial flushing with J PF  X  auto (i.e., the version where a list only participates in a partial flush if the savings stemming from its participation are expected to outweigh the cost of the additional in-place update), the least-squares fit indicates a lower overall complexity for both aspects of the method X  X  performance X  X n-place updates performed and bytes transferred from/to disk. The asymptotical disk transfer activity is roughly on par with J PF  X  1 : The number of in-place list updates, however, is reduced greatly and is actually lower than for HIM C without partial flushing. The reason for this is that J PF  X  auto not only reduces the number of re-merge operations, but also the number of in-place updates carried out for lists that are large enough to be considered long , but that, during a single index update cycle, do not accumulate enough postings for an in-place update to be economical. The method, therefore, is able to reduce both components of the search engine X  X  total index maintenance disk overhead.
 As a final note, it is worth pointing out that the overall performance of HIM C + PF with J
PF  X  auto ; at least for the GOV2 collection ( c 1 = 1.632, c 2 = 1.564), is actually quite close to that of geometric partitioning (Lester et al. 2005 ) with p = 2 (i.e., two on-disk index partitions). The latter exhibits a slightly lower asymptotic disk complexity of H  X  n 1 : 414  X  : HIM C + PF, however, has the advantage that it stores all its on-disk posting lists in a contiguous fashion, leading to better query performance than geometric partitioning. 7 Discussion and conclusions We have presented and analyzed H YBRID I MMEDIATE M ERGE with contiguous posting lists (HIM C ), an index maintenance strategy to be used in dynamic text retrieval systems for incremental text collections. The method is a combination of the well-known R EMERGE and I
NPLACE update policies for on-disk inverted files. Posting lists are divided into short lists and long lists. Short lists are updated according to the R EMERGE policy, while long lists are updated in place. In contrast to previous approaches to hybrid index maintenance, our method is not based on heuristics, but takes into account the actual performance charac-teristics of the storage system (random access performance versus sequential read/write performance) when deciding whether a list should be updated in place or not.

Our experiments with two text collections of non-trivial size showed that HIM C can reduce the search engine X  X  index update overhead by up to 56%, compared to R EMERGE .A refined version of HIM C , called partial flushing , was able to achieve a reduction of 73%.
A theoretical performance analysis of HIM C , based on the assumption that the terms in the text collection being indexed roughly follow a Zipfian distribution, was able to predict the general performance of the method, and was also able to predict the relative perfor-mance of HIM C on different text collections with different term distributions. It did, however, fail to predict HIM C  X  X  exact index maintenance complexity, measured by the number of bytes transferred to disk and the number of in-place list updates performed. We found that the reason for the discrepancy between theoretical performance and measured performance was the disagreement between Zipfian distribution and actual term distribution. For both collections used in our experiments, the Zipfian distribution con-sistently underestimated the frequency of terms in the range 10 1 B rank B 10 5 (the Zipfian distribution was off by up to a factor 5 for one of the collections).

Unfortunately, this discrepancy is impossible to avoid, as a Zipfian distribution (after a log/log transformation) is always represented by a straight line in the rank/frequency plot. The text collections used in our experiments (and in fact all other collections that we looked at), however, exhibit a curved rank/frequency plot. Term distribution models that better reflect the actual term distribution in a given collection are known (e.g., Lavrenko 2001 ), but require more complicated parameter estimation procedures than the simple Zipf distribution with its single parameter a . Furthermore, we do not know yet whether those models can be used to arrive at an asymptotical performance prediction of the type derived in Sect. 5 .
Despite this slightly unsatisfactory result, it is clear that HIM C constitutes a substantial improvement over the basic R EMERGE policy. This was not only confirmed by our experi-mental performance figures, but also by a regression analysis which we conducted and which showed that HIM C with partial flushing reduces the overall disk complexity of the search depending on the term distribution in the text collection. Unlike other index update strategies (e.g., geometric partitioning; Lester et al. 2005 ), HIM C does not achieve this speedup by sacrificing its query performance. HIM C and R EMERGE both store their on-disk posting lists in a contiguous fashion and therefore exhibit the same query performance. Thus, the method does not represent a trade-off, but is indeed superior to the R EMERGE baseline.

In our evaluation of HIM C , we strictly limited ourselves to the case of incremental text collections, where new documents are added to the collection, but existing documents are never removed or modified. Of course, this is not a very realistic assumption. Fortunately, it is not very difficult to add support for document deletions to a maintenance policy designed for incremental updates, for example by defining a garbage collection policy that periodically sweeps the entire index whenever the relative number of garbage postings in garbage collection policy has more flexibility in combination with HIM C than with the non-hybrid R EMERGE strategy, as it may be applied to individual lists instead of the entire index, perhaps even in a piggy-backing fashion, during query processing (Chiueh and Huang 1998 ). At this point, we do not have any experimental evidence indicating the exact effect of document deletions and of HIM C  X  X  greater garbage collection flexibility. We surmise, however, that HIM C  X  X  performance advantage over R EMERGE is slightly lower in the presence of deletions than for the strictly incremental case.
 References
