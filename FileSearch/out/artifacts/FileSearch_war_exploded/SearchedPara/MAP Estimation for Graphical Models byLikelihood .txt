 Graphical models provide an effective framework to model complex systems via simpler local in-teractions and also provide an insight into the structure of the underlying probabilistic model. In particular, we focus on the class of undirected models called Markov random fields (MRFs) for which the joint distribution can be specified as the product of potential functions over the cliques of the graph. For many practical problems modeled using MRFs, finding the maximum a posteriori (MAP) assignment or the most probable assignment to the variables in the graph is a key infer-ence problem. For example, MAP estimation has been applied to image processing in computer vision [17, 11], protein design and protein side-chain prediction problems [17, 11], and natural lan-guage processing [7]. Finding the MAP assignment is NP-hard in general except for tree-structured graphs and graphs with bounded treewidth [5, 3]. This further underscores the need for developing scalable approximation algorithms that provide good solution quality.
 Recently, many algorithms have been proposed for approximating the MAP problem [15, 5, 11, 9, 6]. Particularly, linear programming (LP) relaxation of the MAP problem has emerged as a popular technique to solve large-scale problems such as protein design and prediction problems [17, 10, 11]. Such approaches relax the constraint that the solution for the MAP problem be integral. However, for large problems such as protein design, the large size of the LP prohibits the application of standard LP-solvers [17]. To alleviate such scalability issues, convergent message passing algorithms have been introduced, which monotonically decrease the dual objective of the LP relaxation [5, 11, 9]. Convergence to the global optima is not guaranteed in general, but when the solution is integral, it can be shown to be globally optimal. The main advantage of these approaches lies in their ability to provide an upper bound on the problem and a certificate of optimality when upper bound is sufficiently close to the decoded solution. In our work, we take a different approach to the MAP problem based on mean field methods in variational inference [16]. First, we present an alternate representation of the MAP problem by de-composing the MRF into a finite-mixture of simple Bayes nets in which maximizing the likelihood of a special variable is equivalent to solving the MAP problem. Our approach is inspired by re-cent developments in planning by probabilistic inference and goal-directed planning [1, 13, 14, 12]. Second, using this alternate representation, we derive the EM algorithm for approximate MAP es-timation. EM increases the lower bound on the MAP assignment monotonically until convergence and lends itself naturally to a graph-based message passing implementation.
 The main advantage of the EM approach lies in settings where a good approximation to MAP needs to be generated quickly. In our experiments on some of the largest protein design problems [17, 11], we show that EM increases the lower bound on MAP rapidly. This attribute of EM combined with the Max-Product LP algorithm (MPLP) [5, 11] that decreases the upper bound rapidly (as observed empirically) yields a new hybrid approach that provides quality-bounded solutions signifi-cantly faster than previous approaches. Although convergence to the global optima is not guaranteed, EM achieves an average solution quality within 95 % of optimal for the protein design problems and is significantly faster than both MPLP [5, 11] and max-product (MP) [8]. We show that each iter-ation of EM is faster than that of max-product or MPLP by a factor related to the average degree of the graph. Empirically, the speedup factor can be as high as 30 for densely connected problems. We also show that EM is an embarrassingly parallel algorithm and can be parallelized easily to fur-ther speedup the convergence. Finally we also discuss potential pitfalls that are inherent in the EM formulation and highlight settings in which EM may not perform well. A pairwise Markov random field (MRF) can be described by an undirected graph G = ( V, E ) con-of nodes. A variable can take any value from a set of possible values referred to as the domain of that variable. An edge ( i, j ) between nodes x i and x j specifies a function  X  ij . The joint assignment x has the probability: The MAP problem consists of finding the most probable assignment to all variables under p ( x ;  X  ) . This is equivalent to finding the complete assignment x that maximizes the function f ( x ;  X  ) = P ij  X  E  X  ij ( x i , x j ) . Before describing our formulation of the MAP problem, we first describe the marginal polytope associated with the MAP problem and its outer bound based on LP relaxation. Then we discuss the relation of our approach with these polytopes. For details, we refer to [16, 11]. Let  X  denote a vector of marginal probabilities (also called mean parameters) for each node and arises from some joint distribution p is referred to as the marginal polytope: The MAP problem is then equivalent to solving the following LP: It can be shown that there always exists a maximizing solution  X  which is integral and gives the optimal x . Unfortunately, the number of constraints used to describe this polytope are exponential and thus it cannot be solved efficiently. To remedy this, LP relaxations are proposed that outer bound the polytope M ( G ) . The relaxation weakens the global constraint that  X  arises from some common distribution p . Instead, only pairwise and singleton consistency is required for mean parameters as given by the following conditions:
X The outer bound polytope is expressed as LP relaxation approaches such as MPLP [5, 11] optimize the function  X   X   X  over this outer bound M
L ( G ) and consequently yield an upper bound on the MAP. Next we describe our approach for estimating the MAP.
 Inner bound on the marginal polytope In the definition of marginal polytope M ( G ) (Eq. 1), no restrictions are placed on the probability distribution p . Consider the following class of probability similar to the mean field methods used in variational inference [16]. Our approach is to directly optimize over the following set of mean parameters  X  : where p 0 is the distribution that factorizes according the variables in the MRF. Clearly M lb ( G ) is an inner bound over M ( G ) , because in M ( G ) there is no such restriction on the class of allowed probability distributions. The optimization criterion for estimating MAP under this set is: Let f ? lb denote the optimizing value for the above formulation and f ? for the formulation in Eq. 2. exists a maximizing  X   X  M ( G ) that is integral and thus f ? corresponds to an integral assignment x which is also the MAP assignment [16]. Since all the integral assignments are also allowed by implies f ? lb = f ? and yields the MAP estimate. It is worth noticing that the constraints describing the set M lb ( G ) are only linear in the number of nodes in the MRF and correspond to normalization constraints as opposed to the exponentially large constraint set for M ( G ) .
 It might appear that we have significantly reduced the space of allowed mean parameters  X  while still preserving the MAP. But the problem still remains challenging. The reduced set of parameters M optimization over M lb ( G ) cannot be done using linear programming. To alleviate this problem, we next present another reformulation of the optimization problem in Eq. 6. Then we present the Expectation Maximization (EM) algorithm [4] for this reformulation that monotonically increases the lower bound on the MAP assignment using likelihood maximization until convergence. In this section we reformulate the optimization problem in Eq. 6 and recast it as the problem of likelihood maximization in a finite-mixture of simple Bayes nets. The key idea is to decompose the MRF into a mixture of simpler Bayes nets with many hidden variables  X  all the variables x i of the MRF. To incorporate the potential functions  X   X  X  of the MRF and achieve equivalence between the likelihood and MAP value, a special binary reward variable  X   X  is introduced with its conditional distribution proportional to potentials  X  . The details of the reformulation follow. For each edge ( i, j ) in the graph G corresponding to the MRF, we create a depth-1 Bayes net. It consists of a binary reward variable  X   X  with its parents being the variables x i and x j . The reason for calling it a reward variable will become clear later. Fig. 1(a) shows a pairwise MRF over four variables. Fig. 1(b) shows the equivalent mixture of Bayes nets for each of the four edges in this MRF. The mixture random variable l , which is used to identify the Bayes nets, can take values from 1 to | E | , the number of edges in the graph, with uniform probability. That is, if k = | E | , then P ( l = i ) = 1 /k for any 1  X  i  X  k . In what follows, we will also use the variable l to denote the corresponding edge in the MRF.
 The parameters to estimate in this mixture are the marginal probabilities for each node x i . That is distribution p 0 of the set M lb ( G ) (see Eq. 5).
 Next we set the conditional probability distribution of the variable  X   X  for each of the Bayes nets. This is done as follows: where l indicates a particular Bayes net corresponding to an edge of the MRF, x l 1 and x l 2 are the parent variables of  X   X  in this Bayes net and  X  l the potential function for this edge.  X  max is the maximum value for any potential function  X  , and  X  min the minimum value. For example, for l = 1 in Note that these probabilities are nothing but the normalized potential functions  X  ij of the original MRF. For this reason,  X   X  is also called a reward variable. It is used to establish the equivalence between the MAP value and the likelihood of observing  X   X  = 1 .
 The full joint for a particular Bayes net indicated by the variable l is given by x link between the likelihood and MAP value.  X  l ( x l ) denotes the corresponding potential function  X  l of the MRF, for l = 1 in Fig. 1(b),  X  l ( x l ) =  X  12 ( x 1 , x 2 ) .
 Theorem 1. Let the CPT of binary reward variable  X   X  be selected such that  X   X  x l  X   X  l ( x l ) . Then maximizing the likelihood L p = P (  X   X  = 1; p ) of observing the reward variable in the mixture of Bayes nets is equivalent to the MAP estimation of the original MRF.
 Proof. The likelihood for a single Bayes net is given by For the complete mixture, it is given by Upon substituting the definition of  X   X  x l from Eq. 7 and using simple algebraic manipulations, we get Notice that the LHS of the above equation is the same as the optimization objective in Eq. 6. Thus we have shown that maximizing the likelihood L p provides the MAP estimate.
 The above equation can also be explained intuitively in the context of goal directed planning. The there are only two rewards in the system:  X  min and  X  max . The goal is to achieve the higher reward  X  max for each edge in the MRF. Thus maximizing the probability L p of achieving this goal solves the optimization problem. We now derive the EM algorithm [4] for maximizing the likelihood of the reward variable in the mixture of Bayes nets. In this mixture, only the reward variable is treated as observed (  X   X  = 1 ); all Algorithm 1 : Graph-based message passing for MAP estimation input : Graph G = ( V, E ) for the MRF and potentials  X  for each edge repeat until stopping criterion is satisfied MPLP : Return complete assignment x s.t. x i = argmax  X  x EM : Return complete assignment x s.t. x i = argmax  X  x other variables are latent. We note that EM is not guaranteed to converge to a global optimum. How-ever, our experiments show that EM achieves an average solution quality within 95 % of optimal for the standard MAP benchmark of protein design problems. We also show that the update equations for EM can be implemented efficiently using graph-based message passing and are computationally much faster than other message-passing algorithms such as max-product [8] and MPLP [5]. Below, we derive the update equations for the M-step. The E-step can be directly inferred from that. The parameters p to estimate are the marginal probabilities p i for each variable x i .
 M-step: EM maximizes the following expected complete log-likelihood for the mixture of Bayes nets. The variable p denotes the previous parameters and p ? denotes the new parameters. The full joint is given by: We will omit the parameter p whenever the expression is unambiguous. Taking the log, we get: Substituting the above equation into the definition of Q ( p , p ? ) (Eq. 11) and discarding the terms which are independent of p ? , we get: Upon simplifying the above equation by grouping together the terms associated with the variables x of the MRF, we get: where Ne ( i ) denotes the set of immediate neighbors of the node i in the MRF graph. The above expression can be easily maximized by maximizing for variables x i  X  X  individually. The final update equation for the marginals is given by: where C i is the normalization constant for variable x i , and  X   X  x i x j is the normalized reward: Algorithm 1 shows the graph-based message passing technique for both EM and MPLP. For both EM and MPLP, parameters are initialized randomly. The rest of the steps are self-explanatory. Figure 2: a) Quality achieved by EM for all protein design instances. b) Quality for the largest instance  X 1fpo X ; x -axis denotes time (in sec.), y -axis denotes the quality achieved. Complexity analysis and implementation Consider a single message  X  sent out by a node in MPLP. The complexity of computing  X  is O ( d 2  X  deg ) , where d is the domain size of the variables, and deg represents the average degree of the graph or the average number of neighbors of a node. For EM, this complexity is only O ( d 2 ) . Therefore, the computational complexity of each iteration of EM is lower than that of MPLP by a factor of deg . The same result holds for max-product, because its message-passing structure is similar to that of MPLP. The average number of neighbors in dense graphs such as the ones encountered in protein-design problems can be as high as 30 . This makes EM significantly faster than the previous approaches as we demonstrate empirically in the next section.
 EM X  X  simple message passing scheme also facilitates a very efficient parallel implementation. In particular, all the  X  messages for the current iteration in Alg. 1 can be computed in parallel for each node i , because they depend only on the parameters from the previous iteration. In contrast, MPLP follows a block coordinate descent strategy in which optimization is performed over a subset of variables, keeping all the other variables fixed [5]. Therefore, opportunities for parallelism in the current implementations of MPLP are limited. Our first set of experiments are on the protein design problems (total of 97 instances), which are described in [17]. In these problems, given a desired backbone structure of the protein, the task is to find a sequence of amino-acids that is as stable as possible or has the lowest energy. This problem can be represented as finding the MAP configuration in an MRF. These problems are particularly hard and dense with up to 170 variables, each having a large domain size of up to 150 values. We compare performance with the MPLP algorithm as described in [5, 11] and with max-product [8]. We used the standard setting for MPLP  X  first it is run with edge based clusters for 1000 iterations [5] and then clusters of size 3 are added to tighten the LP relaxation [11]. EM was implemented in JAVA. To speedup the convergence of EM, we used a simple modification of the M-step as described in [12]. All our experiments were done on a Mac Pro with dual quad-core processor and 4GB RAM. All algorithms used only a single processor for computation. We note that another clustering-based improvement of MPLP is presented in [9]. Such clusters can be similarly incorporated into the EM algorithm, which currently does not use any clusters. Therefore, comparisons with such clustering techniques are left for future work.
 The main purpose of our experiments is to show that EM achieves high solution quality, much more quickly than MPLP or max-product. Therefore EM provides a good alternative, particularly when fast near-optimal solutions are desired. As reported in [11], for protein design problems solved exactly, mean running time was 9.7 hours. For all the problems, instead of running MPLP until the near-optimal solution is found, we used a fixed cutoff of 5000 sec. For all the problems, we ran EM and max-product for 1500 iterations. For EM, different runs were initialized randomly and the best of 5-runs is plotted. Empirically, EM achieves a solution quality within 95% of optimal on average much faster than MPLP. The longest time EM took for any protein design instance was 352 sec. for the  X 1fpo X  instance (Fig. 2(b)). Figure 3: Quality comparison with MPLP for six of the largest protein design instances. The x -axis denotes time (in sec.) and the y -axis denotes the quality achieved.
 Fig. 2(a) shows the solution quality EM achieves for all the instances in 1500 iterations. Since a tight upper bound for all the problems except the instance  X 1fpo X  is known [11], we show the percentage of optimal EM achieves. The legend titled  X  X ptimal X  in Fig. 2(a) shows this value. For the unsolved instance  X 1fpo X , we use the best known upper bound MPLP achieved in 10 hours (  X  434 ). As it is clear from this graph, EM achieves near-optimal solution quality for all the instances, within 95 % on average. To show empirically that MPLP decreases the upper bound quickly, we also show the percentage of solution quality EM achieves when instead of using the best known upper bound, we use the upper bound provided by MPLP after 1,000 iterations. The legend in Fig. 2(a) titled  X  X B X  shows this percentage. Even using this bound, EM achieves a quality within 91 % on average (legend  X  X VG-UB X ). This further suggests that combining EM X  X  ability to rapidly increase the lower bound and MPLP X  X  ability to decrease the upper bound quickly, is a good way to create a hybrid approach that can provide provably near-optimal solutions much faster.
 Fig. 2(b) shows the quality achieved by EM and MPLP as a function of time for the largest instance  X 1fpo X . To show the convergence curve of EM clearly, the plot uses a different scale for time T  X  200 and the rest. This graph also shows that EM provides a much better solution quality, much faster than the MPLP. Legend  X  X .B X  denotes the best known upper bound. Empirically, we noticed that the main advantage of the EM approach was for problems which were large in size having many variables. For smaller problems, EM and MPLP were comparable in performance. Fig. 3 shows the quality comparisons with time for some of the largest protein design instances. Each graph title shows the instance name, N denotes the number of variables in the MRF, C denotes the number of potential functions  X  or edges in the graph. For all these problems, EM provided near-optimal solution quality and is significantly faster than MPLP.
 We also compared EM with max-product. Table 1 shows this comparison for some of the largest protein design instances. In this table, MP Quality denotes the best quality max-product achieved in 1500 iterations, Time/Iteration denotes the time and iteration number when it was achieved for the first time. Again, EM outperforms max-product by a significant margin, achieving a higher solution quality much faster. For some of the problems, such as  X 1tmy X  and  X 1or7 X , the quality achieved by EM was much higher. Also, for none of these problems max-product converged. This may be due to the fact that these are highly constrained problems with many cycles in the graph. The average degree of a node for these problems is very high, e.g.,  X  37 for  X 1fpo X . The time required per iteration of max-product was 11 sec. for this instance. Therefore the predicted time per EM X  X  iteration is 11 / 37  X  0 . 298 sec.; the actual time for EM was 0 . 235 sec. The same result holds for other instances as well. This is consistent with the complexity analysis in Sec. 4.
 We also tested EM on the protein prediction problems [17, 11] which are simpler and sparser than the protein design problems. The LP relaxations in this case can be solved even by the standard LP solvers unlike the protein design problems [17]. Surprisingly, EM does not work well on these Table 1: Solution quality and time (in sec.) comparison between EM and Max-Product (MP). U.B. denotes the best known upper bound. problems. For the hardest instance  X 1a8i X  ( 812 variables, 10124 edges, edge density= . 03 ), MPLP achieves the near optimal value of 73 , whereas EM could only achieve a value of  X  374 . The reason for this lies in the reward structure of the problem or the values  X  min and  X  max . For this problem  X  min =  X  5770 . 96 and  X  max = 3 . 88 . As shown earlier, EM works with the normalized rewards assigning 0 to the minimum reward  X  5770 . 96 , and 1 to the maximum reward 3 . 88 . This dramatic scaling of the reward is particularly problematic for EM as shown below.
 According to Thm. 1, the log-likelihood EM converges to is  X  2 . 949  X  10  X  4 . For EM to achieve the value 73 , the log-likelihood should be  X  2 . 913  X  10  X  4 . However the drastic scaling of the reward causes this minor difference to significantly affect solution quality. In such settings, EM may not work well. In contrast, for the largest protein design instance  X 1fpo X , the minimum reward is  X  59 . 2 and maximum is 4 . 37 . We also experimented on a 10  X  10 grid graph with 5 values per variable using the Potts model similar to [5]. We randomly generated 100 instances and found that EM achieved good solution quality, within 95 % of optimal on average. The difference between the maximum and This further supports the previous analysis. A number of techniques have been developed recently to find the MAP assignment of Markov ran-dom fields. Particularly successful are approaches based on LP relaxation of the MAP problem such as MPLP. Such approaches minimize an upper bound relatively quickly, but take much longer to find a good solution. In contrast, our proposed formulation seeks to provide good quality solu-tions quickly by directly maximizing a lower bound on the MAP value over the inner bound on the marginal polytope. The proposed Expectation Maximization (EM) algorithm increases this lower bound monotonically by likelihood maximization and is guaranteed to converge. Furthermore, EM X  X  update equations can be efficiently implemented using a graph-based message passing paradigm. Although EM may get stuck at a local optimum, our empirical results on the protein design dataset show that EM performs very well, producing solutions within 95 % of optimal on average. EM achieves such high solution quality significantly faster than MPLP or max-product for many large protein design problems. Another significant advantage EM enjoys is the ease of parallelization. Using advanced parallel computing paradigms such as Google X  X  MapReduce [2] can further speedup the algorithm with little additional effort. Finally, we examined a setting in which EM may not work well due to a large gap between the minimum and maximum reward. Our ongoing efforts include incorporating some of the advanced clustering techniques based on LP relaxation of the MAP problem with the EM method, and designing heuristics that can help EM avoid getting stuck in local optima for problems with large variations in the reward structure. We thank anonymous reviewers for their helpful suggestions. Support for this work was provided in part by the National Science Foundation Grant IIS-0812149 and by the Air Force Office of Scientific Research Grant FA9550-08-1-0181.
