 Publishing structured data and linking them to Linking Open Data (LOD) is an ongoing effort to create a Web of data. Each newly involved data source may contain duplicated in-stances (entities) whose descriptions or schemata differ from those of the existing sources in LOD. To tackle this hetero-geneity issue, several matching methods have been develope-d to link equivalent entities together. Many general-purpose matching methods which focus on similarity metrics suffer from very diverse matching results for different data source pairs. On the other hand, the dataset-specific ones leverage heuristic rules or even manual efforts to ensure the quality, which makes it impossible to apply them to other sources or domains. In this paper, we offer a third choice, a general method of automatically discovering dataset-specific match-ing rules. In particular, we propose a semi-supervised learn-ing algorithm to iteratively refine matching rules and find new matches of high confidence based on these rules. This dramatically relieves the burden on users of defining rules but still gives high-quality matching results. We carry out experiments on real-world large scale data sources in LOD; the results show the effectiveness of our approach in terms of the precision of discovered matches and the number of missing matches found. Furthermore, we discuss several ex-tensions (like similarity embedded rules, class restriction and SPARQL rewriting) to fit various applications with different requirements.
 D.2.12 [ Software Engineering ]: Interoperability; H.2.8 [ Database Management ]: Database Applications; I.2.4 [ Artificial Intelligence ]: Knowledge Representation For-malisms and Methods Instance matching, semi-supervised learning, EM algorithm, association rule mining The Linking Open Data (LOD) project, also known as Linked Data [4] or the Web of data, is an ongoing effort to connect structured open data published on the Web. En-couraged by the Linked Data principle 1 , these data are rep-resented in a Resource Description Framework (RDF 2 )and interlinked using typed links to constitute a single global data space. With these links, we can navigate from a data item in one data source to related items in other sources.
Currently, LOD already contains abundant data sources of various domains: media, geography, life science, publica-tions, government, etc. Since these data sources are usu-ally designed and built independently, they often contain instances (entities) that refer to the same real-world thing with descriptions using heterogeneous schemata. According to the Linked Data principles, instances use URIs as their identifiers and different data sources contain instances of d-ifferent URIs even though some instances might represent the same entity. Due to the decentralized nature of LOD, while it is difficult or even impossible to assign a unique URI for equivalent instances from different data sources, we can still interlink these instances through equivalence relations (i.e. owl:sameAs , a special link from the OWL standard 3 [8].

The LOD space is continuously evolving; when a data source is newly added or is updated, we should connect in-stances to their possible correspondences in other existing data sources. The procedure for addressing this problem is often called instance matching ,alsoknownas entity match-ing or link discovery in the field of the Semantic Web.
Most practical instance matching methods for LOD are either domain-specific [30, 28] or dataset-specific [12, 3], so they cannot be adapted to other domains or data sources. In recent years, general-purposed approaches have also been proposed. On one hand, link discovery frameworks leverage manually defined dataset-specific matching rules (i.e. link specifications) to discover matched instances [13, 32]. On the other hand, other approaches focus on similarity met-rics on instance descriptions, properties and property values to find correspondences automatically [16, 6, 2, 18, 19, 25]. Their performance depends heavily on the quality of un-derlying data and thus suffers from very diverse matching results for different data source pairs.

In this paper, we offer another choice to tackle the instance matching problem. We try to relieve the burden on user http://www.w3.org/DesignIssues/LinkedData.html http://www.w3.org/TR/rdf-concepts/ http://www.w3.org/TR/owl-ref/ by automatically discovering dataset-specific matching rules while preserving high accuracy and good coverage. These derived rules have the capacity to find the most discrimina-tive data characteristics for a given data source pair; this is similar to methods considered in [14, 15]. We list the main differences between our approach and the above two in detail in the related work section (Section 6).

Specifically, we design a semi-supervised learning algo-rithm (based on an Expectation-Maximization (EM) frame-work) to iteratively refine matching rule sets and find new matches of high confidence using these rules. A small num-ber of existing matches in the form of owl:sameAs proper-ties are used as seeds and the matching rules are treated as parameters (to be estimated) for maximizing the likelihood function. In addition, we proposed a graph-based metric to estimate the matching precision. The metric serves as the likelihood function. We also introduce Dempster X  X  rule to combine confidence values of different matching rules. Fur-thermore, we discuss several extensions to fit various appli-cation scenarios of different requirements. Our approach is evaluated with real-world large scale data sources in LOD. The experimental results demonstrate the effectiveness of our approach in terms of the high precision of newly discov-ered matches and the large number of missing matches it found.

The rest of this paper is organized as follows. In Section 2, we give an overview of the framework of our proposed approach and describe some relevant concepts. Then, in Section 3, we introduce the workflow of our semi-supervised instance matching approach and discuss its convergence and complexity. Some straightforward extensions are also in-troduced in Section 4. Section 5 reports the experimental results. We discuss related work and compare some other approaches with ours in Section 6, and finally, we give a conclusion and outline future work in Section 7.
The problem we want to solve is discovering instance match-es, so firstly, we introduce some basic concepts.
An instance that refers to a real-world thing is usually described by several property-value pairs. A property-value pair along with an instance constitute a triple s, p, o .Gen-erally, the subject ( s ) is the instance mentioned above, the predicate ( p ) stands for the property and the object ( o tands for the value, which can be a literal or another in-stance. The set of triples in a given source constitute a graph G .

Definition 1. Equivalence of Instances , denoted by  X  I , is an equivalence relation. It indicates that two instances are actually the same thing in the world. Two instances, e 1 and e , which are identified by different URIs are equivalent iff. ( e
Definition 2. A Correspondence found by instance match-ers can be represented as e 1 ,e 2 , conf in this paper, where e and e 2 are two instances and they satisfy ( e 1 ,e 2 )  X  X  X  the confidence value of conf (conf  X  [0 , 1]).

Instance matchers usually set a threshold of confidence to determine what correspondences can be output as the final matches.
 The framework of our proposed approach is outlined in Figure 1. After preprocessing, a vector of property-value pairs, similar to document description, is attached to each instance. Then, initial matches (i.e. seeds, we will discuss how to get seeds in Section 5.1) are imported into the system to drive the discovery of new matches. After that, the set of seeds is expanded with newly found high-quality matches and the system enters another loop. The same procedure is repeated until the termination condition is satisfied.
The main method of discovering matches in our approach is learning rules to deduce instance equivalences. First, rules are mined in known matches given potential property equiv-alences (see Section 3.1.1). Then, these rules are applied to unmatched instances to find new equivalent ones (see Sec-tion 3.1.2).
 Here we give an typical example to illustrate this idea. Suppose we have mined three property equivalences from descriptions of known matches as follows, dbpedia:phylum ( p 13 )  X  ( p 23 ) gs:inPhylum .

In practice, a property may be equal to another one. For example,
The property dbpedia:phylum defined by dbpedia.org and the property gs:inPhylum defined by geospecies.org have the same intensional meaning: their values both refer to the same taxonomic rank in biology when given a certain instance (species). We say such properties are connotatively equivalent.

Based on these property equivalences and known instance equivalences, we can mine a rule like this: if two instances e and e 2 satisfy ( p ( e, o ) is the function expression of e, p, o and o 1 o 2 means both o 1 and o 2 refer to the same instance or literal), then we have ( e 1 ,e 2 )  X  X  X  I .

Meanwhile, our data set contains two instances from DB-pedia and GeoSpecies respectively, So the rule above can be applied to them and can deduce that these two instances are equivalent. If the confidence of the correspondence representing this equivalence is high enough, this correspondence will be seen as a known match in the next iteration.

For convenience, we use Z in our example to denote a shared instance linking to both dbpedia:ep and gs:ep (i.e. a known match). But we need a formal expression of comparing property-value pairs.

Definition 3. Equivalence of Property-Value Pairs , denoted by  X  P , is an equivalence relation. Given two conno-tatively equivalent properties ( p 1 and p 2 ) and two values ( and o 2 ), the two property-value pairs ( p 1 ,o 1 and p 2 are equivalent iff. ( o 1 ,o 2 )  X  X  X  I (values are both instances) or o 1 = o 2 (values are both literals).

Now we extend the equivalence relation to property-value pair sets. Given an instance e and a set of properties (prop-erty suite) P , we have a property-value pair set PV e,P = { p, o | p  X  P, e, p, o  X  G } .

Definition 4. Equivalence of Property-Value Pair Set-s , denoted by  X  S , is an equivalence relation. Given t-wo instances ( e 1 and e 2 ) and a set of equivalent property pairs ( P 1 ,P 2 ), two property-value pair sets (PV e 1 ,P PV e 2 ,P 2 ) are equivalent iff. there exists a bijection
Some properties have great discriminating power so that their values can uniquely identify corresponding instances. They are defined by OWL as inverse functional properties (IFP). However, using a single property to uniquely identify instances is quite difficult in most cases. Instead, as the above example shows, using combinations of properties can be an alternative choice. We call such combinations inverse functional property suites (IFPS).
 Definition 5. A set of equivalent property pairs eps is an Inverse Functional Property Suite iff. it satisfies: if
Once the IFPS is determined, we can make rules similar to Rule (1) in the above example. We call such rules IFPS rules.

Definition 6. An IFPS Rule is based on an Inverse Func-tional Property Suite eps. For all property pairs p i 1 ,p eps, an IFPS rule has the form:
Note that the universal quantification  X  o 1  X  o 2 . in Rule (2) indicates that if a property has more than one value, all of its values should have equivalents. Practically, this re-striction is too strict in most cases, so we change it to the existential quantification. Therefore, we have the concept of an Extended IFPS (EIFPS) rule.
 Definition 7. For all property pairs p i 1 ,p i 2 in eps, an EIFPS Rule has the form:
The original universal quantification can be realized by calculating the proportion of overlapping values of the given property for two instances and we will discuss this method in Section 4.
The procedure of discovering rules as well as matches in an iterative way follows the Expectation-Maximization al-gorithm.
The EM algorithm is an iterative procedure to estimate missing data by estimating the model parameter(s) for which the observed data are the most likely.

The EM iteration alternates between performing an ex-pectation step (E-step), and a maximization step (M-step). The E-step estimates the missing data (i.e. correspondences) using the observed data and the current estimate for the pa-rameters. In our scenario, we regard the EIFPS Rules, de-noted by  X  , as the model parameters. The M-step computes parameters maximizing the likelihood function as the data estimated in E-step are used in lieu of the actual missing data.
Algorithm 1: Wrapper algorithm input :Asetof triples , a set of known matches output : A set of newly discovered matches and a set of 1 candidates , matches  X  X  X  ; 2 repeat 3 IFPSs  X  MineIFPSs( triples , seeds , candidates ) ; 4 candidates  X  GetCorrespondences( triples , IFPSs ) ; 5if candidates is large enough then 6 new  X  X  c | c  X  candidates ,c. conf &gt; threshold } ; 7 seeds  X  seeds  X  new ; 8 matches  X  matches  X  new ; 9until new =  X  ( new = null) ;
Before giving the definition of our specific likelihood func-tion, we should introduce the concept of a Correspondence Graph (CG).

Definition 8. A Correspondence Graph is an undirect-ed graph, in which a vertex represents an instance and an edge links two vertices if their corresponding instances are supposed to be equivalent. If the confidence values of cor-respondences are considered as weights of edges, a set of correspondences can map to a unique weighted CG.

The likelihood function is considered to be a function of  X  given the CG M . So we define it as,
Naturally, the probability of M is reflected in the proximi-ty of M and the CG built by real matches. For a set of given correspondences, the proximity is reflected in two aspects: correctness and completeness. Precision and recall are two widely used measures to evaluate these characteristics [10]. However, without complete reference matches, it is difficult to evaluate either of them. Hence we propose an alternative measurement, that is optimizing the precision takes priori-ty (ensuring the precision is higher than a given threshold) and obtaining all potential matches on the premise of that precision value. So the Equation (4) can be continued as,
Given an CG, we can estimate an approximate precision value by evaluating the divergence of this graph: Considering an CG, M , with which instances (vertices) come from two data sources and assuming that no equivalent in-stances exist in a single data source, we can infer that an instance is equivalent to at most one other from the other data source and thus Divergence( M ) should be 1. Incorrec-tmatchesin M may result in a vertex connecting to more than one other vertices, which is contrary to the assumption, and decrease the divergence of M . We should pay attention to the statistical significance of the estimate: the approxi-mate precision is meaningful only if the size of M is large enough.

For each parameter  X  (i.e. an EIFPS rule), we can con-struct an M according to Rule (3), and put it into the final united M . To maximize Precision( M |  X  ), setting a high threshold for each Precision( M |  X  ) is our chosen solution.
Once the precision (threshold) is determined, the number of
M s, as well as the EIFPS rules are also determined. Note that this does not mean that the likelihood will not change, because  X  changes in each iteration due to the different in-puts (line 3 in Algorithm 1).

Algorithm 1 demonstrates the wrapper that implements the EM approach. The first several iterations when the set of candidates is not large enough, as well as the following pass of line 3 can be seen as the startup of the EM algorithm, where the EIFPS rules are initialized. Then, E-step (line 4) and M-step (line 3) execute alternately until the termination condition ( new =  X  ) is satisfied. Note that the conditional statement in line 5 implements the control over the size of M . In practice, this condition is satisfied on every iteration except the first.

Here we introduce the procedures of the M-step and E-step in detail.
The M-step mines EIFPS rules maximizing the likelihood function (Equation 5) given the candidates and seeds ob-tained in the E-step. See Algorithm 2.

Algorithm 2: Mining IFPSuites ( MineIFPSs ) input :Asetof triples , a set of known matches output : A set of Inverse Functional Property Suites 1foreach c  X  seeds do 2 Replace every s, p, c.e 2 with s, p, c.e 1 in triples ; 3 trans  X  X  X  ; 4foreach c  X  seeds  X  candidates do 5 trans  X  trans  X  { p, o | s, p, o  X  triples ,s  X  6 pEquivalents  X  MineAssociationRules( trans ) ; 7 suite  X  X  X  ; 8foreach t  X  trans do 9foreach P  X  pEquivalents do 11 Assign a hash code (hc) computed using 12 suite  X  suite  X  P ; 13 Put t.c into P.M ; 14 if P.M already contains a match with the 15 Merge a vertex of this match with a 16 IFPSs  X  X  X  ; 17 foreach P  X  suite do 18 if Divergence( P.M ) &gt; threshold and 19 IFPSs  X  IFPSs  X  P ;
The newly discovered equivalences (matches with high confidence) from the previous step can be exploited to boost the discovery of potential matches. To make use of them, we should first give equivalent instances a unified identity. The operations in line 2 replace e 2 swith e 1 s.
For each pair of matched instances, their property-value pairs are merged to form a transaction. The procedure of mining association rules in line 6 only uses the properties in transactions. This method, proposed by V  X  olker et al. [31], acquires property subsumption axioms by mining bi-nary association rules. According to Definition 5, prop-erty pairs found here should be connotatively equivalent property pairs. The equivalence relation can be denoted by subsumption relations on both sides. Parundekar et.al [26] used this idea to mine relations between classes. They restricted the confidence values (called P and R )ofsub-sumptions to be larger that 0.9. In practice, some weakly related properties that have overlaps in semantics are also considered to be involved in building rules. For example, there is no inclusion relation between dbpedia:synonym and gs:hasCanonicalName , but there exist certain species in DB-pedia that have canonical names as their synonyms. Such information should not be ignored. So we set a relatively low threshold, 0.1, to output more property pairs with potential relations (weak equivalences).

CGs are constructed for all EIFPS rules. An ideal CG con-tains only connected components that have one edge each. Different instances connected in one component result from sharing the same property-value set on a given property suit-e (line 15). In order to save storage space, property-value sets can be hashed. In our experiments, hash collisions can be neglected if the hash functions is well designed.
The maximization of Precision( M |  X  ) is realized in line 18 by setting a threshold (0.95 in our experiments). Note that the support of an EIFPS rule is also considered although it is not reflected in the algorithm. Sometimes, additional restrictions in a rule are not necessary. For example, we may have S 1  X  A with confidence c and also have S 2  X  A with the same confidence c where S 2  X  S 1 . Naturally, additional restrictions in S 2 are redundant for deducing so we neglect such rules.
The E-step estimates the correspondences (i.e. candi-dates) using the current estimate for the EIFPS Rules. See Algorithm 3.

In line 3, each EIFPS rule is used to find all correspond-ing initial correspondences. Every IFPS brings a confidence value with it. This value is calculated in the M-step (line 18 in Algorithm 2) and it indicates the precision of the CG obtained using that rule in the previous EM iteration.
A correspondence may be derived from several EIFPS rules. As mentioned above, different rules have different confidence values and thus the correspondence will inherit these confidence values. We have to combine them and as-sign the combined confidence to that correspondence (line 5).

Choosing the maximum is a conservative method. But in-tuitively, rules can been regarded as evidence and the more evidence we have, the more likely true the conclusion is. Here we consider two ways to combine such evidence. One is using basic probability theory. That is regarding a con-fidence value of a correspondence as the probability of this
Algorithm 3: Getting candidate correspondences ( GetCorrespondences ) input :Asetof triples and a set of Inverse Functional output : Candidate matches ( candidates ). 1 iniCorrespondences  X  X  X  ; 2foreach eps  X  IFPSs do 3 iniCorrespondences  X  iniCorrespondences  X  4foreach unique e 1 ,e 2 within iniCorrespondences do 5 Calculate final confidence cc by combining 6 candidates  X  candidates  X  X  e 1 ,e 2 , cc } ; correspondence being true, the probability that it is false is 1-confidence. So we have the combination rule,
The other method is to use Dempster-Shafer theory, a mathematical theory of evidence [29]. Dempster X  X  rule of combination is a generalization of a special case of Bayes X  theorem. In the case where only two conditions (true and false) exist, the rule has the form, conf 1  X  conf 2 = conf 1
Although different pieces of evidence cannot be guaran-teed to be independent, which is required by both of the last two methods, these rules have been proven effective in our experiments. Here we give an example of matching in-stances from DBpedia and GeoNames 4 .

Consider a test case consisting of an IFPS eps and two subset of eps (namely eps 1 and eps 2 and eps 1  X  eps 2 =eps). Confidence values of eps 1 and eps 2 are combined using the three combination rules mentioned above. The absolute er-ror equals the combined value minus the reference value which is the confidence of eps. In this example, we have a total of 248 test cases, and the results are shown in Fig-ure 2. Translucent gray boxes representing certain absolute errors are plotted in the graph, so the more often an error value appears, the darker its corresponding position X  X  col-or is. We can see that most error values of Probability and Dempster are near 0.00. In fact, the actual mean er-ror values also indicate that these two rules are better than Maximum . Results of using Probability and Dempster are very close if two confidence values to be combined are relatively high, above 0.95 (a commonly used threshold in our experiments) for example. We choose Dempster X  X  rule of combination finally empirically.
Since the output set of matches is expanded in each it-eration, its size monotonically increases. Meanwhile, the number of instances is finite so the number of equivalen-t instances (including incorrect ones) is also finite. Thus, the termination condition can always be satisfied when no matches are found in a certain iteration.
Detailed information will be introduced in Section 5. Figure 2: Comparison on absolute error among three com-bination rules ( Maximum , Probability and Dempster ) Figure 3: The trends of fresh matches and EIFPS rules
We give a practical example 5 in Figure 3. With the pro-cedure going, mined EIFPS rules are reaching a plateau and the set of fresh matches (newly found matches in each it-eration) gradually becomes smaller, becoming empty in the ninth iteration. Note that the curve of fresh matches re-bounds in the fifth iteration probably because some impor-tant instance equivalences are found in the fourth iteration and given unified identities, which makes many instances share these values from then on.
Within each iteration, the phases can be classified into four categories: Data pre-processing. The first seven lines of Algorith-m 2 fall into this category. The complexity is linear and implementing them in parallel is quite straightforward. Data post-processing. Getting final output from initial candidates/rules in both the E-step and M-step fall into this category. Sorting the initial data is a rational choice. The Map/Reduce framework [7] is suitable for solving such prob-lems.
 Association rule mining. Mining property relations in Algorithm 2 can directly use mature techniques, such as FP-Growth [11] and its parallel implementation [27]. We just implement the simple Apriori algorithm [1] using Map/Re-duce and this method is efficient enough.
 Join of data and rules. The other phases in both the M-step and the E-step fall into this category. In the M-step, we enumerate all possible rules ( K ) for each known match ( N ) to select suitable rules, while in the E-step we enumerate all determined rules ( K ) for each instance ( N to find matched instances. So we have the time complexity: O (
K  X  N ). Note that if a property has more than one value, This example comes from a task of matching DBpedia to GeoNames. the corresponding rule should be joined repeatedly according to the existential quantification in Rule 3, which will expand K dramatically in certain cases. Fortunately, such iterations can be processed in parallel because the joined results are independent.

The outermost loop in Algorithm 1 should be centrally controlled. The Map/Reduce framework as well as exter-nal memory algorithms are adequate for this task because the number of iterations is relatively small in practice (see Section 5.4).
What we discussed in the last section is a general core algorithm. In order to improve its performance in certain scenarios, we propose several (potential) ways to extend our approach.
The property-value pair equivalence measurement current-ly used is based on Definition 3. Literals are matched only when their characters are exactly the same. This method is strict, especially for numeric values. For example, the lat-itude and longitude of a location are usually decimal num-bers, and they cannot be sensibly compared without taking account of allowable errors.

If an IFPS contains properties whose values are numeric, the instances can be indexed by other non-numeric values. Instances with the same index are further compared by their numeric values. Under such circumstances, confidence val-ues of candidate correspondence should be multiplied by the similarities of numeric values. The idea of indexing instances before further comparison is used by several instance match-ing systems such as SILK [32] and Zhishi.links [23]. Note that we do not consider the case that an IFPS contains only properties with numeric values. Some methods have been proposed to solve this problem [17].

Besides numeric values, many other data types (e.g. Date) can also be compared. Even the similarity of two lists of val-ues can be calculated in this way. As we mentioned in Sec-tion 2, universal quantification can be realized by calculating the similarity of lists of values for two instances. Here the similarity is defined as the proportion of overlapping values in two lists.
 In fact, we have implemented the similarity embedded EIFPS rules in discovering matches between DBpedia and GeoNames. We will discuss it in detail in Section 5. One difficult issue of instance matching is disambiguation. For example, we can hardly classify  X  X arry Potter X  to a se-ries of novels, a film series or a character. But if the type (class restriction) of  X  X arry Potter X  is provided, everything becomes easier.

So we can regard the class restriction as one kind of special property and add it to the EIFPS rules. To this end, we should modify the procedure of data pre-processing slightly.
In practice, we find that adding class restrictions has little effect on the final results because several properties have connotative relations with class restrictions. For example, an instance having the property X  X elease date X  X ndicates that it is a work that has a release date like  X  X ilm X . However, this information may be of great importance for some data sources.
The approach we implemented is an offline one. However, since the EIFPS rules are very intuitive, they can be rewrit-ten as SPARQL queries 6 . Reusing the example in Section 2, if we have instance dbpedia:ep and want to find an equiv-alent instance in GeoSpecies, we can construct a SPARQL query based on Rule 1, SELECT ?species WHERE { ?species gs:hasCommonName "X" ;
The execution time of a completely online algorithm may be unacceptable, but it is meaningful to apply it to con-tinually growing data sources when relatively high-quality EIFPS rules have been obtained offline.
Our approach is implemented in Java. All procedures are performed with the Hadoop Map/Reduce framework. All the tests were carried out on a Hadoop cluster which con-tains 40 nodes. Each node is a PC (Intel Core 2 Quad 2.66GHz CPU, 2GB RAM) can run 3 Maps + 3 Reduces simultaneously. This is a shared cluster and we occupy 50 slots in most cases. In our experiments, we considered a general data source DBpedia 7 , and three domain-specific data sources: GeoN-ames 8 ,LinkedMDB 9 and GeoSpecies 10 . They are typical representative data sources in Linked Open Data. The tasks in our experiments is to discover matches between DBpedia and the other three data sources.
 DBpedia is a hub data source in LOD. It structures Wikipedi-a knowledge and make this structured information available on the Web [5]. The DBpedia community occasionally re-lease data dumps and the latest version (3.7) is used in our experiments. From numerous datasets, we chose labels, external links, geo-coordinates and refined ontology infor-mation including instance types and mapping-based proper-ties. Note that we removed instances appeared as objects in  X  X edirect X  and  X  X isambiguation Links X  datasets, but used their labels as aliases.
 GeoNames is a geographical database that integrates tens of data sources and allows users to improve and correct da-ta. The dump version we used was 2012-03-03. GeoNames contains links to Wikipedia pages ( gn:wikipediaArticle ). These links were established manually or by heuristic rules. We converted such links to reference matches.
 LinkedMDB is an open semantic web database dedicated to movie-related information [12]. The dump version we used was 2010-01-29. In our experiments, we only considered instances whose type was film since they are the core of this data source and have sufficient descriptive information. The reference matches are obtained from owl:sameAs links. http://www.w3.org/TR/rdf-sparql-query/ http://dbpedia.org/ http://www.geonames.org/ontology/ http://linkedmdb.org/ http://lod.geospecies.org/ Data Sets Instances Triples References DBpedia 4,071,600 36,565,123 -GeoNames 8,147,136 86,409,073 317,433 LinkedMDB (Film) 97,471 1,269,572 16,447 GeoSpecies 20,939 997,753 11,490 These links were found by ODDLinker, which employs state-of-the-art similarity join techniques.
 GeoSpecies was started to help tie together disparate da-ta about species. The dump version we used was 2010-04-11. Note that some properties which can be seen as cheat-ing information such as skos:closeMatch were removed. GeoSpecies also contains links to Wikipedia pages ( gs:has WikipediaArticle ) and reference matches are converted from them. The mechanism of finding these links is not described by the project owner.
 Statistics of these datasets are illustrated in Table 1.
We measured the precisions, newly found matches, and running times (number of iterations) on different tasks. Be-fore showing the evaluation results in following sections, the evaluation methodology and some basic settings should be discussed.
 Judgements and repeats. Since in many cases, reference matches are far from complete, we have to judge whether a match is correct or not manually. For each matching task, we randomly selected seeds and ran the program three times to measure the average performance. After each execution, 100 randomly selected matches (1000 for DBpedia-GeoSpecies tasks) were judged.
 Seeds. Since our approach is based on the idea of semi-supervised learning, the choice of labeled data (seeds) is an issue of concern. Here we selected different proportions of reference matches as seeds to observe the impact of the num-ber of seeds on algorithm performance. In situations where no references exist, seeds can be found by using some strict heuristic rules or inferred based on the OWL semantics [15]. Parameters. Algorithm 1 needs a threshold (line 6) for selecting outputs. In practice, we set it to 0.98 unless other-wise specified. Meanwhile, all supports for association rules and EIFPS rules in our approach were set to a fixed value, 10. Not many parameters are involved in our approach, and they can be tuned easily using a small amount of training data. We reused the same settings for all data sources, which proved the good stability of our approach in a way [24].
The precision is the ratio of correct matches to total match-es found. We estimate this value by sampling a certain num-ber of output matches as mentioned in the last section. The experimental results are shown in Figure 4.

The X-axis indicates the number of seeds and the propor-tions of selected seeds in complete reference matches. We conceal the absolute seed numbers in the top figure because two matching task series share the same plot.
 When DBpedia was matched to GeoSpecies and Linked-MDB, the precision values of newly found matches were rel-atively high at any number of initial seeds (Figure 4a). In the case of matching DBpedia to GeoNames, the precision decreased slightly (#0.98 in Figure 4b). The precision varied Figure 4: Precisions versus the proportions of selected seeds in complete reference matches partly because differences existed in quality of data sources. Recall that we assume no equivalent instances exist in one single source, however, we have found that several distinct instances may refer to the same place. This naturally affect-ed the estimation of precision in the M-step. Predictably, the precisions were higher when we changed the threshold of outputting matches to 0.99 (#0.99 in Figure 4b).
We also tested an extended version of our approach using similarity embedded EIFPS rules (see Section 4.1). This ex-tended version only applied to matching DBpedia to GeoN-ames because some important numeric values such as lati-tudes and longitudes were involved. The results show that considering similarities had almost no effect on the precision (#0.99(+Sim) and #0.98(+Sim) in Figure 4b).

Note that more seeds did not necessarily lead to higher precision. If things of a certain domain need more property-value information to be uniquely identified, then it is easier to have trouble with over-fitting. Determining a proper sup-port for EIFPS rules will be a solution to this problem.
The newly found matches (new matches for short) are defined as the correct matches apart from ones contained in references. So we calculate their number by where TruePositive means correctly proposed matches with regard to reference matches. The experimental results are shown in Figure 5. Note that the secondary vertical axes on the right side and corresponding curves in first two plots are prepared for the next section (to save space).

The number of new matches is relatively constant for any Figure 5: Newly found matches versus the proportions of selected seeds in complete reference matches number of initial seeds in the cases of matching DBpedia to LinkedMDB, GeoSpecies and GeoNames without using similarity embedded EIFPS rules.
 However, in the case of considering similarity in DBpedia-GeoNames tasks, fewer initial seeds tend to give fewer newly found matches (#0.9X(+Sim) in Figure 5c). This is under-standable because comparing numeric values needs prior in-dexing by non-numeric values, so an EIFPS rule containing properties with both numeric and non-numeric values needs more known matches to support it.

Note that we did not measure recalls and only considered matches out of the set of references so far, because even the reference matches were far from complete. We show the match space constituted by reference matches and newly found matches in Figure 6 to illustrate how many matches can be found by our approach.
 The whole match space is divided into three parts: New Matches, Overlaps and Missing Matches. The last two parts Figure 6: The match space constituted by reference matches and newly found matches Figure 7: Number of iterations versus the proportions of selected seeds in complete reference matches (DBpedia-GeoNames) (streaky and white blocks in Figure 6) together constitute the match space of references. Since we repeated our algo-rithm with different numbers of seeds, we choose the median number of recalls to compute the proportions of  X  X verlap-s X . Naturally, remaining matches that were not proposed by our approach fell into  X  X issing Matches X . The dark blocks that represent  X  X ew Matches X  are using the results under the condition that all references are involved as seeds.
From the results we can see that our approach is qualified for matching DBpedia to GeoSpecies and it also performs better than ODDLinker in matching DBpedia to Linked-MDB. While in the DBpedia-GeoNames matching task, our automatic method cannot discover as many instance equiva-lences as manual methods, but it is complementary to them.
The numbers of iterations we need to accomplish the match-ing tasks are shown in Figure 5a, 5b and 7. We chose the mode of three results since each task was repeated three times. Not too many iterations were needed as the results tell us.

Generally speaking, the number of iterations did not de-crease when fewer seeds were provided. Exceptions existed similar to the situations when we talked about newly found matches in the case of considering similarity in DBpedia-GeoNames tasks. Recall that fewer new instance equiva-lences are found because similarity embedded EIFPS rules have insufficient known matches to support them. Thus, insufficient new matches have difficulty driving the next it-eration.

Besides the number of iterations, we are also interested in the time cost of each iteration. We sample some typical Figure 8: Typical time cost for one iteration in three match-ing tasks running times for a single iteration in Figure 8. The most time-consuming phases are X  X ata pre-processing X  X nd X  X oin of data and rules X  in the E-step (i.e. GetIniCorrespondences) which have been discussed in Section 3.3.
A related problem called record linkage or duplicate de-tection has been investigated for decades in the realm of databases [9, 20]. It focuses on dealing with the matching problems w.r.t. entity-relationship model. Another highly related problem is ontology alignment [10]. Relevant meth-ods can be used for connecting LOD sources, but they focus on schema-level matching and rely on richly structured on-tologies.

Our proposed approach belongs to the category of instance matching in the context of LOD. It has attracted much at-tention and extensively studied by the research community. Some researchers focused on matching instances for specific domains, such as people (FOAF) [30] and music [28]. Some dataset creators designed heuristic methods to discover in-stance equivalents between their data (e.g. LinkedGeoData [3] and LinkedMDB [12]) and DBpedia. Some reusable link discovery frameworks are favored due to the fact that they are domain-independent and customizable (users specify link specifications). SILK [32], LIMES [21] and the framework proposed by Hassanzadeh et al. [13] are typical examples.
Since the similarity of property-value pairs of different in-stances can be used for finding similar instances, various kinds of similarity metrics are employed by many general-purpose (semi-)automatic methods. For example, Castano et al. [6] designed several domain-dependent similarities, Albertoni et al. [2] combined different kinds of similarities, Li et al. [19] used multiple strategies, Ngomo et al. [22] learned weights of pairwise property-values, and there are many others [16, 18, 25].

Some other researchers address the instance matching prob-lem with inference based on OWL semantics. A simple sur-vey can be found in [15]. Specifically, because inverse func-tional properties along with their values have great discrimi-nating power to identify an instance, IFPs play an important role in inferring instance equivalents. Based on this idea, Hogan et al. [14] measured the static discriminating power values of property-value pairs globally and then used these values to compute how likely two instances are matched. In-spired by this work, Hu et al. [15] proposed a self-training algorithm to quantify how discriminating the property-value pairs were dynamically and choose the most discriminating property-value pairs to output current matchable instances iteratively. This method further considered the matchabili-ty between properties and frequent property combinations. The idea of these two approaches is similar to ours, but we choose a quite different and more flexible way to implemen-t it. Unlike [15], we did not pop the most discriminating property-value pairs but the most matchable instances in each iteration, and we did not face the problem of determin-ing the termination condition. At the same time, we used newly found matches in each iteration as new matchable val-ues, while [14] and [15] did not since they focused more on literal values. Furthermore, the cardinality of our EIFPS is unlimited but [14] and [15] compared at most two properties on each side.
In this paper, we proposed a general-purpose approach to automatically mine dataset-specific matching rules and this approach is based on the EM algorithm. We introduced a graph-based metric to estimate likelihood (precision) and Dempster X  X  rule to combine confidence values. Our pro-posed approach discovers new matches by iteratively refining matching rules and integrating known equivalent instances. The number of iterations to accomplish the matching task is relatively small, and the whole process can be implemented parallelly.

Moreover, we discussed some extensions to our approach in order to fit the different requirements of various prac-tical applications. We carried out experiments on several real-world datasets. The results demonstrated the correct-ness of matches discovered by our approach, which achieves the high precision ( &gt; 0.96 in most cases). We also shown more matches are found than existing references (established by dataset-specific methods), which indicates that our ap-proach is qualified for matching instances in LOD.
Since the proposed approach is language independent, we plan to handle the cross-lingual matching problem and per-form more comprehensive experiments on more datasets of different domains in LOD. In addition, we plan to extend the expressivity of our EIFPS rules by considering the negation operator, and also more sophisticated similarity measures will be taken into account.
