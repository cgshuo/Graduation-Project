 We have seen an e xplosive growth of mobile usage, particularly on mobile app s . It is more important than ever to be able to properly evaluate mobile app release. A/B testing i s a standard framework to evaluate new ide as. We have seen much of its applications in the online world across th e industry [9,10,12]. Running A/B tests on mobile apps turns out to be quite different , and much of it is attributed to the fact that we cannot ship code easily to mobile apps other than going through a lengthy build, review and release process . M obile inf rastructure and user behavior differences also contribute to how A/B tests are conducted differently on mobile apps , which will be discussed in details in this paper . In addition to measuring features individually in the new app version through randomized A/B tests, we have a unique opportunity to evaluate the mobile app as a whole using the quasi -experimental framework [21]. Not all features can be A/B tested due to infras tructure changes and wholistic product redesign . We propose and establish q uasi -experiment al techniques for measuring impact from mobile app release, with results shared from a re cent major app launch at LinkedI n.
 A/B te sting, mobile, quasi -experiments , causal inference It is clear that mobile is taking over the Internet . In the U.S., two out of three digital media minutes is now happening on mobile . Without question, mobile app usage has been the single most important driver by far, contributing to nearly 90% of the massive mobile growth in the past two year s [ 5 ]. At LinkedIn, more than 50% our traffic is now coming from mobile, similar to other social networking companies such as Facebook and Pint e rest [3].
 Many companies, including LinkedIn, have shifted to a  X  X obile first X  mindset, focusing their product s trategies around mobile. As a consequence, optimizing for the best user experience in mobile has been more important than ever. However, mobile optimization is a space that is a lot less mature than web optimization. Because of how users consume informatio n on mobile differently, many lessons learnt by optimizing on the web no longer apply. So even companies most experienc ed with web optimization have to start from the bottom and relearn in the mobile space. It is not just about coming up with  X  X hat X  works in mobile, but more importantly, we need to have methodology and system in place to  X  X easure X  whether an idea works. A/B testing, also known as contr olled experiment, is a standard, widely used framework to evaluate new ideas and to make data driven decisions. We have seen much of its applications in the online world discussed in recent publications, including several past KDD papers from Google, Microsoft and LinkedIn [ 9,10,12 ]. However, none of these papers have a focus on mobile , or mobile app s .
 At LinkedIn, we have seen a drastic increase of mobile experiments . The growth is more than just proportional to the amount of development work relative to desktop . We have learnt that because the real estate on mob ile is limited, small changes tend to have big impact . In addition, A/B testing on mobile is used eve n more extensively than it is on desktop, and much of it is attributed to a key difference between desktop and mobile development process. Because we canno t ship code to the mobile app other than building and releasing a new app version, feature release s in general involve not just the app developers, but also the app store s (e.g. Google Play or Apple App Store), and the end users. The app store usually requires a review of the build submitted, and a new app version is not effective until the end users update the app. The consequence is that it may take two weeks or more for any change to reach the majority of users, and even longer for a coverage of greater than 90% . A/B testing is hence heavily leveraged to mitigate risk, as it allows us to evaluate the new feature and gradual ly release it accordingly, without having to release a new version. The fact that we cannot ship code easil y to mobile apps, together with mobile infrastructure and user behavior differences, strongly influences how A/B tests are conducted on mobile apps. We will discuss them in depth in Section 4, following the three steps in the A/B testing process: design, d eployment and offline analysis. In addition to measuring features individually in the new app version through randomized A/B tests, we also have a unique opportunity to evaluate the mobile app as a whole using the quasi -experimental framework [ 21 ] . Not all features can be A/B tested due to infrastructure changes and limitations. For example, LinkedIn recently rewrote the entire flagship app (Project Voyager [3]). Even though some new features were tested and evaluated independently in the old app, many features were launched for the very first time with the new app release. How all the features work together was a key question , but unfortunately we were not able to conduct a randomized experiment on the new app as a whole. However, b ecause not all users adopt the new version at the same time, there is a period of time where we have both versions of the app serving real users. Obviously, simply comparing the adopters with the non -adopters will suffer from self -sel e ction bias . However, we can still reasonab ly establish causal relationship by carefully removing such bias using quasi -experimental techniques (also called observational causal inference methods in some disciplines) . To set the notation and context for both A/B and quasi A/B testing , we review th e Rubin Causal Model [23] , a widely used framework to estimate causal effect. Let  X   X  be the outcome variable for user  X  , e.g. pageviews, clicks. Let  X  treatment variable, where  X   X  = 1 if a user is in treatment (adopted the new app), and  X   X  = 0 otherwise. Note that in the case of mobile adoption,  X   X  is observed rather than randomly generated. We also observe  X   X  , a vector of pre -exposure covariates, such as industry or connection counts. In summary, we observe (  X   X  ,  X   X  ,  X   X  )  X   X  X  X   X  = 1 , ... ,  X  . Under the Rubin Causal framework, each user has two potential outcomes We are interested in knowing the Average Treatment Effect (ATE), the difference of the average outcomes between applying treatment to the entire user population and applying control to the entire user population. By definition, Of course,  X   X  X  X  X  is never known because only one of  X  can be observed (The fundamental problem of causal inference [22 ] ) . Instead, it is usually estimated by Two important assumptions are required for  X   X  unbiased esti mator of  X   X  X  X  X  . (1) Stable Unit Treatment Value Assumption (SUTVA), which states that the behavior of each user in the e xperiment depends only on his own treatment and not on the treatments of others. (2) Ignorable Treatment Assignment Assumpt ion ( also k nown as unconfoundedness ) [21 ], which states that treatment assignment is independent of the two potential outcome s, i.e. Both assumptions are naturally satisfied in most controlled experiments. However, the unconfoundedness assumption is often violated when there exists variables that correlate with both (  X   X   X  ,  X   X   X  ) and  X   X  . As an example, we have observed that LinkedIn members with premium accou nts are more likely to adopt a new version faster. Since premium members are in general m ore engaged on the site, we are likely to conclude a significantly positive impact even if the true  X   X  X  X  X  is 0 or negat ive. In fact, the unconfoundedness assumption can be extended to where  X   X  are the confounding covariates. As a result, if we can identify all such covariates, we can still postulate a model for  X  (  X   X  |  X   X  ,  X   X  ) and estimate ATE based on This is a fundamental concept of quasi -experiment al framework. We will discuss more formally on this in Section 5. Here is a summary of our contribution s in this paper: x As far as we know, we are the first to study extensively how x We identify the key differ ences of conducting A/B tests on x We compare and propose quasi -experimental techniques that x We share insights on how users adopt new app versions, The paper is organized as follows . Section 2 starts with a re view of the existing literature in all the three areas of A/B testing, mobile A/B testing and quasi A/B testing. Section 3 describes the process for mobile app releases and its implications on how we evaluate a new release. Section 4 focuses on A/B testing in mobile app and how it is different from testing on the web. Section 5 proposes a quasi A/B testing framework to evaluate app release, with results shared from evaluating Voyager [3] . Section 6 concludes w ith future work. In this section we review t he theoretical foundations and applications of A/B testing and quasi A/B testing as well as existing tools and products for mobile testing.
 The theory of controlled experiment dates back to Sir Ronald A. Fisher X  X  experiments at the Rothamsted Agricultural Experimental Station in England in the 1920s [ 6 ]. Since then, many textbooks and papers from different fields have provided theoretical foundations [ 7,8 ] for running controlled experiments , incl uding the most widely adopted Rubin Causal Model [22,23] . While the theory may be straightforward, the deployment and mining of experiments in practice and at scale can be complex and challenging [ 15 ]. In particular, several past KDD papers have discussed at length the experimentation systems used at Microsoft Bing , Google , Facebook and LinkedIn [9 ,10,11,12 ], including best practices and pitfalls [ 13, 14 ]. On the other hand, there are many situations where running a controlled experiment is infe asible and o ne has to study causal impact based on observed data . [21] discusses the assumptions of applying the Rubin Causal Model to real evaluations. [2 4, 25] show that the Ignorable Treatment Assignment Assumption is associated with the assumptions of OLS regressi on. Many models have been developed to address the challenges of observational studies, such as propensity score models [21], Heckman X  X  sample selection model [19] and Doubly Robust Estimation [18]. As far as we know, q uasi -experiment techniques have been more widely utilized in econometrics and clinical studies [20,26], and there are few studies of natural experiment in the Internet world. A recent work in this area [17] described a framework for estimating causal effect through identifying mediators and i ts application to advertising. While we focus on in -house A/B testing solutions for mobile app development, there are comp anies specializing in mobile A/B testing , such as Apptimize [27], Optimizely [28] and Mixpanel [29]. These companies provide third party platforms and tools that help mobile developers test in mobile. Other than these commercial products, there seems to be very little literature focusing on mobile A/B testing that we are able to find . In most online web development, agile software development process is highly promoted. Being able to iterate fast and improve continuously is crucial for both risk minimizat ion and feature development. M ost importantly, it is key to ensure the best use r experience because any unexpected issues can be fixed or mitigated as soon as possible . Such fast iterations are feasible for an online website because all changes are controlled on the server side. When a user visits a site, the server pushes the data t o the browser to render. New feature releases can happen constantly and continuously without interruption on the end users. In an A/B test , whether the user sees A or B is fully managed by the server and is entirely independ ent of the end users behavior. T ake Linkedin.com as an example. Whether to show a red or yellow button, whether to show a new ly revamped h omepage or not -these are all changes that can happen instantaneously after server side deployment . However, feature release process and experimentation in a mobile app are quite different. Instead of the app developers having full control over th e deployment and release cycle, the app release process involves all three parties, the app owner (in our case, LinkedIn), the app store (e.g . Google Play or Apple App Store), and the end users. After all the code is ready, the app owner needs to submit a build to the app store for review. Assuming the build passes review (which in the case of iOS takes about a week), releasing it to everyone d oes not mean that everyone who visits the app will have the new version . Getting the new version is a software upgrade, same as update s we get on our desktops where we can delay or ignore while continuin g to use the old version. S ome end users take weeks t o adopt. It is worth pointing out that Google and Apple have very different review policies. While it takes about a week to go through Apple X  X  review process, Google Play store X  X  review is almost instantaneous [ 1 ] . In addition, Google allows staged roll out of a new version [ 2 ] , while Apple app store only supports full roll out. Such differences have important implication on app development and evaluation , as we will cover in Section s 4 and 5 . For most web facing companies who truly believe in agile development, it is painful to have to wait for 2+ weeks for a change to take effect, especially when our end users are dealing with a buggy app . What X  X  worse, depending on how fast users adopt new versions, it may take even longer t o reach more than 90% users. We have more details on the adoption process in Section 5.1. Moreover, even if new app versions can be built and released every other week, it is annoying from the users X  perspective to have to constantly update the app.
 This i s not to say that we cannot A/B test on mobile. As a matter of fact, there are by and large three kinds of experiments that we can conduct on mobile app. 1. Server -side changes : There are many features on mobile 2. Client -side changes : There are many features that need to be 3. Big c hanges that cannot be A/B tested : There are cases As we have mentioned in Section 3, whenever possible, features on mobile apps are tested through controlled experiments. Since server -side changes are usually experimented the same way on app and web , and we have alre ady shared in details how web A/B tests are conducted at LinkedIn in KDD X 15 [ 12 ], we focus our discussion in this section on the kind of experiments that are more specific to mobile -experiments on the client -side features. Recall that these are features that are controlled from the app, and any code changes on these features require a new app release.
 The architecture for a web -based A/B testing system usually has the three essential components, also corresponding to the three steps in an A/B testing proc ess: (1) design (2) deployment (3) offline analysis [ 9,12 ]. The same three components are needed for a mobile A/B test. Therefore, we follow a similar structure for our discussion in this section. W e focus on highlight ing the key differences between app an d web testing , while leaving the similarities for readers to follow up in other papers [ 10, 11 ] . Experimental design is arguably the most important step in the testing workflow to get good and meaningful results. As Sir R. A. Fisher put it [13]  X  X o consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of. X  To be more concrete, design is usually the phase when we determine what we want to ex periment on, the goal, the targeted population, the traffic split among variants, and the duration of the experiment. As described in Section 3, because of the constraint with the mobile app release process (new code cannot be shipped to end users easily) , A/B testing on any client -side changes needs to be  X  X lanned ahead X . In other words, all experiments, including all the variants for each of these experiments, need to be coded and shipped with the current app build . Any new variants, including bug fixes on an y existing variant, have to wait for the next app release. This has the following two implications: (1) A/B testing has becom e more extensive. Whenever possible, new feature s are always rolled out under A/B test s . One essential function of A/B test s is risk minimization. It allows us to roll out a new feature to a small, randomized user group to evaluate. More importantly, in a mobile world, if a feature doesn X  X  perform as we intended, we can instantly revert back to an equivalent of the older app ve rsion by shutting down the traffic to the test , without having to go through a client release cycle (which can take weeks for slow ly adopting users). The benefit of preventing our end users from being stuck with a faulty app for weeks really helps promote a truly  X  X est everything X  culture among our developers. (2) P arameterization is used extensively to allow flexibility in creating new variants without a client release. This is because even though new code cannot be pushed to the client easily, new configu rations can be passed in payload, which effectively creates a new variant as long as the client understands how to parse the configurations. For instance, we want to experiment on the number of feed items to fetch from the backend at a time. We can put our best guesses in the code and experiment only with what we have planned in place, or we can parameterize the number and have the freedom to experiment on any numbers after the release . Real time targeting is more prevalent in mobile experiments, as the three most widely used targeting attributes (platform, operating systems and app version) are only available at runtime. Many features are unique to a particular platform and OS. Moreover, the same feature usually performs very differently between tablet and phone, a nd between iPhone and Android. Therefore, when designing a mobile experiment, we usually need to consider targeting a specific platform and OS combination. In addition, many experiments only exist in certain app versions. Obviously, newly added experiments do not apply to older app versions. At the same time, some experiments may only exist in older versions. It is important to target the correct version when rolling out an experiment. After design is complete, deploying an experiment in web set ting usually just involves two components, the application layer that implements the alternative variant behavior according to experiment assignment , and the service layer that is a distributed cache and experiment definition provider [ 12 ]. In the mobile s etting , the application layer is logically broken into two parts: 1. Server side: It communicates with the servi ce layer, pass es 2. Mobile client side: It p eriodically fetch es the experiment As we mentioned earlier, the experiment implementation on the mobile client side needs to be shipped with the new app version. Once that is done , we can activate the experiment to enable a small percentage of traffic on treatment. Similar to web applications, the activation information propagates from the service layer to the application server every 5 minutes. However, this does not mean the exper iment is fully deployed. As a matter of fact, the deployment is usually delayed on the mobile client by at least one user session. This is because new assignment information is usually only fetched at the beginning of each session. In addition, these new a ssignment usually does not take effect till the next session. This is because we do not want to change a user X  X  experience after they have started the session already . For heavy users who visit multiple sessions a day, such delay is small, but for light us ers visiting once a week, the experiment does not actually take place till a week later. Such delay and how it depends on user X  X  visitation frequency imply that not only does the signal appear to be weaker at the beginning of the experiment, it is also mor e biased towards reactions from heavy users. Moreover , to reduce the number of communications with the server, assignment information is fetched for all active experiments at once and then cached on the mobile client. This is the case regardless of whethe r an experiment is actually triggered or not. For example, there is an experiment that only affects users who visit the  X  X e X  tab. However, the assignment for that experiment will still be fetched even if a user doesn X  X  visit  X  X e X  during that session. Becau se experiment information is tracked on the server side, s uch  X  X ver fetching  X  means we are marking more users as affected by the experiment than there actually are , effectively diluting the signal when we want to evaluate the  X  X eal X  impact. One potential f ix t o this problem is to only track experiment event after an experiment assignment is actually used, not when it is fetched. By and large, the analysis of mobile app A/B tests is done similarly as web experiments. As mentioned in Section 4.1, experiments can be ramped differently for different platforms and operating systems. Even experiments that are ramped uniformly across the population, the performance in these segments can be drastically different. In many experiments we have conducted, it is not unusual to see a positive lift on one platform and a negative impact on the other. It is therefore important to always drill down to examine the per -segment impact in addition to the overall site -wide impact.
 Besi des the dilution and the delay effect discussed in Section 4.2, one thing to watch out for is the potential interactions between different user interfaces. Many of our membe rs access LinkedIn through desktop, mobile app and mobile web. While it is unusual to drive traffic between desktop and mobile in a session or a day (we don X  X  expect to change user X  X  mobile usage drastically in a short period of time), it is, however, comm on to shift traffic between mobile app and mobile web. For example, many mobile users visit LinkedIn via clicking on their emails. The email link can either take the user directly to the app (assuming he or she has the app installed), or to the mobile web. When analyzing an experiment, it is important to know whether an experiment may cause or suffer from such an interaction, in which case, we cannot evaluate app performance in isolation, but need to look at user behavior holistically on both mobile app and mobile web. In addition, because user experience on the app tends to be better, directing traffic from the app to the web tends to bring down the total engagement, which may be a confounding effect not intended by the experiment itself. We have so far highlighted the special challenges when conducting randomized experiments on mobile app compared with web testing. Most of the differences result f rom the fact that mobile app release cycle involves not just the app owner, but also a pp store s and end users . On the other hand, t he exact same differences also create an opportunity to evaluate changes that we are not able to A/B test on . These tend to be big changes that involve fundament al infrastructure improvements and holistic product redesign. One such example is LinkedIn X  X  recent release of the new flagship app (Project Voyager [3] ) .
 To be more concrete, the opportunity to evaluate such big releases exists because of the following: First, users take time to adopt the new app . The adoption period creates a natural experiment. Second, Google supports staged roll out of an app. Even though neither enables a fully randomized experiment, we can leverage both to measure the performance of a new app version through a quasi -experime nt al study. As we will lay out in this s ection, such quasi -experiment, coupled with thorough validation, is able to estimate the impact from a new app release with little bias , as demonstrated by results from our recent Voyager launch. This section is org anized as follows. We start in Section 5.1 with a deep dive into how mobile app adoption works, who the adopters are and how the adopter group evolves over time. This is crucial in helping us come up with quasi -experimental techniques to adjust for adoptio n bias. Section 5.2 reviews the basic quasi -experiment methods in the context of evaluating app release. This is not meant to be a comprehensive literature review, but to establish the notation and building blocks for the next section. In the final Section 5.3, we go into depth discussing and comparing the techniques we use to evaluate the Voyager release. Separate methods are needed for iOS and Android, as the Android release was a 20% staged roll out during the evaluation period. Every new app version takes a while to propagate through the user population, as it requires upgrade from the end users. Both Android and iOS give users a lot of flexibility in managing how they want to upgrade. By and large, there are three options: (1) automa tically update (2) automatically update only if there is Wi -Fi access (3) manually update. Obviously the choice affects the probability to adopt. The default setting of both operating systems is option (2), with the difference that Android notifies users a fter the update completes, but iOS updates quietly in background.
 The choice of the update options and the ease of access to Wi -Fi are the two most important variables that impact the likelihood of adoption over time. To add to the complexity , we have no visibility into whether a user has updated to the new version until he or she visits the app. One can think of modeling such mechanisms and relationships with a Latent Factor Model (with missing data) to reveal and understand these underlying variables [ 30 ]. Since our focus is to evaluate the app performance by adjusting for the intrinsic adoption bias, we instead look to understand who these adopters are and what the key differences are between the adopters and non -adopters. Because our member populations are intrinsically different across OS, we observe very different adoption curves between Android an d iPhone (Figure 1 , Left ). Adoption is much faster on iPhone, especially during the first couple of days. As we can see in Figure 1, the adoption rate that takes Android a week to reach takes only about three days on iPhone. I n general , adoption tends to slow down after a week on both OS . There are some people who stays on the older version for weeks or months before they adopt, and they te nd to be the same group of users for each release, likely those who select manual update option on their device. Note that even though t he p lot only uses data from the recent major release with significant PR pushes , almost exact same plots are observed from other minor historical releases, indicating little social effects on adoption of newer app versions. From the adoption mechanism described above, it is not surprising to see that adopters tend to consistently adopt through mu ltiple historical ap p releases. As shown in Figure 1 (Right) , a previous adopter is in general 2  X  3 times more likely to adopt again than a previous non -adopter. Note that the risk ratio tends to be smaller in the earlier days. This indicates that histori cal adoption pattern is less predictive of future adoption shortly after launch. This can be explained by the fact that whether a user adopts or not is largely driven by his or her choice of auto -update options and access to Wi -Fi. Conditional on the same user, we can treat the Wi -Fi availability as a random process independent of users X  visit to LinkedIn. Therefore, whether a user adopts shortly after release has a large random component and is hard to predict based on historical adoptions.
 Among the adopters, there is a big difference between those who adopt on day one and those who adopt on day 14. We group the adopters into cohort 1 through 14 depending on the day they adopt. As show in the heatmap (Figure 2 ), it is clear that users who adopt earlier are more engaged than the users who adopt later . Also note that even though the diagonal has higher values , it is not an indication that users are more engaged on adoption day . It is an artifact that we don X  X  know users X  adoption status until they visit the app, and hence by definition users are active on their adoption day.
 Now let X  X  turn to adopter and non -adopter differences. Note that both adopters and non -adopters are ac tive during the analysis period, and their adoption labels can change over time as more people adopt. Figure 3 (Left) plots the difference as percentage delta of a key engagement metric bet ween adopters and non -adopters. The data used is from a minor historical release that is not supposed to create any actual impact on metrics. Interesting ly enough, when the analysis period is a single day, the difference starts at over 50%, then drastically decreases to below 10% after a week. However, if we look at cumulative differen ces cross day, the percentage delta stays at above 30% throughout the entire week! Similar trend is observed for almost all metrics we looked at . It turns out adopters tend to be active on more days. So if we accumulate their activities over time, the adop tion bias is bigger. This can be explained by the simple equation below , where the cumulative metric can be decomposed as users X  engagement on an active day with a multiplier of total active days . Indeed, by day seven , an average adopter is active on 62 % more days, explaining the majority of the difference in the cumulative engagement. There are two implications. First, without modeling for adoption bias, we can ef fectively mitigate it by comparing adopters and non -adopters in a smaller time window . However it assumes the number of visit days is not impacted by the treatment, which should be tested separately. Second, as shown in Figure 3 (Right), if we control for the days users are active, the difference between adopters and n on -adopte rs become much smaller (from ove r 50% to less than 20% for day one ), indicating historical active days is a strong covariate for adjusting adoption bias.
 It is worth noting that some users take more than two weeks to adopt, but they are as active as the adopters. These are likely users with auto -update off. It seems that users X  choice of auto -update option is fairly independent of their engagement with Link edIn. As we have already seen in Section 5.1, there are big differences between adopters and non -adopters. Simply comparing those who are on the new app with those on the old app will give a misleading estimate of treatment ef fect. In this section, w e evaluate common statistical techniques to reduce adoption bias; in particular, methods based on OLS regression and propensity score model s . Assume our metric of interest follows a linear model : w here  X  = (  X   X  , ...  X   X  )  X  is the outcome vector for users  X  = (  X   X  , ...  X   X  )  X  is the dichotomo us variable of treatment, and  X  = (  X   X  , ...  X   X  )  X  is a matrix representing all the covariate s that correlates with both  X  and  X  (e.g . historical engagement metrics ). Under this model, the coefficient  X   X  is the ATE u nder the Rubin Causal framework. If the covariates  X  are omitted , the OLS estimator of  X   X  can be biased , and w here  X  = (  X  ,  X  ) . Note that t he bias is zero only if X is independent of D (so  X   X   X  = 0 , which is true for randomized experiments ) or that X does not predict Y (so  X   X  = 0 ).
 Practically such bias gets smaller if major covariates that confound the treatment assignment (  X  ) and outcome variable (  X  ) are included in the regression model. However, it is not possible to prove that all non -ignorable covariates are included . In our applications, we are able to evaluate the models based on a validation framework to be discussed in Section 5.3.
 There are some potential improvements on the OLS model : 1. It is unlikely that the relationship between the response 2. W e can generalize the treatment model (1) to an endogenous where two equations are fitted separately using data from adopters and non -adopters respectively.
 Interestingly, looking at Equation (3) alone, the problem becomes a classical missing value problem where non -adopters are clearly missing at  X  X on -rand om X . Therefore, we can also apply many techniques that address missing value problems to estimate Equation (3) and (4) separately. The method we considered in Section 5.3 is the one proposed by Heckman [ 19 ], which models the selection process based on probit regression and assumes that the error term in the probit model and the linear regression are bivariate normally distributed. Another family of approaches that have been developed to correct for selection bias is throug h the propensity score, i.e. the probability of receiving treatment. The propensity score is usually estimated through a logistic regression , defined as The score can be used for 1. M atching or subclassification . The intuition is that samples 2. W eighting . With the Rubin -causal counterfactual As we have mentioned earlier, LinkedIn has recently redesigned and rebuilt our flagship app from scratch [3]. The new app, code name  X  Voyager  X  , should be conside rably more intuitive and useful. One key challenge we faced was how to evaluate whether users indeed are benefiting from using the new app. If we were able to A/B test the app, it would have been an easy question to answer. However, with the drastic changes on both backend and frontend infrastructure, it is impossible to test all the changes in a randomized, controlled setting. The closest to A/B te st we can get is to leverage Google Play store X  X  staged roll out functionality. But even with the staged roll out, we s til l have to face adoption bias, and therefore cannot analyze the res ults as a simple A/B comparison.
 As we mentioned earlier, the bigg est challenge of any quasi -experimental method is how to validate that the causal relationship is indeed true and not due to some omitted variables in the model. Fortunately, we are able to have a validation framework based on historical data. Historically, we release new app versio ns to app stores periodically. These regular releases usually ship no more than minor bug fixes, which create a unique opportunity to (1) understand the adoption behavior (section 5.1) and consequently utilize it to build better quasi experiment models; ( 2) evaluate the validity of a model. These historical releases essentially serve as A/A tests, as it is reasonable to assume that such incremental releases only cause minor impact and the true average treatment effect (ATE) is close to 0. In addition, we h ave also observed that those validation results hold consistently across multiple historical versions. Unless specified, all results discussed from here on are cumulative up till a specific day. Even though single -da y results tend to suffer less from adop tion bias (as seen in Section 5.1), it makes the assumption that the new app does not make users visit more (or less) days. With a release as big as Voyager, it is not a plausible assumption.
 Because of the differences in how we roll out the app in iOS an d in Android, we have to use different quasi A/B methodologies to evaluate iOS and Android separately, which we share in sections below. Unlike in Android, there is no opportunity to release an app version to a randomized percent of users in iOS. This makes the adoption process fully observational . Evaluating Voyager in iOS becomes a  X  X lassical X  quasi -experiment problem and existing techniques discussed in Section 5.2 should apply directly with only minor improvements. In this section, w e compute the b ias of each method during the week after release (in terms of percentage delta) for a key engagement metric. To make the comparison fair, the same set of covariates are used across methods. All results are based on a minor historical iOS release (an A/A re lease).
 Before we get to the comparison results, there are a couple of key learning from building the quasi -experiment models .
 First of all, many non -adopters become adopters after a couple of days. As we have observed in Section 5.1, whether a user adopts shortly after release is somewhat random due to Wi -Fi availability . Such random perturbation creates samples that are very similar in terms of covariates, but with opposite adoption status. These samples confuse the propensity model a nd make it harder to predict , as we can see in Figure 4 (Left) . The baseline AUC is only at 0.65 in the first day compared with 0.82 on day seven . This observation leads us to remove from the non -adopter group those users who in fact are very likely to ad opt based on historical adoptions. By removing these  X  X andom X  non -adopters in the early days, we are able to get an AUC score of close to 0.8 even on day one (green curve). Any improvement for the early days is substantially more valuable in the production environment, as it really helps identify problems early and iterate fast. Second, it is crucial to include the right features . We h ave explained the rationale on the choice of some features based on users X  adoption behavior in Section 5.1. As directly illustrated in Figure 4 (left) , removing one covariate ( e.g. OS Version) can decrease the AUC substantially.
 As shown in Figure 4 (Righ t) , the r aw comparisons are extremely biased , especially in the first days afte r launch. The amount of adoption bias reduces from about 65% to 40% after a week . The various methods have similar performance over time , with the endogenous OLS model having a slightly smaller bias. Overall, these methods are able to largely reduce ado ption bias to about 18% on day one to about 7% on day seven . Play store makes it easy for app developers to do a staged roll out [ 2 ]. We can choose to make the new version available to only a percentage of users. This applies to both existing app users and new users downloading from the Play store. It is a great way to minimize risk and collect feedback from real users. It may not be intuitive at first, but even though thes e users are randomly selected, we are not able to measure the staged roll out as a regular A/B test. The reason is because from our own tracking, we can only tell whether a user visits with a new app or an old app, but not their actual randomized experimen t assignm ent. As demonstrated in Fi gure 5 , among those visiting with an old app, we cannot differentiate those who are in the control bucket (entire B group) from those who are non -adopt ers in the  X  X ligible X  bucket (  X   X  group). If we naively compare member s on the new app with members on the old app, we would be comparing  X   X  with the rest, and hence suffer from the same adoption bias (or self -selection bias) as a full roll out.
 The problem, however, is a little different. In a full roll out, other than the minor random perturbation to the label during the first couple of days (due to Wi -Fi access) , the adopters are intrinsically different from non -adopters. Most quasi -experiment techniques described in Section 5.2 try to remove such intrinsic difference by modeling directly on adoption status. For example, propensity score models build a logistic reg ression using adoption status as response. When the actual adoption status is determined by a combination of randomization and user intrinsic behavior, such models work poorly. This is because the signal that comes with the status is weak. T he  X   X  users wh o were non -adopters can become adopters with simply a reshuffle, which obviously cannot be modeled by any user variables. At the same time, there is a bright side to the unique situation. In a 20% roll out, for every adopter we expect there to be four non -adopters who are similar to him. If we have a methodology that can identify those  X  X ould -be X  adopters, we are all set. To successfully reduce adoption bias, such method needs to have an extremely low false positive rate, even if that leads to more false n egatives. We illust rate the idea in the following.
 Assume we have a selection criteria  X  to pick out potential adopters. Apply such criteria to the two groups of users observed (  X   X  vs.  X   X  +  X   X  +  X   X  ). With high false positives and zero false negatives, we have  X  (  X   X  ) =  X   X  , and Since  X   X  and  X   X  are the two comparable user groups, such s election criteria create bias. T he higher the false positive rate is , the worse the bias gets. On the other hand, if we have only f alse negatives and no false positives , we get  X  (  X   X  )  X   X   X  Since  X   X  and  X   X  are comparable,  X  (  X   X  ) and  X  (  X  as well. As long as the false ne gative rate is not too high and  X  (  X   X  ) is still a representative subset of  X   X  ( i.e. out -of -sample bias is small), the comparison between  X  (  X   X  ) and  X  (  X   X  In Section 5.3. 2. 1 and 5.3.2 .2 below, we propose two methods that are guided by su ch intuition. The idea is to separate the  X  X ould -be X  adopters (  X   X  ) from the rest of the non -adopters by directly modeling their adoption probabilities. By selecting only users with high propensity to adopt in both adopter and non -adopter groups, we hope to achieve high precision in identifying comparable users . Note that different from the usual propensity score modeling, we only use historical data to model the adoption probabilities. The actual a doption status itself is only used in the final step as treatment assignment label. As we discussed in Section 5.1, the adoption probability increases as time goes by. We therefore need to model  X   X  X  for member  X  to adopt on day t . As described in section 5.1, we do not know whether a member has adopted the new app version until he or she visits our app. To simplify the problem , we assume that every member  X  has a chance of  X   X  to show up as an adopter each day they visit, and that th e probability of adopting on day  X  follows a geometric distribution where  X   X  X  is the number of active days a user has had by day t . Based on data from historical adoptions, w e can compute the maximum likelihood estimator (M LE) for  X   X  to be where  X   X  X  X  is the number of active days user  X  had before adopting historical app version  X  , with  X  = 1 , 2 ...  X  , and  X  function of whether user i adopted version j . Note that in our application, we capped  X   X  X  X  at 14 days, so if user  X  did not adopt by day 14 for version  X  , we have  X   X  X  X  = 14 and  X   X  X  X  = 0 .
 Finally, on each day  X  after Voyager release, we compute the probability that user  X  adopts by day  X  to be 1  X  ( 1  X   X   X  which is the cumulative probability based on the actual number of active days we observe . To get the treatment effect, we only select users with 1  X  ( 1  X   X   X   X  )  X   X  X  X  &gt; threshold. The threshold is chosen based on A/A cross -validation results.
 The results (Figure 6 , Left) show that such method is able to correct for almost all the selection bias towards the end of the adoption week . However, regardless of how high a thr eshold we pick, the precision in the first few days is poor . This echoes our analysis in Section 5.1 where we observed that adoption in the first days is hard to predict .
 As we mentioned early, propensity score models do not work well in the randomized setting. However, because there is supposed to be more users in the non -adopter group who are similar to the adopters, matching directly on covariates themselves is supposed There are various methods that perform matching based on covariates, and they all trace back to two that are the most fundamental: 1. Exact matching : This is the technique that matches eac h 2. Nearest neighbor matching : This method selects the non -Neither the local nor the global nearest ne ighbor approaches can be easily computed in a parallelizable fashion, which makes scaling it to millions of users a hard problem of its own. In addition, on a smaller test data set we tried, the nearest neighbor technique tends to under perform, and highly affected b y the distance measure chosen, particularly because we have many categorical covariates. Exact matching, on the other hand, does not work well when there are many covariates (or when some covariates can take many values, such as continuous varia bles), as it becomes impossible to find sufficient exact matches. Continuous or ordinal variables have an additional challenge in exact matching because of the lack of distance measures. A difference of 1 pageview is treated the same as a difference of 100 0 pageviews, which is clearly suboptimal. Because of these challenges, we have decided to go with a  X  X oubly Robust X  approach [ 18 ], where we fit an exact matching model first and then fits a linear regression model on the matched user sets. The exact match ing takes in only about 10 important covariates with the continuous variables carefully bucketized to ensure sufficient matched samples. The regression model has a lot more covariates, including several continuous variables, which can compensate for the so mewhat coarse matching and offers more granularity into the covariates. Because of its good perfo rmance in validation from first day after release, this is the approach we used in production.
 Here are the steps we took for the Doubly Robust estimation: 1. Onl y variables that cannot be impacted by the treatment itself 2. For all the variables, ensure common support i n both adopter 3. Select a small set of variables (about 10) to be used for exact 4. Covariates used in e xact matching are bucketized to reduce 5. Feed the matched sam ples into a weighted linear regression  X  X  X  X  = where  X   X  is the weight from matching and  X   X  is the observed value . WLOG we assume users  X  = 1 , ... ,  X  are adopters.
 We have noticed that it is important to have sufficient number of samples that are matched . During the first day post launch, as the number of adopters is still relatively small (especially in 20% roll out) , the set of variables we include in the matching step is much smaller. As a result, we matched about 60% of ad opters on day one , while that n umber reduces to 20% after day one . On the other hand, the matched no n -adopters went from about 5% to 10%. The median number of non -adopters matched per adopter is 3.
 The validation results look very positive . As shown in Figure 6 (Right) , f or the two key metrics we tracked, we had only about 3  X  4 % bias on the first day , compared with an over 70% bias in the raw comparison shown in the left plot . T he bias reduces to less than 1% after day three . For big releases that involve drastically different user experience, we usually expect strong novelty effect, as users tend to explore the new experience more at the beginning. One obvious question when it comes to evaluating the performance of a new app version is: is there novelty effect and if so, how long does it take for it to go away? I n a randomized A/B te st, a simple and practical approach is to check whether the daily treatment effect, measured as the percentage delta between treatment and control , dies down as days go by. With adoption bias, novelty effect is confounded by the fact that people who adopt earl ier are simply different from people who adopt later. The quasi -A/B framework in Section 5.3. 2. 2 provides us with a way to separate out the inconsistent adoption bias and the novelty effect.
 We start with identifying two cohorts of users: the adopter cohort and the non -adopter cohort. Obviously, if we construct the cohort s on day one , we will have some of the non -adopters converted to be adopters during the period of evaluation . Because this is a retrospective study, we can overcome this problem by removing users who ended up adopting during this period from the non -adopter cohort (we can safely assume there is no conversion the other way around, going from adopter to non -adopte r). As a second step, we match the cohorts using the same exact matching method as outlined in Section 5.3.2. 2. This gives us two matched cohorts with weights. Finally, we remove further bias by applying a weighted linear regression on the matched cohorts as in the doubly robust approach. It is important to note that even though the matched co horts are constructed once, the evaluation is done on the same cohorts daily, regardless whether a cohort user is active in th at particular day . As shown in Figure 7, the treatment effect (measured as percent delta ) is much larger in the first couple of days and then settles down to a smaller , consistent value, providing compelling evidence that the novelty effect lasted for just a couple of days. In this paper we discussed many differences when experimenting on mobile compared with on desktop. We also proposed and established a quasi A/B testing framework to evaluate mobile app release. Many insights around how users adopt a new app version were shared and discussed. One very interesting problem that we didn X  X  cover is to understand the latent factors underlying users X  adoption behavior (Section 5.1), which should help further i mprove the quasi models. Moreover, notice how the quasi models have a lot less bias for Android (Figure 6 ) than they have for iOS (Figure 4 ). It is because Android was a randomized 20% roll out. If Apple App Store also supports a staged roll out feature, it will greatly benefit not only the app developers, but also app users, as they will get apps that are better evaluated and optimized. The authors wish to thank Bryan Ng, Aarthi Jayaram , Yael Garten and Kiran Prasad for insightful discus sions , Romer Rosales and Eytan Bakshy for feedback. We would also like to thank all members of the experimentation team , especially Weitao Duan who was involved in the quasi A/B effort . Finally this work wouldn X  X  have been possible without the guidance and support of Igor Perisic. [1] Sarah Perez [Online] http://techcrunch.com/2015/03/17/app -[2] Google Play [Online] [3] Jordan Novet " L inkedIn shows off Project Voyager, its new [4] Fisher, Ronald A. Presidential Address. Sankhya: The Indian [5] The 2015 U.S. Mobile App Report [Online] [6] Yates, Frank, Sir Ronald Fisher and the Design of [7] Box, George EP, J. Stuart Hunter, and William Gordon [8] Gerber, A. S., and Green, D. P. Field Experiments: Design, [9] Tang, Diane, et al. Overlapping Experimen t Infrastructure: [10] Kohavi, Ron, et al. Online Controlled Experiments at Large [11] Bakshy, Eytan, Eck les, Dean and Bernstein, Michael S. [12] Xu, Ya, et al. "From Infrastructure to Culture: A/B Testing [13] Kohavi, Ron, et al. Trustworthy online controlled [14] Kohavi, Ron, et al. Seven Rules of Thumb for Web Site [15] Kohavi, Ron. et al. Controlled experiments on the web: [16] Imbens, Guido M., and Jeffrey M. Wooldridge. Recent [17] Hill, Daniel N., et al. "Measuring Causal Impact of Online [18] Funk, Michele Jonsson, et al. "Doubly robust estimation of [19] Heckman, James J. "Sample selection bias as a specification [20] Heckman, James J., and Richard Robb. "Alternative [21] Rosenbaum, Paul R., and Donald B. Rubin. "The central role [22] Holland, Paul W. "Statistics and causal inference." Journal of [23] Rubin, Donald B. "Estimating causal effects of treatments in [24] Kennedy, Peter. A guide to econometrics . MIT press, 2003. [25] Greene, William H. Econometric analys is . Pearson [26] Harris, Anthony D., et al. "The use and interpretation of [27] Apptimize [Online] http://apptimize.com/ [28] Optimizely [Online] http://optimizely.com/ [29] Mixpanel [Online] https://mixpanel.com/ [30] Agarwal, Deepak, and Bee -Chung Chen. "Regression -based [31] Box, George EP, and David R . Cox. "An analysis of 
