 engines of IT industry. For a large commercial system, execution anomalies , including erroneous behavior or unexpected long response times , often result in user dissatisfaction and loss of revenue. These anomalies may be caused by hardware problems , network co m-munication congestion or software bugs in distributed system components. Most systems generate and collect logs for troubl eshooting, and developers and admini s-trators often detect anomalies by manually checking system printed logs. However, as many large scale and complex applications are deployed, manually detecting anomalies becomes very difficult and inefficient. At first, it is very time consuming to diagnose through manually examining a great amount of log messages produced by a large scale distributed system. Secondly, a single developer or system administrator may not have enough knowledge of the whole system, be cause many large enterprise systems often make use of Co m-mercial -Off -the -Shelf components (e.g. third party components). In addition , the increasing complexity of distributed systems also lowers the efficiency of manual problem diagnosis further. Th erefore, developing aut o-matic execution anomal y monitoring and detection tools becomes an essential requirement of many distr i-buted systems to ensure the Quality of Service. work flow errors -errors occur ring during the execution path s ; the other is execution low performance -the e x-ecution time takes much longer than normal cases a l-though its execution path is correct. In this paper, we present an unstructured log analysis technique that can automatically detect system anomalies using commonly available system logs. It requires neither additional sy s-tem source code instrumentation nor any runtime code profiling . The technique mainly consists of two processes: the learning process and the detection process. The goal of the learning process is to obtain models that represent the normal execution behavior of the system from those logs produced by normally co m-pleted jobs . The input data f or the learni ng process is training log files printed by different machines. At f irst , we convert the log message sequences in the log files into log key sequences. Log keys are obtained by a b-stracting log messages. Then, we a derive Finite State Automaton (FS A ) to mod el the execution path of the system. With the learned FSA s , we can identify the corresponding state sequences from training log s e-quences. Next, we count the execution time of each state transition in state sequences , and obtain a perfo r-mance measurement model through statistic al analysis . In the detection process, for newly input log sequences, we check them with those learned models to automat i-cally detect anomalies . It should be noticed that the system  X  s normal behavior may change after an upgrade . Ther efore, it is necessary to re -train the model after each system upgrade. tection is based on the cues gained from the previous normally completed jobs X  log files. We assume that e ach log message has a corresponding time stamp that indicates its generat ion time. We further assume that the logs are recoded using thread IDs or request IDs to di s-tinguish logs of different threads or work flows . Most modern operating systems (such as Wi ndow s and Linux) and platforms (such as Java and .NET) provide thread IDs. We can therefore work with sequential logs only. eral related research efforts are briefly surveyed. The log key extraction and FSA construction are introduced in section 3 and section 4. In section 5, we discuss the performance measurement model construction. After that, anomal y detection is described in section 6 . Then, experimental results are presented in section 7 . F inally, section 8 concludes the paper.
 use of execution logs are the least invasive and most applicable, because execution logs are often available during a system  X  s daily running . Therefore, ana lyzing logs for problem diagnosis has been an active research area for several decades. In this paper, we only survey the approaches that perform the analysis automatically. trace sequence as a whole , wher e a log sequence is o f-ten simply recognized as a symbol string. Dickenson et al [1] collect execution profiles from program runs, and use classification techniques to categorize the collected profiles based on some string distance metrics. T hen, an analyst examines the profiles of each class to determine whether or not the class represents an anomaly. Mirg o-rodskiy et al [2] also use string distance metrics to cat e-gorize function -level traces, and identify outlier traces or anomalies that subs tantially differ from the others. Yuan et al [4] propose a supervised classification alg o-rithm to classify system call traces based on the simila r-ity to the traces of known problems. In other literature , a quantitative feature is extracted from each log s e-quence for error detection. For example, in [3], the a u-thors preprocess the logs to extract the number of log occurrence times as a log feature, and detect anomalies using principal component analysis (PCA). T h ese kind s of algorithms can find whether the job is abnormal , while can hardly obtain the insight and accurate info r-mation about abnormal jobs .
 a series of footprints of systems  X  execution. They try to learn FSA models from the traces to model the system behavior . In the work of Cotroneo et al [ 5 ], FSA mo d-els are first derived from the traces of Java Virtual M a-chine collected by the JVMMon tool [ 6 ]. T hen , logs of unsuccessful workloads are compared with the inferred FSA models to detect anomalous log sequences. SA L-SA [ 7 ] examines Hadoop logs to construct FSA models of the Datanode module and TaskTracker module. I n [ 8 ], based on the traces that record the sequences of components traversed in a system in re sponse to a user request, the authors construct varied -length n -grams and a FSA to characterize the normal system behavior. A new trace is compared against the learned FSA to detect whether it is abnormal. In their algorithm, a varied -length n -gram represe nts a state of the FSA . Unlike these methods , which heavily depend on application specific knowledge including some predefined log t o-kens and the stage structure of Map -Reduce, our alg o-rithm can work in a black -box style. In addition, our algorithm is the only one that use s timing information in the log sequence to detect the low performance pro b-lem.
 perform troubleshooting related tasks in different scen a-rio s. GMS [ 17 ] detect s abnormal machines with wrong configuration s . It extracts features from the data source and applies the distributed HilOut algorithm to identify the outliers as the misconfigured machines . Its data source includes log files, utility statistics and configur a-tion files. In [ 18 ] , a decision tree is learned to identify the causes of detected failures where the failures have been detected beforehand . It records the run time pro p-e r ties of each request in a multi -tier Web server, and applies statistical learning techniques to identify the causes of failures. Unlike them, our algorithm mainly tries to detect anomalies through exploiting the timing and circulation informatio n .
 haviors, including events, states and inter -component interactions. A n unstructured log message often co n-tains two types of information: one type is free -form text string that is used to describe the semantic meaning of a recorded program behavior; the other type is a p a-rameter that is used to express some important system attributes. In general , the number of different log me s-sage types is often huge or even infinite because of va r-ious parameter values . Therefore, during log data mi n-ing, directly c onsidering log messages as a whole may lead to the curse of dimension .
 log message by its corresponding log key to perform analysis. The log key is defined as the common content of all log message s which are printed by the same log -print statement in the source code . In other words, a log key equals to the free -form text string of the log -print statement without any parameter s . For example, the log key of log message 5 (shown in F igure 1 ) is  X  Image file of size saved in seconds  X  . We analyz e logs based on log keys because: (1) In general cases, different log -print statements often out put different log text messages . It mean s that e ach type of log key corresponds to one specific log -print statement in the source code . Ther e-fore, a sequence of log keys can reveal the execution path of the program. (2) T he number of log key types is finite and is much less than the number of log message types . It can help us to avoid the curse of dimension during data mining .
 which log messages are printed by the same log -print statement nor wh ere parameters are in log messages . Therefore, it is very difficult to identify log keys. Ge n-erally , the log messages printed by the same statement are often highly similar to each other, while two log messages printed by different log -print statements are often quite different. Ba sed on this observation, we can use clustering techniques to group log messages printed by the same statement together, and then find their common part as the log key. mistakes because the log messages pri nted by different statements may also be similar enough if they contain a lot of identical parameter values . In order to reduce the parameters  X  influence on clustering, we first erase th e contents that are obvious parameter values ac cording to some empirical knowledge . Then, we further apply a raw log key clustering and group splitting algorithm to obtain log keys. Figure 1 gives an example to illustrate the procedure of extracting log keys from log messages. A. Erasing parameters by empirical rules bers, URIs, IP addresses ; or they follow the special symbols such as the colon or equal -sign; or they are embraced by braces , square brackets , or Parentheses . These contents can be easily ide ntified. Therefore, e m-pirical rules are often used to recognize and remove these parameters [9]. By roughly going through the log files, we can define some empirical regular expression rules to describe those typical parameter cases, and erase the matched contents. After that, the left contents of log messages are defined as raw log keys. The second block of F igure 1 gives some examples of raw log key s . We can see that the IP addresses, the numbers, and the full path of a file are all removed from the log messages.
 some parameters that could not be completely removed in raw log keys. The main reason is that the empirical rules can  X  t exhaust all parameter patterns without appl i-cation specific knowledge .
 B. R aw log key clustering as separator. We use words as primitives to represent raw log keys because words are minimal meaningful elements in a sentence. So, each raw log key can be represented as a word seque nce. represent the similarity of two raw log keys. The string edit distance is a widely used metric to represent the similarity between word sequences. It equals to the number of edit operations required to transform one word sequence to the other. One edit operation can o p-erate only one word. T he operation can be adding, d e-leting or replacing. Obviously, the edit distance only cou nts the number of operated words ; it does not co n-sider the positions of the operated words. However, for our problem, the positions of operated words in raw log keys are meaningful for measuring similarity. It is b e-cause most programmers tend to write text messages (log keys) firstly, and then add parameters afterwards. 
Therefore, words at the beginning of raw log keys have more probability to be parts of log keys than words at the end of raw log keys do. Therefore, the operated word at the beginning of the raw log keys should be more significant for measuring raw log keys  X  diffe r-ence. Based on this observation, we measure raw log keys  X  similarity by the weighted edit distance , in which we use sigmoid similar function to compute weights at different posi tions . For two raw log key s  X   X  1 and  X   X  we denote the necessary operations required to tran s-ber of necessary operations. The weighted edit distance between  X   X  1 and  X   X  2 is denoted as  X   X   X   X   X  1 ,  X   X   X   X   X   X   X  1 ,  X   X  2 = 1 of the word that is operated by the i th operation  X   X  is a parameter controlling weight function. 
F igure 2. The histogram of raw log key pair number two log keys, if the weighted edit distance between them is smaller than a threshold  X  , we connect them w ith a link. Then, each connected component corre s-pond s to a group which is called as an initial group . The initial group examples are shown in the third block in Figure 1 . according to the following procedure . For every two raw log keys, we compute the weighted edit distance between them. T hen we obtain a set of distance values . Each distance should be either inner -class distance or tance is the distance between two raw log keys corre s-ponding to the same log key ( or different log keys ) . In general, the inner -class distances are usually small while the inter -class distances are large. Therefore, we use a k -means cluster ing algorithm to cluster all di s-tances into two groups . The distances in the two groups roughly correspond to the inner -class and the inter -class distances respectively. Finally, we select the largest distance from the inner -class distance group as the va l-ue of threshold  X  . Hadoop and SILK respectively (the experiments  X  d e-tails are described in section 7). W e calculate the di s-tances between every two raw log keys , and show t he histogram of raw log key pair number over dista nce in F igure 2 . The x -coordinate is the value of the weighted edit distance . T he y -coordinate is the number of raw log key pair s . The figures show that: (1) There are two si g-nificant peaks in each histogram . It seems that the pr o-posed weighted edit distance is a good similarity metric for raw log key clustering . (2) There is a flat region between two peaks. It implies that our raw log key clu s-tering algorithm is not sensitive to the threshold  X  . C. G roup split ting respond to the same log key. In such cases, a log key can be obtained by extracting the common part of the raw log keys in the same initial group. However, raw log keys in one initial group may correspond to diffe r-ent log keys be cause those log keys are similar enough. To handle those cases, we propose a group split ting algorithm to obtain log keys .
 keys in th is group. The common word sequence of the raw log keys wit hin the group could be represented by  X   X  1 ,  X   X  1 , ... ,  X   X   X  . For example, the initial group 2 in F igure 1 contains raw log key 5, 6, 7, and t he common word sequence in the raw log keys are  X  file  X  ,  X  of  X  ,  X  size  X  ,  X  in  X  ,  X  seconds  X  .
 i th log key, the common word sequence  X   X  1 ,  X   X  2 , ... ,  X   X   X  separate s the raw log key into N +1 parts which is denoted as  X   X  1  X  ,  X   X  2  X  , ... ,  X   X   X  where  X   X   X   X  ( 2  X   X   X   X   X  1 ) is the i th raw log key  X  s content between CW j -1 and CW j ;  X   X  1  X  is the i th key  X  s content on the left side of CW 1 ;  X   X   X  + 1 raw log key  X  s content on the right side of CW N . We call  X   X   X   X  as the private content at position j of the i log key. In the above example, t h e private content s e-quence of raw log key 7 is  X  Edits  X  ,  X  ,  X  ,  X  edits # loaded  X  ,  X  ,  X  . In the paper ,  X  represents that there is not any word in the private content.  X   X  private contents at position j from  X   X  raw log keys in the group, and they are  X   X   X  1 ,  X   X   X  2 , ... ,  X   X  denote the number of different values (not including  X  ) among those  X   X  values as  X   X   X  , and  X   X   X  is called the private number at position j . For the initial group 2 in F igure 1 ,  X   X  1 =2,  X   X  2 =0,  X   X  3 = 0,  X   X  4 = 3,  X   X   X   X  6 = 0. tion j are parameters,  X   X   X  is often a large number b e-cause parameters may probably have many different values. However , if the private contents at position j are a part of log keys,  X   X   X  should be a small number. Based on this observation , we find the smallest positive one among  X   X  1 ,  X   X  2 , ... ,  X   X   X  ,  X   X   X  + 1 , e.g.  X   X  equal to or bigger than a threshold  X  , which means that the private contents at position J have at least  X  diffe r-ent values, then we consider that the private contents at position J are parameters. In such a situation, this initial group does not split anymore. Otherwis e, if  X   X   X  smaller than the threshold  X  , we consider that the pr i-vate contents at position J are a part of log keys. In such a situation, this initial group splits into  X   X   X  sub -groups, satisfying that the raw log keys in the same sub -group have the same private content at position J . In the p a-per, we set  X  as 4 according to experiments.
 value 2 and is smaller than the threshold 4, so the initial group 2 split s into 2 sub -groups according to raw log keys  X  private contents at position 1. T he raw log key 5 and 6 are in one sub -group, because they have the same private content  X  Image  X  ; the raw log key 7 is in the other sub -group.
 ent positions that have the same smallest positive value smaller than the threshold, we further compare the e n-tropies at those positions respectively , select the one position with the minimal entropy, and split the group according to the pri vate contents at that position. We d enote the entropy at position j as  X   X   X  . We compute  X   X  according to the distribution of private content values at position j . For example, for the initial group 2 and j =1, we can obtain 3 values of the private content which are  X  Image  X  ,  X  Image  X  , and  X  Edits  X  . The value  X  s distrib u-tion is p (  X  Image  X  )=2/3, p (  X  Edits  X  )=1/3, so  X   X   X  able because a smaller entropy indicates lesser divers i-ty, which means the private contents at that position have more possibility to be part s of log keys.
 same private number and the same entropy, then we split the group according to the private contents at the most left one among those positions. there is no group satisfying the split condition . Finally , we extract t he common part of raw log keys in each group as a log key.
 D. Determine log key s for new log message s from the training log messages in the training log files . W hen a new log message comes, we determine its log key according to the following two steps : First, we use the empirical rules to extract th e raw log key from the log message. Second, we select the log key which has the minimal edit distance to the raw log key of the log message. If the weighted edit distance between the raw log key and the selected log key is smaller than a thr e-shold  X  , the s elected log key is considered as the log key of the log message . Otherwise, the log message is considered as an error log message, and its log key is its raw log key. Here, we set  X  as the large st one of the weighted edit distances between all raw log keys of training log messages and their corresponding log keys. ing log key, a log message sequence can be converted into a log key sequence.
 Finite S tate Automaton ( FS A ) to model the execution behavior of each system module. Although there are some other alternate models, such as Petri -Net , we adopt FSA because it is simple but effective . FSA has been widely used in testing and debugging software applications [11]. A FSA consists of a finite number of states and transitions between the states. A set of alg o-rithms have been proposed in previous literature to learn FSA from sequential log se quences [10, 11, 12]. In this paper, we use the algorithm proposed by [11] to learn a FSA for each system component from training log key sequences which are produced by normally completed jobs . Each transition in the learned FSA s corresponds to a log key. All training log key sequences can be interpreted by the learned FSA s . Therefore , each training log key sequence can be mapped to a state s e-quence. Fig ure 3 shows the example of the learned FSM of JobTracker of Hadoop (refer to Section 7.1) . We give the state interpretations acco rding to the log message in T able 1 . From the learned the FSM, we o b-tain the following work flow: f rom S87 to S96, the J ob T racker carries out some initialization tasks when a new job is submitted. After initialization, the state m a-chine enters S197 to add a new Map/Reduce task. For each map task, it selects local or remote data source for processing. Then, the task is completed. When the last task is finished, the job is completed, and all resources of tasks are cleared iteratively . In fact, the learned FSM correctly reflects the real work flow of the JobTracker. F igure 3 . Example of a learned FSM Table 1 . The interpretations of states State Interpretation S87~ S96 S197 Add a new map/reduce task S103 Select remote data source S99 Select local data source S198 Task complete S106 Job complete S107 Clear task resource terize the performance of the normally completed jobs. By comparing with normal performance characteristic s , we can detect low performance in new jobs.
 log key sequences . The time stamp of a log key is the same as the time stamp of its corresponding log me s-sage. In order to derive a performance measurement model, we need to know application s  X  execution state s . Therefore, we first convert each log key sequence to its corresponding state sequence . A state  X  s time stamp is specified by the time stamp of its corresponding log key in the log key sequence .
 performance problems . One is that the time interval that a system component transits from a state to the ne xt state is much longer than normal cases ; we name it transition time low performance . The other is that the circulation numbers of a loop structure are far more than normal cases ; we name that loop low performance . W e use the transition time between adjacent states and the circulation numbers of all loop structures to chara c-terize the normal performance of jobs.
 A. Transition time measurement model message sequences to its local disc independently . Therefore, different training state sequences may be derived from logs in different machines. Suppose we have M machines in a distributed system. For each state transition in the FSA , e.g. from S a to S b , the time inte r-vals between two adjacent states ( S a , S b ) in the training state sequences produced by i th machine are denoted as  X  Here , K i is the total number of the time intervals in all state sequences produced by the i th machine.
 of the state transition interval. In practice, the comput a-tional capacity of machines in a distributed system is often heterogeneous . The different computing capacit y of machines results in the state transition time intervals in different machines being quite different. In order to handle this problem, we introduce a capacity parameter for each machine . Our model contains machine ind e-pendent Gauss ian distribution parameters {  X   X   X  ,  X   X  ,  X  2 (  X   X  ,  X   X  ) } and machine dependent capac i-ty parameters {  X  1  X   X  ,  X   X  ,  X  2  X   X  ,  X   X  , ... ,  X  Here, t he Gauss ian distribution  X  (  X   X   X  ,  X   X  ,  X  2  X   X  ,  X   X  ) is used to represent the di s tr i-bution of the stat e trans i tion time on an imaginary co m-puter with a standard comp u ting capacity. It is only determined by the pro p e r ty of the specified state trans i-tion, and does not d e pend on the property of any speci f-ic machine . The computer s  X  propert ies are modeled by the computer s X  computing capacity parameters  X  (  X   X  ,  X   X  ) , 1  X   X   X   X  . The co m puter s X  computing c a-pacity parameters are also ass o ciated with the state transition , because different state transitions often co r-respond to different computing tasks and t he same computer may have a different co m puting capacity u n-der different work load characteri s tics.
 specified, we abridge state indicators in expressions or formulas for simplicity. We assume that the mean value of state transition time in the i th machine is proportional to its computing capacity parameter  X   X  , and the variance is proportional to  X   X  2 . With that assumption, the o b-tained transition time instances in the i th machine s atisfy the Gauss i an distribution  X  (  X   X   X  , (  X   X   X  ) 2 ) , 1  X   X   X   X  . We further assume that the obtained transition time i n stances are independent, and then the likelihood fun c-tion is as follows. =  X  (  X   X   X  ;  X   X   X  , (  X   X   X  ) 2 )  X   X  With the variable substitutions of  X   X  =  X   X   X  and  X  =  X  we can obtain the log -likelihood function:  X   X  1 ,  X  2 , ... ,  X   X  ,  X  =  X  [ 2 ln  X   X  + ln  X  + 1 criterion , the optimal parameters should m axi mize  X   X  1 ,  X  2 , ... ,  X   X  ,  X  . Because the optimal parameters should satisfy that the partial differentiate s of  X   X  1 ,  X  2 , ... ,  X   X  ,  X  equal to 0 , we have: above equation group; we can only use an iterati ve pr o-cedure to ob tain an approximation of the optimal par a-meters. It could be prove d that after each iteration step, procedure is shown in T able 2 . When the difference of  X  in two iterations is small enough (&lt; Th  X  ) , the iterati ve procedure terminate s .
 ment model : the transition time from S a to S b in the i machine satisf ies the Gaussian distribution  X  (  X   X   X   X  ,  X   X  ,  X   X  2  X   X  ,  X   X   X   X   X  ,  X   X  ) . can be easily implemented in a parallel mode. Accor d-ing to formula (3), when given  X  ,  X   X  can be determined by the sample data in the i th machine, i.e.  X   X   X  ( 1  X   X   X   X  Th us , each  X   X  can be calculated separately at the i chine . When give n  X   X  ( 1  X   X   X   X  ), the intermediate r e-lated by machine s separately . Then, it is very easy to integrate those intermediate results to obtain  X  . Ther e-fore , our algorithm can be used to learn models from the logs of very large scale systems .
 B. Circulation numbers measurement model ningful measurement s for low performance detection because s ome executions X  low performance is caused by abnormal ly more loops although each of its adjacent state transition time s seem normal. A loop structure is defined as a directed cyclic chain composed by the state transition in the lea rned FSA . For example, for the FSA shown in F igure 4 , one loop structure is { S 2 , S other is { S 1 , S 2 , S 3 }. A loop structure execution instance is formed by consecutively repeating several rounds of a loop structure from its beginning to its end ; and the number of execution rounds is defined as a circulation number. For example, in the state sequence  X  S 0 S 1 S S 3 S 1 S 2 S 3 S 4  X  , t he subsequences, e.g.  X  S 2 S 3  X  S 2 S 3  X  , are two execution instances of the loop stru c-ture { S 2 , S 3 } in the state sequence , and the circulation numbers are 2 and 1 respectively ; the subsequence, e.g.  X  S 1 S 2 S 3 S 2 S 3 S 1 S 2 S 3  X  is a n execution instance of the loop structure { S 1 , S 2 , S 3 }, and the circulation number is 2. each loop structure , e.g. L , we find the execution i n-stances of L in all training state sequences , and record their circulation number s as  X  1  X  ,  X  2  X  , ... ,  X   X  (  X  ) (  X  ) where H ( L ) is the amount of L  X  s execution instances. Similarly, we use Gauss ian distribution  X  (  X   X  ,  X  2 (  X  ) ) to model them.  X   X  = 1 T able 2 . Iterative procedure to compute parameters tain the corresponding log key sequence according to section 3. 4 . If the log key sequence can be generated by the learned FSA , then we consider that there is no work flow error . Otherwise, the first log key in the sequence that can X  X  be generated by the learned FSA is detected as a work flow error . The details of work fl ow error detection can be found in paper [ 11 ] . In this paper , we mainly focus on the low performance detection. A. Transition time low performance detection the testing log key sequence s to the corresponding state sequence s according to the learned FSA . For each state transition in the state sequence produced by the i th chine , e.g. from S a to S b , we then compare its execution time with the learned transition time measurement model of the i th machine . If the execution time is larger
Initialization : While true
End tion time low performance. Here , t he threshold is d e-fined as the sum of the mean value and  X  times standard deviation of the learned transition time distribution.  X   X   X   X  ,  X   X  =  X   X   X   X  ,  X   X  ( 1 +  X   X  (  X   X  ,  X   X  ) ) ( 6 ) tions are detected as low performance problems . At the same time, there will be more false posi tive s and less false negative s . When applying our t echnique, users can adjust the value of  X  according to real requirement s . In the experiments, we set  X  as 3.
 B. Loop low performance detection each loop structure L , we calculate its threshold  X  (  X  ) as follows  X   X  =  X   X  +  X   X  (  X  ) ( 7 ) tion numbers are larger than  X  (  X  ) as loop low perfo r-mance.
 through detecting anomalies in two typical distributed computing systems: Hadoop and SILK (a privately owned distributed computing system) . In this section , we represent some typical cases to demonstrate our technique, a nd give out some over all evaluations on our experiment results.
 A. Case study on Hadoop mentation of Google  X  s Map -Reduce [ 14 ] framework and distributed file system (GFS)[ 15 ]. It enables distr i-buted computing of la rge scale, data -intensive and stage -based parallel applications. Hadoop is designed with master -slave architecture. NameNode is a master of the distributed file system, which manages the met a-data of all stored data chunks, while DataNodes are slaves used t o store the data chunks. JobTracker acts as a task scheduler that decomposes the job into smaller tasks and assigns the tasks to different TaskTrackers. A TaskTracker is a worker of a task instance.
 messa ge sequences in its original forms. The log me s-sages for different tasks interleave together. However, we can easily extract sequential log message sequences from logs by the task IDs. machines (from PT3 to PT17) connected with a 1G Ethernet switch . The basic configurations are listed in Table 3 . Among them, PT17 is used as a master that hosts NameNode and JobTracker components . The others are used as slaves, and each slave hosts Dat a-Node and TaskTracker components . During the exp e-riments, we run the stand -alone program (namely CPUEater) which consumes a predefined ratio of CPU so that we can better simulate a h eterogeneous env i-ronment . T able 4 shows the utility ratios (i.e. 100% -consumed CPU ratio of CPUEater) and the learned model para meters. We can see that the more powerful machine, the smaller the average transition time is. words in the test bed and collect the produced log files of these jobs as training data. The counting words job gives out the word frequency in the input text files. E ach input text file for a job is about 10G. In the testing stage, we run 30 counting words jobs to produce testing data.
 cases in Table 5 . In this case, w e manually insert a low perform ance problem by limit ing the bandwidth of m a-chine PT 9 to 1M bps when running a job , and check whether our algorithm can detect it. The result shows that our algorithm can successfully detect the low pe r-formance problem that the transition time from stat e #21 to state #1 is much larger than the normal cases (i.e. 60s &gt; 38.04s).
 B. Case stud y on SILK for large scale data intensive computing. Unlike M a-pReduce, SILK uses a Directed A cyclic G raph (DAG) framework similar to Dryad [ 16 ]. SILK is also designed based on the master -slave architecture. A Schedule r-Server component works as a master to decompose the job into smaller tasks, and then schedul e and manage the tasks. SILK produces many log files during exec u-tion . For example, it generates about 1 million log me s-sages every m inute ( depend ing on workload intensity) in a 256 -machine system. Each log message contains a process ID and a thread ID . W e can group log messages with the same process ID and thread ID into sequential log sequences . The test bed of SILK contains 7 m a-chines (1 master, 6 slaves), which is set up for daily -build testing . As our training data, w e collect the trai n-ing log files of all successful jobs during a ten -day ru n-ning in the test -bed . The test logs are generated during one month of daily -build testing. Our algorithm can detect several system execution anomalies (shown in Table 7 ). In this subsection, we give two typical exa m-ples.
 slave task (CopyDatabase) tries several times to connect to a data base , which makes a response to the master (SchedulerServer) and is largely delayed. From the log sequence of the master, our algorithm finds that the transition time from state # 424 to state # 428 is much larger than expected (see Table 6 ) . According to the learned model, the average time interval is 12.32 s, while th e time interval in this case is 42.53 s. Therefore, our algorithm detect s it as a n anomaly of transition time low performance.
 Table 6 . Case 1: Low performance transition of SILK send s a job finish message to a client , but the client never replies . This causes the master to repe a t the a t-tempt more than 20 times before giving up . C ompared with 1 in normal situations , it is detected as a loop low performance a nomaly by our algorithm .
 C. Overall results tion on Hadoop and SILK. In the experiments on H a-doop, we detect 15 types of anomalies, 2 of them being false positives (FP). In the experiments on SILK, we detect 91 types of anomalies, 22 of which are FPs. Looking into these FPs, we find that our current loop low performance detection is sensitive to different workloads . This is because the circulation numbers of some loop structures largely depend on the work load. W ith the help of user  X  s feedback, such FPs can be r e-duced by relaxing the threshold  X  . for the corresponding loop structures .
 D. Comparison of log key extraction we compare our method with the method proposed by Jiang et. a l . [9]. The comparison results are shown in Table 8 , where the numbers of real log key types are manually identified, and are used as the ground truth. For our algorithm, the number s of obtained log key types are very close to the ground truth . Furthermore, more than 95% of the log keys extracted by our method are identical with the real log keys . By comparison , our algorithm significantly outperform s the algorithm of [9]. continuously increas e s , the traditional problem of dia g-nosis approach es ; experienced developers manually check ing system logs and explor ing problems according to their knowledge becomes inefficient. Therefore, a lot of automatic log analysis techniques have been pr o-posed. However, the task is still very challeng ing b e-cause log messages are usually unstructured free -form text strings and application behaviors are often very complicated.
 for automated problem diagnosis. Our contribution s include : (1) We propose a technique to detect anom a-lies , including work flow errors and low performance, by analyzing unstructured system logs. The technique requires neither additional system instrum entation nor any application specific knowledge. (2) We propose a novel technique to extract log keys from free text me s-sages. Those log keys are the primitives in our model used to represent system behaviors. T h e limited number of log key types avoids the curse of dimension in the statistic learning procedure. (3) Model the two types of low performance. One is for modeling execution time of state transitions; the other is for modeling the circul a-tion number of loops. In the model, we take into a c-count the factors of heterogeneous environments. (4) The detection algorithm can remove false positive d e-tection of low performance caused by inputting large workload s . Experimental results on Hadoop and SILK demonstrate the power of our proposed technique.
 rameter information to conduct further analysis , pe r-form ing analysis on parallel logs that are produced by multi -thread or event based system s , visualizing the models and the anomalies detection results to giv e i n-tu i tive explanation for human operators , and designing a user -friendly interface.

