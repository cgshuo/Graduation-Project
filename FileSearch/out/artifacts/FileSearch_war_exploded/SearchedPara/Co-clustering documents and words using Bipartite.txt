
In this paper, we present a novel graph theoretic ap-proach to the problem of document-word co-clustering. In our approach, documents and words are modeled as the two vertices of a bipartite graph. We then propose Isoperimet-ric Co-clustering Algorithm (ICA) -a new method for par-titioning the document-word bipartite graph. ICA requires a simple solution to a sparse system of linear equations in-stead of the eigenvalue or SVD problem in the popular spec-tral co-clustering approach. Our extensive experiments per-formed on publicly available datasets demonstrate the ad-vantages of ICA over spectral approach in terms of the qual-ity, efficiency and stability in partitioning the document-word bipartite graph.
A well studied problem of co-clustering in data mining literature has been that of documents and words. The goal is to cluster documents based on the common words that ap-pear in them and to cluster words based on the common doc-uments that they appear in. The entire data set is typically represented by a word-document matrix B where rows of the matrix denote the words in the collection while columns represent the documents. The words are treated as features and documents are represented as a vector such that an entry B ij in the matrix signifies the relevance of word i for docu-ment j . Co-clustering of documents and words is performed by deriving sub-matrices by simultaneously clustering rows and columns of B .

Although much of the earlier research was focussed on performing document and word clustering separately, co-clustering of documents and words has been a topic of much interest in recent years because of its applications to problems arising in text, Web and multimedia documents. Dhillon et al. [1] defined co-clustering of documents and words by a pair of maps from rows to row-clusters (words) and from columns to column-clusters (documents) inducing clustered random variables. Optimal co-clustering is then derived based on the one that leads to the largest mutual information between the clustered random variables. Long et al. [2] factorize the word-document matrix B into three components, the row-coefficient matrix, the block value matrix, and the column-coefficient matrix. The coefficients denote the degrees of the rows and columns associated with their clusters and the block value matrix is an explicit and compact representation of the hidden block structure of B . In [3], a joint distribution is defined over words and doc-uments to first find word-clusters that capture most of the mutual information about the set of documents, and then find document clusters, that preserve the information about the word clusters. Mandhani et al. [4] proposed a two-step partitional-agglomerative algorithm to hierarchically co-cluster documents and words. The partitioning step in-volves the identification of sub-matrices so that the respec-tive row sets partition the row set (i.e. documents) of the original matrix. These sub-matrices form the leaf nodes of the hierarchy subsequently created in the agglomerative step.

In this paper, we model document-word co-clustering as a bipartite graph partitioning problem. Although simi-lar approach has been adopted before by others, the main contribution of this work lies in a new algorithm that we propose -I soperimetric C o-clustering A lgorithm (ICA) for partitioning the bipartite graph. The proposed methodol-ogy heuristically minimizes the ratio of the perimeter of the bipartite graph partition and the area of the partition un-der an appropriate definition of graph-theoretic area. Our work bears resemblance to the popular spectral heuristical approach for partitioning bipartite graphs -in the sense that it does not require the coordinate information of the ver-tices of the graphs and allows us to find partitions of an op-timal cardinality instead of a predefined cardinality. How-ever, the proposed algorithm requires a simple solution to a sparse system of linear equations instead of the eigenvalue or SVD problem in spectral co-clustering. Our extensive ex-periments performed on publicly available datasets demon-strate the advantages of our approach over spectral approach in terms of the quality, efficiency and stability in partition-ing the document-word bipartite graph.
In this Section, we introduce some essential background on graph theory and review related work in the literature.
An undirected homogeneous graph G = { V , E } consists of a set of vertices V= { v 1 ,v 2 , ...., v | V | } and a set of edges E= { e ij | edge between v i and v j , i, j &lt; = | V |} , where the number of vertices. In a weighted graph, each edge e has a positive weight denoted by w ( e ij ) . The weight of the edge signifies the level of association between the vertices. An edge weight of zero denotes the absence of an edge be-tween the two respective vertices. Given a vertex number-ing and the edge weights between the vertices, graphs can be represented by matrices. We begin with definitions of a few graph terminologies that play an essential role in the paper.
 DEFINITION 1 The adjacency matrix A of the graph is defined as, DEFINITION 2 The degree of a vertex v i denoted by d i is, DEFINITION 3 The degree matrix D of the graph is a di-agonal matrix such that, DEFINITION 4 The Laplacian matrix L of a graph is a symmetric matrix with one row and column for each vertex such that, Partitioning of the graph is to choose subsets of the vertex set V such that the sets share a minimal number of spanning edges while satisfying a specified cardinality constraint. DEFINITION 5 Suppose we bipartition set V into subsets V 1 and V 2 , then the corresponding graph cut is defined as, The above definition can be extended to k -partitioning of the graph. The cut in which case is defined as,
A graph partitioning algorithm assigns a set of values to each vertex in the graph. We will refer to a vector consisting of the values for each of the vertices as the indicator vector of the graph. The cutting of the graph is dividing the indica-tor vector based on the values associated with each vertex. Spectral graph partitioning uses eigenvectors of L to com-pute the indicator vector and has been one of the most pop-ular and widely applied methods. It is based on the early work of Fiedler [5] who associated  X  2 , the second smallest eigenvalue of L with connectivity of graph and suggested partitioning by dividing vertices according to their value in the corresponding eigenvector u 2 = { u 1 ,u 2 , ..., u Consequently,  X  2 is referred to as the Fiedler value and u as the Fiedler vector .A splitting value s partitions the ver-tices of the graph into the set of vertices i such that u and the set of vertices such that u i  X  s . Shi and Malik applied spectral graph partitioning to image segmentation in [6] by using the eigenvector corresponding to the sec-ond smallest eigenvalue of the generalized eigenvalue prob-lem [7],
An undirected bipartite graph G = { D , W , E } , has two sets of vertices, viz., D and W and a set of graph edges E . Let B be an m by n graph weight matrix, where n and m represent the number of vertices in D and W , respectively. An entry B ij in this matrix is the weight of an edge appear-ing between a vertex w i  X  W and a vertex d j  X  D . There are no edges between vertices of the same group. Then, the adjacency matrix A of the bipartite graph is expressed as, where the first m vertices index W and the last n index D . A degree vector d of the bipartite graph is, where d W and d D are vectors consisting of degrees of W and D vertices, respectively. The degree matrix D and Laplacian L are, where D W ( i, i )=
If we let W be the set of words and D be the set of doc-uments then we can represent the documents and words by the two vertices of the weighted bipartite graph. Co-clustering is then achieved by partitioning the bipartite graph. In Figure 1, we show the bipartite graph partitioned using dotted lines. In order to compute these partitions, we also need to solve a generalized eigenvalue problem as in Equation (7). However, due to the bipartite nature of the problem, the eigenvalue problem reduces to a much ef-ficient Singular Value Decomposition (SVD) [7] problem and has been widely applied in many co-clustering papers. Dhillon [8] and Zha et al., [9] employed this Spectral-SVD approach to partition a bipartite graph of documents and words. In [10], the two types of vertices of bipartite graph are used to represent sentences of documents of two dif-ferent languages. The Spectral-SVD method is then ap-plied to identify subgraphs of the weighted bipartite graph which can be considered as corresponding to sentences that correlate well in textual contents. This algorithm has also found application in co-clustering multimedia documents (images/videos) and visual keywords (features). In [11], it has been used to co-cluster a bipartite graph of user rele-vance feedback logs and low-level image features. Wu et al. [12] use the same algorithm on a bipartite graph where news stories represent one type of nodes while features (textual and visual) extracted from video keyframes represent the other. In the next section, we derive the proposed algorithm for co-clustering documents and words using weighted bi-partite graphs and show that our algorithm requires a simple solution to a sparse system of linear equations to partition the bipartite graph.
In the co-clustering of documents and words (Fig 1), clustering of documents induces clustering of words and vice-versa. Let us denote the document clus-ters as D 1 ,D 2 , ..., D k and the clusters of words with W 1 ,W 2 , ..., W k . The basic premise of our algorithm is that, if w i belongs to a cluster, say W p , where 1  X  p  X  k , then its association with D p is greater than its association with any other D cluster. From a graph theoretic point of view, the association between w i and the D clusters can be expressed in terms of the sum of the edge weights. Thus,
W p = { w i : where the matrix B is as defined in Equation (8). Similarly, for d j belonging to cluster D p , the following should hold,
The algorithm presented here has been motivated from the combinatorial formulation of the classic isoperimetric problem [13 X 16]: For a fixed area, find the shape with min-imum perimeter . We present a polynomial time heuristic for the NP-hard problem of finding a region with minimum perimeter for a fixed area.
 DEFINITION 6 The isoperimetric constant  X  of a contin-uous manifold is defined as [17], where F is a region in the manifold, Vol F is the volume of F , | F | is the area of the boundary of F , and  X  is the infimum of the ratio over all possible regions F in the man-ifold. Also, for a compact manifold, Vol F  X  1 2 Vol Total and for a noncompact manifold Vol F &lt;  X  .
 DEFINITION 7 The isoperimetric number  X  G for a bipar-tite graph G = { D, W, E } is defined as [15], where F is a subset of the set of vertices { D W } of the graph and
Since, the number of documents and words are finite, the bipartite graph has finite number of vertices. Hence, the infimum in Equation (15) becomes a minimum. The boundary F of the set F can be expressed as, F = { e ij | edges between a vertex in F and its complement F C }
Since the bipartite graph of documents and words is weighted, we define,
The combinatorial volume [13, 14] can be defined in the following two ways: or, Vol Equation (18) defines the volume in terms of the number of vertices in the bipartite subset F while Equation (19) de-fines it in terms of the sum of the edge weights incident on each of the vertices enclosed in F . To co-cluster documents and words, it is more appropriate to represent the volume in terms of the sum of the vertex degrees as it utilizes the information from the weights of the edges instead of repre-senting the volume only in terms of the vertex cardinality which might not necessarily be informative. Consequently, for rest of the algorithm derivation we use Equation (19) to represent volume.
 DEFINITION 8 The isoperimetric ratio  X  F for the bipar-tite subset F is defined to be the ratio of boundary area of F to the volume of F .
 The isoperimetric sets for a graph G are any sets F and F
C for which  X  F =  X  ( G ) . The specification of a set satis-fying Equation (16) together with its complement is consid-ered as a partition. Partition with a low isoperimetric ratio is considered to be an optimal partition. An optimal partition consists of the isoperimetric sets themselves. Throughout the paper, the goal of our algorithm is to derive partitions with a low isoperimetric ratio. In other words, we want to maximize the volume Vol F and minimize the boundary area | F | .

We define an indicator vector x for words as follows, and an indicator vector y for documents as,
For simplicity, we combine the two indicator vectors into a single indicator z as,
Binary values assigned to first m vertices of z indicate the partitioning of words. Similarly, partitioning of docu-ments is inferred from the next n vertices in z .

From Equations (11), (17), (20), (21) and (22), we can express | F | as, Also, using Equations (9), (19) and (22), we can write Vol F as, To achieve the minimum of the ratio | F | Vol that the volume is fixed, then it is enough to minimize the numerator subject to the constraint on the denominator as, where 0 &lt;c&lt; 1 2 r T d , c is an arbitrary constant and r is a vector of all ones.

We are now in a position to write the isoperimetric num-ber of the graph as,
We denote the isoperimetric ratio associated with an in-dicator vector z as  X  ( z ) . In order to get optimal partition, we want to derive z such that it yields the minimum isoperi-metric ratio over all values of z .

For optimization, we relax the binary constraint on z (consequently, on x and y ) to take on real non-negative val-ues and define a cost function C as, where  X  is a Lagrange multiplier.

As L is positive semi-definite [18] and z T d is non-negative, C will be at minimum at its critical point. Dif-ferentiating Equation (27) with respect to z , we get,
Equating the above Equation to zero and ignoring the constants 2 and  X  since we are not concerned in getting the actual values of z but only relative values, we get,
Equation (29) is system of linear equations with m n number of equations with as many number of variables. Also, the matrix L is singular, i.e. its determinant is zero. So, this is a singular system of linear equations and hence does not have a unique solution [7].

We convert the system to non-singular system of equa-tions by removing a single vertex from the graph and assign it to be included in F . That is, its indicator value is assigned to be zero. We illustrate in Section 3.1 that it does not mat-ter whether a document or a word vertex is removed, as long as the removed vertex is densely connected to the bipartite graph. A row and column in L and a row each in z and d are removed corresponding to the removed vertex. We write the new non-singular system of linear equations as,
Solving Equation (30) for x and y results in a real val-ued solution. In order to get partitions, this solution needs to be cut using a splitting value (as explained in Section 2) to convert it to a binary vector as per Equations (20) and (21). Amongst the common methods for cutting the indica-tor vector are the median cut and the ratio cut [19]. Median cut uses the median of the indicator vector z as the split-ting value to produce equally sized partitions while ratio cut chooses one such that the resulting partitions have the lowest isoperimetric ratio. As our goal is to produce opti-mal partitions to gain document-word clusters and not nec-essarily equally sized clusters, we employ the ratio cut to get the partitions. To perform k -partitioning, we apply the algorithm recursively until the isoperimetric ratio obtained after every partition fails to meet a pre-determined thresh-old called the k -parameter. During recursion, it is checked if the partition ratio is less than the k -parameter. If so, the recursion is continued, else it is stopped.
In this Section, we discuss the vertex removal strategy employed in the algorithm to solve the system of linear equations. For a homogeneous graph, the spectral radius of L is  X  twice the maximum degree of the graph [20] sug-gesting that vertex with the maximum degree should be re-moved. However, in the document-word bipartite graph it is not clear whether we should remove the document vertex or the word vertex or is it that it does not matter which one we remove. That is, if we go with the heuristic of remov-ing the maximum degree vertex then should we calculate the maximum across both document and word vertices to-gether or should we remove a maximum degree vertex from within one of the two sets of vertices? We analyze this us-ing the dumbbell shaped graph (Figure 2a) which was dis-cussed in [17] on the relationship of the isoperimetric con-stant and the eigenvalues of the Laplacian on continuous manifolds. We constructed a bipartite dumbbell graph (Fig-ure 2b) consisting of 21 document and word vertices each with uniform weights. In the dumbbell graph, vertices in each of the two lobes are densely connected while the two lobes are sparsely connected to each other. An optimal par-titioning for this graph should result in cutting of the graph into the two lobes. In Figure 3, we show 6 cases where dif-ferent document and word vertices are removed to solve the system of linear equations and the corresponding partition-ing achieved. We removed document and word vertices that were densely connected (i.e. high degree) and vertices that were sparsely connected (like d 11 and w 11 ). We observe that it does not matter which vertex data type is removed in the algorithm as long as the vertex is densely connected in the graph (Figure 3 a, b, e, f). However, if a vertex along the ideal cut is removed, that is sparsely connected vertex to the graph (low degree), then the algorithm produces imbal-anced partitions (Figure 3 c, d). In the experiments (Section 4), we removed vertex with maximum degree, minimum de-gree and also randomly chose a vertex to be removed. Our results show that the algorithm produces optimal partitions with low isoperimetric ratio when the removed vertex is the one with maximum degree.
The main steps of ICA can be summarized as follows: 1. Given the word-document matrix B , construct d and L 2. Find the vertex with the maximum degree in the bipar-3. Solve the system of linear equations in Equation (30) 4. Bipartition z using ratio cut to get two document-word 5. If more than two partitions (clusters) are desired, i.e.
Although spectral methods in graph partitioning have been popular and successfully applied to diverse research problems, it does suffer from some significant drawbacks. In [21], families of graphs have been proposed for which spectral partitioning fails to produce the best partition. For example, the roach graphs that have an approximate shape
Figure 4. (a) Roach graph for k =5 . (b) Bipartite roach graph for k =5 with documents and words Isoperimetric ratio = 0 . 2174 . (d) Solution with ICA.
Isoperimetric ratio = 0 . 1304 . of a cockroach consist of two path graphs, each on 2 k ver-tices. The  X  X ody X  section of the graph consists of edges between the upper and lower paths while the  X  X ntennae X  section has no edges between the two path graphs. These graphs will always be sub-optimally partitioned into two symmetrical halves by the spectral method (using the me-dian cut) relative to the minimum isoperimetric ratio crite-rion. A roach graph for k =5 is shown in Figure 4a. We constructed a bipartite roach graph of document and word vertices for k =5 , shown in Figure 4b. In Figures 4c and d, we show the results for bipartitioning the bipartite graph with the Spectral-SVD algorithm and ICA, respectively. We can see that, spectral approach has undesirably partitioned the bipartite graph into the two symmetrical halves with a much higher isoperimetric ratio. These results for bipartite graph are in agreement with the ones demonstrated in [21] for homogeneous graphs. With this example, we have been able to show that our algorithm is able to perform well on a category of graphs that spectral methods are not able to partition efficiently.
 The proposed algorithm requires solution to a system of linear equations which in general is computationally ef-ficient over solving an eigenvalue problem or performing SVD as in the spectral approach [8, 9]. The Lanczos algo-rithm [7] is a popular method to efficiently compute approx-imations of eigenvalues of large symmetric matrices. How-ever, recently some concerns have been raised about this method in approximating eigenvalues [22]. While outliers in the eigenvalue spectrum are approximated well, eigen-values in the bulk of the spectrum are typically harder to approximate with this method. Also, solution to the eigen-vector problem (and consequently SVD) is less stable to mi-nor perturbations of the matrix than the solution to a system of linear equations if the desired eigenvector corresponds to an eigenvalue that is very close to other eigenvalues of the matrix [7].
Computational time of the proposed algorithm depends on solution to Equation (30). In particular, the time com-plexity is dependent on the number of non-zero entries in L , which asymptotically is O ( | E | ) . We can solve Equation (30) using either a direct method such as Gaussian elimi-nation or an iterative approach like the popular conjugate gradient method. Iterative methods have been popular due to their computational efficiency. Another advantage in fa-vor of iterative methods is that a partial answer may be ob-tained at intermediate stages of the solution by specifying a fixed number of iterations. If we adopt the conjugate gradi-ent method then the complexity of Equation (30) is O ( | E Note that, this only measures the time complexity to com-pute the indicator vector. We also need to include the time complexity to employ the ratio cut which is of the order of O ( hlogh ) where h = m + n . Factoring this in, time complexity of the proposed algorithm is O ( | E | + hlogh Further, if a constant number of recursions are performed, then the time complexity reduces to O ( hlogh ) .
In this Section, we empirically compare ICA with the popular Spectral-SVD bipartite graph partitioning approach [8, 9]. Section 4.1 discusses the evaluation method used to report the experimental results. In Section 4.2, we present the results on co-clustering documents and words using some of the publicly available datasets. We also compare the computational speed of ICA and Specral-SVD in Sec-tion 4.3.
Traditional clustering reporting technique such as a con-fusion matrix has been used earlier to present co-clustering results, specially document-word co-clustering in [1, 8, 9]. In these works, a confusion matrix was used to demon-strate document clustering while top words from each of the clusters are displayed to show word clustering. Although somewhat helpful, this method does not give a complete picture of the co-clustering achieved. This is because, as discussed in Sections 1 and 3, clustering of words induces clustering of the documents and vice-versa. The goal of co-clustering is NOT to achieve perfect clustering of one data type but to achieve the optimal co-clustering of the two data types together. So, a document confusion matrix might sig-nify an optimal clustering on the documents but does not demonstrate the optimality of the document-word cluster-ing by showing the top few words from every cluster. Due to the constraints enforced on the documents by word clus-tering, it should be clear that sub-optimal document con-fusion matrix can still result in an optimal document-word cluster. Similarly, an optimal document confusion matrix does not necessarily translate to an optimal document-word co-clustering.

The relationship between the Fiedler value of a graph and the isoperimetric constant has been demonstrated in some of the classic papers in graph theory papers such as [5, 17]. In-fact the goals of both ICA and the Spectral-SVD algorithms is to minimize the isoperimetric ratio. ICA achieves this by solving a system of linear equations while Spectral-SVD minimizes the ratio by solving the eigenvalue or the SVD problem. Hence, isoperimetric ratio can naturally be used to evaluate the goodness of co-clustering. ICA and Spectral-SVD algorithms are compared in terms of the isoperimetric ratio by employing the ratio cut to find the optimal partition (i.e. not necessarily partitions of equal size).
We have primarily utilized the dataset used in [23] Summary of this dataset is shown in Table II. Data sets oh0, oh5, oh10 and oh15 are from OHSUMED collection [24]. Data sets re0 and re1 are from Reuters-21578 text catego-rization test collection Distribution 1.0 [25]. Data set wap is from the WebACE project (WAP) [26]. Each document corresponds to a web page listed in the subject hierarchy of Yahoo! Data sets tr11, tr12, tr21, tr23, tr31, tr41 and tr45 are derived from TREC-5, TREC-6 and TREC-7 collections . We also used the Medline (1033 medical abstracts),and Cranfield (1400 aeronautical systems abstracts) dataset 3
For bipartitioning tests, we mixed some of the datasets mentioned above. Table I shows the bipartitioning datasets. These datasets were created as follows: 1. Med-Cran dataset is Medline and Cranfield datasets 2. Classes England and Heart-Valve-Prosthesis from oh0 3. Graft-Survival and Phospholipids from oh5 were 4. ArachidonicAcids-Hematocrit was derived from oh10 5. 2 classes from oh15, viz. Enzyme Activation and 6. Interest-Trade was formed by mixing Interest and
In Section 3.1, we discussed that removing the vertex with maximum degree from the bipartite graph is the best choice to solve Equation (29). We now show this empiri-cally by comparing three strategies of removing the max-imum degree, minimum degree and a random vertex from the bipartite graph. For random vertex removal, three ran-domly chosen vertices were removed and the vertex that gave the best isoperimetric ratio was reported. The re-sults on all the 6 bipartitioning datasets are shown in Ta-ble III. We have abbreviated dataset names due to space constraints. Maximum vertex removal is denoted by ICA-MaxVR, minimum by ICAMinVR, random by ICARanVR and the Spectral-SVD by SpecSVD. As can be seen, ICAM-inVR yields partitions with comparatively high isoperimet-ric ratios than the other cases. Moreover, mininum de-gree vertex tends to lie along the ideal cut for the bipar-tite graph and removing such a vertex can be disastrous as was demonstrated in the bipartite dumbbell graph example (Figure 3 c,d). ICARanVR can sometimes slightly outper-form ICAMaxVR ( Med-Cran , Eng-Heart , Interest-Trade ). However, due to the randomness associated with it, it can actually end up being ICAMinVR and perform poorly at times ( Graft-Phos, ArachidonicAcids-Hematocrit, Enzyme-Infections ). As a result, ICARanVR lacks in consistency in terms of guranteed optimal partitioning of the bipartite graph. Moreover, the difference in ratios of ICAMaxVR and ICARanVR when ICARanVR does outperform is very negligible. For the rest of the experiments, we have em-ployed ICAMaxVR and is referred to as ICA from now on.
In Figure 5a, the sparsity pattern of a typical word-document matrix ( Med-Cran in the figure) is shown. Co-clustering this dataset (i.e. bipartitioning the bipartite graph) essentially leads to re-ordering of the rows and columns such that words and documents are co-clustered together. This is denoted by the two dense sub-matrices in matrix (shown here Med-Cran ) before co-clustering (b) Med-Cran matrix after bipartitioning (c) tr21 matrix after k -partitioning Figure 5b.

We performed k-partitioning on the datasets mentioned in Table II by recursively applying ICA and SpecSVD algorithms. Since, we get an isoperimetric ratio for every bipartition, for k -partitioning comparison, we calculated the Mean Isoperimetric Ratio. These results are shown in Table IV. As is evident, ICA consistently outper-forms SpecSVD on all the datasets. Figure 5c shows the tr21 word-document matrix after co-clustering (i.e. k -partitioning with k=6).

Performance in the presence of Noise : Unlike ICA, that solves a sparse system of linear equations, the spec-tral approach requires solution to an eigenvalue or SVD problem. With reference to the discussion in Section 3.3, SpecSVD might not be stable in the presence of noise. Also, it would be interesting to see whether ICA outper-forms SpecSVD in the presence of different kinds of noises. To evaluate this, we compared the performance of the two algorithms in the presence of Gaussian additive and mul-tiplicative noise. Additive noise had zero mean with vari-ance increasing from 1 to the maximum value in the origi-nal data. Multiplicative noise had mean of 1 with its vari-ance going from 1 to a maximum of 5 . We have shown representative behavior of the two algorithms in Figure 6. First two plots are bipartitioning in the presence of ad-ditive noise on Interest-Trade and multiplicative noise on ArachidonicAcids-Hematocrit . Similarly, next two are for k -partitioning with additive noise on wap and multiplicative on re0 datasets. From these results, we can see that inspite of the varying amounts and kinds of noise in the data, ICA is able to perform optimal partitioning indicated by its low isoperimetric ratio. Second noticeable fact is in regards to stability. Rising ratios as the variance increases indicates that the performance of algorithm is gradually decreasing. However, fluctuating ratios indicates instability and incon-sistency to partition optimally. To demonstrate the stability of both the algorithms empirically, we calculated the mean of standard deviation of the isoperimetric ratios of both the algorithms for bipartitioning and k -partitioning in the pres-ence of the noise. These results are shown in Table V. Higher standard deviation of SpecSVD indicates instability in partitioning the noisy datasets. We now compare the computational speed of ICA with SpecSVD. The time complexity for both the algorithms is dependent on the sparseness of the data matrix. In other words, it takes more time to partition a densely connected bipartite graph compared to a sparsely connected one. For this reason, we considered the worst case scenario of a fully connected bipartite graph where every vertex of one type is connected with all the vertices of the other type. Since, the time required to cut the indicator vector is same for both the algorithms, we compare on the basis of the time required to calculate the indicator vector. This experiment was per-formed on a machine with a 3 GHz Intel Pentium 4 proces-sor with 1 GB RAM. In Figure 7, we plot the time required by both the algorithms as the number of vertices in the fully connected bipartite graph increase. Time for ICA gradu-ally increases as the number of vertices increase. However, SpecSVD time increases more rapidly comparatively and hence is clearly outperformed by ICA.

We proposed the Isoperimetric Co-clustering Algorithm -a new method for partitioning the document-word bipar-tite graph. The proposed algorithm requires a solution to a sparse system of linear equations. Experiments performed demonstrate the advantages of our approach over spectral approach in terms of the quality, efficiency and stability in partitioning the document-word bipartite graph.


Both document clustering and word clustering are well stud-ied problems. Most existing algorithms cluster documents and words separately but not simultaneously. In this paper we present the novel idea of modeling the document collec-tion as a bipartite graph between documents and words, us-ing which the simultaneous clustering problem can be posed as a bipartite graph partitioning problem. To solve the par-titioning problem, we use a new spectral co-clustering algo-rithm that uses the second left and right singular vectors of an appropriately scaled word-document matrix to yield good bipartitionings. The spectral algorithm enjoys some opti-mality properties; it can be shown that the singular vectors solve a real relaxation to the NP-complete graph bipartition-ing problem. We present experimental results to verify that the resulting co-clustering algorithm works well in practice. Clustering is the grouping together of similar objects. Given a collection of unlabeled documents, document clustering can help in organizing the collection thereby facilitating fu-ture navigation and search. A starting point for applying clustering algorithms to document collections is to create a vector space model[20]. The basic idea is (a) to extract unique content-bearing words from the set of documents treating these words as features and (b) to then represent each document as a vector in this feature space. Thus the entire document collection may be represented by a word-by-document matrix A whose rows correspond to words and columns to documents. A non-zero entry in A, say Aij, in-dicates the presence of word i in document j, while a zero entry indicates an absence. Typically, a large number of words exist in even a moderately sized set of documents, for example, in one test case we use 4303 words in 3893 docu-ments. However, each document generally contains only a small number of words and hence, A is typically very sparse with almost 99% of the matrix entries being zero. Existing document clustering methods include agglomer-
Words may be clustered on the basis of the documents in 
In this paper, we consider the problem of simultaneous or 
A word about notation: small-bold letters such as ~, u, First we introduce some relevant terminology about graphs. and a set of edges {i,j} each with edge weight Eij. The adjacency matrix M of a graph is defined by Given a partitioning of the vertex set ]) into two subsets 
V1 and 1)2, the cut between them will play an important role in this paper. Formally, The definition of cut is easily extended to k vertex subsets, We now introduce our bipartite graph model for represent-ing a document collection. An undirected bipartite graph is a triple G = (:D,I/V,E) where 2) = {d~,... ,d,}, VV = {wl,... ,w,~} are two sets of vertices and E is the set of of documents and V9 is the set of words they contain. An edge {dl, wj} exists if word wj occurs in document dl; note that the edges are undirected. In this model, there are no edges between words or between documents. 
An edge signifies an association between a document and a word. By putting positive weights on the edges, we can capture the strength of this association. One possibility is to have edge-weights equal term frequencies. In fact, most. of the term-weighting formulae used in information retrieval may be used as edge-weights, see [20] for more details. Consider the m x n word-by-document matrix A such that Aij equals the edge-weight E O. It is easy to verify that the adjacency matrix of the bipartite graph may be written as where we have ordered the vertices such that the first m ver-tices index the words while the last n index the documents. 
We now show that the cut between different vertex sub-sets, as defined in (1) and (2), emerges naturally from our formulation of word and document clustering. A basic premise behind our algorithm is the observation: Duality of word g~ document clustering: Word cluster-ing induces document clustering while document clustering induces word clustering. 
Given disjoint document clusters ~D1,... ,:Dk, the corre-sponding word clusters VV1,... , VV~ may be determined as follows. A given word wi belongs to the word cluster ~Y,~ if its association with the document cluster :Din is greater than its association with any other document cluster. Using our graph model, a natural measure of the association of a word with a document cluster is the sum of the edge-weights to all documents in the cluster. Thus, 
VVm= {wi: ~f'~ A,j&gt; ~-~ Aij, Vl=l,...,k}. 
Thus each of the word clusters is determined by the docu-ment clustering. Similarly given word clusters W1 ,.  X   X  , VVk, the induced document clustering is given by 
Note that this characterization is recursive in nature since document clusters determine word clusters, which in turn determine (better) document clusters. Clearly the "best" word and document clustering would correspond to a par-titioning of the graph such that the crossing edges between partitions have minimum weight. This is achieved when cut(}&amp; X  U :Dx,... , Wk t.J :Dk) = min cut(l)1,... , ])k) where ])1,... , ])k is any k-partitioning of the bipartite graph. 
Given a graph G = (V, E), the classical graph bipartition-ing problem is to find nearly equally-sized vertex subsets ])~,])~ off) such that cut())~,])~) = min~ ~, cut(V1 V2). Graph partitioning is an important problem and arises in various applications, such as circuit partitioning, telephone network design, load balancing in parallel computation, etc. However it is well known that this problem is NP-eomplete[12]. But many effective heuristic methods exist, such as, the 
Kernighan-Lin(KL) [17] and the Fiduccia-Mattheyses(FM) [10] algorithms. However, both the KL and FM algorithms search in the local vicinity of given initial partitionings and have a tendency to get stuck in local minima. 
Spectral graph partitioning is another effective heuristic that was introduced in the early 1970s[15, 8, 11], and pop-ularized in 1990119]. Spectral partitioning generally gives better global solutions than the KL or FM methods. 
We now introduce the spectral partitioning heuristic. Sup-pose the graph G = (1), E) has n vertices and m edges. The n  X  m incidence matrix of G, denoted by ZG has one row per vertex and one column per edge. The column corresponding to edge {i,j} of IG is zero except for the i-th and j-th en-tries, which are ~ and -~ respectively, where Eij is the corresponding edge weight. Note that there is some am-biguity in this definition, since the positions of the positive and negative entries seem arbitrary. However this ambiguity will not be important to us. 
DEFINITION 1. The Laplacian matrix L = LG of G is an n  X  n symmetric matrix, with one row and column for each vertex, such that 
Lij = -Eij, i  X  j and there is an edge {i, j} (3) TttEOREM 1. The Laplacian matrix L = LG of the graph 
G has the following properties. 1. L = D -M, where M is the adjacency matrix and D 2. L = IGIc T. 3. L is a symmetric /)ositive semi-definite matrix. Thus 4. Let e = [1,..., 1] T. Then Le = O. Thus 0 is an 
Proof. consists of exactly one connected component. We now see how the eigenvalues and eigenvectors of L give us informa-tion about partitioning the graph. Given a bipartitioning tion vector p that captures this division, partition vector p, the Rayleigh Quotient 
Proof. Clearly pTp = n. By part 6 of Theorem 1, pTLp = ~(ij}E~ Eij(pi -pj)~. Thus edges within ])1 or V~ do not contribute to the above sum, while each edge between ])~ and ])z contributes a value of 4 times the edge-weight. [] solution when all pi are either -1 or +1. Informally, the cut captures the association between different partitions. We need an objective function that in addition to small cut val-ues also captures the need for more "balanced" clusters. 
We now present such an objective function. Let each vertex i be associated with a positive weight, denoted by weight(i), and let W be the diagonal matrix of such weights. ~-~-ie~, weight(i) = ~ie]), W~i. We consider subsets ]2~ and 'l)z to be "balanced" if their respective weights are equal. The following objective function favors balanced clusters, 
Given two different partitionings with the same cut value, the above objective function value is smaller for the more balanced partitioning. Thus minimizing Q0Yz,'l)z) favors partitions that have a small cut value and are balanced. 
We now show that the Rayleigh Quotient of the follow-ing generalized partition vector q equals the above objective function value. 
LEMMA 1. Given graph G, let L and W be its Laplacian and vertex weight matrices respectively. Let ~h = weightOYz ) and ~2 = weightO)2). Then the generalized partition vec-tor q with elements satisfies qrWe = O, and qrWq = weightO) ). Proof. Let y --We, then yl = weight(i) = Wil. Thus qTWe = r/t/-~-~2 Z weight(i)-~/-~-1 ~ weight(i) = 0. 
THEOREM 3. Using the notation of Lemma 1, Proof. It is easy to show that vector q may be written as where p is the partition vector of (6). Using part 7 of The-orem 1, we see that Substituting the values of pTLp and qTWq, from Theo-rem 2 and Lemma 1 respectively, proves the result. [] 
Thus to find the global minimum of (7), we can restrict our attention to generalized partition vectors of the form in Lemma 1. Even though this problem is still NP-complete, the following theorem shows that it is possible to find a real relaxation to the optimal generalized partition vector. 
THEOREM 4. The problem is solved when q is the eigenvector corresponding to the 2nd smallest eigenvalue A2 of the generalized eigenvalue problem, Proof. This is a standard result from linear algebra[13]. [] 
Thus far we have not specified the particular choice of vertex weights. A simple choice is to have weight(i) = 1 for all vertices i. This leads to the ratio-cut objective which has been considered in [14] (for circuit partitioning), 
An interesting choice is to make the weight of each ver-tex equal to the sum of the weights of edges incident on it, i.e., weight(i) = ~-'~k Eik. This leads to the normalized-cut criterion that was used in [22] for image segmentation. Note that for this choice of vertex weights, the vertex weight matrix W equals the degree matrix D, and weight0)i ) = cut0)~,))~) +within(])i) for i = 1, 2, where within(Vi) is the sum of the weights of edges with both end-points in )2i. Then the normalized-cut objective function may be expressed as where S(])l,Ys) = weight0)1) + weight(I)9.)" 
Note that S(])1, ])2) measures the strengths of associations within each partition. Thus minimizing the normalized-cut is equivalent to maximizing the proportion of edge weights that lie within each partition. 
In the previous section, we saw that the second eigenvector of the generalized eigenvalue problem I,z = ADz provides a real relaxation to the discrete optimization problem of finding the minimum normalized cut. In this section, we present algorithms to find document and word clusterings where Dt and D2 are diagonal matrices such that D1(i, i) = ~-~j Aij, Dz(j,j) = ~i Aij. Thus Lz = ADz may be Assuming that both D1 and D2 are nonsingular, we can rewrite the above equations as Letting u = DxI/2~ and v = Dz~/Sy, and after a little algebraic manipulation, we get These are precisely the equations that define the singular value decomposition (SVD) of the normalized matrix A,~ = D~-I/ZADz -1/9. In particular, u and v are the left and right singular vectors respectively, while (1 -A) is the cor-responding singular value. Thus instead of computing the eigenvector of the second (smallest) eigenvalue of (9), we can compute the left and right singular vectors corresponding to the second (largest) singular value of An, where as = 1 -A2. Computationally, working on A,~ is much better since A,~ is of size w  X  d while the matrix L is of the larger size (w + d)  X  (w + d). 
The right singular vector v~ will give us a bipartitioning of documents while the left singular vector u2 will give us a bipartitioning of the words. By examining the relations (10) it is clear that this solution agrees with our intuition that a partitioning of documents should induce a partitioning of words, while a partitioning of words should imply a parti-tioning of documents. 
The singular vectors u2 and v~ of A,, give a real approx-imation to the discrete optimization problem of minimizing the normalized cut. Given u2 and vs the key task is to extract the optimal partition from these vectors. 
The optimal generalized partition vector of Lemma 1 is two-valued. Thus our strategy is to look for a bi-modal distribution in the values of u2 and vs. Let ml and rn2 denote the bi-modal values that we axe looking for. From the previous section, the second eigenvector of/-, is given by One way to approximate the optimal bipartitioning is by the assignment of zs (i) to the bi-modal values mj (j = 1, 2) such that the following sum-of-squares criterion is minimized, The above is exactly the objective function that the classical k-means algorithm tries to minimize[9]. Thus we use the following algorithm to co-cluster words and documents: 1. Given A, form A,, = DI-1/ZADz -1/2. 2. Compute the second singular vectors of A,~, u2 and vs and form the vector zs as in (11). 3. Run the k-means algorithm on the 1-dimensional data z2 to obtain the desired bipartitioning. 
The surprising aspect of the above algorithm is that we run k-means simultaneously on the reduced representations of both words and documents to get the co-clustering. 
We can adapt our bipartitioning algorithm for the more general problem of finding k word and document clusters. One possibility is to use Algorithm Bipartition in a recursive manner. However, we favor a more direct approach. Just as the second singular vectors contain bi-modal informa-vz, v~,... , vt+l often contain k-modal information about the data set. Thus we can form the g-dimensional data set where U = [uz,... ,ut+l], and V = [v~ .... ,vt+i]. From this reduced-dimensional data set, we look for the best k-modal fit to the g-dimensional points rni,... ,mk by as-signing each g-dimensional row, Z(i), to rnj such that the sum-of-squares is minimized. This can again be done by the classical k-means algorithm. Thus we obtain the following algorithm. Algorithm Multipartition(k) 1. Given A, form A,~ = Dx-1/2AD2 -1/~. 2. Compute g = [log~ k] singular vectors of A,~, us,..  X  Ut+l and vs,.., re+l, and form the matrix Z as in (12). 3. Run the k-means algorithm on the g-dimensional data Z, to obtain the desired k-way multipartitioning. Name # Docs # Words # Nonzeros(A) MedCran_AII MedCisi_AII Classic3 Classic3_30docs Classic3_150docs Yahoo_KS 
Yahoo_K1 
For some of our experiments, we used the popular Med-llne (1033 medical abstracts), Cranfield (1400 aeronautical systems abstracts) and Cisi (1460 information retrieval ab-stracts) collections. These document sets can be down-loaded from ftp://ftp.cs.cornell.edu/pub/smart. For testing Algorithm Bipartition, we created mixtures consisting of 2 of these 3 collections. For example, MedCran contains docu-ments from the Medline and Cranfield collections. Typically, we removed stop words, and words occurring in &lt; 0.2% and &gt; 15% of the documents. However, our algorithm has an in-built scaling scheme and is robust in the presence of large number of noise words, so we also formed word-document matrices by including all words, even stop words. 
For testing Algorithm Multipartition, we created the Clas-sic3 data set by mixing together Medline, Cranfield and Cisl which gives a total of 3893 documents. To show that our algorithm works well on small data sets, we also created subsets of Classic3 with 30 and 150 documents respectively. 
Our final data set is a collection of 2340 Reuters news articles downloaded from Yahoo in October 199712]. The articles are from 6 categories: 142 from Business, 1384 from Entertainment, 494 from Health, 114 from Politics, 141 from Sports and 60 news articles from Technology. In the prepro-cessing, HTML tags were removed and words were stemmed using Porter's algorithm. We used 2 matrices from this col-lection: Yahoo_KS contains 1458 words while Yahoo_K1 in-cludes all 21839 words obtained after removing stop words. Details on all our test collections are given in Table 1. 5.1 Bipartitioning Results In this section, we present bipartitioning results on the MedCran and MedCisi collections. Since we know the "true" class label for each document, the confusion matrix cap-tures the goodness of document clustering. In addition, the measures of purity and entropy are easily derived from the confusion matrix[6]. 
Table 2 summarizes the results of applying Algorithm Bi-partition to the MedCran data set. The confusion matrix at the top of the table shows that the document cluster :D0 consists entirely of the Medline collection, while 1400 of the 1407 documents in :D1 are from Cranfield. The bottom of Table 2 displays the "top" 7 words in each of the word clus-ters ~4)o and W1. The top words are those whose internal edge weights are the greatest. By the co-clustering, the word cluster YVi is associated with document cluster :Di. It should be observed that the top 7 words clearly convey the "con-cept" of the associated document cluster. 
Similarly, Table 3 shows that good bipartitions axe also obtained on the MedCisi data set. Algorithm Bipartition uses the global spectral heuristic of using singular vectors which Wo: patients cells blood children hormone cancer renal ~4)x: shock heat supersonic wing transfer buckling laminar YYo: cells patients blood hormone renal rats cancer WI: libraries retrieval scientific research science system boo makes it robust in the presence of "noise" words. To demon-strate this, we ran the algorithm on the data sets obtained without removing even the stop words. The confusion ma-trices of Table 4 show that the algorithm is able to recover the original classes despite the presence of stop words. 
In this section, we show that Algorithm Multipartition gives us good results. Table 5 gives the confusion matrix for the document clusters and the top 7 words of the associ-ated word clusters found in Classic3. Note that since k = 3 in this case, the algorithm uses  X  = Flog2 k] = 2 singular vectors for co-clustering. 
As mentioned earlier, the Yahoo_K1 and Yahoo_K5 data sets contain 6 classes of news articles. Entertainment is the dominant class containing 1384 documents while Technol-ogy contains only 60 articles. Hence the classes are of varied sizes. Table 6 gives the multipartitioning result obtained by using  X  = [log 2 k] = 3 singular vectors. It is clearly diffi-cult to recover the original classes. However, the presence of many zeroes in the confusion matrix is encouraging. Ta-ble 6 shows that clusters :D1 and :D2 consist mainly of the Entertainment class, while :D4 and :D5 are "purely" from 
Health and Sports respectively. The word clusters show the underlying concepts in the associated document clusters (re-call that the words are stemmed in this example). Table 7 shows that similar document clustering is obtained when fewer words are used. 
Finally, Algorithm Multlpartition does well on small collec-tions also. Table 8 shows that even when mixing small (and random) subsets of Medline, Cisi and Cranfield our algorithm is able to recover these classes. This is in stark contrast to the spherical k-means algorithm that gives poor results on small document collections[7]. 
In this paper, we have introduced the novel idea of mod-eling a document collection as a bipartite graph using which we proposed a spectral algorithm for co-clustering words and documents. This algorithm has some nice theoretical prop-erties as it provides the optimal solution to a real relaxation of the NP-complete co-clustering objective. In addition, our :Do: 1014 0 :Dz: 19 1400 ~Yo: patients cells blood hormone renal cancer rats ~Yl: library libraries retrieval scientifc science book system 1412: boundary layer heat shock roach supersonic wing :Do: 120 82 0 52 0 57 :DI: 0 833 0 1 100 0 :D2: 0 259 0 0 0 0 793: 22 215 102 61 1 3 :D4: 0 0 392 0 0 0 :Ds: 0 0 0 0 40 0 }4)0: clinton campaign senat house court financ white 
Wx: septemb tv am week music set top ld)2: film emmi star hollywood award comedi fienne )4)3: world health new polit entertain tech sport 
V~a: surgeri injuri undergo hospit england accord recommen, )&amp;5: republ advanc wildcard match abdelatif ac adolph 
Table 6: Multipartitioning results for Yahoo_K1 algorithm works well on real examples as illustrated by our experimental results. [1] L. D. Baker and A. McCallum. Distributional [2] D. Boley. Hierarchical taxonomies using divisive [3] D. Boley, M. Gini, R. Gross, E.-H. Has, K. Hastings, [4] C. J. Crouch. A cluster-based approach to thesaurus [5] D. R. Cutting, D. R. Karger, J. O. Pedersen, and [6] I. S. Dhillon, J. Fan, and Y. Guan. Efficient clustering [7] I. S. Dhillon and D. S. Modha. Concept [8] W. E. Donath and A. J. Hoffman. Lower bounds for [9] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern [10] C. M. Fiduccia and R. M. Mattheyses. A linear time :Do: 120 113 0 1 0 59 :DI: 0 1175 0 0 136 0 :D2: 19 95 4 73 5 1 :D3: 1 6 217 0 0 0 :D4: 0 0 273 0 0 0 :Ds: 2 0 0 40 0 0 Wo: compani stock financi pr busi wire quote ~VI: film tv emmi comedi hollywood previou entertain )4)2: presid washington bill court militari octob violat )4)3: health help pm death famili rate lead W4: surgeri injuri undergo hospit england recommend disco~ )A)5: senat clinton campaign house white financ republicn Table 7: Multipartitioning results for Yahoo_K5 Table 8: Results for Classic3_3Odocs and Classic3_150docs [11] M. Fiedler. Algebraic connectivity of graphs. [12] M. R. Garey and D. S. Johnson. Computers and [13] G. H. Golub and C. F. V. Loan. Matrix computations. [14] L. Hagen and A. B. Kahng. New spectral methods for [15] K. M. Hall. An r-dimensional quadratic placement [16] R. V. Katter. Study of document representations: [17] B. Kernighan and S. Lin. An efficient heuristic [18] T. Kohonen. Self-organizing Maps. Springer, 1995. [19] A. Pothen, H. Simon, and K.-P. Liou. Partitioning [20] G. Saiton and M. J. McGill. Introduction to Modern [21] H. Schfitze and C. Silverstein. Projections for efficient [22] J. Shi and J. Malik. Normalized cuts and image 
