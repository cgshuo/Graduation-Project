 Discovering temporal dependence structure from multivari-ate time series has established its importance in many appli-cations. We observe that when we look in reversed order of time, the temporal dependence structure of the time series is usually preserved after switching the roles of cause and effect . Inspired by this observation, we create a new time series by reversing the time stamps of original time series and combine both time series to improve the performance of temporal dependence recovery. We also provide theoretical justification for the proposed algorithm for several existing time series models. We test our approach on both synthetic and real world datasets. The experimental results confirm that this surprisingly simple approach is indeed effective un-der various circumstances.
 G.3 [ Probability and Statistics ]: Time Series Analysis Time Series Analysis; Generalized Linear Model
Discovering temporal dependence structures from multi-variate time series is one of the central tasks in time series analysis. It easily finds applications in many domains. For example, in social networks, accurate identification of influ-ence networks from users X  time series activity records is of significant importance for advertising, marketing, and psy-chological studies. In biology, the gene regulatory networks recovered from time series microarray data reveals key in-formation on gene functions.

Inferring dependency network structures from time se-ries data has been extensively studied in the past. The Granger causality framework, which establishes temporal dependence structures based on regression techniques, has become popular due to its simplicity, robustness, and ex-tendability [23 , 7, 16 , 3, 22 ]. Nowadays, as more and more large-scale time series data become available, traditional ap-proaches for identifying Granger causality are confronted with a series of challenges, such as inconsistency, high com-putational complexity, and so on. To address these prob-lems, penalized regression techniques (e.g. lasso or lasso-type regressions) have been applied, leading to major im-provement for applications with sparse temporal dependence structures [ 29, 2, 27 ]. However, the overall performance of existing Granger causality techniques still leaves room for improvement. In this paper, we aim to explore a new direc-tion by considering the procedure of reversing the time in time series data.

The inspiration for our work comes from classical mechan-ics where it is well-known that the basic equations of the classical physics remains valid when we look in reversed or-der of time, i.e., replacing time stamp t with  X  t . In a simple world, if time flows in the opposite direction, objects interact with each other under the same physical laws, and we will not notice the difference. Instead of explaining all phenom-ena from the underlying physical law, we usually apply sim-plified mathematical models to real world events. Since the underlying physics mechanism is time reversible, we would expect our model applies when the time is reversed. The question remains whether we can consolidate and enhance our estimation accuracy by utilizing the information from both directions. 1
To fully utilize such an idea, we need to examine the effect of reversing the time on the temporal dependence structures. One important assumption of Granger causality is that the cause occurs before the effect . If an event A at time t causes an event B to happen at time t + k , we will see a correlation between events A and B with time lag k . By reversing time, the correlation between events A and B still exists, with the difference that B occurs before A . Granger causality-based algorithms should suggest that A causes B with time lag k from the original time series. Similarly, we expect that the same algorithm would also indicate that B causes A with time lag k from the reversed time series. Note that our argument is not limited to Granger causality, it also applies to other algorithms that rely on the correlation with time lags between time series, e.g., transfer entropy [26 ].
It should be noted that, for a closed complex system, the trend of entropy eliminates the ambiguity on the time di-rection, as suggested by the second law of thermodynamics. But since the model only addresses a particular aspect of the system, the restriction usually does not apply.
The link between the original time series and the reversed time series raises the possibility of combining these two di-rections for enhanced temporal dependence inference. This motivates us to propose a novel but simple approach, namely forward backward (FB) Granger causality, to infer the tem-poral dependence structures for multivariate time series. Firstly, we apply Granger causality-based algorithm on both the original time series and the time-reversed time series, then we combine the results by simple averaging. Note that sim-ilar approach has been applied in Natural Language Pro-cessing [21], where they estimate the transition kernel of the Markov chain from both directions. Performance improve-ment has been observed when the size of data is limited. We provide both theoretical analysis and empirical studies on the effectiveness of the proposed approach. The rest of the paper is organized as follows: we first review the pre-liminary and related works in Section 2. In Section 3, we describe our FB Granger causality algorithm and provide theoretical analysis on several existing models. Finally, we show experimental results in Section 4 and conclusion in Section 5.
We define the forward time series as the original multivari-time series { z ( t ) } is defined as z ( t ) := y (  X  t ) { z ( t ) } both contain N time series; univariate time series are denoted by { y ( t ) i } and { z ( t ) i } for i = 1 ,...,N . Both y z ( t ) are vectors of the values for each time series at time t , respectively. If the i th time series at time t is caused by the j th time series at time t  X  k , we say that i is caused by j with lag k . Moreover, we represent this temporal dependence re-lation by the ordered temporal dependence triplet ( i,j,k ). And the inverse of temporal dependence triplet ( i,j,k ) is defined as ( j,i,k ). In addition, C y denotes the set of all temporal dependence triplets for time series { y ( t ) } .
Causal inference has consistently been an important task for researchers in various fields of science. There are two main tasks in causal inference: (1) How to cancel the con-founding bias, e.g., [24 ] and (2) How to discover the causal structures among the given variables when a set of assump-our concern and we intend to improve the existing causal discovery algorithms.

The causal discovery task is challenging and may require many assumptions with weak guarantees of finding the true causal structure, see for example the theoretical discussions in [25 ]. Granger causality is one of the most popular ap-proaches to quantify temporal dependence structures for time series observations. It is based on two major prin-ciples: (i) The cause happens prior to the effect and (ii) The cause makes unique changes in the effect [14 , 15 ]. In practice, Granger causality tests are carried out by fitting a Vector Auto-regression (VAR) model. Up to now, two ma-jor approaches based on VAR model have been developed to uncover Granger causality for multivariate time series. One approach is the significance test [20, ch. 3.6.1]: given multiple time series { y ( t ) } , we run a VAR model as follows, where P is the maximal time lag. We can determine that in the coefficient vector { A ` } ij for ` = 1 ,...,P is nonzero by statistical significant tests. The second approach is the Lasso-Granger approach [29, 2, 27], which applies a lasso-type VAR model to obtain a sparse and robust estimate of the coefficient vectors for Granger causality tests. Specif-ically, the regression task in Eq. ( 1) can be achieved by solving the following optimization problem: where  X  is the penalty parameter, which determines the sparsity of the coefficients A ` .

Several approaches have been proposed for identification of Granger causality for nonlinear time series; among the notable ones, kernelized regression [22 ], nonparametric tech-niques such as [16 , 23, 26 ], non-Gaussian structural VAR [17], generalized linear autoregressive models [ 19 , 5], and the Copula Granger [ 4].

The proposed method in this paper is similar to boot-strap aggregating (bagging) techniques [6 ] in the sense that it averages over the results from multiple datasets. But the fundamental difference between the two techniques stems from the way that the algorithms generate datasets: the bagging techniques sample the original dataset and create subsamples of the dataset and then average over the results of the algorithm on each new datasets. Here we do not sub-sample the original dataset; instead we create a new dataset by reversing time. Randomization techniques [ 12 ] consti-tute another wide class of dataset manipulation techniques. However, note that the our proposed method is purely de-terministic.
In this section, we first describe our algorithms for exploit-ing the information in the backward time series, and then elaborate theoretical bases for the gain achieved by these algorithms.
Given a specific temporal dependence inference algorithm, if it indicates the existence of the temporal dependence triplet ( i,j,k ) based on the forward time series { y ( t ) } , as argued in Section 1, intuitively we would expect the algorithm to find the triplet ( j,i,k ) based on the backward time series { z ( t ) } . This motivates our core design principle for utilizing this property: we can achieve more robust temporal depen-dence inference by appropriately combining the results from both the forward time series and the backward time series produced by the same temporal dependence inference algo-rithm.

The validity of such an approach depends on both the original temporal dependence inference algorithm and how we combine the results. We mainly focus on the Granger causality-based algorithms in this paper. Algorithm 1 Naive Forward Backward Lasso Granger Causality Input: Time series { y ( t ) } , lag P , penalty parameter  X  . Output: Coefficients A F B ` , ` = 1 , 2 ,...,P .

Define the backward time series { z ( t ) } by z ( t ) = y
Get forward coefficients A ` via Lasso-Granger with { y ( t ) P , and  X  .

Get backward coefficients B ` via Lasso-Granger with { z ( t ) } , P , and  X  .
 Return A F B ` = 1 2 ( A ` + B &gt; ` ), ` = 1 , 2 ,...,P . Algorithm 2 Naive Forward Backward Copula Lasso Granger Causality Input: Time series { y ( t ) } , lag P , penalty parameter  X  .
Output: Coefficients A F B ` , ` = 1 , 2 ,...,P . for each i = 1 , 2 ,...,N do end for
Get coefficients A F B ` by calling algorithm 1 with { w ( t ) P , and  X  .
 Return A F B ` , ` = 1 , 2 ,...,P .
 In general, suppose that the assumptions for correctness of Granger causality are satisfied such that the coefficients es-timated by Granger causality indicate the existence of tem-poral dependence relationships; such assumptions have been studied in [13 , 4]. The simplest way to combine the results is to add the coefficients for ( i,j,k ) in the forward time se-ries and ( j,i,k ) in the backward time series, which yields the Naive Forward Backward Lasso Granger Causality Al-gorithm , shown in Algorithm 1. It is important to note that since we only flipped the temporal order of the original dataset, the results from forward and backward time series are expected to be correlated. But the coefficients are not fully correlated for time series with finite length, which is supported by our experimental results.

However, Granger causality is designed for linear time se-ries, which is not always the case for the data of interest. Given a time series { x ( t ) } , we can map the data using the empirical marginal distribution of time series to the Gaus-sian distribution by where  X  F is the empirical cumulative distribution function (CDF) of the i th time series,  X  is the CDF for standard Gaussian distribution and s i is the standard derivative of the i th time series, which helps to retain original informa-tion. { y ( t ) } will be treated as a linear representation for the original time series, to which we can apply Granger causality based algorithms, e.g., the Copula Lasso Granger Causality [4 ], and similarly, Naive Forward Backward Copula Lasso Granger Causality as described in algorithm 2.
In this section 2 , we show that for the time series gener-ated from Vector Autoregressive Model (VAR), the backward time series is also a VAR under some conditions. To do so,
Note that by continuous or discrete time series, we refer to whether the variables take on continuous or discrete values. we assume ( t )  X  X  (0 , X I ), where  X  is a constant which gov-erns the level of noise, and N (  X , X  2 ) denotes the Gaussian (normal) distribution. The VAR model with the Gaussian noise uniquely defines a multivariate Gaussian distribution to derive the conditional distribution of the same form as equation ( 1) for backward time series { z ( t ) | z ( t ) We show that the backward time series is also a VAR model only with different set of coefficients B i and noise. How-ever, for arbitrary VAR, the causation defined on the for-ward time series y ( t ) is not the same as the inverse of the causation defined on the backward time series { z ( t ) the set of causation triplets of the backward time series C z 6 = { ( j,i,k ) | ( i,j,k )  X  C y } where C y = { ( i,j,k ) } is the set of causation triplets of the forward time series. In the following theorem, we show that the temporal dependence triplets on the backward time series { z ( t ) } is closely related to the inverse of the triplets on { y ( t ) } .
 Theorem 3.1. If the forward time series { y ( t ) } is stable and there exists  X  &gt; 0 and a matrix norm ||| X ||| , e.g., the the backward time series { z ( t ) } is also a VAR, defined by where  X  t  X  X  (0 , X  [ I +  X (  X  )]) and B i = A &gt; i + o (  X  ) ,  X  i . Proof. By the definition of { y ( t ) } , we have Because time series { y ( t ) } is stable, so it is also strictly sta-tionary [20 , Ch. 2.1.3] and the marginal distribution of the P consecutive time stamps has the following representation: Y where the Y ( t ) is an NP  X  1 vector, and the {  X  ij } is the precision matrix (represented in blocks), each block  X  ij N  X  N matrix. Given the stationarity of the time series, we can set t = 0 without the loss of generality. The probability density function (PDF) of the marginal distribution of the P + 1 consecutive time stamps are proportional to where A P +1 =  X  I ,  X  P +1 ,i =  X  j,P +1 = 0 . The PDF of the conditional distribution of y (1) given y (2) ,..., y proportional to exp[  X  1
To study the structure of the covariance matrix {  X  ij }  X  1 we recall that the Moving Average representation of VAR model, which is
So we have that where the lower right block of  X  t  X  i is I , otherwise is 0 .
By some derivations, we can show that where the diagonal blocks are I +  X (  X  ) by setting all A And the first row of blocks are ( I + X (  X  ) , A &gt; P + o (  X  ) ,..., A o (  X  )), which can be derived by studying the first P + 1 terms in the series. So by inversion, we have that Together with A T 1 A i = o (  X  ) ,  X  i = 1 , 2 ,...,P , we have By replacing y ( t ) with z (  X  t ) , we have where  X  t  X  X  (0 , X  [ I +  X (  X  )]).

The assumption in Theorem 3.1 implies that the strength of the influence in time series { y ( t ) } has an upper bound, which is sufficient but not necessary. It can be relaxed, since the proof only requires that the higher order prod-uct between A i is negligible comparing with A i itself. This assumption can be easily satisfied when the temporal depen-dence structure is sparse, which is usually true in real world applications.

Theorem 3.1 shows that the first order components of the influence on the backward VAR is exactly the inverse of the forward VAR, i.e., not only ( i,j,k )  X  X  y  X  ( j,i,k )  X  X  they also share the same strength { A k } ij , which indicates a much stronger link between the forward time series and the backward time series. The link also results in a simple form, which justifies our approach of combining the results from both directions, i.e., averaging on the corresponding coeffi-cients. Moreover, Granger causality provides an unbiased estimation for both directions, and the results in [ 27 ] indi-cate that they have the same variance. Therefore the aver-age of both directions is also unbiased with smaller variance, when the correlation of two estimations are strictly less than 1. Additionally, when we have sufficiently long time series, i.e., T N , we would expect that the forward backward ap-proach provides an estimation similar to the original Lasso Granger causality, since both forward and backward should provide accurate coefficients estimation, as suggested by the consistency of the penalized maximal likelihood estimation. This phenomenon has been observed in our experiments on synthetic datasets.

For nonlinear time series, we apply the copula transfor-mation before Granger causality. In order to show similar theoretical results for this approach, we need the data to be generated from the Granger Non-paranormal (G-NPN) model as follows:
Granger Non-paranormal (G-NPN) model We say a distribution G  X  NPN ( x , A ,F ) if there exist monotonically increasing functions { F i } N i =1 such that F i ( x ( t ) are jointly Gaussian and can be factorized according to the VAR model with coefficients A = { A k } P k =1 . More specifi-cally, the joint distribution for the transformed random vari-ables y ( t ) i , F i ( x ( t ) i ) can be factorized as follows p where p N ( y |  X , X  ) is the Gaussian density function with mean  X  and variance  X  2 .
 Proposition 3.1. Using the copula transformation on the data generated from G-NPN model, the forward and back-ward relationships in Theorem 3.1 hold for the transformed random processes.
 Proof. Our proof is mainly to show that the copula trans-formation recovers the original vector auto-regressive pro-cess. The key step to prove this result is to show that if X  X  X  (0 , 1) and Y = F ( X ), where F (  X  ) is a monotonically increasing function, then  X   X  1 ( F Y ( Y ))  X  N (0 , 1). Further-more, the independence relationships will be preserved a the copula mapping, as the transformation  X   X  1 ( F Y (  X  )) is a de-terministic transformation. This is because X  X  X  X  Y if and only if g ( X )  X  X  X  h ( Y ) for any arbitrary random variables X and Y and deterministic one-to-one transformation func-tions g (  X  ) and h (  X  ).

After applying the above result to each variable x ( t ) i can show [4 ] that by using the copula transformation we obtain y ( t ) i which are multivariate Gaussian as defined in the definition of G-NPN.

The definition of the Granger Non-paranormal (G-NPN) model indicates that the underlying mechanism of time se-ries { x ( t ) } is a linear time series, which subsumes numerous circumstances. When { x ( t ) } is an observation of { y deterministic bias, the copula transformation helps to re-store the original information as stated in Proposition 3.1 . And then we can apply our argument for the VAR model.
Nowadays, social media provides a rich source for time series analysis because the interactions among individuals are naturally reflected in the time series of action logs. One way to analyze the social influence among users is to create time series of user activity by assigning 1 to a user at a particular time interval if she has at least one activity in that time interval and 0 otherwise [1 ]. For example, tweeting (i.e., posting on Twitter) activity naturally defines a time series, where y ( t ) i = 1 if user i posts at time interval t , and y otherwise. We can also recover influence relationship among users based on retweeting. For example, if user i retweets user j , we say that i has been influenced by j . Such social network time series pose a unique challenge for the temporal dependence inference algorithms.

In this section, we first define a general type of binary time series, which includes many existing models. Then, with additional Assumption 3.2 , we prove that applying Granger causality to binary time series provides consistent temporal dependence structure recovery. Furthermore, by Assump-tion 3.2 and Lemma 3.4 , we build the connection between the forward times series and the backward time series, which leads to the consistency results on temporal dependence structure recovery for the backward time series. By con-sistency we refer to that with appropriate thresholding on the estimated coefficients, we can correctly recover all tem-poral dependence triplets from the time series. Note that we are applying Granger causality on a misspecified model (i.e., the binary time series is not generated from VAR), the consistency results also justify our approach which applies Granger causality on certain non-VAR time series.

Given a binary multivariate time series { y ( t ) } , we say the i th series is activated at time t if and only if y ( t ) addition to the previous notations, we denote y ( t  X  1: t  X  P ) ( y sumptions: Assumption 3.1.
 Markov Assumption
The probability of activation for any variable at time t only depends on the states of the most recent P times ( t  X  1 ,t  X  2 ,...,t  X  P ) , which is The last equality is because of variables only take binary value, so the status of y ( t  X  1: t  X  P ) is uniquely defined by  X  .
 Activation Rate Monotonicity which means that there is no negative influence on the acti-vation rate if more variables from the histories become acti-vated.
 Influcence Significance  X  .
 And we say j is a cause for i with lag k
The last term in Assumption 3.1 actually implies that the causation is significant under any circumstances, i.e. the activation of time series j at time t  X  k will increase the activation rate of time series i at time t by at least  X  regardless of other variables in y ( t  X  1: t  X  P ) , or the status of y i does not depends on the status of y stress that Assumption 3.1 can be easily satisfied in practical applications and many existing models fall in this category. Example 3.1. Independent Cascade [18] (IC) model is originally proposed for modeling the diffusion process in so-cial networks. We modify it to model the activity on net-works over time. IC model defined on a weighted directed graph {V , E} , with each vertex represents an individual in the network. If vertex v is activated at time t , it attempts to activate its neighbor s with probability p v  X  s at time t + 1 in-dependently. If any neighbor of s activates s successfully, s will be marked as activated at time t + 1 . It can also activate itself by probability  X  s . The activation rate has the following representation And the IC model satisfies Assumption 3.1 .

Note that because we are applying Granger causality as a misspecified model, we need one more assumption to support the consistency results on binary time series.
 Assumption 3.2. Diminishing Influcence Let a binary time series { y ( t ) i } i = 1 , 2 ,...,N satisfy Assumption 3.1. for all possible values of S jk as N  X  +  X  .

And for any vector  X  jk of the same size as S jk , we assume that ||  X  jk ||  X  = O (1) and for all possible value of S jk as N  X  +  X  .

Assumption 3.2 shows that the influence of an individ-ual on the entire network is diminishing as the size of the network increases. For example, the difference of joint dis-the influence of y ( t  X  k ) j on S jk , e.g., when there is no differ-ence, y ( t  X  k ) j and S jk are independent. Moreover, note that, if Assumption 3.2 holds for { y ( t ) } , then { z ( t ) Assumption 3.3. Given a binary time series { y ( t ) i 1 , 2 ,...,N , we have if j is a cause for i with lag k . Otherwise, as N  X  +  X  .
Assumption 3.3 helps us to establish the connection be-tween the forward time series and the backward time series, and it is important for the consistency result in Theorem 3.3 . Furthermore, we have an important lemma connecting the Assumption 3.3 with 3.1 and 3.2 .
 Lemma 3.2. Given a binary time series { y ( t ) i } ,i = 1 , 2 ,...,N satisfy Assumption 3.1 and 3.2 , then it satisfies Assumption 3.3 .
 Proof. Given the binary time series { y ( t ) i } , i = 1 , 2 ,...,N , By Assumption 3.1 and 3.2, if j is a cause for i with lag k , we have Otherwise, we have as N  X  +  X  .

We now state our main theorem in this section, which suggests the consistency of Granger causality on binary time series with appropriate thresholding.
 Theorem 3.3. Let a binary time series { y ( t ) i } , i = 1 , 2 ,...,N satisfy Assumption 3.2 and 3.3 . We denote the coefficients by A k ,k = 1 , 2 ,...,P as in VAR model, which are estimated by applying Granger causality on time series { y ( t ) } , we have if j is a cause for i with lag k . Otherwise, as N  X  +  X  .
 Proof. As T  X  +  X  , we have the objective function of the regression as follows:
X Note that we do not sum over t , since we weight the loss term by its own marginal distribution. Without loss of gen-erality, we study the coefficient { A k } ij (denoted by  X  which connecting y ( t ) i and y ( t  X  k ) j . Let us denote y { y j } by S jk and the associated coefficients by  X  jk . By absorbing the constant into b , we can shift the value of y j from { 0 , 1 } to { X  0 . 5 , 0 . 5 } . The related objective is as follows: X By taking the derivative w.r.t.  X  ijk , we have Recall that { y ( t ) } satisfy Assumption 3.2 , which indicates that if j is a cause for i with lag k . Otherwise, as N  X  +  X  . This proves the theorem.

Theorem 3.3 indicates that by setting an appropriate thresh-old (on the order of min { i,j,k } {  X  ijk } ) on the coefficients, we can reconstruct the correct temporal dependence structure. We now explain the connection between the forward time series { y ( t ) } and the backward time series { z ( t ) Lemma 3.4. Given a binary time series { y ( t ) i } i = 1 , 2 ,...,N satisfy Assumption 3.3, we have if j is a cause for i with lag k . Otherwise, as N  X  +  X  , if P ( y ( t ) i ) =  X (1) ,  X  i,t .
 Proof. Let A and B be two binary random variables with P ( A,B ) =  X (1). And for simplicity, we denote P ( A = i,B = j ) by p ij . We have P ( A = 1 | B = 1) &gt; P ( A = 1 | B = 0) +  X  Replace y ( t  X  k ) j and y ( t ) i by A,B , we proved the Lemma.
Note that y ( t  X  k ) j and y ( t ) i are switched, compared with As-sumption 3.3 . Simply combining Lemma 3.4 with Theorem 3.3 , we have similar consistency results for the backward time series: Theorem 3.5. Let a binary time series { y ( t ) i } ,i = 1 , 2 ,...,N satisfy Assumptions 3.1 and 3.2 . We define the backward by B k ,k = 1 , 2 ,...,P as in VAR model, which are esti-mated by applying Granger causality on time series { z We have if j is a cause for i with lag k w.r.t. { y ( t ) i } . Otherwise, as N  X  +  X  , if P ( y ( t ) i ) =  X (1) ,  X  i,t .
 Proof. We prove this theorem based on existing theorems and lemmas. Forward time series { y ( t ) i } , i = 1 , 2 ,...,N satisfy Assumption 3.2 , so does the backward time series since Assumption 3.2 is symmetric in time. Forward time series { y ( t ) i } , i = 1 , 2 ,...,N satisfy Assumption 3.1 and 3.2 , by Lemma 3.2 , it also satisfies Assumption 3.3 . Then by Lemma 3.4 , the backward time series also satisfy Assump-tion 3.3. Then by Theorem 3.3 , we prove Theorem 3.5 .
Theorem 3.5 indicates that applying Granger causality on the backward time series also provides consistent temporal dependence inference results. Note that we only assume 3.1 and 3.2 for the forward time series. In fact, the backward time series might not satisfy Assumption 3.1 at all.
In Section 3.2 and 3.3 , we investigate several well-known time series models and establish the connection between for-ward and backward time series in terms of temporal depen-dence structures. For VAR, the strength { A k } ij of triplet ( i,j,k ) in forward time series is approximately the same as that for { B k } ji of triplet ( j,i,k ) in backward time series. For binary time series models that satisfy Assumptions 3.1 and 3.2 , the strength of triplets ( i,j,k ) and ( j,i,k ) for for-ward and backward time series, respectively, share the same order of magnitude. These connections justify our approach to combine the results from the forward and the backward time series for better temporal dependence inference.
Note that information inferred from the backward time se-ries is not exactly the same as that inferred from the forward time series, but they indeed share considerable similarities, which can be utilized as suggested. Moreover, when applied to real world data, model misspecification should be consid-ered, which is absent in our current analysis. In addition, one should be aware of the data preprocessing procedure, to make sure it is compatible with our assumptions, especially when the preprocessing relies on a specific order of time.
In the experiments, we evaluate the effectiveness of our proposed approach on several synthetic datasets and two real world datasets. Next, we describe the data collections, baseline methods, evaluation metric and experimental re-sults.
Synthetic Datasets Since we do not have the access to the true underlying temporal dependence structure in most applications, we generate synthetic datasets to evaluate the performance of temporal dependence structure recovery.
For discrete time series, we generate two synthetic datasets: one is generated from the IC model (as discussed in example 3.1 ), and the other is an instance of the generalized linear models (later referred to as LOG model), with a Bernoulli distribution and the link function  X  (  X  )  X  1 as a logistic sigmoid function. Specifically, the distribution of y ( t ) is a Bernoulli random vector with parameter defined as follows: If A k are all nonnegative matrices, it satisfies Assumption 3.1 .

For continuous time series, we also generate two synthetic datasets: one is linear time series generated according to VAR, and the other is nonlinear time series generated from generalized linear model with polynomial link function and Gaussian noises (POLY). Specifically, the distribution of y is a multivariate Gaussian, i.e., where f (  X  ) is defined by f ( x ) = x + bx 3 . We vary b to change the level of nonlinearity.

For each model, we set the lag to 1 and generate a sparse temporal dependence structure A with 5% nonzero entries. For the IC, each nonzero entry in A is drawn from a uni-form distribution Uniform(0 , 1). For LOG,  X  i is drawn from N (0 , 1), and each nonzero entry in A is drawn from N (0 , 1). For VAR and POLY, each nonzero entry in A is drawn from N (0 , 1), and then we normalize A by its Frobenius norm to ensure the stability of the time series. Moreover, for each model, we generate time series { y ( t ) i } , i = 1 ,...,N, t = 1 ,...,T by setting N and T with two scenarios: (1) N = 30 ,T = 2000, which corresponds to the low-dimensional case, and (2) N = 100 ,T = 150, which mimics the high-dimensional case.

Twitter Datasets We collect the Haiti dataset[5 ] with all the tweets published between Oct 2009 and Jan 2010 on  X  X aiti earthquake X . We choose this topic because it was one of the hot topics during that time period and many tweets have been generated around the event.

For the Haiti dataset, we collect the tweets by searching the keyword  X  X aiti X  from Jan. 12, 2010 for 17 days. We then generate multivariate time series datasets by counting the number of tweets from the top 1000 users (who tweet most on the topics) over these 1000 intervals. For accurate model-ing, we remove the users that are highly correlated with each other, most of which are operated by the same persons and tweet exactly the same contents. We also remove robot-like user-accounts who tweet on very regular intervals. Finally, we select a set of users with at least one interaction with another user, which results in a subset of 274 users.
Microarray Dataset Most multicellular organisms rely on their immune systems to defend against the infection from a multitude of pathogens. We collect the time series microarray data on macrophages from human immune cells from the supporting website of [9 , 11 ]. It consists of 1651 genes with 9 time series observations. We apply the pro-posed model to this dataset in order to infer the temporal dependence networks for immune system genes. Due to the Figure 1: The performance of temporal dependence recovery on IC and LOG datasets. Top: N = 30 ,T = 2000 ; Down: N = 100 ,T = 150 . Results suggest that LG benefits from the forward backward approach. space limit, we only show the results of a subset of 6 genes, whose interactions have been well studied.
We use the following baselines for comparison analysis on the synthetic datasets:
For Lasso based algorithm, we choose the penalty param-eter  X  to minimize the prediction error on the validation dataset.

For the Haiti dataset, we also test the Transfer Entropy (TE) algorithm [26 ]. Transfer entropy is another related technique which identifies the temporal dependence struc-ture between two time series by measuring the decrease in uncertainty of one time series in the future, given the past information of the other time series. Namely, the transfer entropy is defined as
T where H ( x ) is the Shannon entropy of the random variable x . We also test the Naive Forward Backward Transfer Entropy (FB TE), where we measure the influence from j to i by T
To evaluate the performance of different methods in recov-ering temporal dependence structures, we choose the Area Figure 2: The performance of temporal dependence recovery on VAR dataset with N = 100 ,T = 150 . Results suggest that LG benefits from the forward backward approach.
 Under the Curve (AUC) measure as it is a good perfor-mance measure for the ground truth with unbalanced ratio of positive and negative labels. The value of AUC is the probability that the algorithm will assign a higher value to a randomly chosen positive (existing) edge than a randomly chosen negative (non-existing) edge in the graph [10 ].
For synthetic datasets, we calculate AUC against the ground truth, i.e., the temporal dependence structure defined by A . The reported results are averaged over 20 randomly gener-ated datasets.

For the Twitter dataset, since we do not have access to the true underlying influence graph in the social network, we use the retweet information as indirect evaluation. It has been argued that the retweet graph in the future time can reflect the influence in social networks to a certain extent [8]. We first represent the retweet information by a weighted graph G RT , where the weight of an edge ( s  X  t ) denotes the number of tweets from user s retweeted by user t . The retweet graph G RT on Haiti earthquake has 867 edges.
For the microarray dataset, we do not have the complete ground truth as well. We therefore compare our results with those reported interactions in the BioGRID database curated biological database of protein-protein and genetic interactions. In this section, we present the result of our experiments. We focus on the difference of performance between the orig-inal version of algorithm and the forward backward version.
Results on Synthetic Datasets We aim to test whether the forward and backward approach can improve the perfor-mance of LG on IC and LOG datasets. We report the AUC scores of LG and FB LG in Figure 1. We can see that FB LG consistently outperforms LG, which suggests that FB LG indeed benefits from combining estimates from forward and backward time series.

On the VAR dataset, we focus on the performance for lin-ear continuous time series. We report the AUC scores of LG and FB LG in Figure 2. For high-dimensional time series, FB LG outperforms LG significantly, which indicates that combining two directions helps to improve the performance. www.thebiogrid.org Figure 4: The performance of temporal dependence recovery on Haiti dataset. The performance of FB LG and FB TE outperform the LG and TE, respec-tively.

On the POLY dataset, we aim to test whether the for-ward and backward approach can further improve the per-formance of LG and CLG for nonlinear time series. We re-port the AUC scores of LG, FB LG, CLG, FB CLG on the POLY datset in Figure 3. As we can see CLG can improve LG thanks to the copula transformation. And FB CLG can further improve CLG for high-dimensional case. For both VAR and POLY datasets, when N = 30 ,T = 2000, we find that the performance of LG and FB LG are similar, as sug-gested in Section 3.2 .

Results on Twitter Dataset We test LG, FB LG, TE, and FB TE on the Haiti dataset (see Figure 4 for results). We set the lag P = 15 for all algorithms for fair compari-son, which corresponds to 6 hours approximately. The AUC is calculated against the retweet graph G RT , and we vary the required number of retweets, so that only if the retweets from j by i passes the required number n , we establish an edge from i to j . Intuitively, n screens the weak influence be-tween users. As we can see, all algorithms perform better as we increase n . In addition, the forward backward approach improves the performance for both baseline algorithms. Table 1: Top 20 predictions for gene interaction by LG and FB LG. Bold terms are ground truth sug-gested by BioGRID database.
 Results on Microarray Dataset We test LG and FB LG on the time series microarray dataset, and achieve AUCs of 0 . 6923 and 0 . 7308 , respectively. Moreover, we list the top edges identified by both algorithms in Table 1. The bold ones are the ground truth suggested by BioGRID database. LG correctly identified 3 interactions while FB LG identified all 4 known interactions.
Inspired by time-reversibility of physical laws and its ef-fect on temporal dependency structure, we proposed the forward backward approach to improve the performance of Granger causality. We developed the forward and backward Lasso Granger causality algorithm, which combines the co-efficients estimated from the forward time series and back-ward time series to provide better performance of temporal dependency structure recovery. We show that with the cop-ula transformation, we can extend our algorithm for non-linear time series. Theoretical analysis on several existing times series models including VAR and IC model confirms our intuition. Our empirical results on both synthetic and real world datasets demonstrate that the forward backward approach can improve the performance of temporal depen-dence inference using forward time series only.

For future work, we will investigate other combination strategies for forward backward approach, other than simply averaging the coefficients. the performance. We will also examine more general types of time series models where the forward backward approach is applicable.
We thank Aram Galstyan, Greg Ver Steeg for discussions, and David C. Kale for helpful suggestions. This research was sponsored by the NSF research grants IIS-1254206, Okawa Foundation Research Award, and U.S. Defense Advanced Research Projects Agency (DARPA) under Social Media in Strategic Communication (SMISC) program, Agreement Number W911NF-12-1-0034. The views and conclusions are those of the authors and should not be interpreted as repre-senting the official policies of the funding agency or the U.S. Government.
 [1] A. Anagnostopoulos, R. Kumar, and M. Mahdian.
 [2] A. Arnold, Y. Liu, and N. Abe. Temporal causal [3] I. Asimakopoulos, D. Ayling, and W. M. Mahmood. [4] M. T. Bahadori and Y. Liu. An examination of [5] M. T. Bahadori, Y. Liu, and E. P. Xing. Fast [6] L. Breiman. Bagging predictors. Mach. Learning , [7] A. Brovelli, M. Ding, A. Ledberg, Y. Chen, [8] M. Cha, H. Haddadi, F. Benevenuto, and P. K.
 [9] D. Chaussabel, R. T. Semnani, M. A. McDowell, [10] C. Cortes and M. Mohri. Confidence intervals for the [11] C. S. Detweiler, D. B. Cunanan, and S. Falkow. Host [12] E. S. Edgington and P. Onghena. Randomization [13] M. Eichler. Graphical modelling of multivariate time [14] C. W. Granger. Investigating causal relations by [15] C. W. Granger. Testing for causality: A personal [16] C. Hiemstra and J. D. Jones. Testing for Linear and [17] A. Hyv  X  arinen, K. Zhang, S. Shimizu, P. O. Hoyer, and [18] D. Kempe, J. Kleinberg, and  X  E. Tardos. Maximizing [19] Y.-H. Kim, H. H. Permuter, and T. Weissman.
 [20] H. L  X  utkepohl. New Introduction to Multiple Time [21] C. D. Manning and H. Sch  X  utze. Foundations of [22] D. Marinazzo, M. Pellicoro, and S. Stramaglia. Kernel [23] C. D. Panchenko and Valentyn. Modified [24] J. Pearl. Causality: Models, Reasning and Inference . [25] J. M. Robins, R. Scheines, P. Spirtes, and [26] T. Schreiber. Measuring Information Transfer. Phys. [27] S. Song and P. J. Bickel. Large Vector Auto [28] P. Spirtes, C. Glymour, and R. Scheines. Causation, [29] P. A. Vald  X es-Sosa, J. M. S  X anchez-Bornot,
