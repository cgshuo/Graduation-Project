 In the information explosion era, social network not only contains relationships, but also much unstructured information such as context. Furthermore, how to effectively dig out latent topics and internal semantic structures from social network is an im-portant research issue. Early work on microblogs mainly focused on user relationship and community structure. [1] studied the topological and geographical properties of Twitter. Others work such as [2] studied user behaviors and geographic growth pat-terns of Twitter. Only little research on cont ent analysis of microblog was proposed recently. [3] was mainly based on traditiona l text mining algorithms. [4] proposed MB-LDA by overall considering contactor relevance relation and document relevance re-lation of microblogs. In this paper, we propose a novel probabilistic generative model based on hLDA, called constrained-hLDA, which focuses on both text content and topic hierarchy.

Previous work on microblog text was mainly based on LDA. To our best knowledge, there was little research on the topic hierarchy on microblog text. However, hierarchical topic modeling is able to obtain the relations between topics. [5] proposed an unsuper-vised hierarchical topic model, called hierarchical Latent Dirichlet Allocation (hLDA), to detect automatically new topics in the dat a space after fixing the level. Based on the stick-breaking process, [6] proposed the fully nonparametric hLDA without fixing the level. After that, some modifications of hLDA were proposed [7 X 9]. Given a parameter L indicating the depth of the hierarchy, hLDA makes use of nested Chinese Restau-rant Process(nCRP) to automatically find u seful sets of topics and learn to organize the topics according to a hierarchy in which mo re abstract topics are near the root of the hierarchy and more concrete topics are near the leaves. However, the traditional hLDA is an unsupervised learning which does not incorporate any prior knowledge. In this pa-per, we attempt to extract some prior knowledge and incorporate them to the sampling process.

The rest of the paper is organized as follo ws. Section 2 introduces the previous work related to this paper. Section 3 describes the hLDA briefly. Section 4 introduces the novel model constrained-hLDA. The experiment is introduced in Section 5, which is followed by the conclusion in Section 6. There have been many variations of probabilistic topic models, which was first in-troduced by [10]. The probabilistic topic model is based on the idea that documents are generated by mixtures of topics which is a multinomial distribution over words. One limitation of Hofmann X  X  model is that it is not clear how the mixing proportions for topics in a document are generated. To overcome this limitation, [11] propose La-tent Dirichlet Allocation(LDA). In LDA, the topic proportion of every document is a K -dimensional hidden variable randomly drawn from the same Dirichlet distribution, where K is the number of topics. Thus, generative semantics of LDA are complete, and LDA is regarded as the most popular appr oach for building t opic models in recent years[12 X 16].

LDA is a useful algorithm for topic modeling, but it fails to draw the relationship between one topic and another and fails to indicate the level of abstract for a topic. To address this problem, many models have been proposed to build the relations, such as hierarchical LDA(hLDA) [5, 6], Hierarchi cal Dirichlet processes(HDP) [17], Pachinko Allocation Model(PAM) [18] and Hierarchical PAM(HPAM) [19] etc. These models extend the  X  X lat X  topic models into hierarchical versions for extracting hierarchies of topics from text collections. [6] proposed the most up-to-date hLDA model, which is a fully nonparametric model. It simultaneous ly learns the structure of a topic hierar-chy and the topics that are contained within that hierarchy. Furthermore, it can also learn the most appropriate levels and hyper-parameters although it is time-consuming. In recent years, some modifications of hLDA has also been proposed. [7] proposed a supervised hierarchical topic model, called h ierarchical Labeled Latent Dirichlet Al-location(hLLDA), which uses hierarchical labels to automatically build correspond-ing topic for each label. [8] propose an unsupe rvised hierarchical topic model, called Semi-Supervised Hierarchical Latent Di richlet Allocation (SSHLDA), which can not only make use of the information from the hierarchy of observed labels, but also can explore new latent topics in the data sp ace. Although our work has some slight resem-blance with their work, there still exist several important differences: 1. Our constrained-hLDA mainly focuses on the text of microblogs or reviews without 2. The prior knowledge is extracted automatically from the corpus instead of first-3. The constraints are alterable by different parameters. The nested Chinese restaurant process (nCRP) is a distribution over hierarchical partitions[5, 6]. It generalizes the Chinese restaurant process (CRP), which is a sin-gle parameter distribution over partitions of integers. It has been used to represent the uncertainty over the number of components in a mixture model. The generative process is as follow: 1. There are N customers entering the restaurant in sequence, which is labeled with 2. First customer sits at the first table. 3. The nth customer sit at: 4. After N customers have sat down, their seating plan describes a partition of N In the nested CRP, suppose there are an infinite number of infinite-table Chinese restau-rants in a city. One restaurant is identified as the root restaurant and its every table has a card with the name that refers to another res taurant. This structure repeats infinitely many times, thus, the restaurants in the city are organized into an infinitely branched, infinitely-deep tree. When a tourist arrives at the city, he selects a table, which is as-sociated with a restaurant at next level, using the CRP distribution at each level. After M tourists have visited in this city, the path collection, which they selected, describes a random subtree of the infinite tree.

Based on identifying documents with the paths generated by the nCRP, the hierar-chical topic model, which consists of an infinite tree, is defined. Each node in the tree is associated with a topic, which is a probability distribution across words. Each document is assumed to be generated by a mixture of topics on a path from the root to a leaf. For each token in the document, one picks a topi c randomly according to the distribution, and draws a word from the multinomial distribution of that topic. To infer the topic hi-erarchy, the per-document paths c d and the per-word level allocation to topics in those paths z d,n must be sampled. Then we will introduce the process briefly.

For the path sampling, the path associ ated with each document conditioned on all other paths and the observed words need to be sampled. Assume the depth is finite and let T denotes it, the posterior distribution of path c d is as denote: In Equation 1, two factors influence the pr obability that a document belongs to a path. The first factor is the prior on paths implied by the nested CRP. The second factor is the probability of observing the words in the document given a particular choice of path with equation organized as follows: p ( w d | c , w  X  d , z , X  )= by c d,t , not including those in the current document, V denotes the total vocabulary size, and  X  (  X  ) is the standard gamma function. When c contains a previously unvisited
After selecting the current path assignments, the level allocation variable z d,n for word n in document d conditioned on the current values of all other variables need to be sampled as: where z  X  ( d,n ) and w  X  ( d,n ) are the vectors of level allocations and observed words leaving out z d,n and w d,n , z d,  X  n denotes the level allocations in document d , leaving In this section, we will introduce a constrai ned hierarchical topic model, i.e., the con-strained herarchical Latent Dirichlet Allocation(constrained-hLDA). As we have known, similar to LDA, the original hLDA is a purely unsupervised model without consider-ing any pre-existing knowledge. However, in semi-supervised clustering framework, the prior knowledge can help clustering algorithm produce more meaningful clusters. In our algorithm, the extracted prior knowledge can help to pre-establish a part of the infinite tree structure. In this section, we will give an introduction to the constraint extraction and the proposed constrained-hLDA which can use pre-existing knowledge expressed as constraints. 4.1 Path Constraints Extraction To construct constrained hierarchical t opic model, we adopt hLDA and incorporate the constraints from the pre-existing knowledge. Compared with hLDA, constrained-hLDA has one more input for improving path sampling. The input is a set of constrained indi-{ w i, 1 ,w i, 2 ,... } , which corresponds to a node in constrained-hLDA, consists of several high correlation words. In our work, these words, which can indicate the correlation of a path and a document, are called constrained indicators . These corresponding nodes, which are pre-allocated several constrained indicators, are called constrained nodes .
The intuition of above idea is very simple and easy to follow. In this paper, we just attempt to solve it based on a correlation appr oach, more novel and efficient method will be further explored in the future. Algorithm 1 summarizes the main steps of constraints extraction. First, the FP-tree algorithm is adopted to extract the one-dimension frequent items according to the minimum support and maximum support(Line 1). The maximum support is used to filter some common words in order to make sure that the occurrences of each candidate are close, therefore, there w ill not be hierarchical relationship of these frequent items. Next, for each fis i , it is added to an empty collection CS i first, and then the correlation of fis i with other items is computed. If the correlation of fis i and fis j is greater than the given threshold, it is assumed that fis i and fis j should constitute a must-link and fis j is appended to CS i (Line 2 -Line 9). In this work, the correlation is calculated by overlap as follows: where P A &amp; B is the co-occurrence of word A and word B , P A is the occurrence of word A ,and P B is the occurrence of word B . The range of equation 4 is between [0 , 1 . 0] ,so the threshold can be easily given for different corpora. In the end, we delete the same set only retaining one from CS (Line 10 -Line 14). Based on Algorithm 1, the prior set CS , each of which contains several high corre lation indicators, can be acquired. In this paper, the threshold of overlap is set as 0.4, the maximum support is set as five times as minimum support, all these parameters are e stimated number. Additionally, we attempt to utilize different minimum supports to obtain d ifferent set so that different experiment results can be made for sure.
 Algorithm 1. Constraints extraction 4.2 Path Constraints Incorporation To integrate constrained indicators into hLDA, we extend the nCRP to a more realistic situation. Suppose the root restaurant has infinite tables, some tables have a menu con-taining some special dishes. Suppose N tourists arrive at the city, some of them have a list of special dishes that they want to taste. When a tourist enters into the root restau-rant, if he has a list, he will select a table whose menu contains the special dishes of his list. Otherwise, according to his willingness to taste the special dishes, he will use CRP equation to select a table among those tables without menus. To keep it simple in this paper, we assume only the r oot restaurant has menus.

In constrained-hLDA model, each constrained set CS i corresponds to a menu and each constrained indicator corresponds to a sp ecial dish. Then the documents in a cor-pus are assumed drawn from the following generative process: 1. For each table k  X  T in the infinite tree 2. For each document, d  X  X  1 , 2 ,...,D } As the example shown in Figure 1, we assume that the height of the desired tree is L = 3 , and the constrained-topics extracted are { A 2 ,A 3 } . The constrained topics amount to the tables containing menu, each of which is p re-defined as the cons trained indicators coming from a CS i , and the constrained indicators amount to special dishes. In our work, because microblogs are mainly short tex ts, the maximum level is truncated to 3. Furthermore, it is notable that the constraints can be extended to the deeper level. For example, the constrained set can be extract ed again from the documents which pass by the node A 2 , and then the constrained indicators set corresponding to A 2 can be drawn from these documents.

In constrained-hLDA, the idea of incorporating prior knowledge derives from [20], and the most important process is incorporating the constraints to the path sampling process according to the probabilities calculated using Equation 5: where  X  ( w d , c d ) is an indicator function, which indicates whether the nodes from c d contain the same constrained indicator with that of w d :If w d contains such node,  X  ( w d , c d )=1 ,otherwise,  X  ( w d , c d )=0 . The hard constraint indicator can be re-laxed by  X  ,Let 0  X   X   X  1 be the strength of our constraint, where  X  =1 recovers a hard constraint,  X  =0 recovers unconstrained sampling and 0 &lt; X  &lt; 1 recovers a soft constraint sampling. 4.3 Level Constraints Extraction and Incorporation After revising the path sampling process and selecting a particular path, some prior knowledge can also integrate into level sampling process. As we have known, hLDA can discover the function words in root topic, furthermore, these words have no effect on the document interpretability and often appear in many documents. Therefore, we hope to improve level sampling process by pre-discriminating some function words and non-function words. In our work, the functio n words are discriminated according to the Part-Of-Speech(POS) and th e term frequency in each document. Algorithm 2 describes our purpose, where RD w denotes the ratio of the documents containing the word w in the current corpus. For each word, if its RD w is greater than the given threshold Algorithm 2 Constraints extraction threshold upper and it does not belong to the pre-defined POS set S POS , it would be likely to be a function word that is allocated to root node directly(Line 2 -Line 3). If its RD w is less than the given threshold threshold below and it belongs to the pre-defined POS set S POS at the same time, it would be likely to be a non-function word without being allocated to root node(Line 4 -Line 5). Finally, we sample the level according to these prior knowledge (Line 9). In this paper, threshold upper and threshold below are set to 0 . 02 and 0 . 005 , and the pre-defined POS set S POS is set as noun, adjective and verb. 5.1 Data Sets experiment data from sina microblog 1 by ourselves. It is generally known that Ya X  X n Earthquake 2 on 20th, April, 2013 was a catastrophe shocking everyone, which is exactly an ideal hot issue for research. We crawled 19811 microblog users all coming from Ya X  X n, and also crawled their posted microblogs from 8am 20th April 2013 to 8am 25th April 2013. There are 58476 original microblogs released by these users, each of which contains several sentences. As time passed by, people X  X  concern level on this issue would decline gradually, therefore, we use the data on a daily level for further analysis. Table 1 depicts the data sets for evaluation. The designed experiments and sampling results can also be referred in [6]. For hLDA and constrained-hLDA, there is a restriction that documents can only follow a single path in the tree. In order to make each sentence of a document can follow differe nt paths, we split texts into sentences, such a change can get a remarkable improvement for hLDA and constrained-hLDA in the corpus of microblogs. In our experiment, hLDA algorithm is completed with Java codes by ourselves according to [6]. In our constrained-hLDA, the stick-breaking procedures are truncated at three levels to facilitate visualization of results. The topic Dirichlet hyper-parameters are fixed at  X  = { 1 . 0 , 1 . 0 , 1 . 0 } , The nested CRP parameter  X  is fixed at 0 . 5 , the GEM parameters are fixed at = 100 and m =0 . 25 .
 5.2 Hierarchy Topic Discovery Figure 2 depicts the hierarchical structure of cluster results. It is natural to conclude that the constrained-hLDA can well discover the underlying hierarchical structure of the content of micorblogs, and each topic and its child node mainly relate to pre-allocated the constrained indicator, which is the underlined word. For example, there are three latent topic of second level is a meaningless topic, which is hard to summarize the inter-pretability of these topics. This phenomenon illustrates that the irrelevant information in microblog context that can be filtered well by our algorithm. 5.3 ComparisonwithhLDA In this section, we compare the experimental results with hLDA, and the per-document distribution over levels is truncated at three levels. In order to evaluate our model, we use predictive held-out likelihood as a measure of performance to compare the two approaches quantitatively. The procedure is to divide the corpus into D 1 observed doc-uments and D 2 held-out documents, and approximate the conditional probability of the held-out set given the training set: For this evaluation method, more details can be found in [6].

Figure 3 depicts the performance of constrained-HLDA on several data sets by dif-ferent minimum support. Table 2 depicts the best performance of different constraints on several data sets. According to these experimental results, we can conclude that: (1) Both path sampling constraints and level sampling constraints can improve hLDA. (2) The smaller minimum support can obtain more constrained indicators so that it can achieve better log likelihood. (3) The likelihood of constrained-hLDA is better than the likelihood of hLDA, but for different corpus, the degree of improvement is different. When the topic of corpus is more concentrat ed, the improvement seems to be better.
In order to avoid interference from the values of hyperparameters, as with [6] X  X ork, we also interleave Metropolis-Hastings (MH) steps between iterations of the Gibbs sampler to obtain new values of m , ,  X  and  X  . Table 3 present the results by sampling the hyperparameters in the same case, from which we can see that constrained-hLDA still performs better than hLDA.
 This paper improves the popular topic modeling method hLDA by considering existing knowledge in the form of path sampling constraints and level sampling constraints. In the experiment, the proposed constrained-hLDA outperforms hLDA by a large margin, showing that constraints as prior knowledge can help unsupervised topic modeling. Moreover, this paper also proposes the extraction method for two types of constraints automatically. Experimental results show that their qualities are relatively higher than that of unsupervised one.
 Acknowledgments. This work is supported by National Natural Science Foundation of China (Grant No: 61175110) and National Basic Research Program of China (973 Program, Grant No: 2012CB316305).

