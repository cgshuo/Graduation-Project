 Building intelligent systems that are capable of extracting high-level representations from high-dime nsional data lies at the core of solving many AI related ta sks, including visual object or pattern recognition, speech perception, and language understanding. Theoretical and biological arguments strongly suggest that building such systems requires deep architectures that involve many layers of nonlinear processing. Many existing learning algorithms use shallow architectures, including neural networks with only one hidden la yer, support vector machines, kernel logistic regression, a nd many others. The internal representations learned by such systems are necessarily simple and are incapable of extracting some types of complex structure from high-dimensional input. In th e past few years, researchers across many different communitie s, from applied statistics to engineering, computer science, and neuroscience, have proposed several deep (hierarchical) models that are capable of extracting meaningful, high-level representa tions. An important property of these models is that they can extract complex statistical dependencies from data and e fficiently learn high-level representations by re-using and co mbining intermediate concepts, allowing these models to generali ze well across a wide variety of tasks. The learned high-level repr esentations have been shown to give state-of-the-art results in many challenging learning problems and have been successfully applied in a wide variety of application domains, including visual object recognition, information retrieval, natural language processing, and speech perception. A few notable examples of such models include Deep Belief Networks, Deep Boltzmann Machines, Deep Autoencoders, and sparse coding-based methods. The goal of the tutorial is to introduce the recent developments of various deep learning methods to the KDD community. The core focus will be placed on algorithms that can learn multi-layer hierarchies of representations, emphasizing their applications in information retrieval, object recognition, and speech perception. I.2.6 [ Artificial Intelligence ]: Learning X  Connectionism and neural nets Deep Learning; Graphical Models; Boltzmann Machines; Autoencoders; Sparse Coding Ruslan Salakhutdinov received his PhD in computer science from the University of Toronto in 2009. After spending two post-doctoral years at the Massachusetts Institute of Technology Artificial Intelligence Lab, he joined the University of Toronto as an Assistant Professor in the De partment of Computer Science and Department of Statistics. Dr. Salakhutdinov X  X  primary interests lie in statistical machine learning, Deep Learning, probabilistic graphical models, and large-scale optimization. He is an action editor of the Journal of Machine Learning Research and served on the senior programme committee of several learning conferences including NIPS and ICML. He is the recipient of the Early Researcher Award, Connaught New Researcher Award, Alfred P. Sloan Research Fellowship, Microsoft Research Faculty Fellowship, Google Faculty Resear ch Award, and a is Fellow of the Canadian Institute for Advanced Research. A major focus on Dr. Salakhutdinov X  X  work is Deep Learning, an active research area that stems, in large part, from his 2006 Science article co-authored with Geoffrey Hinton. This work developed a formulation for stacked Restricted Boltzmann Machines (RBMs), showing an algorithm capable of learning large-scale multi-layer networks. Dr. Salakhutdinov subse quently pioneered a new class of deep generative models, ca lled Deep Boltzmann Machines (DBMs). These are probabilistic gr aphical models that contain multiple layers of latent variables. Each nonlinear layer captures progressively more complex pa tterns of data, which is a promising way of solving visu al object recognition, language understanding, and speech perception problems. Dr. Salakhutdinov X  X  contributions to Deep Learning have already received over 5000 citations according to Google Scholar, and have been applied broadly in speech, language, and image analysis. 
