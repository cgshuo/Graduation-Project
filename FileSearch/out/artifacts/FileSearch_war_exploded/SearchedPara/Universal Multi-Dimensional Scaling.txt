 In this paper, we propose a unified algorithmic framework for solving many known variants of MDS. Our algorithm is a simple iterative scheme with guaranteed convergence, and is modular ; by changing the internals of a single subroutine in the algorithm, we can switch cost functions and target spaces easily. In addi-tion to the formal guarantees of convergence, our algorithms are accurate; in most cases, they converge to better quality solutions than existing methods in comparable time. Moreover, they have sets. We expect that this framework will be useful for a number of MDS variants that have not yet been studied.

Our framework extends to embedding high-dimensional points lying on a sphere to points on a lower dimensional sphere, pre-serving geodesic distances. As a complement to this result, we also extend the Johnson-Lindenstrauss Lemma to this spherical setting, by showing that projecting to a random O (( 1 /" 2 ) dimensional sphere causes only an " -distortion in the geodesic distances.
 H.2.8 [ Database applications ] : Data mining; F.2.2 [ Non-numerical algorithms and problems ] : Geometrical algorithms Algorithms,Theory Multi-dimensional scaling, dimensionality reduction.
Multidimensional scaling (MDS) [ 26, 11, 3 ] is a widely used method for embedding a general distance matrix into a low di- X  This research partially supported by NSF IIS-0712764, NSF CCF-0953066 and a subaward to the University of Utah under NSF award 0937060 to the Computing Research Association permission and/or a fee.
 KDD X 10, July 25 X 28, 2010, Washington, DC, USA.
 Copyright 2010 ACM 978-1-4503-0055-1/10/07 ...$10.00. mensional Euclidean space, used both as a preprocessing step right. MDS has been studied and used in psychology since the 1930s [ 39, 36, 25 ] to help visualize and analyze data sets where the only input is a distance matrix. More recently MDS has be-come a standard dimensionality reduction and embedding tech-nique to manage the complexity of dealing with large high di-mensional data sets [ 9, 10, 34, 7 ] .

In general, the problem of embedding an arbitrary distance matrix into a fixed dimensional Euclidean space with minimum error is nonconvex (because of the dimensionality constraint). Thus, in addition to the standard formulation [ 13 ] , many vari-ants of MDS have been proposed, based on changing the under-lying error function [ 39, 9 ] . There are also applications where the target space, rather than being a Euclidean space, is a man-ifold (e.g. a low dimensional sphere), and various heuristics for MDS in this setting have also been proposed [ 14, 7 ] .
Each such variant is typically addressed by a different heuris-tic, including majorization, the singular value decomposition, semidefinite programming, subgradient methods, and standard Lagrange-multipler-based methods (in both primal and dual set-tings). Some of these heuristics are efficient, and others are not; in general, every new variant of MDS seems to require different ideas for efficient heuristics.
In this paper, we present a unified algorithmic framework for solving many variants of MDS. Our approach is based on an it-erative local improvement method, and can be summarized as follows:  X  X ick a point and move it so that the cost function is locally optimal. Repeat this process until convergence. X  The im-provement step reduces to a well-studied and efficient family of iterative minimization techniques, where the specific algorithm depends on the variant of MDS.

A central result of this paper is a single general convergence result for all variants of MDS that we examine. This single re-sult is a direct consequence of the way in which we break down the general problem into an iterative algorithm combined with a point-wise optimization scheme. Our approach is generic, ef-ficient, and simple. The high level framework can be written in 10-12 lines of MATLAB code, with individual function-specific subroutines needing only a few more lines each. Further, our approach compares well with the best methods for all the vari-ants of MDS. In each case our method is consistently either the best performer or is close to the best, regardless of the data pro-file or cost function used, while other approaches have much more variable performance. A useful feature of our method is that it is parameter-free, requiring no tuning parameters or La-grange multipliers in order to perform at its best. Finally, our method has a small memory footprint, allowing it to scale well for large data sets.

An important application of our approach is the problem of performing spherical MDS. Spherical MDS is the problem of em-bedding a matrix of distances onto a (low-dimensional) sphere. Spherical MDS has applications in texture mapping and image analysis [ 7 ] , and is a generalization of the spherical dimension-ality reduction problem, where the goal is to map points from a high dimensional sphere onto a low-dimensional sphere. This latter problem is closely related to dimensionality reduction for finite dimensional distributions. A well-known isometric em-bedding takes a distribution represented as a point on the d -dimensional simplex to the d -dimensional sphere while preserv-ing the Hellinger distance between distributions. A spherical di-mensionality reduction result is an important step to represent-ing high dimensional distributions in a lower-dimensional space of distributions, and will have considerable impact in domains that represent data natively as histograms or distributions, such as in document processing [ 32, 21, 2 ] , image analysis [ and speech recognition [ 19 ] .

Our above framework applies directly to this setting, where for the local improvement step we adapt a technique first devel-oped by Karcher for finding geodesic means on a manifold. In addition, we prove a Johnson-Lindenstrauss-type result for the sphere; namely, that n points lying on a d -dimensional sphere can be embedded on a O (( 1 /" 2 ) log n ) -dimensional sphere while approximately preserving the geodesic distances between pairs of points, that is, no distance changes by more than a relative ( 1 + " ) -factor. This latter result can be seen as complementary to the local improvement scheme; the formal embedding result guarantees the error while being forced to use log n dimensions, while the local improvement strategy generates a mapping into any k dimensional hypersphere but provides no formal guaran-tees on the error.

The main contributions of this paper can be summarized as follows:
Multidimensional scaling is a family of methods for embed-ding a distance matrix into a low-dimensional Euclidean space. There is a general taxonomy of MDS methods [ 11 ] ; in this paper we will focus primarily the metric and generalized MDS prob-lems.

The traditional formulation of MDS [ 26 ] assumes that the dis-tance matrix D arises from points in some d -dimensional Eu-clidean space. Under this assumption, a simple transforma-tion takes D to a matrix of similarities S , where s i j = These similarities also arise from many psychology data sets di-rectly [ 36, 39 ] . The problem then reduces to finding a set of points X in k -dimensional space such that XX T approximates S . This can be done optimally using the top k singular values and vectors from the singular value decomposition of S .

A more general approach called SMACOF that drops the Eu-clidean assumption uses a technique known as stress majoriza-tion [ 30, 13, 14 ] . It has been adapted to many other MDS vari-surfaces and specifically spheres [ 14 ] .
 Since the sum-of-squares error metric is sensitive to outliers, Cayton and Dasgupta [ 9 ] proposed a robust variant based on an ` subgradient heuristic, followed by a singular value decomposi-tion to enforce the rank constraints.

Many techniques have been proposed for performing spher-ical MDS. Among them are majorization methods ( [ 33 SMACOF-Q [ 14 ] ), a multiresolution approach due to Elad, Keller and Kimmel [ 16 ] and an approach based on computing the clas-sical MDS and renormalizing [ 34 ] .

A complementary line of work in dimensionality reduction computing an average error), and asks for the minimum dimen-sion a data set can be embedded in while maintaining this er-ror. The Johnson-Lindenstrauss Lemma [ 22 ] states that any col-lection of n points in a Euclidean space can be embedded in a O (( 1 /" 2 ) log n ) dimensional Euclidean space that preserves all distances within a relative error of " . If the points instead de-fine an abstract metric space, then the best possible result is an embedding into O ( log n ) -dimensional Euclidean space that pre-of the different methods for dimensionality reduction is beyond the scope of this paper -the reader is directed to the survey by Indyk and Matousek for more information [ 20 ] .

The Johnson-Lindenstrauss lemma can be extended to data lying on manifolds. Any manifold M with  X  X inearization dimen-sion X  k (a measure of its complexity) can be embedded into a O (( 1 /" 2 ) k log ( kn )) dimensional space so that all pairwise Eu-clidean distances between points on M are distorted by at most linearization dimension O ( k ) , so this bound applies directly for preserving the chordal (i.e Euclidean) distance between points on a sphere. The geodesic distance between points on a sphere can be interpreted as the angle between the points in radians, and a result by Magen [ 29 ] show that O (( 1 /" 2 ) log n sions preserve angles to within a relative factor of 1 + within a relative factor of 1 + " ).
Let D =( d i j ) be an n  X  n matrix representing distances be-assume that D is symmetric (i.e d i j = d ji ), although our method does not formally require this. The multidimensional scaling problem takes as input Y , D and k , and asks for a mapping  X  : Y  X  X from Y to a set of points X in a k -dimensional space T such that the difference between the original and resulting distances is minimized.

There are many different ways to measure the difference be-tween the sets of distances, and these can be captured by the following general function: where Err measures the discrepancy between the source and tar-get distances, and f denotes the function that measures distance in the target space.
It will be convenient to split the expression into component terms. We define which allows us to write C ( X , D )= P i C i ( X , D , x
The actual measure studied by Cayton and Dasgupta [ 9 ] is not rMDS. It is a variant which takes the absolute difference of the squared distance matrices. We call this measure r 2 MDS. Also, classical MDS does not appear in this list since it tries to min-imize the error between similarities rather than distances. We refer to this measure as cMDS.
We now present our algorithm P LACE C ENTER ( X , D ) that finds a mapping Y  X  X minimizing C ( X , D ) . For now we assume that we are given an initial embedding X 1  X  R k to seed our algo-rithm. Our experiments indicate the SVD-based approach [ is almost always the optimal way to seed the algorithm, and we use it unless specifically indicated otherwise.
 Algorithm 1 P LACE C ENTER (D)
Run any MDS strategy to obtain initial seed X . repeat until (  X   X  C ( X , D ) &lt; t ) { for a fixed threshold t Return X .

The algorithm operates by employing a technique from the expressed as a sum of costs for each point x i , and so in each step of the inner loop we find the best placement for x i keeping all other points fixed, using the algorithm P LACE A key insight driving our approach is that P LACE i ( X , D
Figure 1: A geometric interpretation of the error term. implemented either iteratively or exactly for a wide class of dis-tance functions. The process terminates when over all i , invok-ing P LACE i ( X , D ) does not reduce the cost C ( X , D P LACE i ( X , D ) will take O ( n ) time and computing C ( O ( n 2 ) time.
The routine P LACE i ( X , D ) is the heart of our algorithm. This routine finds the optimal placement of a fixed point x i with re-spect to the cost function C i ( X , D , x i )= P j Err ( Set r j = d i j . Then the optimal placement of x i is given by the point x  X  minimizing the function ignore their presence in the summation for ease of notation.
There is a natural geometric interpretation of g ( x ) , illustrated in Figure 1. Consider a sphere around the point x j of radius r Let  X  x j be the point on this sphere that intersects the ray from x towards x . Then the distance f ( x ,  X  x j )= | f ( x , x we can rewrite g ( x ) as
This function is well-known in combinatorial optimization as the min-sum problem. For Err (  X  )=  X  2 , g ( x ) finds the point minimizing the sum-of-squared distances from a collection of fixed points (the 1-mean), which is the centroid x  X  = 1 the sum of distances from a collection of fixed points. Although there is no closed form expression for the 1-median, there are numerous algorithms for solving this problem both exactly is sufficiently larger than 2, then convergent methods may not exist [ 6 ] .

While g ( x ) can be minimized optimally for error functions Err of the solution x  X  , which is itself unknown! This motivates an alternating optimization procedure, where the current iterate x min-sum problem to solve for the next value of x . Algorithm 2 P LACE i ( X , D ) repeat until (  X   X  g ( x i ) &lt; t ) { for a fixed threshold t }
Return x i . Up to this point, the description of P LACE C ENTER and P has been generic, requiring no specification of Err and f . In fact, all the domain-specificity of the method appears in R ECENTER which solves the min-sum problem. We now demonstrate how different implementations of R ECENTER allow us to solve the dif-ferent variants of MDS discussed above. Recall from Section 3 that the fMDS problem is defined by Err (  X  )=  X  2 and f ( x , x 0 )= k x  X  x 0 k  X  x tained at x  X  =( 1 / n ) P j  X  x j . Thus, R ECENTER (
The robust MDS problem rMDS is defined by Err (  X  )= g ( x ) yields the famous Fermat-Weber problem, or the 1-median problem as it is commonly known. An exact iterative algorithm for solving this problem was given by Weiszfeld [ 38 ] , and works as follows. At each step of P LACE i the value x i is updated by This algorithm is guaranteed to converge to the optimal solu-tion [ 27, 31 ] , and in most settings converges quadratically the same lines as the Weiszfeld algorithm can be used to mini-range of values for p . It is also known that for p sufficiently larger than 2, this iterative scheme may not converge.
We also can tune P LACE C ENTER to the r 2 MDS problem (using proofs (below) do not hold in this case, the algorithm works well in practice.
Spherical MDS poses special challenges for the implementa-tion of R ECENTER . Firstly, it is no longer obvious what the def-inition of  X  x j should be, since the  X  X pheres X  surrounding points must also lie on the sphere. Secondly, consider the case where Err (  X  )=  X  2 , and f ( x , x 0 ) is given by geodesic distance on the sphere. Unlike in the case of R k , we no longer can solve for the minimizer of g ( x ) by computing the centroid of the given points, because this centroid will not in general lie on the sphere, and even computing the centroid followed by a projection onto the sphere will not guarantee optimality.

The first problem can be solved easily. Rather than draw spheres around each x j , we draw geodesic spheres , which are the set of points at a fixed geodesic distance from x j . On the sphere, this set of points can be easily described as the intersection of stead of computing the intersection of this geodesic sphere with the ray from x j towards the current estimate of x i , we compute the intersection with a geodesic ray from x j towards x i
The second problem can be addressed by prior work on com-puting min-sums on manifolds. Karcher [ 23 ] proposed an it-erative scheme for the geodesic sum-of-squares problem that always converges as long as the points do not span the entire points defined on more general Riemannian manifolds satisfying certain technical conditions. It runs in O ( n ) time per iteration. longer works. For this case, we make use of a Weiszfeld-like adaption [ 18 ] that again works on general Riemannian mani-folds, and on the sphere in particular. Like the Weiszfeld scheme, this approach takes O ( n ) time per iteration.
Here we prove that each step of P LACE C ENTER converges as cost functions. Convergence is defined with respect to a cost function  X  , so that an algorithm converges if at each step creases until the algorithm terminates.
 C (  X  , D ) .
 P
LACE i ( X , D ) . Let  X  X = { x 1 , . . . , x i  X  1 ,  X  x can argue
C ( X , D )  X  C (  X  X , D ) The last line follows because X and  X  X only differ at x otherwise decrease.
 Theorem 4.2. If each call x i  X  R ECENTER (  X  X ) reduces then P LACE i ( X , D , x i ) converges with respect to C Proof. First we can rewrite Since Err ( f ( x i ,  X  x j )) measures the distance to the sphere  X  choosing x 0 i to minimize (or decrease) P n j = 1 Err ( f decrease the sum of distances to each point  X  x j on each sphere  X  Now let  X  x 0 j be the closest point to x 0 i on  X  j . Hence Err Err ( f ( x 0 terminates.
P LACE C ENTER ( D ) takes an n  X  n distance matrix as input, but This means that although the input complexity is O working memory footprint of the algorithm is only O ( n ) is a significant advantage of P LACE C ENTER ( D ) over many exist-ing MDS methods that require the entire matrix D to be stored beyond the point where other methods start to fail.
In this section we evaluate the performance of P LACE C ENTER (PC). Since PC generalizes to many different cost functions, we compare it with the best known algorithm for each cost func-tion, if one exists. For the fMDS problem the leading algorithm is SMACOF [ 14 ] ; for the r 2 MDS problem the leading algorithm is by Cayton and Dasgupta (CD) [ 9 ] . We know of no previous scalable algorithm designed for rMDS. We note that the Cayton-Dasgupta algorithm REE does not exactly solve the r 2 MDS prob-lem. Instead, it takes a non-Euclidean distance matrix and finds a Euclidean distance matrix that minimizes the error without any rank restrictions. Thus, as suggested by the authors properly compare the algorithms, we let CD refer to running REE and then projecting the result to a k -dimensional subspace us-ing the SVD technique [ 39 ] (our plots show this projection after each step). With regards to each of these Euclidean measures we compare our algorithm with SMACOF and CD. We also compare with the popular SVD-based method [ 39 ] , which solves the re-lated cMDS problem based on similarities, by seeding all three it-erative techniques with the results of the closed-form SVD-based solution.

Then we consider the family of spherical MDS problems { c,g { 1,2 } -sMDS. We compare against a version of SMACOF-Q [ 14 that is designed for data restricted to a low dimensional sphere, specifically for the c-2-SMDS measure. We compare this algo-rithm to ours under the c-2-SMDS measure (for a fair compari-son with SMACOF-Q) and under the g-1-SMDS measure which is the most robust to noise.
 The subsections that follow focus on individual cost measures. We then discuss the overall behavior of our algorithm in Sec-tion 5.6.

Test inputs for the algorithms are generated as follows. We start with input consisting of a random point set with n = points in R d for d = 200, with the target space T = R k with k = 10. Many data sets in practice have much larger param-of the experiments because for larger values CD becomes pro-hibitively slow, and both SMACOF and CD run into memory Figure 2: rMDS: A typical behavior of the PC, CD and SMA-COF for rMDS problem. problems. In Section 5.5 we explore the performance of our algorithm on larger data sets (up to 50, 000 points). The data is generated to first lie on a k -dimensional subspace, and then (full-dimensional) Poisson noise is applied to all points up to a magnitude of 30% of the variation in any dimension. Finally, we construct the Euclidean distance matrix D which is provided as input to the algorithms.
 These data sets are Euclidean, but  X  X lose X  to k -dimensional. To examine the behavior of the algorithms on distance matri-ces that are non-Euclidean, we generate data as before in a k -dimensional subspace and generate the resulting distance ma-trix D . Then we perturb a fraction of the elements of D (rather than perturbing the points) with Poisson noise. The fraction per-turbed varies in the set ( 2%, 10%, 30%, 90% ) .

All algorithms were implemented in MATLAB. For SMACOF, we used the implementation provided by Bronstein [ 8 built our own implementation of SMACOF-Q around it. For all other algorithms, we used our own implementation 1 . In all cases, we compare performance in terms of the error function Err as a function of clock time.
Figure 2 shows the cost function Err associated with rMDS plotted with respect to runtime. P LACE C ENTER always reaches the best local minimum, partially because only P LACE C ENTER be adjusted for the rMDS problem. We also observe that the runtime is comparable to SMACOF and much faster than CD in order to get to the same Err value. Although SMACOF initially reaches a smaller cost that PC, it later converges to a larger cost because it optimizes a different cost function (fMDS).
We repeat this experiment in Figure 3 for different values of function of k . Note that PC performs even better for lower k in relation to CD. This is likely as a result of CD X  X  reliance on the SVD technique to reduce the dimension. At smaller k , the SVD technique has a tougher job to do, and optimizes the wrong metric. Also for k = 150 note that CD oscillates in its cost; this is again because the REE part finds a nearby Euclidean distance matrix which may be inherently very high dimensional and the
All of our code may be found at http://www.cs.utah.edu/ ~suresh/papers/smds/smds.html . SVD projection is very susceptible to changes in this matrix for such large k . We observe that SMACOF is the fastest method to reach a low cost, but does not converge to the lowest cost value. The reason it achieves a cost close to that of PC is that for this type of data the rMDS and fMDS cost functions are fairly similar.
In Figure 4 we evaluate the effect of changing the amount of noise added to the input distance matrix D , as described above. We consider two variants of the CD algorithm, one where it is seeded with an SVD-based seed (marked CD + SVD) and one where it is seeded with a random projection to a k -dimensional subspace (marked CD + rand). In both cases the plots show the results of the REE algorithm after SVD-type projections back to a k -dimensional space.

The CD + SVD technique consistently behaves poorly and does not improve with further iterations. This probably is because the REE component finds the closest Euclidean distance ma-trix which may correspond to points in a much high dimen-sional space, after which it is difficult for the SVD to help. The CD + rand approach does much better, likely because the ran-dom projection initializes the procedure in a reasonably low dimensional space so REE can find a relatively low dimension Euclidean distance matrix that is nearby. SMACOF is again the fastest algorithm, but with more noise, the difference between fMDS and rMDS is larger, and thus SMACOF converges to a con-figuration with much higher cost than PC. We reiterate that PC consistently converges to the lowest cost solution among the dif-ferent methods, and consistently is either the fastest or is com-parable to the fastest algorithm. We will see this trend repeated with other cost measures as well.
We next evaluate the algorithms PC, SMACOF, and CD under the fMDS distance measure. The results are very similar to the rMDS case except now both SMACOF and PC optimizing the cor-rect distance measure and converge to the same local minimum. SMACOF is still slightly faster that PC, but since they both run very fast, the difference is of the order of less than a second even in the very worst part of the cost / time tradeoff curve shown in Figure 5. Note that CD performs poorly under this cost function here except when k = 50. For smaller values of k , the SVD step does not optimize the correct distance and for larger k the REE clidean distance matrix, making the SVD projection very noisy.
For the fMDS measure, SMACOF and PC perform very simi-larly under different levels of noise, both converging to similar cost functions with SMACOF running a bit faster, as seen in Fig-ure 6. CD consistently runs slower and converges to a higher cost solution.
In this setting we would expect CD to perform consistently However, this is not always the case because CD requires the SVD step to generate a point set in R k . As seen in Figure 7 this becomes a problem when k is small ( k = 2, 10). For medium values of k , CD converges slightly faster than PC and sometimes to a slightly lower cost solution, but again for large k ( the REE part has trouble handling the amount of error and the solution cost oscillates. SMACOF is again consistently the fastest to converge, but unless k is very large (i.e. k = 150) then it converges to a significantly worse solution because the fMDS and r 2 MDS error functions are different. For the spherical MDS problem we compare PC against SMACOF-Q, an adaptation of SMACOF to restrict data points to a low-dimensional sphere, and a technique of Elad, Keller and Kim-mel [ 16 ] . It turns out that the Elad et.al. approach consistently performs poorly compared to both other techniques, and so we do not display it in our reported results. SMACOF-Q basically runs SMACOF on the original data set, but also adds one ad-ditional point p 0 at the center of the sphere. The distance d between any other point p i and p 0 is set to be 1 thus encour-aging all other points to be on a sphere, and this constraint is controlled by a weight factor  X  , a larger  X  implying a stronger emphasis on satisfying this constraint. Since the solution pro-duced via this procedure may not lie on the sphere, we normal-ize all points to the sphere after each step for a fair comparison.
Here we compare PC against SMACOF-Q in the g-1-SMDS (Figure 8) and the c-2-SMDS (Figure 9) problem. For g-1-SMDS, PC does not converge as quickly as SMACOF-Q with small  X  , but it reaches a better cost value. However, when SMACOF-Q is run with a larger  X  , then PC runs faster and reaches nearly the same cost value. For our input data, the solution has similar g-1-MDS and c-1-MDS cost. When we compare SMACOF-Q with PC under c-2-MDS (Figure 9) then for an optimal choice of  X  in SMACOF-Q, both PC and SMACOF-Q perform very similarly, converging to the same cost function and in about the same time. But for larger choices of  X  SMACOF-Q does much worse than PC. In both cases, it is possible to find a value of  X  that allows SMACOF-Q to match PC. However, this value is different for dif-ferent settings, and varies from input to input. The key obser-vation here is that since PC is parameter-free , it can be run re-gardless of the choice of input or cost function, and consistently performs well.
As mentioned in Section 4, the memory footprint of PC is lin-ear in the number of points. We ran PC for fMDS and compared it to SMACOF and CD (Figure 10). Both SMACOF and CD fail to run after n = 5000 because they run out of memory, while the performance of PC scales fairly smoothly even up to 50, 000 points. Before n = 5000, SMACOF performs quite well, but the performance of CD starts deteriorating rapidly.
 computing each distance d i , j as needed. Thus we only store the original point set, which has size O ( nd ) . This approach works for any dataset where distances d i , j can be quickly recomputed, not just Euclidean data in R d for moderate d . Alternatively, we could have read distances from disk for the point currently being processed instead of recomputing them on the fly. Note, we also seed all algorithms with a random projection instead of cMDS since cMDS also has a memory bottleneck of around n = 5000.
These preliminary results indicate that our method can be ef-fective on larger data sets. In ongoing work, we are comparing PC to scalable MDS methods like FastMap [ 17 ] , Metric Map Landmark MDS [ 15 ] and Pivot-MDS [ 5 ] that sacrifice quality (by reducing the number of  X  X ivot X  or  X  X andmark X  points supplied to the MDS routine) for speed. Preliminary experiments indicate that our method is comparable to these approaches in speed, while delivering significantly better quality. We note that these methods are limited in general to cMDS, unlike PC.
In summary, here are the main conclusions that can be drawn from this experimental study. Firstly, PC is consistently among problem. Occasionally, other methods will converge faster, but ent methods have much more variable behavior with changing inputs and noise levels.
In this section we present a Johnson-Lindenstrauss-style bound Cost function Figure 8: g-1-SMDS: Comparing PC with SMACOF-Q for dif-ferent values of penalty parameter  X  . for mapping data from a high dimensional sphere to a low-dimensional sphere while preserving the distances to within a multiplicative error of ( 1 +  X  ) .

Consider a set Y  X  S d  X  R d + 1 of n points, defining a distance matrix D where the element d i , j represents the geodesic distance between y i and y j on S k . We seek an embedding of Y into that preserves pairwise distances as much as possible. For a set Y  X  S d and a projection  X  ( Y )= X  X  S k we say the X has distortion from Y if these exists a constant c such that for all x , x j  X  X For a subspace H = R k , let  X  H ( Y ) be the projection of Y  X  onto H and then scaled by d / k . For X  X  R k , let S ( in S ( X ) is x / || x || .
 Lindenstrauss ( JL ) Lemma [ 22 ] says that if H  X  R d is a random k -dimensional linear subspace with k = O (( 1 /" 2 ) log X =  X  1  X   X  .

We now present the main result of this section. We note that recent results [ 1 ] have shown similar results for point on a vari-ety of manifolds (including spheres) where projections preserve Euclidean distances. We reiterate that our results extend this  X  x , y between the vectors to points x , y  X  S k . Another recent serves ing result.
 Theorem 6.1. Let Y  X  S d  X  R d + 1 , and let H = R k + 1 be a random subspace of R d with k = O (( 1 /" 2 ) log ( n / X  )) with Let f ( y i , y j ) measure the geodesic distance on S d (or at least 1  X   X  .

This implies that if we project n data points that lie on any high-dimensional sphere to a low-dimensional sphere S k with k  X  log n , then the pairwise distances are each individually pre-
Cost function Figure 9: c-2-SMDS: Comparing PC with SMACOF-Q for dif-ferent values of penalty parameter  X  technical lemma.
 Lemma 6.1. For "  X  [ 0, 0.5 ] and x  X  [ 0, 0.7 ] , (1) sin (( 1  X  2 " ) x )  X  ( 1  X  " ) sin ( x ) , and (2) sin (( 1 + 2 " ) x )  X  ( 1 + " ) sin ( x ) .
 Proof. Let g " ( x )=( 1  X  " ) sin x  X  sin (( 1  X  2 " ) x plies that it achieves its minimum value at the boundary. Now g ( 0 )= 0 for all " , and it can be easily shown that g " ( 0.7 for "  X  [ 0, 0.5 ] . This will therefore imply that g " ( specified range.

It remains to show that g " ( x ) is concave in [ 0, 0.7 ] which is always negative for "  X  [ 0, 0.5 ] and since sin x is in-creasing in the range [ 0, 0.7 ] .

This proves the first part of the lemma. For the second part, observe that h " ( x )= sin (( 1 + 2 " ) x )  X  ( 1 + " ) sin the same lines, by showing that h " ( x ) is concave in the desired range using that h 00 " ( x )= g 00  X  " (  X  x ) .

While the upper bound of 0.7 on x is not tight, it is close. The actual bound (evaluated by direct calculation) is slightly over 0.72.
 Proof of Theorem 6.1. Let X =  X  H ( Y ) . We consider two cases, ( Short Case ) when k y i  X  y j k X  1 / 2 and ( Long Case ) when k y y k X  ( 1 / 2, 2 ] .

Short Case: First consider points y i , y j  X  S d such that || y ||  X  1 / 2. Note that || y i  X  y j || = 2 sin (  X  y || y that Figure 10: The behavior of PC, SMACOF and CD for large val-ues of n . The curves for CD and SMACOF terminate around n = 5000 because of memory limitations.

We need to compare the angle  X  x largest  X  x small as possible, and so || cx i  X  cx j || =( 1 + "/ 8 ) large as possible. See Figure 11. In this case, we have ( || cx which for " &lt; 4 implies Similarly, we can show when  X  x c || x i || = c || x j || =( 1 + " ) and || cx i  X  cx j || then
We can also show (via Lemma 6.1) that since || y i  X  y j || X  1 implies  X  y and
Thus, we have sin (( 1  X  " )  X  y and y j on the shortest great circle connecting them. Let Figure 11: Illustration of the bounds on  X  x || y i  X  y j ||  X  1 of the disks of diameter "/ 8 shifted down from dashed line of length || y i  X  y j || . Bounds for x min i and x min j symmetrically. so by JL we have that ( 1  X  "/ 8 ) || y For notational convenience let y i = y ( 0 ) i , j and y j k y i  X  y j k X  This follows since the geodesic length of the great circular arc geodesic distance. Furthermore, by invoking the short case, for any pair 0 and y ( h ) i , j for h  X  [ 0 : 7 ] are coplanar, hence 0 and x h  X  [ 0 : 7 ] are coplanar), we can add together the bounds on angles which all lie on a single great circle. and thus by  X  x [ 1 ] P. K. Agarwal, S. Har-Peled, and H. Yu. On embeddings of [ 2 ] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet [ 3 ] I. Borg and P. J. F. Groenen. Modern Multidimensional [ 4 ] P. Bose, A. Maheshwari, and P. Morin. Fast approximations [ 5 ] U. Brandes and C. Pich. Eigensolver methods for [ 6 ] J. Brimberg and R. F. Love. Global convergence of a [ 7 ] A. M. Bronstein, M. M. Bronstein, and R. Kimmel.
 [ 8 ] M. Bronstein. Accelerated MDS. http://tosca.cs. [ 9 ] L. Cayton and S. Dasgupta. Robust Euclidean embedding. [ 10 ] L. Chen and A. Buja. Local multidimensional scaling for [ 11 ] T. F. Cox and M. A. A. Cox. Multidimensional Scaling, [ 12 ] N. Dalal and B. Triggs. Histograms of oriented gradients [ 13 ] J. de Leeuw. Applications of convex analysis to [ 14 ] J. de Leeuw and P. Mair. Multidimensional scaling using [ 15 ] V. de Silva and J. B. Tenenbaum. Global versus local [ 16 ] A. E. Elad, Y. Keller, and R. Kimmel. Texture mapping via [ 17 ] C. Faloutsos and K.-I. Lin. Fastmap: a fast algorithm for [ 18 ] P. T. Fletcher, S. Venkatasubramanian, and S. Joshi. The [ 19 ] R. Gray, A. Buzo, A. Gray Jr, and Y. Matsuyama.
 [ 20 ] P. Indyk and J. Matousek. Low-distortion embeddings of [ 21 ] T. Joachims. Learning to Classify Text Using Support Vector [ 22 ] W. B. Johnson and J. Lindenstrauss. Extensions of [ 23 ] H. Karcher. Riemannian center of mass and mollifier [ 24 ] I. N. Katz. Local convergence in Fermat X  X  problem. [ 25 ] J. B. Kruskal. Multidimensional scaling by optimizing [ 26 ] J. B. Kruskal and M. Wish. Multidimensional scaling. In [ 27 ] H. W. Kuhn. A note on Fermat X  X  problem. Mathematical [ 28 ] D. G. Lowe. Distinctive image features from [ 29 ] A. Magen. Dimensionality reductions that preserve [ 30 ] A. W. Marshall and I. Olkin. Inequalities: Theory of [ 31 ] L. M. Ostresh. On the convergence of a class of iterative [ 32 ] F. Pereira, N. Tishby, and L. Lee. Distributional clustering [ 33 ] R. Pietersz and P. J. F. Groenen. Rank reduction of [ 34 ] R. Pless and I. Simon. Embedding images in non-flat [ 35 ] T. Sarl X s. Improved approximation algorithms for large [ 36 ] W. S. Torgerson. Multidimensional scaling: I. theory and [ 37 ] J. T.-L. Wang, X. Wang, K.-I. Lin, D. Shasha, B. A. Shapiro, [ 38 ] E. Weiszfeld. Sur le point pour lequel la somme des [ 39 ] G. Young and A. S. Householder. Discussion of a set of
