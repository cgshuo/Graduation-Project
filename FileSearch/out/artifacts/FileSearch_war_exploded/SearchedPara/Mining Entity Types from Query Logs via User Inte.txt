 ppantel@microsoft.com Commercial search engines are providing ever-richer experiences around entities. Querying for a dish on Google yields recipe filters such as cook time, calories, and ingredients. Querying for a movie on Yahoo triggers user ratings, cast, tweets and showtimes. Bing further allows the movie to be directly added to the user X  X  Netflix queue. En-tity repositories such as Freebase, IMDB, Facebook Pages, Factual, Pricegrabber, and Wikipedia are in-creasingly leveraged to enable such experiences.
There are, however, inherent problems in the en-tity repositories: (a) coverage: although coverage of head entity types is often reliable, the tail can be sparse; (b) noise: created by spammers, extraction errors or errors in crowdsourced content; (c) am-biguity: multiple types or entity identifiers are of-ten associated with the same surface string; and (d) over-expression: many entities have types that are never used in the context of Web search.

There is an opportunity to automatically tailor knowledge repositories to the Web search scenario. Desirable capabilities of such a system include: (a) determining the prior type distribution in Web search for each entity in the repository; (b) assigning a type distribution to new entities; (c) inferring the correct sense of an entity in a particular query context; and (d) adapting to a search engine and time period.
In this paper, we build such a system by lever-aging Web search usage logs with large numbers of user sessions seeking or transacting on entities. We cast the task as performing probabilistic inference in a graphical model that captures how queries are generated, and then apply the model to contextually recognize entity types in new queries. We motivate and design several generative models based on the theory that search users X  (unobserved) intents gov-ern the types of entities, the query formulations, and the ultimate clicks on Web documents. We show that jointly modeling user intent and entity type signifi-cantly outperforms the current state of the art on the task of entity type resolution in queries. The major contributions of our research are:  X  We introduce the idea that latent user intents  X  We propose generative models and inference  X  We propose an efficient learning technique and  X  We empirically show that our models outper-2.1 Finding Semantic Classes A closely related problem is that of finding the se-mantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clus-tering (Sch  X  utze, 1998; Pantel and Lin, 2002), hy-ponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically lever-age large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes.
The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently a high chance of eBay being a website and an employer , but does not specify which usage is more common. This is especially problem-atic, for example, if one wishes to leverage Freebase but only needs the most commonly used senses (e.g., Al Gore is a US Vice President ), rather than all possible obscure senses (Freebase contains 30+ senses, including ones such as Impersonated Celebrity and Quotation Subject ). In scenarios such as this, our proposed method can in-crease the usability of systems that find semantic classes. We also expand upon text corpora meth-ods in that the type priors can adapt to Web search signals. 2.2 Query Log Mining Query logs have traditionally been mined to improve search (Baeza-Yates et al., 2004; Zhang and Nas-raoui, 2006), but they can also be used in place of (or in addition to) text corpora for learning seman-tic classes. Query logs can contain billions of en-tries, they provide an independent signal from text corpora, their timestamps allow the learning of type priors at specific points in time, and they can contain information such as clickthroughs that are not found in text corpora. Sekine and Suzuki (2007) used fre-quency features on context words in query logs to learn semantic classes of entities. Pas  X ca (2007) used extraction techniques to mine instances of semantic classes from query logs. R  X  ud et al. (2011) found that cross-domain generalizations learned from Web search results are applicable to NLP tasks such as NER. Alfonseca et al. (2010) mined query logs to find attributes of entity instances. However, these projects did not learn relative probabilities of differ-ent senses. 2.3 User Intents in Search Learning from query logs also allows us to lever-age the concept of user intents . When users sub-mit search queries, they often have specific intents in mind. Broder (2002) introduced 3 top level intents: Informational (e.g., wanting to learn), Navigational (wanting to visit a site), and Transactional (e.g., wanting to buy/sell). Rose and Levinson (2004) fur-ther divided these into finer-grained subcategories, and Yin and Shah (2010) built hierarchical tax-onomies of search intents. Jansen et al. (2007), Hu et al. (2009), and Radlinski et al. (2010) examined how to infer the intent of queries. We are not aware of any other work that has leveraged user intents to learn type distributions. 2.4 Topic Modeling on Query Logs The closest work to ours is Guo et al. X  X  (2009) re-search on Named Entity Recognition in Queries. Given an entity-bearing query, they attempt to iden-tify the entity and determine the type posteriors. Our work significantly scales up the type posteriors com-ponent of their work. While they only have four potential types ( Movie , Game , Book , Music ) for each entity, we employ over 70 popular types, allow-ing much greater coverage of real entities and their types. Because they only had four types, they were able to hand label their training data. In contrast, our system self-labels training examples by search-ing query logs for high-likelihood entities, and must handle any errors introduced by this process. Our models also expand upon theirs by jointly modeling entity type with latent user intents, and by incorpo-rating click signals.

Other projects have also demonstrated the util-ity of topic modeling on query logs. Carman et al. (2010) modeled users and clicked documents to personalize search results and Gao et al. (2011) ap-plied topic models to query logs in order to improve document ranking for search. We turn our attention now to the task of mining the type distributions of entities and of resolving the type of an entity in a particular query context. Our approach is to probabilistically describe how entity-bearing queries are generated in Web search. We theorize that search queries are governed by a latent user intent, which in turn influences the entity types, the choice of query words, and the clicked hosts. We develop inference procedures to infer the prior type distributions of entities in Web search as well as to resolve the type of an entity in a query, by maximiz-ing the probability of observing a large collection of real-world queries and their clicked hosts.

We represent a query q by a triple { n 1 ,e,n 2 } , where e represents the entity mentioned in the query, n 1 and n 2 are respectively the pre-and post-entity contexts (possibly empty), referred to as refiners . Details on how we obtain our corpus are presented in Section 4.2. 3.1 Intent-based Model (IM) In this section we describe our main model, IM , il-lustrated in Figure 1. We derive a learning algorithm for the model in Section 3.2 and an inference proce-dure in Section 3.3.
 Recall our discussion of intents from Section 2.3. The unobserved semantic type of an entity e in a query is strongly correlated with the unobserved user intent. For example, if a user queries for  X  song  X , then she is likely looking to  X  X isten to it X ,  X  X ownload it X ,  X  X uy it X , or  X  X ind lyrics X  for it. Our model incorporates this user intent as a latent vari-able.

The choice of the query refiner words, n 1 and n 2 , is also clearly influenced by the user intent. For example, refiners such as  X  X yrics X  and  X  X ords X  are more likely to be used in queries where the intent is to  X  X ind lyrics X  than in queries where the intent is to  X  X isten X . The same is true for clicked hosts: clicks on  X  X yrics.com X  and  X  X onglyrics.com X  are more likely to occur when the intent is to  X  X ind lyrics X , whereas clicks on  X  X andora.com X  and  X  X ast.fm X  are more likely for a  X  X isten X  intent.

Model IM leverages each of these signals: latent intent, query refiners, and clicked hosts. It generates entity-bearing queries by first generating an entity type, from which the user intent and entity is gen-erated. In turn, the user intent is then used to gen-erate the query refiners and the clicked host. In our data analysis, we observed that over 90% of entity-bearing queries did not contain any refiner words n 1 and n 2 . In order to distribute more probability mass to non-empty context words, we explicitly represent the empty context using a switch variable that deter-mines whether a context will be empty.

The generative process for IM is described in Ta-ble 1. Consider the query  X  ymca lyrics  X . Our model first generates the type song , then given the type it generates the entity  X  ymca  X  and the intent  X  X ind lyrics X . The intent is then used to generate the pre-and post-context words  X  and  X  lyrics  X , respectively, and a click on a host such as  X  lyrics.com  X .
For mathematical convenience, we assume that the user intent is generated independently of the entity itself. Without this assumption, we would require learning a parameter for each intent-type-entity configuration, exploding the number of pa-rameters. Instead, we choose to include these depen-dencies at the time of inference, as described later.
Recall that q = { n 1 ,e,n 2 } and let s = { s 1 ,s 2 } , where s 1 = 1 if n 1 is not empty and s 2 = 1 if n 2 is not empty, 0 otherwise. The joint probability of the model is the product of the conditional distributions, as given by:
We now define each of the terms in the joint dis-tribution. Let T be the number of entity types. The probability of generating a type t is governed by a multinomial with probability vector  X  : where I is an indicator function set to 1 if its condi-tion holds, and 0 otherwise.

Let K be the number of latent user intents that govern our query log, where K is fixed in advance. Then, the probability of intents i is defined as a multinomial distribution with probability vector  X  t such that  X  = [  X  1 , X  2 ,..., X  T ] captures the matrix of parameters across all T types:
Let E be the number of known entities. The prob-ability of generating an entity e is similarly governed by a parameter  X  across all T types:
The probability of generating an empty or non-empty context s given intent i is given by a Bernoulli with parameter  X  i :
Let V be the shared vocabulary size of all query refiner words n 1 and n 2 . Given an intent, i , the probability of generating a refiner n is given by a multinomial distribution with probability vector  X  i such that  X  = [  X  1 , X  2 ,..., X  K ] represents parame-ters across intents:
Finally, we assume there are H possible click val-ues, corresponding to H Web hosts. A click on a host is similarly determined by an intent i and is gov-erned by parameter  X  across all K intents: 3.2 Learning Given a query corpus Q consisting of N inde-pendently and identically distributed queries q j = { n c , we estimate the parameters  X  ,  X  ,  X  ,  X  ,  X  , and  X  by maximizing the (log) probability of observing Q . The log P ( Q ) can be written as:
In the above equation, P j ( t,i | q,c ) is the poste-rior distribution over types and user intents for the j th query. We use the Expectation-Maximization (EM) algorithm to estimate the parameters. The parameter updates are obtained by computing the derivative of log P ( Q ) with respect to each parame-ter, and setting the resultant to 0.

The update for  X  is given by the average of the posterior distributions over the types:
For a fixed type t , the update for  X  t is given by the weighted average of the latent intents, where the weights are the posterior distributions over the types, for each query:
Similarly, we can update  X  , the parameters that govern the distribution over entities for each type:
Now, for a fixed user intent i , the update for  X  i is given by the weighted average of the clicked hosts, where the weights are the posterior distribu-tions over the intents, for each query:
Similarly, we can update  X  and  X  , the parameters that govern the distribution over query refiners and empty contexts for each intent, as: and 3.3 Decoding Given a query/click pair { q,c } , and the learned IM model, we can apply Bayes X  rule to find the poste-rior distribution, P ( t,i | q,c ) , over the types and intents, as it is proportional to P ( t,i,q,c ) . We com-pute this quantity exactly by evaluating the joint for each combination of t and i , and the observed values of q and c .

It is important to note that at runtime when a new query is issued, we have to resolve the entity in the absence of any observed click. However, we do have access to historical click probabilities, P ( c | q ) . We use this information to compute P ( t | q ) by marginalizing over i as follows: 3.4 Comparative Models Figure 1 also illustrates the current state-of-the-art model Guo 0 09 (Guo et al., 2009), described in Sec-tion 2.4, which utilizes only query refinement words to infer entity type distributions. Two extensions to this model that we further study in this paper are also shown: Model M0 adds the empty context switch parameter and Model M1 further adds click infor-mation. In the interest of space, we omit the update equations for these models, however they are triv-ial to adapt from the derivations of Model IM pre-sented in Sections 3.1 and 3.2. 3.5 Discussion Full Bayesian Treatment: In the above mod-els, we learn point estimates for the parameters (  X ,  X  ,  X  , X ,  X  ,  X  ). One can take a Bayesian ap-proach and treat these parameters as variables (for instance, with Dirichlet and Beta prior distribu-tions), and perform Bayesian inference. However, exact inference will become intractable and we would need to resort to methods such as variational inference or sampling. We found this extension un-necessary, as we had a sufficient amount of training data to estimate all parameters reliably. In addition, our approach enabled us to learn (and perform infer-ence in) the model with large amounts of data with reasonable computing time.
 Fitting to an existing Knowledge Base: Al-though in general our model decodes type distribu-tions for arbitrary entities, in many practical cases it is beneficial to constrain the types to those ad-missible in a fixed knowledge base (such as Free-base). As an example, if the entity is  X  X mca X , admissible types may include song , place , and educational institution . When resolving types, during inference, one can restrict the search space to only these admissible types. A desirable side effect of this strategy is that only valid ambigu-ities are captured in the posterior distribution. We refer to QL as a set of English Web search queries issued to a commercial search engine over a period of several months. 4.1 Entity Inventory Although our models generalize to any entity reposi-tory, we experiment in this paper with entities cover-ing a wide range of web search queries, coming from 73 types in Freebase. We arrived at these types by grepping for all entities in Freebase within QL , fol-lowing the procedure described in Section 4.2, and then choosing the top most frequent types such that 50% of the queries are covered by an entity of one of these types 1 . 4.2 Training Data Construction In order to learn type distributions by jointly mod-eling user intents and a large number of types, we require a large set of training examples containing tagged entities and their potential types. Unlike in Guo et al. (2009), we need a method to automatically label QL to produce these training cases since man-ual annotation is impossible for the range of entities and types that we consider. Reliably recognizing en-tities in queries is not a solved problem. However, for training we do not require high coverage of en-tities in QL , so high precision on a sizeable set of query instances can be a proper proxy.
 To this end, we collect candidate entities in QL via simple string matching on Freebase entity strings within our preselected 73 types. To achieve high precision from this initial (high-recall, low-precision) candidate set we use a number of heuris-tics to only retain highly likely entities. The heuris-tics include retaining only matches on entities that appear capitalized more than 50% in their occur-rences in Wikipedia. Also, a standalone score fil-ter (Jain and Pennacchiotti, 2011) of 0.9 is used, which is based on the ratio of string occurrence as an exact match in queries to how often it occurs as a partial match.

The resulting queries are further filtered by keep-ing only those where the pre-and post-entity con-texts ( n 1 and n 2 ) were empty or a single word (ac-counting for a very large fraction of the queries). We also eliminate entries with clicked hosts that have been clicked fewer than 100 times over the entire QL . Finally, for training we filter out any query with an entity that has more than two potential types 2 This step is performed to reduce recognition er-rors by limiting the number of potential ambiguous matches. We experimented with various thresholds on allowable types and settled on the value two.
The resulting training data consists of several mil-lion queries, 73 different entity types, and approx-imately 135K different entities, 100K different re-finer words, and 40K clicked hosts. 4.3 Test Set Annotation We sampled two datasets, HEAD and TAIL , each consisting of 500 queries containing an entity be-longing to one of the 73 types in our inventory, from a frequency-weighted random sample and a uniform random sample of QL , respectively.

We conducted a user study to establish a gold standard of the correct entity types in each query. A total of seven different independent and paid pro-fessional annotators participated in the study. For each query in our test sets, we displayed the query, associated clicked host, and entity to the annotator, along with a list of permissible types from our type inventory. The annotator is tasked with identifying all applicable types from that list, or marking the test case as faulty because of an error in entity identifi-cation, bad click host (e.g. dead link) or bad query (e.g. non-English). This resulted in 2,092 test cases ( { query, entity, type } -tuples). Each test case was annotated by two annotators. Inter-annotator agree-ment as measured by Fleiss X   X  was 0.445 (0.498 on HEAD and 0.386 on TAIL ), considered moderate agreement.

From HEAD and TAIL , we eliminated three cat-egories of queries that did not offer any interesting type disambiguation opportunities:  X  queries that contained entities with only one  X  queries where the annotators rated all potential  X  queries where judges rated none of the potential
The final test sets consist of 105 head queries with 359 judged entity types and 98 tail queries with 343 judged entity types. 4.4 Metrics Our task is a ranking task and therefore the classic IR metrics nDCG (normalized discounted cumula-tive gain) and MAP (mean average precision) are applicable (Manning et al., 2008).

Both nDCG and MAP are sensitive to the rank position, but not the score (probability of a type) as-sociated with each rank, S ( r ) . We therefore also evaluate a weighted mean average precision score MAP W , which replaces the precision component of MAP , P ( r ) , for the r th ranked type by: where I ( r ) indicates if the type at rank r is judged correct.

Our fourth metric is Prec @ 1 , i.e. the precision of only the top-ranked type of each query. This is espe-cially suitable for applications where a single sense must be determined. 4.5 Model Settings We trained all models in Figure 1 using the training data from Section 4.2 over 100 EM iterations, with two folds per model. For Model IM , we varied the number of user intents ( K ) in intervals from 100 to 400 (see Figure 3), under the assumption that multi-ple intents would exist per entity type.
 We compare our results against two baselines. The first baseline is an assignment of Freebase types according to their frequency in our query set B FB , and the second is Model Guo 0 09 (Guo et al., 2009) illustrated in Figure 1. Table 2 lists the performance of each model on the HEAD and TAIL sets over each metric defined in Section 4.4. On head queries, the addition of the empty context parameter  X  and click signal  X  to-gether (Model M1 ) significantly outperforms both the baseline and the state-of-the-art model Guo 0 09 . Further modeling the user intent in Model IM re-sults in significantly better performance over all models and across all metrics. Model IM shows its biggest gains in the first position of its ranking as evidenced by the Prec @ 1 metric.

We observe a different behavior on tail queries where all models significantly outperform the base-line B FB , but are not significantly different from each other. In short, the strength of our proposed model is in improving performance on the head at no noticeable cost in the tail.

We separately tested the effect of adding the empty context parameter  X  . Figure 2 illustrates the result on the HEAD data. Across all metrics,  X  im-proved performance over all models 3 . The more expressive models benefitted more than the less ex-pressive ones.

Table 2 reports results for Model IM using K = 200 user intents. This was determined by varying K and selecting the top-performing value. Figure 3 illustrates the performance of Model IM with dif-ferent values of K on the HEAD .
Our models can also assign a prior type distribu-tion to each entity by further marginalizing Eq. 1 over query contexts n 1 and n 2 . We measured the quality of our learned type priors using the subset of queries in our HEAD test set that consisted of only an entity without any refiners. The results for Model IM were: nDCG = 0 . 86 , MAP = 0 . 80 , MAP W = 0 . 75 , and Prec @1 = 0 . 70 . All met-rics are statistically significantly better than B FB Guo 0 09 and M0 , with 95% confidence. Compared to Model M1 , Model IM is statistically signifi-cantly better on Prec @1 and not significantly dif-ferent on the other metrics.
 Discussion and Error Analysis: Contrary to our results, we had expected improvements for both HEAD and TAIL . Inspection of the TAIL queries revealed that entities were greatly skewed towards people (e.g., actor , author , and politician ). Analysis of the latent user in-tent parameter  X  in Model IM showed that most people types had most of their probability mass assigned to the same three generic and common in-tents for people types:  X  X ee pictures of X ,  X  X ind bio-graphical information about X , and  X  X ee video of X . In other words, latent intents in Model IM are over-expressive and they do not help in differentiating people types.

The largest class of errors came from queries bearing an entity with semantically very similar types where our highest ranked type was not judged correct by the annotators. For example, for the query  X  philippine daily inquirer  X  our system ranked newspaper ahead of periodical but a judge rejected the former and approved the latter. For  X  ikea catalogue  X , our system ranked magazine ahead of periodical , but again a judge rejected magazine in favor of periodical .

An interesting success case in the TAIL is high-lighted by two queries involving the entity  X  ymca  X , which in our data can either be a song , place , or educational institution . Our system learns the following priors: 0.63, 0.29, and 0.08, respectively. For the query  X  jamestown ymca ny  X , IM correctly classified  X  ymca  X  as a place and for the query  X  ymca palomar  X  it correctly classified it as an educational institution . We further issued the query  X  ymca lyrics  X  and the type song was then highest ranked.

Our method is generalizable to any entity collec-tion. Since our evaluation focused on the Freebase collection, it remains an open question how noise level, coverage, and breadth in a collection will af-fect our model performance. Finally, although we do not formally evaluate it, it is clear that training our model on different time spans of queries should lead to type distributions adapted to that time period. Jointly modeling the interplay between the under-lying user intents and entity types in web search queries shows significant improvements over the current state of the art on the task of resolving entity types in head queries. At the same time, no degrada-tion in tail queries is observed. Our proposed models can be efficiently trained using an EM algorithm and can be further used to assign prior type distributions to entities in an existing knowledge base and to in-sert new entities into it.

Although this paper leverages latent intents in search queries, it stops short of understanding the nature of the intents. It remains an open problem to characterize and enumerate intents and to iden-tify the types of queries that benefit most from intent models.
