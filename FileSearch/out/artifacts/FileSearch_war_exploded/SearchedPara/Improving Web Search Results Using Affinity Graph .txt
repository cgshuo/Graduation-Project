 In this paper, we propose a novel ranking scheme named Affinity Ranking (AR) to re-rank search results by optimizing two metrics: (1) diversity --which indicates the variance of topics in a group of documents; (2) information richness --which measures the coverage of a single document to its topic. Both of the two metrics are calculated from a directed link graph named Affinity Graph (AG). AG models the structure of a group of documents based on the asymmetric content similarities between each pair of documents. Experimental results in Yahoo! Directory, ODP Data, and Newsgroup data demonstrate that our proposed ranking algorithm significantly improves the search performance. Specifically, the algorithm achieves 31% improvement in diversity and 12% improvement in information richness relatively within the top 10 search results. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  retrieval models , search process ; H.2.8 [ Database Management ]: Database Applications  X  Data Mining Keywords : Affinity Ranking, Information Retrieval, Link Analysis, Diversity, and Information Richness Most current web search engines tend to provide a list of search results to users X  queries according to the relevance score of each document to the query. This paradigm is very useful when users X  information needs (represented by the queries) are clear and they care more about precision than r ecall in the returned results. Unfortunately, many of the queries presented to a web search engine nowadays are ambiguous [5] and the user X  X  actual information needs are unknown. Users may suffer from the vast number of redundant and yet not very relevant documents that are related to a few most popular topics listed in the top of search results. Such search experience often makes users frustrated. Several approaches have been proposed to improve such situation. Carbonell et al [3] proposed a re-ranking method based on maximal marginal relevance criterion to reduce redundancy while maintaining query relevance in re-ranked documents. A marginal relevance of a document is defined as the relevance with a query minus that of previously selected documents. Maximizing this marginal relevance will help achieve a low redundancy in a group of documents. But there is no direct criterion about diversity evaluation to ensure that the group of documents with low redundancy can achieve large topic coverage. R ecently proposed subtopic retrieval method [18] is another useful approach to improve the high redundancy search result. Different from Carbonell X  X  work, statistical language model is applied to calculate the document relevance and measure the novelty of a document. However, as the subtopic retrieval method is concerned mostly on covering as many subtopics of a query topic as possible, it may not achieve the lowest redundancy of a group of documents. As reported in [3], the majority of people in the experiments said they preferred the method which provides them search results with the most broad and interesting topics. However, since the top search results are very often dominated by a set of closely related documents on some specific topic, users often have to face the following two situations: (1) the top search results can hardly cover a sufficient variety of topics to meet the users X  diversified information need; (2) there is no indication about how informative a returned document is on the query topic. In traditional information retrieval research, precision and recall [1] have been used as metrics to evaluate information retrieval systems. Both metrics only concern about the relevance of the documents returned, without concerning the number of various topics that the returned document list covers, or the range of topics a single returned document covers. In web link analysis research, the popularity of a web page [9, 12] has been widely adopted to measure the quality of a web page. However, this kind of quality is computed based on web page link graph and is independent to the content of a web page. All these observations motivate us to introduce two novel metrics, diversity and information richness, which measure the quality of 
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that requires prior specific permission and/or a fee. SIGIR X 05 , August 15 X 19, 2005, Salvador, Brazil. 
Copyright 2005 ACM 1-59593-034-5/05/0008...$5.00. search results by considering the content based link structure of a group of documents and the content of a single document in the search results. Diversity measures the variety of topics in a group of documents. It shows the holistic property of documents set. Information richness measures how many different topics a single document contains. Based on the two metrics, a novel algorithm named Affinity Ranking (AR) is proposed to re-rank the top search results. In particular, we first model the content based link structure of a group of documents as a directed graph which we call an Affinity Graph (AG) based on the asymmetric similarities between document pairs. Similar to web page link analysis, an  X  X mportance X  score is computed based on Affinity Graph for each document indicating its information richness. Secondly, we apply a greedy algorithm to assign a penalty score to each returned document considering the diversity property of query-related topics. Thirdly, the AR score of each document is obtained as a combination of the information richness and diversity penalty scores. AR scores are then used to re-rank the top search results. Our experimental results in Yahoo! Directory and ODP Dataset demonstrate that our proposed AR algorithm significantly improves the coverage of query-related topics in the top 10 search results over the K-Means clustering algorithm. Meanwhile, experiments on a newsgroup data set show that the AR algorithm achieves about 31% improvement in diversity and 12% improvement in information richness in the top 10 search results without loss in precision and r ecall. The rest of the paper is organized as follows. In Section 2, we introduce the background by explaining the state-of-art link analysis algorithms. In Section 3, we introduce the Affinity Ranking algorithm, as well as the formal definitions of diversity and information richness . Experiments and evaluations are reported in Section 4. We conclude and discuss future works in Section 5. Recently, there have been growing research interests on mining the relationship between data objects, which is usually referred to as  X  X ink X  in the literature. Link structure has been proved to be very useful in various applications such as information retrieval [9, 12], classification [10] and clustering [8]. Two of the most famous works on link analysis are Google X  X  PageRank algorithm [12] and Kleinberg X  X  HITS algorithm [9]. Both of them make use of the hyperlink structure among web pages to model a group of web pages as a link graph.  X  X xplicit link analysis X  and  X  X mplicit link analysis X  [4, 16, 17] are currently two major sub-areas in link analysis research field. Hyperlinks embedded in web pages can be considered as  X  X xplicit links X  since they explicitly provide a connection from one page to another.  X  X mplicit links X  refers to those linkages inferred from users X  behavior, such as the user X  X  access pattern on web pages. The difference between them is that explicit link represents web editor X  X  view since hyperlinks are edited by them, while implicit links represent end-users X  view. Two typical examples of implicit link analysis are DirectHit [6] and Small Web Search [17], which assumes that two web pages are implicitly linked if they are visited sequentially by the same end-user. DirectHit and Small Web Search can be considered as modified versions of HITS and PageRank algorithms applied on implicit link structure. However, the metrics used to evaluate these methods discussed are intrinsically subjective, and they can not quantify the information contained in web pages objectively. In this work, we develop objective metrics to measure the amount of information contained in a single document and also the topic variety in a group of documents. The framework of Affinity Ranking is illustrated in Figure 1. It includes three steps: (1) Affinity Graph (AG) based on the content link structure is constructed for the entire documents collection; Information richness of each document is then calculated based on AG. (2) For a given query, a result set of relevant documents are produced by the full-text search process. Based on AG and the information richness score, diversity penalty is imposed to each document in the result set. (3) The information richness and diversity penalty scores are combined to obtain the Affinity Rank score so as to re-rank the top returned document list. We now give the formal definitions of information richness and diversity. diversity ) ( R Div to denote the number of different topics contained in R.
 document document of generality, we let ] 1 , 0 [ ) (  X  i d InfoRich . For a set of documents } , , { 2 1 l l d d d R L = which contain ) ( R Div topics (i.e. diversity = ) ( R Div ), its average information richness can be calculated as: Where i k d represents one of the k N documents associated with the k -th topic. In the rest of this paper, we use average information richness to refer to the information richness of a set of documents. Let } 1 | { n i d D i  X   X  = denote a document collection. According to vector space model [15], each document i d can be represented as a vector i d d can be calculated as For further measurement on the significance of the similarity between each document pair, we define the affinity of j d to What is worthy to be noted is that the affinity defined here is asymmetric because ) , ( ) , ( j i j i d d aff d d aff  X  . If we consider documents as nodes, the document collection can be modeled as a graph by generating the link between documents using the following rule: Thus, each link in the graph has been assigned a weight indicating the similarity relationship between the corresponding document pair. Since all links are constructed according to the affinity value between document pairs, we call the graph as Affinity Graph. Usually, documents of the same topic are similar to each other. Hence, in Affinity Graph, a group of heavily linked documents naturally represents a topic group, documents connected by weak or no links belong to different topics. After obtaining Affinity Graph, we apply a link analysis algorithm to compute the information richness for each node in AG. Similar to PageRank [10], we proposed the following algorithm. First, an adjacency matrix M is used to describe AG with each entry M is defined as below: Without loss of generality, M is normalized to make the sum of each row equal to 1. The normalized adjacency matrix n n j i M  X  = ) ~ compute the information richness score for each node. Our computation is based on the following two intuitions: 1. The more neighbors a document has, the more informative it is; 2. The more informative a document X  X  neighbors are, the more Thus, the score of document i d can be deduced from those of all other document linked to it and it can be formulated in a recursive form as follows: And in a matrix form: where 1 )] ( [  X  = n i d InfoRich  X  is the eigenvector of Since M ~ is normally a sparse matrix, all-zero rows could possibly appear, i.e. some documents have no other documents with significant affinity to them. To compute a meaningful eigenvector, we introduce a dumping factor c (similar to the random jumping factor in PageRank): And as a matrix form: dumping factor ) 1 , 0 (  X  c is set at 0.85 in our experiments. The computation of information richness can be explained in a way similar to the random surfer model, and we call it random information flow model. Imagine the information is flowing among the document nodes at each iteration and we assume it which i d links. In the next iteration, the information can choose where to flow according to the following two rules: 1. With a probability c (i.e. the dump factor), the information 2. With a probability of c  X  1 the information will randomly Figure 2 gives an illustration of the  X  X andom information flow X  model. On the Affinity Graph, beside links constructed by the  X  X ink generation X  rule, we label an additional link by dotted line Link generation 
A directional link from i d to j d ( j i  X  ) with weight threshold); otherwise no link is constructed (or the weight o f the link is regarded as zero). which indicates the possibility of random information flow as described in Rule 2. A Markov chain can be induced from the above process, where the states are given by the documents and the transition (or flow) matrix is given by U M stationary probability distribution of each state is given by the principal eigenvector of the transition matrix, which is equivalent to Equation (9). Computing information richness helps us choose more informative documents to be presented in top search results. However, in some cases two most informative documents could be very similar (or in an extreme case they can be duplicates). To increase the coverage on the top search results, different penalty is imposed to the information richness score of each document in terms of its influences to the topic diversity. The diversity penalty is calculated by a greedy algorithm. At each iteration of the algorithm, penalty is imposed to documents topic by topic, and the Affinity Ranking score gets updated with it. The Greedy Algorithm for Diversity Penalty Step 0. Initialize the two sets {} n i d i L , 2 , 1 | , Step 1. Sort the documents in  X  by their current Affinity Rank Step 2. Suppose the document ranked highest in  X  is Step 3. Re-sort the documents in  X  by the updated rank scores in Step 4. Go to Step 2 until  X  =  X  or the iteration reaches a The crucial part of the above greedy algorithm is Step 2, which embodies a basic idea of penalty --decrease the Affinity Ranking scores of less informative documents by the part conveyed from the most informative one. The more a document is similar to the most informative one, the more penalties it receives and its Affinity Ranking score is decreased. It ensures only the most informative one in each topic becomes distinctive in the ranking process. The re-ranking mechanism is a combination of results from full-text search and Affinity Ranking. There are two schemes of combination: score-combination and rank-combination. A user query is denoted by q . A set of relevant documents by full-text search is denoted by  X  . The score-combination scheme uses a linear combination of two parts: one comes from the score of full-However the two scores are always on different order of magnitudes and their raw values vary in a different range. Therefore, we perform different normalization (average normalization and log average normalization) for the two scores, and then combine the two parts together: where 1 = +  X   X  and The rank-combination scheme of re-ranking uses a linear combination of the ranks based on full-text search and Affinity Ranking, shown as follow: The  X  and  X  in both two combination schemes are parameters which can be tuned. When 0  X  = , no re-ranking is performed, and the search results are equivalent to full-text search; with the increase of  X  , more weight is put on the Affinity Ranking in the re-ranking process; when 0 =  X  (and 1 =  X  ), we totally rely on Affinity Ranking score to re-rank the search results. We conducted experiments on Yahoo! Directory, ODP Data and a Newsgroup data set to demonstrate the effectiveness of our proposed Affinity Ranking scheme. Yahoo! Directory is one of the most famous Web directories. We downloaded the directory in June, 2004. It contained a total of 292,216 categories (including leaf categories and non-leaf categories). All categories are organized into a 16-level hierarchy. Similar to many previous works [2, 7], we downloaded the index pages of the websites listed in Yahoo! directory as the labeled documents. As a result, we have downloaded 792,601 documents in total. ODP (Open Directory Project) is another famous Web directory. It is probably the largest, most comprehensive human-edited directory on the Web, which is constructed and maintained by a vast, global community of volunteer editors [11]. We downloaded the directory in August, 2004. ODP includes a total of 172,565 categories. Similar to the Yahoo! dataset, we downloaded the index pages of the websites listed in ODP as labeled documents. As a result, we have downloaded 1,547,000 documents in total. The Newsgroup data is composed of 256,449 posts collected from 117 commercial applications related newsgroups over a period of 4 months with a total size of about 400M. A post parser is applied to remove the stop words and unrelated words such as  X  X rom X ,  X  X o X ,  X  X ime X ,  X  X ignature X , and  X  X itations X , et al. The title and content of the post are given a 3:1 weighting ratio in indexing process. Porter stemming [13] is also performed over the entire dataset. For the Newsgroup dataset, there are two specific considerations. Newsgroup is a typical collection composed of documents with repetitive content because large am ount of posts are very likely to be devoted to the same topic. Traditional information retrieval which purely relies on the full-text content will result in more redundancy due to similar posts in the top search results. Our proposed Affinity Ranking scheme can be used to solve this problem. We used the Okapi system as our baseline retrieval system. For each query, Okapi provides a set of documents ranked by text-based similarity score. We conducted experiments on Yahoo! Directory and ODP Data set to compare AR and the traditional clustering method K-Means to see which method can cover more query-related topics in top 10 search results. We selected 20 queries from Yahoo! Directory category labels and ODP category labels, respectively. Table 1 and Table 2 give the queries. The top 1000 search results of each query are passed to AR or K-Means algorithm to re-rank top 10 results. For K-Means algorithm, we set K=10 and use the top 1 document of each cluster to construct the top 10 results. F value is used to measure the performance of Affinity Ranking and K-Means clustering. The recall ( R ), precision ( P ), and 
F are defined as follows: N denotes the number of different sub-category labels in Yahoo! Directory or ODP. s ys N denotes the corresponding sub-category label number in the top 10 search results re-ranked by AR or K-Means algorithm. Figures 3 and 4 show that AR significantly improves the coverage of topics compared to K-Means method on both Yahoo! and ODP Data. Figure 3: F Values of AR and K-Means in Yahoo! Directory newsgroup dataset of documents, and apply the proposed AR scheme to re-rank top 50 documents returned from the baseline system (OKAPI) [14]. The queries vary from 1 word to 3 words, covering several commercial software products. 1 Blue screen 11 System requirement 3 activate product 13 Excel crash 4 Excel formula 14 Office 2003 5 Office assistant 15 Office uninstall 6 outer join 16 Outlook print error 8 print preview 18 save attachment 10 word font 20 Word print We compare our approach with the Okapi system in three aspects: diversity, information richness and relevance. The diversity for a document set and information richness for a single document have already been defined in Section 3. Similarly, the average relevance defined as follows: where ] 1 , 0 [ ) , (  X  q d Rlv i is the relevance of document q . Four researchers in web search and mining area are hired to independently evaluate the experimental results. They labeled the top 50 search results for each of the 20 queries based on the following steps: 1. Make an overview of the 50 search results, and then manually 2. In each topic gr oup, give each document a score indicating 3. Give each document a score indicating their relevance to the Finally, the scores in the step 2 and step 3 are normalized into [0, 1] according to the definitions of information richness and relevance. The labeled data served as the ground truth to evaluate the diversity, information richness and relevance of the top N search results ( 50  X  N ). Since the labeled ground truth (e.g. the number of topics in the top 50 search results) varies from user to user, our improvement measures are presented in the form of macro relative change which is defined as: where N is the number of users, i.e. 4, X could be diversity , information richness , or relevance of the top search results, the superscript i denotes the i-th user X  X  ground truth, and the subscripts A and F represent results from our ranking scheme and full-text search, respectively. As the top 10 search results always receive the most attention of end-users, we also conduct experiments to show how Affinity Ranking affects the top 10 search results from the newsgroup data set. Table 4 shows the relative improvement of AR re-ranking over Okapi system. 
Relative Change +31% +12% +0.72% p value at t-test 0.004632 0.002225 0.067255 In this experiment, we use the rank-combination scheme and which 0 and 1 = =  X   X  . From Table 4, we can see that our proposed Affinity Ranking achieves 31% and 12% improvements in diversity and information richness compared over the full-text search system. T-test result indicates that this improvement is statistically significant. The experiments results confirm that our proposed algorithm can improve the diversity and information richness of the top 10 search results without loss in relevance. We also measure the improvements of AR within different number of search results. Figure 5 illustrates the relative improvement in diversity as the number of search results increases. It is shown that our method always improves the diversity in the search results. Initially, the diversity improvement increases sharply with the N value and reaches a maximum when 10 = N , which is usually the number of results fitting into the first search result page and browsed by most end-users. Then the diversity improvement gradually falls down to zero * when N reaches 50. We can conclude from the figure that the relative order of results is changed so that documents from different topics are shifted forward to the top of the returned search list; and consequently the topic diversity of the top returned results is improved. Since re-ranking the top 50 results only changes their order, the definition of diversity. (The same for  X  X nformation richness X  in Figure 4). Figure 6: Information Richness improvement by Affinity Rank Figure 6 illustrates the relative improvement in information richness as the number of top results increases. We found that an approximate 10% improvement can be achieved within the top 15 improvement gradually gets less distinct since more overlapping between full-text search results and re-ranked results appears. We conclude from this figure that more informative documents had been promoted towards the top position. As mentioned in previous section, there are two ranking combination schemes to be used and a pair of parameters to be tuned. The ratio between the parameter pair, i.e.  X   X  : , determines the weight of the Affinity Ranking score versus the full-text search score. Taking the top 10 search results as an instance, we give a range of values for  X   X  : and compare the relative improvement in diversity and information richness. We also compare the two ranking combination schemes, and the results are shown in Figure 7 and Figure 8, respectively. Regardless of which scheme is used, as long as : enough (i.e., putting enough weight on Affinity Ranking), the improvement in both diversity and information richness will stay around the maximum value without much change. What X  X  more, the range for large value of  X   X  : is quite significant. Although the optimum value of  X   X  : is hard to formulate, the empirical results show that if we simply re-rank totally by Affinity Ranking, i.e. 0 and 1  X  X  == (shown as :0  X  X  = in the figures), the improvement in both diversity and information richness is very close to the maximum value we can achieve. From the above two figures, it is easy to see that the rank-combination scheme is slightly better than score-combination when the ratio of :  X   X  is sufficiently large. We provide a case study here to give an illustration on how our ranking method works. This example is extracted from our experiments on the Newsgroup search for the query  X  X utlook print error X . In this scenario, a user has a printing error while using Microsoft Outlook. He comes to the Newsgroup to ask for help. Quite naturally, he starts with  X  X utlook print error X  to search and hopes to find a solution to the problem. Since there are many possible reasons that can lead to an Outlook print error, it is hard for him to find the right posts answering his specific error problem in a short time. By using full-text search, we can obtain an initial rank, part of which is shown in Table 5. The Affinity Rank score is given for each listed result with its topic indicated by some abbreviations. Since those search results are all newsgroup posts, we also label their threads with Roman numbers. For convenience, we name the retrieved post in the i -th position in the initial rank as post In the top 50 retrieved posts, there are roughly 6-8 reasons for the Outlook print error, such as: 1. With prompted error code of  X  X nspecified Error X , abbreviated 2. With prompted error code of  X  X nvalid argument X , abbreviated 3. Error caused by some function not implemented, abbreviated 4. A special error occurred only when print mails in the public newsgroup threads. For instances, in Table 5, 1 p and from different threads but belong to the same topic, while discusses a new topic other than most other posts X  in its thread. As can be seen from Table 5, the initial top 10 retrieved posts there only contain two topics involved  X  u. e. and i. a., and the top 10 is dominated by posts discussing the  X  X . e. X  error. After re-ranking, the topic number in top 10 results increases to four. Posts p and 24 p are promoted to top 10 and bring two new topics. Also, 6 p moves to the first position. Further analysis shows that 6 p , 13 p and 24 p are the most informative posts describing the  X  X . a. X ,  X  X . i. X  and  X  X . f. X  problems, respectively. The ranks of the three posts are promoted because they have relatively large Affinity Rank scores (shown in Table 5). This case provides a typical example on how Affinity Ranking helps improve the diversity and information richness in the top search results. High-quality search results depend on many factors. The well-recognized metrics such as relevance and importance do not necessarily guarantee the satisfaction from end-users. In this paper, we proposed two new metrics, diversity and information richness, to measure the search performance. Further, a novel ranking scheme, Affinity Ranking, is proposed to re-rank the search results to improve the diversity and information richness of the top search results. Our experiments showed that the proposed metrics and new ranking method can effectively improve the search performance by presenting wider topic coverage and more highly informative results in each topic in the top results. The improvement is significant compared with the traditional full-text search and brings no loss to relevance. Our future work includes scaling our Affinity Ranking computation, for example, to the Web scale. [1] Baeza-Yates, R. and Ribeiro-Neto, B. Modern Information [2] Calvo, R.A., Lee, J.-M. and Li, X. Managing Content with [3] Carbonell, J. and Goldstein, J., The use of MMR, diversity-[4] Chen, Z., Tao, L., Wang, J., Liu, W. and Ma, W.-Y., A [5] Croft, W.B., Cronen-Townsend, S. and Larvrenko, V., [6] DirectHit. http://www.directhit.com. [7] Dumais, S. and Chen, H., Hierarchical classification of Web [8] Gibson, D., Kleinberg, J.M. and Raghavan, P., Inferring Web [9] Kleinberg, J.M. Authoritative sources in a hyperlinked [10] Lu, Q. and Getoor, L., Link-based Classification. In [11] ODP. http://dmoz.org/. [12] Page, L., Brin, S., Motwani, R. and Windograd, T. The [13] Porter, M.F. An algorithm for suffix stripping Program, 1980, [14] Robertson, S.E., Walker, S., Hancock-Beaulieu, M., Gull, A. [15] Wong, S.K.M. and Raghavan, V.V., Vector space model of [16] Xi, W., Zhang, B., Chen, Z., Lu, Y., Yan, S., Ma, W.-Y. and [17] Xue, G.-R., Zeng, H.-J., Chen, Z., Ma, W.-Y., Zhang, H.-J. [18] Zhai, C.X., Cohen, W.W. and Lafferty, J., Beyond 
