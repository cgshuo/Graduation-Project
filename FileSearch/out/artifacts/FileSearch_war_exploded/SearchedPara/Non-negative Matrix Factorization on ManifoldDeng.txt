
Recently Non-negative Matrix Factorization (NMF) has received a lot of attentions in information retrieval, com-puter vision and pattern recognition. NMF aims to find two non-negative matrices whose product can well approx-imate the original matrix. The sizes of these two matrices are usually smaller than the original matrix. This results i n a compressed version of the original data matrix. The so-lution of NMF yields a natural parts-based representation for the data. When NMF is applied for data representa-tion, a major disadvantage is that it fails to consider the geometric structure in the data. In this paper, we develop a graph based approach for parts-based data representation in order to overcome this limitation. We construct an affin-ity graph to encode the geometrical information and seek a matrix factorization which respects the graph structure. W e demonstrate the success of this novel algorithm by applying it on real world problems.
The techniques of matrix factorization have become pop-ular in recent years for data representation. In many prob-lems in information retrieval, computer vision and pattern recognition, the input data matrix is of very high dimen-sion. This makes learning from example infeasible. One hopes then to find two or more lower dimensional matrices whose product provides a good approximation to the origi-nal matrix. The canonical matrix factorization techniques include LU-decomposition, QR-decomposition, Cholesky decomposition, and Singular Value Decomposition (SVD).
SVD is one of the most frequently used matrix factor-ization tool. A singular value decomposition of an m  X  n matrix X is any factorization of the form where U is an m  X  m orthogonal matrix, V is an n  X  n orthogonal matrix, and S is an m  X  n diagonal matrix with S ij = 0 if i 6 = j and S ii  X  0 . The quantities S ii are called the singular values of X , and the columns of U and V are called left and right singular vectors , respectively. By re-moving those singular vectors corresponding to sufficientl y small singular value, we get a natural low-rank approxima-tion to the original matrix. This approximation is optimal i n the sense of reconstruction error and thus optimal for data representation when Euclidean structure is concerned. For this reason, SVD has been applied to various real world ap-plications, such as face recognition ( Eigenface , [26]) and document representation ( Latent Semantic Indexing , [8]).
Previous studies have shown there is psychological and physiological evidence for parts-based representation in hu-man brain [23], [27], [20]. The Non-negative Matrix Fac-torization (NMF) algorithm is proposed to learn the parts of objects like human faces and text documents [22], [14]. NMF aims to find two non-negative matrices whose product provides a good approximation to the original matrix. The non-negative constraints lead to a parts-based representa -tion because they allow only additive, not subtractive, com -binations. NMF has been shown to be superior to SVD in face recognition [16] and document clustering [29]. NMF is optimal for learning the parts of objects. However, it fails to consider the geometrical structure of the data space which is essential for data clustering and classification problem s.
In this paper, we propose a novel algorithm, called Graph regularized Non-negative Matrix Factorization (GNMF), to overcome the limitation of NMF. We encode the geometri-cal information of the data space by constructing a nearest neighbor graph. One hopes then to find a new representation space in which two data points are sufficiently close to each other if they are connected in the graph. To achieve this, we design a new matrix factorization objective function and incorporates the graph structure into it. We also develop an optimization scheme to solve the objective function based on iterative updates of the two factor matrices. This leads to a new parts-based data representation which respects the geometrical structure of the data space. The convergence proof of our optimization scheme is provided.

The rest of the paper is organized as follows: in Section 2, we give a brief review of NMF. Section 3 introduces our algorithm and give a convergence proof of our optimization scheme. Extensive experimental results on clustering are presented in Section 4. Finally, we provide some conclud-ing remarks and suggestions for future work in Section 5.
Non-negative Matrix Factorization (NMF) [14] is a ma-trix factorization algorithm that focuses on the analysis o f data matrices whose elements are nonnegative.

Given a data matrix X = [ x 1 , , x n ]  X  R m  X  n , each column of X is a sample vector. NMF aims to find two non-negative matrices U = [ u ij ]  X  R m  X  k and V = [ v ij ]  X  R where kk F denotes the matrix Frobenius norm 1 .

Although the objective function O in Eqn. (1) is convex in U only or V only, it is not convex in both variables to-gether. Therefore it is unrealistic to expect an algorithm t o find the global minimum of O . Lee &amp; Seung [15] presented an iterative update algorithm as follows: mimimum of the objective function O [15].

In reality, we have k  X  m and k  X  n . Thus, NMF essentially try to find a compressed approximation of the original data matrix, X  X  UV T . We can view this approxi-mation column by column as where u j is the j -th column vector of U . Thus, each data vector x i is approximated by a linear combination of the columns of U , weighted by the components of V . Therefore U can be regarded as containing a basis that is optimized for the linear approximation of the data in X . Since relatively few basis vectors are used to represent many data vectors, good approximation can only be achieved if the basis vec-tors discover structure that is latent in the data [15].
The non-negative constraints on U and V only allow ad-dictive combinations among different basis. This is the mos t significant difference between NMF and other other ma-trix factorization methods, e.g ., SVD. Unlike SVD, no sub-tractions can occur in NMF. For this reason, it is believed that NMF can learn a parts-based representation [14]. The advantages of this parts-based representation has been ob-served in many real world problems such as face analysis [16], document clustering [29] and DNA gene expression analysis [4].
By using the non-negative constraints, NMF can learn a parts-based representation. However, NMF performs this learning in the Euclidean space. It fails to to discover the i n-trinsic geometrical and discriminating structure of the da ta Section, we introduce our Graph regularized Non-negative Matrix Factorization (GNMF) algorithm which avoids this limitation by incorporating a geometrically based regular -izer. 3.1. The Objective Function
Recall that NMF tries to find a basis that is optimized for the linear approximation of the data which are drawn ac-cording to the distribution P X . One might hope that knowl-edge of the distribution P X can be exploited for better dis-covery of this basis. A natural assumption here could be that if two data points x i , x j are close in the intrinsic geometry of the data distribution, then the representations of this t wo points in the new basis are also close to each other. This as-sumption is usually referred to as manifold assumption [2], which plays an essential rule in developing various kinds of algorithms including dimensionality reduction algorithm s [2] and semi-supervised learning algorithms [3, 32, 31].
Let f k ( x i ) = v ik be function that produce the mapping of the original data point x i onto the axis u k , we use to measure the smoothness of f k along the geodesics in the intrinsic geometry of the data. When we consider the case that the data is a compact submanifold M X  R m , a natural choice for k f k k 2 where  X  M is the gradient of f k along the manifold M and the integral is taken over the distribution P X .
In reality, the data manifold is usually unknown. Thus, k f k 2 M in Eqn. (5) can not be computed. Recent studies on spectral graph theory [7] and manifold learning theory [1] have demonstrated that k f k k 2 mated through a nearest neighbor graph on a scatter of data points.

Consider a graph with n vertices where each vertex cor-responds to a data point. Define the edge weight matrix W as follows: where N p ( x i ) denotes the set of p nearest neighbors of x Define L = D  X  W , where D is a diagonal matrix whose en-tries are column (or row, since W is symmetric) sums of W , D ii = P j W ij . L is called graph Laplacian [7], which is a discrete approximation to the Laplace-Beltrami operator  X 
M on the manifold [1]. Thus, the discrete approximation of k f k k 2 M can be computed as follows: R k can be used to measure the smoothness of mapping function f k along the geodesics in the intrinsic geometry of the data set. By minimizing R k , we get a mapping func-tion f k which is sufficiently smooth on the data manifold. A intuitive explanation of minimizing R k is that if two data points x i and x j are close ( i.e . W ij is big), f k ( x are similar to each other.

Our GNMF incorporates the R k term and minimize the objective function with the constraint that u ij and v ij are non-negative. Tr( ) denotes the trace of a matrix. The  X   X  0 is the regulariza-tion parameter. 3.2. An Algorithm
The objective function O of GNMF in Eqn. (8) is not convex in both U and V together. Therefore it is unrealistic to expect an algorithm to find the global minimum of O . In the following, we introduce an iterative algorithm which can achieve a local minimum.
 The objective function O can be rewritten as:
O = Tr ( X  X  UV T )( X  X  UV T ) T +  X  Tr( V T LV ) where the second step of derivation uses the matrix property Tr(
AB ) = Tr( BA ) and Tr( A ) = Tr( A T ) . Let  X  ij and  X  ij be the Lagrange multiplier for constraint u ij  X  0 and v ij 0 respectively, and  X  = [  X  ij ] ,  X  = [  X  ij ] , the Lagrange
L = Tr XX T  X  2 Tr XVU T + Tr UV T VU T The partial derivatives of L with respect to U and V are: Using the KKT conditions  X  ij u ij = 0 and  X  ij v ij = 0 , we get the following equations for u ij and v ij : These equations lead to the following update rules: Regarding these two update rules, we have the following theorem: Theorem 1 The objective function O in Eqn. (8) is nonin-creasing under the update rules in Eqn. (15) and (16). The objective function is invariant under these updates if and only if U and V are at a stationary point.
 Theorem 1 grantees that the update rules of U and V in Eqn. (15) and (16) converge and the final solution will be a local optimum. Please see the Appendix for a detailed proof. Several authors have noted the shortcomings of standard NMF, and suggested extensions and modifications of the original model.

One of the shortcomings of NMF is that it can only be applied to data containing non-negative values. Ding et al . [10] proposed a semi-NMF approach which relaxes the non-negative constraint on U . Thus, semi-NMF can be used to model data containing negative values. Xu &amp; Gong [28] proposed a Concept Factorization approach in which the in-put data matrix is factorized into three matrix X  X  XWV T . Both W and V are non-negative. Such modification makes it possible to kernelize concept factorization. This conce pt factorization approach is also referred as convex-NMF [10] .
Another shortcoming of NMF is that it does not always result in parts-based representations. Several researche rs addressed this problem by incorporating the sparseness con -straints on U and/or V [11], [19], [12]. These approaches extended the NMF framework to include an adjustable sparseness parameter. With a suitable sparseness parame-ter, these approaches are guaranteed to result in parts-bas ed representations.

Besides the most well known multiplicative update algo-rithm [15], there are many other optimization methods that can solve the NMF problem in Eqn. (1). One of the most promising approaches is projected gradient method. Lin [18] shows that projected gradient method converges faster than the popular multiplicative update algorithm. Moreove r, it is easy to use projected gradient method to solve the NMF problem with sparse constraints [12].

The above extensions and modifications focus on the dif-ferent aspects of the original NMF. However, they all fail to consider the geometrical structure in the data. Our approac h discussed in this paper presents a new direction for extend-ing NMF. For more discussions on the relationship between various NMF extensions, please refer [17], [12], [6].
Previous studies show that NMF is very powerful on clustering [29, 24]. It can achieve similar or better per-formance than most of the state-of-the-art clustering algo -rithms, including the popular spectral clustering methods [29]. In this section, we also evaluate our GNMF algorithm on clustering problems.
 Two data sets are used in the experiment. The first one is COIL20 image library 2 , which contains 32  X  32 gray scale images of 20 objects viewed from varying angles. The sec-ond one is the CMU PIE face database 3 , which contains 32  X  32 gray scale face images of 68 persons. Each person has 21 facial images under different light conditions.
There are two parameters in our GNMF approach: the number of nearest neighbors p and the regularization pa-rameter  X  . Throughout our experiments, we empirically set the number of nearest neighbors p to 5, the value of the reg-ularization parameter  X  to 100. 5.1. Evaluation Metric
The clustering result is evaluated by comparing the ob-tained label of each sample with that provided by the data set. Two metrics, the accuracy ( AC ) and the normalized mutual information metric ( MI ) are used to measure the clustering performance [29][5]. Given a data point x i , let and s i be the obtained cluster label and the label provided by the corpus, respectively. The AC is defined as follows: where n is the total number of samples and  X  ( x,y ) is the delta function that equals one if x = y and equals zero oth-erwise, and map( r i ) is the permutation mapping function that maps each cluster label r i to the equivalent label from the data corpus. The best mapping can be found by using the Kuhn-Munkres algorithm [21].

Let C denote the set of clusters obtained from the ground truth and C  X  obtained from our algorithm. Their mutual information metric MI ( C,C  X  ) is defined as follows:
MI ( C,C  X  ) = X where p ( c i ) and p ( c  X  arbitrarily selected from the data set belongs to the cluste rs c and c  X  that the arbitrarily selected sample belongs to the cluster s as well as c  X  the normalized mutual information MI as follows: where H ( C ) and H ( C  X  ) are the entropies of C and C  X  spectively. It is easy to check that MI ( C,C  X  ) ranges from 0 to 1. MI = 1 if the two sets of clusters are identical, and MI = 0 if the two sets are independent. 5.2. Performance Evaluations and Comparisons
To demonstrate how the clustering performance can be improved by our method, we compared GNMF with other four popular clustering algorithms as follows:  X  Canonical K-means clustering method (K-means in k 4 48.8 54.6 99.0 69.9 98.4 42.1 47.5 98.6 63.6 98.4 6 43.2 50.9 94.7 76.1 97.2 48.3 54.7 96.4 76.3 98.0 8 41.3 44.4 86.5 78.9 91.0 50.2 53.2 92.3 81.8 95.6 10 40.8 41.4 80.3 78.3 88.4 53.0 53.9 89.6 83.6 94.9 12 40.1 40.9 79.6 78.3 85.9 55.8 55.8 89.5 85.1 94.0 14 38.4 39.2 79.3 76.5 85.0 56.1 56.9 89.6 85.1 93.9 16 37.7 38.6 78.4 77.4 85.1 57.3 58.2 89.4 86.5 94.3 18 38.3 38.8 73.9 77.9 82.2 59.2 59.6 87.6 87.4 93.1 20 37.1 37.5 77.0 77.0 80.7 59.1 59.3 88.4 87.4 92.8
Avg 40.6 42.9 83.2 76.7 88.2 53.5 55.5 91.3 81.9 95.0 k is the number of clusters  X  K-means clustering in the Principle Component sub- X  Normalized Cut [25], one of the typical spectral clus- X  Nonnegative Matrix Factorization based clustering
Table 1 and 2 show the evaluation results on the PIE data set and the COIL20 data set, respectively. The evaluations were conducted with the cluster numbers ranging from two to ten. For each given cluster number k , 20 test runs were conducted on different randomly chosen clusters. The aver-age performance is reported in the tables.

These experiments reveal a number of interesting points:  X  The ordinary NMF approach outperforms K-means  X  Both NCut and GNMF consider the geometrical struc-k 2 90.0 90.3 95.0 88.4 96.7 70.0 71.0 86.9 64.0 90.8 3 84.8 85.1 90.0 79.4 92.8 71.9 72.3 84.2 64.9 88.4 4 81.7 82.0 89.0 78.7 92.7 74.3 74.9 87.4 71.1 90.3 5 75.9 76.7 83.0 72.1 91.1 71.7 72.3 82.0 67.2 89.1 6 76.5 76.9 82.2 72.1 91.0 74.4 75.0 83.3 70.3 91.5 7 72.9 74.0 77.3 68.8 87.4 72.4 72.7 80.1 67.7 89.5 8 71.8 72.4 77.9 70.2 85.2 74.0 74.6 81.9 71.6 89.1 9 69.4 70.5 75.9 68.3 86.1 72.8 73.8 82.6 71.5 89.2 10 69.3 70.7 77.8 70.3 85.0 74.8 75.4 83.5 73.9 89.6
Avg 76.9 77.6 83.1 74.3 89.8 72.9 73.6 83.5 69.1 89.7 k is the number of clusters 5.3. Parameters Selection
Our GNMF model has two essential parameters: the number of nearest neighbors p and the regularization pa-rameter  X  . Figure 3 and Figure 4 show how the performance of GNMF varies with the parameters  X  and p , respectively. As we can see, the GNMF is very stable with respect to both the parameter  X  and p . It achieves consistent good perfor-mance with the  X  varying from 50 to 1000 and p varying from 3 to 6.
We have presented a novel method for matrix factoriza-tion, called Graph regularized Non-negative Matrix Factor -ization (GNMF). GNMF models the data space as a sub-manifold embedded in the ambient space and performs the non-negative matrix factorization on this manifold in ques -tion. As a result, GNMF can have more discriminating power than the ordinary NMF approach which only consid-ers the Euclidean structure of the data. Experimental resul ts on visual objects clustering show that GNMF provides bet-ter representation in the sense of semantic structure.
Several questions remain to be investigated in our future work: 1. There is a parameter  X  which controls the smoothness 2. It would be very interesting to explore different ways good performance with the  X  varying from 50 to 1000. [1] M. Belkin. Problems of Learning on Manifolds . PhD [2] M. Belkin and P. Niyogi. Laplacian eigenmaps and [3] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold reg-[4] J.-P. Brunet, P. Tamayo, T. R. Golub, and J. P. [5] D. Cai, X. He, and J. Han. Document clustering us-[6] M. Chu, F. Diele, R. Plemmons, and S. Ragni. Opti-[7] F. R. K. Chung. Spectral Graph Theory , volume 92 [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. [9] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maxi-[10] C. Ding, T. Li, and M. Jordan. Convex and to 6.
 [11] P. O. Hoyer. Non-negative sparse coding. In Proc. [12] P. O. Hoyer. Non-negative matrix factorizaiton with [13] I. T. Jolliffe. Principal Component Analysis . Springer-[14] D. D. Lee and H. S. Seung. Learning the parts of [15] D. D. Lee and H. S. Seung. Algorithms for non-[16] S. Z. Li, X. Hou, H. Zhang, and Q. Cheng. Learn-[17] T. Li and C. Ding. The relationships among various [18] C.-J. Lin. Projected gradient methods for non-[19] W. Liu, N. Zheng, and X. Lu. Non-negative ma-[20] N. K. Logothetis and D. L. Sheinberg. Visual object [21] L. Lovasz and M. Plummer. Matching Theory . [22] P. Paatero and U. Tapper. Positive matrix factoriza-[23] S. E. Palmer. Hierarchical structure in perceptual rep -[24] F. Shahnaza, M. W. Berrya, V. Paucab, and R. J. Plem-[25] J. Shi and J. Malik. Normalized cuts and image seg-[26] M. Turk and A. Pentland. Eigenfaces for recognition. [27] E. Wachsmuth, M. W. Oram, and D. I. Perrett. Recog-[28] W. Xu and Y. Gong. Document clustering by con-[29] W. Xu, X. Liu, and Y. Gong. Document cluster-[30] H. Zha, C. Ding, M. Gu, X. He, , and H. Simon. Spec-[31] D. Zhou, O. Bousquet, T. Lal, J. Weston, and [32] X. Zhu and J. Lafferty. Harmonic mixtures: com-
The objective function O of GNMF in Eqn. (8) is cer-tainly bounded from below by zero. To prove Theorem 1, we need to show that O is nonincreasing under the update steps in Eqn. (15) and (16). Since the second term of O only related to V , we have exactly the same update formula for U in GNMF as the original NMF. Thus, we can use the convergence proof of NMF to show that O is nonincreas-ing under the update step in Eqn. (15). Please see [15] for details.

Now we only need to prove that O is nonincreasing un-der the update step in Eqn. (16). we will follow the similar procedure described in [15]. Our proof will make use of an auxiliary function similar to that used in the Expectation-Maximization algorithm [9]. We begin with the definition of the auxiliary function .
 Definition G ( v,v  X  ) is an auxiliary function for F ( v ) conditions are satisfied.

The auxiliary function is very useful because of the fol-lowing lemma.
 Lemma 2 If G is an auxiliary function of F , then F is non-increasing under the update Proof
Now we will show that the update step for V in Eqn. (16) is exactly the update in Eqn. (17) with a proper auxiliary function.

We rewrote the objective function O of GNMF in Eqn. (8) as follows O = k X  X  UV T k 2 F +  X  Tr( V T LV ) = Considering any element v ab in V , we use F ab to denote the part of O which is only relevant to v ab . It is easy to check that
F  X  ab =  X  O  X  V Since our update is essentially element-wise, it is sufficie nt to show that each F ab is nonincreasing under the update step of Eqn. (16).
 Lemma 3 Function is an auxiliary function for F ab , the part of O which is only relevant to v ab .
 Proof Since G ( v,v ) = F ab ( v ) is obvious, we need only show that G ( v,v ( t ) Taylor series expansion of F ab ( v ) with Eqn. (21) to find that G ( v,v ( t ) to We have and Thus, Eqn. (23) holds and G ( v,v ( t ) We can now demonstrate the convergence of Theorem 1: Proof of Theorem 1 Replacing G ( v,v ( t ) Eqn. (21) results in the update rule: Since Eqn. (21) is an auxiliary function, F ab is nonincreas-
