 Consider this: two isolated prisoners, communi-cating by letters scrutinised by the prison warden. They cannot write openly about escape plots, and the warden destroys any written in code. They must hide the true message within the letter, us-ing steganography : the art of hiding information.
In cover modification 1 steganography, a cover object is tweaked so that it carries a hidden mes-sage: this is called embedding , the tweaked ver-sion is the stego object, and the message is the pay-load (Fridrich, 2009). The message should not be detectable to any observer (the standard terminol-ogy for this is the warden , taken from the prisoner metaphor) who knows the system is deployed, but does not know the original cover.

We are concerned here with linguistic steganog-raphy , in which the cover is a piece of text, and the message is embedded using textual transfor-mations intended to preserve the meaning of the original: synonym substitutions, syntactical trans-formations, etc. Note that we are not concerned with the subset of linguistic steganography that hides in file formatting (e.g. white space, as in Por et al. (2008)), which has no security against an informed warden (in the case of hiding informa-tion by adding extraneous white space, the warden simply has to look for consistent irregular use of spaces to spot an active steganographer).

The field suffers from a number of issues: com-pared to images, text covers have low capac-ity (Chang and Clark, 2010); certain methods are weak against human attackers (Grosvald and Orgun, 2011) (most paraphrase systems cannot guarantee perfectly fluent stego objects); finally, authors are generally concerned with the perfor-mance of the transformation (whether they pro-duce grammatically/semantically correct transfor-mations), rather than whether the generated stego objects are detectable or not (e.g. Chang and Clark (2010)).
 However, there is a new challenger in the field. We proposed a new linguistic stegosystem (Wilson et al., 2014) and verified its security against hu-man judges, who were unable to distinguish gen-uine covers from manipulated stego objects.
This paper aims to attack CoverTweet statisti-cally. We are in the shoes of the warden, attempt-objects. We propose techniques new to linguis-tic steganalysis, including a large set of features that detect unusual and inconsistent use of lan-guage and the aggregation of evidence from mul-tiple sentences. This last development, known in the steganographic literature as pooled steganaly-sis (Ker, 2007), represents a first in both linguistic and image steganalysis. T-Lex (Winstein, 1998) is the oldest available cover-modification based linguistic stegosystem. It uses a dictionary containing a small number of disjoint synonym sets extracted from Word-Net (Miller, 1995). Each set is unambiguously or-dered (e.g. alphabetically), then values are embed-ded by changing cover words for their synonyms. Due to the the small dictionary, the capacity of covers is only  X  0 . 1 bits per sentence.
 CoverTweet is a modern evolution of T-Lex. It hides information in tweets by applying para-phrase rules taken from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013), a set of 169M rules. The system applies suitable rules to a given cover, generating a set of possible stego objects. These are ranked by a distortion measure (derived from the probabilities of applied rules and from sentence probabilities given by a language model), and assigned a keyed hash. A human operator fil-ters the options for fluency, and chooses the best stego object with the desired hash.

CoverTweet uses a subset of the PPDB, re-stricted to lexical and phrasal substitutions. Even with the reduced set of rules, 4 bits can be embed-ded per tweet, and it was proven secure against human judges (Wilson et al., 2014).
 Twitter is a realistic setting for steganography. There is precedent for information hiding and mass monitoring on micro-blogging sites, such as the use of code words and government censorship on the Chinese website Sina Weibo (Chen et al., 2013). For this reason, we are attacking this set-ting.

There are many other linguistic stegosystems, using an array of different hiding methods (e.g. adjective deletion, word order, anaphora reso-lution: (Chang and Clark, 2012a), (Chang and Clark, 2012b), (Vybornova and Macq, 2007)). Approximately 1 bit of payload per cover sentence is usual, making CoverTweet the exception. Un-fortunately for steganalysis literature, the vast ma-jority of these require data that is unavailable (and too expensive to reproduce); beyond CoverTweet, the only system that can be evaluated is T-Lex. To our knowledge, there have been only five prior attempts at linguistic steganalysis on cover mod-ification based systems; of these, four attack T-Lex, the other attacks an equivalent proprietary system. Taskiran et al. (2006) was the first, us-ing n -gram language models to extract features from stego text, before training a support vector machine (SVM) on the features. We will adopt some of these features for our attack.

Subsequent work (Xin-guang et al. (2006); Yu et al. (2009); Chen et al. (2011); Xiang et al. (2014)) has used smaller models: they have all de-signed a single feature to exploit a weakness, and used this (or the mean and variance of it) to train a classifier for attack. Analysis of results, especially the effect of embedding rate on detection, has been lacking or non-existent. This focus on individual features echoes the early work on image steganal-ysis, which has since shifted towards feature-rich models. We will be utilising the latter here, in ad-dition to the pooled steganalysis paradigm. Below we describe four classes of proposed fea-tures for individual tweets. Each class will be eval-uated individually, and in combination.

Basic features Including: word count (includ-ing tokenised punctuation); the mean and variance of the number of characters in each word; the total ual stop word. (131 features) n -gram features Using a 5-gram model, for n from 1 to 5, the mean, variance and total log like-lihood of the n -grams in the tweet. (15 features)
Word length Equivalent to the n -gram features, using a 10-gram model of word length. We expect the PPDB to replace common words with uncom-mon longer words, or multiple shorter words. (30 features)
PPDB features Kerckhoffs X  principle (Fridrich, 2009), which states that a stegosystem must be se-cure even when the attacker ( us ) knows how the system works, introduces an interesting opportu-nity for linguistic steganalysis. As all linguistic stegosystems rely on automatic paraphrasing, and generally require a source of data, it is conserva-tive to assume that the attacker has access to this data source. As the attacker, we can derive fea-tures from this data.

Applying this, we extracted features using the same subset of the PPDB that CoverTweet uses. To our knowledge, this is the first work to ex-plicitly apply Kerckhoffs X  principle to linguistic steganalysis. This set includes: a repeat of the n -gram features, but only taking into account n -grams that contain at least one word in 4 the PPDB; the mean and variance of character count for words in the PPDB; for phrase lengths j from 1 to 5, the proportion of j -grams in the tweet that are in the PPDB; likelihood of the most proba-ble paraphrased sentence, extracted by generating sentences in the same way as CoverTweet. Finally, a substitution score: each word or phrase in the PPDB is replaced by its most likely substitute, ac-cording to the language model. The max, min, mean and variance of the log likelihoods for the resulting sentences are used. (27 features) 4.1 Pooling the features Individual linguistic stego objects carry a tiny pay-load, making detection an incredibly difficult task. In some cases, the stego tweet will be identical to the cover, making the task impossible. Although we do not hope to reliably identify stego tweets individually, the warden can still try to catch the active steganographers: through pooled steganal-ysis (Ker, 2007). Here, the warden considers all the evidence for each user, and aims to identify those actively using steganography. We combine features from batches of tweets, by the same user: taking the max, min, mean and variance of each feature. If performing steganography causes small consistent changes to a feature, this will be appar-ent in the mean. If it causes inconsistent changes, it may be apparent in the variance. If it occasion-ally produces an extreme result, the max and min will capture this. (812 features) Here we take the role of an automatic censor, at-tempting to identify users on Twitter who are send-ing secret messages. We imagine a scenario where CoverTweet is openly available, and so can as-sume this is the system utilised by the active users. 5.1 Data and Embedding From the Harvard TweetMap (Mostak, 2013), we gathered 72M English language tweets, from 1.2M users. Each tweet was canonicalised as fol-lows: tokenised; made lowercase; usernames re-placed by a uniform token; URLs replaced by a uniform token. We randomly selected 1000 users with 1000-2000 tweets to train and test a classifier. On the remaining tweets, we trained a 10-gram word length model, and a 5-gram language model, using SRILM (Stolcke, 2002) with Kneyser-Ney smoothing. For vocabulary, the language model was given every word in the PPDB, and every word in the set of tweets, including the removed tweets. We do not expect this to provide an unre-alistic advantage to the censor: any word not in the PPDB cannot hide information.

We randomly took 10 users from the set of 1000, and produced 100 stego tweets for each, for three payload sizes: 1, 2 and 4 bits. The stego tweets were generated using CoverTweet, with a human operator selecting the most fluent option containing a desired (randomly generated) pay-load. If there were no fluent options, the tweet was skipped. If the tweet already contained the desired payload (if the hash value of the tweet already matched the message), it was left unchanged. We refer to this data as Manual CoverTweet (M-CT).
Due to the expensive nature of generating data with a human judge, and to assess the value of the human in-the-loop, we also automatically gen-erated 1M stego tweets, by embedding data in 1000 tweets for each of the 1000 users. The same three payload sizes were used. Here, the tweet with the highest probability (provided by the lan-guage model and the PPDB) was selected. Again, the tweet was left unchanged if the tweet already contained the desired payload. Tweets were only skipped if there were no options with the correct payload. We refer to this as Automatic Cover-Tweet (A-CT). Finally, we embedded 1 bit in the same tweets using T-Lex, rejecting any that con-tained no words in the T-Lex dictionary; the result was approximately 100 tweets on average per user.
We split the users in half, training a linear ensemble classifier (Kodovsk ` y et al., 2012) (de-signed to work with large feature sets for steganal-ysis) on feature instances from one half, testing it on the other. We only classified tweets by users for whom the classifier had no prior knowledge. All error rates are averaged over 10 different random user splits. For the M-CT data, where we have a much smaller set of data, we performed 10-fold cross validation, leaving one user out for testing each fold.

In each experiment we matched training and testing data: the training data was produced by the Figure 1: ROCs for each embedding method, for individual tweets or batches containing 1 bit. Table 1: AUC values for the ROCs shown in Fig-ure 1 same method as the testing data. For M-CT we tried an alternative scheme, by training on A-CT data, but testing on M-CT; this did not work reli-ably, and the accuracy of the resulting model went down as pooled batch size increased. The explo-ration of this phenomenon is left for later work. 5.2 A note on unchangeable covers Some tweets cannot hide the payload, either when there are no paraphrase options for that hash, or when the human (for M-CT) vetoes all the op-tions. This is the non-shared selection channel problem in steganography. Methods such as syn-drome trellis codes (Filler et al., 2011) allow the unchangeable cover elements to be sent without compromising the message. These have not been applied to linguistic steganography, but we simu-late their use: we remove the unchangeable tweets from the cover and stego sets, essentially giving the steganographer and detector the ability to ig-nore such tweets. Figure 2: The effect of batch size on error rate. Batches of 100 are the maximum for M-CT and T-Lex, but we go up to 1000 with A-CT. 5.3 Results As expected, the performance of the classifiers trained on individual tweets is poor (see Figure 1). In particular, the models trained on A-CT and M-CT data have very low accuracy on data with 1 bit payload, performing only slightly better than ran-dom guessing. T-Lex tweets prove slightly easier to detect. This does not mean that the systems are secure however. Though we cannot identify in-dividual stego tweets, when we pool evidence we find we are able to train a model that can identify active users with high accuracy, for all data sets.
Figure 2 shows the change in error rate as the batch size (the number of tweets we pool) is in-creased. We can clearly see that increasing batch size improves accuracy. We also see that in-creasing payload size makes active users easier to spot. This is unsurprising: CoverTweet is forced to choose from fewer options when payload in-creases.

The experiment showed us that M-CT is the most secure stegosystem, though not immune to the effects of pooling. When combining features from 100 tweets, the classifier had an error rate of 0.21 on 4 bit M-CT data; data generated in the same way, with the same payload, was previously shown to be secure against human judges. With large batches, detection of A-CT is almost perfect.
To establish which class of features had the biggest effect on detection, we trained models on each combination of features described in Sec-tion 4. A subset of these combinations are shown in Table 2, for A-CT and T-Lex.We can see that for A-CT, the PPDB features easily outperform oth-ers, with an error rate of 0.217 when used alone; the warden X  X  knowledge of the system is power-ful. The second best is the n -gram set, though it performs better on T-Lex than on A-CT. This is most likely due to CoverTweet X  X  use of the lan-guage model in the embedding stage: the system is attempting to minimise the distortion that these features are looking for. T-Lex has no such distor-tion measure, leaving it open to this sort of attack. Basic and PPDB feature sets fare worse on T-Lex. The basic features are aimed at changes in word count and stop word usage: neither of these are affected by T-Lex substitutions. The PPDB features are at a disadvantage with T-Lex, as they are designed for CoverTweet. If T-Lex X  X  data source were used instead of CoverTweet X  X , the per-formance would likely improve significantly.
The combination of PPDB and n -gram features on T-Lex gives us some interesting insight: despite the mismatch of substitution source, we still see an improvement over the n -gram features used on their own. This suggests that the warden does not need the exact data source as the steganographer for these features to be useful. It was believed that linguistic steganography was weak against humans, but CoverTweet disproved it. We have shown that individual stego objects are seemingly also strong against statistical attacks. However, by pooling multiple pieces of evidence against a user, the warden can drastically improve detection rate. With each steganographic tweet sent, the user creeps closer to being caught. This is the first steganalytic classifier, in any domain, that successfully exploits pooled evidence. The design of steganographic systems must now take this type of attacker into account. It would inter-esting to determine whether human judges are ca-pable of pooling large amounts of scant evidence: we conjecture not.

Results suggest that detection is improved by utilising rich-feature models; we only scratched the surface with regards to this. There are many avenues to explore, such as using multiple lan-guage models from which to extract features (this is the analogue of filter banks used in contempo-rary image steganalysis (Fridrich and Kodovsk ` y, 2012)).

The security of a system should first be mea-sured against a powerful (informed) attacker. We played this role by using features extracted using exact knowledge of the CoverTweet system (the PPDB features); this class of features was partic-ularly effective against CoverTweet. The system should now be evaluated against a weaker attacker. We have seen that detection is still reliable when the warden knows the wrong system (T-Lex), but further experiments are required to determine ex-actly how detection rate is affected by mismatches in paraphrase sources, or language model.

