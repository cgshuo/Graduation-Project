 Yueshen Xu 1( Recommender systems have been an indispensable component in e-commerce sites, which help users select their favorite products from a large number of candidates. There are mainly three kinds of recommendation algorithms, i.e., CF (Collaborative Filtering)-based, content-based and hybrid approaches. Among all CF-based algorithms, models based on MF (Matrix Factorization) have been verified to achieve satisfactory accuracy in rating prediction, and therefore widely studied in academia and industry [ 9 ]. Meanwhile, content-based recommendation algorithms still play an important role in recommender systems [ 15 , 18 ]. ous kinds of content created by users on web sites [ 14 ], including social tag, user review, question answer, blog, tweet, etc. User generated content widely exists in e-commerce sites and social networking sites, and has been employed in rec-ommendation problems, such as tag-aware recommendation and review-based recommendation [ 10 , 13 , 23 ]. For example, Liang et al. [ 10 ] proposed a weighted tag-based top-N recommendation algorithm. Each item and each user had a pro-file consisting of tags, which were used to calculate the similarity between each pair of items and each pair of users. Although UGC has been verified to be helpful in existing works, there are still several problems stated below: 1. Not every web site allows users to tag products. For example, Ebay Epinion 2 do not allow consumers to tag products. 2. Since the item-tag space is highly sparse, it is difficult to get accurate results when we conduct similarity computation between items or tags. 3. The problems of synonym and polysemy are ubiquitous, since words that are used in UGC are highly personalized and 60% of words are personally pro-prietary [ 3 ]. 4. The existing researches only focus on one kind of UGC, and utilize different kinds of UGC in very different ways. This makes it hard to find common features that are shared among various types of UGC (such as tags and reviews). To solve the above problems, we study various types of UGC in recommender systems. Since user reviews and social tags are the two common types of UGC, in this paper, we mainly focus on reviews and tags. Because user reviews almost exist in all e-commerce sites and social networking sites, our work can be used in extensive web sites. We utilize topic modeling technique to transform highly spare word space into low topic space. In the topic space, we compute similarity for each two items and each two users. Also, topic modeling techniques can find the semantic relations among words. Besides, we integrate both types of UGC into recommendation algorithms in a unified manner.
 There have been many works that utilize the social or trust relationship beyond the single user-item rating matrix to help infer users X  preferences [ 11 , 12 , 22 ]. These methods assume that a user X  X  preference tends to be influ-enced by his friends or trusted people. Indeed, social recommendation makes a great progress and improves the prediction accuracy. However, in many domi-nating e-commerce sites, like Amazon 3 , Ebay 4 , Newegg 5 are no social relationships so that these sites can hardly benefit from the social recommendation techniques. In such a case, we still have to infer a user X  X  inter-est according to his consumption records. Note that a user X  X  interest is usually related to some certain topics. For example, if John is a fan of Harry Potter , he may not only like the related movies, but also the related books, DVDs, gadgets and clothes. Additionally, he may also mark tags and write reviews on the related items. Although these products are in different categories, they have common features that can be inferred by UGC. However, most of existing works just conduct collaborative filtering based on users X  feedbacks on each individ-ual product separately. In contrast, in this paper, we study a user X  X  interest distribution under different topics based on clustering to better understand his preference. are edited by website editors, to help learn item features [ 6 , 16 , 21 ]. Their rec-ommendation algorithms are based on the collaborative topic regression model (CTR) [ 21 ], which employs LDA (Latent Dirichlet Allocation) [ 4 ] to learn the intrinsic features of items. However, there are two problems in these works: that are under the same category. This is because a large part of words used in item descriptions overlap with each other. For example, the descriptions of laptops that are produced by Samsung and Lenovo are similar, since both of them consist of similar words that are used to depict product features, such as memory , cpu and price . are independent to users.
 Last.fm 7 , among all tags that the song My Heart Will Go On receives, love , pop , ballad and soundtrack are frequently used, which can well describe this song X  X  main topics. Also, UGC can reflect a user X  X  preference well. For example, if Bill wrote a lot of reviews containing words like superhero , technology and fic-tion , it can be inferred that Bill may like movies such as Iron Man and Batman , since these movies have related themes with fiction and superhero .
 1. It studies the function of UGC in learning users X  interests and learning items X  2. It proposes a user-oriented collaborative filtering model and an item-oriented 3. It conducts sufficient experiments on three real-world datasets, which attest As expounded in Sect. 1 , a user X  X  interest is specific to certain topics. For exam-ple, if Tom is a fan of Jackie Chan, he may not only watch his movies, but also buy relevant posters, clothes and books, even though these items are in different categories. In this paper, we employ the user X  consumption records on different item groups to infer a user X  X  interest. In the rest of the paper, the word item refers to all kinds of things that are provided to consume on the Internet, such as electronic products, restaurants, songs and movies. 2.1 Matrix Factorization Model In this paper, we employ Matrix Factorization (denoted by MF in this paper) as the basic prediction model for its effectiveness and popularity [ 9 ]. As a typical latent factor model, MF can factorize the high dimensional space into two low latent dimensional spaces. Concretely, let R  X  R M  X  N ing matrix, which is usually extremely sparse. M , N are the numbers of users and items separately. Through MF-based technique, R can be estimated by the multiplication of latent user feature matrix U ( U  X  R M  X  D feature matrix V ( V  X  R N  X  D ), where D is the number of latent features. D is far smaller than M and N .Eachentry R ij (1  X  i  X  M, 1  X  j estimated as the inner product of the two corresponding column vectors of U and V as where U i is the i th column vector of U ,and V j is the j th column vector of V . To minimize the squared estimation error, the objective function of MF is con-structed as follows: where I ij is an indicator function. If user i gave a rating to item j before, I equals 1. Otherwise, I ij equals 0.  X  2 F denotes the Frobenius norm, and  X   X  are both small constants. Gradient descent algorithm can be used to achieve the local optima of U and V , and thus R can be filled completely by estimating those missing values according to Eq. 1 . 2.2 Topic Analysis for Items Through LDA In this subsection, we aim to find the shared topics among items through topic modeling technique.
 UGC records a user X  X  personal impression and understanding on an item, which also reflects this item X  X  characteristics. In UGC, a word that has been used frequently and uniquely is usually corresponding to one of the item X  X  significant features. Therefore, an item X  X  specialty can be emphasized by its highly frequent words that are written in UGC. For example, in Douban 8 , among all tags that the movie Titanic receives, love , romantic , disaster and USA are frequently used, which can well describe this movie X  X  main topics. We combine the words used in UGC together to compose the whole words set W as follows: where T is the number of all unique words. Then, each item j will own a word vector W ( j )=( n ( j ) w 1 ,n ( j ) w 2 ,...,n ( j ) w t t  X 
T ) is the times of word w t that item j has received. Specially, if word w has never been applied to item j , n ( j ) w t in W ( j ) will equal 0. Thus, we can construct a weighted item-word co-occurrence matrix (a toy example is shown in Fig. 1 ), in which each row is W ( j ) corresponding to each item j . analysis in text mining and information retrieval [ 2 ]. In this paper, we chose Latent Dirichlet Allocation (LDA) [ 4 ] as the topic modeling technique for its popularity. In LDA, a document is assumed to be constructed in the gener-ative process. In a corpus, for each document j , a topic k is chosen accord-ing to the document-topic multi-nominal distribution  X  , where  X  j =(  X  j 1 , X  j 2 ,..., X  jK ) denotes the topic distribution under the document j , and  X  jk = p ( k | j )(1  X  k  X  K ) is the probability that topic k is chosen for document j . K and N are the numbers of topics and documents respectively. Then, a word w is sampled from the assigned topic k according to the topic-word multi-nominal distribution  X  , where  X  = {  X  k } K k =1 . Therefore, the probability distribution of topics over a document describes the hidden aspects of this docu-ment. As for each item, words extracted from UGC compose its text descriptions. Thus, in this paper, each item is regarded as a document with a collection of words W ( j ), and the item-word co-occurrence matrix is the corpus. With col-lapsed Gibbs sampling [ 8 ], we estimate the posterior parameters in LDA, i.e., (1  X  j  X  N )and  X  k (1  X  k  X  K ). 2.3 User Interest Distribution After topic analysis through LDA, to measure the topic similarity between two items j and h , we calculate the cosine similarity [ 1 ] between where  X  h is the topic distribution vector of item h .
 based on their similarities of topic distribution vectors (see Eq. 3 ). We use K-Means as the clustering algorithm for its easy implementation and popularity. For each user, we count the number of his historical consumption records on each cluster C q (1  X  q  X  Q ), by summing the times of his feedbacks on all items that belong to cluster C q . Thus, each user i will be associated with a vector that represents his consumption distribution on Q clusters. In this paper, we call this vector as user i  X  X  interest vector I i : where n i ( C q ) represents the total times of user i  X  X  feedbacks on cluster C fore, we can construct a user-interest distribution matrix, with a toy example showninFig. 2 . In this paper, we use both ratings record and interest vector to infer a users X  preference instead of only ratings. It is because a user X  X  interest is relatively stable, and is not easily affected by one or two unpleasant consumption experiences. For example, that once Tom gave a low rating to a T-shirt printed with a portrait of Jachie Chan does not mean that he will not be a fan of Jachie Chan anymore.
 Based on users X  interest vectors, the interest similarity of user i and l is calculated with cosine similarity as: users with him, denoted as L ( i )= { l | Sim ( i, l ) is in the top S similarity list Therefore, the weight of each neighbor l can be calculated as where e il  X  [0 , 1]. 2.4 Matrix Factorization with User Topic Regularization Since a user X  X  interest is positively correlated to his latent feature vector, the feature vectors of users having similar interests also tend to be similar. Therefore, user i  X  X  feature vector U i should be similar to those of his neighbors X . To model such a kind of similarity relation, the average feature vector of his neighbors is calculated as For each user, we try to minimize the difference between each pair of U as We call Eq. 6 user topic regularization (UTR) , and integrate it into the MF model to build the UTR-based MF model ( UTR-MF )as where  X  is a regulatory factor to regulate the effect of UTR. Gradient descent algorithm can also be used to achieve the local optima of U and V . Since an item X  X  specialty can be reflected by UGC, it can be inferred that two items that share similar UGC tend to share similar latent features. For example, the DVDs and books, which are affiliated to the movie The Lord of the Rings , receive many similar words in their reviews in Amazon. Naturally, they also share many similar latent features. After topic analysis presented in Sect. 2.2 , for each item j , those items that are highly similar to it (see Eq. 3 ) compose j  X  X  neighborhood H ( j ): H ( j )= { h | Sim ( j, h ) is in the top S similarity list where S is the size of H ( j ). Similar to Eq. 5 , each neighbor h  X  X  weight is where w jh  X  [0 , 1]. Since item j  X  X  topic distribution is similar to those of its neighbors X  in H ( j ), it can be inferred that their latent feature vectors should also be similar to each other. This is because a large part of latent factors reflect the item X  X  characteristics on different topics. To utilize this similarity relation, the average feature vector of the whole neighborhood is computed as Similar to the user topic regularization, we construct the item topic regularization (ITR) to minimize the difference among items X  feature vectors as The item topic regularization (ITR) is integrated into the MF model to build the ITR-based MF model ( ITR-MF )as where  X  is a regulatory factor, which is the same with the notation  X  in Eq. 7 . The partial derivatives of Eq. 10 over U i and V j are computed as follows:  X  E v  X  X  i =  X  E v  X  X  j = where G ( j ) contains those items whose neighborhoods include item j . Finally, gradient descent algorithm can be employed to achieve local optima of U and V based on the two partial derivatives. In this section, we conduct experiments on real-world datasets to evaluate our proposed models X  performance. 4.1 DataSet and Evaluation Metrics We use three real-word datasets in the following experiments, which are Movie-lens dataset, Last.fm dataset and Yelp dataset. Movielens dataset and Last.fm dataset are published by GroupLens research group 9 in the workshop HetRec X 11 [ 5 ]. Movielens dataset contains 2113 users, 10197 movies, 13222 tags and ratings. Last.fm dataset contains 1892 users, 17632 artists, 11946 tags and listening times of users to artists. We map listening times into the range 1 dataset is published by Yelp 10 , which contains 23152 users, 11537 restaurants, reviews and ratings.
 For Movielens dataset and Yelp dataset, we divide the whole dataset into the training set and testing set according to each user X  X  consumption timestamps. For example, in the training set of 80% density, if a user has given 10 ratings in time order, the first 8 old ratings will be allocated to the training set, and the last 2 new ratings will be the testing data. Since there is no timestamp in Last.fm dataset, we randomly select a certain percentage of the whole data as the training set and the remaining data will be the testing set. In this paper, we test the prediction accuracy on training sets with four different data densities, which are 60%, 70%, 80% and 90% respectively.
 as the evaluation metrics, which are shown below: where T S and | T S | represent the testing set and its size. R rating and corresponding estimated rating in T S respectively. 4.2 Performance Comparison and Parameter Setting Five well-known models are chosen as baselines to compare with our models: 1. UserCF : This is the well-known user-based collaborative filtering algorithm. 2. ItemCF : This is the well-known item-based collaborative filtering algo-3. PMF (Probabilistic MF model): This model provides the probabilistic inter-4. TF-IDF MF : This method first constructs the item content matrix, in 5. CTR : It is the collaborative topic regression model (CTR) proposed in [ 21 ]. number of clusters ( Q ), the number of latent features ( D ) and the regulatory factor (  X  ) are all set to 10. The neighborhood sizes ( S ) of items and users are set to 100.  X  U ,  X  V are set to 0 . 1 in all experiments. 4.3 Performance Comparison in Social Tag Case This subsection presents the model results when UGC is the social tag. The experiments are conducted on Movielens dataset and Last.fm dataset. Tables 1 and 2 show that for both datasets, our proposed models UTR-MF and ITR-MF outperform other baseline models in all cases of training set densi-ties. For example, in Last.fm dataset, on average over four training set densities, ITR-MF achieves 14 . 98% improvement than PMF, and 8 . 18% improvement than CTR for RMSE. Such an improvement demonstrates that user topic regulariza-tion (see Eq. 6 ) and item topic regularization (see Eq. 9 ) are both effective. In detail, it means that those users who have similar interest distributions indeed tend to have similar rating behaviors, and such a similar relation can be reflected by user topic regularization effectively. Also, it indicates that the items which have similar topic distributions indeed tend to be given similar ratings. Addi-tionally, the prediction accuracy of ITR-MF is higher than that of UTR-MF in all cases. It means that item topic regularization can make more contributions to improving MF model X  X  performance than user topic regularization. It also indicates that users X  interests are harder to infer than items X  features. Compared with all baseline models, the improvements achieved by UTR-MF and ITR-MF are both significant according to the paired t-tests ( p&lt; 0 . 0001). 4.4 Performance Comparison in User Review Case This subsection compares the prediction accuracy of models when UGC is user review. The parameter setting is the same with that in Sect. 4.3 , and the exper-iment is conducted on Yelp dataset.
 racy than other baseline models in all training set densities. For example, in the case that the training set density is 60%, ITR-MF gains 6 . 58% improvement than PMF, and 4 . 33% improvement than CTR for RMSE. It is verified that for reviews, user topic regularization can also help infer users X  interests, and item topic regularization can also help infer items X  characteristics. Since user reviews exist in almost all e-commerce sites and social networking sites, it indicates that our models have wide applicability. The improvements achieved by UTR-MF and ITR-MF are also both significant according to the paired t-tests ( p&lt; 0 . 0001). Besides, it can be seen that along with the increasing of training set density, the prediction errors decline. This is because more training data can provide more historical records to learn user and item latent features more accurately. This paper proposes two MF-based collaborative filtering models, which utilize UGC to solve the problem of rating prediction in recommender systems. Suffi-cient experiments on real-world datasets verify the effectiveness of our models. First, this paper demonstrates that UGC, such as tags and reviews, can be inte-grated into the MF model in a unified way to significantly improve the prediction accuracy. Second, this paper verifies that users X  interests and items X  features can indeed be reflected by different types of UGC. Third, this paper proposes two novel regularization terms, which can model the similarity between each pair of users and each pair of items effectively. These contributions are instructive for the utilization of other kinds of UGC to build more accurate recommender systems. One example is that we plan to employ tweets to infer users X  interests in social networking sites.

