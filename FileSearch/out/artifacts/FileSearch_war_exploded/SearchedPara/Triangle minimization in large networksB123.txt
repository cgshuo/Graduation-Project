 Rong-Hua Li  X  Jeffrey Xu Yu Abstract The number of triangles is a fundamental metric for analyzing the structure and function of a network. In this paper, for the first time, we investigate the triangle minimiza-tion problem in a network under edge (node) attack, where the attacker aims to minimize the number of triangles in the network by removing k edges (nodes). We show that the triangle minimization problem under edge (node) attack is a submodular function maxi-mization problem, which can be solved efficiently. Specifically, we propose a degree-based edge (node) removal algorithm and a near-optimal greedy edge (node) removal algorithm for approximately solving the triangle minimization problem under edge (node) attack. In addition, we introduce two pruning strategies and an approximate marginal gain evaluation technique to further speed up the greedy edge (node) removal algorithm. We conduct exten-sive experiments over 12 real-world datasets to evaluate the proposed algorithms, and the results demonstrate the effectiveness, efficiency and scalability of our algorithms. Keywords Triangle minimization  X  Large networks  X  FM sketch  X  Submodular function 1 Introduction A network comprises a set of nodes and a set of edges representing the connections between the nodes. In the past decade, network science has attracted growing attention in both industry and research communities due to a large number of applications. Many real-world problems can be modeled as a network. For example, in online social networks, a user can be modeled by a node and a social tie between two users can be represented by an edge. In computer networks, every computer can be modeled by a node and each link between two computers can be represented by an edge. Another example is the Internet, where each web page can be represented by a node and each hyperlink signifies an edge. As observed by Watts and Strogatz [ 50 ], many real-world networks exhibit small world phenomenon that the diameter of the network is extremely small regarding to the network size. Subsequently, Barab X si and Albert [ 5 ] reported that most real-world networks also exhibit power-law degree distribution, where the degree of the most nodes in a network is relatively small and only a few nodes have large degree. These two crucial discoveries inspire comprehensive research on network science. The studies on network science mainly address the structure and function of a network.
One important topic of network science is to study the error and attack tolerance of a to targeted attacks. In general, there are two types of attacks in a network: edge attack and node attack. For edge attack, the attacker aims to disconnect certain edges of the network, while for node attack, the goal of the attacker is to remove some nodes from the network. For example, a recent news in CNET 1 reports an edge attack event. The fast-food company Burger King created a Facebook application, namely Whopper Sacrifice, which will give a Facebook user a free coupon for a free hamburger if he/she deletes ten people from his/her friends list. By statistics, 82,771 Facebook users participated in this campaign, and 233,906 Facebook friendships were deleted. After then, Facebook stops this application due to a large number of link deletions. The node attacks frequently happen to computer networks. For instance, a hacker attacks a server in the network and makes it break down. Clearly, this attack is equivalent to removing a node from the network, thereby it is a node attack.
In practice, for edge (node) attack, the attacker, due to resource limitation, has a small budget of k edges (nodes) to attack. We assume that if an edge (a node) is attacked by the attacker, then the edge (node) will be removed from the network. Generally, the attacker wants to maximize some utility functions by attacking k edges (nodes). Previous studies have been addressed to such edge and node attacks. For example, in Albert et al. [ 1 ], study the diameter of the resulting network after k edges removal. In [ 1 ], the utility function of the attacker corresponds to the diameter of the network, and the attacker aims to maximize the diameter of the network, which will make the network as incohesive as possible. In Schneider et al. [ 42 ], study the minimal size of the maximal connected component (MCC) of the remaining network after k nodes deletions. Based on this, they define a metric for measuring the robustness of anetwork.In[ 42 ], the utility function of the attacker corresponds to the inverse of the size of the maximal connected component of the network, and the attacker aims to minimize the size of the MCC of the network, which will reduce the robustness of the network as much as possible. More recently, Tong et al. [ 45 ] investigate a problem of deleting k edges to minimize the leading eigenvalue of the adjacency matrix of the graph. Clearly, in their work, the utility function of the attacker is the inverse of the leading eigenvalue.

In contrast to the diameter, the size of the maximal connected component and the leading eigenvalue of a network, in this paper, we consider another important metric, i.e., the number of triangles of the network. It is well known that triangles play an crucial role in social network analysis [ 18 ]. The theories of homophily [ 36 ] and transitivity in social network analysis are based on the triangles of the social network. In particular, by the homophily theory, users in a social network tend to select friends who are similar to themselves [ 36 ]. In terms of the transitivity theory, users who have common friends tend to be friends themselves [ 18 ]. Due to the significance of the triangles in social network analysis, a lot of metrics, such as the number of triangles, clustering coefficient [ 50 ] and transitivity ratio [ 18 ], are proposed for triangle analysis in social networks [ 18 ]. Among them, the number of triangles is a very basic metric, and it is closely related to the other metrics. Specifically, the number of triangles is proportional to the clustering coefficient and transitivity ratio of a network. In general, the tighter, the community, the more likely it has a large number of triangles. A community with a large number of triangles typically exhibits many important functions (e.g., homophily phenomenon, transitivity and the formation of positive social norms [ 12 ]). However, in many real-world applications, the network could be attacked by malicious individuals, which results in edge or node removal. It is therefore crucial to study how the number of triangles of a network changes after removing a certain number of edges or nodes.

To address this problem, in this paper, we study two triangle minimization problems in a network subject to two different attack models, which are edge attack and node attack, respectively. To the best of our knowledge, we are the first group to study the triangle mini-mization problems in networks. Specifically, in our problems, the attacker aims to minimize the number of triangles of the resulting network by attacking k edges (edge attack model) or k nodes (node attack model). Unlike the diameter, the size of the MCC and the leading eigenvalue of a network, we show that the triangle minimization problems can be approx-imately solved by an efficient greedy algorithm via exploiting the submodularity property. Additionally, we emphasize that the solutions of triangle minimization problems can be used in two different scenarios. On the one hand, from the attacker X  X  perspective, the attackers can use the proposed method to attack a network so as to achieve maximal utilities. On the other hand, from the network administrator X  X  perspective, the network administrators can protect some  X  X mportant X  edges (or nodes) identified by our methods so as to defend attacks.
The main contributions of this paper are summarized as follows. First, we introduce the triangle minimization problem in a network under both edge and node attacks. For edge (node) attack, we formulate the problem as a submodular function maximization problem and show that the problem is NP-hard. Second, to approximately solve the problem, we propose a degree-based edge (node) removal algorithm and an efficient greedy edge (node) removal algorithm with near optimal performance guarantee (  X  .0.63 approximation factor). For both greedy edge removal algorithm and greedy node removal algorithm, we propose two pruning strategies to accelerate the algorithms. The time complexity of the degree-based edge (node) removal algorithm is linear with respect to the size of the network, and the time complexity of the greedy edge (node) removal algorithm is O ( km 1 . 5 ) ,where m is the number of edges of the network and O ( m 1 . 5 ) is the time complexity for triangle counting. To further reduce the time complexity of the greedy algorithms, we propose a near-optimal linear time greedy algorithm based on a well-known probabilistic counting structure Flajolet X  X artin (FM) sketch [ 16 ] where the FM sketch is utilized to approximately estimate the marginal gain. We show that the space complexity of all the proposed algorithms is linear with respect to the network size. Moreover, all of our algorithms are easy to implement. Finally, we conducted extensive experimental studies over 12 real-world networks. The results demonstrate the effectiveness, efficiency and scalability of the proposed algorithms. Furthermore, we show that the proposed greedy algorithms are also very effective to decrease the clustering coefficient of a network, suggesting that our greedy algorithms are also good heuristic algorithms to minimize the clustering coefficient of a network under edge (node) attack.

The rest of this paper is organized as follows. We give the problem statement in Sect. 2 .We propose the degree-based edge removal algorithm and the greedy edge removal algorithm, and the degree-based node removal algorithm and the greedy node removal algorithm in Sects. 3 and 4 , respectively. Extensive experiments are reported in Sect. 5 , and the related work is discussed in Sect. 6 . We conclude this work in Sect. 7 . 2 Problem statement We model a network by an undirected and unweighted graph G = ( V , E ) ,where V signifies a set of nodes and E denotes a set of undirected edges. Let n =| V | and m =| E | be the number of nodes and the number of edges in G , respectively. Given a graph G , the problem that we address in this paper is to minimize the number of triangles in G given a budget of k edges/nodes removal. As mentioned in the introduction, our problem is motivated by the following attack model under network setting.

We assume that there is an attacker who has a budget of k edges (nodes) to attack. If an edge (a node) is attacked by the attacker, then the edge (node) will be deleted from the network. The goal of the attacker is to minimize the number of triangles in the network after removing k edges/nodes. For convenience, we refer to the attack model based on k edges and k nodes removal as the edge attack model and the node attack model, respectively. Below, we define our problems based on these two attack models.
 First, we consider the triangle minimization problem based on edge attack model. Let A  X  R n  X  n be the adjacency matrix of the graph G , A ( i , j ) be the element in the i th row and we have A ( u ,v) = 1. Further, we let S be a subset of edges, i.e., S  X  E ,and | S | be the cardinality of the set S .Andlet B S  X  R n  X  n be a matrix such that B S ( u ,v) = 1if ( u ,v)  X  S and B S ( u ,v) = 0 otherwise. If S only contains one edge e , we simply denote the matrix B S as B e .

Based on these notations, we first describe a well-known lemma in algebraic graph theory [ 17 ], and then define our problem.
 Lemma 2.1 Given a undirected and unweighted graph G and its adjacency matrix A. The number of triangles in G is equal to the trace of A 3 divided by six, i.e., tr ( A 3 )/ 6 . Based on Lemma 2.1 , our problem can be formulated as a discrete optimization problem as follows.
 Problem 1 Given a graph G , its adjacency matrix A , and a budget size k . Then, the triangle minimization problem under the edge attack model can be formulated as where S is a subset of edges, i.e., S  X  E . For any subset S ,wedefineafunction F ( S ) as F ( is equivalent to The following theorem shows that the problem defined in Eq. ( 2 )isNP-hard.
 Theorem 2.1 For a general undirected graph G, the optimization problem defined in Eq. ( 2 ) is NP-hard.
 Proof See  X  X ppendix X .

Second, in a similar manner, we can formulate the triangle minimization problem based on the node attack model. Let X be a subset of nodes, i.e., X  X  V ,and C X  X  R n  X  n be a matrix such that C X ( u ,v) = 1if u  X  X and ( u ,v)  X  E or v  X  X and ( u ,v)  X  E ,and C
X ( u ,v) = 0 otherwise. If X only consists of one node v , then we denote C X as C v . Based on these notations, we describe our problem in Problem 2 .
 Problem 2 Given a graph G , its adjacency matrix A , and a budget size k . The triangle minimization problem based on the node attack model can be formulated as where J ( X ) = tr ( A 3 )  X  tr (( A  X  C X ) 3 ) .Notethat tr (( A  X  C X ) 3 ) denotes the number of triangles in the residual graph after removing the nodes in X . Likewise, Problem 2 is also NP-hard.
 Theorem 2.2 For a general undirected graph G, the optimization problem defined in Eq. ( 3 ) is NP-hard.
 Proof See  X  X ppendix X .

Given the hardness of our two problems, we resort to design heuristic algorithms for approximately solving them. In the next two sections, we will devise several heuristic algo-rithms for Problems 1 and 2 , respectively. 3 Algorithms for edge removals As discussed in the previous section, both of Problems 1 and 2 are NP-hard, thus we cannot develop an algorithm to solve them in polynomial time unless P = NP. In this section, we concentrate on devising practical algorithms for Problem 1 .Inparticular,wefirstpresenta degree-based edge removal algorithm, which is very efficient. But unfortunately, the degree-based algorithm is without performance guarantee. To tackle this issue, we propose a greedy edge removal algorithm, which is shown to be near-optimal (1  X  1 / e approximation). More-over, we also present two pruning techniques to further accelerate the greedy edge removal algorithm. The detailed descriptions of these algorithms are given in the following subsec-tions. 3.1 Degree-based edge removal algorithm First, we define a degree function for each edge e = ( u ,v) in E as d ( e ) = d ( u ,v) = min { d u , d v } ,where d u and d v denote the degree of node u and v , respectively. Then, our algorithm iteratively removes k edges with maximal d ( e ) . We describe our algorithm in Algorithm 1 . First, Algorithm 1 computes d ( e ) for all the edges in G (line 1). Second, the algorithm works in k rounds, and at each round, the algorithm finds an edge e with maximal d ( e ) denoted as e max (line 4). Then, the algorithm adds the edge e max into the answer set S and deletes it from E (line 5 X 6). Let u and v be the two end nodes of e max . Then, the algorithm decreases the degree of u and v by 1 (line 7 X 8) and recomputes d ( e ) for all the incident edges of u and v , because only the degree function d ( e ) of those edges may need to be updated.

The time complexity of Algorithm 1 is O ( km ) . First, computing d ( e ) for all the edges takes O ( m ) time. Second, at each round, finding the edge e max consumes O ( m ) time and the time complexity for updating d ( e ) for all the incident edges of the end nodes is dominated by Algorithm 1 Degree-based edge removal algorithm Algorithm 2 Greedy edge removal algorithm O ( m ) . Put it all together, the time complexity of Algorithm 1 is O ( km ) . The space complexity of Algorithm 1 is O ( m + n ) . The reason is because we only need to store the graph as well as two additional arrays d ( e ) and d u , which are dominated by O ( m + n ) . 3.2 Greedy edge removal algorithm Recall that our intention is to minimize the number of triangles after deleting k edges. Intuitively, a better way to achieve this goal is to delete k edges that are contained in as many triangles as possible. Based on this intuition, here we propose a greedy edge removal algorithm. Let  X  e =  X ( u ,v) be the number of triangles that contain edge e = ( u ,v) . Unlike the degree-based edge removal algorithm, the greedy edge removal algorithm iteratively deletes k edges with maximal  X  e . We outline our algorithm in Algorithm 2 . Specifically, Algorithm 2 first initializes the answer set S to be an empty set. Second, Algorithm 2 removes k edges in k rounds. At each round, the algorithm iteratively finds the edge e max with maximal  X  e in the graph G = ( V , E \ S ) (line 5 X 9). Then, the algorithm adds the edge e max into the answer set S and then deletes it from E (line 10 X 11).

We analyze the time complexity of Algorithm 2 as follows. First, at each round, the time complexity for computing  X  e for every edge in G = ( V , E \ S ) is O ( m 1 . 5 ) ,which can be achieved by the fast main-memory triangle counting algorithm [ 26 ]. Second, after computing  X  e for all the edges in G = ( V , E \ S ) , the time complexity for finding the edge e max is O ( m  X  X  S | ) . Therefore, the total time complexity of Algorithm 2 is O ( km 1 space complexity of Algorithm 2 is O ( m + n ) . The rationale is that we only need to store the graph and maintain  X  e for every edge e  X  E , which are dominated by O ( m + n ) . 3.3 Justification In this subsection, we present theoretical justifications for Algorithms 1 and 2 , respectively. First, we give a simple interpretation for Algorithm 1 based on the following theorem. Theorem 3.1  X  e  X  d ( e ) .
 Proof Let e = ( u ,v) be an edge in G .Since d ( e ) = min { d u , d v } , the number of common neighbors of node u and v is bounded by d ( e ) . Note that the number of common neighbors of node u and v is equal to the number of triangles that contain e . Therefore, we have  X  e  X  d ( e ) .
Based on Theorem 3.1 , the degree-based edge removal algorithm (Algorithm 1 ) can be interpreted as deleting the edge to the current graph that minimizes the upper bound on the number of triangles of the resulting graph.

Second, we present a theoretical justification for Algorithm 2 by exploiting the sub-modularity of the function F ( S ) . Below, we first introduce the definition of nondecreasing submodular set function. Let U be a finite set. A set function f defined on the subsets of U is a nondecreasing submodular function if the following condition holds. For any two subsets Y and Z such that Y  X  Z  X  U , and for any element j /  X  Z ,wehave  X  j ( Y )  X   X  j ( Z )  X  0, where  X  j ( Y ) signifies the marginal gain defined as  X  j ( Y ) = f ( Y  X  X  j } )  X  f ( Y ) . Then, the following theorem shows that F ( S ) defined in Sect. 2 is a nondecreasing submodular function.
 Theorem 3.2 F ( S ) = tr ( A 3 )  X  tr (( A  X  B S ) 3 ) is a nondecreasing submodular function with F (  X  ) = 0 .
 Proof First, F (  X  ) = 0 obviously holds, because B  X  is a n  X  n zero matrix. Second, we prove that F ( S ) is nondecreasing. Let S  X  T  X  E be two subsets of E ,and e /  X  T is an edge in E . Further, let  X  e ( S ) = F ( S { e } )  X  F ( S ) be the marginal gain. Note that the nondecreasing property of function F ( S ) can be guaranteed by  X  e ( T )  X  0. Below, we show that  X  e ( T )  X  0. By definition, we have The last two equalities hold due to an important property of trace, which is invariant under cyclic permutations, i.e., tr ( ABC ) = tr ( BCA ) = tr ( CAB ) for any n  X  n matrices A , B ,and C .Since A  X  B T , B e ,and A  X  B T  X  B e are nonnegative matrices, the trace of their product is also nonnegative. Therefore, the last inequality holds. Similarly, we have  X  ( S ) = 3 tr (( A  X  B S  X  B e ) B e ( A  X  B S )) + tr ( B 3 of F ( S ) , i.e.,  X  e ( S )  X   X  e ( T ) . Using the result in Eq. ( 4 ), we have where the last inequality holds as the matrices A  X  B T  X  B e , B e , B T  X  B S ,and A  X  B S are nonnegative. This completes the proof.

Based on Theorem 3.2 , Problem 1 is a nondecreasing submodular function maximization optimal greedy algorithm for solving such problem. The greedy algorithm iteratively selects an element in the ground set with maximal marginal gain. In the following, we show that Algorithm 2 is an instance of such greedy algorithm. Formally, we let  X  e ( S ) be the number of Theorem 3.3  X  e ( S ) =  X  e ( S )/ 6 .

To prove Theorem 3.3 , we first give some useful lemmas as follows. The proofs can be easily obtained, thus we omit for brevity.
 Lemma 3.1 tr ( B e ) = 0 , and B 3 e = B e .
 Lemma 3.2 For any n  X  n matrix H, we have tr ( B 2 e H ) = H ( u , u ) + H (v, v) ,where e = ( u ,v) .
 Lemma 3.3 Let e = ( u ,v) . Then, for any n  X  n symmetric matrix H, we have tr ( B e H ) = 2 H ( u ,v) .
 Armed with the above lemmas, we prove the Theorem 3.3 as follows.
 Proof of Theorem 3.3 According to Eq. ( 4 ), we have In the above equation, the second equality holds due to Lemma 3.1 , and the last equality holds due to Lemma 3.2 and the element of the u th row and u th column as well as the element of the v th row and v th column of matrix ( A  X  B S ) equal to 0. Let H = ( A  X  B S ) . Clearly, both H and H 2 are symmetric. Then, by Lemma 3.3 ,wehave  X  e ( S ) = 6 H 2 ( u ,v) . By definition, the matrix H = ( A  X  B S ) is an adjacency matrix of the graph G = ( V , E \ S ) . Then, H 2 ( u ,v) = n w = 1 H ( u ,w) H (w, v) . Note that n w = 1 H ( u ,w) H (w, v) denotes the Hence, we have  X  e ( S ) =  X  e ( S )/ 6. This completes the proof.
BasedonTheorem 3.3 , we can conclude that Algorithm 2 iteratively finds the edge with maximal marginal gain. By a well-known result in [ 38 ], Algorithm 2 achieves a 1  X  1 / e approximate factor as stated in the following theorem, where e = 2 . 718 denotes the Euler X  X  number.
 Theorem 3.4 Algorithm 2 is a 1  X  1 / e approximate algorithm for Problem 1 .
 Proof We can prove it using a similar argument presented in [ 38 ], thus we omit for brevity.
We remark that the submodular function maximization with cardinality constraint problem is known to be non-approximate in 1  X  1 / e + where is a very small constant [ 14 , 49 ]. Since our problem is an instance of submodular function maximization subject to cardinality constraint problem, thus there is no algorithm that can beat the proposed one. 3.4 Pruning techniques Recall that Algorithm 2 needs to evaluate  X  e for each edge e in graph G = ( V , E \ S ) at each round, which is expensive. To further speed up Algorithm 2 , here we introduce two pruning techniques without loss of approximation factor of the algorithm. The first pruning technique is based on the so called lazy evaluation strategy [ 28 , 37 ], and the second pruning technique is based on the upper bound developed in Theorem 3.1 . We refer to the greedy algorithm with these two pruning strategies as the accelerated greedy edge removal algorithm.
Lazy evaluation (CELF optimization, [ 28 , 37 ] ) The lazy evaluation is based on the sub-modularity property of the objective function F ( S ) , which is also called CELF optimization in [ 28 ]. Specifically, let  X  X  X  S 1  X  ,  X  X  X  ,  X  S k be the optimal edge set found by Algorithm 2 at iteration i = 0 , 1 ,..., k , respectively. The key idea of CELF optimization is that at round i , the algorithm does not need to evaluate  X  e ( S i ) for those edges that their marginal gain at This idea can be implemented by a priority queue. In particular, we set the priority of each edge by its marginal gain. At each round, we get the top element from the priority queue and recompute its marginal gain. Then, we update the priority of the top element by the newly computed marginal gain and then record the current maximal marginal gain as well as the corresponding edge ( e max ). If the current maximal marginal gain is greater than or equal to the current maximal priority of the queue, then we are done. Because we do not need to update the marginal gain of the remaining edges in the queue based on the property  X  ( S i )  X   X  e ( S i  X  1 )  X   X  e
Further lazy evaluation (degree-based pruning) According to Theorem 3.1 , for each edge is smaller than the current maximal marginal gain, we do not need to recompute  X  e because  X  e  X  d ( e )  X   X  max . This idea can be combined with CELF optimization described above. We present our algorithm that combines these two pruning strategies in Algorithm 3 .More specifically, Algorithm 3 first initializes a priority queue q and sets the priority of every edge by + X  (line 2 X 5). Then, the algorithm works in k rounds. At each round, the algorithm first gets the top element e from the priority queue (line 10) and computes d ( e ) for e in G = ( V , E \ S ) (line 11). The upper bound pruning strategy is implemented in line 12 X 14. If d ( e ) is smaller than the current maximal marginal gain (  X  max ), the algorithm updates the priority of edge e by d ( e ) and adds it back to the priority queue (line12 X 14). This pruning strategy can significantly reduce the computational cost in the first round, because we do not need to compute  X  e for every edge e .If d ( e )&gt; X  max , then the algorithm recomputes  X  e for Algorithm 3 Accelerated greedy edge removal algorithm the top element e and updates its priority (line 16 X 18). And then the algorithm records the current maximal marginal gain (  X  max ) as well as its corresponding edge ( e max , line 19 X 21). Based on the CELF optimization, if the current maximal marginal gain is greater than or equal to the maximal priority of the element in the queue, then we are done. Finally, the algorithm adds the edge e max into the answer set S and removes e max from q and E (line 22 X 24). It is result, because the algorithm will recompute the true priority when it is needed (line 16 X 17). The worse case time complexity of Algorithm 3 is at most O ( km 1 . 5 ) by assuming that the algorithm needs to evaluate  X  e for all the edges in G = ( V , E \ S ) . However, in our experi-ments, Algorithm 3 significantly reduces the running time of Algorithm 2 by several orders of magnitude. Also, we can easily derive that the space complexity of Algorithm 2 is O ( m + n ) . 3.5 Approximate marginal gain evaluation Recall that in Algorithm 3 , the most time-consuming step is to compute the marginal gain  X  e (line 16). Here, we propose a more efficient greedy algorithm with approximate marginal gain evaluation using the well-known FM sketch [ 16 ]. We refer to this new greedy algorithm as the approximate greedy edge removal algorithm.

The FM sketch is a probabilistic counting data structure, which can be used to estimate the cardinality of a multi-set efficiently [ 16 ]. Denote by N the cardinality of a multi-set A . TheFMsketchisabletoestimate N accurately using only log N + r bits, where r is a small constant. More specifically, the FM sketch is a bitmap with size l = log N + r .Thereisahash function h : A  X  X  1 ,..., l } , which maps an element a ( a  X  A ) to a bit i ( i  X  X  1 ,..., l } )in initialized to 0. To process an element a ( a  X  A ), we set the corresponding h ( a ) th bit of the bitmap to 1. Then, it has turned out that 2 z / 0 . 77351 is an asymptotically unbiased estimator of the cardinality N where z is the position of the least-significant zero bit in the bitmap. An important property of the FM sketch is that it can also be applied to estimate the cardinality of the union of two multi-sets if these multi-sets come from the same domain. In particular, we create two FM sketches with the same size for two multi-sets, respectively. To estimate the cardinality of the union of two multi-sets, we can perform a bitwise-OR between the two FM sketches and then estimate the cardinality based on the resulting FM sketch. To improve the estimation accuracy, we can utilize multiple hash functions. We remark that there are many other probabilistic counting structures, such as Loglog sketch [ 13 ] and Hyper Loglog sketch [ 15 ]. Here, we choose FM sketch because it is easy to implement and its performance is very good as indicated in the experiments.

The basic idea of our algorithm is described as follows. Initially, for each node v , we create an FM sketch FM (v) to sketch the set of all the neighbor nodes of v denoted by N (v) . Then, in each round, we estimate  X  e = d u + d v  X  X  N ( u )  X  N (v) | for each edge e = ( u ,v) using theFMsketches FM ( u ) and FM (v) . In particular, we first perform a bitwise-OR operation between FM ( u ) and FM (v) and then we estimate | N ( u )  X  N (v) | using the resulting FM sketch. Denote by e max = ( u max ,v max ) the edge with maximal estimated marginal gain. Then, we add e max into the answer set S . After that, we rebuild the FM sketches for the nodes u max and v max . Note that a bitwise-OR operation can be done in constant time [ 39 ]. Thus, each marginal gain evaluation can be done in constant time using FM sketches. In addition, we can also use the pruning techniques presented in the previous subsection. The detailed description of the approximate greedy edge removal algorithm is very similar to Algorithm 3 , thus we omit it for brevity. Theoretically, by a similar analysis presented in [ 22 ], the approximate greedy edge removal algorithm can achieve a 1  X  1 / e  X  approximation factor with high probability by setting an appropriate number of hash functions, where is a small constant. We analyze the time and space complexity of the approximate greedy algorithm as follows. First, the algorithm has to traverse the graph one time to create the FM sketches for all the nodes. Clearly, this procedure takes O ( m + n ) time complexity. Second, in each round, the algorithm needs to rebuild the FM sketches for the end nodes of the selected edge. Clearly, the time overhead of this step is dominated by O ( m ) . Finally, in the worse case, the algorithm needs to evaluate the marginal gains for every edge in each round. Since each marginal gain computation takes constant time using FM sketches, the time complexity in each round is O ( m ) .Thereare k rounds in total. Thus, the time complexity of the approximate greedy edge removal algorithm is O ( km ) , which is linear with respect to the graph size. For the space complexity, the algorithm has to store the graph using O ( m + n ) space. Each FM sketch takes O ( log n ) bits, which is equivalent to O ( 1 ) words. Therefore, the total space overhead of the FM sketches is O ( n ) . Put it all together, the space complexity of the approximate greedy edge removal algorithm is O ( m + n ) . 4 Algorithms for node removals algorithms for Problem 1 , first, we propose a degree-based node removal algorithm, which is without performance guarantee. Then, we present a greedy node removal algorithm by exploiting the submodularity property of the objective function J ( X ) ,whichisshowntobe near optimal. Also, we propose two pruning techniques to speed up the greedy algorithm. Algorithm 4 Degree-based node removal algorithm 4.1 Degree-based node removal algorithm Similar to the degree-based edge removal algorithm, here we introduce a degree-based node removal algorithm. The algorithm iteratively deletes k nodes with maximal degree. We describe our algorithm in Algorithm 4 . First, Algorithm 4 calculates the degree for every node in G (line 1). Second, the algorithm works in k rounds. At each round, the algorithm finds the node v max with maximal degree (line 4) and adds it into the answer set X . Then, the algorithm updates the degree of the neighbors of v max and deletes v max as well as its incident edges from G (line 6 X 8). We can easily derive that the time and space complexity of Algorithm 4 is O ( kn + m ) and O ( m + n ) , respectively. 4.2 Greedy node removal algorithm Similar to the greedy edge removal algorithm for Problem 1 , we propose a greedy node removal algorithm for Problem 2 .Let  X  v be the number of triangles that contain node v .The greedy node removal algorithm iteratively removes k nodes with maximal  X  v .Wedepict our algorithm in Algorithm 5 .Let G X be the induced subgraph by the nodes in V \ X .The algorithm works in k rounds. At each round, the algorithm computes  X  v for every node in G
X (line 6) and then finds the node v max with maximal  X  v (line 7 X 9). Then, the algorithm adds v max into the answer set (line 10), and deletes v max as well as its incident edges from G . The most time-consuming step in Algorithm 5 is line 6, which requires O ( m 1 . 5 ) time. Since the algorithm works in k rounds, the time complexity of Algorithm 5 is O ( km 1 . 5 ) . Also, we can derive that the space complexity of Algorithm 5 is O ( m + n ) . 4.3 Justification We justify our algorithms presented above. First, we give an explanation for Algorithm 4 . Then, we prove that Algorithm 5 achieves a 1  X  1 / e approximate factor. Our interpretation for Algorithm 4 is based on the following theorem.
 Theorem 4.1  X  v  X  d v ( d v  X  1 )/ 2 .
 Proof For every node v , the triangle contains a node v if and only if the triangle contains two different incident edges of node v . There are at most d v ( d v  X  1 )/ 2 different combinations of two incident edges of v . Therefore, the number of triangles that contain node v , i.e.,  X  v ,is bounded by d v ( d v  X  1 )/ 2. Algorithm 5 Greedy node removal algorithm
According to Theorem 4.1 , Algorithm 4 can be interpreted as removing node to the current graph that minimize the upper bound on the number of triangles of the resulting graph.
Second, we prove that Algorithm 5 is a 1  X  1 / e approximate algorithm by exploiting the submodularity of J ( X ) .Below,wefirstprovethat J ( X ) is a nondecreasing submodular function.
 Theorem 4.2 For any subset of nodes X  X  V,J ( X ) is a nondecreasing submodular function with J (  X  ) = 0 .
 Proof Since C  X  is a zero n  X  n matrix, we have J (  X  ) = 0. Let X  X  Y  X  V be two subsets of nodes and v/  X  Y be a node in V .Further,wedefine  X  v ( X ) = J ( X { v } )  X  J ( X ) as the marginal gain. Then, to prove the nondecreasing property of J ( X ) , we need to show  X  ( Y )  X  0. By definition, we have where the last inequality holds due to C v , A  X  C Y ,and A  X  C Y  X  C v are nonnegative matrices. 0. To prove the submodularity property of J ( X ) ,wehavetoshow  X  v ( X )  X   X  v ( Y ) .By definition, we have where the last inequality holds because C X  X  C Y , A  X  C X ,and A  X  C Y are nonnegative matrices. This completes the proof.

BasedonTheorem 4.2 , Problem 2 is a submodular function maximization subject to cardinality constraint problem. By a well-known result in [ 38 ], there is a near-optimal greedy algorithm for solving Problem 2 . Below, we prove that our Algorithm 5 is indeed such a greedy algorithm. Let  X  v ( X ) be the number of triangles in G X that contain v . Our goal is to prove that the marginal gain  X  v ( X ) is proportional to  X  v ( X ) . The following theorem shows that  X  v ( X ) = 6  X  v ( X ) .
 Theorem 4.3  X  v ( X ) = 6  X  v ( X ) .
 Proof Without loss of generality, we assume that | X |= x , X ={ v n  X  x + 1 ,...,v n } ,and v = v n  X  x . Then, the matrices A  X  C X and A  X  C X  X  X  v } can be represented as two block matrices as follows. and subgraph by the nodes in V \{ X  X  X  v }} ), U is a ( n  X  x  X  1 )  X  ( x + 1 ) matrix and Then, we have and Further, we have and By Eq. ( 6 ), we have By substituting Z and U [Eq. ( 7 )] into Eq. ( 8 ), we can get This completes the proof.

BasedonTheorem 4.3 , Algorithm 5 is a greedy algorithm that iteratively removes k nodes with maximal marginal gain. By a well-known result in [ 38 ], Algorithm 5 achieves 1  X  1 / e approximation factor as stated in the following theorem.
 Theorem 4.4 Algorithm 5 is a 1  X  1 / e approximation algorithm for Problem 2 . 4.4 Pruning techniques In Algorithm 5 , at each round, the most time-consuming step is line 6, which requires to compute  X  v for each node v in G X . Like Algorithm 3 , here we present two similar pruning techniques to accelerate Algorithm 5 without loss of approximation factor. The first pruning technique is based on the submodularity property of the objective function J ( X ) (CELF optimization) [ 28 , 37 ], and the second pruning technique is based on Theorem 4.1 (degree-based pruning). We refer to Algorithm 5 with these two pruning techniques as the accelerated greedy node removal algorithm. We outline our algorithm in Algorithm 6 .

Like Algorithm 3 , Algorithm 6 also makes use of a priority queue to find the node with maximal marginal gain. First, the algorithm initializes the answer set X to an empty set (line 1), and adds all the nodes with priority + X  into the priority queue (line 2 X 5). Then, the algorithm works in k rounds to find k nodes. At each round, the algorithm first initializes the current maximal marginal gain  X  max to  X  X  X  and the corresponding node v max to null (line 7 X 8). Then, the algorithm accesses the top element v of the priority queue (line 10) and computes d v for node v w.r.t. graph G X .If d v  X  ( d v  X  1 )/ 2  X   X  max ,wehave  X  v  X   X  max by Theorem 4.1 . Hence, the algorithm does not compute  X  v . Instead, the algorithm updates the priority of node v by d v  X  ( d v  X  1 )/ 2 (line 12 X 14). If d v  X  ( d v  X  1 )/ 2 &gt; X  max ,the algorithm computes  X  v for v in graph G X and then updates the priority of node v by the newly computed  X  v (line 16 X 18). If  X  v  X   X  max , the algorithm updates  X  max and v max by  X  v and v , respectively (line 19 X 21). Finally, the algorithm adds v max into the answer set X and deletes v max from the priority queue and the graph G . Note that the worse case time complexity of Algorithm 6 is at most O ( km 1 . 5 ) by assuming that  X  v for every node needs to evaluate at each round. In practice, we show that Algorithm 6 is faster than Algorithm 5 by several orders of magnitude. It is easy to derive that the space complexity of Algorithm 6 is still O ( m + n ) . Algorithm 6 Accelerated greedy node removal algorithm 4.5 Approximate marginal gain computation Similar to the approximate greedy edge removal algorithm, here we present an approxi-mate greedy node removal algorithm where the marginal gains  X  v for a node v is esti-mated using FM sketch. Specifically, for each node v ,wecreateanFMsketch FM (v) to sketch the neighbor-set N (v) . Then, in each round, we estimate  X  v using the formula  X  v = e = ( u ,v)  X  E  X  e / 2, where  X  e can be estimated by the FM sketches FM ( u ) and FM (v) . After selecting the node v max with maximal estimated marginal gain, we rebuild the FM sketches for all the nodes in N (v max ) . Also, we can use the pruning techniques devel-oped in the previous subsection to further speed up the algorithm. The detailed description of this algorithm is similar to Algorithm 6 , thus we omit it for brevity.

Below, we show that the time and space complexity of the approximate greedy node removal algorithm is linear with respect to the graph size. First, it is easy to show that the total time cost for FM sketches construction and reconstruction is O ( m + n ) . Second, in the worse case, the algorithm has to evaluate the marginal gains for all nodes in each round. Clearly, the time complexity to estimate the marginal gain for a node v is O ( d v ) using FM sketch. Therefore, in each round, the worse case time complexity of marginal gain evaluation algorithm is O ( km ) . For the space complexity, the algorithm needs to maintain the graph G and n FM sketches, which takes O ( m + n ) space. 5 Experiments Different algorithms We compare the proposed algorithms with four baseline algorithms. (1) Random : this algorithm removes k edges (nodes) randomly. (2) PageRank : it removes k edges (nodes) based on the PageRank scores [ 7 ]. Specifically, let p u be the PageRank score of node u and p e = min ( p u , p v ) be the PageRank score of an edge e . Then, for Problem 1 , PageRank removes k edges with the highest p e for e  X  E . For Problem 2 , PageRank deletes k nodes with the highest p u for u  X  V .(3) NetMelt [ 45 ]: It removes k edges based on the leading eigenvector of the adjacency matrix of the graph. (4) NetShield [ 46 ]: similar to NetMelt , it deletes k nodes based on the leading eigenvector. Detailed descriptions of NetMelt and NetShield can be found in [ 45 ]and[ 46 ], respectively.

We implement five proposed algorithms which are: (1) Degree , i.e., the degree-based edge (node) removal algorithm (Algorithms 1 and 4 ), (2) Greedy , i.e., the greedy edge (node) removal algorithm (Algorithms 2 and 5 ), (3) GreedyCELF , i.e., the greedy algorithms with CELF optimization, (4) GreedyAll , the greedy algorithms with both CELF optimization and degree-based pruning (Algorithms 3 and 6 ), and (5) Approx , i.e., the greedy algorithms with approximate marginal gain evaluation. In all the experiments, we set the number of hashing functions in Approx denoted by R to 100 (i.e., R = 100) because R = 100 is sufficient to guarantee a very good performance.
 Evaluation methodology We compare the number of triangles in the resulting graph after k edges (nodes) removal to evaluate the effectiveness of various algorithms. In the resulting graph, the less the number of triangles is the more effective the algorithm is. We use the running time, which is measured by wall-clock time, to evaluate the efficiency of different algorithms.
 Datasets We use 12 real-world datasets in the experiments. The datasets include two co-authorship networks (Astroph [ 27 ] and DBLP), five online social networks (Epinions [ 27 ], Douban, Delicious, Digg, and Flickr [ 51 ]), one location-based social network (Gowalla [ 27 ]), and two communication networks (EmailEuAll and WikiTalk [ 27 ]), one road network (road-a directed graph, we ignore the direction of the edges in the graph. The detailed statistic information of the datasets is described in Table 1 .
 Experimental environment: We implement all the algorithms in C++. All of our experiments are conducted on a Windows Server 2007 with 4xDual-Core Intel Xeon 2.66 GHz CPU and 8G memory.
 5.1 Experimental results Effectiveness of different algorithms Figures 1 and 2 depict the result of different edge removal algorithms and node removal algorithms over 12 datasets, respectively. For the greedy edge (node) removal algorithm, we only report the results of GreedyAll and Approx in Fig. 1 (Fig. 2 ), because both GreedyCELF and Greedy do not reduce the effectiveness of GreedyAll .FromFig. 1 , we can find that GreedyAll consistently outperforms the other algorithms over all 12 datasets given k ranging from 0 to 500. The performance of Approx is very close to that of GreedyAll , which confirms the theoretical analysis in Sects. 3.5 and 4.5 . Moreover, in most datasets, the gap between the curves of GreedyAll and Approx (red and black solid curves) and the curves of the other algorithms increases as k increases. This is because GreedyAll and Approx are the near-optimal approximation algorithms, which achieve 1  X  1 / e and 1  X  1 / e  X  approximation factors, respectively. Furthermore, such approximation factors are independent of k . Therefore, when k increases, these algorithms can still achieve a very good performance. Instead, Degree (Algorithm 1 ) and the baseline algorithms are without any performance guarantee, thereby the performance could decrease as k increases. In addition, we can find that Degree outperforms the baseline algorithms in most datasets. This could be because Degree selects the edge with highest degree ( d ( e ) defined in Sect. 3.1 ), which is an upper bound of the marginal gain. This result suggests that Degree is a very good heuristic algorithm for the triangle minimization problem. It also indicates that the degree can be a very effective pruning strategy in GreedyAll .We will illustrate this point in the following experiment. Also, we can find that Random per-forms poorly over all the datasets, as it does not take any graph-structural information into account.

Similarly, for the node removal algorithms, we can observe that GreedyAll (Algorithm 6 ) and Approx significantly outperform the other competitors over all the datasets from Fig. 2 . Likewise, the results of Approx are very similar to the results of GreedyAll which further confirm that the approximate greedy algorithm is very effective. In addition, we can find that Degree , PageRank and NetShield achieve relatively good performance in certain datasets (e.g., in Astroph, Gowalla and EmailEuAll datasets), but in the remaining datasets they perform poorly. Moreover, in all the datasets, the gap between the curves of GreedyAll and Approx and the curves of the other algorithms increases with increasing k . This is because Degree and the baselines algorithms are without any performance guarantee. These results are consistent with the theoretical analysis presented in Sects. 3.3 and 4.3 .
 Efficiency testing and the power of pruning In this experiment, we show the efficiency of different algorithms and the effective of the pruning strategies in GreedyAll .Below,weonly report the results of the edge removal algorithms, because similar results can be observed for the node removal algorithms. Table 2 depicts the running time of different edge removal algorithms given that k = 100. Similar results can be obtained for other k . As can be seen, Random is the most efficient algorithm, because it removes edges randomly. GreedyAll and Approx are also very efficient, which take around 99 and 63 s in the WikiTalk dataset (more than 2 million nodes and 8 million edges), respectively. In general, Approx is more efficient than GreedyAll . As desired, the running time of Approx is comparable to those of PageRank and NetMelt because all of them have linear time complexity. Among Greedy , GreedyCELF , and GreedyAll ,the GreedyAll algorithm is clearly the winner as it integrates two pruning strategies. Now, we analyze the power of these two pruning strategies described in Sect. 3.4 . Specifically, let us focus on the columns 6 X 8 in Table 2 . GreedyAll reduces the running time of Greedy by 524.8 times on average over all the datasets, and GreedyCELF shortens the running time of Greedy by 68.4 times on average. These results imply that the CELF optimization is a very effective pruning rule, which consists with the previous observation in [ 28 ], and the degree-based pruning strategy is also very powerful in real-word graphs. Scalability testing To test the scalability of our algorithms, we generate five large synthetic graphs based on a power-law random graph model [ 5 ]. Specifically, we generate five synthetic graphs G 1 ,..., G 5 with G i has i million nodes and 5  X  i million edges for i = 1 ,  X  X  X  , 5. Figure 3 a, b show the results of the edge removal algorithms and node removal algorithms, respectively, given that k = 100. Similar results can be observed for other k .Forthegreedy edge (node) removal algorithm, we only report the results of GreedyAll and Approx as they are faster than GreedyCELF and Greedy .FromFig. 3 a, we can see that Degree (Algo-rithm 1 )and Approx scale linear with respect to the number of edges because the complexity of Degree and Approx are linear. The curve of GreedyAll (Algorithm 3 ) is a concave-up and increasing curve, which consists with the complexity of the algorithm, i.e., O ( m 1 . 5 ) . Similarly, for the node removal algorithms, from Fig. 3 b, we can observe that Degree (Algo-rithm 4 )and Approx scale linearly with respect to the number of edges, because their time complexity are linear. Surprisingly, we find that GreedyAll (Algorithm 6 ) also scales lin-gies (both CELF optimization and degree-based pruning) are very effective for the greedy node removal algorithm. Additionally, we remark that the linear scalability (w.r.t. the graph size) of Approx is independent of the number of hashing functions used (i.e., the parame-ter R ). In fact, we have observed (not shown) that for a large R , the scalability of Approx is still linear with respect to the graph size, which is consistent with the time complexity of Approx .
 Clustering coefficient versus k edges (nodes) removal Here, we examine the variation of clustering coefficient of a network when k edges (nodes) of the network are removed by different algorithms. For brevity, we only report the results observed in Astroph dataset, and very similar results can be observed from the other datasets. Also, for the greedy edges (nodes) removal algorithms, we only report the results of GreedyAll and Approx . Similar results can be observed from other datasets. Figure 4 depicts our results. For GreedyAll (both edge removal and node removal) and Approx , we can see that the clustering coefficient decreases near linearly as k increases. This result indicates that the greedy algorithms are very effective for decreasing the clustering coefficient of a network. In general, for Degree (both edge removal and node removal) and the other baseline algorithms, we can find that the clustering coefficient slightly decreases as k increases and the curves of these algorithms tend to be flat with increasing k . These results suggest that the clustering coefficient of a network is very robust to such algorithms. Discussions Here, we discuss the advantages and disadvantages of the proposed algorithms PageRank and NetShield algorithms, the proposed Degree algorithm is much faster than PageRank and NetShield as shown in Table 2 . Moreover, in most datasets, the effectiveness of Degree is significantly better than that of PageRank , and it is comparable with NetShield . Second, by effectiveness, the proposed GreedyAll and Approx significantly outperform the other competitors. This is because both of them are near optimal in theory, whereas the other algorithms are without any performance guarantee. As a consequence, the performance gap between GreedyAll (or Approx ) and the other competitors increases as k increases (Figs. 1 , 2 ). The running time of Approx is lower than that of GreedyAll , but it is much higher than the proposed Degree algorithm. However, the scalability testing results (Fig. 3 ) show that Approx scales near linearly, indicating that Approx can also work on very large graphs. To summarize, in some real-world applications, when the running time is a crucial factor, we recommend the proposed Degree algorithm, as it not only runs faster than the state-of-the-art algorithms, but it also performs as good as the state-of-the-art algorithms. For the applications where the effectiveness of the algorithm is more crucial than the running time, we recommend the proposed Approx algorithm, because it is near optimal for any k , and it can also scale to handle very large graphs. 6 Related work Triangle counting Triangle counting has been well studied in the literature. The crude triangle counting algorithm is to enumerates all possible triples of nodes, which results in a O ( n 3 ) time complexity. The first fast triangle counting algorithm is based on spanning-tree, which counting algorithm based on fast matrix multiplication. However, their algorithm requires O ( n 2 ) space, which is impractical in large graphs. To overcome this issue, Schank and Wagner [ 41 ] presented several more practical triangle counting algorithms with O ( m + n ) space complexity based on node or edge iteration. Subsequently, Latapy proved that the time complexity of the algorithm presented in [ 41 ]is O ( m 1 . 5 ) , and also he suggested several improved algorithms for triangle counting. All the aforementioned algorithms are in-memory algorithms. For a good survey of these algorithms, we point out to [ 40 ]. For the graph that is too big to fit into memory, Chu and Cheng [ 10 ] proposed an I/O-efficient triangle counting algorithm. Suri and Vassilvitskii [ 44 ] proposed a parallel counting algorithm using Map-Reduce framework. Besides exact triangle counting, another line of research is to count the number of triangles approximately. For example, Bar-Yossef et al. [ 4 ] proposed a streaming algorithm for estimating the number of triangles in large graphs using sketch techniques. Further improved streaming algorithms are presented in [ 8 , 21 ]. Becchetti et al. [ 6 ] proposed a semi-streaming algorithm to estimate the number of local triangles for a given node in large graphs. More recently, Tsourakakis et al. [ 47 ] presented a sampling-based method for triangle counting in large graphs. Their method first reduces the edges in the graph by sampling and then assigns a weight to each residual edge. Then, they estimate the number of triangles based on the resulting weighted graph. Avron [ 3 ] proposed an approximate triangle counting algorithm based on randomized trace estimators. Seshadhri et al. [ 43 ] presented an efficient approximate triangle counting algorithm based on wedge sampling. Their algorithm can also be used to estimate the number of directed triangles in a directed graphs.
 Submodular function maximization Our work is also related to the submodular function maximization problem [ 38 ]. Generally, the problem of submodular function maximization subject to cardinality constraint is NP-hard. Nemhauser et al. [ 38 ] proposed a greedy algo-rithm with 1  X  1 / e approximate factor for solving this problem. Recently, many applications have been formulated as the submodular function maximization subject to cardinality con-straint problem [ 14 , 22 , 23 , 28 , 30 , 34 ]. Examples include the maximal k coverage problem [ 14 ], the influence maximization problem in social networks [ 22 ], the outbreak detection problem in networks [ 28 ], the observation selection and sensor placement problem [ 23 , 25 ], the document summarization problem [ 34 , 35 ], the privacy preserving data publishing prob-lem [ 24 ], the diversified ranking problem [ 29 , 30 ], and the random walk domination problem [ 31 ]. All of these problems can be approximately solved by the greedy algorithm given in [ 38 ]. Here, we study the triangle minimization problem, and we show that it can be formu-lated as a submodular function maximization problem. Also, we present a 1  X  1 / e greedy algorithm to solve it.
 Network attack problems In the literature, there are several network attack problems related to our problems. We summarize these problems in Table 3 . More specifically, Albert et al. [ 1 ] studied a network attack problem where the utility function is the diameter of a network. Subsequently, Schneider et al. [ 42 ] investigated a network attack problem where the utility function is the size of the MCC of a network. Both of these studies only focus on the robustness of a network under edge or node attacks. Both of them did not study the complexity of their problem and also no efficient algorithm was developed in these work. More recently, Li et al. [ 32 ] studied another network attack issue where the utility function is the size of the residual network. They shown that the problem is NP-hard, and they also proposed several efficient algorithm for solve their problems. However, the size of the residual network cannot capture the cohesiveness of a network. To achieve that end, in this work, we propose to use the number of triangles as an utility function, and we also devise several algorithms to solve our problems efficiently. 7 Conclusions This paper presents a study of triangle minimization problem in large networks subject to a small fraction of edges (nodes) removal. In particular, under either edge removal or node removal, we formulate the triangle minimization problem as a submodular maximization problem. For the triangle minimization problem under edge (node) removal, we propose a degree-based edge (node) removal algorithm and a near-optimal greedy edge (node) removal algorithm. In addition, we further introduce two pruning strategies and a technique of approx-imate marginal gain computation to accelerate the greedy algorithms. Finally, we evaluate the proposed algorithms using 12 real-world graphs. The results show that our algorithms is very effective with respect to the baseline methods, and all of our algorithms scale very well in large graphs. Our idea of developing upper bound (Theorems 3.1 , 4.1 ) for pruning in the greedy algorithm can also be used for other submodular maximization problem, such as the influence maximization problem [ 22 ]. It would be interesting to develop upper bound techniques to accelerate the greedy algorithm for such problems. In addition, the objective functions of Problems 1 and 2 are submodular. An interesting problem is to plus these two objective functions (or by a positive weight, it is still submodular) and then optimize both the edge attack and node attack simultaneously.
 Appendix Proof of Theorem 2.1 First, it is known that the set cover with frequency constraint (SCFC) problem is NP-hard [ 19 , 48 ]. Given a ground set U , a collection of n subsets S = {
S 1 , S 2 ,  X  X  X  , S n } where i S i = U , and a frequency parameter t ( t &lt; n ), the SCFC prob-lem is to find the minimum number of subsets in S that covers all elements in U . Here, the frequency parameter t denotes that every element in U is included in t subsets in S .Letus consider a special case of the SCFC problem, which has an additional constraint that the inter-section of any three subsets in S has at most one element (i.e., for any i , j , k and i = j = k , |
S i  X  S j  X  S k | X  1). For convenience, we refer to this problem as the intersection-bounded SCFC (IBSCFC) problem. Below, we show that the IBSCFC problem is also NP-hard. Sup-pose to the contrary that there is a polynomial algorithm A to solve the IBSCFC problem. For any | S i  X  S j  X  S k | &gt; 1 in the SCFC problem, we can discard the  X  X edundant-common S after discarding the redundant-common elements (i.e., for S i , S j , S k , only one common element is left). Then, the SCFC problem becomes the IBSCFC problem, and we invoke algorithm A to solve it. It is important to note that the optimal solution (the selected subsets ID) obtained by algorithm A is the optimal solution for the SCFC problem. The reason is as in these three subsets (by our constraint, each element is included in three subsets), thus they do not affect the optimal solution. Moreover, the optimal solution obtained by algorithm A must contain at least one subset from S i , S j , S k , because these three subsets have one com-mon element left which must be covered by a subset in the optimal solution. By the above process, there is a polynomial algorithm for the SCFC problem, which is a contradiction. Second, we consider the maximum coverage version of the IBSCFC problem, called IBM-CFC, where the goal is to find k subsets in S to maximize the cardinality of their union. It is easy to show that this problem is also NP-hard. Because if not, there is a polynomial algo-rithm B to solve the IBMCFC problem. Since i S i = U , we can invoke B at most n times to get an optimal solution of the IBSCFC problem (enumerating k from 1 to n ). That is to say, there is a polynomial algorithm for the IBSCFC problem, which is a contradiction.
Third, to prove the theorem, we show a reduction from the IBMCFC problem. Specifically, for each subset S i ,wecreateanedge e i with 2 | S i | stubs, which are used to combine the end nodes of different edges. Each end node of an edge is associated with | S i | stubs, and these stubs are labeled by the element ID in S i . Then, for any three subsets S i , S j ,and S k ( i = j = k ) with | S i  X  S j  X  S k |= 1, we combine the end nodes of their corresponding edges with the S and any three subsets have at most one common element. Then, for each subset, we create an edge with stubs as shown in the left part of Fig. 5 . Then, we can construct a graph as shown in the right part of Fig. 5 . By this construction, each triangle is represented by an element in
U , and each edge e i in the resulting graph is represented by a subset S i in S . As a result, the optimal solution of the triangle minimization problem (by edge removal) in the resulting graph is the optimal solution of the IBMCFC problem. Since IBMCFC is NP-hard, the triangle minimization problem by edge removal is also NP-hard. This completes the proof.
 Proof of Theorem 2.2 Similar to the proof of Theorem 2.1, we can show a reduction from the IBMCFC problem. Following the notations used in the proof of Theorem 2.1, we create agraph G for the instance of triangle minimization problem by node removal as follows. an edge (v i ,v j ) if and only if S i S j = X  . By this construction, each node is represented by a subset, and each triangle is represented by an element. One can easily check that the optimal solution of the triangle minimization problem by node removal is the optimal solution of the IBMCFC problem. Thus, the theorem is established.
 References
