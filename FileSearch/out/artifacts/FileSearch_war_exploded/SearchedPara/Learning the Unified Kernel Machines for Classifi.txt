 Kernel machines have been shown as the state-of-the-art learning techniques for classification. In this paper, we pro-pose a novel general framework of learning the Unified Ker-nel Machines (UKM) from both labeled and unlabeled data. Our proposed framework integrat es supervised learning, semi-supervised kernel learning, and active learning in a unified solution. In the suggested framework, we particularly fo-cus our attention on designing a new semi-supervised ker-nel learning method, i.e., Spectral Kernel Learning (SKL), which is built on the principles of kernel target alignment and unsupervised kernel design. Our algorithm is related to an equivalent quadratic programming problem that can be efficiently solved. Empirical results have shown that our method is more effective and robust to learn the semi-supervised kernels than traditional approaches. Based on the framework, we present a specific paradigm of unified kernel machines with respect to Kernel Logistic Regresions (KLR), i.e., Unified Kernel Logistic Regression (UKLR). We evaluate our proposed UKLR classification scheme in com-parison with traditional solutions. The promising results show that our proposed UKLR paradigm is more effective than the traditional classification approaches.
 I.5.2 [ PATTERN RECOGNITION ]: Design Methodol-ogy X  Classifier design and evaluation ; H.2.8 [ Database Man-agement ]: Database Applications X  Data mining Design, Algorithms, Experimentation Classification, Kernel Machines, Spectral Kernel Learning, Supervised Learning, Semi-Supervised Learning, Unsuper-vised Kernel Design, Kernel Logistic Regressions, Active Learning Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00.
Classification is a core data mining technique and has been actively studied in the past decades. In general, the goal of classification is to assign unlabeled testing examples with a set of predefined categories. Traditional classification meth-ods are usually conducted in a supervised learning way, in which only labeled data are used to train a predefined clas-sification model. In literature, a variety of statistical models have been proposed for classification in the machine learn-ing and data mining communities. One of the most popu-lar and successful methodologies is the kernel-machine tech-niques, such as Support Vector Machines (SVM) [25] and Kernel Logistic Regressions (KLR) [29]. Like other early work for classification, traditional kernel-machine methods are usually performed in the supervised learning way, which consider only the labeled data in the training phase.
It is obvious that a good classification model should take advantages on not only the labeled data, but also the un-labeled data when they are available. Learning on both la-beled and unlabeled data has become an important research topic in recent years. One way to exploit the unlabled data is to use active learning [7]. The goal of active learning is to choose the most informative example from the unlabeled data for manual labeling. In the past years, active learning has been studied for many classification tasks [16].
Another emerging popular technique to exploit unlabeled data is semi-supervised learning [5], which has attracted a surge of research attention recently [30]. A variety of machine-learning techniques have been proposed for semi-supervised learning, in which the most well-known approaches are based on the graph Laplacians methodology [28, 31, 5]. While promising results have been popularly reported in this research topic, there is so far few comprehensive semi-supervised learning scheme applicable for large-scale classi-fication problems.

Although supervised learning, semi-supervised learning and active learning have been studied separately, so far there is few comprehensive scheme to combine these tech-niques effectively together for classification tasks. To this end, we propose a general framework of learning the Uni-fied Kernel Machines (UKM) [3, 4] by unifying supervised kernel-machine learning, semi-supervised learning, unsuper-vised kernel design and active learning together for large-scale classification problems.

The rest of this paper is organized as follows. Section 2 re-views related work of our framework and proposed solutions. Section 3 presents our framework of learning the unified ker-nel machines. Section 4 proposes a new algorithm of learning semi-supervised kernels by Spectral Kernel Learning (SKL). Section 5 presents a specific UKM paradigm for classifica-tion, i.e., the Unified Kernel Logistic Regression (UKLR). Section 6 evaluates the empirical performance of our pro-posed algorithm and the UKLR classification scheme. Sec-tion 7 sets out our conclusion.
Kernel machines have been widely studied for data clas-sification in the past decade. Most of earlier studies on kernel machines usually are based on supervised learning. One of the most well-known techniques is the Support Vec-tor Machines, which have achieved many successful stories in a variety of applications [25]. In addition to SVM, a series of kernel machines have also been actively studied, such as Kernel Logistic Regression [29], Boosting [17], Reg-ularized Least-Square (RLS) [12] and Minimax Probability Machines (MPM) [15], which have shown comparable per-formance with SVM for classification. The main theoretical foundation behind many of the kernel machines is the the-ory of regularization and reproducing kernel Hilbert space in statistical learning [17, 25]. Some theoretical connections between the various kernel machines have been explored in recent studies [12].

Semi-supervised learning has recently received a surge of research attention for classification [5, 30]. The idea of semi-supervised learning is to use both labeled and unlabeled data when constructing the classifiers for classification tasks. One of the most popular solutions in semi-supervised learning is based on the graph theory [6], such as Markov random walks [22], Gaussian random fields [31], Diffusion models [13] and Manifold learning [2]. They have demonstrated some promising results on classification.

Some recent studies have begun to seek connections be-tween the graph-based semi-supervised learning and the ker-nel machine learning. Smola and Kondor showed some theo-retical understanding between kernel and regularization based on the graph theory [21]. Belkin et al. developed a frame-work for regularization on graphs and provided some anal-ysis on generalization error bounds [1]. Based on the emerg-ing theoretical connections between kernels and graphs, some recent work has proposed to learn the semi-supervised ker-nels by graph Laplacians [32]. Zhang et al. recently pro-vided a theoretical framework of unsupervised kernel design and showed that the graph Laplacians solution can be con-sidered as an equivalent kernel learning approach [27]. All of the above studies have formed the solid foundation for semi-supervised kernel learning in this work.

To exploit the unlabeled data, another research attention is to employ active learning for reducing the labeling efforts in classification tasks. Active learning, or called pool-based active learning, has been proposed as an effective technique for reducing the amount of labeled data in traditional super-vised classification tasks [19]. In general, the key of active learning is to choose the most informative unlabeled exam-ples for manual labeling. A lot of active learning meth-ods have been proposed in the community. Typically they measure the classification uncertainty by the amount of dis-agreement to the classification model [9, 10] or measure the distance of each unlabeled example away from the classifi-cation boundary [16, 24].
In this section, we present the framework of learning the unified kernel machines by combining supervised kernel ma-chines, semi-supervised kernel learning and active learning techniques into a unified solution. Figure 1 gives an overview of our proposed scheme. For simplicity, we restrict our dis-cussions to classification problems.

Let M ( K,  X  ) denote a kernel machine that has some un-derlying probabilistic model, such as kernel logistic regres-sions (or support vector machines). In general, a kernel ma-chine contains two components, i.e., the kernel K (either a kernel function or simply a kernel matrix), and the model pa-rameters  X  . In traditional supervised kernel-machine learn-ing, the kernel K is usually a known parametric kernel func-tion and the goal of the learning task is usually to determine the model parameter  X  . This often limits the performance of the kernel machine if the specified kernel is not appropriate.
To this end, we propose a unified scheme to learn the uni-fied kernel machines by learning on both the kernel K and the model parameters  X  together. In order to exploit the un-labeled data, we suggest to combine semi-supervised kernel learning and active learning techniques together for learn-ing the unified kernel machines effectively from the labeled and unlabeled data. More specifically, we outline a general framework of learning the unified kernel machine as follows. Figure 1: Learning the Unified Kernel Machines
Let L denote the labeled data and U denote the unlabeled data. The goal of the unified kernel machine learning task is to learn the kernel machine M ( K  X  , X   X  ) that can classify the data effectively. Specifically, it includes the following five steps:
This is a general framework of learning unified kernel ma-chines. In this paper, we focus our main attention on the the part of semi-supervised kernel learning technique, which is a core component of learning the unified kernel machines.
We propose a new semi-supervised kernel learning method, which is a fast and robust algorithm for learning semi-supervised kernels from labeled and unlabeled data. In the following parts, we first introduce the theoretical motivations and then present our spectral kernel learning algorithm. Finally, we show the connections of our method to existing work and justify the effectiveness of our solution from empirical ob-servations.
Let us first consider a standard supservisd kernel learn-ing problem. Assume that the data ( X, Y ) are drawn from an unknown distribution D . The goal of supervised learn-ing is to find a prediction function p ( X ) that minimizes the following expected true loss: where E ( X,Y )  X  X  denotes the expectation over the true un-derlying distribution D . In order to achieve a stable esimia-tion, we usually need to restrict the size of hypothesis func-tion family. Given l training examples ( x 1 , y 1 ), ... ,( x typically we train a predition function  X  p in a reproducing Hilbert space H by minimizing the empirical loss [25]. Since the reproducing Hilbert space can be large, to avoid over-fitting problems, we often consider a regularized method as follow: where  X  is a chosen positive regularization parameter. It can be shown that the solution of (1) can be represented as the following kernel method:  X  =arg inf where  X  is a parameter vector to be estimated from the data and k is a kernel, which is known as kernel func-tion . Typically a kernel returns the inner product between the mapping images of two given data examples, such that k ( x i , x j )=  X ( x i ) ,  X ( x j ) for x i , x j  X  X  . Let us now consider a semi-supervised learning setting. Given labeled data { ( x i ,y i ) } l i =1 and unlabeled data we consider to learn the real-valued vectors f  X  R m by the following semi-supervised learning method: where K is an m  X  m kernel matrix with K i,j = k ( x i , x Zhang et al. [27] proved that the solution of the above semi-supervised learning is equivelent to the solution of standard supervised learning in (1), such that The theorem offers a princple of unsuperivsed kernel de-sign: one can design a new kernel  X  k (  X  ,  X  ) based on the unla-beled data and then replace the orignal kernel k by  X  k in the standard supervised kernel learning. More specifically, the framework of spectral kernel design suggests to design the new kernel matrix  X  K by a function g as follows: where (  X  i , v i ) are the eigen-pairs of the original kernel ma-trix K , and the function g (  X  ) can be regarded as a filter func-tion or a transformation function that modifies the spectra of the kernel. The authors in [27] show a theoretical justifi-cation that designing a kernel matrix with faster spectral de-cay rates should result in better generalization performance, which offers an important prici npleinlearninganeffective kernel matrix.

On the other hand, there are some recent papers that have studied theoretical principles for learning effective ker-nel functions or matrices from labeled and unlabeled data. One important work is the kernel target alignment ,which can be used not only to assess the relationship between the feature spaces by two kernels, but also to measure the simi-larity between the feature space by a kernel and the feature space induced by labels [8]. Specifically, given two kernel matrices K 1 and K 2 , their relationship is defined by the following score of alignment :
Definition 1. Kernel Alignment: The empirical align-ment of two given kernels K 1 and K 2 with respect to the sample set S is the quantity: where K i is the kernel matrix induced by the kernel k i and  X  ,  X  is the Frobenius product between two matrices, i.e.,
K
The above definition of kernel alignment offers a princi-ple to learn the kernel matrix by assessing the relationship between a given kernel and a target kernel induced by the given labels. Let y = { y i } l i =1 denote a vector of labels in which y i  X  X  +1 ,  X  1 } for binary classification. Then the tar-get kernel can be defined as T = yy .Let K be the kernel matrix with the following structure where K ij =  X ( x i ) ,  X ( x j ) , K tr denotes the matrix part of  X  X rain-data block X  and K t denotes the matrix part of  X  X est-data block. X 
The theory in [8] provides the principle of learning the kernel matrix, i.e., looking for a kernel matrix K with good generalization performance is equivalent to finding the ma-trix that maximizes the following empirical kernel alignment score: This principle has been used to learn the kernel matrices with multiple kernel combinations [14] and also the semi-supervised kernels from graph Laplacians [32]. Motivated by the related theorecial work, we propose a new spectral ker-nel learning (SKL) algorithm which learns spectrals of the kernel matrix by obeying both the principle of unsupervised kernel design and the principle of kernel target alignment.
Assume that we are given a set of labeled data L = { x i ,y i } l i =1 , a set of unlabeled data U = { x i } n an initial kernel matrix K . We first conduct the eigen-decomposition of the kernel matrix: where (  X  i , v i ) are eigen pairs of K andareassumedina decreasing order, i.e.,  X  1  X   X  2  X  ...  X   X  n . For efficiency consideration, we select the top d eigen pairs, such that where the parameter d n is a dimension cutoff factor that can be determined by some criteria, such as the cumulative eigen energy.

Based on the principle of unsupervised kernel design, we consider to learn the kernel matrix as follows where  X  i  X  0 are spectral coefficients of the new kernel ma-trix. The goal of spectral kernel learning (SKL) algorithm is to find the optimal spectral coefficients  X  i for the following optimization where C is introduced as a decay factor that satisfies C  X  v i are top d eigen vectors of the original kernel matrix K ,  X  K tr is the kernel matrix restricted to the (labeled) training data and T is the target kernel induced by labels. Note that C is introduced as an important parameter to control the decay rate of spectral coefficients that will influence the overall performance of the kernel machine.

The above optimization problem belongs to convex opti-mization and is usually regarded as a semi-definite program-ming problem (SDP) [14], which may not be computation-ally efficient. In the following, we turn it into a Quadratic Programming (QP) problem that can be solved much more efficiently.

By the fact that the objective function (7) is invariant to the constant term T, T F , we can rewrite the objective function into the following form The above alignment is invariant to scales. In order to re-move the trace constraint in (11), we consider the following alternative approach. Instead of maximizing the objective function (12) directly, we can fix the numerator to 1 and then minimize the denominator. Therefore, we can turn the optimization problem into: This minimization problem without the trace constraint is equivalent to the original maximization problem with the trace constraint.

Let vec ( A ) denote the column vectorization of a matrix A and let D =[ vec ( V 1 ,tr ) ...vec ( V d,tr )] be a constant matrix with size of l 2  X  d ,inwhichthe d matrices of V i = v i v with size of l  X  l . It is not difficult to show that the above problem is equivalent to the following optimization Minimizing the norm is then equivalent to minimizing the squared norm. Hence, we can obtain the final optimization problem as This is a standard Quadratic Programming (QP) problem that can be solved efficiently.
The essential of our semi-supervised kernel learning method is based on the theories of unsupervised kernel design and kernel target alignment. More specifically, we consider a dimension-reduction effective method to learn the semi-supervised kernel that maximizes the kernel alignment score. By exam-ining the work on unsupervised kernel design, the following two pieces of work can be summarized as a special case of spectral kernel learning framework:
In our case, in comparison with semi-supervised kernel learning methods by graph Laplacians, our work is similar to the approach in [32], which learns the spectral transfor-mation of graph Laplacians by kernel target alignment with order constraints. However, we should emphasize two im-portant differences that will explain why our method can work more effectively.

First, the work in [32] belongs to traditional graph based semi-supervised learning methods which assume the kernel matrix is derived from the spectral decomposition of graph Laplacians. Instead, our spectral kernel learning method learns on any initial kernel and assume the kernel matrix is derived from the spectral decomposition of the normalized kernel.

Second, compared to the kernel learning method in [14], the authors in [32] proposed to add order constraints into the optimization of kernel target alignment [8] to enforce the constraints of graph smoothness. In our case, we suggest adecayfactor C to constrain the relationship of spectral coefficients in the optimization that can make the spectral coefficients decay faster. In fact, if we ignore the difference of graph Laplacians and assume that the initial kernel in our method is given as K  X  L  X  1 ,wecanseethatthemethod in [32] can be regarded as a special case of our method when the decay factor C is set to 1 and the dimension cut-off parameter d is set to n .
To argue that C = 1 in the spectral kernel learning al-gorithm may not be a good choice for learning an effective kernel, we illustrate some empirical examples to justifiy the motivation of our spectral kernel learning algorithm. One goal of our spectral kernel learning methodology is to attain a fast decay rate of the spectral coefficients of the kernel matrix. Figure 2 illustrates an example of the change of the resulting spectral coefficients u sing different decay factors in our spectral kernel learning algorithms. From the figure, we can see that the curves with larger decay factors ( C =2 , 3) have faster decay rates than the original kernel and the one using C = 1. Meanwhile, we can see that the cumulative eigen energy score converges to 100% quickly when the num-ber of dimensions is increased. This shows that we may use much small number of eigen-pairs in our semi-supervised kernel learning algorithm for large-scale problems.
To examine more details in the impact of performance with different decay factors, we evaluate the classification performance of spectral kernel learning methods with dif-ferent decay factors in Figure 3. In the figure, we compare the performance of different kernels with respect to spectral kernel design methods. We can see that two unsupervised kernels, K Trunc and K Cluster , tend to perform better than the original kernel when the dimension is small. But their performances are not very stable when the number of di-mensions is increased. For comparison, the spectral kernel learning method achieves very stable and good performance when the decay factor C is larger than 1. When the decay factor is equal to 1, the performance becomes unstable due to the slow decay rates observed from our previous results in Figure 3. This observation matches the theoretical jus-tification [27] that a kernel with good performance usually favors a faster decay rate of spectral coefficients.
Figure 4 and Figure 5 illustrate more empirical examples based on different initial kernels, in which similar results can be observed. Note that our suggested kernel learning method can learn on any valid kernel, and different initial kernels will impact the performance of the resulting spectral kernels. It is usually helpful if the initial kernel is provided with domain knowledge.
In this section, we present a specific paradigm based on the proposed framework of learning unified kernel machines. We assume the underlying probabilistic model of the ker-nel machine is Kernel Logistic Regression (KLR). Based on the UKM framework, we develop the Unified Kernel Lo-gistic Regression (UKLR) paradigm to tackle classification tasks. Note that our framework is not restricted to the KLR model, but also can be widely extended for many other ker-nel machines, such as Support Vector Machine (SVM) and Regularized Least-Square (RLS) classifiers.

Similar to other kernel machines, such as SVM, a KLR problem can be formulated in terms of a stanard regularized form of loss + penalty in the reproducing kernel Hilbert space (RKHS): where H K is the RKHS by a kernel K and  X  is a regular-ization parameter. By the representer theorem, the optimal f ( x ) has the form: where  X  i are model parameters. Note that we omit the con-stant term in f ( x ) for simplified notations. To solve the KLR model parameters, there are a number of available techniques for effective solutions [29].

When the kernel K and the model parameters  X  are avail-able, we use the following solution for active learning, which is simple and efficient for large-scale problems. More specifi-cally, we measure the information entropy of each unlabeled data example as follows
Repeat where N C is the number of classes and C i denotes the i th class and p ( C i | x ) is the probability of the data example x belonging to the i th class which can be naturally obtained by the current KLR model (  X , K ). The unlabeled data ex-amples with maximum values of entropy will be considered as the most informative data for labeling.

By unifying the spectral kernel learning method proposed in Section 3, we summarize the proposed algorithm of Uni-fied Kernel Logistic Regression (UKLR) in Figure 6. In the algorithm, note that we can usually initialize a kernel by a standard kernel with appropriate parameters determined by cross validation or by a proper deisgn of the initial kernel with domain knowledge.
We discuss our empirical evaluation of the proposed frame-work and algorithms for classification. We first evaluate the effectiveness of our suggested spectral kernel learning algo-rithm for learning semi-supervised kernels and then com-pare the performance of our unified kernel logistic regression paradigm with traditional classification schemes.
We use the datasets from UCI machine learning reposi-tory 1 . Four datasets are employed in our experiments. Ta-ble 1 shows the details of four UCI datasets in our experi-ments.

For experimental settings, to examine the influences of different training sizes, we test the compared algorithms on four different training set sizes for each of the four UCI datasets. For each given training set size, we conduct 20 random trials in which a labeled set is randomly sampled www.ics.uci.edu/ mlearn/MLRepository.html Table 1: List of UCI machine learning datasets.
 from the whole dataset and all classes must be present in the sampled labeled set. The rest data examples of the dataset are then used as the testing (unlabeled) data. To train a classifier, we employ the standard KLR model for classification. We choose the bounds on the regularization parameters via cross validation for all compared kernels to avoid an unfair comparison. For multi-class classification, we perform one-against-all binary training and testing and then pick the class with the maximum class probability.
In this part, we evaluate the performance of our spectral kernel learning algorithm for learning semi-supervised ker-nels. We implemented our algorithm by a standard Matlab Quadratic Programming solver ( quadprog ). The dimension-cut parameter d in our algorithm is simply fixed to 20 with-out further optimizing. Note that one can easily determine an appropriate value of d by examining the range of the cumulative eigen energy score in order to reduce the com-putational cost for large-scale problems. The decay factor C is important for our spectral kernel learning algorithm. As we have shown examples before, C must be a positive real value greater than 1. Typically we favor a larger decay factor to achieve better performance. But it must not be set too large since the too large decay factor may result in the overly stringent constraints in the optimization which gives no solutions. In our experiments, C is simply fixed to constant values (greater than 1) for the engaged datasets.
For a comparison, we compare our SKL algorithms with the state-of-the-art semi-supervised kernel learning method by graph Laplacians [32], which is related to a quadrati-cally constrained quaratic program (QCQP). More specif-ically, we have implemented two graph Laplacians based semi-supervised kernels by order constraints [32]. One is the order-constrained graph kernel (denoted as  X  X rder X ) and the other is the improved order-constrained graph kernel (denoted as  X  X mp-Order X ), which removes the constraints from constant eigenvectors. To carry a fair comparison, we use the top 20 smallest eigenvalues and eigenvectors from the graph Laplacian which is constructed with 10-NN un-weighted graphs. We also include three standard kernels for comparisons.

Table 2 shows the experimental results of the compared kernels (3 standard and 5 semi-supervised kernels) based on KLR classifiers on four UCI datasets with different sizes of labeled data. Each cell in the table has two rows: the upper row shows the average testing set accruacies with standard errors; and the lower row gives the average run time in sec-onds for learning the semi-supervised kernels on a 3GHz desktop computer. We conducted a paired t -test at signifi-cance level of 0.05 to assess the statistical significance of the test set accuracy results. From the experimental results, we found that the two order-constrained based graph ker-nels perform well in the Ionosphere and Wine datasets, but they do not achieve important improvements on the Heart and Sonar datasets. Among all the compared kernels, the semi-supervised kernels by our spectral kernel learning algo-rithms achieve the best performances. The semi-supervised kernel initialized with an RBF kernel outperforms other ker-nels in most cases. For example, in Ionosphere dataset, an RBF kernel with 10 initial training examples only achieves 73 . 56% test set accuracy, and the SKL algorithm can boost the accuracy significantly to 83 . 36%. Finally, looking into the time performance, the average run time of our algorithm is less than 10% of the previous QCQP algorithms.
In this part, we evaluate the performance of our proposed paradigm of unified kernel logistic regression (UKLR). As a comparison, we implement two traditional classification schemes: one is traditional KLR classification scheme that is trained on randomly sampled labeled data, denoted as  X  X LR+Rand. X  The other is the active KLR classification scheme that actively selects the most informative examples for labeling, denoted as  X  X LR+Active. X  The active learn-ing strategy is based on a simple maximum entropy criteria given in the pervious section. The UKLR scheme is imple-mented based on the algorithm in Figure 6.

For active learning evaluation, we choose a batch of 10 most informative unlabeled examples for labeling in each trial of evaluations. Table 3 summarizes the experimental results of average test set accuarcy performances on four UCI datasets. From the experimental results, we can ob-serve that the active learning classification schems outper-form the randomly sampled classification schemes in most cases. This shows the suggested simple active learning strat-egy is effectiveness. Further, among all compared schemes, the suggsted UKLR solution significantly outperforms other classification approaches in most cases. These results show that the unified scheme is effective and promising to inte-grate traditional learning methods together in a unified so-lution.
Although the experimental results have shown that our scheme is promising, some open issues in our current solution need be further explored in future work. One problem to in-vestigate more effective active learning methods in selecting the most informative examples for labeling. One solution to this issue is to employ the batch mode active learning meth-ods that can be more efficient for large-scale classification tasks [11, 23, 24]. Moreover, we will study more effective ker-nel learning algorithms without the assumption of spectral kernels. Further, we may examine the theoretical analysis of generalization performance of our method [27]. Finally, we may combine some kernel machine speedup techniques to deploy our scheme efficiently for large-scale applications [26].
This paper presented a novel general framework of learn-ing the Unified Kernel Machines (UKM) for classification. Different from traditional classification schemes, our UKM framework integrates supervised learning, semi-supervised learning, unsupervised kernel design and active learning in a unified solution, making it more effective for classification tasks. For the proposed framework, we focus our attention on tackling a core problem of learning semi-supervised ker-nels from labeled and unlabled data. We proposed a Spectral quadprog function.
 Kernel Learning (SKL) algorithm, which is more effective and efficient for learning kernels from labeled and unlabeled data. Under the framework, we developed a paradigm of unified kernel machine based on Kernel Logistic Regression, i.e., Unified Kernel Logistic Regression (UKLR). Empirical results demonstrated that our proposed solution is more ef-fective than the traditional classification approaches.
The work described in this paper was fully supported by two grants, one from the Shun Hing Institute of Advanced Engineering, and the other from the Research Grants Coun-cil of the Hong Kong Special Administrative Region, China (Project No. CUHK4205/04E). [1] M. Belkin and I. M. andd P. Niyogi. Regularization [2] M. Belkin and P. Niyogi. Semi-supervised learning on [3] E. Chang, S. C. Hoi, X. Wang, W.-Y. Ma, and [4] E. Chang and M. Lyu. Unified learning paradigm for [5] O. Chapelle, A. Zien, and B. Scholkopf.
 [6] F.R.K.Chung. Spectral Graph Theory .American [7] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active [8] N. Cristianini, J. Shawe-Taylor, and A. Elisseeff. On [9] S. Fine, R. Gilad-Bachrach, and E. Shamir. Query by [10] Y. Freund, H. S. Seung, E. Shamir, and N. Tishby. [11] S. C. Hoi, R. Jin, and M. R. Lyu. Large-scale text [12] S. B. C. M. J.A.K. Suykens, G. Horvath and [13] R. Kondor and J. Lafferty. Diffusion kernels on graphs [14] G. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, [15] G. Lanckriet, L. Ghaoui, C. Bhattacharyya, and [16] R. Liere and P. Tadepalli. Active learning with [17] R. Meir and G. Ratsch. An introduction to boosting [18] A. Ng, M. Jordan, and Y. Weiss. On spectral [19] N. Roy and A. McCallum. Toward optimal active [20] B. Scholkopf, A. Smola, and K.-R. Muller. Nonlinear [21] A. Smola and R. Kondor. Kernels and regularization [22] M. Szummer and T. Jaakkola. Partially labeled [23] S. Tong and E. Chang. Support vector machine active [24] S. Tong and D. Koller. Support vector machine active [25] V. N. Vapnik. Statistical Learning Theory . John Wiley [26] G. Wu, Z. Zhang, and E. Y. Chang. Kronecker [27] T. Zhang and R. K. Ando. Analysis of spectral kernel [28] D. Zhou, O. Bousquet, T. Lal, J. Weston, and [29] J. Zhu and T. Hastie. Kernel logistic regression and [30] X. Zhu. Semi-supervised learning literature survey. [31] X. Zhu, Z. Ghahramani, and J. Lafferty.
 [32] X. Zhu, J. Kandola, Z. Ghahramani, and J. Lafferty.
