 dimensional scaling and hyperbolic spaces for an interactive display. Compared to the HSOM it is not based on a discrete grid and can be used also in situations where only dissimi-laxity data but not a vector representation is available. 
In Sec. 2 and 3 we introduce the hyperbolic space and the standard multi-dimensional scaling. Sec. 4 explains the combination of both concepts into H-MDS. Even though the look and feel of an interactive visualization and navigation is hardly compressible to paper format, we report several we apply the H-MDS for spanning a space of unstructured text documents, i.e., special "averaged" film critiques. This approach allows to navigate in the space of selected movies. 2300 years ago, the Greek mathematician Euclid founded his geometry on five axioms. The fifth, the "parallel axiom", appeared unnecessary to many of his colleagues. And they tried hard to prove it derivable -without success. After almost 2000 years Lobachevsky (1793-1856), Bolyai (1777-1855), and Gauss (1802-1860) negated the axiom and in-dependently discovered the non-Euclidean geometry. There exist only two geometries with constant non-zero curvature. Through our sight of common spherical surfaces (e.g. earth, orange) we are familiar with the spherical geometry and its constant positive curvature. Its counterpart with con-stant negative curvature is known as the hyperbolic plane H2 (with analogous generalizations to higher dimensions) [2, 18]. Unfortunately, there is no "good" embedding of the H2 in IR a, which makes it harder to grasp the unusual prop-erties of the H2. Local patches resemble the situation at a saddle point, where the neighborhood grows faster than in fiat space. Standard textbooks on Riemannian geometry (see, e.g. [8]) examine the relationship and expose that the This bears two remarkable asymptotic properties, (i) for c(r) .~ 27rr. (ii) For larger r both grow exponentially with the radius. As observed in [7, 6], this trait makes the hyperbolic space ideal for embedding hierarchical structures. Fig. 1 illustrates the spatial relations by embedding a small patch of the H2 in IR 3. 
To use the visualization potential of the H2 we must solve two problems: (i) the data must be "accommodated" in the the foreseeable future). Fortunately, the projection problem was solved more than a century ago. 
It lays in the nature of a curvated space to resist the per-fect projection into the flat Euclidean surface. Each attempt compromises one or more correct representations of length, area, and angle (form) relations, as is well known from the spherical geometry (e.g., the Mercator, Lambert, and stere-ographic projection). Similarly to the spherical geometry and area a(r) is experienced if a "circle" with radius is mapped entirely into a circle, the Poincax~ disk PD. This infinity representation fascinated M. Escher and inspired him to Fig. 2. without touching it. in H2, like a '~fovca". The zooming factor is 0.5 in the center ~md falls (exponentially) off with distance to the fovea. Therefore, the context appears very natural. As more remote things are, the less spatial representation is assigned in the current display. segments or (centered) straight lines in PD. There ex-tensions cross the PD-rim always perpendicular on both ends 2, see Fig. 2. tions are preserved in PD, area and length relations obviously not (in contrast, e.g., to the Klein-Beltrami model which is length preserving). space accommodates exactly one parallel 3 to a line through a given point (not laying on this line), the 
H2 offers infinitely many. In PD a line is an arc who's ends are perpendicular to the circle rim. It is easy to draw many non-crossing circles through any given isolated point (see again Fig. 2). allels": (i) asymptotic parallels, circles which touch at the rim in the same "ca-point", and (ii) ultra paral-lels, circle arcs which do not intersect within PD. One can anticipate, there is much more space in cx~ than "usual" -sometimes the H2 is also called "more inten-sive infinite" than the IR 2. This extra space is desired for finding good accommodation for our data. 
C = 02ClC2 @ 1' 02e1~2 + 1 disparities Dij = D(6ii). In See. 6.3 we will discuss a trans-formation of adjusting a potential dimeusionality mismatch. 
The goal of the MDS algorithm is to find the spatial rep-resentation xi of each objects i in the L-dimensional space, where the pair distances d~j -_--d(x~,xj) match the dispar-typically two or three, since again the main purpose of MDS is visualization and explorative data analysis. The pair dis-tance is usually measured by the Euclidian distance: du=llxl-xJll with xiE]R L, i,j E {1, 2, ..N} (5) 
One of the most widely known MDS algorithms was intro-duced by Summon [17] in 1969. He formulates a minimiza-tion problem of a cost function which sums over the squares of disparities-distance misfits, here written as The Cactors w~j are introduced to weight the disparities in-dividually and also to normalize the cost function E to be independent to the absolute scale of the disparities Dij. De-pending on the given analysis task the factors can be cho-sen to weight all the disparities equally -the #lobal variant (wij" (u) =const) -or to emphasize the local structure by re-ducing the influence of larger disparities (w~)) w!q) 1 . (t) 2 I Note that the latter is undefined if any pair has zero dispar-ity. In his original work [17] Summon suggested an interme-diate normalization which we are using in the following. Summon proposed a steepest gradient method, in particular, the (diagonal) New-ton method to iteratively minimize the remaining cost or stress E. He ignored the off-diagonal part of the Hessian matrix and used a step length reduced by a "magic" factor r/of 0.3-0.4. Starting from a random {xi} initialization, in each iteration step, one object i" is considered, and Eq. 6 minimized with respect to xi. with Ai* here written per component q 6 {I, .., L} The algorithm usually needs several epochs (with random sequence of selected i*) to converge to an cost-function min-imum. The usual methods to deal with therisk of converging to a local minimum are restarting with different initial con-ditions and selecting the result with the lowest cost function or stress E. To save CPU time one may want to start with a good initial configuration {xi} for example by using the first L principal components found by PCA. 
The reader is referred to [1] for further details on this and other MDS algorithms. 
The principtd idea is rather straight forward: instead of using the Euclidean L-dimensional p,L as target space we apply Sammon's idea in the hyperbolic space PD. I.e. we replace the distance metric Eq. 5 by the appropriate distance metric for the Poincard model (see, e.g. [8]) Several aspects need consideration: 
Stay on disk: Care must be taken to stay within the circle PD when updating the point i*. Simple restriction of the update step ~/Ai* in Eq. 9 is not appropriate. Instead the Mfbius transformation Eq. 3 leads in the right direction and handles the exponential shrinkage in the vicinity of the circle rim. x~. new) = T(X~. X 'd); WAi., 1); (12) Gradients: While the gradients ~dij,~/~Xi,q required in 
Eq. 10, are rather simple to compute for the Euclidean ease (= (zl,q -zj,q')/dij), the case becomes complex for Eq. 11 with X, = (Xi,i + ixi,2) E PD C C xj = (zj,t+izj,2) 6PDCC vs = xi,,xj,, + zi,2zj,z --1 with two special cases: (i) the denominator v~ +v] becomes does not occur for valid points inside the unit circle. 
Cost Minimization without Second Derivatives: Due to the complexity of these results we do not compute sec-ond derivatives and rather improve the cost minimization step Eq. 10. We employed the Levenberg-Marquardt ap-proach and followed the argumentation in [13] by dropping the higher derivatives. Each point i has an individual and bounded A-parameter. More details will be reported else-where (in preparation). Disparity preprocesslng: Due to the non-linearity of 
Eq. 11 the preprocessing function D(.) (see Sec. 3.1) has more influence in H2. Consider, e.g., linear rescaiing of the dissim-ilarities Dij = ot6ij: in the Euclidean case the visual struc-ture is not affected -only magnified by a. In contrast in 
H2, a scales the amount of curvature felt that is by the data. In the PD this has the effect that the increasing a shifts all points further outside -but there is also much more (exponentially more) space. It depends on the given dataset and its dissimilarity structure, what is the optimal ~. In the experiments reported below, we set a manually. 
We use this extra degree-of-freedom to choose a compromise between visibility of the entire structure and space for navi-gation in the detail-rich outer areas. It is certainly possible to integrate the optimization of a in the overall minimiza-tion. However, for mapping problems with IRa topology, this would obviously lead to zeroing a, which is not intended. 
We conducted first experiments and present several real and synthetic datasets. 
Fig. 5(a-d) displays the obligatory "Hello World" exam-ple of data mining -Fisher's Iris data set. It describes 150 flower samples of three types: ir/s setosa ("A"), iris ver-sicolour ("x"), and iris virginica ("+"). 6 is here the Eu-clidean pair distance in the four components sepal length, sepal width, petal length, and petal width. They demon-strat~ that MDS performs well in the hyperbolic plane and visualize the focus + context effect with three navigation snap-shots. 
Feature-rich datasets are inherently high-dimeusional. With growing dimeusionality most of their volume is contained in a thin, outer shell. This has consequences of the proximi-ties distribution of those feature-rich, possibly heterogenous datasets. Using Monte-Carlo simulations we examined M-dimensional point distributions with (i) M i.i.d. Gauss ran-dom variables for each component, (ii) uniform distribution inside the unit sphere, (iii) on its surface, (iv} inside the Md-hypercube and (v) on its corners. 6000 5000 4000 3000 2000 1000 
Figure 4: Histogram of pair distances 6 shown for var-ious dimensions M (124,750 pairs from 500 Gauss dis-trlbuted, random points with unit variance; binning width is 0.1). 
Fig. 4 illustrates the first case. With growing M, the dis-tance distribution 6 shifts to larger distances without sig-nificant widening. Even though the peak structures differ for smaller M, the observation holds also for the other cases (ii-v) and can be generalized to inherently high-dimensional data, i.e. not laying in a low-dimensional sub-space. 
As noted by several other authors [3, 5] the optimal em-bedding in low-L spaces tends to a circular ring structure with growing M-dimensionality. Unfortunately, the situa-tion bears numerous local minimal and the Sammon algo-rithm often returns sub-optimal results (some available im-plementations exhibit numerical problems and return here bizarre structures, e.g. the sompak-package). 
How does the H2 accommodate such a dataset? It turns out, that circumstances are much better since the cost func-tion landscape offers more space to circumvent local min-ima. Fig. 5 shows the two target mappings for IRa and H2 together with their disparity--distance scatterplots {Dij, dlj} (c~ = 0.33). The remaining stress was 40% higher for the Euclidean embedding with ERa = 0.40 versus EH2 = 0.285, which is comprehensible also in the two {&amp;j, Dij } scatter-plots in Fig. 5(c,d). 
In times of exponential growth of digital information the semantic navigation in datasets -particularly for the case of unstructured text documents -is a major challenge. In this experiment we demonstrate the application of the hyperbolic MDS to this situation. 
In the domain of the information retrieval and text min-ing text is very often treated as bag-of-words and represented as very high-dimensional vector. One may argue that this ignores completely semantic information word order -amaz-ingly the results justify this drastic step. 
Given a collection of N text documents, first a vocabulary -a set of words {wi} is determined. Words in text are always preprocessed by a suitable word stemming procedure and filtering out of stop words. The vocabulary is then the interesting part of all unique word stems, i.e. the most and the least frequent words are rejected. Each text document t is represented by a feature vector ft, where the components ft,i are determined by ft,~ = TF(t, wi) log ~ . TF(t, wi) is the term frequency and counts the number of times the term wt occurred in document t. DF(wi) denoted the document frequency and counts the number of docu-ments where the term occurred. This standard weighting scheme emphasizes rare words as more significant than com-mon words, for more details see [16]. Proximity (= 1 -6) and therewith diesimflarities of two documents is computed with the cosine metric 6ij = 1 --cos(~, fit ) = 1 -f~, f,j, with fl = and efficiently implemented by storing the normalized doc-ument feature vectors p. x x x \ i [1] 'It'evor F. Cox and Micheal A. Cox. Multidimensional Scaling. Monographs on Statistics and Appied Probability. Chapman and Hall, 1994. [2] H.S.M. Coxeter. Non-Euclidean Geometry. University of Toronto Press, 1957. [3] J. deLeeuw and I. Stoop. An upper bound for SSTR.ESS. Psyehometrika, 51:149--153, 1986. [4] J.A. Hartigan. Representation of similarity matrices by trees. J. Am. Statist, Ass., 62:1140-1158, 1967. [5] HansjSrg Kloek and Joachim Buhmann. Multidimensional sealing by deterministic annealing. In Proc EMMCVPR Venice, 1997. [6] J. Lamping, R. Rao, and P. Pirrolli. A focus+context technique based on hyperbolic geometry for viewing large hierarchies. In ACM SIGCHI Conference on Human Factors in Computing Systems, pages 401-408, 1995. [7] John Larnping and Ramana Rao. Laying out and visualizing large trees using a hyperbolic space. In ACM Symposium on User Interface Software and Technology, pages 13-14, 1994. [8] Frank Morgan. Hiemannian Geometry: A Beginner's Guide. Jones and Bartlett Publishers, 1993. [9] Tamara Munzner. H3: Laying out large directed graphs in 3d hyperbolic space. In Proceedings of the 1997 IEEE Symposium on Information Visualization, Phoeniz, AZ, pages 2-10, 1997. site manager. In Proceedings of Graph Drawing '98, Montreal, Canada, Springer-Verlag, Lecture Notes in Computer Science 1547, pages 384-393, 1998. semantic browsing with self-organizing maps on non-euclidean spaces. In Proe PKDD-~O01, pages 338-349. Springer LNAI 2168, 2001. Visual information foraging in a focus + context visualization. In CHI, pages 506-513, 2001. Numerical Hecipes in C -the Art of Scientific Computing. Cambridge Univ. Press, 1988. Daniel B. Cook. An initial examination of ease of use for 2d and 3d information visualizations of web content. International Journal of Human Computer Studies, 53(5):695-714, 2000. S. Oja, E. &amp; Kaski, editor, Kohonen Maps, pages 97-110. Elsevier, Amsterdam, 1999. approaches in automatic text retrieval. Information Processing and Management, 5(24):513--523, 1988. structure analysis. IEEE Transactions on Computers, 18:401-409, 1969. Springer Verlag~ 1979. mathematics of three-dimensional manifolds. Scientific American, July:94-107, 1984. 
