 1. Introduction
News through the Internet is an important information source, is reported anytime and anywhere, and is affects clustering performance, and how cross-document co-references are resolved.
 focus on the main topic and important content. Moreover, for deeper document understanding, the co-refer-salient information, various practical tasks can be done more reliably, i.e., text summarization ( Azzam,
This paper shows that using summarization as pre-processing in event clustering is a viable and effective technique. Furthermore, we integrate co-reference chains from more than one document by unifying cross-document co-references of nominal elements. Instead of using the traditional clustering approaches, we pro-pose a novel threshold model that incorporates time decay function and spanning window to deal with on-line streaming news. The rest of the paper is organized as follows. Section 2 reviews the previous work and shows tackles the issues surrounding mining controlled vocabulary. A normalized chain edit distance and two algo-rithms are proposed to incrementally mine controlled vocabulary from cross-document co-reference chains.
Section 5 proposes an algorithm for on-line event clustering using dynamic threshold model. Section 6 spec-algorithm is proposed to improve the quality of auto-tagged co-reference chains and the related experimental results are shown. Finally, Section 8 is a conclusion. 2. Basic architecture
Kuo and Chen (2004) employed co-reference chains to cluster streaming news into event clusters. They think the co-reference chains and event words are complementary in some ways, hence they also introduced the event words as defined by Fukumoto and Suzuki (2000) . Kuo and Chen X  X  (2004) experimental results showed that both factors are useful. Furthermore, they present two approaches to combine the two factors for event clustering, which are called summation model and two-level model. The summation model simply adds the scores for both co-reference chains and event words. On the contrary, a two-level model is designed
However, the best performance was by the summation model and improved only 2%, in terms of detection from co-reference chains of different documents for event clustering on streaming news.
Fig. 1 shows the architecture of event clustering. We receive documents from multiple Internet sources, such as newspaper sites, and then send them for document pre-processing. The pre-processing module deals with the sentence extraction and language idiosyncracy, e.g., Chinese segmentation and co-reference resolu-tion. Document Summarization module analyzes each document and employs the co-reference chains and the related feature words, such as event words or high TF-IDF words, to produce the respective summaries.
The controlled vocabulary mining module integrates the co-reference chains to generate controlled vocabulary cluster the documents. 3. Document summarization using co-reference chains
Kuo and Chen (2004) only used the event words as features for clustering. The basic hypothesis is that an event word associated with a news article appears across in a number of paragraphs, but a topic word does not. Moreover, the domain dependency among words is a key clue to distinguish a topic and an event. This appears across paragraphs (documents), and the latter tells if a word appears frequently. Event words are extracted by using these two values. Take an event  X  X  X ir Accident of China Airlines X  X  which happened on
May 25, 2002 as an example. Each related news article has different event words, e.g.,  X  X  X ody recovery X  X , Extracting such keywords is useful to understand the events, and distinguish one document from another.
Nevertheless, due to the strict decision thresholds in the related formulas there are only a few event words extracted and we may lose some important feature words.

Thus, this paper introduces the higher TF-IDF words to be our document features. Document summari-zation module extracts the event words and the 20 highest TF-IDF words from each document. Then, the score of each sentence in a document is computed by adding three scores, i.e., the number of event words, the number of the highest TF-IDF words, and the co-reference scores discussed in the following paragraphs.
Rather than using fixed number of sentences to generate summary, the sentence selection procedure is repeated until a dynamic number of sentences is retrieved. This number is equal to the compression rate mul-and 15, respectively. In this case, the length of summary is 5, i.e., 0.35
The co-reference score of a sentence is computed as follows. The headlines of a news story can be regarded as its short summary. That is, in some sense, the words in the headline represent the content of a document. which cover more co-reference chains contain more information, and should be selected to represent a docu-ment. Five scores shown below are computed sequentially to break the ties during sentence selection. (1) For each sentence that is not selected, count the number of noun co-reference chains from the headline (2) For each sentence that is not selected, count the number of noun co-reference chains from the headline (5) The position of a sentence.

Score 1 only considers nominal features. Score 2 considers both nominal and verbal features and both scores are initiated by the headlines. Scores 3 and 4 consider all the co-reference chains no matter whether these chains are initiated by the headline or not. These two scores rank those sentences on which scores 1 of the same scores (1) X (4). The smaller the position number of a sentence, the more it is preferred. 4. Creating controlled vocabulary from individual co-reference chains
Streaming news stories are disseminated from different sources and written by different conventions and styles. The expression of an entity in a document may be different from the expression of the same entity in another document. Fig. 2 shows an example of four short co-reference chains in four different documents DOC1-DOC4, which are selected from our manual co-reference corpus.

Considering the co-reference chain in DOC1,  X  X   X  X  (President George W. Bush) and  X  X   X  X  (President Bush) denote the same person. There are two identical words  X  X   X  X  (President) and  X  X   X  X  (President Bush) between the chains in DOC1 and DOC2, so that word matching tells us these two chains have the same denotation. However, direct word matching between two co-reference chains may suffer from the following two problems. (1) Because streaming news stories are disseminated from different sources anytime, the arrival sequence of (2) Because there are two matching words  X  X   X  X  (President) and  X  X   X  X  (he) between the co-reference 4.1. Normalized chain edit distance
Instead of using word matching, the concept of normalized chain edit distance is proposed. The edit dis-
AAABB and BBAAA, respectively. The edit distance between s1 and s2 is calculated by the function edit_dis-is extended to determine whether two given co-reference chains are similar or not. Assume there are two co-reference chains  X  say, Given and Incoming. Algorithm 1 computes the chain edit distance of Incoming and wise, they are regarded as different entities.

Algorithm 1. Compute the normalized chain edit distance of incoming and given co-reference chains 1. Let len1 and len2 be the length (i.e., number of words) of Incoming and Given co-reference chains, respectively. 2. Let word1[i] and word2[j] be the i th and the j th elements in Incoming and Given co-reference chains, respectively. 3. Initialize score to be 0. 4. for i = 1 to len1 { (1) let d = edit_distance(word1[ i ], word2[ j ]) (2) d = d /max(length(word1[ i ]), length(word2[ j ])) (3) if d &lt; min then min = d } 5. Compute score = score/len1 and output the score.

Consider the sample co-reference chains shown in Fig. 2 . Assume DOC1 and DOC2 are Given and Incoming be 0.45. The normalized chain edit distance between these two co-reference chains is (0 + 0 + 1 + 1 + 1/ 3 + 0 + 0 + 1/3)/8 = 0.33. Hence, the two co-reference chains in DOC1 and DOC2 are deemed the same entity. Similarly, the edit distance between the Given chain in DOC1 and the Incoming chain in DOC4 is (3/ 5 + 0 + 1 + 3/5)/4 = 0.55. In contrast, this co-reference chain in DOC4 denotes a different entity from that in DOC1. On the other hand, although there is no matching word between the chains in DOC1 and chains can also be deemed to denote the same entity. In summary, the above two issues can be solved in Algo-rithm 1 .
 controlled vocabulary. DOC2 and DOC4 show an example. ((3/5 + 0 + 0 + 3/5)/4 = 0.30 &lt; 0.45) which is an incorrect instance. In such a case, an alternative solution may be: pronouns and personal title words are excluded from cross-document co-reference chains during mining controlled vocabulary. 4.2. Creating controlled vocabulary
As temporal reference denotes a specific time or date, it is not meaningful to unify cross-document temporal references into controlled vocabulary. Thus, we ignore the temporal references in our approach. Algorithm 2 specifies how to mine controlled vocabulary incrementally. Fig. 3 shows some examples in controlled vocab-ulary. The term in bold font is a header (canonic form) of a unified co-reference chain.
Algorithm 2. Mining controlled vocabulary 1. Set the threshold value to be a . 2. Get the first news document and the accompanying co-reference chains.
 3. Initialize the controlled vocabulary to be the co-reference chains. 4. Get the next news document and its co-reference chains until all are processed. d. If there is a normalized chain edit distance whose score is lower than a , the elements in Incoming chain 4.3. Evaluation
We adopted the B-CUBED metric ( Bagga &amp; Baldwin, 1998 ) shown below to measure the precision and recall of the created controlled vocabulary.

The numerator of both formulas (1) and (2) means the number of the same elements between the true chain clustering system proposed in Section 5 . 4.3.1. Data set
In our experiment, we used the knowledge base provided by the United Daily News ( http://udndata.com/ ), which has collected 6,270,000 Chinese news articles from six Taiwan local newspaper companies since 1975/1/ 1. To prepare a test corpus, we first set the topic to be  X  X   X  X  (Air Accident of China Airlines), and the been manually tagged with co-reference chains. Furthermore, we asked three research assistants separately to merge the related co-reference chains into controlled vocabulary and then we used majority rule to create the gold answer.
 4.3.2. Experimental results discrimination among chains, two alternative experiments are conducted. M1 used the original co-reference system only uses the word matching. Normalized chain edit distance is superior to word matching no matter which co-reference chains are adopted. The experimental results also verify that pronouns and personal title words in a co-reference chain contribute little information no matter whether word matching or edit distance approaches are employed. When the approach of edit distance using M2 with threshold 0.33 is adopted, the best performance, i.e., precision 96.49%, recall 96.67%, and F-score 96.58%, is achieved.
Analyzing the created controlled vocabulary using M2, we found that there are three major types of errors shown below. (1) Ambiguous abbreviation problem, e.g.,  X  X   X  X  (Macau) and  X  X   X  X  (Australia) have the same abbre-(2) Lack of semantic information, e.g.,  X  X   X  X  (southern area) and  X  X   X  X  (eastern area) were (3) Word order problems, e.g.,  X  X   X  X  (Remason Typhoon) cannot be merged with chain 5. Event clustering
A single-pass complete link clustering algorithm incrementally divides the documents into several event clusters. Initially, the first document d 1 is assigned to cluster t controlled vocabulary (refer to Steps 2 X 3 of Algorithm 2 ). Assume there already are k clusters when a new article d i is considered. That is, clusters t 1 , t 2 , ... , t below.

At first, we mine new controlled vocabulary from current controlled vocabulary and the incoming news story. The procedure refers to Step 4 of Algorithm 2 . Then we compute the similarities of the summary of the incoming news story with each summary in a cluster. The newly mined controlled vocabulary is global
D and D 2 . Event clustering module uses the headers of the mined controlled vocabulary to replace the related terms in the processing summary. The 20 highest TF-IDF words are used as the feature words for each doc-ument. Moreover, whenever new documents are processed, the related feature words are recomputed. Each document is represented as a vector of normalized TF-IDF weights shown as follows. ined, df j is number of summaries that term t j occurs, and s The similarity between V 1 and V 2 is computed as follows.
 ter. Otherwise, it forms a new cluster by itself.

The motivation for this approach is that news stories appearing on the stream closer together in time are using a fixed detection threshold for comparison strategy, a dynamic detection threshold using a time decay thresholds. Assume the publication day of document D 2 is later than that of document D where dist (denoted as day distance) denotes the number of days away from when the event happens, and w _size (denoted as window size) keeps the threshold unchanged within the same window. 6. Experimental results 6.1. Data set events or mark them as  X  X  X ther X  X . A news article which reports more than one event may be classified into set are Fly right negotiation between Taiwan and Hong Kong (20), Cause of air accident (57), Confirmation (8), Influence on Peng-Hu archipelagoes (26), Punishment for persons in charge (10), News reporting (18),
Wreckage found (28), Remains found (57), Rescue status (65), Solatium (34) and unused events (664) included  X  X  X ther X  X  events and inconsistent coding. The number in the parentheses denotes the documents in the cluster. 6.2. Evaluation metric
We also adopt the metric used in topic detection and tracking (TDT) ( Fiscus &amp; Doddington, 2002 ). The evaluation is based on miss and false alarm rates and both rates are penalties. They can measure more accu-with the clustering results. The performance is characterized by a detection cost, C ability of miss and false alarm: where C Miss and C FA are costs of a miss and a false alarm, P
Feng, and Allan (2002) indicated that the standard TDT cost function used for all evaluations in TDT is C
Det = 0.02  X  P Miss + 0.098  X  P FA . They think that false alarm should be penalized much more heavily than miss. 6.3. Experimental results
Table 1 shows the four model types used in the experiments. For comparison, the centroid-based single-pass clustering model is used as a baseline model. Conventional TF-IDF scheme selects 20 features for each ence model, the algorithm described in Section 3 studies the effects of document summarization using co-with various thresholds are shown in Table 2 . The best results of the two approaches are 0.012990 and 0.013137, respectively, when the threshold is set to 0.05.

With fixed thresholds strategies, the performance of co-reference model is worse than that of the centroid under window 2. Moreover, dynamic threshold using window size is more efficient than the best fixed thresh-Summation model ( Kuo &amp; Chen, 2004 ) are also shown in Table 3 .

Next, we use the co-reference model to consider the length of each document X  X  summary. Previous analysis was with fixed summary length. Dynamic lengths with different compression rates are now adopted. The detec-model using fixed length summary (&lt;0.012647) and summarization model (&lt;0.011603). The experimental results are shown in Table 4 . We conclude that the flexible length summary conveys more information than the fixed length.

In addition to the summary length issue, we use the summarization module described in Section 3 to select shows the approach of including 20 highest TF-IDF words can select more informative sentences in document summarization.

Next, we introduce controlled vocabulary mined incrementally from co-reference chains. Tables 6 and 7 that the more occurrences a word has in a co-reference chain, the more important it is.
In the final experiment, we kept the occurrences of topic elements except pronouns and personal title words, and mined controlled vocabulary from the resulting chains. As the quality of the controlled vocabulary is improved, the experimental results show that the performance of the final model is further improved to 0.010915. Comparing with the best detection costs of the centroid model (0.012990) and the summation model (0.011603), the best result of the final model has 15.97% and 5.93% performance gain. 7. Experiments using noisy co-reference chains
The experiments in Section 6 show that using either co-reference chains or controlled vocabulary improve the performance of the baseline system. Here we deal with the effects of noisy co-reference chains on cross-document event clustering. In other words, the co-reference chains employed in the clustering are created automatically rather than manually. MUC (1998) indicated that the best F-measure of automatic co-reference ing, we introduce a Chinese co-reference resolution system. 7.1. Flow of a Chinese co-reference resolution system
Fig. 5 shows the flow of a Chinese co-reference resolution system. The first four modules, including seg-mentation, named entity recognition (NER), part of speech tagging, and noun phrase chunking, aims to find the possible NP candidates. The statistical information for segmentation and tagging is extracted from Academia Sinica Balanced Corpus ( ASBC, 1998 ). Then the attributes of the candidates are retrieved.
Finally a co-reference resolution algorithm partitions the candidates into equivalence classes using the attributes.

Besides the named entities extracted by a Chinese NER system ( Chen, Ding, Tsai, &amp; Bian, 1998 ), we also employed NP chunkers to extract noun phrases. The maximal NPs, i.e., those NPs not covered by the other NPs, are selected as candidates.
 of speech of head nouns, named entity types, positions, number, pronouns, gender, and semantics of head nouns. To determine the feature values, some linguistic cues are employed. For example, we use morphemes such as  X  X   X  X  (men),  X  X   X  X  (qun),  X  X   X  X  (dui), and so on, to determine plurality. Monetary and percentage expressions are regarded as plural. Numerals are also a cue. Gender is determined by the cues proposed by
Chen and Lee (1996) . In Chinese, a married woman may place her husband X  X  surname before her surname, proposed by Chen, Lin, and Lin (2002) was adopted.
 A clustering algorithm similar to Cardie and Wagstaff X  X  (1999) is used to generate the co-reference chains. co-reference resolution system are 57.52%, 34.28%, and 42.96%, respectively. The F-measure of co-reference resolution in Chinese documents is lower than that (61.8%) in English documents.
 7.2. Experimental results of using noisy co-reference chains
Table 8 shows the results of using noisy co-reference chains. Compared with the cost of centroid model erence chains.

Because the metric of chain edit distance is adopted to generate the controlled vocabulary, the lower the the controlled vocabulary mined from auto-tagged co-reference chains, the performance is worse than without using controlled vocabulary. The size of controlled vocabulary is 2230 and the best detection cost with con-trolled vocabulary is increased to 0.012479. There are two major types of errors: (1) Noun phrase errors (2) Accuracy of co-reference chains 7.3. A co-reference chain filter
The co-reference resolution system specified in Section 7.2 employs only the information in a document to capability of Algorithm 2 .

For each term in an auto-tagged co-reference chain, we extract all the sentences containing the term from a document set. For each extracted sentence, we include the previous and the following two sentences to form co-reference chain. Using the context, a term extraction algorithm similar to Chien (1997) is employed. The related noun phrases are corrected according to the newly extracted terms. Then we compare the similarity in the chain.

Two approaches are adopted to measure the context similarity. The first one is an Overlap Ratio method defined as follows.
 where
Context( t 1 ) and Context( t 2 ) denote the contexts of terms t
Context( t 1 ) \ Context( t 2 ) denotes the term overlap, and j Context( t 1 ) j , j Context( t 2 ) j , and j Context( t designated contexts.

Some examples are shown in Table 9 . The correct term pair has a larger ratio, in contrast, the wrong one has a smaller ratio.
 degree of freedom ( r 1 * s 1) are used to obtain the critical value by looking up the Chi-square table.
Assume the null hypotheses H 0 and H 1 are the two terms denoting the same entities and different entities, the null hypothesis is rejected and the term pair is removed from co-reference chain. An example using Chi-square test is shown in Appendix A . where f ij denotes the number in row i and column j , f i . denotes the sum of numbers in row i , f . j denotes the sum of numbers in column j , and N  X 
For evaluating these two approaches, we randomly selected 2400 correct word pairs and 2400 wrong word pairs from the above noisy co-reference chains. Thus, if we just guess by chance in such a way that all are correct or all are wrong, the probable accuracy is 0.5, which can be deemed as the lower bound for accuracy. corpus is collected from the streaming news for more than 3 months, we introduce the frequency difference measure (FD) to ignore the fluctuation of event focus. For example, the frequency of the word  X  X   X  X  (res-observed words. In addition, for the Chi-square approximation to be valid, the observed frequency should be only outperforms the Chi-square testing only . When combining these two approaches in Table 10 (c) the accu-racy is improved significantly (refer to threshold = 0.60 and FD = 35).

Consider an example to demonstrate why combining these two approaches is more effective. The frequen-cies of the terms  X  X   X  X  ( X  X  X hina Airlines X  X ) and  X  X   X  X  ( X  X  X ai-Hwa ship X  X ) are 17,140 and 357, respec-tively, and the number of their co-occurring terms is 269. If we use the Overlap Ratio measure only (269/ way, false positives are decreased and the overall performance is improved. 7.4. Performance of event clustering using clearer co-reference chains
The co-reference chain filter not only revises the error terms in co-reference chains, but also deletes the of the created controlled vocabulary is improved as well. Appendix B demonstrates an example of controlled vocabulary before/after employing chain filter.

Table 11 shows the performance of event clustering using clearer co-reference chains. Compared with the (0.011809) without controlled vocabulary from Table 8 . Thus, we conclude that controlled vocabulary is promising in event clustering regardless of using manual or auto-tagged co-reference chains. 8. Concluding remarks
This paper proposes a normalized chain edit distance to mine, incrementally, controlled vocabulary from cross-document co-reference chains, and uses the results to unify the features used in event clustering on streaming news. Time decay function and spanning window capture the specific characteristics of on-line news. The experiments using manual co-reference chains show that occurrences of discriminative elements onstrates 15.97% and 5.93% improvement compared to the centroid and the summation models, respectively.
Furthermore, a Chinese co-reference resolution system is introduced to investigate the performance of event clustering under a noisy environment. A chain filtering algorithm is proposed, and the related experiments
In the future, we plan to apply controlled vocabulary to other applications, such as summary generation and construction of named entity ontology. Furthermore, we will also use the documents of other topics to study the validity of our proposed model in event clustering.
 Acknowledgement Research of this paper was partially supported by National Science Council, Taiwan, under the contracts NSC94-2752-E-001-001-PAE and NSC95-2752-E001-001-PAE.
 Appendix A
An example of Chi-square test for a term pair  X  X   X  X  (Peng-Hu) and  X  X   X  X  (The island of chrysanthe-mum, alias of Pen-Hu) using constraints of both frequency &gt; =5 and frequency difference &lt; =10.
Assume H 0 is:  X  X   X  X  (Peng-Hu) and  X  X   X  X  (The island of chrysanthemum, alias of Pen-Hu) denote the same entity, and H 1 is: two terms denote the different entity.
A level of significance P = 0.05 is selected. The critical value of Chi-square with (2 1) x (24 1) degree of that  X  X   X  X  (Peng-Hu) and  X  X   X  X  (The island of chrysanthemum) denote the same entity based on the hypothesis H 0 .

Item f . j 28 21 18 13 20 16 18 15 10
Item f . j 16 14 23 24 19 22 77 43 22 f . j 22 18 19 11 18 18 525
Appendix B. Controlled vocabulary before/after employing chain filter References
