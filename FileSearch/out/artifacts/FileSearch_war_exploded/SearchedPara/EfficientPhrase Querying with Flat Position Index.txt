 A large proportion of search engine queries contain phrases, name-ly a sequence of adjacent words. In this paper, we propose to use flat position index (a.k.a schema-independent index) for phrase query evaluation. In the flat position index, the entire document collection is viewed as a huge sequence of tokens. Each token is represented by one flat position, which is a unique position offset from the beginning of the collection. Each indexed term is associ-ated with a list of the flat positions about that term in the sequence. To recover DocID from flat positions efficiently, we propose a nov-el cache sensitive look-up table (CSLT), which is much faster than existing search algorithms. Experiments on TREC GOV2 data col-lection show that flat position index can reduce the index size and speed up phrase querying substantially, compared with traditional word-level index.
 H.3.3 [ Information Search and Retrieval ]: [Search process, effi-ciency] Algorithms, Performance, Experimentation Flat position index, Phrase query evaluation
With the explosive growth of Web data, how to seek informa-tion efficiently and effectively has been a very important problem to both research community and industry. Search engines have be-come the most commonly used tools for Web information retrieval, and it is necessary to process thousands of queries per second. A significant fraction of the submitted queries contain phrases, name-ly a sequence of adjacent words. Users can submit explicit phrase queries to search engines typically by enclosing them in quotation marks. [3] reported that there were 8 : 3% of explicit phrase queries in excite log during 1997-1999. In addition, users can also submit implicit phrase queries. [7] analyzed a large amount of AltaVista X  X  query logs, finding that many queries without explicit phrase op-erator were actually implicit phrase searches. Recently, [9]ana-lyzed Twitter search log and found that about 15.22% percent of the queries are celebrity names, which are possibe phrase queries. It indicates that phrase querying is very important for social net-work websites, too.

Usually, search engine employs a two-step approach for evalu-ating phrase queries based on the traditional word-level index [2]. In the traditional word-level index, each indexed term is associated with a posting list, and each posting is a triplet, namely a docu-ment identifier ( DocID ), an in-document term frequency ( TF ), and a list of term offsets in that document. With the traditional word-level index, search engines first intersect DocID sets to get a list of candidate documents that possibly contain the phrase, and then checks whether the query terms are adjacent or not in the candidate documents. However, as suggested in [3], the traditional word-level index is not efficient since the cost for processing common term X  X  posting list is very high. To solve this problem, one crude method is to remove stop words in queries, which may result in in-correct query evaluation. Other methods [3, 10] add auxiliary struc-tures (e.g. N-gram, partial next word index, phrase index, etc) to speed up phrase query evaluation. One shortcoming of these meth-ods is that not all the candidate documents retrieved in the first step contain the target phrase, which adds un-necessary overloads and slows down the overall processing.

To seek one more efficient way for phrase search, in this paper, we propose to use the flat position index for phrase query evalua-tion. Flat position index (a.k.a schema-independent index ) was first proposed by [5] to process queries on structured text, and [6] p-resented an overall description of its structure. In the flat position index, it views the whole document collection as a single sequence of tokens. Thus, each token can be represented by a unique posi-tion offset from the beginning of the collection. For each indexed term, the posting list is composed by a list of position offsets. Flat position index has many potential advantages over the traditional word-level index due to the simple structure. First, DocID and T-F information need not be stored, which results in smaller index. Second, the flat position structure is able to support the queries with position constraints very flexibly, e.g. proximity search and phrase search.

Though the structure of flat position index is very simple and elegant, few studies have presented the implementation details of one real retrieval system with it, and none reported the real per-formance of it in public literature. It is challenging to deploy flat position index into real systems at least in three aspects: In this paper, we build a real retrieval system with the flat posi-tion index, and we empirically examine the performance of phrase query evaluation based on that. To the best of our knowledge, it X  X  the first study to explore the flat position index for evaluating phrase queries. To solve the first challenge, we divide the whole collection space into several subspaces. Each subspace is a subset of the w-hole document collection, and each token in that can be represented as a 32-bit integer identifier. To recover DocID efficiently, we pro-pose and implement a novel cache sensitive look-up table(CSLT) to map the flat position into corresponding DocID efficiently. The algorithmic complexity for each mapping operation is O (1) . The experiments show that it is three times faster than CSS (cache sen-sitive search) tree [4].

We construct extensive experiments based on TREC GOV2 col-lection and compare our results with the traditional word-level in-dex for phrase evaluation. Without nextword structures, flat posi-tion index can reduce the index size by 6.3% and speed up phrase evaluation by 31.3%, compared with traditional word-level index. With nextword structures, flat position index can reduce the index size by 7.3% and speed up phrase evaluation by 33.5%, compared with traditional word-level index. We also examine how auxiliary structures affect the performance of phrase querying for flat posi-tion index.
In the traditional word-level index [2], each posting of a term t is represented by a triplet, formally we have where d is the document identifier, f d;t is the number of occur-rences of t in the document ( TF ) and the o d; ( ) are the increasingly ordered offsets of t occuring in the document d . With this struc-ture, traditional phrase querying is a two-step approach: perform DocID set intersection to get a list of candidate documents, and then check whether candidate documents contain the target phrase query. However, not all the documents retrieved in the first step are the final results. To address this problem, in this section, we introduce flat position index and discuss how to use it for phrase querying.
Flat position index [5, 6] views the whole document collection as a single sequence of tokens, and each token in the collection has one global and unique identifier (i.e. one flat position). Flat posi-tion index is composed by three main components: dictionary, in-verted index file and a document boundary array. Similar to that in traditional word-level index, the dictionary contains various statis-tics information such as document frequency ( DF ) and collection frequency ( CF ) for the terms in the collection. Posting lists of in-dexed terms are stored in the inverted index file. The posting list is composed by a list of flat positions, i.e. global offset values. The document boundary array stores all boundaries of documents in the collection, which is used to map a flat position into corresponding DocID . We present one example in Figure 1 to illustrate the struc-ture of the flat position index. In this example, the whole document collection consists of five documents. The first occurrence of term Figur e 1: An example for the structure of flat position index. A is the first token in this collection, so its flat position is 1. Sim-ilarly, the flat positions for the first and the third occurrences of term B are respectively 2 and 12. Interestingly, we can see that flat position index doesn X  X  store DocID and TF information.

When the size of collection is very large, the maximum flat po-sition in the collection may not be represented by a 32-bit integer, e.g., GOV2 collection has 23 billion tokens. One na X ve method is to use a 64-bit integer to represent a flat position. However, it has two limitations: 1) more space is needed to store a position in memory; 2) some state-of-the-art compression algorithms can X  X  work for 64-bit integers [1]. To address those problems, we propose to use the subspace dividing method, which divides the whole col-lection into several subspaces. In each subspace, flat positions can be represented by a 32-bit integer, and each subspace is indexed by an subspace identifier. After subspace dividing, each flat position is represented by two 32-bit integers: the subspace identifier and the offset in that subspace. In practice, flat positions in one block usually have the same subspace identifier, so we can just store one subspace identifier for one block to reduce the space.
Since there X  X  no explicit DocID information in the flat position index, we have to recover the DocID from flat positions in query evaluation. The problem of recovering DocID can be formulated in a more general problem: given an integer m and an ordered array { T i } n 1 i =0 , find out an index number k ( 0  X  that T k &lt; m and T k +1  X  m .

A couple of methods can be used to solve this problem: bina-ry search, m-array search trees, CSS-tree (cache sensitive search tree) and CPSS-tree (cache/page sensitive search tree) [4] . Though CSS-tree and CPSS-tree are cache conscious data structures, the time complexity is still O (log m ( n )) , which is not efficient enough for an extremely large arrays.

To speed up the position mapping, we propose a novel cache sensitive look-up table (CSLT). The idea is to reduce the number of cache misses by making most of searching operations in a cache line [4].

Let the size of cache line be 2 a bytes , the average document length be l and each element in document boundary array cost 2 bytes. We assume that the lengths of documents in the collection do not vary too much. The number of documents in a cache line is nearly 2 a b , and a cache line can cover l  X  2 a b positions on average. The position space that a cache line cover is called as a cache line position space (CLPS), which is a sub-range of flat positions in the whole collection. The whole position space can be divided into several CLPSs. If the biggest position value in the collection is L , then the number of CLPS is  X  L l 2 a b  X  a two-layer structure to implement CSLT. The first layer is a index array to record the offsets where each CLPS starts; the second layer is the document boundary array. The index array can be built very efficiently as follows: linear scan the document boundary array, assign each boundary value in it to the corresponding CLPS and set the i -th entry in the index array with the offset of the smallest document boundary value in the i -th CLPS.

The time complexity of building the index array is O ( n ) , n is the number of documents in collection. For recovering DocID from one flat position, it includes two steps: 1) Compute the offset in the index array by dividing the position by the size of CLPS. Read the value S in corresponding entry of the index array . 2) Linear scan the document boundary array from S , and stop when the value of current entry is bigger than the searching position. Return the boundary offset in document boundary array.

Figure 2 illustrates the structure of CSLT. In this example, we have a = 3 , b = 1 and l = 8 , so a cache line can cover 32 posi-tions and there are totally four CLPS. Then we show how to set the entries of the index array. For example, the second CLPS ranges from 32 to 63, and the fifth entry of the document boundary array is the first boundary value belonging to this CLPS. So we set sec-ond entry of the index array to the corresponding offset, i.e. 4. For recovering DocID from one flat position 45, we first derive that this position belongs to the second CLPS (45 divided by 32), then we load the second entry of index array to get 4 . Second we begin to scan the document boundary array from the fifth element. We skip the first entry 40 since it X  X  smaller than 45. We stop at the second entry 46 (bigger than the searched number 45). The offset of this entry in the document boundary array is 5. So we know that the flat position 45 is mapped to DocID 5.

The time complexity of CSLT is O(1). The expected number of cache miss for mapping a position to DocID is 2 on average: one in the index array and the other in the document boundary array.
For flat position index, we consider two kinds of auxiliary struc-tures common used in traditional word-level index, namely skip lists and nextword structure [10]. Skip lists can reduce the amount of data for decoding while nextword can speed up phrase querying. Both of them can be easily adaptive to flat position index since flat position index is also one variation of word-level inverted index. See Section 3.1 for details. Dataset : We use the TREC GOV2 collection, which consists of 25.2 million web pages crawled from the .gov Internet domain. We use two different sets of queries in our experiments. One query set was used in TREC06 efficiency task of Terabyte track. The oth-er query set is sampled from the MSN Search Spring 2006 Query logs. The detailed statistics are shown in Table 1. We run al-l the experiments on the server with two Quad-Core Intel Xeon 5310(1.66GHz) processors (we only use one core for our experi-ments) and 8GB of RAM. For each query evaluation, we first load all the necessary posting lists into the main memory.
 Query processing : There are two main approaches to process queries [2], either using DAAT(document-at-a-time) or using TAAT(term-at-a-time) for the standard word-level inverted index. These t-wo approaches can be employed in flat position index, too. In our experiments, we empirically find that DAAT is more efficient for phrase querying for both flat position index and the tradition-al word-level inverted index, so in the following sections, all our experiments are based on DAAT .
 Compression algorithms : We test four state-of-art compressions algorithms in both the standard word-level inverted index and flat position index, including PForDelta, Group Varint [6], Rice and VarByte. In our experiments, we find that PForDelta and Rice re-sults in smaller index size than the others, and PForDelta is much faster than Rice (about 6 times) in decoding data. To make a trade-off between decoding speed and index size, we select PForDelta as the major compression algorithm for both the standard word-level inverted index and flat position index in the following experiments. When the length of one posting list is less than 128, we use VarByte algorithm.
 Skip lists : Skip list has been commonly used to speed up process-ing queries. When adding skip list to an index, it X  X  very important to set a reasonable skip distance. Here we follow [8] X  X  approach to set list-length-dependent skip lengths. We show our setting of skip lengths for different posting lists in Table 2. After adding skip list, the index size increased by about 1%. Skip lists affect the index size very little but speed up the query evaluation substantially. In the following experiments, all indexes are added with skip lists by default if not particularly mentioned.
 Nextword index : Nextword index [10] is proposed to speed up phrase querying. To see how it affects the performance of phrase query evaluation, we add top-5 partial nextword index to both the traditional word-level index and the flat position index. For com-parisons, we also keep the original indexes without nextword index. Performance of DocID mapping : We first evaluate the perfor-mance of CSLT for DocID mapping. We compare four search al-gorithms, namely linear search, binary search, CSS tree, and our proposed CSLT. We collect the posting lists of 5 K query terms as our test data. We use a 25 M document boundary array, which is sampled from GOV2 test collection. In Figure 3 , we can see that 1) our CSLT performs much better than all the others when the length of posting list is smaller than 5  X  10 6 , the major reason is that the number of expected cache misses for CSLT is smaller than the others; 2) while for very long posting lists (e.g. posting lists of stopwords), linear search performs best since we need to perform DocID mapping for most of documents in the collection. Based on the analysis above, we use the linear search algorithm when the length of boundary array is bigger than N 4 , where N is the docu-ment number in collection; otherwise, we use the proposed CSLT. Index size : We examine the index sizes with/without top 5 partial nextword in both traditional word-level index and flat position in-dex. As shown in Table 3, we find that flat position index reduces the index size substantially. The major reason is that it doesn X  X  store DocID and TF .
 Performance of phrase querying : Generally, the overall phrase query evaluation into four steps: 1) loading data; 2) decoding data; Figur e 3: Results of recovering DocID for different algorithms. Table 4: Performance of phrase querying in TREC query set. 3) finding phrase; 4) scoring documents. To see why flat position index is more effective in phrase querying, we further examine time costs in each individual step. The time for loading data is not con-sidered since we X  X e loaded all the necessary data into main memory before each query evaluation. We also ignore the fourth step since it is the same for traditional word-level inverted index and flat po-sition index. We further consider two kinds of index structures: a) inverted index without nextword (denoted as Inverted ); b) invert-ed index with top 5 nextword structure (denoted as Inverted+NW ). The results on TREC and MSN query sets are shown respectively in Table 4 and 5. We can see that 1) flat position index is more efficient for query evaluation with/without nextword; 2) flat posi-tion index reduces decoding time significantly; 3) nextword can improve query evaluation for both traditional word-level index and flat position index.
 Examine the effect of different auxiliary structures : In our ex-periments, we consider two kinds of auxiliary structures for flat po-sition index, skip list and nextword index. We examine how these auxiliary structures affect the performance of phrase querying for flat position index. We consider four kinds of index as compar-isons: raw index, raw index with only skip list, raw index with only top 5 nextwords index, raw index with both skip list and top 5 nextwords index. Fig 4 presents the average processing time of different query lengths using flat position index with different auxiliary structures. The major findings are 1) both skip list and nextword index improve the performance of phrase querying for flat position index, and skip list is more helpful than nextword; 2) skip list together with nextword gives the best performance; 3) when the length of phrase query is long, auxiliary structures is very necessary to improve the performance of phrase querying. We also performed these comparison experiment in traditional word-level index, and we got the similar findings. In this paper, we propose to use flat position index for efficien-Table 5: Performance of phrase querying in MSN query set. Figur e 4: Average processing time of different query lengths using flat position index with different auxiliary structures. t phrase querying. We construct extensive experiments based on TREC GOV2 collection. We find that flat position index is very efficient for phrase evaluation. In the future, we plan to do more exploration on how to apply flat position index to general query e-valuation. One possible way is to explicitly store DocID and TF information in flat position index; since flat position index is very efficient to deal with proximity information, another promising way is to transform non-phrase queries into equivalent or approximate queries with proximity constraints. Flat position index can be also used as an auxiliary structure to support efficient proximity related queries.
 This work has been partially supported by HGJ 2010 Grant 2011ZX01042-001-001 and NSFC with Grant No.61073082, 60933004. [1] V. N. Anh and A. Moffat. Inverted index compression using [2] V. N. Anh and A. Moffat. Structured index organizations for [3] D. Bahle, H. E. Williams, and J. Zobel. Efficient phrase [4] S. B X ttcher and C. L. A. Clarke. Index compression is good, [5] C. L. Clarke, G. V. Cormack, and F. J. Burkowski. An [6] J. Dean. Invited talk: Challenges in building large-scale [7] C. Silverstein, H. Marais, M. Henzinger, and M. Moricz. [8] T. Strohman and W. B. Croft. Efficient document retrieval in [9] J. Teevan, D. Ramage, and M. R. Morris. #twittersearch: a [10] H. E. Williams, J. Zobel, and D. Bahle. Fast phrase querying
