 Causal knowledge is important for facilitating comprehension, diagnosis, prediction and control in automated reasoning. Causal Bayesian networks are extensions to Bayesian networks that explicitly and concisely represent causal knowledge as vari-ables and their directed graphical relationships in uncertain domains [10]. This re-search focuses on learning causal knowledge from data that corresponds to learning the structure of causal Bayesian networks for knowledge discovery. A major research challenge is to learn causal knowledge from both observational and interventional data. Observational data are derived from passive observations when the underlying system evolves autonomously. Interventional data are observed when some variables are actively manipulated to specific values, while the other variables evolve autono-mously according to the underlying system mechanisms; such data directly reflect the effects of the manipulated variables on the other variables of the system. 
Most of the existing Bayesian network structure learning methods deal with obser-vational data [5]. Recently, some new methods have been proposed to combine possible assumptions for probability updates with both observational and interventional data and extended the Bayesian method in Cooper and Herskovits [1] for probability update with a closed form. Tong and Koller [14] applied active learning strategy to Bayesian networks and collected new interventional data for further structure probabil-ity updates. Eberhardt et al. [4] proved that, under ideal conditions with causal Markov assumption and faithfulness assumption (and ideal distributions), the number of inter-ventions required to identify the causal relationships among N variables is 1  X  N when only one variable can be manipulated each time, and the number of interventions is log when multiple variables can be manipulated simultaneously. 
Active learning in Bayesian networks involves interventions by manipulating spe-cific variables, and observing the patterns of change over the other variables to derive causal knowledge. In previous work [14], the interventional data are assumed to be multiple instances are collected when one variable is manipulated at each active measuring protein expression levels with flow cytometry in biology, the expression levels of some proteins (as variables) can be manipulated to certain levels; their ef-fects on the expression levels of the other proteins are observed from many cells at a time. Such observations of protein expression levels from one cell constitute the val-ues in one instance [3, 11]. 
With an interventional data set, we can determine the causal influences of the ma-nipulated variables on the other variables based on the theory of causality with agency : manipulating causes can change the effects but not vice versa. In practice, marginal distributions of the variables are us ed to detect causal influences. If the mar-ginal distribution of variable B changes when variable A is manipulated to different values, we say that variable A precedes variable B in causal ordering. 
There are different definitions of intervention: perfect intervention, imperfect in-tervention [8, 13], and uncertain intervention [3]. Different types of intervention have different effects on the Bayesian network structures learned from data. We will focus on perfect intervention in this work. When we manipulate a variable under perfect intervention, the manipulated variable takes the value we specify in the intervention. This is the meaning of manipulation in the general sense. 
Our objective is to learn the causal Bayesian network structure that achieves the specified structure accuracy with a minimal number of interventions, when the inter-ventional data comprise a data set at each active learning step: 1) What is a good criterion for selecting the nodes for new interventions, with respect to  X  X orrectness X  in terms of entropy of the learned structure? 2) What is the effect of the stop criterion on the learned structure in the learning process? 
We introduce a new active learning algorithm for causal Bayesian networks with a non-symmetrical-entropy-based node selection criterion and an entropy-based stop criterion. The non-symmetrical entropy is motivated by the non-symmetrical nature of the interventions. We examine the effectiveness and efficiency of the proposed method on identifying causal relationships based on three benchmark Bayesian net-works and two Bayesian networks we created; we compare our method with some other major methods including node selection with symmetrical entropy, random node selection, and observational data only, and we observe that the results are promising. 2.1 Causal Bayesian Networks A causal Bayesian network [10] is a directed acyclic graph (DAG), in which each node corresponds to a distinct variable to a causal influence from the parent variable to the child variable. The parent variable of an edge is the variable at the tail of th e edge, and the child variable is the variable at the head of the edge. The meaning of  X  X au sality X  in causal Bayesian networks is as follows: when we manipulate the parent variable by fixing its state to different values, we can observe the change in the probability distribution of the child variable. If there is no causal influence from variable A to variable B , there will be no edge from variable A to variable B in the causal Bayesian network. Moreover, when one vari-able is manipulated, the causal influence relationship between other variables will not the same. Under the causal Markov assumption, each variable is independent of its ancestors given the values of its parents. The joint probabilities in the domain can be represented as where ) ( per, we will use  X  X ode X  and  X  X ariable X  interchangeably. A good definition of causal Bayesian networks and its properties can be found in Pearl X  X  book [10]. 2.2 Active Learning Generally, there are two categories of approaches to learn Bayesian network struc-tures from data: score-and-search-based a pproaches [1, 6] and constraint-based ap-proaches [12]. These methods are considered to be passive learning and the data set does not change in the learning process. 
Active learning is different from passive learning. In active learning, new data will with an available data set, and node selection for intervention is based on the expected posterior loss of the structure entropy. The edge probabilities need to be estimated for expected posterior loss calculation under a ll the possible interventions and the possi-ble outcomes of each intervention. The intervention with the maximal expected poste-new collected instance will be combined with the available data for edge probability estimation. The process can be repeated until the goal is reached. 
Estimating the edge probabilities is an important part of the active learning proc-ess. For every pair of variables, three possible situations between them are usually considered: an edge from A to B ( B A  X  ), an edge from B to A ( B A  X  ), or no edge between A and B ( B A  X  ). The probabilities of the edges given the available data D and domain knowledge K are defined as following discussions, D and K will be omitted for brevity. The probabilities of  X  and B A  X  are similarly defined as the probability of B A  X  . The edge en-tropy is defined as in Tong and Koller X  X  paper [14]: 
The structure entropy of Bayesian network G is defined as 
In Tong and Koller X  X  work [14], the edge probabilities are estimated approximately with Markov Chain Monte Carlo (MCMC). In contrast, we estimate the edge prob-abilities with an exact method proposed by Koivisto [7], since the exact edge probabilities can provide more information for node selection. Koivisto utilized the intuition that the order of the parents of a variable is irrelevant to the variable X  X  prob-ability estimation, and applied forward and backward dynamic programming and fast truncated Mobius transform to estimate all the edge probabilities in ) 2 ( n n O time, where n is the number of variables in the domain. For the interventional data, the instances with the variable manipulated will not be used in calculating the probability of the family with the manipulated variable as the child (Refer to Cooper and Yoo X  X  work [2] for this method). Koivisto X  X  exact method can be applied to domains with a moderate number of variables (around 25). 2.3 Selecting Nodes for New Interventions We consider the situation where a data set will be collected when one variable is un-der one intervention. It is likely that an interventional data set will show whether the manipulated variable will affect the probability distributions of other variables. The method mentioned above could not be effectively applied to this situation due to computational complexity. Suppose that m instances are collected in each active learning step when one variable is manipulated in a domain with n binary variables, 
We propose to select the node with the maximum node uncertainty from the cur-rent data for intervention and will not consider the possible interventions and the possible outcomes from each intervention. This will reduce the computational cost significantly. The node uncertainty between a variable and all the other variables can be estimated under two different conditions: The first case second case other variables: B A  X  , B A  X  , and B A  X  . The second case is generally used in Bayesian network structure leaning. We refer to definition of the non-symmetrical entropy is motivated by the non-symmetrical nature of the intervention. In an intervention, we can only manipulate one variable in a pair of variables to derive the causal information between the pair: whether the manipu-lated variable affects the non-manipulated variable. We cannot derive causal informa-tion from the non-manipulated variable to the manipulated variable. If both variables are manipulated, we cannot derive useful causal information between this pair of variables from the interventional data. 
Besides examining the effects on node selection with these two measures, we also consider random node selection for intervention and observational data. 2.4 Stop Criteria for Ca usal Structure Learning Another main problem in applying Bayesian network learning for causal knowledge discovery in practice is to decide when to stop the learning process  X  when do we think that the learned causal Bayesian network is good enough? The intuitive way is to choose a fixed number of interventions as the stop criterion. The disadvantage of this approach is that there is no guarantee on the quality of the learned Bayesian net-work structure. We propose to use certain  X  X cceptable X  entropy of the learned struc-different values of entropy of the learned structure as the stop criteria on the accuracy of the learned structures. The proposed method has been tested in experiments with the same benchmark Bayesian networks as those reported in Tong and Koller X  X  work [14]: Cancer network (as shown in Figure 1), Asia network, and Car network, and two Bayesian networks created by ourselves: Study network and Cold network [9]. There are 2 variables in Study network, 3 variables in Cold network, 5 variables in Cancer network, 8 variables in Asia network and 12 variables in Car network, respectively. We con-ducted the simulations under MATLAB 1 (version 7) with the support of the BDAGL package 2 . The machine used is a Dell OptiPlex GX280 desktop with 1 Gigabyte memory and 3GigaHz Intel processor. 
The experiment setup is as follows: In the experiments, the edge probabilities are estimated with the exact method from Koivisto [7]. The uniform prior of Bayesian network structures is used. The size of the observational data N_obs is set to 20, and the size of the interventional data N_int in each intervention changes from 1 to 200 instances. Such data size for each inter-vention is more realistic than an ideal distribution as discussed in Eberhardt et al. [4]. We tested two stop criteria in our experiments  X  the number of interventions and the structure entropy of the learned Bayesian networks. In the latter, the maximum num-tional data is 200. This is because we observed that the structure entropy of the learned Bayesian network would not reach certain small values when the manipulated lected. Experiments showed that, the results from the different tested Bayesian net-the results will be demonstrated with the Cancer network and the size of the interven-tional data as 200. More detailed results are available in Li [9]. 
In the experiments, when one variable is selected for intervention, the links point-fixed value. The values of other variables are sampled based on the Bayesian network structure and the original conditional probabilities. In addition, one variable can be selected for more than one round of inte rvention in the active learning process. 
We used the original conditional probabilities in the Bayesian networks first. To test whether the conclusions depend on specific values of the conditional probabilities in the original Bayesian networks, we also conducted experiments with the same Bayesian network structures but with randomized conditional probabilities. The con-clusions from the experiments with the randomized conditional probabilities are simi-lar to the results with the original conditional probabilities, and are consistent over all the Bayesian networks tested. 3.1 Number of Interventi ons vs. Structure Entropy In the first experiment, we tested the relationship between the number of interventions and the entropy of the learned structures. The objective is to show how the entropy of the learned structures varies with the different node selection methods, when the number of the interventions is the same. The total instances to be collected are set to 2000 for Cancer network, 1000 for Study network and Cold network and 5000 for Asia network and Car network. The maximum number of interventions is set to 6 because the structure entropy of the learned Bayesian networks with more than 6 interventions was observed to be very low. When the size of the interventional data in each active learning step is different, the maximum number of possible interventions would change. For the Cancer network, when the number of the total instances from all the active learning steps approaches 1000 to 1200, the entropy of the learned struc-ture with non-symmetrical entropy would converge. 
The programs ran 8 hours and finished 608 repeated experiments 3 on the Cancer network (about 48 seconds for one experiment), and the results are shown in Figure 2. number of interventions. Figure 2 shows that, with the same number of interventions, node selection with non-symmetrical entropy can derive a Bayesian network with the lowest entropy (also with the smallest variance on average), which means the struc-ture of the learned Bayesian network is more certain. 
The structure learned with observational data has the highest entropy. The entropy of Bayesian network structure learned with the random node selection and node selec-tion with the symmetrical entropy fall between those of the node selection with non-symmetrical entropy and the observational data. This is consistent with our ex-pectation, since the intervention is non-sy mmetrical in nature and the interventional data can provide more causal information about the probabilities between the manipu-lated variable and other variables. If there is a real edge from the manipulated variable to another variable, the probability of this edge should increase with the interventional data, and the non-symmetrical entropy will decrease. However, the symmetrical en-tropy may not decrease since we do not know the probability change in other two conditions between these two variables. 
The significance of the entropy difference from different node selection measures was evaluated by one-sided t-test. The p-values between the entropy of the learned Bayesian network structure from non-symmetrical entropy and other methods are all smaller than 10 -10 . This means that the entropy from non-symmetrical entropy is sig-nificantly smaller than that from other methods. 
From Figure 2, we have a surprising observation. When the number of interven-tions is smaller than 6 in the Cancer network, the entropy of the learned structure with nodes selected from the symmetrical entrop y is lower than that from random node selection. When the number of interventions is equal to or greater than 6, the entropy of the learned structure by node selection with symmetrical entropy is higher than that from random node selection. It means that, in the first several interventions, symmet-rical entropy selects the nodes to reduce the structure uncertainty significantly. How-ever, when the number of interventions is greater than 6, the leaf nodes (nodes X 4 and X in Figure 1) are always selected by symm etrical entropy. The data with leaf nodes manipulated can reduce the estimated probabilities of the edges from the leaf nodes to other nodes. But, the data cannot provide information about the influence relation-ships from other nodes to the leaf nodes. The uncertainty of the leaf nodes calculated from symmetrical entropy can still be quite large. However, the random method may select other nodes for intervention, which could generate subsequent interventional data with more causal information about the edges from other nodes to leaf nodes and leaf nodes to other nodes. Such information will reduce the total structure entropy. 
Figure 2 also shows that, with more interventions (or more data), the entropy of the learned structure decreases with all the node selection criteria. The entropy of the interventions. Then, the entropy of the lear ned structure seems to converge to certain values. These results are similar across all the Bayesian networks tested. 3.2 Number of Interventions vs. Distance to the Ground Truth In this experiment, we compared the learned structure with the ground truth Bayesian networks. The difference between the learned structure and the ground truth is meas-ured with Hamming distance. Figure 3 shows that node selection with non-symmetrical entropy leads to Bayesian networks with the smallest average Hamming distance to the ground truth, as compared with other methods for node selection. With 6 or more interventions, the average distance is 0 and the variance is 0 with the Can-cer network when the nodes are selected based on non-symmetrical entropy. The variance of the Hamming distance from non-symmetrical entropy is the lowest, while the variances of the Hamming distances from the symmetrical entropy and observa-tional data are quite high (about 0.55 and 0.33 respectively). In addition, Figure 3 shows the changes of the average Hamming distance with the number of interven-tions. With more interventional data, the average distance from the learned structure to the ground truth will be smaller. 
From Figures 2 and 3, we can observe that, when the number of the interventions increases, the structure entropy converges to a certain low value with either node selection with non-symmetrical entropy or random node selection. The reason is that the true causal Bayesian network structure can be identified with sufficient interven-tional data from any node selection method. We note that, however, when the number of interventions is small, non-symmetrical entropy could outperform all other meth-ods for node selection in active learning. The difference in performance could be significant in applications where the resources are scarce or only a small number of interventions are feasible. 3.3 Structure Entropy vs. Distance of the Learned Structure to the Ground In practice, we do not know the ground truth structure, and cannot use the Hamming distance from the learned structure to the ground truth structure as the stop criteria to learn causal Bayesian networks. This experiment will examine the relationship be-tween the structure entropy and the Hamming distance from the learned structure to the ground truth Bayesian network structure. Figure 4 shows how the entropy of the learned structure approximates the average Hamming distance from the learned struc-ture to the ground truth. The relationship between the average entropy of the learned structure and the average distance from the learned structure to the ground truth is approximately linear, which means that the entropy of the learned structure is a good approximation of the distance of the learned structure to the ground truth Bayesian network and can be used as a stop criterion for the structure learning. 3.4 Structure Entropy as Stop Criterion In the next experiment, we tested the effect of the structure entropy as the stop crite-rion. Figure 5 shows that, with non-symmetri cal entropy as the node selection crite-rion, the program can reach the required structure entropy with a smaller number of interventions. When the interventional node is selected with symmetrical entropy, a large number of interventions are needed. The results from observational data do not show in Figure 5, as the program cannot reach the required structure entropy in the maximum steps allowed (50 steps) in that set of experiments. 3.5 Comparison with Expected-Posterior-Loss-Based Method For comparison, we have implemented the method based on the expected posterior loss [14]. In our implementation, we sample the orderings of variables from the cur-rent data and estimate the probabilities of the possible observations. The edge prob-abilities are estimated with both the exact method by Koivisto [7] and the Markov Chain Monte Carlo (MCMC) method. Experiments show that the MCMC methods take more time to converge to the probabilities estimated with the exact method and will not be discussed. 
We tested our method with the benchmark Study network and the Cold network. In the experiment, the number of instances collected from each intervention is set to 1 when the selected node is manipulated to a distinct value. Due to the computational complexity, the multiple instances from each intervention and other Bayesian net-works with more variables are not te sted with expected posterior loss. 
Fig. 6 shows the results from the Study network. Fig. 6 (a) shows that all the meth-ods with interventional data can reach the required structure entropy with smaller than 50 interventional instances, while the observational data alone cannot reach the re-quires structure entropy with the maximal instances allowed. In this example, node selection with the expected posterior loss re quires the minimal number of instances to reach the structure entropy on average. The next best-performing method is node selection with non-symmetrical entropy. N ode selection with symmetrical entropy and random node selection requires a larger number of instances to reach the required structure entropy. Fig. 6 (b) shows the average running time the different methods spent. We can see that the expected posterior loss requires much more time than other methods for node selection. The time for observational data converges when the maximal number of instances is reached. In summary, active learning based on ex-pected posterior loss can reach the minimal structure entropy with the same number of interventions on average, while the computational cost makes it infeasible for domains with more variables or multiple instances collected in each active learning step. The learned structure with non-symmetrical entropy is similar to that with ex-pected posterior loss, but with much reduced computational time. In this work, we investigate active learning of Bayesian network structure when the symmetrical entropy from the current data to select nodes for intervention. Experi-ments show that non-symmetrical entropy can reach the required structure entropy with smaller number of interventions than symmetrical entropy and random node selection for intervention, and much better than merely estimating the structure with observational data in all the Bayesian networks tested. A possible reason for the better performance of the non-symmetrical entropy is that interventions are non-symmetrical in nature. When compared with expected posterior loss, our method can reach the similar structure entropy with much lower computational complexity. 
Experimental results also show that the learned structure entropy has an approxi-mately linear relationship with the average Hamming distance from the learned struc-ture to the ground truth Bayesian network. This implies that structure entropy is an effective measure for the goodness of the learned causal Bayesian network structure, and can be used as an effective stop criterion. 
We have tested significance of the difference of the learned structure entropy from node selection based on the non-symmetrical entropy and other methods. The statisti-cal test shows that the structure entropy fr om node selection with the non-symmetrical entropy is significantly smaller than that from other methods. 
A surprising observation in the experiments is that the random node selection for intervention can outperform the node selection with symmetrical entropy when the number of interventions is large. When the number of interventions is large, the sym-metrical entropy will often select leaf nodes for intervention, which cannot provide sufficient information to reduce the uncertainty of the edge probabilities. 
Our method is not designed to replace other related work, and does not apply to domains where repeated interventions are no t feasible, such as economics or social science. We have based our investigations on a set of different, complementary, or have also inspired some technical and presentation ideas reported in this paper. There are some general directions to extend our work, such as considering missing values or hidden variables in the causal Bayesian networks. In future, we will try to extend our results to more situations and apply the method to some real-life applications in dif-ferent domains. We would like to thank Daniel Eaton and Kevin Murphy for sharing their BDAGL code publicly. This work is partially supported by an Academic Research Grant No. R-252-000-309-112 from the National University of Singapore. 
