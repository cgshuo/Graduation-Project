 With the rapidly increasing popularity of Social Media sites, a lot of user generated content has been injected in the Web, thus resulting in a large amount of both multimedia items (music  X  Last.fm , MySpace.com , pictures  X  Flickr , Picasa , videos  X  YouTube ) and textual data (tags and other text-based documents). As a consequence, especially for multi-media content it has become more and more difficult to find exactly the objects that best match the users X  information needs. The methods we propose in this paper try to allevi-ate this problem and we focus on the domain of pictures, in particular on a subset of Flickr data. Many of the photos posted by users on Flickr have been shot during events and our methods aim to allow browsing and organization of pic-ture collections in a natural way, by events. The algorithms we introduce in this paper exploit the social information pro-duced by users in form of tags, titles and photo descriptions, for classifying pictures into different event categories. The extensive automated experiments demonstrate that our ap-proach is very effective and opens new possibilities for mul-timedia retrieval, in particular image search. Moreover, the direct comparison with previous event detection algorithms confirm once more the quality of our methods.
 H.3.2 [ Information Storage and Retrieval ]: Metadata; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Human Factors, Measurement, Reliability Event Detection, Event Classification, Collaborative Tag-ging, Machine Learning, Metadata Enrichment
Over the past years we have faced a rapid shift from ana-log to digital photography, due both to the increase in per-formance and to the drop of prices for digital photo cam-eras. With the advent of Social Media sites dedicated to photography ( Flickr , Picasa , etc.), a lot of these users X  dig-ital personal photos become available to the public at large. However, users often do not invest much effort in organiz-ing their own pictures and prefer instead to create quite broad sets including hundreds of pictures. Because of this, a huge amount of digital pictures remains untouched unless powerful techniques for image indexing and retrieval become available. Image retrieval is particularly difficult, given the fact that Flickr data is noisy and, besides, it is not easy to capture the content of photos.

The key idea of this paper is to use events as the primary means to organize media and in a more concrete scenario, pictures. Our lives are a constellation of events, which one after another, pace our everyday activities and build up our memories. Many of the Flickr pictures have been shot dur-ing specific events, therefore enabling users to organize or browse this type of media by events is very natural and can potentially facilitate retrieval.

In our paper we define an event like in [1], as  X  X  specific thing happening at a specific time and place X . Moreover, we consider events having both a local and a global dimension. Events such as birthdays, a marriage, a summer vacation or a car accident are the lens through which we see and memorize our own personal experiences and are therefore events of local type. In turn, global events, such as world sport championships or global natural disasters (e.g. 2010 Haiti earthquake, 2004 Thailand tsunami, climate change, world recession, etc.) or, on a smaller scale, a local festival or a soccer match, build collective experiences. These types of events allow users share personal experiences as a part of a more social phenomenon  X   X  X ollective events X .

Our approach for classifying pictures into events relies en-tirely on user generated information, which we gather from the Flickr Web site. We experiment both with simple types of features, such as tags, time information, photo titles and descriptions, as well as with different combinations among them. Based on this information we construct classifiers, which automatically assign the pictures to their associated event categories. The method we introduce is very intu-itive and the extensive automatic evaluations we perform demonstrate the high accuracy of our algorithms. Compared to similar work (e.g. [5]) trying to solve the same problem, our approach is much simpler and achieves better results. Moreover, the applicability of the methods we introduce in this paper is not restricted to pictures in Flickr , but can be employed for any types of pictures having tags associated with them, as well as to other types of multimedia data, e.g. videos, music, etc. Additionally, the methods can be applied not only for event-based browsing or organization, but also for enabling users to discover other users interested in the same types of events and thus easing social connectivity.
The contributions of this paper are manifold: The rest of the paper is organized as follows: we start in Section 2 with a review of the relevant literature; next, in Section 3 we present the data set on which our experiments have been performed, focusing in particular on the collection of events and Flickr images. In Section 4 we introduce our algorithm, together with details on the data set clustering (Section 4.1) and preprocessing (Section 4.3) steps. The evaluation of our methods and the experimental results are described in Section 5. Finally, in Section 6 we conclude and discuss possible extensions of this work.
The topic of event detection is not new; first papers ad-dressing this domain appeared already in 1998, as part of the Topic Detection and Tracking (TDT) initiative [2]. In [23] the authors introduce two different types of event detection methods: retrospective and online detection. The former refers to discovery of previously unidentified events inside a collection, while the latter strives to identify in real time new events from live news feeds. The experiments show that hi-erarchical clustering methods are highly informative for ret-rospective detection of previously unidentified events, while temporal distribution patterns of document clusters provide useful information for improvement in both retrospective de-tection and online detection of novel events. With the algo-rithms we propose in the present paper we also target the detection of retrospective events.

Arguing that most of the existing research focusing on ret-rospective news event detection (RED) make use of only the contents of the news articles, the authors of [17] propose to do explorations on both content and time information and introduce a probabilistic model to incorporate these both sources of information in a unified framework. Similarly, the authors of [12] also utilize both time and content in-formation. However, in contrast to TDT, which attempts to cluster documents as events using clustering techniques, in [12] the focus is on detecting a set of bursty features for a bursty event. The main technique employed in the paper is a free probabilistic approach which fully utilizes the time information to determine a set of bursty features which may occur in different time windows. Both [17, 12] differ from our approach, as we do not perform our event-analysis with respect to time windows. Instead we make use of time in-formation in order to help us decide whether items belong to a specific event class or not.

Similar to [12], in [15] the authors aim to identify feature bursts and their associated bursty periods, by introducing a simple but effective mixture density-based approach. Word trajectories are analyzed in both time and frequency do-mains, with the specific goal of identifying important and less-reported, periodic and aperiodic events. A set of words with identical trends can be then grouped together to recon-struct an event in a completely unsupervised manner. [11] is also aiming to produce a characterization of the most inter-esting tags associated with a sliding interval of time. Here however, the focus is not on textual documents, but rather on user generated tags attached to Flickr photos. The most important part of the article addresses the problem of visu-alizing the evolution of tags within the Flickr online image sharing community. The authors make a short observation regarding the identified categories of interesting tags, i.e. events, personalities and social media, but no statistics on this are reported.

A different approach for detecting events is presented in [9], where the authors propose to use Web click-through data for this purpose. The click-through data is first transformed to a 2D polar space by considering the semantic and temporal dimensions of the queries. Further, robust subspace esti-mation techniques are applied in order to detect subspaces consisting of only queries with similar semantics and the un-interesting subspaces containing queries not related to real events are pruned. Finally, events are detected from interest-ing subspaces using a non-parametric clustering technique.
Most of the existing work on event detection focused on identifying events from news corpus collections, and only re-cently new methods targeting other types of data have been proposed. For example, in [10] the authors propose an ap-proach for detecting Flickr photos depicting events. Given a set of Flickr photos with both user tags and other meta-data including time and location (latitude and longitude), the algorithm aims to discover a set of photo groups, where each group corresponds to an event. The method consists of three steps: (1) based on temporal and spatial distribu-tions, tags are identified as related to events or not; (2) after detecting event-related tags, they are further classified into periodic-or aperiodic-event tags; (3) finally, for each tag cluster representing an event, the set of photos correspond-ing to the event are retrieved. This approach differs from ours in that it relies on geographical information  X  still in-existent for many pictures. Our method uses solely social information and has thus a broader applicability.

Another dimension of investigations refers to the work pre-sented in [19]. Focusing also on the domain of pictures, the paper tries to extract event and place semantics from tags assigned to photos in Flickr . The proposed approach relies on bursts analysis: tags referring to event names are ex-pected to exhibit high usage patterns over short time peri-ods (maybe also periodical), while location-related tags show these kinds of patterns in the spatial dimension. However like [10], [19] also relies on GPS information and has thus a more restricted applicability than our approach.

In [20], the authors also identify event-related tags by us-ing WordNet. However, event detection is not the main focus of the paper, but rather on providing tag recommendations.
The approach presented in [4] is targeting a broader range of data types, namely it tries to identify events and their as-sociated user-contributed social media documents. It thus not only focuses on pictures, but also on music, videos, news and Facebook data. The validation of the accuracy of the introduced algorithms is performed on Flickr data, having tags corresponding to entries in the Yahoo! X  X  Upcom-ing event database 1 . An extended version of [4] is presented in [5]. [5] discusses in detail the different distinctive rep-resentations of social media documents for analyzing their similarity and for identifying which documents correspond to the same events. The authors define similarity methods for each document representation and also explore various techniques for combining them into a single measure of so-cial similarity. Both ensemble and classification-based sim-ilarity learning techniques are described and these are used in conjunction with an incremental clustering algorithm to generate a clustering solution where each cluster corresponds to an event and includes the social media documents associ-ated with the event. Compared to [4], in [5] the experiments are much more extensive and classification-based similarity learning methods are discussed in addition to the previously introduced ensemble-based techniques. We also compare our results with the measures reported in [5]. Nevertheless, our methods are purely classification-based, whereas in [5] also clustering techniques are employed. Moreover, in our case the event classes are known beforehand, while in [5] they are determined based on clustering techniques. [7, 8] are similar to the approach we introduce in the present paper, but in this case the focus is on music resources and the authors aim to provide tag recommendations in terms of music themes, moods, genres or styles. Our focus instead is on photo clas-sification and not on tag recommendations.
For the purpose of our experiments we collected an exten-sive set of Flickr pictures. Identifying a good ground truth set for events (in particular for pictures) turned out to be quite difficult. The main reason for this is the lack of a veri-fied and largely accepted event taxonomy (ontology). One of the most known events categorization is the Yahoo! Upcom-ing events catalog, however it X  X  thirteen categories are not extensive enough. Using WordNet 2 was also not an option in our case, as Flickr tags can be written in many different ways (intentionally or unintentionally) by the users and are thus not matchable with the WordNet database. Wikipedia 3 has also its X  own event taxonomy 4 , but here the categorization is not easily understandable. Besides, several possibilities are listed regarding how to organize events (e.g. by location, topic, year, etc.), and these different categorizations intro-duce ambiguities, as they are not mutually exclusive and do not cover all possible facets of a good event classification.
To cope with this problem, we decided to make use of http://www.upcoming.yahoo.com http://wordnet.princeton.edu/ http://en.wikipedia.org/wiki/Main_Page http://en.wikipedia.org/wiki/Category:Events the YAGO ontology, which brings together WordNet and Wikipedia knowledge. In the following we present its X  most important features and characteristics.
YAGO[22] 5 is a large and extensible ontology that builds on entities and relations from Wikipedia. Facts in YAGO have been automatically extracted from Wikipedia and uni-fied with semantics from WordNet, achieving an accuracy of around 95%. All objects (e.g., cities, people, even URLs) are represented as entities in the YAGO model. The hierarchy of classes starts with the Wikipedia categories containing a page and relies on WordNet X  X  well-defined taxonomy of homonyms to establish further subClassOf relations. We make use of these subClassOf relations in YAGO, which provide us with semantic concepts describing Wikipedia en-tities. We also rely on the type relation, deducting what higher level concept a page is about.
For collecting event names, we made use of the YAGO on-tology and selected only those entities having a type (YAGO relation) wordnet event . With this method we retrieved a list of 138,794 Wikipedia event page titles, like  X  X eichstag fire X ,  X  X attle of the Nile X ,  X  X eBIT X , or  X  X ranian presiden-tial election, 2009 X . We will refer to this list of events as [ events ] later in our experiments. Starting from [ events ] we retrieved the Wikipedia categories assigned to the Wikipedia pages, i.e. to the events, using the subClassOf relation in YAGO. With this method we retrieved a total of 25,223 dis-tinct Wikipedia categories assigned to [ events ]  X  later called [ categories ]. Furthermore, we also retrieved the WordNet concepts of [ categories ] using again subClassOf relations starting from [ categories ]. This set of 1,521 distinct Word-Net concepts will be referred as [ concepts ]. Thus we have an extensive list of [ events ], along with the corresponding [ categories ] and the super-[ concepts ], forming a three-level hierarchical taxonomy.
Having now an extensive set of event categories, the next step corresponded to gathering the actual ground truth data, consisting of Flickr images. For this purpose we made use of the Flickr API 6 . We started from the 138,794 Wikipedia event pages and crawled the corresponding Flickr groups. Being explicitly created by users and containing pictures contributed by a multitude of users, these Flickr groups quite accurately represent social groups interested in the specific events and represent thus a good ground truth. For gathering the Flickr groups we made use of the f lickr.groups.search 0 method and kept the first hit in the results list. With this method we could gather 29,796 event-related Flickr groups.

In the next step, for all retrieved groups, we collected their corresponding group pools, i.e. all pictures contributed by all users to the corresponding groups. In total the number of pictures gathered was 3,600,520 and among them 2,639,254 unique pictures.

Finally, for all collected photos, we needed the tag infor-mation, i.e. raw and normalized forms of the attached tags,
Available for download at http://www.mpi-inf.mpg.de/ yago-naga/yago/downloads.html http://www.flickr.com/services/api/ title, descriptions, as well as the Id and name of the user assigning the tag 7 . 22,985,996 tags have been gathered with this method (165,009 unique ones). For 2,837,182 pictures we could find non empty titles and only 1,662,075 had de-scriptions attached to them (1,623,667 had both title and description). We also wanted to investigate the influence of the location information for our event detection algorithm. However, we could find latitude and longitude coordinates for only cca. 25% of photos in our collection, so in the experiments we finally ommitted this type of information. On average, the event-related groups had 810.56 pictures in their corresponding group pools and 185.65 contributing users (taggers).
For comparing the performance of our event detection methods with previous work [5] trying to solve the same problem, we made use of the Upcoming data set, which was kindly provided by the authors. This data set consists of Flickr pictures that were manually tagged by users with an event id corresponding to an event from the Upcoming event database 8 . For this collection, the Upcoming tags represent the  X  X round truth X , both in the case of the algorithms pre-sented in [5] and in the case of our classification algorithms.
Each photo corresponds to a single event and each event is self-contained and independent of other events in the data set. In total, there are 9,616 unique events and 275,672 unique pictures, with an average of 28.67 photos per event. All pictures have been taken between January 1st, 2006 and August 9th, 2008.
For classifying images into the different categories of events we base our solution on collaboratively created social knowl-edge, i.e. tags associated with Flickr pictures. Based on already provided user tags, we build classifiers which try to assign the pictures to the corresponding event categories. More specifically, our approach relies on the hypothesis, that the existing tags provided by users for a particular photo carry information which can be used to infer the event cat-egory this picture belongs to. We perform a preprocessing step on the data collected from Flickr (described in Sec-tion 3.3) and we also experiment with different types of clus-tering methods on the original data sets. Below we describe the details of our algorithm, together with the preprocessing and photo clustering steps.
In the approach we propose, we experiment with different ways of organizing the pictures crawled from Flickr . Besides the original, unclustered data set, two additional types of hierarchical clustering are considered, based on Wikipedia and WordNet classes, respectively. Below we present the details:
Original data set (Unclustered). As described in Sec-tions 3.1, 3.2 and 3.3, we start collecting the pictures by con-sidering the YAGO entities having as type wordnet event and their corresponding Wikipedia event pages. The Flickr
We assume that the user tagging a photo belonging to a group pool corresponds in general to the author of the pic-ture.
Upcoming. http://www.upcoming.org groups we can identify as first hits in response to queries consisting of the Wikipedia event page X  X  name, correspond then to the list of [ events ]. For the example in Table 1, the list [ events ] consists of all entries in column The pictures collected for the corresponding Flickr groups identified for these events remain unclustered.

Clustering based on Wikipedia classes. The first method of clustering we applied relies on the Wikipedia classes. In this case, all pictures belonging to the Flickr groups having the same Wikipedia class are merged into one cluster. The list of [ categories ] thus contains all unique en-tries from the column 0 W iki class 0 , i.e. united nations day and auto shows (for the example in Table 1). The pic-tures corresponding to the group Ids in rows 1 and 2 will be merged and the resulting cluster corresponds to the united nations day  X  X ategory X . Similarly, pictures corre-sponding to the groups from rows 3 and 4 will be put to-gether.

Clustering based on WordNet classes. The sec-ond type of clustering we employed made use of the Word-Net classes. For this case, similar to clustering based on Wikipedia classes, all pictures belonging to the Flickr groups having the same WordNet class are merged into one cluster. Rows 1 and 2 from Table 1 will be merged and will corre-spond to the day WordNet class. Rows 3 and 4 will remain untouched, as their WordNet classes are different. For this particular example, the list of [ concepts ] will be composed of day , show and attraction .

The photo event detection algorithm is then run on all these three variations of our data set, i.e. clustered or un-clustered. Moreover, we also perform experiments on the Upcoming data set (see description in Section 3.4) and com-pare our results with the ones reported in [5]. However, for this last set of experiments, no clustering step is applied.
The core of our photo event detection algorithm is a prob-abilistic classifier trained on the Flickr ground truth using tags as input features. Separate classifiers correspond to the different types of event classes that we extracted from YAGO. For building the classifiers, we use the open source machine learning library Weka 9 . In the experiments pre-sented in this paper, we use the Na  X   X ve Bayes Multinomial implementation available in Weka. We also experimented with other classifiers (e.g. Support Vector Machines, De-cision Trees), which resulted in similar classification per-formances, but were much more computationally intensive. Moreover, we also experimented with feature selection based on automatic methods (e.g. Information Gain) but the re-sults showed that the full set is better suitable for learning, even though it contains some noise.

We have one classifier for each event category that we aim to learn to classify. The positive examples are represented by the pictures gathered from the event X  X  corresponding Flickr group / resulted cluster, while the negative ones are ran-domly selected from the pictures corresponding to the rest of the event classes. The number of positive and negative examples is almost equally balanced.

Algorithm 1 presents the main steps of our approach. This corresponds to the general approach, when solely tags are used as features of the classifiers. Additionally, we also ex-periment with classifiers relying on features consisting of the http://www.cs.waikato.ac.nz/~ml/weka words from the photos X  titles, descriptions, time information and combinations of them, i.e. textual  X  tags, title, descrip-tion, and textual+time  X  tags, title, description, time. For the case of features based on time information we substitute the Na  X   X ve Bayes with an SVM classifier, as this one is more suitable for the type of representation used for time. More-over, for the combination of textual and time features, we use a linear combination of Na  X   X ve Bayes classifiers for the pure textual features with the SVM classifier, which we use for time. The two types of classifiers are equally weighted. For the combination of textual features the vectors are con-structed similarly with the case of tag-based feature vectors.
Step 1 of the algorithm above aims at reducing the num-ber of event classes to be predicted for the photos. This step is optional (described in detail in Section 4.1), as we exper-iment with all classes of events extracted from YAGO, as well as with a subset resulted from applying one of the clus-tering methods on the original set. If two or more classes are clustered based on one the methods described in Sec-tion 4.1, the resulted class will contain all pictures which have been originally assigned to the composing classes. As we need a certain amount of input data in order to be able to consistently train the classifiers, we discard those classes containing less than 100 photos (step 2) and the details of this pruning step are described in Section 4.3.

After selecting separate sets of pictures for training and testing (steps 4 -6), we build the feature vectors correspond-ing to each picture in the training set (lines 7 -11). The vectors have as many elements as the total number of dis-tinct tags assigned to the images belonging to the event / category / concept classes. The elements of a vector will have values of either 1 or 0, depending on whether the tag has been assigned to the particular photo, or not. Once the feature vectors are constructed, they are fed into the classi-fier and used for training (step 12). A model is learnt and afterwards is applied to any new, unseen data. For any un-seen picture, if the output of the classifier is greater or equal 0 . 5, the picture will be assigned to the current class.
Since for training the event classifiers we need enough data at our disposal, we need to perform a preprocessing step and remove the groups / group clusters not having sufficient photo instances. The preprocessing actions X  flow looks as follows:
As we can see in lines 3 through 7, all tags appearing in the name of the Wikipedia event page, together with their com-binations and synonyms are removed from the pictures in the collection corresponding to the specific Flickr group (or resulted cluster, as described in Section 4.1). With this step we avoid the potential bias of the classifiers towards words which might indicate the appartenance of the photos to the corresponding classes of events / categories / concepts. This step is necessary due to the crawling methodology. Likewise, for features in the form of titles and descriptions, the match-ing words are discarded. In case of the Upcoming data set, just like the authors of [5], we did not apply any discarding procedure. Here the crawling method was based on the con-straint that pictures have as tag an Upcoming identifier, and in this case the ids corresponding to the Upcoming events do not reveal any information about the topic/scope of the events.

Tags which do not appear together with at least 10 pho-tos throughout the collection are discarded (line 8), as they might represent too obscure annotations, or even misspellings  X  and thus do not have any positive influence on the clas-sification, or might even introduce noise. Similarly, photos with less than 2 tags are removed from the collection, as well as photos having more than 75 tags, since they might contain spam-tags [16] (line 9). Finally, the classes of events / categories / concepts with less than 100 photos are also removed (line 10), since we need sufficient instances in order to be able to train the classifiers. The whole process is re-peated until no more pruning can be made. Except of lines 3 through 7, all the other steps of Algorithm 2 are applied also to the Upcoming data set.
As already described in Section 4, we experiment with four different data sets:
For these four data sets we experiment with classifiers relying on different types of features: tags, titles, descrip-tions, time information, as well as combinations of them, such as tags+titles+descriptions (later referred as  X  X extual X ) and tags+titles+descriptions+time (later referred as  X  X ex-tual+time X ). As already mentioned, for time the Na  X   X ve Bayes is substituted with an SVM classifier, and in the case of  X  X extual+time X -related set of experiments we use a linear combination of Na  X   X ve Bayes and SVM classifiers. The NB classifiers are constructed for the textual features (i.e. tags, titles and descriptions), whereas the SVMs correspond to time information. Equal weight is assigned to the two types of classifiers.

With this evaluation we focus on automatically measuring the quality of the photo event classification algorithm. For the first three sets of experiments, the ground truth data is represented by the information collected from the Flickr photo groups, or the resulted clustered sets. Being man-ually created by humans, the assignments of photos to the different classes of events can be considered correct and thus accepted as ground truth. Besides, through the collabora-tive participation of more users to the groups, i.e. by both joining the emerging networks and by contributing content in terms of pictures, comments, tags, etc., we can ensure that the spam-groups will be filtered out 10 . For the case of the Upcoming data set, the ground-truth consists of Flickr pictures tagged with event ids corresponding to events from the Upcoming database.

For the different types of experiments we present in Ta-ble 2 statistics regarding the number of classifiers built (col-umn  X # Classif. X ), average number of instances for each clas-sifier (column  X  X vg. Inst. X ) and the number of features, respectively ( X # Feat. X ).

The numbers in Table 2 are all computed after perform-ing the pruning step, as described in Algorithm 1 (see Sec-tion 4.2). As we can observe, many groups have been dis-carded because of not containing enough data.

For evaluating the performance of our algorithms, we in-spect the classification accuracy ( Acc ), precision ( P ) and re-call ( R ) measures, when performing 10-fold cross-validation on the data sets. Moreover, for allowing the comparison with the results reported in [5], we also inspect the Normalized Mutual Information (NMI) [18, 21] and B-Cubed [3] values.
NMI measures how much information is shared between the actual ground-truth events (each with an associated set of pictures) and the clustering / classification assignment. For a set of classes C = { c 1 , ..., c j } and events E = { e 1 , ..., e k } , where each c i and e i is a set of documents and n is the total number of documents
Like in the case of tagging, correct and suitable tags will get more and more employed, while obscure / misspelled tags will be pushed to the tail of the power law frequency distribution [13, 14] where
B-Cubed estimates the precision and recall associated with each document in the data set individually, and then uses the average precision P b and average recall R b values for the data set to compute B-Cubed as:
For each document, precision is defined as the proportion of items in the document X  X  cluster that correspond to the same event, and recall is defined as the proportion of docu-ments that correspond to the same event, which are also in the document X  X  cluster. Both NMI and B-Cubed can take values between 0 and 1.
Our experiments do not resume just to the simpler task of event detection for photos. What we also aim to investigate is the suitability of our method for correctly classifying pic-tures into event classes corresponding to  X  X ocal X  and  X  X lobal X  categories. Personal experiences, such as birthdays, mar-riages or vacations are just a few examples of local events. On the other hand, sport competitions, concerts or natu-ral disasters build collective experiences and are thus corre-sponding to the global-type of events. The most distinctive characteristic differentiating the local and global types of events is represented by the number of users contributing information to the corresponding event-data clusters. For local events, a very limited set of users contributes most of the event data, whereas in general for global events a nu-merous user participation is to be expected.

As a social network for sharing pictures, on Flickr we expect that most of the groups we collected correspond to global events. Sharing the personal experiences from a global type of event is the most common motivation for users to up-load their photos to the platform [6]. Moreover also within the Wikipedia pages, from which our crawling mechanism was initiated, the number of global events exceeds the num-ber of local or personal events. This fact directly influences our data collection.

We base our decision for separating the Flickr event groups into local or global events on the assumption that local event groups contain pictures contributed by at most 5 users, whereas global event groups have at least 50 contributors. These values have been selected based on the insights we got by manually assessing some randomly selected event classes. Below we present statistics corresponding to the number of distinct classes, average number of instances and number of features for the two local and global dimensions. All statis-tics are presented for the more general case when using just tag information for the event classification.
 This type of experiments is performed on the initial set of Flickr event groups only. In the case of Wikipedia categories and of WordNet concepts, the initial set of Flickr groups is Table 3: Statistics for Local / Global experimental setups first clustered, such that we lose the local and global aspects of the groups, as created by the platforms X  users.
The results of the evaluation runs are presented in Ta-ble 4. We observe that the average classification accuracy for all four data sets is very good when using tags as in-put features, almost in all cases being above 70%. Using this type of information as input features for the classifica-tion is thus very convenient, as tags can be easily collected along with the resources they are attached to. These re-sults confirm once more the quality of user provided tags  X  a result also observed in [6]  X  as well as the hypothesis on which our approach relies (see Section 4). Using solely ti-tle or description information for the classification translates into poorer classification accuracy and strongly depends on the characteristics of the undelying dataset. On the other hand, combining all types of textual information (i.e. tags, titles and descriptions  X   X  X extual X ) results in similar classifi-cation accuracy as in the case of simple tag-based classifiers. As the results indicate, time information is not sufficient for correctly distinguishing between the different types of events. However, the linear combination of Na  X   X ve Bayes and SVM classifiers, corresponding to textual and time features, respectively, boosts the performance above the one of tag based classifiers.

For the Upcoming dataset we obtain very good values for the NMI and B-Cubed scores. For comparison, the authors of [5] report in the case of the best performing methods val-ues of 0.94 and 0.82 for NMI and B-Cubed, respectively. For the other datasets (i.e. Event Groups , Wikipedia Cate-gories and WordNet Concepts ) these measures are not di-rectly comparable, and the classes are not mutually exclu-sive, as for the Upcoming dataset. Moreover, our approach is complementary to the methods described in [5]  X  in our case the classes of events are known beforehand, while in [5] they are not. One should thus decide on the type of algo-rithm to be used depending whether this type of information is known or not.

The experimental results for the picture classification into local and global events (Table 5) indicate also good classifi-cation accuracy. Here we present the results for the general case, when using solely tags as input features. The per-formance is better for local than for global events. In the case of local events the information on which we base the classification is more heterogeneous, since it is created by a smaller set of users. For global events, the performance of the classification is still very good, with recall values above 80%. Recall is more important for a setting where users X  pictures automatically get classified into the corresponding event clusters. This way the event classes will present a complete view of the different events and will be populated with enough items.

In Figure 1, for a better comparison among the four ex-perimental runs, we depict the averaged values of accuracy, precision and recall. Even if in the case of Event Groups , i.e. Local Ev. 89.32 0.84 0.99 0.99 0.99 Global Ev. 69.67 0.66 0.83 0.82 0.67 Table 5: Averaged classification results for local and global event groups using only tags as features the original, unclustered data set, where we have 4,289 event classes, the average accuracy is 72.46%. For Wikipedia Cat-egories the performance is best among the first three sets of results. In this case we cluster the initial Flickr groups X  pools of pictures. However, even if the resulted clusters gather also all pictures from the composing Flickr groups, the resulting sets are still homogeneous. This is due to the fact that the Wikipedia categories are more abstract than the event classes, yet the abstraction is not introducing any noise in the classification. The clustering in this case is even reducing some of the ambiguities of the Event Groups sets of photos, these initial sets being perhaps a bit too fine grained.
For W ordN et Concepts on the other hand although there are much less classes to distinguish among, the performance is a bit poorer than in the case of Wikipedia Categories . The reason for these results is the fact that the WordNet event categories represent more abstract event-related con-cepts. By clustering the initial set of Flickr social groups based on their common WordNet categories, the resulting sets of pictures corresponding to each of these WordNet event concepts are becoming too heterogeneous. Thus, it becomes more difficult for the classifiers to correctly distin-guish among classes.
 For the Upcoming dataset we achieve the best results. The reason is given by the fact that here the ground truth is manually created by users, who link their pictures to event ids from the Upcoming database. In this case the different classes of events are mutually exclusive, making thus easier for the classifiers to distinguish among them. The performance of the local event classifiers is comparable with the classification quality we achieve for the Upcoming dataset, these two datasets having more similar characteris-tics. Global event classification on the other hand is more similar w.r.t. performance with the event groups.

The results presented so far (Tables 4, 5 and Figure 1) in-dicate the performance of our algorithms in classifying pho-tos into the different classes of [ events ], [ categories ] and [ concepts ], i.e. macro evaluation results. However, we were also interested in micro -evaluating our algorithms. More specifically, we also analyzed the results per specific event / category / concept class to find out which classes offer the best performances and which classes are more difficult to learn. Table 6 shows the Acc , P and R values, together with the available number of photo instances for training the classifiers. We selected some of the best and worst per-forming classifiers and the results are grouped based on the three different experimental runs.

The differences show that while some classes are rela-tively easy to learn, others may require special attention or some level of disambiguation. Also, classes which are hard to learn are ambiguous and the annotations are mostly subjective. As we can see, a higher number of instances available for training, definitely improves the classification accuracy: five of the nine best performing classifiers had more than 500 instances at their disposal. Nevertheless, for very clear event categories, like wikicategory Auto shows or wordnet championship , the classifiers still can achieve very good performance, since in these cases the correspond-ing sets of Flickr photos and their associated tags are very homogeneous.

At the other end, we have the more ambiguous event classes, such as Bef ore Sunrise , wordnet motion , or wordnet execution , for which the classification is much more difficult. For these cases (e.g. wordnet television program ) even a high number of training instances does not improve much the classification accuracy. The main reason is that for these types of event classes, the underlying photo sets contain too many different types of pictures, depending on each user X  X  understanding of the corresponding event con-cept. This of course, translates into a very heterogeneous tag vocabulary, which negatively influences the classification performance.
With the rapid shift from analog to digital photography we have faced over the past years and with the advent of So-cial Media sites dedicated to photography, a huge amount of digital photos became available on the Web. However, advanced techniques for easily browsing and intuitively or-ganizing these photo collections are still missing. Image re-trieval is still in its infant stage, due to the fact that the available data is noisy and it is not easy to capture the con-tent of photos. On the other hand, collaborative tagging is a valuable source of semantically rich metadata, especially useful for repositories covering multimedia resources, whose non-textual content is not easily indexable / searchable. To tap this potential, in this paper we developed a new algo-rithm for automatically classifying pictures into classes of events. With the method we proposed, we enable event-based indexing and browsing of photo collections, i.e. a very intuitive way of organizing one X  X  memories.

The algorithms described in this paper rely on user an-notations in the form of tags, titles, descriptions, time in-formation and combinations of them and we experiment on two subset of pictures crawled from Flickr . The approach is however not restricted to this collection, but being appli-cable to any other photo set or other types of multimedia content (e.g. videos, music, etc.) containing similar meta-data.

For one set of experiments we rely on the YAGO event on-tology, and as ground truth we made use of the Flickr group photo organization, taking thus the users X  judgments regard-ing the pictures X  assignment to classes of events as golden standard. We experimented with different levels of abstrac-tion of the YAGO event ontology and implicitly clustering of the original picture collection and observed that while some classes are relatively easy to learn, others require more at-tention or some level of disambiguation. However, which is the right level of abstraction for events, that is still under-standable and accepted by users is an interesting question for further investigations. Another set of experiments was carried out on a Flickr subset where the ground truth is manually created by users, who link their pictures to event ids from the Yahoo! Upcoming database. Overall, the re-sults of our evaluations show that photo event-based clas-sification is feasible and confirm once more the quality of the user provided tags. Basing the classification decision on this type of data is not only much simpler than approaches combining several other sources of information (e.g. titles, descriptions, time, etc.), but also achieves at least compa-rable performance. As the results indicate, our approach is also suitable for correctly classifying both local and global types of events. Users X  picture collections can thus be cor-rectly automatically categorized into classes of events of ei-ther type. Moreover, these findings open new possibilities for multimedia retrieval, in particular image search.
For the future we plan to improve this algorithm, as well as the feature selection mechanism by automatic identifica-tion of tag types (e.g. Topic, Author, Usage context, etc.). Learning the coefficients for the linear combinations of the Na  X   X ve Bayes and SVM classifiers, or training different types of classifiers for all different kinds of features and combining their results are just a few other steps worth investigating. Additionally we also plan more work on the types of events that are mostly employed by users when referring to their event-related memories, as well as experiments considering also the location information. Moreover, merging our ap-proach with content-based methods trying to solve the same task is worth examining. We are greatly thankful to Hila Becker, Mor Naaman and Luis Gravano (the authors of [4] and [5]) for providing us with the Upcoming data set. This work was partially sup-ported by the GLOCAL project funded by the European Commission under the 7 th Framework Programme (Con-tract No. 248984). [1] Topic detection and tracking evaluation. [2] J. Allan, R. Papka, and V. Lavrenko. On-line new [3] E. Amigo, J. Gonzalo, J. Artiles, and F. Verdejo. A [4] H. Becker, M. Naaman, and L. Gravano. Event [5] H. Becker, M. Naaman, and L. Gravano. Learning [6] K. Bischoff, C. S. Firan, W. Nejdl, and R. Paiu. Can [7] K. Bischoff, C. S. Firan, W. Nejdl, and R. Paiu. How [8] K. Bischoff, C. S. Firan, and R. Paiu. Deriving music [9] L. Chen, Y. Hu, and W. Nejdl. Deck: Detecting events [10] L. Chen and A. Roy. Event detection from flickr data [11] M. Dubinko, R. Kumar, J. Magnani, J. Novak, [12] G. P. C. Fung, J. X. Yu, P. S. Yu, and H. Lu. [13] S. A. Golder and B. A. Huberman. Usage patterns of [14] H. Halpin, V. Robu, and H. Shepherd. The complex [15] Q. He, K. Chang, and E.-P. Lim. Analyzing feature [16] G. Koutrika, F. A. Effendi, Z. Gy  X  ongyi, P. Heymann, [17] Z. Li, B. Wang, M. Li, and W.-Y. Ma. A probabilistic [18] C. D. Manning, P. Raghavan, and H. Schtze.
 [19] T. Rattenbury, N. Good, and M. Naaman. Towards [20] B. Sigurbj  X  ornsson and R. van Zwol. Flickr tag [21] A. Strehl and J. Ghosh. Cluster ensembles  X  a [22] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a [23] Y. Yang, T. Pierce, and J. Carbonell. A study of
