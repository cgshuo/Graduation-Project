 Yasemin Altun altun@cs.br own.edu Thomas Hofmann th@cs.br own.edu Alexander J. Smola Alex.Smola@anu.edu.a u Multiclass classi cation refers to the problem of as-signing class lab els to instances where lab els belong to some nite set of elemen ts. Often, however, the instances to be lab eled do not occur in isolation, but rather in observ ation sequences. One is then interested in predicting the join t lab el con guration, i.e. the se-quence of lab els corresp onding to a sequence of ob-serv ations, using mo dels that tak e possible interde-pendencies between lab el variables into accoun t. This scenario subsumes problems of sequence segmen tation and annotation, whic h are ubiquitous in areas suc h as natural language pro cessing, speech recognition, and computational biology .
 The most common approac h to sequence lab eling is based on Hidden Mark ov Mo dels (HMMs), whic h de-ne a generativ e probabilistic mo del for lab eled obser-vation sequences. In recen t years, the state-of-the-art metho d for sequence learning is Conditional Random Fields (CRFs) introduced by La ert y et al. (La ert y et al., 2001). In most general terms, CRFs de ne a conditional mo del over lab el sequences given an ob-serv ation sequence in terms of an exp onen tial family; they are thus a natural generalization of logistic re-gression to the problem of lab el sequence prediction. Other related work on this sub ject includes Maxim um Entrop y Mark ov mo dels (McCallum et al., 2000) and the Mark ovian mo del of (Pun yakanok &amp; Roth, 2000). There have also been attempts to extend other dis-criminativ e metho ds suc h as AdaBo ost (Altun et al., 2003a), perceptron learning (Collins, 2002), and Sup-port Vector Mac hines (SVMs) (Altun et al., 2003b; Task ar et al., 2004) to the lab el sequence learning prob-lem. The latter have exp erimen tally compared favor-ably to other discriminativ e metho ds, including CRFs. Moreo ver, they have the conceptual adv antage of be-ing compatible with implicit data represen tations via kernel functions.
 In this pap er, we investigate the use of Gaussian Pro cess (GP) classi cation (Gibbs &amp; MacKa y, 2000; Williams &amp; Barb er, 1998) for lab el sequences. The main motiv ation for pursuing this direction is to com-bine the best of both worlds from CRFs and SVMs. More speci cally , we would like to preserv e the main strength of CRFs, whic h we see in its rigorous prob-abilistic seman tics. There are two imp ortan t adv an-tages of a probabilistic mo del. First, it is very intuitiv e to incorp orate prior kno wledge within a probabilistic framew ork. Second, in addition to predicting the best lab els, one can compute posterior lab el probabilities and thus deriv e con dence scores for predictions. This is a valuable prop erty in particular for applications requiring a cascaded architecture of classi ers. Con-dence scores can be propagated to subsequen t pro-cessing stages or used to abstain on certain predic-tions. The other design goal is the abilit y to use kernel functions in order to construct and learn in Repro duc-ing Kernel Hilb ert Spaces (RKHS), thereb y overcom-ing the limitations of ( nite-dimensional) parametric statistical mo dels.
 A second, indep enden t objectiv e of our work is to gain clari cation with resp ect to two asp ects on whic h CRFs and the SVM-based metho ds di er, the rst as-pect being the loss function (logistic loss vs. hinge loss), and the second asp ect being the mec hanism used for constructing the hypothesis space (parametric vs. RKHS).
 GPs are non-parametric tools to perform Bayesian in-ference, whic h { like SVMs { mak e use of the kernel trick to work in high (possibly in nite) dimensional spaces. Lik e other discriminativ e metho ds, GPs pre-dict single variables and do not tak e into accoun t any dep endency structure in case of multiple lab el predic-tions. Our goal is to generalize GPs to predict lab el sequences. While computationally demanding, recen t progress on sparse appro ximation metho ds for GPs, e.g. (Csat'o &amp; Opp er, 2002; Smola &amp; Bartlett, 2000; Seeger et al., 2003; Zhu &amp; Hastie, 2001), suggest that scalable GP lab el sequence learning may be an achiev-able goal. Exploiting the comp ositionalit y of the ker-nel function, we deriv e a gradien t-based optimization metho d for GP sequence classi cation. Moreo ver, we presen t a column generation algorithm that performs a sparse appro ximation of the solution.
 The rest of the pap er is organized as follo ws: In Sec-tion 2, we introduce Gaussian Pro cess classi cation. Then, we presen t our form ulation of Gaussian Pro-cess sequence classi cation (GPSC) in Section 3 and describ e the prop osed optimization algorithms in Sec-tion 4. Finally , we rep ort some exp erimen tal results using real-w orld data for named entity classi cation and pitc h accen t prediction in Section 5. In sup ervised classi cation, we are given a training set of n lab eled instances or observ ations ( x i ; y i ) with y f 1 ; : : : ; m g , dra wn i.i.d. from an unkno wn, but xed, join t probabilit y distribution p ( x ; y ). We denote the training observ ations and lab els by X = ( x 1 ; : : : ; x and y = ( y 1 ; : : : ; y n ), resp ectiv ely. GP classi cation constructs a two-stage mo del for the conditional probabilit y distribution p ( y j x ) by intro-ducing an intermediate, unobserv ed stochastic pro cess u ( u ( x ; y )) where u ( x ; y ) can be considered a com-patibility measure of an observ ation x and a lab el y . Giv en an instan tiation of the stochastic pro cess, we assume that the conditional probabilit y p ( y j x ; u ) only dep ends on the values of u at the input x via a multi-nomial resp onse mo del, i.e. p ( y j x ; u ) = p ( y j u ( x ; )) = It is furthermore assumed that the stochastic pro cess u is a zero mean Gaussian pro cess with covariance func-tion C , typically a kernel function. An additional as-sumption typically made in multiclass GP classi ca-tion is that the pro cesses u ( ; y ) and u ( ; y 0 ) are uncor-related for y 6 = y 0 (Williams &amp; Barb er, 1998). For notational con venience, we will iden tify u with the relev ant restriction of u to the training patterns X and represen t it as a n m matrix. For simplicit y we will (in sligh t abuse of notation) also think of u as a vec-tor with multi-index ( i; y ). Moreo ver we will denote C (( x i ; y ) ; ( x j ; y 0 )). Notice that under the above as-sumptions K has a blo ck diagonal structure with blo cks K ( y ) = ( K ij ( y )), K ij ( y ) C y ( x i ; x C y is a class-sp eci c covariance function.
 Follo wing a Bayesian approac h, the prediction of a la-bel for a new observ ation x is obtained by computing the posterior probabilit y distribution over lab els and selecting the lab el that has the highest probabilit y: Thus, one needs to integrate out all n m laten t variables of u . Since this is in general intractable, it is common to perform a saddle-p oint appro xima-tion of the integral around the optimal point esti-mate, whic h is the maxim um a posterior (MAP) es-timate: p ( y j X ; y ; x ) p ( y j u map ( x ; )) where u map = argmax dep endence assumptions, the posterior of u can { up to a multiplicativ e constan t { be written as Com bining the GP prior over u and the conditional mo del in (1) yields the more speci c expression log p ( u j X ; y ) = The Represen ter Theorem (Kimeldorf &amp; Wahba, 1971) guaran tees that the maximizer of (4) is of the form with suitably chosen coecien ts . In the blo ck diago-Using the represen tation in (5), we can rewrite the optimization problem as an objectiv e R parameter-ized by . Let e ( i;y ) be the ( i; y )-th unit vector, then of Eq. (4) can be written as follo ws: R ( j X ; y ) = T K A comparison between (6) and a similar multiclass SVM form ulation (Crammer &amp; Singer, 2001; Weston &amp; Watkins, 1999) clari es the connection between GP classi cation and SVMs. Their di erence lies primar-ily in the utilized loss functions: logistic loss vs. hinge loss. Because the hinge loss truncates values smaller than to 0, it enforces sparseness in terms of the parameters. This is not the case for logistic regression as well as other choices of loss functions. 2 For non-linear link functions like the one induced by Eq. (1), u map cannot be found analytically and one has to resort to appro ximate solutions. Various ap-pro ximation schemes have been studied to that ex-ten t: Laplace appro ximation (Williams &amp; Barb er, 1998; Williams &amp; Seeger, 2000; Zhu &amp; Hastie, 2001), variational metho ds (Jaakk ola &amp; Jordan, 1996), mean eld appro ximations (Opp er &amp; Win ther, 2000), and exp ectation propagation (Mink a, 2001; Seeger et al., 2003). Performing these metho ds usually involves the computation of the Hessian matrix as well as the in-version of K , a nm nm matrix, whic h is not tractable for large data sets (of size n ) and/or large lab el sets (of size m ). Sev eral techniques have been prop osed to appro ximate K suc h that the inversion of the appro x-imating matrix is tractable (cf. (Sc h X olkopf &amp; Smola, 2002) for references on suc h metho ds). One can also try to solv e (6) using greedy optimization metho ds as prop osed in (Bennett et al., 2002). 3.1. Sequence Lab eling and GPC In sequence classi cation, our goal is to learn a dis-criminan t function for sequences, i.e. a mapping from observ ation sequences X = ( x 1 ; x 2 ; : : : ; x t ; : : : ; x lab el sequences y = ( y 1 ; y 2 ; : : : ; y t ; : : : ; y ists a lab el y t 2 = f 1 ; : : : ; r g for every observ ation x t in the sequence. Thus, we have T multiclass classi -cation problems. Because of the sequence structure of the lab els ( i.e. every lab el y t dep ends on its neigh bor-ing lab els ), one needs to solv e these T classi cation problems join tly. Then, the problem can be consid-ered as a multiclass classi cation where for an obser-vation sequence of length l , the possible lab el set Y is of size m = r l . 3 We call lab el set of observ ations or micr o-lab el set , and Y the set of lab el sequences of observ ation sequences or macr o-lab el set . We assume that a training set of n lab eled sequences Z f ( X i ; y i ) j i = 1 ; : : : ; n g is available. Using the notation introduced in the con text of GP classi cation, we de ne p ( y i j u ( X i )) as in (1), treating every macro lab el as a separate lab el in GP multiclass classi cation and using the whole sequence X i as the input. 3.2. Kernels for Lab eled Sequences The fundamen tal design decision is then the engineer-ing of the kernel function k that determines the kernel matrix K . Notice that the use of a blo ck diagonal ker-nel matrix is not an option in the curren t setting, since it would prohibit generalizing across lab el sequences that di er in as little as a single micro-lab el. We de ne the kernel function for lab eled sequences with resp ect to the feature represen tation. Inspired by HMMs, we use two types of features: Features that capture the dep endency of the micro-lab els on the at-tributes of the observ ations ( x s ) and features that capture the inter-dep endency of micro-lab els. As in other discriminativ e metho ds, ( x s ) can include over-lapping attributes of x s as well as attributes of obser-vations x t where t 6 = s . Using stationarit y, the inner pro duct between the feature vectors of two observ ation sequences can be stated as: k = k 1 + k 2 , where k 1 (( X ; y ) ; ( X ; y )) X k 2 (( X ; y ) ; ( X ; y )) X k 1 couples observ ations in both sequences that are classi ed with the same micro-lab els at resp ectiv e po-sitions. k 2 simply coun ts the num ber of consecutiv e lab el pairs both lab el sequences have in common (ir-resp ectiv e of the inputs). One can generalize (7) in various ways, e.g. by using higher order terms between micro-lab els in both con tributions, without posing ma-jor conceptual challenges. k is a linear kernel function for lab eled sequences. This can be generalized to non-linear kernel functions for lab eled sequences by replacing h ( x s ) ; ( x t ) i with a standard kernel function de ned over input patterns. We can naiv ely follo w the same line of argumen tation as in the GPC case of Section 2, evoke the Represen ter Theorem and ultimately arriv e at the objectiv e in (6). Since we need it for subsequen t deriv ations, we will restate the objectiv e here Notice that in the third term, the sum ranges over the macro-lab el set, Y , whic h gro ws exp onen tially in the sequence length. Therefore, this view su ers from the large cardinalit y of Y . In order to re-establish tractabilit y of this form ulation, we use a tric k simi-lar to the one deplo yed in (Task ar et al., 2004) and reparametrize the objectiv e in terms of an equiv alen t lower dimensional set of parameters. The crucial ob-serv ation is that the de nition of k in (7) is homo-geneous (or stationary). Thus, the absolute positions of patterns and lab els in the sequence are irrelev ant. This observ ation can be exploited by re-arranging the sums inside the kernel function with the outer sums, i.e. the sums in the objectiv e function. 3.3. Exploiting Kernel Structure In order to carry out this reparameterization more for-mally we pro ceed in two steps. The rst step consists of nding an appropriate low-dimensional summary of . In particular, we are looking for a parameteriza-tion that does not scale with m = r l . The second step consists of re-writing the objectiv e function in terms of these new parameters.
 As we will pro ve subsequen tly, the follo wing linear map solving (8): where whether the input sequence is the j -th training se-quence and whether the lab el sequence y con tains micro-lab els and at position t and t + 1, resp ec-over lab el sequences y that con tain the -motif at position t .
 We de ne two reductions deriv ed from via further linear dimension reduction, ery position in the sequence y that con tains -motif. i;s; , on the other hand, is the sum of all ( i; y ) that has micro-lab el at position s in macro-lab el y . We can now sho w how to represen t the kernel matrix using the previously de ned matrices , P , Q and the Prop osition 1. With the de nitions from above: wher e H = diag( G ; : : : ; G ) .
 Proof. By elemen tary comparison of coecien ts. We now have r 2 parameters for every observ ation x s in the training data ( nlr 2 parameters) and we can rewrite the objectiv e function in terms of these variables: 3.4. GPSC and Other Lab el Sequence We now brie y point out the relationship between our approac h and the previous discriminativ e metho ds of sequence learning, in particular, CRFs, HM-SVMs and MMMs.
 CRF is a natural generalization of logistic regression to lab el sequence learning. The probabilit y distribution over lab el sequences given an observ ation sequence is given in Eq. (1), where u ( X ; y ) = h ; ( X ; y ) i is a linear discriminativ e function over some feature repre-sen tation parameterized with . The objectiv e func-tion of CRFs is the minimization of the negativ e condi-tional likeliho od of training data. To avoid over tting, it is common to multiply the conditional likeliho od by a Gaussian with zero mean and diagonal covariance matrix K , resulting in an additiv e term in log scale. From a Bayesian point of view, CRFs assume a uni-form prior p ( u ), if there is no regularization term. When regularized, CRFs de ne a Gaussian distribu-tion over a nite vector space . In GPSC, on the other hand, the prior is de ned as a Gaussian dis-tribution over the function space of possibly in nite dimension. Thus, GPSC generalizes CRFs by de n-ing a more sophisticated prior on the discriminativ e function u . This prior leads to the abilit y of using kernel function in order to construct and learn over Repro ducing Kernel Hilb ert Spaces. So, GPSC, a non-parametric Bayesian inference tool for sequence lab el-ing, can overcome the limitations of CRFs, parametric (linear) statistical mo dels. When the kernel that de-nes the covariance matrix K in GPSC is linear, u in both mo dels become equiv alen t.
 The di erence between SVM and GP approac hes to sequence learning is the utilized loss function over the training data, i.e. hinge loss vs. log loss. GPSC ob-jectiv e function parameterized with (Eq. (8)) corre-sponds to HM-SVMs where the num ber of parameters scale exp onen tially with the length of sequences. The objectiv e function parameterized with (Eq. (12) ) cor-resp onds to MMMs, where the num ber of parameters scale only linearly . 4.1. A Dense Algorithm Using optimization metho ds describ ed in Section 2 re-quires the computation of the Hessian matrix. In se-quence lab eling, this corresp onds to computing the ex-pections of micro-lab els within di eren t cliques, whic h is not tractable to compute exactly for large training sets. In order to minimize R with resp ect to , we pro-pose a 1 s t order exact optimization metho d, whic h we call Dense Gaussian Pro cess Sequence Classi cation (DGPS).
 It is well-kno wn that the deriv ativ es of the log partition function with resp ect to is simply the exp ectation of sucien t statistics: r where E Y denotes an exp ectation with resp ect to the conditional distribution of the lab el sequence y given the observ ation sequence X i . Then, the gradien ts of R is trivially given by: r R = 2 K 0 The remaining challenge is to come-up with an ecien t way to compute the exp ectations. First of all, let us more explicitly examine these quan tities: E In order to compute the above exp ectations one can once again exploit the structure of the kernel and is left with the problem of computing probabilities for every neigh boring micro-lab el pair ( ; ) at positions ( t; t + 1) for all training sequences X i . The latter can be accomplished by performing the forw ard-bac kward algorithm over the training data using the transition probabilit y matrix T and the observ ation probabilit y matrices O ( i ) , whic h are simply decomp ositions and reshapings of K 0 : where [ x ] m;n denotes the reshaping operation of a vec-tor x into an m n matrix, A I;J denotes the j I jj J j sub-matrix of A and ( : ) denotes the set of all possible indices.
 A single optimization step of DGPS is describ ed in Algorithm 1. The complexit y of one optimization step is O ( t 2 ) dominated by the forw ard-bac kward algorithm Algorithm 1 One optimization step of Dense Gaus-sian Pro cess Sequence Classi cation (DGPS) Require: Training data ( X i ; y i ) i =1: n ; Prop osed pa-1: Initialize (1) c ; (2) c (Eq. (11)). 2: Compute T wrt (2) c (Eq. (17a) , Eq. (17b)). 3: for i = 1 ; : : : ; n do 4: Compute O ( i ) wrt (1) c (Eq. (17c)). 5: Compute p ( y i j X i ; c ) and 6: end for 7: Compute r R (Eq. (15)). over all instances where t = nlr 2 . We prop ose to use a quasi-Newton metho d for the optimization pro cess. Then, the overall complexit y is given by O ( t 2 ) where &lt; t 2 . The memory requiremen t is given by the size of , O ( t ).
 During inference, one can nd the most likely lab el sequence for an observ ation sequence X by performing Viterbi deco ding using the transition and observ ation probabilit y matrices describ ed above. 4.2. A Sparse Algorithm While the above metho d is attractiv e for small data sets, the computation or the storage of K 0 poses a serious problem when the data set is large. Also, clas-si cation of a new observ ation involves evaluating the covariance function at nl data points, whic h is more than acceptable for man y applications. Hence, as in the case of standard Gaussian Pro cess Classi cation discussed in Section 2, one has to nd a metho d for sparse solutions in terms of the parameters to speed up the training and prediction stages.
 We prop ose a sparse greedy metho d, Sparse Gaussian Pro cess Sequence Classi cation (SGPS), that is simi-lar to the metho d presen ted by (Bennett et al., 2002). SGPS starts with an empt y matrix ^ K . At eac h iter-ation, SGPS selects a training instance X i and com-putes the gradien ts of the parameters asso ciated with X , ( i;: ) , to select the steep est descen t direction(s) of R over this subspace. Then ^ K is augmen ted with these columns and SGPS performs optimization of the curren t problem using a Quasi-Newton metho d. This pro cess is rep eated until the gradien ts vanish (i.e. they are smaller than a threshold value ) or a maxim um num ber of coordinates, p , are selected (i.e. some sparseness level is achiev ed). Since the bottlenec k of this metho d is the computation of the exp ectations, E
Y [[ Y t = ^ Y t +1 = ]] , we pick the steep est d direc-tions, once the exp ectations are computed.
 One has two options to compute the optimal at every iteration: by updating all of the parameters selected until now, or alternativ ely, by updating only the pa-rameters selected in the last iteration. We prefer the latter because of its less exp ensiv e iterations. This ap-proac h is in the spirit of a boosting algorithm or the cyclic coordinate optimization metho d.
 Algorithm 2 Sparse Gaussian Pro cess Sequence Clas-si cation (SGPS) algorithm.
 Require: Training data ( X i ; y i ) i =1: n ; Maxim um 1: K [] 2: for i = 1 ; : : : ; n do 5: ^ K [ ^ K ; K e s ] 6: Optimize R wrt s . 7: Return if r &lt; or p coordinates selected. 8: end for SGPS is describ ed in Algorithm 2. Its complexit y is O ( p 2 t ) where p is the maxim um num ber of coordinates allo wed. 5.1. Pitc h Accen t Prediction Pitc h Accen t Prediction is the task of iden tifying more prominen t words in a sen tence. The micro-lab el set is of size 2, accen ted and not-accen ted. We used phonet-ically hand-transcrib ed Switc hboard corpus consisting of 1824 sen tences (13K words) (Green berg et al., 1996). We extracted probabilistic, acoustic and textual infor-mation from the curren t, previous and next words for every position in the training data. We used 1 s t order Mark ov features to capture the dep endencies between neigh boring lab els.
 We compared the performance of CRFs and HM-SVMs with the GPSC dense and sparse metho ds ac-cording to their test accuracy in 5-fold cross valida-tion. CRFs were regularized and optimized using lim-ited memory BF GS, a limited memory Quasi-Newton optimization metho d. When performing exp erimen ts on DGPS, we used polynomial kernels with di eren t degrees (denoted with DGPS X in Figure 1a where X 2 f 1 ; 2 ; 3 g is the degree of the polynomial kernel). We used third order polynomial kernel in HM-SVMs (denoted with SVM3 in Figure 1). As exp ected, CRFs and DGPS1 performed very similar. When 2 n d or-der features were incorp orated implicitly using second degree polynomial kernel (DGPS2), the performance increased dramatically . Extracting 2 n d order features explicitly results in a 12 million dimensional feature space, where CRFs slow down dramatically . We ob-serv ed that 3 r d order features do not pro vide signi -can t impro vemen t over DGPS2. HM-SVM3 performs sligh tly worse than DGPS2.
 To investigate how the sparsit y of SGPS a ects its per-formance, we rep ort the test accuracy with resp ect to the sparseness of SGPS solution in Figure 1b. Sparse-ness is measured by the percen tage of the parameters selected by SGPS. The straigh t line is the performance of DGPS using second degree polynomial kernel. Us-ing 1% of the parameters, SGPS achiev es 75% accu-racy (1.48% less than the accuracy of DGPS). When 7.8% of the parameters are selected, the accuracy is 76.18% whic h is not signi can tly di eren t than the performance of DGPS (76.48%). We observ ed that these parameters were related to 6.2% of the obser-vations along with 1.13 lab el pairs on average. Thus, during inference one needs to evaluate the kernel func-tion only at 6% of the observ ations whic h reduces the inference time dramatically .
 In order to exp erimen tally verify how useful the pre-dictiv e probabilities are as con dence scores, we forced DGPS to abstain from predicting a lab el when the probabilit y of a micro-lab el is lower than a threshold value. In Figure 1c, we plot precision-recall values for di eren t thresholds. We observ ed that the error rate for DGPS decreased 8.54%, abstaining on 14 : 93% of the test data. The impro vemen t on the error rate sho ws the validit y of the probabilities generated by DGPS. 5.2. Named Entity Recognition Named Entity Recognition (NER), a subtask of In-formation Extraction, is nding phrases con taining names in a sen tences. The micro-lab el set consists of the beginning and con tinuation of person, location, organization and miscellaneous names and non-name. We used a Spanish newswire corpus, whic h was pro-vided for the Special Session of CoNLL 2002 on NER, to randomly select 1000 sen tences (21K words). We used the word and its spelling prop erties of the cur-ren t, previous and next observ ations.
 Error 4.58 4.39 4.48 4.92 4.56 The exp erimen tal setup was similar to pitc h accen t prediction task. We compared the performance of CRFs with and without the regularizer term (CRF-R, CRF) with the GPSC dense and sparse metho ds. Qualitativ ely, the beha vior of the di eren t optimiza-tion metho ds is comparable to the pitc h accen t predic-tion task. The results are summarized in Table 1. Sec-ond degree polynomial DGPS outp erformed the other metho ds. We set the sparseness parameter of SGPS to 25%, i.e. p = 0 : 25 nlr 2 , where r = 9 and nl = 21K on average. SGPS with 25% sparseness achiev es an accu-racy that is only 0 : 1% below DGPS. We observ ed that 19% of the observ ations are selected along with 1 : 32 lab el pairs on average, whic h means that one needs to compute only one fth of the gram matrix.
 We also tried a sparse algorithm that does not exploit the kernel structure and optimizes Equation 8 to ob-tain sparse solutions in terms of observ ation sequences X and lab el sequence y , as opp osed to SPGS, where the sparse solution is in terms of observ ations and lab el pairs. This metho d achiev ed 92 : 7% of accuracy , hence, was clearly outp erformed by all the other metho ds. We presen ted GPSC, a generalization of Gaussian Pro-cess classi cation to lab el sequence learning problem. This metho d com bines the adv antages of the rigor-ous probabilistic seman tics of CRFs and overcomes the curse of dimensionality problem using kernels in order to construct and learn over RKHS. The exp eri-men ts on named entity recognition sho w the comp et-itiv eness and the exp erimen ts on pitc h accen t predic-tion sho w the sup eriorit y of our approac h in terms of the achiev ed error rate. We also exp erimen tally veri-ed the usefulness of the probabilities obtained from GPSC.
 Ackno wledgments
