 Community-based Question Answering (CQA) sites, such as Yahoo! Answers, Baidu Knows, Naver, and Quora, have been rapidly growing in popularity. The resulting archives of posted answers to questions, in Yahoo! Answers alone, already exceed in size 1 billion, and are aggressively indexed by web search engines. In fact, a large number of search engine users benefit from these archives, by finding existing answers that address their own queries. This scenario poses new challenges and opportunities for both search engines and CQA sites. To this end, we formulate a new problem of predicting the satisfaction of web searchers with CQA an-swers. We analyze a large number of web searches that result in a visit to a popular CQA site, and identify unique charac-teristics of searcher satisfaction in this setting, namely, the effects of query clarity, query-to-question match, and answer quality. We then propose and evaluate several approaches to predicting searcher satisfaction that exploit these character-istics. To the best of our knowledge, this is the first attempt to predict and validate the usefulness of CQA archives for external searchers, rather than for the original askers. Our results suggest promising directions for improving and ex-ploiting community question answering services in pursuit of satisfying even more Web search queries.
 H.3.3 [ Information Search and Retrieval ]: Search pro-cess Algorithms, Experimentation Searcher satisfaction, community question answering, query clarity, query-question match, answer quality
Community-based Question Answering (CQA) sites, such as Yahoo! Answers, Baidu Knows, and Naver Ji-Sik-In, as well as more social-oriented newcomers such as Vark and Quora, have gained substantial popularity over the recent years, effectively filling a niche left by the mainstream Web search engines. People around the globe resort to commu-nity help for a variety of reasons, from lack of proficiency in Web search to seeking an answer  X  X ith a human touch X . Al-though some of these sites allow for monetary payments in exchange for answering questions ( e.g. , JustAnswer, or the now discontinued Google Answers), answerers are usually at-tracted by social reward and less tangible incentives, such as reputation or points, as demonstrated by Raban [23]. The CQA communities are mainly volunteer-driven, and their openness and accessibility appeal to millions of users; for example, the size of Yahoo! Answers surpassed 1 billion an-swers in 2010 1 , and Baidu Knows had over 100 million an-swered questions as of January 2011 2 .

To date, prior studies of community question answering have mainly considered first-order effects, namely, the satis-faction of the original question asker by the posted answers. However, we believe CQA has significant secondary bene-fits, whereas previously answered questions are likely to be useful for future askers of substantially similar or related questions. Indeed, today many additional users already ben-efit from the public accessibility of CQA archives via all the major web search engines. 3 Existing answers often satisfy information needs of users who submit queries to a Web search engine, obtain results from a CQA site, such as the ones shown in Figure 1, select one of these results, and fi-nally reach a resolved question page on the CQA site, as illustrated in Figure 2.

This scenario poses new challenges and opportunities for both search engines and CQA sites. For search engines, it provides a unique (semi-structured) source of human an-swers, which are particularly useful for satisfying tail queri-es [10]. For CQA sites, it creates substantial incoming traf-fic, which has its own multiple benefits from growing the http://yanswersblog.com/index.php/archives/2010/05/03/1-billion-answers-served/ http://zhidao.baidu.com/ , visited on January 19, 2011.
Unfortunately, specific numbers describing the amount of incoming traffic to CQA sites from search engines are not publicly available. F igure 1: A subset of Google search results including
F igure 2: A resolved question on Yahoo! Answers community to providing advertising revenue. More sophis-ticated symbiosis is also possible. For example, imagine if a search engine could detect that a user is struggling with a search session (e.g., by using techniques described in [7]). A search engine could then suggest posting a question on a CQA site, optionally suggesting relevant categories for such posting, and could even assist the user in transforming the query into an effective question. But in order to make this vision a reality, it is necessary to understand what it means for a searcher to be satisfied by an existing answer from a CQA archive, and to be able to predict this satisfaction.
This paper proposes and addresses this new problem of predicting the satisfaction of Web searchers with existing CQA answers. One way to approach this problem would be to define features of queries, questions, and answers (and possibly pairs thereof), and solve it within the machine learn-ing paradigm. We call this a direct approach. This can be done by constructing a labeled dataset of queries and answers, tagged by human judges based on how well they believe a query is satisfied by a given answer.

Since queries are often quite short, and questions, which we view as an intermediate link between queries and an-swers, are often not much longer, another way to approach the problem is through exogenous knowledge. To this end, we identify three key characteristics of searcher satisfaction, namely, query clarity, query-question match, and answer quality. We then collect separate human labels for each, and build regression models for predicting these characteristics. Learning from these task-specific labels explicitly makes use o f domain knowledge about the problem structure, which is not available in the above direct approach. We then use the output of these individual regressors as features in a subse-quent regression task, which aims to predict searcher satis-faction. We call this method a composite approach. This approach also allows us to better understand how much the performance in the main prediction task can be improved by improving each of the individual regressors (we model this by replacing the intermediate regression predictions with actual human labels). This additional interpretability of the model provides further insights into the problem.

We conduct our experimental evaluation using data from one of the leading CQA sites, namely, Yahoo! Answers. We collect human labels for each of the above tasks using Ama-zon Mechanical Turk. These include labeling searcher sat-isfaction with a given answer (our main prediction task), as well as intermediate labels for query clarity and query-question match. We gauge the answer quality by using a combination of signals shown to be highly correlated with it [1], namely, the answer length, and the asker and commu-nity ratings. The labeled data is publicly available through Yahoo X  X  Webscope program 4 .

Since we define a new task (predicting searcher satisfac-tion with CQA answers), it is difficult to identify a suitable baseline. The closest previously addressed problem, at least in spirit, is asker satisfaction -predicting whether the orig-inal question asker (on the CQA site) was satisfied with the answers [19]. Intuitively, searcher satisfaction should be related to asker satisfaction. However, as we show later, asker satisfaction and searcher satisfaction appear to be very weakly related, if at all. Hence, an entirely new method is needed to compute searcher satisfaction . It is essential to note that, in our scenario, Web search queries are differ-ent from the questions originally posted to CQA sites, and more importantly, these queries are issued by different users with different information needs. In fact, the users in these two scenarios are drastically different. Whereas users of the community site are willing to clarify their questions, provide additional details, and even provide feedback to the answer-ers, Web search users seek immediate satisfaction, and es-sentially treat existing answers as a static resource.
The main contributions of this paper are threefold. First, we formulate a new problem, namely, predicting searcher satisfaction with existing CQA answers. Second, we pro-pose two methods for solving this problem, a direct method and a composite method, which uses the outputs of sec-ondary regressors as features. Finally, we apply our meth-ods to a standard ranking task, where we treat answers as a semi-structured document collection. We show that in-corporating our predictor of searcher satisfaction leads to a significant improvement in ordering the answers for a given query. To the best of our knowledge, this is the first attempt to predict and validate the usefulness of CQA archives for external searchers, rather than for the original askers. Our results suggest promising directions for improving and ex-ploiting community question answering services in pursuit of satisfying even more Web search queries.
Our work spans the areas of filtering, recommending and ranking Community Question Answering (CQA) content, h ttp://webscope.sandbox.yahoo.com/catalog.php?datatype=l, Dataset L16 -Yahoo! Answers Query to Question. and estimating searcher satisfaction with the retrieved CQA content. In this section we first give an overview of the CQA site we chose for our experiments, namely Yahoo! An-swers, as it currently holds the largest repository of ques-tions. Then related research will be discussed.
With millions of active users, Yahoo! Answers hosts over a billion answers on a wide variety of topics. The system is question-centric: users are interacting by engaging in multi-ple activities around a specific question. The typical lifecycle of questions is the following: users post new questions and assign them to a predefined category, such as  X  Education &amp; Reference &gt; Homework Help  X  in the example shown in Fig-ure 2. The new question remains  X  X pen X  for four days with an option for extension, or can be closed if the asker chooses a best answer earlier. If the question remains unresolved, its status changes from  X  X pen X  to  X  X n-voting X , where other users can vote for a best answer until a clear winner arises. In addition to asking and answering questions, users can also provide feedback by X  X tarring X  X nteresting questions, and rat-ing answers with X  X humbs up X  X r X  X humbs down X , and voting for best answer as mentioned above. Users can also receive activity updates on questions and follow the activity of other users. Finally, the community is self-moderated, and users can report and block questions and answers that violate the community guidelines (inappropriate language, spam etc.).
We consider here only resolved questions, e.g., questions that have been answered and for which a best answer has been chosen, since they are well indexed by Web Search en-gines and often have better quality than open or in-voting questions. In addition, we consider only best answers, based on the intuition that an external searcher reaching a Ya-hoo! Answers page will in most case ignore the other an-swers. This simplifying assumption is based on the fact that the best answer is prominently displayed below the ques-tion (see Figure 2) and users rarely browse results below the fold [22]. A resolved question will thus be represented by a pair (question, best-answer) and their associated additional signals such as stars, thumbs-up etc.

An additional type of information that we make use of is the click data-set : pairs of queries and the related CQA pages on which users, who issued the queries, clicked within the search results for these queries. In our experiments we utilized a click data-set that consists of pages from Yahoo! Answers that were clicked on within Google X  X  search engine (see Section 4.1.1).
One of the main problems in Yahoo! Answers, and in CQA sites in general, is the high variance in the perceived question and answer quality. Recently, this problem attracted a lot of research attention. Some studies attempted to assess the quality of answers or users [17, 1, 28, 32, 12], or questions [1, 29, 31], and filter or rank them accordingly [33]. Most re-cently, Horowitz and Kamvar [14] attempted to match ques-tions to possible answerers, aiming at improving the quality of generated answers. Another related effort was estimating the archival value of questions for subsequent recommenda-tion and retrieval [11].

Relatively few studies addressed the satisfaction of a user from the service provided by CQA sites. Most closely re-lated to our work, Agichtein et al. [19] attempted to predict whether the asker of a question will be satisfied with the re-ceived answers. Our work goes beyond previous efforts as we propose and empirically evaluate techniques to estimate the satisfaction of searchers , as opposed to the original askers and answerers in CQA.
Significant research has been done on estimating the qual-ity of web search results, and the satisfaction of searchers. Query difficulty has been actively studied in the IR commu-nity, [6, 40]. A related problem of query ambiguity can also affect search quality and searcher satisfaction [34]. We adapt these techniques as a first step in predicting searcher satis-faction in CQA, since the latter clearly depends on query interpretation.

Searcher satisfaction in web search was addressed in [15, 8, 13], which utilized query log information for the task, such as relevance measures, as well as user behavior during the search session, including mouse clicks and time spent between user actions. What makes this task such a chal-lenging problem is the large diversity in user goals [26], with a different definition of satisfaction for each, which requires developing unique satisfaction prediction models for the re-spective information needs. In our work we focus on satisfy-ing types of queries that arguably are the most difficult for a web search engine to satisfy and often require other peo-ple to answer [21]. Specifically, we argue that some of these needs can often be satisfied with existing answers from CQA archives. Hence, we aim at harnessing the unique struc-ture of such archives for detecting web searcher satisfaction, which is not captured by standard query and session logs.
Searching CQA archives has been an active area of re-search, and several retrieval models specialized to CQA con-tent have been proposed, [39, 4]. Such dedicated models are clearly deployed in practice, if only for specialized lay-out as demonstrated by Google specialized Yahoo! Answers snippets. Examples of other approaches include incorporat-ing category information into retrieval [5], and exploiting the question-answer relationship [36]. While our main fo-cus is on estimating the satisfaction with a given retrieved question-answer pair for a query, we adapt and extend these techniques for matching the query to the question and the answer content. Additionally, we show how our work could be applied for effective re-ranking of the retrieved question-answer pairs for a query, resulting in a significant improve-ment over a state-of-the-art baseline.

In the spirit of XML and semi-structured retrieval [2], it also makes sense to consider document structure in CQA, as has been done for Web Search [25], book retrieval [18] or sponsored search [3]. Thus in the case of Yahoo! An-swers, we will consider the resolved question structure and distinguish between a question and its best answer (per our simplifying assumption, cf. Section 2.1), and their associ-ated meta-data, while constructing features for our predic-tion tasks (see Section 3.2).
In this section we first introduce the task of predicting searcher satisfaction by a CQA page. Then, we propose approaches for representing and tackling this problem using regression algorithms. We now define searcher satisfaction by a CQA answer :
Given a search query S , a question Q , and an answer A originally posted in response to Q on a CQA site, predict whether A satisfies the query S .
 For example, for a query  X  X xample of monologue X , the best answer shown in Figure 2 is considered satisfactory because it clearly and comprehensively addresses the search intent.
Thus, instead of a Web search satisfaction task that exam-ines a (query,Web page) pair, we consider a different tuple (query,question,answer) , where the (question,answer) pair has been extracted from the CQA page. The reason for us-ing a more refined representation of (question,answer) rather than a full Web page (a Yahoo! Answers page in our case) is mostly for interpretability at a finer level. In practice, when experimenting with Yahoo! Answers in the remainder of this paper, we will use the simplification of considering only the best answer (cf. Section 2.1) 5 as A .

To solve our prediction problem, we propose to break it down into three sub-tasks: query clarity , query-question match and answer quality . More specifically:  X  The query clarity task , which should not be confused with traditional query difficulty in IR, consists of estimat-ing whether the query may be viewed, and understood, as a question. We hypothesize that if a query is not under-standable or ambiguous, a CQA site is unlikely to have an existing satisfactory answer for this query.  X  The query-question match task consists of estimating whether the question is driven by the same or by a similar enough information need as the query. This is a prerequisite for the answer to have a chance to address the query. Fur-thermore, since most search result snippets will only show the question title (such as shown in Figure 1), this match is a key driver for the searcher to select a specific CQA page: the question plays the role of a critical intermediary between the query and the answer.  X  The answer quality task allows estimating the prior qual-ity of the answer, with respect to the original question, and thus relates to the previously studied asker satisfaction task [19]. In our approach, answer quality characteristics are used not as the goal, but rather as additional input for our main task of predicting searcher satisfaction .
There are multiple advantages of breaking the main task into subtasks. First, we can better understand and analyze the problem structure, and devise more effective algorithms, as described next. Second, the resulting models become more interpretable and informative, by allowing us to an-alyze performance for each subtask. Finally, answer quality and related prior information (taking advantage of meta-information in particular) may be computed offline within the CQA site using methods such as described in [1].
The searcher satisfaction task seems to be better modeled as a graded task, since we found that it is easier for humans to judge satisfaction as a score within some range (see our human annotation in Section 4.1.2). Therefore, we treat the searcher satisfaction task as a regression problem. To this end, we need to define appropriate features for learning the
N ote that the same model could be generalized to consider-ing other, and not necessarily best, answers one at a time if the CQA site could isolate clicks or views of these answers. This is not the case with Yahoo! Answers where all answers are featured one after the other on the same page. reg ressor. We now describe the features used to represent the information in our task, and then formulate our direct and composite approaches.
By breaking down the main task into three subtasks, we distinguish between query clarity, query-question match, and answer quality features.

Since some of these subtasks were previously studied in-dependently, such as [6, 34, 37] for query clarity, and [19, 1] for answer quality, we leveraged this prior work in the construction of our feature set. We now describe each of the feature groups, while the complete list of features is shown in Table 1.  X  Query clarity features include query length, click statis-tics for the query, and query entropy computed based on the click dataset. We also compute a query clarity score based on a language model of the CQA collection, as well as an indicator whether the query starts with a  X  X H X  or other question word.  X  Query-question match features include match scores com-puted by popular retrieval models such as cosine similar-ity, TFIDF, BM25, and KL-divergence language model. For measuring these scores, we treat parts of a CQA page (the question title, question details and the best answer) as sep-arate documents, and match each such part against the query. Additional features include measures of the overlap between the query and question, such as Jaccard coefficient and length ratios, and co-occurrence statistics between the query and the given question from the click data.  X  Answer quality features are of two types. The first type of features deals with the quality of the answer, and is mainly based on the analysis given in [1]. The second type of fea-tures addresses the answer quality as predicting asker satis-faction, which directly maps to our third subtask. To this end, we mostly used the top performing features for predict-ing asker satisfaction as reported in [19].

Before using these features to build regression models, pre-processing was performed, as described in Section 4.2, to deal with missing values and to normalize the data.
Our first approach to estimating searcher satisfaction, which we call the direct approach , consists of simply training a re-gressor over all the features defined for a given (query, ques-tion, answer) tuple. The rationale here is to rely on the power of discriminative learning to optimally use all avail-able features to predict the final target.

While many regression algorithms could be employed for this task, our preliminary experiments with a wide range of models, including Linear Regression, Gaussian Processes, Ridge Regression and Random Forests, indicated Logistic Regression to be the most promising approach due to high variability and non-linear distribution of many of the input features. We now present our adaptation of the  X  X lassical X  logistic regression algorithm to our problem.

Logistic regression uses the logistic function f ( t ) = exp( t ) / (1 + exp( t )) to model an output variable restricted to the open set (0 , 1). This property makes the logistic function, properly scaled, a natural candidate for modeling our rating targets, all constrained to the range of (1 , 3) (see our human annotation in Section 4.1.2).

In what follows, we denote by ( x i , y )  X  R n  X  [1 , 3] , i = 1 , . . . , l , a generic example in the training set, where n is the number of features. We use f w,b ( x i ) = 1 + 2 exp( w T b ) / (1+exp( w T x i + b )) as an estimate for the target variable y . The parameter vector of the model, w , and the scalar b are obtained by minimizing the squared loss The second term is introduced for regularization, where  X  controls the strength of regularization.

Since there is no closed form solution for the parameters w and b that minimize Equation 1, we resort to Stochastic Gra-dient Descent [30], a fast and robust optimization method. It is an online algorithm where the parameters, initially ran-dom, are being updated using the gradient of the loss. In our case, the update is b  X  b +  X  b and w  X  w +  X  w , where W e cycle through random permutations of the observations to achieve convergence. For the learning rate, we use a schedule of the form  X  =  X  t =  X  0 t +  X  wh ere  X  &gt; 0, and t is the number of update steps taken thus far. The schedule satisfies the Robbins-Monro conditions [24], P  X  t =  X  and P  X  2 t &lt;  X  , hence convergence is guaranteed. In light of the small number of examples in our dataset, we did not attempt to optimize the hyper-parameters of the model. Specifically, since the data is dense and the number of features is much smaller than the number of examples, we used weak regular-ization  X  = 0 . 01 in all our experiments. We used moderate values for the learning rate schedule,  X  0 = 10 and  X  = 10, and stopped iterating when the training set Mean Squared Error fell below a predefined threshold (0.0001).
Our second approach, which we call the composite ap-proach , first trains a separate logistic regression model for each of the three subtasks defined above, and then combines their results for main task (predicting searcher satisfaction). Figure 3 depicts the high-level workflow of this approach. In this approach, each regressor is trained using a subset of features relevant for each subtask. Considering that query clarity may affect the question match, we also added query clarity prediction as a feature to the query-question match regressor. Finally, the regression predictions for the three subtasks are provided as features for the final regressor to predict the overall searcher satisfaction.

This composite approach presents several advantages over the direct approach. First, it is more flexible in terms of feature selection. The individual regressors could be trained either on the same feature set, i.e., the large feature vec-tor used in the direct approach, or on different feature sets selected by suitable feature selection methods for each sub-task. More importantly, the composite approach can take advantage of the advances made by the research community in each of the sub-tasks, to improve the prediction of overall searcher satisfaction.
This section first describes how we assembled a dataset from a sample of Google queries and a log of visits to Ya-hoo! Answers. We then describe the rating procedure to
F igure 4: MTurk interface for human labeling. acquire the  X  X round truth X  for searcher satisfaction, and the characteristics of the resulting data.
To explore how searchers are satisfied with the existing answers in CQA sites, we used a large sample of queries issued to Google X  X  search engine from Aug 24, 2010 to Aug 30, 2010 by users who selected as result (by clicking on it) a Yahoo! Answers link. This click dataset contains more than 37M clicks on 6M questions by 20M users following around 20M queries. By analyzing the distribution of this click data, we found that 86% of the queries are issued by only one user; therefore, most of the queries are tail queries.
Since it is hard for human to label searcher satisfaction for such a big dataset, we randomly sampled it to generate a smaller dataset consisting of 614 clicked questions follow-ing 457 queries issued by at least two users. These ques-tions and corresponding answers may be biased to satisfy the searchers X  information needs, as they are clicked from the search results. To correct this effect, we further issued a random sample of 118 queries to Google X  X  search engine with site restriction to Yahoo! Answers and crawled the top 20 results (all question pages due to the site restriction). Only questions posted in 2009-2010 are kept based on the available associated meta-data. In total, our final dataset comprised of 1681 query-question pairs.
Amazon Mechanical Turk (MTurk) was used to collect hu-man judgments on how an answer satisfies a search query. To better understand the effects of query clarity and query-question match on searcher satisfaction with answers, we also asked the MTurk workers to label how clear the query is and how the question matches the query. Figure 4 shows the interface we used in MTurk. We used 3-scale rating method for all the rating tasks, { clear=1, medium=2, vague=3 } for query clarity, { well matched=1, partially matched=2, not matched=3 } for question match, and { highly satisfac-tory=1, somewhat satisfactory=2, not satisfactory=3 } for searcher satisfaction. Each MTurk hit consists of 15 (query, question, answer) triples as shown in Figure 4, and each hit is labeled by 5-7 workers.

To validate the labels of MTurk workers, we also asked 6 researchers to label the query clarity for all the queries. Then we analyzed the agreement between the researchers and the MTurk workers. We first computed the average rating by researchers as well as by MTurk workers for each query, then used a threshold t to cast each average numer-ical rating nr into a binary rating br (if nr &lt; = t then br =clear, else br =not clear), and finally we computed the Fleiss X  X  kappa coefficient[9] based on these binary ratings between the two sources. The highest kappa value 0.38 was achieved with a threshold of 1.3 (average agreement=0.70, average majority percentage=0.85). This analysis showed that the ratings from MTurk workers were reasonable.
For query-question match and searcher satisfaction, we only had ratings from the Mechanical Turk. So we used the same threshold strategy to cast each ordinal rating into a binary rating, then computed the Fleiss X  X  kappa coeffi-cient for each MTurk HIT, and finally computed the average kappa value. The highest kappa value (0.34) was achieved with a threshold of 2 for query-question match (average agreement=0.85, average majority percentage=0.91), and the highest kappa value (0.25) was achieved with a thresh-old of 2 for searcher satisfaction (average agreement=0.76, average majority percentage=0.84).

From the above agreement analysis, we can see that al-though the kappa coefficient among MTurk workers is not high, possibly due to the careless rating of some MTurk workers, the average rating by all the MTurk workers shows a moderate agreement with researchers. Therefore, we use the average rating by MTurk workers as our ground truth in order to evaluate the prediction of query clarity, query-question match, and searcher satisfaction with answers.
Figure 5 shows the distributions of the resulting ground truth set. The x axis represents the mean over the rat-ings by all MTurk workers, with 1 standing for the highest score, e.g., clear/well-match/highly satisfactory for respec-tively query clarity/query-question match/searcher satisfac-tion with answers, and with 3 standing for the lowest score for each. The y axis represents the frequency count of rat-ings in each bucket of x. We can see that all the distri-butions are skewed, especially the query clarity one due to the bias of the click data. Distribution for question match and searcher satisfaction are more balanced after we add the search engine results. To better understand the rela-tions between the three variables, we computed the Pearson correlation between them and obtained the following result: the correlation between query clarity and searcher satisfac-tion is 0.1428, and the correlation between question match and searcher satisfaction is 0.6970.
To make the data more amenable for modeling we used a three-stage preprocessing, performed on each feature: 1. null values have been replaced by the mean value. An 2. features obtained by counting or summation, such as 3. features were shifted and scaled to have a zero mean F igure 5: Distributions of the mean ratings of MTurk workers for query clarity, query-question match, and searcher satisfaction with answers.
We consider four methods for estimating a searcher X  X  sat-isfaction on Yahoo! Answers:  X  Google-derived baseline : As described in Section 4.1.1, we crawled the top 20 results by submitting our queries to Google search, with site restriction to the Yahoo! Answers site. As a result, we obtained a ranked list of question pages for each query from the search engine. The rank of each question page indicates how well this page satisfies the query. Since a search engine ranks results by maximizing searcher X  X  satisfaction with the overall result ordering, we use Google X  X  ranking of question pages as our baseline.  X  Direct approach: This method implements the logistic regression approach described in Section 3.3.  X  Composite approach: This method implements the composite approach described in Section 3.4.  X  Composite upper-bound: We also trained the com-posite approach with the intermediate predictions for the query clarity and query-question match subtasks replaced with their human ratings . Since human judgments are ex-pected to be more reliable than the automatic predictions, this method serves as an upper bound for the possible per-formance of the fully-automatic composite approach. Estimating searcher satisfaction : Our main prediction task estimates searcher satisfaction for a query with one given answer, independently of other query-answer pairs. Hence our main evaluation of the direct and composite ap-proaches over all pairs is via root mean squared error (RMSE) and Pearson correlation between predictions and the human judgments. Both RMSE and Pearson correlation are stan-dard performance estimators for regression problems.
When comparing our results to the Google-derived base-line described above, however, we could not use the above metrics, since Google does not divulge an independent score for each query-answer pair. Therefore, we propose to use two different metrics for ranking : (1) Kendall X  X  tau (  X  ) cor-relation that has often been used in IR [27] to compare two ranked lists, and (2) the popular NDCG metric often used in IR evaluation [16]. As ground truth, we use the MTurk ratings described above to calculate these metrics as follows: Kendall X  X   X  : First for each search query s , we identify the set Q ( s ) of all questions associated with s in our dataset. We then generate the following ranked lists. 1. The ground truth MTurk rating scores between each 2. We identify the rank of each question q in Q ( s ) in the 3. Similarly the direct and composite approach introduced Kendall X  X   X  correlation is then computed between the ground truth list L M and each of the system-generated lists L G and L c . We process the answers A ( s ) associated with s in the same way, and generate and evaluate the four respective lists in exactly the same manner.
 NDCG metric : The ranked lists to compare are generated exactly in the same way as described above for Kendall X  X   X  evaluation. Following Long et al. [20], we do not discretize the ground truth MTurk ratings, and use them directly (in place of relevance) to calculate the gain in the NDCG com-putation as follows: where the relevance values are computed as rel i = (3  X  r )  X  [0 , 2], where r is the average assessor rating of query-question match or searcher satisfaction, respectively.
 Evaluation setup: For training and testing, we use strat-ified 10-fold cross-validation. This guarantees that the data distribution in each fold is similar to that of the entire dataset.
We first present our results for the main task of predicting searcher satisfaction, and compare the direct and the com-posite approaches. Then, we analyze the performance of the proposed methods to identify key factors that affect the pre-diction accuracy. Finally, we present the results of applying our models to re-rank CQA answers in search engine re-sults, showing significant improvements of our ranking over Google X  X .
Table 2 shows the results on predicting searcher satisfac-tion using our proposed direct and composite approaches. We report the mean (  X  standard deviation) RMSE and Pearson correlation over the ten cross-validation folds.
In the first two rows of Table 2, we see that the composite approach performs better than the direct approach on both T able 2: Regression results on searcher satisfaction. T able 3: Regression results on individual sub-tasks. c orrelation and RMSE 6 metrics. This difference is statisti-cally significant according to the Wilcoxon two-sided signed ranks test at p = 0 . 01 [38]. This observation is quite intu-itive, since the composite approach takes advantage of ad-ditional knowledge, which is learned from the human labels for the query clarity and query-question match sub-tasks.
Now consider the last row in Table 2, which reports the upper bound performance of the composite approach. To es-timate the upper bound, we replace the individual regressors we trained for the query clarity and query-question match sub-tasks with the actual (average) human scores for those tasks, and plug these scores as features into the compos-ite regressor. Evidently, the performance of the compos-ite method can be improved dramatically if its components, namely, the query clarity and the query-question match pre-dictors, are improved. We believe this flexibility of the com-posite approach constitutes a substantial advantage over the simpler direct approach.
Table 3 details the performance of the individual regres-sors that were combined in the composite approach. Here the query-question match regressor also uses the query clar-ity prediction as a feature, as explained in Section 3.4. The answer quality regressor is trained using the asker satisfac-tion ground truth [19] (An asker is considered satisfied iff he selected the best answer and gave at least 3  X  X tars X  for the quality). Again, we report the mean (  X  standard deviation) RMSE and Pearson correlation over the ten folds.

By analyzing the predictions for searcher satisfaction by the composite regressor, we see that it can predict accu-rately both when the searcher is satisfied and not satisfied. We show a number of actual examples in Table 4. In the first example (E1), our method is able to correctly detect that the query is a little vague 7 , the query-question match is low 8 , and the answer is too simple to convince the searcher On the other hand, in the second example (E2), the query is quite clear and matches the question well, and the an-swer provides helpful advice to the searcher. Again, our method successfully predicts the overall searcher satisfac-tion with the answer, as well as individual sub-task scores (query clarity and query-question match).

To better understand the effectiveness of our methods, we also performed error analysis on the 30 cases where the dif-ference between our prediction and the target was larger than 1. We found two cases, E3 and E4 (Table 4), for
N ote that lower RMSE values reflect better performance.
Higher values mean lower query clarity.
Higher values reflect poorer query-question match. Higher values mean lower searcher satisfaction.
 Table 5: Mean Kendall X  X   X  and NDCG results on ranking questions and answers for queries.
 wh ich our system predicted lower searcher satisfaction than the ground truth. In the other cases, our system predicted higher than actual satisfaction X  X verage prediction of 1.66 versus average ground truth of 2.65. We believe the main reason for these large differences lies in the answer quality. In fact, more than half of the answers are not helpful at all (e.g., E5); other answers show negative opinions towards the askers, or contain ads. Thus, our error analysis confirms the importance of answer quality to searcher satisfaction, and also poses the challenge of more intelligent prediction of answer quality.
One possible application of predicting searcher satisfac-tion is using this prediction for ranking CQA pages in Web search. To this end, in Table 5 we compare the quality of ranking produced by our methods to that of Google X  X  rank-ing of results retrieved from the Yahoo! Answers site. We compared the entire ranked lists of results returned by our methods and by Google to the ranking induced by human (MTurk) labels, therefore, we report different metrics than above, namely, NDCG and Kendall X  X   X  . Our prediction of searcher satisfaction results in improvements over Google X  X  on both metrics. All improvements are statistically signif-icant according to the Wilcoxon double-sided signed ranks test at p = 0 . 01 [38]. Interestingly, Google X  X  ranking of the questions (as opposed to answers) for a query is supe-rior, which is to be expected due to additional information Google maybe used for ranking the questions (such as link structure and clicks) X  X hereas our work focuses on predict-ing searcher satisfaction with the answers , where indeed our methods perform better.

We further analyze these results by plotting the improve-ments over the Google baseline, for individual queries (Fig-ure 6). Interestingly, it appears that the results do not depend on the number of answers to re-rank. In another experiment (omitted for lack of space), we found that the improvements are not correlated with query length. These results suggest that our methods are robust for a wide range of queries, and are likely to remain stable for other condi-tions. In summary, our results show that our satisfaction prediction allows our re-ranker to consistently outperform the state-of-the-art Google baseline, and could provide valu-able input for other tasks, as we plan to explore in the future.
In this paper we formulated a novel task of predicting searcher satisfaction with answer pages from CQA sites. Prior research mainly concentrated on the first-order effects of community question answering, studying satisfaction of original askers of questions or potential answerers (both on the CQA site itself). In contrast, we study second-order ef-fects of CQA archives, which repeatedly benefit many more baseline on ranking answers for queries.
 Web users when these answers are included in Web search results for a variety of queries. We utilize the unique struc-ture of CQA pages as well as all available community signals (e.g., ratings, thumbs-up) to improve the quality of match-ing between these pages and Web search queries.

We proposed to break the task of predicting searcher satis-faction into three sub-tasks, namely, predicting query clarity, query-question match, and answer quality. We then formu-lated two methods for solving the main prediction task. Our direct method simply uses all the available features in a sin-gle regression model. Our composite method first learns three separate regressors for each of the three sub-tasks, and then uses their predictions as features for solving the main task. Predictably, the performance of the composite method is statistically significantly superior to that of the direct method. This can be explained due to its use of addi-tional exogenous knowledge, which is learned from the hu-man labels for each of the sub-tasks while training the three individual regressors. Furthermore, the composite approach is more flexible, and it can immediately benefit as the pre-dictions in individual sub-tasks are improved. Indeed, when we replace the predictions in each sub-task with actual hu-man labels, the performance of the composite regressor is dramatically improved.
 We believe that modeling the searcher satisfaction with CQA answers has multiple benefits. For example, if a search engine detects that a user is struggling with a search session, it could suggest posting a question on a CQA site, offer-ing help with formulating a natural language question and choosing an appropriate category. On the search engine side, an accurate predictor of searcher satisfaction can be used for improved ranking of CQA results in Web search. Indeed, our results show that this can be achieved. To this end, we com-pared the quality of ranking of CQA answers produced by our methods with, and demonstrated significant improve-ments over the quality of the ranking provided by Google X  X  search engine (both compared to the  X  X deal X  ranking gener-ated from the ground truth labels provided by humans).
In our future work, we plan to further investigate the types of queries that are likely to be satisfied by CQA pages. We also plan to improve query-question matching using (mono-lingual) machine translation models. Another brunch of po-tential work is to develop semi-supervised or unsupervised methods to predict searcher satisfaction, as large numbers of human labels are hard to obtain. Finally, we intend to study searcher satisfaction with other types of community-generated Web pages that possess interesting structure, such as Facebook pages (subject to appropriate privacy policies).
This work was supported by the National Science Founda-tion grant IIS-1018321 and by the Yahoo! Faculty Research and Engagement Program. [1] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and [2] S. Amer-Yahia and M. Lalmas. XML search: [3] M. Bendersky, E. Gabrilovich, V. Josifovski, and [4] J. Bian, Y. Liu, E. Agichtein, and H. Zha. Finding the [5] X. Cao, G. Cong, B. Cui, C. S. Jensen, and C. Zhang. [6] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. [7] D. Donato, F. Bonchi, T. Chi, and Y. Maarek. Do you [8] H. Feild, J. Allan, and R. Jones. Predicting searcher [9] J. L. Fleiss. Measuring nominal scale agreement [10] S. Goel, A. Broder, E. Gabrilovich, and B. Pang. [11] F. Harper, D. Moy, and J. Konstan. Facts or friends?: [12] F. M. Harper, D. Raban, S. Rafaeli, and J. A. [13] A. Hassan, R. Jones, and K. Klinkner. Beyond DCG: [14] D. Horowitz and S. Kamvar. The anatomy of a [15] S. B. Huffman and M. Hochster. How well does result [16] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [17] J. Jeon, W. B. Croft, J. H. Lee, and S. Park. A [18] G. Kazai and A. Doucet. Overview of the INEX 2007 [19] Y. Liu, J. Bian, and E. Agichtein. Predicting [20] B. Long, O. Chapelle, Y. Zhang, Y. Chang, Z. Zheng, [21] M. Morris, J. Teevan, and K. Panovich. A Comparison [22] J. Nielsen. User interface directions for the web. [23] D. Raban. Self-presentation and the value of [24] H. Robbins and S. Monro. A stochastic approximation [25] S. Robertson, H. Zaragoza, and M. Taylor. Simple [26] D. E. Rose and D. Levinson. Understanding user goals [27] M. Sanderson and I. Soboroff. Problems with kendall X  X  [28] C. Shah and J. Pomerantz. Evaluating and predicting [29] Y.-I. Song, C.-Y. Lin, Y. Cao, and H.-C. Rim. [30] J. C. Spall. Introduction to Stochastic Search and [31] K. Sun, Y. Cao, X. Song, Y.-I. Song, X. Wang, and [32] M. Surdeanu, M. Ciaramita, and H. Zaragoza.
 [33] M. A. Suryanto, E. P. Lim, A. Sun, and R. H. L. [34] J. Teevan, S. T. Dumais, and D. J. Liebling. To [35] A. Tsotsis. Just because g oogle exists doesn X  X  mean [36] X. Wang, X. Tu, D. Feng, and L. Zhang. Ranking [37] Y. Wang and E. Agichtein. Query ambiguity revisited: [38] F. Wilcoxon. Individual comparisons by ranking [39] X. Xue, J. Jeon, and W. Croft. Retrieval models for [40] E. Yom-Tov, S. Fine, D. Carmel, and A. Darlow.
