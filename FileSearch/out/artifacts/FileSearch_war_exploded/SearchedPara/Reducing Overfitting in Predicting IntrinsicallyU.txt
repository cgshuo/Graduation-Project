 Proteins are linear chains composed of 20 amino acids (also called residues), linked together by polypeptide bonds and folded into complex three-dimensional (3D) structures. Disordere d regions (DRs) in protein sequence are structurally flexible and usually have low sequence complexity [1,2,3,4]. Physicochemically, DRs are enriched in charged or polar amino acids, and depleted in hydropho-bic amino acids [5,6,7]. Proteins containing DRs are intrinsically unstructured proteins (IUPs) and DR prediction can also be called IUP prediction.
Many computational studies of predicting DRs are based on the biased amino acid composition (AAC) of DRs, which is simple and effective [3,8,9]. However, due to the scarce of disordered training dataset and inevitable noise during physiological experiments, the issue of o verfitting [9,10,11] has been raised. The common tackles against overfitting in IUP prediction include attribute selec-tion [9,12] in which only the compositions with the best separating effect are selected; or deriving profile, which is a kind of combination of 20 AAC [11]; or choosing a less overfitting model [13] such as SVM.

However drawback still exists for some current approaches. Attribute selec-tion may not always improve the IUP prediction accuracy [12]. Checking every combination of whole feature subset might help but it is prohibitively expensive. Less overfitting models like SVM are less accessible to domain scientists.
In our research we consider two approaches alleviating overfitting in IUP prediction. Approach one, we present a system predicts DRs using reduced AAC with the decision tree model. It achieves the same effect as pruning and a simpli-fied tree structure is created by limiting the number of input attributes. With the help of domain knowledge, this solution works similar to attribute selection but is easier and computationa lly more efficient. Predicti on accuracy improves and a limited set of rules are produced, which qu antifies complex amino acid composi-tion information that is previously unknown. In the second approach, we present a novel application of a recent model in the machine learning field called ran-dom forest (RF) [14] in IUP prediction. A special property of RF is that it does not overfit [15,16], which generally is not the case for numerous other machine learning algorithms. Our results demonstrate that random forest performs much more accurate than the decision tree and is able to stand overfitting impact. We first describe how the training dataset is constructed in this section. Then we present our decision tree learning and prediction approach. 2.1 Training Data, 20-AAC and Windowing Different from UCI [17] machine learning repository, IUP prediction has no stan-dard training dataset. In this study, the training datasets come from DisProt (version 2.2) [18] and PDB-Select-25 (the Oct.2004 version) [19]. DisProt is a collection of disordered reg ions of proteins based on literature description. Only disordered segments of more than 30 residues are extracted, which includes 204 disordered segments and 28386 residues. This disordered training set is called D -train hereafter. The ordered training set i s extracted from PD B-Select-25, a representative set of protein data bank (PDB) chains that shows less than 25% sequence homology. We sel ected 366 high-resolution ( &lt; 2  X  A) segments of stable structures which has no missing backbone or side chain coordinates and contains at least 80 residues. This training set includes a total of 80324 residues, and is referred to as O -train hereafter.

The windowing technique was introduced in [20], where a sequence of residues including the same number of residues on its both sides predicts for the residue at the center of the window. The AAC in a window is represented by 20 numbers (elements), denoted by n . When a window of w residues slides along a sequence i , the content of the sequence is represented by n  X  ( L i  X  w +1) elements, where L i is the length of sequence i . As a result the disordered training segments are represented by 204 i =1 n  X  ( L i  X  w +1) elements, denoted as D -M , and the ordered training segments are represented by 366 i =1 n  X  ( L i  X  w + 1) elements, denoted as O -M .
 2.2 The C4.5 Decision Tree System There are two different kinds of decision tr ees: classification and regression trees. C4.5 [21] is a popular classification tree learning system and employed as our tree based IUP predictor.

Given a set T of D (disorder) and O (order) fragments the information content (entropy) for T is inf o ( T ). After T has been partitioned into T 1 and T 2 following atest F i , the information needed to classify T is inf o F i ( T )
The information gain inf o ( T )  X  inf o F i ( T ) measures the information that is gained by partitioning T with F i . This gain is normalized by the information generated by the split of T ( split inf o ( F i )) into ordered and di sordered to rectify the bias towards attributes with a large number of values. Finally, the best test 2.3 Overfitting We found that decision tree and AAC based IUP predictor suffers from over-fitting after comparing results of self-test and 10-fold cross validation. The pre-dictor achieves a nearly perfect accura cy in self-test, 99.8%; however 10-fold cross validation decreases dramatically to 76.1%. Meanwhile, around 1100 rules are generated after the training procedure. Many of them involving complicated amino acid relationship have a fairly low usage according to the statistics. Some rules are contradictory to structure biology knowledge. To tackle overfitting, we generat e reduced AAC from training data D -M and O -M . 20 AAC in a window are grouped into four compositions, according to hy-drophobicity and polarity properties of amino acid. They are positively charged ( P ), negatively charged ( N ), hydrophobic ( H ) and others ( E ), as shown in Table 1.

The training and prediction procedure of reduced AAC is the same as that of AAC. Decision tree is constructed from the training data with reduced AAC. After training, every path from the root of a tree to a leaf gives one if-then rule. To classify a query protein sequence, its corresponding reduced AAC is calculated for a given window equivalent to training. Then, starting from the root, the tree node determines which composition has to be checked and what is the residue status. The Figure 1 is a sample decision tree.

As will be discussed in the result and dis cussion section, pred iction accuracy of reduced AAC is significantly higher than that of AAC. Meanwhile, the number of rules has dropped dramatically to 150, which is much easier to be analyzed. The length of rules is shorten and these rules reveal more explicit AAC information. A random forest is an ensemble of unpruned decision trees, where each tree is grown using a subset (bootstrap) of the training dataset [14]. Bootstrap is the training set drawn randomly from original training sets with the same number of training samples. Each tree induced from bootstrap samples grows to full length without pruning and in different from information gain in C4.5, the splitting criterion of random forest is the Gini index.
 Ifadataset T contains D and O fragments, Gini index is defined as gini ( T ). After T is split into subsets T 1 and T 2 with a test F i , the gini index of the split data is defined as gini F i ( T ). The split provide the smallest gini F i ( T )ischosen to split.

In real implement, there can be a few tens even hundreds of trees. Figure 2 is our development approach based on random forest. The number of trees in the forest is adjustable. To classify a query sequence, each single tree in the forest works similar to the decision tree and giv es a residue status either ordered or disordered. As a forest forms with a large number of trees, the final classification having the most votes is chosen.

Random forest provides a reliable estimate of error using the data that is randomly withheld from each iteration of tree development (the  X  X ut-of-bag X  or OOB portion). The error rate of a RF d ecreases as more trees are added until a certain point but will never get larg er no matter how many more component predictors are added. Thus employing more trees will not lead to overfitting [15], which is a desirable feature for IUP prediction. In this section we compare the overfitting influence on decision tree and random forest based IUP predictors. 5.1 Overfitting Comparison Figure 3 are the ROC curves for the deci sion tree and random forest models. The bigger the area under the ROC curve the more precise the predictor is. The two curves on top are OOB test results of the random forest including 50 trees. The other two curves are 10-fold cross validation results of the decision tree. We keep 90% protein sequences to do the tra ining then predict those 10% sequences left. After that we shift the training and predicting sequences until all protein sequences are predicted.
For the decision tree, smaller windows ge nerate less accuracy results, overall random forest has a much higher ROC curve than the decision tree. The random forest trained on AAC performs around 5% more accurate than that trained on reduced AAC. We also tested the relations hip between prediction accuracy and different window size and the number of trees. Generally, with an increasing number of trees, the prediction accuracy improves. This improvement becomes marginal and tends to stabilize at 300 trees. Increasing window sizes also im-proves accuracy. However big windows have the problem that many residues in the beginning and at the end of the sequence are ignored. Given different win-dow and number of trees, results of OOB test with AAC always perform superior than the RF model with reduced AAC. So th ere is no obvious ov erfitting caused by AAC in the RF model.

The result of C4.5 is less accurate comp ared to random forest. However, the decision tree with reduced AAC totally contains the curve of AAC. With re-duced AAC, 10-fold cross validation has improved from 76.1% to around 80%. Given X axis is false positive rate and Y axis is true positive rate, it means reduced AAC makes less mistakes and finds more true DRs during IUP predic-tion. Grouping strategy makes some compositions originally vague well estab-lished and some compositions originally redundant simplified. So overfitting is alleviated by grouping.

Reduced AAC also significantly decrea ses the number of rules generated by the decision tree model. Our experiments showed that the rule number reduced from 1100 to around 150. Besides by reduced AAC, each rule gets more concise which improves the rule quality, and is much easier to be studied. As a summary, reducing the number of input parameters can be an approach complementing current techniques to avoid overfitting.
 In this paper we focused on reducing overfitting in IUP prediction. We have demonstrated that overfitting can be reduced by simplifying the input param-eters with domain knowledge, rather than complicating the model. Our initial attribute feature AAC demonstrates that overfitting happens which decreases the performance of the predictor. As a simple approach of grouping them ac-cording to amino acid physicochemical property, overfitting is assuaged in our decision tree model. Furthermore, wi th reduced AAC, decision tree based IUP predictor generates significantly less amount of rules, which are simpler and more precise.

Our second approach tackles overfitting by applying ensemble learning. Ran-dom forest as a model proven have no overfitting has outstanding accuracy in IUP prediction. With the simple AAC information, it performs much better than decision tree and does not suffer from the overfitting. Apart from the same drawback as other ensemble learning where output is less accessible to domain scientists, random forest is a very suitable tool to be used in comparative pro-teome studies and protein structure studies.

For future work, we will study if grouping amino acids can also help reducing overfitting in other models for IUP prediction, such as Hidden Markov Model and Support Vector Machine. In addition to AAC we have tested, grouping may also help reduce the overfitting in input like Markov Chain and amino acid replacement matrix [22]. They are all gro upable with physicochemical properties. Acknowledgments. The authors thank Dr Marc Cortese for his explanation of the DisProt database. Z.P. Feng is supported by an APD award from the Australian Research Council.

