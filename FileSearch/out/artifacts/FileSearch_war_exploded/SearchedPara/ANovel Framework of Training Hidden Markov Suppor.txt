 Natural language understanding (NLU) aims to map sen-tences to their semantic mean representations. Statistical approaches to NLU normally require fully-annotated train-ing data where each sentence is paired with its word-level semantic annotations. In this paper, we propose a novel learning framework which trains the Hidden Markov Sup-port Vector Machines without the use of expensive fully-annotated data. In particular, our learning approach takes as input a training set of sentences labeled with abstract semantic annotations encoding underlying embedded struc-tural relations and automatically induces derivation rules that map sentences to their semantic meaning representa-tions. The proposed approach has been tested on the DARPA Communicator Data and achieved 93.18% in F-measure, which outperforms the previously proposed approaches of training the hidden vector state model or conditional random fields from unaligned data, with a relative error reduction rate of 43.3% and 10.6% being achieved.
 I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing X  Text analysis Algorithms, Experimentation Hidden Markov support vector machines (HM-SVMs), Nat-ural language understanding, Semantic parsing
The natural language understanding problem can be con-sidered as a mapping problem where the aim is to map a sentence to its semantic meaning representation (or abstract semantic annotation) such as the one shown below.
This is a structured classi cation task which predicts out-put labels (semantic tag or concept sequences) C from in-put sentences S where the output labels have rich internal structures. It is a highly challenging problem because the derivation from each sentence to its abstract semantic anno-tation is not annotated in the training data and is consid-ered hidden. Although a hierarchical hidden state structure could be used to model embedded structural context in sen-tences, such as the Hidden Vector State (HVS) model [4], which learns a probabilistic push-down automaton, it can-not include a large number of correlated lexical or syntac-tic features in input sentences, and it cannot handle any arbitrary embedded relations since it only supports right-branching semantic structures. Other approaches learn se-mantic parsers that map natural language sentences into a formal meaning representation such as lambda calculus or first-order logic [6, 2]. However these systems either require a hand-built, ambiguous combinatory categorial grammar template to learn a probabilistic semantic parser [6], or as-sume the existence of an unambiguous, context-free gram-mar of the target meaning representations [2].

Conditional Random Fields (CRFs) have been extensively studied for sequence labeling. Most applications require the availability of fully annotated data, i.e., an explicit align-ment of sentence and word-level labels. Mann and McCal-lum [5] use labeled features instead of fully labeled instances to train linear-chain CRFs. Generalized expectation criteria are used to express a preference for parameter settings in which the model X  X  distribution on unlabeled data matches a target distribution. They tested their approach on the clas-sified advertisements data set ( Classified ) [3] and achieved 68.3% accuracy with only labeled features. Zhou and He [7] proposed an iterative learning approach based on expecta-tion maximization (EM) to train the CRFs from abstract semantic annotations. They achieved 92% in F-measure on the DARPA Communicator Data.

In this paper, we propose a novel learning approach to train the HM-SVMs from unaligned data. It first computes expectations using initial model parameters. Parsing results are then filtered based on a measure describing the level of agreement with the sentence abstract semantic annotations and fed into model learning using the cutting-plane algo-rithm. With the re-estimated parameters, the learning of HM-SVMs goes to the next iteration until no more improve-ments could be achieved. The rest of this paper is orga-nized as follows. Section 2 introduces HM-SVMs. The pro-p osed learning procedure to train HM-SVMs from abstract semantic annotations is presented in Section 3. Experimen-tal setup and results are discussed in Section 4. Finally, Section 5 concludes the paper. function that assigns to a sequence of words S = ( s 1  X   X   X  s ; i = 1 ; : : : T , a sequence of semantic concepts or tags C = ( c c 2 : : : c T ) ; c i  X  c ; i = 1 ; : : : T , a common approach is to find a discriminant function F : S  X  C  X  R that assigns a score to every input S  X  S and every semantic tag sequence C  X  C . In order to obtain a prediction f ( S )  X  C , the func-tion is maximized with respect to f ( S ) = argmax
In particular, the function F ( S; C ) is assumed to be linear in some combined feature representation of S and C in HM-SVMs [1], F ( S; C ) :=  X  w;  X ( S; C )  X  . The parameters w are adjusted so that the true semantic tag sequence C i scores higher than all other tag sequences C  X  C i := C\ C i with a large margin. To achieve the goal, the following optimization problem is solved instead: where i is non-negative slack variables allowing one to in-crease the global margin by paying a local penalty on some outlying examples, and Cons dictates the desired trade off between margin size and outliers. To solve Equation 1, the dual of the equation is solved instead. The solution  X  w can be written as where i ( C ) is the Lagrange multiplier of the constraint associated with example i and C i .
To train HM-SVMs from abstract semantic annotations, we extended the use of expectation maximization (EM) al-gorithm to estimate model parameters. The EM algorithm is an efficient iterative procedure to compute the maximum likelihood (ML) estimate in the presence of missing or hid-den data. The EM algorithm is divided into two steps. In the E-step, the missing data are estimated given the ob-served data and current estimate of model parameters. In the M-step, the likelihood function is maximized under the assumption that the missing data are known. We summarize the procedure of learning HM-SVMs from abstract semantic annotations in Figure 3. The details of each step are given in the subsequent sections.
Given a sentence labeled with an abstract semantic anno-tation as shown in Table 1, we first expand the annotation to the flattened semantic tag sequence as in Table 1(a). In
In put: A set of sentences S = { S i ; i = 1 ; : : : ; N Ou tput: The trained HM-SVMs with parameters  X 
Pro cedure: Fi gure 1: Procedure of learning HM-SVMs from ab-stract semantic annotations. order to cater for irrelevant input words, a DUMMY tag is introduced in the preterminal position. Hence, the flattened semantic tag sequence is finally expanded to the semantic tag sequence as in Table 1(b).
 Table 1: Abstract semantic annotation and its flat-tened semantic tag sequence.
 Sen tence: I want to return to Dallas on Thursday. Annotation: RETURN(TOLOC(CITY(Dallas)) (a ) Flattened semantic tag list: RETURN RETURN+TOLOC RETURN+TOLOC+CITY(Dallas)
RETURN+ON RETURN+ON+DATE(Thursday) (b) Expanded semantic tag list: RETURN RETURN+DUMMY RETURN+TOLOC RETURN+TOLOC+DUMMY RETURN+TOLOC+CITY(Dallas) RETURN+TOLOC+CITY(Dallas)+DUMMY RETURN+ON RETURN+ON+DUMMY RETURN+ON+DATE(Thursday)
RETURN+ON+DATE(Thursday)+DUMMY
We first need to calculate the most likely semantic tag se-quence  X  C for each sentence S = ( s 1  X   X   X  s T ),  X  C = argmax where F : S  X  C  X  R is a discriminant function and can be decomposed into two components, F ( S; C ) = F 1 ( S; C ) + F ( S; C ), where
Here, ( ; ) is considered as the co-efficient for the tran-sition from state (or semantic tag) to state while ( s l ; ) can be treated as the co-efficient for the emission of word s from state . They are defined as follows, the input patterns  X  between word s l and word s m i , the m th word in the training example i , i ( C ) is a set of dual parameters or Lagrange multiplier of the constraint asso-ciated with example i and semantic tag sequence C as in Equation 2. Using the results derived in Equations 5 and 6, Viterbi decoding can be performed to generate the best semantic tag sequence.

To incorporate the constraints as defined in the abstract semantic annotations, the values of ( ; ) and ( s l ; ) are modified for each sentence, ( ; ) = ( s l ; ) = where g ( ; ) and h ( ; s l ) are defined as follows, g ( ; ) = h ( ; s l ) = g ( ; ) and h ( ; s l ) in fact encodes the two constraints im-plied from abstract annotations. Firstly, state transitions are only allowed if both incoming and outgoing states are listed in the semantic annotation defined for the sentence. Secondly, if there is a lexical item attached to a preterminal tag of a flattened semantic tag, that semantic tag must ap-pear bound to that lexical item in the training annotation. For example, in the annotation shown in Table 1,  X  X allas X  be-longs to a lexical class  X  X ITY X . Hence, it can only be tagged with semantic tags containing a preterminal tag  X  X ITY X .
For each sentence, the semantic tag sequences generated in the Expectation step are further processed based on a measure on the agreement of the semantic tag sequence T = { t 1 ; t 2 ; :::; t n } with its corresponding abstract seman-tic annotation A . The score of T is defined as number of the semantic tags in T which also occur in A , n is the number of semantic tags in T , and p is the number of semantic tags in the flattened semantic tag sequence for A . The score is similar to the F-measure which is the har-monic mean of precision and recall. It essentially measures the agreement of the generated semantic tag sequence with the abstract semantic annotation. We filter out sentences with their score below certain predefined threshold and the remaining sentences together with their generated semantic tag sequences are fed into the next Maximization step. In our experiments, we empirically set the threshold to 0.1.
Given the filtered training examples from the Filtering step, the parameters w are adjusted so that the true se-mantic tag sequence C i scores higher than all the other tag sequences C  X  C i := C\ C i with a large margin. To achieve the goal, the optimization problem as stated in Equation 1 is solved using an online learning approach as described in [1]. In short, it works as follows, a pattern sequence S i is pre-sented and the optimal semantic tag sequence  X  C i = f ( S computed by employing Viterbi decoding. If  X  C i is correct, no update is performed. Otherwise, the weight vector w is updated based on the difference from the true semantic tag sequence  X  X  =  X ( S i ;  X  C i )  X   X ( S i ; C i ).
Experiments have been conducted on the DARPA Com-municator data 1 which were collected in 461 days. From these, 46 days were randomly selected for use as test set data and the remainder were used for training. After clean-ing up the data, the training set consist of 12702 utterances while the test set contains 1178 utterances.

The abstract semantic annotations used for training only list a set of valid semantic tags and the dominance rela-tionships between them without considering the actual real-ized semantic tag sequence or attempting to identify explicit word/concept pairs. Thus, it avoids the need for expensive tree-bank style annotations. For example, for the sentence  X  Show me ights from Boston to New York  X , the abstract an-notation would be FLIGHT(FROMLOC(CITY) TOLOC(CITY)) .

To evaluate the performance of the model, a reference frame structure was derived for every test set sentence con-sisting of slot/value pairs. An example of a reference frame is:
P erformance was then measured in terms of F-measure on slot/value pairs, which combines the precision (P) and recall (R) values with equal weight and is defined as F = ( P + R ) = 2 P R . We modified the open source SV M hmm 2 train and test the HM-SVMs on abstract annotations. h ttp://www.bltek.com/spoken-dialog-systems/ cu-communicator.html http://www.cs.cornell.edu/people/tj/svm_light/ svm_hmm.html F X  X easure Fi gure 2: Comparison of performance on models learned with feature sets chosen based on different window sizes. F X  X easure With Filtering Step Fi gure 3: Comparisons of performance with or with-out the Filtering stage.

We employed word features (such as current word, previ-ous word, and next word, etc) and part-of-speech (POS) fea-tures (such as current POS tag, previous one, and next one, etc) for training. To explore the impact of the choices of fea-tures, we explored with feature sets comprising of words or POS tags occurring before or after the current word within some predefined window size. Figure 2 shows the perfor-mance of our proposed approach with the window size vary-ing between 0 and 3. Surprisingly, the model learned with feature set chosen by setting window size 0 gives the best overall performance. Varying window size between 1 and 3 only impacts the convergence rate and does not lead to any performance difference at the end of the learning procedure.
In a second set of experiments, we compare the perfor-mance with or without the Filtering step as discussed in Section 3.3. Figure 3 shows that the Filtering step is indeed crucial as it boosted the performance by nearly 3%. We compare the performance of HM-SVMs with HVS and CRFs, all trained on abstract semantic annotations. The HVS model [4] was previously proposed based on the hy-pothesis that a suitably constrained hierarchical model may be trainable without treebank data whilst simultaneously retaining sufficient ability to capture the hierarchical struc-ture need to robustly extract task domain semantics. Such a constrained hierarchical model can be conveniently im-plemented using the HVS model which extends the HMM model by expanding each state to encode the stack of a push-down automaton. While it is hard to incorporate arbi-trary input features to HVS learning, both HM-SVMs and CRFs have the capability to deal with overlapping features. A learning approach of training CRFs from abstract anno-tations was previously proposed in [7]. Table 2 shows that HM-SVMs outperforms both HVS and CRFs with a relative error reduction of 43.3% and 10.6% being achieved respec-tively. The superior performance of HM-SVMs over CRFs shows the advantage of HM-SVMs on learning non-linear discriminant functions via kernel functions.
 Table 2: Overall comparison with other models.

In this paper, we proposed an effective learning approach which can train HM-SVMs without the expensive annotated data. It takes as input a training set of sentences labeled with abstract semantic annotations encoding underlying em-bedded structural relations and automatically induces deriva-tion rules that map sentences to semantic meaning repre-sentation. We evaluated the performance of our proposed learning approach on the DARPA Communicator Data and showed that it outperforms two other models, HVS and CRFs, also trained on abstract annotations. [1] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden [2] R. Ge and R. Mooney. Learning a Compositional [3] T. Grenager, D. Klein, and C. D. Manning.
 [4] Y. He and S. Young. Semantic processing using the [5] G. S. Mann and A. Mccallum. Generalized expectation [6] L. Zettlemoyer and M. Collins. Learning to map [7] D. Zhou and Y. He. Learning conditional random fields
