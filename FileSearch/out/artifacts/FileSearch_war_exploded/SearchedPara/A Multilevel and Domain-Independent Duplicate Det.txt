 Scientific instruments and computer simulations are obtaining and generating massive data in the domains of astronomy, oceanography, geognosy, meteorology, biomedical and so on, creating an explosion of available data to scientists. The data volumes are cleaning is adopted to improve the data quality, by detecting and removing the errors and inconsistencies from data [2]. 
Duplicate detection is the approach that identifies multiple representations of a same real-world object. It is a crucial task in data cleaning, and it has applies in many scenarios such as data integration, customer relationship management, and personal information management [3]. The duplicate problem has been studied extensively [6], or reference reconciliation [7], too names but a few. In the scientific database, the duplicate problem is caused by multiple importing or combining same dataset, same bers. Since many works have been done on data de-duplicate in recent years, few of them is suitable for scientific database. Our motivations and related works are intro-duced from following aspects:  X  Numerical Data: Most of duplicates are approximate dupli cate problem, sometimes it is also named as merge/purge, de-duplicate, record linkage problems [4, 8, 5]. In previous domain-independent methods for duplicate detection, the similarity is usually measured by duplicates [4, 5, 8]. However, scientific data mainly consist of numbers and symbols. These approaches are suitable for the textual data well but not the numerical data. 
Firstly , the ranges of scientific attributes are quite different, the classical measures will be effected by the attributes with larger value mostly. For example, the Euclidean trolled by the weight because its value is great larger than the blood_sugar_concentra-Mahalanobis distance, but it is only suitable to the attributes in normal distribution[12], and expensive especially dealing with high-dimensions (many attributes). Weight-ing[13] is also the solution but it is domain-dependent, it is hard to predefine the suit-able weights for all kinds of data in different domains. 
Secondly , the classical measures are used to measure the vector whose elements each word. In a scientific database, the numerical is the most common type in scien-tific database, but different numerical attribute has different meaning, dimensionality and significance (detailed explanation in section 2 C1 ). Cosine metric is also not suit-attributes is not treated as a vector, instead that, the similarity of each attribute is cal-method. So the problem is shifted to the similarity measuring of the simple attribute. The data is more dissimilar where the distance is more larger. But when the dissimi-cerned. 
Fourthly , "loss of precision" is the most common reason which lead to data dupli-cate, Scale is the number of digits to the right of the decimal point, any decimal can 10 -scale . The same data may be presented in different scale because the precision is lost sion" problem. For example the body_temperature is decimal with maximum two same value in different precision , on the contrary, 37.62 and 37.69 ( d =0.07) possibly measure can not deal the duplicate caused by inconsistent precision. 
All in all , the classical similarity measures are not suitable to detect the numerical into the similarity of the records, meanwhile the difference of the range and duplicate possibility of each attribute are also considered.  X  Multilevel Detection: Most duplicate detection algorithms apply to a single relation with sufficient attributes should also be well considered. For example, two patient s even with the same name , more complex than detection in a single table. Some previous works such as [16, 17] considered the relationships between entities in a XML file, but not in database. The challenges are, first, proposing a similarity measuring approach for the record (vector) which are connected by parent -child or top -sub relationships. 
Delphi [18] is the most related to this paper. There are several differences between the proposed model and Delphi , for example, Delphi is general algorithm for textual data. As far as the multilevel detection is concerned, both of this paper and [18] con-potential rules which are hidden in the relationships between records, such as different state should belong to different country. But in scientific database, such relationships keys, the batch of connected records in pare nt table and child tables are possibly both on the relationships.  X  Domain-independent Researches in this area mostly focus on the efficiency and effectiveness of algorithms. model. In this paper, the generalization of the model is highlight. 
All challenges mentioned above are studied in this paper, and the Multilevel Du-plicate Detection Model (MD 2 M) is proposed. MD 2 M is concluded from several pro-jects. It focuses on the domain-independent, proposes the similarity of numerical been successfully applied in two projects about the management of oceanographic data and biomedical data. Theories and experiments show that MD 2 M is more gen-eral, effective and efficient. summarized as the preliminaries and foundations of MD 2 M:  X  C1: Characteristics of the Data. Decimal values are the mostly common data to present the scientific phenomenon, other data types such as string, datetime and binary, but these data types are not popu-duplicate detection because they are primar y and enough to present the feature of the record. Nominal data is the special numerical data, MD 2 M will not consider its simi-larity because it is enumerative and has large possibility to be duplicate. 
The scientific database is massive. Firstly, the data volume is enough to shown the statistical and distributed feature of the values. The column is fully filled by the val-also high dimensional. In similarity measuring, the record of the table is often treated as a vector, so that values of columns are dimensions of the vector. Records in scien-tific database are high dimensional vectors even only numerical columns are included. medical database, each column present a constituent of the blood. Even more, the calculating the similarity of record (vector) directly. 
In scientific database, there are three kinds of numerical duplicate. The exactly integration; Tiny difference data are probably duplicate, because the different quality cate too, it is caused by rounding or losi ng the precision when gathering, preprocess-ing and integrating the data.  X  C2: Multilevel Detection hierarchy. duplicates, it always impliedly refers to the concept itself and its sub-concepts, but not the table hierarchy in once detection is a sub-schema of the whole schema, it satisfies:  X  Because only table and its sub-tables are considered, there are no multiple par- X  There are one-to-one or one-to-many relationships among the tables, many-to- X  The uniform algorithm can traverse all involved tables in hierarchies. Fig. 1 shows an example of hierarchical struct ure, the right part of Fig. 1 is a simpli-fied schema of scientific database from real project. If table A is selected for duplicate detection, four involved tables are shown in left part of Fig. 1 as a hierarchy. 
Characteristic C1 and C2 are concluded by investigating the scientific data of many domains, they are not exceptions but the common groups. Base on these common characteristics, the duplicate detection model is proposed in the rest of the paper, and finally the experiments analysis and case study proved that the proposed model meet these characteristics well. MD 2 M includes several elements and algorithms. In the section, the similarities tion of these similarity measures are also introduced. 3.1 Similarity some definitions as following: Definition 1 (Comparison Column). Comparison column is used to test whether two parison column c includes some attributes:  X  c.max means the maximum value of c in table t ;  X  c.min means the minimum value of c in table t ;  X  c.size means the data amount of c in table t;  X  c.scale means the maximum retained decimal place (number of digits to the right of the decimal point) of c. The comparison column s are selected and their attribute values are assigned by query-ing the metadata of the table and column. It is performed automatically on each table. for excluding nominal data. The numerical column is selected as comparison column if: equation 1, the numerator presents the number of possible value of column c , and the denominator is number of records in c . As we know, the data is more denseness, the potentially duplicate is more great. Coefficient p c can be treated as density of values if the data is sufficient and fully distributed. It is impossible to evaluate the precise den-sity of every column, so p c is used as an approximate possibility of the duplicate. The column whose p c is far less than one is nominal, dense, and/or less precision column, (0.0  X  24.9) of the patient s are not suitable to be a comparison column . Definition 2 (Relative Distance). Relative distance is used to measure the distance of mals, the relative distance Rd ( Va c , Vb c ) is calculated by: precision, the denominator is the maximum distance of the values in column c under tance, and further more, removes the range difference of each comparison column . On the other hand, rounding mechanism make sure that the distance is constrained to the sion" are include. Equation 2 is simplified as =c.scale .
 Definition 3 (Column Similarity). Column similarity measures the similar degree of comparison column c is calculated by the following function: C.size is number of comparison column in C, then : sonable because the column similarity is the dimensionless value. In fact, the record equivalent by dividing the same number. The range of column similarity is ] (0,1 . Definition 5 (Record Set Similarity). Record set similarity measures the similar degree of two record set in a same table, let A and B are two record sets which con-definition 6) . the record set similarity is : Record set similarity is the similarity of two vector arrays, it can be explained as the too". The duplication of two records is defined next. If there is no duplicate in A and B themselves, then N A = N B = N , equation 6 is simplified as : 3.2 Duplication means duplicate, function Sub () means getting the referred set:  X  Sub ( table ) returns the table set that refer to the table ;  X  Sub ( record, table ) returns the record set that refer to the record in the table , Definition 6 (Record Duplication). Two records a and b are duplicate iff themselves and their sub record sets are all duplicate. Definition 7. Record Set Duplication: Record sets A and B are duplicate iff Noticing the definition 6 is used when Similarity ( A , B ) is calculated, so that the defi-nition 6 and 7 are recursive definition. definitions in section 3. Firstly, there are two different approaches of duplicate detec-considered as a search problem, it checks wh ether the given records duplicate with the rithm by changing the scope of input data and regroup some process steps, so in the rest of the paper, only complete algorithm is focused. 4.1 Pairs or Cluster proaches: one approach detects duplicates pairwisely and then clusters duplicate pairs not be used in scientific data duplication. 
The "clustering" approach rely on an equivalence relation  X  of set S . To the dupli-duplicate detection can not rely on the appr oach of "clustering", duplicate records can any "clustering" approach. So the record pair and candidate set are defined: than  X  t are combined as a tuple &lt; a, b &gt;, named as record pair.
 Definition 9. Candidate Set: The candidate set Cs is the set of records pair s in the target table, it is candidate of th e next-level duplicate detection. The candidate set of the root table in table hierarchy will be the final results of dupli-hierarchy, they are results of current-level detection and inputs of next-level detection. 4.2 Initial Detection Initial detection intends to find the candidate set, in root table of table hierarchy. The pairwise comparison is performed as Algorithm 1. 4.3 Multilevel Detection data in A and B themselves: whether there are more than Line 6 to 14 find all similar record pair s by parallel comparison of two records set. the simulation in section 4.3, Cs '. Size  X  invocation. The experiments are performed on a PC server with 2.8 Hz Pentium 4 CPU, windows server 2003 operation system and Oralce10g database. Artificial data is used to com-pare the proposed similarity measure and other classic measures The experiments are cords are prepared, and then th e duplicate is introduced by:  X 
Repeatedly Import: all records are copied and re-import to the database;  X 
Tiny Difference: four columns are selected randomly, and a tiny value is added to each column of all records, then re-impor ted to the database. The new value is cal-culated as v c ' = v c + random [0,4] X 10 -vc . scale .  X 
Inconsistent Precision : same as the duplicate introduction of " Tiny Difference " but v c ' = round ( v c , random (0, v c . scale ] ). After introducing the duplicate, the data is doubled. The approximate duplicate rate is cosine metric approach even the threshold is 0.99, the recall is large but precision is tween MD 2 M and Euclidean distance is focused. 
Both two approaches are based on the dist ance. In Euclidean distance approach, the threshold is also assigned as minimum distance (10 -c.scale ) of comparison column . It is really duplicate or not in the artificial data. We support all the detected record pair s are duplicate and evaluate recall instead of precision . The results are shown as Fig 3:  X 
Repeatedly Import: Both two approaches successfully detect all the repeatedly import records which are exactly same, recall is 0.5;  X 
Tiny Difference: Recall of MD 2 M is about 0.37 because the floating value is duced duplicate are missed. But recall of Euclidean distance is 0.24. After investi-gating the results, we find if four randomly selected columns NOT include column no matter how larger the four floating values are. On the contrary, if floating values duplicate no matter how larger the rest two values are. Obviously, Euclidean dis-tance is controlled by the distance of larger elements, the similarity of small scale columns are hidden by larger scale ones.  X 
Inconsistent Precision : Recall of MD 2 M is 0.5, MD 2 M is designed for detected the duplicate caused by inconsistent precision. Recall of Euclidean distance is only 0.07 , it misses most duplicate, except the values are very close and only one scale of precision is lost, for example 54.321 and 54.32, Euclidean distance approach treat these values as "tiny difference" duplicate. The performance of two approaches is simila r (see Fig. 4), the executed time is almost linear to the data volume in both approaches, duplicate rate affect performance a little. MD 2 M has good executed performance. In this paper, the Multilevel Duplicate Detection Model (MD 2 M) for scientific data is presented and implemented. The primary works of this research are following five aspects:  X  Analyze and conclude the duplicate related characteristics of scientific database,  X  Propose the domain-independent duplicate detection model for scientific data- X  Propose the algorithms for multilevel duplicate detection in scientific database;  X  Prove the model is domain-independent, effective and efficient through some Generally, compared with previous studies, MD 2 M proposed a domain-independent, multilevel duplicate detection approach and a numerical duplicate detection model for scientific database. Theories and experiments show that MD 2 M can assure the quality of the data and further improve the accuracy of data analysis and mining. All of these other formats, and the duplicate detection based on non-referred tables should be also considered in further. 
