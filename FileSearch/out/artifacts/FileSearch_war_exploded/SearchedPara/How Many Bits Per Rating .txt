 Most recommender systems assume user ratings accurately represent user preferences. However, prior research shows that user ratings are imperfect and noisy. Moreover, this noise limits the measurable predictive power of any recommender system. We propose an information theoretic framework for quantifying the preference information contained in ratings and predictions. We computationally explore the properties of our model and apply our framework to estimate the effi-ciency of different rating scales for real world datasets. We then estimate how the amount of information predictions give to users is related to the scale ratings are collected on. Our findings suggest a tradeoff in rating scale granularity: while previous research indicates that coarse scales (such as thumbs up / thumbs down) take less time, we find that rat-ings with these scales provide less predictive value to users. We introduce a new measure, preference bits per second, to quantitatively reconcile this tradeoff.
 H.1.2 [ Information Systems ]: User/Machine Systems X  Human information processing ; H.5.2 [ Information inter-faces and presentation ]: User Interfaces X  Evaluation / methodology Design, Measurement, theory ratings, recommender systems, information theory, evalua-tion, metrics
Collaborative filtering (CF) recommender systems rely on ratings to understand user preferences and make recommen-dations. While past research has focused on algorithms that use ratings to accurately predict user preferences [6], more recent work has begun to explore the quality of the ratings themselves [1, 2, 3, 14, 15, 21]. Researchers believe that in-consistencies in user ratings limits CF prediction accuracy. First suggested by Hill et al. [12], this limit has been named the  X  X agic barrier X  [11]. CF systems that understand and adapt to noise in user ratings may be able to raise the magic barrier and generate more accurate recommendations.
CF system researchers have explored various ways of un-derstanding and reducing noise in ratings. O X  X ahony [15] defines two types of noise that can appear in ratings and affect the quality of predictions: natural noise and malicious noise . Natural noise is noise associated with how a recom-mender system collects user preferences. Malicious noise is biased noise purposely introduced into a recommender sys-tem in order to affect predictions. Several researchers have measured natural noise. Both Hill et al. and Cosley et al. measured natural noise by asking users to re-rate items after a period of time has passed [12, 3]. Using this methodology both studies found evidence of noise in user ratings. Amatri-ain et al. analyzed how characteristics such as rating value, time elapsed, and rating speed affect user noise [1] and found that ratings are a good psychological measure according to classical test theory. Following this work Amatrian et al. pro-posed an algorithm that collects multiple ratings to reduce noise and increase recommendation quality [2].

Psychology research suggests that a CF system X  X  choice of rating scale may crucially affect the level of rating noise. In work that inspires our approach, Garner uses information theory to measure how much information different rating scales transmit [8]. While he generally finds that increases in scale granularity reduce rating noise, this effect is limited by the  X  X iscriminability inherent in the particular rated stimuli. X  This limitation has been observed in practice on the website YouTube. Prior to 2010, YouTube users could rate movies on a five star scale. However in 2009 YouTube found that almost all ratings were either one or five stars, with the other ratings rarely used. By 2010, YouTube switched to a like/dislike scale, having seen no value in keeping the five-star scale [22].
Although recommender system research has not directly studied how rating scales affect rating noise, they have ex-plored other issues related to rating scales. Cosley et al. [3] found that users preferred using a relatively fine-grained 5-star rating scale with half star increments. They found evidence that the more descriptive rating scale improved pre-diction quality, but were hesitant to draw strong conclusions. Sparling et al. [21] investigated how cognitive load and rating time varies with rating scale. They found that more granular rating scales require more time, and suggested that system designers may face a fundamental tradeoff between a coarse rating scale that is relatively quick and noisy, or a granular rating scale that is slower with less noise.

We seek to provide system designers with tools to address the rating quality / rating quantity tradeoff raised by Sparling et al. We propose the preference bits framework to capture the amount of user preference information in ratings and predictions. We ground the framework in an information theoretic model of human preference, and then explore the model X  X  properties. We apply the preference bit framework to answer three research questions: RQ1: How much preference information is captured by rat-RQ2: How much preference information is provided by pre-RQ3: How should a system designer reconcile the tradeoffs
The reminder of this paper is organized as follows: first we discuss the preference bits framework, followed by an evaluation of how the framework can be used to measure rating and prediction quality. Finally, we discuss limitations and future work.
In order to address our research questions, we need some method of measuring the quality of information entering and exiting a recommender system. To do this, we introduce the preference bits framework for measuring the quantity of information about user preferences. Before we can describe the framework, we must first define preference and introduce the information theoretic concepts it relies on.
There is a spectrum of views concerning how preferences are formed and what happens when we measure them [7]. At one end of the spectrum, the philosophy of articulated values holds that humans form well reasoned, stable preferences for most things they have experienced. Variance in elicited answers in the philosophy of articulated values is said to come from subtle differences in understanding of questions leading to measuring different preferences. At the other end of the spectrum, the philosophy of basic values holds that humans store a small number of opinions or beliefs from which they derive momentary preferences when needed [7]. Variance in elicited answers in the philosophy of basic values is said to come from priming effects, mood, and other psychological phenomena affecting the process used to derive preference.
This paper assumes an intermediate model. We assume that humans can form stable preferences for all items in the item domain. However, we also assume that these preferences are only partially articulated, and that processing is still required when mapping these partial preferences to a rating.
Claude Shannon developed information theory as a way of reasoning about information [20]. One of the fundamental measures of information is called entropy , which measures how hard a random variable is to predict. Entropy is defined as follows where X is a random variable and P ( x ) denotes the probability mass function of X .
 We measure entropy with bits where one bit is the amount of information in a fair coin flip.

The mutual information between two random variables is a symmetric measure of how much information the value of one random variable gives us about the value of the other. Mutual information is defined as follows where X and Y are both random variables: We measure mutual information with bits where one bit of mutual information between two variables X and Y means we expect knowing X to reduce the entropy in Y by one bit. Mutual information ranges from zero when the two variables are independent, to the smaller of the two variables X  entropy when one variable entirely explains the other.

As an example consider the relationship between ratings ( R ) on the movie Titanic and the gender ( G ) of the rater. Using information from the MovieLens rating system we can compute I ( G ; R ) = 0 . 011 which would not significantly help us predict the rating given the user X  X  gender, or the user X  X  gender given their rating.

One important result about mutual information is the data processing inequality [4]. The data processing inequality applies for any three random variables X , Y , and Z such that X and Z are conditionally independent on Y . This means that knowing Y and Z gives us no more information about X than just knowing Y , formally P ( X | Y,Z ) = P ( X | Y ). When this property holds, the data processing inequality shows: We will use the data processing inequality to derive measures of the amount of information about user preferences that enters and exits the recommender.
To measure preference bits of any factor, we will use in-formation theory. We define the random variable  X ( u,i ) to represent the user u  X  X  unknown partial preference for an item i . Because this preference is only partially articulated, we do not assume users will express their preference perfectly. Therefore one  X ( u,i ) may lead to several different ratings.
The preference bits of any variable X is I ( X ;  X ), the amount of information X gives us about  X . Of particu-lar interest to us is the input preference bits and the output preference bits defined as I ( R ;  X ) and I ( P ;  X ) respectively. Input preference bits measures how much we expect a rating R ( u,i ) to reduce the recommender X  X  uncertainty about the user X  X  preference. Output preference bits measures how much we expect a prediction P ( u,i ) to reduce a user X  X  uncertainty about her (potentially unformed) preference.

While the preference bit framework provides logical ways to measure the quality of recommender input and output, we cannot directly measure the metrics it produces due to our assumption that preferences are not fully articulated. To allow measurement of these metrics we will prove the following theorem Figure 1: Graphical model of input preference bit measurement.
 Theorem 1. Let X 6 = R,  X  be any random variable: Theorem 1 follows directly from our assumption that prefer-ence  X ( u,i ) is the only measurable variable responsible for R ( u,i ). This means that X and R must be conditionally inde-pendent on  X . Therefore by the data processing inequality, X tells us less about our ratings than it does about preferences. While theorem 1 does not give us an exact measure of the preference bits in X , it can give us a reasonable proxy.
Measuring input preference bits: We would like to derive a measure for the input preference bits ( I ( R ;  X )) of an interface. To overcome the problems with directly measuring preference we will collect multiple ratings on one item. Let R 1 and R 2 represent two ratings for the same item and user drawn independently from P ( R |  X ) for preference  X . We call such pairs of ratings re-ratings . Figure 1 shows this model. Because we assume that  X  is the only factor determining ratings, any information between R 1 and R 2 must come from information between  X  and any single rating. Formally, I ( R 1 ; R 2 )  X  I ( R ;  X ) because R 1 and R 2 are both samples of R therefore we can apply theorem 1 with R 1 as R and R 2 as X to get I ( R 1 ; R 2 ) tells us how much we expect a user X  X  rating for an item will reduce uncertainty about the user X  X  future ratings on that item. We will use this measure as an estimate for input preference bits per rating for the rest of this paper.
Assumptions and limitations: To show the connection between I ( R ;  X ) and I ( R 1 ; R 2 ), we relied on two assump-tions that have bearing on how re-ratings should be collected. First, our assumption that two ratings were drawn indepen-dently implies that when a user is asked to re-rate an item, they should not remember their previous rating. This can be accomplished by waiting a period of time before asking a user to re-rate. We also assume that preferences are constant and do not drift over time, but this has been shown to be false [1]. To minimize this effect, re-ratings should be collected in a relatively short time span. Any practical experimental design for collecting multiple ratings must balance these two as-sumptions, inserting an appropriate amount of time between ratings. Because the effects of violating our assumptions will be time dependent, experiments should be designed to place consistent amounts of time between ratings for all conditions. In practice, we expect that as the time differential between re-ratings increases the amount of measured preference bits per rating will decrease. 1
To understand how rating scale affects preference bits per rating, we built a probabilistic model of a rating system. Using this model we considered the implications of changing rating scale on true and measured preference bits per rating. We then used this model to assess a strategy for measuring I ( R 1 ; R 2 ) in practice.
 Figure 2 shows a diagram of the model. The true preference  X  of a user is modeled as a beta distribution with parameters  X  and  X  bucketed into m T bins. The beta distribution is a natural choice for modeling a random variable over the unit range. By varying  X  and  X  many qualitatively different distributions can be achieved. m T represents the true scale upon which users internally evaluate an item. To pick a user X  X  value for  X , we draw a random value from the beta distribution and take the value of the center of the bucket containing it.

We model the input rating scale as the unit scale divided into m R buckets. To generate R from  X  we calculate  X , add noise from a normal distribution N (  X  =  X  , X  ) and choose the rating bucket (between 1 and m R ) that contains the result. Because a normal distribution is not clamped to the unit range, we treat the smallest and highest rating as extending to negative and positive infinity respectively. This allows us to partially account for the extra stability of extreme ratings noticed in prior research [2]. From P ( X ) and P ( R |  X ) we can compute P ( R,  X ) and P ( R 1 ,R 2 ).

We first consider the relationship between the true pref-erence bits per rating (Pbpr) and the rating scale. Figure 3 shows that as we increase the rating scale we find an asymp-totic increase in preference bits per rating. We found that the asymptote reached is a function of the preference scale and the amount of noise. As the preference scale increases, we see logarithmic increase in the maximum Pbpr. Likewise,
We confirmed this empirically with Xavier Amatriain X  X  dataset [1] where we see a decrease in preference bits per rating from 1.1227 with less than 15 day delay to 0.9757 with a more than 15 day delay. Figure 3: Pbpr rating increases as the size of the rating scale increases (  X  = 3 ,  X  = 1 , m T = 50 ,  X  = 0 . 05 ) when we increase noise the maximum Pbpr decreases. We found this result holds for a wide range of  X  and  X  values. This result suggests that there is some number of ratings after which we cannot gain more information by increasing the resolution of our rating interface.
 Next we will use our model to assess the accuracy of I ( R 1 ; R 2 ) as a measure of I ( R ;  X ). For I ( R a useful measure of I ( R ;  X ), we want I ( R 1 ; R 2 ) to corre-late with I ( R ;  X ). To evaluate this we compute I ( R 1 and I ( R ;  X ) for a range of model parameters for prefer-ence distribution (  X , X   X  X  0 . 5 , 1 , 2 , ... , 6 } ), internal and in-put scales ( m T ,m R  X  { 2 , 3 , 4 , 5 , 10 , 15 , ... , 60 } ) and noise dependency between I ( R 1 ; R 2 ) and I ( R ;  X ) ( R 2 = 0 . 984). Therefore, we expect I ( R 1 ; R 2 ) to be a good proxy measure of the true preference bits per rating.

Finally, we used our model to assess our strategy for mea-suring I ( R 1 ; R 2 ) on sampled data. We use the Miller-Madow method to estimate mutual information [13]. We estimate the joint probability P ( R 1 ,R 2 ) as the number of times the two rating value co-occurred divided by the number of ratings. Where m R 1 and m R 2 are the number of possible values for R 1 and R 2 ( m r in our case), the Miller-Madow estimator is
X The second term corrects for the first order of magnitude in our estimate and substantially improves our estimation. Un-fortunately, it has been shown [16] that there is no unbiased estimator for mutual information, therefore it is important that large sample sizes ( N &gt;&gt; m 2 R ) be used to avoid estima-tor bias. This requirement will make measuring input Pbpr difficult on a per user basis, or for high precision interfaces. While more complex estimators for mutual information may reduce these issues [16], we will focus only on large datasets for this paper.

We used our model to perform a simulation study to eval-uate the performance of the Miller-Madow estimator on re-rating data. We discovered that noise was consistently less than bias, and that the correction term can over-correct when noise is small. We found that a sample size of 1000 was suffi-cient to reduce bias and standard deviation to less than 0.1 bits for rating scales with up to 20 points. This can be used as a guideline for a preferred sample size.
Next we use real-world datasets to evaluate the amount of input preference bits in rating inputs. These evaluations then serve as guidelines when we discuss the trade-offs when choosing different rating interfaces.

Datasets: The perfect dataset for this analysis would be a large dataset with re-ratings performed on a variety of rating interfaces. Unfortunately, no such dataset exists. However, we can approximate this dataset using only single ratings on several interfaces with a historical set of ratings to compare against. Therefore we will use the SCALES dataset from Cosley et al. [3]. As can be see in table 1 this dataset is large enough to avoid significant estimator bias but has relatively few users. 2 Therefore we consider these only preliminary re-sults until a more complete dataset can be collected. Ratings in this dataset were entered on three rating interfaces: a 2-point thumbs up / thumbs down, a 6-point scale from -3 to +3 with no zero, and a 10-point 5-star scale with half stars. Re-ratings are made by pairing these user X  X  original ratings (made on a five star interface) with their new ratings. This gives us a way to measure input preference bits. Note, however, that these re-ratings were not entered on the same scale. While our measure is still a lower bound on true input bits for either interface, this does add the confound that some of the three new scales may map to the original interface better than others regardless of actual preference bits.
The 2-point, 6-point, and 10-point scale yielded 0.423, 0.825, and 0.813 input preference bits respectively. As expected the 2-point interface captured much less preference information than the other two interfaces. Surprisingly, the amount of information gained with the 10-point interface was less than the amount gained with the 6-point interface. This contradicts our previous results and suggests that the 6-point interface is more efficient at collecting preference information than the five star interface. This may be because the lack of a zero option actually reduces noise in ratings, but it may also be an artifact of our measure. Because our re-ratings are not taken on the same interface this is at best evidence that the 6-point scale may be better. A follow up study with proper methodology should be performed before a conclusion is reached.

Preference bits per second: One reason why the 6-point interface may have a higher preference bits per rating is because the lack of a zero forces users to think more carefully before rating. While this would lead to more information per rating, it X  X  not clear if it would lead to more overall user information. If the extra information means each rating takes longer this interface could be less efficient overall. Because user attention is a limited resource [10], a general-purpose recommender system should balance the value of information gathered with the value of user attention.

To directly address the quality/effort tradeoff we introduce rating time into our analysis and compute preference bits per second (Pbps). Sparling et al. [21] found that the time required to rate is the best available measure of mental effort. Therefore Pbps is a way to balance the value of information with the value of user effort. We compute Pbps for an interface as Pbpr for that interface divided by the number of seconds per rating on that interface.
The original analysis of the scales dataset implies that only 2795 ratings were collected [3]. We contacted the lead author, it appears that the 431 remaining ratings were excluded from the analysis there. Figure 4: Graphical model of the output preference bit measurement.

Table 2 lists the average time per rating Sparling et al. report [21]. Because they do not asses the 10 or 20 point scales, the listed values are estimated by linear interpolation. We do not estimate a time for the 6-point scale because it is not clear how this interface compares with a traditional six star rating scale. Combining the times per rating from table 2 with our measured Pbpr for the thumbs and five star with half star interfaces we get 0 . 423 3 . 91 4 . 33 = 0 . 1878 respectively. While this is a rough estimate, is suggests that, despite the faster ratings, the binary scale gives us information slower than the five star interface. This is evidence that the five star interface with half stars matches how users think about movies better than a thumbs interface.
Measuring output preference bits: We would like to derive a measure for the output preference bits I ( P ;  X ). Ide-ally, we would compute this value directly, however, as in the last analysis, a user X  X  true preference is hidden. Therefore, we analyze the relationship between a user X  X  rating and the rec-ommender X  X  prediction for that rating. Let P ( u,i ) represent the recommender X  X  prediction for user u on item i . Figure 4 shows the relationship between preference, prediction, and ratings. If a prediction gives us information about an un-known rating, that prediction should also give us information about the unknown preference that formed the rating. For-mally I ( P ; R ) is a lower bound to the output preference bits using theorem 1 with P as X .
 I ( P ; R ) tells us how much we expect a recommender X  X  pre-diction to reduce uncertainty about the user X  X  rating. We will use this measure as an estimate of the output preference bits per prediction for the rest of this paper.
We apply our methods to compare the efficiency of rating interfaces with different scales.

Datasets: The perfect dataset for this analysis would be large enough to generate meaningful predictions and it would ask the same users to rate the same items on several different rating scales. Unfortunately, no such dataset exists. Re-rating datasets such as Cosley et al X  X  are too small to provide insights into predictive accuracy [3]. Larger datasets with different rating scales for the same items (i.e. Netflix and MovieLens) represent different users and applications, making it difficult to compare results. Therefore, we use two relatively high-precision datasets (MovieLens 10M [18] and Jester [9]) to create synthetic datasets with different scales. Table 1 summarizes both datasets. The Jester dataset is very dense and uses a continuous ratings scale from -10.0 to 10.0. The MovieLens 10M dataset employs a scale from 0.5 to 5.0 with half point precision.

Assumptions and limitations: To show the connection between I ( P ; R ) and I ( P ;  X ) we assumed that prediction and ratings are conditionally independent on preference. This means that prediction is not a noise factor, and should have no ability to effect ratings. Unfortunately, this is not true in practice [3]. The Jester interface does not display predictions, and therefore does not suffer from this. Because the Movie-Lens interface does display predictions, our measured output preference bits may overestimate true output preference bits on the MovieLens dataset.

Choice of scales: In this analysis we must consider three separate rating scales. Users of each system generated ratings using the system X  X  native scale (0.5 to 5.0 for MovieLens and -10.0 to 10.0 for Jester). Since we want to analyze the efficiency of different scales, we generate synthetic datasets for different input scale s whose precision is less than or equal to the native scale. The procedure for generating the mapping between native scale and input scale is outlined in the next section. Finally, we incorporate a separate prediction output scale for two reasons. First, this is common in practice; Netflix only supports whole-star rating input but displays tenths of a star for predicted rating outputs. Second, we found that requiring each input scale to be paired with the same output scale unfairly penalized input scales with less precision.

We considered a variety of input scales. For MovieLens, we chose 10-point (native), 5-point, and 2-point because they are common in practice. The choice for Jester was more complex because the native scale was continuous and our mutual information metric requires a discrete scale. We chose 100-point (for comparison against Sparling et al. X  X  data [21]), 20-point (buckets of length 1.0) and 10-point, 5-point, and 2-point to enable comparison with the MovieLens dataset. We consider a variety of output scales for both datasets: 2-point, 5-point, 10-point, 20-point, and 100-point.
Methodology: Our analysis uses the LensKit recom-mender software package [5] with five-fold cross validation. We use an item-item CF recommendation algorithm [17] with recommended parameter settings. Finally, we compute I ( R ; P ) using the Miller-Madow estimator described in sec-tion 3.1.

We constructed synthetic datasets that map a native scale to a variety of input scales using a greedy search over possible mappings from native scale to input scale. We initialized the best mapping to a uniform mapping where each input rating represents roughly the same number of native rating values. We evaluated the preference bits captured by predictions with this mapping. We then generated two possible candi-date mappings for each threshold between input scale rating bins: one mapping that moves the threshold one native rating Table 2: Mean rating time per item in seconds for each scale and domain based on Sparling et al. Rat-ing times with a * indicate interpolated values. to the left and one that moves it to the right. We evaluated the preference bits captured by each of these candidate map-pings and recursively repeated this procedure until the best mapping could not be improved. Because this process reduces the number of rating values in our datasets, it also reduces our maximum input preference bits. Therefore, we expect more granular scales to produce higher quality output than coarser scales.

Results: Figure 5 shows the relationship between input scale, output scale, and output preference bits for MovieLens (left) and Jester (right). Each line corresponds to a different input rating scale. The x-axis corresponds to different pre-diction output scales, and the y-axis shows output preference bits. For example, in Jester, the 5-point input scale (green line) and 10-point output scale (x = 10) gives 0.24 output preference bits. In general, increases in input scale granular-ity up to 10 (orange) lead to increases in output preference bits. For Jester, the 20-point input scale (purple) performs slightly worse, and the 100-point scale (red) performs sig-nificantly worse. Since the native scale for the MovieLens dataset is 10-point, we do not know whether performance would decrease for 10-point and 20-point input scales. This suggests that there is a  X  X weet spot X  in rating scale gran-ularity that balances between theoretical maximum input bits and natural rating noise. The prediction output scale (X axis) seems to reach a plateau when it is twice the granularity of the input scale. For example, in the Jester dataset the curve for the 2-point input scale (blue) levels off dramatically at the 5-point output scale. Increases in output rating scale precision never seem to lead to a meaningful decrease in preference bits. Therefore, we consistently use the 100 point output scale in subsequent analysis.
We have shown that a relatively granular 10-point rating scale seems to capture more preference bits for both rating input and prediction output. This does not, however, tell the whole story. Because user attention is finite [10], and rating on high precision scales takes longer [21], we again face a tradeoff between quality of ratings and the effort needed to obtain them.

This tradeoff is especially relevant with new users. System designers seek to provide new users with valuable predictions as quickly as possible -a challenge often referred to as the new user problem. To explore how a rating scale affects this challenge, we break down the previous results according to how many items a test user has rated. More precisely, for each test and train set in the cross fold validation, we build the algorithm X  X  predictive model on the entire training set. We play each test user X  X  ratings forward chronologically and measure the preference output bits in the withheld ratings. This analysis allows us to compare output preference bits per rating (output Pbpr) for different input scales.
To directly address the quantity / quality rating trade-off raised by Sparling et al. [21] we calculate the output preference bits per second (output Pbps).

Table 2 lists the rating times Sparling et al. report. We use the rating times reported for product reviews as rating times for jokes because in both domains users form an opinion after reading text on the page. Although these assumptions may decrease the accuracy of our analysis, we believe they are close enough to shed light on the relative differences between rating scales.

Figure 6 shows the results for Pbpr and Pbps for MovieLens (left) and Jester (right). 3 Each line corresponds to an input scale. The x-axis shows total rating seconds for a test user calculated by multiplying the number of ratings for a user by the time per rating for that scale. The vertical dotted lines indicate the rating times for a user X  X  n X  X h rating (n = 1, 2, ...). It is interesting to note that for Jester, while the 20-point scale (purple) clearly beats the 5-point scale (green) in the previous analysis (Figure 5), when we incorporate rating time into the analysis they essentially draw even.
Figure 6 clearly illustrates tradeoffs a designer might con-sider. For example, consider the times at which the 5-point and 2-point scales reach 0.28 preference bits for the Jester dataset. The 5-point scale gives 0.28 preference bits at ap-proximately 5 ratings or 82 seconds. The binary scale meets the same level of preference bits at approximately 41 ratings or 634 seconds. The Pbps of the 5-point scale ( 0 . 28 82 and 2-point scale ( 0 . 28 634 = . 0004) clearly indicates that the 5-point scale is more efficient.

The testing set in our analysis was constant size. This corresponds to an assumption that recommender system will eventually capture the same quantity of historical ratings under different scales, despite the varying amounts of time required to generate those ratings. We also tested an assump-tion of fixed amount of user attention, resulting in smaller training datasets for systems with higher precision scales. However, because rating times only vary slightly across scales, our results were essentially identical to Figure 6.
We close our discussion of output preference bits by com-paring it to other normalized measures of prediction quality. CF researchers most commonly measure prediction accuracy using root mean square error (RMSE) and mean absolute error (MAE) [11]. One attractive aspect of output prefer-ence bits is that it is scale free. If everything else is held constant, two scales with rating points associated with differ-ent numeric values should yield the same values for output preference bits. Although the standard versions of MAE and RMSE are not scale free, normalized versions of both metrics (NMAE, NRMSE) simply divide the typical measures by the range of the rating scale [19].

We compared output preference bits (for the 100-point output scale), NMAE, and NRMSE across all the analysis we conducted in this section: each rating scale, dataset (Movie-lens vs Jester), etc. In total, this yields 168 different datasets and observed values for each metric. These 168 observations are not independent because they are based on the same two datasets. However, this is the same general approach
The output preference bit values for Jester are generally higher in Figure 6 than 5 because the median number of ratings in Jester is only15 ratings left to right.
 Herlocker et al. used to evaluate accuracy metrics in [11], and it should provide some intuition.
 All three metrics are highly correlated across datasets. However, we found the correlations between NMAE and NRMSE to be much larger (0.98) than the correlations be-tween output preference bits and the other two metrics (-0.73 for each). In addition, the standard version of mutual infor-mation and the version with the Miller-Maddow correction exhibited a very high correlation (0.99). Table 3: Correlation between preference bits, MAE, and RMSE controlling for input scales.
 We wondered whether the strong correlations between NMAE and NRMSE were due to their shared normalization procedure. To explore this, we analyzed metric correlations within each input scale and averaged the results across all input scales. Table 3 shows the mean correlation across all input scales for each metric. Note that because the normaliz-ing term is constant given scale, the within-scale correlations for NMAE and NRMSE will be identical to those for MAE and RMSE. The correlation between NMAE and RMSE decreases from 0.98 to 0.94, while the correlations between output preference bits and MAE (-0.92) and RMSE (-0.94) both increase.

These findings suggest that when comparing within an input scale, output preference bits, MAE, and RMSE provide similar evaluations and any one might be preferred. However, across input scales, output preference bits seems to capture different characteristics from NRMSE and NMAE. Because of the theoretical grounding and inherently scale-free nature of output preference bits, we believe output preference bits will make reasoning about differences between rating scales easier, and should therefore be preferred. However, these findings should be verified on other datasets.
Natural noise in user ratings can decrease the accuracy of recommender systems. We develop the preference bits framework to help system designers measure, understand, and reduce noise in recommender systems. We derive measures of the preference bits in ratings and predictions to measure the amount of information users give the a recommender system, and how much information they receive in return. We show how this framework can be used to compare different rating interfaces and find the one that is most efficient.
To look at how noise effects predictions we introduce a technique for reducing the precision of a rating dataset to produce synthetic datasets. This technique enables us to analyze the amount of information we can give to users if we have them rate on different scales. Using these datasets we confirmed ratings on a higher precision rating scale im-prove predictive accuracy. Future research should verify that these findings with the synthetic datasets holds for natural datasets with different scales. A full user study could reveal insights into the ways users might change their mental rating process for different scales. For example, users may think more carefully about the distinction between an up and down rating than the distinction between 4.5 and 5.0 stars.
To capture the tradeoff between the cost and quality of rating systems we introduced the preference bits per second metric. While we believe this metric is a major step forward, future research can refine it more. For example, the metric should be extended to account for user satisfaction because a good interface is worthless if users do not want to use it. Ultimately, reducing noise should always be balanced against the desire of the users.

We investigated the effect of different rating scales on the preference bits per rating. While we focused on rating scale, many other aspects of a recommender system may affect the amount of noise in ratings. For example, tags displayed alongside a movie ( X  X ilent movie X ,  X 1920s X ) may either reduce noise by aiding recall, or, increase noise by biasing ratings. Work is still needed to understand how these types of contextual information reduce noise in ratings. This work has been supported by the National Science Foundation under grants IIS-0964697, IIS 10-17697, IIS 09-68483. [1] X. Amatriain, J. Pujol, and N. Oliver. I like it... i like it [2] X. Amatriain, J. M. Pujol, N. Tintarev, and N. Oliver. [3] D. Cosley, S. K. Lam, I. Albert, J. A. Konstan, and [4] T. M. Cover and J. A. Thomas. Elements of [5] M. Ekstrand, M. Ludwig, J. Konstan, and J. Riedl. [6] M. D. Ekstrand, J. T. Riedl, and J. A. Konstan. [7] B. Fischhoff. Value elicitation: is there anything in [8] W. R. Garner. Rating scales, discriminability, and [9] K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. [10] F. Harper, X. Li, Y. Chen, and J. Konstan. An [11] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. [12] W. Hill, L. Stead, M. Rosenstein, and G. Furnas. [13] G. Miller. Note on the bias of information estimates. [14] M. P. O X  X ahony, N. J. Hurley, N. Kushmerick, and [15] M. P. O X  X ahony, N. J. Hurley, and G. C. Silvestre. [16] L. Paninski. Estimation of entropy and mutual [17] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [18] S. Sen, J. Vig, and J. Riedl. Tagommenders: [19] G. Shani and A. Gunawardana. Evaluating [20] C. Shannon. A mathematical theory of communication. [21] E. I. Sparling and S. Sen. Rating: How difficult is it? In [22] Youtube. New video page launches for all users
