
Outlier or anomaly detection refers to the task of identifying abnormal or inconsistent patterns from a dataset. While they may seem to be undesirable entities, identifying them has many potential applications in fraud and intrusion detection, financial market analy-sis, medical research and safety-critical vehicle health management. Broadly speaking, outliers can be detected using either supervised or semi-supervised or unsuper-vised techniques [13] [5]. Unsupervised techniques, as the name suggests, do not require labeled instances for detecting outliers. In this category, the most popular ones are the distance-based and density based tech-niques. The basic idea of these techniques is that outliers are points in low density regions or those which are far from other points. In their seminal work, Knorr et al. [15] proposed a distance-based outlier detection technique based on the idea of nearest neighbors. The naive solution has a quadratic time complexity since every data point needs to be compared to every other to find the nearest neighbors. To overcome this, researchers have proposed several techniques such as the work by Angiulli and Pizzuti [1], Ramaswamy et al. [17], and Bay and Schwabacher [2]. Density-based outlier detection schemes, on the other hand, flag a point as an outlier if the point is in a low density region. The density of a point can be evaluated using several tech-niques such as the ones proposed in [12]. Supervised techniques require labeled instances of both normal and abnormal operation data for first building a model ( e.g. a classifier) and then testing if an unknown data point is a normal one or an outlier. The model can be probabilistic such as Bayesian inference [9] or deterministic such as decision trees, Support Vector Machines (SVMs) and neural networks [14]. Semi-supervised techniques only require labeled instances of normal data. Hence they are more widely applicable than the fully supervised ones. These techniques build models of normal data and then flag as outliers all those points which do not fit the model.

Since this paper proposes a variant of unsupervised anomaly detection technique using support vector ma-chines, we discuss more about this here. Support vector machines [21] [7] have been widely used for clas-sification and regression. While the original idea of using SVM has been around for many years, recent interest has been kindled by the need for analyzing large datasets. Fehr et al. [10] presents a scheme for efficient learning of SVMs based on the intuition that most of the training time for non-linear SVMs is wasted in evaluating the kernel matrix. In their approach, they approximate a single SVM using a collection of simpler linear SVMs. Each of these simpler ones can be trained and tested in constant time, leading to low running time without any loss of accuracy. Such a construction can be viewed as a tree in which any intermediate node represents a hyper-plane and the leaf nodes correspond to pure labels of one class type.

Burges and Sch  X  olkopf [4] present a different tech-nique for speeding up SVMs. Let be the normal to the decision surface where  X  the Lagrange multipliers corresponding to the support vectors s j , y the kernel function, and N support vectors. This computation scales linearly with the number of support vectors. To achieve speedup, the authors propose to approximate the normal using fewer support vectors ( N The goal is then to minimize the L2-norm of the two normal vectors As has been shown in [4], there exists nontrivial values of
The work most closely related to this one is the reduced support vector machine (RSVM) idea presented in [16] and [6]. In these, an initial SVM is trained not on the entire training set, but rather on a subset of the training set called the active training set. Then, the SVM is evaluated on a validation set. If the accuracy is accept-able, the algorithm converges, else a set of misclassified points are selected from the remaining training set and added to the active training set. The approach in [6] first sorts the misclassified points according to their scores on the validation set and then divides the points into equal size subsets. When additional points are needed, it selects new points from each subset. In our approach we do not sort the points and thereby achieve lower running time.

The proposed  X  -Anomica algorithm is faster than the standard benchmark one class SVMs while preserving the accuracy. It achieves this by developing the hy-perplane in an incremental fashion. We show that, in many cases,  X  -Anomica has similar prediction accuracy compared to classical one class SVM while reducing the running time dramatically. Our main contributions in this paper are:  X  We propose a variant of one class SVM-based  X  We demonstrate the capability of the algorithm in  X  We measure the performance of the proposed tech- X  We provide some useful insights regarding the
One class SVMs, an unsupervised learning method for estimating the density of the target support objects was introduced by Sch  X  olkopf [18]. Throughout this paper, we have considered positive labeled data points as normal and negative label data points as outliers. The model consists of a parameter  X  that denotes the maximum allowance of outliers in the training data. The idea is to draw a separating hyperplane that can separate these outliers from the rest of the training examples, as shown in Fig. 1. Unlike the 2  X  class SVMs classifier, in one class SVMs model, the separating hyperplane is constructed using positive labeled training data set only. Since a N  X  1 dimensional hyperplane can exist in the N -dimensional feature space, the primary task is to find the optimal separating plane that maximizes the margin between the hyperplane and the origin, which is the lone representative of the second class with negative label. A. The Model
We assume a set of labeled training data D = { ( ~x i ) } n i =1 in the input space R , where ~x i further assume that there exists a function  X  that can be used to map variables from the input space to the feature space F , i.e.  X  : R d  X  F . In feature space the inner product h x i , x j i property, where x i :=  X  ( x holds. Also Cover X  X  theorem [21] states that nonsep-arable or nonlinearly separable features in the input space R is more likely to be linearly separable in the feature space F , provided the transformation  X  ( . ) nonlinear and the dimensionality of the feature space is high enough. While evaluating the dot product in the feature space, the explicit calculation using  X  can be avoided by simply evaluating the kernel function i.e. to hold, this the chosen inner-product kernel must satisfy Mercer X  X  theorem [3]. For the majority of this paper, we 2 have used Radial Basis Function (RBF) kernel (Eqn. 1) that evaluates the distances between data points as, where || . || denotes the Euclidean norm and  X  defines the kernel width.

Sch  X  olkopf [18] showed that in the high dimensional feature space it is possible to construct an optimal hy-perplane by maximizing the margin between the origin and the hyperplane in the feature space by solving the following primal optimization problem, minimize P ( w ,  X ,  X  subject to ( w . X  ( x where  X  is an user specified parameter that defines the upper bound on the training error, and also the lower bound on the fraction of training points that are support vectors,  X  is the non-zero slack variable,  X  is the offset,  X  ( x i ) represents the transformed image of x i in the Euclidean space and i  X  [  X  ] . Throughout this study, we will use the scaled version [8] of the dual problem which takes the form of, minimize Q = 1 subject to 0  X   X  where  X  solution must satisfy the exact Karush-Kuhn-Tucker (KKT) conditions which can be summarized as, where g ( ~x  X  can be recovered from the constraint of the primal problem by exploiting the solution w and pattern x corresponding to 0 &lt;  X  equality condition. There exist at least  X  X  training points with non-zero Lagrangian multipliers ( ~ X  ) and these points { x Let I I nm = { i :  X  i = 1 } multipliers corresponding to non-SVs, marginal and non-marginal support vectors respectively. Once ~ X  is known, SVMs compute the following decision function. f ( ~x j ) = sign ( X
If the decision function predicts a negative label for a given test point x classified as outlier. Test examples with positive labels are considered as normal.
 B. Virtual Decision Surface
The decision boundary is defined by a normal vec-tor w (also referred as weight vector) is orthogonal to the plane and an offset  X  . All points x lying on this hyperplane must satisfy g ( x )  X   X  = 0 where { g ( x ) = w . x ,  X  w  X  F} . Since the weight vector is a weighted sum of the features corresponding to the support vectors, one may be motivated to define two normal vectors  X  and  X  both perpendicular to the decision plane such that, where  X  too difficult to prove that, where  X  to normal vectors  X  and  X  . This is because the distance of the hyperplane from the origin remains unchanged a fixed test point z , the ratio of the decision values evaluated using two different normal vectors (defined by two different sets of points) orthogonal to the same hyperplane is constant. This can further be expressed as, where  X  is a constant, f decision functions (Eqn. 5) expressed in terms of Sup-port Vectors corresponding to Lagrange X  X  multiplier  X  and  X   X  I construction of the weight vector does not depend on the number of support vectors. It is well known that the positive semidefiniteness of the dual problem may result in redundant support vectors which defines the normal 3 vector. This means that some of the support vectors are a linear combination of other support vectors and implies that the removal of some of these linearly dependent support vectors will not change the hyperplane. In previous work [4], Burges and Sch  X  olkopf pointed out that the solution of the SVMs may not be the sparsest one and suggested ways of approximating the solution using virtual Support Vectors. For one-class SVMs, the existence of the parameter  X  may be the source that introduces redundancies in the solution because it leads to a minimum required number of support vectors. In this research we are motivated to develop a scheme that searches for a reduced set of the transformed features in F which is sufficiently close to approximate the normal vector of the exact solution of one-class SVMs and thus retaining the same accuracy with lower running time.  X  -Anomica proposes an approximate solution that permits one-class SVMs to train on huge data sets in much reduced time. The main idea of this algorithm is to start with an initial  X  X easible solution X  of classical one-class model trained on a very reduced data set and guide the current solution towards the  X  X arget solution X . Here the solution of the optimal hyperplane from the exact solution is set as the target. To achieve this goal, a controlled updating of the existing training pool with new examples in an iterative fashion has been adopted. In order to select the appropriate subset of new examples, we propose a two stage strategy. In the first step, we ensure that at each iteration the solution of the most updated model is along the direction of the optimal solution. Secondly, at each step the number of new members which control the step length is decided based on some model feedback. The work presented here exploits the fact that the  X  parameter of one-class SVMs plays a very important role in defining the highest allowable fraction of misclassification of the training data. This means the one-class model, once built, should be able to correctly classify 1  X   X  fraction of the entire training set as normal examples. For the rest of the paper we will refer to this as the  X   X  -criterion X . Any newly developed model (based on a subset of the entire data set) which is a close approximation to the exact solution is bound to meet the  X   X  -criterion X . Such a data set can be considered as a representative working set.

In the following, we will demonstrate the core idea of the proposed algorithm in steps. The  X  -Anomica algorithm (Algorithm 1) starts with the assumption that two non overlapping data sets have been randomly chosen from the same distribution. One of these two sets was assigned for training purpose while the second set was kept for validation purpose. The model also assumes that the optimal value of the kernel parameter  X  (Eqn. 1) has already been evaluated for a fixed  X  Under this condition, if a standard one-class model is successfully built on the entire training set, the model should satisfy the  X   X  -criterion X .
 Algorithm 1 Anomica
In the proposed technique, we start by randomly selecting a small subset from the entire training set and using this small subset to develop the initial One-Class SVMs model. Once the SVs are obtained, we validate the resulting model on the validation set. Since the current model is based on a very small subset of the entire training set, the classification accuracy of the model may not satisfy the  X   X  -criterion X  on the hold out set. This is based on the fact that a correct model should be able to achieve the same level of classification accuracy (in this case 1  X   X  because of  X   X  -criterion X ) on a hold out set which has been generated from a similar distribution to that of the training set. Here it is important to note that the proposed algorithm uses the  X   X  -criterion X  as the target classification rate.
If the classification rate on the validation set is greater than (1  X   X  ) , it means that either the small subset of the training set has fewer positive examples or that the data points corresponding to the support vectors of this model are not good representative of the positive 4 examples. This is analogous to saying that the most recently evaluated support vectors have defined a normal vector ( w ) corresponding to a hyperplane (Fig. 2-b) that predicts too many positive members in the hold out set and thus does not satisfy the  X  -criterion. Similarly, if classification rate is less than (1  X   X  ) , it implies that the current working set has too few negative examples (Fig. 2-a). Hence there is a necessity to update the initial working set with additional positive or negative examples only when any of the above two situations arises. Pseudo code of our algorithm for doing this is shown in Algorithm 2. This procedure is repeated until the  X  -criterion is satisfied or close to being satisfied on the hold out set. The number of examples (positive or negative) to be selected from the entire remaining set is governed by a penalized weight function as shown in line 5 of the pseudo code (Algorithm 2), based on deviation of the classification rate on the validation set from the target (1  X   X  ) . Once the  X  -criterion on the hold out set is satisfied (Fig. 2-c), the algorithm meets the stopping criterion, and hence terminates.
 A. How does the  X  -criterion influence the model?
We will further illustrate the role of  X   X  -criterion X  by using a synthetic  X  one class  X  data set. The data set consists of samples drawn from a d -dimension Gaussian distribution with user specified mean ( ) and covariance ( X ) . For simplicity we will use a 2  X  dimensional data set drawn from a single distribution. We have chosen a linear kernel in the SVMs model to do the mapping. Algorithm 2 UpdateMember( E With a fixed number of instances, the redundancies in the data set were controlled by varying the covariance of the distribution. In the first run, two data sets each of 1001 instances were generated from a distribution with same mean ( 0 . 001 ) but with two very different covariances. For one set the covariance was set to  X  X achine precision X  (eps) which is the minimum al-lowable spacing between two floating point numbers and 10 20  X  eps for the other set. The outcome of the One class SVMs model (with  X  = 0 . 1 ) on these two data sets has been summarized in Table I. It can be observed that even though the redundancies are varying widely from one set to the other, the total number of support vectors still remains the same because of the  X  -criterion. Hence there is a possibility that the  X  parameter may introduce redundancies in the solution.

The algorithm  X   X  Anomica described in the earlier sections is an extension of the classical One-Class SVMs. It has been shown that for both these methods 5 the fundamental optimization procedure is exactly the same. In the following we will present interesting study on how these two techniques may produce a different outcome and try to provide some insight on what makes them different.

We also included a separate experiment where both  X   X  Anomica and classical one class SVMs were de-veloped on the same data set and the corresponding SVs were noted. Each support vector obtained by the classical approach was evaluated using the same hyper-plane constructed by the exact solution itself and the hyperplane constructed by the approximate solution. In Fig. 3, scores for the support vectors from both solutions have been compared. The plots represent the absolute values of the original scores, sorted in descending order. With normalization, these scores almost lie on the top of each other. This is because the decision values for both these method will be proportional (Eqn. 8).
In this study, we have chosen two systems health management related data sets and one real-world as-tronomical data set as benchmark applications. These data sets represent diverse training set sizes, and input dimensionality and therefore builds a good platform to test the accuracy and scalability of these algorithms. Table II summarizes the characteristics of the data sets used for the experiments. Both one class SVMs and  X   X  Anomica algorithms have been tested on a Dual core Pentium4 computer running Windows XP with 4 GByte of memory. The current version of our algorithms is based on the OSU SVM Classifier Toolbox (ver. 3.00) is an adaptation from the LIBSVM and uses Sequential Minimal Optimization (SMO) for solving the quadratic problem (Eqn. 3). To test these algorithms, nonlinear RBF kernel was used and the optimal setting of the kernel parameter was determined using the method described in [20]. In addition to that, it should be noted that for all analysis using  X   X  Anomica the size of the initial subset is chosen to be 15% of the entire training set. However this parameter can vary depending on the problem size.

We first experiment with the emulated OPAD [19] (Optical Plume Anomaly Detection) data which is a set of time varying spectra profiles measured by an optical plume analysis in liquid propulsion engines. A second set of experiments were conducted on Sloan Digital Sky Survey (SDSS) photometry data (SDSS of our algorithms. The Commercial Modular Aero-Propulsion System Simulation (CMAPSS) data set has been used for the final set of analysis. The CMAPSS is a high fidelity system level engine simulation software for simulating user-specified transient engine behavior under normal and faulty conditions over flights. Detailed background on the CMAPSS framework can be found in [11]. The above data sets were split into non-overlapping training, validation and test sets as shown in table II.
 Baseline results were obtained by running one-class SVMs model and compared with those obtained from  X  -Anomica on the above data sets. Three sets of results were reported for analyzing the correct classification accuracy, sensitivity and time complexity of these algo-rithms. For CMAPSS data set, we will only summarize the outcomes of the analysis due to space limitations. A. Run Time Analysis of the  X   X  Anomica
Figures 4(a) shows the resulting training times for exact solution and  X  -Anomica with five different sizes of training set on OPAD data. The exact solution uses the entire training set in all cases.  X  -Anomica starts with an initial model built on a small subset of the entire training data set and updates the training set as it progresses towards the target (1  X   X  ) classification rate on the validation set. In Fig. 4(a), we show the 6 mean training time over 50 runs for varying training sizes and their corresponding error bars. It is clear that with fewer training points the difference in training time for exact solution and  X  -Anomica is low. As the size of the training data set increases, the computing time increases drastically for exact solution, however  X  -Anomica shows much better performance. Table III presents the performance of these algorithms on the SDSS. It can be observed that the proposed technique outperforms one-class SVM model for all the test cases and the performance gain factor increases with increasing training set size. In Fig. 4(b), we present the time required to evaluate the OPAD test sets. As the number of SVs increase the resultant test time proportionally increases and this particular trend can be seen in the plot. Since  X  -Anomica requires fewer SVs while building a model, the test time is lower compared to the classical approach. On SDSS data set, with 275k training and 130k test instances,  X  -Anomica is on an average approximately 15 times faster than the classical method. With increasing training instances such as with CMAPSS data,  X  -Anomica consistently performs on average 18 times faster with 500k training and 100k test instances.
 B. Classification Accuracy and Prediction Performance
It could be of real interest to find out if the com-putational advantage of  X  -Anomica trades off with the detector X  X  ability to match the classification accuracy of the exact solution of one class SVMs. Figure 5 shows a comparison of the detection rates of both algorithms and these results were obtained on the same test set while the sizes of the training sets were varied. It can be seen that  X  -Anomica overall provides similar accuracies when compared to one-class SVM but computed with much reduced training times. As the training size increases, the models get more accurate and as a result the clas-sification rate of both the model gets more closer and 7 consistent. This is because introducing more training examples brings in additional useful information that aid correct detection and classification.
Now we present an analysis on predicting the  X  X ut-lierness X  of new unseen patterns. Figure 6 indicates that  X  -Anomica ranked the points in terms of their  X  X utlierness X  comparably to classical one-class SVMs. This can be observed from the plot where both one-class SVMs and  X  -Anomica have been used to predict a set of outliers in an unlabeled data set and their corre-sponding outlier scores were compared. These outliers were sorted based on the absolute values of their scores and thereafter normalized. Finally, to investigate the accuracy in separating the sequence of outliers from normal patterns, ROC analysis on the predictions of  X  -Anomica was accomplished and the area under the ROC (AUC) was computed for each run. Here we have assumed that the sequence of outliers detected by one-class SVMs are the ground truth. Results obtained show that  X  -Anomica consistently performed well in detecting the presence of these outliers and for each case the AUC was very close to 1 .

In this paper, we presented a new method for faster anomaly detection using a modified one-class SVMs. Compared to classical one-class SVM all our experi-ments showed a competitive speedup (up to factor 15-18 on these data sets). The proposed method reduces the number of the operations needed to compute a reduced and near optimal training set. The model developed on this working set is a close approximation of the exact solution and can be represented with much less number of SVs. Hence both training time and test time is significantly reduced. However  X  -Anomica can achieve very close classification accuracies (losing less than 1% in most cases) compared to one-class SVMs. The paper demonstrates the preliminary success of the proposed method on a wide variety of data sets. Also from all the experimental observations we find that the model converges in finite number of iterations which ensures that the cardinality of the final training set is always less than the cardinality of the entire training set. We note that the current version of the paper doesn X  X  have a theoretical upper bound on the number of support vectors but we intend to consider this in our future research.
 8
