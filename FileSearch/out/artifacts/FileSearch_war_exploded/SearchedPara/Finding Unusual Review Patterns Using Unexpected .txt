 In recent years, opinion mining attr acted a great deal of research attention. However, limited work has been done on detecting opinion spam (or fake reviews) . The problem is analogous to spam in Web search [1, 9 11]. However, review spam is harder to detect because it is very hard, if not impossible, to recognize fake reviews by manually reading them [2]. This paper deals with a restricted problem, i.e., identifying unusual review patterns which can represent suspicious behaviors of reviewers. We formulate the problem as finding unexpected ru les. The technique is domain independent. Using the techniqu e, we analyzed an Amazon.com review dataset and found many une xpected rules and rule groups which indicate spam activities. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Information filtering .
 Algorithms, Experimentation Reviewer behavior, review spam, unexpected patterns With the rapid growth of online reviews, it has become a common practice for people to read revi ews for many purposes. This gives good incentives for review spam , which refers to writing fake reviews to mislead readers or automated systems by giving bogus positive or negative opinions to some target objects to promote them or to damage their reputati ons. Detecting spam reviews is a critical problem for opinion mining [6] and retrieval [8]. The problem can be seen as a classification problem with two classes, spam and not-s pam. However, to obtain training data by manually labeling reviews is very hard as a spammer can easily craft a fake review that is just like any innocent review [2]. In [2], a learning method using duplicate reviews as positive training data is used, but many not duplicated reviews can be spam too. Some researchers also study the helpfulness of reviews [7, 12], but review spam is a different concept. In [4], a user study shows that rating behaviors are good indicators of spam. In this paper, we study a restricted problem, identifying review patterns representing unusual beha viors of reviewers, which can indicate spam activities [4]. For example, if a reviewer wrote all negative reviews on products of a brand but other reviewers are generally positive about the brand, this reviewer is clearly a spam suspect. To find unusual behaviors, the conventional approach is to write an application specific heuristic program to find such behaviors. However, this is not desirable. It is much better to propose a general framework for solving this class of problems so that the resulting system can also be applied to other domains. This paper proposes such an appr oach and shows that the problem can be formulated as finding une xpected rules/patterns from data. The data that we use consists of a set of data records, which are described by a set of normal attributes A = { A from the attributes in A and c i is a class in C . Such a rule gives the conditional probability of Pr( c i | X ) (called the confidence ) and the joint probability Pr( X , c i ) (called the support ) [5]. For our application, the data can be produced as follows: Each review forms a data record with a set of attributes, e.g., reviewer-id , brand-id , product-id , and a class . The class represents the opinion of the reviewer, positive , negative or neutral based on the review rating. In most review sites (e.g., amazon.com), each review has a rating between 1 (low est) to 5 (highest) assigned by its reviewer. We can assign the rating of 4 and 5 as positive, 3 as gives all positive ratings to a particular brand of products. The issue is how we know a rule represents an abnormal behavior of a reviewer. To decide that, we need to know what is expected. This paper first defines several ty pes of expectations based on the natural distribution of the data. It then proposes the corresponding unexpectedness measures to rank rules. This method is domain independent as it only depends on the data and the type of rules but not the application. It can thus be applied to other domains. In our experimental study, we repo rt a case study of discovering suspicious behaviors of revi ewers based on Amazon reviews, which indicate spam activities or at least biased reviewers. For a case study on a different domain dataset, see [3]. Unexpectedness is defined as de viation from expectations. Thus, the definition of expectations is th e key. Before the definitions, we give some more notations about the data and the resulting rules. Given a data set D , let the domain or the set of possible values of attribute A j be dom ( A j ). We use the data to mine class association attribute value pair: A j =v jk ( v jk  X  dom ( A j )). CAR mining finds all rules that satisfy the user-given minimum support and minimum confidence constraints. However, we cannot use them because they create holes in the rule space, and remove the context. Here holes m ean those rules that do not meet the support and confidence cons traints. However, without minimum support and minimum confidence to ensure the feasibility of comput ation, CAR mining can cause combinatorial explosion [5]. Fortunately, practica l applications have shown that users are interested in almost only short rules as it is hard to perform any action on long rules due to low data coverage. Thus, in our system, we focus on mining rules with only 1-3 conditions if their support and confidence is greater than zero. Approach to defining expectations : Our approach begins by assuming the knowledge of cl ass prior probabilities (Pr( c can be easily found from the data automatically. They give us the natural distribution of the data to begin with. Two additional principles govern the defi nition of expectations: 1. Given no prior knowledge, we expect that the data attributes 2. We use shorter rules to comput e the expectations of longer Based on these two principles, we begin with the discussion of unexpectedness of one-c ondition rules, and then two-condition rules. For multi-condition rules, see [3]. We define four types of unexpect edness. A one-condition rule is a rule with only one condition (an attribute value pair, A We want to determine how unexpect ed the confidence of a rule is. To simplify the notation, we use a single value v jk ( v to denote the k th value of attribute A j . A one-condition rule is thus of the following form: v jk  X  c i . The expected confidence of the rule is defined below. Expectation : Since we consider one-condition rules, we use the information from zero-condition rules to define expectations: which is the class prior probability of c i , i.e., Pr( c and no other knowledge, it is reasona ble to expect that attribute values and the classes are inde pendent. Thus, the confidence (Pr( c | v jk )) of the above rule ( v jk  X  c i ) is expected to be Pr( c E (Pr( c i | v jk )) to denote the expected confidence, i.e., Confidence Unexpectedness (Cu) : Confidence unexpectedness confidence to the expected conf idence given a support threshold  X  . Let the actual confidence of the rule be Pr( c Cu ( v jk  X  c i ) to denote the unexpectedness of the rule v Unexpectedness values can be used to rank rules. One may ask if the expected confidences are different. When we discuss two-condition rules in Section 2.2, we will see that even in the same class, a high confidence rule may be completely expected. Significance test : It is important to know whether the actual confidence is significantly different from the expectation, we use the statistical test for proportions. The confidence measure does not consider the proportion of data records involved, for which we need support unexpectedness. Expectation : Given no knowledge, we ex pect that an attribute value and a class are independent. Thus, we have Pr( v Pr( v jk )Pr( c i ). Pr( c i ) is known, but not Pr( v expect that it is the average probability of all values of A we have (Pr( v jk ) is unknown to the user, but is computed), Support Unexpectedness (Su) : Support unexpectedness of a rule is defined as follows, given a confidence threshold ensure that the rule has sufficient predictability): This definition of Su (Equations (3) and (4)) is reasonable as it ranks those rules with high supports high, which is what we want. Significance Test : The test for proportions can also be used here. Confidence or support unexpectedness considers only a single rule. In many cases, a group of rules together shows an interesting scenario. Here we define an un expectedness metr ic based on all values of an attribute and a class, which thus represent multiple rules. This unexpectedness shows how skewed the data records are for the class, i.e., whether the data records of the class concentrate on only a few values of the attribute or they spread evenly to all values, which is expected given no prior knowledge. For example, we may find that most positive reviews for a brand of products are from only one reviewer although there are a large number of reviewers who have reviewed products of the brand. This reviewer is clea rly a spam suspect. We use supports (or joint probabilities) to define attribut e distribution une xpectedness. Let the attribute be A j and the class of interest be c distribution of A j with respect to class c i is denoted by: It represents all the rules, v jk  X  c i , k = 1, 2, ..., | A the total number of values in dom ( A j ). Expectation : We can use the expected value of Pr( v computed above (Equation (3)) for our purpose here. Attribute Distribution Unexpectedness (ADu) : It is defined as the sum of normalized support de viations of all values of A We use Pr( c i ) in Equation (5) because ) Pr( ) , Pr( Note that in this definition nega tive deviations are not utilized because positive and negative deviations ( Dev ( v jk )) are symmetric or equal as Pr( c i ) is constant and ) Pr( ) , Pr( considering one side is sufficient. In this case, we want to discover how the values of an attribute can predict the classes. This is denoted by where A j represents all its values and C indicates all classes. Given no knowledge, our expe ctation is that A j and C are independent. In the ideal case (or the most unexpected case), every rule v has 100% confidence. Then, the values of A j can predict the classes in C completely. For example, we may find that a reviewer wrote only positive reviews to one brand, and only negative reviews to another brand, which is clearly suspicious. Conceptually, the idea is the same as measuring the discriminative power of each attribute in classification learning. The information gain measure can be used for the purpose. The expected information is computed based on entropy. Given no knowledge, the entropy of the original data D is (note that Pr( c confidence of the zero-condition rule on class c i ): Expectation : The expectation is the entropy of the data D : Attribute Unexpectedness (Au) : Attribute unexpectedness is defined as the information gained by adding the attribute A adding A j , we obtain the following entropy: subsets, D 1 , D 2 , ..., D | A A ). The unexpectedness is thus co mputed with (which is the information gain measure in [10]): We now consider two-condition rules. Although we can still assume that the expected confidence of a rule is the class prior appropriate as a two-condition rule is made up of two one-condition rules, which we already know. As mentioned earlier, it is possible that the unexpectedne ss of a two-condition rule is unexpectedness to the tw o-condition rule. For example, let us consider co nfidence unexpectedness. We have a data set with two classes and each class has 50% of the data, i.e., the class prior probabilities are equal, Pr( c 1 ) = Pr( c rule v 1  X  c 1 with 100% confidence (Pr( c 1 | v 1 unexpected based on Equation (2). Now let us look at a two-expected confidence should be 50%. Then, we say that this rule is highly unexpected. However, if we know v 1  X  c confidence for rule v 1 , v 2  X  c 1 is completely expected. The 100% confidence of rule v 1  X  c 1 is the cause for rule, v 1 the 100% confidence. More importa ntly, this example shows that ranking rules according to confidence unexpectedness is not equivalent to ranking rules purel y based to their confidences. With the knowledge of one-conditi on rules, we define different types of unexpectedness of tw o-condition rules of the form: We first compute the expected confidence of the two-condition rule based on two one-condition rules: Expectation: Given the confidences of the two rules, Pr( c and Pr( c i | v gh ), we compute the expected probability of Pr( c v ) using the Bayes X  rule and obtain: The first term of the numerator can be further written as Conditional independence assumption : With no prior knowledge, it is reasonable to expect that all attributes are conditionally independent given class c i . Formally, we expect that Based on Equation (10), the expected value of Pr( c i | v Since we know Pr( c i | v jk ) and Pr( c i | v gh ), we finally have: Confidence Unexpectedness (Cu) : As above, we first compute the expected support of v jk , v Expectation : The expected support Pr( v jk , v gh , c based on the following: Using the conditional independe nce assumption above, we know v ) based on the same assumption: By combining Equations (10) and (17), we obtain, Support Unexpectedness (Su): Since for two-condition rules, tw o attributes are involved. To compute attribute distri bution unexpectedness, we need to fix an attribute. Without loss of generality, we assume v include (or vary) all the values of attribute A g the unexpectedness of: This attribute distributi on represents all rules, v jk , v 2, ..., |A g | , where |A g | is the number of values of attribute A Expectation : We can make use of the expected value of Pr( v v , c i ) computed above in Equation (18). Attribute Distribution Unexpectedness (ADu) : In this case, we compute the une xpectedness of an attribute given a constraint, which is of the form: v jk , A g  X  C . Attribute unexpectedness can be defined easily (see [3]). We present a case study to show the effectiveness of the proposed system. We used reviews of manufactured products from Amazon crawled in 2006 [2]. The class attribute is the review rating. Three classes are made from the ratings. Table 1 shows the classes, ratings and class prior probabilities . Note that we only assigns the rating of 5 to positive, and assigns the rating of 4 to neutral as we want to study extreme behaviors of reviewers. Table 1. Classes, ratings and Table 2: The review data set Class ( c i ) Ratings Pr( c i ) # of Reviews 415179 In our data, each review forms a da ta record with three attributes have a total of 475K reviews, out of which 50K were written by anonymous reviewers, which were removed in our analysis. Then, we have about 415K data records. A brief description of the data set is given below in Table 2. Due to space limitations, we only show some findings of unexpected confidences and unexpected supports. For other results, see [3]. One-Condition Rules Confidence Unexpectedness : The top ranked rules show that out of 17863 reviewers with at least 3 reviews (support threshold), 4340 of them have the confidence of 1, meaning they always gave only one class of rating. Those reviewers who wrote only positive (2602 reviewers) and only negative (807 reviewers) reviews are somewhat suspicious or unexpect ed. Some may be involved in spam activities, writing fake review s. Since the negative class had the lowest expectation (Pr( negative ) = 0.24), the reviewers who wrote many negative reviews had the highest unexpectedness. For example, the top ranked reviewer wrote 16 all negative reviews (for rules with the same unexpectedness values, we also sort them using their supports). This reviewer is quite abnormal. Support Unexpectedness : In this case, people who write more reviews will have higher suppor t unexpectedness. The top ranked rule shows a particular reviewer wrote 626 reviews and all of them have positive ratings, which is highly unusual or suspicious. Two-Condition Rules Confidence Unexpectedness : Here we also found many unexpected/interesting rules. Alt hough in one-condition rules, we know that many people write a combination of positive, neutral and negative reviews, here we found many such reviewers actually wrote only pos itive or only negative reviews on some specific brands. This is suspicio us. For example, the top ranked reviewer wrote 27 positive reviews on products of a particular brand (confidence is 1 for positive class), while the expected confidence is only 0.45 as this re viewer wrote many other reviews with varied ratings (the averag e rating for this brand from all reviewers is only 3.6). There ar e hundreds of such reviewers. Support Unexpectedness : Since the data is sparse in the brands and the reviewers, the expected support of a reviewer writing on a brand is low. So, the support un expectedness is generally high. Using 80% as the confidence cutoff, the top ranked rule shows that a reviewer wrote 30 positive reviews for a particular brand. This paper studied the problem of identifying atypical behaviors of reviewers. The problem was fo rmulated as finding unexpected rules and rule groups. A set of exp ectations was defined, and their corresponding unexpectedness measures were proposed. Unexpected rules and groups represent abnormal or unusual behaviors of reviewers, which i ndicate spam activities. In our experiment, we reported a ca se study using reviews from Amazon.com, where we found many suspicious reviewers. [1] Gyongyi, Z. and Garcia-Molina, H. Web Spam Taxonomy . [2] Jindal, N, Liu, B, Opinion spam and analysis. WSDM , 2008. [3] Jindal, N., Liu, B. and Lim, E-P. Finding atypical review [4] Lim, E-P., Nguyen, V-A., Jindal, N., Liu, B. and Lauw, H. W. [5] Liu, B., Hsu W., and Ma Y. Integrating classification and [7] Liu, J. Cao, Y. Lin, C. Huang, Y. Zhou, M. Low-quality product [8] MacDonald, C. Ounis, I, and So boroff, I. Overview of the [9] Ntoulas, A., Najork, M., Manasse M., Fetterly, D. Detecting [10] Quinlan J.R. C4.5: Programs for Machine Learning . 1993. [11] Wu, B., Goel V. &amp; Davison, B. D. Topical TrustRank: using [12] Zhang, Z. and Varadarajan, B. Utility scoring of product 
