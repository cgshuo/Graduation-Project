 Cyril Allauzen allauzen@google.com Google Research, New York, NY Mehryar Mohri mohri@cs.nyu.edu Ameet Talwalkar ameet@cs.nyu.edu Courant Institute of Mathematical Sciences, New York, NY Identifying the minimal gene set required to sustain life is of crucial importance for understanding the fun-damental requirements for cellular life and for select-ing therapeutic drug targets. Gene knockout stud-ies and RNA interference are experimental techniques for identifying an organism X  X   X  X ssential X  genes, or the set of genes whose removal is lethal to the organism. However, these techniques are expensive and time-consuming. Recent work has attempted to extract from experimental knockout studies relevant features of essentiality, which aid in identifying essential genes in organisms lacking experimental results.
 Several features have been proposed as indicators for essentiality, including evolutionary conservation, protein size, and number of paralogs (Chen &amp; Xu, 2005). Using these basic features, Chen and Xu (2005) constructed a model of essentiality for S. cerevisiae (baker X  X  yeast). Using Naive Bayes Classifiers (NBC), Gustafson et al. (2006) subsequently created a model of essentiality for S. cerevisiae and E. coli using an ex-tended set of features generated from sequence data. This work presents kernel methods to improve upon existing models. We first use several sequence ker-nels recently introduced by the computational biology community and show that the Pfam kernel (Ben-Hur &amp; Noble, 2005) is most effective in selecting essential genes for S. cerevisiae . The Pfam kernel has recently been applied successfully in several biologically moti-vated learning tasks, and is generated from the Pfam database, the leading resource for storing protein fam-ily classification and protein domain data. However, the Pfam database is an ad-hoc solution relying on semi-manually tuned information.
 In the second part of this work, we design general se-quence kernels that produce effective similarity mea-sures while bypassing the manual tuning of the Pfam database. We present two sequence kernels that are in-stances of rational kernels, a class of sequence kernels defined by weighted automata that are effective for an-alyzing variable-length sequences (Cortes et al., 2004). Using automata to represent and compute these ker-nels is crucial in order to handle the large number of Pfam domains and the size of each of domain  X  we work with 6190 domains with the largest domain contain-ing over 3000 protein sequences. These novel kernels are designed from the same domain-specific data used by the Pfam library, and we show how they compare favorably to the Pfam kernel at predicting protein es-sentiality. They are general domain-based kernels that can be used in many problems in bioinformatics or other applications where similarity needs to be defined in terms of proximity to several large sets of sequences. The remainder of the paper is organized as follows. Section 2 describes the various sequence kernels and outlines the model used to improve prediction accu-racy of protein essentiality in S. cerevisiae . Section 3 describes and analyzes the novel rational kernels we present as alternatives to the Pfam kernel. Section 4 presents the results of extensive experiments compar-ing these domain-based kernels to the Pfam kernel. Our first model uses Support Vector Machines (SVMs) (Cortes &amp; Vapnik, 1995) to predict protein essential-ity with choices of kernels including the RBF kernel as well as three sequence kernels. In the following sub-sections, we define the sequence kernels, outline the experimental design, and present our first results. 2.1. Sequence Kernels Pfam Kernel The Pfam database is a collection of multiple sequence alignments and Hidden Markov Models (HMMs) rep-resenting many common protein domains and fami-lies. Pfam version 10.0 contains 6190 domains, and for each domain an HMM is constructed from a set of proteins experimentally determined to be part of the domain ( X  X eed X  proteins). Each HMM is trained using a manually-tuned multiple alignment of the seed pro-teins with gaps inserted to normalize sequence length. Once constructed, the HMM is evaluated in an ad-hoc fashion and the entire process is repeated if the align-ment is deemed  X  X nsatisfactory. X  See (Sonnhammer et al., 1997) for further details.
 When applied to a test sequence, a Pfam domain HMM generates an E-value statistic that measures the likelihood of the test sequence containing the domain. Given a dataset of protein sequences, the Pfam se-quence kernel matrix is constructed by representing each protein in the dataset as a vector of 6190 log E-values and computing explicit dot products from these feature vectors (Ben-Hur &amp; Noble, 2005). The Pfam kernel has recently been applied successfully in learning tasks ranging from protein function (Lanck-riet et al., 2004) to protein-protein interaction (Ben-Hur &amp; Noble, 2005).
 Spectrum and Motif Kernels The Spectrum and Motif kernels are two recently pro-posed sequence kernels used in learning tasks to esti-mate protein similarity (Leslie &amp; Kuang, 2004; Ben-Hur &amp; Brutlag, 2003). Both kernels model a protein in a feature space of subsequences, with each feature measuring the extent to which the protein contains a specific subsequence. The Spectrum kernel models proteins in a feature space of all possible n -grams, rep-resenting each protein as a vector of n -gram counts (in our studies we set n = 3). Alternatively, the Motif ker-nel uses a feature space consisting of a set of discrete sequence motifs (we use a set of motifs extracted from the eMotif database (Ben-Hur &amp; Noble, 2005)). For both kernels, the resulting kernel matrices are com-puted as an explicit dot product using these features. 2.2. Data We used the dataset of S. cerevisiae proteins from Gustafson et al. (2006), consisting of 4728 yeast pro-teins of which 20.4% were essential. We constructed the RBF kernel matrix from a set of 16 features gen-erated directly from protein sequences, corresponding to the  X  X asily attainable X  features from Gustafson et al. (2006). We used data from Ben-Hur and Noble (2005) to construct the Pfam, Spectrum and Motif kernel ma-trices, each of which was constructed following the steps outlined in Section 2.1 and subsequently centered and normalized. In addition to the RBF and the three sequence kernels, we also used a combined Pfam/RBF kernel, which we computed by additively combining the RBF kernel matrix with the normalized Pfam ker-nel matrix (RBF kernels are by definition normalized). 2.3. Experimental Design We ran experiments with 100 trials. For each trial, we randomly chose 8.3% of the data as a training set and used the remaining points as a test set, subject to the constraint that an equal number of essential train an SVM, and used the resulting model to make predictions on the test set in the form of probabilities of essentiality. We used libsvm X  X  functionality (Chang &amp; Lin, 2001) to estimate the outputs of an SVM as probabilities by fitting its results to a sigmoid function (Platt, 2000). To calculate the predicted probability of essentiality for each protein, we took the average over the predictions from each trial in which the protein appeared in the test set.
 We measured the accuracy of the model for the pro-teins with the highest predicted probability of essen-tiality, using positive predictive value (PPV) as a per-formance indicator. Grid search was used to determine the optimal values for parameters C and gamma. Stan-dard deviations were calculated from 10  X  X uper-trials, X  each corresponded to a 100-trial experiment described above. The Naive Bayes classifier (NBC) results were taken from Gustafson et al. (2006) and standard devi-ations were not reported. 2.4. First Results Figure 1 shows SVM X  X  performance using the set of kernels outlined above. The results show that the Pfam kernel is the most effective of the three sequence kernels at predicting essentiality. They also clearly show that the combined Pfam/RBF kernel outper-forms all other models. The importance of the phyletic retention feature is a possible reason for the superior performance of the combined kernel compared with Pfam alone. As shown by Gustafson et al. (2006) and verified in our work, phyletic retention (a measure of gene conservation across species) is a powerful predic-tor of essentiality. This feature is used by RBF but not by Pfam (or by the domain-based kernels defined in Section 3) because it requires comparing sequences across organisms.
 These results improve upon the leading model for pre-diction of protein essentiality while reducing the size of the training set more than five fold. Further, this is the first result showing the effectiveness of the Pfam kernel for this task, a fact that motivates the following sections of this paper, in which we seek a more general alternative to the Pfam kernel. In the previous section, we tested various sequence ker-nels, all introduced precisely to compute the similarity between protein sequences. Our results showed that the Pfam kernel was the most effective of these ker-nels, and we now aim to find a more general solution free of the manual tuning associated with the Pfam database.
 Specifically, we wish to determine a method to extract similarities between protein sequences based on their similarities to several domains, each represented by a set of sequences, i.e., Pfam domains. Although sev-eral sequence kernels have been recently introduced in the machine learning community, e.g., mismatch, gappy, substitution and homology kernels (Leslie &amp; Kuang, 2004; Eskin &amp; Snir, 2005), none of these ker-nels provides a solution to our domain-based learning problem. Indeed, these kernels are not designed to ef-ficiently compute the similarity between a string and a large set of strings, which in our case consists of 6190 Pfam domains each containing tens to thousands of sequences.
 Alternatively, large sets of strings, such as the Pfam domains, can be efficiently represented by minimal de-terministic automata. Hence, an efficient way to de-fine a similarity measure between such sets is to use automata-based kernels. This leads us to consider the framework for automata-based kernels proposed by Cortes et al. (2004). An additional benefit of this framework is that most commonly used string kernels are special instances of this scheme.
 3.1. Representation and Algorithms We will follow the definitions and terminology given by Cortes et al. (2004). The representation and com-putation of the Domain-based kernels are based on finite-state transducers , which are finite automata in which each transition is augmented with an output la-bel in addition to the familiar input label (Salomaa &amp; Soittola, 1978). Input (output) labels are concate-nated along a path to form an input (output) sequence. Weighted transducers are finite-state transducers in which each transition carries some weight in addition to the input and output labels. The weights of the transducers considered in this paper are real values. Figure 2(a) shows an example of a weighted finite-state transducer. In this figure, the input and output labels of a transition are separated by a colon delimiter, and the weight is indicated after the slash separator. A weighted transducer has a set of initial states repre-sented in the figure by a bold circle, and a set of final states, represented by double circles. A path from an initial state to a final state is an accepting path. The weight of an accepting path is obtained by first multiplying the weights of its constituent transitions and multiplying this product by the weight of the ini-tial state of the path (which equals one in our work) and the weight of the final state of the path (dis-played after the slash in the figure). The weight asso-ciated by a weighted transducer T to a pair of strings ( x, y )  X   X   X   X   X   X  is denoted by T ( x, y ) and is obtained by summing the weights of all accepting paths with in-put label x and output label y . For example, the trans-ducer of Figure 2(a) associates the weight 416 to the pair ( aab, bba ) since there are two accepting paths la-beled with input aab and output bba : one with weight 96 and another one with weight 320.
 The standard operations of sum +, product or con-catenation , and Kleene-closure  X  can be defined for weighted transducers (Salomaa &amp; Soittola, 1978). For any pair of strings ( x, y ), ( T 1 + T 2 )( x, y ) = T 1 ( x, y ) + T 2 ( x, y ) For any transducer T , T  X  1 denotes its inverse , that is the transducer obtained from T by swapping the input and output labels of each transition. For all x, y  X   X   X  , we have T  X  1 ( x, y ) = T ( y, x ).
 The composition of two weighted transducers T 1 and T 2 with matching input and output alphabets  X , is a weighted transducer denoted by T 1  X  T 2 when the sum: is well-defined and in R for all x, y (Salomaa &amp; Soit-tola, 1978).
 Weighted automata can be defined as weighted trans-ducers A with identical input and output labels, for any transition. Since only pairs of the form ( x, x ) can have a non-zero weight associated to them by A , we denote the weight associated by A to ( x, x ) by A ( x ) and call it the weight associated by A to x . Similarly, in the graph representation of weighted automata, the output (or input) label is omitted. Figure 2(b) shows an example of a weighted automaton. When A and B are weighted automata, A  X  B is called the intersection of A and B . Omitting the input labels of a weighted transducer T results in a weighted automaton which is said to be the output projection of T . 3.2. Automata-Based Kernels A string kernel K :  X   X   X   X   X   X  R is rational if it co-incides with the function defined by a weighted trans-ducer U , that is for all x, y  X   X   X  , K ( x, y ) = U ( x, y ). Not all rational kernels are positive definite and sym-metric (PDS), or equivalently verify the Mercer condi-tion, which is crucial for the convergence of training for discriminant classification algorithms such as SVMs. But, for any weighted transducer T , U = T  X  T  X  1 is guaranteed to define a PDS kernel (Cortes et al., 2004). Furthermore, most rational kernels used in computa-tional biology and natural language processing are of this form (Haussler, 1999; Leslie &amp; Kuang, 2004; Lodhi et al., 2002; Zien et al., 2000; Collins &amp; Duffy, 2001; Cortes &amp; Mohri, 2005). For instance, the n -gram ker-nel is a rational kernel. The n -gram kernel K n is de-fined as where c x ( z ) is the number of occurrences of z in x . K n is a PDS rational kernel since it corresponds to the weighted transducer T n  X  T  X  1 n where the transducer T n is defined such that T n ( x, z ) = c x ( z ) for all x, z  X   X  with | z | = n . The transducer T 2 for  X  = { a, b } is shown in Figure 3.
 We will now extend this formalism to measure the sim-ilarity between domains, or sets of strings represented by an automaton. Let us define the count of a string z in a weighted automaton A as: The similarity between two sets of strings represented by the weighted automata A and B according to n -gram kernel K n can then be defined by: Other rational kernels can be extended into a similar-ity measure between sets of strings in the same way. We will now define two families of kernels that can be used in a variety of applications where similarity with respect to domains is needed. 3.3. Independent Domain Kernel The Independent Domain kernel (IDK) measures the similarity between two sequences in our dataset D by comparing their similarities to each domain, e.g., Pfam domains. 2 For the i -th Pfam domain (with 1  X  i  X  P = 6190), let P i be the set of all seed pro-tein sequences for that domain. Each sequence in P i is represented as a string in an alphabet,  X , consist-ing of |  X  | = 21 characters, 20 for different amino acids plus an optional gap character used to represent gaps in the seed alignment. We denote by D i the mini-mal deterministic automaton representing the set of strings P i . For a given sequence x in our dataset, we can use the n -gram kernel K n to compute the similarity between x and the i -th Pfam domain P i : K n ( x, D i ). This leads to an overall similarity measure vector s ( x )  X  R P between x and the set of domains: the IDK K I as, for all x , y in  X   X  : K
I is PDS since it is constructed via an explicit dot-product. Any PDS kernel K with positive eigenvalues can be normalized to take values between 0 and 1 by defining K  X  as We apply this normalization to K I to account for the varying lengths of proteins in our dataset, since longer proteins will contain more n -grams and will thus tend to have more n -gram similarity with every domain. The kernel K I can be efficiently computed by comput-ing K n ( x, D i ) for all 1  X  i  X  P as follows: 1. Compute D i for each P i by representing each se-2. For all 1  X  i  X  P compute T n  X  D i , and for each se-3. Compute K n ( x, D i ) by intersecting A i and Y x and The complexity of computing K n ( x, D i ) for a fixed set of domains grows linearly in the length of x , hence the complexity of computing K I ( x, y ) grows linearly in the sum of the length of x and y , i.e. in O ( | x | + | y | ). Thus, this kernel is efficient to compute. However, it does not capture the similarity of two sequences in as fine a way as the next kernel we present. 3.4. Joint Domain Kernel Let us consider two sequences x and y and a given domain P i . Let X be the set of n -grams in common between x and P i , and Y the set of n -grams in common between y and P i . When computing the similarity between x and y according to P i , the IDK K I takes into account all n -grams in common between x and P i and between y and P i , i.e., all the n -grams in X X  X  , regardless of whether these n -grams appear in both x and y . Thus, K I may indicate that x and y are similar according to P i even if X and Y differ significantly, or in other words, even if x and y are similar to P i for different reasons. In contrast, the Joint Domain kernel (JDK) only takes into consideration the n -grams in common to x , y and P i , that is the n -grams in X X  X  , when determining the similarity between x and y . It will thus declare x and y similar according to P i iff x and y are similar to P i for the same reason. For each domain P i , the JDK defines a kernel K i that measures the similarity between two sequences x and y according to P i , using as a similarity measure the count of the n -grams in common among x , y and P i . More precisely, we define K i :  X   X   X   X   X   X  R as follows: Each K i is normalized as shown in Equation 7. We then combine these P kernels to obtain the kernel K J :  X   X   X   X   X   X  R defined as follows: We will now show that each K i and thus K J is a PDS rational kernel. Let A i be the weighted automata ob-tained by composing D i with T n and projecting the result onto its output: A i =  X  2 ( D i  X  T n ). From the definition of T n , it follows that A i ( z ) = c D c ( z ) = T n ( x, z ) for all | z | = n . Thus, for all ( x, y ), K ( x, y ) can be rewritten as: Observe that ( T n  X  A i )  X  1 = A  X  1 i  X  T  X  1 n = A i  X  T for an automaton the inverse A  X  1 i coincides with A i . is of the form T  X  T  X  1 and thus K i is PDS. Since PDS kernels are closed under sum, K J is also PDS. The computation of the kernel K J is more costly than that of K I since a full D  X  D kernel matrix needs to be computed for each Pfam domain. This leads to D 2  X  P rational kernel computations to compute K J , compared to only D  X  P rational kernel computations for K I . This is significant when P = 6190. Thus, it is important to determine an efficient way to compute the kernels K i . The following is an efficient method for computing K J that we adopt in our experiments, in which the complexity of computing K J ( x, y ) for a fixed set of domains linearly depends on the product of the length of x and y , i.e. in O ( | x || y | ): 1. Compute each A i and optimize using epsilon-2. For each sequence x in the dataset, compute 3. K i ( x, y ) is obtained by computing A i  X  Y x and Gap Symbol Handling The sequence alignments in the Pfam domain ( P i ) con-tain a gap symbol used to pad the alignments. In the previous two sections, we either ignored the gap sym-bol (when dealing with raw sequence data) or treated it as a regular symbol (when dealing with aligned se-quences). In the latter case, since this symbol does not appear in the sequences in the dataset, the result is that all n -grams containing the gap symbol are ig-nored during the computation of K I and K J . Alternatively, we can treat the gap symbol as a wild-card, allowing it to match any regular symbol. This can be achieved by modifying the transducer T n to match any gap symbol on its input to any regular sym-bol on its output (these transitions can also be assigned a weight to penalize gap expansion). We denote by T n the resulting transducer and replace T n by T n when composing with D i . Figure 4 shows T 2 for |  X  | = { a, b } with the symbol  X ? X  representing the gap symbol and an expansion penalty weight of 0 . 5. 3.5. Domain Kernels Based on Moments of Although counting common n -grams leads to informa-tive kernels, this technique affords a further general-ization that is particularly suitable when defining ker-nels for domains. We can view the sum of the counts of an n -gram in a domain as its average count after normalization. One could extend this idea to consider higher moments of the counts of an n -gram, as this could capture useful information about how similarity varies across the sequences within a single domain. Remarkably, it is possible to design efficiently com-putable domain kernels based on these quantities by generalizing the domain kernels from Sections 3.3 and 3.4 in a way similar to what is described by Cortes and Mohri (2005). Let m be a positive integer. We can de-fine the m -th moment of the count of the sequence z in a weighted automata A , denoted by c A,m ( z ), as: Both of our kernel families can then be generalized to a similarity measure based on the m -th moment of the n -gram counts as follows: These kernels can be efficiently computed by using, in place of T n , a transducer T n m that can be defined such with | z | = n . We evaluated the domain-based kernels described in Section 3 (with n = 3) using an experimental de-sign similar to Section 2.3. In order to test these ker-nels under various conditions, we chose to work with datasets sampled from the yeast dataset used in Sec-tion 2. We constructed 10 datasets, each containing 500 sampled data points randomly chosen from the 4728 initial points, subject to the constraint that we maintained the same ratio of positively and negatively labeled points. We worked on a large cluster machines and used the OpenFst and OpenKernel libraries to construct similarity matrices for each sample dataset for varying kernels (Allauzen et al., 2007; Allauzen &amp; Mohri, 2007). Generating similarity matrices took less than 30 minutes for IDK, 1 hour for JDK, and 2 . 5 hours for JDK with gaps treated as wildcards. We do not show results for the top 1% since it is an unre-liable statistic when working with 500 points. In all reported results we exclude results from one sampled dataset that generated pathological results for all se-quence kernels.
 Figure 5 shows the average prediction performance over the sampled datasets for various kernels. The fig-ure shows that the average performance of the JDK with gaps treated as wildcards (JDK-GAPS-W) is slightly better than the Pfam kernel, as it outperforms the Pfam kernel for the top 10% and 20% predictions. The figure also presents results for variants of the JDK that either ignore gaps in the seed alignment (JDK) or treat them as a distinct symbol (JDK-GAPS). The results show that, regardless of the treatment of gaps, the JDK drastically outperforms the IDK.
 Based on these results, we next tested the effectiveness of the JDK combined with the RBF kernel. Similar to the results in Figure 1, average prediction performance over the sampled datasets was better using combina-6 shows that the combined JDK is comparable to the combined Pfam kernel. Further, in contrast to the re-sults in Figure 5, the treatment of gaps by the JDK does not significantly alter prediction efficiency. In other words, the JDK is able to match the best results of the Pfam kernel using only raw Pfam sequence data (JDK), while completely ignoring the hand-curated multiple sequence alignments that are vital to param-eterizing the HMMs of the Pfam Library. We did not perform experiments using higher moments of the count, as described in Section 3.5, though we suspect that these more refined kernels would lead to further improvements over the Pfam kernel. We presented novel domain-based sequence kernels that require no hand-crafted information, in contrast to the Pfam kernel. The joint domain kernels we de-fined were shown to match or outperform the best pre-vious results for predicting protein essentiality. These kernels and their generalization based on moments of counts can be used for any application requiring sim-ilarity between sequences that may be extracted from proximity to several large sequence domains. In bioin-formatics, such applications may include remote ho-mology prediction, subcellular localization, and pre-diction of protein-protein interaction. This work was partially supported by the National Sci-ence Foundation award number MCB-0209754 and the New York State Office of Science Technology and Aca-demic Research (NYSTAR), and was also sponsored in part by the Department of the Army Award Num-ber W81XWH-04-1-0307. The U.S. Army Medical Re-search Acquisition Activity, 820 Chandler Street, Fort Detrick MD 21702-5014 is the awarding and adminis-tering acquisition office. The content of this material does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred.
 Allauzen, C., &amp; Mohri, M. (2007). OpenKernel library. http://www.openkernel.org .
 Allauzen, C., Riley, M., Schalkwyk, J., Skut, W., &amp;
Mohri, M. (2007). OpenFst: a general and efficient weighted finite-state transducer library. CIAA 2007 (pp. 11 X 23). Springer. http://www.openfst.org . Ben-Hur, A., &amp; Brutlag, D. L. (2003). Remote ho-mology detection: a motif based approach. ISMB (Supplement of Bioinformatics) (pp. 26 X 33).
 Ben-Hur, A., &amp; Noble, W. (2005). Kernel methods for predicting protein-protein interactions. Bioin-formatics , 21 , 38 X 46.
 Chang, C.-C., &amp; Lin, C.-J. (2001). LIBSVM: a library for support vector machines . Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm .
 Chen, Y., &amp; Xu, D. (2005). Understanding protein dispensability through machine learning analysis of high-throughput data. Bioinformatics , 21 , 575 X 581. Collins, M., &amp; Duffy, N. (2001). Convolution kernels for natural language. NIPS 2001 (pp. 625 X 632). Cortes, C., Haffner, P., &amp; Mohri, M. (2004). Rational
Kernels: Theory and Algorithms. Journal of Ma-chine Learning Research , 5 , 1035 X 1062.
 Cortes, C., &amp; Mohri, M. (2005). Moment kernels for regular distributions. Machine Learning , 60 , 117 X  134.
 Cortes, C., &amp; Vapnik, V. N. (1995). Support-Vector Networks. Machine Learning , 20 , 273 X 297.
 Eskin, E., &amp; Snir, S. (2005). The Homology Kernel: A Biologically Motivated Sequence Embedding into Euclidean Space. CIBCB (pp. 179 X 186).
 Gustafson, A., Snitkin, E., Parker, S., DeLisi, C., &amp;
Kasif, S. (2006). Towards the identification of es-sential genes using targeted genome sequencing and comparative analysis. BMC:Genomics , 7 , 265. Haussler, D. (1999). Convolution Kernels on Dis-crete Structures (Technical Report UCSC-CRL-99-10). University of California at Santa Cruz. Lanckriet, G., Deng, M., Cristianini, N., Jordan, M., &amp;
Noble, W. (2004). Kernel-based data fusion and its application to protein function prediction in yeast. Pacific Symposium on Biocomputing (pp. 300 X 311). Leslie, C. S., &amp; Kuang, R. (2004). Fast String Kernels using Inexact Matching for Protein Sequences. Jour-nal of Machine Learning Research , 5 , 1435 X 1455. Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristian-ini, N., &amp; Watskins, C. (2002). Text classification using string kernels. Journal of Machine Learning Research , 2 , 419 X 44.
 Platt, J. (2000). Probabilities for support vector ma-chines. In Advances in large margin classifiers . Cam-bridge, MA: MIT Press.
 Salomaa, A., &amp; Soittola, M. (1978). Automata-Theoretic Aspects of Formal Power Series . Springer. Sonnhammer, E., Eddy, S., &amp; Durbin, R. (1997).
Pfam: A comprehensive database of protein domain families based on seed alignments. Proteins: Struc-ture, Function and Genetics , 28 , 405 X 420.
 Zien, A., R  X atsch, G., Mika, S., Sch  X olkopf, B.,
Lengauer, T., &amp; M  X uller, K.-R. (2000). Engineer-ing Support Vector Machine Kernels That Recog-nize Translation Initiation Sites. Bioinformatics , 16 ,
