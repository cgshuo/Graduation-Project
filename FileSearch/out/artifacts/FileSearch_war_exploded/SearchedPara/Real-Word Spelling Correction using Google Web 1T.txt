 We present a method for correcting real-word spelling errors using the Google Web 1T n -gram data set and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm. Our method is focused mainly on how to improve the correction recall (the fraction of errors corrected) while keeping the correction precision (the fraction of suggestions that are correct) as high as pos-sible. Evaluation results on a standard data set show that our method performs very well.
 I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing X  Text analysis ; I.2.1 [ Artificial Intelligence ]: Appli-cations and Expert Systems X  Natural language interfaces Algorithms, Experimentation, Performance Real-word, spelling correction, Google web 1T, n-gram
Real-word spelling errors are words in a text that occur when a user mistakenly types a correctly spelled word when another was intended. Errors of this type may be caused by the writer X  X  ignorance of the correct spelling of the in-tended word or by typing mistakes. Such errors generally go unnoticed by most spellcheckers as they deal with words in isolation, accepting them as correct if they are found in the dictionary, and flagging them as errors if they are not. Ironically, errors of this type may even be caused by spelling checkers in the correction of non-word spelling er-rors when the auto-correct feature in some word-processing software sometimes silently change a non-word to the wrong real word [4], and sometimes when correcting a flagged error, the user accidentally make a wrong selection from the choices offered [10]. An extensive review of real-word spelling cor-rection is given in [4].
 berne [9] proposed a trigram-based method for real-word er-rors without explicitly using probabilities or even localizing the possible error to a specific word. This method simply assumes that any word trigram in the text that is attested in the British National Corpus is correct, and any unattested trigram is a likely error.
The proposed method first tries to determine some prob-able candidates and then finds the best one among the can-didates. We consider a string similarity function and a fre-quency value function in our method. The following sections present a detailed description of each of these functions, fol-lowed by the procedure to determine some probable candi-dates along with the procedure to find the best candidate.
We use the same string similarity measure that [6, 7] used with the following different normalization as it gives bet-ter similarity value, as well as it is more computationally efficient: v 2 = NMCLCS 1 ( s i , s j ) = 2  X  len ( MCLCS 1 ( s i , s j )) len ( s v 3 = NMCLCS n ( s i , s j ) = 2  X  len ( MCLCS n ( s i , s j )) len ( s v 4 = NMCLCS z ( s i , s j ) = 2  X  len ( MCLCS z ( s i , s j )) len ( s We take the weighted sum of these individual values v 1 , v 2 , v , and v 4 from equation (1), (2), (3) and (4), respectively, to determine the string similarity score, where  X  1 ,  X  2 ,  X  3 ,  X  4 are weights and  X  1 +  X  2 +  X  3 +  X  4 = 1. Therefore, the similarity of the two strings, S  X  [0 , 1] is: We heuristically set equal weights for most of our experi-ments 2 . Theoretically, v 3  X  v 2 and v 3  X  v 4 .
We determine the normalized frequency value of each can-didate word for a single position with respect to all other candidates for the same position. If we find n replacements frequency of a n -gram (where n  X  { 5 , 4 , 3 , 2 } and any can-didate word w ij is a member of the n -gram), then we deter-mine the normalized frequency value of any candidate word w ij as the frequency of the n -gram containing w ij , over the maximum frequency among all the candidate words for that position.

First, we use Google 5-gram data set to find candidate words of the word having spelling error. If the 5-gram data set fails to generate at least one candidate word then we move forward to 4-gram data set or 3-gram data set or to keep the system unsupervised. If development data would be available, we could adjust the weights.
We use the same steps described in section 3.3.1 to de-termine the candidate words using the 4-gram, 3-gram and 2-gram data set. If we find more than one candidate word, we go to section 3.5. Otherwise, if matched =1 then return no suggestion and exit. Otherwise, we proceed to phase 2.
We follow phase 1 with some small changes: instead of trying to find all the n -grams ( n  X  X  5 , 4 , 3 , 2 } ) where only w i is changed while keeping all of { X  X  X  , w i  X  2 , w i  X  1 } unchanged, we try to find all the n -grams ( n  X  X  5 , 4 , 3 , 2 } ) where w i , and any but the first member of { X  X  X  , w i  X  2 , w i  X  1 } are changed while keeping the rest of { X  X  X  , w i  X  2 , w i  X  1 } unchanged.
We use this section only if we have more than one candi-date word found in section 3.3 or section 3.4. Let us consider that we find n candidate words of w i in section 3.3 or sec-w ij , we use the string similarity value between w ij and w i (already calculated using equation (5)) and the normalized frequency value of w ij (already calculated using equation (6)) and then calculate the weight value using equation (7) by setting  X  = 0 . 5. We find the word having the maximum weight value as the target suggestion word which is:
We used as test data the same data that [10] used in their evaluation of [8] method, which in turn was a replication of the data used by [5] and [4] to evaluate their methods. The data consisted of 500 articles (approximately 300,000 words) from the 1987  X  89 Wall Street Journal corpus, with all headings, identifiers, and so on removed; that is, just a long stream of text.

Malapropisms were randomly induced into this text at a frequency of approximately one word in 200. Specifically, any word whose base form was listed as a noun in WordNet was potentially replaced by any spelling variation found in the lexicon of the ispell spelling checker 4 . A spelling vari-ation was defined as any word with an edit distance of 1 from the original word. Though [10] mentioned that the data contained 1402 inserted malapropisms, there were only 1391 malapropisms. A detailed description of this data can be found in [3, 10].

Some examples of successful and unsuccessful corrections, using Google 5-grams, are shown in Table 1. For each er-ror, our method returns either a suggestion (which is either number of errors where either a suggestion or no suggestion is generated for different combinations of n -grams used. Fig-ure 2 breaks down the numbers shown in Figure 1 into true positive , false positive and false negative . you your errors in the context of the original file, and sug-gests possible corrections when it can figure them out. 5 A returned correct suggestion is also known as true positive . 6 A returned wrong suggestion is also known as false positive . 7 Also known as false negative .
 Table 2: A comparison of recall, precision, and F-measure for three methods of malapropism detec-tion and correction on the same data set. better using different combinations of n -gram while keeping precision as high as possible.

We cannot directly compare our results with the correc-tion results from previous work, because in that work the correction was run on the results of the detection module, cumulating the errors, while our correction module ran on the correctly-flagged spelling errors. Still, we indirectly try to compare our results with the previous work. Table 2 shows our method X  X  results on the described data set com-pared with the results for the trigram method of [10] and the lexical cohesion method of [4]. The data shown here for trigram method are not from [10], but rather are later results following some corrections reported in [3]. That the corrected result of [10] can detect 762 errors and thus cor-rect 688 errors out of these 762 detected errors means each of the correction precision , recall and F-measure is 0.9. It is obvious that the performance of correcting the rest of the undetected errors will not be the same as correcting the de-tected errors because these errors are difficult to correct since they are difficult to detect in the first place. Still, the cor-rection performance of our proposed method is comparable to the correction performance of the method that runs on the results of the detection module, cumulating the errors.
Our purpose in this paper was the development of a high-quality correction module. The Google n -grams proved to be very useful in correcting real-word errors. When we tried with only 5-grams the precision (0.96) was good, though the recall (0.34) was too low. Having sacrificed a bit of the pre-cision score, our proposed combination of n -grams method achieves a very good recall (0.88) while maintaining the pre-cision at 0.91. Our attempts to improve the correction recall while maintaining the precision as high as possible are help-ful to the human correctors who post-edit the output of the real-word spell checker. If there is no postediting, at least
