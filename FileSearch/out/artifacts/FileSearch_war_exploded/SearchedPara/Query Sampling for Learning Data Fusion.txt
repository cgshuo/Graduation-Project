 D ata fusion is to merge the results of multiple independent retrieval models into a single ranked list. Several earlier studies have shown that the combination of different mod-els can improve the retrieval performance better than us-ing any of the individual models. Although many promis-ing results have been given by supervised fusion methods, training data sampling has attracted little attention in pre-vious work of data fusion. By observing some evaluations on TREC and NTCIR datasets, we found that the performance of one model varied largely from one training example to another, so that not all training examples were equivalently effective. In this paper, we propose two novel approaches: greedy and boosting approaches, which select effective train-ing data by query sampling to improve the performance of supervised data fusion algorithms such as BayesFuse , prob-Fuse and MAPFuse . Extensive experiments were conducted on five data sets including TREC-3,4,5 and NTCIR-3,4. The results show that our sampling approaches can significantly improve the retrieval performance of those data fusion meth-ods.
 H .3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval A lgorithms, Experimentation D ata fusion, query sampling, AdaFuse
I n the data fusion task [7], the document collections searched by different models should be the same. It is quite different from so-called collection fusion [21], where the search results c ome from distinct document collections. Data fusion is im-portant in order to exploit multiple models to achieve better performance than any of the individual models. Several ear-lier studies have shown significant improvement using data fusion such as on the datasets of TREC-2 [5] and Encyclo-pedia Britannica [4]. Data fusion can also benefit many in-formation retrieval applications such as meta-search [3] and multimodal search [11].

A general solution to the data fusion problem is to de-velop a score function, which calculates a ranking score for each document in the merged search results; this score deter-mines the final rank of the document. In the previous works, the learning of the function could be unsupervised or super-vised. Unsupervised fusion methods such as BordaFuse [3], Condorect Fusion [17] and CombMNZ [7] heuristically deter-mine such a score based merely on the evidences provided by the search results like similarity, rank, and the number of the models retrieving the document. Supervised methods, for example, SegFuse [20], SlideFuse [14], PosFuse [13] and PLQA [23], get a better approximation of the score based on the given training data, which shows information about how well the models performed for some given queries before. With such training data, supervised methods can further es-timate the quality of the evidences such as the accuracy of a rank position for a specific model [13] and the weight of a model [15], and then adjust the ranking score accordingly.
Supervised methods generally perform better than unsu-pervised ones [7]. Previous work on supervised methods mostly used all of the training data to learn their fusion models [3, 13, 15]. They assumed ineffective training data was tolerable and could be remedied via data fusion in the process of document retrieval. However, by observing some evaluations on several datasets, we found that the perfor-mance of one model varied largely from one training data to another, showing that not all training data should be con-sidered in learning.

Take the probFuse algorithm [13] as an example, which is a supervised method. In probFuse, each search-result ranked list is divided into (ranked) segments. The ranking score S f or document d is defined by: where M is the number of retrieval models, and k i s the segment that d returned by model m appears in. P ( d k | m ) i s the probability that document d returned in segment k by model m , where Q is the number of training queries, R k ,q the number of documents in segment k which is relevant to q uery q , and | k | is the total number of documents in segment k . In our experiments, we observed that the variance of seg-ment relevance ( | R k ,q | | k | ) among different training queries was large. When a retrieval model had a higher density of rel-evant documents at the top for some training queries, the average of the ratios dropped significantly at small k . In this case, sampling all queries for constructing training data could not estimate well for all P ( d k | m ) . This observation is supported by [13], where probFuse was examined on dif-ferent size of training sets. The results (in [13] X  X  Table 4) show that the largest training set didn X  X  achieve the best performance but the size of 20% did. In this paper, we are interested in figuring out how to sample effective queries as training data.

The goal of this paper is to sample a set of queries from given training data, which is used to learn a given data fusion method. Three supervised methods are considered, includ-ing BayesFuse [3], probFuse [13], and MAPFuse [15]. We propose two novel query sampling approaches: greedy and boosting. Extensive experiments have been conducted on five data sets including TREC-3,4,5 and NTCIR-3,4. The experimental results show that our sampling approaches can significantly improve the retrieval performance of those data fusion methods.

In the rest of this paper, we first make a brief review on related work in section 2, and describe the proposed greedy and boosting approaches in Section 3. The experiments and discussions are presented in Sections 4. Finally, in Section 5, we present our conclusions and future work.
I t has been shown that the data instances are not equally informative in information retrieval task [10]. The selections of documents and topics (queries) have been studied to af-fect the efficiency and effectiveness of retrieval evaluation in information retrieval community. With large scale corpora and limited resource, the selection of the best subset of top-ics which is sufficient for evaluation purpose are discussed in some papers [18, 9], and the collection of documents which balances reducing the corpora size and effectiveness of learn-ing are also studied [2]. However, the purpose of our query sampling is quite different to those works. Our sampling al-gorithms is for data fusion, in which the input format is sev-eral outputs of learning-to-rank or retrieval model. Besides, our goal is to increase the fusion performance by selecting informative training queries from a clean and well-defined data set.

Training data selection has been well studied in machine learning community [6], but relatively little research has been conducted on data fusion in such issue. It has been proposed that learning can be more efficient by actively se-lecting particular salient training data [16]. In addition, Se-lection methods for particular learning algorithms such as Support Vector Machine are also discussed [22]. However, there is difficulty in applying above conventional selection methods to the problem of data fusion. The data format of traditional machine learning problem is (feature vector, label) pair, which is different to that of data fusion prob-lem. For performance evaluation, the predictions in machine learning are considered as correctness, but in data fusion, the criteria is the improvement by fusion, i.e., the perfor-mance increasing after fusing results. Those gaps are still not sufficiently investigated.
G iven a supervised data fusion method and its training data, the goal of this paper is to select a set of effective training data from original one by query sampling for maxi-mizing the retrieval performance of the fusion method. Note that our method should be independent to the adopted fu-sion method. The process consists of two phases: sampling phase and fusion phase. In the sampling phase, based on the fusion method, our problem is how to sample a set of queries to construct effective training data. Suppose the given training data is denoted as X and the given fusion method is Fuse . We want to find where Fuse X 0 ( X ) stands for the fusion of X using X 0 training data, and  X  is the measure of evaluating a fusion model, such as MAP, NDCG and p@k, in this work, MAP (Mean Average Precision) is used. The selected training data X X s hould be able to maximize  X ( Fuse X ( X ) ). In the fusion phase, fusion model Fuse X i s built by ap-plying the given fusion method on sampled training data X . The fusion model Fuse X i s then used to merge a set of search results from the retrieval models for a new query. The formal terminology of training data X is listed in Ta-ble 1. In this section, two approaches to query sampling for learning data fusion are presented: greedy and boosting.
T o select the most effective training data for improving query performance, one can evaluate the effectiveness of each training instance by sampling a single query. To achieve this, one can exhaustively select each training instance and de-cide whether a selected data instance improves the query success rate. One can apply either incremental (take-one-in) or decremental (leave-one-out) strategies for the training A lgorithm 1 Independent Greedy Approaches T op-k take-one-in Indep. Algorithm 1. Given X = { T ( q n ) } | Q | n = 1 , T ( q n ) = ( R ( q 2. For each training example T ( q n ) 2 X : 3. Sorting: score ( T ( q  X  ( 1) ) )  X  score ( T ( q  X  ( 2) Top-k leave-one-out Indep. Algorithm 1. Given X = { T ( q n ) } | Q | n = 1 , T ( q n ) = ( R ( q 2. For each training example T ( q n ) 2 X : 3. Sorting: score ( T ( q  X  ( 1) ) )  X  score ( T ( q  X  ( 2) d ata selection process. Algorithm 1 illustrates the take-one-in procedure, which samples a query data instance at each iteration and determines whether that instance improves the performance. Similar to Take-one-in , we also consider Leave-one-out for comparison, which iteratively removes the most ineffective training example.

We note that this strategy might not be preferred in tra-ditional machine learning tasks such as classification, since using only few data instances in the first few iterations is typ-ically insufficient to describe the associated learning model, but such method is reasonable in data fusion because that a training example is not just a (feature vector, label) pair. For data fusion purposes, the take-one-in procedure actu-ally utilizes a set of ranked lists and relevant judgments, i.e ( R ( q n ) , J ( q n ) ). Besides, most of the time top-ranked docu-ments from most of the retrieval models have more chances to be relevant to a query. It X  X  adequate to sample a single query for estimating such probabilities.

The dependent greedy approach, as shown in Algorithm 2, always considers those have been collected in the training data when sampling new query. Generally, the independent greedy approach tends to sample queries with high individ-ual co-relations. The dependent one is helpful in sampling those queries that are individually uncorrelated but jointly more correlated for learning data fusion. The parameter k is the specified preferable size of sampled subset, i.e. the k most effective queries.
T he greedy approaches only sample a set of queries as training data. In this section, we extend the AdaBoost algo-rithm [8] to sample a set of weighted queries; the given fusion method is treated as a base learner. We name our boost-ing approach as AdaFuse , which repeatedly constructs base learners on the basis of re-weighted training examples and finally linearly combines them for fusion. Multiple subsets from training data are selected in the process of boosting; each subset improves fusion performance based on partial (weighted) training data.
 A lgorithm 2 Top-k Take-one-in Dependent Algorithm 1 . Given X = { T ( q n ) } | Q | n = 1 , X S = { } , X U S 2 . Repeat k times: 3 . Return X S
S imilar to the greedy approaches, AdaFuse is flexible for data fusion since any supervised fusion method can serve as a base learner. The hypothesis set is a set of possible base learners, i.e., fusion models. In this paper, we focus on sampling effective training data; therefore the base learners are constructed by the given fusion method learning with different subset of training data. We define the hypothesis set H in the form of W e start to achieve H by { Fuse { q n } ( ( R ( q )) } idea of decision stump [1]. We construct a base learner for each training query q n , and repeatedly select the most ef-fective one. This simple hypothesis set improves the fusion performance efficiently. Moreover, considering those queries that are not effective individually but jointly more informa-tive for learning, we enlarge the { q n } t o a very small subset with particular size  X  , so the H is { Fuse X 0 ( R ( q ) ) } w here set X (  X  ) includes C | Q |  X   X  -elements-subsets, i.e., the  X  -combinations from training query set. The  X  is the de-gree of dependency, which controls the impact of query co-occurrence.
T he AdaBoost algorithm is designed for prediction, namely, the binary classification. Besides that the format of training examples of data fusion differs from that of classification, the main difference between the two tasks is how to eval-uate the performance. In classification, a prediction to an example by a hypothesis is evaluated in terms of correctness, i.e., error or correct, but in data fusion, an example fused by a hypothesis is measured by a real value which presents the quality of the merged ranked list, namely, the improvement in performance measure, such as MAP, NDCG and P@k.

It has been proposed that the effectiveness of learned hy-pothesis can be evaluated based on the concept of prediction margin [12, 19]. Given hypothesis h is a function in the form of h : x i  X  &lt; ; the output { +1 ,  X  1 } is the sign of h ( x the magnitude | h ( x i ) | i s interpreted as the confidence in this prediction [19]. For a prediction h ( x i ) , its prediction margin is defined as y i h ( x i ) , and thus the correct predictions with high confidence may have large prediction margin. The pre-diction margin represents the  X  X ffectiveness of prediction X , that is to say, how effective this prediction is.

We interpret the  X  X ffectiveness of fusion X  as the perfor-m ance improvement after fusion. Therefore, we propose an new concept of the improvement margin  X  as w here  X  is a function, and { f m ( D ,q n ) } m 2 M i s the average performance of basis models; we simplify it as  X  ( J ( q n The larger the improvement margin is, the better the per-formance comparing to the basis retrieval models is. This conversion of margin: y n h t ( x n ) = )  X  ( J ( q n ) , f ( D,q us to extend the AdaBoost to a variation of boosting algo-rithm for data fusion. We give a definition of margin  X  which is transformed to [  X  1 , +1] by tan  X  1 . = t an  X  1  X  ( f ( D,q n ) )  X  Mean ( {  X ( f m ( D ,q n ) ) } Besides, any definition in the range [  X  1 , +1] can be used.
B y extending the Adaboost with our improvement mar-gin, we propose AdaFuse as the boosting algorithm for data fusion, which learns a simple rule to maximize the weighted sum of improvement margin from weighted training exam-ples; the confidence of each hypothesis is evaluated in terms of such rule. The details are shown in Algorithm 3. A lgorithm 3 AdaFuse: Boosting for Data Fusion 1 . Given the training data: { ( R ( q n ) , J ( q n ) ) } 2 . Initialize: P 1 ( n ) = 1 /N for all n 3. For t = 1 , 2 , 3 ,...,T : 4. Output the final hypothesis:
T wo data sets were used in the experiments. One was from the TREC-3, TREC-4 and TREC-5 (CategoryA only) ad-hoc IR tasks. The other was from NTCIR-3 and NTCIR-4 ad-hoc IR tasks, and the relax judgment was used rather than the rigid one. We used the Lemur toolkit 1 t o support the construction of the 7 retrieval models in our experiments.
Two experimental settings were considered. The first set-ting is evaluated by the output of the 5-folds cross validation (CV) with all models used. We followed [13, 15] to perform five random runs (RR) as the second setting. Each run had 6 models without repetitiveness. 50% of queries were treated as training data and the rest as testing data. Each algo-rithm in our experiments was compared in the same division of training data, testing data and retrieval models.
We tested our query sampling approaches on 3 fusion methods, including BayesFuse [3], MAPFuse [15] and probFuse [13], which have been shown to superior than the papular unsupervised fusion method CombMNZ [7]. These methods without training data selection were viewed as the baselines.
The BayesFuse algorithm is based on Bayesian inference [3] and ranks documents by where Pr [ r m ( d ) | r el ] is the probability that a relevant docu-ment would be ranked at level r by retrieval m , and Pr [ r is the probability of an irrelevant document.

MAPFuse approximates the probability of a document returned at rank position x by model m is relevant by M AP M AP m i s the MAP score of training query set by retrieval model m . The final ranking score R d f or document d is where M is the set of retrieval models and p m ( d ) is the rank-ing position that retrieval model m ranks document d . The training process only needs the MAPs of training queries.
We compared the MAPs of applying Top-k Take-One-In indep., Leave-One-Out indep., and Top-k Take-One-In dep. with the k most effective queries. Figures 2 lists the per-formance evaluated by CV and RR. In general, the MAP scores for CV are higher than those for RR because all mod-els were used in CV where only some models were used in RR. Most of the MAP curves share an interesting tendency: the curves keep going up in the first few interactions. After the peak is reached, the curves go down. Notice that the peaks were achieved for k &lt; 10 in BayesFuse cases, k &lt; 5 for probFuse cases and even less for MAPFuse, meaning only a few training examples were critical to constructing a high-performance fusion model. Take-One-In dep. performed better than Take-One-In indep., but some cases they both failed in improvement because unsuitable queries were se-lected in early stages. Although there was significant perfor-mance improvement in TREC data sets, greedy approaches were only observed limited gain in NTCIR.

To observe the boosting behavior of each iteration, we examined the five CV folds of boosting process. Figure 1 h ttp://www.lemurproject.org/, Build-in retrieval models: (1)TFIDF (2)Okapi (3)Simple KL (4)INQUERY (5)CORI CS (6)Cosine Similarity and (7)INDRI. demonstrates the analysis of individual fold in NTCIR-3 with AdaFuse. The MAP-in was MAP score obtained by fusion only training queries, and the MAP-out is that from testing set. We marked the two curves in order to observe the tendency of learning; note that the MAP-in increased with each training iteration and the MAP-out followed the same tendency except in the fold 1, which maybe slightly over-fitting. The baseline was compared with only MAP-out, since the MAP-in and MAP-out are fused from different queries and could not be compared.

The greedy approaches selected only 5-6 queries to achieve promising results, so we tried the degree of dependency  X  = 1 , 2 , 3 , 4 and reported the best MAP of the four results. The union of each multiple subset selection by each iterator was larger than 5-6 queries, but in our experiments with 100 iterators, none of our approaches sampled more than half of original training queries, which still supports our assumption that not every query is effective.

We summarized the experiments in Table 2. In most of the cases, the two-tailed P-value of t-test less being than 0.01 showed that the difference is statistically significant compar-ing with the baseline: probFuse, MAPfuse and BayesFuse. The boosting approaches performed much higher than the greedy approaches, but the time complexity of the boosting approach is much larger than the greedy approach, and both fusion processes take the same time. Note that the NTCIR data set, on which greedy approaches did not performed well, got a large improvement by boosting approaches; im-proving even more than the TREC dataset. In general, the boosting process needs more training time, but the perfor-mance is more stable than greedy approaches because if some biased queries are chosen at an early stage, greedy approaches fail to compensate, but boosting processes have a chance to emphasize other effective queries by iteratively updating weight.
B y means of query sampling, the retrieval performance can be improved significantly for conventional supervised data fusion methods. In this paper, we proposed two useful approaches for query sampling. Extensive experiments have been conducted on five data sets, including TREC-3,4,5 and NTCIR-3,4. The results show the feasibility of the proposed approaches. Our AdaFuse is a framework for any fusion algorithm and possible hypothesis set, not limited in queries sampling purpose. Future work will include the adoption of more complicated and powerful base learners and use new definition of margin in our boosting framework. [ 1] W. I. Ai and P. Langley. Induction of one-level [2] J. A. Aslam, E. Kanoulas, V. Pavlu, S. Savev, and [3] J. A. Aslam and M. Montague. Models for [4] B. T. Bartell, G. W. Cottrell, and R. K. Belew. [ 5] N. J. Belkin, P. Kantor, E. A. Fox, and J. A. Shaw. [6] A. L. Blum and P. Langley. Selection of relevant [7] J. A. S. Edward A. Fox. Combination of multiple [8] Y. Freund and R. E. Schapire. A decision-theoretic [9] J. Guiver, S. Mizzaro, and S. Robertson. A few good [10] C. Hauff, D. Hiemstra, F. de Jong, and L. Azzopardi. [11] L. S. Kennedy, A. P. Natsev, and S.-F. Chang. [12] L. Li, A. Pratap, H. tien Lin, and Y. S. Abu-mostafa. [13] D. Lillis, F. Toolan, R. Collier, and J. Dunnion. [14] D. Lillis, F. Toolan, R. Collier, and J. Dunnion. [15] D. Lillis, L. Zhang, F. Toolan, R. W. Collier, [16] D. J. C. MacKay. Information-based objective [17] M. Montague and J. A. Aslam. Condorcet fusion for [18] S. Robertson. On the contributions of topics to system [19] R. E. Schapire and Y. Singer. Improved boosting [20] M. Shokouhi. Segmentation of search engine results for [21] E. M. Voorhees, N. K. Gupta, and B. Johnson-laird. [22] J. Wang, P. Neskovic, and L. N. Cooper. Training [23] R. Yan and A. G. Hauptmann. Probabilistic latent
