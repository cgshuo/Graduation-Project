 Abstract This paper describes a multi-modal corpus of hand-annotated meeting dialogues that was designed for studying addressing behaviour in face-to-face con-versations. The corpus contains annotated dialogue acts, addressees, adjacency pairs and gaze direction. First, we describe the corpus design where we present the meetings collection, annotation scheme and annotation tools. Then, we present the analysis of the reproducibility and stability of the annotation scheme.
 Keywords Addressing  X  Multi-party dialogues  X  Multimodal corpora  X  Annotation schemas  X  Reliability analysis 1 Introduction Current tendencies in modelling human X  X omputer as well as human X  X uman inter-actions are moving from a two-party model to a multi-party model. One of the issues that becomes salient in interactions involving more than two parties is addressing. Addressing as an aspect of every form of communication has been extensively studied by conversational analysts and social psychologists (Clark &amp; Carlson, 1992 ; Goffman, 1981 ; Goodwin, 1981 ). Recently, addressing has received considerable attention in interaction modelling in the context of mixed human X  X uman and hu-man X  X omputer interaction (Bakx, van Turnhout, &amp; Terken, 2003 ; van Turnhout, Terken, Bakx, &amp; Eggen, 2005 ), human X  X uman X  X obot interaction (Katzenmaier, Stiefelhagen, &amp; Schultz, 2004 ), mixed human-agents and multi-agents interaction (Traum, 2004 ) and multi-party human X  X uman interaction (Jovanovic &amp; op den Akker, 2004 ; Otsuka, Takemae, Yamato, &amp; Murase, 2005 ).

Addressing is carried out through various communication channels, such as speech, gestures or gaze. To explore interaction patterns in addressing behaviour Natasa Jovanovic  X  Rieks op den Akker  X  Anton Nijholt and to develop models for automatic addressee prediction, we need a collection of audio and video interaction recordings that contains a set of annotations relevant to addressing. Meetings as complex interplays of interacting participants represent a relevant domain for the research on different aspects of interactions involving more than two participants who employ a variety of channels to communicate with each other.

In the context of the meeting research, several corpora have already been developed. Some of the existing meeting corpora, such as the ICSI (Janin et al., 2004 ) and ISL (Burger &amp; Sloane, 2004 ) corpora X  X urrently widely used to study linguistic phenomena in natural meetings X  X re limited to audio data only. The NIST audio X  X isual meeting corpus (Garofolo, Laprun, Michel, Stanford, &amp; Tabassi, 2004 ) is designed to support the development of audio and video recognition technologies in the context of meetings. Currently, it provides transcriptions of the meetings to enable the research on automatic speech recognition in meetings. To support the research on higher-level meetings understanding, the VACE (Chen et al., 2006 ) and AMI (Carletta et al., 2006 ) multi-modal meeting corpora are currently being pro-duced. The VACE corpus is being developed to support research on multimodal cues, such as speech, gaze, gestures and postures, for understanding meetings. The AMI data collection is being developed to enhance research in various areas related to the development of meeting browsing technologies and remote meeting assistants, including speech recognition, computer vision, discourse and dialogue modelling, content abstraction, human X  X uman and human X  X omputer interaction modelling. It contains a range of annotations including, among others, speech transcription, dia-logue acts, topic segmentation, focus of attention, head and hand communicative gestures, and summaries.

In this paper, we describe a multi-modal corpus of hand-annotated meeting dia-logues, designed for studying addressing behaviour in face-to-face conversations. The meetings were recorded in the IDIAP meeting room in the research program of the European M4 1 and AMI 2 projects. The recordings are available through the MultiModal Media File Server. 3 Currently, the corpus contains hand-annotated dialogue acts, adjacency pairs, addressees and gaze directions of meeting partici-pants. A set of the corpus X  annotations of the M4 meetings is available as a part of the M4 meeting collection. 4
Apart from the corpus description which includes the description of the meeting data, annotation scheme, annotation tools and the corpus format, this paper reports detected sources of unreliability. 2 Meeting data The corpus consists of 12 meetings recorded at the IDIAP smart meeting room (Moore, 2002 ). The room is equipped with fully synchronized multi-channel audio and video recording devices (see Fig. 1 ). Of the 12 meetings, 10 were recorded within the scope of the M4 project. These meetings are scripted in terms of type and schedule of group actions that participants perform in meetings such as pre-unconstrained. Spontaneous behaviour of participants in these meetings allows us to examine observable patterns of addressing behaviour in small group discussions. More natural, scenario-based, meetings have been recorded in the scope of the AMI project. One of the AMI pilot meetings recorded at the IDIAP meeting room is included in our corpus. The meeting involves a group focused on the design of a recorded at IDIAP for the exploration of argumentative structures in meeting dialogues.
 Research on small group discussions presented in (Carletta, Anderson, &amp; Garrod, 2002 ) has shown that there is a noticeable difference in the interaction patterns between large and small groups. A small group discussion involving up to seven participants resembles two-way conversations that occur between all pairs of participants and every participant can initiate conversation. A large group dis-cussion is more like a series of conversations between a group leader and various individuals with the rest of participants present but silent. In the M4 and AMI data collection each meeting consists of 4 participants. Hence, the meetings in our corpus satisfy the interaction patterns of small group discussions. There are 23 participants in the corpus. The total amount of recorded data is approximately 75 min.
 3 Annotation scheme In two-person dialogues, it is usually obvious to the non-speaking participant who is the one being addressed by the current speaker. In a multi-party case, the speaker has not only the responsibility to make his speech understandable for the listeners, but also to make clear to whom he is addressing his speech.

Analysis of the mechanisms that people use in identifying their addressees leads mechanisms. Our annotation scheme is based on the model presented in (Jovanovic verbal, nonverbal and contextual. For example, utterances that contain the proper name of a conversational participant may be addressed to that participant. Also utterances that are related to the previous discourse are addressed to one of the recent speakers.

Although the model contains a rich set of features that are relevant for observers meetings were annotated with a subset of the selected properties. In addition to addressee annotation, the corpus currently contains annotations of dialogue acts, adjacency pairs and gaze direction. We also considered coding of deictic hand ges-tures as they can be used as a means of addressing. However, it was found that deictic hand gestures occur very rarely in the data. 3.1 Dialogue acts Annotation of dialogue acts involves two types of activities: marking of dialogue acts segment boundaries and marking of dialogue acts themselves.

Utterances within speech transcripts, also known as prosodic utterances, were segmented in advance using prosody, pause and syntactical information. In our scheme, a dialogue act segment may contain a part of a prosodic utterance, a whole prosodic utterance, or several contiguous prosodic utterances of the same speaker. Our dialogue act tag set is based on the MRDA (Meeting Recorder Dialogue Act) set (Dhillon, Bhagat, Carvey, &amp; Shriberg, 2004 ). The MRDA tag set represents a modification of the SWDB-DAMSL tag set (Jurafsky, Shriberg, &amp; Biasca, 1997 ) for an application to multi-party meeting dialogues. Each functional utterance in MRDA is marked with a label, made up of one or more tags from the set. The analysis of the MRDA tag set presented in (Clark &amp; Popescu-Belis, 2004 ) shows that the number of possible labels reaches several millions. For that reason, the usage of the complete set may lead to a low quality of manual annotations.

Unlike MRDA, each utterance in our dialogue act annotation scheme is marked as Unlabelled or with exactly one tag from the tag set that represents the most specific utterance function. For addressee identification, it is less important whether an utterance is a suggestion in the form of a question or in the form of a statement. More important is that the speaker suggests to the addressee to perform an action, informing all other participants about that suggestion.

Our dialogue act tag set is created by grouping some of the MRDA tags into 17 categories that are divided into seven groups, as follows:  X  Statements  X  Statement [MRDA: Statement]. The Statement tag marks utterances which are  X  Acknowledgements and Backchannels  X  Acknowledgement [MRDA: Acknowledgement, Backchannel]. The Acknowl- X  Assessment/Appreciation [MRDA:Assessment/Appreciation]. The Assessment/  X  Questions  X  Information Request [MRDA: Wh-Question, Y/N Question, OR-Question, Or  X  Open-ended Question [MRDA: Open-ended Question]. The Open-ended  X  Rhetorical Question [MRDA: Rhetorical Question]. The Rhetorical Question tag  X  Responses  X  Positive Response [MRDA: (Partial) Accept, Affirmative Answer]. The Positive  X  Negative Response [MRDA: (Partial) Reject, Dispreferred Answer, Negative  X  Uncertain Response [MRDA: Maybe, No Knowledge]. The Uncertain Response  X  Action Motivators  X  Influencing-listeners-action [MRDA: Command, Suggestion]. The Influencing- X  Committing-speaker-action [MRDA: Command, Suggestion]. The Committing- X  Checks  X  Follow Me [MRDA: Follow Me]. The Follow Me tag marks utterances by which  X  Repetition Request [MRDA: Repetition Request]. The Repetition Request tag  X  Understanding Check [MRDA: Understanding Check]. The Understanding  X  Politeness Mechanisms  X  Thanks [MRDA: Thanks]. The Thanks tag marks utterances in which a speaker  X  Apology [MRDA: Apology]. The Apology tag marks utterances in which a  X  Other polite [MRDA: Welcome, Downplayer, Sympathy]. The Other polite tag
The MRDA scheme also allows the annotation of turn-taking (e.g., floor grabber) and turn-maintaining (e.g., floor holder) mechanisms. The turn managing dimension of utterances X  functions is excluded from our scheme. Utterances that function only as turn taking, turn giving or turn holding signals are marked as Unlabelled . Turn-taking and addressing as two aspects of conversational interactions are related, but we were specifically interested in studying how addressing actually works, i.e., how people address each other, in order to build predictive models for addressee iden-tification. The scheme also excludes (1) a set of MRDA tags that are related to restating information such as repetitions and corrections, (2) a set of MRDA tags that are related to rhetorical roles such as explanations or elaborations and (3) a set of MRDA tags that provide further descriptions of utterance functions such as self talk, third party talk, jokes, meeting agendas or topic change. 3.2 Adjacency pairs Adjacency pairs (APs) are minimal dialogic units that consist of paired utterances such as question X  X nswer or statement X  X greement. The paired utterances are pro-duced by different speakers. Utterances in an adjacency pair are ordered with the first part (A-part, the initiative) and the second part (B-part, the response). In multi-party conversations, adjacency pairs do not impose a strict adjacency requirement, since a speaker has more opportunities to insert utterances between two elements of an adjacency pair. For example, a suggestion can be followed by agreements or disagreements from multiple speakers.
 In our scheme, adjacency pairs are labelled at a separate level from dialogue acts. Labelling of adjacency pairs consists of marking dialogue acts that occur as their A-part and B-part. If a dialogue act is an A-part with several B-parts, for each of these B-parts, a new adjacency pair is created. Furthermore, each dialogue act is marked pairs. Although it is theoretically possible that a B-part is related to several A-parts, for example, an utterance may answer two questions, the analysis of the data showed that these cases hardly occur. 3.3 Addressees In a group discussion, many of the speaker X  X  utterances are addressed to the group as a whole. However, the speaker may show by verbal or non-verbal behaviour that he intends to affect one selected participant or a subgroup of participants in particular, that he expects that participant or that subgroup to react on what he says. In this performed by the speaker.

Given that each meeting in the corpus consists of four participants, the addressee tag set contains the following values:  X  a single participant: P x  X  a subgroup of participants: P x ,P y  X  the whole audience: P x ,P y ,P z  X  Unknown x , y , z 2 {0,1,2,3}; P x denotes speaker at the channel x . The Unknown tag is 3.4 Gaze direction Annotation of gaze direction involves two types of activities: labelling the changes in the gazed targets and labelling the gazed targets themselves.
 For addressee identification, the only targets of interest are meeting participants. Therefore, the tag set contains tags that are linked to each participant (P x ) where x 2 {0,1,2,3} and the NoTarget tag that is used when the speaker does not look at any of the participants. The set can be further refined by adding some objects of interests in the meeting room such as whiteboard, projector screen or notebook.

Since the meeting room was not equipped with close-up cameras the gaze information was induced based on the side and central cameras (see Fig. 1 ). This was the main reasons for not imposing the requirement for a high precision in labelling changes in the gazed targets. 4 Annotation tools The corpus was created using two annotation tools developed at the University of Twente: the DACoder (Dialogue Act Coder) and the CSL (Continuous Signal Labelling) tools (Reidsma, Hofs, &amp; Jovanovic, 2005 ). The DACoder supports annotation of dialogue acts, addressees and any kind of relations between dialogue acts such as adjacency pairs or rhetorical relations. The CSL tool supports labelling of time-aligned annotation layers directly related to the signal files. Any annotation layer that consists of simple labelling of non-overlapping segments of the time line can be coded using this tool (e.g., gaze directions or postures).
 The tools were built using NXT (NITE XML Toolkit) (Carletta et al., 2003 ). NXT uses a stand-off XML data storage format which consists of several inter-related xml-files. The structure and location of the files are represented in a  X  X  X etadata X  X  file. The NXT stand-off XML format enables the capture and efficient manipulation of complex hierarchical structures across different modalities. Fur-thermore, it supports an easy extension of the corpus with new annotation layers without influencing exiting annotations. For exploitation of annotated data, NXT provides the NXT Search tool for the execution of the queries expressed in the NXT Query Language (NQL). 5 5 Distributional statistics addressee tags in the corpus. The corpus contains 1457 dialogue act segments out of which 131 segments (8.99%) are labelled as Unlabelled . Table 1 shows the distri-bution of DA tags after discarding those segments that are marked as Unlabelled . The distribution of the addressee tags over those segments that are marked with a DA label is presented in Table 2 . All subgroup addressee tags (P x ,P y ) are grouped grouped into the ALLP category. 6 Reliability In order to obtain valid research results, data on which they are based must be reliable. We have performed two reliability tests proposed by Krippendorff ( 1980 ): stability (intra-annotator reliability) and reproducibility (inter-annotator reliability). Stability is the degree to which an annotator X  X  judgments remain unchanged over time. It is measured by giving the same annotator a set of data to annotate twice, at different times. Reproducibility is the degree to which different annotators can produce the same annotation. It is measured by giving several annotators the same data to annotate independently, following the same coding instructions.

Reliability is a function of agreement achieved among annotators. In the dialogue and discourse processing community, the Kappa agreement coefficient ( j ) has been adopted as a standard (Carletta, 1996 ; Cohen, 1960 ). In recent years, there have been some discussions about the usage of Kappa as an appropriate reliability metric. Krippendorff X  X  Alpha ( a ) has been proposed as a more adequate metric for assessing reliability of subjective codings (Krippendorff, 1980 , 2004 ).

To estimate reliability of dialogue act, addressee and gaze annotation, we applied both agreement coefficients. The obtained Kappa and Alpha values were identical. Therefore, in the following sections we report only Kappa values. In contrast to dialogue act and addressee annotation, adjacency pairs annotation cannot be con-sidered as a simple labelling of annotation units with categories. Therefore, we developed our own approach that represents annotated APs in a form of categorical labelling and measures agreement on APs annotation using Alpha.

For the evaluation of Alpha and Kappa values, we used Krippendorff X  X  scale that has been adopted as standard in the discourse and dialogue processing community (Krippendorff, 1980 ). According to that scale, any variable with an agreement coefficient below .67 is disregarded as unreliable, between .67 and .8 allows drawing tentative conclusions and above .80 allows drawing definite conclusions. 6.1 Detecting sources of unreliability Detecting causes of disagreement may be of great use to obtain reliable data or to improve data reliability. A source of unreliability can be a coding unit, a category, a subset of categories or an annotator (Krippendorff, 1980 ). Even if a category is well defined, annotators may still have different interpretations of the category. Fur-thermore, annotators may show a correlated disagreement. For example, annotator A 1 uses category C 1 whenever annotator A 2 uses category C 2 .

To identify which categories are sources of unreliability we measured single-category reliability (Krippendorff, 1980 ). Single-category reliability assesses the extent to which one category is confused with all other categories in a set. It is estimated by grouping the remaining categories into one category and measuring the agreement among annotators regarding the assignment of units to these two categories. A low agreement can be the result of an ambiguous definition of the category or of the coders X  inability to interpret the meaning of the category. 7 Inter-annotator reliability In this section we present the analysis of inter-annotator reliability of the annotation scheme applied on the M4 meeting data.

Six trained annotators were involved in the corpus creation. They were divided into two groups: the DA (Dialogue Act) group and the VL (Video Labelling) group. The DA group, involving 4 annotators, annotated dialogue acts, addressees and adjacency pairs. The VL group, involving 2 annotators, annotated gaze direction. The corpus was divided into two sets of meetings. The DA group was divided into 2 subgroups of 2 annotators: the B&amp;E group and the M&amp;R group. Each of these subgroups annotated exactly one set of meeting data. Each annotator in the VL group annotated one set of meeting data. Additionally, two meetings were annotated by both annotators in the VL group in order to test reliability of gaze annotation. In summary, each meeting in the corpus was annotated with dialogue acts, addressees and adjacency pairs by exactly two annotators, and with participants X  gaze directions by at most two annotators. 7.1 Reliability of dialogue acts annotation We first measured agreements among annotators on how they segmented dialogues into dialogue act segments. Then, we tested reliability of dialogue act classification on those segments for which annotators agreed on their boundaries. 7.1.1 Segmentation reliability In the discourse and dialogue community, several approaches have been proposed for assessing segmentation reliability using various metrics: percent agreement and recall (Passonneau &amp; Litman, 1997 ), and j (Carletta et al., 1997 ; Hirschberg &amp; Nakatani, 1996 ).

Since there is no standardized technique to estimate segmentation agreement, we developed our own approach based on percent agreement. We defined four types of segmentation agreement:  X  Perfect agreement (PA)  X  X nnotators completely agree on the segment  X  Contiguous segments of the same type (ST)  X  X  segment of one annotator is  X  Unlabelled-DA (UDA)  X  X  segment of one annotator is divided into two seg- X  Conjunction-Floor(CF)  X  X wo adjacent segments differ only in a conjunction or a (1) I can do that X  X ut I need your help (2) I can do that but X  X  need your help
The approach takes one annotator X  X  segmentation as a reference ( R ) and com-pares it with the other annotator X  X  segmentation ( C ) segment by segment. As a match the reference segmentation ( R ) according to identified types of agreement. In addition to measuring segmentation agreement, the modified segmentation ( C  X  )is and adjacency pairs annotation. Table 3 shows overall segmentation results for each annotation group.

Most of the segmentation disagreements are of the following three types. First, while one annotator labelled a segment with the Acknowledgement tag, the other one included the segment in the dialogue act that follows. Second, while one annotator marked a segment with one of the response tags, the other annotator split the segment into a response and a statement that has a supportive function such as explanation, elaboration or clarification. Third, while one annotator split a segment into two or more segments labelled with the same dialogue act tag but different addressee tags, the other annotator marked it as one segment. 7.1.2 Reliability of dialogue act classification Reliability of dialogue act classification is measured over those dialogue act seg-ments for which both annotators agreed on their boundaries. Since the number of agreed segments for each R X  X  pair is different, we calculated reliability of dialogue act classification for each pair. The results are shown in Table 4 . According to Krippendorff X  X  scale annotators in each DA group reached an acceptable level of agreement that allows drawing tentative conclusions from the data.

We applied a single-category reliability test for each dialogue act tag to assess the extent to which one dialogue tag was confused with the other tags in the set. Table 5 shows the results of performing the Kappa tests for only one R X  X  pair in each DA group.

Annotators in the B&amp;E group used different ranges of categories: the Other polite and Rhetorical Question categories, which occur rarely in the data, were employed only by annotator B. For that reason, Kappa values for these categories are zero. Negative Kappa values for Understanding Check and Follow me categories indicate that annotator agreement is below chance: in all cases where one annotator identifies one of these two categories, the other annotator does not. The results show an unacceptably low agreement on Assessment/Appreciation and Understanding Check categories in both groups. The Assessment/Appreciation category was mainly con-fused with Positive Response and Statement categories. The Understanding Check category was mostly confused with Information Request and Statement categories. Annotators in the M&amp;R group reached a lower agreement on the responses tags than annotators in the B&amp;E group. The responses tags were mostly confused with the Statement tag. Additionally, annotators in the M&amp;R group had a little more difficulty distinguishing Positive Response from Assessment/Appreciation and Acknowledgement . The low Kappa value for the Influencing-listener-actions category in the B&amp;R group is a result of the confusion with the Statement category. 7.2 Reliability of addressee annotation As for dialogue act classification, reliability of addressee annotation is measured over those dialogue act segments for which both annotators agreed on their boundaries.

The Kappa values for addressee annotation are shown in Table 6 . The results show that annotators in the B&amp;E group reached good agreement on addressee annotation, whereas annotators in the M&amp;R group reached an acceptable level of agreement.

Annotators mainly disagreed on whether an individual or a group had been ad-dressed. When annotators agreed that an individual had been addressed, they agreed in almost all cases which individual it had been. There were only a few instances in the data labelled with categories that represent subgroup addressing. In both DA groups, annotators failed to agree on those categories. Annotators had problems distinguishing subgroup addressing from addressing the group as a whole.

We measured single-category reliability for those addressee tags that represent individual and group addressing. Single-category reliability is measured using the Kappa test for one R X  X  pair in each group. Addressee values that consist of three sents the whole audience (ALLP). Annotators in the B&amp;E group reached a good agreement ( j  X  0.80; N = 369) on all categories representing a single participant.
Agreement on ALLP was j = 0.77. Annotators in the M&amp;R group reached a lower agreement on each category than annotators in the B&amp;E group. They had a little more difficulty distinguishing ALLP ( j = 0.63; N = 366) as well as p 3 ( j = 0.59; N = 366) from a remaining set of categories. For all other categories representing a single participant Kappa was 0.71  X  j &lt; 0.80. 7.3 Reliability of adjacency pairs annotation According to our scheme for annotation of adjacency pairs, each dialogue act can be marked as a B-part of at most one and as an A-part of an arbitrary number of adjacency pairs. The sets of adjacency pairs produced by two annotators may differ in several ways. First, the annotators may disagree on dialogue acts that are marked as A-parts of adjacency pairs. Second, they may assign a different number of B-parts as well as different B-parts themselves to the same A-part.

Since there seems to be no standard associated metric for agreement on APs annotation in the literature, we developed a new approach that resembles a method for measuring reliability of co-reference annotation proposed in (Passonneau, 2004 ). The key of the approach is to represent annotated data as a form of categorical labelling in order to apply standard reliability metrics.

Adjacency pairs annotation can be seen as assigning to each dialogue act a context that represents the relations that the dialogue act has with surrounding dialogue acts. To encode the contexts of dialogue acts, we define a set of classes that contain Therefore, a class is characterized with its A-part and a set of B-parts (b-set): dialogue act is encoded with an AP label (L) that is compounded of its A-class and B-class ( L =A-class|B-class).

Given a list of dialogue acts DA = [ da 1 ,..., da n ], a class can be represented in two different ways: with fixed or relative position of the dialogue acts. The former en-codes each dialog act in the class with the index of the dialog acts in the list. The latter encodes the dialogue acts in the class with relative positions to the dialogue act representing the A-part of the class. In this paper, we use the approach with relative positions because it significantly decreases the number of possible classes. In our the class and O is a set of offsets of the dialogue acts in the b-set from the A-part of the class. Note that for the A-class, n is always 0 since the labelled dialogue act is the A-part of the class. For the B-class, n is always positive because the labelled dialogue act is in the b-set and the A-part always precedes dialogue acts in the b-set. Thus,  X  n labelled dialogue act is not an A-part or a B-part of an adjacency pair, one or both of the A-class and the B-class can be empty (&lt; 0,{} &gt;).

The proposed encoding makes patterns of disagreements between annotators directly visible. For example, (1) if one annotator marks the dialogue act 43 as an A-part of two adjacency pairs with B-parts 44 and 45, respectively, and the dialogue act 45 as an A-part of an adjacency pair with the B-part 47, and (2) the other annotator marks the dialogue act 44 as an A-part of an adjacency pair with the B-part 45 and the dialogue act 45 as an A-part of two adjacency pairs with B-parts 46 and 47, respectively, then the dialogue acts will be labelled as presented in Table 7 . Fig. 2 illustrates the relation between the context of the dialogue act 45 and the AP label that encodes this context.

Encoding context in this way enables us to estimate for each dialogue act to what extent annotators agree on relating that dialogue act with surrounding dialogue acts in several ways: (1) as being an A-part related to a number of B-parts, (2) as being a B-part related to other B-parts with the same A-part and (3) not being related at all. example, it is possible to label each dialogue act that is marked as an A-part with its b-set. In this way, the actual disagreement is estimated only over A-parts. As context labels are not assigned to dialogue acts marked as B-parts, these dialogue acts would always be considered as agreed.

Agreement on APs annotation is measured over those dialogue acts for which annotators agreed on their boundaries. For computing agreement between annotators we use Krippendorff X  X  a measure. This measure allows the usage of an appropriate user defined distance metric on the AP labels. For nominal categories, otherwise d = 1. We need to use a more refined distance metric, one that is sensitive for partial agreement of annotators on the context they assign to a dialogue act. The agreement on the contexts is translated to agreements on the corresponding A-classes and B-classes. When annotators disagree, their disagreement should be penalized based on the difference between classes.

The intuition is that similarity of two classes with the same A-part depends not only on the number of elements in the intersection of their b-sets, but also on the size of both sets. Therefore, we define a distance metric d  X  that uses the following sim-ilarity measure on sets: 6
The distance metric ( d  X  ) between the corresponding A-classes (or B-classes) of two AP labels is defined as:
The distance between two AP labels, L 2 = A 1 | B 1 and L 2 = A 2 | B 2 , is defined as: between the corresponding classes the labels consist of.

Applying d 0.5 to the data of exactly one R X  X  pair in each group gave the following results: M X  X : a = 0.71 ( N = 260), B X  X : a = 0.83 ( N = 322). The most frequently occurring disagreement is when one annotator marks a dialogue act with the empty label, the other annotator with a non-empty one. If annotators agreed that a dialogue act is an A-part of an adjacency pair, they mostly agreed, either partially or fully, on the B-set of this dialogue act. In most cases, the confusion between (1) an AP label with both A-class and B-class non-empty and (2) an AP label with one of the classes empty is related to the disagreement on the DA tags assigned by annotators. This concerns the confusion between (i) Statement and Assessment/Appreciation tags, (ii) Statement and Response tags and (iii) Understanding Check and Information Request tags. 7.4 Reliability of gaze annotation To evaluate reliability of gaze annotation, we first measured annotators agreement on marking the changes in gazed targets. Then, we measured agreement on labelling of time segments with gazed targets.

Marking the changes in gazed targets results in a segmentation of the time-line into non-overlapping, continuous segments that cover the whole input. In other words, the start time of a segment coincidences with the end time of the segment that precedes. A segment boundary indicates a change in gazed target.

The segmentation agreement is measured over all locations where any of the annotators marked a segment boundary. The number of locations where both annotators agree to some tolerance level is averaged over the total number of locations marked as a boundary. A tolerance level is introduced because the gaze annotation schema does not impose the requirement for a high precision on labelling change is marked at the moment when the speaker starts changing the gaze direction or at the moment when the new target has been reached. It also adjusts the differ-ence in the reaction of the annotators to the observed changes. Empirical analysis of the data shows that two points of the time-line can be considered equal with a tolerance level of 0.85 s.

The agreement on locations where any coder marked a segment boundary is 80.40% ( N = 939). Annotators mostly disagreed on marking the cases when a par-ticipant briefly changes the gaze direction and then looks again at the previous target. Annotators reached very good agreement on gaze labelling ( j = 0.95) mea-sured over those segments where boundaries were agreed. 8 Intra-annotator reliability remain consistent over time. We assessed intra-annotator reliability of dialogue act and addressee annotation. One meeting from each data subset has been annotated twice by each annotator in the DA group over a period of three months. The results presented in Table 8 show that agreement on dialogue act annotation was good for each annotator indicating intra-annotator consistency in applying the dialogue act scheme. Furthermore, the results show that annotator R had a little more difficulty with addressee annotation than other annotators who reached good agreement.
 9 Discussions and conclusions We presented a multi-modal corpus of hand-annotated meeting dialogues that is designed for studying addressing behaviour in face-to-face conversations involving four participants. The corpus currently contains dialogue acts, addressees, adjacency pairs and gaze directions of meeting participants.

Annotators involved in the corpus design were able to reproduce the gaze labelling reliably. The annotations of dialogue acts and addressees were somewhat addressing in the data and annotators failed to agree on them, the corpus cannot be used for exploring the patterns in addressing behaviour when a subgroup is ad-dressed. In this paper, we have also presented a new approach for measuring reli-ability of adjacency pairs annotation. The key of the approach is to represent AP annotated data as a form of categorical labelling in order to apply standard reliability metrics.

The corpus has already been used for the development of models for automatic addressee prediction (Jovanovic, op den Akker, &amp; Nijholt, 2006 ). Apart from addressing, the corpus can be exploited for studying other interesting aspects of conversations involving more than two participants. As the NXT stand-off XML format enables an easy extension of the corpus with new annotation layers without influencing existing annotations, the corpus can be extended to include, for example, coding of turn-taking mechanisms which would enable studying this aspect of con-versational interaction independently as well as in relation to addressing. References
