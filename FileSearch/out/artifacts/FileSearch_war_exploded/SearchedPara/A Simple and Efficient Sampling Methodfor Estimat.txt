 We consider the problem of large scale retrieval evaluation. Recently two methods based on random sampling were pro-posed as a solution to the extensive effort required to judge tens of thousands of documents. While the first method pro-posed by Aslam et al. [1] is quite accurate and efficient, it is overly complex, making it difficult to be used by the com-munity, and while the second method proposed by Yilmaz et al., infAP [14], is relatively simple, it is less efficient than the former since it employs uniform random sampling from the set of complete judgments. Further, none of these methods provide confidence intervals on the estimated values.
The contribution of this paper is threefold: (1) we de-rive confidence intervals for infAP, (2) we extend infAP to incorporate nonrandom relevance judgments by employing stratified random sampling, hence combining the efficiency of stratification with the simplicity of random sampling, (3) we describe how this approach can be utilized to esti-mate nDCG from incomplete judgments. We validate the proposed methods using TREC data and demonstrate that these new methods can be used to incorporate nonrandom samples, as were available in TREC Terabyte track  X 06. Categories and Subject Descriptors: H.3 Information Storage and Retrieval; H.3.4 Systems and Software: Perfor-mance Evaluation General Terms: Experimentation, Measurement, Theory Keywords: Evaluation, Sampling, Incomplete Judgments, Average Precision, nDCG, infAP
We consider the problem of large scale retrieval evaluation, in particular, retrieval evaluation with incomplete relevance judgments.
We gratefully acknowledge the support provided by NSF grants IIS-0533625 and IIS-0534482.

The most commonly employed methodology for assessing the quality retrieval systems is the Cranfield methodology adopted by TREC [7]. The main assumption behind this methodology is that the relevance judgments are complete , i.e., for each query, all documents in the collection relevant to this query are identified.

When the document collection is large, obtaining com-plete relevance judgments is infeasible due to the need for extensive human effort. Instead, TREC employs depth-k pooling (typically, depth-100 pooling) to overcome this bur-den, combining top k documents retrieved by the submitted systems and assuming the rest of the documents are nonrel-evant. While a depth-100 pool is significantly smaller than the document collection, it still requires extensive judgment effort (e.g. 86,830 judgments for TREC 8). Furthermore, even though depth-100 pools were shown to contain most of the relevant documents [8], the size of document collections tends to increase and thus these pools may be inadequate for identifying most of the relevant documents.

Recently, evaluation with incomplete relevance judgments has gained attention as a solution to the problem of exten-sive judgment effort. New evaluation measures have been proposed in the literature [1, 5, 11, 12, 14] since standard evaluation measures such as average precision are not robust to incomplete judgments [3].

As a solution to this problem, Buckley and Voorhees [3] proposed bpref , a now commonly used measure by informa-tion retrieval community. Sakai [12] instead applied tradi-tional measures to condensed lists of documents obtained by filtering out all unjudged documents from the original ranked lists and showed that these versions of measures are actually more robust to incompleteness than bpref.
Carterette et al. [5] and Moffat et al. [11] select a subset of documents to be judged based on the benefit documents provide in fully ranking systems or identifying the best sys-tems, respectively.

Even though the aforementioned approaches are all shown to be more robust to incompleteness than standard evalu-ation measures, these methods are not guaranteed to com-pute or estimate the values of standard evaluation measures. Hence, the values of measures obtained by these methods are difficult to interpret.

Yilmaz and Aslam [14] and Aslam et al. [1] instead use random sampling to estimate the actual values of average precision when relevance judgments are incomplete. Both of these methods are based on treating incomplete relevance judgments as a sample drawn from the set of complete judg-ments and using statistical methods to estimate the actual values of the measures. These methods are both shown to (1) produce unbiased estimates of average precision even when relevance judgments are incomplete and (2) be more robust to incomplete relevance judgments than any other measures such as bpref [3] or the condensed versions of the measures (referred to as induced AP in the paper)[14].

The measure proposed by Yilmaz and Aslam, infAP [14], became a commonly used measure by information retrieval community [2, 13] and was used in TREC VID and Terabyte tracks in 2006 [10, 4].

A limitation of infAP accrues from the measure X  X  assump-tion that incomplete relevance judgments are a simple ran-dom sample drawn from the set of complete judgments. Typical evaluation measures give more weight to documents retrieved towards the top of a retrieved lists and therefore, a  X  X op-heavy X  sampling strategy would lead to more accu-rate results with higher efficiency in terms of judgment effort needed.

On the other hand, according to the method by Aslam et al. [1] samples are drawn according to a carefully chosen non-uniform distribution over the documents in the depth-100 pool. Even though this method is more efficient in terms of judgment effort than infAP, it is very complex both in con-ception and implementation and therefore less applicable.
Furthermore, although average precision estimators as pro-posed by both of the aforementioned methods are unbiased in expectation, in practice, when calculated using a single sample of relevance judgments, may vary in value. This necessitates the derivation and use of confidence intervals around the estimated values in order to allow confident con-clusions regarding the actual value of average precision and thus the ranking of retrieval systems.

In this paper, we mainly focus on inferred average preci-sion. First, we derive confidence intervals for the measure and validate them using TREC data. We show that in-fAP along with the corresponding confidence intervals can allow researchers to reach confident conclusions about ac-tual average precision, even when relevance judgments are incomplete.

We then focus on the efficiency of the measure. We em-ploy a stratified random sampling methodology and extend the measure to incorporate relevance judgments created ac-cording to any such sampling distribution. This extended infAP combines the simplicity of random sampling with the efficiency of stratification and thus it is simple and easy to compute while, at the same time, it is much more efficient than infAP in terms of reducing the judgment effort. We further claim that the same methodology can be applied to other evaluation measures and demonstrate how nDCG (a commonly used measure that incorporates graded relevance judgments [9]) can be estimated using incomplete relevance judgments.
The inferred average precision, by statistical construction, is an unbiased estimator of average precision and thus it is designed to be exactly equal to average precision in expec-tation. However in practice, it may be low or high due to the nature of sampling (especially when the subsets of docu-ments whose binary relevance is available is small). In other words, there is variability in the values of infAP because dif-ferent samples from the collection of documents give rise to different values of infAP. The amount of the variability in infAP is measured by its variance .

Before computing the variance of infAP let X  X  revisit the random experiment whose expectation is average precision [14] and identify all sources of variability in the outcome of this random experiment. Given a ranked list of documents with respect to a given topic: 1. Select a relevant document at random and let the rank 2. Select a rank, j , at random from the set { 1 , ..., k } . 3. Output the binary relevance of the document at rank In expectation, steps (2) and (3) effectively compute the precision at a relevant document and in combination, step (1) computes the average of these precisions.

The aforementioned experiment can be realized as a two-stage sampling. At the first stage  X  step (1)  X  a sample of cut-off levels at relevant documents is selected. The infAP value is computed as an average of the estimated precision values at the sampled cut-off levels. Even if we assume that these precision values are the actual precision values, infAP varies because different samples of cut-off levels will result in different values of infAP. Therefore, computing infAP using precision values only at a subset of cut-off levels introduces the first component of variability.

Let rel be the set of the judged relevant documents of size r . This first variance component can be estimated as 1 where p  X  100% is the sampling percentage and s 2 the variance among the precision values at the judged relevant documents calculated as s 2 =  X  P k  X  rel ( d P C k  X  infAP)  X  /r .
At the second stage  X  step (2)  X  for each one of the se-lected cut-off levels, a sample of documents above that cut-off level document is used to estimate the corresponding to the cut-off precision value. Therefore, even for a given sam-ple of cut-off levels, infAP has variability because different samples of documents give rise to different values of preci-sions and thus different values of infAP. Hence, computing the precision at some cut-off using only a subset of the doc-uments above that cut-off introduces a second component of variability.

Assuming that precisions at different cut-off levels are in-dependent from each other, this second variance component can be estimated as, where var [ d P C k ] is the variance of the estimated precision at cut-off k .

According to the Law of Total Variance, the total variance of infAP can be computed as the sum of the two aforemen-tioned variance components; hence,
The complete formula of infAP variance along with the derivation can be found at the Appendix When evaluating retrieval systems, the average of infAP val-ues across all topics (mean infAP) is employed. The vari-ance of the mean infAP can be computed as a function of the variance of infAP as According to the Central Limit Theorem one can assign 95% confidence intervals to mean infAP as a function of its vari-ance. A 95% confidence interval centered at the mean infAP intimate that with 95% confidence the actual value of MAP is within this interval.

We used TREC 8,9 and 10 data to validate the derived variance of the mean infAP when relevance judgments are incomplete. We simulated the effect of incomplete relevance judgments as in [14]. For each TREC, we formed incomplete judgments sets by sampling from the entire depth-100 pool over all submitted runs. This is done by selecting p % of the complete judgment set uniformly at random, where p  X  { 10 , 20 , 30 } . The results of our experiments led to identical conclusions over all TREC dataset and therefore, due to space limitations, we report only results for TREC 8.
Figure 1 illustrates the mean infAP values computed from a single random sample of documents per topic for each run against the actual MAP values for p  X  X  10 , 20 , 30 } for TREC 8. The 95% confidence intervals are depicted as er-ror bars around the mean infAP values. As one can observe, the greatest majority of the confidence intervals intersect the 45 o dashed line indicating that the greatest majority of the confidence intervals cover the actual MAP values.
Furthermore, we computed the mean infAP values and the corresponding confidence intervals for 100 different sampling trials over TREC 8 data and we accumulated the deviation of the computed mean infAP values from the actual MAP values in terms of standard deviations. This way we gen-erated a Cumulative Distribution Function of divergence of mean infAP values per system. According to the Central Limit Theorem each of these CDF X  X  should match the CDF of the Normal Distribution. We performed a Kolmogorov-Smirnov test of fitness and for 90% of the systems the hy-pothesis that the two CDF X  X  match could not be rejected (  X  = 0 . 05) which validates our derived theoretical results.
In the previous section we derived confidence intervals for infAP in a setup where documents to be judged were a ran-dom subset of the entire document collection. Confidence intervals can be further reduced (i.e. the accuracy of the estimator can be improved) by utilizing a  X  X op-heavy X  sam-pling strategy. In this section we consider a setup where relevance judgments are not a random subset of complete judgments and show how infAP can be extended to produce unbiased estimates of average precision in such a setup. We denote the extended infAP measure as xinfAP .

Similar to the infAP paradigm, consider the case where we would like to evaluate the quality of retrieval systems with respect to a complete pool and assume that relevant judg-ments are incomplete. Further assume that the set of avail-able judgments are constructed by diving the complete col-lection of documents into disjoint contiguous subsets (strata) and then randomly selecting (sampling) some documents from each stratum to be judged. The sampling within each stratum is performed independently, therefore, the sampling percentage can be chosen to be different for each stratum. For instance, one could choose to split the collection of doc-uments into two strata (based on where they appear in the output of search engines), and sample 90% of the documents from the first stratum and 30% of the documents from the second stratum. In effect, one could think a large variety of sampling strategies in terms of this multi-strata strategy. For example, the sampling strategy proposed by Aslam et al. [1] can be thought as each stratum containing a single document, with different sampling probabilities assigned to different strata.

Let d AP be the random variable corresponding to the es-timated average precision of a system. Now consider the first step of the random experiment whose expectation cor-responds to average precision, i.e. picking a relevant docu-ment at random. Note that in the above setup, this relevant document could fall into any one of the different strata s . Since the sets of documents contained in the strata are dis-joint, by definition of conditional expectation, one can write E [ d AP ] as: where P s corresponds to the probability of picking the rele-vant document from stratum s and E s [ d AP ] corresponds to the expected value of average precision given that the rele-vant document was picked from stratum s .

Let R Q be the total number of relevant documents in the complete judgment set and R s be the total number of rele-vant documents in stratum s if we were to have all complete relevance judgments. Then, since selecting documents from different strata is independent for each stratum, the prob-ability of picking a relevant document from stratum s is, P
Computing the actual values of R Q and R s is not pos-sible, since the complete set of judgments is not available. However, we can estimate their values using the incomplete relevance judgments. Let r s be the number of sampled rele-vant documents and n s be the total number of sampled doc-uments from stratum s . Furthermore, let N s be the total number of documents in stratum s . Since the n s documents were sampled uniformly from stratum s , the estimated num-ber of relevant documents within stratum s ,  X  R s , can be com-puted as  X  R s = ( r s /n s )  X  N s . Then the number of relevant documents in query Q can be estimated as the sum of these estimates over all strata, i.e.  X  R Q = P  X  s  X  R s . Given these es-timates, the probability of picking a relevant document from stratum s can be estimated by,  X  P s =  X  R s /  X  R Q .
Now, we need to compute the expected value of estimated average precision, E s [ d AP ], if we were to pick a relevant doc-ument at random from stratum s .

Since the incomplete relevance judgments within each stra-tum s is a uniform random subset of the judgments in that stratum, the induced distribution over relevant documents within each stratum is also uniform, as desired. Therefore, the probability of picking any relevant document within this stratum is equal. Hence, the expected estimated average precision value within each stratum, E s [ d AP ], can be com-puted as the average of the precisions at judged (sampled) relevant documents within that stratum.
Now consider computing the expected precision at a rele-vant document at rank k , which corresponds to the expected outcome of picking a document at or above rank k and out-putting the binary relevance of the document at this rank (steps 2 and 3 of the random experiment).

When picking a document at random at or above rank k and outputting the binary relevance of that document, one of the following two cases may occur. With probability 1 /k , we pick the current document, and since this document is by definition relevant the outcome is 1. With probability ( k  X  1) /k we pick a document above the current document, in which case we need to calculate the expected precision (or expected binary relevance) with respect to the documents above rank k . Thus,
Let N k  X  1 s be the total number of documents above rank k that belong stratum s , n k  X  1 s be the total number of judged (sampled) documents above rank k that belong to stratum s and r k  X  1 s be the total number of judged (sampled) relevant documents above rank k that also belong to stratum s .
When computing the expected precision within the ( k  X  1) documents above rank k , with probability N k  X  1 s / ( k  X  1) we pick a document from stratum s . Therefore, the expected precision above rank k can be written as: where E s [ d P C above k ] is the expected precision above rank k within stratum s . Since we have a uniform sample of judged documents from stratum s , we can use these sampled doc-uments to estimate the expected precision within stratum s . Since the incomplete relevance judgments from each stra-tum is obtained by uniform random sampling, this expected precision can be computed as r k  X  1 s /n k  X  1 s .
Note that in computing the expected precision in stratum s , we may face the problem of not having sampled any docu-ments from this stratum that are above the current relevant document at rank k . Adapting the same idea used in infAP, we employ Lindstone smoothing [6] to avoid this problem. Therefore, expected precision above rank k can be computed as: It is easy to see that when complete judgments are available, xinfAP is exactly equal to average precision (ignoring the smoothing effect). Further, note that infAP is a particular instantiation of this formula with a single stratum used.
Overall, the advantage and real power of the described stratified random sampling and the derived AP estimator, xinfAP, is the fact that it combines the effectiveness of the sampling method proposed by Aslam et al. [1] by employing stratification of the documents and thus better utilization of the judgment effort with the simplicity of infAP by employ-ing random sampling within each stratum.
As mentioned, xinfAP can be used with a large variety of sampling strategy. In this section, we focus on the sampling strategy used in TREC Terabyte 2006 [4] and we show that (1) xinfAP is highly effective at estimating average precision and (2) it better utilizes the judgment effort compared to infAP.
 First, let X  X  briefly consider the sampling strategy used in TREC Terabyte 2006. In this track, three different sets of relevance judgments were formed, with only two of them being used for evaluation purposes. Out of these two sets, the first set of judgments, constructed by the traditional depth-50 pooling strategy, was used to obtain a rough idea of the systems average precision. The second set of judg-ments was constructed using random sampling in such a way that there are more documents judged from topics that are more likely to have retrieved more relevant documents. Since, in Terabyte track, the size of the document collection is very large, the systems may continue retrieving relevant documents even at high ranks (deeper in the list). This set of judgments was created to obtain an estimate of average precision if complete judgments were present.

To estimate average precision, infAP was used as the eval-uation measure. Since, by design, infAP assumes that the set of relevance judgments is a random subset of complete judgments, even though the entire depth-50 pool was judged, infAP was computed only using the random sample of judg-ments (second set) without utilizing judgments from the depth-50 pool. Therefore, many relevance judgments were not used even though they were available.

Note that xinfAP can easily handle this setup and it could be used to utilize all the judgments, obtaining better esti-mates of average precision.

To test how xinfAP compares with infAP we simulate the sampling strategy used in TREC Terabyte 06 on data from TREC 8. The TREC Terabyte data was not used due to the fact that in TREC Terabyte the actual value of average precision is not known since complete judgments are not available.

To simulate the setup used in TREC Terabyte, we first form different depth-k pools where k  X  X  1 , 2 , 3 , 4 , 5 , 10 , 20 , 30 , 40 , 50 } and obtain judgments for all documents in each one of these pools. Then, for each value of k , we com-pute the total number of documents that are in the depth-k pool and we randomly sample equal number of documents from the complete judgment set 2 excluding the depth-k pool. After forming these two sets of judgments (depth-k and ran-dom) we combine them and compute xinfAP on these com-bined judgments.

This setup exactly corresponds to a sampling strategy where complete judgments are divided into two strata and judgments are formed by uniformly and independently sam-pling within each stratum.

Note that in TREC, there are some systems that were submitted but that did not contribute to the pool. To fur-ther evaluate the quality of our estimators in terms of their robustness for evaluating the quality of unseen systems (sys-tems that did not contribute to the pool), when we form the incomplete relevance judgments, we only consider the sys-tems that contribute to the pool but we compute the xinfAP estimates for all submitted systems.

Figure 2 demonstrates how xinfAP computed using judg-ments generated by combining (left) depth-10, (middle) depth-5 and (right) depth-1 pools with equal number of randomly sampled judgments compares with the actual AP. Each of these depths correspond to judging 23 . 1%, 12 . 7% and 3 . 5% of the entire pool, respectively. The plots report the RMS er-ror (how accurate are the estimated values?), the Kendall X  X   X  value (how accurate are the estimated rankings of sys-tems?) and the linear correlation coefficient,  X  , (how well do the estimated values fit in a straight line compared to the actual values?). The dot signs in the figures refer to the systems that were used to create the original pools and the plus signs refer to the systems that did not contribute to the pool.

The results illustrated in these plots reinforce our claims that xinfAP is an unbiased estimator of average precision. Furthermore, it can be seen that the measure can reliably be used to evaluate the quality of systems that were not used to create the initial samples, hence the measure is robust to evaluating the quality of unseen systems.

Figure 3 illustrates how xinfAP computed on a non-random judgment set compares with infAP computed on a random judgment set for various levels of incompleteness. In a sim-ilar manner to the experimental setup of the original infAP work, for each value of k , we generated ten different sample trials according to the procedure described in the previous paragraph, and for each one of the ten trials we computed the xinfAP for all systems. Then, all three statistics were computed for each one of the trials and the averages of these statistics over all ten trials were reported for different levels of judgment incompleteness. Using the same procedure, we also created ten different sample trials where the samples were generated by merely randomly sampling the judgment
Throughout this paper, we assume that the complete judg-ment set corresponds to the depth-100 pool as the judgments we have are formed using depth-100 pools and assuming the remaining documents are nonrelevant. set and the infAP values were computed on them. For com-parison purposes, to show how the original version of infAP behaves when this randomness assumption is violated, we also include infAP run on the same judgment set as ex-tended infAP (marked as infAP depth+random judgments in the Figure).

It can be seen that for all levels of incompleteness, in terms of all three statistics, xinfAP is much more accurate in estimating average precision than the other two measures.
We further compared xinfAP to the sampling method pro-posed by Aslam et al. [1]. The robustness of xinfAP to in-complete relevance judgments is comparable to (and in some cases even better than) this method. (These results were omitted due to space limitations.)
There are different versions of the nDCG metric depend-ing on the discount factor and the gains associated with relevance grades, etc. In this paper, we adopt the version of nDCG in trec_eval .

Let = denote a relevance grade and gain ( = ) the gain as-sociated with = . Also, let g 1 , g 2 , . . . g Z be the gain values associated with the Z documents retrieved by a system in response to a query q , such as g i = gain ( = ) if the relevance grade of the document in rank i is = . Then, the nDCG value for this system can be computed as, and DCG I denotes the DCG value for an ideal ranked list for query q .

The estimation of nDCG with incomplete judgments can be divided into two parts: (1) Estimating DCG I and (2) Estimating DCG. Then, the DCG and the DCG I values can be replaced by their estimates to obtain the estimated nDCG value. 3
The normalization factor, DCG I , for a query q can be defined as the maximum possible DCG value over that query. Hence, the estimation of DCG I can be derived in a two-step process: (1) For each relevance grade = such as gain ( = ) &gt; 0, estimate the number of documents with that relevance grade; (2) Calculate the DCG value of an optimal list by assuming that in an optimal list the estimated number of documents would be sorted (in descending order) by their relevance grades.

Using the sampling strategy described in the previous sec-tion, suppose incomplete relevance judgments were created by diving the complete pool into disjoint sets (strata) and randomly picking (sampling) documents from each stratum to be judged, possibly with different probability for each stratum. Note that this assumes that E [ nDCG ] = E [ DCG ] /E [ DCG I ], i.e., that DCG I and DCG are in-dependent of each other, which is not necessarily the case. This assumption may result in a small bias and better estimates of nDCG can be obtained by considering this dependence. However, for the sake of simplicity, throughout this paper, we will assume that these terms are independent. random subset of complete judgments.

For each stratum s , let r s ( = ) be the number of sampled documents with relevance grade = , let n s be the total num-ber of documents sampled from strata s and N s be the total number of documents that fall in strata s . Since the n s uments are sampled uniformly from strata s , the estimated number of documents with relevance grade = within this strata can be computed as Then, the expected number of documents with relevance grade = within the complete pool can be computed as Once these estimates are obtained, one can estimate DCG I
Given Z documents retrieved by a search engine with rel-evance gain g i for the document at rank i , for each rank i , define a new variable x i such as x i = Z  X  g i lg ( i +1) DCG can be written as the output of the following random experiment: 1. Pick a document at random from the output of the 2. Output the value of x i .

It is easy to see that if we have the relevance judgments for all Z documents, the expected value of this random ex-periment is exactly equal to DCG.

Now consider estimating the outcome of this random ex-periment when relevance judgments are incomplete. Con-sider the first step of the random experiment, i.e. picking a document at random. Let Z s be the number of documents in the output of a system that fall in stratum s . When pick-ing a document at random, with probability Z s /Z , we pick a document from stratum s .

Therefore, the expected value of the above random exper-iment can be written as: Now consider the second step of the random experiment, computing the expected value of x i given that the document at rank i falls in strata s . Let sampled s be the set of sampled documents from strata s and n s be the number of documents sampled from this strata. Since documents within stratum s are uniformly sampled, the expected value of x i can be computed as Once E[DCG I ] and E[DCG] are computed, infNDCG can then be computed as infNDCG = E [ DCG ] /E [ DCG I ].
Until now, we have shown that using a similar sampling strategy as the one used in TREC Terabyte 06 (complete judgments divided into 2 different strata), xinfAP is highly accurate. In this section, we show that (1) this claim is con-sistent over different TRECs for both xinfAP and infNDCG and that (2) the two measures can be used with the complete judgments divided into more than two strata.

In order to check (2), we use a different sampling strategy than the one in Terabyte; we divide the complete judgment set (assuming depth-100 pool is the complete judgment set) into 4 different strata. The first stratum is the regular depth-k pool, fully judged. Instead of randomly sampling equal to the depth-k pool number of judgments from the remainder of the collection, we now divide the rest of the documents into three other strata and distribute the remaining judgments with a ratio of 3:1.5:1 (judge 55% of the documents in the top depth stratum, 27% of the documents in the middle depth stratum and 18% in the lowest depth stratum). This way, more weight is given to judging documents retrieved towards the top of the ranked lists of the search engines. Note, however, that as the number of strata increase, there values of the estimates may slightly deviate from the actual values since the effect of smoothing also increase (smoothing is needed for each stratum).

Figure 4 shows the quality of xinfAP and infNDCG (re-ferred as extended infNDCG to avoid confusion) computed on these samples according to Kendall X  X   X  and RMS Error statistics, for TRECs 8, 9 and 10. For comparison purposes, the plots also contain infAP and nDCG (the standard for-mula computed on random judgments, assuming unjudged documents are nonrelevant).

Looking at all plots, it can be seen that according to both statistics, using the same number of judgments, the extended infAP (xinfAP) and infNDCG consistently outper-form infAP and nDCG on random judgments, respectively. The high RMS error of nDCG on random judgments is due to the fact that nDCG is computed on these judgments as it is, without aiming at estimating the value of the measure.
In this work, we extended inferred AP in two different ways. First, we derived confidence intervals for infAP to cap-ture the variability in infAP values. Employing confidence intervals enables comparisons and eventually ranking of sys-tems according to their quality measured by AP with high confidence. Second, we utilized a stratified random sampling strategy to select documents to be judged and extended in-fAP to handle the non-random samples of judgments. We applied the same methodology for estimating nDCG in the presence of incomplete non-random judgments. Stratified random sampling combines the effectiveness of stratification and thus better utilization of the relevance judgments with the simplicity of random sampling. We showed that xinfAP and infNDCG are more accurate than infAP and nDCG on equal number of random samples.

Note that the sampling strategy (i.e. the number of strata, the size of each stratum and the sampling percentage from each stratum) used here is rather arbitrary. The confidence intervals as described in the first part of this paper could be used as an objective function to determine an optimal sam-pling strategy. The sampling strategy is highly important for the quality of the estimates and identifying an optimal strategy is a point of future research.

Furthermore, confidence intervals as a function of the sam-ple size could be used to determine the appropriate number of documents to be judged for an accurate MAP estimation which is a point we plan to investigate. Let s d be a sample of cut-off levels at relevant documents. According to the Law of Total Variance, the variance in infAP can be calculated as, Let X  X  consider the first term of the right-hand side of the above equation, which corresponds to the variance due to sampling cut-off levels.
Let r the number of relevant documents in s d . Then, the conditional expectation of infAP is, where d P C k and P C k denote the estimated and actual pre-cision at cut-off k , respectively. Thus, where p 100% is the sampling percentage of documents from the entire depth-100 pool and  X  2 is the actual variance among the precision values at all cut-off X  X  of relevant documents and it can be estimated by,  X  P k  X  s
Now, let X  X  consider the second term of the right-hand side of the equation deduced by the Law of Total Variance, that is the variance due to sampling documents above a cut-off level in order to estimate the precision at that cut-off level, var [infAP | s d ] = var If we consider precisions at different cut-off levels indepen-dent from each other the variance of infAP for a given set of sampled cut-off levels depends on the summation of the precision variances at each individual cut-off level, The precision at cut-off 1 is always 1 and therefore the vari-ance is 0. Moreover, the precision at relevant documents not in the retrieved list is always assumed to be 0 and therefore, the variance at those cut-off levels is also 0. In all other case d P C k is calculated as, d P C k = 1 /k + (( k  X  1) /k )  X  and therefore, Let r k  X  1 and n k  X  1 be the number of relevant documents and total number of documents sampled above cut-off k , respectively and let | d100 | k  X  1 be the number of documents in the depth-100 pool above cut-off k . The precision above cut-off k is estimated by 4 , d P C k  X  1 = | d100 | k  X  1 follows a hypergeometric distribution and its variance can be calculated as, By considering the expected value of var [infAP | s d ] over all samples of cut-off levels we get, If we do not consider precisions at different cut-off levels independent from each other the covariance between preci-sions can be calculated as,
For simplicity reasons we ignore the effect of smoothing that is introduced in the formula of infAP. Smoothing was considered in all experiments ran and it was observed that the effect of smoothing in variance is negligible.
