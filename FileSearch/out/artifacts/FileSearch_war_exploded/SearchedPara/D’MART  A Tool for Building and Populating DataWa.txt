 Many organization today use IT to support decision making and planning. A precursor for achieving this is the need to integrate disparate IT systems and the data that flows through them. Data Warehousing has been a w idely accepted approach for doing this. However, many firms, especially those with extensive legacy systems find themselves at a great disadvantage. Having started late, these firms have to play catch up with new companies that are way ahead in their adoption and use of Decision Support Systems (DSS). Tools/Processes that can accelerat e the data warehouse or data mart develop-ment in such an environment are of significan t value to such compan ies. Implementing a data warehouse solution is a challenging and time consuming process. The various challenges in building a data warehouse include:  X  Most warehousing projects begin by understanding how data is currently managed  X  The development of the ETL scripts (which populate the existing tables in the orga- X  The above problem is exacerbated by the fact that the data vocabulary could be One option that has been used by companies is to use a domain dependent warehouse data model and move all the data from different Lines of Business (LOB) to this new data model. However, using such a pre-defined data model does not avoid any of the problems mentioned above. Furthermore, a primary requirement of customers is to en-sure that all BI Reports that were running on existing data model be re-engineered to run on the new data model. Considering these ch allenges and requireme nts what is required is an automated approach that utilizes the information present in the existing data mod-els and BI Reports to recommend an  X  X ptimal X  and  X  X ufficient X  data warehouse model and ETL scripts.
 The D X  X ART tool presented in this paper analyzes the existing data model and BI Report generation scripts. It uses this information to do the following: 1. D X  X ART proposes a new data warehouse schema (Fact and Dimensions) such that 2. D X  X ART identifies common attributes across the merging data models that have 3. D X  X ART ensures that the data characteristics of the newly proposed data ware-4. D X  X ART generates a skeleton of the ETL scripts for populating the data from the DMARTs is designed to reuse the existing data model as far as possible. This en-sures that the amount of effort needed to understand, build and migrate to the new data warehouse/mart is kept to a minimum. In this section we describe the different Data Warehouse development approaches and the uniqueness of the D X  X ART approach. Existing Data Warehouse development pro-cesses can be broadly categorized into three basic groups: 1. Data-Driven Methodology : This approach promotes the idea that data warehouse 2. Goal Driven Methodology : (Business Model driven data warehousing methodol-3. User-Driven Methodology [6]: This approach is based on the BI needs. Business The User-Driven and Data-driven methodologies are two ends of a spectrum with their fair share of pros and cons. Our approach adopts a middle ground. The D X  X ART tool simultaneously analyzes the schema of the ope rational databases (bottom up) and the re-porting requirements i.e. BI Reports (top down) to recommend a data warehouse model. Paper Organization: We present an overview of the usage scenario of D X  X ART in the Indian Railways context in Section 3. The details of the D X  X ART tool are presented in Section 4. The experimental evaluation of D X  X ART is presented in Section 5 and Section 6 concludes the paper. Indian Railways is the state-owned railway company of India, which owns and operates the largest and busiest rail networks in the world, transporting 20 million passengers and more than 2 million tons of freight daily. CRIS (Center for Railway Information System), which is an umbrella organization of the Indian Railways, caters to all IT needs of the Indian Railways. CRIS is entrusted with the design and development of IT applications that serve different LOB (Lines of Business) within the railways such as freight, passenger services etc.

The IT application landscape at CRIS is as follows. Each application has its own operational data store and an off-line data s ummary store. Periodically, data is moved from the applications operational data store to its off-line data summary store. This is done using a set of complex stored procedures that read data from the operational data source perform the required summarization and copy the data to the off-line data sum-mary store. Reports are generated from the off-line summary database. As each of these applications was developed at different poi nts in time, each application uses a separate and isolated data model and vocabulary that is unique to itself. The LOBs (line of busi-ness) are a logical representation of the morphology of the enterprise, and therefore of its data. The D X  X ART tool was conceived t o accelerate the design and development of the data mart schema for different lines of business and to merge these marts into a single, integrated data warehouse. While the tool has this integration capability, in this paper we focus on the features of D X  X ART which were used in CRIS to build a spe-cific data mart. CRIS is currently in the process of building a data warehouse, and the efficacy of the D X  X ART tool in assisting the process would be clearer as this exercise is taken to its conclusion. The D X  X ART tool recommends a schema for the data warehouse/mart by analyzing the existing data model and the scripts used to generate the BI reports. The process of identifying the best possible data warehouse/mart schema consists of the following steps: 1. Fact Identification 2. Dimension Identification 3. Push Down Analysis 4. Data Analysis 5. Redundancy Analysis.
 The first two steps are responsible for finding the set of tables which will form the fact and dimension tables in the proposed data warehouse. Another task done by the D X  X ART tool is that of generating the ETL scripts for populating the new data ware-house using the existing base table. This task is accomplished by the Push Down Anal-ysis step. The Data Analysis step ensures that the select ed dimensions and facts adhere to the standard design principles. The final task is that of Redundancy Analysis which tries to merge similar attributes and tables from multiple base tables. This task might look similar to the problem of schema matching ([9] [7] [3]), however it has some subtle but significant differences which are gainfully used by D X  X ART to improve its performance. We now explain each of these steps in the following sections. 4.1 Fact Identification The fact identification step finds the set of tables which will form the fact tables of the proposed data mart. This process of identifying the fact table consists of Fact Attribute Identification followed by Affinity Analysis .
 Fact Attribute Identification. In the first step the tool scans the BI report generation SQLs to identify set of attributes on which aggregate operation (such as sum, min, max, average, etc.) is defined. In addition to these attributes, the tool also identifies those at-tributes which are referred directly in the reports. These attributes can be of two types namely direct projection attribute and indirect projection attribute . The first type of at-tribute (i.e. direct projection attribute ) is that which is present in the outermost  X  X elect X  clause of the report generation SQL, whe reas the second type of attribute (i.e. indirect projection attribute ) is the one which is used in the inner query, but is projected out unchanged (possibly after being renamed) and used in the report. In order to understand the use of an indirect projection attribute, consider a Delay Report that displays the list of delayed trains along with the delay (in minutes) during their last run. This report shows only those trains which were delayed more than 80% of the times in the last one month. Notice that this report will find the difference between the scheduled arrival time and actual arrival time for each train in the last one month and will do a count to identify whether the train was delayed more than 80% of the times. It will then report the difference between the scheduled arrival time and actual arrival time during its last run. Thus these two attributes will be indirect projection attributes. They will also have an aggregate operation defined on it (count), but it will not be used in the report. The SQL query used to generate this report is given below: In our work with Indian Railways, we found that there were very few direct projection attributes and a large number of indirect projection attributes. Finding the indirect pro-jection attribute is a very challenging but important task. In the above query, the two attributes SCHEDULED ARR and ACTUAL ARR are used at multiple places. How-ever, we are only interested in those attributes which are projected out. In order to find the right attributes, D X  X ART uses a graph based representation system to address this challenge.

D X  X ART represents the report generation SQL in the form of a Dependency Analy-sis Graph . This graph represents the transformation that each attribute undergoes before it is eventually used in the report. At the lowest level of the graph are the (source) at-tributes of the various tables. At the highest level of the graph are the attributes which are present in the report. There are multiple paths from the source to the output dur-ing which the data undergoes various transformations such as sum, min, count, case statement, etc. We categorize each operation as either being an aggregate operation or a cardinality preserving operation. The aggregate operation generates one output for multiple rows in the input, where as the cardi nality preserving operation generates one row in output for each input row. We are interes ted in identifying both types of attributes as they will be part of the fact table. In addition to this, the aggregate attribute are used for Dimension identification (details in next section).

Building the Dependency Analysis Graph becomes very tricky when an attribute is renamed, merged and reused in a different form. The Dependency Analysis Graph of the Delay Report described earlier is shown in Figure 1. In this figure, notice that at first glance the output attribute ii1.Name appears to be a non aggregate attribute. However, a closer analysis shows that it has a path to an aggregate operator via T2.Name, AGG, T2. In order to find such paths, D X  X ART uses the following rules for traversing the Dependency Analysis Graph:  X  Rule 1: A path starting from the output attribute must not traverse through another  X  Rule 2: The path should always start from an output attribute and should terminate  X  Rule 3: If for a given output node, there exists at least one path that goes via an D X  X ART uses a Breadth First Search based a lgorithm to start searching from each of the output nodes. For each of these output node s, it finds all paths to the base tables such that they adhere to Rule 1 above. Once these paths are found, all those output attributes which do not have any path traversing through an aggregation operation are categorized as either direct or indirect projection attributes. The rest of the attributes are those which either belong to the dimension tables or are used in the  X  X here X  clause of the report generation SQL. Once the direct and indirect projection attributes have been found the next step that of Affinity Analysis is performed. Affinity Analysis. Many-a-time, multiple fact tables are required either because these fact tables contain unrelated data (E.g., in voices and sales) or for reasons of efficiency. For example, multiple fact tables are often used to hold various levels of aggregated (summary data). The Affinity Analysis st ep checks whether there is a need to have multiple fact tables. Typically when multiple fact tables are used to store unrelated data no (or very few) reports access data from these different fact tables. D X  X ART uses this idea to identify the grouping of attributes identified in Section 4.1. We map this problem to that of finding a minimum cut in a graph as follows.
 Inversely, let R( A i ) denote the set of reports, in which attribute A i is used. We map this set to an undirected graph G = { V , E } where V is the set of vertices in the graph and E vertex for each attribute in the schema. The function A( v i ) above takes as input a vertex v and gives the attribute of A which is represented by the input vertex. Notice that the function A( v i ) is overloaded and it can take as input either a report or an attribute. If the input is a report then it gives as output the set of attributes accessed by that report whereas if the input is a vertex, then it outputs the attribute which is represented by that vertex. The set of edges E in the graph G is defined a follows Thus, there is an edge between nodes a.k.a attributes, if both the attributes are accessed by the same report. Thus all the attributes which are accessed in a report will form a strongly connected component in the graph. We construct this undirected graph by adding edges corresponding to all the reports. Given such a graph, if there is a need for multiple fact tables, then notice that the re will either be two (or more) disconnected components in this graph or the graph could be partitioned into multiple sub-graphs such that the number of edges crossing across each of these sub-graphs is very few (as compared to the number of edges within the sub-graph). This problem maps to that of finding the minimum cut in a graph. The minimum-cut of a graph is the partition of the graph into two disjoint sub-sets such that the number of edges whose endpoints are in different subsets is the minimum possible. The minimum-cut problem can be solved in polynomial time using the Edmonds-Karp algorithm [4]. D X  X ART uses this algorithm to find the minimum cut [1] [8]. Each sub-set identified by the cut can map to an independent fact table. In some cases wh ere no natural cut exists, the algorithm finds a cut whose cut size (i.e., the number of edges whose ends points are in different sub-sets) is very large (as compared to the number of edges in the smaller sub-set). In that case D X  X ART does not suggest the use of multiple fact tables.

Another scenario where D X  X ART can sugge st the use of multiple fact tables is when multiple reports aggregate data from the fact table at different levels of aggregation. For example, if 50% of the reports are reporting results on a daily basis where as the rest of the reports are reporting results on a monthly basis. In such cases D X  X ART suggests the use of two fact tables, one aggregating data on a daily basis, whereas the other aggregating data on a monthly basis. Due to space constraints we skip the details of this approach.
 4.2 Dimension Identification A dimension in a data warehouse is responsible for categorizing the data into non-overlapping regions. In other words, a dimension captures (in a loose sense) the distinct values of some attributes present in the fact table. Hence, attributes of the dimension table are typically used as a  X  X roup by X  column in the BI Report generation SQL. We use this fact to find the set of attributes that can be part of the dimension table. The procedure of finding the dimension table is divided into two parts namely Candidate Set Generation and Hierarchy Generation .
 Candidate Set Generation. In the first part, D X  X ART identifies the set of all attributes which are used in a  X  X roup by X  clause of a report generation SQL. Notice that finding these attributes is a non-trivial task as the report generation SQLs are fairly complex and large. D X  X ART uses the Dependency Analysis Graph to find the set of attributes on which  X  X roup by X  is defined. These attributes could be anywhere within the SQL such as a nested query, sub-query, etc. The set of attributes identified by the above procedure form what we call as the  X  Candidate Attribute Set  X . The set of tables which have at least one of the attributes from the  X  Candidate Attribute Set  X  form the  X  Candidate Table Set  X . The  X  Candidate Table Set  X  is the set of tables which can potentially form a dimension in our new data warehouse schema. Once the candidate table set has been identified, we need to identify whether we need to generate a star schema or a snow-flake schema using these candidate dimension attributes. This is done in the Hierarchy Generation step. Hierarchy Generation. In order to identify whether we need to use a star schema or a snowflake schema, we essentially need to find whether any of the candidate dimension table can be represented as a hierarchy of multiple tables or if a single table represen-tation suffices. In case we can split a dimension table into multiple tables then we need to use the snowflake schema, else we use the star schema. As we explain next, there are two steps for identifying the presence (or absence) of a hierarchy in the dimension ta-ble. When the data warehouse has a hierarchical dimension, the reports which use these dimensions would involve multiple joins across all the dimensions in the hierarchy. We use this information to decide between using a star schema or a snow flake schema as follows. Notice that if a set of attributes are used together in the  X  X roup by X  clause, we would exploit this fact to suggest a hierarchical dimension to improve the efficiency of report generation. For example, consider the following report generation SQL: In the above query the attributes T2.location id and T3.city id appear together in a  X  X roup by X  clause and have a  X  X oin X  between them. This suggests that T2 and T3 form a hierarchy. In cases where the tables are incorrectly designed, we could have a case where the city and location information is present in a single table. Even in those cases, we are able to suggest the use of dimension hierarchy.

We use the fact that attributes appearing together in a  X  X roup by X  clause could sug-gest the use of a hierarchy of dimensions. We first identify the set of mutually exclusive super-sets of the candidate attribute set which are used together in the  X  X roup by X  clause of the various report generation SQLs. We explain this task with the following example: {
A, B } , { D, E, F } and { D } be each used together in the same  X  X roup by X  clause of a report generation SQL, i.e., { A, B, C } is used in one  X  X roup by X  clause of a report gen-eration SQL where as { A, B } is used in another  X  X roup by X  clause of (possibly) another report generation SQL. What we are interested in are those set of attributes which are used together in the same  X  X roup by X  clause. The mutually exclusive super-set, for the above example, will be { A, B, C } and { D, E, F } . The key property of this set is that any member (attribute) of one super set is never used with a member of another super set, i.e., A is never used together with say, D in the same  X  X roup by X  clause of a report generation SQL. This property helps us to identify the set of attributes which will be part of the same dimension (or dimension hierarchy).

Given the mutually exclusive super-set, for each super-set we form the set of tables whose attributes are part of the super-set. As the existing schema in the enterprise may not well defined, we could end up with a case where the same table could be part of multiple super set. For example, we could have the following super set for the above scenario, { T1, T2 } , { T1, T3, T4 } (mapping to { A, B, C } and { D, E, F } ). A common reason for this is that the table (T1) is not in second normal form, i.e., the table T1 has some amount of redundant data. If we remove t his redundant data, then we can possibly avoid the overlap across the two super sets. In order to do this we convert the table T1 into second normal form which leads to a split of the table into multiple tables. We then reconstitute the super set of tables and check if there is any overlap across super set. In case the overlap still exists, then the same procedure is repeated. This is done for a fixed number of times (currently set at 3). If the overlap problem is still not solved, then we report this to the administrator. However, this is rarely required in practice. During our work for Indian railways, D X  X ART could automatically remove the overlap in all the cases.

Once the overlap has been removed, we identify the dimension for each of the super set. If the set of tables in the super set already have a primary key-foreign key rela-tionship amongst them, then we use it to form a snowflake schema. In case there is no relationship, we check if each of these tables is in second normal form. If yes, then each of these tables form a separate dimension as part of a star schema. If a table is found as not being in the second normal form, then we convert it to second normal form and repeat the same procedure again. 4.3 Push Down Analysis The Push Down Analysis phase tries to suggest the right granularity for the fact table. As described earlier, D X  X ART scans the repor t generation SQLs and identifies whether all the reports are using a common aggregation before generating the reports. In such a case, D X  X ART suggests changes to the granularity of the fact table. In order to do so, it suggests the aggregation to be pushed to the ETL scripts used to populate the fact table. It extracts the aggregation operator from the report generation SQL and sug-gests the same to be used in the ETL scripts. D X  X ART also suggests changes in the report generation SQL due to the changes in the fact table. This immensely helps the administrator to quickly generate the necessary ETL scripts for populating the newly defined data warehouse from the base tables. 4.4 Redundancy Analysis A key aspect of the bottom-up approach is the identification of conformed dimensions . A conformed dimension is a set of data attributes that have been physically implemented in multiple database tables using the same structure, attributes, domain values, defi-nitions and concepts in each im plementation. Thus, conformed dimension define the possible integration  X  X oints X  between the data marts from different LOBs. The final phase of D X  X ART, Redundancy Analysis pha se, is responsible for finding candidates for conformed dimension. At first glance, this looks very similar to schema matching. However, the key advantage that we have in our setting is that we also have access to the SQL queries which are used to populate the data in the original data model. We make use of these queries to identify the commonalities in the schema across departments.
D X  X ART creates a data model tree for each attribute of the original schema. The data model tree tries to capture the origins of the attribute, i.e., from where is the data populated in this attribute, what kinds of transformations are applied to the data before it is populated in the attribute, etc. The data model tree is created by analyzing the SQL scripts which populate the attribute. D X  X ART scans the scripts and converts it into a graph (Data model tree) similar to that used in Section 4.1. The only difference is that the graph in Section 4.1 was used to analyze how reports are generated from the existing schema, whereas the data model tree is used to analyze how existing summary tables are populated from the source tables. Further, there is only one graph model per report, where as there is one data model tree per attribute of the original schema.
Once the data model tree has been generate d, we find similar trees by comparing the structure and source of the trees. If two trees are similar, then D X  X ART suggests them as candidates for conformed dimension to the a dministrator. Once these candidates have been generated, D X  X ART can then use exis ting schema matching tools to do further redundancy analysis. Thus using the five ste ps described in this section, D X  X ART finds the new schema for the enterprise. A byproduct of this is the suggestion for writing ETL scripts that can be used to populate data in a new data warehouse. The next section presents an overview of the experimental evaluation of the D X  X ART tool. The D X  X ART tool was applied to multiple Lines of Business at CRIS. In this section we talk about our experience in applying D X  X ART to a specific system namely the PAMS systems (Punctuality Analysis and Monitoring System). The PAMS system is used to generate reports on punctuality of diff erent trains running in different zones and division within the Indian Railway network. We were provided with the operational database schema, BI Reports and corresponding stored procedure details. For instance for the PAMS system we received 27 reports and 65 stored procedures. The following SQL snippet has be taken from the Train Status Report, which is one of the 27 reports of the PAMS systems. This report displays the current status (e.g. canceled , rescheduled etc) of a particular train running on a given date in a particular region or division. For illustration the extracted Dependency Analysis Graph is shown in Figure 2. The recommended Fact and Dimension attributes for this SQL are: Fact Attributes =
ST SCHEDULE TRAIN DIV.EXCEPTION TYPE } , Dimension Attributes = D X  X ART performed similar processing on the other SQLs from the PAMS system. For the PAMS system D X  X ART recommended 1 Fact and 4 Dimension tables. The Fact table has a set of 18 attributes. Two of the four dimensions have a hierarchy which is one level deep. To validate the correctness and usefulness of the recommended schema a user-study was conducted. Data administrators were shown the old and new schema and asked to rate it on certain parameters. These parameters included factors such as completeness, correctness ( Are all modeled aspects correct with respect to the require-ments and terms of the domain? ), consistency ( Are all modeled aspects free of contra-dictions? ), coverage, level of detail and minimality ( Is the schema modeled compactly and without redundancies? ). The schema recommended by the D X  X ART tool was rated highly on all these parameters.
 In this paper we presented the D X  X ART tool that helps companies accelerate the devel-opment of a data warehouse/data mart from existing data models. The key advantage of D X  X ART is that it proposes a new data warehouse schema with minimal changes to the existing setup. Such an approach is extrem ely useful when companies need a phased approach for building the warehouse. D X  X ART works by analyzing the existing data model and BI Reports of the enterprise. D X  X ART models the problem of identifying Fact/Dimension attributes of a warehouse model as a graph cut problem on a Depen-dency Analysis Graph DAG. The DAG is built using the existing data models and the BI Report generation SQL scripts. The D X  X ART tools also uses a variant of the DAG for generation of ETL scripts that can be used to populate the newly proposed data ware-house from data present in the existing schemas. D X  X ART was developed and validated as part of an engagement with Indian Railways which operates one of the largest and busiest rail networks in the world.
 Acknowledgement. We would like to acknowledge and express our heartfelt gratitude to Mr Vikram Chopra, without whose support this work would not have been possible, and also thank the CRIS team led by Ms Priya Srivastava for generously contributing their effort and knowledge.

