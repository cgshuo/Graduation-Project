 REGULAR PAPER Glenn Fung  X  Jonathan Stoeckel Abstract Alzheimer X  X  disease is the most frequent type of dementia for elderly patients. Due to aging populations, the occurrence of this disease will increase in the next years. Early diagnosis is crucial to be able to develop more power-ful treatments. Brain perfusion changes can be a marker for Alzheimer X  X  disease. In this article, we study the use of SPECT perfusion imaging for the diagnosis of Alzheimer X  X  disease differentiating between images from healthy subjects and images from Alzheimer X  X  disease patients. Our classification approach is based on a linear programming formulation similar to the 1-norm support vector machines. In contrast with other linear hyperplane-based methods that perform simultaneous feature selection and classification, our proposed formulation incorporates prox-imity information about the features and generates a classifier that does not just select the most relevant voxels but the most relevant  X  X reas X  for classification re-sulting in more robust classifiers that are better suitable for interpretation. This approach is compared with the classical Fisher linear discriminant (FLD) classi-fier as well as with statistical parametric mapping (SPM). We tested our method on data from four European institutions. Our method achieved sensitivity of 84.4% at 90.9% specificity, this is considerable better the human experts. Our method also outperformed the FLD and SPM techniques. We conclude that our approach has the potential to be a useful help for clinicians.
 Keywords Alzheimer X  X  disease  X  Support vector machines  X  Medical imaging  X  Mathematical programming 1 Introduction Alzheimer X  X  disease (AD) is the most frequent type of dementia for elderly pa-tients. Due to aging populations, its occurrence will still increase. Even though no definitive cure has been found for this disease, reliable diagnosis is useful for excluding other dementias, choosing the right treatment and for the development of new treatments.
 and Communicative Disorders and Stroke and Alzheimer X  X  Disease and Related Disorders Association (NINCDS-ADRDA) [ 1 ]. These criteria include dementia established by examination and objective testing; deficits in two or more cognitive areas; progressive worsening of memory and other cognitive functions; no distur-bance in consciousness; and onset between ages 40 and 90. Absence of systemic disorders or other brain diseases, which could account for the deficits in memory and cognition, should also be established. In practice the main tool for evaluating patients are neuro-psychologic tests, that test abilities like memory and language. The Mini Mental State Examination (MMSE) is the most widely used of these tests [ 2 ].
 onance imaging (MRI) is used to study possible anatomical changes of the brain [ 3 ]. The shrinkage of the hippocampus, a region of the brain showing some of the first signs of Alzheimer X  X  disease, occurs very early in the disease process, long before the illness spreads to the cerebral cortex and results in cognitive and memory impairment and its volume change is an important sign for the detection of Alzheimer X  X  disease in MRI images [ 3 ]. Images showing the local perfusion (amount of blood flow) of the brain can be used for the diagnosis of AD because the perfusion pattern is affected by the disease. In this article, we will look into the use of cerebral perfusion imaging acquired by single photon emitting computer tomography (SPECT) using technetium-99m hexamethylpropylene amine oxime (HMPAO) as the tracer. SPECT imaging is a largely accepted clinical modality for AD diagnosis. Even though the perfusion pattern and its evolution is not the same for all patients some hypo-perfusion patterns seem to be typical for the disease. There are three main regions mentioned in literature attained by hypo-perfusion [ 4 ]: Temporo-parietal region : Many studies have shown this region to be typical for
Posterior cingulate gyri and precunei : Kogure et al. [ 11 ] show these regions
Medial temporal lobe : Hypo-perfusion in these regions was only noticed during sis of SPECT images for AD can be found in literature. The first family is based on the analysis of regions of interest. The mean values for these regions are analyzed using some discriminant functions (see e.g. [ 18 , 19 ]).
 ous variants. Statistical parametric mapping is widely used in the neuro-sciences. Its framework was first developed for the analysis of SPECT and PET studies, but is now mainly used for the analysis of functional MRI data. It was not developed specifically to study a single image, but for comparing groups of images. One can use it for diagnostics by comparing the image under study to a group of normal images.
 our case a t -test, comparing the values of the image under study to the mean values of the group of normal images. Subsequently, the significant voxels are inferred by using random field theory (see e.g. [ 20 ] for a full description of SPM). A largely used freely available implementation called SPM99 [ 21 ] has been developed and is used in this article as comparison to our approach.
 mation about the pathology as possible, by obtaining it implicitly from image databases. Another important aspect is that our approach is global. that all the in-formation in the image can be used at once in contrast to more local approaches, e.g mono-variate methods like SPM. A multi-variate approach generally increases sensitivity at the price of loosing regional specificity (e.g. depicting local hypo-perfusion regions). However, in the approach presented in this paper compared to our earlier work [ 22 ], we use feature selection while trying to add spatial con-straints to the classification.
 describe our proposed mathematical programming formulation. Unlike the tradi-tional SVM-like formulations, spatial information about the feature (voxels) lo-cations is incorporated into the optimization problem. This leads to feature selec-tion where the classifier depends on regions in the brain instead of isolated non-connected voxels. In Sect. 3 , we present the data we used for our experiments. It consists of real brain SPECT images obtained from four different institutions. The results on the data are presented in Sect. 4 and discussed in Sect. 5 . 1.1 Notation We now describe the notation used in this paper. The notation A  X  R m  X  n will signify a real m  X  n matrix. For such a matrix, A will denote the transpose of A and A i will denote the i -th row of A . All vectors will be column vectors. For x  X  R n , x space of arbitrary dimension will be denoted by e . Thus, for e  X  R m and y  X  R m , e y is the sum of the components of y . A vector of zeros in a real space of arbitrary dimension will be denoted by 0. A separating hyperplane , with respect to two given point sets A and B , is a plane that attempts to separate R n into two halfspaces such that each open halfspace contains points mostly of A or B . 2 Methods 2.1 Spatial normalization In the classifier-based approach, we need the assumption that the same position in the volume coordinate system within different volumes corresponds to the same anatomical position. This makes it possible to do meaningful voxel-wise compar-isons between images. However, this assumption is not met by the images without pre-processing: First of all, the subject which is being imaged, is not always po-sitioned at the same position in the reference frame of the imaging device. This reference frame defines where e.g. the brain is positioned in the image. Secondly, the anatomy does not always have the same shape and size between different sub-jects. For example, the size and shape of the skull can already be largely differ-ent between subjects. This means that we need to spatially register the volumes. In recent years, many registration algorithms have been developed, we point the interested reader to the following reviews for more information [23 X 26]. In our application, we do not have detailed knowledge of the anatomy of our subjects as only HMPAO-SPECT images of the subjects were available. These images are so-called functional images. They only depict the regional blood flow of the subject. The regional cerebral blood flow provides us of course with some gross infor-mation about the anatomy, but only based on the fact that there is a relationship between the blood flow, and the underlying anatomy. Understanding this charac-teristic of HMPAO-SPECT images is fundamental for the choice of the registration method.
 well. CT images, or even better MRI images would provide detailed insight in the anatomy. The registration of images of these modalities of different subjects with each other, would provide us with the transformations between the images based on the anatomy of the subjects. Subsequently, these transformations could be applied to the functional images. Due to practical clinical limitations, however, these modalities of images were not available. Hence, we tried to deduce the shape and size of the anatomy based on the functional images.
 chose to estimate affine transformations between the volumes and not use trans-formations with a larger number of degrees of freedom. We used the correlation ratio as the similarity measure [ 27 ] that we minimized using Powell optimization [ 28 ]. To obtain a more robust result, we used the following procedure. First of all, we registered all volumes to a single volume, then we calculated a mean volume. This mean volume was first put on the mid-sagittal plane by registering it with a flipped version (see [ 29 ]). Subsequently, it was made to be symmetrical by taking the mean of itself with a flipped version. Finally, all volumes were matched to this volume. 2.2 Intensity normalization HMPAO-SPECT imaging generates volumes that only give a relative measure of the blood flow. The blood flow measure is relative to the blood flow in other re-gions of the brain. Direct comparison, of the voxel intensities, between images, even different acquisitions of the same subject, is thus not possible without nor-malization of the intensities.
 transformation to the intensities. The transformation parameters are estimated on the training set of each experiment such that the intensities for each voxel position have zero mean and standard deviation of one for all the training subjects. We choose this very common data normalization since it provides numerical stability to the algorithms involved. In [ 30 ] it is shown that the relationship between the tracer activity and the blood flow is not completely linear. So the HMPAO-SPECT images show less contrast between high and low activity flow regions than would be expected of the regional blood flow. Even though this effect exists, we assumed it to be small, and not to influence the ratio between high and low blood flow as a function of the tracer concentration in the blood. Thus, for our work, we assumed a linear scaling of the intensities between the images to be sufficient. region, we might be tempted to conclude that for this case, the blood flow in this latter region has decreased, when comparing to other images where these effects have not taken place. This might happen when comparing images of Alzheimer X  X  disease patients with images of normal subjects. This measured reduction, even if it does not correspond to the physical reality, can of course still be used as a diagnostic sign. 2.3 Classification Because the hypo-perfusion pattern for early AD is not very well defined we choose to develop a method where we do not use any explicit knowledge about the typical perfusion patterns. We use implicit knowledge about the perfusion patterns by using a database of images of AD patients and normal subjects. To separate the images we use a classifier using the voxel intensities as features and this database to train the classifier. Using the voxel intensities as features makes it possible not to introduce any particular knowledge about the exact location of the hypo-perfusion area(s). the problem of the exact definition of the typical perfusion pattern for early AD. In general, the number of images available in the training databases is significantly smaller ( &lt; 100) than the number of voxels ( &gt; 1000). Thus, the number of features (voxels) is much larger than the number of samples (training images). The number of samples is considered to be small if it is about the same or smaller than the number of dimensions. In this case, we speak of almost empty spaces, the small sample size problem or the so-called curse of dimensionality. In classical pattern recognition, it is believed that no good generalization could be obtained for these cases when using the whole feature space [ 32 ]. Generalization is the capacity of generalization of our final classifier, minimal feature dependency (small amount of features) of the classifier is desired. 2.3.1 The linear support vector machine We consider the problem, depicted in Fig. 1 , of classifying m points in the n -dimensional real space R n , represented by the m  X  n matrix A , according to mem-bership of each point A i in the class A + or A  X  as specified by a given m  X  m diagonal matrix D with plus ones or minus ones along its diagonal. For this prob-lem the standard support vector machine with a linear kernel [ 32 ]isgivenbythe following quadratic program with parameter  X &gt; 0: x w =  X   X  1 bounds the class A  X  points as follows: The linear separating surface is the plane x w =  X  midway between the bounding planes ( 2 ). The quadratic term in ( 1 ) maximizes the distance or  X  X argin X  between the bounding planes. Maximizing the margin enhances the generalization capa-bility of a support vector machine [ 32 ]. In order to make use of a faster linear programming based approach, instead of the standard quadratic programming for-mulation ( 1 ), we reformulate ( 1 ) by replacing the 2-norm by a 1-norm as follows [ 33 ]: between the two bounding planes of Fig. 1 , using a different norm, the  X  -norm, and results with a margin in terms of the 1-norm, 2 w mathematical program ( 3 ) is easily converted to a linear program as follows: tage of generating very sparse solutions. This results in the normal w to the sep-arating plane x w =  X  having many zero components, which implies that many input space features do not play a role in determining the linear classifier. This makes this approach suitable for feature selection in classification problems. We note that in addition to the conventional interpretation of smaller  X  as emphasizing a larger margin between the bounding planes ( 2 ), a smaller  X  here also results in a sparse solution. The  X  X ight X  value of  X  is determined by a tuning procedure where the performance is adjusted to the desired compromise between the classification performance and the sparseness of the solution. Next, we will revisit some reg-ularization theory results that would motivate the SVM-like formulation we are proposing in this paper. 2.4 Regularization theory and SVMs Let f : n  X  with f ( x ) = w x  X   X  the our prediction or classification func-tion. Then, Formulation ( 4 ) and support vector machine (SVM) formulations in general can be seen as a particular case of regularization networks [ 35 ] where the functional R reg [ f ]= R emp +  X  G ( Pf ) that is often referred as the regularized risk, is minimized. R reg [ f ] is equal to the empirical risk functional R emp [ f ] plus a regularization term G ( Pf ) that is usually defined as Pf 2 .  X  = 1  X  is the regu-larization parameter and P is a called the regularization operator. P maps the the classifier function f into some dot product space [ 36 ]. For example, in the case of SVMs, the type of regularization and the class of functions that form the basis for the prediction function are intimately related. The SVM algorithm is equivalent to that the kernel k is chosen as a Green X  X  function of P  X  P [ 36 ]. For example, in Formulation ( 4 ) the regularization term is G ( Pf ) = w 1 .and K ( x i , x j ) = x i x j (the linear kernel). Our proposed formulation also minimize the regularized risk R information (in the form of spatial information) about the classification task at hand. 2.4.1 The contiguous linear SVM (CSVM) There are two drawbacks related to standard SVM formulations, especially when they are applied to imaging classification problems. The first drawback is related to the fact that little or no spatial information about the imaging problem is incor-porated into the optimization problem to solve, discarding very valuable informa-tion that could lead to better and more robust classifiers. In the case of imaging problems where the features are related to voxel/pixel intensities a relation can be predefined among the voxels using spatial information or previous knowledge about the problem. The second drawback is related to the interpretability of the results. In several applications a feature selection scheme is implemented not only to get sparse models but also to determine which of the input features are rele-vant for the classification task, leading to insights about the problem in question. For example in the problem that we are addressing in this article it is easier to interpret a final classifier depending on contiguous voxels defining regions than a subset of independent voxels with no apparent connection among them. Our goal in this paper is to incorporate spatial information about every voxel into the opti-mization problem in a manner that the final obtained hyperplane classifier depends on regions or clusters of features rather than on isolated voxels. Let us consider a similarity function r that defines binary relations among any two features ( f i , f j ) of any given training datapoint. Let R be a matrix such that: undirected graph representing the relation among the features according to the re-lation function r . R is a pseudo-adjacency matrix of a graph where every node has a self-loop. For most problems in real life R is based on local relations and there-fore it is a very sparse matrix (see e.g. Fig. 3 ). The function r could be defined in a more general way, where instead of a binary relations it can be a similarity func-tion or any other kind of function encoding extra information about the features or the datapoints in the training set. Figure 2 shows three examples of graphs rep-resenting possible relations among features. For example, Graph (a) represents a dataset where f x and f y are directly related to feature f w and feature f w is related feature f z , even when there is not an explicit edge in the graph connecting them. Graph (b) represents a graph where every feature is related to each other and graph (c) represents a graph that is not fully connected , where the features are clustered (two clusters) according to the relations among them.
 mask defining the 26-closest neighbors of each voxel. Note that this very local simple mask allows to encode the sense of contiguity among voxels in a global sense across the whole volume. This mask size was chosen because it provided excellent results while maintaining the sparsity of the relation r A very simple but effective way to incorporate this extra information about the features into the 1-norm SVM formulation ( 4 ) is to use the relationship matrix R as a regularization operator and then minimize the the regularized risk: This can be formulated as the following linear programming problem: At a solution of problem ( 4 ), v is the absolute value | w | of w . This fact follows from the constraints v  X  w  X  X  X  v which imply that v i  X | w i | , i = 1 ..., n . Hence at optimality, v =| w | , otherwise the objective function can be strictly decreased without changing any variable except v . In this new formulation ( 4 )wehaveat optimality that P v =| w | ,thisis In other words, this means that the magnitude of the weight w i of the related feature i , not only depends on itself but it also depends on all the features j that are related to i according to the relation function r . Moreover, R can be interpreted as a covariance matrix such that the prior over the vector of weights w is given by P (w) = X  exp ( R  X  1 w 3 Materials 3.1 Subjects The images we used for our experiments were taken from a concurrent study in-vestigating the use of SPECT as a diagnostic tool for the early onset of AD. A detailed description of this data can be found in [ 37 ]. Subjects of four different centers, Edinburgh (Scotland), Nice (France), Genoa (Italy), and Cologne (Ger-many) were included for this study. In total 158 subjects participated, including 99 patients with AD, 28 patients suffering from depression (not used in this arti-cle), and 31 healthy volunteers. An example of this data is seen in Fig. 4 . Con-firmation of Alzheimer X  X  disease was obtained by clinical follow-up. There was no statistically significant age difference between the AD patients and the healthy subjects. For technical acquisition related reasons images of 7 AD subjects had to be excluded. 3.1.1 Pre-processing Applying the registration procedure as described above results in images of 128 by 128 by 89 voxels, with a voxelsize of 1.71 mm by 1.71 mm by 1.88 mm for all four centers. The SPECT images have an effective resolution of about 7 mm full-width at half-maximum (FWHM). Therefore, we can subsequently subsam-ple the images a factor of two in each dimension by taking the average value over the subsampled areas without loosing much information. We only use the voxel intensities for the voxels in the part of the brain that has been imaged for all sub-jects. Applying this procedure results in 3816 features per subject available for classification/feature selection. 3.1.2 Experts All real images were rated in four categories (very probable, probably, proba-bly not and very unlikely to have AD) by 16 European expert nuclear medicine physicians. The possible ratings were as follows: very probably Alzheimer X  X  dis-ease, probably Alzheimer X  X  disease, probably not Alzheimer X  X  disease and very unlikely Alzheimer X  X  disease. To be able to compare the data from the experts with that of the automatic methods, we considered the first two ratings as positive and the other two as negative. 4 Experiments In all of our experiments, we divided the data into two disjoint training and testing sets. The idea is to tune the parameters in our model only using data from the train-ing set, once the final model is fixed, it is tested in the unseen testing set. We used leave-one-out cross validation to tune the model parameter  X  of the contiguous SVM. Performance of our Contiguous SVM algorithm, in terms of generaliza-tion ability, is compared with a Fisher X  X  Linear Discriminant (FLD) classifier as previously presented in [ 22 ]. The FLD algorithm used here is based on the FLD mathematical programming formulation introduced by Mika et al. [ 38 ]. For solv-ing all the optimization problems involved in this paper we used the widely used commercial solver CPLEX 6.5 [ 39 ]. Next, we outline the results of our compara-tive testing. Two set of experiments were performed: 1. We randomly divided the 123 cases into 90 training examples and 33 testing 2. In order to test the generalization performance of our approach across institu-nected areas. Figure 5 shows part of the selected features (a subset that can easily be visualized in 2D). Most selected groups of features are in the ventricles. This is consistent with the general atrophy of the brain observed in Alzheimer X  X  dis-ease patients which enlarges the ventricles relative to the other parts of the brain. This result shows the potential of the proposed approach at selecting meaning-ful grouped features which can be interpreted more easily than traditional fea-ture selection approaches. The experts had an average sensitivity of 56.6% and a specificity of 82.4% for all 123 cases. In the SPM approach, we use SPM at a significance level of 0.1 at the cluster level. We consider each image where some significant clusters were found to be a positive result, this leads to a sensitivity of 55.9% and a specificity of 77.4% for SPM. Our classification approach as shown in Tables 1 and 4 outperforms both the experts and the SPM approach. Results in Table 4 show that even if the performance decreases on the training set due to differences in the way the images were aqcuired at the different institutions the contiguous SVM approach still shows good generalization capabilities. 5Conclusion Based on the experiments described in this article, we conclude that our automatic approach to the classification of images performs at least as well as human ob-servers. In general, our contiguous support vector machine is more sensitive and more specific. One would need more data, especially of control subjects to be able to state that automatic methods always significantly outperform human observers in clinical practice.
 outperforms the local SPM approach. We have shown that classification without using any specific knowledge related to the pathology is possible.
 a specific image. However, only providing global information might not be suffi-cient for clinicians. Therefore, we proposed a method that might do useful feature selection which might provide useful information to the clinician, at least at the group level. A trained classifier represents the group of images it was trained on, it does not show which areas where discriminative for any specific single image. Further research should focus on how to obtain subject specific local information while still retaining the advantage of a global approach.
 steps. We used the registration methods that were readily available. Grova et al., and Grova [ 40 , 41 ] carried out large-scale simulations of SPECT images, not for Alzheimer X  X  disease but for epileptic patients. In future work one could test the influence of different registration and intensity normalization methods on the the classification approaches.
 ferential diagnosis (other dementias versus Alzheimer X  X  disease) which might be an even more important clinical issue. ROC analysis of the classifier as well as of the experts will be useful to better compare performances. This will also pro-vide means to handle the differences in operating points for the different experts (e.g. some experts are more specific while others are more sensitive). Also an in-teresting future direction would be to extend the Contiguous SVM formulation, where a relation among datapoints is considered instead of a relation among the features. This approach can potentially be used for a general semi-supervised SVM approach where only some of the labels for the training data are available. References
