 PATRICK YE and TIMOTHY BALDWIN University of Melbourne 1. INTRODUCTION Prepositional phrases (PPs) are both common and semantically varied in open
English text. In general, learning the semantics of prepositions is not a trivial task. It may seem that the semantics of a given PP can be predicted with rea-sonable reliability independent of its context. However, it is actually common for prepositions or even identical PPs to exhibit a wide range of semantic fuc-tions in different contexts. For example, consider the PP to the car : this PP will generally occur as a directional adjunct (e.g., walk to the car ), but it can also occur as an object to the verb (e.g., refer to the car ) or contrastive argument (e.g., the default mode of transport has shifted from the train to the car ); to further complicate the situation, in key to the car it functions as a complement to the
N-bar key . Based on this observation, we may consider the possibility of con-structing a semantic tagger specifically for PPs, which uses the surrounding context of the PP to arrive at a semantic analysis. It is this task of PP semantic role labeling that we target in this paper.

A PP semantic role labeler would allow us to take a document and identify all adjunct PPs with their semantics. We would expect this to include a large portion of locative and temporal expressions, e.g., in the document, providing valuable data for tasks, such as information extraction and question answering.
Indeed, our initial foray into PP semantic role labeling relates to an interest in geospatial and temporal analysis and the realization of the importance of PPs in identifying and classifying spatial and temporal references.

The contributions of this paper are to propose a method for PP semantic role labeling and evaluate its performance over both the Penn Treebank (includ-ing comparative evaluation with previous work) and the data from the CoNLL semantic role Labeling shared task. As part of this process, we identify the level of complementarity of a dedicated PP semantic role labeler with a conven-tional holistic semantic role labeler, suggesting PP semantic role labeling as a potential avenue for boosting the performance of existing systems.

In the remainder of this paper, we outline the propose a method for PP seman-tic role disambiguation and evaluate it over both the Penn Treebank (Section 2) and the CoNLL 2004 semantic role labeling shared task (Section 3). We then contrast the relative success of the proposed method over the two data sets (Section 4), and, finally, conclude the paper with a discussion of future work (Section 5). 2. PREPOSITION SEMANTIC ROLE DISAMBIGUATION IN PENN TREEBANK
Significant numbers of prepositional phrases (PPs) in the Penn Treebank [Marcus et al. 1993] are tagged with their semantic role relative to the gov-erning verb. For example, Figure 1 shows a fragment of the parse tree for the sentence [ Japan X  X  reserves of gold, convertible foreign currencies, and special drawing rights ] fell by a hefty $ 1.82 billion in October to $ 84.29 billion [ the
Finance Ministry said ], in which the three PPs governed by the verb fell are tagged as, respectively: PP-EXT ( X  X xtend X ), meaning how much of the reserve fell;
PP-TMP ( X  X emporal X ), meaning when the reserve fell; and PP-DIR ( X  X irection X ), meaning the direction of the fall.

According to our analysis, there are 143 preposition semantic roles in the treebank. However, many of these semantic roles are very similar to one another; for example, the following semantic roles were found in the treebank: PP-LOC, PP-LOC-1, PP-LOC-2, PP-LOC-3, PP-LOC-4, PP-LOC-5,
PP-LOC-CLR, PP-LOC-CLR-2, PP-LOC-CLR-TPC-1 . Inspection of the data re-vealed no systematic semantic differences between these PP types. Indeed, for most PPs, it was impossible to distinguish the subtypes of a given superclass (e.g., PP-LOC , in our example). We, therefore, decided to collapse the PP semantic roles based on their first semantic feature. For example, all semantic roles that start with PP-LOC are collapsed to the single class PP-LOC . Table I shows the distribution of the collapsed preposition semantic roles. 2.1 System Description
O X  X ara and Wiebe [2003] describe a system 1 for disambiguating the semantic roles of prepositions in the Penn Treebank according to seven basic semantic classes. In their system, O X  X ara and Wiebe used a decision tree classifier, and the following types of features:
POS tags of surrounding tokens: The POS tags of the tokens before and after the target preposition within a predefined window size. In O X  X ara and Wiebe X  X  work, this window size is two.
 POS tag of the target preposition The target preposition
Word collocation: All the words in the same sentence as the target preposition; each word is treated as a binary feature.

Hypernym collocation: The WordNet hypernyms [Miller 1995] of the open class words before and after the target preposition within a predefined win-dow size (set to five words); each hypernym is treated as a binary feature.
O X  X ara and Wiebe X  X  system also performs the following preclassification fil-tering on the collocation features:
Frequency constraint: f ( coll ) &gt;1 , where coll is either a word from the word collocation or a hypernym from the hypernym collocation semantic role and coll is from the word collocation or a hypernym from the hypernym collocation
We began our research by replicating O X  X ara and Wiebe X  X  method and seek-ing ways to improve it. Our initial investigation revealed that there were around 44,000 word and hypernym collocation features, even after the frequency con-straint filter and the conditional independence filter have been applied. We did not believe all these collocation features were necessary, and deployed an addi-tional frequency threshold-based filtering mechanism over the collocation fea-tures to only select collocation features that occur in the top N frequency bins.
This frequency threshold-based filtering mechanism allows us to select col-location feature sets of differing size and, in doing so, not only improve the training and tagging speed of the preposition semantic role labeling, but also observe how the number of collocation features affects the performance of the
PP semantic role labeler and which collocation features are more important. 2.2 Results
Since some of the preposition semantic roles in the treebank have extremely low frequencies, we decided to build our first classifier using only the top nine semantic roles, as detailed in Table I. We also noticed that the semantic roles
PP-CLR, PP-CD, and PP-PUT were excluded from O X  X ara X  X  system, which only used PP-BNF, PP-EXT, PP-MNR, PP-TMP, PP-DIR, PP-LOC, and PP-PRP , and, therefore, built a second classifier using only the semantic roles used by
O X  X ara X  X  system. 2 The two classifiers were built with a maximum entropy [Berger et al. 1996] learner. 3
Table II shows the results of our classifier under stratified 10-fold cross-validation 4 using different parameters for the ranking-based filter. We also list the accuracy reported by O X  X ara and Wiebe for comparison.

The results show that the performance of the classifier increases as we add more collocation features. However, this increase is not linear, and the improve-ment of performance is only marginal when the number of collocation features formance difference between classifiers 1 and 2, which suggests that PP-CLR may be harder to distinguish from the other semantic roles. This is not totally surprising given the relatively vague definition of the semantics of PP-CLR .We return to analyze these results in greater depth in Section 4. 3. PREPOSITION SEMANTIC ROLE LABELING OVER THE CONLL 2004 DATA SET
Having built a classifier, which has reasonable performance on the task of treebank preposition semantic role disambiguation, we decided to investigate whether we could use a similar set of features to perform PP semantic role labeling over alternate systems of PP classification. We chose the 2004 CoNLL semantic role labeling (SRL) data set [Carreras and M ` arquez 2004] because it contained a wide range of semantic classes of PPs, in part analogous to the
Penn Treebank data, and also because we wished to couple our method with a holistic SRL system to demonstrate the ability of PP semantic role labeling to enhance overall system performance.

Since the focus of the CoNLL data is on SRL relative to a set of predetermined verbs for each sentence input, 5 our primary objective is to investigate whether the performance of SRL systems, in general, can be improved in any way by an independent preposition SRL system. We achieve this by embedding our PP classification method within an existing holistic SRL system X  X hat is, a system which attempts to tag all semantic role types in the CoNLL 2004 data X  X hrough the following three steps: 1. Perform SRL on each preposition in the CoNLL data set; 2. Merge the output of the preposition SRL with the output of a given verb SRL system over the same data set; 3. Perform standard CoNLL SRL evaluation over the merged output.
 The details of preposition SRL and combination with the output of a holistic
SRL system are discussed below. 3.1 Breakdown of the Preposition Semantic Role Labeling Problem
Preposition semantic role labeling over the CoNLL data set is considerably more complicated than the task of disambiguating preposition semantic roles in the Penn Treebank. There are three separate subtasks, which are required to perform preposition SRL: 1. PP attachment: determining which verb to attach each preposition to. 2. Preposition semantic role disambiguation 3. Argument segmentation: determining the boundaries of the semantic roles.
The three subtasks are not totally independent of each other, as we demon-strate in Section 3.8, and improved performance over one of the subtasks does not necessarily correlate with an improvement in the final results. 3.2 Preposition Verb-Attachment Classification
Preposition X  X erb-attachment (VA) classification is the first step of preposition semantic role labeling and involves determining the verb-attachment site for a given preposition, i.e., which of the preidentified verbs in the sentence the preposition is governed by. 3.2.1 Verb-Attachment Classification Using a Maximum Entropy Classifier.
This classifier uses the following features, all of which are derived from infor-mation provided in the CoNLL data:
POS tags of surrounding tokens: The POS tags of the tokens before and after the target preposition within a window size of two tokens ([ POS tag of the target preposition The target preposition
Verbs and their relative position (VerbRelPos): All the (preidentified) verbs in the same sentence as the target preposition and their relative positions to the preposition are extracted as features. Each (verb, relative position) tuple is treated as a binary feature. The relative positions are determined in a way such that the first verb before the preposition will be given the position the second verb before the preposition will be given the position on.
 The type of the clause containing the target preposition
Neighboring chunk type: The types (NP, PP, VP, etc.) of chunks before and after the target preposition within a window of three chunks.

Word collocation (WordColl): All the open class words in the phrases before and after the target preposition within a predefined window of three chunks.
Hypernym collocation (HyperColl): All the WordNet hypernyms from all the senses of the open class words in the phrases before and after the target preposition within a predefined window of three chunks.

Named entity collocation NEColl: All the named entity information from the phrases before and after the target preposition within a predefined window of three chunks.

Chunk-based N -gram features: A series of N -gram features were used to cap-ture the more abstract syntactic and contextual features around the relevant preposition. In this study, the first five chunks after the relevant preposition were used to derive these features. These features are:  X  Regular expression representation of the chunk types: This feature is cre- X  The first word of each chunk  X  The last word of each chunk  X  The first part of speech tag of each chunk  X  The last part of speech tag of each chunk
The VA classifier outputs the relative position of the governing verb to the target preposition, or None if the preposition does not have a semantic role. Such prepositions include those that are attached to noun phrases and those that are attached to verb phrases but are not semantically labeled by CoNLL 2004.
We trained the VA classifier over the CoNLL 2004 training set and tested it on the testing set. Table III shows the distribution of the classes in the testing set.
The same maximum entropy learner used in the treebank SRL task was used to train the VA classifier. The accuracy of this classifier on the CoNLL 2004 testing set is 80 . 14%. 3.2.2 Verb Attachment Classification Using the Charniak Parser. We also experimented with the Charniak parser [Charniak 2000] in the verb X  preposition attachment classification. Since this parser was trained on the Penn
Treebank data, which was also the source for the CoNLL 2004 data, we expect its accuracy to be reasonably high.

In this experiment, the parser was used to identify which verb a given prepo-sition was attached to, or whether the given preposition was attached to a verb at all. However, it must be noted that since the parse trees produced by the
Charniak parser do not contain any semantic role information, it would not be possible to distinguish prepositions that have semantic roles from prepositions that do not have semantic roles. Algorithm I shows the process of the verb X  preposition attachment extraction.

Using the Charniak parser, the accuracy of the verb X  X reposition attachment classification is 71 . 19%. 3.2.3 Verb-Attachment Classification Error Analysis. Table III shows that more than one-half of the prepositions classified by the verb attachment clas-sifier actually did not have any semantic roles. In other words, most of the prepositions in the VA classification will not play a direct role in determining the performance of the entire preposition SRL system. Therefore, these prepo-sitions are not as important as the ones that do have semantic roles. However, this factor was not taken into account by the naive accuracy metric used to measure the performance of the VA classifier and, as a result, the naive accu-racy metric may not be able to accurately reflect the real difficulty of the VA classification task.

To address this issue, we reanalyzed the performance of the VA classifier by only looking at its accuracy on prepositions, which have semantic roles. Table IV shows the distribution of verb X  X reposition attachments without the None clas-sification. Table V shows the breakdown of the verb X  X reposition attachment classification accuracy on the test data set of both the maxent-based classifier and the Charniak parser-based classifier.

One interesting observation that can be made from Table V is that the parse tree-based classifier performed extremely poorly when the preposition was attached to the first verb after it. This suggests that either the parser did a poor job on these sentences or there is a flaw in Algorithm 1.

Recall that the None classification is assigned to prepositions not attached to any verbs, which, therefore, have no direct impact on the preposition SRL task as a whole. However, since these prepositions account for the majority of the data, if they are included in the classification, the accuracy would be appear to be higher. Hence, we are much more interested in the classification accuracy of the prepositions that are actually attached to verbs. As Table V shows, the classification accuracy of these prepositions is rather poor and, as a result, in the best-case scenario, the overall preposition SRL system can only achieve an accuracy of 66 . 99%. It would be highly desirable to significantly improve this upper bound.

Another interesting observation about Table V is that the two VA classi-fiers performed quite differently with respect to the different verb X  X reposition attachments. This means that there is a certain level of difference in the capa-bilities of these two classifiers. Therefore, even though the parse tree classifier performs noticeably poorer than the maxent classifier, it is quite possible that a significant portion of the mistakes made by the two classifiers are actually made on different test instances. A further analysis of the mistakes made by the two classifiers confirmed this: for the test set, if the accuracy was calculated in a way such that an example is considered correctly classified when one of the two classifiers produces the right classification, then the overall accuracies with and without the None classification would, respectively, become 92 83 . 24%. This would be a much better upper bound for the accuracy of the overall preposition SRL system. 3.2.4 A New Maxent Classifier Incorporating the Parse Tree Classifier for
Verb Attachment. In order to take advantage of the different strengths of the two existing VA classifiers, we constructed a new maxent classifier using all the features of the first maxent classifier and one additional feature: the classifica-tion result of the parse tree VA classifier. Table VI shows the breakdown of the accuracies of this new maxent classifier and it is obvious that this new maxent classifier performs much better than both the old maxent and the parse tree classifier. It was, therefore, used in the final preposition SRL system. 3.3 Preposition Semantic Role Disambiguation
For the task of preposition semantic role disambiguation (SRD), we constructed differences and additional features: 1. The window size for the POS tags of surrounding tokens is five tokens. 2. The window sizes for the WordColl, the HyperColl, and the NEColl features 3. The chunk tags (in the IOB format [Sang and Veenstra 1999]) of the words
We trained the SRD classifier once again on the CoNLL 2004 training set and tested it on the testing set. Table VII shows the distribution of the classes in the testing set.

We used the same maximum entropy learner as for the VA classifier to train the SRD classifier. The accuracy of the SRD classifier on the CoNLL 2004 testing set is 63 . 36%. 3.4 Argument Segmentation
Once the semantic role and verb attachment of a preposition has been deter-mined, it would then be necessary to determine the boundary of the semantic role, i.e., argument segmentation. For this task, we have experimented with both a simple regular expression based method and a more complex statistical parser approach. The details are given below. 3.4.1 Argument Segmentation Using A Regular Expression. This method determines the extent of each NP selected for by a given preposition (i.e., the span of words contained in the NP), and is based on a simple regular expression (RE) over the chunk parser analysis of the sentence provided in the CoNLL 2004 data, namely, PP NP + . Details are shown in Algorithm 2.

The performance of the regular expression-based argument segmenta-tion cannot be independently evaluated. This is because the segmentation convention used by the CoNLL 2004 data seems to differentiate between argument-type semantic roles (such as A 0 ,A 1) and modifier-type semantic roles (such as AM X  X OC, AM X  X MP ). In the case of an argument type, the semantic role boundary will start from the preposition, but, in the case of a modifier type, the semantic role boundary will start from the first word after the prepo-sition. During the boundary extraction process, the segmentation module has no access to the semantic role information, so it is not possible to determine where exactly the argument boundary should start and, therefore, the first word after the preposition of interest is always assigned to be the start of the argument boundary. In the process of combining the final outputs of all the sub-tasks, we then use the semantic role information produced by the preposition
SRD module to finally determine where exactly the relevant argument should start.

Based on the above, for the purpose of evaluation, we decided to use the per-fect preposition SRD results to first compensate the output of the segmentation module, and then compare it against the correct segmentation. The accuracy of the regular expression based segmentation method is 53.08%. 3.4.2 Argument Segmentation Using Statistical Parsers. We realized that the RE, based segmentation method was only capable of extracting arguments that were just noun phrases and was not robust enough, as a result of this limitation. Therefore, we decided to experiment with the Charniak and the
RASP parsers [Briscoe and Carroll 2002] to see if better segmentation results could be achieved.

Similar to the RE method, we assumed that the boundary of the argument starts from the first word after the preposition of interest. Algorithm 3 shows how the parse trees are used to perform the task of argument segmentation.
The evaluation of the parser-based segmentation method was performed in the same way as the RE segmentation method. The Charniak parser-based clas-sifier achieved an accuracy of 71 . 48% and the RASP-based classifier achieved an accuracy of 50 . 05%. Since the Charniak parser-based classifier worked sig-nificantly better than the other two methods, it was used in the final preposition SRL system.

We were not surprised by the significant gap between the performances of the Charniak parser and RASP. As stated before, the Charniak parser was trained over a superset of the CoNLL 2004 data, whereas RASP was trained on independent data. 3.5 Combining the Output of the Subtasks
Once we have identified the association between verbs and prepositions and dis-ambiguated the semantic roles of the prepositions, we can begin the process of creating the final output of the preposition semantic role labeling system. This takes place by identifying the data column corresponding to the verb governing each classified PP in the CoNLL data format (as determined by the VA classifier) and recording the semantic role of that PP (as determined by the SRD classifier) over the full extent of the PP (as determined by the segmentation classifier). 3.6 Parameter Tuning of the Maxent-Based Classifiers
Since the maxent-based machine-learning package can be tuned, based on the number of iterations i and the Gaussian prior smoothing parameter g ,wede-cided to train all the maxent-based classifiers on the training set of the CoNLL 2004 data, with a wide range of combinations of the two parameters. We then applied each combination on the development set, then chose the best one to apply to the test set of the data. 3.7 Merging the Output of Preposition SRL and Verb SRL
Once we have generated the output of the preposition SRL system, we can proceed to the final stage where the semantic roles of the prepositions are merged with the semantic roles of an existing holistic SRL system.

It is possible and, indeed, likely, that the semantic roles produced by the two systems will conflict in terms of overlap in the extent of labeled constituents and/or the semantic role labeling of constituents. To address any such conflicts, we designed three merging strategies to identify the right balance between the outputs of the two component systems:
S1. When a conflict is encountered, only use the semantic role information from the holistic SRL system.

S2. When a conflict is encountered, if the start positions of the semantic role are the same for both SRL systems, then replace the semantic role of the holistic SRL system with that of the preposition SRL system, but keep the holistic SRL system X  X  boundary end.

S3. When a conflict is encountered, only use the semantic role information from the preposition SRL system.
 3.8 Results
To evaluate the performance of our preposition SRL system, we combined its outputs with the three top-performing holistic SRL systems from the CoNLL 2004 SRL shared task. 6 The three systems are Hacioglu et al. [2004],
Punyakanok et al. [2004] and Carreras et al. [2004]. Furthermore, in order to establish the upper bound of the improvement of preposition SRL on verb SRL, and investigate how the three subtasks interact with each other and what their respective limits are, we also used oracled outputs from each subtask in com-bining the final outputs of the preposition SRL system. The oracled outputs are what would be produced by perfect classifiers, and are emulated by inspection of the gold-standard annotations for the testing data.

Table VIII shows the results of the preposition SRL systems before they are merged with the verb SRL systems. These results show that the coverage of our preposition SRL system is quite low relative to the total number of arguments in the testing data, even when oracled outputs from all three subsystems are used (recall = 18.15%). However, this is not surprising, because we expected the majority of semantic roles to be noun phrases.

In Table IX, X, and XI, we show how our preposition SRL system performs when merged with the top three systems under the three merging strategies introduced in Section 3.7. In each table, ORIG refers to the base system without preposition SRL merging.
 We can make a few observations from the results of the merged systems.
First, out of verb attachment, SRD, and segmentation, the SRD module is both: (a) the component with the greatest impact on overall performance, and (b) the component with the greatest differential between the oracle performance and classifier (AUTO) performance. This would thus appear to be the area in which future efforts should be concentrated in order to boost the performance of holistic SRLs through preposition SRL.

Second, the results show that, in most cases, the recall of the merged system is higher than that of the original SRL system. This is not surprising given that we are generally relabeling or adding information to the argument structure of each verb, although with the more aggressive merging strategies (namely, S2 and S3) it sometimes happens that recall drops, by virtue of the extent of an argument being adversely affected by relabeling. It does seem to point to a com-plementarity between verb-driven SRL and preposition-specific SRL, however.
There are a few aberrations in the results. Sometimes, an all-auto method achieved better results than when one of the subtasks was oracled. For ex-ample, in Table IX, when merge scheme 1 was used, the all-auto combina-tion yielded a precision of 72 . 20%, and an F score of 69 the segmentation was substituted to oracled results, the precision dropped to 72 . 12% and the F score dropped to 69 . 48%. This behavior is caused by the poor accuracy of the SRD classifier and the merging strategy. The perfect seg-mentation results would reduce the number of argument boundary conflicts between the preposition SRL system and the original system, thereby increas-ing the recall of the combined system. However, because of the poor perfor-mance of the preposition SRD subsystem, these additional arguments were not correctly classified, and this was why the precision dropped while the recall improved.

Finally, it was somewhat disappointing to see that in no instance did a fully automated method significantly surpass the base system in precision or F score.
Having said this, we were encouraged by the size of the margin between the base systems and the fully oracle-based systems, as it supports our base hypothesis that preposition SRL has the potential to boost the performance of holistic SRL systems, up to a margin of 10% in F score for S3. 4. ANALYSIS AND DISCUSSION
In the previous two sections, we presented the methodologies and results of two systems that perform statistical analysis on the semantics of prepositions, each using a different data set. The performance of the two systems was very different. The SRD system trained on the treebank produced highly creditable results, whereas the SRL system trained on CoNLL 2004 SRL data set produced somewhat negative results. In the remainder of this section, we will analyze these results and discuss their significance.

There is a significant difference between the results obtained by the tree-bank classifier and that obtained by the CoNLL SRL classifier. In fact, even with a very small number of collocation features, the treebank classifier still outperformed the CoNLL SRL classifier. This suggests that the semantic tag-ging of prepositions is somewhat artificial. This is evident in three ways. First, the proportion of prepositional phrases tagged with semantic roles is small X  around 57,000 PPs out of the million-word treebank corpus. This small pro-portion suggests that the preposition semantic roles were tagged only in cer-tain prototypical situations. Second, we were able to achieve reasonably high results even when we used a collocation feature set with fewer than 200 fea-tures. This further suggests that the semantic roles were tagged for only a small number of verbs in relatively fixed situations. Third, the preposition
SRD system for the CoNLL data set used a very similar feature set to the treebank system, but was not able to produce anywhere near comparable re-sults. Since the CoNLL data set is aimed at holistic SRL across all argument types, it incorporates a much larger set of verbs and tagging scenarios; as a re-sult, the semantic role labeling of PPs is far more heterogeneous and realistic than is the case in the treebank. Therefore, we conclude that the results of our treebank preposition SRD system are not very meaningful in terms of predict-ing the success of the method at identifying and semantically labeling PPs in open text.

A few interesting facts came out of the results over the CoNLL data set. The most important one is that by using an independent preposition SRL system, the results of a general verb SRL system can be significantly boosted. This is evident, because when the oracled results of all three subtasks were used, the merged results were around 10% higher than those for the original systems, in all three cases. Unfortunately, it was also evident from the results that we were not successful in automating preposition SRL. Because of the strictness of the
CoNLL evaluation, it was not always possible to achieve a better overall per-formance by improving just one of the three subsystems. For example, in some cases, worse results were achieved by using the oracled results for VA and the results produced by SRD classifier, than using the VA classifier and the SRD classifiers in conjunction. The reason for the worse results is that in our exper-iments, the oracled VA always identifies more prepositions attached to verbs than the VA classifier. Therefore more prepositions will be given semantic roles by the SRD classifier, thus increasing the recall of the final system. However, since the performance of the SRD classifier is not high, and the segmentation subsystem does not always produce the same semantic role boundaries as the
CoNLL data set, most of these additional prepositions would either be given a wrong semantic role or wrong phrasal extent (or both), thereby causing the overall performance to fall.

Finally, it is evident that the merging strategy also plays an important role in determining the performance of the merged preposition SRL and verb SRL systems: when the performance of the preposition SRL system is high, a more preposition-oriented merging scheme would produce better overall results and vice versa. 5. CONCLUSION AND FUTURE WORK
In this paper, we have proposed a method for labeling preposition semantics and deployed the method over two different data sets involving preposition semantics. We have shown that preposition semantics is in general, not a triv-ial problem, and also that it has the potential to complement other semantic analysis tasks, such as semantic role labeling.

Our analysis of the results of the preposition SRL system shows that signif-icant improvement in all three stages of preposition semantic role labeling X  namely, verb attachment, preposition semantic role disambiguation, and ar-gument segmentation X  X ust be achieved before preposition SRL can make a significant contribution to holistic SRL. The unsatisfactory results of our
CoNLL preposition SRL system show that the relatively simplistic feature sets used in our research are far from sufficient. Therefore, we will direct our future work toward using additional NLP tools, information repositories, and feature engineering to improve all three stages of preposition, semantic role labeling.
We would like to thank Phil Blunsom and Steven Bird for their suggestions and encouragement, Tom O X  X ara for providing insight into the inner workings of his semantic role disambiguation system, and the anonymous reviewers for their comments. National ICT Australia is funded by the Australian Government X  X  Department of Communications, Information Technology, and the Arts and the Australian Research Council through Backing Australia X  X  Ability and the ICT Research Centre of Excellence Programs.

