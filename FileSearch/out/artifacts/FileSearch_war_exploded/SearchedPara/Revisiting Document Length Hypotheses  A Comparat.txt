 SUMIO FUJITA PATOLIS Corporation 1. INTRODUCTION invalidate rights to monopolize the relevant commercial activities. Patent documents are bibliographic information. In some aspects, patent documents are considered as a form of techno-scientific writing that describes t echnological inventions. The NTCIR-3 patent task addressed such features of patent documents: simulated information needs were motivated by newspaper articles and simulated relevance assessments by a group of corporate intellectual property administrators from various industry domains. retrieval against diverse forms of documents, looking for a prior art possibly invalidating publications or uses of the invention by products or processes. The term  X  X nvalidity search X , which is the title of the main subt ask of the NTCIR-4 patent task, is understood in its broader sense to be an aspect of patent document investigation. Information-seeking efforts are similar to the  X  X echnology survey X  task, i.e., subject topic search, but invention in question is also required. Such a broad definition of the text retrieval aspects of an invalidation investigation may be a pplicable to patentability, novelty, validity, and infringement investigations when adapting to different search environments. Whatever the name, according to the functional roles in information-seeking situations, such types material, than an ordinary subject topic search of technological documents. This leads to a smaller number of relevant documents for each query. 
Among many parameters of the traditional re trieval methods that might be calibrated according to the collections and tasks, we focus on the issues of document length NTCIR-4 workshop. We examined comparativel y two different search tasks: a traditional as the main task runs of the NTCIR-4 patent task. Our examination also targeted two different retrieval models, namely a traditi onal TF*IDF approach with BM25 TF and the Kullback X  X eibler divergence (KL-divergence) approach that is one of the probabilistic language modeling approaches recently intr oduced by some information retrieval researchers [Lafferty and Zhai 2001; Zhai and Lafferty 2001b]. 
In our CLIR J-J runs, the TF*IDF runs outperformed the KL-divergence runs and both methods tend to retrieve documents of gr eatly differing lengths although the achieved performance measures are similar. These collec tions have different causes for differences in document length so the retrieval met hods should be calibrated according to the collection characteristics. NTCIR-3 and -4 test collections and Section 3 describes our experimental system for the NTCIR-4 evaluation experiments. Section 4 explains the language modeling approach for information retrieval (IR), while Section 5 contains a discussion of the document length issues in IR and an analysis of the NTCIR test collections in view of the document length hypotheses. Section 6 describes the e xperiments with newspaper retrieval using the NTCIR-4 CLIR J-J test collection and S ection 7 describes patent document retrieval experiments using the NTCIR-4 patent test collection. Section 8 concludes the paper. 2. NTCIR TEST COLLECTIONS The NTCIR (NACSIS-NII Test Collection for In formation Retrieval Systems; for details see, for example, Kando [2004]) project group has organized a series of  X  X istributed experiments/centralized evaluation X  style workshops. They provide workshop participants (and other researchers) with re usable test collections for experimental research of diverse information access technol ogies. These test collections consist of diverse forms of document collections includi ng scientific paper abstracts, newspapers, Web, and patent documents, mainly in Japanese but with some in English, Chinese, and Korean. 
In this paper, we report studies using the NTCIR-4 CLIR Japanese collection and patent task collection together with some analyses of the NTCIR-3 CLIR J-J subtask and NTCIR-3 patent task collections. 
Relevance assessments are graded in four levels as follows:  X  X ighly relevant (S) X ,  X  X elevant (A) X ,  X  X artially relevant (B) X  or  X  X on-relevant (C) X . According to these relevance levels, two types of relevance judgment files are provided: S or A documents are marked relevant in  X  X igid X  relevance judgment files; S, A or B documents are marked files are used unless otherwise noted. 2.1 CLIR J-J Test Collections The NTCIR-3 CLIR J-J (Japanese monolingual) test collection consists of 1998 X 99 Mainichi newspaper documents (220,078 documen ts) and 42 search topics with assessed document lists [Chen et al. 2002]. For the NTCIR-4 CLIR J-J test collection, Yomiuri newspaper documents (375,980 documents) we re added to the NTCIR-3 CLIR J-J 2004]. Each topic has four fields, TITLE, DE SC, NARR, and CONC, as shown in Figure 1. The TITLE field consists of a few words a nd the DESC field consists of one sentence describing the information needs. The NARR field contains two or three sentences describing the conditions of relevant docum ents and the CONC fi eld contains some participants were asked to submit at least one run using only the TITLE field and another run using only the DESC field as mandatory runs. 2.2 Patent Test Collection The NTCIR-3 patent test collection consis ts of the unexamined patent application documents of 1998 X 99 (697,330 full text, SGML-formatted documents) released from the Japanese Patent Office (JPO), plus 31 t opics cited from Mainichi newspaper articles [Iwayama et al. 2002]. The official workshop task using the collection, which was intended to simulate a  X  X echnology survey search X , was designed to retrieve patent documents by creating queries from the newspa per articles. Topic descriptions contain DESCRIPTION, NARRATIVE, CONCEPTS, a nd SUPPLEMENT fields in addition to the newspaper article texts, and they provide assessors with criteria for relevance. Patent considered relevant, although the topic may not relate to the essence of the invention. 
The NTCIR-4 patent test collection consis ts of the unexamined patent application documents of 1993 X 1997 (1,707,184 documents), plus 34 main topics for which pooling and relevance assessments were carried out , and 69 additional topics for which no relevance assessment was done but JPO citations were used as relevant documents [Fujii et al. 2004]. JPO citations are patent document re ferences used to justify the rejection of the original patent application. Both topic sets are independent claim sentences extracted application used as one of the formal run topi cs; task participants were asked to submit at relevant documents should have been published before the date stipulated in the FDATE case of  X  X onvention application X . 3. SYSTEM DESCRIPTION Our evaluation environment consisted of th e PLLS system we developed based on the Lemur toolkit 2.0.1 for indexing systems [Ogilvie and Callan 2002] with the PostgreSQL RDB system integrated for treating bibliographic information. The system was operated on a dual-CPU PC server (Xeon 2.0 GHz, 4 GB RAM) running a RedHat Linux operating system. 3.1 Indexing Language and Indexing Strategies The Chasen version 2.2.9 Japanese morphological analyzer with the IPADIC dictionary version 2.5.1 was utilized for Japanese text segmentation. Output single words are indexed, excluding stop words. Stop word lists for patent documents and for newspaper documents were prepared separately. whole patent collection at once, the NTCIR-3 and NTCIR-4 patent collections are partitioned into subcollections accord ing to the year of publication. 
Besides full-text indexing for the patent collections, selected field indexing, which uses only the author, abstract, and claim fields, was prepared. Another indexing strategy documents marked by the official tool [Fujii et al. 2004] as an independent retrieval unit matching scores of its constituent passages. 3.2 TF*IDF retrieval model The first retrieval model, which was used for the baseline runs for any new methods, is TF*IDF with Okapi BM25 TF (TF*IDF). Because we have considerable expertise with calculated as the dot product between the document term vector and the query term vector, where each term is weighted by either TF ( d , t ) IDF ( t ) or TF ( q , t ) IDF ( t ). 
As the following formula shows, BM25 TF [Robertson and Walker 1994; Robertson et al. 1995] is incorporated in these weighting functions. Typical parameters, such as k 1 or b , must be adjusted empirically [Fujita 2000]. parameter as described in R obertson and Walker [1997]. The same weighting is applied to the query but with a different value for k 1 and without length normalization, i.e., b = 0. When k 1 of TF( q, t ) is large, the query TF is usually equivalent to freq( q, t ). 3.3 KL-Divergence Retrieval Model The second retrieval model applied, the KL-d ivergence of probabilistic language models with Dirichlet prior smoothing (KL-Dir), is a comparatively new method, which we describe in more detail in Section 4. alternative smoothing method (KL-JM). For the document-dependent prior probability instead of a uniform probability. application as a search topic. Such doc ument priors are domain dependent and only modeling approaches including TF*IDF models as a promoter of matching scores of documents with appropriate IPCs. 3.4 Feedback Strategies Pseudo-relevance feedback was a pplied in both tasks. The top n documents from the result list of a pilot search were used for f eedback term extraction. We adopted Rocchio feedback [Rocchio 1971] for TF*IDF, a Ma rkov chain query update method for the KL-divergence retrieval model [Lafferty and Zhai 2001] and a mixture model query update method [Zhai and Lafferty 2001a] for some co mparative experiments for KL-divergence runs. Salient terms from these pseudo-relevant documents were extracted and added to the original query. 
For example, in Rocchio fee dback, the term precision measure to select salient terms was calculated as an element of the centr oid vector of pseudo -relevant documents. Finally, an updated query vector Q' was computed from the original query vector Q and a set of (pseudo) relevant document vectors R. 
The parameters, such as the number of doc uments for the pseudo-relevant set, the coefficients of feedback te rms against original terms ( posCoeff ), were decided by presubmission experime nts using the NTCIR-3 test collections. 4. LANGUAGE MODELING FOR IR to be theoretically sound. Thus, theoreti cally motivated retrieval models based on probabilistic language models are introduced by some researchers: Ponte and Croft first applied a document unigram model to comput e the probability of the given query being generated from a document [Ponte and Crof t 1998]. In TREC-7, Hiemstra and Kraaij [1999] used a hidden Markov model to mix tw o distributions. Berger and Lafferty [1999] information need into a succinct query. 4.1 Basic model The adopted model is simple: estimate a language model for each document and rank documents by the likelihood of their generati ng the submitted query. This is exactly the retrieval version of a na X ve Bayes classifi er, which estimates a language model for each class and ranks the classes by the likelihood of generating the document to be classified. eliminating the document-i ndependent part, we have: the probabilities of each query term q i given document d as follows: Taking the logarithm, the retrieval function becomes: 
A document-dependent prior probability p ( d ) can be either a uniform probability or any document-dependent factors that may aff ect the relevance, such as document length or hyperlink-related information. Assumi ng a uniform prior probability and dropping the first term, transforming the summation over query term positions into a summation over words in the vocabulary, and dividi ng by the query length, we have: language model, which measures the differen ce between the two probability distributions. This is equivalent to the KL-divergen ce of a query language model from a document language model when ranking docum ents against the given query. 4.2 Smoothing methods Zhai and Lafferty suggested that a smoothi ng method plays a crucial role in language modeling IR [Zhai and Lafferty 2001b]. They an alyzed the role of smoothing in language modeling IR from two aspects: to avoid zero probabilities for unseen words and  X  X o accommodate generation of common words in a query X . In this re spect, smoothing plays a role similar to IDF in TF*IDF weighti ng. They proposed three types of smoothing document model and a background model p ( w | C ); Bayesian smoothing using the Dirichlet priors method, which computes ma ximum a posteriori parameter values with a  X  X ollection X , and denotes the collection language model. 
The Dirichlet prior method is: 
The smoothing factor in the first case is  X  and  X  /(| d | +  X  ) in the second case. Document length is taken into considerati on in Dirichlet prior smoothing. 4.3 Document-dependent priors and mixture language models Two language models, which normally repres ent the textual characteristics of each document, can be combined using a parameter  X  : 
For example, Westerveld et al. [2002] used such a mixture model to integrate anchor information and URL information as docum ent priors. Any document-dependent and typically query independent factors that ma y affect the relevance can be taken into consideration by the scoring process as document prior probabilities. Some studies suggest that document length is a good choi ce in TREC experiments, because it is predictive of relevance agains t the TREC test set [Miller et al. 1999; Singhal et al. 1996], and its effectiveness has been studied by Kraai j et al. [2002] in the two different tasks of the TREC-2001 Web track. 
In our experiments, the follo wing document prior probability p ( d ) dependent on document length was incorporated to promote the scores of longer documents. By giving  X  2 a large value, the impact of the prior can be controlled. 
As a collection-dependent prior, we used the International Patent Classification [ IPC ] code attributed to each patent document in the collection. First, to estimate the IPC of the given search topic, the top n documents in the result list we re examined and for each IPC documents R ( q ), is estimated. 
The documents attributed as IPC c in the result list are promoted according to heuristic promoter, although it should work as a prior probability. Furthermore, IPC URL priors examined in Kraaij et al. [2002] are query independent. 4.4 Feedback strategies in the KL-divergence model method X , where language models for pse udo-relevant documents are distilled by eliminating background noise using EM itera tion as described by Zhai and Lafferty [ 2001], and the feedback document model p ( F |  X  ) is estimated given the mixture parameter  X  . 
Another feedback strategy performed in the patent task is the so-called  X  X arkov chain method X  proposed by Lafferty and Zhai [2001], which involve s computing the feedback and a set of relevant or pseudo-relevant documents R ( q ), as follows: 
Finally, the original query model is updated by interpolating the estimated feedback model p ( w | q , R ( q )) with a feedback coefficient  X  . 
The parameters introduced in Section 3.4, such as the number of documents for the positive documents. 5. A STUDY OF DOCUMENT LENGTH 5.1 Why the Emphasis on Document Length? During the submission procedures of the pate nt task, we found that the average number of passages in retrieved documents differs considerably; consequently, document length differs also, depending on the adopted retrieval method. For example, TF*IDF (PLLS2) retrieved documents with an average of 72 passages whereas KL-Dir (PLLS6), retrieved documents with an average of only 46 passages.
 Table 1: Effectiveness (Mean Average Precision) of official runs and their baseline runs NTCIR-4 CLIR J-J 0.3801 (PLLS-J-J-T-03) 0.3145 NTCIR-4 patent 0.1703 0.2408 (PLLS6) 
Table 1 compares the effectiveness of some runs of NTCIR-4 CLIR J-J and NTCIR-4 patent tasks. 
TF*IDF outperforms in NTCIR-4 CLIR J-J and KL-Dir outperforms in the NTCIR-4 patent task. The following relationship was observed: 
All of this suggests that: 1) the behavior of these retrieval methods varies with and 3) a combination of these characteris tics causes the reversed order of search effectiveness. 
Iwayama et al. [2003] compared the document length statistics from the NTCIR-3 patent retrieval collection and CLIR J-J coll ection. They found that the average document length in words of the patent collection is 24 times that of the CLIR J-J collection and the standard deviation of the patent collection is 20 times that of the CLIR J-J. In this paper, we investigate why patent documents are di fferent from newspaper documents in length distribution and how this difference affects retrieval effectiveness. 5.2 Document length normalization Document length normalization is a typical technique adopted by term weighting and query X  X ocument matching for document ranking in IR systems. As longer document has more words, the terms have a higher frequency than in a shorter document. In addition, a length normalization prevents the document ranking from favoring l onger documents by penalizing matching scores of longer documents. If the document length in the search target collection is uniform, no document lengt h normalization is necessary. Because this is generally not true, one way to  X  X ake it X  is to split a document into chunks of the same retrieval in TREC-1 [Kwok et al. 1992] and -2 [Evans and Lefferts 1993] experiments. This approach is endorsed by the releva nce  X  X act X  at TREC, where a document is relevant if it mentions the subject topic of the information need in a portion of the whole document. Because cosine normalization, adopted by the vector space model [Salton documents in TREC evaluation, TREC syst ems tend to adopt a revised TF function. Methods such as log TF, maximum TF norma lization, Okapi TF [Robertson et al. 1995], penalize scores of longer documents , which may have more matches. 5.3 Document length hypotheses The question to be asked is: why are l onger documents longer than shorter ones? be more likely to be relevant against diverse que ries, so that it is fair that they receive a higher matching score. Robertson and Walker [1994] postulated two hypotheses to explain different lengt hs of documents, the  X  X cope hypothesis X  and the  X  X erbosity hypothesis X . 
The scope hypothesis considers a long docum ent as a concatenation of a number of document covers the same scope as a short document but uses more words. These two hypotheses represent the extreme cases; real doc uments are always a mixture of the two. The natural consequence of adopting the sc ope hypothesis is that a long document is more likely to be relevant, irrespective of s earch requests, because it covers more subject implies that document properties, such as relevance and eliteness, are independent of document length. Because longer documents ar e more informative than shorter ones, even if the subject coverage is the same, l onger documents are more likely to be relevant even under the verbosity hypothesis. From anothe r point of view, the topic is denser in a short document, such that it should be give n a higher score if ot her matching conditions document length increases? 
The Okapi probabilistic retrieval model, also known as BM25 [R obertson et al. 1995], uses a document length correction factor as follows, assuming the verbosity hypothesis. In the TREC experiments, they always ignored the correction factor by setting k 2 to zero. Instead, they leaned toward using passage-bas ed retrieval assuming the scope hypothesis. 
Fang et al. [2003] proposed four formalized retrieval heuristics, including two length-normalization constraints as follows: 
LNC1 stipulates that the score should decrease when the document has one more nonrelevant word. LNC1 requires simply pena lizing longer documents and the constraint is normally observed by scoring functions. LN C2 prevents overpenalizing by saying that document should not be lower than the original document X . LNC2 was satisfied by Okapi, then consequently by BM25TF, but only conditi onally satisfied by KL-Dir or by pivoted normalization. This suggests that KL-Dir overpenalizes longer documents under some conditions where constraints are violated. 5.4 Likelihood of Relevance and Retrieved in NTCIR-3 and NTCIR-4 To validate the document length hypotheses fo r different types of document collections, the NTCIR-3/4 CLIR J-J and patent test co llections were examined by reapplying the analyses against the TREC test collec tions described by Singhal et al. [1996]. 
The NTCIR-3 CLIR Japanese document collection (Mainich i newspaper 1998 X 1999: 220,078 documents) and patent document collecti on (unexamined patent applications of 1998 X 1999: 697,330 documents) were put into bins of 1000 and 5000 documents, respectively, in the order of the length of documents counted by the number of indexed terms. The last bins (221st and 140th) contain the longest 78 and 2330 documents, test collection and 2311  X  X opic X  X e levant document X  pairs for 31 topics of the patent test collection. Partially relevant documents were included in these pairs to augment the data. From these pairs, p ( d in Bin i | d is relevant) for each i th bin was computed. From 42,000  X  X opic X  X etrieved document X  pairs of the CLIR collection and 31,000  X  X opic X  X etrieved document X  pairs of the patent collection, p ( d in Bin i | d is retrieved) was computed. 
Figure 3 shows p ( d in Bin| d is Relevant) (indicated as p (Bin|Relb) in the figures), p ( d p (Bin|KL_Ret) in the figures), plotted against the median document length in each bin, in the NTCIR-3 CLIR Japanese collection, and Fi gure 4, in the NTCIR-3 patent collection. In Figure 3, approximation lines of plotte d dots indicate that the ratio of  X  X F*IDF the ratio of  X  X elevance X  X  X  X ocument length X  ( P ( d in Bin|d is Relevant)) whereas the line Relevant)) increases slightly. The line of  X  X F*IDF retrieved X  X  X  X ocument length X  ( P ( d in Bin| d is Retrieved by TF*IDF)) increases linear ly whereas the line of KL-Dir decreases. Different document length hypot heses may be assumed for these two evaluation tasks. Newspaper documents typically conform to the scope hypothesis, as in the TREC collections, where the longer documents always mention more subject topics. Patent documents may be seen as conforming to the verbosity hypothesis, where longer documents use more words to de scribe a specific subject topic. As required by the  X  X nity of Invention X  principle, a patent document is about a single subject so that the document length may not affect relevance or eliteness. in each patent document in the NTCIR-3 patent collection. Figure 5 shows p ( d in Bin| d is Relevant) for 140 bins plotted against the me dian claim numbers in each bin. Observing no clear correlation between the  X  X elevance X  ( P ( d in Bin| d is Relevant)) and the number of claims suggests that a large number of claims does not necessa rily signify many  X  X echnology survey X . documents in the collection were put in to 171 bins of 10,000 documents; 459  X  X opic X  relevant document X  pairs a nd 34,000  X  X opic X  X etrieved documen t X  pairs were used. The summary, TF*IDF always tends to retrieve longer documents; this may be optimal for newspaper documents, whereas KL-Dir (PLLS 6) tends to retrieve much shorter documents. KL-Dir seems to be overpenaliz ing the matching scores of long documents because the approximation lines of P ( d in Bin| d is Retrieved by KL-Dir) is almost flat or even decreasing against document length in Figures 4 and 6. 5.5 Are Long Patent Documents Simply Verbose? The question arises in this context: if docum ent length does not affect the relevance, why coverage or topical relevance, are they ver bose as well in terms of  X  X nvalidating posterior patent applications X ? The length of newspa per documents is controlled by editorial policies, whereas no control is imposed on patent documents. Statistics from NTCIR-3 and NTCIR-4 patent document collections s how that patent documents are becoming longer every year, as shown in Table 2. Ev en with no restriction on document length, a writer must prepare more written material, po ssibly prepare graphic elements, write more document, and proofread the entire document. 
There must be reasons people exert the effo rt required to write longer documents with reasonable stylistic quality. Such reasons s eem to concern the motivation to write a patent document, i.e., to claim rights. 
Longer patent documents are stronger because: 1) they can broaden the extensions of the ri ghts covered by the cl aim, by describing 2) they can cover and describe augmen ting complexities of the technological 5.6 Document Length Hypotheses in Summary Table 3 lists the average lengths of documents in the four categories: highly relevant or relevant (SA); partially relevant (SAB ); pooled documents during the NTCIR-3/4 relevance assessment processes that are retr ieved by some participating systems and judged by human assessors (SABC); and all the documents in the collection (All docs). 
The NTCIR-3 and NTCIR-4 CLIR J-J test collections are typical examples representing the scope hypothesis, where re levant documents are 67 and 59 points longer than the whole collection. The NTCIR-3 patent test collection is a case of the verbosity hypothesis, where relevant documents ar e only nine points longer than the whole document normally falls into one technology s ubject of this granularity. Consequently, longer patent documents are not necessarily relevant to more than one such topical subject. 
The NTCIR-4 patent collection appears to fall between these two typical cases, where relevant documents are 27 poi nts longer than the whole collection. Longer patent documents possibly contain desc riptions of more prior art, which may invalidate the documents are possibly related to more than one subject topic with such granularity. Note that pooled documents are longer than the whole collection, especially in the NTCIR-4 patent [Fujii et al. 2004]. This may pa rtly suggest that the participating systems tend to have retrieved longer doc uments, but more investigati on is required for the patent tasks, where the pooling procedures are different from the CLIR tasks. 
Table 3: Average document length, counted by indexed terms, of relevant (SA), partially NTCIR-3 SA docs 315 (167%) 308 (159%) 3,164 (109%) 3,137 (127%) SAB docs 290 (153%) 289 (150%) 3,075 (106%) 2,946 (119%) SABC docs 232 (123%) 277 (143%) 3,123 (107%) 3,321 (134%) 
All docs 189 (100%) 193 (100%) 2,906 (100%) 2,478 (100%) 6. NEWSPAPER RETRIEVAL EXPERIMENTS Similar to the TREC ad hoc retrieval coll ections, the NTCIR-3/4 Japanese newspaper collections satisfy the scope hypothesis, wh ere longer documents X  preferred relevance is observed. 6.1 TF*IDF Experiments At the NTCIR-4 workshop, we submitted a title-only run, a description-only run, and two description-only run used the TF*IDF method with BM25 TF [Robertson et al. 1995] and Rocchio pseudo-relevance feedback. Because one of our aims was to compare the adopted a baseline strategy in which we had sufficient expertise. We successfully applied dot-product matching between BM25 TF weight ed vectors to TREC-9 and -10 Web ad hoc search tasks characterized by very s hort queries and considerable diversity in document length, where subdocument-based retrieval was applied [Fujita 2000]. Web scope hypothesis and the verbosity hypothesis. shows the parameters and performance of the official runs and th eir no pseudo feedback baseline runs. 
Table 5. Effectiveness and average lengths of retrieved documents counted by indexed terms (ALRD) of CLIR unofficial runs with JM smoothing and Diri chlet prior smoothing TD-01 and TD-02 runs, which are title and de scription runs, are a fusion of T-03 and D-04 with different mixture parameters. where  X  is either 0.43 (TD-01) or 0.5 (TD-02), re spectively, as shown in Table 4. These parameters were calibrated using the NTCIR-3 J-J test collection. MAP indicates mean documents whereas  X  X elax X  evaluations us e  X  X elevant X  and  X  X artially relevant X  documents. The average length of retrieved documents (ALRD) is the average length, counted by indexed terms, of the top 1000 documents retrieved. 6.2 Language Modeling Approach Experiments Table 5 compares the experimental runs w ith Jelinek X  X ercer smoothing/Dirichlet prior smoothing. As described in Section 4, Jelin ek X  X ercer smoothing is a traditional version of smoothing, which was adopted by some participating TREC groups [Hiemstra and Kraaij 1998; Miller et al. 1999]. with the original query models for the sake of comparison with the TF*IDF runs. As was suggested by Zhai and Laffe rty [2001a], the mixture model update method is very coefficient values from 0.0 to 1.0 with interval s of 0.1, and then selected the best settings. Although we tried to find the best settings, th ese MAPs, after some parameter tuning, are pseudo feedback applied they are outperfo rmed by the TF*IDF baseline runs. The TF*IDF runs gain 23.0 to 27.8% improvement in MAPs and 16.9 to 20.9% in KL-Dir runs by applying pseudo feedbacks. KL-JM r uns have a 29.7 to 41.6% improvement by pseudo feedbacks, which is exceptionally good in our experience. This is because KL-JM no-feedback baseline runs are poorer than th e TF*IDF or KL-Dir runs. Improvements are overestimated by these poor baseline runs. We suspected that one of the reasons for the failure is the long-document-preferred re levance judgment observed in the NTCIR-3 CLIR J-J test collection. The best-perfo rming TF*IDF runs retrieve much longer documents than the poor-performing KL-Dir or KL-JM runs. To validate this hypothesis, we apply document length priors and prom ote matching scores of longer documents. them. Slight improvements are observed in the KL-JM runs but no improvement is promotion by document length does not seem to help, even in such an evaluation environment. 7. PATENT DOCUMENT RETRIEVAL EXPERIMENTS The NTCIR-3 patent document collection satis fies the verbosity hypothesis whereas the NTCIR-4 patent task falls between the two hypotheses. 7.1 Overview of NTCIR-4 Patent Task Experiments We submitted six mandatory runs, which use only the  X  X LAIM X  field, of full-automatic query construction. The TF*IDF runs use the same scoring as the CLIR J-J runs. The KL-divergence runs use the scoring method described earlier in this paper. Pseudo-document lists, patent main task participants are asked to rank all the passages in addition to each ranked document. Because we fo cused on evaluation on a document basis, passage ranking and passage-based eval uation are ignored in this paper. 
Three topic sets (main, additional, and a ll), three different relevance judgment sets (MAP and an average search-length-based meas ure) lead to a combinatorial explosion of evaluation results such that as many as 20 evaluation scores (and c onsequently different ranks between submitted runs) are assigned for each run. The sources of unstable intersystem ranking seem to be the coexisten ce of a small number of relevant documents and unstable judgment. If only one relevant document was given where the average precision is equivalent to the Mean Recipr ocal Rank, the system failed to rank the relevant document at the top of the list, but somehow ranked it in second place, thereby precision. 
N -precision measures are not adequate becau se the number of relevant documents is document is not considered by these measures. Consider r -precision, where r is normally one to five. Many systems might produce an r -precision of 0.0 for most search topics. 
Search length basis measures, which the orga nizers adopted as the alternative measure, the rank orders of relevant documents in sear ch results, to compare search engines. The problem is how to average the values from each result set against different search topics (micro average or macro average) and what penalty should be given when the system fails to retrieve any relevant documents. 
Although there is controversy over the use of MAPs as the evaluation measure for patent document retrieval (especially with invalidation search, where the number of relevant documents is so small that eval uation results may be unstable), we adopted MAPs for the sake of comparison with ne wspaper retrieval. The observations were carefully examined to determin e if they were sufficiently st able across different settings. are analyzed in the following subsections. Note that, because relevant documents must be differs depending on the topic; consequently, the average length of documents to be documents of our official pa tent runs. Although PLLS1 to P LLS5 use selected indexing, the document lengths are counted based on the full text document. Tables 8 and 9 show the parameter settings of TF*IDF and KL-Dir used in official runs and the subdocument-based runs described in the next subsection. 7.2 Indexing Strategies: Full Text/Selec ted Fields/Subdocument-Based Indexing whereas PLLS6 uses full-text indexing. The average length of documents in indexed 
Table 6. Effectiveness (MAP) of patent o fficial runs (A, A&amp;B) and average length terms of the full-text index is 2478 whereas that of the abstract and claim fields index is only 298. The indexing strategy a ppears to be a crucial factor in patent document search, task revealed the predominan ce of full-text indexing over se lective indexing [Iwayama et keywords of the document and claim fields w ould describe the essence of the invention, so that these fields could act as a surr ogate index of the entire document. Some commercial patent full-text search services index only these fields. patent terminologies, e.g., inte ntional uses of nonstandard te rminology and idiosyncrasies of the high average precision. This also app ears to be the case in NTCIR-4, although we underestimated this and it was very misleading for us. We spent most of the preparation time tuning the system to perform optimally ag ainst the selected indexing databases, but selected field index. approximately the same length, to retrieve against such chunks, and to decide the document score according to the scores from constituent chunks. This strategy, the so-called subdocument-based retrieval, was successfully adopted by some TREC for the Web document search task at TREC [Fujita 2000], where document length varied considerably, so that some extremely l ong documents achieved high matching scores against almost any query submitted. those in the Web collection, but subdocument-based retrieval does not work as well. The organizers X  tool for passage marking. The average number of indexed terms for each selected field indexing) a nd the maximum score from all the chunks apart from the abstract. Although subdocument-based retrieval assumes the scope hypothesis, i.e., a document consists of many subject scopes and each c hunk split from the document falls into one of The results show that the best performance was achieved when  X  was close to zero, i.e., selected field indexing. The av erage length of the retrieved documents is almost the same even if the mixture coefficien t changes, because the passage-b ased scoring is essentially a reranking of the selected field indexing given our implementation. collection seem to relate to chunks that were too small. Even in the Web ad hoc search, the best performance was achieved when the passage length was much longer [Fujita 2000]. 7.3 Distributed Retrieval Strategy for Gr id Computing vs Centralized Retrieval A Distributed Selective Search is one way to seek trade-offs between efficiency and collection is partitioned by some criterion, such as publication date order, author X  X  name subcollections. The search process consists of: 1) selecting the subcollections to be subcollections, if the user requests it. 
Many studies on distributed re trieval carried out by resear chers of the IR Society tend to focus more on the subcollection selection (a lso called database selection) [Callan et al. 1995; Larkey 1999; Larkey et al . 2000; Fujita 2001]; failing to properly select the target grid-style highly distributed computing environment. This is because large collections can be exhaustively searched using a divi de-and-conquer strategy on inexpensive PC networks. 
PLLS6 used a simple score merge strategy of five subcollecti ons partitioned by the partitioning the collection by the year of publi cation is desirable, not only because search constraints using the published year are ve ry common in commercial patent retrieval tasks but also because it is efficient for pe rforming the NTCIR-4 patent task, where there information need be propagated through the network. Ranking lists from each retrieval process are merged into one ranking, which is ordered by raw scores from each process. This simplicity is very advantageous when applied to a grid-style highly distributed computing environment, in terms not only of the search time but also of allowing separate management of a large volume of collections. 
In the TF*IDF approach, IDF and documen t length normalization use collection-wide the KL-divergence language modeling a pproach, the background language models p ( w | C ), which are global statistics, cause a sc ore comparability problem across different collections, the KL-divergence approach seems to be robust for score merging because, unlike TF*IDF, KL-Dir does not use a coll ection-wide average document length to normalize each RSV. In fact, the KL-Dir distri buted runs were found to be slightly better by merging each subcollection of statistics into collection-wide statistics at run time. We from all the subcollections, to make the ma tching score from the subcollections more comparable. 
We carried out a comparative evaluation fo cused on a distributed/centralized search with different retrieval models (TF*IDF / KL -Dir / KL-JM), as shown in Table 9. There comparing the cases where pseudo feedback was or was not applied, the situation did not give different outcomes. This result of comparing the distributed and centralized searches confirms the result reported by Larkey et al . [2000] where the USPTO patent collections were partitioned into 401 subco llections by chronological orde r and retrieval results from each subcollection were merged by nor malization methods, although normalization methods did not affect the effectiv eness measured by high precision. 
In the TF*IDF runs, the centralized runs tended to retrieve longer documents than distributed runs, while there was no consider able difference in the KL-Dir or KL-JM runs. The centralized TF*IDF runs used the collection-wide average document length ( avdl ) for length normalization but the di stributed runs used different avdl s, according to the publication year of the patent. Because of publication date constraints set by the task collections where document lengths are shorter th an in later years, as shown in Table 2; runs. 7.4 KL-Divergence vs TF*IDF Compare the MAP of PLLS1 (best-performing TF*IDF) with PLLS3 (KL-divergence) in Table 6. Both use selected indexing. PLLS1 is slightly better for three evaluation points three points (add_rel.a, add_rel.b , and all_rel.b). Comparing these using other evaluation measures also gives the impression that there is no significant difference in effectiveness. As seen in the analyses of probabilities of relevance/retrieved made in the previous sections, there seems to be no specific adva ntage of TF*IDF over KL-divergence in the patent collection with respect to document length issues. Paying special attention to document length normalization factors, we adjust ed some parameters as described in the difference ( P &lt; 0.05) between the best TF*IDF run and the best KL-Dir run, irrespective are clearly longer than KL*Dir, whereas KL-JM retrieves documents with lengths between these two models. In the CLIR J-J e xperiments, KL-JM tende d to retrieve longer documents than KL-Dir but the situation is contrary here. Presumably, KL-Dir penalizes extremely long documents much more than KL -JM does. In view of the average length of the retrieved documents, KL-JM is the closes t to that of the relevant documents but its performance is no better than that of the other models. 7.5 Patent Task with Document Length Penalization To achieve our best TF*IDF performance, 0. 2625 (MAP, centralized), we assigned 0.9 to 1.0 to the parameter b of BM25 TF, which means we are maximizing the penalization of long documents. same length. If the document length is controlle d, as in subdocument-based retrieval, 0.2 [2001]. Our NTCIR-4 CLIR J-J runs used 0.35 to 0.4 for b . Therefore document length penalization helped considerably in patent document retrieval. In other words, the KL-Dir retrieval model, which performs similarly to the best TF*IDF run against the NTCIR-4 patent collection and retrieves much shor ter documents, appears to incorporate very strong document length penalization. Note that TF*IDF retrieves longer documents than the other models even with such strong length normalization. We assigned 0.9 to k 1 here whereas k 1 was 1 to 1.2 in NTCIR-4 CLIR J-J. This means that a slightly flat TF curve performs better. be better than the query part of BM 25 TF, i.e., actually a raw query TF. 7.6 Pseudo Feedback vs No Pseudo Feedback PLLS6 has a 15.0% improvement over its no pseudo feedback baseline run (MAP of main_rel.a set: 0.2094). As shown in Table 10, the best KL-Dir runs improved by 9.5 to 9.7% by pseudo feedback while the best KL -JM runs improved by only 0.7 to 6.2%. The best TF*IDF runs were degrad ed by pseudo feedback. In the patent task runs, we adopted feedback. 
Table 11 shows the best KL-Dir and KL-J M runs with pseudo feedback using the mixture model query update method. The KL-D ir runs improved by 0.08 to 5.7% while the KL-JM runs were degraded. The feedback coefficient was only 0.1 when the best performances were achieved, whereas it was 0. 8 to 0.9 in the newspaper experiments. This indicates that feedback document models are not a good representation of relevance, so that they increased e ffectiveness only when the baseline performance was poor. 7.7 IPC Priors vs Uniform Priors For PLLS6, where IPC priors were applied, the MAPs of the baseline runs were 0.2347 (main_rel.a) and 0.1702 (main_rel .b). PLLS6 achieved a +2.5% gain in A judgment and  X 1.0% in B judgment. Although the improvement is not statistically significant, small but consistent improvements throughout the KL -Dir run settings were observed while performance was degraded in the TF*IDF runs . One of the reasons improvements are not very large is presumably because a significan t change of the IPC systems from Version 5 to Version 6 occurred in 1995, i.e., in the middle of the period of document collections. The  X  X ubclass X  classification items of IPC, as shown in the next example, of which there category after the major change. 
As discussed in Section 4.3, the IPC prior is not a document prior probability but it is where pilot search results were reranked acco rding to the promotion scores based on the IPCs. 8. CONCLUSIONS A comparative study of Japanese newspape r and patent retrieval using the NTCIR-4 CLIR J-J and patent collections has been reported with the focus on the document length normalization of the retrieval functions. Docume nt length issues for different collections were examined using NTCIR-3/4 CLIR J-J and patent collections and two document length hypotheses, i.e., the reasons for the doc ument length variation were assumed to be because of either the scope hypothe sis or the verbosity hypothesis. 
The TF*IDF approach and the KL-diver gence language modeling approach were applied to two test collections with diffe rent document characteristics and different retrieve longer documents, outperformed th e KL-Dir method while no statistically promotion by document length prior does not improve the performance of KL-Dir. In the patent document retrieval, a retrieval functi on with a strong documen t length penalization, i.e., TF*IDF BM25TF with a higher value for parameter b or KL-Dir, which intrinsically strongly penalizes very long documents, performed well. 
An evaluation of comparative results s uggests that we have not yet achieved a newspaper retrieval, where an adjustable document length normalization factor intrinsic to the smoothing method ideally should be in corporated. In patent retrieval, a good document prior probability, e.g., estimated usi ng IPC information, may help. More work on this issue is still required. ACKNOWLEDGMENTS We extend thanks to the NII-NTCIR proj ects for providing the NTCIR-3/4 CLIR and patent test collections. We are grateful to the CMU/Umass Lemu r Project for making the Lemur toolkit helpful comments and advice. REFERENCES 
