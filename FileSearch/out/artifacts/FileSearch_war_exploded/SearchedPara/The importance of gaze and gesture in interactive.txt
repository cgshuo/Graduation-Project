 Kristine Lund Abstract The objective of this research is twofold. Firstly, we argue that gaze and gesture play an essential part in interactive explanation and that it is thus a multi-modal phenomenon. Two corpora are analyzed: (1) a group of teacher novices and experts and (2) a student teacher dyad, both of whom construct explanations of students X  reasoning after viewing videos of student dyads who are solving physics problems. We illustrate roles of gaze in explanations constructed within a group and roles of gesture in explanation constructed within a dyad. Secondly, we show how the analysis of such knowledge-rich empirical data pinpoints particular difficulties in designing human X  X omputer interfaces that can support explanation between humans, or a fortiori, that can support explanation between a human and a computer. Keywords Collaboration Computer-mediated human interaction Interactive explanation Gaze Gesture Multimodal communication 1 Introduction The study of human interaction has recently moved from considering the exchange of language as its primary mode (Norris 2006 ) to viewing interaction as a complex phenomenon consisting of different communicative channels that function collec-tively, allowing for what can be called  X  X  X ultimodal interaction X  X . In particular, gaze (e.g. Bavelas et al. 2002 ), and gesture (e.g. Cosnier 2000 ; McNeil 2000 ) have been studied as playing an integral role in human interaction. The term multimodal encompasses a wide variety of phenomena in the literature, including emotions and attitudes conveyed through prosody (Allwood 1998 ), applause, laughter or silence in answer to a question (Pallotta et al. 2004 ), body movements, object manipulations (Mondada 2006 ) and proxemics, layout and posture (Norris 2006 ). Clearly, human face-to-face communication occurs not only through speech.

In a different vein, the term multimodal is also often used to signify the medium in which a particular message can be expressed, for example text or graphics (e.g. Pineda and Garza 2000 ). Kress and Van Leeuwen ( 2001 ) distinguish between the two: modes are the abstract, non-material resources of meaning-making whereas media are the specific material forms in which modes are carried out. The mode of gesture is carried out in the media of movements of the body. Different media afford different kinds of meaning (Dicks et al. 2006 ), e.g. expressing an idea in writing or speech affects what is conveyed. Here, we use the term multimodal to describe the addition of non-verbal human face-to-face (or video-conference mediated) interac-tive phenomena such as gesture, gaze, posture, object manipulations, etc. to speech and we use the term multimedia to describe the channels that convey them, e.g. through a computer interface: text, graphics, pictures, sound, animations, etc. This distinction becomes necessary when we want to see how multimodal interaction between humans can be supported by either multimedia representations on the computer or by computer interpretation of multimodal communication.

Researchers have begun to study various multimodal corpora of human interaction in order to inform design of different aspects of computer-mediated communication with humans. In one such approach, researchers seek to learn how non-verbal modalities are used in natural dialogues in order to design user interfaces that use these modalities in similar ways. The focus is that either a computer produce appropriate multimodal communication (Rehm and Andre  X  2005 ) or is able to interpret it (Cohen et al. 2002 ). We will argue that the two are inextricably intertwined.
In this article, we analyze face-to-face human collaborative construction of explanations of high school physics students X  knowledge and reasoning. Participants use different physical resources in order to aid them in their explanation construction (e.g. transcriptions of student dialogue and videos of their talk, gesture and experimental manipulation). They are also guided by instructions as their explana-tions take place within the context of teacher education. We focus on the role of participants X  gesture and gaze during their collaborative interactive explanation. Our objective is to illustrate how gesture and gaze play a role in the collaborative production of explanations and to show how production is related to interpretation. In what follows we will present our theoretical framework, describe two empirical studies, and illustrate the phenomena of gaze and gesture in interactive face-to-face human explanation. Finally, we will discuss how these results could inform design of computer supported human explanation and offer conclusions. 2 Theoretical framework If human interaction is multimodal, in what specific ways is explanation multimodal? Ruben ( 1993 ) remarked that the word  X  X xplanation X , like many words ending in  X  X ion X , suffered from the ambiguity of referring either to a process (activity of explaining) or to a product (information transmitted by the means of such an activity). Each discipline sees explanation as one or the other. For example, most of the theories of explanation in philosophy focus on the products of the act of explaining (Achinstein 1988 ). In cognitive science, Schank ( 1986 ) considers explanation as a process, carried out by an individual who defines steps (find the anomaly, establish how to render it less abnormal, etc.). Finally, in linguistics, de Gaulmyn ( 1991 ) also speaks of explanatory processes, but within conversational sequences occurring between people.

In this work, we are interested in explanatory activity as a process within human interaction and more precisely within goal-oriented dialogues. Explanation is a process that addresses the nature, choice and the structure of knowledge within interaction but that also addresses the construction and the negotiation of this knowledge in dialogue (Baker et al. 2000 ). We take this postulate for explanation further, extending dialogue to include multimodal forms of interaction (Lund 2003 ). 2.1 Gaze in interactive explanation While the direction of a speaker X  X  gaze is not a completely reliable indicator of whom he or she is addressing, it is still one of the best ones (Kerbrat-Orecchioni 1998 ). As participants gaze at and address their speech to one another, they regulate conversational turns. However, gaze is also simultaneously related to informational content (Cassell et al. 1999 ). The difficulty in analyzing gaze lies in mapping observed eye movements to the user intentions that produced them (Salvucci 1999 ). Illustrations of how gaze can signify such intentions are what we would like to illustrate in this article. 2.2 Gesture in interactive explanation According to Kendon ( 2004a ), gesture is related to speech in terms of how they coordinate temporally and in terms of meaning. Kendon ( 2004b ) calls for an exploration of how different uses of gesture work in relation to the uses of spoken language. This is one of our objectives, specifically for interactive explanation.
In some research, the role of gesture focuses on objects either in the environment or that are the focus of discussion. For example, Roth and Lawless ( 2002 ) showed that students produce metaphorical gestures that embody conceptual and abstract entities when they talk in the presence of material objects. Beattie and Shovelton ( 2002 ) found that the communicative power of gestures varied with the viewpoint (character or observer) from which a gesture is generated. In other research, the role of gesture focuses on interaction between participants, e.g. Cohen et al. ( 2002 ) have found many instances of collaborative speech and gesturing, in which one party gestures during another X  X  utterance with either the goal of correcting or completing the utterance or to show understanding (see also Bolden X  X  study ( 2003 )of collaborative completions). And according to Hollerand and Beattie ( 2003 ), speakers use gesture to clarify verbal ambiguity. Here, we will focus on the role of gestures within human interaction, but in relation to objects in the environment as well as conceptual entities.
 3 Two empirical studies We studied a group and a dyadic teacher education situation, both occurring at the French teacher-training institute in Lyon. Table 1 shows their global structure.
The group interaction was an experimental teacher training exercise analogous to a research meeting where data are collaboratively analyzed. Five people participated, a teacher educator (JM), a didactics researcher (Ch), a new teacher (Ge), a teacher in training (La) and this author (Kr). The new teacher taught the labwork that was subsequently studied. The dyadic interaction was an institutionalized version (part of a teacher education class) of this exercise for which a web-based computer interface was developed (Lund 2003 ). The group interaction occurred around a table and was filmed and transcribed. The two students from the class worked side by side, on a computer and their interaction was also filmed and transcribed. 3.1 A group of teacher novices and experts The group studied the verbal and non-verbal activity of high school students during physics labwork. They individually watched a video of student labwork and collectively read a transcription of the student dialogue. The goal was to justify proposed modifications to the didactics tools used to design handouts for physics teachers and students. A set of questions was given to participants to guide them.
Table 2 shows the objectives that X  X s a function of this task X  X rive interaction roles 1 and by consequence, influence explanatory activity.

These objectives are inherent to the actions that participants X  speech accom-plishes. Actions  X  X  X o things X  X  (Edwards and Potter 1992 ) and thereby also influence explanatory activity. The focus is not on what participants are thinking, rather, it is on what they are visibly doing through discursive action. Antaki ( 1994 ) has explored how participants  X  X  X o things X  X  during explanation, in the ways mentioned by Edwards and Potter and we shall see through our analyses, how gaze and gesture participate.  X  How does an objective form part of an action?  X  How do objectives demonstrate the interest of the explainer or of the person  X  How do givers and receivers of explanations demonstrate their own and the 3.2 A dyad of student teachers The student teacher dyad analyzed videos and transcriptions on screen (cf. Fig. 1 ). In one video (top right), high school students studied a rock suspended from a string on a scale as they discussed the physical forces acting on these objects (see gesture analyses below). The left-hand frame in Fig. 1 contains a set of links towards different resources.  X  X  X our instructions for today [ votre consigne pour aujourd X  X ui ] X  X , is currently showing in the middle window. The link  X  X  X abwork instructions [ la fiche TP ] X  X  shows the high school students X  instructions. Four other links lead to two videos of high school physics labwork and two corresponding transcriptions of the verbal and non-verbal activity of the high school students during those labwork. The bottom right contains a text editor in which the student teachers answered questions. 4 Analyses We will illustrate roles of gaze and gesture with excerpts from the group and dyadic interactions, respectively. All excerpts have been translated from the original French.
 4.1 Gaze Using gaze, as well as syntactic, semantic and sequential analysis of speakers X  utterances, we will show that explanatory activity is characterized by how speakers choose addressees as a function of mobilized knowledge. We will first refer to how participants in the group oriented their gaze during each speech turn 2 (cf. Fig. 2 ). These analyses were done manually from the video by noting the gaze patterns of each speaker on a printed transcription of the dialogue. 3 The key at the top shows the participants that are gazing. In this corpus, a participant looked: (1) at an individual, (2) at his/her own papers (P), (3) elsewhere (LE), e.g. in the distance, (4) downwards (D), (5) between two people at least three times (MG Ch La) where MG is moving gaze or (6) between more than two people at least three times (MG).
In Fig. 2 , for a particular participant, gazes elsewhere and downwards were combined (and labelled LE) and all moving gazes were combined (MG). A participant X  X  gaze was coded only during his or her speech turn. Thus, the gaze of a participant who was being spoken to was not coded, barring co-occurring speech, as both participants are speakers. Within a given speech turn, the beginning and ending of a particular gaze were not recorded. The group discussion included 1,579 speech turns for an hour 1/2 of interaction.

This same figure shows the differences in the way each participant manages his or her gaze. For example, apart from looking at Ge and La, Ch often uses the moving gaze. La often looks at her papers (of which each participant had a copy). We now turn to how the participants address their speech to one another the complexity of which is illustrated by Fig. 3 .

In our analysis, we have considered that Ch has addressed speech to 13 different combinations of different participants. This representation shows the chronology of addressing during speech turns: addressing Ge, then La vs. addressing La, then Ge. Figure 3 shows that Ch mostly addresses his speech either to La, to Ge or to the group (collective addressing, hereafter CA). Other participants also address their speech either to one person or to the collectivity most of the time. In Fig. 4 we thus collectivity. In this study, CA included both addressing everyone in the group as well as addressing parts of the group (at least two participants). An utterance could be coded as CA with a variety of gaze patterns, depending on the other indicators (syntactic, semantic and sequential analysis): La + Ge + JM, LE + La + Ge + P or P + LE.

In Fig. 4 , the participants who are addressing speech are listed at the top right. Is there a difference between our speakers in regards to addressing speech, as there was for gaze? Let X  X  take our five speakers one after another: Ch, La, Ge, JM and Kr:  X  Ch addresses most speech to La, does collective addressing (CA), and finally to  X  Ge addresses the same amount of speech to Ch and to La, followed by collective  X  La addresses primarily Ch (114), then Ge (79) and finally the collectivity (22)  X  Speaker JM addresses very little speech, 3 times to Ch, and twice each to Ge et  X  Kr also addresses very little speech: 10 times to the collectivity and 8 times to
The person to whom most speech is addressed is Ch and the people that address speech to him are La and Ge. In terms of quantity, speech is addressed to La by Ch and Ge and finally to Ge by Ch and La. Nevertheless, Ch addresses speech 71 times to the collectivity, Ge 40 times and La 25 times. Dialogue is principally taking place between Ch, Ge and La. La addresses the most speech. Kr X  X  weak intervention rate is due to her participant observer status. In general (with the exception of JM), the institutional roles of the speakers influence their speech addressing (e.g. Ch often addresses the collectivity as the  X  X  X xplainer X  X , one of his interaction roles). The fact that Ch has more speech addressed to him than any other participant could also be attributed to his dominant socio-institutional role (Argyle and Cook 1976) as keeper of the didactic knowledge, used to interpret the high school physics labwork discussed by the group. Alternatively, the participants could be questioning his knowledge but the semantic content of their utterances does not reflect this. 4.1.1 Roles of gaze during explanation The analyses of our corpora uncovered five uses of gaze during interactive explanation. In this section we will describe each type of gaze, illustrate it with a corpus extract 4 and place it within the context of the existing literature.
In the extract in Table 3 , gaze does not imply addressing speech. Here, Ge explains to La (who was not present for a previous analysis session) why she speaks about  X  X  X omething important X  X  in the transcribed and printed out labwork interaction the group is studying. Initially, Ge looks at La and addresses her with  X  X  X ou X  X , but on four occasions, her gaze also goes to Kr (intervention Nos. 31, 37, 39 and 40). Here, interpretation requires contextual information. Firstly, Ge justifies her speech on the transcript, because student teachers learn that they should not give answers, but let students discover how to solve problems themselves. Secondly, Kr knows what Ge is explaining as she was present for the previous session. Thirdly, Kr also filmed the labwork when Ge taught it and Ge explains to La how she repeated the important things students said so that the camera X  X  microphone could pick it up. Ge X  X  gaze moves to Kr when Ge speaks about aspects of her explanation that implicate Kr, but we don X  X  consider that Ge stops addressing La. Indeed, La responds to her. In the extract in Table 4 , Ge responds to Kr, but since in addition to looking at Kr, she looks at Ch, we presume she is addressing the collectivity (note the  X  X  X e X  X ). Indeed, both Kr and Ch were present at the previous analysis session Ge is referring to and thus have also experienced what she refers to and Kr confirms. This excerpt differs from the first in that the speaker addresses those she gazes at and that the situation she refers to is shared knowledge for all participants concerned.
In Table 5 , Ch and Ge are discussing how students understand the term  X  X erify X  during labwork. In No. 134, Ch affirms that the most common usage is to verify a law while glancing at his teacher educator colleague, JM. This could be a (possibly failed) request for consensus, as JM does not respond to the gaze, at least verbally.
Table 6 shows how Ch solicits a response from a second person (La) by gazing at her (No. 472) and by using  X  X  X ou X  X , while in conversation with a first person (Ge). Ch refers to a previous intervention La made with his use of  X  X  X ou X  X .

In these examples, speakers use gaze to address speech, but also as either acknowledging a state of affairs between participants (e.g. common ground) or attempting to obtain a reaction from the participant they are looking at. In both cases, gaze is wholly interactive and a function of communicative intent. 4.2 Gesture In this section, we will show how explanatory activity is characterized by a series of relations between gesture and speech. The extracts below are taken from the dyadic interaction: two student teachers who participated in an institutionalized version of the group interaction (from which the gaze examples were taken). 4.2.1 Roles of gesture during explanation Our first example (Table 7 ) is classical gesturing in relation to a physical object. Li makes a deictic gesture in an attempt to designate a referent (Cosnier 2000 ), the spot in the transcript that interests her and Pa.

The example in Table 8 combines both speech and gesture in that the gesture makes explicit the verbal activity. Gesture finishes a phrase begun verbally.
In Table 9 , speaker Pa completely replaces her verbal activity with a gesture that describes how the scale can act on the rock. She begins a phrase  X  X  X ou know the fact that... X  X  and finishes it with a descriptive gesture.

In Table 10 , the speaker Li accompanies the phrase  X  X  X  movement of speed X  X  with a gesture and this begins a long discussion. She reads aloud  X  X  X ovement of speed X  X  from the transcript of the video and reproduces the gesture the student made while he said  X  X  X ovement of speed X  X . Their task is to understand the student X  X  reasoning and since a  X  X  X ovement of speed X  X  does not make immediate sense, Li looks to the multimodal expression of the student in order to discuss this understanding with her partner.

A few turns later, Table 11 shows how Li again reproduces the student X  X  gesture, but modifies it X  X nstead of moving her hand up and down, like the student did originally (and like she did the first time she reproduced it), she moves it from side to side. Then she says:  X  X  X e X  X  making speed X  X . We could conclude that the salient characteristic of the gesture is not its direction of movement, but simply its movement.

This hypothesis about gestures having a salient characteristic seems to be confirmed in the next extract (Table 12 ). Pa transforms the student X  X  gesture in a much more radical way, by rocking her body back and forth in her chair.

In Table 13 , Li types their interpretation of the student X  X  reasoning in their document and Pa agrees. And again, she reproduces the gesture in yet a different way.
Clarification of verbal ambiguity (Hollerand and Beattie 2003 ) through gesture or deictic gestures, are already present in the literature. However, reformulating a student X  X  gesture in different ways (while maintaining the salient characteristic) through pedagogical discussion of it seems to be a rather novel phenomenon for teachers, but see also affect attunement between mothers and infants (Stern 1985 ). 5 Designing human X  X omputer interfaces to support explanation between humans Authors have begun to analyze human multimodal activity during face-to-face collaborative tasks with the goal of creating multimodal systems capable of integrating input from gestures, gaze and haptic modalities, input that would support and not hinder collaborative work (Cohen et al. 2002 ). But how feasible is this?
According to Monk and Gale ( 2002 ), providing full gaze awareness (knowing what someone is looking at and having the possibility of eye-contact) in a computer-mediated system saves the turns and words required to complete a task. This is partly because full gaze awareness provides an alternative to the linguistic channel for checking one X  X  own and the other person X  X  understanding of what was said. Regardless of the role of gaze, when full gaze awareness between humans is supported in a computer-mediated system that supports human interaction, these roles should continue to function. However, in systems where a human interacts with a computer, the types of gaze interaction presented in this article seem to be out of our reach for now. Gaze happens because of a speaker X  X  intention or because of a speaker X  X  reaction to an object or a person in his or her environment, both of which are not currently reproducible by a computer in a general way.

What about supporting gesture in computer-mediated systems? Fussell et al. ( 2004 ) suggest that simple surrogate gesture tools (such as a pen) can replace the hands and be used to convey gestures from remote sites, but that the tools need to be able to convey representational as well as pointing gestures to be effective. For these authors, representational gestures are used to represent the form of task objects and the nature of actions to be used with those objects. In the examples we presented, it seems that the use of surrogate gesture tools would change the way in which the student gestures were reproduced. Our future teachers could use a pen to recreate the student X  X  hand movements when they were attempting to understand what he meant by  X  X ovement of speed X , but would they? In terms of human X  X omputer interaction, the same argument for gaze holds for gesture X  X ow could a computer  X  X ntentionally X  produce and react to other speakers with the gestures that we have shown here? 6 Conclusions In this article, we have shown that different participants have different gaze profiles during a given explanatory task and that these profiles are a function of their socio-institutional roles when they interact. We have also shown how interactive explanation is multimodal by illustrating some of the roles that gaze and gesture play in interactive face-to-face explanation between humans. In our gaze examples, communication (e.g. common ground) by gazing at the person they were addressing or attempted to obtain a reaction from the participant they were looking at (e.g. an intervention of some kind). In the gesture examples, some were more classical such as clarification of verbal ambiguity or deixis. But others were more atypical, such as reproducing gestures of others (while modifying them) in conjunction with their speech in an attempt to understand what the others meant. The interpretation of both gaze and gesture was related to their production within the context of an on-going collaborative interaction and in relation to the construction of social actions.
Both gaze and gesture within the type of explanations shown here, require the support of full visualization of human interactive distance activity in computer systems if humans are to continue to communicate in the ways they are used to. Human gaze and gesture in interactive explanation are driven by speaker intention or occur in reaction to co-participants. They are also interpreted in relation to the interactional context. These difficulties make supporting interactive explanation between a human and a computer currently insurmountable, at least in a general way.
 References
