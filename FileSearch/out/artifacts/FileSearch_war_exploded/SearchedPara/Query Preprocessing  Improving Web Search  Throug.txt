 In this poster paper, we propose a novel approach to improve web search relevancy by tokenizing a Vietnamese query text prior submitting it to a search engine . Evaluations demonstrate its effectiveness and practical value. H.3.3 [Information Search and Retrie val]: Query formulation. I.2.7. [Artificial Intelligence]: Natural Language Processing. Algorithms, Experimentation Vietnamese word tokenization, query preprocessing, and search As the web content becomes more accessible to Vietnamese across the globe, there is a need to process Vietnamese query texts properly for finding relevant in formation. The Vietnamese language, even though there is a space to separate between sound units, there is nothing to be used to identify word boundary [1]. Commercial search engines do not pa y attention to how Vietnamese words are constructed. Rather these search engines use the white spaces between syllables (morphemes) for words identification. Because of misrecognizing words, s earch engines often return too many unwanted documents that cau sed disappointments to users. Cohen [2] suggested that incorrectly recognized words through N-Gram analysis will result in dissa tisfaction to locale search users who have high expectation on how their languages should be processed. There have been significant researches on the Vietnamese word tokenization problem. The most recent work [3] uses conditional random fields (CRFs) and suppor t vector machines (SVMs) statistical learning method to solve the segmentation task. It built annotated corpus (about 8000 sentences with word boundary marked) for evaluation and tr aining different CRF and SVM features according to different feature configurations; and then compare and contrast their results. Wang et al [4] proposed a novel Chinese word segmentation method with leverage web documents and search technology. It collected the highlights from returned snippets as the segments when they are available or by using characters position indicated by inverted indices. Our approach differs from this in that it submits the original query to a search engine API to obtain a search re sult consists of document titles and their summary text. From the text, we then extract syllables to build a potential words list. Our approach does not require any training corpora. The approach consists of the main steps: (1) Data gathering via a WWW search engine API call and words extraction, (2) Sentences construction, (3) Sentences refi nement and reduction, and (4) Scoring and query reformulation. In the first step, the original s earch query text is submitted to a search engine API and request for N returned documents. The engine returns a search result list. We parse the data and extract required text. Syllables in the search query are then matched against text in returned documents to extract potential words covering both monosyllabic and pol ysyllabic words. Proper names are recognized through capitalization rule. New words are recognized when there is a strong collocation (cohesiveness) between its syllables. That is one the following condition is met: (1) For two syllables word: P(s 1 s 2 ) &gt; P(s 1 ) P(s syllables word: P(s 1 s 2 s 3 ) &gt; MAX{ P(s P(s 1 )P(s 2 s 3 ), P(s 1 s 2 ) P(s 3 ) } where w = s s s 2 s 3 , P(s 1 ...s n ) = Freq(s 1 ... s n )/N, Freq(s number of times syllables s 1 ...s n occurs in N retrieved documents. To improve on the quality of the suggestion, we filter out any suggestion which has a syllable be longs to a stop word list. For handling morphological derived wo rds (MDW), a new polysyllabic word must have a syllable, appear s at the beginning or the end of MDW feature is referred as a lexical element of an Unique Membership Class in [5]. In addition, for accuracy, we impose that a MDW must have a strong cohesi veness between its syllables. The sentences constructor is to assemble the identified words in such a way that they appear in the same order as the original query. We use Greedy algorithm to construct sentences using the following heuristic strategies (1) preferen ce of polysyllabic over monosyllabic words whenever possible (2) newly added word must be appeared substring of the original query (3 ) eliminating segments which have already examined. Since there is only a single solution that can be used to perform search, we implemented an al gorithm to improve upon existing sentences and reduce them to a manageable size. Iteratively, the algorithm formulates an optimized sentence by extracting common words from two most similar sentences. The conflicted words will be resolved through either word co llocation strength test (section 3.1 -for a conjunctive ambiguity) or by using probability distribution of word segments using language m odel (section 3.4 -for disjunctive ambiguity). Figure 2 illustrates a process where constructing sentences are refined and improved through elimination of disambiguating words. The final segmented sentence is translated in English as  X  X he speed of info rmation transmission will increase X . Figure 2. An Example of Sentences Refinement The task in this phase is to score and order the candidates. To handle the data sparse issue, we use language model with additive smoothing [6] to estimate which of the sentences is best one. The best sentence, with segmented words in double quoted, will submit to a search engine. We carried out two experiments to investigate on the pre-processing a Vietnamese query text. We would like to know the accuracy of our Vietnamese tokenization system when comparing to the latest approach [3]. We selected a random of 100 queries in clude news titles and short texts from various sites includes: VOA (Voice of America), BBC (British Broadcasting Company), and VNExpress. Out of 100 queries, both methods produced similar results for 71 queries. We found that 21 queries are correctly segmented by our approach and 4 queries were in the favor of the SVM method. Both approaches segmented 4 queries incorrectly. The 21 queries which were judged to be better than the SVM method due to: 1) basic segmentation function: Our approach is better in recognizi ng morphological derived words and Out of Vocabulary (OOV) words; 2) disambiguation: SVM method had severe limitations in handli ng both conjunctive and disjunctive ambiguities; 3) Name entity recognition: Our approach recognized person, location, and organization names that was not recognized by the SVM method. Table 1 gives an example of the comparison results between our approach and the SVM method. Table 1. Sample Output WebBSA (Our approach) vs. SVM In the second experiment, we evaluate the precision, relative recall, and F-measure by comparing query preprocessing approach with non-preprocessing query via Yahoo search engine [7]. Relevance is scored based on the number of matched words found from a document snippet. Figure 3 shows a result of performing search on 1000 Vietnamese queries (query texts are extracted from the Google X  X  Adwords). Figure 3. Performance result for query preprocessing vs. query non-preprocessing. The result showed that the precision, relative recall, and F-measure have an improvement of 13.29% , 12.41%, and 12.97% respectively when query preprocessing is applied to Yahoo search. There have been significant researches on the Vietnamese word tokenization problem. Most of the approaches are looking at a perspective of natural language pro cessing. In this paper, we focus on the problem further and apply it to query preprocessing to improve search relevancy. The e xperiments show the approach is usefulness and possibly offering a practical solution. [1] Le An Ha. 2003. A method for word segmentation in [2] Steve Cohen. 2006. Morphological Analysis searches for you  X  [3] Cam-Tu Nguyen et al, 2007. Vietnamese Word Segmentation [4] Xin-Jin Wang, Wen Liu, Y ong Qin, 2007. A Search-based [5] Thanh Bon Nguyen et al, 2006. A lexicon for Vietnamese [6] Stanley F. Chen, J. Goodman, 1998. An empirical study of [7] Yahoo Search API. http ://developer.yahoo.com/search/ 
