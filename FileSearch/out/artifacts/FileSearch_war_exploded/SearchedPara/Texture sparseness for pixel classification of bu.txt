 ORIGINAL PAPER Melissa Cote  X  Alexandra Branzan Albu Abstract Contemporary business documents contain diverse, multi-layered mixtures of textual, graphical, and pictorial elements. Existing methods for document segmen-tation and classification do not handle well the complex-ity and variety of contents, geometric layout, and elemental shapes. This paper proposes a novel document image classi-fication approach that distributes individual pixels into four fundamental classes (text, image, graphics, and background) through support vector machines. This approach uses a novel low-dimensional feature descriptor based on textural prop-erties. The proposed feature vector is constructed by con-sidering the sparseness of the document image responses to a filter bank on a multi-resolution and contextual basis. Qualitative and quantitative evaluations on business docu-ment images show the benefits of adopting a contextual and multi-resolution approach. The proposed approach achieves excellent results; it is able to handle varied contents and com-plex document layouts, without imposing any constraint or making assumptions about the shape and spatial arrangement of document elements.
 Keywords Business documents  X  Document image segmentation  X  Pixel classification  X  Sparseness  X  Support vector machines  X  Texture 1 Introduction Business intelligence refers to a set of methodologies, tech-nologies, practices, and skills that are used to derive mean-ingful information from business data and support fact-based decision making. Business documents are designed to facil-itate rapid interconnections between data presented in tex-tual and graphical formats. Therefore, business documents typically exhibit a challenging multi-layered mixture of tex-tual, graphical, and pictorial elements. This makes them a highly interesting data source for developing and testing new document image analysis (DIA) techniques. In the busi-ness community, automatic means for analyzing and inter-preting business documents are highly desired tools. These tools will greatly facilitate the manipulation, query, search and retrieval, and sharing of electronic business document resources.

This paper deals with content classification of business document images. A segmentation process aims to parti-tion the document image into homogeneous regions, whereas a classification process goes a step further by labeling the regions. One of the primary difficulties encountered in region-based segmentation lies in the formulation of a valid homogeneity criterion, which is addressed here with texture. According to Haralick et al. [ 1 ], texture is one of the impor-tant characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicro-graph, an aerial photograph, or a satellite image. We can certainly add  X  X ocument image X  to this list without loss of veracity. Another important issue pertains to the applicabil-ity of segmentation and classification methods to a variety of document images in terms of contents, layout, and shape, which is of great importance in business documents. This is addressed through a pixel-based approach that avoids the constraints of region-based algorithms.
Our contributions are as follows. From a theoretical view-point, we propose a novel texture descriptor that exploits the concept of sparseness. The key idea is to analyze the sparse-ness of responses to a well-known bank of filters on a pixel-per-pixel basis. Several versions of this texture descriptor are presented. The original, one-dimensional version describes every pixel by its scalar sparseness value. The other versions integrate multi-resolution and/or contextual information into the pixel description. The proposed texture description can be in principle applied to any segmentation problem where image regions exhibit distinctive texture patterns. From a practical viewpoint, we build a document image classifica-tion system using the proposed texture descriptor. Extensive experimental results show that the proposed approach is suc-cessful in identifying four main classes of pixels, namely text, image, graphics, and background, in business document images. An additional contribution to the field of document image analysis is the proposal of a novel dataset (the Busi-ness Document Image Dataset) composed of pages extracted from public business documents. A sample of the proposed dataset is available online. 1 The full dataset is available on demand.

The paper is structured in the following way. Section 2 reviewsrelatedworkontexture-basedmethodsinDIA,witha particular focus on document image segmentation and classi-fication. Section 3 introduces our Business Document Image Dataset, which provides the training and testing samples for the proposed approach. Section 4 describes our proposed approach. Section 5 discusses both quantitative and quali-tative experimental results for the proposed approach com-pared with a baseline method for texture description and with another recent pixel-based and texture-based method, along with resource consumption considerations. Section 6 presents concluding remarks and future work. 2 Related work In DIA, methods for texture analysis can be broadly classified as either statistical or spectral. Statistical methods attempt to describe the spatial distribution of gray-level values within a region of interest. Examples of simple statistical texture descriptors include the gray-level co-occurrence matrices (GLCM) [ 1 ] and the gray-level run length matrices [ 2 ]. The former estimate second-order statistics properties, from which standard features (homogeneity, contrast, entropy, cor-relation, energy, uniformity, etc.) are derived. The latter relate to the number of same-value consecutive collinear pixels. Spectral methods rely on psychophysical evidence showing that the human brain analyzes texture based on spatial fre-quency information. They typically describe texture by com-puting the response of an image region to a predefined filter bank [ 3 ]. Gabor filter banks [ 4 ], composed of frequency and orientation selective filters, constitute one of the most widely used filter banks.

Texture has been analyzed for many purposes in DIA: image binarization [ 5 ], font recognition [ 6  X  9 ], script identi-fication [ 8 , 10  X  13 ], geometric rectification of warped images [ 14 , 15 ], image retrieval [ 16 , 17 ], and most relevant for our study, image segmentation and classification. This section will focus on texture analysis with statistical and spectral methods for document image segmentation and classifica-tion. 2.1 Statistical texture analysis One may identify two broad categories of statistical methods for DIA. Some approaches aim for the textural description of pre-segmented regions of a document image. Others do not need prior segmentation.

In the first category, Wang and Srihari [ 18 ] proposed the use of two matrices, the Black X  X hite Pair Run Lengths and the Black White X  X lack Combination Run Lengths matri-ces. They derive three features emphasizing different run lengths (number of consecutive collinear pixels sharing a specific characteristic). The matrices are used for classifying newspaper blocks into halftones, text with large/small letters, and line drawings. Their method assumes binarized images and works on pre-segmented homogeneous blocks. Chet-verikov et al. [ 19 ] introduced the feature-based interaction map (FBIM). It displays the structure of statistical pairwise pixel interactions evaluated through the spatial dependence of a gray-level difference histogram (GLDH) feature. The map is used for classifying already partitioned homogeneous blocks into text or non-text. Their method is limited block-size-wise: It cannot, for instance, correctly classify single lines of text, as the characteristic structure of the interac-tion map cannot be recovered. Eglin and Gagneux [ 20 ]used complexity (entropy), directional compactness (horizontal and vertical), and visibility (density), as features for clas-sifying pre-localized text blocks into headings, paragraphs, and notes on in-house images of scientific documents. One limitation of their method is its specificity to a particular class of documents. In a similar work, Allier et al. [ 21 ] presented a method for the labeling of pre-segmented text lines based on histogram entropy, compactness index in a given direction, eccentricity, and the number of connected components. They tested their method with three different datasets to show its versatility, but did not provide label-wise detailed results, nor examples of results.

In the second category (i.e., methods not requiring prior region segmentation), Payne et al. [ 22 ] described the Texture Co-occurrence Spectrum technique. It computes the occur-rence of neighboring pixel values to classify regions into text, image, etc. Binarized document pages are assumed as input, and results have a  X  X locky X  appearance due to the use of rectangular non-overlapping processing windows. Chen [ 23 ] used the sequential directional energy of pixels as texture features for the separation of text from textured back-ground. Starting with coarse block classification into text, background, or boundary regions, the algorithm then refines the segmentation at the pixel level. The method is unable to handle other types of document elements. Working with historic documents, Journet et al. [ 17 ] proposed five textural features derived from frequencies of some specific events (ink X  X aper transition, white space presence) and orientations (autocorrelation function) at multiple scales. They classify document image pixels into text, drawing, and background. Their method works well but has difficulties with some doc-ument elements such as titles, mostly misclassified as draw-ings. Baird et al. [ 24 ] also proposed a pixel classification-based approach using texture features computed from the luminosity channel, including the line luminosity average and average difference in different directions. Tested on in-house datasets containing various types of documents, their method extracts machine-printed text, handwritten text, pho-tographs, and blank space. The authors reported interest-ing visual results but a low per-pixel classification accuracy (62.4%), mainly due to misclassification of handwriting as blank space. Kim and Kim [ 25 ] worked on the segmentation and classification of in-house grayscale Korean and English images into text, tables, pictures, and graphics. They used six standard features extracted from the GLCM, computed from the entropy of the document images. The initial segmentation uses morphological cleaning as a preprocessing step, which may wrongfully merge distinct regions. 2.2 Spectral texture analysis Spectral approaches to texture analysis can be applied directly on input images, eliminating the need for binariza-tion. Most spectral methods do not rely on prior segmenta-tion. Filter banks (mostly Gabor) and wavelet analysis con-stitute the two main tools encountered in literature.
AccordingtoJainetal.[ 26 ],Gaborfiltersareideallysuited fortexturediscriminationproblemsbecause(1)mosttextures can be characterized based on the local spatial frequency and orientation information and (2) these filters exhibit optimal localization properties in the spatial and frequency domains. Therefore, they used a bank of 20 Gabor filters (four orienta-tions and five spatial frequencies) for the text X  X raphics sep-aration problem on scanned newspaper images. They only presented qualitative results for an image subsection, but showed the applicability of their method for the localization of bar codes and of address blocks on envelopes. Arguing that the Gabor filtering technique is not tuned for a given tex-ture segmentation or classification task, Jain and Zhong [ 27 ] proposed the use of optimal texture discrimination masks derived from Gabor filters. They segmented English and Chi-nese document images into text, halftones, and line drawings. Their method necessitates several post-processing  X  X lean-ing X  steps, and only visual results were presented. Vieux and Domenger [ 28 ] reported a pixel classification approach to separate text from other classes based on a Gabor filter bank at five scales and six orientations. Evaluating the per-formance of their algorithm on the PRImA Layout Analy-sis Dataset [ 29 ], they displayed results for text, image, and backgroundonly,mentioningthatotherclasses(linedrawing, graphics, tables, charts, separator, etc.) are rare. Zhong and Cheriet [ 30 ] proposed to use dimensionally reduced multi-channel Gabor filters for text block identification on image patches from ancient documents. They did not provide any visual results for the performance analysis of their method. Etemad et al. [ 31 ] favored the use of wavelet packets over Gabor filters for several reasons. One reason is that wavelet packet components are critically decimated without informa-tion loss, yielding to reductions in data size and segmenta-tion computational complexity. Also, since the tree structure of wavelet packets can be adaptively designed, the feature extraction process can efficiently provide class separation with a small number of features. Their method utilizes multi-scale domain-dependent wavelet packet representation with local moments. They identify text, image, and non-text non-image on in-house document images and provide no quanti-tative evaluation. Li and Gray [ 32 ] proposed two textural fea-tures extracted from the distribution patterns of wavelet coef-ficients. Their method starts by classifying large blocks with extreme features into background, photograph, text, or graph and divides the unclassified into smaller blocks for further processing. They reported very good accuracy but limited their evaluation to nine in-house images. Lee and Ryu [ 33 ] also utilized texture features derived from wavelets, but only to confirm ambiguous regions of text, images, tables, and ruling lines. A pyramidal quad-tree limits their segmentation output to rectangular regions, leading to a  X  X locky X  appear-ance. Focusing on text versus non-text separation, Acharyya and Kundu [ 34 ] used a scale-space signature derived from the local energy around each pixel at different scales. It is com-puted using M-band wavelets and aims to transform texture boundaries into detectable discontinuities. They provided qualitative results for in-house Indian newspaper images, but did not give any quantitative assessment. 2.3 Discussion To summarize, both statistical and spectral methods for tex-ture analysis have been and are still equally used by the DIA community. Several limitations have been exposed through the literature review: 1. Many of the (earlier) reported statistical methods relied 2. Several methods, both statistical and spectral, assume 3. Several methods focus on very specific segmentation 4. Many works only reported qualitative results, which may
This paper addresses all four limitations. We propose a novel texture-based feature descriptor that exploits the sparseness of the responses of the input document image to a filter bank. It is a multi-resolution and contextual pixel-level method. Individual pixels are classified into four fundamen-tal classes (text, image, graphics, and background) that make up all types of document elements. Our method, designed for business documents, is able to handle complex document layouts and document elements of variable shape. It is eval-uated both qualitatively and quantitatively on color business document images. 3 Business document image dataset In this paper, we focus on document images that originate from business documents. To the best of our knowledge, there is currently no publicly available dataset of business document images, which is why we evaluate our proposed approach on our own dataset. As part of a project on smart digital libraries for business documents, our team has been collecting public business documents from the web for the creation of the Business Document Image Dataset (BDID). This section describes the features of BDID, as well as its corresponding ground truth. 3.1 Characteristics The current BDID dataset is comprised of 1,232 one-page document images available at several resolutions. Each doc-ument image contains text and at least a chart, an image (picture), or a table. There is a total of 488 charts (281 bar, 60 line, 89 pie/doughnut, 41 mixed, 17 other), 59 images, and 1,682 tables. The dataset offers a lot of variability in terms of layouts and formats, and includes color, grayscale, and black and white document images (all available in 24 bits), one-and two-column pages, and various background colors.
One characteristic that distinguishes the BDID from other existing DIA datasets (such as the PRImA Layout Analysis Dataset [ 29 ]) lies in the source of the documents. The docu-ment images are extracted from publicly available business documents on the web. Examples include annual reports of publicly traded companies, and evaluation reports, reviews, or plans from government agencies and universities. The doc-ument images originate from digitally born documents, as thisisastandardinthebusinessintelligenceworld.Thisagain differs from most other DIA datasets (such as the MediaTeam Oulu Document Database [ 35 ], the Medical Article Records Groundtruth dataset [ 36 ], the UvA-CDD [ 37 ], or the PRImA Layout Analysis Dataset [ 29 ]). As a result, there is no need to preprocess the images for typical artefacts of digitization such as skewing. 3.2 Ground truth The document image ground truth has been defined with Aletheia 1.5 [ 38 ]. The software allows to specify regions on a document image and assign them to one of the many predefined types of objects, set their properties, and save the data in the PAGE [ 39 ] format framework through an XML schema.

The ground truth includes four types of document objects: text, graphics/charts, images, and tables. Embedded text, such as text inside tables, on images, or in charts, is defined on a separate layer to make the ground truth compatible with pixel-based classification tasks. Figure 1 shows samples of typical business document images from BDID, with overlaid ground truth displayed as colored boxes. A BDID sample of 60 document images and their corresponding ground truth was made publicly available online. 2 The entire dataset can be released upon request. 4 Proposed method 4.1 System overview As shown in Fig. 2 , the proposed approach consists of two main stages: (1) feature vector computation and (2) pixel classification. In a normal (testing) context, a business docu-ment image is fed as input. First, a feature vector is computed on a pixel-per-pixel basis as described in Sect. 4.2 . Accord-ing to the contents of its feature vector, each pixel is then automatically assigned to one of four possible classes, pro-ducing as output a  X  X ixel-labeled X  image. O X  X orman and Kasturi [ 40 ] consider three major types of document compo-nents: text, graphics, and pictures (images). We extend this classification to add a fourth type, background, to obtain a complete partition of the document image. All types of high-level document elements are composed of these four funda-mental types. For instance, tables contain text, background, and possibly graphics pixels. A training phase, utilizing a set of training pixels with corresponding known labels, is car-ried out once to obtain a pixel classification model through supervised machine learning. The remainder of this section presents each stage in details. 4.2 Feature vector computation Let us consider a document image. The objective of the first stage is to represent a given pixel by a feature vector that will ultimately enable its classification into one of four pre-defined classes (text, graphics, image, and background). As a novel approach to represent individual pixels for a classifi-cation task, we propose a contextual, multi-resolution, low-dimensional feature vector. It is based upon the sparseness of the image responses to the Leung X  X alik filter bank [ 41 ]for texture analysis. We compare our proposed representation to a related baseline approach consisting of the direct use of the Leung X  X alik filter bank responses as texture descriptors. The computation of the filter bank responses is detailed in Sect. 4.2.1 . The concept of sparseness and its proposed appli-cation to texture analysis are described in Sect. 4.2.2 .The proposed feature vector with contextual and multi-resolution components is presented in Sect. 4.2.3 . 4.2.1 Leung X  X alik filter bank The Leung X  X alik filter bank [ 41 ] consists of 48 filters and has multi-orientation and multi-scale capabilities. More specifically, it includes Gaussian derivative filters (first and second order) at three scales and six orientations, Gaussian filters at four scales, and Laplacian of Gaussian filters at eight scales. The largest filter operates on a 49  X  49 window. This filter bank was initially designed to characterize the natural textures of materials such as concrete, fur, and pebbles [ 41 ]. It has been successfully utilized by other researchers in various contexts such as image feature similarity measurements [ 42 ], indoor/outdoor scene recognition [ 43 ], and remote sensing of oil slicks [ 44 ]. The document image is convolved with the fil-ter bank, generating 48 responses per pixel. These responses are aggregated into a 48-dimensional array on a pixel-by-pixel basis.

Figure 3 shows examples of typical filter bank responses associated with pixels belonging to the four basic types of document image components (text, graphics, image, and background), and illustrates the textural distinctiveness of each type. 4.2.2 Sparseness The representation of texture through pixel-based responses to a filter bank is highly versatile, but can present redun-dancies [ 41 ]. Moreover, its high dimensionality, in addi-tion to processing and computational costs considerations, brings increased difficulty to the classification and cluster-ing tasks [ 45 ]. Here, we propose to drastically reduce the dimensionality of the filter bank responses from 48 to one by recording only the sparseness of the responses. This reduc-tion in dimensionality is further motivated by the small num-ber of classes of textures (four) that are involved, which do not require as much descriptive power as more generic image segmentation applications.

The concept of sparseness (also called sparsity in other works, e.g., [ 46 ]) generally refers to the property of being scattered, thinly distributed. When applied to data process-ing, it refers to the concentration of information into a small number of coefficients (in our case, a small number of nonzero responses to the filters in the bank). In classical sig-nal processing applications, sparse representation has proven to be an extremely powerful tool for acquiring, representing, and compressing high-dimensional signals. It has played an important role in the success of many machine learning algo-rithms and techniques such as matrix factorization, denois-ing, dictionary learning, and signal recovery/extraction [ 46 ]. It has also been popular in computer vision applications, as sparse representations can facilitate the uncovering of seman-tic information from images [ 47 ].

This paper exploits the concept of sparseness in a novel way compared to previous computer vision applications, which mostly use it for constructing a dictionary to repre-sent data [ 48  X  51 ]. We use the sparseness of the texture filter bank responses to describe the texture of each pixel, in a similar way to Alpert et al. [ 52 ]. In their paper, the sparse-ness of the gradient magnitude histogram of a region is an indicator for the presence/absence of texture. However, we propose to go further: Instead of considering sparseness as a cue for detecting texture, we use it to characterize texture and discriminate between texture classes.

Many sparseness measures have been proposed in litera-ture. Hurley and Rickard [ 46 ] compared 16 commonly used sparseness measures according to six attributes that they deem as desirable. Most attributes were originally applied in a financial setting to measure wealth distribution inequity. The six attributes are as follows: 1. Robin Hood: stealing from the rich and giving to the poor 2. Scaling: multiplying wealth by a constant does not alter 3. Rising Tide: giving everyone a trillion dollars renders the 4. Cloning: if there is a twin population with identical 5. Bill Gates: as one individual becomes infinitely weal-6. Babies: in populations with nonzero total wealth, adding
We selected Hoyer X  X  sparseness measure [ 53 ]asitsat-isfies five out of six attributes. This measure only fails the  X  X loning X  attribute, which is irrelevant for our application. Hoyer X  X  original sparseness definition [ 53 ] is based on the ratio between the L 1 and the L 2 norms: S where x denotes the vector containing the filter bank responses, and n is the dimensionality of x (here n = 48). This sparseness measure maps a vector from R n to R and quanti-fies how much energy is packed into only a few components. A maximum sparseness of one is measured for vectors with only a single nonzero element, whereas vectors with all ele-ments equal (but nonzero) have a sparseness of zero. One should note that the sparseness of a null vector cannot be measured with the Hoyer formula ( 1 ).

We slightly modified the Hoyer formula to accommo-date the case of background pixels for which all filter bank responses are zero (absence of texture). By extension, if all components have an equal value of zero, the sparseness S should also be minimal: S ( x ) =
Figure 4 shows sparseness maps computed at several res-olutions of the input image. Examples of sparseness values for pixels from each of the four classes (text, graphics, image, and background) are shown in Fig. 3 . 4.2.3 Contextual multi-resolution texture description The idea of describing the texture at each pixel with a scalar value (i.e., the sparseness of all pixel responses to a fil-ter bank) is appealing. However, its ability to discriminate between the four considered classes can be further improved at little computational cost. For instance, Fig. 4 b shows that pixels located in some parts of a text region and in graphical elements (line) share similar sparseness values; this is illus-trated by similar shades on the grayscale images. The expla-nation is quite simple: On a small scale, letters do share some textural features with lines because of their stroke arrange-ment. We therefore propose to moderately increase the fea-ture vector dimensionality by adding information at two lev-els: (1) the pixel neighborhood level and (2) the document image resolution level. The rationale for taking the neighbor-hood into account is to place the pixels in context. In cases where two pixels exhibit similar sparseness but are part of separate classes (e.g., text and graphics), the sparseness of their neighborhood will most likely be distinct. Moreover, context may help improve classification accuracy (see for instance [ 32 ] in which wavelet coefficient distributions are used contextually to segment background, photograph, text, and graph elements, or [ 54 ] where shapes and sizes of con-nected components are used contextually to segment text and non-text elements). The rationale behind using multi-ple resolutions is to be able to handle, up to a certain point, varying sizes of detail from one document image to another or within the same image (e.g., text of different font sizes). Multi-resolution may also help improve classification accu-racy (see [ 17 ] for instance).

Through the proposed contextual multi-resolution texture description, each pixel is represented by a 10-dimensional feature vector: The first five components are the pixel sparse-ness values at various resolutions (100, 150, 200, 250, 300 DPI), and the last five are the mean sparseness values over the pixel X  X  neighborhood, also at various resolutions. The neighborhood size is chosen to be proportional to the image resolution, to ensure consistency of the visual information content in the neighborhood across resolutions. It is set to the same size as the maximal filter bank window (49  X  49) at maximal resolution (300 DPI). Pixel coordinates at lower resolutions are derived from the coordinates at the highest resolution according to the proportional size of the images. For instance, the feature vector of pixel (100, 200) would therefore include the sparseness values found at coordinates (33, 67), (50, 100), (67, 133), (83, 167), and (100, 200) on the 100, 150, 200, 250, and 300 DPI images, respectively. Figure 5 illustrates the feature vector computation process. 4.3 Pixel classification The objective of the second stage is to obtain a classifica-tion of document image elements at the pixel level. Through supervised machine learning, pixels are classified into one of the four basic types of textures introduced in Sect. 4.1 (text, graphics, image, or background). 4.3.1 Supervised machine learning Support vector machines (SVMs) [ 55 , 56 ] constitute a pop-ular supervised machine learning approach for classification tasks in computer vision. The concept of a binary SVM clas-sifier is to represent data as points in space, mapped in a way that points from two different classes are divided by a hyper-plane that yields a maximal margin between the classes. The model is built from training samples that include data (feature vectors) and their corresponding class label. New examples are mapped into that same space and predicted to belong to one of the two classes, based on which side of the hyperplane they fall. One particularly interesting advantage of SVMs is that they focus on the training samples that lie at the edge of the class distributions (the support vectors, SVs). They are thus expected to generalize more accurately on previously unseen data, in comparison with classifiers that aim to mini-mize the training error such as neural networks [ 57 ].
One standard way of extending binary SVMs to the mul-tiple class case implies creating a set of binary classifiers followingaone-against-oneapproach.Considering k classes, k ( k  X  1 )/ 2 classifiers are trained (one for each pair of classes), separating data of one class from data of the other class. The winning class is the one receiving the most votes.
In this paper, we train a set of 6 ( k = 4) one-against-one binary SVM classifiers to distinguish between text, graphics, image, and background pixels. We use LIBSVM [ 58 ]forthe SVMs implementation. Following recommendations in [ 59 ], we employ a Gaussian radial basis function (RBF) kernel. Two parameters need to be set: C , which controls the trade-off between the size of the margin between classes and the acceptablemisclassificationpenalty,and  X  ,thekernelscaling factor. We do not know beforehand which C and  X  are best suited for the given pixel classification problem. Therefore, a parameter search is conducted to identify a pair of C and  X  that will maximize the classifiers prediction accuracy. We adopted the strategy proposed in [ 59 ], which is a grid search on C and  X  using cross-validation. Various pairs of ( C , values from a grid constructed with exponentially growing sequencesof C and  X  aretried,usinga5-foldcross-validation scheme on the training data. The training data are divided into five subsets of equal size. Each subset is tested using the classifier trained on the remaining four subsets, thus testing all training data once. The cross-validation accuracy is given by the percentage of training data that are correctly classified. Thepairof( C ,  X  ) values yielding the highest accuracy is then selected to train the set of k ( k  X  1 )/ 2 one-against-one SVM classifiers on the entire training data.

The training set is composed of the feature vectors of 12,000 pixels randomly sampled from 30 document images of the BDID (Sect. 3 ), along with their known label (text, graphics, image, or background). Each class is represented by an equal number of sample pixels (3,000). Figure 6 shows the pixel classification stage, including the training phase and a test with an unknown (i.e., not used for training) document image.

The proposed method goes one step further than pure seg-mentation approaches by generating labeled results. Obtain-ing a region segmentation from this output, which can be used forconsequentDIAprocessing,isamatterofpost-processing using standard techniques. Masks of each segment can be extracted from connected pixels of the same class. Morpho-logical cleaning can be applied to remove noise from the pixel classification results. This is, however, out of the scope of this paper. 5 Experimental results This section presents the pixel classification results for the proposed texture representation (contextual multi-resolution sparseness X  X MRS) on business document images. We compare its performance to four alternative representations: the direct filter bank responses, their sparseness, their sparse-ness at multiple resolutions, and their contextual sparseness. For quantitative and qualitative evaluation purposes, a test-ing set of 12,000 pixels is randomly sampled similarly to the training set (Sect. 4.3.1 ), from BDID document images not Feature vector Abbr. Filter bank responses (baseline) Single resolution sparseness Multi-resolution sparseness Contextual single resolution sparseness Contextual multi-resolution sparseness (proposed method)
For a 8.5  X  11 X  document image used for the training set. We also compare our results with those from another pixel-based and texture-based method ([ 17 ]) on the same test data. A description of the compared featurevectors,quantitativeandqualitativeevaluations,com-parisons with the other method, along with resource con-sumption considerations, are provided next. 5.1 Compared feature vectors In addition to the proposed contextual multi-resolution tex-ture sparseness feature vector (CMRS), we also trained SVM classifiers with four other feature vectors, using the same training pixels. We evaluated them on the same testing pix-els for comparison purposes. The goal of such comparisons is twofold: (1) analyze the benefits of a contextual and multi-resolution approach; (2) study the trade-off between pixel classification accuracy and dimensionality of the fea-ture space. Table 1 presents a description of these compared feature vectors, along with the proposed CMRS. The Leung X  Malik filter bank responses have been selected as the baseline feature vector (FB) because of the bank X  X  popularity for char-acterizing texture and its successful usage in various contexts (Sect. 4.2.1 ). As its values are not restricted to the [0, 1] range (which is the case for all sparseness-based feature vectors), each feature of FB was scaled. 5.2 Quantitative evaluation The last column of Table 1 shows the overall classification performance of the SVM classifiers trained with the four compared feature vectors and with the proposed CMRS. Fig-ure 7 presents the confusion matrices, in which a perfect clas-sification algorithm would have ones as diagonal elements and zeros elsewhere. Table 2 shows the class-specific preci-sion (fraction of pixels predicted to belong to class A that are actual class A pixels), recall (fraction of pixels belonging to class A that are predicted as such), and F-measure (harmonic mean of precision and recall) of the compared and proposed feature vectors.

From Table 1 , the proposed feature vector yields the high-est overall pixel classification accuracy at 83.36%, a sub-stantial improvement over the baseline. The 16.64% error can be mainly traced back to semantic overlaps between classes: For instance, some graphical elements may con-tain uniform regions similar to the background from a tex-tural viewpoint; some image elements may contain lines and sharp transitions similar to graphics; and some larger fonts, in spite of the multi-resolution approach, may contain lines and curves similar to graphics. Misclassification specifics can be found through confusion matrices (see next paragraph and Fig. 7 ). As expected, going from the full 48-dimensional filter bank responses (FB) to the 1-dimensional sparseness Feature vector Precision Recall F-measure T text, G graphics, I image, and B background (SRS) decreases the classification accuracy, but only by 11%. Interestingly, adding multiple resolutions through only 5-dimensional sparseness (MRS) brings the accuracy back up to a comparable level to the baseline, whereas adding con-textual information through only 2-dimensional sparseness (CSRS) surpasses the baseline accuracy slightly. There is no loss in classification accuracy with the reduction in dimen-sionality of the feature space for the proposed method: A lower dimensionality of 10, compared to 48 for the base-line, achieves better overall accuracy. Incorporating contex-tual information, as well as multiple resolutions, is benefi-cial: Moving from single to multiple resolution increases the accuracy by 9 and 16% for the non-contextual and contex-tual cases, respectively; going from non-contextual to con-textual generates 12 and 19% increases for the cases of single and multi-resolution, respectively. Another advantage of the proposed CMRS over the compared feature vectors lies in the smaller number of support vectors required by the SVM model (  X  5,000 vs. 8,000 X 9,000, see penultimate column of Table 1 ).

The confusion matrices (Fig. 7 ) provide details on class-specific classification performance. From the matrix of the proposed feature vector CMRS, we can see that it performs best for text (92.3%), followed by background (84.4%), image (84.1%), and graphics pixels (72.6%). The lowest misclassification rate for CMRS is for image pixels con-fusedwithbackground(0.1%),closelyfollowedbytheoppo-site case of background misclassified as image (0.3%). The largest misclassification rate is for graphics confused with background (13.3%), closely followed by the opposite case (12.4%). The largest misclassification rate can be explained by the semantic overlap between the two classes: graph-ical elements contain uniform regions that are similar to background from a textural viewpoint. The middle part of a bar in a bar chart constitutes one such example. The sec-ond largest misclassification rate can be traced back to an imprecision in the location of graphics object boundaries (more details in Sect. 5.3 ). From the matrices of all compared and proposed feature vectors, text pixels are best classified using the proposed CMRS. Graphics pixels are best classi-fied via CSRS, followed closely by the proposed CMRS. The proposed CMRS substantially outperforms all other feature vectors for the image class. Background pixels are slightly more accurately classified with the baseline FB, the proposed CMRS a close second. Overall, the proposed 10-dimensional feature vector CMRS outperforms the 48-dimensional base-line feature vector FB in terms of classification accuracy for all classes except for background, due, as explained earlier, to an imprecision in the location of graphics object boundaries.
From Table 2 , the proposed CMRS yields the highest pre-cision for text, graphics, and image pixels, and the highest recallfortextandimagepixels.ThelowerprecisionofCMRS for background pixels can be seen on the last column of the confusion matrix (Fig. 7 ), where more graphics pixels are confused with background (semantic overlap). The recall of CMRS for graphics pixels is only slightly lower than that of CSRS, and slightly lower than that of FB for background pix-els,whicharealsorelatedtoconfusionsbetweengraphicsand background pixels. The F-measure of the proposed CMRS, which captures the overall performance in terms of both pre-cision and recall with a single number, not only is the highest among all compared feature vectors for all pixel classes, but also shows significant improvements in all classes over the baseline. This improvement is particularly noteworthy for image pixels (39%), which is clearly reflected in the visual examples presented next (Sect. 5.3 ;Fig. 8 ), and is attributable to the contextual and multi-resolution aspects of the proposed CMRS. 5.3 Qualitative evaluation For qualitative analysis purposes, Fig. 8 shows a typical doc-ument image from the BDID (Sect. 3 ), along with the result-ingpixel classificationfor thecomparedandproposedfeature vectors. Table 3 summarizes performance issues. Following a simple system associating penalty points with every identi-fied issue, a penalty score is assigned to each feature vector. We consider all issues as independent and as having simi-Issue # Description FB 1 Text exhibits widespread noise (inside text)  X  ** ** *  X  3 Text near line classified as image pixels  X   X   X  * ** 4 Text with low contrast missed **  X   X   X   X  6 False graphics pixels appear between close objects  X   X   X   X  * 7 False graphics boundaries appear around text * ** * *  X  8 False graphics boundaries appear around images * ** * * * 9 Images exhibit a lot of noise ** ** * **  X  10 Imprecise localization of graphics boundaries  X  * * * * Penalty Score (/20) 8 13 9 9 5 lar importance/weight, allowing the penalty score to be the summation of individual points. Figure 9 shows additional results for the proposed feature vector.

Based on Table 3 , the proposed CMRS yields the best (lowest) penalty score, followed by the baseline FB. Issue #3 is the only important one for CMRS: Text pixels located near lines are systematically classified as image pixels. This close proximity changes the textural aspect locally: Having a line near text (close enough to be included in the filter bank window for text pixels) produces a mix of the typical responses of text and graphics. The text pixel responses are increased for some of the filters, which translates into an increased sparseness value. The resulting sparseness value is moreinlinewiththat of typical imagepixels, whichgenerally falls between text sparseness (low) and graphics sparseness (high).Weintendtoaddressthisissueinfutureworksthrough multi-stage classifiers. Issue #8 appears for all feature vec-tors. It can be explained by the fact that the sharp transition from image to background (image border) has a similar tex-tural appearance to that of a line. It could be taken care of through a simple post-processing step that would remove graphics pixels bordering a group of image pixels. Issue #4 regarding low contrast only applies to the baseline FB (see the lower left footer of the document image of Fig. 8 for an example). Issue #10 affects the sparseness-based feature vectors lightly. It comes from the sparseness map for a graph-ical line element and its neighborhood, in which a thin line appears larger due to the maximal window size of the fil-ter bank (see Fig. 4 b). The proposed CMRS outperforms the baseline FB significantly for issues #4, 5, and 9, with improvements for issue #7 as well. Again, adding contex-tual and multi-resolution information is beneficial: Context helps addressing issues #1, 5, and 7, and also issues #2 and 9 to some extent, whereas multi-resolution helps addressing issues #2, 7, and 9, and to some extent issues #1 and 8. 5.4 Comparisons with another pixel-based method In the previous subsections, we demonstrated through com-prehensive quantitative and qualitative evaluation proto-cols that the proposed CMRS feature vector clearly sur-passes the baseline and has many advantages over non-contextual as well as single-resolution features. To further extend this assessment, we also perform comparisons of the proposed CMRS with an established pixel-based, texture-based method [ 17 ] on the same dataset.

Journet et al. X  X  pixel-based method [ 17 ], referred to earlier in Sect. 2.1 , yields good results on old documents containing several textural classes (text, drawings, background), with classification accuracies of 92 and 83% for text and drawing pixels, respectively. Each pixel is characterized by five fea-tures computed at four different scales through a statistical texture analysis approach. The first three features are linked to orientation information and are derived from the rose of directions, a nonparametric tool based on the autocorrelation function. The last two features are linked to frequency infor-mation. More specifically, the features, computed over the neighborhood of each pixel, are as follows: (1) angle match-ing the main orientation of the rose of directions, (2) value of the autocorrelation function computed for the main orienta-tion, (3) standard deviation of the rose of directions, except for the main orientation, (4) ink/paper transitions computed as the average per-line sum of the difference between the intensity of a pixel and that of its left neighbor, (5) white spaces computed as the mean of the average per-line and per-column sums of pixel intensities over an area obtained through X-Y cuts. These texture descriptors are then used by the Clustering LARge Applications (CLARA) algorithm, an unsupervised k-medoid clustering method, to separate pixels into different content classes.

Our implementation of Journet et al. X  X  method employs the authors X  available C++ code for feature computation and R[ 60 ] for clustering as in [ 17 ]. It was validated using a doc-ument image from [ 17 ] to ensure reproducibility of their results. Tests were conducted on BDID document images with a resolution (75 DPI) close to that of the images used in [ 17 ] to recreate similar testing conditions, with 16 32  X  32, 64  X  64, and 128  X  128 windows. For a consistent comparison, we evaluated the method in [ 17 ] using the same testing pixel set as for our own method (see Sect. 5.2 ). We provide quantitative comparisons of the two methods through overall pixel classification accuracies (Table 4 ) and confu-sion matrices (Fig. 10 ), and qualitative comparisons through visual examples of typical pixel clustering results over entire document images (Fig. 11 ). For the testing pixels, we set the desired number of clusters k to 4 to detect the four basic textural types (text, graphics, image, background) present on BDID images. When testing entire document images for the visual examples, k was set for each image according to its contents, as not all images include all four basic types.
From Table 4 , our proposed method outperforms signifi-cantly Journet et al. X  X  method [ 17 ] in terms of overall accu-Journet et al. [ 17 ] CMRS (proposed method) 0.5321 0.8336 racy, for business document images. As for class-specific performance, the confusion matrices show that our approach performs best as well. As can be seen on Fig. 11 ,clusters found by Journet et al. X  X  method are generally coarser than the results of our pixel classification. This behavior yields to more uniformly recovered image and text regions by [ 17 ], but also yields to missing isolated elements such as small text blocks (e.g., header and footer of document in Fig. 11 b), Figure 11 a shows an example where Journet et al. X  X  method performs well. The example in Fig. 11 b shows a poor perfor-mance of Journet et al. X  X  method; text was split into several clusters, while graphics and background pixels were merged together.

One advantage of Journet et al. X  X  unsupervised approach over our supervised approach is that there is no need to cre-ate training data and train a model, as pixel features are directly clustered into homogeneous groups. However, in drawbacks consequent of the unsupervised approach, user intervention is necessary on two levels: (1) for setting the number of expected clusters and (2) for assigning the most plausible label to each resulting cluster. Our method does not have these drawbacks as the SVM model, once trained, is parameter-free, and pixels are automatically labeled. Both quantitative and qualitative results show that our method sur-passes Journet et al. X  X  method [ 17 ] for business document images. 5.5 Resource consumption We implemented our system in Matlab 8 (R2012b) and per-formed the tests on a PC (AMD Quad-Core 1.6 GHz CPU with8GBRAM).

In terms of memory usage, each pixel is represented by a 10-dimensional feature vector of single precision (32 bits) values, which amounts to 40 bytes per pixel. For a 8 . 5  X  document image, the total memory required to hold the fea-ture vector values amounts to 321 MB, which can be loaded all at once in RAM on a standard modern PC. However, we found that loading one document image column in RAM at a time, for classification purposes, was working better than the entire image at once. As pixels are classified inde-pendently, data parallelism is feasible, but not yet part of our implementation. Saved on disk in compressed format (as in Matlab X  X  .mat format), the feature vector values for a 8 . 5  X  11 X  document image take up on average 165  X  32 MB, depending on the specific contents of the document images.

Table 5 presents the average computational costs of the different steps of the proposed method. Testing one docu-Step Per document Sparseness map computation Feature vector computation Pixel classification 3 , 623  X  70 434  X  1 ment image therefore takes about 82min in total, with the pixel classification (stage II of Fig. 2 ) costing the majority of processing time. We expect that a C/C++ implementation of the code, combined with data parallelism on a multi-core PC, would yield to substantial improvements in terms of process-ing time. In its current state, our method is still nonetheless faster than Journet et al. X  X  method [ 17 ] implemented in C++, which takes about 180min in total per document image at a much lower resolution. This is due to the fact that their autocorrelation function, which has to be computed for each pixel, is more demanding than our Eq. 2 for the sparseness, and our LM filter bank is only applied once per document image. 6 Conclusion This paper presents a pixel classification method for business document images based on a novel, low-dimensional texture feature descriptor. The descriptor is derived from the contex-tual and multi-resolution sparseness of image responses to a filter bank. It is used in the characterization of all funda-mental document elements (text, graphics, image, and back-ground) for document image classification through super-vised machine learning. The proposed approach, evaluated qualitatively and quantitatively on color business document images, achieves high classification accuracy, precision, recall, and F-measure rates and shows substantial improve-ment over a baseline feature vector. It also clearly outper-forms the reference pixel-, texture-based method in [ 17 ]. It does not impose any constraint on the shape of document ele-ments and accommodates varied document layouts. Masks of each document region are derivable from the pixel classifi-cation output for consequent DIA processing such as optical character recognition (OCR).

Our proposed approach does not handle composite objects such as charts and tables. This limitation can be addressed via post-processing, which can provide ways to aggregate pixel classification results for the detection of composite objects; this is the focus of our current work. Future work will incorporate multi-stage classifiers to improve our pro-posed texture-based approach so that it is able to handle doc-uments containing text near lines, color gradients, textured background, and compression artefacts. We will also explore improvements of pixel classification results by combining texture information with edge detection.
 References
