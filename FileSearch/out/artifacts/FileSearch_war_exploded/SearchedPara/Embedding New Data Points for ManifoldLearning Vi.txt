 Recently, some manifold learning algorit hms have been proposed for nonlinear di-mensionality reduction (NLDR). Typical algorithms include Isomap [1], local lin-ear embedding (LLE) [2], Laplacian eigenmap [3], local tangent space alignment (LTSA) [4], charting [5], Hessian LLE (HLLE) [6], semi-definite embedding [7], conformal eigenmap [8], spline embeddi ng (SE) [9], etc. Real performances on many data sets show that they are effective methods to discover the underlying structure hidden in the high-dimensional data set.

All the manifold learning algorithms are initially developed to obtain a low-dimensional embedding for a set of given data points. The problem in general is formulated as an optimization problem, in which the low-dimensional coordi-nates of all the given data points need to be solved. Matrix eigen-decomposition or other mathematical optimization tools (for instance, the semidefinite pro-gramming [7]) are used to obtain the final results, i.e., the intrinsic embedding coordinates. Accordingly, the algorithms run in a batch mode, once for all the in-put data points. When new data points arrive, one needs to rerun the algorithm with all data points. In applications, however, rerunning the algorithm may be-come impractical as more and more data po ints are collected sequentially. On the other hand, rerunning means the previous results are simply discarded. This may be very wasteful in computation.

Our interest here is to embed the new data points into the previously-learned results. This is also known as out-of-sample problem. In literature, out-of-sample extensions for LLE, Isomap, Laplacian Eigenmap are given by Bengio et al., using kernel tricks [10]. This problem i s further formulated as an incremental learning problem, and extensions for LLE and Isomap are given by several re-searchers [11,12]. These approaches are suitable for once dealing with one new data point. For more than one new data points, however, we need to embed them one by one.

Let us use Fig. 1 to explain our motivation. We are given two sets of data points which are well sampled from a manifold embedded in a high-dimensional Euclidean space. The low-dimensional embedding coordinates of one data set are also given, saying in Fig. 1(a), the data points with (dark) purple color. Under these conditions, our task is to embed the new data points (yellow points). In this work setting, the coordinates of the neighbors of a new data point may be known, partly known or even all unknown (Fig. 1(a)).

Our idea to solve this problem is to propagate the known coordinates to the new data points. To this end, we first consider this problem in view of coordinate reconstruction in the intrinsic space and formulate it as a quadratic program-ming. In this way, we can get a global embedding for the new data points. Then, we develop an iterative algorithm through a regularization framework. Through iterations, each new data point gradually obtains a coordinate (Fig. 1(b)). We call this process coordinate propagation . We also use smoothing splines to gen-erate an initial coordinate for each n ew data point to speed up the precess. 2.1 Problem Formulation The NLDR problem. Given a set of data points X = { x 1 , x 2 ,  X  X  X  , x n } X  R m , which lie on a manifold M embeddedina m -dimensional Euc lidean space. The goal is to invert an underlying generative model x = f ( y ) to find their low-dimensional parameters (embedding coordinates) Y = { y 1 , y 2 ,  X  X  X  , y n } X  R d with d&lt;m . In this form, NLDR is also known as manifold learning. The out-of-sample extension problem. Given a set of data points X = { x 1 ,  X  X  X  , x , x l +1 ,  X  X  X  , x n } X  R m and the low-dimensional embedding coordinates Y L = { y according to their relevances to the first l data points. 2.2 Model A large family of nonlinear manifold learning algorithms can be viewed as the approaches based on minimizing the low-dimensional coordinate reconstruction error. The algorithms in this family include LLE, Laplacian eigenmap, LTSA, and spline embedding (SE). The optimization problem can be uniformly formu-lated as follows: where tr is a trace operator, M is a n  X  n matrix which is calculated according to the corresponding geometrical preserving criterion, C is a n  X  n matrix used to constrain Y to avoid degenerate solutions, and Y =[ y 1 ,  X  X  X  , y n ] T is a n  X  d matrix to be solved, in which y i is a d -dimensional embedding coordinate of x i ( i =1 , coordinate of a data point in X .
 Specifically, in LLE, M =( I  X  W ) T ( I  X  W ); in Laplacian eigenmap, M = D  X  W ;inLTSA, M = S T W T WS , and in SE, M = S T BS . In these algorithms, C is a n  X  n identity matrix. Problem (1) can be easily solved via matrix eigen-decomposition.

Now we use problem (1) to solve the out-of-sample problem. Introducing the known low-dimensional embedding coordinates of the first l data points in X , naturally we can obtain a linearly constrained optimization problem:
It seems that problem (2) is very compl ex since the variable to be optimized is a matrix which has d  X  n unknown components. Directly solving it may be very expensive due to different constraints from problem (1).

Now we rewrite Y in terms of column vectors, and denote it by Y =[  X  y 1 ,  X  X  X  ,  X  y ], in which  X  y i  X  R n ( i =1 ,  X  X  X  ,d )isthe i -th coordinate component vector of all the n data points. Then i =1 ,  X  X  X  ,l . Based on the Lagrange multiplier method, problem (2) can be converted, equivalently, into the following d subproblems, each of which is used to optimize a coordinate component vector : where y =[ y 1 ,  X  X  X  ,y n ] T is a n -dimensional vector to be solved. Note that each subproblem is a convex quadratic programming (QP) since M is a positive semi-definite matrix 1 . Therefore, we can easily solve them.
 To reduce the number of variables to be solved, we write M as follows [13]: y Note that y l is known in each subproblem. Substituting it into the correspond-ing objective function, problem (4) can be further reduced to the following QP problems:  X  each QP subproblem in (6) has only n  X  l variables to be solved. Meanwhile, M uu is also positive semidefinition 2 . Thus, each QP in (6) is a convex QP, which has a global optimum.

Finally, we can combine the d global optima of problem (6) together into n  X  ld -dimensional coordinates. In this way, we achieve a low-dimensional global embedding for n  X  l new data points. 2.3 Iterative Algorithm for Coordinate Propagation In this subsection, we develop an iterative algorithm for solving the out-of-sample extension problem. The iterative algorithm can reduce the computational com-plexity and need much less computation r esources. In an iterative framework, it would be possible for us to embed a very large number of new data points.
Here we consider one of the subproblems in problem (4) since they have the same form. For convenience, we omit the s uperscripts and the subscripts in the constraints, and rewrite the problem as follows: Converting the hard constraints in (7) into soft constraints and introducing a predicting term for the new data points, we have the following regularization representation: where  X  1 &gt; 0and  X  2 &gt; 0 are regularization parameters, and g i is a predicted We use spline interpolation to solve this problem (in Section 3).

In (8), the first term is the smoothness constraint, which means that the embedding coordinates should not change too much between neighboring data points. The second term is the fitting constraint, which means that the estimated function y should not change too much from the given values. The third term is the predicting constraint, which means that function y should not bias too much from the predicted values. The trade-off among these three constraints are stipulated by  X  1 and  X  2 . We can see that the second term is equivalent to the hard constraints in (7) in case of  X  1  X  X  X  .

Let us assume for the moment that each g i is known. Differentiating the objective function with respect to y  X  =[ y T l , y T u ] T ,wehave sample extension pro blem, we need not solve y l since y l = f l is known. Thus we only need to consider y u . Here we can see that the second equation in (9) is equivalent to the subproblem in (6) when  X  2 = 0. Now it can be transformed into where S = I  X  M uu and I is a ( n  X  l )  X  ( n  X  l ) identity matrix. Let us introduce two new variables:  X  = 1 1+  X  equation: Some Remarks. (1). According to the theories of linear algebra, the sequence { y than any kinds of operator norms 3 .Herewecantake  X  =1/( || S || 1 +1). Thus points, while the second term is the contribution from the previously learned data points on the manifold. Note that these contributions are decreased since  X &lt; 1 holds. If we only consider these two terms, then the sequence will converge  X 
M  X  1 uu M ul f l , especially when  X  is a small number. To avoid attenuations, we introduce a compensation term, i.e., the third term in Eq. (10). Here we call it a prediction to the new data points in the iterative framework, and its contribution to y u is stipulated by the positive parameter  X  , which is a parameter in (0 , 1). Therefore, it is necessary for us to get a good prediction to g u . We will evaluate it along the manifold via spline interpolation on the neighbors.

The steps of the iterative algorithm can be summarized as follows:
Through the iterations, e ach new data point gradually receives a value (here it is a coordinate component). To construct d coordinate component vectors, we need to perform the iterative algorithm d times. In this way, the known coordinates are finally propagated to the new data points.

In computation, we set the maximum iteration times to 100 when performing and generate a g ( i +1) u in step (4). Details will be introduced in Section 3. we first select to predict the new data point  X  X  X  since it has the maximum number of neighbors with known coordinates. After  X  X  X  is treated, then we select  X  X  X . After  X  X  X  and  X  X  X  have been treated,  X  X  X  is one of the candidates in next time.
In the above process, a basic task ca n be summarized as follows. Given a new that the low-dimensional coordinates of the first r data points are known. The task is to generate a coordinate for the center point x .

Our method includes two steps (Fig. 2(b)). (1). Construct a local coordinate system to represent x and its k neighbors, and calculate r + 1 local coordinates t , t 1 ,  X  X  X  , t r  X  R d . (2). Construct a function g : R d  X  R through which we can get a value f = g ( t )for x .Here f can be considered as a low-dimensional coordinate component. Furthermore, g should meet the following conditions: where f j is a known coordinate component of x j ( j =1 ,  X  X  X  ,r ). Actually, we use g to map the local coordinate t of x into the global coordinate system with lower dimensionality, in which the original data points are represented.
Suppose that the n data points in X are densely sampled from the manifold, then the tangent space of the manifold M at x  X  X  can be well estimated from the neighbors of x [14]. We use this subspace to define the local coordinates [4,6]. To be robustness, we simultaneously coordinatize the k + 1 data points. Note map them into the global coordinate system. Fig. 2(b) shows the above process.
To satisfy the conditions in (11), spline regression method is used to construct the function g . The spline we use is developed from the Sobolev space, and has the following form [15,16]: where the first term is a linear combination of r Green X  X  functions  X  j ( t )( j = 1 ,  X  X  X  ,r ), and the second term is a polynomial in which all p constitute a base of a polynomial space. Here we take the one-degree polynomial have p 1 ( t )=1, p 2 ( t )= t 1 and p 3 ( t )= t 2 .Inthiscase, p is equal to 3. of d =3,  X  j ( t )= || t  X  t j || .

To avoid degeneracy, we add the following conditionally positive definition con-straints [17]: Now substituting the interpolation conditions (11) into Eq. (12) and Eq. (13), we can get a linear system for solving the coefficients  X   X  R r and  X   X  R p : r  X  p matrix with elements P
We p oint out that g is a smooth function and f j = g ( t j )holdsforall j , j = 1 ,  X  X  X  ,r . Faithfully satisfying the given conditions in (11) is necessary for us to rely on it to interpolate a new point x .

To avoid error accumulation, the above p erformance is only used to yield a co-ordinate component for the center point x and not used to map the rest new data points x r +1 ,  X  X  X  , x k .Togetthe d coordinate components of x , we need to con-struct d splines. That is, we need to solve Eq. (14) d times. Note that in each time the coefficient matrix keeps unchanged since it is only related to the r local coor-each new data point can get an initial coordinate.

To predict a new vector g ( i +1) u during the iterations, we only need to set r = k and perform the above algorithm again for each new data point. We evaluated the algorithm on several data sets including toy data points and real-world images. Here we give some results obtained by the global optimization model (GOM) and the coordinate propagation (CP). These experiments can give us a straightforward explanation on the data and the learned results. In addition, the computation complexity is also analyzed in this section.

Fig. 3(a) illustrates 1200 data points sa mpled from a S-surface. Among these data points, 600 data points below the horizontal plane  X  p  X  are treated as original data points, and the rest 600 data points above the plane  X  p  X  are treated as new data points. The intrinsic manifold shape hidden in these data points is a rectangle be viewed as the 2-dimensional (2D) parameter domain of the original data pints. Our goal is to use the new data points to extend it to the right sub-rectangle.
We use LLE, LTSA and SE to learn the original data points. The results with k = 12 nearest neighbors are shown in the left region of the dash line in Fig. 4(a)/ 4(b), Fig. 4(d)/4(e), and Fig 4(g)/4(h), resepctively. Naturally, the learned struc-ture is only a part of the whole manifold shape. The intrinsic manifold structure hidden in these 600 original data points is a small rectangle.
 The new data points are embedded into the right region by GOM and CP. In Fig. 4(a) and Fig. 4(b), the M matrix in Eq. (5) is calculated according to LLE with k = 12, i.e., M =( I  X  W ) T ( I  X  W ). In Fig. 4(d) and Fig. 4(e), the M matrix is calculated according to LTSA with k = 12, i.e., M = S T W T WS ;In Fig 4(g) and Fig. 4(h), the M matrix is calculated according to SE with k = 12, i.e., M = S T BS . From the ranges of the learned coordinates, we can see that the learned manifold shape is extended along the right direction by the new data points.

For comparison, Fig. 4(c), Fig. 4(f) and Fig. 4(i) show the 2D embedding results of all the 1200 data points directly by LLE, LTSA and SE. As can be seen, the results are confined into a square region, not extended to be a long rectangle, which is the real low-dimensional structure hidden in the data points.
 Fig. 5 shows the results by GOM and CP, which use a combination of SE and LLE. That is, the original data points are learned by SE to get their low-dimensional coordinates, but the M matrix in Eq. (5) is calculated via LLE. Compared with the results purely based on LLE (see Fig. 4(a) and Fig. 4(b)), here the shape of the manifold is better preserved.

Fig. 6 shows two experiments on image data points. In Fig. 6(a) and Fig. 6(b), a face moves on a noised background image from the top-left corner to the bottom-right corner. The data set includes 441 images, each of which includes 116  X  106 grayscale pixels. The manifold is embedded in R 12296 . Among these images, 221 images are first learned by SE with k = 8. The learned results are shown with (red) filled squares. The rest data points are treated as new data points. From Fig. 6(a) and Fig. 6(b), we can see that they are f aithfully embedded into the previously learned structure by GOM and CP. Here, the M matrix in Eq. (5) is calculated according to SE.

In Fig. 6(c) and Fig. 6(d), 400 color images are treated, which are taken from a teapot via different viewpoints aligning in a circle. The size of the images is 76  X  101. The manifold is embedded in R 23028 . Among the images, 200 images are first learned by SE with k = 5, and the results are illustrated with (red)filled circles. The rest 200 images are treated as new data points. They are embedded into the right positions by GOM and CP, using SE to calculate the M matrix in Eq. (5). Computation Complexity. Both GOM and CP require to calculate the M ma-trix in Eq. (5). Differently, in GOM we need to solve d QP problems. The compu-decompositions (SVD) of n  X  l matrices in R ( k +1)  X  ( k +1) when computing k +1 local coordinates in tang ent space [4,6] for each of n  X  l new data points. We also need to solve d  X  ( n  X  l ) linear systems formulated as Eq. (14). The computa-tion complexity of SVD is O (( k +1) 3 ), while that of the linear system is near to O (( k + d +1) 2 ) (using Gauss-Seidel iteration). In addition, the computation com-plexity in Eq. (12) is near to O ( k + d + 1). Thus, totally the complexity in each a coordinate propagation is about O (( n  X  l )[( k +1) 3 )+( k + d +1) 2 + k + d +1]). Compared with O ( d ( n  X  l ) 3 ) in GOM method, the computation complexity in CP is only linear to the number of new data points.

In most experiments, the real performan ce of CP is convergent when iteration the convergence is achieved during iterating Eq. (10). This is reasonable since the data points are assumed to be well sampled in manifold learning. Thus we can get a good prediction to each new data point along the manifold via spine interpolation. A parallel work related to out-of-sample extension for manifold learning is the semi-supervised manifold learning [13,18,19]. In a semi-supervised framework, the coordinates of some landmark points are provided to constrain the shape to be learned. The landmark points are usually provided according to prior knowledge about the manifold shape or simply given by hand. The goal is to obtain good embedding results via a small number of landmark points. In generally, it is for-mulated as a transductive learning problem. Intrinsically, the corresponding al-gorithm runs in a batch mode. In contrast, out-of-sample extension starts with a known manifold shape which is learned from the original data points, and focuses on how to embed the new data points, saying, in a dynamic setting which are col-lected sequentially. During embedding, the coordinates of previously learned data points can maintain unchanged. We have introduced an approach to out-of-sample extension for NLDR. We de-veloped the global optimization model and gave the coordinate propagation al-gorithm. Promising experimental results have been presented for 3D surface data and high-dimensional image data, demonstrating that the framework has the po-tential to embed effectively new data poin ts to the previously learned manifold, and has the potential to use the new points to extend an incomplete manifold shape to a full manifold shape. In the future, we would like to automatically in-troduce the edge information about the manifold shape from the data points so as to obtain a low-dimensional embedding with better shape-preserving. This work is supported by the Projection (60475001) of the National Nature Sci-ence Foundation of China. The anonymous reviewers have helped to improve the representation of this paper.

