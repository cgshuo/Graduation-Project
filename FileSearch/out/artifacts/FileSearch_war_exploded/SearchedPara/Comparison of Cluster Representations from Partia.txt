 are becoming an increasingly pervasive media of commu-nications with wide applicability. Data stream clustering attempts to detect patterns or clusters from such data. The real-time nature of data streams makes it essential to rep-resent historical data accurately and compactly. Traditional data mining techniques are mainly built on the concept of persistent data sets that are finite, static, and convenient to process in memory. Conversely, a data stream, consisting of temporally ordered points, is transient, continuous, and time-stamped [2]. Thus, data stream clustering aims at sep-arating incoming data, by computation in limited memory, into distinct groups without using historical data, requiring a compact and efficient intermediate form to store summary statistics of past data. A typical data stream clustering al-gorithm has the following characteristics: a collection, C k , of summary statistics is constructed to represent cluster k from historical data; if an incoming point x or a sequence of points is determined to be in cluster k , summary statistics C k will be updated with x or the sequence; if x or the sequence is determined to be not in any existing cluster, a new clus-ter that contains only x or the sequence will be created. To simplify the terminology, we define two terms, cluster and class, only for use in this paper: cluster refers to the parti-tions found by an algorithm, and class to the partitions given in the ground truth.
 2nd-order cluster representations. The BIRCH algorithm [13] creates an in-memory hierarchical data structure called clustering feature ( CF ) tree, in which each node is a clus-ter and its C k includes a mean, the variances of each di-mension, and a cluster size. The STREAM algorithm clus-ters each chunk of the data using the LocalSearch algorithm [11], where C k contains a weighted center. CluStream is a flexible framework with online and offline components [1]. The online component extends the CF node in BIRCH by including an additional weight in C k . The offline compo-nent performs global clustering based on all historical sum-and The entropy is an uncertainty index of a random variable. The conditional entropies of C given C  X  and vice versa are and The conditional entropy is an uncertainty index of one ran-dom variable given another. The mutual information be-tween C and C  X  is which is a statistical dependency index between two random variables.

Two ECE measures [14] that signify consistency of class labels w.r.t. clusters and have been used in data stream clus-tering are and Purity ranges from 0 to 1: the higher the value is, the purer the class labels in a cluster. Cluster-based cross entropy also ranges from 0 to 1: but the lower the value, the smaller the uncertainty of C  X  given C . Purity increases and cross entropy decreases as classes in each cluster become more uniform. But they are insensitive to the distribution of clus-ters in each class, and the performance of both is likely to improve when the number of clusters increases. To the ex-tremity, a  X  X erfect X  clustering can be achieved by assigning each distinct point to a unique cluster, though the clusters could be far off from the classes. To counteract this effect, we introduce and define which considers clustering consistency w.r.t. classes. Class-based cross entropy ranges from 0 to 1: the lower the value, the smaller the uncertainty of C given C  X  .

ECE measures that consider consistency of both the clus-ter and the class labels w.r.t. each other are seeing increasing importance in clustering. We introduce them to measure the performance of data stream clustering specifically. the total number (weight) of points in cluster k . This cluster representation highly compactly summarizes historical data using a non-oblique ellipsoid for each cluster.

HPStream [1] is a recent data stream clustering algo-rithm that uses a partial 2nd-order cluster representation for each cluster called fading cluster structure, with effective dimension reduction and merging strategies. It utilizes an iterative approach for incremental updating historical clus-ters, by assigning a newly arrived point to the closest cluster based on the Manhattan segmental distance. Each cluster has a limiting radius. A point is added to the nearest cluster if it lies inside the limiting radius of that cluster. Otherwise, a new singleton cluster is created for the point; an old clus-ter with the least recent updating is deleted.
A predicament of the partial 2nd-order cluster represen-tation is that it is not expressive enough for clusters in an oblique ellipsoidal shape. We extend it to the full 2nd-order representation by including the full covariance matrix for cluster k in C k : where  X  k is a (weighted) mean vector,  X  k is a (weighted) full covariance matrix corresponding to the 2nd cross moments, and  X  k is the total number (weight) of points in cluster k . This representation is equivalent to the Gaussian mixture model ( GMM ), a statistically mature semi-parametric clus-ter analysis tool for modeling complex distributions. Geo-metrically, mean is the location of a cluster; covariance is an ellipsoidal approximation of the shape and orientation of a cluster. Sample mean and covariance together are suf-ficient statistics for a multivariate normal ( MVN ) distribu-tion. GMM can be estimated from a static sample using the Expectation Maximization ( EM ) algorithm, which guaran-tees local convergence, but not readily applicable to data streams due to unavailable historical data. Thus we use EM only for estimating the GMM on each window of newly arrived data. By testing the statistical equality between a newly detected cluster and a historical one on their mean and covariance, using the 2nd cross moments, we determine whether to merge the two clusters. In the two tests below, we use the following notations. Let x 1 ,..., x n  X  R d be a sample of size n for X with mean vector  X  x and covariance matrix  X  x , and y 1 ,..., y m  X  R d be a sample of size m for Y with mean vector  X  y and covariance matrix  X  y .
 Equality between two mean vectors  X  The Hotelling X  X  T 2 statistic can determine whether  X  x statistically equals  X  y . The null hypothesis is  X  x =  X  y . T 2 is defined by [7]
In the streaming data model, points may not come ran-domly over time from each class. Those points from a same MVN class may break into pieces and fall into different windows of time. Since each isolated piece may have dis-tinct means and covariance matrices, the merging strategies based on 2nd-order cluster representations may not recog-nize these pieces as being in the same class. To splice these isolated pieces back into the original MVN class, we add the 3rd and 4th cross moments, M 3 and M 4 , respectively, to the cluster representation as follows: The definitions of the 3rd and 4th cross moments of X are
M 3 : E ( X q X r X s )= M 4 : E ( X q X r X s X t )= 1 where q , r , s and t are the dimension indices of X .
This higher-order cluster representation fundamentally expands the landscape beyond the 2nd-order paradigm, and provides a much wider playground for incremental clus-ter merging strategies. Using statistical tests for normal-ity through skewness and kurtosis computable from the 3rd and 4th cross moments, one can discover new MVN clusters by combining cluster pairs with distinct 2nd-order statistics. We shall see below that the choice of both statistics lies in the fact that they can be decomposed into combinations of the cross moments of subsets of data, though their original definitions involve original data.

Characterizing the asymmetry of a probability distribu-tion, the multivariate skewness is defined by [9] where Z rr is the element at the r -th row and the r -th column the 3rd central cross moment which can be estimated from the sample by are the r , s , t -th dimensions of sample mean vector  X  x .The null hypothesis is that the sample comes from a normal dis-tribution. Under the null hypothesis, the statistic  X   X  no merge  X   X  no merging is done between clusters de- X   X  2nd order diagonal covariance  X  X  C  X   X  HPStream  X  X  C  X   X  2nd order  X  X  C  X   X  4th order  X  X  C  X   X  4th order with one-dim  X  X  C We included lower-order cluster representations for com-parison without the effect due to different merging strategies for the lower-order statistics. The performance is compared as a function of exponentially increasing window sizes. The EM algorithm we used was implemented in R package MClust [6]. The maximum number of clusters set for EM was 30. To correct the multiple simultaneous testing effect in comparing multiple pairs of clusters, we used Bonferroni adjusted p -values with an  X  -level of 0.05. The parameters for HPStream were set as follows: InitNumber = speed v = window size , decay-rate  X  = 0 . 5, spread radius factor  X  = 2. We also set the number of projected dimensions to the original number of dimensions such that the results can be compared. All the experiments were per-formed on a Xeon 5135 CPU computer with 16GB memory running on SuSE Linux.

The synthetic data stream  X  Five data streams of 5,000 time points each were randomly generated from a 2-D five-component GMM as defined in Table 1. Figure 1 shows a tation. The different ECE measures are also consistent. The under-performing lower-order cluster representations im-prove significantly over increasing window sizes, and even-tually come close to the performance of the higher-order ones, because the various issues addressed by the higher-order cluster representations in data stream clustering be-come less dominant when the window size becomes larger. It may be worthwhile to point out that the performance of HPStream is very similar to the partial 2nd-order cluster representation, despite of different merging strategies.
The PDMC data stream  X  This data set was collected by BodyMedia, Inc. and used by the Physiological Data Modeling Contest (PDMC) in the International Conference on Machine Learning in 2004. Each point is described by sixteen attributes: userID, sessionID, sessionTime, two characteristics, annotation, gender and nine sensor values, with a class label of various physical activities. The train-ing data set was collected by 9 sensors observing for ap-proximately 10,000 hours, when a person wore a device. The objective was to cluster the points with several possible physical activity labels. We extracted the 9 sensory values from the data set containing a total of 46,209 observations. We used the window sizes of 723, 1,445, 2,889, 5,777, and 11,553 for performance evaluation. It took 1 hour and 44 minutes for the window size of 723. Other window sizes used less time. Figure 5 to 7 plot the performance on three types of ECE measures. Figure 5(a) shows the legend used for each picture. Figure 5 displays the performance eval-uated by the measures which consider the consistency of classes w.r.t. clusters. Although the improvement by using higher-order strategies is not obvious, our merging strate-gies under various configurations always have better perfor-mance than HPStream. Figure 5(b) and Fig. 5(d) show that the purity and homogeneity of our method are always higher (better) than HPStream. Figure 5(c) shows that the cluster-based cross entropy of our method is always lower (better) than HPStream. Figure 6 shows the performance evaluated by the measures which consider the consistency of clusters in each class. In Fig. 6(a), although HPStream can achieve similar or even better performance than our method when the window size is small, its class-based cross entropy is al-ways higher (worse) than our method when the window size is large enough. Figure 6(b) shows that the completeness of our method is always higher (better) than HPStream. Fig-ure 7 displays the performance evaluated by the measures which consider both class and cluster consistencies. The V -Measure of our algorithm is always higher (better) than HPStream. The VI of our algorithm is always lower (better) than HPStream.

The CovType data stream  X  The forest CovType data set was obtained from the UCI KDD archive, used by several other papers for data stream clustering evaluation. There are a total of 581,012 instances in the data set and ter representation, though it is the most complex cluster de-scription one can do so far. This opens up the problem of finding even more complex cluster representations for many real data streams. The higher-order cluster representation fundamentally expands the landscape beyond the 2nd-order paradigm, and provides a much wider playground for in-cremental cluster merging strategies. Additionally, multi-variate kurtosis and skewness are only one avenue of uti-lizing the 3rd and 4th cross moments. One may expect to see other potentially powerful merging strategies based on the higher-order cross moments in the development of data stream clustering.

