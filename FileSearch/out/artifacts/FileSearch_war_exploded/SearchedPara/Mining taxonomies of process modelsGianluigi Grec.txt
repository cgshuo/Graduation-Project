 1. Introduction
Designing and comprehending business processes are often difficult and time-consuming tasks, which typically require the intervention of a business consultant armed with an appropriate level of experience and knowledge on the process itself.
In this context, process mining techniques have been getting increasing attention in recent years, for their ability to auto-matically extract useful knowledge about the actual behavior of a process, which can be a valid support to both analysis and design tasks. Indeed, these techniques are conceived to (automatically) discover a workflow-based model for a process, given some information about past executions, as for it is encoded in audit trails of workflow management systems as well as in transaction logs of various infrastructures for managing business processes (e.g., Enterprise Resource Planning and Supply
Chain Management systems).
Different approaches to process mining [2,9,19,20,32,43,46] have been proposed in the literature, which mainly differ in the language used for representing process models, and in the specific algorithm employed for their discovery (see [44] for a matically supporting further enactments of the process. Therefore, algorithms are commonly designed to maximize the accu-racy of the mined model, i.e., they equip the model with as many details as they are required to explain the events registered in the logs. Consequently, when a large number of activities and complex behavioral patterns are involved in the analysis, schema, while being well-suited for supporting the enactment, might be less useful for a business user who wants to monitor and analyze business operations at some lower level of detail.

In these scenarios, it is indeed desirable to deal with abstraction mechanisms offering the capability of discovering mod-ular, yet expressive, descriptions for processes. In fact, the need and the usefulness of process hierarchies/taxonomies have already emerged in several applicative contexts, and process abstraction is currently supported in some advanced platforms for business management (e.g, iBOM [7] and ARIS [25] ). Actually, these tools allow users to define the relationships among the real process and the abstract views in a manual way only. Hence, providing the user with some kinds of automatic tool capable of building a description of the process at different abstraction levels might represent an important capability for such platforms, in order to effectively reuse, customize, and semantically consolidate process knowledge.
A first step towards achieving a satisfactory trade-off between the accuracy and the comprehensibility of mined process process by means of a structural clustering algorithm, and then equip each of these variants with a specific workflow sche-compact and easier to understand workflows can be discovered. Yet, the approach does not offer any support for restructur-ing the knowledge encoded in the various usage scenarios into in a taxonomy of process models that capture the behavior of the process at hand, by providing different views at different levels of details. 1.1. Overview of the approach and contribution
By continuing along the line of research paved in [19] , the main aim of the paper is to combine the core idea of clustering process executions with ad hoc abstraction techniques that produce a compact and handy representation for each high-level schema, by emphasizing the most relevant behavioral features while abstracting from specific details. The result is a novel process mining approach capable of building a taxonomy of process models. In a nutshell, the taxonomy is modelled as a tree of workflow schemas, where the root encodes the most abstract view, which has no pretension of being an executable work-flow, whereas any level of internal nodes encodes a refinement of this abstract model, in which some specific details are introduced. In other words, leaf nodes stand for concrete usage scenarios (computed through the clustering), whereas each non-leaf node (computed through abstraction mechanisms) is meant to provide a unified representation for all the process models associated with its children. Technically, the combination of process mining and abstraction methods, which is the main distinguishing feature of our approach, is carried out in two phases:
In the first phase, we use a generalization of the top-down clustering algorithm presented in [19] to hierarchically decom-pose the process model into a number of sub-processes. Since, at each step, the algorithm splits a cluster whose associated schema is expected to mix different usage scenarios, those clusters over which no further splitting is applied effectively model different concrete variants for the process and can support further-coming enactments. Note that, because of the application of a hierarchical clustering, the result of the algorithm is a tree-like schema whose nodes correspond to set of executions and where, for each node, its children correspond to the splits originated by that executions. In fact, while the algorithm in [19] exploits a peculiar approach to equip each leaf with a workflow schema, the generalization discussed in this paper accommodates the use of any arbitrary process mining technique.

In the second phase, we navigate bottom-up the tree, i.e., from the leaves to the root, in order to build a taxonomy. In par-ticular, we equip each non-leaf node with an abstract schema that generalizes all the schemas associated with its children.
To this aim, an abstraction method is defined which supports the generalization by properly replacing groups of  X  X  X pecific X  activities with higher-level ones.

Both above phases have been implemented in a prototype system. Even though the resulting tool could be a valuable add-on for any advanced process management platform, we decided to implement it as a plugin for the process mining frame-work ProM [47] , which is a well-established platform for developing and testing novel process mining algorithms. The sys-tem (in the respect of both above phases) has thoroughly been tested on syntectic and real datasets. In particular:
As for the first phase, an experimental analysis has been conducted to compare the quality of the schemas induced by the clusters with those that can be obtained with other process mining techniques without clustering the process executions.
In these experiments, differently from [19] , several process mining algorithms have also been used to equip with a model the various clusters.

As for the second phase, a case study has been discussed where the salient features of the abstraction mechanisms have been stressed. In this respect, it is worthwhile noticing that the availability of some background knowledge about a given domain might be quite useful to improve the quality of the mined taxonomy, especially in order to find the appropriate level of details. Actually, in this case study, we have been experiencing that, even in the absence of context-dependent background knowledge, the taxonomical representation of the process model effectively enables for an explorative anal-ysis where users may navigate within the tree-like structure, focusing on those concrete execution scenarios that most cation-depended issue), a major advantage of having higher-level abstract schemas is to facilitate such an interactive anal-ysis of process executions. Investigating on how to incorporate context-dependent information within the algorithm for inducing process taxonomies is outside the scope of this paper, but yet constitutes an interesting avenue of further research. 1.2. Plan of the paper
The rest of the paper is organized as follows. In Section 2 , a few preliminaries on process models are discussed. The top-down clustering algorithm is illustrated in Section 3 , while the abstraction techniques aimed at building the schema taxon-omy are presented in Section 4 . Section 5 then provides an overview of the system that was integrated in the ProM frame-work. The practical application of the approach to different log data is discussed in Section 6 , where several experimental 7 , we briefly survey some works recently appeared in the literature which are connected with our research, yet catching the opportunity to emphasize the main distinguishing points in our approach. A few concluding remarks are drawn in Section 8 , which also depicts some directions of future work. 2. Process models and process logs
A significant amount of research has been done in the context of specification mechanisms for process modelling (see, e.g. [48,42,41,15,36,45] ). Process models are aimed at representing all possible execution flows along the activities of a given process, by means of a set of constraints defining  X  X  X egal X  execution in terms of simple relationships of precedence and/or more elaborate constructs such as loops, parallelism, synchronization, and choice (just to cite a few).
The whole methodology discussed in this paper is completely orthogonal to the specific model adopted to represent pro-cesses (in fact, experiments will be discussed showing applications on a few relevant process models). Thus, to our aims, it schemas that can be built by using the constructs in M over the activities in A .

Example 1. Fig. 1 shows a possible workflow schema for the OrderManagement process of handling customers X  orders in a business company, which refers to a simple and intuitive modelling language where precedence relationships are depicted as arrows between nodes in a control flow graph, and where some constructs drawn by means of labels beside the tasks (cf. and , or , xor ) are used to state more elaborate constraints of execution.

Roughly speaking, a node whose input is an  X  X  X nd X  acts as synchronizer (i.e., it can be executed only after all its predecessors have been completed), whereas a node whose input is an  X  X  X r X  can start as soon as at least one of its predecessors has been completed. Furthermore, once finished, a node whose output is an  X  X  X nd X  (resp.,  X  X  X r X ,  X  X  X or X ) activates all (resp., some of, exactly one of) its outgoing activities.

Each time a workflow schema W 2 M  X  A  X  is enacted in a workflow management system, the activities in A are executed according to the constraints of W , till some final configuration is reached for which there is no constraint in W forcing the execution of some further activity. As an example, we can refer again to the schema of Fig. 1 , and notice that the sequence acbgih might be the result of an enactment. In fact, the sequencing of the events related to these various executions is stored by the system over each enactment, and will constitute the input for the process mining problem [46,44,43,2,9,32,19] . More formally, given a workflow schema W over a modelling language M and over the activities in A , we shall denote by goal is to discover a workflow schema, say W 0 , such that L  X  L  X  W
W would behave in practice as W .

Clearly enough, in real applicative scenarios, we have little chance of discovering such an ideal W noise and/or malfunctioning occurred in some enactments. Thus, several measures of qualities for the discovered workflow schema W 0 can be used to asses how much W 0 is close to W .

Again, we note that adopting some specific measure is an orthogonal issue w.r.t. our approach (and, in fact, different measures have experimentally been tested). For the sake of exposition, we hence shall assume the availability of a tem-plate function conformance that on input W 0 and L reports a real number in [0..1] estimating how much W model the logs in L (the higher is the score, the better is the model). Then, process mining algorithms are naturally aimed at deriving a schema W 0 such that conformance  X  W 0 ; L  X  is maximized for some conformance function and on any input log
L . In the following section, we shall discuss how this problem can be generalized to cope with taxonomies of process models. 3. Hierarchically mining workflow schemas
As outlined in Section 1, our method for discovering expressive process models at different levels of detail is articulated in two phases. First, we exploit a hierarchical top-down clustering algorithm, called mined model in a bottom-up way, and we restructure it at several levels of abstraction, by means of the algorithm BuildTaxonomy .

In this section, we provide details on the former phase. More precisely, after formally defining the notion of schema decomposition , we describe the algorithm HierarchyDiscovery 3.1. Algorithm HierarchyDiscovery
A process mining technique that is specifically tailored for complex processes, involving lots of activities and exhibiting means of a collection of different, specific, workflow schemas. Here, we propose a new algorithm that extends the one pre-sented in [19] , and where the mined model is meant to represent the process at different levels of granularity. Indeed, the algorithm will compute a tree-like decomposition of the process where each node corresponds to a schema, and where the set of schemas associated with the children of a node v models the behavior encoded in the same set of executions supported introduce the following notion of schema decomposition.
 Definition 2 ( Schema decomposition ).
 Let A be a set of activities, and let M be a modelling language. Then, a schema decomposition ( over M and A ) is a tuple H  X h WS ; T ; k i , such that: WS M  X  A  X  is a set of workflow schemas;
T  X h V ; E ; v 0 i is a tree, where V (resp. E ) denotes the set of vertices (resp. edges), and v k : V 7 ! WS is a bijective function associating each vertex v 2 V with a workflow schema k  X  v  X  in WS .
The input for the process mining problem is, as usual, a log L . Based on L , our aim is to build a schema decomposition, where for each vertex v in V , the set S v of the schemas associated with the children of v , i.e., S ma for each of these clusters.

An algorithm, named HierarchyDiscovery , implementing this approach is reported in Fig. 2 , where the function mineWFschema is exploited for associating a single workflow schema in W  X  A  X  with each cluster of the hierarchy X  X ne may use any standard process mining algorithm here. The algorithm starts by building a workflow schema W is the first attempt to represent the behavior captured in the log traces, and that will be the only component of WS (Line 2).
The schema W 0 is associated with the whole log via the auxiliary structure Traces (Line 3), which enables for recording the set of traces each discovered schema was derived from. Moreover, the tree T is initialized with a single node (its root) v which is associated with W 0 by properly setting the function k (Lines 4 and 5).

In order to get a more accurate model, the refinement is carried out with the aim of reducing as much as possible the number of spurious executions supported by the model that do not correspond to any of the instances registered in the input log L . Indeed, algorithm HierarchyDiscovery is designed to greedily select to refine the  X  X  X east conforming X  schema W
WS (i.e., the one that gets the lowest score by the chosen conformance function) and to derive a set of more refined schemas (Lines 7 X 18) as children of the node corresponding to W q partitioned through the procedure partition-FB (Line 9) into a set of k clusters which, in a sense, are more homogeneous from a behavioral viewpoint.

Note that, different clustering algorithms could be used to partition the traces associated with the selected schema W into a feature space, where classical clustering methods can be eventually applied. From this exposition, we omit details on the specific way partition-FB is implemented, and we address the reader to, e.g. [19] , where a special kind of sequences of current workflow schema (i.e., W q ,in Fig. 2 ), and which can efficiently be discovered via a level-wise search strategy.
Once that traces associated with W q have been partitioned, for each of the new clusters so found, say L flow schema W n  X  h is discovered (again with function mineWFschema keeps trace of the connection between the schema W n  X  h and the trace cluster, L
Moreover, W n  X  h is associated with a new node v n  X  h , which is added to the tree T as a child of the node v to the schema W q being just refined (Lines 15 and 16).

The whole process of refining a schema can then be iterated in a recursive way, by selecting again the least conforming leaf schema in the current hierarchy. The operations are in fact repeated until the number of schemas stored in WS is great-minimum conformance value over the schemas associated with the leaves of T that have some corresponding trace in L , exceeds a threshold c .
 We conclude this section by noticing that, by a direct application of computational results in [19] , the covery algorithm can be implemented to perform O  X  maxSize k p  X  scans of the input log, where p is the number of scans of L required by each call to function partition-FB in algorithm tion-FB requires a number of scans which is bounded by the length of the task sequences used as features. 3.2. An example scenario
In this section, we illustrate the application of the mining algorithm to a simple scenario, which refers to the workflow schema shown in Fig. 1 . Based on this schema, we randomly generated 100,000 traces compliant with it by using the gen-erator described in [17]  X  X otice that more traces actually encode the same task sequence. In this generation, we also re-quired that task m could not occur in any execution containing f , and that task o could not appear in any trace containing ing procedure cannot be performed whenever some external supplies were asked for. These additional constraints allow us to simulate the presence of different usage scenarios that cannot be captured by a simple workflow schema.
The output of HierarchyDiscovery , for maxSize  X  5 and c  X  0 : 85, is the schema decomposition reported in Fig. 3 . More specifically, Fig. 3 a sketches the tree-like structure of the decomposition, where each node logically corresponds to both a cluster of traces and a workflow schema induced from that cluster, by means of traditional algorithms for process mining.
The workflow schemas eventually extracted for the leaves of this tree are shown in Fig. 3 b X  X . responds to the whole set of traces and to an associated (mined) workflow. Actually, the algorithm tering algorithm ( k -means in our implementation).

In the example, we fix k  X  2 and the algorithm generates two children v high conformance), while traces associated with v 1 are split again into v leaves of the tree are those shown in the figure. As a matter of facts, schemas W tively) are only preliminary attempts to model executions that are, indeed, modelled in a better way by the leaf schemas.
Nevertheless, the whole decomposition is an important result as well, for it structures the discovered execution classes and for it forms the basis for deriving a schema taxonomy representing the process at different abstraction levels (cf. defi-nition of W 0 and W 1 ). 4. Building schema taxonomies The second phase of our approach is devoted to exploit the schema decomposition produced by in order to derive a taxonomy of workflow schemas that collectively represent the process at hand at several abstraction levels. In this section, after formally defining the concept of schema taxonomy, we overview the main algorithm, called
BuildTaxonomy , that restructures each non-leaf schema in the input hierarchy by making it a generalization over all its children. The general idea of the algorithm is discussed in Section 4.1 , while some important details will be illustrated in
Sections 4.2 and 4.3 . An exemplification on our OrderManagement process will finally be discussed in Section 4.4 . 4.1. Algorithm BuildTaxonomy
Based on a schema decomposition, we next consider the problem of consolidating the knowledge about a process behav-assuming they are gathered in a suitable repository, called abstraction dictionary, which is defined as follows. Definition 3 ( Abstraction dictionary ).

An abstraction dictionary is a tuple D  X h A ; I sa ; P artOf i , where A is a set activities . I sa A A , P artOf A A and, for each a 2 A ,  X  a ; a  X  6 2 P artOf and  X  a ; a  X  6 2 I sa .
 b is a component of a . Based on the content of the abstraction dictionary, we can define some more general relationships that are meant to capture scenarios where an activity is an abstract version of another one.
 Definition 4 ( Implied activities ).
 a ! D a 0 ,if a ; a 0 2 A and either  X  a 0 ; a  X 2 I sa or  X  a 0 ; a  X 2 P artOf ,or there exists an activity x 2 A such that a ! D x and x ! D a 0 .
 Finally, the set of all basic activities implied by a w.r.t. D is denoted by impl :9 a 00 s : t : a 0 ! D a 00 g .

Notice that complex activities represent high-level concepts defined by aggregating or generalizing basics activities actu-the efforts for comprehending and reusing process models, for they structuring process knowledge into different abstraction levels. Before providing a formal definition of taxonomical process models, we next illustrate the simple underlying notion of generalization between workflow schemas.
 Definition 5 ( Schema generalization ).

Let W 1 and W 2 be two workflow schemas over the sets of activities A
W 1 ( W 1 generalizes W 2 ) w.r.t. a given abstraction dictionary D , denoted by W a 2 A 1 or there exists at least one activity a 1 in A 1 such that a
Example 6. Consider the workflow schema W 4 in Fig. 3 d, and the schema W  X  p ; x 1  X  already belong to the P artOf relationship of a given abstraction dictionary X  X he containment of d and p in x sized in a graphical way in both those figures. Then, W 1 schema W 3 depicted in Fig. 3 c.

We are now in position to formalize the concept of schema taxonomy, which is the output of the second phase of our approach.
 Definition 7 ( Schema taxonomy ).
 w.r.t. D if for any pair of nodes v and v c in V such that  X  v ; v
Note that schema taxonomies found on a notion of generalization that refers to basic abstraction relationships between activities and disregards the ordering of activities and other routing constraints they are involved in. Indeed, these taxono-mies are primarily devoted to provide a multi-layered description of the process, where only the leaf schemas are meant as pact and possibly approximated description over heterogeneous behavioral classes, which can eventually support high-level explorative analysis of the process. We can hence admit a loss in precision in the representation of these latter schemas.
Therefore, our perspective is completely orthogonal w.r.t. other proposals where inheritance notions are defined to take into
Armed with these notions, we can now discuss the BuildTaxonomy takes in input a schema decomposition H (computed by HierarchyDiscovery abstraction dictionary D that G has been built according to.

The basic task in the generalization consists of replacing groups of  X  X  X pecific X  activities appearing in the schemas to be generalized, with new  X  X  X irtual X  (i.e., complex) activities representing them at a higher level of abstraction. By this way, a more compact description of the process is obtained, yet providing that the abstraction dictionary D is updated to maintain the relationships between the activities that were abstracted and the new higher-level concepts replacing them. This dictio-nary is simply initialized in a way that it just contains all the (basic) activities appearing in any schema of H , while both are already available (as a form of background knowledge), by suitably encoding them into it.

The algorithm works in a bottom-up fashion (Lines 2 X 7): starting from the leaves of the input decomposition, it produces, for each non-leaf node v , a novel workflow schema that generalizes all the schemas associated with the children of v . Nota-bly, this schema is meant to accurately represents only the features that are shared by all the subsets of executions corre-complex ) activities. This generalization is carried out by providing the procedure associated with the children of v , along with the abstraction dictionary D (Line 5). As a result, a new generalized schema stracted with the complex ones replacing them in the generalized schema.

As a final step, after the schema taxonomy G has been computed, the algorithm also restructures the abstraction dictio-nary D by using the procedure normalizeDictionary (Line 9), which actually removes all  X  X  X uperfluous X  activities that were created during the generalization. In particular, this step will eliminate any complex activity a not appearing in any schema of G , which can be abstracted into another, higher-level, complex activity b , provided that this latter can suitably cial for ensuring the correctness of the approach.

Clearly enough, the effectiveness of the technique depends on the way the generalization of the activities and the updat-ing of the dictionary are carried out. Procedure generalizeSchemas workflow schemas into a preliminary workflow schema W (Line g2), which represents all the possible flow links in the input workflows X  X asically, this procedures can be seen as performing the union of the control flow graphs corresponding to each
W . Subsequently, activities are abstracted by applying the procedure merging activities in the reference set it receives as the first parameter, and by updating the associated constraints and the abstraction dictionary D . In particular, abstractActivities are derived from the same input schema X  X t step h , the activities coming from the h th schema can only be merged (Line g4). A further application is then performed to abstract non-shared activities in the current schema, independently of their origin (line g5).

Details on this procedure are reported in the following. Here, we just note that that for every activity a occurring in some schema of WS , either (i) a is kept in W or (ii) a is abstracted into some chain of high-level activities, the last of which appears in W . Therefore, we will immediately be guaranteed that the output re-turned by algorithm BuildTaxonomy complies with Definition 5 . 4.2. Matching activities for abstraction purposes
In order to discuss the implementation of abstractActivities exploit for singling out those activities that can safely be abstracted into higher-level ones. In particular, we consider two nary D , and another devoted to compare activities from a topological viewpoint. To introduce similarity functions of the for-mer kind, we preliminary need some further concepts related to abstraction dictionaries.
 noted by a " D a 0 , if there is a sequence of activities a w.r.t. D , denoted by dist D G , is the minimal length of the genpaths connecting a 0 to a . As a special case, we let dist any activity a .

The notions of genpath and dist D G introduced above are exploited in the following in order to define the most specific gen-is maximally close to both of them.
 Definition 8 ( Most specific generalization ).
 follows: maximally close to both of them according to the function dist this case we hereinafter assume that the function msg D  X  x ; y  X  just returns one of them, chosen at random. We can now define some dictionary-based similarities.
 Definition 9 ( Dictionary-based similarities ).
 between x and y , denoted by sim D  X  x ; y  X  ,is while the generalization-oriented similarity between x and y , denoted by sim where, for any two sets B and C , b  X  B ; C  X  X  j B \ C j j B [ C j ing their implied activities (see Definition 4 ), extended with a and b , respectively. Conversely, function sim count the generalization relationships that derive from D , and evaluates the semantical similarity of two activities, by measuring their distance to their most specific generalization (i.e., their closest common ancestor in the hierarchy induced ilar to each other the two activities are deemed by function sim
We now turn to describe a function aimed at comparing activities from a topological viewpoint. In the following, let E be a directed binary relation encoding the set of precedence constraints in some given workflow schema W (elements in E can  X  X  X erged X  by limiting the creation of spurious control flow paths among the remaining activities in the workflow schema. In this respect, we focus on two cases that can lead to a meaningful merging without upsetting the topology of the control flow graph, as formalized in the following definition.
 Definition 10 ( Merge-safety ).

Given a binary relation E , we say that a (unordered) pair of activities  X  x ; y  X  is merge-safe if either
E and after removing these edges no other path exists connecting x and y ; or, (b) f X  x ; y  X  ;  X  y ; x  X g \ E  X ;  X  X ntuitively, there is no path connecting x and y , where, for any set F E , F denotes the transitive closure of F .

Notably, only in the case (b) of Definition 10 the merging of x and y may lead to spurious dependencies among other activ-ities in the schema. Indeed, this happens when there are two other activities z and w such that  X  z ; w  X  6 2 E , and either one of the following conditions holds: where P E a (resp. S E a ) denotes the set of predecessors (resp. successors) of activity a , according to the arcs in E .
The function sim E  X  x ; y  X  , reported below, incorporates these measure in a smoothed way, with the aim of evaluating the similarity of a pair of activities according to the number of spurious flows that would be generated when merging them (the more spurious flows are introduced, the lower is the score): Definition 11 ( Topological similarity ).

Let x and y be two activities, and E be a set of activity pairs, encoding control flow (precedence) relationships,
Notice that sim E produces a maximal value whenever one of the  X  X  X trong X  conditions discussed before holds, and, in gen-eral, tends to assign high values to activities matching over many of their predecessors (successors). 4.3. Procedure abstractActivities
We can now discuss the abstractActivities procedure in Fig. 5 . The procedure takes as input a workflow schema W , stracted activities with the associated complex ones, and modifies D in order to record the performed abstraction transformations.

The procedure works in a pairwise fashion by repeatedly abstracting two activities m means of a complex activity p . These activities are identified by the function indicating, besides p , m 1 and m 2 , the kind of abstraction relationship to be used, i.e., P artOf or I sa . Since either m they are really distinct from p (Line 4). Anyway, the dictionary is correctly updated to include the activity p (Line 5). The algorithm then calls the procedure deriveConstraints (see Line 10), which is responsible of deriving some local con-straints on the basis of those activities m 1 and m 2 that are being merged into it. Note that when carrying out procedure deriveConstraints , problems may occur when two constraints are discovered over m when they can not both hold at the same time. In our implementation, we decided to tolerate loss of accuracy in representing removed. As an example, when join and or constraints are in conflict, their merging causes the removal of the join one. Fi-in order to reiterate the whole abstraction procedure.

A crucial aspect in this approach pertains the way activities to be abstracted are identified. To this end, procedure
BestAbstraction takes as input a set S of activities and an associated set E , along with an abstraction dictionary D and exploits the similarity measures introduced in Section 4.2 , which evaluate how much two activities are suitable to be merged sim  X  a ; b  X  (see Definition 11 ) compares them from a topological viewpoint according to a given set E , whereas the two other functions (see Definition 9 ), measure semantical affinities between a and b according to an abstraction dictionary D . More getBestAbstraction returns the tuple h e ; e ; e ; e i (Lines b3 and b6), simply meaning that no abstraction can be performed over the activities in S .

In the other case, the procedure computes a tuple whose elements, respectively, specify the two activities to be ab-both of them. The choice of both the abstracting activity and the abstraction mode are essentially based on the similarity are deemed similar enough to be looked at as two variants of some activity that generalizes them both. In fact, we heuris-tically prefer to abstract two given activities by means the of ISA relation, whenever their mutual similarity is quite enough to consider them as different specializations of the same abstract activity.

Before considering the creation of a new activity for generalizing m not produce any real modification in the abstraction dictionary, in the calling procedure m and m 2 by using either an I sa relationship (Line b11) or a P artOf relationship (Lines b12). 4.4. An example scenario (contd.) Consider again the schema decomposition shown in Fig. 3 . Then, algorithm leafs, thus first processing the schemas W 3 , shown in Fig. 3 c, and W respectively.

The result of this generalization is the schema W 1 shown in Fig. 3 e, which is obtained by first merging all the activities activities, yet trying to minimize the number of spurious flow links that their merging introduces between the remaining activities, and yet considering their mutual similarity w.r.t. the content of the abstraction dictionary.
In particular, when deriving the schema W 1 , only the activities d and p are aggregated again into the complex activity x created previously, as it actually contains both of them; consequently, d and p are replaced with x ma W 1 is then merged with the schema W 2 associated with v the root v 0 . In fact, when abstracting activities coming from W activity x 1 . Furthermore, the activities e and f are aggregated into the complex activity x x . As a consequence, the pairs  X  e ; x 2  X  ;  X  f ; x 2  X  ;  X  m ; x 5. A system for discovering expressive process models
The whole approach discussed in the paper has been implemented in Java and integrated as a mining plugin in the process mining framework ProM [47] . This framework is quite popular in the Process Mining research community, and provides a platform for effectively developing algorithms for mining and analyzing process log data. In this section, we first discuss the conceptual architecture of our plugin and its implementation in ProM, and then we illustrate its application on our run-ning example. 5.1. System architecture and integration in ProM
The ProM framework has been developed as a completely plug-able environment. An important feature of ProM is that it provides various components for implementing several basic functionalities (e.g., loading log files, setting mining parame-ters, and visualizing mining results), as well as data structures for representing workflow schemas and intermediate results. The framework can be extended by adding five different kinds of plugins: Import plugin, for implementing  X  X  X pen X  functions for external data, e.g., loading instance-EPCs from ARIS PPM; Export plugin, for developing  X  X  X ave as X  functions for some objects, e.g., saving a workflow model as an EPC, or Petri net, or AND = OR graph, etc.; Mining plugin, for implementing mining algorithms; Analysis plugin, for implementing analysis functions on either log data or mining results; Conversion plugin, for enabling conversions between different data formats, e.g., from EPCs to Petri nets.
All the algorithms presented in Sections 3 and 4 have been embedded in the AWS (Abstraction Workflow Schema) plugin, which is loaded dynamically at the start up of the framework. Fig. 6 shows the conceptual architecture of the implementa-tion, where colored modules are components made available by ProM. For instance, the plugin exploits the Log filter com-ponent of ProM, which allows to load an MXML file (i.e., a process log in an XML-based format) in the framework and to make it available for data processing.

For the sake of clarity and conciseness, the major functional elements in AWS are labelled with the names of the algo-rithms and procedures previously presented in the paper. Notably, different repositories are exploited to specifically manage the main kinds of information involved in the process mining task: log data, schema taxonomies, and abstraction relation-clustering algorithm and the schemas generated during both the mining phase and the restructuring one.
By reflecting the nature of our approach, the plugin works in two phases: first, a schema decomposition is computed by means of the HierarchyDiscovery module implementing the hierarchical clustering algorithm. Then, in the second phase, the mined model is visited in a bottom-up way, and it is restructured at several levels of abstraction, by means of the
BuildTaxonomy module. The results are made accessible to other plugins through the standard Result Frame component, while they are presented to the user by exploiting the Visualization Engine component of ProM.

Note that the submodule Partition-FB has been realized by exploiting some of the components in the  X  X  X WS analysis X  features for the clustering) X  X n accordance with the approach proposed in [19] . In fact, besides the desired number of clus- X  X  X xpectedness X  w.r.t. the current workflow model (the lower is c rent implementation of AWS both these thresholds are chosen in an automated way, so freeing the user of such an uneasy task. In more detail, while always fixing r DWS  X  0 : 05, AWS first tries to make DWS use highly unexpected patterns for the clustering, by setting c DWS  X  0 : 01; if no patterns are found, DWS is launched with c can be set by interleaving DWS and AWS.

Finally, we note that within the architecture of ProM different process mining algorithms could be exploited to imple-ment the submodule MineWFSchema . By default, we use the ProM HeuristicMiner plugin for this purpose, which is basically an implementation of the approach discussed in [48,13] . 5.2. The system in action
We next illustrate the application of the AWS plugin on a sample log generated for the process Order Management de-picted in Fig. 1 , which we have been using as running example throughout the paper. As shown in Fig. 7 , three groups of parameters have to be specified in the input panel of the plugin, which conceptually correspond to the functional modules HierarchyDiscovery , BuildTaxonomy , and MineWFSchema .
 In the upper part of the input panel, the user can set the parameters needed by the ilar activities threshold (corresponding to parameter q in the minimal score necessary for merging together a pair of activities, and the Score threshold (corresponding to parameter options allow the user to choose whether either the whole hierarchy of schemas or simply its root shall be shown. The second set of parameters (in the middle part of the input panel) are those required by the i.e.: the lower threshold c and the two upper bounds maxSize and k . Finally, the last section of the panel concerns the parameters required by the algorithm used for mining each single workflow schema in the hierarchy (i.e., the functional module MineWFSchema ). In fact, in the bottom of the input panel we can see the parameters of the Heuristic Miner algorithm.

In Fig. 8 , we report the results obtained by applying AWS plugin when Similar activities threshold = 0.3, Score thresh-old = 0.5, Tree View button is checked, One dictionary for each abstract schema button is checked. As we expected, the algo-rithm generates a schema taxonomy where the non-leaf schemas w : 0 and w : 1 are abstract schemas. Notice that when Dictionary is made available to the user.
 6. Experimental results
This section is meant to illustrate a thorough experimentation work we have conducted over different process logs to as-sess the effectiveness of our approach with regard to both the clustering scheme and the abstraction mechanisms. In partic-ular, we shall describe: (i) the application of the approach to a simple benchmark log, in order to provide some further intu-(iii) a concrete case study concerning a maritime freight harbor, providing some hints of the benefits that the combined usage of hierarchical clustering and abstraction mechanisms could yield in real-life application contexts. 6.1. Qualitative analysis on a benchmark log
In order to give some further intuition on the behavior of our approach, we considered one of the example log files (namely, ExamplesOntologiesSAMXML.mxml ) that accompany the current version (4.2) of the ProM framework. This log con-sists of 500 traces and regards a process for the repairing of phone devices, which appears as a running example in a tutorial of ProM (available at is.tm.tue.nl/ cgunther/dev/prom/PromTutorial.pdf pair hereinafter. In short, the process starts by registering a phone device (activity customer. As soon as it is identified which sort of defect affects the phone (through the activity is sent to the customer (task Inform User ), while the device is sent to the Repair department. Actually, two kinds of repair activities can be performed based on the severity of the defect that has to be fixed, which are indeed named ple) and Repair (Complex) . Every time a repair employee finishes working on a phone, this latter is sent to the QA depart-ment, in order to check whether the defect has really been fixed (activity the case is archived and the phone is returned to the customer (activity again to the Repair department (activity Restart Repair ). Fig. 9 reports the workflow schemas that were mined out of these log traces when using two classical process mining techniques, which are available as mining plugins in the ProM framework: HeuristicMiner , which essentially implements the approach introduced in [48,13] , and a  X  X  , which implements the approach in [49] .

We then launched the AWS plugin described in Section 5 , which implements our approach by using the HeuristicMiner for most 3 clusters, and set both abstraction thresholds (i.e., q and q s ) to their default value.

Fig. 10 shows a screenshot of AWS, where the whole clustering hierarchy is sketched in the leftmost panel. The workflow schemas discovered for the leaves of the hierarchy, reported in Fig. 11 , evidence three main usage scenarios for the process, 232 cases, the defect was fixed by just one complex repair (schema T : 1); and in 143 cases, after trying to perform a simple repair, some further repairs (either complex or simple) were necessary to fix the problem.

In addition to offering a way to classify the executions registered in the log under analysis, the three schemas described above collectively provide a sounder representation than the flat (single schema) models discovered with the algorithms HeuristicMiner and a  X  X  . Indeed, it can be verified X  X .g., with the help of the LTL Checker analysis plugin available in ProM X  X hat the task Repair (Simple) does not occur in any of the traces where to model 22 log traces where, indeed, multiple repair actions occur with a complex one as the first of them. In any case, if completeness is a major issue, then we could perform some finer clustering of the log, or mine each cluster schema with an algorithm different than HeuristicMiner , or simply use some different parameters setting for this latter X  X urther analysis in such a direction is omitted for the sake of presentation.
 The workflow schema that is derived by abstracting T : 0, T : 1, and T : 2 is again shown in Fig. 10 . Notably, the activities pair (Complex) , Repair (Simple) and Restart Repair , which do not appear in all of the leaf schemas, have been re-placed with a new activity, named ABS0 , abstracting them all. In fact, the abstraction dictionary computed by the AWS plugin contains an IS-A link between each of these three activities and in Fig. 10 . 6.2. Conformance analysis on different benchmark logs
We next discuss some experiments we carried out to evaluate the effectiveness of our clustering-based process mining approach (sketched in Fig. 2 ). Experiments have been performed on 10 log files, provided as benchmark datasets in the ProM framework and which have actually been used in the literature for the evaluation of other process mining approaches. These logs were generated synthetically, by considering different kinds of behavior, ranging from basic routing constructs, such as sequence, choice, parallel execution, and loop, to more complex ones, such as non-free choice and invisible tasks. In order to compare our approach with a few well-known process mining techniques, we took advantage of the following ProM plugins:
HeuristicMiner , substantially implementing the technique described in [48] , a  X  X  , which implements the technique in [49] , and GeneticMiner , implementing the approach in [14] .

The two former techniques have also been used within our hierarchical clustering scheme as two different implementa-tions for the procedure MineWorkflow , devoted to build a workflow for each discovered cluster. As a matter of fact, due to the expensive computation performed by the GeneticMiner , we did not explore the integration of this latter algorithm within configurations for our approach will be considered in the remainder of this subsection: AWS-HN , where MineWorkflow is implemented with HeuristicMiner , and AWS -a  X  X  , where MineWorkflow is instantiated with a  X  X  .

As discussed in Section 2 , in order to evaluate the effectiveness of any process mining technique, some quality measure is needed to express the capability of the discovered model to accurately capture the behavior recorded in the log, yet avoiding to introduce an excessive number of features and execution paths. In actual fact, different validation approaches have been proposed in the literature for this purpose, although none of them is a standard. Interestingly, the ProM plugin Conformance-
Checker provides a shared basis to evaluate the conformance of a process model w.r.t. the log it was discovered from, on the condition that the model is represented as a Petri net. 2 usual, in [0..1]) available with the ConformanceChecker plugin:
Fitness , a sort of completeness measure defined in [34] , which evaluates the compliance of the log traces with respect to a given process model. Roughly speaking, this measure considers the number of mismatches that occur when performing a non-blocking replay of all the log traces through the model (i.e., the tokens that must be created artificially, as well as those left unconsumed): the more the mismatches the lower the measure. In a sense, it quantifies the ability of the model to parse all the traces in the log.

Simple behavioral appropriateness (shortly denoted by SB-Precision hereinafter), a precision measure defined in [34] , which aims at estimating the amount of the  X  X  X xtra behavior X  allowed by the model, with respect to that actually registered in the log: the more extra behavior, the lower the precision of the model. To this purpose, the possible behavior admitted by the model is quantified according to the average number of transitions that are enabled during a replay of the log X  X ndeed, an increase in this number hints some higher degree of choice and parallelism.

Advanced Behavioral Appropriateness (shortly referred to as AB-Precision henceforth), which is still a precision measure defined in [34] to express the amount of model flexibility (i.e., alternative or parallel behavior) that was not exploited to produce the real executions registered in the log. Notably, this measure is normalized by the so-called degree of model flexibility  X  X hich is 0 when just one particular sequence of the activities is admitted, and 1 when every possible sequenc-ing of them is estimated via a state space analysis of the model.
The above measures, being defined for a single workflow schema, cannot be directly applied over the tree-schemas mined since any other schema just offers some compact and approximated view over heterogeneous process instances. Moreover, in order to easy the comparison with classical process mining techniques, for each conformance measure, we report a single overall score for an entire schema decomposition, by averaging the values computed for all the leaf schemas. In more detail, the conformance values of these latter schemas are added up in a weighted way, where the weight of each schema is the fraction of original log traces that constitutes the cluster it was mined from.
 Fig. 12 illustrates the precision of the models discovered with the techniques mentioned above, for each benchmark log.
Note that, our clustering-based approach achieves outstanding results w.r.t. both precision metrics, almost independently of which basic technique is used to mine the workflow schema of each cluster. A finer grain analysis of the SB-precision mea-following non-local (non-free choice) constraints, such as a6nfc , Drivers licence , and herbstFig6p36 . Moreover, our methods outperform classical ones and, in particular, the best results are achieved when our clustering scheme is combined with Heu-classical techniques seem to go worse with a number of logs. Interestingly enough, our approach succeeds in ensuring excel-lent results not only when it encompasses the a  X  X  algorithm X  X n this case, in fact, it keeps achieving maximal precision X  X ut also when the cluster schemas are mined with HeuristicMiner .

In fact, this latter method seems to get nearly optimal results over all the logs but a7 , which actually contains parallel branches. Very good results are obtained as well by both implementations of our approach, which always outperform their respective basic versions. In particular, the method AWS -a  X  X  achieve optimal fitness against all the logs.
As a general remark, we may observe that over each possible conformance measure, both a  X  X  and HeuristicMiner tend to ports the increase (in percent) in the value of the three conformance measures that is achieved when passing from either a  X  X  or HeuristicMiner to the respective clustering-enhanced version (i.e., AWS-HN and AWS -a  X  X  , respectively). It is quite remarkable the behavior of HeuristicMiner , which suffers considerably when applied to logs involving complex routing con-seems to be well overcome when the same algorithm is combined with our approach (i.e., in the case of method AWS-HN ).
This proves that more complete and precise process models can often be discovered with our approach, where different exe-cution scenarios are modeled separately, and where some suitable kind of behavioral patterns, capable of capturing complex
SB-Precision ; on the other hand, the other two measures do not receive any substantial increase, if we exclude the only case already reached an optimal performance for both these measures.

Finally, a further series of tests were carried out to evaluate the impact of noise on the results obtained by the default instantiation of our approach (namely, method AWS-HN ). Fig. 15 refers to one of these experiments, where the benchmark of a task, swap between two tasks). More precisely, the figure reports the conformance measures obtained by the method AWS-appreciate that, even in the presence of some notable amount of noise in the input log, our approach manages to achieve very good results with all the conformance measures. 6.3. Experiments on real data
In order to assess the validity of the approach in practical application contexts, we applied it on some real log data coming from an important Italian harbor (Gioia Tauro) acting as a maritime freight hub.

The application scenario . The operational systems used in the harbor continually support and register several logistic activ-( X  X  X ranshipment X  flows), and focus on the different kinds of moves they undergo over the  X  X  X ard X , i.e., the main area used in the harbor for storage purposes, which measures about 800,000 m
The yard is logically partitioned into a finite number of bi-dimensional slots , which constitute the unitary amounts of yard organized in sectors . Conventionally, the name of a sector, say and
A _ 2 . Moreover, inside most yard sectors, multiple containers can be stocked in a single slot, by piling them up, one on top
In a sense, containers are the atomic subject of the logistic processes in the hub system. The container life cycle can be summarized as follows. The container is initially unloaded from the ship, with the help of a crane, and temporarily stocked within a zone near to the dock. Then, it is carried to some slot of the yard, which is chosen based on expectation information that concerns both the ship on which it is going to embark and the boarding time. Different kinds of vehicles can be selected for carrying the container, which are chosen mainly based on both the sector which must be reached and the distance to container. Symmetrically, at boarding time, the container is first placed in a yard area close to the dock, and then loaded on the cargo by means of a (dock) crane. This basic life cycle may be extended with a number of additional movements X  X las-sified as  X  X  X ouse-keeping X  in the jargon used in the harbor X  X hich are meant to let the container approach its final embark point, or to leave room for other containers.

Most of the operations traced in the hub systems correspond to some move performed on a container by a human oper-ator, with the help of some suitable harbor vehicle. As far as concern the experimental setting considered here, the following basic operations can be applied to any container c : MOV , when c is moved from a yard position to another by a straddle carrier; DRB , when c is moved from a yard position to another by a multi-trailer; DRG , when a multi-trailer moves to get c ; LOAD , when c is charged on a multi-trailer; DIS , when c is discharged off a multi-trailer; SHF , when c is moved upward or downward, possibly in order to switch its position with another container; OUT , when c is embarked on the ship with a dock crane.
 logistic processes. Critical performance measures are the latency time elapsed when serving a ship (where, typically, a num-ber of containers are both discharged off and charged on), and the overall costs of moving the containers around the yard. A key factor impacting on both these measures is the number of  X  X  X ouse-keeping X  moves that are applied to the containers.
Minimizing these operations is a major goal of the policies established to decide where to place a batch of containers which are coming to the harbor, based on different features of the containers, such as the kind of conveyed goods (possibly needing some refrigerating equipment), their origin and their next destination. These decisions are eventually taken by harbor man-agers with the help of sophisticated planning tools, which mainly rely on some, necessarily simplified, model of the opera-tional environment as well as on the knowledge of future events (e.g., which ships are going to take each container and when this is going to happen). Unfortunately this information is often unavailable or even incorrect, while diverse types of delays or malfunctioning are likely to occur.

The conspicuous level of complexity and unpredictability that affects the application scenario described above calls for the introduction of techniques for the ex-post analysis of yard operations, which could give some feedback for the policies discovery of workflow models can be very beneficial in this context, in order to gain a compact representation of the logistic processes captured in the log, by illustrating the flows of work that were really carried out.

Application of the approach to the harbor logs . We selected a subset of the historical data registered in the harbor systems, corresponding to the logistic operations performed on all the containers that completed their entire life cycle in the hub along the first two months of year 2006, and that were exchanged with other ports around the Mediterranean sea X  X bout 50Mb log data pertaining 5336 containers. In order to apply our analysis approach, we regarded those data according to a process-oriented perspective, by considering the transit of any container through the hub as a single enactment case of the (logistic) process under analysis.

Test (A) . Our first experimentation was focused on extracting a model describing the life cycle of the containers, in terms of the basic logistic yard operations mentioned above (i.e., antee the existence of well specified start and end activities, we also introduced two dummy activities to mark the beginning and the end of each log trace, denoted by START and END , respectively.

Fig. 16 is a screenshot of the AWS plugin, showing the schema taxonomy discovered on these data, and the workflow schema associated with the root node. This schema provides a high-level view over the logistic process, which features only three of the original basic operations, namely MOV , SHF , and fact, the remaining four operations have been abstracted into two higher-level activities, as it is shown in the bottom of the figure, where the contents of the abstraction dictionary are pictured. Note that the tool has grouped those operations related (i.e., LOAD and DIS ).

The leaves of the taxonomy are illustrated in Fig. 17 , from which it emerges the existence of two different behavioral sce-schema represents the most frequent way of handling containers, for it captures 4736 log traces out of the original 5336 ones. positions of disembarkation/embark, and to perform just a few short movements by means of straddle-carriers.
Test (B) . A second kind of experiments were conducted to gain some expressive model for the flowing of containers overly intricate or long (and hence costly) moving patterns. Different options exist for such a kind of analysis w.r.t. how to deal with the granularity of yard positions. each log trace encodes the sequence of yard sectors occupied by a single container during its stay. Fig. 18 shows three of the five leaf schemas discovered with the AWS tool from these data, which describe quite different scenarios for the movement of containers. In particular, the upmost schema (see Fig. 18 a) give some insight on the path followed by a group of containers that incurred into exceptional events, as it is witnessed by the label evidences a rather ineffective, and yet not so sporadic, pattern for the handling of containers. Similar considerations apply to the schema in Fig. 18 c, which models a subset of handling cases, where the storage of containers substantially hinged on the usage of sectors A-NWB and A-NEW , which are quite close to each other. In particular, an interesting moving pattern is revealed, indeed, for the containers passed through sector made inside that sector, while a quota of containers were even displaced to further sectors (namely, ysis performed on all the 5 clusters, with the help of classical data mining techniques, has allowed to discover that some of them are correlated with the occurrence of critical environmental conditions, as well as with differences in the modus ope-randi of some yard managers. In this way, in addition to support the analysis and tuning of the yard allocation policies, the approach contributed to foster the elicitation of some know-how, which was not yet encoded explicitly in the process models used at the harbor.

Test (C) . In our final experimentation, in order to provide an idea of the practical advantages that can come from enhanc-ing a process mining approach with abstraction mechanisms, we looked at the paths of containers over the yard, but at the level of the (nearly 100) blocks forming the yard space.

By applying the AWS plugin to these data with maxSize  X  5, k  X  3 and c  X  0 : 8, we obtained a taxonomy of workflow shown in Fig. 19 . Note that a neat difference in complexity exists between the schemas T : 0 1 and T : 1 (shown in Fig. 19 b and c, respectively) on the one hand, and the other leaf schema T : 0 0 (shown in Fig. 19 a, and about 90 nodes) on the other hand. However, a more succinct representation has been obtained for the higher-level views: the schema of node T : 0, shown in Fig. 19 d, which consists of 37 nodes, and the root schema, reported in Fig. 20 , where the number of nodes falls down to 32.

It is interesting to observe that the simplification in the description of high-level schemas is not merely syntactical. In-deed, many of the activities that have been abstracted together exhibit some sort of affinity, thereby evidencing that the algorithm went beyond the structural properties. As an evidence for such an outcome, we next report a few couples of activ-ities (i.e., yard blocks in this case) that were merged together by means of IS-A or PART-OF relationships: ( 004D , 04D ) X  X ubsequently reckoned as two different names for the same block, due to a mispelling error; ( REF01 , REF5 ) X  X wo blocks of the same sector, both equipped with refrigerating systems; ( TR5 , TRF5 ), ( TR7 , TRF7 ) X  X odes for multi-trailer vehicles registered as (mobile) positions in the hub system; ( 45D , 8D ), ( 012D , 12D ), ( A , A-2 ) X  X ach of which is a couple of blocks belonging to the same sector.
We note that, due to the high number of activity labels that come in this case scenario, whatever flat representation of the container flows, which just consisted of a single workflow schema, would have been rather cumbersome and unsuitable for analysis purposes. By contrast, the taxonomies of schemas computed with our approach have been proved to effectively en-and to the compactness of higher-level schemas. 7. Related work
Several process mining approaches have been proposed in the literature, and many of them have been already integrated in the process mining framework ProM [47] . Most of the differences among these proposals resides in the modelling features that can be used to represent a workflow model and in the specific algorithms used for discovering it. For example, in [2] , processes are intuitively represented through pure directed graphs, which allow to express precedence relationships only, while disregarding richer control flow constructs, such as concurrency, synchronization and choice. Many other proposals exploit, instead, more expressive languages, which sometimes enjoy deep formal foundation for modelling and analyzing workflow processes, as in the case of Petri-nets, used, e.g., in [45,44,46] .

In particular, in all these latter works, a special kind of Petri nets, named Workflownets (WF-net),is adopted for modellinga choice. The general problem of discovering a WF-net workflow model is specifically analyzed in [46] , where the concept of structured workflow (SWF) net is introduced to capture a class of WF-nets that a process mining algorithm should be able to rediscover.Hereanalgorithm,named a ,ispresentedwhichcanrediscoveranSWSnetoutofalog,providedthatthelogisguar-anteed to enjoy some well-specified properties. The a algorithm was extended in [13] with some pre-processing and post-pro-can explicitly capture non-free choice constructs, which are a form of implicit dependencies between the process tasks.
In [48] , a heuristic approach is presented that exploits simple metrics concerning task dependency and task frequency, in order to eventually produce a graph-based process model, called  X  X  X ependency/frequency graph X . Notably, the approach is meant to cope with the presence of noise in the logs.

A different approach to mining a process model from event logs is described in [35] , where a mining tool is presented that can discover hierarchically structured workflow processes. Such a model corresponds to an expression tree, where the leafs represent tasks (operands) while any other node is associated with a control flow operator. In this context, the mining algo-rithm mainly consists of suitable term rewriting systems.

Yet another approach is adopted in [23,24] , where a subset of the ADONIS definition language [26] is used to represent a block-structured workflow model. The peculiarity of the approach mainly resides in its capability of recognizing duplicate tasks in the control flow graph, i.e., many nodes associated with the same task. The algorithm proposed there, named  X  X  X n-
WoLvE X , solves the process mining problem in two steps: an induction step, where a stochastic activity graph (SAG) is ex-tracted out of the input log, and a transformation step, where the SAG is transformed into a block-structured workflow-model. Recently, some extensions to this approach have been proposed in [20] , in an interactive setting, where the analyst can iteratively refine the process mining results by evaluating the mined models and varying the parameters of the mining tool. After discussing a number of issues involved in interactive process mining, the authors of [20] introduce some tech-niques supporting such a setting, which primarily include a validation procedure for checking the (preliminary) mined mod-el, and a structured layout algorithm that is stable against small changes of the mined model.
 The problem of discovering a process model from execution logs is also considered in [8] , as a special case of the Maximal
Overlap Sets problem in graph matching. The paradigm of planning and scheduling by resource management is used there in order to devise an efficient approach tackling the combinatorial complexity of the problem.

The approach proposed in [14] tries to overcome the difficulty encountered by previous process mining techniques when algorithm offers a way to discover non-trivial constructs, mainly due to the global search they perform over candidate pro-cess models.

A similar motivation inspired the work in [19] , where a process mining algorithm is proposed that can account for the identification of different variants of the process at hand. The technique mainly founds on clustering log traces according to structural patterns, and eventually produce a different, specific, workflow schema for each of the discovered process vari-ants. The technique proposed in this paper shares with the one presented in [19] the basic idea of explicitly recognizing dif-ferent use cases of a process by means of a clustering process. However, three main points make different our approach from the one in [19] : (i) the core, clustering-based, mining algorithm is made able to compute a hierarchy of workflow schemas, rather than just a flat collection of workflow schemas, (ii) the algorithm is extended to accommodates the use of any arbi-trary process mining technique for equipping each node with a model, and (iii) the clustering is integrated with an abstrac-tion-based restructuring method that allows to eventually produce a taxonomy of process models that represent an articulated view of the process, at different abstraction levels. As a matter of facts, the latter feature makes our approach neatly different from any other process mining approach as well.

Taxonomical structures are widely recognized as a valuable tool for eliciting, consolidating and sharing relevant knowl-edge in disparate application contexts, which can profitably support a variety of tasks (see, e.g., [39,40,1,12,50] for some works on this topic). However, defining a taxonomy and, more generally, an ontology is quite a difficult and time-consuming task, especially when it is intended to capture the structure of a rich and complex application domain. Therefore, some ef-forts have been spent to facilitate this task, by developing automatic techniques supporting the extraction of abstract con-cepts (see, e.g., [11] for the case of tendering systems in an e-commerce scenario).

The possibility of defining taxonomies for business processes was first considered in [30] , where a repository of process descriptions is envisaged to support both design and sharing of process models. Several frameworks for precisely defining a specialization/generalization of a process model, according to some suitable behavioral semantics, have been proposed for different modelling formalisms, such as Object Behavior Diagrams [38] , UML diagrams [37] , process-algebra specifications and Petri-nets [6,43] , DataFlow diagrams [27] . On the other side, a large body of work was done with regards the transfor-mation of various kinds of schemata by means of abstraction techniques, in order to reduce their complexity. In particular, ing that all original ordering relationships among the activities are preserved.

However, we pinpoint that no substantial human intervention is required by our technique for generalizing process sche-distinguishing feature of our approach is the combination of mining and abstraction methods for automatically produce a hierarchical process model, which satisfactorily captures the behavior of the process at hand, without any pretension of being an executable workflow. Indeed, very few efforts have been paid to support some kinds of abstraction (e.g., techniques in [10,48,22] are able to produce models that focus on the main behavior as reported in the log, by properly dealing with noise).
As a final remark, we notice that, in our context, abstraction is exploited to make more compact the schemata at higher
Therefore, we do tolerate the introduction of some approximation in the control flow relationships, especially as concerns those involving abstracted activities, differently from [29] and other formal methods for dealing with process specializations [37,38,6,43,27] . 8. Conclusions
We have proposed an automatic process mining approach that is meant to discover a taxonomical model representing the analyzed process through different views, at different abstraction levels. The approach consists of several mining and abstraction techniques, which are exploited in an integrated way. In particular, a preliminary schema decomposition, accu-tured into a taxonomy, by equipping each non-leaf node with an abstract schema that generalizes all the different schemas in the corresponding subtree. The approach has been encoded in a series of algorithms, which have been implemented as a plugin for the ProM framework.

A number of issues are still open and can be subject of future work. First, the recognition of activities to be abstracted to-gether, which currently relies on simple matching functions, could benefit from the availability of background knowledge coveredprocesstaxonomycanprofitablybeexploitedtoanalyzingrelevantmeasures,suchasusagestatisticsandperformance metrics,alongthedifferentusagescenariosoftheprocessathand.Specifically,byusingataxonomyasanaggregationhierarchy for multi-dimensional OLAP analysis, it is possible to enable the user to interactively evaluate such measures over different groups of process instances. Analogously, abstract activities and their associated abstraction relationships produced during the abstraction process can be a basis for defining aggregation hierarchies. Notably, the extension of the proposed approach with OLAP features can be a valuable tool in an interactive process mining setting, like the one considered in [20] , since it mining sessions. On the other hand, the discovered taxonomies can be exploited as a basis for further knowledge discovery tasks,suchas theminingofgeneralizedassociationrulesbetween,e.g., theusersortheresourcesinvolvedintheworkflowpro-as [39] ) can effectively help recognizing interesting deviations or exceptions in the enactments of the process.
Finally, we are planning to make the approach parametric w.r.t. the algorithm used for recursively partitioning the input in the ProM framework.
 Acknowledgements
The authors thank the anonymous referees for their useful comments and suggestions, which helped improving the qual-ity of the paper. The work was partially supported by M.I.U.R. under project TOCAI.IT and by the R&amp;D.LOG Consortium under project PROMIS.
 Appendix A. Computational issues on algorithm BuildTaxonomy by procedure getBestAbstraction (i.e., the size of the set S it takes in input). Then, every computation of procedure getBestAbstraction during the restructuring of a process hierarchy for P requires O  X  m 2 n log  X  n  X  X  time.
Proof. First, we note that for any workflow schema produced in our approach (in both the mining and the restructuring most (i.e., it cannot have more than one parents).

Let us now examine the computation of all the functions defined in Sections 4.1 and 4.2 that are used in the procedure getBestAbstraction . Notice that, even though not explicitly specified in Fig. 5 , we here assume that function impl we can compute the values of function impl D incrementally, by first assigning (at the starting of algorithm an empty set to each activity in the schema X  X hich actually contains basic activities only, while the abstraction dictionary is still empty. Each time two activities m 1 and m 2 are merged into an activity p in the procedure possible to update only the implied activities of p (which might actually coincide with m impl D  X  p  X  :  X  impl D  X  m 1  X [ impl D  X  m 1  X [f m 1 g[f m
O  X  n  X  time is enough to perform the above computation, assuming that all of them are kept ordered. Function msg D can be the main computational burden involved in the evaluation of function sim Since both these sets contain at most n elements, and they are not ensured to be ordered, this computation can be done in O  X  n log  X  n  X  X  . Similar considerations apply to the case of function sim pairwise comparisons over a (fixed) number of sets (namely, P of function score (Line b1 in Fig. 5 ) for two activities can be done in O  X  n log  X  n  X  X  .

In order to compute the overall similarity score for all pairs of activities in the set S (taken as input by straction , and containing m elements at most), O  X  m 2 n log  X  n  X  X  steps are needed. In fact, this is also the complexity of implied by one activity are a subset of those implied the other one (cf. Lines b9 and b10, in the same figure). h by procedure AbstractActivities (i.e., the size of the set S it takes in input). Then, O  X  m 3 n log  X  n  X  X  time.
 main loop X  X ndeed, as long as getBestAbstraction returns a not null pair of activities, the set ActuallyAbstracted contains at least one of them (see Fig. 5 ). At each step, the most expensive computation consists in evaluating function straction , which takes O  X  m 2 n log  X  n  X  X  (see Lemma 12 ), since both procedures
Edges only requires O  X  n  X  time. As the total number of steps performed in procedure total complexity of the procedure is O  X  m 3 n log  X  n  X  X  . h
Theorem 14. Let H be a schema decomposition involving n (basic) activities. Let w be the number of schemas in H and k be its number of activities that must be abstracted in every computation of procedure Taxonomy on H correctly produces a taxonomy in O  X  w k  X  n 2  X  m 3 log  X  n  X  X  steps.

Proof. The main cost for computing the output taxonomy arises from applying procedure operations in generalizeSchemas but the k  X  1 calls to procedure of Lemma 13 , the total cost of generalizeSchemas is O  X  k  X  n 2  X  m 3 log  X  n  X  X  X  , and hence the overall cost of algorithm BuildTaxonomy is O  X  w k  X  n 2  X  m 3 log  X  n  X  X  . Finally, the correctness of of the procedure abstractActivities , each invocation of generalizeSchemas  X  returns a workflow schema which generalizes each of the workflows stored in the set ChildSchemas . h Remarks and extensions . We leave the section by noticing that the running time of depends on parameters expressing the size of the schema hierarchy being restructured, and, primarily, on the number of activities that compose the process under analysis. It is worthwhile noticing that these parameters are (usually exponen-processes X  X ike the ones our approach has been devised for X  X ypically exhibiting a number of execution paths that is com-binatorial on the number of activities. As a consequence, the application of algorithm the total computation time, which is rather mostly devoted for the preliminary mining of the schema hierarchy. Incidentally, a major source of inefficiency in algorithm decides the pair of activities to merge together, as well as the kind of abstraction to apply. In fact, the approach proposed above simply searches the pair of activities in S that achieves the highest similarity value of the function score , among all conceived to store and retrieve objects based on some proper features of them that influence the chosen similarity measure. [16,5] ), since all of the similarity measures adopted in our approach can be estimated, in an approximated way, by comput-set of features (i.e., predecessors/successors function sim
However, we believe that any further detail on these issues is somewhat beyond the scope of the paper, since any possible improvement gained through such a method will not significantly impact on the performances of the entire approach to the discovery of process taxonomy, in most practical cases, as discussed above.

References
