 Entity resolution (ER) is the process of deciding which records from one or more databases correspond to the same entities. Typically, two types of matching decisions are involved in the ER process: match (two records refer to the same entity) and non-match (two records refer to two different entities). Based on matching decisions, a clustering algorithm is often used to group records into to one entity [ 5 ]. Due to its importance in practical applications, ER has been or badly formatted), an ER result may contain errors that cannot be detected at the time of performing the ER task. Instead, such errors are often detected later If we repair these errors incrementally whenever being detected, the quality of ER can be improved over time, which would accordingly improve the quality of applications that use ER. However, in many real-life situations, repairing ER is complicated and labour-intensive. For example, we may find that two records r 3 and r into two different entities. Since there are many possible ways of splitting r r into two entity clusters, such as { r 3 , r 1 ,r 2 ,r 4 ,r and { r on all the records in the entity cluster, their match or non-match decisions, and possible inconsistencies. Such repairing tasks can become even more difficult if some constraints, such as hard matches and hard non-matches (often dictated by domain experts or users), are required to be preserved in the repairing process. For example, suppose that we have another entity cluster r know that ( r 3 ,r 8 ) is an erroneous non-match, but ( r ( r ,r 9 ) is a hard non-match. In this case, it is not easy to repair the erroneous non-matches.
 This paper aims to develop an efficient method for incrementally repairing ER. Our observation is that repairing errors in an ER result should look at how an entity was produced, e.g., matches used to determine its entity cluster over procedure and also a reasonable level of credibility for performing ER repairing, we propose to build a provenance index that allows us to trace the evidences of matches used in the ER process, and interact with the evidence of non-matches. As illustrated in Fig. 1 , such a provenance index can be built during the ER clustering process, then iteratively maintained in the repairing process. Based on this provenance index, our repairing algorithms can leverage the evidence of matches and non-matches to repair errors accurately and efficiently. Contributions. In summary, our key contributions are as follows: (1) We develop an algorithm that can solve the inconsistencies of matching deci-sions to generate entity clusters.
 (2) We design a provenance index structure which can keep track of matches (3) Based on the provenance index structure, we propose a novel approach to (4) We experimentally validate the feasibility of the proposed provenance index and database repair. Below we discuss the related works from these two areas. has studied various aspects of the ER process, such as blocking, similarity com-works incorporated constraints into similarity-based ER techniques to improve the quality of ER [ 2 , 10  X  12 ]. Nonetheless, these works focused on preventing errors in ER results, rather than repairing ER results that contain errors. which was concerned with data consistency and accuracy. Existing approaches mostly deal with data errors either by obtaining consistent answers to queries over an inconsistent database [ 1 ] or by developing automated methods to find a repair that satisfies required constraints and also  X  X inimally X  differs from the original database [ 6 ]. These approaches are often computationally expensive and not applicable to repairing ER, particularly when ER is used in applications where the processing time is critical.
 this paper we show how to use a provenance index structure to incrementally and automatically repair errors in ER results. We consider not only matches but also non-matches in the ER process. Our work not only generalizes the clustering and non-matches, but also improve the work in [ 13 ] by automating the repairing their work on ER repairing requires the manual review of suspicious matches by domain experts. Let R be a database that contains a set of relations, each having a finite set of records. An entity relation R  X   X  R has three attributes, two of which refer to records in R  X  X  R  X  } and the third refers to values in [ R  X  indicates a match ( r hard if it has | a | =1,and soft otherwise. Accordingly, there exist two kinds of ER rules: match rules and non-match rules. A match rule (resp. non-match rule ) is a function that, given R as input, generates a set of matches (resp. non-matches). Any ER algorithm that generates matches (resp. non-matches) may be considered as a match (resp. non-match) rule in our work, including pairwise similarity and machine-learning algorithms we call a rule u with | w ( u ) | =1asa hard rule .Weuse u ( r an ER rule u generates a match or non-match ( r i ,r j ).
 Given a set U of ER rules, applying U over R generates matches and non-matches in R  X  . For each pair ( r i ,r j ) of records, we use U 0 } use | X | to denote the absolute value with lower-case letters and the cardinality of a set with upper-case letters. If there exists a hard rule u ( r ,r ), then we have ( r i ,r j ,w ( u ))  X  R  X  with | w ( u ) ( r ,r ,a )  X  R  X  where the confidence value a is determined by: We have a&gt; 0if u  X  U + An entity cluster c is a set of records in R .An ER clustering C is a set { c ,...,c k } of entity clusters which are pairwise disjoint. We use C ( r )todenote the cluster of C to which a record r belongs. A clustering C is valid if, (i) for each hard match ( r i ,r j , 1)  X  R  X  , C ( r i )= C ( r j ( r ,r ,  X  1)  X  R  X  , C ( r i ) = C ( r j ).
 Users can provide feedback on erroneous matches or non-matches, and accordingly user feedback may be viewed as hard matches or hard non-matches. In this paper, we address two related problems: (1) entity clustering and (2) entity repairing .The entity clustering problem is, given a set R and non-matches, to find a valid clustering w.r.t. R  X  .The entity repairing prob-lem is, given a valid clustering C w.r.t. R  X  and a collection R to find a valid clustering C w.r.t. R  X   X  R f .Wesay C repairs C w.r.t. R We assume no conflicts among hard matching decisions because such con-flicts have to be manually resolved by a domain expert. Our focus here is to automatically resolve conflicts among soft matches and soft non-matches. We first present clustering graphs to describe how matches and non-matches are related in a graphical structure. Then we develop an efficient clustering 4.1 Clustering Graphs Conceptually, each clustering graph is a graph with labelled edges. Let L = L L
N of negative labels ,and L P has a set V of vertices where each vertex represents a record, a set E edges where each edge represents a match or non-match, and a labelling function  X  that assigns each edge e  X  E with a label  X  ( e )  X  L . For the labels in L ,we matches and non-matches represented by edges, respectively. For convenience, we use E (1) = { e | e  X  E and  X  ( e )=1 } and E (  X  1) = to refer to the subsets of edges in a clustering graph G which represent hard matches and non-matches.
 clustering C G over V such that for each edge ( r i ,r j ) r and r j are in the same entity cluster; otherwise (i.e.,  X  ( r r j are in two different entity clusters. Such an ER clustering C R  X  if every hard match (resp. non-match) in R  X  is represented by an edge with a positive (resp. negative) label in G .
 R  X  , two questions naturally arise: (1) Can we efficiently decide whether G is consistent? (2) If G is not consistent, can we efficiently generate a consistent clustering graph from G to provide a valid ER clustering w.r.t. R these questions, we will discuss two clustering algorithms. The first algorithm and, as proven by the authors, is a 3-factor approximation algorithm for the generalizes the first one to improve the efficiency by checking through cycles. 4.2 Triangle-Based Clustering (TriC) The central idea of TriC is to eliminate occurrences of inconsistent triangles (as depicted in Fig. 2 (a)) in a clustering graph. Thus, two transitive closure (TC) rules are used in the clustering process:  X  TC1 :If( r 1 ,r 2 ) and ( r 1 ,r 3 ) are two hard matches, then ( r a hard match, as depicted in Fig. 2 (b).
 must be a hard non-match, as depicted in Fig. 2 (c).
 Algorithm 1 describes the main steps of TriC. Firstly, the TC1 and TC2 rules are applied to harden all possible edges over V , including the edges that do not exist in the initial clustering graph G 2 (line 2). Then, a queue Q of soft edges is determined (line 3), which reflects the relative importance of hardening soft or as a non-match if the label is negative). Different strategies may be used to determine such a queue. We will discuss and evaluate two of such strategies in Sect. 6 . Each time when an edge is hardened, the TC1 and TC2 rules are applied again to further harden other edges if any (lines 5 X 10). In each iteration, newly hardened edges are removed from the queue (line 11). The iterations continue until the queue Q is empty. Then the algorithm returns the clustering graph G as output (line 12).
 As TriC ensures the consistency of clustering through triangles, it unavoid-ably leads to a clique over V after the clustering (i.e., for any v v = v 2 ,wehave( v 1 ,v 2 )  X  E 2 in G 2 ). The complexity of the algorithm is O ( in the worst case (i.e., it needs | V | ! 2  X  ( | V | X  3)! putationally expensive when clustering over large databases.
 4.3 Cycle-Based Clustering (CyC) To improve the efficiency of clustering, we now propose an optimized algorithm (i.e. CyC) which ensures the consistency based on cycles, instead of triangles. In a nutshell, CyC generalizes TriC by eliminating inconsistent cycles, as depicted impossible to have a cycle ( r 1 ,...,r k ,r k +1 ,...,r k + m eliminate such inconsistent cycles, two extended transitive closure rules are used in the clustering process:  X  ETC1 :Ifeach( r i ,r i +1 )for i  X  [1 ,...,k + m  X  1] is a hard match, then ( r 1 ,r k + m ) must also be a hard match, as depicted in Fig. 3 (b).  X  ETC2 :If( r k ,r k +1 ) is a hard non-match, and other edges from r are hard matches, then ( r 1 ,r k + m ) must be a hard non-match, as depicted in Fig. 3 (c).
 Q of soft edges is determined, and each of these soft edges is hardened according to their orders (lines 3 X 9). Again the queue reflects the relative importance of hardening different soft edges. Next, CyC applies the ETC1 and ETC2 rules to ensure the consistency of clustering (line 11). Newly hardened edges in each iteration are removed from the queue (line 12). The iterations continue until the queue is empty. Finally, the resulting graph G 2 is returned (line 13). which has the complexity O ( | E | X | V | ) in the worst case. It is more efficient than TriC because | E | is often much smaller than | V | 2 practice, we may simply merge records that are connected by hard matches and treat each of such merged records as a single record. In this section, we discuss an efficient provenance index structure and repairing algorithms for entities. 5.1 Provenance Index In our work, a provenance index is constituted by a number of entity trees. Each entity tree describes how the records of an entity are identified from a number subtree ( v ) to the subtree rooted at v ,and subtree ( v/v replaces subtree ( v 1 )by t 1 in subtree ( v ), where v 1 entity tree t is a binary tree with a labelling function such that (1) each leaf by parent ( v ) such that ( parent ( v ) ,v )  X  leaf ( t )for t = subtree ( v ). Entity trees are constructed from bottom up during the clustering process. In terms of Algorithm 2, hard matches are first added into an entity tree and represented by vertices that are close to leaves, and these vertices must be under vertices for soft matches (line 2 of Algorithm 2). Then each time when a soft edge ( r ,r 2 ) is hardened as a match (i.e., line 5 of Algorithm 2), a vertex v representing that match is added as the root of the corresponding entity tree t if leaf ( t ). Otherwise, we discard the match because this match has already been implicitly captured in the provenance index. Thus, entity trees enjoy two nice the importance of matches, which is determined by the queue Q in Algorithm 2, e.g., the closer a vertex is to a leaf (in terms of the number of edges between them), the more important the match represented by it is. 5.2 Entity Tree Splitting Algorithm Algorithm 3 describes the proposed splitting algorithm for repairing erroneous property of ordered vertices, the algorithm first finds the lowest common parent vertex v of r 1 and r 2 (line 1). Then the child vertices of v are split into two subtrees (lines 2 X 3). After that, to propagate the effect of splitting two child vertices of v , each vertex in the path from v to the root is checked. Depending 6 X 9). The complexity of Algorithm 3 is O ( | V | ) which is linear in the number of vertices in an entity tree t .
 5.3 Entity Tree Merging Algorithm Algorithm 4 describes the proposed merging algorithm for repairing erroneous of v in addition to v i . Given an erroneous non-match ( r the algorithm first creates an entity tree t with one internal vertex representing ( r 1 ,r 2 ) (line 1). Then for each vertex on the paths from the leaf r the vertex is added into an entity tree in T based on its closeness to r the other child vertex of the vertex (line 11). The complexity of this algorithm is O ( | V 1 | + | V 2 | ) which is linear in the number of vertices in t We have implemented our framework to evaluate how efficiently our repairing algorithms can improve the quality of ER based on the provenance indexing. 6.1 Experimental Setup Our implementation is written in Python. The experiments were performed on a Linux machine with Intel Core i7-3770 CPU at 3.40 GHz, and 8 GBytes RAM. Data Sets. We used two real-world data sets in our experiments: CORA and NCVR. The CORA data set contains 1,878 machine learning publications and is publicly available together with its ground truth 1 . The NCVR data set is a pub-lic voter registration data set from North Carolina 2 , which contains the names and addresses of over 8 million voters. Each record includes a voter registration number, providing us with the true match status of record pairs. The number of true duplicate records in the full NCVR data set is below 2 % of all records. We therefore extracted a smaller subset containing 448,134 records that includes all duplicates as well as randomly selected individuals (i.e. singleton clusters). Table 1 presents some characteristics of these two data sets.
 ER Rules. For these data sets, we used the ER rules as described in Table 2 to for evaluating our ER repair algorithms. The better quality an initial clustering has, the less repairs we would need for achieving a high-quality clustering. Thus, we used simple ER rules that can be easily setup by domain experts. To our knowledge, this work is the first report on developing automatic tech-niques for repairing errors in ER. There are no baseline techniques which we can compare our repairing approach with.
 6.2 Quality of Repairing In our experiments, we used two different strategies to determine the queue in Algorithm 2: (a) Random : i.e., randomly selecting soft edges from a clustering graph and adding them to the queue; (b) Weight : i.e., adding soft edges to the queue in terms of their weights in descending order. We simulated user feedback by randomly selecting erroneous matches or non-matches in accordance with the clustering result after each repair, and used six metrics to evaluate the quality of ER: (1) Precision , Recal l and Fmeasure , which are based on pairs of records Fmeasure (written as CC-Precision , CC-Recall and CC-Fmeasure , respectively), which compare clusterings by counting completely correct clusters [ 4 ]. data sets, although the Recal l values for NCVR are dropping slightly before going up. The CC-Precision , CC-Recall and CC-Fmeasure values also increase with increasing repairs for both data sets, and particularly repairing over time can lead to ER clustering results that are close to the ground truth. Compared with NCVR, CORA has clusters of larger sizes and its initial clustering result contains more conflicts. Thus, the difference between the random-based and weight-based strategies in CORA is also larger than in NCVR. 6.3 Efficiency of Repairing We have also evaluated the efficiency of repairing over NCVR since it contains nearly half a million records. We measured the memory usage and time resources required during the repairing process. As depicted in Fig. 6 , the weighted-based strategy is less time efficient than the random-based strategy because it requires random-based strategy, the weighted-based strategy has more uneven but gen-erally less memory usage requirement. We have developed a framework for incrementally repairing errors existing in an ER clustering. During the clustering process, we establish a provenance index for capturing how records are clustered into each entity. This index provides useful information which enables us to efficiently repair errors when they are detected later on. In the future, we plan to continue this line of research by looking into how the provenance index structure can be generalized to support repairing for entity resolution that collectively resolves entities of different types.
