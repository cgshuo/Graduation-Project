
Daehoon Kim, Seungmin Rho, Eenjun Hwang n 1. Introduction problem in many areas including computer vision, pattern analysis and surveillance. One main difficulty in the development of reliable object detection method results from the fact that objects might appear in the image under a wide range of conditions such as different viewpoints, lighting and imaging techniques ( Ullman, 1996 ). Hence, robust object detection methods should be able to handle such intra-class variations, and at the same time distinguish objects of interest from other objects. In general, such methods require huge amount of computation. Recently, with the popularity of small scale surveillance system there has been increasing demand for methods by which mobile devices such as smart phones could recognize objects in captured images. Since such mobile devices are known to have limitations such as computation power and memory capacity, existing computation-intensive object recognition methods might not be appropriate.
 recognition scheme that can be executed effectively on mobile devices. Our proposed scheme starts with learning the objects of interest using their local features. For that purpose, we carry out several pre-computations on a training set. (i) We calculate interest points for each image and then construct their local feature descriptors using SURF ( Bay et al., 2006 ). (ii) Among the interest points, we determine representative points based on statistical analysis. (iii) We also extract some extra features and thresholds from the training sets to facilitate accurate object recognition. User query is processed in a similar way. A given query image X  X  local feature descriptors are constructed and compared with the trained representative points to find all similar objects in the training set. Since our scheme treats all the objects independently, it can be used for recognizing multiple objects in the query image. Furthermore, to reduce the number of comparisons required for the object recognition, we describe how to merge similar descriptors.

The rest of the paper is organized as follows. In Section 2 we discuss the background of the problem and some related work. In
Section 3 we first describe our basic object recognition scheme and then in Section 4 we show how to improve its performance in terms of accuracy and execution time. Section 5 presents an experimental result. The conclusion and our future plans are briefly described in Section 6 . 2. Related work Object recognition accuracy can be improved in many ways. For instance, image processing methods such as de-blurring ( Chong, Tanaka, 2010 ) can be used to reduce various types of image noises or amplify certain patterns. However, we do not consider this issue in this paper. Rather, we focus on how to extract features from images and manipulate them for object recognition. So far, many different approaches to object detection have been proposed. Most of them represent images using some set of features, and then use a learning method to identify regions in the feature space that represent the object class. In this section, we discuss some of these approaches in detail. 2.1. Interest points and feature descriptors
The concept of interest points is widely used in various areas such as computer vision, stereo matching, object recognition, and image retrieval. In general, an interest point of an object repre-sents a specific point on the object around which the local image structure is rich in terms of local information content. There are several approaches to scale-invariant interest point detection, which include the Difference-of-Gaussians in SIFT (scale-invariant feature transform) proposed by Lowe (2004 ), the maximally stable extended regions (MSER) proposed by Matas et al. (2002) , the Harris-affine and Hessian-affine corners proposed by Mikolajczyk and Schmid (2004) , and the Hessians approximated using the Haar-basis proposed by Bay et al. (2006) . Mikolajczyk et al. (2005) performed a comprehensive comparison on these detectors. In addition, the Canny edge detector has been widely used in various works ( Islam et al., 2010 ).

On the other hand, a feature descriptor provides a canonical frame for such interest points and usually contains a high-dimensional feature vector for each region. Descriptors are designed to be invariant to lighting, scale, and rotational changes. Examples include the very popular SIFT and SURF descriptors. Mikolajczyk and Schmid (2005) performed a comparative study on these descriptors. Both SIFT and SURF have been successfully used in various object recognition tasks. Usually, SURF requires less computational overhead than SIFT.

There have also been many successful attempts to speed up the descriptor computation. Grabner et al. (2006) proposed to speed up SIFT computation by using integral images. There have also been many recent GPU-based methods aiming to improve the object detection performance for various purposes ( Gong et al., 2011 ). Sinha et al. (2006) described an efficient SIFT implementa-tion on a GPU. 2.2. Applications of feature descriptors
Feature descriptors have been popular in the object recogni-tion and image retrieval fields. For example, Sivic and Zisserman (2003) applied SIFT to the retrieval of key frames in video. Grauman and Darrell (2005) proposed a fast kernel-based object identification method using SIFT features. Nist er and Stew enius (2006) used hierarchical k -means to construct a search tree of local features for fast and efficient image retrieval. Ta et al. (2009) presented an algorithm for continuous image recognition and feature descriptor tracking based on SURF. They reduced the search space by using a 3D image pyramid.

Feature descriptors can be used effectively in surveillance applications. For example, Heikkil  X  a and Pietik  X  ainen (2005) devel-oped an image-mosaicking module for wide-area surveillance based on SIFT. Zhang et al. (2008) proposed a strategy for object classification by boosting different local feature descriptors in motion blobs for traffic scene surveillance. Baugh and Kokaram (2008) proposed an implicit object modeling method for visual surveillance that performs feature descriptor clustering to char-acterize objects. 2.3. Object recognition
Many different object recognition methods have been pro-posed based on the interest points of objects and their feature descriptors. For instance, Csurka et al. (2004) proposed a robust method based on the bag-of-words model, which is usually used for text document classification ( Klyuev and Yokoyama, 2010 ;
Klyuev and Oleshchuk, 2011 ; Ye et al., 2011 ). They quantized and clustered SIFT feature descriptors for each object class in order to extract a bag of key points. Sivic et al. (2005) used pLSA (probabilistic latent semantic analysis) and LDA (latent
Dirichlet allocation) to find important parts of an object. They also improved the detection accuracy by using an incremental training algorithm ( Fei-Fei et al., 2004 ). In this approach, objects are represented by a hierarchy of fragments that are extracted while learning from the training examples collected by Ullman (2007) . The fragments, which are class-specific features, are selected to deliver information for categorization. Del Bimbo et al. (2009) presented a model for recognizing generic 3D structures such as statue based on the information-theoretic notion of mutual information to quantify the saliency of descriptors.

Our proposed scheme is similar to these in that representative points and feature descriptors are used for object recognition.
However, to meet new requirements, such as the ability to perform surveillance task on the mobile devices, we both improve object recognition accuracy and reduce the computational over-head in this paper. 3. Feature-based object recognition
For efficient multi-object recognition, we first calculate the interest points of an image and then construct their local feature descriptors. Among the interest points, we select representative points based on statistical analysis. These representative points are then trained for object recognition. User queries are processed in a similar way. The local descriptors of a user query image are constructed and then compared with trained representative points of the objects in the database. If the number of matched descriptors for some object is above some pre-defined threshold, then we consider the image to contain the object. 3.1. Pre-processing stage
Representative points have been used in a similar way to ours in some previous works ( Fei-Fei et al., 2004 ; Ullman, 2007 ). Fei-Fei et al. (2004) manually classified objects for training and
Ullman (2007) automated this classification task. However, since they used very complex feature descriptors, their cost for object recognition was very high.

Fig. 1 shows the overall flow of our scheme. First, for each image in the training set, we select a set of interest points and construct their local feature descriptors using SURF. Then, through statistical analysis, we select representative points among the interest points. Intuitively, representative points of an object are interest points that deliver rich and distinguishing information about the object for recognition purposes. Agarwal et al. (2004) considered the similarity of all interest patch pairs and selected the patches whose similarity score was higher than some threshold as representative patches. Our approach for selecting representative points is quite similar to this. If an interest point has an enough number of similar interest points in terms of the SURF descriptor, we consider the interest point to be a representative point.
 a threshold for each object type from the training set. Fig. 2 shows the algorithm for calculating such thresholds. The algorithm takes the representative points as input and calculates the threshold for each object that will be used when testing whether the object appears in an image. More specifically, we first calculate the rate indicating how many interest points of each image in the training set match the representative points of all other training images.
To determine whether the points match, we used the matching algorithm provided by the SURF.
 at the cost of more false alarms. There are two compromises to dealing with this tradeoff. One is to use the minimum matching ratio of the object, and the other is to use the maximum matching ratio of the other objects. The former gives a low false alarm rate at the cost of a lower hit rate. The latter gives high hit rates at the cost of more false alarms. Surveillance applications are safer when they use more conservative approaches. That is, we need to minimize the number of missed hits, even though that incurs more false alarms. Thus, we use the latter as threshold value in our paper. 3.2. Query processing
The aforementioned pre-processing steps provide a set of representative points and thresholds for objects. A user query image can be processed in a similar way to identify the objects in the image. For each query image, we first find a set of interest points and then construct their feature descriptors using SURF.
These interest points are compared with the representative points of the objects in the training set. If the matching ratio for an object is higher than the object X  X  threshold, then we consider the query image to have the object. By comparing the interest points to representative points of all the objects in the training set, we can recognize all the trained objects in the image.

In this paper, we implemented two object recognition algo-rithms. They are shown in Figs. 3 and 4 . Although they look similar, the latter algorithm can reduce the false alarm rate by considering the average matching ratio. More specifically, the former algorithm counts the number of images in the trained set whose matching ratio is higher than the threshold. If the number is larger than the half of the object images in the training set, we consider the query image to have the object. The latter computes the average matching ratio for all the images in the training set. If the average ratio is larger than the threshold of an object, we consider this query image to contain the object. In this way, we can find out all the trained objects contained in the query image. 4. Performance improvements
In this section, we describe how to improve the performance and robustness of our object recognition scheme. To improve the robustness, we need to consider various situations in which objects appear at the training stage. For instance, we should be able to handle multiple objects with diverse backgrounds. To improve the performance, we merge similar representative points to avoid redundant comparisons during matching.
 4.1. New features from pre-processing
In some cases, the calculated representative points do not represent the object correctly. For instance, Fig. 5 shows two stop sign images with their representative points. The representative points are marked with white circles in the images. In the figure, several incorrect representative points appear on the background. As shown in Fig. 5 (a), complicated backgrounds usually produces more incorrect representative points. If we use the algorithm described in Section 3 as is, all the representative points are treated with same weight. However, as can be seen in this example, some representative points are more important than others. This can be accounted for by assigning dynamic weights to the representative points. Intuitively, we know that true representative points will appear in most images in the training set, and false representative points will appear rarely. Based on this intuition, the weight of each representative point can be defined as follows: W
Here, W p is the weight of the representative point p . Then, the weights of all the representative points of an object o can be normalized based on the average weight as follows:
AW  X 
Here P i is 1 when a representative point is matched and is 0 otherwise, and W i is the weight of representative point i. AW is an important feature for representing the properties of the object. For instance, Fig. 6 shows some of the training images. As can be seen in the figure, the motor bike images are very similar, but the Stop sign images have quite different details. That is, the motor bike image training set has more similar points than the
Stop sign image training set. Hence, the representative points in the motor bike images will have higher weights and their average weight will also increase. Thus, we can recognize differences between two objects using this feature, even if the average values of their matching ratios are accidentally similar. 4.2. Merging the representative points
Most object recognition time is occupied by SURF descriptor extraction and matching. If we have few objects to recognize, we can guarantee good performance just by using a linear search.
However, the object recognition time will increase if we need to recognize many objects at the same time, since we need much more comparisons for matching. Hence, if we can reduce the number of comparisons, we can reduce the recognition time. As mentioned before, we need to compare representative points of each object with the interest points of a query image. In practice, this requires a huge number of comparison operations. If we can merge the representative point sets of each object type, we can recognize the object using only one comparison operation. considers the merge operation. This merge process is performed before choosing the recognition threshold. Using the SURF algo-rithm, we extract 64 values for each interest point. There are four different types of values for each of 16 sub-regions ( Bay et al., 2006 ). We calculate boundaries for these types of values. First, we classify all the representative points into several sets. A classified representative point set consists of similar points that have similar SURF feature descriptors. Then we choose SURF descriptor boundaries for each type of value by using a set of classified representative points. After this operation is performed for each classified set, we finally have a merged representative point set for each object class and calculate some thresholds.
 from typical SURF descriptors in that its descriptor component represents not a single feature value but a feature range covering all merged representative points. This has the advantage that similarity computation between the descriptors of representative points in DB image and the descriptors of interest points in query image can be done very quickly. The similarity between merged SURF descriptor and a SURF descriptor is calculated as follows:
Dissimilarity  X  D , d  X   X 
Here, D and D ( i ) represent a merged SURF descriptor and its i th component, respectively. Similarly, d and d ( i ) represent a regular
SURF descriptor and its i th component, respectively. If d ( i )is above the upper bound of D ( i ) or below the lower bound of D ( i ), the dissimilarity is the square of their difference. By accumulating all dissimilarities of 64 dimensions, we can get the dissimilarity between a merged SURF descriptor and a regular SURF descriptor. 5. Results and discussion
We have implemented a prototype multi-object recognition system to evaluate the performance of our scheme. In the experiment, we compared four different schemes based on the basic recognition algorithm as follows: S1: basic algorithm S2: basic algorithm with dynamic weights S3: basic algorithm with merging S4: basic algorithm with dynamic weights and merging
We used Caltech101 ( Fei-Fei, et al. 2004 ) for the training and test image sets. This image set consists of about 8500 images and 101 classes, and each class has more than 50 images. The resolutions and displayed object sizes are diverse. We used images of four classes (stop signs, motor bikes, yin-yang symbols and faces) that were selected from the entire set of 101 classes. Our system is imple-mented using MathWorks MATLAB 2010b and the OpenCV SURF algorithm library, which was compiled to mex files. The experiment was performed on a desktop PC with an Intel Core 2 Duo 2.67 GHz processor, 4 GB of RAM and the Windows 7 Enterprise operating systems. To compare the algorithms, we measured their object recognition accuracy, execution time and scalability. 5.1. Experiment setup
As in typical learning-based methods, our object recognition method shows different recognition performance and execution time depending on the number of training images. Fig. 8 shows the recognition rates for different numbers of training images. We used 30 images per object for the training and test image sets and used S2 algorithm for object recognition. We also considered different numbers of images in the training sets, from 5 to 19 images in steps of 2. We first performed two fold cross validation to find the combination of training set images that produced the best recognition rate. We measured the recognition rates while changing the number of training images. Fig. 8 (a) and (b) show the hit rates and false alarm rates, respectively. As can be seen, the hit rates are not dependent on the number of training set images. However, for the false alarm rates, we can get a reason-able result when the number of training set images is larger than 9. On the other hand, when the number of training set image increases beyond 9, we gain little recognition accuracy improve-ment. In addition, the execution time for the recognition increases with the number of objects. For this reason, we used 10 images in the training set for an object.

The representative point selection threshold plays an impor-tant role in determining object recognition performance. In the next experiment, we measure the effect of different representa-tive point selection thresholds. In the experiment, we used 10 images for training set and the remaining 30 images for test set.
We also used S2 algorithm for object recognition. Fig. 9 shows the recall, precision and accuracy of different thresholds. As can be seen, the precision increases with increasing thresholds. On the other hand, the recall decreases with increasing thresholds. This is because with high representative point selection threshold, many interest points are not selected as representative points even though they are sufficiently qualified. Overall, selection threshold 0.3 shows highest accuracy and we use it as selection threshold in this paper. 5.2. Recognition accuracy Table 1 shows the object recognition accuracy comparisons.
Each image set has 40 images. We used 10 images in the training set and the remaining 30 as test set images. In order to choose those 10 training set images, we first performed fourfold cross validation and then selected the set with the best recognition rate. As can be seen from the table, with the basic algorithm, we can get hit rates of about 76% X 93%, and the false alarm rates of about 0% X 23%. When using the basic method only, there exist large deviations in the results. However, when we used the additional features, such as the average weight, we could achieve improved recognition accuracy. Meanwhile, the effect of using the merge method is very interesting. The false alarm ratios increased, because we lost some detailed feature information during the merging. Hit rates(%) False alarm rates(%) Recall / Precision / Accuracy 5.3. Execution time and scalability the first experiment, each image set had 40 images. We used 10 images in the training set and the remaining 30 images as the test set. We measured the execution time while increasing the number of objects from 2 to 8. Fig. 10 (a) shows the average execution time for 2 X 8 objects per image, where each object class has 30 test images. The SURF value is the elapsed time for extracting the SURF descriptors from query images, and the total value is the elapsed time for recognizing the object using the basic method. As can be seen, the time for extracting the SURF descriptors is almost the same for any number of objects, but the total recognition time increases with increasing numbers of objects. This is because the SURF descriptors are extracted only once for each query image. Fig. 10 (b) shows the total execution time for the four different schemes S1 to S4. Both S1 and S2 show increasing execution times with increasing numbers of objects.
However, the methods adopting merge operation, S3 and S4, show little difference for different numbers of objects. If we have to find more than five objects simultaneously, we should use the merge method to achieve reasonable performance.

As can be seen in the figure, our method took about 2.5 times longer to recognize 8 objects compared to the execution time of
SURF algorithm. Recently, many network IP cameras are intro-duced for surveillance purpose. AXIS IP camera and many other network cameras adopt the ARM core as their CPU. The ARM core is widely used in many other mobile devices and performs various
SURF-based applications. Hence, according to the experiment result, our proposed method is fully applicable to IP cameras for surveillance purpose. 6. Conclusions
In this paper, we proposed an efficient multi-object recognition scheme based on interest points and their local feature descriptors for surveillance. In this scheme, we first collect various sample images for a predefined set of objects of interest. For each image, we identify interest points and construct their descriptors using SURF.
Then, we choose representative points among them using statistical analysis. Finally, we perform training for recognizing objects and their representative points. A user query processing can be done in a similar manner. A given query image X  X  local feature descriptors are extracted from the query image and then compared with trained representative points of objects in the database.
Since our method treats all the objects independently, it can recognize multiple objects in the query image efficiently. To achieve reasonable performance, we added two more features: dynamic weighting and descriptor merging. We showed via experiments that our proposed scheme can achieve reasonable performance. In the future, we will investigate better methods for finding representative points and merging representative sets to improve the scalability.
 Acknowledgments
This research was supported by Basic Science Research Pro-gram through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology (2011 X 0026448) and the MKE (The Ministry of Knowledge Econ-omy), Korea, under IT/SW Creative research program supervised by the NIPA (National IT Industry Promotion Agency.) (NIPA-2011-C1820-1102-0018).
 References
