 Research on the problem of morphological disambiguation of Arabic has noted that techniques developed for lexical disambiguation in English do not easily transfer over, since the affixation present in Arabic creates a very different tag set than for English in terms of the number and complexity of tags. In addition to inflectional morphology, the Part-of-Speech (POS) tags encode more complex tokenization sequences, such as preposition + noun or noun + possessive pronoun .

However, it is less commonly noted that there are important differences between words that function as one of the closed-class category items (preposition, relative pro-noun, etc.) and those that are open-class category (noun, verb, proper nouns, etc.). Closed-class items have different frequency properties, in that systems require much less training data to learn all such words. And with regard to Arabic in particular, they are different in terms of the possible morphological affixations in that the usual collections of prefixes and suffixes do not apply to them as they do for nouns or verbs.
However, given data to analyze, tokens do not come marked as  X  X pen-class X  or  X  X losed-class X , since that classification is itself part of the analysis of the tokens. Here another aspect of distinction between the open-and closed-class categories comes into play. There is in fact very little overlap between the categories. That is, if a given input token is one that has a possible closed-class interpretation, it is very unlikely, in real data, that the actual interpretation is an open-class one. For example, the token fy in Arabic is overwhelmingly a preposition, although it can sometimes function as an abbreviation as well.

We take advantage of this lack of ambiguity to simplify the machine learning prob-lem. We use regular expressions (regexes) to express the tokenization and POS tag possibilities for all the closed-class items, and simple regexes for the open-class items that provide only the generic templates for affixation. We find that a simple baseline works quite well for the closed-class items. For the open-class items, we train a Con-ditional Random Field classifier that identifies only the stem and POS tag, but this in turn then identifies the appropriate open-class regex, which then in turn makes explicit, simultaneously, the tokenization and POS information.

This work, which extends an earlier version described in Kulick [2010], therefore occupies a middle ground between earlier work on this problem. One approach taken to this problem is to use a morphological analyzer such as BAMA [Buckwalter 2004] or SAMA [Maamouri et al. 2010] 1 , which generates a list of all possible morphological analyses for a given token. Systems such as the MADA system [Habash and Rambow 2005; Roth et al. 2008] can use the morphological analyzer by modeling separate as-pects of a solution (e.g.,  X  X as a pronominal clitic X ) and then combining them to select the most appropriate solution from among analyzer possibilities for an input token. A benefit of this approach is that by picking a single solution from the morphological analyzer, the part-of-speech and tokenization comes as a unit.

In contrast, other approaches have used a pipelined approach, with separate models to first do tokenization and then part-of-speech tagging, such as with the AMIRA system [Diab et al. 2007; Diab 2009]. While these approaches may perhaps have somewhat lower performance than the joint approach, they have the advantage that they do not rely on the presence of a full-blown morphological analyzer, which may not always be available or appropriate as the data shifts to different genres or Arabic dialects.

On the one hand, our treatment of closed-class categories has some commonalities with MADA, in that our closed-class regexes are essentially a small-scale version of a morphological analyzer. On the other hand, our treatment of open-class items has more in common with an approach such as AMIRA, in that we do not rely upon an extensive lexicon for such words. We take advantage of the lack of ambiguity between open-and closed-class items to partition the work between the two approaches. Our system is also somewhat like MADA for both open-and closed-class words in that we produce a joint solution for tokenization and POS tagging, instead of a pipelined approach as in AMIRA.

We are particularly interested in using a tokenizer/tagger as a preprocessor for parsing, and so we design our regexes to return the tokenization as represented in the treebank annotation in the Arabic Treebank (ATB) [Maamouri et al. 2010]. The result is a system that produces the tokenization appropriate for parsing, along with the corresponding  X  X ore X  part-of-speech tags. Based on an evaluation that measures both tokenization and core POS-tagging, our system actually outperforms, at least on the test section used, a system using a full morphological analyzer. However, the systems produce different forms of the tokens, and we further evaluate this difference by using each output as parsing input. The Arabic Treebank (ATB) contains two levels of annotation, which we call here the  X  X os X  and  X  X reebank X  levels. The pos level separates the source text into  X  X ource to-kens X , whitespace/punctuation-delimited strings from the source text. To avoid any ambiguity we refer to the characters in the source token as the  X  X  TEXT X  (for  X  X ource token text X ) field of the source token.

It also contains a morphological analysis for each S TEXT, usually taken from the SAMA-v3.1 morphological analyzer [Maamouri et al. 2009], which lists possible analyses for a S TEXT input. This analysis contains one or more segments for each S TEXT. Each segment includes three fields: VOC : A vocalized (including diacritics) form.
 POS : A part-of-speech tag.
 GLOSS : An English gloss.

The treebank level contains a syntactic tree for each sentence. We call the tokens used for treebank annotation the  X  X ree tokens X , where each source token corresponds to one or more tree tokens. Subsequences of segments of a source token are partitioned off into separate tree tokens, with the partition based on the POS tags for the segments so that independent syntactic units (prepositions, direct objects, etc.) are separated. Each tree token consists of one or more of the segments from the source token, with the following four fields: VOC, POS, GLOSS : Simply the VOC, POS, and GLOSS fields for the one or more source token segments that together make up this tree token.
 T TEXT (for  X  X ree token text X ): A subsequence of the S TEXT string of the source token. The concatenation of all the T TEXT values from each segment for one source token equals the S TEXT for that source token.

Two examples are shown in Tables I and II. 2 The upper part of each table shows the source token annotation for each of two source tokens, ktbh and Alktb . 3 The mid-dle part of each table shows how the source token is separated into tree tokens. For ktbh , the three segments of the source token annotation are partitioned into two tree tokens, with the pronoun forming a separate tree token and the noun and its inflec-tional material (the case marker) as another tree token. The S TEXT field ktbh is split and distributed among the two tree tokens, to form the T TEXT fields ktb and h .
The ATB does not split off a determiner as a separate tree token, although it is marked as a separate segment. So the source token Alktb has three segments, with the case marker and Al as separate segments. All three segments together form one tree token. 4 Each tree token X  X  POS tag can be considered to include a single  X  X TB core tag X , with the full POS tag for a tree token therefore consisting of a core tag together with some ad-ditional material (determiner, inflectional material). For example, in Table I, the ATB core tags for the tree tokens corresponding to ktbh are NOUN and POSS PRON 3MS. We include the determiner in the core tag, due to its importance for parsing, and so in Table II the single tree token for Alktb has the core tag DET+NOUN. 5
We are focusing in current work on using this segmentation/tagging system as a preprocessor for syntactic parsing work, which is why we are concerned with recover-ing the tree tokens corresponding to a source token. We likewise aim to recover not just the T TEXT values, but also a  X  X educed core tag X  for each tree token that can be used for parsing. We derive the reduced core tag from the full TAG pos tags for training and therefore our system output by a two-step process.
 First, inflectional material is stripped off six ATB core tags: PRON, POSS PRON, DEM, IVSUFF DO, PVSUFF DO, CVSUFF DO. 6 Second, a few ATB core tags are slightly modified, as listed in Table III. The first row shows the reduction of the di-rect objects of the three verb types to just OBJ PRON. The differentiation of object pronouns based on what kind of verb they follow is redundant and can easily be un-done simply by noticing what kind of verb an OBJ PRON follows. The collapse of NOUN.VN and ADJ.VN to NOUN and ADJ is a special case, since there are no such tags in SAMA, and the  X .VN X  on the POS tag is added to the POS value after the syntactic annotation is done, indicating that it is a NOUN or ADJ acting in a verbal context, with the VN standing for  X  X erbal noun X . Note that we are modifying only the tags. The actual lexical data is untouched. 7 Given a source token, we wish to recover (1) the tree tokens and (2) the reduced core POS tag for each tree token. For example, in Table I, given the input source token S TEXT ktbh , we wish to recover the tree tokens ktb /NOUN and h /POSS PRON, and in Table II, given the input source token S TEXT Alktb , we wish to recover the tree token Alktb /DET+NOUN.

There are 58 reduced core tags, with each tree token having exactly one such re-duced core tag. We work with the ATB3-v3.2 release of the ATB [Maamouri et al. 2010], which has 339,710 source tokens and 402,291 tree tokens, where the latter are derived from the former as discussed above. Table IV lists the 58 reduced tags we use, and their frequency among the ATB3-v3.2 tree tokens. 8 We base the open/closed-class distinction on the ATB morphological guidelines [Maamouri et al. 2009], which contains listings of all words in the various closed-class categories. 9 The words included in such lists are encoded in regexes for the closed-class items. This is a simple process, since it is roughly just importing the information in the lists in the guidelines into the regexes. These guidelines are independent of any particular section of the ATB, and of course by their various nature closed-class words occur throughout the corpus. In Section 3.3.1 we evaluate how well this actually covers the possibilities in the data section we are using.

Tags that are not included in the closed-class lists are the open-class tags, and are marked with an asterisk in Table IV. A source token is considered to have an open-class solution if any of the tree tokens in that solution have an open-class tag (i.e., the stem). For example, ktbh in Table I has an open-class solution because one of the tree tokens has an open-class tag (NOUN), even though the other is closed-class (POSS PRON).

We describe in this section how the regexes for the closed-class items contain the necessary information for analysis, and how the regexes for the open-class items en-code only the affix possibilities but are used to generate features for the classifier. Figure 1 shows two of the closed-class regexes. Each  X  X roup X  (substring unit) in a regex is assigned an internal name, and a list is maintained of the possible reduced core POS tags that can occur with that regex group. For example, the prefix w can have one of the four tags listed.

We say a source token  X  X ext-matches X  a regex if the S TEXT matches the regex regardless of the POS (i.e., the S TEXT can be split to equal the T TEXT values in the regex). We say that a source token  X  X os-matches X  a regex if it text-matches and in addition the POS tags match (for each group in the regex, the actual POS tag is in the set of possible POS tags for that group in the regex).

It is crucial for our approach that while some given single source token may text-match many regexes, it can only pos-match one regex. For example, the text wlm can text-match either REGEX #1 or #2, but when the POS tag for lm is taken into account, only one can pos-match.

As discussed more in Section 4.1, we store during training the most frequently seen pos-matching regex for a given S TEXT (which can occur in different instances in the treebank, with different POS tags, and so pos-match different regexes), and also the most frequently seen POS tag for each group within a pos-matching regex. The former is used as the baseline (and final) method of determining solutions for closed-class input words, and the latter to resolve closed-class segments within all solutions. We also encode regular expressions for the open-class source tokens, but these are simple generic templates expressing the usual affix possibilities, such as: [wf] [blk] stem poss pronoun where there is no list of possible strings for stem , but which instead can match anything. While all parts except for the stem are optional, we do not make such parts optional in a single expression. Instead, we multiply out the possibilities into different expressions with different parts (e.g., [wf]) being obligatory). The reason for this is that we give different names to the stem in each case, and this is the basis of the features for the classifier. 10
For example, Figure 2 shows four example open-class regexes, illustrating how the possibilities are multiplied out into different expressions. REGEX #3 is the simplest case, of a word that is simply a noun with no affixes. REGEX #4 is the regex for when a noun is followed by a possessive pronoun, and in this case the regex group name for the stem is stem spp to indicate that it is a stem followed by a possessive pronoun. Likewise, REGEX #5 is for the case of when the stem has a prefix, in which case the stem name is stem p 11 , and in REGEX #6 p stem spp is for the case when it has both the prefix and suffix. As with the closed-class regexes, we associate a list of possible POS tags for each named group within a regular expression, such as NOUN for the possible stems in the regexes in Figure 2.

We create features for a classifier for the open-class words as follows. Each word is run through all of the open-class regular expressions. For each expression that text matches, we make a feature which is the name of the stem part of the regular expression, along with the characters that match the stem. The stem name encodes whether there is a prefix or suffix, but does not include a POS tag. However, the source token POS-matches exactly one of the regular expressions, and the POS tag for the stem is appended to the named stem for that expression to form the gold label for training and the target for testing.

For example, Table V lists the matching regular expressions and gold labels for four words. Some sample features resulting from the text-matching regular expressions are show in column 4. For each example, there is exactly one regex that not only text-matches, but also POS-matches. The combination of the stem name in these cases together with the gold tag forms the gold label, as indicated in column 3. (1) The first, yjry , text matches the generic regular expressions for any string /NOUN, (2) Similarly, wAfAdt text-matches the same generic regular expression for any (3) The third example, ktbh , shows the different stem names that occur when there (4) The fourth example is the earlier one with the determiner. Aside from the (un-
Therefore, for each source token TEXT, the features include the ones arising from the named stems of all the regexes that text-match that TEXT, as shown in column 4, and the gold label is the appropriate stem name together with the POS tag, as shown in column 3.

We also create some  X  X erived X  features for each stem, of the usual sort, such as length, first and last two letters, etc. We show in column 5 one example derived stem, namely the one for the stem length. The name for this feature is simply the particular stem appended with _len . For example, wAfAdt has the derived features stem_len=6 and p_stem_len=5 ,aswellas stem_fl=w (first letter is w ), p_stem_fl=A (first letter with a prefix already taken off is A ).

The idea of the derived features is to simply add the information necessary to help the classifier decide which of the stem/part-of-speech combinations is appropriate. For example, the length feature helps it decide to not assign the tag PV for a stem that is two characters long, or the second letter feature can be useful information for deciding whether a stem is appropriate for an IV (imperfective verb), and so on. Essentially the derived feature is a proxy for a more serious attempt at using morphological patterns to determine the potential POS values for a stem.

We also extract a list of proper nouns from SAMA-v3.1 as a temporary proxy for a named entity list, and include a feature for a stem if that stem is in the list ( stem_in_list , p_stem_in_list ,etc.) 13
The inclusion of the DET as part of the reduced core tag requires some special handling. One possibility would have been to treat the determiner as just another possible prefix position. While this is worth exploring, for now we modified our nom-inal regexes to include both DET+NOUN and NOUN as possibilities (and similarly for DET+NOUN PROP), including special features for when the first two letters are Al . 14 This is seen in the last example, Alktb . There is another derived feature, stemDET_len=3 , not present in the other examples. This indicates that if the (poten-tial) determiner is taken into account, the length is actually 3. Likewise, all the usual derived features (first and last letter, presence in proper noun list, etc.) are duplicated for the alternative stem with DET.

We do not model separate classifiers for prefix possibilities. There is a dependency between the possibility of a prefix and the likelihood of the remaining stem, and so we focus on the likelihood of the possible stems, where the open-class regexes enumerate the possible stems. The key point for our feature system is that the gold label name itself encodes the information specifying the correct stem and the POS tag. A gold label together with the source token S TEXT maps back to a single regex, and so for a given label, the S TEXT is parsed by that regular expression, resulting in a tokenization along with list of possible POS tags for each affix group in the regex. 15
During training and testing, we run each word through all the open and closed regexes. Text-matches for an open-class regex give rise to features as just de-scribed. Also, if the word matches any closed-class regex, it receives the feature MATCHES CLOSED . During training, if the correct match for the word is one of the closed-class expressions, then the gold label is CLOSED . The classifier is used only to get so-lutions for the open-class words, although we give the classifier all the words for the sentence. The existing cases of the cross-product of the stem name and (open-class) re-duced core POS tags, plus the CLOSED tag, results in 72 labels for a Conditional Random Field classifier using Mallet [McCallum 2002].

This is pushing the edge of what is reasonable to do, and the training time increases to six days from the three if all the NOUN* and ADJ* labels are collapsed into a single  X  X oun or adjective X  tag. Nevertheless, it does converge, which it would not have done if the closed-class tags had been included in set of target labels. 16 As described above, we use regular expressions that in effect describe all the possi-bilities for determining the possible (T TEXT field, POS field) pairs for each source token S TEXT, where the POS field is for the reduced core tag. We furthermore have partitioned all tags into open and closed-class. This partition will be crucial for the complete implementation as described in Section 4.2. However, two questions arise at this point which need to be considered:  X  How complete is the coverage? that is, do the regular expressions do an adequate job of describing all the possibilities?  X  X ow many S TEXT values have open-class solutions but also match closed-class regular expressions? 3.3.1 Regular Expression Coverage. Table VI gives a breakdown of the matching of the source tokens among the various regular expressions, split into the training and dev/test sections that we use (as detailed more in Section 4.2). There are very few cases (281) that are not handled by one of the regular expressions. Spot-checking these 281 cases, most are simply cases that mistakenly got left out of the regular expressions. For example, 30 cases are those in which the correct solution has a NOUN PROP and a possessive pronoun, a combination not included in our regular expressions. Some are oddities in the treebank that fall outside of the normal behavior of source tokens (in particular, missing whitespace in the source text that causes one source token to ac-tually result in two full words, with a NOUN+NOUN solution, which is not accounted for in our open-class regexes).

In the evaluation in the next section, the 19 cases in the Test section are all misses for scoring our tagger, since there is simply no hope of getting the correct solution. This is because the correct solution (e.g., NOUN PROP+POSS PRON or NOUN+NOUN) is not even a potential target label for the classifier. In future work, clear holes on our part (e.g., NOUN PROP+POSS PRON) will be included, although we will probably not want to model oddities such as the NOUN+NOUN case, preferring to take the very minimal hit on performance.

We note that the regular expressions for both the closed-and open-class items were easy to construct, and independent of any particular ATB section. The closed-class information was taken from the ATB guidelines, which is not tied to any particular section of the ATB, and the open-class regexes are simply the standard affix possibili-ties for Arabic. 3.3.2 Open/Closed Class Ambiguity. As mentioned above, and described in more detail in the next section, the distinction between open and closed-class solutions plays a crucial role not just in the design of the regular expressions, but in the actual operation of the tagger. Since priority is sometimes given to looking for a closed-class expression, it is important to know how often it is the case that the solution for a source token is an open-class regular expression, but the S TEXT values matches both open and closed-class regular expressions.

In the dev and test sections considered together, there are only 305 such cases. In terms of POS tags, many of these such cases (109 overall) are, as might be expected, NOUN PROP or ABBREV. For example, there are 15 cases in the DevTest data of y , which is normally the PREP fiy (closed-class), being used as an abbreviation (with tag ABBREV) (open-class). Of course the closed-class reading is by far the more prevalent, but due to the way our tagger works, as described in Section 4.2, we are guaranteed to get such unusual cases wrong.

This is the tradeoff we are making. Our entire approach relies on there being little ambiguity between open-and closed-class words. A given input word is run through the closed-class regexes, and this is used to determine whether it should be treated as an open-or closed-class word. This allows us to create the CRF for joint tokeniza-tion/tagging, as well as to take advantage of the very high baseline for the closed-class items. However, it is sometimes the case that this will give the wrong result, namely when an input word matches a closed-class regex but yet should have an open-class solution.

What we found, exploring this approach in this work, is that there is so little overlap between the open and closed-class regular expressions, that it pays to take the minor hit on those cases where they do overlap. In a sense, this is not surprising, since it is difficult to see how language could be processed if it was not the case. However, we feel that it is still interesting and worthwhile to see quantitively just how sharp this division is, especially since we can use it to our advantage for the tagger. We worked with ATB3-v3.2, following the training/dev/test split in [Roth et al. 2008] on a previous release of the same data. To provide a proper comparison, we run our system only on the test section, effectively ignoring the dev section, since we do not do any tuning of the data, for which Roth et al. [2008] reserve the dev section.
We keep a listing (which we refer to as the  X  X  text-solution X  list) of all (S TEXT, solution) pairs seen during training. For an open-class word,  X  X olution X  is the gold label as described in Section 3. For a closed-class word,  X  X olution X  is the name of the single POS-matching regex. In addition, for every regex seen during training that POS-matches some S TEXT, we keep a listing (which we refer to as the  X  X egex-group-tag X  list) of all ((regex-group-name, T TEXT), POS-tag) tuples. We use the information in List  X  X  text-solution X  to choose a solution for source tokens seen in training. For the Baseline and Run priority-stored, we use this for all source tokens seen in training, while in Run priority-classifier, only for source tokens that text-match a closed-class expression. We use the  X  X egex-group-tag X  list to disambiguate all remaining cases of POS ambiguity, wherever a solution comes from.

For example, if wlm is seen during testing, List  X  X  text-solution X  will be consulted to find the most common solution (REGEX #1 or #2 from Figure 1), and in either case, List  X  X egex-group-tag X  will be consulted to determine the most frequent tag for the T TEXT w as a prefix. While there is certainly room for improvement here, this works well since the tags for the affixes do not vary much.

With four possible sources of solutions (stored from training, random choice from text-matching open and closed class lists, and Mallet output), we can choose the out-put in different ways, which we do for three different runs. We report here results from three such selections of solutions, as described for the Baseline and Run Priority-Stored and Run Priority-Classifier in Figure 3, with the results shown in Table VII.
For all runs, we score the solution for a source token instance as correct for tok-enization if it exactly matches the T TEXT split for the tree tokens derived from that source token instance in the ATB. It is correct for POS if correct for tokenization and if each tree token has the same POS tag as the reduced core tag for that tree token in the ATB. The tokenization/POS score (99.8%/95.2%) in the Baseline and Run priority-stored for the words seen during training (88.4% of the words (22386/25305)), is already quite high. As expected, almost all (except 23) instances of closed-class words were seen dur-ing training. The score for words not seen in training in the Baseline is horrendous, but goes up significantly with the use of the classifier in Run Priority-Stored. This run pro-duces our best tokenization score, 99.3%, with 93.5% POS. For Run Priority-Classifier, the classifier score is significantly higher than in Run Priority-Stored (99.0%/92.2% compared to 95.6%/80.2%), almost certainly because the score is now including results on words that were seen during training. The overall score with Run Priority-Classifier is just slightly lower that of Run priority-stored for tokenization.

We do not consider the difference in results between Runs A and B to be very sig-nificant, although we prefer the approach in Run Priority-Classifier since it relies less upon memorization of the open-class items. 17 The crucial point however is that both Run Priority-Stored and Run Priority-Classifier are significant improvements over the baseline, and both Run Priority-Stored and Run Priority-Classifier rely on the distinc-tion between open-and closed-class items that is at the core of this work. Run Priority-Classifier shows in part that the baseline for the closed-class items is already so high, once they have been separated, that the vast bulk of the work concerns the open-class items. It also shows that of the 7933 tokens that match a closed-class expression, 99.7% (7910/7933) had been seen in the training section. It will be interesting in fu-ture work to see how this varies across other treebank sections and general test data. We expect much less variation in the closed-class than the open-class information.
Two facts are immediately apparent with an error analysis. Seven percent of all the tokenization errors concern the single source token kmA , which can receive the analysis k/PREP + mA/(REL PRON or SUB CONJ) or kmA/CONJ . This in fact seems to be the only serious case of tokenization ambiguity for an input source token that matches a closed-class expression, and might well be worthy of particular attention in future work. For POS tagging, many of the errors relate to NOUN/NOUN PROP ambiguity, particularly with multi-word names, and better use of a proper noun list should help. Comparison with previously published work is difficult due to differing evaluation techniques, data sets, and POS tag sets. In particular, both Habash and Rambow [2005] and Diab et al. [2007] assume gold tokenization for evaluation of POS results, whichwedonot. 18 We therefore compare our results by running a recent version of MADA [Habash and Rambow 2010], MADA 3.0, as described in Roth et al. [2008]. This system is trained on the the same train/dev/test split of a slightly earlier version of the same corpus 19 that we used. We evaluate their system by running it on the same test split 20 , and performing the same POS tag modifications, and splitting it into tree tokens. A slight complication in evaluation still arises because the two systems pro-duce different forms of the token text. We produce what we called the T TEXT value, while their system produces the VOC form. To simplify the evaluation, we scored a MADA result as being correct for tokenization simply if it has the correct number of tree tokens for a given S TEXT input, without checking the actual content of the tree token text. This likely inflates the score slightly. 21 As can be seen, our system outper-forms MADA on both tokenization (99.3% vs. 99.0% and combined tokenization/POS (93.5% vs. 92.0%).

Of course, this test is very limited and should be run on other data (perhaps other segments of the ATB) to measure robustness. And it bears repeating that the data set is not exactly the same (ATB3-v3.1 vs. ATB3-v3.2). Also, MADA produces more output than our system does, including the vocalized form and a lemma for each token 22 , and this is unquestionably useful and even critical information to have for certain applications. However, these results do show that the closed-class and open-class items lend themselves to very different solutions, and it is possible to use a sequence tagger for simultaneous POS tagging and tokenization, as long as the problem is simplified by removal of closed-class items from the model. 23
There are some further possibilities that one can imagine for combining our work with MADA and SAMA. For example, there is no reason why we need to use the CRF classifier we have designed for the open-class items. We could instead plug in the results from MADA for such items. We could also use the results of our system as they are to at least narrow down the range of possible solutions from SAMA for a given input item, and therefore also end up with a complete SAMA solution. As discussed above, our aim is to use the output of this tokenizer/tagger as input for a parser, and so we conclude this evaluation with a brief report on using it for this purpose. 24 The tagger training/dev/test split proposed in Zitouni et al. [2006] and followed by Roth et al. [2008] and in this work is unfortunately different than the split of ATB3 earlier proposed for parsing work 25 , and the tagger training section overlaps with the parsing dev section, although not with the parsing test section, and so we utilize the tagger output only for the test section.

The results are shown in Table VIII. As noted in footnote 4, the data is available in an UNVOC version, which is somewhat different than the T TEXT form. Since the former is simply the VOC version with diacritics stripped out [Kulick et al. 2010], it contains normalizations present in the SAMA solutions. Earlier parsing work, such as Bikel [2004] has utilized the UNVOC form. The MADA output is able to supply the UNVOC form, simply by stripping the diacritics from its VOC output, while our system provides the T TEXT form.

Run gold-VN uses gold tags with inflectional material stripped out, nearly iden-tical to that described here, except that the .VN is not removed from ADJ.VN and NOUN.VN. Mode 1 allows the parser to choose its own tags, while Mode 2 forces the parser to use the given tags. As expected, the UNVOC version performs somewhat bet-ter than the T TEXT version, probably because of its normalization of the data. Run gold-noVN removes the .VN part of the ADJ.VN and NOUN.VN tags, which decreases the score somewhat, since the .VN tag is encoding significant syntactic information. Finally, Run tagger uses the the MADA output for the UNVOC form, and our system for the T TEXT form. Due to the non-gold tokenization, we cannot use evalb, and so we evaluate using Sparseval [Roark et al. 2006]. 26
Run tagger reverses the trend of Run1 and Run2 in two ways. First, the T TEXT score is better than the UNVOC (75.4/72.7 vs. 74.8/72.0). This is likely due to the im-proved tokenization resulting from our system, which overcomes the usual disadvan-tage the T TEXT system faces. Second, the score for the two systems in Run tagger is actually better when the parser can select its own tags, rather than being forced to use the given ones (74.8 vs. 72.0, and 75.4 vs. 72.7). We suspect that the parser is in correcting for POS errors on the closed-class items, although this requires more analysis in future work.

It is worth briefly comparing this approach with the only other Arabic parsing work we know of that uses non-gold tokenization and tags [Green and Manning 2010]. They use lattice parsing for a joint segmentation and parsing model by including all the tok-enization options in the parse chart, with some pruning based on  X  X  hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines. X  They compare this ap-proach to one using MADA as a preprocessor for parsing, similar to our experiment here, although the data set and parsing results are not directly comparable. They find that the joint model gives somewhat lower results for tokenization and parsing com-pared to using MADA as a preprocessor for parsing, but that while MADA relies on a manually constructed dictionary, the lattice parser  X  X equires no linguistic resources X . Our pipeline here offers an alternative possibility, There is some cost paid for the cre-ation and maintenance of the limited lexicon (for the closed-class regular expressions), although not nearly as much as with a complete analyzer such as SAMA. On the other hand, there is no loss of tokenization and POS-tagging accuracy, and even some im-provement, although our work also does not require an extensive manually constructed dictionary. In this work we have described an approach to simultaneous tokenization and part-of-speech tagging that relies on distinguishing the closed and open-class items, and focusing on the likelihood of the possible stems of the open-class words. While the work here focuses on Arabic, the distinction between open and closed-class items seems to be universal, and this can perhaps be taken advantage of for work on other languages as well. 27 Indeed, to some extent this has always been the case, since one of the reasons why POS taggers for English have high baselines is because many of the closed-class items come for  X  X ree X  (e.g., the as a determiner). This work has, we think, adapted this insight for Arabic in a novel way.

Future work includes following up on comments at the end of Section 4.2 for im-proving tokenization, and more serious analysis of the closed-class items, which are still at the simple baseline. Other future work includes recovering the complete in-flectional information that we have ignored here. One possibility for doing this is to follow MADA in selecting a solution from SAMA, choosing the nearest match to our tokenized/core-POS solution. However, we are also interested in trying the current approach for work with Arabic dialects, for which a full-blown morphological analyzer such as SAMA is not available. We are also eager to follow up on the possibility men-tioned in footnote 16 for recovering some of the more fine-grained tags as a joint model with a chunking phase.

