 Controlled and reproducible laboratory experiments, enabled by reusable test collections, represent a well-established meth-odology in modern information retrieval research. In order to confidently draw conclusions about the performance of different retrieval methods using test collections, their re-liability and trustworthiness must first be established. Al-though such studies have been performed for ad hoc test col-lections, currently available resources for evaluating question answering systems have not been similarly analyzed. This study evaluates the quality of answer patterns and lists of relevant documents currently employed in automatic ques-tion answering evaluation, and concludes that they are not suitable for post-hoc experimentation. These resources, cre-ated from runs submitted by TREC QA track participants, do not produce fair and reliable assessments of systems that did not participate in the original evaluations. Potential solutions for addressing this evaluation gap and their short-comings are discussed.
 Categories and Subject Descriptors: H.3.4 [Informa-tion Storage and Retrieval]: Systems and Software X  Per-formance evaluation General Terms: Measurement, Experimentation Keywords: question answering, pooling
The use of test collections to assess the performance of in-formation retrieval systems is a well-established methodol-ogy, dating back to the Cranfield experiments in the 60 X  X  [5]. Test collections enable the effectiveness of different retrieval methods to be compared without human involvement to as-sess the relevance of the returned documents. Thus, they al-low information retrieval experiments to be conducted with rapid turnaround in a controlled laboratory setting.
In the past decade or so, large test collections for the so-called ad hoc task have been created by pooling the ranked Copyright 2005 ACM 1-59593-034-5/05/0008 ... $ 5.00. lists returned by participants in large-scale evaluations such as TREC. With pooling, the top n documents (the pool depth ) from each run are gathered, and after removing du-plicates, are presented to human assessors for evaluation. These judgments are then used to evaluate the complete ranked lists of all runs.

Previous work [17] has shown that the pooling methodol-ogy creates trustworthy and reliable resources for post-hoc experimentation, i.e., pooled judgments can accurately as-sess the retrieval performance of systems that did not par-ticipate in the original evaluation. Researchers have probed other aspects of the TREC test collections, including the ef-fect of topic size [15], the effect of incomplete judgments [3], the effect of different evaluation metrics [2], and the effect of different notions of relevance [12, 13, 11]. In general, these studies have confirmed the reliability of using TREC test collections as a laboratory tool for measuring the effective-ness of different retrieval methods.

In the past few years, question answering has emerged as an active area of research. Combining traditional informa-tion retrieval and natural language processing technologies, question answering systems aim to directly return answers to questions posed in everyday language such as  X  X ow old was Nolan Ryan when he retired? X , instead of returning a ranked list of documents that a user must then manually examine. Although the community is moving towards more difficult questions such as ones that require reasoning, these so-called  X  X actoid X  questions, which can be answered by a named-entity or a short noun phrase, remain a staple of question answering research.

Since 1999, the NIST-organized question answering tracks at TREC (see, for example, [14]) have served as a locus of research in the field, providing an annual forum for the eval-uation of diverse systems fielded by teams from all over the world (the model has been duplicated and elaborated on by CLEF in Europe and NTCIR in Asia, which have also introduced cross-language elements in question answering). Drawing lessons from other TREC efforts, the need for a question answering test collection has been identified since the earliest days of the TREC QA tracks [16]. However, to date, a truly reusable test collection still does not exist. Each year, NIST releases evaluation resources intended to guide the development of future question answering systems. However, these resources have been used in ways they were never intended for, e.g., to directly assess the accuracy of question answering systems and to make comparative judg-ments about the performance of different question answering techniques. Such results are potentially misleading because the reliability and trustworthiness of measures derived from these resources in post-hoc experiments have not been estab-lished. To date, no researcher has analyzed existing question answering evaluation resources in the same way that ad hoc test collections have been dissected.

This study presents an evaluation of presently available resources for evaluating factoid question answering. Specifi-cally, the following questions will be addressed: Why is cre-ating a reusable test collection for question answering so difficult? Can existing resources be employed to measure the performance of new question answering techniques, i.e., can they be used for post-hoc experimentation? Results will demonstrate that presently available resources do not produce reliable scores and should not be used to evaluate systems that did not participate in the original TREC eval-uations. In addition to a detailed analysis of present prac-tices, this paper will discuss potential solutions for building reusable question answering test collections in the future.
In the TREC instantiation of the question answering task, a system response to a natural language question is a pair consisting of an answer string and a supporting document (from which the answer string was extracted). To this re-sponse unit, a human assessor assigns one of four labels: correct , inexact , unsupported , or wrong . In order for a re-sponse unit to be judged as correct , the answer string must contain exactly the information requested by the question and the supporting document must present the answer in a manner such that a human reading the document could verify its correctness. If the answer string contains extra-neous words, the entire response is judged inexact . If the answer string is correct (i.e., exact), but the document does not clearly answer the question, an unsupported label is as-signed. Otherwise, the response unit is wrong . In a typical QA track at TREC, participating systems are given four to five hundred factoid questions and are allowed to return one response per question. Performance is measured by answer accuracy (precision), i.e., the fraction of the questions that were correctly answered by a system.

Question answering test collections are difficult to con-struct for two reasons. Because judgments are made with respect to both an answer string and a particular document, identical answer strings may be correct or unsupported, de-pending on the specific document from which that string came. This issue is compounded by the fact that there is no such thing as a  X  X anonical answer form X , akin to the do-cid in ad hoc retrieval. Small changes in the answer string might affect the judgment value, and there is (currently) no algorithmic way to determine whether a change between a judged string and a string to be evaluated is significant or not. 1 Thus, human assessors are needed to deal with intrica-cies such as exactness, answer granularity, and acceptability of variants.

Despite these challenges, NIST has been compelled to pro-vide resources to guide future system development. Each year, answer patterns in the form of regular expressions are created by hand, and a list of relevant documents containing these answers (the reldocs list) are compiled by pooling runs submitted by the participants. In addition, NIST provides I must thank Ellen Voorhees for pointing this out. Figure 1: Histogram of questions (in terms of frac-tion of the entire testset) binned by number of rel-evant documents for TREC 2002 and TREC 2003. a scoring script that matches the answer patterns against system responses.

Although TREC organizers never intended for the answer patterns and reldocs lists to be used as a test collection, they nevertheless have X  X esults based thereon, for example, com-paring the relative effectiveness of different techniques, have been widely reported in the literature. Typically, two mea-sures of answer accuracy, strict and lenient , are presented. Under the strict measure, a response is considered correct only if the answer string matches the answer patterns and its supporting document is among those marked as relevant (from the reldocs list). Under the lenient measure, the sup-porting document is ignored, and response units are scored only on the answer strings.

It is well-known that the automatically-generated strict measure underestimates answer accuracy because document-level relevance judgments are incomplete: an acceptable an-swer may be judged incorrect simply because its supporting document does not appear in the reldocs list. On the other hand, the lenient measure overestimates performance be-cause documents often coincidentally contain the correct an-swer string without actually answering the question. How-ever, it is implicitly assumed that the combination of the two different evaluation criteria would closely approximate true question answering effectiveness. However, no one to date has rigorously verified this assumption, and indeed, this study will reveal that TREC answer patterns and reldocs lists are not suitable for post-hoc experimentation. Before delving into details, the relation of this work to previous meta-evaluations of ad hoc retrieval will be surveyed.
Existing resources for automatic question answering eval-uation were essentially created by employing the pooling methodology. Fortunately, aspects of this process as ap-plied to ad hoc retrieval have been well studied. Although Zobel [17] demonstrated that a pool depth of one hundred documents produces a fair ranking of systems, he started noticing adverse effects on system rankings when the pool size was reduced below fifty documents. In the current setup of the question answering task, the pool depth is one, because each system is only allowed to return one re-sponse per question. This result alone should raise suspi-Figure 2: Artificially created runs with a known fraction of correct answers, plotted against automatically-derived strict answer accuracy (refer-ence line has a slope of one). cions regarding the reliability of these resources, considering that Buckley and Voorhees [3] have shown test collections not to be robust with respect to massively incomplete rele-vance judgments. For the TREC 2002 testset, the list of known relevant documents is quite small, averaging 3.95 relevant documents per question (  X  =4.07, max=23); rel-docs for the TREC 2003 testset average 3.90 documents per question (  X  =3.84, max=25). A histogram of questions from both testsets binned by their count of relevant documents is shown in Figure 1. 2 A casual examination of the corpus reveals many relevant documents that are not in the offi-cial reldocs list; the danger here is that systems may not be properly rewarded for extracting answers from these doc-uments (if support is considered a necessary condition for successfully answering a question).

Two other reasons for the soundness of the pooling strat-egy in ad hoc retrieval are that participants contributing to the pool employ a relatively diverse set of strategies and that they achieve reasonable performance. Unfortunately, the same cannot be said of question answering systems. In the most recent QA track at TREC 2004, Ellen Voorhees re-ported in her overview talk that the median score for 92 . 2% of factoid questions was zero. On the diversity front, ques-tion answering systems fare no better: because many teams focus only on the answer extraction aspect of the task, they simply use the ranked list of documents supplied by the PRISE system, made available by NIST. By Monz X  X  [10] count, 10 out of 36 participants (28%) used only these doc-uments in TREC 2001. For TREC 2002, the number was 7 out of 34 participants, or 21%. Naturally, this reduces the diversity of documents that contribute to the pool.
To quantify the effect of missing document relevance judg-ments, a  X  X erfect X  run was created by a human for the first one hundred questions from the TREC 2002 testset. Since a human manually extracted answers for each question from the document collection, the run should achieve an accu-racy of 1 . 0 when automatically evaluated with existing re-sources. 3 This, unfortunately, is not the case: the human-
Since the TREC 2002 testset has 500 factoid questions, while the TREC 2003 testset has only 413 factoid questions, the number of questions has been normalized as a fraction. modulo variations caused by differences in opinion generated run achieved a strict accuracy of only 0 . 53 using current evaluation resources.

Given this human-generated run, one can vary the per-centage of correct answers and observe changes in the auto-matically-derived strict score. The one hundred questions in this set were ordered from  X  X asiest X  to  X  X ardest X , based on the number of participating systems that answered the ques-tion correctly. From the human-generated run, 101 different  X  X ake X  runs were produced, each with a known number of correct answers X  X ampled in order of increasing difficulty, such that easier questions were always chosen first. Each of these runs were automatically evaluated; the resulting strict score plotted against the fraction of known correct answers is shown in Figure 2. With an  X  X deal X  set of judgments, this graph should be a straight line with a slope of one through the origin, as shown by the reference line. However, this is clearly not the case X  X e can see that existing evaluation resources underestimate answer accuracy.

In the following sections, quantitative evidence cautioning against using existing evaluation resources for post-hoc ex-perimentation will be presented. Note that within the con-text of a single evaluation, the reliability of human-generated scores have been verified to within a known margin of error (see, for example, [14]). That is, comparisons between dif-ferent submitted runs are trustworthy and valid.
To assess the reliability of available answer patterns and reldocs lists, they were employed to assess runs that were actually submitted to TREC. For the 500 factoid questions in the TREC 2002 testset, the Kendall X  X   X  correlation be-tween the official system rankings and the system rankings produced using the automatically-generated strict measure (matching both answer patterns and reldocs) is 0 . 820; for the lenient measure (matching answer patterns only), the corre-lation is 0 . 817. Kendall X  X   X  computes the  X  X istance X  between two rankings as the minimum number of pairwise adjacent swaps necessary to convert one ranking into the other. The correlation is above the 0 . 8 threshold generally considered  X  X ood X , but less than the 0 . 9 threshold NIST aims for. For the TREC 2003 testset (considering only the 413 factoid questions), the Kendall X  X   X  correlation between the official score and the strict measure is 0 . 883, and 0 . 807 between the official score and the lenient measure. Figure 3 shows the official score for each run (answer accuracy), along with the strict and lenient automatically-generated scores for both TREC 2002 and TREC 2003, arranged by descending offi-cial score. 4
What exactly do the above correlations mean? The Ken-dall X  X   X  gives us an idea about the prevalence of rank swaps, instances where different evaluation conditions give differ-ent conclusions about which run out of a pair is better. However, there is measurement error associated with any evaluation, so rank swaps between runs whose official scores differ by less than this error margin are inconsequential. For TREC 2002, Voorhees [14] determined this score difference to be 0 . 05, i.e., in order to confidently conclude that one
The official score is sometimes higher than both the strict and lenient automatically-derived scores because the NIST-supplied scoring script does not correctly handle questions in which there are no known answers in the corpus; this, however, does not affect the results of this study and the conclusions drawn therefrom. labeled 25 has 0 . 02  X   X &lt; 0 . 025 . run is better than another, there must be an absolute score difference  X  of at least 0 . 05. 5 Figure 4 shows histograms of observed rank swaps (under the automatically-generated strict measure) binned by their differences in official score. For TREC 2002, 192 rank swaps were observed, out of 2211 pairwise comparisons (67 runs); of those, 38 rank swaps oc-curred when  X  was greater than 0 . 05. For TREC 2003, 154 rank swaps were observed, out of 2275 pairwise comparisons (75 runs); of those, 13 rank swaps occurred when  X  was greater than 0 . 05. It appears that resources from TREC 2003 replicate the official judgments better; nevertheless, it is noted that existing resources are not completely reliable, even when applied to submitted runs.

These results highlight the difficulty in crafting a good set of answer patterns and the problems associated with treat-ing answers and supporting documents independently (re-call that in the manual assessment process, they are always considered as a single unit). Answer patterns are often too
The same analysis has not been performed for TREC 2003, but it is assumed that the figure is comparable. restrictive, i.e., they do not accept correct answers, and at the same time, not sufficiently restrictive, i.e., they accept answers that are not supported. An often-observed mode of failure is the inability of answer patterns to weed out answer strings contained in documents that do not directly answer the question. Consider the following example:
Although Jack Johnson was indeed the first black heavy-weight champion, the document does not support the an-swer, i.e., it does not state that he was the first one. Thus, the above response would be marked unsupported . In many cases, the lenient measure greatly overestimates answer ac-curacy when considering support, particularly for a class of the precision drop for TREC 2002 (left) and TREC 2003 (right). systems that extracts answers from the World Wide Web, and then  X  X rojects X  these answers onto the corpus used by the TREC evaluations [1, 7]. Lin et al. [7], for example, re-ported that their system could not find relevant supporting documents for approximately a quarter of otherwise correct answer strings. Thus, for certain classes of question answer-ing techniques, the discrepancy between official and lenient scores can be quite large.

The lenient measure is also a poor indicator of perfor-mance if one views question answering as an extension of finer-grained document retrieval techniques (i.e., the logi-cal extreme of passage retrieval). Under such a view, the importance of the supporting document is undeniable. Fur-thermore, user studies suggest that answers to factoid ques-tions should be presented in context [9]; this finding further highlights the importance of supporting documents, the cor-rectness of which is not captured by the lenient measure.
The previous experiment employed available resources to assess the performance of systems that participated in the TREC QA tracks. It attempted to answer the question: Can the answer patterns and reldocs lists duplicate human judgments on submitted TREC runs? The Kendall X  X   X  val-ues and analysis of rank swaps appear to suggest a tentative yes  X  X lthough caution is warranted because a number of sig-nificant rank swaps were still observed. What about use of these resources for post-hoc evaluation, i.e., to score systems that were not TREC participants? This section presents two experiments that explores this question, whose answers ap-pears to be no .
The  X  X ake-one-out X  experiment was designed to simulate the effect of using the answer patterns and the reldocs to score a run that was not evaluated by TREC assessors; the methodology is similar to the study conducted by Zobel [17]. For each run in a particular year X  X  evaluation, a new reduced set of relevant documents was created by removing the con-tributions of that run. 6 This resulted in 35 variant reldocs
In actuality, multiple runs from the same organization were removed together because they tended to return very similar documents. lists for TREC 2002 and 33 variant reldocs lists for TREC 2003, one with contributions from each run removed. Each of these judgment sets were then used to evaluate the run whose contributions were removed, simulating the effect of post-hoc evaluation. Ideally, the score of a run evaluated us-ing the reduced judgments should be very close to the score generated using the complete set of judgments. This would indicate that the score of a run is invariant with respect to its participation in TREC, and hence these resources can be used for post-hoc evaluation. Zobel discovered exactly this result for TREC ad hoc test collections.

The results of this experiment, however, present a dif-ferent story. Scores generated with the reduced judgments were invariably lower than scores generated with the com-plete set of judgments, indicating that existing resources underestimate the performance of systems that did not par-ticipate in the original TREC evaluations. Figure 5 shows scatter plots of the difference in precision between the two evaluation conditions against the original score. The higher the original score, the more the answer accuracy is under-estimated by available resources, which is consistent with Figure 2.

In Figure 6, the strict scores with the original and reduced judgments are shown as a bar graph, in order of descend-ing original strict scores (TREC 2002 on left, TREC 2003 on right). As an example, the top scoring run from TREC 2003 achieved a precision of 0.649; removing its contribu-tions from the pool drops the score to 0.446, nearly thirty percent lower (an absolute difference of twenty percentage points). Without its contributions to the pool, the top scor-ing run would now be ranked seventh. Such flips in relative rankings suggest that a run evaluated with existing resources should not be compared with an official TREC run.
The previous experiment demonstrated that presently a-vailable resources for question answering evaluation under-estimate answer accuracy when used in post-hoc experimen-tation. Specifically, a new run cannot be reliably compared to an existing run. What about two fresh runs? If the re-sources underestimate performance in a consistent manner, might they be reliable when used to compare two runs that were not originally part of TREC?
The  X  X ake-two-out X  experiment was devised to answer this (right). Bin units are thousandths. question. For every pairwise comparison between runs from TREC 2002 and TREC 2003, a reduced judgment set with contributions removed from both runs was created. 7 This simulates the effect of using the resources to score two new runs that did not participate in the original evaluation. For TREC 2002, 230 rank swaps were observed (out of 2211 pairwise comparisons with 67 runs); in 48 of those, the dif-ference in official score was greater than 0 . 05. For TREC 2003, 245 rank swaps were observed (out of 2275 pairwise comparisons with 75 runs); in 49 of those, the difference in official score was greater than 0 . 05. The histogram of these rank swaps binned by the difference in official score is shown in Figure 7. The maximum observed score difference that produced a rank swap was 0 . 254 for TREC 2002 and 0 . 262 for TREC 2003.

These results indicate that existing evaluation resources are not suitable for use in post-hoc experimentation if one wishes to evaluate not only the correctness of the answer string, but also the supporting document from which it was extracted. As a note, it is not possible to perform the equiv-alent of the take-one-out and take-two-out analysis with re-spect to the lenient measure because answer patterns are created by hand, not generated by pooling.
Once again, multiple runs from the same organization were removed together.
Given the important role that reproducible, accurate, and reliable laboratory experiments play in information retrieval, how can we address the current lack of a truly reusable test collection for question answering? One solution that has been suggested is to simply increase the pool depth to a point where a good set of judgments can be gathered (say, one hundred documents). Instead of returning a response unit consisting of an answer string and supporting docu-ment, systems could return a traditional ranked list of doc-uments (which presumably contain the answer). Currently, there are plans to implement this solution for the TREC 2005 QA track.

This pooling approach, however, has the downside that it essentially adds a completely separate ad hoc retrieval component to the already difficult-to-evaluate question an-swering task. Although we can be fairly confident that this strategy will produce a set of judgments for post-hoc ex-perimentation, one wonders if there is a more cost-effective method for achieving the same effect.

Given that answer strings must be  X  X xact X  in the current setup of the TREC question answering track, it becomes possible to  X  X ork backwards X  from an answer to fetch all relevant supporting documents. This approach has been em-ployed to create a small reusable test collection for question answering [8]. Working from known answers, it is possible to craft very specific boolean queries using selected terms from both the question and the answer. Naturally, not all re-trieved documents will answer the question X  X hey must still be manually examined. However, it is hoped that this list of documents will be much smaller than the one gathered by pooling. Given the poor performance of most current systems, the vast majority of documents in any pool will be irrelevant.

To illustrate this process, consider the question  X  X hat is the name of the volcano that destroyed the ancient city of Pompeii? X , whose answer is  X  X esuvius X  (or some variant thereof such as  X  X t. Vesuvius X ). One might assume that all relevant documents must, at the very least, contain the terms  X  X ompeii X  and  X  X esuvius X . Thus, documents satis-fying the boolean query  X  X ompeii AND Vesuvius X  should encompass the set of relevant documents. As it turns out, there are only twenty-eight such documents in the entire AQUAINT corpus (used in the TREC QA tracks). These documents still must be manually examined to see if they actually contain the correct answer and provide the neces-sary support. An example of a clearly relevant document is APW19990823.0165 , which states that  X  X n A.D. 79, long-dormant Mount Vesuvius erupted, burying the Roman cities of Pompeii and Herculaneum in volcanic ash. X  An exam-ple of an irrelevant document containing both  X  X ompeii X  and  X  X esuvius X  is NYT20000704.0049; the article discusses winemaking in Campania, the region of Italy where both Pompeii and Vesuvius are located X  X t describes vineyards near the ruins of Pompeii and grape varieties that grow in the volcanic soil at the foot of Mt. Vesuvius.

In this particular example, the strategy described above would involve less manual labor than the standard pool-ing setup with a depth of one hundred documents. In gen-eral, one could argue that this technique of searching for the known answer is more efficient at gathering judgments because users do not have to examine as many documents on average. However, there are a few drawbacks to this approach, described below.

First, there are many types of questions where a good boolean query cannot be constructed to gather an initial set of candidate documents for manual examination. Con-sider a question such as  X  X hat country is Berlin located in? X , which potentially has many answer instances in the document collection. A reasonable boolean query such as  X  X erlin AND Germany X  would return too many hits to be practically examined manually. It is unclear how one might further restrict this query or at what point one should sim-ply stop judging documents. Questions involving numeric answers represent another difficult category of information needs: take, for example,  X  X hat is the population of Nige-ria? X  Population figures are highly variable because journal-ists report these numbers to different levels of granularity, e.g., some round to the nearest million, others give official census figures, and still others provide projected numbers. Furthermore, since population figures change over time, it is difficult to formulate an appropriate boolean query to retrieve a sufficiently narrow set of documents for manual assessment. These types of questions are relatively preva-lent, and potentially problematic even when the numeric answers remain (relatively) constant. For example, there are at least two different heights of Mt. Everest reported in the AQUAINT corpus. Since current guidelines do not take into account issues such as factual error, all of these answers would be accepted as  X  X orrect X  for the question  X  X ow tall is Mount Everest? X , even though some heights may be more accurate than others. Because it is not possible to a priori predict all possible answer variations in these cases, working backwards from known answers could potentially result in incomplete judgments.

Another limitation of this proposed approach stems from the assumption that relevant documents must contain cer-tain terms from either the question or the answer, or both. However, this may simply not be the case due to such prob-lems as vocabulary mismatch. For example, George Wash-ington may never be mentioned by name, but might rather be simply referred to as  X  X he father of our nation X . In princi-ple, these variations are impossible to predict a priori with-out examining the collection. Consider the question  X  X hen did Lenin die? X , whose answer is January 21, 1924. Working backwards from the answer, the query  X  X enin AND 1924 X  might be a reasonable start to generating the initial set of documents to assess. Unfortunately, this method would not retrieve the following:
Some relevant documents simply don X  X  contain obvious terms. Unfortunately, it is unknown how prevalent this phe-nomenon is, or what effects such documents might have on the evaluation of question answering systems. Documents in which answers are not obviously stated represent opportu-nities for sophisticated linguistic techniques that, for exam-ple, involve temporal reasoning. Since the question answer-ing task was in part designed to exercise advanced natural language processing techniques, relevance judgments should encompass these more difficult-to-extract answers.
Search-guided relevance assessment [4] is an alternative strategy to gathering relevance judgments that addresses many of the abovementioned limitations. Instead of a single-shot process, assessors interactively search the collection (typically with a ranked retrieval system), iterating between topic research and relevance assessment. This technique overcomes many vocabulary mismatch problems because us-ers engage in iterative query refinement, drawing terms from the very documents they are assessing. Since answer terms can still be part of the query, this procedure has the poten-tial advantage of producing documents that are more likely to be relevant. At the same time, since assessors are working with a ranked retrieval system, documents that do not have all query terms present may still be retrieved, allowing the possibility that documents with difficult-to-extract answers are judged. Typically, search-guided relevance assessment either stops after a fixed interval or after a period where fewer and fewer relevant documents are found. Cormack et al. [6] compared a less-structured variant of search-guided relevance assessment called Interactive Searching and Judg-ing with traditional pooling and concluded that the tech-nique is an attractive alternative. In addition, search-guided relevance assessment can be further enhanced with pooling of unjudged documents to serve as a quality control [4].
The notion of quantitative evaluations is central to in-formation retrieval research. Although this focus is consid-ered the mark of a mature discipline and has contributed to steady advances in the state of the art, we must confirm that our evaluations are meaningful and that their results are reliable. The work presented in this paper suggests that one must be cautious when employing existing resources to evaluate question answering systems X  X hey should not be used to make quantitative comparisons regarding the effec-tiveness of different techniques. The errors associated with scores derived from answer patterns and reldocs lists prevent reliable conclusions from being drawn.

This meta-evaluation of question answering points out shortcomings in existing evaluation methodology as it re-lates to building reusable test collections for post-hoc exper-imentation, but potential solutions and their shortcomings are also discussed. Somewhat ironically, a good evaluation infrastructure may spell the end of the evaluation that cre-ated it; in the same way that the TREC ad hoc test collec-tions have obviated the need to run the ad hoc tracks an-nually at TREC, the construction of a large-scale, reusable factoid question answering test collection may obviate the need to evaluate factoid questions on an annual basis. The resulting resources could then be employed to explore new directions in question answering.
I would like to thank Ellen Voorhees and Doug Oard for comments on earlier drafts of this paper, and anonymous reviewers for helpful suggestions on improving the paper. The entire community is indebted to NIST for their tireless efforts in making TREC possible. I would also like to thank Kiri for her kind support. The study reported here was in part motivated by the work in [8]; there are a couple of similar figures shared between these two papers. This work was supported in part by ARDA X  X  Advanced Question Answering for Intelligence (AQUAINT) Program. [1] E. Brill, J. Lin, M. Banko, S. Dumais, and A. Ng. [2] C. Buckley and E. M. Voorhees. Evaluating evaluation [3] C. Buckley and E. M. Voorhees. Retrieval evaluation [4] C. Cieri, S. Strassel, D. Graff, N. Martey, K. Rennert, [5] C. W. Cleverdon, J. Mills, and E. M. Keen. Factors [6] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. [7] J. Lin, A. Fernandes, B. Katz, G. Marton, and [8] J. Lin and B. Katz. Building a reusable test collection [9] J. Lin, D. Quan, V. Sinha, K. Bakshi, D. Huynh, [10] C. Monz. From Document Retrieval to Question [11] E. Sormunen. Liberal relevance criteria of [12] E. M. Voorhees. Variations in relevance judgments [13] E. M. Voorhees. Overview of the TREC-9 question [14] E. M. Voorhees. Evaluating the evaluation: A case [15] E. M. Voorhees and C. Buckley. The effect of topic set [16] E. M. Voorhees and D. M. Tice. Building a question [17] J. Zobel. How reliable are the results of large-scale
