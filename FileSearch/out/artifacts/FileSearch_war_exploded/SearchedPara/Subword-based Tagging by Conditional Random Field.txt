 The character-based  X  X OB X  tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as  X  X  X  if it is the first character of a multiple-character word, or  X  X  X  if the character func-tions as an independent word, or  X  X  X  otherwise. X  For ex-ample,  X  (whole) (Beijing city) X  is labeled as  X  (whole)/O (north)/B (capital)/I (city)/I X .

We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which as-signs tags to a pre-defined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example men-tioned above,  X  (whole) (Beijing city) X  is la-beled as  X  (whole)/O (Beijing)/B (city)/I X  in the subword-based tagging, where  X  (Beijing)/B X  is la-beled as one unit. We will give a detailed description of this approach in Section 2.
In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov). In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tag-ging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower. While OOV recog-nition is very important in word segmentation, a higher IV rate is also desired. In this work we propose a confi-dence measure approach to lessen the weakness. By this approach we can change R-oovs and R-ivs and find an optimal tradeoff. This approach will be described in Sec-tion 2.2.

In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method. Section 3 presents our experimental results. Section 4 describes current state-of-the-art methods for Chinese word segmentation, with which our results were compared. Section 5 provides the concluding remarks. Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword-based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merg-ing the results of both the dictionary-based and the IOB tagging. An example exhibiting each step X  X  results is also given in the figure.

Since the dictionary-based approach is a well-known method, we skip its technical descriptions. However, keep in mind that the dictionary-based approach can pro-duce a higher R-iv rate. We will use this advantage in the confidence measure approach. 2.1 Subword-based IOB tagging using CRFs There are several steps to train a subword-based IOB tag-ger. First, we extracted a word list from the training data sorted in decreasing order by their counts in the training data. We chose all the single characters and the top multi-character words as a lexicon subset for the IOB tagging. If the subset consists of Chinese characters only, it is a character-based IOB tagger. We regard the words in the subset as the subwords for the IOB tagging.

Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them. For a character-based IOB tagger, there is only one possibility of re-segmentation. How-ever, there are multiple choices for a subword-based IOB tagger. For example,  X  (Beijing-city) X  can be segmented as  X  (Beijing-city)/O, X  or  X  (Beijing)/B (city)/I, X  or  X  (north)/B (capital)/I (city)/I. X  In this work we used forward maximal match (FMM) for disambiguation. Of course, backward max-imal match (BMM) or other approaches are also appli-cable. We did not conduct comparative experiments be-cause trivial differences of these approaches may not re-sult in significant consequences to the subword-based ap-proach.

In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data. We downloaded and used the package  X  X RF++ X  from the site  X  X ttp://www.chasen.org/  X  taku/software. X  According to the CRFs, the probability of an IOB tag sequence, T = t t 1  X  X  X  t M , given the word sequence, W = w 0 w 1  X  X  X  w M , is defined by p ( T | W ) = exp Z = X cause the features trigger the previous observation t i  X  1 and current observation t i simultaneously; g k ( t i , W ), the unigram feature functions because they trigger only cur-rent observation t i .  X  k and  X  k are the model parameters corresponding to feature functions f k and g k respectively.
The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradi-ent descent optimization method. In order to overcome overfitting, a gaussian prior was imposed in the training.
The types of unigram features used in our experiments included the following types: w where w stands for word. The subscripts are position in-dicators. 0 means the current word;  X  1 ,  X  2, the first or second word to the left; 1 , 2, the first or second word to the right.

For the bigram features, we only used the previous and the current observations, t  X  1 t 0 .

As to feature selection, we simply used absolute counts for each feature in the training data. We defined a cutoff value for each feature type and selected the features with occurrence counts over the cutoff.

A forward-backward algorithm was used in the train-ing and viterbi algorithm was used in the decoding. 2.2 Confidence-dependent word segmentation Before moving to this step in Figure 1, we produced two segmentation results: the one by the dictionary-based ap-proach and the one by the IOB tagging. However, nei-ther was perfect. The dictionary-based segmentation pro-duced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results. In this sec-tion we introduce a confidence measure approach to com-bine the two results. We define a confidence measure, CM ( t iob | w ), to measure the confidence of the results pro-duced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary-based word segmentation. Its calculation is defined as:
CM ( t iob | w ) =  X  CM iob ( t iob | w ) + (1  X   X  )  X  ( t where t iob is the word w  X  X  IOB tag assigned by the IOB tagging; t w , a prior IOB tag determined by the results of the dictionary-based segmentation. After the dictionary-based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging. Each subword is given a prior IOB tag, t w . CM iob ( t | w ), a confidence probability derived in the process of IOB tag-ging, is defined as where the numerator is a sum of all the observation se-quences with word w i labeled as t .  X  ( t w , t iob ) ng denotes the contribution of the dictionary-based segmentation. It is a Kronecker delta function de-fined as
In Eq. 2,  X  is a weighting between the IOB tagging and the dictionary-based word segmentation. We found the value 0 . 7 for  X  , empirically.
 By Eq. 2 the results of IOB tagging were re-evaluated. A confidence measure threshold, t , was defined for mak-ing a decision based on the value. If the value was lower than t , the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging seg-mentation was used. A new OOV was thus created. For the two extreme cases, t = 0 is the case of the IOB tag-ging while t = 1 is that of the dictionary-based approach. In a real application, a satisfactory tradeoff between R-ivs and R-oovs could find through tuning the confidence threshold. In Section 3.2 we will present the experimental segmentation results of the confidence measure approach. We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections. The data contain four corpora from different sources: Academia Sinica (AS), City University of Hong Kong (CITYU), Peking University (PKU) and Microsoft Re-search in Beijing (MSR). Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only. Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score( F), OOV rate(R-oov) and IV rate(R-iv). For detailed info. of the corpora and these scores, refer to (Emerson, 2005).
For the dictionary-based approach, we extracted a word list from the training data as the vocabulary. Tri-gram LMs were generated using the SRI LM toolkit for disambiguation. Table 1 shows the performance of the dictionary-based segmentation. Since there were some single-character words present in the test data but not in the training data, the R-oov rates were not zero in this experiment. In fact, there were no OOV recognition. Hence, this approach produced lower F-scores. However, the R-ivs were very high. 3.1 Effects of the Character-based and the The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation. For the character-based tagging, we used all the Chinese characters. For the subword-based tagging, we added another 2000 most frequent multiple-character words to the lexicons for tagging. The segmen-tation results of the dictionary-based were re-segmented Table 1: Our segmentation results by the dictionary-based approach for the closed test of Bakeoff 2005, very low R-oov rates due to no OOV recognition applied. Table 2: Segmentation results by a pure subword-based IOB tagging. The upper numbers are of the character-based and the lower ones, the subword-based. using the FMM, and then labeled with  X  X OB X  tags by the CRFs. The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based. We found that the proposed subword-based approaches were effec-tive in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus. There were no F-score changes for AS and PKU corpora, but the recall rates were improved. Com-paring Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary-based approach. However, the R-iv rates were getting worse in return for higher R-oov rates. We will tackle this problem by the confidence measure approach. 3.2 Effect of the confidence measure In section 2.2, we proposed a confidence measure ap-proach to re-evaluate the results of IOB tagging by com-binations of the results of the dictionary-based segmen-tation. The effect of the confidence measure is shown in Table 3, where we used  X  = 0 . 7 and confidence threshold t = 0 . 8. In each slot, the numbers on the top were of the character-based approach while the numbers on the bot-tom were the subword-based. We found the results in Ta-ble 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmen-tation and the IOB tagging approach. The act of con-fidence measure made a tradeoff between R-ivs and R-oovs, yielding higher R-oovs than Table 1 and higher R-Table 3: Effects of combination using the confidence measure. The upper numbers and the lower numbers are of the character-based and the subword-based, respec-tively Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2.

Even with the use of confidence measure, the word-based IOB tagging still outperformed the character-based IOB tagging. It proves the proposed word-based IOB tag-ging was very effective. The IOB tagging approach adopted in this work is not a new idea. It was first used in Chinese word segmentation by (Xue and Shen, 2003), where maximum entropy meth-ods were used. Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maxi-mum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).

Our main contribution is to extend the IOB tagging ap-proach from being a character-based to a subword-based. We proved the new approach enhanced the word segmen-tation significantly. Our results are listed together with the best results from Bakeoff 2005 in Table 4 in terms of F-scores. We achieved the highest F-scores in CITYU, PKU and MSR corpora. We think our proposed subword-based tagging played an important role for the good re-sults. Since it was a closed test, some information such as Arabic and Chinese number and alphabetical letters cannot be used. We could yield a better results than those shown in Table 4 using such information. For example, inconsistent errors of foreign names can be fixed if al-phabetical characters are known. For AS corpus,  X  X dam Smith X  are two words in the training but become a one-word in the test,  X  X damSmith X . Our approaches pro-duced wrong segmentations for labeling inconsistency.
Another advantage of the word-based IOB tagging over the character-based is its speed . The subword-based approach is faster because fewer words than characters were labeled. We found a speed up both in training and test.

The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to rec-ognize the OOVs. In this work we used it more delicately. By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance. In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character-based method using the CRF approaches. We also suc-cessfully employed the confidence measure to make a confidence-dependent word segmentation. This approach is effective for performing desired segmentation based on users X  requirements to R-oov and R-iv.
 The authors appreciate the reviewers X  effort and good ad-vice for improving the paper.
 Thomas Emerson. 2005. The second international chi-nese word segmentation bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing , Jeju, Korea.
 John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In Proc. of ICML-2001 , pages 591 X 598.
 Fuchun Peng and Andrew McCallum. 2004. Chinese segmentation and new word detection using condi-tional random fields. In Proc. of Coling-2004 , pages 562 X 568, Geneva, Switzerland.
 Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-tional random field word segmenter for Sighan bakeoff 2005. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing , Jeju, Korea.
 Nianwen Xue and Libin Shen. 2003. Chinese word segmentation as LMR tagging. In Proceedings of the
Second SIGHAN Workshop on Chinese Language Pro-cessing .
