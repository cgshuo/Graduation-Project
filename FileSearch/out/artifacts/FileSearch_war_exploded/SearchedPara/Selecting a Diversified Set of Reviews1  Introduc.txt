 Online user reviews have great effects u pon decision-making process of users. Customers would like to read reviews to get a full picture of an item (i.e., a product or service) when they are select ing items. For example, according to the reviews of a camera, a customer can have knowledge of the size, color and convenience of usage though they have never bought it.

However, since user-generated reviews proliferate in recent years, e-commerce sites are facing challenges of information overload. In eBay.com, there are typi-cally several hundreds of reviews for popular products such as the latest model of iPhone. The massive volume of reviews prevent users from catching the clear view of products. On one hand, it is not easy for users to read all of the re-views. Users may be unable to go through all the reviews for each item, for it is too time-consuming. On the other hand, an increasing number of users choose to shop online via mobile phones, such as reserving a table at a restaurant or picking a movie to watch when they are outside. Those users have to make their purchase decision in a short time, so they can only read a small fraction of re-views for each item. In addition, due to th e limited screen size of mobile phones, it is inconvenient for users to scan more reviews.

To address the problem of information overload, e-commerce sites have adopted several kinds of review ranking and selection methods. Review ranking ranks re-views according to their helpfulness votes, so as to provide top-k reviews to users. Helpfulness votes are evaluated by users t o those reviews that are helpful to them. And there are also a number of researches on automatically estimating the quality of reviews [8][11][15][6]. However, there are two drawbacks in these review ranking work. First, the resulting top-k reviews of an item may contain redundant infor-mation while some important attributes may not be covered. For example, top-k reviews of a mobile phone may comment on display, camera and battery life, but mention nothing about whether it is easy to use or carry. Second, since previous experiments [3] showed that users tend to consider helpful the reviews that fol-low the mainstream, the resulting top-k reviews may lack diversity of opinions. By these two observations, review select ion based on attributes coverage [14] is proposed. It prefers to select reviews cov ering as many attributes as possible. But it is found that such kind of method does not reflect the original customer opinion distributions and may lead to unfair picture to users. Then [9] comes to solve the problem by selecting a set of diversified re views that keep the proportion of posi-tive and negative opinions. But we have found that this work does not perform well especially for selecting a smaller set of reviews, which is caused by the overlooking of attribute importance. For example, for a T-shirt, there are a lot of reviews com-plaining about its delivery speed while a smaller number of reviews complaining on the product quality. In such a case, it may be better to show reviews for clothes quality.

Hence, to improve the overall value of the top-k reviews, we view the top-k reviews as a review set rather than simple aggregation of reviews. We propose an approach to select a small set of reviews that cover important attributes with high quality, while at the same time have better diversified attributes and opinions by clustering. So our contributions in this paper are as follows: 1. We propose to evaluate attribute importance by calculating weights to them. We suppose to return users reviews c overing attributes to their concerns. 2. In order to improve the diversity of the top-k results, we propose to cluster reviews to different topic groups. We des ign an algorithm to select diversified reviews from different groups which can help improve diversification results. 3. We perform experiments on real data crawled from e-commerce site to evaluate our algorithm.

The rest of the paper is organized as follows. Section 2 introduces the re-lated work. Section 3 defines the problem. Section 4 gives the algorithms to the problem. Section 5 presents the experimental evaluation. Section 6 reaches the conclusion. Our work is related to the existing work that addresses the problem of review selection, review assessment and rankin g and review summarization. A sequence of recent work has focused on these problems.
 Review Selection: The review selection problem is to select a set of reviews from a review collection. Lappas and Gunopulos [10] proposed to select a set of reviews that represent the majority opinions on attributes. The drawback of this approach is that it reduces the diversity of opinions, regardless of the fact that users tend to make purchase decision after viewing different opinions on an item. Tsaparas et al. [14] intended to sel ect a set of reviews that contain at least one positive and one negative opinion on each attribute. This method fails to reflect the distribution of opinions in the entire review collection, thus misleading users. Lappas et al. [9] proposed the selection of a set of reviews that capture the proportion of opinions in the entire review collection. The shortcoming of this approach falls in the lack of consideration on the quality of reviews. Furthermore, the three methods view all the attributes as the same, which may lead to the overload of attributes.
 Review Assessment and Ranking: The review assessment and ranking prob-lem is to rank reviews according to their estimated quality. Kim et al. [8] trained a SVM regression system on a variety of features to learn a helpfulness function and applied it to automatically rank reviews. They also analyzed the importance of different feature classes to capture review helpfulness and found that the most useful features were the length, unigrams and product rating of a review. Hong et al. [6] used user preference featur es to train a binary classifier and a SVM ranking system. The classifier divides revi ews into helpful and helpless reviews, and the ranking system ranks reviews based on their helpfulness. Their evalu-ation showed that the user preference features improved the classification and ranking of reviews, and jointly using textual features proposed by Kim et al. [8] can achieve further improvement. However, these methods suffer from low coverage of attributes as the resulting top-k reviews may contain redundant in-formation. It is in that they estimated the review quality separately and did not consider the overall quality of the top-k reviews.
 Review Summarization: The review summarization problem is to extract attributes and opinions from the review collection and summarize the overall opinion on each attribute. Hu and Liu [7] proposed a three-step approach: first extract attributes from reviews, then id entify opinion sentences and whether they are positive or negative, and finally summarize the number of positive and negative opinion sentences for each attribute. Yu et al. [12] intended to pre-dict rating for each attribute from the overall rating, and extract representative phrases. Although these methods extract useful information from the review col-lection, they fail to provide the context of reviews from which users can view the opinions in a more comprehensive and objective way. In this section, we formally define our problem and the properties of our problem. Givenanitem i  X  I ,where I denotes a set of items in a certain category, let R = { r 1 ,r 2 ,...,r n } be a set of reviews commenting on i which cover a set of at-We assume that reviewers may express t wo types of opinions on each attribute: positive or negative opinion. Hence, the size of O is z =2 m . We also say that a review r covers a subset of attributes A r  X  A and expresses a subset of opinions O
Our task is to select a small set of reviews that have high quality, while at the same time cover important attributes and have different opinions. We now formalize our problem as shown in Problem 1.
 Problem 1 (Diversified Review Selection Problem). Given a set of reviews R of an item i that cover a set of attributes A and express a set of opinions O , find a subset of reviews S  X  R of size | S | X  k that maximize the overall value of S : where Q ( S ) measures the quality of S , D ( S ) rewards the diversity of S , k is an integer number, and  X   X  [0 , 1] is a diversity factor. The definition of Q ( S )and D ( S ) are given in the rest of the section. 3.1 The Quality of Review Set Since the overall quality of a review set is impacted by the quality of every single review in the set, we regard the quality of a review set as the average quality of reviews in the set. Therefore, the review set quality function Q ( S ) is defined as follows: where q ( r )  X  [0 , 1] measures the quality of review r .
 Assessment of Review Quality. Motivated by the helpfulness vote mecha-nism adopted in e-commerce sites, we view the quality of a review as its propor-tion of helpfulness votes. Therefore, the review quality function q ( r ) is defined as follows: where q + ( r )(or q  X  ( r )) represents the number of users that consider helpful (or helpless) the review r .

However, some reviews such as newly-written reviews have few votes, which cannot properly estimate the quality of reviews, so instead of computing q ( r ) directly, we use SVM regression to learn the function q ( r ). It takes the following two steps to finish the task.
 Step 1: define features impacting review quality. We consider two types of fea-tures: textual features [8] and user preference features [6], as shown in [6] that the combination of the two feature classes c an improve the assessment. The textual features include the length and unigrams of a review, while the user preference features include the coverage of important attributes and the divergence from mainstream viewpoint of a review. The us er preference features are defined as follows.
 Attribute Coverage: Covering as many attributes as possible is not enough, users pay more attention to those key attributes, so we consider the coverage of important attributes. Let A r  X  A be a set of attributes covered in review r ,the coverage of important attributes cov ( r ) is defined as follows: where w ( a )  X  (0 , 1) represents the weight of attribute a . We will introduce the attribute weight later.
 Mainstream Divergence: Users tend to consider helpful the reviews that fol-low the mainstream, so we measure the gap between the viewpoint of a review and the mainstream viewpoint. In e-commerce sites, users are required to give an item an overall rating ranging from one to five along with their reviews, so we can say that the overall rating expresses the same viewpoint as the review, and the average rating reflects the mainstream viewpoint. Therefore, the divergence from mainstream viewpoint div ( r ) is defined as follows: stream viewpoint, and max r  X  R v ( r ) is the maximum rating.
 Step 2: construct training data for SVM regression. We randomly choose reviews with a certain amount of votes and estimate their quality according to q ( r )so as to form a labeled dataset of { r, q ( r ) } pairs. We employ the labeled dataset to train our regression model and use the model to estimate review quality. Assignment of Attribute Weight. Users read reviews to acquire information on attributes. However, the review co llection of an item may cover dozens of attributes, while users are only interested in part of them. In order to provide key information to users, we measure the importance of attributes and assign relevant weights.

According to user behavio rs of review-writing, we regard attributes that are frequently referred in the reviews of an item and items in the same category as important attributes. Here, attributes that appear in the reviews of most items in a category are taken into consideration since they are common attributes for a category. When users are selectin g items of this category, they are more likely to refer to those attributes first. Therefore, let R a  X  R be a set of reviews commentingonanitem i  X  I that cover attribute a ,where I denotes a set of items in a certain category, and I a  X  I be a set of items that have reviews covering a , the attribute weight w ( a ) is defined as follows: measures the importance of a for a category.

The definition of w ( a ) is similar to that of TF-IDF, as an attribute can be viewed as a term, and the review collection of an item as a document. Hence, the importance of an attribute for an item is equal to TF, while the importance of an attribute for a category is equal to DF instead of IDF since we value attributes that are frequently referred in the reviews of items in the same category. 3.2 The Diversity of Review Set Since each time we select one review to add into the review set, we regard the diversity of a review set as the sum of the diversity of each review in the set when it is selected. Therefore, the review set diversity function D ( S ) is defined as follows: where d ( r ) rewards the diversity of review r .
 Diversification of Opinions. To diversify opinions in a review set, we take the following three-step method.

In step 1 , we cluster reviews with similar concerns. We assume that there are l clusters, and randomly select l reviews to each initialize a cluster. Since there may exist opinion sparsity in the r eview collection, we represent a review as a vector of TF-IDFs for the unigrams in the review. We compute the cosine distance between two review vectors as t he similarity between them. Each time a review is added into a cluster based on the similarity between the review vector and the cluster centroid vector, and the c luster centroid vector will be updated accordingly.

In step 2 , we decide the number of reviews selected from each cluster. We compute the proportion of each cluster in the review collection as the propor-tion of reviews from different clusters in t he selected review set. Therefore, the proportion of reviews from cluster C i in the selected review set is defined as follows: Givenaninteger k as the size of the selected review set, the number of reviews selected from each cluster is defined as follows: In step 3 , we use MMR [1] to select reviews from different clusters. That is, in addition to the assumption that review r has not been added into S ,itisin cluster C i , and the number of reviews selected from C i is less than n ( C i ), select r if it is similar with C i , while at the same time not similar with S . Therefore, the review diversity function d ( r ) is defined as follows: where  X   X  [0 , 1] is a trade-off coefficient.

Here, let O r  X  O (or O R  X  O ) be the set of opinions expressed by review r (or review set R ), the review similarity sim ( r, R ) is defined as follows: where a o represents the attribute corresponding to opinion o .
In this section, we give the algorithms to our problem including the algo-rithms for opinion extraction and revie w selection. Before we select reviews, we apply opinion extraction algorithm that extracts attributes and opinions from the original review collection to generate input for review selection algorithm. 4.1 Opinion Extraction There are a number of researches [7][5][12] on extracting attributes and mining opinions from reviews. Our method is lexicon-based as well as rule-based to handle context-dependent opinion words. Given a review, it takes the following three steps.

First , extract opinion phrases. We define an opinion phrase { a, o } as a pair of attribute a and opinion word o . We refer to the approach proposed in [13]. They employ the grammatical relations produced by Stanford Dependency Parser [4] to define a set of dependency patterns, and generate opinion phrases according to the patterns.
 Second , identify opinions expressed by opinion words in opinion phrases. There are two types of opinions: positive or negative opinion. Opinions are de-cided by opinion word lists [7], including a list of positive words and a list of negative words. If opinions cannot be determined according to the opinion word lists, we apply intra-sentence rule and inter-sentence rule in [5].

Intra-Sentence Rule: givenanopinionword o and the sentence s where o is, identify its opinion by the other opinion words in s . This rule is based on the fact that if there are conjunctions such as  X  X nd X  and  X  X ut X  near o in s ,wecan decide its opinion by the opinion word o on the other side of the conjunction.
Inter-Sentence Rule: givenanopinionword o and the sentence s where o is, if s (or the next sentence to s ) begins with a word like  X  X ut X  or  X  X owever X , it means o has different opinion from the opinion word in the last sentence (or the next sentence), otherwise they have the same opinion.

Finally , summarize opinions of attributes. Since an attribute may appear several times in a review, we aggregate all the opinions to produce an overall opinion. Given an attribute, if the number of positive opinions is larger than that of negative opinions, the attribute gains a positive opinion, or otherwise it gains a negative opinion. 4.2 Review Selection We use a greedy algorithm to implemen t review selection as shown in Algo-rithm 1. It first clusters reviews accord ing to their concerns, and then selects reviews proportionally from different clusters to maximize the overall value of the selected review set. In this section, we evaluate our algor ithm for review selection. We set the size of the selected review set as 5. We apply the diversity factor with  X   X  { review set with the increase on  X  .
 5.1 Datasets In our experiments, we use data crawle d from e-commerce site eBay.com. This dataset includes reviews from three categories: tablets and e-book readers, portable audio and digital cameras. There are 4789 items and 110480 reviews in all. Af-ter pruning items having less than 20 reviews, there are 121 items on tablets and e-book readers with 9555 reviews, 286 items on portable audio with 38799 reviews, and 583 items on digital cameras with 42783 reviews. 5.2 Experiment Setup In assessment of review quality, we select reviews having at least 5 votes as labeled data and use LIBSVM [2] to train regression model. To measure our algorithm for quality assessment, we select 60% of the labeled data as training data and the rest as test data to predict review quality. We adopt Pearson correlation coefficient as the metric to the correlation between the estimated quality and the labeled quality, so as to find whether the quality function is well learned. According to the evaluation in [8 ], the resulting coefficient is around 0.5, so we set 0.5 as the borderline between well-learned quality functions and poorly-learned quality functions. Our algorithm results in a coefficient of 0.6405749, which is a little higher than 0.5. In diversification of opinions, since the size of the selected review set is small, we set the number of clusters as 3 to ensure the selection of reviews from each cluster. We set the trade-off coefficient  X  as 0.7. 5.3 Result Analysis We are interested in how quality beh aves when raising diversity factor  X  from 0.1 up to 0.9, we hypothesize that the quality will reduce with the increase on  X  .On the other hand, we will focus on whether attributes and opinions are diversified through our algorithm. In addition, we will compare our algorithm with other algorithms for review selection in both the quality and diversity of the selected review set.
 Review Set Quality Analysis. We apply our algorithm on all three categories of items and compute the average quality o f all the selected review sets. As shown in Fig. 1, the quality goes down smoothly when raising  X  by 10% each time, which agrees with our hypothesis, and reveals that the diversification of review set have detrimental effects on the quality.
 Review Set Diversity Analysis. To decide whether attributes and opinions are diversified through our algorithm, we compute the average opinion coverage and the average cluster coverage of all the selected review sets.

The opinion coverage includes opinion coverage and weighted opinion cover-age. Let O S  X  O be the set of opinions expressed by the selected review set tribute corresponding to opinion o . The opinion coverage measures the diversity of opinions while the weighted opinion coverage measures the coverage of impor-tant attributes.

The cluster coverage includes cluster co verage and weighted cluster coverage, the definitions of which are similar with that of cov op and cov wop , except that O in the denominator is replaced by O C i  X  O , the set of opinions expressed by cluster C i , and after computing the cluster coverage for each cluster, use the average cluster coverage as the result.

As depicted in Fig. 2 and Fig. 3, the opinion coverage grows rapidly at first, peaking at 31.5% when  X  is 0.3, while the weighted opinion rises up to around 70% and remain so afterwards. Hence, with the introduction of diversity factor  X  , the opinion coverage improves.

In Fig. 4 and Fig. 5, the trend of cluster coverage is in line with that of opinion coverage, while the weighted cluster co verage goes up faster than the weighted opinion coverage in the first step. Hence, the diversity factor  X  enhances the cluster coverage as well.

In conclusion, the attributes and opinions of the selected review set are diver-sified through our algorithm.
 Comparison of Algorithms. We compare our algorithm with the charac-teristic method proposed in [9] that s elects a set of reviews that capture the proportion of opinions in the review collection of an item. We implement the characteristic method with a greedy algorithm.

As presented in Review Set Diversity A nalysis, the diversity of the selected review set reaches its peak when  X  is 0.3, so we set  X  as 0.3, and compare the average quality QLTY , the average (weighted) opinion coverage (W)OP-COV and the average (weighted) cluster coverage (W)C-COV of all the selected review sets produced by the two algorithms. We will also consider the average error of the proportion of opinions PROP-ERR defined in [9].

In Table 1, our algorithm outperforms the characteristic algorithm in all as-pects except PROP-ERR . Since the characteristic algorithm aims at reflecting the distribution of opinions in the revi ew collection of an item, it is as expected to perform better than our algorithm in PROP-ERR . However, since the size of the selected review set is small and the characteristic algorithm diversifies the selected review set by maintaining the proportion of opinions in the original re-view collection, some rarely mentioned attributes may not be covered, resulting in the low coverage of opinions and important attributes as well.

In our work, we define important attributes as those attributes that are fre-quently referred in the reviews of an i tem and items in the same category. Some attributes may only appear in a small fra ction of reviews of an item, but they are likely to be important for the category where the item belongs to. For example, the reviews selected by our algorithm for Canon EOS 1D 4.2 MP Digital SLR Camera cover not only those common attributes such as: picture quality :  X . . . It delivers excellent image quality, with great colors and detail... X  shooting :  X . . . I also like the 6.5fps shooting, which is handy for my wildlife shooting... X  price :  X . . . This is one of the best cameras out there for the price in my opinion. . .  X  but also attributes that are not frequently mentioned in the reviews of this item while in fact are of great importance to the category of cameras, such as: screen and interface :  X . . . What I love in it? Large viewfinder, great ISOs range, nice speed, large screen and comfortable interface. . .  X  menu system :  X . . . The menu system is also very intuitive and easy to learn and the my menu feature is good for me about 90% of the time and keeps me from having to go through the whole menu interface. . .  X 
Users may concern about screen, inter face and menu system when selecting cameras as they are key to user experience. Though they are not frequently referred in the reviews of this item, they are definitely important for the category of cameras.
 We have proposed an approach to select a small set of reviews from the review collection of an item that have high quality, while at the same time have diversi-fied attributes and opinions. We first train a regression model to estimate review quality. Then, to cover important attributes, we assign weights to attributes ac-cording to their importance, and to cover different opinions, we cluster reviews according to their opinions and select reviews proportionally from different clus-ters. Finally, we use a diversity factor to balance the quality and diversity of the selected review set, and implement our method with a greedy algorithm to maximize the overall value of the select ed review set. The experimental results show that our algorithm helps div ersify the selected review set.
 Acknowledgements. The work is partially supported by the Key Program of National Natural Science Founda tion of China (Grant No. 61232002), Na-tional High Technology Research and Development Program 863 (Grant No. 2012AA011003), National Natural Science Foundation of China (Grant No. 61103039, No.61021004, No.60903014).

