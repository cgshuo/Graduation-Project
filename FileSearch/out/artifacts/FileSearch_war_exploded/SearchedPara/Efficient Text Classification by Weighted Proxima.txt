 classify large-scale text data with high classification quality and fast training speed. Our method is based on a novel extension of the proximal SVM mode [3]. Previous studies on proximal SVM have focused on classification for low dimensional data and did not consider the unbalanced data cases. Such methods will meet difficulties when classifying unbalanced and high dimensional data sets such as text documents. In this work, we extend the original proximal SVM by learning a weight for each training error. We show that the classification algorithm based on this model is capable of handling high dimensional and unbalanced data. In the experiments, we compare our method with the original proximal SVM (as a special case of our algorithm) and the standard SVM (such as SVM light) on the recently published RCV1-v2 dataset. The results show that our proposed method had comparable classification quality with the standard SVM. At the same time, both the time and memory consumption of our method are less than that of the standard SVM. classifier by some labeled documents and then using the classifier to predict the labels of unlabeled documents. Many methods have been proposed to solve this problem. SVM (Support Vector Machine), which is based on the statistical learning theory [11], has been shown to be one of the best methods for text classification problems [6] [8]. Much research has been done to make SVM practical to classify large-scale dataset [4] [10]. The purpose of our work is to further advance the SVM classification technique for large-scale text data that are unbalanced. In particular, we show that when the text data are largely unbalanced, that is, when the positive and negative labeled data are in disproportion, the classification quality of standard SVM deteiorates. This problem has been solved using cross-validation based methods. But cross-validation methods are very inefficient due to their tedious parameter adjustment routines. In response, we propose a weighted proximal SVM (WPSVM) model, in which the weights can be adjusted, to solve the unbalanced data problem. Using this weighted proximal SVM method, we can achieve the same accuracy as the traditional SVM while requiring much less computational time. proximal SVM (PSVM) model. The original proximal SVM was proposed in [3]. According to the experimental results of [3], when classifying low dimensional data, training a proximal SVM is much faster than training a standard SVM and the classification quality of proximal SVM is comparable with the standard SVM. However, the original proximal SVM is not suitable for text classification because of the following two reasons: 1), text data are high dimensional data, but the method proposed in [3] is not suitable for training high dimensional data; 2), data are often unbalanced in text classification, but proximal SVM does not work well in this situation. Moreover, in the experiments we found that the classification quality of proximal SVM deteriorates more quickly than standard SVM when the training data becomes unbalanced. (WPSVM) model in this paper. We show that this method can be successfully applied to classifying high dimensional and unbalanced text data through the introduction of the following two modifications: 1) in WPSVM, we added a weight for each training error and developed a simple method to estimate the weights. We then adjusted the weights automatically solves the unbalanced data problem; 2) Instead of solving the problem by KKT (Karush-Kuhn-Tucker) conditions and Sherman-Morrison-Woodbury formula as shown in [3], we use a iterative algorithm to solve WPSVM, which makes WPSVM suitable for classifying high dimensional data. the classification quality of WPSVM are as accurate as traditional SVM and more accurate than proximal SVM when the data are unbalanced. At the same time WPSVM is much more computationally efficient than traditional SVM. Section 2, we review the text classification problems and the SVM and proximal SVM algorithms. In Section 3, we propose the weighted proximal SVM model and explore how to solve it efficiently. In Section 4, we discuss the implementation issues. Experimental results are given in Section 5. In Section 6, we give the conclusions and future work. in the Vector Space Model [1]. In this model, each document is represented by a vector of weighted term frequencies using the TF*IDF [1] indexing schema. classification problem, where there are only two class labels in the training data: positive (+1) and negative (-1). Note that multi-class classification problem can be solved by combining multiple binary classifiers; this will be done in our future work. Suppose that there are m documents and n terms in the training data, we use &gt; &lt; i i y x , to denote each training data, where i R im  X = x are training vectors and yim  X +  X  = are their corresponding class labels The binary text classification problem can be formulated as follows, { , | , { 1,1}, 1, 2... } n yRy im ii i i &lt;&gt; X   X  X  X = xx , finding a classifier (): {1,1} n fR  X +  X  x , such that for any unlabeled data x  X  we can predict the label of x by () f x . SVM. More details could be found in [2] and [3]. This paper will follow the notations of [2] which may differ somewhat from those used in [3]. The SVM algorithms introduced in this paper all use the linear kernel; it is also possible to use non-linear kernels, but there are no significant advantages of using non-linear kernel for text classification. optimal hyperplane 0 b  X += wx and use this hyperplane to separate the positive and negative data. The classifier can be written as: parameters w and b . The objective of the SVM training algorithm is to find w and b from the information in the training data. Standard SVM algorithm finds w and b by solving the following optimization problem. the positive and negative data. i  X  represents the training error of the i th training example. Minimizing the objective function of (1) means minimizing the training errors and maximizing the margin simultaneously. C is a parameter that controls the tradeoff between the training errors and the margin.  X += wx and 1 b i  X += X  wx are two bounding planes. The distance between the two bounding planes is the margin. The optimization problem (1) can be converted to a standard Quadratic Programming problem. Many efficient methods have been proposed to solve this problem on large scale data [2] [4].  X += wx as the separating surface between positive and negative training examples. But the parameter w and b are determined by solving the following problem. proximal SVM (2) is the constraints. Standard SVM employs an inequality constraint whereas proximal SVM employs an equality constraint. The intuition of Proximal SVM is shown in Figure 2. We can see that standard SVM only considers points on the wrong side of 1 b i  X += wx and 1 b i  X += X  wx as training errors. However, in proximal SVM, all the points not located on the two planes are treated as training errors. In this case the value of training error i  X  in (2) may be positive or negative. The second part of the objective function in (2) uses a squared loss function 2 i instead of i mainly for efficiency consideration. [3] proposed an algorithm to solve (2) us ing KKT conditions and Sherman-Morrison-Woodbury formula. This algorithm is very fast and has comparable effectiveness with standard SVM when the data dimension is far less than the number of training data ( n &lt;&lt; m ). However, in text classification n usually has the same magnitude with m best of our knowledge, little research works has been conducted to show the performance of proximal SVM with high dimensional data. suitable for high dimensional data, Formula (2) can be solved efficiently for high dimensional data using iterative methods. We have applied the proximal SVM for text classification but found that when the data are unbalanced, i.e. when the amount of positive data are much more than negative data, or vice versa, the effectiveness of proximal SVM deteriorates more quickly than standard SVM. Data unbalance is common in text classification, which motivates us to search for an extension to proximal SVM to deal with this problem. is not suitable for classifying unbalanced data in this section. To the unbalanced data, without lose of generality, suppose the amount of positive data is much fewer than the negative data. In this case the total accumulative errors of negative data are much higher than that of positive data. Consequently, the bounding opposite to the negative data to produce a larger margin at the price of increasing the positive errors. Since the positive data are rare, this action will lower the value of objective function (2). Then the separating plane will be biased to the positive data and result in a higher precision and a lower recall for the positive training data. weight i  X  to each training error i  X  and convert the optimization problem (2) to the following form: equally weighted, but in Formula (3) we use a non-negative parameter i  X  to represent the weight of each training error i  X  . tradeoff parameter C from 2 i purpose of this movement is for notation simplicity in the later development of our solving method. and Sherman-Morrison-Woodbury formula as showed in [3], this solving strategy is inefficient for high dimensional data like text documents. Instead, we convert (3) to an unconstrained optimization problem that can be directly solved using iterative methods. 222 (1())(()) yby b iii ii  X  = X   X  + =  X   X  + wx wx (4) of (3), we get an unconstrained optimal problem: TF*IDF matrix of documents whose row vectors are i x . Suppose e is a vector whose elements are all 1. Let (1) (1) [, ] , [,] mn n AX R bR let mm R zero elements are ii i  X   X =  X  then (5) can be written as: negative, it is easy to prove H is positive definite. The solution of (6) is found when ( ) f  X   X  =0, that is: A'A)x=A'b, where A is a high dimensional sparse matrix. The CGLS /LSQR [9] algorithm is dedicated to efficiently solve this problem. how to set the parameters and how to solve Equation (7) efficiently. We will address these concerns in this section. training algorithm. Parameter v controls the tradeoff between maximizing the margin and minimizing the training errors. Parameters ,1,2,..., im i  X  = control the relative error weights of each training example. To simplify the parameter setting for unbalanced data problem, we set the error weight of all positive training data to  X  + and all negative training data to we only need to set three parameters: v ,  X  + and These parameters can be decided by statistical estimation methods on the training data, such as LOO (Leave-One-Out cross-validation), k-fold cross validation, etc. If we iteratively update the weights by the separating plane obtained from previous round of training, we essentially obtain a boosting based method such as AdaBoost [13]. However, a disadvantage of using these boosting based and cross-validation based methods is that they need too much training time for parameter estimation. based methods, we have developed a simple method that can estimate the parameters based on the training data. It can achieve comparable effectiveness as compared to algorithms that using standard SVM plus cross validation techniques. Our parameter estimation method is as follows. positive and negative data, it is better to have the following condition: negative training data has the same expectation, we can get: where N+ is the number of positive training examples and N-is the number of negative training examples. Then we set the parameter  X   X  and  X  + as follows. Equation (8). Instead, we use a conservative setting strategy to make the precision of a minor class a little higher than recall. This strategy usually results in higher accuracy for unbalanced data. positive examples is equal to the number of negative examples), this method will result in  X   X  =  X  + =1 and make WPSVM equal to PSVM. Therefore, PSVM can be viewed as a special case of WPSVM. between WPSVM and PSVM, we manually generated a balanced data set and an unbalanced dataset in a two dimensional space. Then we calculated the separating plane of WPSVM and PSVM respectively. The results are shown in Figure 3 and Figure 4. and WPSVM are almost the same when the data are balanced. Figure 4 shows when the data is unbalanced, the separating plane for WPSVM resides in the middle of the positive and negative examples, but the separating plane for PSVM is inclined to the positive examples. 
Figure 4. Separating planes for unbalanced data found CGLS [9] has the best performance. However, many other iterative optimal methods can also be used to solve Equation (7). dominated by the algorithm used for solving Equation (7). Usually this kind of algorithms has O(KZ) time complexity and O(Z) space complexity where K is the number of iterations and Z is the number of non-zero elements in the training vectors. solution to the problem. The more the number of required and the iterative solution is closer to the optimal solution. However, when the iteration count archives a certain number, the classification result will not change when the number of iterations continues to increase. Therefore it is important to select a good terminating condition to obtain a better tradeoff between training time and classification accuracy. Since the number of required iterations may vary for different dataset, we make the terminating condition as an adjustable parameter when implementing the WPSVM algorithm. WPSVM and other SVM based methods. We will verify the following hypotheses for text datasets: 1. WPSVM (with default parameter settings) has the same classification power as standard SVM plus cross-validation, has slightly better classification power than standard SVM (with default parameter settings) and has much better classification power than PSVM 2. WPSVM is much more efficient than standard SVM RCV1-v2 [8]. RCV1 (Reuters Corpus Volume I) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Lewis, et al [8] made some corrections to the RCV1 dataset and the resulting new dataset is called RCV1-v2. documents. The benchmark results of SVM, weighted k-NN and Rocchio-style algorithms on RCV1-v2 are reported in [8]. The results show that SVM is the best method on this dataset. To make our experimental results comparable with the benchmark results, we strictly follow the instruction of [8]. That is, we use the same vector files, training/test split and effective measures as in [8]. from the concatenation of text in the &lt;headline&gt; and stopword removal. 47,219 terms that appears in the training data are used as features. The features are weighted using the TF*IDF indexing schema and then being cosine normalized. The resulting vectors are published at [7]. We directly use these vectors for our experiments. time of the documents. Documents published from August 20, 1996 to A ugust 31, 1996 are treated as training data. Documents published from September 1, 1996 to A ugust 19, 1997 are treated as test data. This split produces 23,149 training documents and 781,256 test documents. three different category sets: Topics, Industries or Regions. For each single category, the one-to-rest strategy is used in the experiments. In other words, when classifying category X, all the examples labeled X are defined as positive examples, and the other examples are defined as negative examples. quality of different methods. F1 is determined by Precision and Recall. The Precision, Recall, and F1 measures for a single category are defined as follows. Precision= Recall = F1 = (2*Precision*Recall) / (Precision + Recall) average micro-F1 and average macro-F1. Average macro-F1 is the average value of each single F1 in the category set. Average micro-F1 is defined as follows. Ave micro-F1=(2*microP*microR)/(microP+microR) compare the F1 measure on the following: parameter estimating method presented in section 4.1. make it equivalent to the proximal SVM algorithm. parameter settings. threshold adjustment. It is a benchmark method used in [8]. In this algorithm, SVM light was run using default parameter settings and was used to produce the score. The threshold was calculated by the SCutFBR.1 [12] algorithm. LOO cross validation. It was first introduced in [6] and named as SVM.2 in [8]. In this algorithm, SVM light was run multiple times with deferent  X  X  parameters and the best  X  X  parameter was selected by LOO validation. The -j parameter controls the relative weighting of positive to negative examples. This approach solved the data unbalance situation by selecting the best  X  X  parameter. The experiments were separately performed on each category using the one-to-rest strategy. The dataset scale for each category is shown in table 1. Number of training examples 23149 Number of test examples 781256 Number of features 47219 Average Number of non-zero elements 123.9 categories. There are total 101 Topics categories that at least one positive example appears in the training data. We calculate the F1 value for the five algorithms on each category (The F1 value of SVM.1 and SVM.2 is calculated by the contingency table published at [7]). Figure 5 shows the changes of F1 value from algorithms. Categories are sorted by training set frequency, which is shown on the x-axis. The F1 value for a category with frequency x has been smoothed by replacing it with the output of a local linear regression over the interval x X 200 to x+200. data is relatively balanced (the right part Figure 5), the F1 measure for the five algorithms has no big differences. When the training data is unbalanced (the left part of Figure 5), the classification quality of WPSVM is between SVM.1 and SVM.2. Both have better classification quality than SVM light and PSVM. Figure 5 also shows the classification quality of PSVM deteriorates more quickly than that of SVM light when the data become unbalanced. categories. The results of SVM.1 and SVM.2 are the values reported in [8]. It can be seen that the overall performance of WPSVM, SVM.1 and SVM.2 are better than that of SVM light and PSVM. SVM.1 has the best average effectiveness, especially in average macro-F1. This is mainly because when the training data are extremely unbalanced (e.g. the positive ratio is less than 0.1%), the threshold adjustment method is better than both WPSVM and SVM.2. Algorithms Average micro-PSVM 0.767 0.354 SVM light 0.804 0.472 WPSVM 0.808 0.589 SVM.2 0.810 0.557 SVM.1 0.816 0.619 Table 3. Average F1 for Industries and Regions 
Algorithms Average (313) WPSVM 0.520 0.301 (228) WPSVM 0.862 0.558 313 Industries categories and 228 Regions categories. The average F1 measures of these categories are shown in Table 3. The results of SVM.1 shown in table 3 are the values reported in [8]. We can see that in the Industries and Regions Split, the effectiveness of WPSVM is also comparable with SVM.1. classification quality of WPSVM is comparable with SVM.1 and SVM.2, which are the best methods of [8], and is better than SVM light and PSVM. However, SVM.1 and SVM.2 require training many times to estimate a good parameter whereas WPSVM only require training once. 
The computational efficiency is measured by the actual training time and memory usage respectively. Since SVM.1 and SVM.2 require running SVM light many times, their efficiency must be less than SVM light. Thus in the experiments, we only compare the efficiency of WPSVM and SVM light. We run each algorithm on 5 training dataset with different size. The vector files of [8] are published as one training file and and then incrementally append the remaining four test files to form the other four datasets. The number of training examples for the 5 datasets is 23149, 222477, 421816, 621392 and 804414 respectively. The training time is measured in second. Both algorithms ran on an Intel Pentium 4 Xeon 3.06G computer. 
We found that when using SVM light for the same training size, balanced data required more training time than the unbalanced data. Thus, we did two groups of efficiency experiments. One group uses category CCAT as positive examples. The ratio of CCAT is 47.4% and it makes this group as a balanced example. The other group is an unbalanced example. It uses GDIP as positive examples. The ratio of GDIP is 4.7%. Table 4 shows the training time of WPSVM and SVM light V6.01 on the two groups. We can see that the training time of WPSVM is far less than the training time of SVM light and is not affected by the data unbalanced-ness problem. SVM light is determined by the training size, regardless of whether the data are balanced or unbalanced. Figure 6 shows the memory requirements of the two algorithms with different training sizes. We can see that the memory requirement of WPSVM is slightly less than SVM light. This is because WPSVM almost only require the memory to store the training data but SVM light requires additional working space. SVM model, which assigns a weight to each training error. We successfully applied the WPSVM model to text classification problem by a simple parameter estimation method and an algorithm for solving the equations directly instead of using KKT conditions and the Sherman-Morrison-Woodbury formula. The experiments showed that our proposed method can achieve comparable classification quality as the standard SVM when supplemented with validation techniques, but is more computationally efficient than the standard SVM. We only validated the effectiveness of our algorithm on text classification in this paper. As a general linear SVM classification algorithm, it can also be used in other classification tasks. It is worth pointing out that in this paper we only demonstrated the advantage of WPSVM in solving the data unbalanced-ness problem. However the WPSVM model may have other potential use. In WPSVM, the relative importance of each training point can be adjusted based on other prior knowledge. 
