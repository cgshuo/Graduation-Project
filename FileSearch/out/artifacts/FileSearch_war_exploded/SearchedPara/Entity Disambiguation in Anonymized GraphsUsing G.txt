 This paper presents a novel method for entity disambigua-tion in anonymized graphs using local neighborhood struc-ture. Most existing approaches leverage node information, which might not be available in several contexts due to pri-vacy concerns, or information about the sources of the data. We consider this problem in the supervised setting where we are provided only with a base graph and a set of nodes labelled as ambiguous or unambiguous. We characterize the similarity between two nodes based on their local neighbor-hood structure using graph kernels; and solve the resulting classification task using SVMs. We give empirical evidence on two real-world datasets, comparing our approach to a state-of-the-art method, highlighting the advantages of our approach. We show that using less information, our method is significantly better in terms of either speed or accuracy or both. We also present extensions of two existing graphs kernels, namely, the direct product kernel and the shortest-path kernel, with significant improvements in accuracy. For the direct product kernel, our extension also provides sig-nificant computational benefits. Moreover, we design and implement the algorithms of our method to work in a dis-tributed fashion using the GraphLab framework, ensuring high scalability.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval entity resolution; entity disambiguation; graph kernels; sup-port vector machines
Modern data mining techniques and large data sources like news media allow for extraction of vast amounts of informa-tion about entities, such as people and companies. For ex-ample, a user might be interested to know which cities TED talks curator Chris Anderson is visiting this year. An auto-mated reply to such a query requires extraction of names and places from texts. This task is made difficult by the existence of Chris X  X  namesake, former Wired Magazine editor-in-chief Chris Anderson. The two individuals work in overlapping fields, and deciding whom is referred to in a news article may be difficult, even when considering context. Resolution of ambiguities in data is a well-studied problem and meth-ods for entity resolution [6, 5, 15, 27], entity matching [7, 24] and entity disambiguation [11, 12, 13, 23] are all aimed towards associating references in text sources with the cor-rect underlying entity. These methods typically make use of similarities in names [6, 5], meta-data [7] or source informa-tion [23], to decide which entities underly which references. Relational entity disambiguation makes use of network struc-ture between entities [5, 4, 3, 23], sometimes together with additional information.

Big data analysis increasingly faces the challenge of how to preserve user anonymity [1]. While a number of privacy preserving mechanisms have been studied [14], the practical applications are still at a nascent stage [20]. Often, a simple approach to address this privacy concern is using anonymiza-tion at source, prior to subsequent data mining, by assigning pseudorandom identifiers to entities. In this setting, existing techniques [7, 11, 12, 13, 24, 15, 27, 5, 6], building on sim-ilarity of entity attributes, are rendered inapplicable, while the method presented in this paper is inherently well-suited. To the best of our knowledge, very little work has been done in this setting.

We investigate a form of relational entity disambiguation in which the entities have been assigned a pseudo-random (possibly ambiguous) identifier, and network structure is the only information available. A node in the network represents the identifier of one or several entities, and an edge a rela-tion between identifiers, such as co-occurrence in articles. We define an ambiguous node , in such a network, to be one representing several underlying entities. Such entities are as-sumed to have been assigned the same identifier in the data mining process, possibly because of sharing the same name. A node representing only one entity is called unambiguous . We target a scenario where the original data is inaccessible or expensive to access. Such situations occur for example when large amounts of texts are parsed in an online fashion, not to be looked at again.
We develop a novel formulation of relational entity dis-ambiguation as a graph classification problem of detecting ambiguities in anonymized data with network structure. We solve the problem leveraging kernel methods and a highly scalable SVM implementation. Given a graph with some nodes labeled as ambiguous or unambiguous, our method trains a classifier based on graph kernels using the neigh-borhoods of labeled nodes as input. The method is designed to operate in a distributed scheme and is implemented in the distributed computation framework GraphLab 1 , allow-ing for web-scale data to be processed. While scalable entity disambiguation has been addressed previously [7], existing methods rely on entity attribute information, and do not lend themselves to the anonymized setting.

Further, we extend two existing graph kernels, designing a domain-specific adaptation of the shortest-path kernel [8], and a fast approximate algorithm for the direct product kernel [17] for unlabeled graphs. We show theoretical and empirical computational benefits of the extension compared to the original kernels. We evaluate our method on two real-world datasets, comparing the performance of different graph kernels. Our extensions are shown to be both signif-icantly faster and more accurate than the original kernels. We also show that implementing our methods in GraphLab gives a significant speedup in our experiments.

We compare our approach to a state-of-the-art method [23] at the task of detecting anonymized, ambiguous nodes on a well-studied, public dataset. Our experiments show that our method is significantly better than the state-of-the-art, ei-ther in terms of speed or accuracy or both. Moreover, our method demands less information since it does not require knowledge of data sources.

The main contributions of this paper can be summarized as follows: The remainder of this paper is organized as follows. In Section 2, we survey related research. In Section 3, we de-fine the problem of anonymized relational entity disambigua-tion, briefly covering necessary background, and present our approach. In Section 4, we make extensions to existing graph kernels for increased accuracy and scalability. Sec-tion 5 presents results of experiments on real-world data. In Section 6 we conclude and discuss future work.

We use lower-case bold letters a = [ a 1 ,...,a n ] T to denote vectors and a i denotes the i -th element of the vector. We http://graphlab.org/ use upper-case letters A,B,C,... to denote matrices with A ij referring to the element at the i -th row and j -th col-umn of A . Let A n denote the n -th power of A. Let a denote the n -th vector in a set of vectors. The Euclidean norm of a is denoted by k a k . Let 1 (  X  ) denote the indicator function and b X c the floor function. We use e = [1 ,..., 1] to denote a vector of all 1 X  X  of the appropriate size. We let e = [0 ,..., 1 ,..., 0] T denote a vector with the i -th element set to 1 and all other elements zero. We let A  X  B = C (of size mp  X  nq ) denote the Kronecker product [10] of matrices A (of size m  X  n ) and B (of size p  X  q ). We let G = ( V,E ) denote a graph with vertex set V = { v 1 ,...,v n } and edge set E = { e k : e k = ( v i ,v j )  X  v i  X  v j , v i ,v i  X  V } , A ( G ) the adja-cency matrix of G and  X ( G ) the maximum degree of G . Let V ( i )  X  V ( j ) denote the Cartesian product of vertex sets V and V ( j ) . We let S ( G ) ij denote the shortest path from node i to j in graph G . K ( G ( i ) ,G ( j ) ) =  X   X  ( G ( i ) denotes a kernel [26] on graphs G ( i ) , G ( j ) , where  X  : X 7 X  X  is a map into a Hilbert space H [26]. Whenever clear from context, we will omit G .
A large family of techniques are devoted to entity resolu-tion , the process in which references are matched with their underlying entities. This problem has two difficulties, 1) the same identifier may be used for several entities (e.g. Chris Anderson (TED) and Chris Anderson (WIRED)), and 2) the same entity may be referred to using several identifiers (e.g. Chris Anderson, Mr. Anderson). Bhattacharya and Getoor approach the problem using a probabilistic graphi-cal model [6] and hierarchical clustering [5]. In both meth-ods, similarity between identifiers is used, which makes them unusable in the anonymized setting. This notion can be gen-eralized to include methods using any set of entity attributes in the resolution process [7, 11, 12, 13, 24, 15, 27]. In this paper, entities are anonymized and have no such attributes. Further, we approach only the first difficulty stated above, rendering the above methods unsuitable for direct compari-son.

A specialized problem, related to entity resolution, is rela-tional entity disambiguation . Here, various kinds of network structure between entities are exploited. Bekkerman and MacCallum [3] use link structure of personal web pages to disambiguate people in social networks. Bhattacharya and Getoor [5] use author lists of research papers, forming a net-work, and name similarity, to disambiguate authors. Both of these methods however, leverage information that is not available in our setting.

Malin [23] approaches the problem of disambiguating en-tities based on network structure alone. In his setting, en-tities are related through a set of sources. His canonical example is that of entities being actors and sources being movies. Two actors are deemed connected if they appear in the same movie. Malin makes two attempts to solve the problem, one using hierarchical clustering, and one based on random walks. Since Malin X  X  random walk method per-formed the best in his experiments, and that to the best of our knowledge no other methods are applicable to our prob-lem, we use Malin X  X  random walk method for a state-of-the-art comparison. We denote Malin X  X  random walk method Malin .

Each entity (actor) has an identifier (name) which may or not be ambiguous. For each identifier of interest, Malin creates a graph in which the distinguished identifier is as-signed one node (here called source node) for every source in which it appears, and every other identifier is assigned a single node. An edge is placed between two nodes if the underlying identifiers co-occur in a source, the weight of the edges corresponds to the number of times the identifiers have co-occurred. Then, a series of random walks are made from each of the source nodes for the identifier of interest, to estimate the probability of reaching another source node. These probabilities are then used to decide, using a simple threshold, whether two different source nodes refer to the same underlying entity. Malin defines the similarity mea-sure between two source nodes a and b , as the average of the probability of a random walk starting in a reaching b and a walk starting in b reaching a . If the similarity measure of two source nodes is higher than the threshold, then these source nodes are deemed to refer to the same entity.
Malin allows for anonymized data, but demands knowl-edge of in which sources entities co-occur, e.g. a list of names for each movie in the dataset. This is a restriction on the type of datasets that can be used with the method since gen-eral networks do not have this type of source information. For example, most social networks have weighted relations connecting people, but no notion of sources. This means that Malin uses more information than the method pre-sented in this paper. In the experiments section we will see that this makes Malin inapplicable to datasets where the sources are not observed. Further, the output of Malin is a clustering of source nodes. The clusters represent the predicted underlying entities, associating each source node with only one entity. When using Malin for comparison, we label any identifier, in which all source nodes become part of a single cluster, as unambiguous and identifiers where the source nodes are part of more than one cluster as ambiguous .
This work attempts to classify nodes in a network as am-biguous or unambiguous. Building a classifier on nodes de-mands for a way of comparing them. While there are ker-nels for straight-forward node comparison in graphs, such as the diffusion kernel [19], these are defined on the full en-tity graph and consequently prohibitively computationally expensive for real-world graphs. Instead, we may compare the neighborhood structure of nodes, resulting in a graph classification (rather than node classification) problem.
Common graph kernels include shortest-path kernels [8], direct product kernels [17] and graphlet kernels [31]. We evaluate the performance of all of them for the entity disam-biguation task in our experiments. Moreover, we make ex-tensions to the shortest-path and direct product kernels and evaluate them in terms of computation speed and classifica-tion accuracy. While graph kernels have been used for e.g. protein structure prediction [9] or character recognition [2], to the best of our knowledge, no existing work uses this ap-proach for the problem of relational entity disambiguation.
For completeness, we note that several other graph ker-nels exist such as Weisfeiler-Lehman [30] kernels, inappli-cable to graphs without node attributes, and Fast Subtree Figure 1: (Left) Local neighborhood (fictive) of the ambiguous vertex Chris Anderson. (Right) Correct splitting of the vertex into its two true underlying entities. kernels [29]. We limit our scope, however to the three kernels mentioned above.
We let the term entity refer to a person or a company etc. while an identifier is a name or a label. If several entities have the same identifier, we say that the identifier is am-biguous . While a single entity may have several identifiers, we do not address this here; we focus only on ambiguities. In our setting, entities have hidden relationships which are observed through co-occuring identifiers such as names men-tioned in news articles. We let an identifier graph be the graph with one node for each identifier and an edge between every pair of co-occuring identifiers. Edges are weighted by the significance of the relationships, such as number of co-occurrences. Further, we assume that the identifiers have been assigned in a pseudo-random way.

This setting leads us to the definition of anonymized re-lational entity disambiguation as the following classification problem.

Problem definition 1. Given an undirected identifier graph G = ( V,E ) with edge weights w ij  X  R + and train-ing data S = { ( v i ,y i ) : 1  X  i  X  m,v i  X  V,y i  X  { X  1 }} that labels certain nodes as ambiguous (+) or unambiguous (  X  ) , classify new nodes as +1 or  X  1 . Each node of G may refer to a single entity or several underlying entities. The weight of an edge signifies the importance of the connection between two nodes.

Our problem is illustrated by the example of Chris An-derson, stated in the introduction and further explained by Figure 1. In the figure, two individuals called Chris An-derson have been assigned only one , common identifier and thus one common node in the graph. This error creates a strong connection between the two communities, TED and WIRED, something we would not expect in reality. It is ex-actly this type of unexpected property of the network that we aim to capture with our classifier. Although this exam-ple involves only people, we would like to emphasize that nodes can represent any type of entity; an equally trouble-some example would be that of the two cities Paris, France and Paris, Texas.

We approach the problem of identifying ambiguities in graphs with the intuition that the neighborhoods of am-biguous nodes have structure different from those of unam-biguous nodes. While Figure 1 is only an example, it illus-Algorithm 1 DetectAmb.Nodes( G = ( V,E ) , X  ) Input: G = ( V,E )
Input:  X  -Neighborhood size. for v i  X  V do end for Compute graph kernel matrix K ij ,  X  G ( i ) ,G ( j ) Train an SVM with K and labels { y i : y i  X  X  X  1 }} .
Output: SVM classifier. trates the concept of two communities having fused around an ambiguous node. This motivates us to build a classifier using features of the neighborhood structure. In a graph G = ( V,E ), we define the neighborhood N ( i ) G of a node v  X  V to be the subgraph of G induced by set of nodes connected to v i through an edge. This notion can easily be extended to larger neighborhoods by considering neigh-bors of neighbors. In general, we define the  X  -neighborhood N
G, X  of v i  X  V as the subgraph induced by the set of nodes { v j }  X  V for which there exists a path from v i to v j with s ( v i ,v j )  X   X  edges. Leaving out the subscript G for conve-nience, we have With  X  = 1 we recover the common neighborhood consisting of the distinguished node and its immediate neighbors. Our method consists of three steps, illustrated in Figure 2. First, an identifier graph G = ( V,E ) is constructed from the raw data. Each node represents one identifier that may correspond to one or several underlying entities. Some of the nodes are labeled + (ambiguous) or  X  (unambiguous) respectively. An edge between two nodes is present if the two corresponding identifiers are related in some way, and the edge weight represents the strength of the relation. For example, two identifiers may be related by co-occurring in the same article. The number of such co-occurrences is the weight of the edge. Note that we do not assume to have access to information about in which articles two identifiers co-occurred, only that they co-occurred in some articles. In the second stage, we extract the local neighborhoods of the nodes that we want to classify. For these local neighbor-hoods we then compute one of several graph kernels K in order to measure similarity between nodes. New nodes are labeled using a standard classifier. The overall approach is summarized in Algorithm 1.

Next, we describe a selection of graph kernels that are applicable to our approach.
The direct product (DP) kernel compares pairs of graphs based on the number of common walks [17]. The common walks are computed by creating the direct product graph [17] G  X  = ( V  X  ,E  X  ) of two graphs G (1) = ( V (1) ,E (1) ) and G ( V (2) ,E (2) ), defined for unlabeled 2 graphs [8, 17] as: The kernel value is computed using where  X  &lt;  X ( G  X  )  X  1  X  R + is a decay factor for making the sum converge.

The graphlet (GL) kernel compares graphs through their distributions of graphlets of size 3  X  5 [31], i.e. induced subgraphs of k  X  { 3 , 4 , 5 } nodes. As exhaustive enumera-tion of all graphlets is infeasible, the graphlet distribution is approximated using sampling, and by considering a finite number of values of k . Shervashidze et al. [31] show that for  X  &gt; 0, &gt; 0, samples suffice to ensure that P {k D  X   X  D n k 1  X  } X   X  , where D is the true distribution and  X  D n is the approximate distri-bution using n samples.

The shortest path (SP) kernel compares graphs based on the similarity of their shortest paths [8]. All the shortest paths in a graph can be obtained in O ( n 3 ) time using the Floyd-Warshall algorithm [16]. We let S G ij denote the short-est distance between nodes v i and v j in graph G . For un-weighted graphs, the shortest distance between v i and v j the number of steps on the shortest v i  X  v j path, and for weighted graphs it is the sum of all weights on the minimum weight v i  X  v j path. The shortest-path kernel is defined as: where k (1) walk is a positive definite kernel on edge walks of length 1.
In this section, we design simple extensions of the DP and SP kernels, which we show can be computed quickly by explicitly knowing the kernel mapping  X  .
For unlabeled graphs, the DP kernel (4) can be decom-posed into components that can be calculated independently for each graph. For our proof, we need first to note that [32] Kronecker product powers have the property that [10](p.775)
For labeled graphs, see G  X  artner et al. [17] selected, as would be the case when training the classifier. Algorithm 2 u = NumRandWalk ( G = ( V,E ) ,K ) Input: G = ( V,E ) Input: K -Maximum walk length.

Initialize t (0) i = 1 ,  X  i : v i  X  V ; u 0 = | V | for n  X  X  1 ,...,K } do end for Output: u -Random walk counts.
 From the definition of the Kronecker product we get
Now, we can derive a useful representation of the DP ker-nel as follows: be written as an inner product  X   X  ( G (1) ) , X  ( G (2) )  X  , making it a valid kernel [26]. We approximate (10) by only considering walks up to a finite length K , setting the kernel value of all longer paths to zero. This makes the dimension of the feature vectors  X  finite, and trivially, the kernel is still valid. We call this the truncated direct product (TDP) kernel. For clarity, the kernel is defined as: calculated using Algorithm 2. Note that making the sum in (10) finite allows us to set  X  &gt;  X ( G  X  )  X  1 TDP kernel defined where the DP kernel is not. As the average degree of a graph is 2 | E | | V | , the time complexity for calculating the TDP kernel becomes O ( K | E | ).

For general graphs, exact approaches take O ( | V | 6 and O ( | V | 3 ) [32] time to calculate the DP kernel for exact solutions. If the graph is sparse, i.e. has O ( | V | ) edges, then the calculation can be done in O ( | V | 2 ) [32] time. For an approximate solution in the general case, the kernel can be calculated in O ( | V | 2 ) [18] time. Therefore, our approach is advantageous if K | V | and the graph is sparse.

Long walks (large K ) tend to result in a phenomenon known as tottering in which the walks of the DP kernel will go back and forth along the same nodes, over and over. Tottering reduces the expressivity of the kernel as the same cycles of nodes will be visited repeatedly [22]. This suggests that for good generalization performance, as suggested em-pirically by Borgwardt and Kriegel [8], K should be small.
The TDP kernel can be modified further to suit to our specific classification problem. We note that the graphs we are comparing are in fact pointed graphs, in that they are the neighborhood of one distinguished vertex. Instead of counting the number of random walks from all vertices, we can choose to only count walks from the distinguished ver-tex in the middle of the local neighborhood graph (i.e. the vertex  X  X hris Anderson X  in Figure 1). This enables us to collect more specific information concerning only the local neighborhood of the vertex of interest. Thus, we create a distinguished vertex modification of the TDP kernel as fol-lows: of index d . Essentially, only the definition of the vector u has changed. Algorithm 2 can be easily modified to account for walks only from the distinguished vertex by setting t (0) only for the distinguished vertex in the initialization step of the algorithm, and 0 for all other vertices.

We make a note that this definition is a special case of the alternative definition of the DP kernel in Vishwanathan et al. [32]. They incorporate starting and stopping probabili-ties p  X  and q  X  for the random walks, giving the following graph kernel: where  X  is a discrete measure and W  X  is the weight matrix associated with A ( G  X  ). By ignoring edge weights and set-ting  X  ( n ) =  X  n , p  X  = e and q  X  = e d , we recover (12) with K =  X  .
We seek to construct a computationally efficient version of the SP kernel for weighted graphs. Consider using the indicator function for k (1) walk in the shortest-path kernel as in Borgwardt and Kriegel [8]. Inserting this into (6) leaves us with For unweighted or integer weighted graphs, S ij are integers, making the indicator function a reasonable choice of ker-nel. However, for real weighted graphs, this formulation is less sensible as it involves comparing real numbers for equality. Therefore, we design a simple heuristic extension of (14) for general weighted graphs with the idea of compar-ing rounded-off values of the real-valued weights. Formally, we define a function h : R + 7 X  X  1 , 2 ,...,M } that maps dis-tance values to a finite set of M bins. This gives us the definition of the binned shortest distance (BSD) kernel as where  X  G ijk = 1 ( h ( S G i,j ) = k ). This definition can be thought of as a relaxation of (14) in which the indicator has value 1 if the compared values are similar enough. Equation (15) can easily be written as the inner product  X   X  ( G (1) , X  ( G showing that it is a valid kernel [26].

By applying different types of binning (choosing the func-tion h ), the kernel can be made to consider similar distances to be equal, as opposed to the indicator version of the SP kernel (14). We believe this is advantageous in weighted graphs, as we will show empirically in our experiments. A simple binning function is the linear binning defined by where S max = max G,i,j S G ij is the maximum shortest dis-tance S G ij encountered in the dataset and  X  R + is a small constant. Since the kernel mappings  X  are explicitly known for the TDP and BSD kernels, we avoid expensive computation of the kernel values K ij for each pair of graphs. In fact, we do not need to compute the Gram matrix K at all, since ex-plicitly knowing  X  allows us to use a fast linear SVM solver, making our approach in Section 3 usable in O ( mT ) time. This procedure is shown in Algorithm 3. We use Pega-sos [28], a state-of-the-art iterative subgradient method for training the SVM classifier. Pegasos has the property of be-ing independent of the training set size m when using linear kernels, which is to our advantage, as previously described. Note that this independence only holds when  X  is known, as the Pegasos algorithm becomes directly dependent on m when using kernels in the general case [28].
 We make the computation of  X  scalable by leveraging GraphLab [21], a library for doing distributed computa-tion. By following the restrictions of the GraphLab frame-work, scalable computation on graphs can be easily achieved. Algorithm 3 DetectAmb.NodesFast( G = ( V,E ) , X  ) Input: G = ( V,E )
Input:  X  -Neighborhood size. for v i  X  V do end for Compute feature mappings  X  ( G ( i ) ) ,  X  G ( i ) Train a linear SVM with labels { y i : y i  X  X  X  1 }} . Output: SVM classifier.
 Since we explicitly know the mapping  X  , we can easily design algorithms that fit the GraphLab framework for computing  X  for each graph in an efficient manner.
We can make the TDP and BSD kernels less sensitive to the graph size by normalizing the feature vectors, giving the kernels the form This definition of the TDP and BSD kernels is equivalent to the cosine similarity measure [25]. We show in our experi-ments that normalization indeed helps increase classification performance.
We evaluate our approach by comparing it, using a large selection of graph kernels, to the approach of Malin [23], denoted Malin . We address two questions, 1) how well have our extensions improved existing graph kernels, 2) how well does our method fair against a state-of-the-art method. The experiments are conducted on two real-world datasets, one of which is readily available. We investigate a proprietary dataset (RF) from Recorded Future 3 , a company specializing in web intelligence and pre-dictive analytics. The data has been gathered using au-tomatic processing of articles from news and social media. When an entity, e.g. a person, place or company, was found in an article, it was assigned a (possibly ambiguous) canon-ical identifier.

The RF dataset contains information from around 10 mil-lion articles. The identifier graph of the set has a node for each identifier in the set of articles and an edge ( v identifiers v i and v j have co-occurred in an article. Each edge is associated with a weight w ij equal to the number of times i and j have co-occurred. The graph contains 2,155,893 nodes and 13,935,815 edges and has 91 nodes that have been manually labeled as either ambiguous (+1) or unambiguous (  X  1). The average size for each local neigh-borhood graph of the labeled nodes was 267 nodes and 5,830 edges. 39.6% of the labeled nodes have a positive label. Note that in this dataset we do not have access to the actual ar-ticles that is the cause of the relations, only the fact that certain names co-occurred in a number of articles, making Malin X  X  random walk method inapplicable to this dataset. http://www.recordedfuture.com/
The Internet Movie Database 4 (IMDb) is a database gath-ering information about the televised entertainment indus-try, including movies, actors and production personnel.
Following Malin [23], we artificially introduced ambigui-ties by treating all actors with the same last name as one, merging them into a single node corresponding to a sin-gle identifier e.g.  X  X mith X . An edge was added between two nodes if the corresponding identifiers were part of the same cast list in a movie. Edges were weighted by the total number of times a pair of nodes starred in movies together. Additionally, we only included movies with more than one actor, thus ensuring that every node is adjacent to at least one other node. Actors who had not starred together with any other actor were removed from the dataset. Just like in Malin [23], we only considered movies between 1994-2003. It is important to note that our dataset is not the exact same as the one used in Malin [23]. This is due to the fact that the IMDb database has been updated since. The dataset used in by Malin [23] had  X  37,000 movies and  X  180,000 distinct entities, while our dataset consists of 52,004 movies and 2,272,504 distinct entities.

When all actors with the same last name had been merged, the resulting graph contained 150,072 nodes, corresponding to the distinct last names, and 9,283,233 edges. In Malin [23] they reported  X  85,000 distinct last names. We created a training set by randomly selecting 100 identifiers, such that half of them were ambiguous. The average size for each local neighborhood graph of the labeled nodes was 156 nodes and 12,028 edges. We denote this dataset IMDb 1.

In IMDb 1 there exists several identifiers that appear in one movie only. In fact, out of the 100 identifiers chosen, 38 was part of only one source. Out of these 38 identifiers, 36 were non-ambiguous, while 2 were ambiguous. Consistent with intuition, it is very unlikely that an actor appearing in only one movie is ambiguous. This, perhaps undesired, property of the dataset led us to create a second set based on the IMDb data. This dataset contains the same data, the only difference being that the 100 identifiers, chosen for evaluation, are required to be a part of more than one movie. In the resulting set, the average number of nodes and edges in each local neighborhood were 279 and 24,550 respectively. We denote this alternative dataset IMDb 2.
We evaluate the performance of five different graph ker-nels (GL, DP, SP, TDP, BSD), including extensions, within our approach using 10-fold cross validation with the Pega-sos SVM solver [28]. For all kernels we use only the 1-neighborhood (  X  = 1) for comparing nodes. This reduces computation time, but is also motivated from the intuition that the larger the neighborhood included, the less specific it is to the node of interest.
 We transform all edge weights w ij with a function  X  : R + 7 X  R + , X  0 &lt; 0 so that strong connections (many co-occurrences) become small weights, in the belief that short-est paths should go along strong connections. In our ex-periments, we try logarithmic and inverse edge transforms, defined as:  X  log ( w ij ) = ln w max  X  ln w ij and  X  inv where w max is the maximum edge weight in the graph. Note that the transformations are applied prior to the computa-http://www.imdb.com/interfaces Table 1: Classification accuracy (%) on 10-fold cross-validation. The subscript n stands for normal-ization and d for the distinguished vertex version of the TDP kernel. The DP kernel on IMDb2 did not finish within in 48 hours and was left out. Note that Malin is not applicable to the RF dataset as explained in Section 5.1. tion of the distances S G ij with the Floyd-Warshall algorithm. The transforms only affect the BSD and SP kernels, as the other kernels do not consider edge weights. For the BSD kernel, we use only linear binning, as defined in (16).
We believe that using linear binning is sufficient, since the edge transform  X  already preprocesses the edges in a non-linear manner. The logarithmic edge transform allows our kernel to capture the intuition that there exist many weak connections and few very strong connections. Achiev-ing good binning resolution in both cases motivates the use of the logarithm. Figure 3, depicting the weight distribution of the two datasets, supports this intuition.

For the TDP kernel, we perform a grid search over the pa-rameters, setting  X   X  X  0 . 2 , 0 . 4 , 0 . 6 , 0 . 8 } and K = ln giving K  X  { 7 , 12 , 22 , 51 } . For the BSD kernel, we try set-ting the number of bins M  X  { 5 , 10 , 25 , 50 } . As both the above kernels can be normalized, we try enabling and dis-abling normalization for these. For the GL kernel, we used =  X  = 0 . 05. The parameter  X  of the DP kernel was set for each dataset so that  X  &lt;  X ( G  X  )  X  1 , giving  X  = 0 . 00067 for RF and  X  = 0 . 00098 for IMDb 1. The DP kernel on IMDb 2 did not finish within 48 hours and was left out be-cause of this. For all kernels, we optimize the Pegasos SVM parameters using cross-validation.

For Malin , we started 100 and 1000 random walks from each source of the node being classified, and for each walk we did 50 steps. Early tests showed that increasing the number of steps did not increase performance. Note that in Malin [23], only 100 walks were used, albeit on a different dataset. On IMDb 1 and IMDb 2, however, more walks could be needed, as they are of larger size. We then tested for the best threshold from the set { 0 . 00 , 0 . 01 ,..., 1 . 00 } .
The best results of our experiments, in terms of classifica-tion accuracy on the three datasets, are shown in Table 1. The GL , SP and DP kernels are the original kernels as de-scribed in Section 3. The DP kernel was computed using the Sylvester equation method of Vishwanathan et al. [32] and the SP kernel used the indicator function as in (14). The BSD and TDP kernels are the extensions of the SP and DP kernels respectively, proposed in Section 4. In Table 2, the Figure 3: Histogram of edge weights in the RF (left) and
IMDb (right) datasets, with the number of edges Table 2: Wall clock time for detecting ambiguities for all training examples, measured in seconds 5 . The subscript n stands for normalization and d for the distinguished vertex version of the TDP kernel.  X  The GL kernel was implemented in MATLAB. All others were run in C ++ using GraphLab. The DP kernel on IMDb2 did not finish within in 48 hours and was left out. Note that Malin is not applicable to the RF dataset as explained in Section 5.1. wall-clock times needed for detecting ambiguities using the various methods are presented.

Preliminary results indicated that the performance did not improve with  X  &gt; 1. Further experiments with  X  &gt; 1 were not performed because of this fact and the intuition that the larger the size of the extracted neighborhoods, the less they will describe the specific characteristics of the cen-tral vertex. When using larger neighborhoods, the graph sizes increase rapidly, reaching close to the size of the origi-nal identifier graph already for  X  = 2. It was thus infeasible to run these experiments within reasonable time.
 We proceed to compare the results of our method with Malin based on the results in Table 1. In the RF dataset, there is no source information available, making it is impos-sible to run experiments for Malin on this dataset.
We now summarize the parameters giving the best per-formance during experiments. The TDP kernel performed best with  X  = 0 . 2 for the RF dataset,  X  = 0 . 8 on the IMDb dataset and  X  = 0 . 6 on the IMDb 2 dataset. The SP ker-8 threads on a cluster with 5 computation nodes and 8 Intel Xeon 2.4 GHz cores per node was used. nel performed best with inverse edge transform on the RF dataset and logarithmic edge transform on the IMDb 1 and IMDb 2 datasets. The BSD kernel with logarithmic edge transform on the RF dataset gave the best result, using 10 linear bins. On the IMDb 1 and IMDb 2 datasets, the BSD kernel with logarithmic edge transform and 25 and 5 linear bins, respectively, gave the best result.

For Malin , on IMDb 1, the best number of walks was 100, and the best threshold 0 . 00. On IMDb 2, Malin got the best accuracy with 1000 walks and the threshold being 0 . 17.
Our approach ( GL , BSD ) gets comparable results to Ma-lin on IMDb 1, while being significantly faster ( GL ), see Ta-ble 2. Our approach outperforms Malin by a wide margin on IMDb 2. IMDb 1 contains identifiers that only appeared in one movie, which in our setting means that we have very little information to base the classification on. Also, lin predicts all single-source nodes to be non-ambiguous by default. Since most of these nodes actually are non-ambiguous, this means that Malin works quite well on the IMDb 1 dataset. In fact, any classifier that has access to the sources would be able to get a very good result on IMDb 1 by classifying all nodes with more than one source as ambigu-ous and the rest as non-ambiguous. Our method however does not have access to the sources and still gives similar performance, with many of our kernels taking significantly smaller amount of time. This simple classification method, for classifiers that have access to the sources, is however not very useful on the more challenging dataset IMDb 2, and thus Malin gets a worse result on that dataset. We also note that Malin takes significantly longer time to run on the IMDb 2 dataset, compared to IMDb 1. This is because of the fact that IMDb 2 contains more sources per node on av-erage, increasing the number of walks performed by Malin . Also, Malin got the best result with 1000 walks per source node, instead of 100 like on IMDb 1, this also increased the running time significantly. With 100 walks Malin got only 56% accuracy and took 3,957 seconds to run.

In our graph kernel-based approach, the time complexity for training the classifier is independent of the number of sources the data contained. Although the computation time will increase if the number of edges increases, increasing the number of sources will not increase the computation time un-less it introduces new edges into the graph. New edges will be added if a source introduces a new pair of identifiers not previously encountered together in a source. New sources without any new pairs of identifiers will then only increase the weight of edges and therefore not affect the computation time of our method. This behavior is in contrast to Malin which increases linearly in computation time with the num-ber of sources. Since the number of sources potentially can be very big in real world datasets, this time dependence on the number of sources could in some cases be a problem. If we assume a scenario where we have access to a very large amount of sources, then as long as a new source does not introduce a new edge, including that source in the dataset will not increase the computation time of our graph kernel approach, giving us an advantage over Malin in this setting. When it comes to running time, the GL and TDP kernels finished faster than Malin on both datasets, with the TDP kernel being particularly noteworthy.

On a final note, given the speed of our method, it could be used as a potential preprocessing step, whereafter a clus-tering method such as Malin could be run on the filtered smaller selection of nodes, dividing them into their correct underlying entities. This, of course, assumes a scenario where the sources are observed. It would not work on the RF dataset, for which further investigation needs to be done.
In terms of accuracy, the BSD and GL kernels outperform the other kernels when it comes to the RF dataset. On the IMDb 1 dataset GL performs the best of the kernels, with the normalized BSD kernel close behind. On the more challenging dataset IMDb 2, the GL kernel performed the best.

We note that both of our extended kernels outperformed their original counterparts. For the DP kernel, this is espe-cially noteworthy due to the timing results of Table 2. The TDP kernel variations all perform significantly better to a fraction of the computational cost. A possible reason for why the TDP kernel would perform better than the DP kernel is tottering , as described in Section 4. Since calculating the DP kernel is the same as calculating all walks of length up to infinity, tottering actually outweighs the relevant walks, making the DP kernel perform worse than a kernel that only calculates walks up to a certain length, i.e. the TDP kernel.
Using normalization helped the BSD and TDP kernels get better or equal accuracy on all datasets, indicating that the normalization might help the kernels become less sensi-tive to graph size. We also note that the BSD kernel out-performs the SP kernel on both datasets, indicating that binning seems to help the kernel get a better classification accuracy on weighted graphs.

The experiments with the distinguished node for the TDP kernel ( TDP dn ), gave a slight increase in performance, but not enough to say that the TDP dn kernel is better than the TDP n kernel.

The experiments indicate that the edge transform  X  log gen-erally performs better than  X  inv . The only exceptions to this was the BSD kernel on the IMDb 2 dataset and the SP kernel on the RF dataset.

As the BSD and TDP kernels can be calculated algorith-mically an order of magnitude faster than na  X   X ve pairwise ker-nel computation, it is uninteresting to compare this increase in computation speed. Indeed, the TDP kernel runs much Figure 4: Speedup of BSD kernel precomputation as the number of CPUs increase on the RF dataset.
 Linear speedup is shown by the solid line. faster than any of the other approaches in our experiments. However, with a running time of a few seconds, investigat-ing the scalability of the TDP kernel would require a larger dataset. Rather, we focus on investigating how well the com-putation of shortest distances for the BSD kernel scales when using GraphLab. The experiments were carried out on the RF dataset. A cluster with 5 computation nodes with 8 Intel Xeon 2.4 GHz cores and 23 . 5 GB RAM per node was used. Figure 4 shows a comparison with linear speedup. Speedup was defined as t 1 t pute the graph kernels using k CPUs. In the figure, serial means running GraphLab on each local neighborhood graph separately, whereas batch denotes putting all local neigh-borhoods in one big graph, on which GraphLab is run just once. Spread denotes running the threads evenly divided on several physical machines, whereas tight denotes running as many threads per machine as there are CPU cores. We see that the batch-tight computation mode gets the best speedup, increasing almost linearly up until 8 CPUs.
In this paper we have developed a novel formulation of anonymized relational entity disambiguation as a graph clas-sification problem. We have devised a method for detecting ambiguities in graph data based on local neighborhoods us-ing graph kernels. Our approach was compared to a state-of-the-art method, showing significantly better performance in terms of either accuracy or speed or both.

We have shown how to enable fast computation of the direct product (DP) kernel [17] by extending it to a trun-cated direct product (TDP) kernel that outperformed the DP kernel in our experiments. We have also presented a simple extension of the shortest-path kernel [8], creating the binned shortest distance (BSD) kernel, as a way of measur-ing similarity between general weighted graphs. The BSD kernel was shown to give good classification results, outper-forming the base shortest-path kernel on weighted graphs, which motivates the need for binning. Normalization of the TDP and BSD kernels also helped boost performance. We show a significant speedup in the computation of the kernels when using GraphLab and by explicitly knowing the kernel mapping  X  .

Future research should examine the possibility of applying our method in a semi-supervised setting, by considering un-labeled samples. Other aspects we will look into is the prob-lem of actually associating ambiguous identifiers with their underlying entities. It should be noted that our method can be used as a fast indicator of which identifiers to examine more closely for ambiguity. Larger datasets should be ex-amined, in order to further investigate the scalability of our method. Given the good performance of the GL kernel in our experiments, it is of great interest to attempt at design-ing an extension to the GL kernel, which considers weighted graphs.
We would like to thank Recorded Future, a software com-pany, and their employees for their insight and help and for providing data. We would also like to thank Bhavishya Goel and Jacob Lidman for help with initial setup of the GraphLab framework. This work was supported by the SSF grant  X  X ata Driven Secure Business Intelligence X . [1] C. C. Aggarwal and S. Y. Philip. A condensation [2] F. R. Bach. Graph kernels between point clouds. In [3] R. Bekkerman and A. McCallum. Disambiguating web [4] I. Bhattacharya and L. Getoor. Iterative record linkage [5] I. Bhattacharya and L. Getoor. Entity resolutions in [6] I. Bhattacharya and L. Getoor. A latent dirichlet [7] C. B  X  ohm, G. de Melo, F. Naumann, and G. Weikum. [8] K. M. Borgwardt and H.-P. Kriegel. Shortest-path [9] K. M. Borgwardt, C. S. Ong, S. Sch  X  onauer, [10] J. Brewer. Kronecker products and matrix calculus in [11] R. Bunescu and M. Pasca. Using encyclopedic [12] S. Cucerzan. Large-scale named entity disambiguation [13] C. P. Diehl, L. Getoor, and G. Namata. Name [14] C. Dwork. Differential privacy: A survey of results. In [15] T. Elsayed, D. W. Oard, and G. Namata. Resolving [16] R. W. Floyd. Algorithm 97: shortest path.
 [17] T. G  X  artner, P. Flach, and S. Wrobel. On graph kernels: [18] U. Kang, H. Tong, and J. Sun. Fast random walk [19] R. I. Kondor and J. Lafferty. Diffusion kernels on [20] K. Liu, K. Das, T. Grandison, and H. Kargupta. [21] Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, [22] P. Mah  X e, N. Ueda, T. Akutsu, J.-L. Perret, and J.-P. [23] B. Malin. Unsupervised name disambiguation via [24] V. Rastogi, N. N. Dalvi, and M. N. Garofalakis. [25] G. Salton and M. J. McGill. Introduction to Modern [26] B. Sch  X  olkopf and A. J. Smola. Learning with kernels: [27] P. Sen. Collective context-aware topic models for [28] S. Shalev-Shwartz, Y. Singer, N. Srebro, and [29] N. Shervashidze and K. M. Borgwardt. Fast subtree [30] N. Shervashidze, P. Schweitzer, E. J. van Leeuwen, [31] N. Shervashidze, S. Vishwanathan, T. Petri, [32] S. Vishwanathan, N. N. Schraudolph, R. Kondor, and
