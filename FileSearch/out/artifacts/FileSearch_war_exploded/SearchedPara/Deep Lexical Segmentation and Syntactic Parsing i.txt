 Lexical segmentation is a crucial task for natural language understanding as it detects semantic units of texts. One of the main difficulties comes from the identification of multiword expressions [MWE] (Sag et al., 2002), which are sequences made of mul-tiple words displaying multidimensional idiomatic-ity (Nunberg et al., 1994). Such expressions may ex-hibit syntactic freedom and varying degree of com-positionality, and many studies show the advantages of combining MWE identification with syntactic parsing (Savary et al., 2015), for both tasks (Wehrli, 2014). Indeed, MWE detection may help parsing, as it reduces the number of lexical units, and in turn parsing may help detect MWEs with syntactic free-dom (syntactic variations, discontinuity, etc.).
In the dependency parsing framework, some pre-vious work incorporated MWE annotations within syntactic trees, in the form of complex subtrees ei-ther with flat structures (Nivre and Nilsson, 2004; Eryi  X  git et al., 2011; Seddah et al., 2013) or deeper ones (Vincze et al., 2013; Candito and Constant, 2014). However, these representations do not cap-ture deep lexical analyses like nested MWEs. In this paper, we propose a two-dimensional repre-sentation that separates lexical and syntactic layers with two distinct dependency trees sharing the same tion of complex lexical phenomena like embedding of MWEs (e.g. I will (take a (rain check)) ). Given this representation, we present two easy-first depen-dency parsing systems: one based on a pipeline ar-chitecture and another as a joint parser. This section describes a lexical representation able to handle nested MWEs, extended from Constant and Le Roux (2015) which was limited to shallow MWEs. Such a lexical analysis is particularly rele-vant to perform deep semantic analysis.

A lexical unit [LU] is a subtree of the lexical seg-mentation tree composed of either a single token unit or an MWE. In case of a single token unit, the sub-tree is limited to a single node. In case of an MWE, the subtree is rooted by its leftmost LU, from which there are arcs to every other LU of the MWE. For instance, the MWE in spite of made of three single token units is a subtree rooted by in . It comprises two arcs: in  X  spite and in  X  of . The MWE make big deal is more complex as it is formed of a single token unit make and an MWE big deal . It is repre-sented as a subtree whose root is make connected to the root of the MWE subtree corresponding to big deal . The subtree associated with big deal is made of two single token units. It is rooted by big with an arc big  X  deal . Such structuring allows to find nested MWEs when the root is not an MWE itself, like for make big deal . It is different for the MWE Los Angeles Lakers comprising the MWE Los Ange-les and the single token unit Lakers . In that case, the subtree has a flat structure, with two arcs from the node Los , structurally equivalent to in spite of that has no nested MWEs. Therefore, some extra infor-mation is needed in order to distinguish these two cases. We use arc labels.

Labeling requires to maintain a counter l in or-der to indicate the embedding level in the leftmost LU of the encompassing MWE. Labels have the form sub l mwe for l  X  0 . Let U = U 1 ...U n be a LU composed of n LUs. If n = 1 , it is a sin-gle token unit. Otherwise, subtree ( U, 0) , the lexical subtree 2 for U is recursively constructed by adding arcs subtree ( U 1 , l +1) sub l mwe  X  X  X  X  X  X  X  X  X  subtree ( U i i 6 = 1 . In the case of shallow representation, every LUs of U are single token units.

Once built the LU subtrees (the internal depen-dencies ), it is necessary to create arcs to connect them and form a complete tree : that we call ex-ternal dependencies . LUs are sequentially linked together: each pair of consecutive LUs with roots ( w i , w j ), i &lt; j , gives an arc w i and Figure 2 respectively display the deep and shal-low lexical segmentations of the sentence The Los Angeles Lakers made a big deal out of it .

For readibility, we note mwe for sub 0 mwe and submwe for sub 1 mwe . 3.1 Easy-first parsing Informally, easy-first proposed in Goldberg and El-hadad (2010) predicts easier dependencies before risky ones. It decides for each token whether it must be attached to the root of an adjacent subtree and which these decisions are made is not decided in ad-vance: highest-scoring decisions are made first and constrain the following decisions.

This framework looks appealing in order to test our assumption that segmentation and parsing are mutually informative, while leaving the exact flow of information to be learned by the system itself: we do not postulate any priority between the tasks nor that all attachment decisions must be taken jointly. On the contrary, we expect most decisions to be made independently except for some difficult cases that need both lexical and syntactic knowledge.
We now present two adaptations of this strategy to build both lexical and parse trees from a unique se-tures linking information from the two dimensions. 3.2 Pipeline Architecture In this trivial adaptation, two parsers are run sequen-tially. The first one builds a structure in one dimen-sion (i.e. for segmentation or syntax). The second one builds a structure in the other dimension, with the result of the first parser available as features. 3.3 Joint Architecture The second adaptation is more substantial and takes the form of a joint parsing algorithm. This adap-tation is provided in Algorithm 1. It uses a single classifier to predict lexical and syntactic actions. As in easy-first, each iteration predicts the most cer-tain head attachment action given the currently pre-dicted subtrees, but here it may belong to any di-mension. This action can be mapped to an edge in the appropriate dimension via function E DGE . Func-tion score(a,i) computes the dot-product of feature weights and features at position i using surrounding Algorithm 1 Joint Easy-first parsing We can reuse the reasoning from Goldberg and Elhadad (2010) and derive a worst-case time com-plexity of O ( n log n ) , provided that we restrict fea-ture extraction at each position to a bounded vicinity. 4.1 Datasets We used data sets derived from three different ref-erence treebanks: English Web Treebank (Linguis-tic Data Consortium release LDC2012T13)[EWT], French treebank (Abeill  X  e et al., 2003) [FTB], Se-quoia Treebank (Candito and Seddah, 2012) [Se-quoia]. These treebanks have MWE annotations available on at least a subpart of them. For EWT, we used the STREUSLE corpus (Schneider et al., 2014b) that contains annotations of all types of MWEs, including discontiguous ones. We used the train/test split from Schneider et al. (2014a). The FTB contains annotations of contiguous MWEs. We generated the dataset from the version described in Candito and Constant (2014) and used the shallow lexical representation, in the official train/dev/test split of the SPMRL shared task (Seddah et al., 2013). The Sequoia treebank contains some limited annotations of MWEs (usually, compounds having an irregular syntax). We manually extended the cov-erage to all types of MWEs including discontiguous ones. We also included deep annotation of MWEs (in particular, nested ones). We used a 90%/10% train/test split in our experiments. Some statistics about the data sets are provided in table 4.1. Tokens were enriched with their predicted part-of-speech as in Candito and Constant (2014). 4.2 Parser and features Parser. We implemented our systems by modify-line. We trained all models for 20 iterations with dynamic oracle (Goldberg and Nivre, 2013) using the following exploration policy: always choose an oracle transition in the first 2 iterations ( k = 2 ), then choose model prediction with probability p = 0 . 9 . Features. One-dimensional features were taken directly from the code supporting Goldberg and Nivre (2013). We added information on typograph-ical cues (hyphenation, digits, capitalization, . . . ) and the existence of substrings in MWE dictionaries in order to help lexical analysis. Following Constant et al. (2012) and Schneider et al. (2014a), we used dictionary lookups to build a first naive segmenta-tion and incorporate it as a set of features. Two-dimensional features were used in both pipeline and joint strategies. We first added syntactic path fea-tures to the lexical dimension, so syntax can guide segmentation. Conversely, we also added lexical path features to the syntactic dimension to provide information about lexical connectivity. For instance, two nodes being checked for attachment in the syn-tactic dimension can be associated with information describing whether one of the corresponding node is an ancestor of the other one in the lexical dimension (i.e. indicating whether the two syntactic nodes are linked via internal or external paths).

We also selected automatically generated features combining information from both dimensions. We chose a simple data-driven heuristics to select com-bined features. We ran one learning iteration over the FTB training corpus adding all possible combi-nations of syntactic and lexical features. We picked the templates of the 10 combined features whose scores had the greatest absolute values. Although this heuristics may not favor the most discriminant features, we found that the chosen features helped accuracy on the development set. 4.3 Results For each dataset, we carried out four experiments. First we learned and ran independently two distinct baseline easy-first parsers using one-dimensional features: one producing a lexical segmentation, an-other one predicting a syntactic parse tree. We also trained and ran a joint easy-first system predict-ing lexical segmentations and syntactic parse trees, using two-dimensional features. We also experi-mented the pipeline system for each dimension, con-sisting in applying the baseline parser on one dimen-sion and using the resulting tree as source of two-dimensional features in a standard easy first parser applied on the other dimension. Since pipeline ar-chitectures are known to be prone to error propaga-tion, we also run an experiment where the pipeline second stage is fed with oracle first-stage trees.
Results on the test sets are provided in table 2, where LAS and UAS are computed with punctua-tion. Overall, we can see that the lexical information tends to help syntactic prediction while the other way around is unclear.
 The first striking observation is that the syntactic di-mension does not help the predictions in the lexi-cal dimension, contrary to what could be expected. In practice, we can observe that variations and dis-continuity of MWEs are not frequent in our data sets. For instance, Schneider et al. (2014a) notice that only 15% of the MWEs in EWT are discontigu-ous and most of them have gaps of one token. This could explain why syntactic information is not use-ful for segmentation. On the other hand, the lexical dimension tends to help syntactic predictions. More precisely, while the pipeline and the joint approach reach comparable scores on the FTB and Sequoia, the joint system has disappointing results on EWT. The good scores for Sequoia could be explained by the larger MWE coverage.

In order to get a better intuition on the real impact of each of the three approaches, we broke down the syntax results by dependency labels. Some labels are particularly informative. First of all, the preci-sion on the modifier label mod , which is the most frequent one, is greatly improved using the pipeline approach as compared with the baseline (around 1 point). This can be explained by the fact that many nominal MWEs have the form of a regular noun phrase, to which its internal adjectival or preposi-tional constituents are attached with the mod label. Recognizing a nominal MWE on the lexical dimen-sion may therefore give a relevant clue on its cor-responding syntactic structure. Then, the dep cpd connects components of MWE with irregular syntax that cannot receive standard labels. We can observe that the pipeline (resp. the joint) approach clearly improves the precision (resp. recall) as compared with the baseline (+1.6 point). This means that the combination of a preliminary lexical segmentation and a possibly partial syntactic context helps im-proving the recognition of syntax-irregular MWEs. Coordination labels ( dep.coord and coord ) are par-ticularly interesting as the joint system outperforms the other two on them. Coordination is known to be a very complex phenomenon: these scores would tend to show that the lexical and syntactic dimen-sions mutually help each other.

When comparing this work to state-of-the-art sys-tems on data sets with shallow annotation of MWEs, we can see that we obtain MWE recognition scores comparable to systems of equivalent complexity and/or available information. This means that our novel representation which allows for the annotation of more complex lexical phenomena does not dete-riorate scores for shallow annotations. In this paper we presented a novel representation of deep lexical segmentation in the form of trees, forming a dimension distinct from syntax. We ex-perimented strategies to predict both dimensions in the easy-first dependency parsing framework. We showed empirically that joint and pipeline process-ing are beneficial for syntactic parsing while hardly impacting deep lexical segmentation.

The presented combination of parsing and seg-menting does not enforce any structural constraint future work. We will explore less redundant, more compact representations of the two dimensions since some annotations can be factorized between the two dimensions (e.g. MWEs with irregular syntax) and some can easily be induced from others (e.g. se-quential linking between lexical units).
 This work has been partly funded by the French Agence Nationale pour la Recherche , through the PARSEME-FR project (ANR-14-CERA-0001) and as part of the Investissements d X  X venir program (ANR-10-LABX-0083)
