 Text document clustering discovers groups of related documents in large document col-lections. It achieves this by optimizing an objective function defined over the entire data collection. The importance of document clustering has grown significantly over the years as the world moves toward a paperless environment and the Web continues to dominate our lives. Efficient and effective document clustering methods can help in better document organization (e.g. digital libraries, corporate documents, etc) as well as quicker and improved informatio n retrieval (e.g. online search).

Besides the need for efficiency, document clustering methods should be able to han-dle the large term space of document collections to produce readily understandable clusters. These requirements are often not satisfied in popular clustering methods. For example, in K -means clustering, documents are compared in the term space, which is typically sparse, using generic similarity measures without considering the term-document semantics other than their vectorial representa tion in space. Moreover, it is not straightforward to interpret and understand the clusters formed by K -means clus-tering; the similarity of a document to its cluster X  X  mean prov ides little understanding of the document X  X  context or topic.

In this paper, we present a new document clustering method based on discrimination information maximization (CDIM). CDIM X  X  semantically motivated objective function is maximized via an efficient iterative procedure that repeatedly projects documents onto a K -dimensional discrimination information space and assigns documents to the cluster along whose axis they have the largest value. The discrimination information space is defined by term discrimination information estimated from the labeled docu-ment collection produced in the previous iteration. This procedure maximizes the sum of discrimination information provided by all documents. A key advantage of using term discrimination information is that each cl uster can be interpreted by a list of highly discriminating terms. These terms serve as units of understanding, as demonstrated in linguistics studies [1,2], describing a cluster in the document collection. We evaluate the performance of CDIM on ten popular text data sets. In clustering quality evalua-tion, CDIM is found to produce high quality cl usters superior to those produced by non-negative matrix factorization (NMF) and several K -means variants. Our results suggest the practical suitability of CDIM fo r clustering and understanding of document collections.

The rest of the paper is organized as follo ws. We discuss the related work and moti-vation for our method in Section 2. CDIM, our document clustering method is described in detail in Section 3. Section 4 presents our experimental setup. Section 5 discusses the results of our experiments, and we conclude with future directions in Section 6. Content-based document clustering conti nues to be challenging because of (1) the high dimensionality of the term-document space, (2) the sparsity of the documents in the term-document space, and (3) th e difficulty of incorporatin g appropriate term-document semantics for improved clustering quality an d understandability. Moreover, real-world document clustering often involves large document collections thus requiring the clus-tering method to be efficient.

The K -means algorithm continues to be popular for document clustering due to its efficiency and ease of implementation [3]. It is a partitional clustering method that opti-mizes an objective function via an iterative two-step procedure. Usually, documents are represented by terms X  weights, and documents are compared in the term space by the cosine similarity measure. Several clust ering objective functions can be optimized [4] with the traditional objective of maximizing the similarity of documents to their clus-ter means producing reliable clusterings. The Repeated Bisection clustering method, which splits clusters into two until the desired number of clusters are obtained, has been shown to produce better clusterings especially when K is large (greater than 20) [5]. These K -means based methods are efficient a nd accurate for many practical ap-plications. Their primary shortcoming is poor interpretability of th e clusters where the cluster mean vector is often not a reliable indicator of the documents in a cluster.
Some researchers have used external kno wledge bases to semantically enrich the document representation for document clustering [6,7]. In [6], Wikipedia X  X  concepts and categories are adopted to enhance the document representation, while in [7] several ontology-based (e.g. WordNet) term relatedness measures are evaluated for semanti-cally smoothing the document representation. In both works, it has been shown that the quality of clusterings produced by the K -means algorithm improves over the base-line ( X  X ag of words X ) document representation. However, extracting information from knowledge bases is computationally expensive. Furthermore, these approaches suffer from the same shortcomings of K -means regarding cluster understandability.
The challenge of high dimensional data clustering, including that of document clus-tering, has been tackled by clustering in a lower dimensional space of the original term space. One way to achieve this is through Non-Negative Matrix Factorization (NMF). NMF approximates the term-document matrix by the product of term-cluster and document-cluster matrices [8]. Extensions to this idea, with the goal of improv-ing the interpretability of th e extracted clusters, have also been proposed [9,10]. An-other way is to combine clustering with dimensionality reduction techniques [11,12]. Nonetheless, these methods are restricted by their focus on approximation rather than semantically useful clusters, and furthermore, dimensionality reduction based tech-niques are often computationally expensive.

Recently, it has been demonstrated that the relatedness of a term to a context or topic in a document collection can be quantified by its discrimination information [2]. Such a notion of relatedness, as opposed to the t raditional term-to-term relatedness, can be effectively used for data mining tasks like classification [13]. Meanwhile, mea-sures of discrimination information, such as relative risk, odds ratio, risk difference, and Kullback-Leibler divergence, are gaining popularity in data mining [14,15]. In the biomedical domain, on the other hand, meas ures like relative risk have been used for a long time for cohort studies and factor analysis [16,17]. CDIM (Clustering via Discrimination Information Maximization) is an iterative par-titional document clustering method that finds K groups of documents in a K -dimensional discrimination information s pace. It does this by following an efficient two-step procedure of document projection and assignment with the goal of maximiz-ing the sum of documents X  discrimination scores. CDIM X  X  clusters are describable by highly discriminating terms related to the context/topic of the documents in the cluster. We start our presentation of CDIM by formally stating the problem. 3.1 Problem Statement Let X =[ x 1 , x 2 ,..., x N ]  X  M  X  N be the term-document matrix in which the i th document x i =[ x 1 i ,x 2 i ,...,x Mi ] T is represented by an M -dimensional vector ( i th column of matrix X ). M is the total number of distinct terms in the N documents. The weight of term j in document i , denoted by x ji , is equal to the count of term j in document i .

Our goal is to find K (usually in practice K ) min { M,N } )clusters C k ( k = 1 , 2 ,...,K ) of documents such that if a document x  X  X  k then x  X  X  j ,  X  j = k . Thus, we assume hard partitioning of the documents among the clusters; however, this as-sumption can be relaxed trivially in CDIM but we do not discuss this further in our current work. In addition to the cluster composition, we will also like to find signifi-cant describing terms for each cluster. Let T k be the index set of significant terms for cluster k . 3.2 Clustering Objective Function CDIM finds K clusters in the document collection by maximizing the sum of discrimi-nation scores of documents for their respective clusters. If we denote the discrimination information provided by document i for cluster k by d ik and the discrimination informa-tion provided by document i for all clusters but cluster k by  X  d ik , then the discrimination score of document i for cluster k is defined as  X  d ik = d ik  X   X  d ik . CDIM X  X  objective function can then be written as where r ik =1 if document i is assigned to cluster k and zero otherwise. Document discrimination information ( d ik and  X  d ik ) is computed from term discrimination infor-mation that in turn is estimated from the current labeled document collection. These computations are discussed in the following subsections.

Intuitively, CDIM seeks a clustering in which the discrimination information pro-vided by documents for their cluster is highe r than the discrimination information pro-vided by them for the remaining clusters. It is not sufficient to maximize just the dis-crimination information of documents for their respective clusters as they may also provide high discrimination information for the remaining clusters.

The objective function J is maximized by using a greedy two-step procedure. In one step, given a cluster assignment defined by r ik ,  X  i, k , J is maximized by estimating d using maximum likelihood estimation. In the other step, given estimated discrimina-tion scores  X  d ik ,  X  i, k of documents, J is maximized by assigning each document to the cluster k for which the document X  X  discrimination score is maximum. This two-step procedure continues until the change in J from one iteration to the next drops below a specified threshold value. Convergence is guaranteed because J is non-decreasing from one iteration to the next and J is upper-bounded by a local maxima. 3.3 Term Discrimination Information The discrimination information provided by a document is computed from the discrimi-nation information provided by the terms in the document. The discrimination informa-tion provided by a term for cluster k is quantified with the relative risk of the term for cluster k over the remaining clusters. Mathematically, the discrimination information of term j for cluster k and term j for all clusters but k is given by clusters but cluster k . The term discrimination information is either zero (no discrimina-tion information) or greater than one with a larger value signifying higher discriminative power. The conditional probabilities in Equations 2 and 3 are estimated via smoothed maximum likelihood estimation. 3.4 Relatedness of Terms to Clusters In Equations 2 and 3, t  X  0 is a term selection parameter that controls the exclusion of terms that provide insignificant discrimination information. As the value of t is in-creased from zero, fewer terms will have a dis crimination information greater than one.
The index set of terms that provide significant discrimination information for cluster information provide a good understanding of the context of documents in cluster k in contrast with those in other clusters in the document collection. In general, T k  X  T information for more than one cluster. Also, depending on the value of t ,theremaybe terms that do not provide significant discrimination information for all clusters.
In a study discussed in [1], it has been shown that humans comprehend text by asso-ciating terms with particular contexts or topics. These relationships are different from the traditional lexical relationships (e.g synonymy, antonymy, etc), but are more funda-mental in conveying meaning and understanding. Recently, it has been shown that the degree of relatedness of a term to a context is proportional to the term X  X  discrimination information for that context in a corpus [2]. Given these studies, we can consider all terms in T k to be related to cluster k and the strength of this relatedness is given by the term X  X  discrimination information. This is an important characteristic of CDIM whereby each cluster X  X  context is describable by a set of related terms. Furthermore, these terms and their weights (discrimination information) define a K -dimensional space in which documents are comparable by their discrimination information. 3.5 Document Discrimination Information A document i is describable by the terms it contains. Each term j in the document vouches for the context or cluster k according to the value of the term X  X  discrimina-tion information w jk . Equivalently, each term j in the document has a certain degree of relatedness to context or cluster k according to the value w jk . The discrimination information provided by document i for cluster k can be computed as the average term discrimination information for cluster k : A similar expression can be used to define  X  d ik . The document discrimination informa-tion d ik can be thought of as the relatedness (discrimination) of document i to cluster is, the more likely that document i belongs to cluster k . Note that a term contributes to the discrimination information of document i for cluster k only if it belongs to T k and it occurs in document i . If such a term occurs multiple times in the document then each of its occurrence contributes to the discri mination information. Thus, the discrim-ination information of a document for a particular cluster increases with the increase in occurrences of highly discrimin ating terms for that cluster. 3.6 Algorithm CDIM can be described more compactly in matrix notation. CDIM X  X  algorithm, which is outlined in Algorithm 1, is described next.

Let W (  X  W )bethe M  X  K matrix formed from the elements w jk ,  X  j, k (  X  w jk ,  X  j, k ),  X  D be the N  X  K matrix formed from the elements  X  d ik ,  X  i, k ,and R be the N  X  K matrix formed from the elements r ik ,  X  i, k . At the start, each document is assigned to one of the K randomly selected seeds using cosine similarity, thus defining the matrix R . Then, information matrices ( W and  X  W ) are estimated from the term-document matrix X and the current document assignment matrix R . The second step projects the documents onto the relatedness or discrimination score space to create the discrimination score matrix  X  D . Mathematically, this transformation is given by where  X  is a N  X  N diagonal matrix defined by elements  X  ii =1 / j x ji . The matrix  X  D represents the documents in the K -dimensional discrim ination score space.
Documents are re-assigned to clusters based on their discrimination scores. A doc-ument i is assigned to cluster k if  X  d ik  X   X  d ij ,  X  j = k (ties are broken arbitrarily). In matrix notation, this operation can be written as where  X  X axrow X  is an operator that works on each row of  X  D and returns a 1 for the maximum value and a zero for all other values. The processing of Equations 5 and 6 are repeated until the absolute difference in the objective function becomes less than a specified small value. The objective function J is computed by summing the maximum values from each row of matrix  X  D .

The algorithm outputs the final document assignment matrix R and the final term discrimination information matrix W . It is easy to see that the computational time complexity of CDIM is O ( KMNI ) where I is the number of iterations required to reach the final clustering. Thus, the com putational time of CDIM depends linearly on the clustering parameters. Our evaluations comprise of two sets of experiments. First, we evaluate the clustering quality of CDIM and compare it with other clustering methods on 10 text data sets. Second, we illustrate the understanding that is provided by CDIM clustering. The results of these experiments are given in the next section. Here, we describe our experimental setup.
 Algorithm 1. CDIM  X  Document Clustering via Discrimination Information Maxi-mization 4.1 Data Sets Our experiments are conducted on 10 standard text data sets of different sizes, contexts, and complexities. The key characteristics of these data sets are given in Table 1. Data set 1 is obtained from the Internet Content Filtering Group X  X  web site 1 , data set 2 is available from a Cornell University web page 2 , and data sets 3 to 10 are obtained from Karypis Lab, University of Minnesota 3 . Data sets 1 (stopword removal) and 3 to 10 (stopword removal and stemming) are available in preprocessed formats, while we per-form stopword removal and stemming of data set 2. For more details on these standard data sets, please refer to the links given above. 4.2 Comparison Methods We compare CDIM with five clustering methods. Four of them are K -means variants and one of them is based on Non-Negative Matrix Factorization (NMF) [8].

The four K -means variants are selected fro m the CLUTO Toolkit [18] based on their strong performances reported in the literature [5,3]. Two of them are direct K -way clustering methods while the remaining two are repeated bisection methods. For each of these two types of methods, we consid er two different objective functions. One objective function maximizes the sum of similarities between documents and their clus-ter mean. The direct and repeated bisection methods that use this objective function are identified as Direct-I2 and RB-I2, respect ively. The second objective function that we consider maximizes the ratio of I2 and E1, where I2 is the intrinsic (based on cluster cohesion) objective function defined above and E1 is an extrinsic (based on separa-tion) function that minimizes the sum of the normalized pairwise similarities of docu-ments within clusters with the rest of the doc uments. The direct and repeated bisection methods that use this hybrid objective function are identified as Direct-H2 and RB-H2, respectively.

For NMF, we use the implementation provided in the DTU:Toolbox 4 . Specifically, we use the multiplicative update rule with Euclidean measure for approximating the term-document matrix.

In using the four K -means variants, the term-docum ent matrix is defined by term-frequency-inverse-document-frequency (TF-IDF) values and the cosine similarity mea-sure is adopted for document comparisons. For NMF, the term-document matrix is defined by term frequency values. 4.3 Clustering Validation Measures We evaluate clustering quality with the BCubed metric [19]. In [20], it has been shown that the BCubed precision and recall are th e only measures that satisfy all desirable constraints for a good clustering validation measure.

The BCUbed F-measure is computed as follows. Let L ( o ) and C ( o ) be the cate-gory and cluster of an object o . Then, the correctness of th e relation between objects o and o in the clustering is equal to one, Correct ( o, o )=1 ,iff L ( o )= L ( o )  X  C ( o )= C ( o ) ;otherwise Correct ( o, o )=0 . BCubed precision ( BP ) and BCubed re-BR = Avg o [ Avg o .L ( o )= L ( o ) [ Correct ( o, o )]] . The BCubed F-measure is then given by BF =2  X  BP  X  BR BP + BR . The BCubed F-measure ( BF ) ranges from 0 to 1 with larger values signifying better clusterings. 5.1 Clustering Quality Table 2 gives the results of the clustering quality evaluation. The desired number of clusters K for each data set is set equal to the number of categories in that data set (see Table 1). The shown values are average BCubed F-measure  X  standard deviation, computed from 10 generated clusteri ngs starting with random initial partitions.
These results show that CDIM outperforms the other algorithms on the ten data sets with five highest performance scores (shown in bold) and within 0.005 of the highest scores on three more data sets. CDIM is much better than NMF while its performances are closer to those of the K -means variants. We verified the consistency of these results using the Freidman X  X  test, which is a non-parametric test recommended for evaluat-ing multiple algorithms on multiple data sets [21]. At 0.05 significance level, CDIM is found to be significantly better than Direct-H2, RB-H2, and NMF, while its perfor-mance difference with Direct-I2 and RB-I2 is not statistically significant at this level.
An observation from our analysis is that CDIM consistently produces higher qual-attributable to the lesser resolution power of the multi-way comparisons (  X  d ik = d ik  X   X  d shortcoming for larger number of clusters is to use a repeated bisection approach rather than a direct K -way partitioning approach.
 5.2 Cluster Understanding and Visualization A key application of data clustering is corpus understanding. In the case of document clustering, it is important that clustering methods output information that can readily be used to interpret the clusters and their documents. CDIM is based on term discrim-ination information and each of its cluster is d escribable by the hi ghly discriminating terms in it. We illustrate the understanding provided by CDIM X  X  output by displaying the top 10 most discriminating terms (s temmed words) for each cluster of the ohscal data set in Table 3. The ohscal data set contains publications from 10 different medical subject areas (antibodies, carcinoma, DNA, in-vitro, molecular sequence data, preg-nancy, prognosis, receptors, risk factors, and tomography). By looking at the top ten terms, it is easy to determine the category of m ost clusters: cluster 2 = carcinoma, clus-ter 3 = antibodies, cluster 4 = prognosis, cluster 5 = pregnancy, cluster 6 = risk factors, cluster 7 = DNA, cluster 9 = receptors, cluster 10 = tomography. The categories molec-ular sequence data and in-vitro do not appear to have a well-defined cluster; molecular sequence data has some overlap with cluster 7 while in-vitro has some overlap with clusters 1 and 9. Nonetheless, clusters 2 and 8 still give coherent meaning to the docu-ments they contain.

As another example, in hitech data set, the top 5 terms for two clusters are: (1)  X  X ealth X ,  X  X are X ,  X  X atient X ,  X  X ospit X ,  X  X edic X  , and (2)  X  X iti X ,  X  X ouncil X ,  X  X roject X ,  X  X uild X ,  X  X ater X . The first cluster can be mapped to the health category while the second clus-ter does not have an unambiguous mapping to a category but it still gives sufficient indication that these articles discuss hi-tech related development projects.
Since CDIM finds clusters in a K -dimensional discrimin ation information space, the distribution of documents among clusters can be visualized via simple scatter plots. The 2-dimensional scatter plot of documents in the pu data set is shown in Figure 1 (left plot). The x-and y-axes in this plot correspond to document discrimination information for cluster 1 and 2 ( d i 1 and d i 2 ), respectively. and the colored makers give the true categories. It is seen that the two clusters are spread along the two axes and the vast majority of documents in each cluster belong to the same category. Similar scatter plots for Direct-I2 and NMF are shown in the middle and right plots, respectively, of Figure 1. However, these methods exhibit poor separation between the two categories in the pu data set.

Such scatter plots can be viewed for any pair of clusters when K&gt; 2 .SinceCDIM X  X  document assignment decision is based upon document discrimination scores (  X  d ik ,  X  k ), scatter plots of documents in this space are a lso informative; each axis quantifies how relevant a document is to a cluster in comparison to the remaining clusters. In this paper, we propose and evaluate a new document clustering method, CDIM, that finds clusters in a K -dimensional space in which docum ents are well dis criminated. It does this by maximizing the sum of the discrimination information provided by doc-uments for their respective clusters minus that provided for the remaining clusters. Document discrimination information is computed from the discrimination informa-tion provided by the terms in it. Term discrimination information is estimated from the document collection via its relative risk. An advantage of using a measure of discrim-ination information is that it also quantifies the degree of relatedness of a term to its context in the collection. Thus, CDIM produces clusters that are readily interpretable by their highly discriminating terms.

Our experimental evaluations confirm th e effectiveness of CDIM as a practically useful document clustering method. Its core idea of clustering in spaces defined by corpus-based discrimination or relatedness information holds much potential for future extensions and improvements. In particular, we would like to investigate other measures of discrimination/relatedness information, extend and evaluate CDIM for soft cluster-ing, and develop a hierarchical and r epeated bisection version of CDIM.

