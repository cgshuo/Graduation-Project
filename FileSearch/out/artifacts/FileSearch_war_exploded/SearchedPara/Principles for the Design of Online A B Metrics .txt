 In this paper, we describe principles for designing metrics in the context of A/B experiments. We sh are some issues that comes up in designing such experiments and provide solutions to avoid such pitfalls. A/B measurement; controlled experiments; metric design principles. Online experimentation is be coming more and more popular. Controlled experiments with thousands or even millions of users are applied to establish causal relationships between a new treatment and a change in user behavior. Such A/B experimentation is used widely no w in industries related to social media, e-commerce, online publishing, search engines, etc., which try to optimize for engage ment, revenue, user success, among other aspects [2]. One of the key factors in evaluating online controlled experiments are metrics. They help discern whether the treatment effect on users was desired or not and therefore guide ship decisions of the teams building the new treatments. For that reason, good A/B metrics are of critical importance in order to make sound data-driven decisions. Yet, it is very easy to build A/B metrics that suffer from undetected weaknesses and which eventually point in the wrong direction leading to  X  unknowingl y  X  incorrect ship decisions. Therefore, great care has to be devoted to proper design of A/B metrics that are expressive , robust, and trustworthy. In this paper we report a few important lessons learnt while designing A/B ship metrics for the large search engine Bing.com. Based on many years of experien ce, we elaborate on a number of important aspects that should be taken into account when designing online A/B metrics, specifically having user satisfaction metrics in mind. The eventual purpose of A/B metrics, particularly OECs (Overall Evaluation Criterion) is to tell whether the treatment user group A demonstrated more desirable behavior than the control user group B. In the context of web se arch, this often means measuring whether treatment users had an improved or impaired search experience in terms of user satisfaction. When designing an A/B metric, a question of critical importance is: How do you know you found a good metric? Traditionally, in research, this question has been framed and treated as a prediction problem. Given logs with recorded user behavior, and given ground truth labels for the successfulness / outcome of the logged user interactions, build a predictor (classifier) that ingests signals from the logs and optimizes precision/recall/etc. against the ground truth labels [1,2]. Then, the assumption often is that the best predictor will also result in the best A/B metric. While this type of evaluation is a reasonable first step in designing a good A/B metric, it can easily yield something ineffective and flawed for the following reasons. 1) Insensitivity to changes the metric is supposed to measure: When designing a metric, one needs to be cognizant of the nature of treatment changes it is supposed to measure and be sensitive to. Even a very good predictor might not pick up certain user behavior changes and therefore miss measuring an A/B difference. This can happen particularly for rare user interaction patterns that a predictor might, e.g., prune away to avoid over-fitting. Yet, a particular treatment might change the occurrence of this specific rare user interaction pattern, and not much else, and hence the predictor would be  X  X lind X  to the change. 2) The usage of endogenous signals: Search engine logs usually contain information on how the search results page served to the user looked like, and how the user interacted with the page. A predictor might use signals based on both types of information, yet only the latter type of information is generally safe to use when the application is an A/B metric. We call the former type of signals  X  X ndogenous signals X  which encode the system response from the search engine, and the latter type  X  X xogenous signals X  which encode the user response. Using endogenous signals in the predictor opens it up for loopholes that can seriously affect its trustworthiness as an A/B metric. As an obvious example, if a predictor declares user success whenever the weather forecast element is displayed on the search results page (e.g., for the query  X  X eather X  that could make sense), then this predictor would be in favor of showing the weather forecast on every single results page, no matter what query, a nd it would not guard against showing a broken forecast or a forecast for the wrong location to the user, all of which is certainly undesired. Hence, rather than treating the initial question as a prediction problem, it should be treated as an A/B measurement problem: given an experiment corpus, i.e., a set of controlled A/B experiments with ground truth labels specifying whether an entire experiment is considered an impr ovement or a regression, design an A/B metric that directionally aligns best with the ground truth labels. Ground truth labels for enti re experiments can be attained from a panel of experts that don X  X  only take a large collection of metrics into account, but also deep-dive to get a detailed technical understanding of the treatment change and how it could affect the user experience. It is critical to cover a diverse set of experiments in the experiment corpus, and ideally include representative experiments for all types of changes to the search engine that need to be evaluated by the metric, since otherwise the metric might have  X  X lind spots X  regarding certain treatments. Based on such an experiment corpus, the best metric should both have the best directional alignment with the labels as well as be most sensitive to the treatment effects (statistical sensitivity in terms of, e.g., t-value of a t-test). No metric is perfect  X  all of them have weaknesses or loopholes and incorrectly move for certain treatments they are not designed for. Hence, it is important to design a good metric system for making ship decisions, i.e., a coll ection of metrics that measure treatment effects from various diffe rent angles, so that based on their entirety it is possible to get a comprehensive understanding of the implications of the exper iment. The challenge with having a large number of metrics, though, is that 1) some metrics will move statistically significantly by chance (for independent metrics typically 1 in 20 with a pv al threshold of 0.05), and 2) for strong treatment effects many metric s will move, some of them in seemingly contradictory ways. As a consequence, experimenters might be allured to cherry-pick the metrics that are most in-line with their intuition, which can easily lead to seemingly data-driven yet unsound ship decisions. To address these challenges, we generally design metric systems in a hierarchical way. At the top are the most robust metrics, in our case a metric like  X  X essions per user X . They are defined at the user-level, have the fewest built-in assumptions on user behavior interpretation, yet are usually are the least sensitive. On the next level come session-level metrics such as Session Success Rate. They have built-in assumptions on what success looks like, therefore are less robust, yet more sensitive. On the third level, there are many feature-specific metrics, often on the impression level, such as Web Result Success Click Rate, etc. These have even more assumptions on how user success will show up for certain experiments, are least robust, yet most sensitive. As part of this hierarchy, such a system should have metrics that covers different areas of the page . This is what defines the scope of a metric. A metric that measures the change in user behavior on a specific feature is a featur e-level metric, e.g. Web Result Success Click Rate. A metric that measures success across all the page is a page-level metric a nd has a higher scope than feature-level metrics, e.g. Page Click Through Rate. Similarly, we can define metrics that consider success across one session of the user, or even across all their traffic during the experiment. A metric system that targets all the different scopes guarantees that experiments are not missing global effects of their changes that cannot be captured by the feature-level metrics. When interpreting a scorecard, we would look at metrics in a top-down fashion. Particularly when metrics on the first or second level are moving already, then metrics on the third level have to be interpreted very carefully due to, e.g., denomination effects as explained in the next section. There are few very important aspects to keep in mind when working on concrete metric definition formulas. 1) When counting events, time, or anything that doesn X  X  have an upper bound, then using plain counts in the metric can introduce large amounts of noise making the metric less sensitive. For a metric like  X  X ime to click X  where time from the page request to the first click is measured, varian ce is usually very high due to outliers. Effective ways to address this are to apply truncation (i.e., treating any value &gt; x as x), or functional transformation (e.g., using the log function reducing effects from outliers). 2) The metric can be invalid if it changes due to denominator movements. Figure 1 shows an example where the objective is to measure user engagement in th e form of clicks. Depending on which denominator is used, the me trics can move in contradictory directions or not at all, and this can be very confusing for experimenters making ship decisions. Generally, the more fine-grained the denominator, the higher the sensitivity of the metric, yet the higher the chance that the denominator moves on its own. Our guidance is to use the metric with the most fine-grained denominator which doesn X  X  yet move on its own. In the figure below that would be Clicks/Session. Figure 1 : Denominator changes can lead to invalid and contradictory metric movements. Once an A/B metric system is developed, experimenters will rely on the movements in the metric to decide whether to ship the change they are testing. However, even if these changes are localized to a specific feature, they can affect users X  behavior outside these features and influenc e their interaction with the rest of the page. Due to these effects, it is important for an online metric system to be easily debuggable and its m ovements clearly understood. A metric need to be easily decomposable to the different signals and user behavior that contributes to its movements. Simple linear functions that combine basic features representing users X  interactions guarantee that exper imenters are able to pinpoint to the main drivers behind a metric movement. Compare that with machine-learned metrics that can easily become a black box, where the inner workings of the metric are opaque. This would make debugging the metric m ovements and identifying the failings of the change being tested hard for the experimenters. The authors would like to thank Aidan Crook, Anand Oka, Siamak Faridani and Victor Hu for interesting discussions on the topic of this paper. [1] H. Feild et al. 2010. Predicting searcher frustration. In [2] Hassan. 2012. A semi-supervis ed approach to modeling [3] Ron Kohavi, Alex Deng, Bria n Frasca, Roger Longbotham, 
