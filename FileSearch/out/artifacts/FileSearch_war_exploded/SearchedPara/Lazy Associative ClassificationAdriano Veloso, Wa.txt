
Decision tree classifiers perform a greedy search for rules by heuristically selecting the most promising featur es. Such greedy (local) search may discard important rules. As-sociative classifiers, on the other hand, perform a global search for rules satisfying some quality constraints (i.e. , minimum support). This global search, however, may gen-erate a large number of rules. Further, many of these rules may be useless during classification, and worst, important rules may never be mined. Lazy (non-eager) associative classification overcomes this problem by focusing on the features of the given test instance, increasing the chance of generating more rules that are useful for classifying the test instance. In this paper we assess the performance of lazy associative classification. First we demonstrate that an associative classifier performs no worse than the corre-sponding decision tree classifier. Also we demonstrate that lazy classifiers outperform the corresponding eager ones. Our claims are empirically confirmed by an extensive set of experimental results. We show that our proposed lazy associative classifier is responsible for an error rate redu c-tion of approximately 10% when compared against its eager counterpart, and for a reduction of 20% when compared against a decision tree classifier. A simple caching mech-anism makes lazy associative classification fast, and thus improvements in the execution time are also observed.
The classification problem is defined as follows. We have an input data set called the training data which con-sists of a set of multi-attribute records along with a specia l variable called the class . This class variable draws its value from a discrete set of classes. The training data is used to construct a model which relates the feature variables (or at -tribute values) in the training data to the class variable. T he test instances for the classification problem consist of a set of records for which only the feature variables are known while the class value is unknown. The training model is used to predict the class variable for such test instances.
Classification is a well-studied problem (see [12, 20] for excellent overviews) and several models have been pro-posed over the years, which include neural networks [17], statistical models like linear/quadratic discriminants [ 14], decision trees [2, 19], and genetic algorithms [11]. Among these models, decision trees are particularly suited for da ta mining. Decision trees can be constructed relatively fast compared to other methods. Another advantage is that de-cision tree models are simple and easy to understand [19].
As an alternative to decision trees, associative classifier s have been proposed [8,16,18]. These methods first mine as-sociation rules from the training data, and then build a clas -sifier using these rules. This classifier produces good resul ts and yields improved accuracy over decision trees [18].
Decision trees perform a greedy search for rules by heuristically selecting the most promising features. They start with an empty concept description, and gradually add restrictions to it until there is not enough evidence to con-tinue, or perfect discrimination is achieved. Such greedy (local) search may prune important rules. Associative clas -sifiers, on the other hand, perform a global search for rules satisfying some quality constraints. This global search, however, may generate a large number of rules, and many of the generated rules may be useless during classification (i.e., they are not used to classify any test instance).
In this paper we propose a novel lazy associative classi-fier, in which the computation is performed on a demand-driven basis. We place our associative classifier within an information gain framework that allows us to compare it to decision tree classifiers. Our method can overcome the large rule-set problem of traditional (eager) associative clas-sifiers, by focusing on the features that actually occur with in the test instance while generating the rules. We show that the proposed lazy classifier outperforms its eager counter-part, since in the lazy approach only the  X  X seful X  portion of the training data is mined for generating the rules ap-lazy classifier can better classify a test instance, for whic h a global, eager rule-set may not work that well. Simple caching mechanisms are used to avoid work replication dur-ing lazy associative classification. First we demonstrate t hat associative classifiers perform no worse than decision tree classifiers. Then we show that lazy classifiers outperform the corresponding eager classifiers. Our claims are empiri-cally confirmed by an extensive set of experimental results. Timings are also showed in order to evaluate different clas-sifiers with respect to computational complexity.
Most existing work on associative classification relies on developing new algorithms to improve the overall clas-sification accuracy. CBA [18] generates a single rule-set and ranks the rules according to their confidence/support values. Then it selects the best rule to be applied to each test instance. Enhancements to CBA were proposed in [8, 16, 21, 23]. HARMONY [21] uses an instance-centric rule-generation approach in the sense that it assures the in -clusion of at least one rule for each training instance in the final rule-set. CMAR [16] uses multiple rules (instead of a single best one) to perform the classification. CPAR [23] adopts a greedy technique to generate a smaller rule-sets. CAEP [8] explores the concept of emerging patterns , and, as a result, it usually predicts accurately all classes, eve n if their populations are unbalanced. It has been empirically shown that these associative classifiers usually perform be t-ter than decision trees. However, there is a lack of studies showing the theoretical implications of associative class i-fiers. Therefore, there is little intuition regarding the ac tual reasons behind the better performance of associative class i-fiers when compared to decision trees.

Rule induction classifiers, such as RISE [6], RIPPER [3], and SLIPPER [4], use greedy heuristics which are driven by global metrics. RISE performs a complete overfiting by considering each instance as a rule, and then it generalizes the rules. RIPPER and SLIPPER extend the  X  X verfit and prune X  paradigm, that is, they start with a large rule-set an d prune it using several heuristics. Further, the SLIPPER al-gorithm also associates a probability with each rule, weigh t-ing the contribution of the rule during classification.
Most work on lazy classification [1] was based on near-est neighbor algorithms [5]. The problem of small disjuncts was first noted in [13], where it was showed that existing classifiers create models that are good for large disjuncts b ut are far from ideal for small disjuncts (which correctly clas -sifies only few training instances). It is hard to assess the a c-curacy of small disjuncts because they cover few instances, yet removing all of them is unjustified since many of them may be significant and the overall accuracy would degrade.
Play Outlook Temperature Humidity Windy yes rainy cool normal false yes overcast hot high false yes rainy cool normal false yes sunny cool normal false yes rainy cool normal false yes sunny hot normal false yes overcast mild high true ?(yes) sunny cool high false A lazy decision tree was proposed in [10] and it was shown that the lazy approach is superior than the corresponding ea -ger one (i.e., C4.5). Despite all the improvements obtained by using lazy algorithms, we are not aware of any proposals of lazy associative classification algorithms, as well as an assessment that demonstrates why they perform better than both decision trees and eager associative classifiers.
In this section we describe eager associative classifiers, and demonstrate why they perform better than decision trees. We start by discussing how decision rules may be generated from decision trees. Then we describe associa-tive classifiers that are based on information gain, so that we may compare them regarding the rules that are gener-ated by each approach.
Given any subset of training instances S , let s the number of instances with class c be the total number of training instances. Then p denotes the probability of class c is then given as E ( S ) = P of S into m subsets S split entropy is given as E ( { S information gain for the split is then given as I ( S, { S E ( S )  X  E ( { S i } ) .

A decision tree is built using a greedy, recursive splitting strategy, where the best split is chosen at each internal nod e according to the information gain criterion. The splitting at a node stops when all instances are from a single class or if the size of the node falls below a minimum support thresh-old, called minsup . Figure 1 shows an example of training data, and Figure 2 shows the corresponding decision tree. The last row of the table in Figure 1 shows one test instance which is recognized by the decision tree in Figure 2. The decision tree can be considered as a set of disjoint decision rules, with one rule per leaf. In that way, a decision tree can be simulated by a set of decision rules. In this case, the information gain for each decision rule is calculated in the same way as it is calculated for each path of the decision tree. Thus, a decision rule has the same value of informa-
We denote as class association rules (CARs) [18] those association rules of the form X X  c , where the antecedent (
X ) is composed of feature variables and the consequent ( c is just a class. CARs may be generated by a slightly mod-ified association rule mining algorithm. Each itemset must contain a class and the rule generation also follows a tem-plate in which the consequent is just a class. CARs are es-sentially decision rules, and as in the case of decision tree s, CARs are ranked in decreasing order of information gain. Finally, during the testing phase, the associative classifi er simply checks whether each CAR matches the test instance; the class associated with the first match is chosen. Note that , seen in the light of CARs, a decision tree is simply a greedy search for CARs, using a level-wise search algorithm, that only expands the current best rule with other features. On the other hand, an eager associative classifier mines all pos-sible CARs with a given minsup . It is also interesting to note that sorting the final rule-set on information gain, and using the best CAR for classification, is also a greedy strat-egy. While the greedy approach has its limitations, eager associative classifiers are not limited by the prefix problem of decision rules, that is, once the best feature is chosen at each node, all nodes under that subtree must contain it.
Figure 3 shows the basic steps of the eager associative classifier. In the initial step, the algorithm mines all frequent CARs, and sort them in descending order of information gain. Then, for each test instance t CAR matching t shows an associative classifier built from our example set of training instances in Figure 1, using the algorithm showed in Figure 3. Three CARs match the test instance of our example (last row of Table 1): 1. { windy=false and temperature=cool  X  play=yes } 2. { outlook=sunny and humidity=high  X  play=no } 3. { outlook=sunny and temperature=cool  X  play=yes }
Rule { windy=false and temperature=cool  X  play=yes } would be selected, since it is the best ranked CAR. By ap-plying this CAR, the test instance will be correctly classi-fied. Intuitively, associative classifiers perform better t han decision trees because associative classifiers allow sever al CARs to cover the same partition of the training data. In our example, the test case is recognized by only one rule in the decision tree, while the same test case is recognized by three CARs in the associative classifier. Selecting the proper CAR to be applied is an issue in associative classifi-cation.

Next we present a theoretical discussion about the per-formance of decision trees and eager associative classifier s. Theorem 1 The rules derived from a decision tree are a subset of the CARs mined using an eager associative clas-sifier based on information gain.
 Proof 1 Let maxE be the maximum entropy of all decision tree rules. Select a set C e from all CARs such that their entropy is at most maxE . It is clear that the decision tree rules are a subset of C e .

Theorem 1 states that, for a given minsup , CARs con-tain (at least) all information of the corresponding decisi on tree. Since each decision tree rule may be seem as a CAR, and since all possible CARs were enumerated, then the de-cision tree can be built by choosing the proper CARs. Theorem 2 CARs perform no worse than decision tree rules, according to the information gain principle. Proof 2 Given an instance to be classified, and, without loss of generality, a decision tree with just pure leaves, th e decision tree predicts class c for that instance. We analyze two scenarios: first, just one CAR matches the instance, and second, more than one CAR matches. When just one CAR matches, it is the same as the decision tree rule, since the se t of CARs subsumes the set of decision rules. In this case, the associative classifier and the decision tree make the same prediction. When more than one CAR matches an instance, the prediction may be either the same class (say c ) as the matching decision rule or another class. If the associative classifier predicts c then the two approaches are equivalent. In case a class other than c is predicted, by definition, the best matching CAR provides a better information gain than the decision rule, and thus, according to the information gain principle, the CAR will make a better prediction.
Theorem 2 states that the additional CARs of the asso-ciative classifier that are not in the decision tree, cannot d e-grade the classification accuracy. This is because an addi-tional CAR is only used if it is better than all decision rules (according to the information gain principle).

However, eager associative classifiers generate a large number of CARs, most of which are useless during classi-fication. For instance, from the set of 13 CARs showed in Figure 4, only 3 match the test instance (the remaining 10 CARs are useless). Next, we present a lazy classifier and compare it to the eager version described in this section.
Unlike the eager associative classifier that extracts a set of ranked CARs from the training data, the lazy associative classifier induces CARs specific to each test instance. The lazy approach projects the training data, D , only on those features in the test instance, A . From this projected train-ing data, D best CAR is used. From the set of all training instances, D , only the instances sharing at least one feature with the test instance A are used to form D is generated from D A , all CARs generated from D associative classifier is presented in Figure 5.
Now we demonstrate that the lazy associative classifier produces better results than its eager counterpart. Given a test instance A , and a set of CARs C , we denote by C CARs {X X  c } in C where X X  X  .
 Theorem 3 Let A be the set of features in a given test in-stance. Let C e associative classifier induced by A , and C l CARs obtained from the lazy associative classifier induced by A . For a given minsup , we have C e A  X  X  l A .
 Proof 3 By definition, both C e CARs {X X  c } in which X X  X  , that is, all CARs contain only features in A . Also the training instances matching (i.e., the projected training data) are a subset of the set of
Play Outlook Temperature Humidity Windy yes overcast hot  X   X  yes  X  hot  X   X  yes overcast  X   X  true ?(yes) overcast hot low true all training instances (i.e., D minsup , if a rule {X X  c } is frequent in D , then it must also be frequent in D
The next example illustrates Theorem 3. Figure 6 shows the training data given in Figure 1 (i.e., D ), projected by the features in the test instance (i.e., A ) showed in the last row in Figure 6. The projected training data (i.e., D composed of the 5 instances showed in the figure. Suppose minsup is set to 40%. In this case, the set of CARs, C e found by the eager classifier is composed of the two CARs: 1. { windy=false and humidity=normal  X  play=yes } 2. { windy=false and temperature=cool  X  play=yes }
None of the two CARs matches the test instance, and thus, C e has less instances ( D frequent in D may be frequent in D a frequent CAR must occur at least 4 times in D (since |D| =10), but only 2 times in D A (since |D A | =5). The lazy classifier found two CARs in D 1. { outlook=overcast  X  play=yes } 2. { temperature=hot  X  play=yes }
These lazy CARs not only predict the correct class, but also are simpler than the eager CARs. Next we discuss how the lazy CARs perform when compared to the eager CARs. Theorem 4 Lazy CARs perform no worse than eager CARs, according to the information gain prnciple.
 Proof 4 Theorem 3 showed that, for a given minsup , C e tion gain principle), and let R l be the best rule in C l scenarios have to be considered when determining a class for the test instance A . In the first scenario, R l is identical to
R e ; in this case the same class is predicted by both ea-ger and lazy classifiers. In the second scenario, R l is better than R e , and thus R l must provide a better prediction.
Theorem 4 states that the CARs added by the lazy clas-sifier do not degrade the classification accuracy. This is be-cause an additional lazy CAR is only used if it is better than all eager CARs (according to the information gain princi-ple). Intuitively, lazy classifiers perform better than eag er classifiers because of two characteristics:  X  Missing CARs: Eager classifiers search for CARs in  X  Highly Disjunctive Spaces: Eager classifiers generate
The aforementioned discussion shows an intuitive con-cept, that is, the more CARs are generated, the better is the classifier. However, the same concept also leads to over-fitting, reducing the generalization and affecting the clas -sification accuracy. In fact, overfiting and high sensitivit y to irrelevant features are shortcomings of lazy classifiers . A natural solution is to identify and discard the irrelevant features. Thus, feature selection methods have been pro-posed [7]. As the experimental results show in the next section, we have no evidence that our lazy classifiers were seriously affected by overfitting. We explain these results because just the best and more general CARs are used.
Another disadvantage is that lazy classifiers typically re-quire more work to classify all test instances. However, sim -ple caching mechanisms are very effective to decrease this workload. The basic idea is that different test instances ma y induce different rule-sets, but different rule-sets may sh are common CARs. In this case, memorization or caching of CARs is very effective in reducing work replication.
Our cache is a pool of entries, and it stores CARs of the form {X X  c } . Each entry has the form &lt; key,data where key = {X , c } and data = { support,confidence,info gain } . A given CAR has only one entry in the cache, and our implementation stores all cached CARs in main mem-ory. Before generating a CAR, the algorithm first checks whether this CAR is already in the cache. If an entry is found, the CAR in the cache entry is used. Otherwise, the CAR is processed and then it is inserted into the cache.
The cache size is limited, and when the cache is full, some CARs are discarded to make room for others. Since it is impossible to predict how far in the future a specific CAR will be used, we choose the LFU (Least Frequently Used) heuristic (which counts how often a CAR is used, and those that are used least are discarded first), in order to improve cache efficiency. Rule caching is extremely effective in re-ducing the computation time for lazy classification. In fact , caching can make lazy classification faster than eager clas-sification, as we will show in the next section.
In this section we show the experimental results for the evaluation of the proposed classifiers in terms of classifi-cation effectiveness and computational performance. Our evaluation is based on a comparison against C4.5 [19] and LazyDT [10] decision tree classifiers. We also compare our numbers to some results from other associative classifiers, such as CPAR [23], CMAR [16] and HARMONY [21], and to some results from rule induction classifiers, such as RISE [6], RIPPER [3], and SLIPPER [4]. We used 26 datasets from the UCI Machine Learning Repository to
In all the experiments we used 10-fold cross-validation and the final results of each experiment represent the av-erage of the ten runs. We quantify the classification ef-fectiveness of the classifiers through the conventional err or rate (the percentage of test instances incorrectly classifi ed). We used the entropy method [9] to discretize continuous at-tributes. In the experiments we set minimum confidence to 50% (for confidence based classifiers) and minsup to 1% . All other parameters were tuned according to [10,16,18,21] . The experiments were performed on a Linux-based PC with a Intel Pentium III 1.0 GHz processor and 1.0 GByte RAM. We start our analysis by comparing the effectiveness of C4.5 decision trees and eager classifiers. Table 1 shows the error rates obtained by each classifier and the error reduc-tion relative to C4.5. As can be seen, EAC always outper-forms C4.5, which is not true for CBA, CMAR, and HAR-MONY. Further, CBA performs better when the dataset is sparse, that is, there is usually a large number of features and each instance contains just few of them. On average, EAC shows to be slightly better than CBA. CMAR performs better than EAC because it uses multiple CARs to classify a test instance, while EAC uses only the best ranked CAR.
Some datasets deserve special discussion. The hepati-tis dataset, for instance, has two very unbalanced classes. EAC is able to reduce 20.8% of the errors in this dataset. We investigated the reason for such an improvement, and we observed that the effectiveness of C4.5 varies heavily with each class. For the most frequent class, C4.5 and EAC present a similar result. However, for the less frequent cla ss, C4.5 performs poorly. Similar results were observed in the labor and sonar datasets. The gains provided by associative classification do not come exclusively from its capacity of performing equally accurately in all classes when they are unbalanced, since EAC performed much better than C4.5 in the waveform dataset, which has very well balanced classes. C4.5 was superior than CBA in the horse dataset. Investi-gating the reason for that, we observed that CBA generates a very large number of 100% confident rules (homogeneous partitions), so that breaking ties and selecting a single be st rule becomes hard and prone to error.
We continue our analysis by comparing the effectiveness of eager (EAC) and lazy classifiers (LAC). Table 1 shows their corresponding error rates. For very small datasets ea -ger and lazy classifiers perform similarly, since the CARs that were generated by both classifiers were essentially the same for the parameters used. For instance, the result ob-tained with labor and zoo datasets were exactly the same. Also, we can observe that lazy classifiers perform better when the dataset is sparse (i.e., auto, pima, diabetes, ger-man, and wine datasets). The error reduction in these datasets range from 13.9% to 52.7%. This result is ex-pected, since the small disjuncts problem is more likely to happen in sparse datasets. Further, we can also notice that the lazy classifiers always outperform the corresponding ea -ger ones, except for the ionosphere dataset. We believe that , for this dataset, the lazy classifiers have overfitted the dat a.
Figure 7 shows the rule-set utilization for eager and lazy classifiers. A CAR is useful if it is used to classify a test instance. Utilization is given by the number of useful CARs divided by the total number of distinct CARs that were gen-erated. Ideally, only the best CAR matching each test in-stance should be generated. This ideal case corresponds to a total rule-set utilization (100%), that is, all generated C ARs are used to classify at least one test instance. Since the num -ber of test instances is constant, the rule-set utilization de-pends on the number of distinct CARs that are generated. Increasing minsup reduces the number of CARs, and there-fore, it increases rule-set utilization. However, the chan ce of having no CAR matching a test instance will also increase (i.e., the missing rule problem). On the other hand, lower-ing minsup obviously reduces the percentage of CARs that are used, since the number of CARs will increase. As we can see, reducing minsup always reduces the percentage of CARs that are used. Further, lazy classifiers always pro-vide a better rule-set utilization. This is because lazy cla ssi-fiers focus only on the features of each test instance during rule generation. Differently, eager classifiers use all fea tures within the training data, and therefore, the search space fo r CARs is augmented. The rule-set utilization of lazy and ea-ger classifiers approaches for dense datasets, since in this case the search space for CARs tends to be similar.
We also compared the proposed eager and lazy associa-tive classifiers against RISE, RIPPER and SLIPPER, using results reported in [3,4,6]. Table 2 shows the relative perf or-mance for each classifier (i.e., the accuracy of one classifie r divided by the accuracy of the other classifier), when com-pared to eager associative classifiers. Each number in this table indicates how many times EAC is superior than the corresponding adversary RISE, RIPPER or SLIPPER (in terms of accuracy). The SLIPPER algorithm won in 6 of the 26 datasets (and lost in 11), showing to be most compet-itive rule induction classifier. The RIPPER classifier won in 4 datasets and the RISE classifier won in only one dataset.
Table 2 also shows the relative performance of rule in-duction classifiers when compared to the lazy associative classifiers. RISE and RIPPER lost in almost all datasets, and SLIPPER was the most competitive one. Compared to LAC (inf. gain), SLIPPER won in one dataset (and matched in 6), and compared to LAC (confidence), SLIPPER won in 4 datasets (and matched in 2).
We also analyze the sensitivity of the classifiers regard-ing the complexity of the models they induce. The model complexity has an intuitive relation with the size of the rul es (or the height of the tree) that constitute the model. The longer the rules within the model, the more complex it is. By using complex rules, the classifier tends to overfit the data, hurting the classifier accuracy. On the other hand, by using simple rules, the classifier will underfit the data also hurting the accuracy. Thus, the choice of the complexity of the rules is a trade-off between underfitting and overfit-ting, and may vary according to the dataset being used. Fig-ure 8 shows the error rates associated with different classi -fiers for some of the datasets by varying the complexity of the model. The datasets anneal, cleve, crx, glass, led7, sic k and zoo benefit from more complex models. These datasets are more dense and need more complex rules. We notice that the effectiveness of both eager and lazy classifiers are similar for these datasets. On the other hand, for datasets such as iris, auto, lymph and diabetes there is a demand for simpler rules. We observed that the lazy classifier is able to provide improvements in the majority of the cases. Table 3 shows the execution times obtained by C4.5, RIPPER, EAC, and LAC classifiers. In all executions, minsup was set to 1%, and the cache size is set to 10,000 CARs. All times correspond to the total time spent (in sec-onds) using 10-fold cross-validation. In general, C4.5 is the worst performer. RIPPER and EAC are very competi-tive, but RIPPER shows to be slightly superior. For some datasets (such as hypo, sick and waveform), EAC and RIP-PER generate a very large number of CARs. Although LAC implements extra work, namely feature projection and rule generation for every test instance, it usually generates mu ch less distinct CARs than EAC and RIPPER, and thus, LAC is the best performer on average.
Decision tree classifiers perform a greedy search which may discard relevant information. Based on this observatio n we present an assessment of associative classification. The generated CARs are ranked based on their information gain, so that we can compare the performance of decision trees and associative classifiers. We present evidence regarding the superiority of associative classifiers. However, it is w ell known that no classifier can outperform others in all set-tings [22], and thus there may be certain specific situations where decision trees outperform associative classifiers.
We also propose improvements to associative classifica-tion by introducing a novel lazy classifier. The lazy classi-fier searches a larger hypothesis space than the correspond-ing eager classifier, because it uses many different local models to form its implicit global approximation to the tar-get function. Eager classifiers commit at training time to a single global approximation. An important feature of the proposed lazy classifier is its ability to deal with the small disjuncts problem. Based on this observation, we present evidence showing that a lazy associative classifier outper-forms the corresponding eager one. Our claims were con-firmed by empirical comparisons to C4.5 and LazyDT deci-sion tree classifier, using datasets from the UCI data repos-itory. We also compared the proposed classifiers against other three associative classifiers and three rule inductio n classifiers and outperformed them in most of the cases.
So far, our classifiers use only the best CAR for sake of classification. In the future we will combine simple and complex CARs in order to enhance classification. Finally, we will explore more realistic application scenarios.
Dataset C4.5 RIPPER EAC LAC anneal 24.2 19.8 18.7 14.5 australian 9.1 5.4 7.0 6.1 auto 4.4 2.9 4.4 4.2 breast 1.6 1.1 0.9 1.9 cleve 5.9 4.5 5.9 6.8 crx 9.3 5.6 4.7 5.6 diabetes 0.8 0.9 0.3 1.1 german 14.4 6.6 6.3 4.6 glass 1.8 0.8 1.1 1.7 heart 4.1 3.8 4.7 3.9 hepatitis 3.8 2.7 2.7 4.2 horse 9.7 5.7 6.4 6.2 hypo 69.9 42.7 48.9 34.2 ionosphere 13.2 7.2 5.9 3.6 iris 5.5 1.8 2.3 2.7 labor 1.3 1.1 1.2 1.2 led7 3.6 1.9 2.4 2.9 lymph 11.5 4.2 4.8 4.2 pima 5.4 2.2 1.9 2.5 sick 65.8 32.5 38.8 27.6 sonar 12.1 5.5 7.3 6.1 tic-tac-toe 2.8 1.4 2.0 1.8 vehicle 14.4 8.6 8.2 6.5 waveform 19.8 12.8 16.6 11.7 wine 9.2 4.5 3.3 6.8 zoo 7.1 3.2 4.7 6.6 Average 12.6 7.2 8.0 6.9
