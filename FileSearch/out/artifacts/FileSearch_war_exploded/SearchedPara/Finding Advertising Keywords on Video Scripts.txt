 A key to success to contextual in-video advertising is find-ing advertising keywords on video contents effectively, but there has been little literature in the area so far. This paper presents some preliminary results of our learning-based sys-tem that finds relevant advertising keywords on particular scene of video contents using their scripts. The system is trained with not only features proven useful in earlier stud-ies but novel features that reflect the situation of a targeted scene. Experimental results show that the new features are potentially helpful for enhancing the accuracy of keyword extraction for contextual in-video advertising.
 H.3.1 [ Content Analysis and Indexing ]: Abstracting methods Algorithms, experimentation Keyword extraction, contextual in-video advertising
Contextual advertising refers to the placement of relevant advertisements based on the content displayed to the user. Due to the rapid growth of online video consumptions and  X  X n-video X  advertising demands, many companies have re-cently shown interest (and some even began) providing such form of targeted advertising on online videos. In this pa-per, we focus specifically on the advertising keyword (hence-forth  X  X dword X ) extraction, which is an essential technology of any contextual advertising systems. Adword extraction is extremely important since better adwords may lead to presenting more relevant advertisements, directly increasing click rates and potential revenue, but there has been a few formal researches published in the area.

To find adwords in a video, one must get access to the textual content of the video. Existing speech recognition or  X  This work was done while Young-In Song was with the Dept. of Computer Science and Engineering, Korea Univer-sity.
 automated visual analysis techniques can help but may not be able to consistently capture the whole content without er-ror. Therefore, as a preliminary step, we have experimented with online videos of which written scripts (consisting of dia-logues and instructions) are also available online. We believe the use of video scripts for extracting adwords is practically applicable today, because there are many ways to acquire them easily via the Web in these days.

A major shortcoming of applying prior adword extraction methods directly on video scripts is that those methods ba-sically rely on  X  X ithin-document X  term statistics features, such as TF-IDF scores. In case of contextual in-video ad-vertising, things that are visually shown to the user on the video screen ( e.g. settings, character X  X  actions) may be good subjects for advertising. Imagine a video scene where a char-acter is driving a fancy sports car while talking to someone sitting next to him about what happened at work. In this case,  X  X ports car X  would be a wonderful choice for adword, but term statistics-based methods may be unable to select it as an adword if either it has very low TF values within the scene or some other words related to the character X  X  work have higher TF-IDF scores.

The final goal of our adword extraction system is, given a scene, to find not only words topically important but those relevant to the particular situation of the scene as adwords. In this paper, we report preliminary results of our adword extraction experiments with knowledge-rich features reflect-ing the targeted scene X  X  situation, along with other features previously used in earlier adword extraction studies [1, 3] that are potentially useful and directly applicable to our problem.
Our adword extraction system considers each unigram and bigram that appears in the script as an adword candidate. We use logistic regression models ( i.e. maximum entropy models) as the learning algorithm, as in [3]. Simply put, given training data (in our case, an adword candidate) with label Y (where Y = 1 if the data is relevant and 0 otherwise) and a feature set x , the model learns a vector of weight parameters w for features in x . 1 Giventestdatawithfeature vector x , the model returns the conditional probability of predicting the data as relevant, P ( Y =1 | X = x ). Our system ranks the candidates by this output value and selects the top-ranked ones as adwords.
The Limited Memory Variable Metric (L-BFGS) method [2] is used for estimating model parameters. IR : Includes TF and DF, and their log 2 values.

List : [3] reports that the use of query logs significantly improves the performance of adword extraction. Since such resources are generally not opened to public, we collected ad-word samples instead from a well-known Sponsored Search company website where a number of adwords are suggested according to business types. We consider whether the can-didate exists in this list, the number of its clicks recorded for a month, and its bid price as features.

Loc : Includes the relative location of the occurrence of the candidate and whether the candidate belongs to dialogue or instruction part of the script.
 Len : Includes the lengths of candidate and of scene.
Lin : Includes the candidate X  X  part-of-speech (POS) infor-mation. Such features were reported not useful in [3], but we still considered them potentially useful in our case.
H : [1] shows that TF  X  X istory X  ( i.e. TF value in previous scenes) is also useful when extracting adwords from online broadcasting data, which is continuous and temporally or-dered. We calculate the TF history as in [1].

P : Language patterns based on sequential pattern mining were also proven to be useful in [1]. We have mined both lexical and POS patterns in a similar way. We consider whether the candidate occurs in a specific pattern and the pattern X  X  confidence value [1] as features.
Our hypothesis is that being aware of the situation in a scene would allow more distinguishable power to adword extraction engines. Although it is hardly possible to specif-ically cover all possible real world situations, we pre-defined a general situation classes for adword extraction task as fol-lows. Category names of businesses/products were collected from various sources ( e.g. shopping websites). Then, for each category, four researchers were asked to suggest at most three general situations that the category reminds of. For example, a category  X  X ealth X  reminds of exercise scenes and hospital scenes. Finally, all the suggested situations were manually clustered into 20 different clusters (classes).
Thefirsttypeofsituationfeatureweuseisthesituation class the targeted scene belongs to. In this paper, we as-sume that we have a  X  X lack box X  that maps a given scene against at least one of the situation classes and use the data of which the mapping is done manually by human, leaving the construction of the  X  X ox X  as future work.

Another type of feature we use is the degree of association between the candidate and each situation, assuming that the higher the associativity, the higher the chance the candidate is an adword. Specifically, at offline, we calculate pointwise mutual information (PMI) between a term t and situation s based on their co-occurrence information obtained from situation-annotated scripts as follows: where count ( t, s )isthenumberoftimes t occurred in s , count ( t ) is the collection frequency of t ,and count ( number of all words in scenes mapped to s .

The last information we use is the difference between the candidate X  X  PMI and the average PMI of all content words ( i.e. nouns, verbs, adjectives, and adverbs) in the targeted scene for each situation class. Our intuition is that the Table 1: Performance of different feature set combi-nations. feature combination top-1 top-5 top-10 [3] (IR+List+Loc+Len+Lin) 28.44 b 46.78 b 60.76 b [3] + [1] (H+P) 28.71 46.39 60.76 [3] + [1] + Situation 29.75  X  50.79  X  65.51  X  Table 2: Performance by removing one feature set. feature combination top-1 top-5 top-10 All ([3] + [1] + Situation) 29.75 b 50.79 b 65.51 b -List 21.69  X  38.95  X  56.07  X  -Loc 22.32  X  41.96  X  56.55  X  -Lin 25.67  X  42.46  X  55.72  X  -Situation 28.71  X  46.39  X  60.76  X  -Len 28.34 47.53  X  63.19  X  -IR 28.65 48.54  X  63.70  X  -P 29.49 49.62  X  64.83  X  -H 29.86 50.46 65.64 higher the difference, the more the relative chance that the candidate is an adword among other terms in the scene.
We selected 10 popular drama shows and collected the first 5 episode scripts per show from various sources (50 scripts, 3404 scenes total). Three graduate students majoring in mass communications manually annotated adwords in each scene, if any, using 1-3 scale depending on their relativity to the scene X  X  situation (1=not related, 2=somewhat related, 3=highly related). In this paper, we consider only adwords with scale-3 as relevant; among 3404 scenes, 1909 had at least one scale-3 adwords and were used for experiments.
Top-n scores [3] at top ranks were used for evaluation measure. We performed 10-fold cross validation by using one show X  X  scripts for testing and the rest for training. For all experiments, we picked a reasonable baseline(with symbol ) and computed statistical significance between results and the baseline with two-tailed paired t-test (95% level with symbol  X  and 99% level with symbol  X  ).

The result in Table 1 indicates that situation information makes a statistically significant improvement in the perfor-mance of adword extraction in scripts. Table 2 shows that features that do not directly depend on  X  X ithin-document X  term statistics ( e.g. List , Loc , Lin , Situation )tendtocon-tribute more than the ones that do ( e.g. IR , H ). In future work, we plan to explore more te rm statistics-independent, knowledge-rich features for adword extraction engines in contextual in-video advertising systems. [1] H. Li, D. Zhang, J. Hu, H. Zeng, and Z. Chen. Finding keyword from online broadcasting content for targeted advertising. In ADKDD  X 07 , pages 55 X 62, 2007. [2] R. Malouf. A comparison of algorithms for maximum entropy parameter estimation. In CoNLL-02 , pages 1 X 7, 2002. [3] W. Yih, J. Goodman, and V. R. Carvalho. Finding advertising keywords on web pages. In WWW  X 06 , pages 213 X 222, 2006.
