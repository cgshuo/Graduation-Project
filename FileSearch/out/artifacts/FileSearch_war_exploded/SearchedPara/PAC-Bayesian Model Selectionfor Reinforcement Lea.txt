 Bayesian methods in machine learning, although elegant and concrete, have often been criticized not only for their computational cost, but also for their strong assumptions on the correctness of the prior distribution. There are usually no theoretical guarantees when performing Bayesian inference with priors that do not admit the correct posterior. Probably Approximately Correct (PAC) learning techniques, on the other hand, provide distribution-free convergence guarantees with polynomially-bounded sample sizes [1]. These bounds, however, are notoriously loose and impractical. One can argue that such loose bounds are to be expected, as they reflect the inherent difficulty of the problem when no assumptions are made on the distribution of the data.
 Both PAC and Bayesian methods have been proposed for reinforcement learning (RL) [2, 3, 4, 5, 6, 7, 8], where an agent is learning to interact with an environment to maximize some objective function. Many of these methods aim to solve the so-called exploration X  X xploitation problem by balancing the amount of time spent on gathering information about the dynamics of the environment and the time spent acting optimally according to the currently built model. PAC methods are much more conservative than Bayesian methods as they tend to spend more time exploring the system and collecting information [9]. Bayesian methods, on the other hand, are greedier and only solve the problem over a limited planning horizon. As a result of this greediness, Bayesian methods can converge to suboptimal solutions. It has been shown that Bayesian RL is not PAC [9]. We argue here that a more adaptive method can be PAC and at the same time more data efficient if an informative prior is taken into account. Such adaptive techniques have been studied within the PAC-Bayesian literature for supervised learning.
 The PAC-Bayesian approach, first introduced by McAllester [10] (extending the work of Shawe-Taylor et al. [11]), combines the distribution-free correctness of PAC theorems with the data-efficiency of Bayesian inference. This is achieved by removing the assumption of the correctness of the prior and, instead, measuring the consistency of the prior over the training data. The empirical results of model selection algorithms for classification tasks using these bounds are comparable to some of the most popular learning algorithms such as AdaBoost and Support Vector Machines [12]. PAC-Bayesian bounds have also been linked to margins in classification tasks [13]. This paper introduces the first results of the application of PAC-Bayesian techniques to the batch RL problem. We derive two PAC-Bayesian bounds on the approximation error in the value function of stochastic policies for reinforcement learning on observable and discrete state spaces. One is a bound on model-based RL where a prior distribution is given on the space of possible models. The second one is for the case of model-free RL, where a prior is given on the space of value functions. In both cases, the bound depends both on an empirical estimate and a measure of distance between the stochastic policy and the one imposed by the prior distribution. We present empirical results where model-selection is performed based on these bounds, and show that PAC-Bayesian bounds follow Bayesian policies when the prior is informative and mimic the PAC policies when the prior is not consistent with the data. This allows us to adaptively balance between the distribution-free correctness of PAC and the data-efficiency of Bayesian inference. In this section, we introduce the notations and definitions used in the paper.
 A Markov Decision Process (MDP) M = ( S,A,T,R ) is defined by a set of states S , a set of actions A , a transition function T ( s,a,s 0 ) defined as: and a (possibly stochastic) reward function R ( s,a ) : S  X  A  X  [ R min ,R max ] . Throughout the paper we assume finite-state, finite-action, discounted-reward MDPs, with the discount factor denoted by  X  . A reinforcement learning agent chooses an action and receives a reward. The environment will then change to a new state according to the transition probabilities.
 A policy is a (possibly stochastic) function from states to actions. The value of a state X  X ction pair agent acts according to that policy after taking action a in the first step. The value function satisfies the Bellman equation [14]: The optimal policy is the policy that maximizes the value function. The optimal value of a state X  action pair , denoted by Q  X  ( s,a ) , satisfies the Bellman optimality equation [14]: There are many methods developed to find the optimal policy for a given MDP when transition and reward functions are known. Value iteration [14] is a simple dynamic programming method in which one iteratively applies the Bellman optimality operator , denoted by B , to an initial guess of the optimal value function: For simplicity we write BQ when B is applied to the value of all state X  X ction pairs. Since B is a contraction with respect to the infinity norm [15] (i.e. k BQ  X  BQ 0 k  X   X   X  k Q  X  Q 0 k  X  ), the value iteration algorithm will converge to the fixed point of the Bellman optimality operator, which is the optimal value function ( BQ  X  = Q  X  ). In model-based RL, one aims to estimate the transition and reward functions and then act optimally according to the estimated models. PAC methods use the empirical average for their estimated model along with frequentist bounds. Bayesian methods use the Bayesian posterior to estimate the model. between these two extremes, which is both data-efficient and has guaranteed performance. Assuming that the reward model is known (we make this assumption throughout this section), one can build empirical models of the transition dynamics by gathering sample transitions, denoted by where n s,a,s 0 and n s,a are the number of corresponding transitions and samples. Trivially, E  X  T = T . The empirical value function, denoted by  X  Q , is defined to be the value function on an MDP with the empirical transition model. As one observes more and more sample trajectories on the MDP, the empirical model gets increasingly more accurate, and so will the empirical value function. Different forms of the following lemma, connecting the error rates on  X  T and  X  Q , are used in many of the PAC-MDP results [4]: Lemma 1. There is a constant k  X  (1  X   X  ) 2 / X  such that: As a consequence of the above lemma, one can act near-optimally in the part of the MDP for which we have gathered enough samples to have a good empirical estimate of the transition model. PAC-MDP methods explicitly [2] or implicitly [3] use that fact to exploit the knowledge on the model as long as they are in the  X  X nown X  part of the state space. The downside of these methods is that without further assumptions on the model, it will take a large number of sample transitions to get a good empirical estimate of the transition model.
 The Bayesian approach to modeling the transition dynamics, on the other hand, starts with a prior distribution over the transition probability and then marginalizes this prior over the data to get a posterior distribution. This is usually done by assuming independent Dirichlet distributions over the transition probabilities, with some initial count vector  X  , and then adding up the observed counts to this initial vector to get the conjugate posterior [6]. The initial  X  -vector encodes the prior knowledge on the transition probabilities, and larger initial values further bias the empirical observation towards the initial belief.
 If a strong prior is close to the true values, the Bayesian posterior will be more accurate than the empirical point estimate. However, a strong prior peaked on the wrong values will bias the Bayesian model away from the correct probabilities. Therefore, the Bayesian posterior might not provide the optimal estimate of the model parameters. A good posterior distribution might be somewhere between the empirical point estimate and the Bayesian posterior.
 The following theorem is the first PAC-Bayesian bound on the estimation error in the value function when we build a stochastic policy 1 based on some arbitrary posterior distribution M q . Theorem 2. Let  X   X  T 0 be the optimal policy with respect to the MDP with transition model T 0 and  X  any i.i.d. sampling distribution U , with probability no less than 1  X   X  over the sampling of U  X  X  : where n min = min s,a n s,a and D ( . k . ) is the Kullback X  X eibler (KL) divergence.
 The above theorem (proved in the Appendix) provides a lower bound on the expectation of the true value function when the policy is taken to be optimal according to the sampled model from the posterior: This lower bound suggests a stochastic model-selection method in which one searches in the space of posteriors to maximize the bound. Notice that there are two elements to the above bound. One is the PAC part of the bound that suggests the selection of models with high empirical value func-tions for their optimal policy. There is also a penalty term (or a regularization term) that penalizes distributions that are far from the prior (the Bayesian side of the bound). Margin for Deterministic Policies One could apply Theorem 2 with any choice M q . Generally, this will result in a bound on the value of a stochastic policy. However, if the optimal policy is the same for all of the possible samples from the posterior, then we will get a bound for that particular deterministic policy.
 We define the support of policy  X  , denoted by T  X  , to be the set of transition models for which the optimal policy is  X  . Putting all the posterior probability on T  X  will result in a tighter bound for the value of the policy  X  . The tightest bound occurs when M q is a scaled version of M p summing to 1 over T  X  , that is when we have: In that case, the KL divergence is D ( M q k M p ) =  X  ln M p ( T  X  ) , and the bound will be: Intuitively, we will get tighter bounds for policies that have larger empirical values and higher prior probabilities supporting them.
 Finding M p ( T  X  ) might not be computationally tractable. Therefore, we define a notion of margin for transition functions and policies and use it to get tractable bounds. The margin of a transition function T 0 , denoted by  X  T 0 , is the maximum distance we can move away from T 0 such that the optimal policy does not change: The margin defines a hypercube around T 0 for which the optimal policy does not change. In cases where the support set of a policy is difficult to find, one can use this hypercube to get a reasonable bound for the true value function of the corresponding policy. In that case, we would define the posterior to be the scaled prior defined only on the margin hypercube. The idea behind this method is similar to that of the Luckiness framework [11] and large-margin classifiers [16, 13]. This shows that the idea of maximizing margins can be applied to control problems as well as classification and regression tasks.
 To find the margin of any given T 0 , if we know the value of the second best policy, we can calculate its regret according to T 0 (it will be the smallest regret  X  min ). Using Lemma 1, we can conclude most  X  min / 2 , and thus the optimal policy will not change. Therefore,  X  T 0  X  k X  min / 2 . One can then define the posterior on the transitions inside the margin to get a bound for the value function. In this section we introduce a PAC-Bayesian bound for model-free reinforcement learning on dis-crete state spaces. This time we assume that we are given a prior distribution on the space of value functions, rather than on transition models. This prior encodes an initial belief about the optimal value function for a given RL domain. This could be useful, for example, in the context of transfer learning, where one has learned a value function in one environment and then uses that as the prior belief on a similar domain.
 We start by defining the TD error of a given value function Q to be k Q  X  BQ k  X  . In most cases, we do not have access to the Bellman optimality operator. When we only have access to a sample set U collected on the RL domain, we can define the empirical Bellman optimality operator  X  B to be: Note that E [  X  BQ ] = BQ . We further make an assumption that all the BQ values we could observe are bounded in the range [ c min ,c max ] , with c = c max  X  c min . Using this assumption, one can use Hoeffding X  X  inequality to bound the difference between the empirical and true Bellman operators: When the true Bellman operator is not known, one makes use of the empirical TD error, similarly defined to be k Q  X   X  BQ k  X  . Q-learning [14] and its derivations with function approximation [17], and also batch methods such as LSTD [18], often aim to minimize the empirical (projected) TD error. We argue that it might be better to choose a function that is not a fixed point of the empirical Bellman operator. Instead, we aim to minimize the upper bound on the approximation error (which might be referred to as loss) of the Q function, as compared to the true optimal value. The following theorem (proved in the Appendix) is the first PAC-Bayesian bound for model-free batch RL on discrete state spaces: over the space of value functions, with probability no less than 1  X   X  over the sampling of U  X  X  : This time we have an upper bound on the expected approximation error: This suggests a model-selection method in which one would search for a posterior J q to minimize the above bound. The PAC side of the bound guides this model-selection method to look for posteriors with smaller empirical TD error. The Bayesian part, on the other hand, penalizes the selection of posteriors that are far from the prior distribution.
 One can use general forms of priors that would impose smoothness or sparsity for this model-selection technique. In that sense, this method would act as a regularization technique that penalizes complex and irregular functions. The idea of regularization in RL with function approximation is not new to this work [19]. This bound, however, is more general, as it could incorporate not only smoothness constraints, but also other forms of prior knowledge into the learning process. model-based RL domain and one model-free problem. The model-based domain is a chain model in which states are ordered by their index. The last state has a reward of 1 and all other states have reward 0 . There are two types of actions. One is a stochastic  X  X orward X  operation which moves us to the next state in the chain with probability 0 . 5 and otherwise makes a random transition. The second type is a stochastic  X  X eset X  which moves the system to the first state in the chain with probability 0 . 5 and makes a random transition otherwise. In this domain, we have at each state two actions that do stochastic reset and one action that is a stochastic forward. There are 10 states and  X  = 0 . 9 . When there are only a few number of sample transitions for each state X  X ction pair, there is a high chance that the frequentist estimate confuses a reset action with a forward. Therefore, we expect a good model-based prior to be useful in this case. We use independent Dirichlets as our prior. We experiment with priors for which the Dirichlet  X  -vector sums up to 10 . We define our good prior to have  X  -vectors proportional to the true transition probabilities. A misleading prior is one for which the vector is proportional to a transition model when the actions are switched between forward and reset. A weighted sum between the good and bad priors creates a range of priors that gradually change from being informative to misleading.
 We compare the expected regret of three different methods. The empirical method uses the optimal policy with respect to the empirical models. The Bayesian method samples a transition model from the Bayesian Dirichlet posteriors (when the observed counts are added to the prior  X  -vectors) and then uses the optimal policy with respect to the sampled model. The PAC-Bayesian method uses counts +  X  X  prior as the  X  -vector of the posterior and finds the value of  X   X  [0 , 1] , using linear search within values with distance 0 . 1 , that maximizes the lower bound of Theorem 2 (with a more optimistic value for k and  X  = 0 . 05 ). It then samples from that distribution and uses the optimal policy with respect to the sampled model. The running time for a single run is a few seconds. Figure 1 (left) shows the comparison between the maximum regret in these methods for different sample sizes when the prior is informative. This is averaged over 50 runs for the Bayesian and PAC-Bayesian methods and 10000 runs for the empirical method. The number of sampled transitions is the same for all state X  X ction pairs. As expected, the Bayesian method outperforms the empirical one for small sample sizes. We can see that the PAC-Bayesian method is closely following the Bayesian one in this case. With a misleading prior, however, as we can see in Figure 1 (center), the empirical method outperforms the Bayesian one. This time, the regret rate of the PAC-Bayesian method follows that of the empirical method. Figure 1 (right) shows how the PAC-Bayesian method switches between following the empirical estimate and the Bayesian posterior as the prior gradually changes from being misleading to informative (four sample transitions per state action pair). This shows that the bound of Theorem 2 is helpful as a model selection technique. Figure 1: Average regrets of different methods. Error bars are 1 standard deviation of the mean. The next experiment is to test the model-free bound of Theorem 3. The domain is a  X  X uddle world X . An agent moves around in a grid world of size 5  X  9 containing puddles with reward  X  1 , an absorbing goal state with reward +1 , and reward 0 for the remaining states. There are stochastic actions along each of the four cardinal directions that move in the correct direction with probability 0 . 7 and move in a random direction otherwise. If the agent moves towards the boundary then it stays in its current position.
 We first learn the true value function of a known prior map of the world (Figure 2, left). We then use that value function as the prior for our model-selection technique on two other environments. One of them is a similar environment where the shape of the puddle is slightly changed (Figure 2, center). We expect the prior to be informative and useful in this case. The other environment is, however, largely different from the first map (Figure 2, right). We thus expect the prior to be misleading. We start with independent Gaussians (one for each state X  X ction pair) as the prior, with the initial map X  X  Q -values for the mean  X  0 , and  X  2 0 = 0 . 01 for the variance. The posterior is chosen to be the product of Gaussians with mean  X  X  0  X  2  X   X  2 is the empirical variance. We sample from this posterior and act according to its greedy policy. For  X  = 1 , this is the Bayesian posterior for the mean of a Gaussian with known variance. For  X  = 0 , the prior is completely ignored. We will, however, find the  X   X  [0 , 1] that minimizes the PAC-Bayesian bound of Theorem 3 (with an optimistic choice of c and  X  = 0 . 05 ) and compare it with the performance of the empirical policy and a semi-Bayesian policy that acts according to a sampled value from the Bayesian posterior.
 Table 1 shows the average over 100 runs of the maximum regret for these methods and the average of the selected  X  , with equal sample size of 20 per state X  X ction pair. Again, it can be seen that the PAC-Bayesian method makes use of the prior (with higher values of  X  ) when the prior is informative, and otherwise follows the empirical estimate (smaller values of  X  ). It adaptively balances the usage of the prior based on its consistency over the observed data. This paper introduces the first set of PAC-Bayesian bounds for the batch RL problem in finite state spaces. We demonstrate how such bounds can be used for both model-based and model-free RL methods. Our empirical results show that PAC-Bayesian model-selection uses prior distributions when they are informative and useful, and ignores them when they are misleading.
 For the model-based bound, we expect the running time of searching in the space of parametrized posteriors to increase rapidly with the size of the state space. A more scalable version would sample models around the posteriors, solve each model, and then use importance sampling to estimate the value of the bound for each possible posterior. This problem does not exist with the model-free approach, as we do not need to solve the MDP for each sampled model.
 A natural extension to this work would be on domains with continuous state spaces, where one would use different forms of function approximation for the value function. There is also the possibility of future work in applications of PAC-Bayesian theorems in online reinforcement learning, where one targets the exploration X  X xploitation problem. Online PAC RL with Bayesian priors has recently been addressed with the BOSS algorithm [20]. PAC-Bayesian bounds could help derive similar model-free algorithms with theoretical guarantees.
 Acknowledgements: Funding for this work was provided by the National Institutes of Health (grant R21 DA019800) and the NSERC Discovery Grant program.
 The following lemma, due to McAllester [21], forms the basis of the proofs for both bounds: Note that even when we have arbitrary probability measures Q and P on a continuous space of  X  (1) ,  X  (2) ,... such that Q ( n ) , P ( n ) and  X  ( n ) satisfy the condition of the lemma and We will then take the limit of the conclusion of the lemma to get a bound for the continuous case [21]. Proof of Theorem 2 (Model-Based Bound) Lemma 5. Let  X  T 0 = k  X  Q  X   X  T 0  X  Q  X   X  T 0 k  X  . With probability no less than 1  X   X  over the sampling: Before proving Lemma 5, note that Lemma 5 and Lemma 4 together imply Therorem 2. We only need to apply the method described for arbitrary probability measures. To prove Lemma 5, it suffices to prove the following, swap the expectations and apply Markov X  X  inequality: bound. Let a s =  X   X  T 0 ( s ) . We have: The first line is by Lemma 1. The second line is a concentration inequality for multinomials [22]. The maximum occurs when the inequality is tight and the p.d.f. for  X  T 0 is: We thus get: This concludes the proof of Lemma 5 and consequently Theorem 2.
 Proof of Theorem 3 (Model-Free Bound) Since B is a contraction with respect to the infinity norm and Q  X  is its fixed point, we have: And thus k Q  X  Q  X  k  X   X  1 1  X   X  k Q  X  BQ k  X  .
 Lemma 6. Let  X  Q = max(0 , k Q  X  Q  X  k  X   X  k Q  X   X  BQ k  X  1  X   X  ) . With probability no less than 1  X   X  : Similar to the previous section, Lemma 6 and Lemma 4 together imply Theorem 3.
 To prove Lemma 6, similar to the previous proof, we only need to show that for any choice of Q , E Pr {  X  Q  X  } = Pr n k Q  X  Q  X  k  X   X  + k Q  X   X  BQ k  X  / (1  X   X  ) o (27) Eqn (28) follows from the derivations at the beginning of this section. Eqn (29) is by the union bound. Eqn (31) is by the definition of infinity norm. Last derivation is by Hoeffding inequality of Equation (12). Now again, similar to the model-based case, when the inequality is tight the p.d.f. is: We thus get: This concludes the proof of Lemma 6 and consequently Theorem 3.
