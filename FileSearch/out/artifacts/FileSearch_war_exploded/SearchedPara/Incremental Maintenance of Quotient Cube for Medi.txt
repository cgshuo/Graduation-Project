 Data cube pre-computation is an important concept for sup-porting OLAP(Online Analytical Processing) and has been studied extensively. It is often not feasible to compute a complete data cube due to the huge storage requirement. Recently proposed quotient cube addressed this issue through a partitioning method that groups cube cells into equiva-lence partitions. Such an approach is not only useful for distributive aggregate functions such as SUM but can also be applied to the holistic aggregate functions like MEDIAN.
Maintaining a data cube for holistic aggregation is a hard problem since its difficulty lies in the fact that history tu-ple values must be kept in order to compute the new ag-gregate when tuples are inserted or deleted. The quotient cube makes the problem harder since we also need to main-tain the equivalence classes. In this paper, we introduce two techniques called addset data structure and sliding window to deal with this problem. We develop efficient algorithms for maintaining a quotient cube with holistic ag-gregation functions that takes up reasonably small storage space. Performance study shows that our algorithms are effective, efficient and scalable over large databases. Categories and Subject Descriptors: H.2.8 [Database applications]: Data mining General Terms: Algorithms Keywords: Data Cube, Holistic Aggregation.
Data cube pre-computation is an important concept for supporting OLAP(Online Analytical Processing) and has been studied extensively. It is often not feasible to com-pute a complete data cube due to the huge storage require-ment. Recently proposed quotient cube [10] addressed this issue through a partitioning method that groups cube cells into equivalence classes, thus reducing computation time and storage overhead.

The intuition behind quotient cube is that many cube cells in a data cube are in fact aggregated from the same set of  X 
Work done while the author was visiting National Univer-sity of Singapore  X  Contact Author: atung@comp.nus.edu.sg base tuples (and thus have the same aggregate value). By grouping these cube cells together, the aggregate values of these cells need only to be stored once. This gives substan-tial reduction in cube X  X  storage while preserving cube X  X  roll up and drill down semantics.

Consider a data warehouse schema that consists of three dimensions: A, B, and C. The schema includes a single mea-sure M. The domain values for these three dimension at-tributes are domain(A)= { a 1 ,a 2 ,a 3 } , domain(B)= { b b } , domain(C)= { c 1 ,c 2 ,c 3 ,c 4 } . We use relation R in Fig-ure 1(a) as the base relation table. A data cube lattice expressed by the following query: has 22 distinct cube cells as shown in Figure 1 (b). The cube lattice can be partitioned into 9 disjointed equivalence classes, each represented by a circle as shown in Figure 1(b).
All cells in an equivalence class are aggregated from the same set of the base relation tuples. For example, cells a 1 and a are both aggregated from tuple 3 and tuple 4, thus they are grouped together and their aggregate values can be stored only once. The nine equivalence classes in Figure 1(b) can be represented with another lattice (so called quotient cube) shown in Figure 1(c), where a class C (e.g. C 1 )isabove another class C (e.g. C 2 ) exactly when we can drill down from some cell in C(e.g. b 1 ) to some cell in C (e.g.a 1
Thus the roll up and drill down semantics among the cube cells are preserved.

Such an approach is not only useful for distributive aggre-gate functions such as SUM but can also be applied to holis-tic aggregate functions like MEDIAN which will require the storage of a set of tuples for each equivalence partition. Un-fortunately, as changes are made to the data sources, main-taining the quotient cube is non-trivial since the partitioning of the cube cells must also be updated. If some tuples are in-serted to the base relation R, some equivalence classes need to be updated or split.

Existing proposals for incremental quotient cube main-tenance [11] are not able to maintain a quotient cube with holistic aggregation functions such as MEDIAN and QUAN-
TILE. Incremental updating holistic aggregations is difficult since in the case of changes to base tuples, the new aggre-gate value cannot be computed incrementally based on the previous aggregate value and the new values of the changed tuples. In this paper, we will propose a solution for the incremental maintenance of a quotient cube with holistic aggregation. We identify our contributions as follows:  X  We introduce the concept of addset data structure  X  We propose a novel sliding window technique to ef- X  By relating quotient cube to well established theory  X  We conduct a comprehensive set of experiments on
The remaining of the paper is organized as follows. Sec-tion 2 gives the background information of our study. Sec-tion 3 introduces our techniques for maintaining holistic ag-gregation function median. Section 4 presents two incremen-tal maintenance algorithms for holistic aggregate function
MEDIAN. A performance analysis of our methods is pre-sented in section 5. We give other related works in section 6 and make some conclusions in section 7.
In this section, we will provide the necessary background for discussion in the rest of this paper. We first define some notations in section 2.1 and then briefly explain the main-tenance principles of quotient cube in section 2.2.
The base relation of a data warehouse is composed of one or more dimensions D 1 ,..., D n and a measure M .Wedenote the domain of a dimension D i as dom ( D i )
Definition 1 (Relational Tuple). Atuple t in a base relation R of a data warehouse has the form t =(tid, dvalue, m ), where tid is the unique tuple identification of t , dvalue  X  dom ( D 1 )  X  dom ( D 2 ) ...  X  dom ( D n ) is the dimension value set of t ,and m is the measure value of t .Weuse t.tid, t.dvalue, and t.m to represent each component of t respectively.  X 
As an example, for tuple t =(2, a 3 b 2 c 1 , 10), t.tid is 2, t.dvalue is { a 3 ,b 2 ,c 1 } ,and t.m is 10. For convenience, we use a 3 b 2 c 1 to represent the dimension value set { a 3
Definition 2 (Cell). A cell, c , in a data cube is a tuple over the dimension attribute domains where the spe-cial value  X  X ll  X  X s allowed i.e. c  X  ( dom ( D 1 )  X   X  all ) ... ( dom ( D n )  X   X  all ) .Acell c = { d 1 , ...d n } is said to be more general than another cell c = { d 1 , ..., d n } if for all i , either d i = d i or d i = X  all .  X 
The special value  X  X ll X  in this case represents a don X  X  care condition in a particular dimension. For clearer representa-tion, we will assume it to be the default whenever a dimen-sional value is missing. For example the cell { a 1 ,all,c be represented as { a 1 ,c 1 } (or a 1 c 1 to simplify further).
Definition 3 (Matching). Atuple t is said to match acell c if t.dvalue matches c in all dimensions except for those dimensions in which the value for c is  X  X ll X . Given a set of cells, C ,atuple t is said to match C if it matches all the cells in C .  X 
For example, the tuple t =(2, a 3 b 2 c 1 , 10) matches both the cell a 3 b 2 and also the set of cells in Class C 8 of Figure 1(b).

All the possible cells in a data cube can be organized into a lattice and each cell is represented with an element of a lattice. A lattice is a partially order set ( L , )inwhich every pair of elements in L has a Least Upper Bound(LUB) and a Greatest Lower Bound(GLB) within L .

Formally, a partially ordered set is defined as an ordered pair P =( L , ), where L is called the ground set of P and is the partial order of P. For the case of data cube, the set of cells are in the set L and will be defined in Definition 5. Since the lattice theories are well studied, we can borrow some ideas from lattice to design incremental maintaining algorithms by relating data cubes with lattices.

Definition 4 (LUB and GLB). Given a set of ele-ments E in a lattice ( L , ) , the least upper bound(LUB) of
E is an element u  X  X  such that e u for all e  X  E and there exists no u such that e u for all e  X  E and u u .
Likewise, the greatest lower bound(GLB) of E is an element l  X  X  such that l e for all e  X  E and there exists no l such that l e for all e  X  E and l l .LUBandGLB are unique.  X 
If L is finite, then ( L , ) is a finite lattice. A finite lattice can be represented using a lattice diagram in which elements in
L are nodes and there is an edge from a node representing an element e to a node representing another element e iff e e and there exists no other element e such that e e e .

Definition 5 (Cube Lattice). A cube lattice for a data cube is a finite lattice ( L , ) in which L contains all possible cells in the data cube plus a special cell  X  X alse  X (the least general cell) and two cells c , c satisfies c c iff c is more general than c or c equals to  X  X alse X .  X 
An example of a cube lattice is shown in Figure 1(b)(cell  X  X alse X  is not shown for clarity). For example, the cell  X  ALL  X  a 1 and there is an edge from cell  X  ALL  X  X ocell a We can now formally define the concept of an equivalence class of cells in a cube lattice.

Definition 6 (Equivalence Class). Asetofcells in a cube lattice is said to belong to the same equivalence class, C ,if 1. Given any two cells c , c in C which satisfy c c ,any 2. C is the maximal set of cells that are matched by the
Since all cells in an equivalence class are matched by the same set of tuples, it is possible to find a unique cell which is the upper bound for the whole class by simply selecting those dimension values that are the same for every tuple in the matching set. For example, all cells in Class C 2 of Figure 1(c) are matched by tuples 3 and 4 in Figure 1(a) and thus the upper bound of Class C 2 is a 1 b 1 whichiscommontoall the tuples.
 In general, a class C can be represented by a structure, C =( upp, m ), in which upp is the upper bound of the class and m is the aggregated value for the set of tuples that match the cells in C .Weuse C.upp and C.m to represent each component of C respectively. For example, class C 2 in Figure 1(c) has the form of C 2 =(a 1 b 1 ,9).
In this section, we will revise the underlying principles for maintaining the equivalence classes in a quotient cube [11]. We observe that the cube lattice that is formed from the upper bounds of all equivalence classes in the quotient cube in fact has similar structure with a Galois lattice [6, 5]. Because of space limitation, we will not explain the re-lationship between the Galois lattice and the QC-tree [11] here.

We denote the set of equivalence classes in the original quotient cube as  X , and the new set of equivalence classes as  X  . In this section, we only consider the case that the incremental update is composed of a single tuple t . We will extend our method for bulk update with a set of new tuples later in Section 4.

New tuple t can affect an equivalence class in  X  in several ways. First, it can cause the aggregate value of the equiv-alence class to change without affecting the partitioning of the lattices. Second, it might cause the equivalence class to be split, creating some new equivalence classes. The final possibility is that the equivalence class might not be affected at all. When a new tuple matches the upper bound of an equivalence class, the new tuple t will cause the aggregate value of the equivalence class to be changed. More impor-tantly, we know that the tuple will match every cell in the equivalence class since the class X  X  upper bound is the most specific cell in the whole equivalence class. The equivalence class in this case needs not to be split.

Proposition 1 (Value Modified Class). Given an equivalence class C in  X  ,ifanewtuple t matches C. upp, then C needs not to be split but C.m must be modified.  X 
For example, in Figure 2, because c 1 matches a 1 b 3 c 1 new inserted tuple), we update the aggregation sum of C 7 from 20 to 25.

When t matches only a certain portion of C.upp , i.e. t can only match a portion of the cells in C , C must be split into two portions, one in which all cells match t and one in which all cells do not. A new tuple t affects a class C only if there is some intersection between t and C.upp .
Definition 7 (Intersection). We say that a tuple t has an intersection with (or intersects) a class C if t does not match C but t.dvalue  X  C.upp =  X  . Given  X  ,weuse intersect ( X  ,t ) to denote the set of classes in  X  that intersect t .  X 
Proposition 2 (New Class Generator). Given a new tuple t , an existing equivalence class C must be split if (1) C intersects t and C is the class that contributes the
GLB { Y | Y = C .upp  X  t } , C  X   X  ; and (2) there does not ex-ist any class C  X   X  such that C .upp = t  X  C.upp ;Ifthese two conditions are satisfied, we call C a new class generator since the splitting will result in a new equivalence class.
The first condition of Proposition 2 ensures that given all classes which generate the same upper bound for the new class C n , the one that is the most general (i.e. the GLB) will be the new class generator. For example in Figure 2, given the new tuple t =(5 ,a 1 b 3 c 1 , 5), we have C 2  X  = { a 1 } .Since C 4 is an upper bound of C 2 , C 2 will become a new class generator for t if it satisfies the second condition of Proposition 2. Note that C 4 will definitely not be split since none of the cells in C 4 matched t .Thisistobe expected because C 4 is more specific than C 2 and since even
C .upp can not match t , all cells in C 4 will also not match t .
Explaining the second condition in Proposition 2 is more simple. Since t  X  C represents the upper bound of the po-tential new class, there is no need to generate a new class if such an equivalence class already exists as indicated by the existence of a class C  X   X  such that C .upp = t  X  C.upp .
Note that in the situation in which t appears in R for the first time, i.e., t has no duplication in R, in this case, it is a new class itself but there will be no generator for it.
To ensure a generator for every new class, a virtual class is added in  X . The upper bound of the virtual class are the union of all the possible dimension value set, DV . The split operation of a class is defined as follows:
Definition 8 (Split Operation). Given a generator class C g ,C g  X   X  and a new tuple t , a split operation on C based on t generates a new class C n and a modified generator
C as follows:
The last proposition involves a simple category of equiv-alence classes that neither match or intersect the new tuple t .
 Proposition 3 (Dum bClass). If an equivalence class
C in  X  is neither a modified class nor a generator, there is no need to change C d .Wecall C d as a dumb class.  X 
Propositions 1-3 lay the foundation for maintaining a quo-tient cube. Updating the value of distributive aggregation is relatively simple. The new aggregate value can be computed incrementally based on the previous aggregate value and the new values of the changed tuple [11]. However, to update the value of holistic aggregation, all history tuple values must be kept and the new aggregate value needs to be recomputed even after one tuple is inserted or deleted. Expensive space and time cost make it to be unrealistic to incrementally up-date a holistic aggregation. In the following section, we will introduce two techniques called addset data structure and sliding window to deal with this problem.
MEDIAN is a holistic function which  X  X as no constant bound on the size of the storage needed to describe a sub-aggregate X  [7]. It is obvious that MEDIAN cannot be main-tained just by storing the final aggregation result from a set of tuples. One naive approach to maintaining MEDIAN value can be figured out as follows: (1) for each cell, we ex-plicitly store a set of measures from the tuples which match the cell. We call the set of measures, the measure set of the cell; (2) for each cell, we update its measure set and re-compute the aggregation MEDIAN value when new tuples are inserted.

In the above naive approach, storing the measure set for each cell can become prohibitively expensive because of the large number of cells and tuples. The concept of quotient cube helps to reduce this storage requirement as we can group cells into equivalence classes and store only one mea-sure set for each equivalence class. However, the size of Agg(a,b) means to apply the corresponding aggregate func-tion to a and b measure sets of quotient cube can be still prohibitively large since each measure set can be large. Moreover, new arriving tuples can result in more equivalence classes which again bring up the storage requirement substantially.

In this section, we present our techniques for updating a quotient cube with aggregate function MEDIAN. We will leave it to readers to see that these techniques are also ap-plicable in the maintenance of other holistic functions like
QUANTILE.
This subsection first describes intuitively how the concept of addset data structure can reduce the storage requirement for maintaining measure set, then proposes a more practical technique of addset data structure including both material-ized nodes and pseudo nodes.

Because different equivalence classes in a quotient cube may share some base tuples, there are some redundances among their measure sets. Figure 3(a) shows the quotient cube formed by the base table of Figure 1(a). The measure set of class C 9 is { 6,3,4,10 } and that of class C 7 is
It can be observed that { 6,4,10 } is actually redundant be-tween class C 9 and class C 7 . If we can remove this kind of redundance, lots of storage space can be spared.

Let us further the discussion to the scenario of updating quotient cube when new tuples come. We find that the tu-ples matching a newly generated equivalence class are always the superset of the tuples matching its generator (Proposi-tion 2).For example in Figure 2, C 10 is a newly generated equivalence class, and C 2 is its generator.The tuples match-the tuples matching C 2 are T 2 = { (3 ,a 1 b 1 c 2 , 3),(4 ,a
We have T 1  X  T 2 . Based on this property, we know that main-taining the list of measures in the new equivalence class can be done by simply storing the difference between the mea-sure set of the new class and that of its generator. We call this difference the addset of the new equivalence class.
Assuming that the four base tuples are inserted into a null quotient cube one by one, Figure 3(b) shows the naive addset data structure associated with the quotient lattice of
Figure 3(a)(detailed updating algorithm will be explained in section 4). For each new class, it only stores the difference between its measure set and the measure set of its generator.
For example, class C 5 is the generator of class C 6 , so class C only stores { 6 } which is the measure set difference between { 6,4 } and { 4 } . Class C 0 is specially introduced as the virtual class so that it can be the generator of the new classes formed by the four base tuples themselves.

Note the space saving we have by adopting the concept of addset in the simple example with only 4 tuples. Instead of storing 18 measures in the naive quotient cube approach in Figure 3(a), we now store only 10 measures (i.e. about 2 times better). The saving is expected to be much more when the number of tuples is large. Figure 4: Dynamical Materialization of Addset There is a linkage between the new class and its generator. Since each new class has a unique generator, the addset data structure is actually a tree. We call it a family tree 2 .
We can get the actual measure set of a class by combining the addsets along its family linkage path from the node rep-resenting the class to the root. For example, to find the full measure set of class C 1 , we combine the addsets of C 1 , C and C 6 i.e. { 3 } X  X  6 } X  X  4 } = { 3,6,4 } .

The naive addset data structure can work well if the fam-ily path is not very long. However, the computation cost of obtaining the measure set will increase with the length of family path. This may deteriorate performance when the family path is very long. In order to achieve some tradeoff between the space and time, we can dynamically material-ize some classes in the process of maintenance, i.e., compute the measure sets of these classes and store them explicitly.
Henceforth, we will refer to an equivalence class that stores the addset as a pseudo (equivalence) class and a class that stores the actual measure set as a materialized (equiv-alence) class . To obtain the actual measure set of a pseudo class, we only need to trace to its nearest materialized an-cestor class instead of the tree root. Figure 4 shows an example of dynamic materialization of addset, the grey cir-cles in Figure 4 represent materialized classes and the blank circles represent pseudo classes. The set of numbers besides a materialized class is its measure set and the set of numbers besides a pseudo class is its addset. (Note that the example in Figure 4 is different from the example in Figure 3 since the latter is too simple to explain the concept of dynamic materialization.) To compute the measure set of class K in Figure 4, we only need to trace to its first materialized ancestor(class E ) instead of the root node in naive addset structure.

Next we will address the problems of which classes should be materialized and when are they materialized. Similar to the problem of the materialized view selection [16], we should materialize those classes which can produce the largest benefit. If there is sufficient space, we can materialize more classes to save maintenance time; otherwise, we should keep more classes to be pseudo to save storage space. In this paper, we use a distance threshold to control the materi-alization of pseudo classes. When the distance between a pseudo class and its nearest materialized ancestor exceeds the given threshold, it will be materialized.

Definition 9 (Distance). Given a pseudo class C v and its nearest actual ancestor C a , the distance between C
Although the construction of family tree from scratch is not the focus of the paper, interested reader can find the Algorithms 4 and 5 to be described can be used for the purpose when assuming the existing quotient cube to be null and C a is the total numberofmeasures in the addset on the path from C v to C a .  X 
For example in Figure 4, assuming that the distance thresh-old is 3, the distance from classes E to its nearest material-ized ancestor (tree root) is 3 (while there are only two link-ages from E to the root), therefore E is materialized. Class J is also materialized because its distance to the root is 3. Once a pseudo class is materialized, its distance becomes 0.
When an equivalence class is generated or modified, we determine whether to materialize the equivalence class or make it a pseudo class. The details will be explained in
Section 4.
The addset data structure (including both materialized and pseudo nodes) seems to be promising to reduce the stor-age requirement for maintaining MEDIAN values. However, the computation of updated MEDIAN values for all equiv-alence classes is still expensive by itself. Moreover, extra processing is required to obtain the measure sets for pseudo equivalence classes. In this subsection, we propose a novel sliding window technique to compute updated MEDIAN val-ues efficiently.

One important observation contributing to the sliding win-dow technique is that given a set of n measures, the number of elements that are larger and smaller than the median of the n measures is the same. As a result, by keeping track of the k (1  X  k  X  n ) measure values around the median in a sliding window, we are able to ensure that the median will still lie within the sliding window even with k inser-tions. This forms the basis of our sliding window technique for maintenance of MEDIAN.

With the above observation, we will look at how the slid-ing window technique can be used to compute the median for a pseudo class using the addset and its nearest materi-alized ancestor. Given a materialized equivalence class C with a sorted measure set S = { x 0 ,x 1 ,...,x n  X  1 } ,theme-dian of one of its descendant pseudo class C d is able to be efficiently computed as follows: (1) Let x med represent the median of the n measures. We maintain a sliding window of size k to keep track of the middle k measures around x med in S .Notethat k must be greater than the distance value between C m and C d (the reason will be clear later). The sliding window is shown as the area between x low and x high in Figure 5, where x low x high are the lowest and the highest measures in the sliding window respectively. (2) We insert each measure of the addsets of the nodes between C m and C d into S .Asanewmeasure x from an addset is inserted into S , we adjust x med , x low and x cording to the following criteria (implemented in algorithm 3 in section 4.1): 1. x &lt; x low :inthiscase,x med needs to move 1/2 position 2. x &gt; x high :inthiscase,x med needs to move 1/2 position 3. x med &lt; x &lt; x high :inthiscase,x med needs to move 1/2 4. x low &lt; x &lt; x med : in this case, x med needs to move 1/2
By doing so, the median for a pseudo class can be effi-ciently computed only based on the sliding window of its nearest materialized ancestor so long as distance between them does not exceed the size of the sliding window. When the distance exceeds the size of the sliding window, extra
I/O is required to read more measures to compute new me-dian. In order to avoid the extra I/O, we require the distance threshold defined in section 3.1 to be equal to or smaller than the size of sliding window. When maintaining the quotient cube, we will materializes a pseudo class if its distance to its materialized ancestor exceeds the distance threshold.
Note that the size of the sliding window can be set flex-ibly by the user. For example, we might let the size of the sliding window to fit within a page so that I/O cost is min-imized. Alternatively, we can let the size of sliding window to be the sum of distance threshold and the batch size of insertion. In this case, the new aggregation value of both the materialized classes and pseudo classes can be computed using sliding windows without extra I/O cost. Interestingly, it can be shown that when the size of the sliding window is equal to 1, all equivalence classes in the family tree are ma-terialized classes, which is the implementation of the naive quotient cube maintenance we mentioned at the beginning of section 3. On the other hand, when the size of the slid-ing window is equal to or larger than the total number of tuples in the base relation, all classes in the family tree are pseudo classes. Although we can obtain the highest space reduction with such a setting, efficiency is affected as we need to sort the whole measure set of a class when comput-ing the median. The size of the sliding window can thus be seen as a parameter to balance the space-time tradeoff in the maintenance of a quotient cube for MEDIAN.
This section illustrates how to maintain the MEDIAN quotient cube incrementally using addset data structure and sliding window technique. Four components dist , msset , par-ent and chdlist are added to the structure of an equivalence class as defined in Section 2. An equivalence class C is now represented by the structure ( upp,m,dist,msset,parent,chdlist ) .
For a materialized class, dist =0and msset registers the actual measure set. For a pseudo class, dist refers to the distance to the nearest materialized ancestor and msset reg-isters the addset relative to its generator. When a new equivalence class is generated or when an existing equiva-lence class is modified, the values of components dist and msset are updated simultaneously. If the value of dist for a pseudo class is larger than the size of the sliding window, it is converted into a materialized class by backtracking to its nearest materialized ancestor to compute the complete measure set for the pseudo class. Parameters parent and chdlist register the parent-child relationship between a new class and its generator in a family tree.

In what follows, we first introduce the algorithm Inc Single, which updates a quotient cube for one new tuple. Based on
Inc Single , a more practical algorithm, Inc Batch, which up-dates a quotient cube in batches will be given.
In this section, we first look at algorithm Inc Single ,which applies the three propositions in section 2, addset structure and sliding window for updating a quotient cube.

Algorithm 1 shows the pseudo code for Inc Single . Having generated a virtual class VC for reason explained in Section 2, Inc Single divides VC andallclassesof X intobuckets
B [0] , ..., B [ n + 1] in line 2. A bucket B [ i ] contains all equiv-alent classes C , such that | C.upp | = i i.e. there are exactly i dimensions in C.upp which do not have  X  X ll X  as their values.
The only exception here is for VC whichisinthe( n +1) th bucket. We will henceforth refer to | C.upp | as the cardi-nality of C . A different set of buckets B [0] , ..., B [ n ]are initialized to store the updated and new equivalent classes for  X  (line 3).

The main loop (lines 4-17) iterates through the classes in each bucket in the order B [0],..., B [ n +1]. For each class C in a bucket B [ i ], Inc Single first tests for a value modified class (line 6) by checking whether C.upp is a subset of t.dvalues . Corresponding update is performed (line 7) for such a case.
For example, if tuple ( a 1 b 1 c 1 , 15) is added to Figure 3(b), all update is done in line 7 and split will not occur.
Otherwise, a test for a new class generator is done (line 9-12) by computing MaxMatch = C.upp  X  t.dvalue and testing for its existence in line 12. In between, dumb classes are filtered off if C does not intersect t (line 11). Having confirmed that C is a new class generator, C will be split based on Definition 8. The new classes, C n and updated generator C will be added into B [ k ]and B [ i ] respectively.
The algorithm ends when all equivalence classes in  X  are processed.

Note that checking the buckets in ascending cardinality order is important in verifying two conditions of Proposition 2. This order guarantees that the first encountered class, C which produces MaxMatch as the intersection of C f .upp and t must be the Greatest Lower Bound (GLB) for all sub-sequent classes, C s ,thatalsohave C s .upp  X  t = MaxMatch . Also, since MaxMatch is a subset of C.upp , k will be less than i . Thus bucket B [ k ] is already updated before classes in B [ i ] are processed, making it possible to check for the sec-ond condition of Proposition 2 by verifying that MaxMatch is not already in bucket B [ k ].

Now we will explain how procedure ModifyClass (Algo-rithm 2) works. If a class satisfies the proposition 1 de-scribed in section 2, procedure ModifyClass is called. In case that the class is a materialized class, its measure set should be modified (line 1). If it is a pseudo equivalence class, the updating is a bit complicated. First, not all the pseudo classes that satisfy Proposition 1 need to be modi-fied. For example, when a new tuple t 5 =(5, a 4 b 1 c 1 , 12) is added to Figure 3(b), both equivalence classes C 6 and C 1 satisfy Proposition 1. We only need to modify the addset of C 6 while the addset of C 1 needs not to be modified since the new measure can be obtained from the addset of its parent (i.e. C 6 ). Second, the parameter dist must be updated for all pseudo equivalence classes. For example in Figure 4,if pseudo class B is modified, the parameter dist of class F must also be updated.

The new median values of all new and modified classes must be computed after the measure sets and addsets are updated. Algorithm 3 computes the median value for the updated equivalence classes in a depth-first order. Note that the depth-first order is extremely important for the sliding window technique to be efficiently adopted. Variable LRDiff registers the distance that the window should be slided to the left or right. For a materialized class, line 3 sorts all measures and selects the middle measure as the median. Lines 4-5 initialize LRDiff to 0 and place middle k measures into the sliding window, which makes preparation for later computation of its pseudo descendants. For a pseudo class, it only needs to compare and slide the window (line 7-11). Since the number of the measures in addsets cannot exceed the size of the sliding window k , this method needs at most k comparisons and thus is very efficient. After outputting the information of the current class, the algorithm is recursively called for each of its child (line 15).
We next introduce Algorithm Inc Batch for batch updat-ing of a MEDIAN quotient cube. Inc Batch is inspired by the BUC algorithm proposed by Bayer and Ramakrishnan [3] which recursively partitions tuples in a depth-first man-ner so that tuples involved in computing the same cell are grouped together at the time of computation for the cell X  X  value. The partitioning is performed on different dimensions at each level of the recursion so that different groupings can be formed.
 The novelty of Algorithm Inc Batch over BUC is that
Inc Batch is a maintenance algorithm which performs par-titioning on both the existing classes in  X  (represented by their upper bounds) and the new set of tuples. We will refer to a partition of the new tuples as a tuple partition and a partition of equivalence classes as class partition .
To ensure the effectiveness of Inc Batch , we  X  X ynchro-nize X  the tuple and class partitioning in such a way that a particular tuple partition that is being processed at one time is guaranteed to affect only the corresponding class partition that is being processed at the same time. This enhances ef-ficiency in two ways. First, by grouping tuples that share similar dimensional values together, the search for affected equivalence classes needs only to be done once. Second, as the partitioning of equivalence classes done in synchroniza-tion with the tuple partitioning, the number of equivalence classes that are being checked is substantially reduced. This  X  X ynchronization X  is performed in a function of Inc Batch called Enumerate() .

We now explain Inc Batch in details. The pseudo code of Inc Batch is shown in Algorithm 4. The main algorithm simply calls the Enumerate () function by providing the set of new tuples R , the original set of equivalent classes  X , the number of dimensions in the cube and the size of the sliding window. The function Enumerate () will then perform recur-sive partitioning of both the tuples and equivalent classes and update the changes that will be made to various classes in  X . The main algorithm will then output these changes which will produce value modified classes and new classes.
We next look at the function Enumerate (). Given the in-put tuple and class partition, input t and input c , Enumer-ate () iterates through all the remaining dimensions (from dim onwards) and partitions both input t and input c based on the dimensional values of each individual dimension D (line 3 and 4). The inner loop from line 5 to 11 will then go through each individual dimensional value of D and recur-sively call Enumerate() to perform further partitioning on the corresponding partitions of the dimensional value.
Finally, we look at procedure CheckandUpdate in the first line of function Enumerate() . Given the input cell, the tuple partition input t and the cell partition input c , CheckandUp-date  X  X  task is to determine how input t will affect the equiv-alence classes in input c . The approach in this procedure is similar to Inc Single except for some changes due to the batch processing. One main difference is that the cell from the input is used as a representative to compare against the equivalent classes in input c .
 Algorithm 5 lists the pseudo code for procedure Checkand-
Update . The tuple-class comparison is again made in in-creasing order of cardinality for the equivalent classes. Lines 5 and 6 in the procedure will call procedure ModifyClass to update C.msset or to materialize C if it detects that a class
C is a value modified class. If C.upp  X  cell , we compute uppcell by appending all dimensional values that have 100% occurrence in input t to the cell .If uppcell equals C.upp ,
C.upp will be updated from input t in future recursion and no action needs to be taken . However, if uppcell = C.upp , we will create a temporary class C t at Line 13. If the class
C t is not in C.tempset , which contains all new classes that are generated from C and will be output later in the main algorithm of Inc Batch ,weadditto C.tempset and modify its msset and dist . If there already exists a temporary class
C t such that C t .upp = C t .upp , C t is simply discarded since they are in fact the same class.
To evaluate the efficiency and effectiveness of our update techniques, extensive experiments are conducted. In this section, we report only part of our results due to space lim-itation. All our experiments are conducted on a PC with an Intel Pentium IV 1.6GHz CPU and 256M main memory, running Microsoft Windows XP. Experiment results are re-ported on both synthetic and real life datasets.

All run time reported here includes I/O time. We com-pare our update algorithms with a re-run of the depth-first search algorithm in [10] when an update is made to the orig-inal base table. Although we realize that it is not viable to re-generate quotient cube every time the base table is up-dated, there is no other reasonable benchmark for compar-ison. Our experiments show that single tuple maintenance algorithm Inc Single can be up to a hundred time slower than batch maintenance, as such we will only report results for batch maintenance algorithm Inc Batch which is fed with the update tuples and existing quotient cube. Note that the order of update tuples does not have any effect on the performance of Inc Batch while it may affect the performance of algorithm Inc Single .
We randomly generated two synthetic datasets with uni-form distribution. Both datasets contains 1 million tuples and each tuple has 9 dimensions. Cardinality C is set at 100 for all 9 dimensions of one dataset and 1000 for the other dataset. Measure for the tuples are randomly gener-ated within the range of 1 to 1000. By default, we set the size of sliding window as 1000, the number of tuples as 200k, the dimensionality of each tuple as 6, the cardinality of each dimension as 100, and the update ration as 50%. An update ratio of k % implies |  X  X  | =( k %)* | R | tuples are added to the base datasets.

Efficiency: We vary the update ratio from 5% to 50%.Fig-ure 6(a) shows the run time of both Inc Batch (represented with Inc Med B) and the depth-first algorithms (represented with Dfs Med) on dataset with cardinality C =100. Fig-ure 6(a) shows that Inc Batch achieves substantial saving in time than a re-run of the depth-first algorithm. For a update ratio of 50%, we enjoy a 75% saving in processing time. The results clearly indicate that our maintenance al-gorithm for aggregate function MEDIAN is efficient. The savings in time mainly come from the fact that Inc Batch reuse previous computation.

Figure 6(b) shows the run time of both algorithms when the dimensionality is increased from 2 to 9. The performance gap between the batch maintenance algorithm Inc Batch (represented with Inc Med B) and the depth-first algorithm (represented with Dfs Med) grows with the dimensionality of the dataset.

Data Skew: To study the effects of data skew, we vary the distribution of the dimension values in each dimension by changing the zipf factor from 0.0 to 3.0. A zipf factor of 0 means that the dimensional values are uniformly distributed while a high zipf factor will generate a highly skewed dataset.
Figure 7 shows the run time of both algorithms as the zipf factor is varied. As the zipf factor increases, the run time of both algorithms decreases. This is because as the zipf factor increases tuples in the dataset are highly similar to each other and the number of equivalence classes will de-crease, thus requiring less time for both maintenance and re-computation of the quotient cube.
 Scalability: We next look at the run time of algorithm
Inc Batch as the number of tuples increases. We increase the number of tuples from 100k to 1 million. Figure 8 shows that although both algorithms have linear scalability, the run time of the incremental maintenance algorithm scales better than a complete re-computation.

Effectiveness of Addset: To study the effect of addset in reducing the storage requirement for maintaining the ag-gregate function MEDIAN, we vary the size of the sliding window from 1 to 200k on both two datasets , which means that the distance threshold also changes from 1 to 200k. The measure set and addset are stored in binary files and thus we use the size of the binary files as a measure for space requirement. Figure 9(a) shows the space requirement for maintaining MEDIAN using Inc Batch . Table 1 gives more detailed data. As shown in Table 1, when the size of the win-dow is set to 100, the addset only needs 10% of the space compared to the full measure set (when window size equals 1). We observe two tendencies:
First, as the size of sliding window increases, the space requirement decreases sharply and then levels off. Second, the reduction ratio decreases as the cardinality increases. In other words, the lower the cardinality, the more effective the addset data representation. This is due to the fact that low cardinality dataset are denser which result in more redun-dancies if the full measure sets are stored.
 Effectiveness of sliding window: Figure 9(b) shows the run time of algorithm Inc Batch with respect to varying sliding window sizes. We can see that when the size of the window increases from 1 to 1000, the run time of Inc Batch decreases. However when the window size continues to in-crease, the run time begins to increase a bit. This is because too small a window size will result in many materialized classes that require sorting computation. Too big a window size will lead to more backtracking when computing the me-dian for pseudo equivalence classes.
We also evaluate our update techniques on a real life weather dataset 3 which is commonly used in experiments in-volving computation of data cubes [18, 17, 11]. The dataset contains 1,015,367 tuples and the cardinalities of the di-mensions are as follows: station-id (7037), longitude (352), solar-altitude (179), latitude (152), present-weather (101), day (30), weather-change-code (10), hour (8), and bright-ness (2). We use the first 100k tuples to form the base relation.

Figure 10 shows the maintenance efficiency of both algo-rithms. As expected, Inc Batch (represented with Inc Med B) has the modest run time growth as the update ratio in-creases. The performance trends revealed by Figure 10 is remarkably similar to those revealed by Figure 6.
We test the effectiveness of addset, and again obtain a sharp decrease in space requirement when the size of sliding window increases. The graph in Figure 11(a) shows that substantial space reduction is obtained even with a sliding window size of 100. Figure 11(b) shows the run time of Inc Batch with respect to varying window sizes. The re-sult of the algorithm is consistent with the observations we obtain for the synthetic datasets.

In summary, our experiments show that Inc Batch is a highly efficient algorithm and achieves a substantially bet-ter run time reduction than deep-first algorithm. They also show the effectiveness of the addset and sliding window tech-niques. http://cdiac.esd.ornl.gov/cdiac/ndps/ndp026b.html
Plenty of efforts have been devoted to fast computation of the cube [1, 19]. Since the complete cube consists of 2 n cuboids (n is the number of dimensions), the size of the union of 2 n cuboids is often too large to be stored due to the space constraints. Thus it is unrealistic to compute the full cube from scratch. There are currently many solutions to the problem, such as choosing views to materialize [8], cube compression [15], approximation [2], handling sparsity [13], and computing the cube under user-specified constraints [3].
Recently, from a different aspect, Wang et al. proposed a concept of condensed cube [18] that explores  X  X ingle base tuple X  and  X  X rojected single tuple X  to compress a data cube.
Lakshmanan et al. proposed a concept of quotient cube [10] that extracts succinct summaries of a data cube based on partition theory. Dwarf [17] identifies prefix and suffix struc-ture redundancies and factors them out by coalescing their storage. All three methods reduced the data cube (hence its computation time and storage overhead) efficiently.
However, as changes are made to the data at the sources, the maintenance work to these compressed data cube is non-trivial. The incremental maintenance of quotient cube is the most challenging since it not only has the largest data compress ratio but also preserves a semantic structure. [11] proposed a efficient data structure called QC-tree. While the important incremental maintenance problem is tackled in the paper, aggregation was considered only in a limited sense. For example, aggregation with holistic aggregation function was not allowed. In this paper, we introduced two techniques called addset data structure and sliding win-dow to maintain holistic function like MEDIAN. The con-cept of a sliding window is also used in both [20] and top-k X  view in [21] but no in the context of a QC-tree.

Works on data warehouse maintenance such as [12, 9, 14] are of clear relevance to us. However, none of them ad-dresses the MEDIAN maintenance problem. Our study is also closely related to incremental concept formation algo-rithms based on Galois lattice [6, 4, 5].
In this paper, we address the problem of updating the ex-isting MEDIAN quotient cube incrementally. We developed a new data structure addset which is able to dramatically cut down the size of storage space required to store measure set for each equivalence class. Moreover, we proposed a sliding window technique to compute the median over not the entire past history of the data, but rather only the sliding windows of middle data from the history. We designed two incremen-tal maintenance algorithms: Inc Single and Inc Batch .The former maintains the quotient cube tuple by tuple and the latter maintains the quotient cube in batch. A comprehen-sive set of experiments on both synthetic and real data sets were conducted. Our results show that our maintenance algorithms are efficient in space and time.
Figure 8: Scalability with the number of tuples
Figure 10: Maintenance Efficiency for Real Data
