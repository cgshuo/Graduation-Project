 We give a fresh look into score normalization for merging result-lists, isolating the problem from other components. We focus on three of the simplest, practical, and widely-used linear methods which do not require any training data, i.e. MinMax, Sum, and Z-Score. We provide theoretical ar-guments on why and when the methods work, and eval-uate them experimentally. We find that MinMax is the most robust under many circumstances, and that Sum is X  in contrast to previous literature X  X he worst. Based on the insights gained, we propose another three simple methods which work as good or better than the baselines.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Score Normalization, Distributed Retrieval
Merging ranked-lists produced by several engines requires two steps: score normalization and combination. In dis-tributed setups with disjoint collections, the combination step becomes trivial since each document is assigned a single score from one of the participating engines. This makes such setups ideal in isolating the normalization problem. Dis-tributed setups usually include another step before normal-ization, i.e. resource selection (RS). While useful in improv-ing efficiency, RS has been also shown to improve effective-ness significantly. We argue that this is due to the far-from-perfect quality of state-of-the-art normalization methods: in an ideal normalization, e.g. scores are normalized to prob-abilities of relevance, any kind of RS will hurt effectiveness by excluding sources with relevant documents; in such an ideal situation, the more systems one combines, the better the effectiveness. Consequently, the theoretical ceiling of ef-fectiveness can only be achieved with an ideal normalization without RS, and distributed setups with disjoint collections are best for experimenting with normalization [1].
Previously proposed normalizations vary from linear to non-linear functions which may require or not training data for their estimation. While there is a rich literature on the subject, most experiments reported do not isolate the prob-lem as we described above [3, 4, 5]. We give a fresh look into the simplest normalization methods: linear functions which do not require any training or search engine cooper-ation. Being the most practical, they are also the fastest and generally have been proved to be robust and effective. Beyond an empirical comparison, we also give theoretical justifications on their implicit assumptions. To our knowl-edge, these assumptions have never been made explicit or they are  X  X ost X  in the large volume of the related literature. Based on the gained insights, we propose three alternative linear normalization methods.

In the experiments we use the gov2.1000 and gov2.30 splits of the TREC GOV2 dataset [2]. In gov2.1000 the largest 1000 hosts of the GOV2 dataset are treated as 1000 sources, and the number of sources that contain relevant documents is usually much less than 1000. In gov2.30 the collections of gov2.1000 are clustered into 30 sources; here, relevant doc-uments appear in most of the sources. We use ten retrieval functions implemented by the Terrier toolkit 1 , namely, BM25, tf-idf (Terrier and Lemur versions), language modeling (orig-inal and with Dirichlet smoothing), and a number of DFR-based functions (BM25, BB2, IFB2, InL2 and PL2). Re-trieval functions are randomly assigned to sources. Topics 701-850 are used as queries. Operationally, engines return truncated rankings for efficiency reasons, thus, we only con-sider either the top 10 or 1000 results. Our results are sum-marized in Tables 1 and 2, which we will refer to throughout the paper. Statistical significance is measured with a paired t-test.  X  shows the significantly best baseline (MinMax, Sum or Z-Score) and  X  marks the modification that is significantly better than any baseline, both at the 0.05 level.
The MinMax method [3] scales the output score range per engine to [0 , 1]. We argue that the most important as-sumption behind MinMax is that each source contains at least 1 relevant document, and that this document will most-likely get ranked 1st. By assigning the same highest score to all 1st documents, they are ranked before any other in the merged list. Since it is also assumed that these documents are most-likely relevant, a high early precision is achieved which pushes higher other evaluation measures sensitive to early rank positions, e.g. MAP. Thus, MinMax owes its suc-cess to getting right the early ranks in the merged list. How-ever, due to fact that the 1st document of each engine is assigned the same highest score, MinMax produces a round-robin effect in merging which ma y impact effectiveness neg-http://terrier.org atively as the number of engines increases. Indeed, MinMax shows high performance when run on 30 sources that contain sufficient relevant documents, i.e. the above assumption is satisfied. On the contrary, when run on 1000 sources, where the assumption is weakened by the sparseness of relevant documents and additionally the round-robin effect is more prominent, MinMax performance degrades considerably.
The Sum method [5] is similar to MinMax, but with-out using a fixed highest score eliminating the undesirable round-robin effect: the minimum score is shifted to 0 while the sum of all scores per ranked-list is scaled to 1: s = s  X  min, s norm = s / i s i . The intuition given in [4] is: un-der the assumption of exponentially-distributed scores, the normalization is equivalent to setting the means of score distributions of sources to be equal to each other. However, many studies on modeling scores show that the aforemen-tioned assumption of exponentially-distributed scores does not hold in practice, especially for top-ranked documents. Our experimental results show that the Sum method almost always has the worst performance. This contradicts the re-sults of the original work [5], however, the authors in the last-mentioned study used a meta-search setup and evalu-ated normalization and combination methods together X  X  setup that does not isolate the normalization problem.
In [5] also the Z-Score method is proposed, which normal-izes each score to its number of standard deviations that it is away from the mean score. However, as previously noted in [1], this method rather assumes a bell-like distribution of document scores (e.g. a Normal), where the mean would be a meaningful  X  X eutral X  score. However, in practice score distri-butions are highly skewed and clearly violate this assump-tion. Still if only the top documents are considered from each result list, they are likely to be relevant, and therefore the distribution of their scores may be close to that of rel-evant documents, i.e. likely a Normal. Indeed, our results show that when only the top-10 documents are retrieved from each source, Z-Score usually shows higher performance than when applied on the top-1000 documents.

So far we have seen that methods that assume partic-ular score distribution shapes, such as Sum and Z-Score, are worse, and that MinMax which does not make such as-sumptions is better. In other words, the sum or the mean of scores do not seem to be a good statistics for normal-ization, and that we should be looking into the direction of MinMax for developing a better normalization. We also know that using a fixed highest score should be preferably avoided so as to eliminate the round-robin effect, as well as, to be able to down-weigh sources of no relevance (haunting the main assumption of MinMax). Another problem rarely mentioned that all three methods above have is that they are greatly affected by the minimum score seen, or else, the chosen truncation point of the rankings (e.g. the minimum score of top-10 documents is usually very different from the minimum score of top-1000).
 We will first try to deal with the minimum-score problem. The lowest theoretical score of many scoring functions is 0. Thus, making the assumption that the most non-relevant documents usually score at 0, the minimum in MinMax func-tion should also be set to 0. This way we obtain the Max method: s norm = s/max . Our results show that Max, al-though simpler, almost always has the same or higher per-formance than that of the MinMax method.

Let us now try to deal with non-relevant sources which vio-late the main assumption of MinMax. Scoring functions aim at assigning scores in a way that relevant documents have very different scores from non-relevant documents. There-fore, if the standard deviation  X  of scores in a ranked-list is high, the list is likely to contain both relevant and non-relevant documents. If  X  is low, the list is likely to contain either only relevant or only non-relevant documents. On one hand, long ranked-lists (e.g. 1000 documents) are likely to contain many non-relevant documents and, therefore, we prefer those that have high standard deviation (i.e. they also contain many relevant documents). In this case, the follow-We call it MM-Stdv. On the other hand, short ranked lists (e.g. 10 documents) may contain only relevant documents and, therefore, we might prefer the lists with low standard deviation. Unit-variance linear modification (UV) is similar to Z-Score and scale-invariant but does not shift the mean to 0: s norm = score/ X  . Our results support both formulas depending on the situation. We isolated the normalization problem and found that MinMax is the most robust method under many circum-stances, Z-Score and Sum may perform well when some con-ditions are met, and Sum is worse than previous literature suggested. Furthermore, we gave theoretical insights on why and when these methods work or fail, and proposed three new methods that work as good or better than the baselines. [1] A. Arampatzis and J. Kamps. A signal-to-noise [2] J. Arguello, J. Callan, and F. Diaz. Classification-based [3] J. H. Lee. Analyses of multiple evidence combination. [4] R. Manmatha and H. Sever. A formal approach to [5] M. Montague and J. A. Aslam. Relevance score
