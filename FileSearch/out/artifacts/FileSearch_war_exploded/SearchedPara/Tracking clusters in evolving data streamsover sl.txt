 REGULAR PAPER Aoying Zhou  X  Feng Cao  X  Weining Qian  X  Cheqing Jin Abstract Mining data streams poses great challenges due to the limited mem-ory availability and real-time query response requirement. Clustering an evolving data stream is especially interesting because it captures not only the changing dis-tribution of clusters but also the evolving behaviors of individual clusters. In this paper, we present a novel method for tracking the evolution of clusters over sliding windows. In our SWClustering algorithm, we combine the exponential histogram with the temporal cluster features, propose a novel data structure, the Exponen-tial Histogram of Cluster Features (EHCF). The exponential histogram is used to handle the in-cluster evolution, and the temporal cluster features represent the change of the cluster distribution. Our approach has several advantages over exist-ing methods: (1) the quality of the clusters is improved because the EHCF captures the distribution of recent records precisely; (2) compared with previous methods, the mechanism employed to adaptively maintain the in-cluster synopsis can track the cluster evolution better, while consuming much less memory; (3) the EHCF provides a flexible framework for analyzing the cluster evolution and tracking a specific cluster efficiently without interfering with other clusters, thus reducing the consumption of computing resources for data stream clustering. Both the the-oretical analysis and extensive experiments show the effectiveness and efficiency of the proposed method.
 Keywords Cluster tracking  X  Evolving  X  Data streams  X  Sliding windows 1 Introduction Recently, substantial amount of researches are devoted to mining continuous data toring, credit card fraud detection, and sensor network data processing, etc. As tures among data records. In data stream scenario, the clustering techniques are applied to group multidimensional records such that the intracluster similarity is maximized, and the intercluster similarity is minimized. Due to the restriction of the computing resources, clustering data stream has to be performed online with limited memory requirement and one-pass data scan.
 cause of its wide and potential applications. Generally speaking, clustering over an entire data stream is dominated by the outdated historic information of the stream. The resulting clusters are of less use from an application point of view. Therefore, clustering evolving data streams over sliding windows becomes a natural choice, since the most recent N records are considered to be more critical and preferable in many applications [ 5, 12 , 17]. However, since data stream clustering is applied with limited memory availability (relative to the size of the sliding window), it is impossible to load the entire data set into memory. Thus, approaches to approxi-mate the clustering results are necessary. Fortunately, a small deviation from the exact value in a sliding window is acceptable in most practical applications. For example, in network traffic monitoring the administrator may often query:  X  X hat is the distribution of the most recent 10,000,000 network connections with the tolerance of 10,000 connections? X  a global uniform strategy to maintain the synopses. However, it is preferable to adaptively maintain separate information for each cluster. Since every cluster has its own evolving behavior, the uniform strategy may lose a lot of valuable infor-mation. Characteristics of a cluster (e.g., the number of objects, the center and radius of the cluster) often change as data streams proceed. Within a certain pe-riod of time, the evolving behaviors of different clusters are usually different. For 3, 2, and 1, respectively. At time t 3 , the numbers of newly arrived records in those clusters change to 1, 2, and 3, respectively. Conceivably, the evolving behavior of each cluster may also change significantly as new records arrive continuously. to track the clusters in evolving data streams over sliding windows. Sliding win-dows conveniently eliminate the outdated records. Previously proposed clustering algorithms cannot be easily extended to this scenario. For example, a direct exten-sion of CluStream [ 1] to sliding windows requires that the intermediate clustering results (i.e., current set of microclusters, called a snapshot in CluStream) be main-tained simultaneously whenever a new record arrives. Such an expensive operation will result in large overhead for online processing even if all the snapshots can be maintained in memory.
 precise distribution of recent records. The old records have great influence on the formation of the microcluster, which is illustrated in Fig. 2a. In CluStream, old records arriving before t 3 are not eliminated. To answer a clustering request for microcluster at t 4 from that at t 3 , resulting in only one microcluster for recent records, as shown in Fig. 2b.
 ways maintains the microcluster with growing radius, instead of splitting it into multiple microclusters. Since it introduces more memory consumption, splitting operation is not implemented in CluStream. CluStream creates a new microcluster if and only if arriving records cannot be incorporated into existing microclusters. When a record is within the maximum boundary of a microcluster (e.g., a factor of the Root Mean Square (RMS) deviation of the records from the microcluster centroid), it will be absorbed by the microcluster, which may result in a larger boundary of the microcluster. A microcluster with large boundary will absorb more records, which leads to further increase on the boundary of microclusters, as shown in Fig. 2b.
 The formation of the two microclusters is shown in Fig. 2c. Taking the history into account, it is natural to put the newly arriving records into one microcluster asshowninFig. 2a. However, the final resulting clusters with two microclusters in Fig. 2d better capture the distribution of recent records. Our experimental results (refer to Figs. 7 and 9) support this claim.
 tinuously. It is very important to provide an efficient mechanism to eliminate the effect of the expired records and incorporate new records into the synopses. How-ever, this is not a trivial task. Once two synopses have been merged, there is no way to split them, but the splitting is required when old records expire from the sliding window.
  X  We propose a novel synopsis called Exponential Histogram of Cluster Fea- X  Based on EHCFs, we present a new clustering algorithm, SWClustering, for  X  The memory consumption of SWClustering is bounded. The optimization work. Section 3 presents the synopsis EHCF along with the technique for merging two EHCFs. SWClustering algorithm is described in Section 4. The experiment setting and the analysis of experimental results are presented in Section 5. Section 6 concludes this paper. 2 Related work Previous algorithms on clustering data streams can be classified into two cate-gories: one-pass approach and evolving approach. 2.1 One-pass approach The one-pass approach clusters data stream by scanning the data stream only once. Traditional clustering methods such as the k -means were extended to their data stream versions under the assumption that the data objects arrived in chunks [ 11, 23 , 24].
 O ( kN ) time and O ( N ) space, where k is the number of centers, N is the is O ( kN ) for any k -median algorithm achieving constant factor approximation. M / k is the sample size determined by the available memory size, then a local search algorithm is used to cluster O ( M ) medians of level i to 2 k medians of level i + 1. This process is repeated until the O ( k ) medians is clustered into k groups.
 quires O ( kpoly log n ) space. They theoretically addressed the problem of in-creasing approximation factors in algorithm [ 23] where the increase in the number of levels leads to the increasing approximation factors in the final results.
 STREAM first determines the size of sample. If the size of data chunk exceeds the sample size, a LOCALSEARCH procedure is invoked to obtain the clusters of the chunk. Finally, the LOCALSEARCH is applied to all the cluster centers generated in the previous iterations.
 algorithm. It is guaranteed that the model produced does not differ significantly from the one that would be obtained with infinite data. The Hoeffding bound is adopted to determine the number of examples needed in each step of k -means algorithm.
 variant of the k -means algorithm, incremental k -means, was proposed to ob-tain high quality solutions. Incremental k -means requires O ( kNT ) ,where T number of clusters. Sparse matrix operations and simple sufficient statistics tering the categorical data streams based on the squeeze algorithm [ 26]. A Lossy Counting [ 33] procedure is adopted to maintain the histogram. Aggar-methodology.
 streams clustering algorithms come up with meaningless results in subsequence clustering, so a solution using k -motif to choose the subsequences was proposed in their paper. Rodrigues et al. [ 38] proposed a time-series clustering system which incrementally constructed a hierarchy of clusters. The correlation between time-series is used as similarity measure. Cluster splitting or aggregation is carried out at each time step. 2.2 Evolving approach The evolving approach [ 1, 16] views the behavior of streams as an evolving pro-cess over time. For this approach, there are three kinds of window models [ 42], i.e., landmark window, sliding window, and fading window. Among them, the sliding window model is widely adopted in stream mining [ 5, 12, 13, 17, 32].
 shape clusters in an evolving data stream with noise. A core-microcluster struc-ture is introduced to summarize the clusters with arbitrary shape, while the potential core-microcluster and outlier microcluster structures are proposed to of the microclusters is guaranteed with limited memory. TECNO-STREAMS, an artificial immune system (AIS) based clustering approach, was proposed by Nasraoui [ 35]. The system is based on a dynamic weighted B-cell model to data [ 36]. Different from these approaches aiming at discovering arbitrary-shape clusters, our method focuses on the clustering problem over sliding windows.
 HPStream. The main contributions are the fading cluster structure and the method-ology of projection-based clustering. HPStream is effective in finding clustering in subspace.
 windows based on the previous work [ 23]. They focused on the theoretical bound of the performance. In contrast, our work is devoted to the problem of analyzing the evolution of clusters in sliding windows.
 which is designed to cluster data streams over different time horizons in an evolving environment. Since the snapshots of clustering results over the landmark window have to be stored, CluStream consumes more space, thus not suitable for the clustering problem over sliding windows. In addition, the influence of expired records cannot be promptly eliminated during on-clustering.
 tering multiple data streams based on scalable online transformation of the raw data. The transformation allows a fast computation of approximate dis-tances between streams. COD, a general framework of clustering multiple data streams, was introduced by Dai et al. [ 15]. COD dynamically clusters multi-problem of clustering data streams with increasing dimensionality over time is addressed in Yang [ 39], where a weighted distance metric between two streams is used, and an incremental algorithm is proposed to produce stream clusters. 3 Exponential histogram of cluster feature 3.1 EHCF synopsis Assume that a data stream consists of a set of multidimensional records x ,..., x window model, only the most recent N records are considered at any time. The most recent N records are called active records, and the rest are called expired records which no longer contribute to the clustering.
 a novel synopsis data structure, called Exponential Histogram of Cluster Feature (EHCF). Every bucket in an EHCF is a Temporal Cluster Feature (TCF) for a set of records.
 Definition 1 (Temporal Cluster Feature (TCF)) A Temporal Cluster Feature (TCF) for a set of d -dimensional records x 1 ... x n with time stamps t 1 ... t n is defined as a ( 2  X  d + 2 ) -dimension vector ( of records; is the linear sum of the n records, i.e., of the most recent record, i.e., t = t i .
 et al. [ 40]. Aggarwal et al. [ 1] extends the cluster feature vector by summing up the time stamps in CluStream algorithm. However, with the time stamp of the most recent record in a TCF, it is possible to determine the time span of records rather than approximating it as CluStream does.
 V ( C ) = 2 l . TCFs can be formed in an additive way, as shown later.
 Property 1 C 1 and C 2 are two sets of records.
 of the corresponding fields in t of Definition 2 (Exponential Histogram of Cluster Feature (EHCF)) Given a user-defined parameter (0 &lt;&lt; 1), an EHCF is defined as a collection of TCFs on a set of records C i with the following constrains. (1) All records in C i record, while any other set C i ( i &gt; 1) contains either the same or as twice much (3) 1 or 1 + 1 l -level TCFs can be found for each l (0  X  l &lt; L ) except for the L -level, where L is the highest level of TCFs in the EHCF. where n is number of records in the EHCF. This is because only the last TCF in the EHCF may contain the expired records, it contains at most n records. 1 [17 , 22 ]. Datar et al. [ 17 ] proposed an EH-based method to maintain aggregates over sliding windows. In our EHCF structure, we adopt the Exponential Histogram technique to maintain the TCFs.
 Definition 3 (Center of EHCF) The center c of an EHCF H with m TCFs is defined as the mean of the TCF i = ( The center of an EHCF is used to determine the distance between the EHCF and a newly arrived record. 3.2 Updating an EHCF When a record x p arrives, an EHCF synopsis can be updated incrementally as follows: 1. Generate 2. Append 3. Check the field t of the last TCF in the EHCF. If the time stamp t is not one of Example 1 (Merging Process of an EHCF) Figure 3 illustrates the merging pro-1 ,..., 10. The user-defined parameter is set to be 0.5. In real applications, the should be far less than 1. At time stamp 4, a new TCF for record x 4 is gener-ated, which results in 4 ( = 1 + 2) 0-level TCFs in the EHCF. A new synopsis  X  X  X  X  TCF ( { x 1 , x 2 } ) is generated by merging operations occur at time stamps 6 and 8. At time stamp 10, the arrival of record x 10 triggers the merge of of  X  X  X  X 
TCF ( { x 1 , x 2 } ) and ory consumption ( 1 + 1 )( log ( n + 1 ) + 1 ) . Following theorems give the memory usage of EHCF synopsis. Theorem 1 (Datar et al. [ 17]) Exponential Histogram (EH) Structure computes an -deficient synopsis using at most ( 1 + 1 )( log ( N + 1 ) + 1 ) buckets, where N denotes the size of the sliding window.
 Theorem 2 Given an EHCF synopsis of n records, there are at most ( 1 + 1 ) ( log ( n + 1 ) + 1 ) TCF synopses in the EHCF.
 Proof Given any EHCF of n records and a user-defined parameter , we can con-struct an EH structure with window size n and parameter . As each TCF in the EHCF can be mapped to a bucket in the EH structure and vice versa, from Theorem 1,thereareatmost ( 1 + 1 )( log ( n + 1 ) + 1 ) TCF synopses in the EHCF.
 By observing the process of updating EHCF, we obtain the following theorem: Theorem 3 Assume that an EHCF synopsis is constructed using n records. For any TCF F i in the EHCF, F i can be constructed from its following 2 l records if there are ( 1 ( 2 l  X  1 ) records before F i .
 3.3 Merge two EHCFs Merging two EHCFs is essential to bound the space complexity while keeping the correctness of newly generated EHCFs. Assuming that H 1 and H 2 denote two nearest EHCF synopses containing m 1 and m 2 records, respectively, two cases need to be considered when merging H 1 and H 2 .
 in H 1 arrive before all records in H 2 . Merging H 1 and H 2 in this case can be done by simply placing the TCFs in H 1 followed by placing the TCFs in H 2 ,then sweeping from right to left to merge the TCFs.
 Theorem 4 Assume that H 1 and H 2 are two nonoverlapping EHCF synopses containing m 1 and m 2 records, respectively. The time complexity of merging H 1 and H 2 is O O ( 1 log (( m the records in H 1 and H 2 are interleaved. Here H 1 and H 2 cannot be merged in a straightforward way. Instead, a new EHCF H new is created where two pointers are maintained to point to H 1 and H 2 . Because no record is absorbed at the very beginning of the merging, H new does not contain any TCFs initially. The center of H new is initialized to be the mean of the and H 2 being temporarily kept, while in the first case, a new EHCF is created to replace H 1 and H 2 directly .
 to keep the EHCF structures of merging-EHCFs, we create new TCFs (called void-TCF s) conceptually in merging-EHCFs. In fact, the void-TCF s does not need to be created. Only a counter is maintained instead.
 EHCFs. Each entry in the tree is an EHCF synopsis. The records in a parent entry arrive after all the records in child entries. For example, the records in H new arrive after all the records in H 1 and H 2 .
 merge with an overlapping EHCF H 3 . This merging operation consists of two steps: (1) H new is merged into H 1 (or H 2 ) directly, because H new does not overlap with H 1 or H 2 . After this step, there is no TCF in H new .(2) H new is merged with H 3 ,andanewEHCF H new is created.
 is as follows: 1. The root creates a new TCF and maintains its own structure, as discussed in 2. One of the child nodes of the root creates a 0-level void-TCF and maintains its 3. Assume V H l and V H r denote the number of records contained in the left and EHCF has absorbed 2 ( 1 )( 2 l  X  1 ) records, according to Theorem 3, all the TCFs in their two child merging-EHCF s can be merged into new TCFs with level  X  l . Thus, the number of TCFs in merging-EHCF s is gradually reduced. Furthermore, TCF and appended to H new . Since the real merging operation is performed after the very beginning of merging, such process is called late merging . The following example demonstrates the late merging . Example 2 (Late Merging of EHCFs) Assume that two overlapping EHCFs ( H 1 and H 2 ) are to be merged. V H 1 = 16, V H 2 = 8, and H new is the newly created EHCF. The late merging of H 1 and H 2 is illustrated in Fig. 5. The shadowed boxes in Fig. 5 represent the void-TCF s. The number in the box indicates the number of records contained in the TCF. For the sake of simplicity, we assume = 1 here. ( can be set to a positive value far less than 1 in a real application.) So, there are at least 1 = 1andatmost 1 + 1 = 2 i -level TCFs for each i .New of merging. We do not consider the expiring process here.
 contains 4 TCFs. A new EHCF H new is initialized to contain 0 TCF. In Step(2), x 1 and x 2 have been absorbed by H new ,sotherearetwo0-levelTCFsin H new , while one 0-level void-TCF is created in H 1 and H 2 , respectively. Because the new 1-level TCFs, and the numbers of TCFs in H 1 and H 2 are reduced to 4 and 3, respectively. Step(3) illustrates the state after x 6 arrives. Because there are 2 ( 1 )( 2 2  X  1 ) = 6 records in H level TCFs. Step(4) shows the state when H 2 becomes a 3-level TCF, since there contracts to one TCF. Step(6) represents the final merging. When x 31 arrives, there one new 5-level TCF which is then appended to H new . Notice that the number of records incorporated in the last TCF of H new is 24, slightly smaller than 32. But it has no impact on the whole EHCF structure. 3.4 Comparing EHCF with pyramidal time frame The EHCF is originally designed for sliding window applications, while the pyra-midal time frame and microcluster in CluStream [ 1] are proposed for landmark windows. The differences between them are discussed in the following. 3.4.1 Granularity of microclusters As the clusters in an evolving data stream always change, the synopses used in clustering methods have to capture the distribution of recent records at any given time. The EHCF synopsis employs a mechanism for eliminating old records in online clustering, which prevents the radius of the EHCF from becoming larger and larger when the center of the cluster shifts. In such sense, an EHCF is a fine granularity  X  X icrocluster X .
 tonically increases. Although subtraction is performed to reduce the influence of old records when processing a clustering request, the influence is not totally elim-inated when incorporating new records. As a result, the microcluster becomes coarse as its radius increases, which may result in undistinguishable clusters. 3.4.2 Maintenance of synopses and the impact of outliers EHCF synopsis adaptively adjusts the frequency of creating new TCFs. As each EHCF maintains its own histogram, a rapidly changing cluster will maintain more TCFs than a slowly changing one. Since the pyramidal time frame is designed for all possible time horizons, it has to store a snapshot for every microcluster reach-ing the pyramidal time. However, this strategy ignores the property of individual by different clusters.
 new record if it can not be absorbed by any existing microcluster. Such a record is probably an outlier. In CluStream, a new microcluster is created for such an outlier, and the storage of the microcluster is repeatedly allocated until it is eliminated. By contrast, the new EHCF creates only 1 TCF for the outlier. Therefore, EHCF saves a lot of memory at the presence of outlier. 3.4.3 Record-level analysis Because EHCF synopsis adaptively creates a TCF for every newly arrived record, it supports record-level clustering analysis. This property is very important for analysis and network intrusion detection. A simple extension of CluStream would require storing a snapshot whenever a new record arrives, which results in heavy I/O cost and memory consumption.
 4 Sliding windows clustering 4.1 SWClustering algorithm In this section, we describe the SWClustering algorithm on a data stream over a sliding window. SWClustering consists of two parts as shown in Algorithm 1. The first part maintains a group of EHCF synopses (Lines 1 X 18). The second part calculates the clustering result based on the collection of synopses (Lines 19 X 22). Algorithm 1 SWClustering( D S, , N C ) for clustering an evolving data stream D S in a sliding window with a relative error parameter .AtmostNC EHCFs can be kept in memory.
 4.1.1 Maintenance of EHCFs There are three parameters in the SWClustering Algorithm: DS , the data stream to be processed; (0 &lt;&lt; 1), the relative error of windows; and NC , the maximal number of EHCFs kept in memory. The value of NC is determined by the amount of memory available. Our experiments show that even if NC is set to be a small value, the accuracy of the algorithm is still very high.
 distance between x and the center of h . For each x , the nearest EHCF synopsis from x is obtained first.
 absorbed by h . It can be easily calculated by using the cluster feature vector of C [40 ]. We can define  X  (  X &gt; 0), a radius threshold, to determine whether to absorb the record x or not. When dist ( x , h )&gt; X   X  R , x is viewed as far away from h ; otherwise, x is absorbed by h . One problem left is how to determine the value follows Gaussian distribution, then  X  = 2 results in that more than 95% of the data points lie within the corresponding cluster boundary. Therefore,  X  is set to 2 in our experiments. For an EHCF with only one record, the radius is heuristically defined as the distance to the nearest EHCF.
 in Sect. 3.2 . Lines 7 X 12 show the operations in case record x can not be absorbed by h . If the number of EHCF synopses reaches the maximum value NC ,two nearest EHCFs must be merged to generate a new EHCF synopsis, and the count is decreased by 1. A new EHCF synopsis is then generated for record x ,andthe count is increased by 1.
 is invoked repeatedly whenever a new record arrives, at most one EHCF (denoted as h e if existed) contains expired records at any time. Remember that an EHCF synopsis is actually a collection of TCF synopses, each of which represents a set of records. The oldest TCF synopsis in h e is removed when its time stamp does not belong to the most recent N time stamps. If the final TCF in h e expires, h e must be removed, and the count is decreased by 1. 4.1.2 Cluster generation When a clustering request arrives, clusters are calculated from all EHCF synopses immediately, as shown on Lines 19 X 22. For each EHCF synopsis h i , all the TCF synopses in h i are summed up to generate a large TCF synopsis F i , from which the number of records in h i and the center of h i can be obtained. Calculating the clus-ters from cluster feature vectors has been widely studied [ 1, 40 ]. The basic idea is to treat the EHCF h i as a pseudopoint locating at the center of h i with weight m i , where m i is the number of records contained in h i .The k -means algorithm [ 28 ] can be employed to produce clusters of all pseudopoints. 4.1.3 Optimization Algorithm 1 is a space-efficient method, but the processing of each new record is time consuming. Therefore, some optimization techniques are introduced to speed up the computation.
 to get the center of h . According to Theorem 2, the number of TCF synopses can reach O ( 1 log ( n )) . Thus, the cost of a merge operation is O ( 1 log ( n )) . F ) for each EHCF. This will reduce the cost of the merge operations to O ( 1 ) . When a new TCF synopsis (denoted as F ) is generated and inserted into an EHCF, F is updated by F + F , as shown on Line 5. On Line 14, an EHCF must be updated when some records expire. F can also be incrementally maintained due to the following property.
 Property 2 Assume that an EHCF h contains m TCF synopses, F 1 , F 2 , ... , F m , and F denotes the TCF synopsis over all records in h .When F 1 expires, a new TCF synopsis F over all active records in h can be incrementally generated. the corresponding fields in F and F 1 ,andfield t of F equals to the t of F . records. An index can be built on field t of these special synopses F m , then the oldest field is checked to see whether it is expired or not. 4.2 Analysis of SWClustering First, we discuss why late merging operation can efficiently reduce the memory consumption with guaranteed correctness.
 H 2 ,and V H 1 are absorbed by H new . From Theorem 1, the total number of TCFs in H 1 is ( 1 + 1 )( log (( m H 1 ,since H 1 has virtually absorbed n 2 records. It follows that the number of TCFs in H 1 is: + 1 , and the number of TCFs in H new is 1 + 1 ( log ( n + 1 ) + 1 ) . So, the total number of TCFs in H 1 , H 2 ,and H new is: B com = 1 + 1 log m 1 + n 2 + 1 + log m 2 + n 2 + 1  X  2log n 2 + 1 + log n + 1 + 1 .As n 1, we have B Theorem 1, the number of TCFs in H 0 is: where m max = max ( m 1 , m 2 ) .As n increases, the difference diminishes gradually. For example, when n = 2 m max , the difference is only 2 ( 1 + 1 ) . When the number of newly absorbed records exceeds 1 2 log ( m 1 + m 2 ) ,thetwo merging-EHCF can be merged into a new EHCF according to Theorem 3, and there is no difference between the late merging EHCFs and H 0 .
 SWClustering algorithm. Because the impact of late merging on memory dimin-ishes gradually, it is ignored in the following analysis.
 Theorem 5 The SWClustering algorithm uses at most ( 1 + 1 )( log k i = 1 ( N i + 1 )) + k ) TCFs, where N number of EHCFs.
 Proof From Theorem 1, it is known that for each EHCF H i the space needed is at most ( 1 + 1 )( log ( N i + 1 ) + 1 ) . Thus, the memory requirement of the algorithm ( 1 + 1 )( log ( k According to Theorem 5, the memory requirement of SWClustering depends on , N ( 1  X  i  X  k ) ,and k . The following lemma can be used to reduce the dependent variables.
 Lemma 1 If k natural numbers N 1 , N 2 ,..., N k satisfy k i = 1 N i = N, then Theorem 6 The SWClustering algorithm uses O ( k log ( N k )) TCFs, where N denotes the window size, and k is current number of EHCFs.
 Proof From Lemma 1, k i = 1 N i  X  N k k . Thus, O ( 1 log ( k i = 1 N i )) = O uses O ( k log ( N k )) TCFs. 4.3 Evolutionary analysis With the EHCFs maintained by SWClustering, we are able to track the evolving behavior of clusters. The evolutionary analysis of data streams has been a hot topic because of its practical applications. For example, people may concern  X  X ow is (are) current cluster(s) formed X  or  X  X hat is the special time in the formation of the cluster(s) X  while getting the clustering results. Fortunately, the proposed EHCF synopsis can provide sufficient information to answer such questions off-line. cluster, there are m i EHCFs associated with cluster C i .InanEHCF,everyTCF contains the number of arrived records during the time interval between itself and the next TCF. Based on these m i EHCFs, a curve can be drawn to reveal the relation between the number of records in the cluster and the time interval from current time T c to T c  X  N intuitively. Such curve can be analyzed further by domain experts to reveal other characteristics of the cluster.
 ments, and the percentage of the number of arriving records ( arriving ratio )in every segment is then computed. The time when the arriving ratio exceeds some threshold  X  high always corresponds to the ascending phase, while the time when the arriving ratio is below some threshold  X  low corresponds to the descending phase. 5 Experimental results 5.1 Experimental setting All experiments are conducted on a 2.4 GHz PentiumIV PC with 512MB mem-ory, running Microsoft Windows 2000 Professional. To demonstrate the accuracy and efficiency of the SWClustering algorithm, the well-known CluStream algo-rithm [ 1] is used as the comparing algorithm. Both algorithms are implemented in Microsoft Visual C++.
 tering algorithm, both real and synthetic data sets are used in our experiments. 5.1.1 Real life data sets In order to demonstrate the effectiveness of SWClustering, the KDD-CUP X 99 The data set consists of raw TCP connection records from a local area network. Features collected for each connection include the duration of the connection, the number of bytes transmitted from source to destination, etc. Each record in the data set corresponds to either a normal connection or one of four attack types: denial of service, unauthorized access from a remote machine (e.g., guessing password), unauthorized access to root, and probing (e.g., port scanning). Most of the connections in this data set are normal, but occasionally there could be a burst of attacks at certain times. As in Aggarwal et al. [ 1] and Chalaghan et al. [ 10], all 34 out of the total 42 continuous attributes available are used for clustering with one outlier point being removed.
 is also used. It has been used to evaluate several clustering algorithms [ 1, 21]. This data set contains 95,412 records about people who made charitable donations in response to direct mailing requests. Clustering can be used to group donors with similar donation behaviors. As in Aggarwal et al. [ 1] and Farnstrom et al. [ 21], total 56 out of 481 fields are used, and the data set is converted into a stream by taking the data input order as the order of streaming. 5.1.2 Synthetic data sets Synthetic data sets are generated to test the scalability of SWClustering. The data sets have between 100K and 800K points each and vary in the number of natural clusters from 5 to 40. The dimensionality ranges from 10 to 80. The points in each synthetic data set follow a series of Gaussian distributions. As in Aggarwal et al. [1], the mean and variance of the current Gaussian distribution are changed for every 10K points during the synthetic data generation. The following notations are used to characterize the synthetic data sets:  X  X  X  indicates the number of data points in the data set,  X  X  X  and  X  X  X  indicate the number of natural clusters and the dimensionality of each point, respectively. For example, B100C20D30 means the data set contains 100K data points of 30-dimensions, belonging to 20 different clusters.
 [1, 10, 24] to evaluate the accuracy, as defined below. For each point x i in current window, the nearest centroid C x i is first obtained for it, then the distance between x the sum of d 2 ( x i , C x i ) for all the points in current window.
 points in cluster i .
 tures as that of microclusters used by CluStream. The parameters for CluStream are chosen to be the same as those adopted in Aggarwal et al. [ 1]. If the relative stamp of a microclusters in CluStream is not one of the most recent N stamps, then the microcluster is removed. Unless mentioned otherwise, the parameters for SWClustering are set to = 0 . 1, window size N = 10000, and radius threshold  X  = 2. 5.2 Clustering quality evaluation At first, the clustering quality of SWClustering is compared with that of CluStream for different window sizes at different time stamps. In order to make the results more accurate, these two algorithms are executed five times and their average SSQs are calculated. The results on the Network Intrusion data set show that SWClustering produces better clustering output than CluStream in the sce-nario of sliding windows.
 SWClustering is usually better than CluStream by several orders of magnitude. For example, at time stamp 200,000, the average SSQ of SWClustering is about five orders of magnitude smaller than that of CluStream. The quality of cluster-ing with the Charitable Donation data set is examined as well. It is assumed that CluStream should get fairly better results, because the data set is a relatively stable data stream. Figures 10 and 8 show that the result of SWClustering is still much better than that of CluStream for such a relatively stable stream. when N = 1000. We test the data set at selected time points when some at-tacks happen. For example, at time stamp 80,000 there are 752  X  X atan X  attacks, 12  X  X arezmaster X  attacks, 10  X  X uess-passwd X  attacks, and 226 normal connections. It can be seen that SWClustering clearly outperforms CluStream and the purity of SWClustering is always above 90%. For example, at time stamp 80,000 the purity of SWClustering is about 91% and 19% higher than that of CluStream.
 are promptly eliminated in online clustering. This expiring mechanism keeps the small radius of an EHCF when its cluster center drifts, which leads to finer granularity than CluStream, as illustrated in Fig. 2. In particular, the clusters in SWClustering are always formed by the most recent N records with error bound
N . However, in CluStream the time stamps of the records in a microcluster are summed up, a microcluster is deleted when its relevance stamp is below the thresh-old. Such strategy eliminates the microcluster as a whole, not only the individ-ual records. Furthermore, the radius of a microcluster may continuously increase when the cluster center drifts, which may mess up different clusters and reduce the clustering quality. Therefore, SWClustering can separate different attacks into different clusters, while CluStream may merge different attacks into one attack (or normal connections). Average SSQ This is because: (1) the underlying distribution of the data stream is always chang-ing, and the SSQs over sliding windows are different at different time stamps in records, and get a relatively precise result, when a lot of old records in CluStream happen to be eliminated as a whole. For example, in Figure 7, the difference be-tween CluStream and SWClustering is relatively small at time stamp 400,000. By checking the execution of CluStream, it is found that a lot of old microclusters are eliminated at that time. 5.3 Scalability results The following experiments are designed to evaluate the scalability of SWCluster-ing. The first part is used to evaluate the execution time. The second part is used to study the memory usage. 5.3.1 Execution time As we know, the CluStream algorithm needs to periodically store the current snap-shot of microclusters under the Pyramidal Time framework. In the implementation of the CluStream algorithm, the snapshots of microclusters are stored in memory to save the execution time. that of the on-line component of CluStream on the real data sets. In CluStream, the frequency of storing snapshot affects the processing throughput and the precision. High storing frequency can improve the precision of CluStream at the cost of the throughput. In order to get the same precision as SWClustering, CluStream has to store a snapshot whenever a new record arrives. This inevitably leads to large overhead. To compare these two algorithms, we lower the precision requirement for CluStream 100 times, and the snapshot storing frequency is set to 1 snapshot per 100 records. Figures 11 and 12 show the execution time increases linearly when the data stream proceeds, and the curves are of different slopes. Notice that the curve with lower slope implies higher processing throughput. Hence, it can be concluded that SWClustering is much more efficient than CluStream.
 various dimensionality and different numbers of natural clusters. Synthetic data natural clusters and dimensions can be obtained during the generating of data sets. clusters from 10 to 40, while fixing the size and dimensionality of the data streams. Figure 13 shows that the execution time of SWClustering is linear with respect to the number of natural clusters. For example, when the number of clusters increases from 10 to 40 for data set series B400D40, the running time increases from 30 to 54 s with nearly 8 s per 10 clusters. Excution Time(in seconds) from 10 to 80, while fixing the stream size and number of natural clusters. Figure 14 shows that the execution time increases linearly with respect to the dimension-ality. 5.3.2 Memory usage upper bounds for the memory usage. Since the memory usage may fluctuate in the progress of data streams, the maximum memory usage is used as the measurement. The entity used in CluStream is a microcluster, and in SWClustering it is a TCF. Since these two kinds of entities require similar memory space, the number of entities can be used to evaluate the memory usage.
 10,000 to 100,000 for the Network Intrusion data set, and from 10,000 to 60,000 for the Charitable Donation data set. As the snapshot storing frequency will affect the memory usage of CluStream, different frequencies (from 1 snapshot per 1 record to 1 snapshot per 100 records) are set to check the fluctuation of memory usage. As shown in Figures 15 and 16 , the memory usage of SWClustering is consistently much lower than CluStream as the window size increases. In all cases, SWClustering outperforms CluStream by a factor of 2 X 5.
 a significant impact on memory usage. In the experiment with Network Intrusion Execution Time(in seconds) data set, varies from 0.01 to 0.1 (and the number of snapshots in the same level of pyramidal time frame varies from 100 to 10 accordingly). The goal is to investigate the fluctuation of memory usage. The storing frequency of CluStream is fixed to be 1 snapshot per 100 records and window size is set to 100,000. Figure 17 illustrates the results. It can be seen that when the value of increases, the memory usage decreases significantly. 5.4 Sensitivity analysis In Sect. 4.1 , it is known that the number of EHCFs should be larger than the num-ber of natural clusters in order to perform effective clustering. However, maintain-ing a large number of EHCFs leads to the increase of memory usage and slow-down of execution. We define EHCF-ratio as the number of EHCFs divided by the number of natural clusters.
 EHCF-ratio . The current time stamp T c is set to be 100,000 for the Network In-trusion data set and 50,000 for the Charitable Donation data set. Figure 18 shows that if EHCF-ratio = 1, i.e., the number of EHCFs is exactly the same as the nat-ural clusters, the clustering quality is quite poor. When the EHCF-ratio increases, the average SSQ reduces. The average SSQ becomes stable when EHCF-ratio is about 10. This implies that to achieve quality clustering, EHCF-ratio needs not to be too large.
 error parameter . is used to control the number of expired records in sliding windows. The SSQ is evaluated by varying the . For the Network Intrusion data set, the window size N is set to 20,000 with current time stamp T c = 100 , 000, and for the Charitable Donation data set, N is set to 5000 with T c = 80 , 000. Figure 19 shows that if 0 . 005  X   X  0 . 16, the quality is relatively stable. However, when becomes a large value like 0.32, the quality of SWClustering deteriorates quickly. We note that the choice of = 0 . 32 represents a case in which the clusters are determined by the data set with more than 30% outdated data. 5.5 Analysis of tracking clusters Our SWClustering algorithm is able to track clusters in evolving data streams. The experiment reported here is to demonstrate that the clusters can be tracked at different time stamps based on the results of SWClustering.
 is set to be 4000 with current time stamp T c being 10,000. SWClustering finds 58 EHCFs. Three of them are displayed in Figure 20 , each of which corresponds to one type of real-life network attacks. It can be seen that a snmpget attack ends at 8200, an ipsweep attack begins at 7700 and ends at 8100, while a smurf attack starts at 8400 and lasts till the end of the window. The evolving of these attacks can be fully recorded. Furthermore, different characteristics of the attacks are illustrated by the corresponding curves. For example, it can be seen that the snmpget attack goes up slowly, the ipsweep attack seems relatively scattered Average SSQ and the smurf attack goes up sharply. Further analysis on the evolving clusters provides a deep understanding of the nature of different network attacks. This can be used to predict new attacks in the future. 6 Conclusions An effective and efficient clustering approach for analyzing evolving data streams over sliding windows is proposed in this paper. An EHCF structure is introduced by combining Exponential Histogram with Cluster Feature to record the evolu-tion of each cluster and to capture the distribution of recent records. Comparing to CluStream with batch updating and storing the whole snapshot, an EHCF is up-dated only when new records are collected. This approach has several advantages: (1) the quality of clustering is improved due to the fine granularity of EHCFs; (2) theoretical analysis shows that the memory consumption is limited and bounded, and the experiments show that its performance is better than CluStream in sliding lution, which is very useful for online mining tasks over data streams. Moreover, several innovative ideas such as the late merging of EHCFs are proven to be useful for further improvement of the performance. Future work will focus on applying EHCF to other data mining tasks such as outlier detection, and providing more evolution analysis functionalities based on SWClustering.
 Appendix A Proof Theorem 3 of in Sect. 3.2 B Proof Theorem 4 of in Sect. 3.3 C Proof Lemma 1 of in Sect. 4.2 References Author Biographies
