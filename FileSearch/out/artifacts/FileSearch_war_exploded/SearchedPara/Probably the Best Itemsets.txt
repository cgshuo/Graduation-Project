 One of the main current challenges in itemset mining is to discover a small set of high-quality itemsets. In this paper we propose a new and general approach for measuring the qual-ity of itemsets. The method is solidly founded in Bayesian statistics and decreases monotonically, allowing for efficient discovery of all interesting itemsets. The measure is defined by connecting statistical models and collections of itemsets. This allows us to score individual itemsets with the proba-bility of them occuring in random models built on the data.
As a concrete example of this framework we use exponen-tial models. This class of models possesses many desirable properties. Most importantly, Occam X  X  razor in Bayesian model selection provides a defence for the pattern explosion. As general exponential models are infeasible in practice, we use decomposable models; a large sub-class for which the measure is solvable. For the actual computation of the score we sample models from the posterior distribution using an MCMC approach.

Experimentation on our method demonstrates the mea-sure works in practice and results in interpretable and in-sightful itemsets for both synthetic and real-world data. H.2.8 [ Database management ]: Database applications X  Data mining; G.3 [ Probability and Statistics ]: Markov processes Algorithms, Theory Itemset mining, exponential models, decomposable models, junction trees, Bayesian model selection, MCMC
Discovering frequent itemsets is one of the most active fields in data mining. As a measure of quality, frequency pret and as it decreases monotonically there exist efficient algorithms for discovering large collections of frequent item-sets [2]. However, frequency also has serious drawbacks. A frequent itemset may be uninteresting if its elevated fre-quency is caused by frequent singletons. On the other hand, some non-frequent itemsets could be interesting. Another drawback is the problem of pattern explosion when mining with a low threshold.

Many different quality measures have been suggested to overcome the mentioned problems (see Section 5 for a more detailed discussion). Usually these measures compare the observed frequency to some expected value derived, for ex-ample, from the independence model. Using such measures we may obtain better results. However, these approaches still suffer from pattern explosion. To point out the prob-lem, assume that two items, say a and b are correlated, and hence are considered significant. Then any itemset contain-ing a and b will be also considered significant.

Example 1. Assume a dataset with K items a 1 ,...,a K such that a 1 and a 2 always yield identical value and the rest of the items are independently distributed. Assume that we apply some statistical method to evaluate the significance of itemsets by using the independence model as the ground truth. If an itemset X contains a 1 a 2 , its frequency will be higher than then the estimate of the independence model. Hence, given enough data, the P-value of the statistical test will go to 0 , and we will conclude that the itemset X is inter-esting. Consequently we will find 2 K  X  2 interesting itemsets.
In this work we approach the problem of defining quality measure from a novel point of view. We construct a connec-tion between itemsets and statistical models and use this connection to define a new quality measure for itemsets. To motivate this approach further, let us consider the following example.

Example 2. Consider a binary dataset with 5 items, say a ,...,a 5 , generated from the independence model. We ar-gue that if we know that the data comes from the indepen-dence model, then the only interesting itemsets are the sin-gletons. The reasoning behind this claim is that the frequen-cies of singletons correspond exactly to the column margins, the parameters of the independence model. Once we know the singleton frequencies, there is nothing left in the data that would be statistically interesting.
Let us consider a more complicated example. Assume that data is generated from a Chow-Liu tree model [4], say Again, if we know that data is generated from this model, then we argue that the interesting itemsets are a 1 a the same as with the independence model. If we know the frequencies of these itemsets we can derive the parameters of the distribution. For example, p ( a 2 = 1 | a 1 fr ( a 1 a 2 ) / fr ( a 1 ) .

Let us now demonstrate that this approach will produce much smaller and more meaningful output than the method given in Example 1.

Example 3. Consider the data given in Example 1. To fully describe the data we only need to know the frequencies of the singletons and the fact that a 1 and a 2 are identical. This information can be expressed by outputting the frequen-cies of singleton itemsets and the frequency of itemset a This will give us K + 1 interesting patterns in total.
Our approach is to extend the idea pitched in the pre-ceding example to a general itemset mining framework. In the example we knew which model generated the data, in practice, we typically do not. To solve this we will use the Bayesian approach, and instead of considering just one spe-cific model, we will consider a large collection of models, namely exponential models. A virtue of these models is that we can naturally connect each model to certain itemsets. A model M has a posterior probability P ( M | D ), that is, how probable is the model given the data. The score of a single itemset then is just the probability of it being a parameter of a random model given the data. This setup fits perfectly the given example. If we have strong evidence that data is coming from the independence model, say M , then the posterior probability P ( M | D ) will be close to 1, and the posterior probability of any other model will be close to 0. Since the independence model is connected to the singletons, the score for singletons will be 1 and the score for any other itemset will be close to 0.

Interestingly, using statistical models for defining signif-icant itemsets provides an approach to the problem of the pattern set explosion (see Section 3.2 for more technical de-tails). Bayesian model selection has an in-built Occam X  X  razor, favoring simple models over complex ones. Our con-nection between models and itemsets is such that simple models will correspond to small collections of itemsets. In result, only a small collection of itemsets will be considered interesting, unless the data provides sufficient amount of ev-idence.

Our contribution in the paper is two-fold. First, we intro-duce a general framework of using statistical models for scor-ing itemsets in Section 2. Secondly, we provide an example of this framework in Section 3 by using exponential models and provide solid theoretical evidence that our choices are well-founded. We provide the sampling algorithm in Sec-tion 4. We discuss related work in Section 5 and present our experiments in Section 6. Finally, we conclude our work with Section 7. The proofs are given in Appendix [18]. The implementation is provided for research purposes 1 . http://adrem.ua.ac.be/implementations
As we discussed in the introduction, our goal is to define a quality measure for itemsets using statistical models. In this section we provide a general framework for such a score. We will define the actual models in the next section. We begin with some preliminary definitions and notations. In our setup a binary dataset is a collection of N transac-tions, binary vectors of length K . We assume that these vectors are independently generated from some unknown distribution. Such a dataset can be easily represented by a binary matrix of size N  X  K . By an attribute a mean a Bernoulli random variable corresponding to the i th column of the data. We denote the set of attributes by A = { a 1 ,...,a K } .
 An itemset X is simply a subset of A . Given an itemset X = { a i 1 ,...,a i L } and a transaction t we denote by t ( t i 1 ,...,t i L ) the projection of t into X . We say that t covers X if all elements in t X are equal to 1.

We say that a collection of itemsets F is downward closed if for each member X  X  X  any sub-itemset is also included. This property plays a crucial point in mining frequent pat-terns since it allows effective candidate pruning in level-wise approach and branch pruning in a DFS approach.

Our next step is to define the correspondence between statistical models and families of itemsets. Assume that we have a set M of statistical models for the data. We will dis-cuss in later sections what specific models we are interested in, but for the moment, we will keep our discussion on a high level. Each model M  X  M has a posterior probability P ( M | D ), that is, how probable the model M is given data D . To link the models to families of itemsets we assume that we have a function fam that identifies a model M with a downward closed family of itemsets. As we will see later on, there is one particular natural choice for such a function.
Now that we have our models that are connected to cer-tain families of itemsets, we are ready to define a score for individual itemsets. The score for an itemset X is the poste-rior probability of X being a member in a family or itemsets,
The motivation for such score is as follows. If we are sure that some particular model M is the correct model for D , then the posterior probability for that model will be close to 1 and the posterior probabilities for other models will be close to 0. Consequently, the score for an itemset X will be close to 1 if X  X  fam ( M ), and 0 otherwise.

Naturally, the pivotal choice of this score lies in the map-ping fam . Such a mapping needs to be statistically well-founded, and especially the size of an itemset family should be reflected in the complexity of the corresponding model. We will see in the following section that a particular choice for the model family and mapping fam has these properties, and leads to certain important qualities.

Proposition 4. The score decreases monotonically, that is, X  X  Y implies sc ( X )  X  sc ( Y ) .

Proof. We are allowing fam only to map on downward closed families. Hence the inequality holds, and consequently we have sc ( X )  X  sc ( Y ). This com-pletes the proof.
In this section we will make our framework more concrete by providing a specific set M of statistical models and the function fam identifying the models with families of item-sets. We will first give the definition of the models and the mapping. After this, we point out the main properties of our model and justify our choices. However, as, it turns out that computing the score for these models is infeasible, so instead, we solve this problem by considering decomposable models.
Models of exponential form have been studied exhaus-tively in statistics, and have been shown to have good the-oretical and practical properties. In our case, using expo-nential models provide a natural way of describing the de-pendencies between the variables. In fact, the exponential model class contains many natural models such as, the inde-pendence model, the Chow-Liu tree model, and the discrete Gaussian model. Finally, such models have been used suc-cessfully for predicting itemset frequencies [13] and ranking itemsets [17].

In order to define our model, let F be a downward closed family of itemsets containing all singletons. For an itemset X  X  F we define an indicator function S X ( t ) mapping a transaction t into binary value. If the transaction t covers X , then S X ( t ) = 1, and 0 otherwise. We define an expo-nential model M associated with F to be the collection of distributions having the exponential form where r X is a parameter, a real value, for an itemset X . Model M also contains all the distributions that can be ob-tained as a limit of the distribution having the exponential form. This technicality is needed to handle distributions with zero probabilities. Since the indicator function S  X  equal to 1 for any t , the parameter r  X  acts like a normaliza-tion constant. The rest of the parameters form a parameter vector r of length |F| X  1. Naturally, we set F = fam ( M ).
Example 5. Assume that F consists only of singleton itemsets. Then the corresponding model has the form exp r  X  + independence model. The other extreme is when F consists of all itemsets. Then we can show that the corresponding model contains all possible distributions, that is, the model is in fact the parameter-free model.

As an intermediate example, the tree model in Example 2 is also an exponential model with a corresponding family { a 1 ,...,a 5 } X  X  a 1 a 2 ,a 2 a 3 ,a 2 a 4 ,a 4 a 5 } .
The intuition behind the model is that when an itemset X is an element of F , then the dependencies between the items in X are considered important in the corresponding model. For example, if F consists only of singletons, then there are no important correlations, hence the corresponding model should be the independence model. On the other hand, in the tree model given in Example 2 the important correlations are the parent-child item pairs, namely, a 1 a 2 ,a 2 a 3 These are exactly, the itemsets (along with the singletons) that correspond to the model.

Our choice for the models is particularly good since the complexity of models reflects the size of itemset family. Since Bayesian approach has an in-built tendency to punish com-plex families (we will see this in Section 3.2), we are punish-ing large families of itemsets. If the data states that simple models are sufficient, then the probability of complex mod-els will be low, and consequently the score for large itemsets will also be low. In other words, we casted the problem of pattern set explosion into a model overfitting problem and used Occam X  X  razor to punish the complex models!
Now that we have defined our model M , our next step is to compute the posterior probability P ( M | D ). That is, the probability of M given the data set D . We select the model prior P ( M ) to be uniform. Recall that in Eq. 2 a model M has a set of parameters, that is, to pinpoint a single distribution in M we need a set of parameters r . Following the Bayesian approach to compute P ( M | D ) we need to marginalize out the nuisance parameters r , P ( M | D ) = In the general case, this integral is too complex to solve analytically so we employ the popular BIC estimate [14],
P ( M | D )  X  C  X  P ( D | M,r  X  )  X  exp where C is a constant and r  X  is the maximum likelihood estimate of the model parameters. This estimate is correct when | D | approaches infinity [14]. So instead of computing a complex integral our challenge is to discover the maximum likelihood estimate r  X  and compute the likelihood of the data. Unfortunately, using such model is an NP -hard prob-lem (see, for example, [16]). We will remedy this problem in Section 3.4 by considering a large subclass of exponen-tial models for which the maximum likelihood can be easily computed.
In this section we will provide strong theoretical justifica-tion for our choices and show that our score fulfills the goals we set in the introduction.

We saw in Example 2 that if the data comes from the independence model, then we only need the frequencies of the singleton itemsets to completely explain the underlying model. The next theorem shows that this holds in general case.

Theorem 6. Assume that data D is generated from a dis-tribution p that comes from an exponential model M . Let F = fam ( M ) be the family of itemsets. We can derive the maximum likelihood estimate from the frequencies of F . Moreover, as the number of transactions goes to infinity, we can derive the true distribution p from the frequencies of F .
The preceding theorem showed that fam ( M ) is sufficient family of itemsets in order to derive the correct true distribu-tion. The next theorem shows that we favor small families: if the data can be explained with a simpler model, that is, using less itemsets, then the simpler model will be chosen and, consequently, redundant itemsets will have a low score.
Theorem 7. Assume that data D is generated from a dis-tribution p that comes from a model M . Assume also that if any other model, say M 0 , contains this distribution, then number of data points in D goes into infinity, sc ( X ) = 1 if X  X  fam ( M ) , otherwise sc ( X ) = 0 .
We saw in Section 3.2 that in practice we cannot compute the score for general exponential models. In this section we study a subclass of exponential models, for which we can easily compute the needed score. Roughly speaking, a decomposable model is an exponential model where the cor-responding maximal itemsets can be arranged to a specific tree, called junction tree. By considering only decomposable models we obviously will lose some models, for example, the discrete Gaussian model, that is, a model corresponding to all itemsets of size 1 and 2 is not decomposable. On the other hand, many interesting and practically relevant mod-els are decomposable, for example Chow-Liu trees. Finally, these models are closely related to Bayesian networks and Markov Random Fields (see [5] for more details).

To define a decomposable model, let F be a downward closed family of itemsets. We write G = max( F ) to be the set of maximal itemsets from F . Assume that we can build a tree T using itemsets from G as nodes with the following property: If X,Y  X  G have a common item, say a , then X and Y are connected in T (by a unique path) and every itemset along that path contains a . If this property holds for T , then T is called junction tree and F is decomposable . We will use E ( T ) to denote the edges of the tree.
Not all families have junction trees and some families may have multiple junction trees. (a) Decomposable family F 1 of itemsets Figure 1: Figure 1(a) shows that the itemset fam-ily given in Example 2 is decomposable. Fig-ure 1(b) shows the junction tree for the family after Merge ( { a 2 } ,a 3 ,a 4 ) and Figure 1(c) shows the junc-tion tree after Merge ( { a 4 } ,a 3 ,a 5 ) .

Example 8. Let F 1 be the family of itemsets connected to the Chow-Liu model given in Example 2. The maximal a junction tree for this family, making the family decompos-able. On the other hand, family { a 1 a 2 ,a 1 a 3 ,a decomposable since there is no junction tree for this family.
The most important property of decomposable families is that we can compute the maximum likelihood efficiently. We first define the entropy of an itemset X , denoted by H ( X ), as where q D is the empirical distribution of the data.
Theorem 9. Let F be a decomposable family and let T be its junction tree. The maximum log-likelihood is equal to  X  log P ( D | M,r
Example 10. Assume that our model space M consists only of two models, namely the tree model M 1 given in Ex-ample 2 and the independence model, which we denote M 2 . Assume also that we have a dataset with 9 transactions,
To compute the probabilities P ( M 1 | D ) and P ( M 2 we need to know the entropies of certain itemsets
H ( a 1 ) = H ( a 2 ) = H ( a 3 ) = H ( a 5 ) = 0 . 68 ,H ( a
The log-likelihood of the independence model is equal to We use the junction tree given in Figure 1(a) and Theorem 9 to compute the log-likelihood of M 1 , implies that P ( M 1 | D )  X  P ( D | M 1 ,r  X  ) exp(  X  9 / 2 log 9) = 6 . 27  X  10 P ( M 2 | D )  X  P ( D | M 2 ,r  X  ) exp(  X  5 / 2 log 9) = 2 . 43  X  10 We get the final probabilities by noticing that P ( M 1 | D ) + P ( M 2 | D ) = 1 so that we have P ( M 1 | D ) = 0 . 72 and P ( M 2 | D ) = 0 . 28 . Consequently, the scores for itemsets are equal to sc ( a 1 a 2 ) = sc ( a 2 a 3 ) = sc ( a 2 a 4 ) = sc ( a sc ( a i ) = 1 , for i = 1 ,..., 5 , and sc ( X ) = 0 otherwise.
Now that we have means for computing the posterior probability of a single decomposable model, our next step is to compute the score of an itemset namely, the sum in Eq. 1. The problem is that this sum has an exponential number of terms, and hence we cannot solve by enumerat-ing all possible families. We approach this problem from a different point of view. Instead of computing the score for each itemset individually, we will divide our mining method into two steps: 1. Sample random decomposable models from the poste-2. Estimate the true score of an itemset by computing
In order to sample we will use a MCMC approach by mod-ifying the current decomposable family by two possible op-erations, namely
Naturally, not all splits and merges are legal, since some operations may result in a family that is not decomposable, or even downward closed.

Example 11. The family F 2 given in Figure 1(b) is ob-tained from the family F 1 given in Figure 1(a) by perform-ing Merge ( { a 2 } ,a 3 ,a 4 ) . Moreover, F 3 (Figure 1(c)) is ob-tained from F 2 by performing Merge ( { a 4 } ,a 3 ,a 5 versely, we can go back by performing Split ( a 3 a 4 a 5 first and Split ( a 2 a 3 a 4 ,a 3 ,a 4 ) second.
 The next theorem tells us which splits are legal.

Theorem 12. Let F be decomposable family and let X  X  max( F ) and let x,y  X  X . Then the resulting family after a split operation Split ( X,x,y ) is decomposable if and only, there are no other maximal itemsets in F containing x and y simultaneously.

Example 13. All possible split combinations are legal in families F 1 and F 2 given in Figure 1(a) and Figure 1(b). However, for F 3 given in Figure 1(c) Split ( a 2 a 3 operation Split ( a 3 a 4 a 5 ,a 3 ,a 4 ) is illegal.
In order to identify legal merges, we will need some addi-tional structures. Let F be a downward closed family and let G = max( F ) be its maximal itemsets. Let S be an itemset. We construct a reduced family , denoted by rf ( F ; S ) with the following procedure. Let us first define To obtain the reduced family rf ( F ; S ) from X , assume there are two itemsets X,Y  X  X  such that X  X  Y 6 =  X  . We remove these two sets from X and replace them with X  X  Y . This is continued until no such replacements are possible. We ignore any reduced family that contains 0 or 1 itemsets. The reason for this will be seen in Theorem 15, which implies that such families will not induce any legal merges.

Example 14. The non-trivial reduced families of the fam-ily given in Figure 1(a) are rf ( F 1 ; a 2 ) = { a family given in Figure 1(b) are rf ( F 2 ; a 2 ) = { a 1 ,a family given in Figure 1(c) are rf ( F 3 ; a 2 ) = { a 1 ,a rf ( F 3 ; a 3 a 4 ) = { a 2 ,a 5 } .
 The next theorem tells us when Merge ( S,x,y ) is legal.
Theorem 15. Let F be decomposable family. A merge operation is legal, that is, F is still decomposable after adding Z = S  X  X  x,y } if and only if there are sets V,W  X  rf ( F ; S ) , V 6 = W , such that x  X  V and y  X  W .

Example 16. Family F 2 in Figure 1(b) is obtained from the family F 1 in Figure 1(a) by Merge ( a 2 ,a 3 a 4 legal operation since rf ( F 1 ; a 2 ) = { a 1 ,a 3 ,a merge transforming F 2 to F 3 is legal since rf ( F 2 { a 2 a 3 ,a 5 } . However, this merge would not be legal in F since we do not have a 3 in rf ( F 1 ; a 4 ) . Sampling requires a proposal distribution Q ( M 0 | M ). Let M be a current model. We denote the number of legal opera-tions, either a split or a merge, by d ( M ). Let M 0 be a model obtained by sampling uniformly one of the legal operations and applying it to M . The probability of reaching M 0 from M with a single step is Q ( M 0 | M ) = 1 /d ( M ). Similarly, the probability of reaching M from M 0 with a single step is Q ( M | M 0 ) = 1 /d ( M 0 ). Consequently, if we sample u uni-formly from the interval [0 , 1] and accept the step moving from M into M 0 if and only if u is smaller than then the limit distribution of the MCMC will be the poste-rior distribution P ( M | D ) provided that the MCMC chain is ergodic. The next theorem shows that this is the case.
Theorem 17. Any decomposable model M can be reached from any other model M 0 by a sequence of legal operations.
Our first step is to compute the ratio of the models given in Eq 4. To do that we will use the BIC estimate given in Eq. 3 and Theorem 9. Let us first define a function gain ( X,x,y ) = where X is an itemset and x,y  X  X are items.
 Theorem 18. Let M be a decomposable model and let M 0 = Split ( X,x,y ; M ) be a model obtained by a legal split. Let A be the BIC estimate of P ( M | D ) and let B be the BIC estimate of P ( M 0 | D ) . Then Similarly, if M 0 = Merge ( S,x,y ; M ) , then
To compute the gain we need the entropies for 4 item-sets. Let X be an itemset. To compute H ( X ) we first order the transactions in D such that the values corresponding to X are in lexicographical order. This is done with a radix sort Sort ( D,X ) given in Algorithm 1. This sort is done compute the entropy with a single data scan: Set e = 0 and p = 0. If the values of X of the current transaction is equal to the previous transaction we increase p by 1 / | D | , other-wise we add  X  p log p to e and set p to 1 / | D | . Once the scan is finished, e will be equal to H ( X ). The pseudo code for computing the entropy is given in Algorithm 2.

Algorithm 1: Sort ( D,X ). Routine for sorting the transactions. Used by Entropy as a pre-step for com-puting the entropy. 1 if X =  X  or D =  X  then return D ; 2 a i  X  first item in X ; 3 D 0  X  X  t  X  D | t i = 0 } ; D 1  X  X  t  X  D | t i = 1 } ; 4 D 0  X  Sort ( D 0 ,X  X  a i ); D 1  X  Sort ( D 1 ,X  X  a i ); 5 return D 0 concatenated with D 1 .

Algorithm 2: Entropy ( D,X ). Computes the entropy of X from the dataset D . 1
Sort ( D,X ); 2 e  X  0; p  X  0; 3 u  X  first transaction in D ; 4 foreach t  X  D do 5 if u X 6 = t X then 6 e  X  e  X  p log p ; 7 u  X  t ; 8 p  X  1 / | D | ; 9 else 10 p  X  p + 1 / | D | ; 11 e  X  e  X  p log p ; 12 return e ;
Our final step is to compute d ( M ) and actually sample the operations. To do that we first write sd ( M ) for the number of possible Split operations and let sd ( M,X ) be the number of possible Split operations using itemset X . Similarly, we write md ( M,S ) for the number of legal merges using S and also md ( M ) for the amount of legal merges in total.
Given a maximal itemset X we build an occurrence table, which we denote by st ( X ), of size | X | X | X | . For x,y  X  X , the entry of the table st ( X,x,y ) is the number of maximal itemsets containing x and y . If st ( X,x,y ) = 1, then Theo-rem 12 states that Split ( X,x,y ) is legal. Consequently, to sample a split operation we first select a maximal itemset weighted by sd ( M,X ) / sd ( M ). Once X is selected we select uniformly one legal pair ( x,y ).

To sample legal merges, recall that Merge ( S,x,y ) in-volves selecting two maximal itemsets X and Y such that S = X  X  Y , x  X  X  X  S , and y  X  Y  X  S . Instead of selecting these itemsets, we will directly sample an itemset S and then select two items x and y . This sampling will work only if two legal merges Merge ( S 1 ,x 1 ,y 1 ) and Merge ( S 2 ,x result in two different outcomes whenever S 1 6 = S 2 .
Theorem 19. Let S 1 and S 2 be two different itemsets and let x 1 ,y 1 /  X  S 1 , and x 2 ,y 2 /  X  S 2 be items. Assume Z = S i  X  X  x i ,y i } for i = 1 , 2 . Then Z 1 6 = Z 2 .
The construction of a reduced family states that, if V,W  X  rf ( F ; S ), V 6 = W , then V  X  W =  X  . It follows from Theo-rem 15 that To sample a merge we first sample an itemset S weighted by md ( M,S ) / md ( M ). Once S is selected, we sample two different itemsets V,W  X  rf ( F ; S ) (weighted by | V | and | W | ). Finally, we sample x  X  V and y  X  W .

Sampling S for a merge operation is feasible only if the number of reduced families for which the merge degree is larger than zero is small.

Theorem 20. Let K be the number of items. There are at most K maximal itemsets. There are at most K  X  1 item-sets for which the degree md ( M,  X  ) &gt; 0 .
 Pseudo-code for a sampling step is given in Algorithm 3.
Algorithm 3: MCMC step for sampling decomposable models. 1 u  X  random integer between 1 and d ( M ); 2 if u  X  sd ( M ) then 3 Sample X from max( F ) weighted by sd ( M,X ); 4 Sample x,y  X  X such that X is the only maximal 5 M 0  X  Split ( X,x,y ; M ); 6 g  X  gain ( X,x,y )  X  log | D | 2 | X | X  3 ; 7 else 8 Sample S weighted by md ( M,S ); 9 Sample V  X  rf ( F ; S ) weighted by | V | ; 10 Sample W  X  rf ( F ; S ), V 6 = W , weighted by | W | ; 11 Sample x  X  V and y  X  W ; 12 M 0  X  Merge ( S,x,y ; M ); 13 g  X  X  X  gain ( S  X  X  x,y } ,x,y ) + log | D | 2 | S | X  1 ; 14 z  X  random real number from [0 , 1]; 16 else return M ;
We have demonstrated what structures we need to com-pute so that we can sample legal operations. After a sample, we can reconstruct these structures from scratch. In this sec-tion we show how to optimize the sampling by constructing the structures incrementally using Algorithms 4 X 7.

First of all, we store only maximal itemsets of F . Theo-rem 20 states that there can be only K such sets, hence split and merge operations can be done efficiently.

During a split or a merge, we need to update what split operations are legal after the split. We do this by updating an occurrence table st ( X ). An update takes O ( | X | 2 The next theorem shows which maximal itemsets we need to update for legal split operations after a merge.
Theorem 21. Let F be a downward closed family of item-sets and let G be the family after performing Merge ( S,x,y ) . Let Y be a maximal itemset in max( F )  X  max( G ) . Then legal split operations using Y remain unchanged during the merge unless Y is the unique itemset among maximal itemsets in F containing either S  X  X  x } or S  X  X  y } .

The following theorem tells us how reduced families should be updated after a merge operation. To ease the notation, let us denote by link ( F ,S,x ) the unique itemset (if such exists) in rf ( F ; S ) containing x .

Theorem 22. Let F be a downward closed family of item-sets and let G be the family after performing Merge ( S,x,y ) . Then the reduced families are updated as follows: 1. Itemsets link ( F ,S,x ) and link ( F ,S,y ) in rf ( F ; S ) are 2. Itemset { x } is added into rf ( G ; S  X  X  y } ) . Itemset { y } 3. Let T ( S and let z  X  S  X  T . The itemset containing z 4. Otherwise, rf ( F ; T ) = rf ( G ; T ) or md ( F ; T ) = 0 and
Theorems 21 and 22 only covered the updates during merges. Since Split ( S  X  X  x,y } ,x,y ) and Merge ( S,x,y ) are opposite operations we can derive the needed updates for splits from the preceding theorems.

Corollary 23 (of Theorem 21). Let F be a down-ward closed family of itemsets and let G be the family af-ter performing Split ( X,x,y ) . Let Y be a maximal item-set in max( F )  X  max( G ) . Then legal split operations us-ing Y remain unchanged during the merge unless Y is the unique itemset among maximal itemsets in G containing ei-ther X  X  X  x } or X  X  X  y } .

Corollary 24 (of Theorem 22). Let F be a down-ward closed family of itemsets and let G be the family after performing Split ( X,x,y ) . Let S = X  X  X  x,y } . Then the reduced families are updated as follows: 1. Itemset containing { x,y } in rf ( F ; S ) is split into two 2. Itemset { x } is removed from rf ( G ; S  X  X  y } ) . Itemset 3. Let T ( S and let z  X  S  X  T . Item x is removed from 4. Otherwise, rf ( F ; T ) = rf ( G ; T ) or md ( F ; T ) = 0 and
We keep in memory only those families that have posi-tive merge degree. Theorem 20 tells us that there are only K  X  1 such families. By studying the code in the update algorithm we see that, except in two cases, the update of a family is either a insertion/deletion of an element into an itemset or a merge of two itemsets. The first complex case is given on Line 2 in MergeSide which corresponds to Case 2 in Theorem 22. The problem is that this family may have contained only itemset before the merge, hence we did not store it. Consequently, we need to recreate the missing item-set, and this is done in O ( P X  X  X  X ) time. The second case occurs on Line 2 in SplitSide . This corresponds to the case where we need to break the itemset W  X  rf ( S ) containing x and y apart during a split (Case 1 in Corollary 24). This is done by constructing the new sets from scratch. The con-struction needs O ( | W | K ( | W | + M )) time, where M is the size of largest itemset in F .

Algorithm 4: SplitUpdate ( X,x,y ). Routine for up-dating the structures during Split ( X,x,y ). 1 Update F ; 2 Remove link ( S,x ) from rf ( S ); 3 S  X  X  X  X  x,y } ; 4 SplitSide ( X,x,y,S ); SplitSide ( X,y,x,S ); Algorithm 5: Subroutine SplitSide ( X,a,b,S ) used by
SplitUpdate . 1 Z  X  S  X  X  a } ; 2 while changes do 3 Z  X  Z  X  X  X  X  max( F ) ; S ( Z  X  X } ; 4 Add Z into rf ( S ); 5 if rf ( S  X  X  a } ) exists then 6 Remove { b } from rf ( S  X  X  a } ); 7 for T ( S , rf ( T  X  X  a } ) exists do 8 z  X  (any) item in S  X  T ; 9 Remove b from link ( S,z ); 10 if there is unique Z  X  max( F ) s.t. S  X  X  a } ( Z then 11 Update st ( Z );
Algorithm 6: MergeUpdate ( S,x,y ). Routine for up-dating the structures during Merge ( S,x,y ). 1 Merge link ( S,x ) and link ( S,y ) in rf ( S ); 2 A  X  itemset in max( F ) such that S  X  X  x } X  A ; 3 B  X  itemset in max( F ) such that S  X  X  y } X  B ; 4 Build st ( S  X  X  x,y } ) from st ( A ) and st ( B ); 5
MergeSide ( S,x,y ); MergeSide ( S,y,x ); 6 Update F ; Algorithm 7: Subroutine MergeSide ( S,a,b ) used by
MergeUpdate . 1 U  X  S  X  X  a } ; 2 if U /  X  max( F ) and rf ( U ) does not exists then 4 Add b into rf ( U ); 5 for T ( S , rf ( T  X  X  a } ) exists do 6 z  X  (any) item in S  X  T ; 7 Augment link ( S,z ) with b ; 8 if there is unique Z  X  max( F ) s.t. U ( Z then 9 Update st ( Z ); Many quality measures have been suggested for itemsets. A major part of these measures are based on how much the itemset deviates from some null hypothesis. For example, itemset measures that use the independence model as back-ground knowledge have been suggested in [1, 3]. More flexi-ble models have been proposed, such as, comparing itemsets against graphical models [10] and local Maximum Entropy models [11, 17]. In addition, mining itemsets with low en-tropy has been suggested in [8].

Our main theoretical advantage over these approaches is that we look at the itemsets as a whole collection. For ex-ample, consider that we discover that item a and b deviate greatly from the null hypothesis. Then any itemset contain-ing both a and b will also be deemed interesting. The reason for this is that these methods are not adopting to the dis-covered fact that a and b are correlated, but instead they continue to use the same null hypothesis. We, on the other hand, avoid this problem by considering models: if itemset ab is found interesting that information is added into the statistical model. If this new model then explains bigger itemsets containing a and b , then we have no reason to add these itemsets, into the model, and hence such itemsets will not be considered interesting.

The idea of mining a pattern set as a whole in order to reduce the number of patterns is not new. For example, pattern reduction techniques based on minimum description length principle has been suggested [9, 15, 20]. Discovering decomposable models have been studied in [19]. In addi-tion, a framework that incrementally adopts to the patterns approved by the user has been suggested in [7]. Our main advantage is that these methods require already discovered itemset collection as an input, which can be substantially large for low thresholds. We, on the other hand, skip this step and define the significance for itemsets such that we can mine the patterns directly.
In this section we present our empirical evaluation of the measure. We first describe the datasets and the setup for the experiments, then present the results with synthetic datasets and finally the results with real-world datasets. We used 2  X  3 synthetic datasets and 3 real-world datasets.
The first three synthetic datasets, called Ind , contained 15 independent items and 100, 10 3 , and 10 4 transactions, respectively. We set the frequency for the individual items to be 0 . 1. The next three synthetic datasets, called Path , also contained 15 items. In these datasets, an item a i were generated from the previous one with P ( a i = 1 | a 1) = P ( a i = 0 | a i  X  1 = 0) = 0 . 75. The probability of the first item was set to 0 . 5. We set the number of transactions for these datasets to 100, 10 3 , and 10 4 , respectively.
Our first real-world dataset Paleo 2 contains information of species fossils found in specific paleontological sites in Europe [6]. The dataset Courses contains the enrollment records of students taking courses at the Department of Computer Science of the University of Helsinki. Finally, our last dataset is Dna is DNA copy number amplification data collection of human neoplasms [12]. We used 100 first
NOW public release 030717 available from [6]. items from this data and removed empty transactions. The basic characteristics of the datasets are given in Table 1.
For each data we sampled the models from the poste-rior distribution using techniques described in Section 3.4. We used singleton model as a starting point and did 5000 restarts. The number of required MCMC steps is hard to predictm since the structure of the state space of decompos-able models is complex. Further it also depends on the ac-tual data. Hence, we settle for heuristic: for each restart we perform 100 K log K MCMC steps, where K is the number of items. Doing so we obtained N = 5000 random models for each dataset. The execution times for sampling are given in Table 1. Let {F 1 ,..., F N } be the discovered models. We estimated the itemset score sc ( X )  X  X {F i | X  X  X  i }| /N and mined interesting itemsets using a simple depth-first approach.
 Table 1: Basic characteristics of the datasets. The fourth column contains the number of sample steps and the last column is the execution time.
Our main purpose for the experiments with synthetic data-sets is to demonstrate how the score behaves as a function of number of data points. To this end, we plotted the num-ber of significant itemsets, that is itemsets whose score was higher than the threshold  X  , as a function of the threshold  X  . The results are shown in Figures 2(a) and 2(b).
Ideally, for Ind , the dataset with independent variables we should have only 15 significant itemsets, that is, the sin-gletons, for any  X  &gt; 0. Similarly, for Path we should have 15 + 14 = 29 itemsets, the singletons and the pairs of form a a i +1 . We can see from Figures 2(a) and 2(b) that as we increase the number of transactions in data, the number of significant itemsets approaches these ideal cases, as pre-dicted by Theorem 7. The convergence to the ideal case is faster in Path than in Ind . The reason for this can be explained by the curse of dimensionality. In Ind we have 15  X  14 / 2 = 105 combinations of pairs of items. There is a high probability that some of these item pairs appear to be correlated. On the other hand, for Path , let us assume that we have the correct model. That is, the singletons and the 14 pairs a i a i +1 . The only valid itemsets of size 3 that we can add to this model are of the form a i a i +1 a i +2 . There are only 13 of such sets, hence the probability of finding such itemset important is much lower. Interestingly, in Path we actually benefit from the fact that we are using decomposable models instead of general exponential models.
Our first experiment with real-world data is to study the number of significant itemsets as a function of the threshold  X  . Figure 2(c) shows the number of significant itemsets for all three datasets. We see that the number of significant itemsets increases faster than for the synthetic datasets as the threshold decreases. The main reason for this difference, is that with real-world datasets we have more items and less transactions. This is seen especially in the Paleo dataset for which the number of significant itemsets increases steeply between the interval 0 . 4 X 1 . 0 when compared to Dna and Courses .

Our next experiment is to compare the score against base-lines, namely, the frequency fr ( X ) and entropy H ( X ). These comparisons are given in Figures 3(a) and 3(b). In addition, we computed the correlation coefficients (given in Table 2). From results, we see that sc ( X ) has a positive correlation with frequency and a negative correlation with entropy. The correlation with entropy is expected, since low-entropy im-plies that the empirical distribution of an itemset is different than the uniform distribution. Hence, using the frequency of such an itemset should improve the model and consequently the itemset is considered interesting.

Both datasets Paleo and Dna have a natural order be-tween the items, for example, for Paleo dataset it is the era when particular species was extant. Our next experiment is to test whether this order is represented in the discovered patterns. In our experiments, the items in these datasets were ordered using this natural order. Let X = a i a j be an itemset of size 2. In Figure 3(c), we plotted j  X  i as a func-tion of sc ( X ). We used only itemsets of size 2, since larger itemsets have inherently smaller scores and larger difference between the indices. In addition, we compute the correla-tion coefficients (given in Table 2). From the results we see that for Paleo and Dna the more significant itemsets tend to have a smaller difference between their indices. On the other hand, we do not discover any significant correlation in Courses .
 Finally, we report some of the discovered patterns from Courses . The 4 most significant itemsets of size 2 are ( Com-puter Architectures , Performance Analysis ) with a score of 0 . 95, ( Design &amp; Analysis of Algorithms , Principles of Func-tional Programming ) scoring 0 . 94, ( Database Systems II , In-formation Storage ) scoring 0 . 94, ( Three concepts: probabil-ity , Machine Learning ) scoring 0 . 92.
In this paper we introduced a novel and general approach for ranking itemsets. The idea behind the approach is to connect statistical models and collections of itemsets. This connection enables us to define the score of an itemset as the probability of itemset occurring in a random model. Doing so, we transformed the problem of mining patterns into a more classical problem of modeling.

As a concrete example of the framework, we used exponen-tial models. These models have many important theoretical and practical properties. The connection with itemsets is natural and the Occam X  X  razor inherent to these models can be used against the pattern explosion problem. Our experi-ments support the theoretical results and demonstrate that the measure works in practice.
Nikolaj Tatti is funded by FWO postdoctoral mandate. [1] C. C. Aggarwal and P. S. Yu. A new framework for [2] R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and [3] S. Brin, R. Motwani, and C. Silverstein. Beyond [4] C. Chow and C. Liu. Approximating discrete [5] R. G. Cowell, A. P. Dawid, S. L. Lauritzen, and D. J. [6] M. Fortelius, A. Gionis, J. Jernvall, and H. Mannila. [7] S. Hanhij  X  arvi, M. Ojala, N. Vuokko, K. Puolam  X  aki, [8] H. Heikinheimo, J. K. Sepp  X  anen, E. Hinkkanen, [9] H. Heikinheimo, J. Vreeken, A. Siebes, and [10] S. Jaroszewicz and D. A. Simovici. Interestingness of [11] R. Meo. Theory of dependence values. ACM Trans. [12] S. Myllykangas, J. Himberg, T. B  X  ohling, B. Nagy, [13] D. Pavlov, H. Mannila, and P. Smyth. Beyond [14] G. Schwarz. Estimating the dimension of a model. [15] A. Siebes, J. Vreeken, and M. van Leeuwen. Item sets [16] N. Tatti. Computational complexity of queries based [17] N. Tatti. Maximum entropy based significance of [18] N. Tatti. Probably the best itemsets. Technical Report [19] N. Tatti and H. Heikinheimo. Decomposable families [20] N. Tatti and J. Vreeken. Finding good itemsets by
