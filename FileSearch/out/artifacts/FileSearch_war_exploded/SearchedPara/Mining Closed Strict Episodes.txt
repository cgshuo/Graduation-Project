
Discovering frequent patterns in an event sequence is an important field in data mining. A pattern in a sequence is usually considered to be a set of events that reoccurs in the sequence within a window of a specified length. Gaps are allowed between the events and the order in which the events occur is often also considered important. Frequency, the number of sliding windows in which the episode occurs, is monotonically decreasing so we can use the well-known W
INEPI [1] method, essentially a level-wise approach, to mine all frequent episodes.

The order restrictions of an episode are described by a directed acyclic graph (DAG): the set of events in a sequence covers the episode if and only if each event occurs only after all its parent events (with respect to the DAG) have occurred (see the formal definition in Section II). Usually, only two extreme cases are considered. A parallel episode poses no restrictions on the order of events, and a window covers the episode if the events occur in the window, in any order. In such a case, the DAG associated with the episode contains no edges. The other extreme case is a serial episode. Such an episode requires that the events occur in one, and only one, specific order in the sequence. Clearly, serial episodes are more restrictive than parallel episodes. If a serial episode is frequent, then its parallel version is also frequent.
General episodes have, in practice, been over-shadowed by parallel and serial episodes, despite being defined at the same time [1]. The main reason for this is the pattern explosion demonstrated in the following example.

Example 1.1: As an example of pattern explosion we will use text data, namely inaugural speeches by presidents of the United States (see Section VI for more details). By setting the window size to 15 and the frequency threshold to 60 we discovered a serial episode with 6 symbols, ( preserv  X  protect  X  defend  X  constitut  X  unit  X  state ) . In total, we found another 4823 subepisodes of size 6 of this episode. However, all these episodes had only 3 distinct frequencies, indicating that the frequencies of most of them could be derived from the frequencies of only 3 episodes, so 4821 episodes could safely be left out of the output.
Motivated by this example, we approach the problem of pattern explosion by using a popular technique of closed patterns. A pattern is closed if there exists no more specific pattern with the same frequency. Mining closed patterns has been shown to reduce the output. Moreover, if we can estab-lish a specific property called the Galois connection, we can discover closed patterns efficiently. However, adopting the concept of closedness to episodes is not without problems. patterns we need a subset relation between patterns to describe whether a pattern G is a subpattern of pattern H . Essentially the same episode can be described by multiple DAGs and if we would base our definition of closedness simply on a subset relationship of DAGs we will run into problems as demonstrated in the following example.
 Example 1.2: Consider episodes G 1 , G 2 , and G 3 given in Figure 1. Episode G 1 states that for a pattern to occur a must precede b and c . G 2 and G 3 , meanwhile, state that a must be followed by b and then by c . Note that G 2 and G 3 represent essentially the same pattern that is more restricted than the pattern represented by G 1 . However, G 1 is a subgraph of G 3 but not a subgraph of G 2 . This reveals a problem if we base our definition of a subset relationship of episodes solely on the edge subset relationship. We solve this by generating only transitively closed graphs, thus ignoring graphs of form G 2 . We will not lose any generality since we are still going to discover episodes of form G 3 . isfy the Galois connection. In fact, given an episode G there can be several more specific closed episodes that have the same frequency. So the closure operator cannot be defined as a mapping from an episode to its frequency-closed version.
Example 1.3: Consider sequence s = abcbdacbcd and episode G 4 given in Figure 1(d). Assume that we use a sliding window of size 5 . There are two windows that cover episode G 4 , namely s 1  X  X  X  s 5 and s 6  X  X  X  s 10 . Hence, the frequency of G 4 is 2 . There are two serial episodes that are more specific than G 4 and have the same frequency, namely, H 1 = ( a  X  b  X  c  X  d ) and H 2 = ( a  X  c  X  b  X  d ) . Moreover, there is no superepisode of H 1 and H 2 that would has the frequency 2 . In other words, we cannot define a unique closure for G 4 .

The contributions of our paper address these issues: 1) We introduce strict episodes, a new subclass of general 2) We introduce a natural subset relation between 3) We introduce a milder version of the closure concept 4) Finally, we present an algorithm that generates
We begin by presenting the preliminary concepts and notations that will be used throughout the paper. In this section we introduce the notions of sequence and episodes.
A sequence s = ( s 1 ,...,s L ) is a string of symbols coming from an alphabet  X  , so that for each i , s i An episode G is represented by an acyclic directed graph with labelled nodes, that is, G = ( V,E,lab ) , where V = ( v 1 ,...,v K ) is the set of nodes, E is the set of directed edges, and lab is the function lab : V  X   X  , mapping each node v i to its label. We denote the set of nodes of an episode G with V ( G ) , and its set of edges with E ( G ) .
Given a sequence s and an episode G we say that s covers G if there is an injective map f mapping each node v i to a valid index such that the node v i in G and the corresponding sequence element s f ( v s f ( v i ) = lab ( v i ) , and that if there is an edge ( v parents of v j must occur in s before v j .

Episode mining is based on searching for episodes that are covered by windows of certain fixed size often enough. The frequency of a given episode is then defined as the number of such windows that cover it.

We now provide a canonical form for episodes, which will help us in further theorems and algorithms. We define an episode that has the maximal number of edges using a fundamental notion familiar from graph theory.

Definition 2.1: The transitive closure of an episode G = ( V,E,lab ) is an episode tcl ( G ) , where G and tcl ( G ) have the same set of nodes V , the same lab function mapping nodes to labels, and the set of edges in tcl ( G ) is equal to
E ( tcl ( G ))
Note that, despite its name, the transitive closure has nothing to do with the concept of closed episodes.

Generally, a pattern is considered closed if there exists no more specific pattern having the same frequency. In order to speak of more specific patterns, we must first have a way to describe episodes in these terms. In this section we define a subset relationship among episodes, that would allow us to describe one episode as more specific than another one. Definition 3.1: Assume two transitively closed episodes G and H with the same number of nodes. An episode G is called a subset of episode H , denoted G H if the set of all sequences that cover H is a subset of the set of all sequences that cover G . If G is a proper subset of H , we denote G  X  H . If | V ( G ) | &lt; | V ( H ) | , then G is a subset of H if there is a subgraph H 0 of H such that G H 0 .

The problem with this definition is that we do not have the means to compute this relationship for general episodes. To do this, one would have to enumerate all possible sequences that cover H and compute whether they cover G . We approach this problem by restricting ourselves to a class of episodes where this comparison can be performed efficiently.
Definition 3.2: An episode G is called strict if for any two nodes v and w in G sharing the same label, there exists a path either from v to w or from w to v .

Using strict episodes will also allow us to build an algorithm to efficiently discover closed episodes. In the remaining text, we consider episodes to be strict. However, as can be seen in Figure 2, this, unfortunately, means that some episodes will never be discovered.
For notational simplicity, we now introduce the concept of two episodes having identical nodes . Given an episode G with nodes V ( G ) = { v 1 ,...,v N } , we assume from now on that the order of the nodes is always fixed such that for i &lt; j either lab ( v i ) &lt; lab ( v j ) , or lab ( v i ) = lab ( v ancestor of v j . We say that two episodes G and H , with V ( G ) = { v 1 ,...,v N } and V ( H ) = { w 1 ,...,w N identical nodes if lab ( v i ) = lab ( w i ) . To simplify notation, we often identify v i and w i .

Crucially, we can easily compute the subset relationship between two episodes, if they have identical nodes.

Theorem 3.3: For transitively closed episodes G and H with identical nodes, E ( G )  X  E ( H ) if and only if G H . E ( G )  X  E ( H ) . Let s = { s 1 ,...,s N } be a sequence covering H and let f be the corresponding mapping. Then f is also a valid mapping for G . Thus, G H .
 To prove the other direction, assume that E ( G ) * E ( H ) . We therefore must have an edge e = ( x,y )  X  E ( G ) , such otherwise H would not be strict. We know that for every node v it holds that the number of ancestors of v in G having the label lab ( v ) is equal to the number of such ancestors of v in H . To prove that G H we will construct a sequence s that covers H but not G . We build s by first visiting every parent of y in H in a valid order with respect to H , then y itself, and then the rest of the nodes, also in a valid order. This sequence covers H . Let s i be the event corresponding to x and let s j be the event corresponding to y . Assume now that s covers G so that there is a map f mapping the nodes of G to indices of s . Let k be the number of ancestors of x having the same label as x in H . Let l be the number of descendants having the same label as x in H . Since s covers H and lab ( x ) 6 = lab ( y ) we must have k occurrences of lab ( x ) events before s i and l occurrences after. Let v be such that f ( v ) = s i . We see that v must also have k ancestors with the same label so we must have v = x , and f ( x ) = i . Similarly, we have f ( y ) = j . This is a contradiction since ( x,y )  X  E ( G ) but j &lt; i .
Note that we do not need to define a subset relation for episodes that do not have identical nodes, as will be explained in detail in Section V.

We now define what we exactly mean when we say that two episodes are essentially the same.

Definition 3.4: Episodes G and H are said to belong to the same class , denoted by G  X  H , if each sequence that covers G also covers H , and vice versa.

Corollary 3.5 (of Theorem 3.3): For transitively closed episodes G and H , G  X  H  X  E ( G ) = E ( H ) .
 equivalent to G H and H G , and that E ( G ) = E ( H ) is equivalent to E ( G )  X  E ( H ) and E ( H )  X  E ( G ) .
Note that by generating only transitively closed strict episodes, we have obtained an efficient way of computing the subset relationship between two episodes. At first glance, though, it may seem that we have completely omitted certain parallel episodes from consideration  X  namely, all non-strict parallel episodes (i.e. those containing multiple nodes with same labels). Note, however, that for each such episode G , there exists a strict episode H , such that G  X  H . To build such an episode H , we just need to create edges that would strictly define the order among nodes with the same labels. From now on, when we talk of parallel episodes, we actually refer to their strict equivalents.

Having defined a subset relationship among episodes, we are now able to speak of an episode being more specific than another episode. However, this is only the first step towards defining the closure of an episode. We know that the closure must be more specific, but it must also be unique and well-defined. We have already seen that basing such a closure on the frequency fails, as there can be multiple more specific closed episodes that could be considered as closures. In this section, we will base the closure on another criterion, which will result in each episode having a unique closure.
We begin by associating each sequence with a correspond-ing serial episode.

Definition 4.1: Given a sequence s = ( s 1 ,...,s N ) , we define its corresponding serial episode G s as the transitive closure of with lab v m ( i ) = s i . Mapping m makes sure that the nodes of G s are ordered, that is, for i &lt; j , lab ( v i )  X  lab ( v
Based on this definition, we can now build a maximal episode that is covered by a set of sequences.

Definition 4.2: Given a set of nodes V , and a set S of sequences containing the events corresponding to labels of nodes in V , we define the maximal episode covered by set S as the episode H , where V ( H ) = V and E ( H ) = T
We now show that we can make a Galois connection between the set of all episodes with a fixed set of nodes V and the power set S containing all sets of subsequences consisting only of labels of nodes V in all windows of length k in our sequence s . For all episodes G containing nodes V , we define a function f as For all sets of subsequences S in S , we define a function g as g ( S ) = G , with G the maximal episode covered by S .
Theorem 4.3: Given a set of nodes V , a sequence s , and the power set S containing all sets of subsequences consisting only of labels of nodes V in all windows of length k in s , f and g , as defined above, satisfy the Galois connection: S  X  f ( G )  X  G g ( S ) .
 sequences in S cover G . g ( S ) gives us the maximal episode that covers all sequences in S . Clearly, G must be a subset of such an episode.

Assume now g ( S ) = H and that G H . Therefore, all sequences that cover H also cover G . We know that all sequences in S cover H , and, therefore, G , so S must be a subset of f ( G ) , a set of all sequences that cover G . We are now ready to define closure using f and g .

Definition 4.4: We define the instance-closure , or i-closure of an episode G , denoted icl ( G ) , as g ( f ( G )) . We say an episode G is i-closed , if g ( f ( G )) = G .
The following example demonstrates how f and g work in practice.

Example 4.5: Consider sequence s = abcbdxyacbcd and the parallel episode G = ( a,b,c,d ) , given that the cho-sen window length is 5. We begin with a set of nodes V = { v 1 ,v 2 ,v 3 ,v 4 } , labelled a , b , c and d respectively. f ( G ) is defined as the set of all subsequences consisting of nodes V in all windows of length 5 that cover G . In our example, there are two windows of length 5 that cover G , s 1  X  X  X  s 5 and s 8  X  X  X  s 12 , and each window contains the same two subsequences that satisfy our criteria. Therefore, f ( G ) = { abcd,acbd } . The serial episodes corresponding to these two subsequences are the transitive closures of ( a  X  b  X  c  X  d ) and ( a  X  c  X  b  X  d ) , so g ( f ( G )) , or icl ( G ) , is obtained by taking the intersection of the edges of these two serial episodes, and is given in Figure 1(d).
As we intend to allow the user to also obtain closed episodes based on frequency, we now provide a formal definition of such episodes.

Definition 4.6: An episode G is frequency-closed , or f-closed , if there exists no episode H , such that G  X  H and fr ( G ) = fr ( H ) .

Note that, unlike the i-closure, we do not define an f-closure of an episode at all. As shown in Section I, such an f-closure would not necessarily be unique. For this reason, our algorithm identifies instance-closed episodes. Should the user wish to find only frequency-closed episodes, we provide this option as a post-pruning step. The following proposition proves that such a step is possible.

Proposition 4.7: A frequency-closed episode is always instance-closed.
 not instance-closed. Note that the definition of i-closure effectively says that if G is not i -closed, then there exists an episode H , such that G  X  H , and for each possible mapping of G in each window of length k in the sequence s , this mapping is also a mapping of H . Clearly, in this case, fr ( G ) = fr ( H ) , which is a contradiction.

In this section we will present our mining algorithm for discovering closed episodes. First we give an overview of the algorithm. Next, we explain in detail how candidate episodes are tested before being scanned. We continue by presenting techniques for generating the candidate episodes. Finally, we discuss how closures are computed in practice.
 A. Overview of the algorithm
We showed in Section IV that icl ( G ) has a Galois connection. This allows us to use a standard level-wise approach for mining closed patterns (see, for example, [2]). The outline of the algorithm is given in Algorithm 1.
The algorithm consists of two loops. In the outer loop we discover parallel episodes. For each parallel episode we call M
INE DAG given in Algorithm 2. M INE DAG discovers all general episodes in a level-wise fashion by adding edges. During the generation M INE DAG calls G ENERATE C AN DIDATE (Algorithm 3) which ensures that the candidates are transitively closed. In the next step, for each candidate episode, we test whether all its subepisodes are frequent, and that the candidate episode is not contained in the closure of one of its subepisodes (see T EST C ANDIDATE in Algorithm 4). Finally, we compute the frequency, and if the episode is frequent, we compute its instance-closure.
Mining episodes requires an additional step that does not occur in mining itemsets: we need to add some episodes that are not generators as candidates. This step is done on Line 13 of M INE DAG and is explained in detail in Section V-D. B. Generating Transitively Closed Candidate Episodes
Theorem 3.3 implies that if we generate only transitively closed episodes, then the subset relationship between the episodes is simply the subset relationship between the edges. In this section we define an algorithm, G ENERATE C
ANDIDATE , which generates the candidate episodes from the episodes discovered previously. G ENERATE C ANDIDATE makes sure that the candidates are transitively closed. These candidates are then tested by the T EST C ANDIDATE algo-rithm described in the next section. If the candidates pass the test they are tested for frequency.

To describe G ENERATE C ANDIDATE we need the follow-ing definition.

Algorithm 1: M INE E PISODES . An algorithm discover-ing all frequent closed episodes. input : Sequence s . Frequency threshold. output : f -closed frequent episodes.
G  X  frequent episodes with one node; while G is not empty do 3 H X  next level of parallel frequent episodes 4 foreach G  X  X  do M INE DAG ( G ) ; 5 G  X  X  ; return F-C LOSURE (episodes outputted by M INE DAG)
Algorithm 2: M INE DAG. An algorithm that discovers frequent episodes from a fixed parallel episode. input : Parallel episode G . output : i -closed frequent episodes having identical
G 1  X  frequent episodes with one edge and
N  X  X  V ( G ) | ( | V ( G ) | X  1) / 2 ; foreach i = 1 ,...,N do 5 foreach H  X  X  do 6 if T EST C ANDIDATE ( H ) and H is frequent 8 icl ( H )  X  I-C LOSURE ( H ) ; 9 output icl ( H ) ; 10 foreach e /  X  icl ( H ) do 11 Z  X  E ( tcl ( H + e ))  X  E ( H + e ) ; 12 if Z 6 =  X  and Z  X  E ( icl ( H )) then
Definition 5.1: An edge ( v,w ) in a transitively closed episode G is called a skeleton edge if there is no node u such that ( v,u,w ) is a path in G . If v and w have different labels, we call the edge ( v,w ) a proper skeleton edge .
As pointed out in Section V-A we will first generate parallel episodes and then in a level-wise fashion add edges. Let G be a transitively closed episode. It is easy to see that if we remove a proper skeleton edge e from G , then the resulting episode G  X  e will be transitively closed. We can reverse this property in order to generate candidates: Let G be a previously discovered transitively closed episode, add an edge e and verify that the new episode is transitively closed. However, we can improve on this naive approach with the following proposition describing the sufficient and necessary condition for an episode to be transitively closed.
Proposition 5.2: Let G be a transitively closed episode and let e = ( x,y ) be an edge not in E ( G ) . Let H = G + e . Assume that H is a DAG. Then H is transitively closed if and only if there is an edge in G from x to every child of y and from every parent of x to y .
 definition of transitive closure. To prove the  X  X f X  part, we will use induction. Let u be an ancestor node of v in H . Then there is a path from u to v in H . If the path does not use edge e , then, since G is transitively closed, ( u,v )  X  E ( G ) and hence ( u,v )  X  E ( H ) . Assume now that the path uses e . If v = y , then u must be a parent of y in G , since G is transitively closed, so the condition implies that ( u,v )  X  E ( G )  X  E ( H ) . Assume that v is a descendant of y in H . To prove the first step in the induction, assume that v = x , then again ( u,v )  X  E ( G ) . To prove the induction step, let w be the next node along the path from u to v in H . Assume that ( w,v )  X  E ( G ) . Then the path ( u,w,v ) occurs in G , so ( u,v )  X  E ( G ) , which completes the proof.
We now show when we can join two episodes to obtain a candidate episode.

Theorem 5.3: Let G 1 and G 2 be two transitively closed episodes with identical nodes and N edges. Assume that G 1 and G 2 share N  X  1 mutual edges. Let e 1 = ( x 1 ,y 1 )  X  E ( G 1 )  X  E ( G 2 ) be the unique edge for G 1 and let e ( x 2 ,y 2 )  X  E ( G 2 )  X  E ( G 1 ) be the unique edge of G 2 H = G 1 + e 2 . Assume that H has no cycles. Then H is transitively closed if and only if one of the following conditions is true 1) x 1 6 = y 2 and x 2 6 = y 1 . 2) x 1 6 = y 2 , x 2 = y 1 , and ( x 1 ,y 2 ) is an edge in G 3) x 1 = y 2 , x 2 6 = y 1 , and ( x 2 ,y 1 ) is an edge in G Moreover, if H is transitively closed, then e 1 is a skeleton edge in H .
 H . If it is not, then there is a path from x 1 to y 1 in H not using e 1 . The edges along this path also occur in G 2 , thus forcing e 1 to be an edge in G 2 , which is a contradiction. The  X  X nly if X  part is trivial so we only prove the  X  X f X  part. Let v be a child of y 2 in G 1 and f = ( y 2 ,v ) an edge in G
If the first or second condition holds, then x 1 6 = y 2 , and consequently f 6 = e 1 , so f  X  G 2 . The path ( x 2 ,y 2 nects x 2 and v in G 2 so there must be an edge h = ( x 2 in G 2 . Since h 6 = e 2 , h must also occur in G 1 . If the third condition holds, it may be the case that f = e 1 (if not, then we can use the previous argument). But in such a case v = y 1 and the edge h = ( x 2 ,y 1 ) occurs in G 1 .
If now u is a parent of x 2 in G 1 , we can make a similar argument that u and y 2 are connected, so Proposition 5.2 now implies that H is transitively closed.

Theorem 5.3 provides with the means to generate tran-sitively closed episodes in the following manner. Since our nodes are ordered, we can also order the edges using a lexicographical order. Given an episode G we define last ( G ) to be the last proper skeleton edge in G . Let H be a transitively closed episode. Let e 2 = last ( H ) be its last proper skeleton edge. Define G 1 = H  X  e 2 . Let e 1 = last ( G 1 ) be the last proper skeleton edge in G and assume that e 1 is also a proper skeleton edge in H . Then in order for H to be transitively closed, G 1 and G defined as H  X  e 1 , must satisfy one of the conditions given in Theorem 5.3.

In other words, to generate a candidate, we take two previously discovered episodes with identical nodes , say G 1 and G 2 , with N edges. Let e 1 and e 2 be the last proper skeleton edges in G 1 and in G 2 , respectively. Assume that e &lt; e 2 and that G 1 and G 2 share the rest of the edges. Then if G 1 + e 2 satisfies one of the conditions in Theorem 5.3, we will generate it for the next stage.

This approach will not generate all candidates. The crucial assumption we made above is that e 1 is also a skeleton edge in H . Hence to generate all candidates we also need to generate episodes from G 1 such that e 1 , the last proper skeleton edge of G 1 , is no longer a skeleton edge in G 1
Theorem 5.4: Let G be a transitively closed episode, let e = ( x 1 ,y 1 ) be a skeleton edge of G , and let e 2 = ( x be an edge not occuring in G and define H = G + e 2 Then H is a transitively closed episode such that e 1 is not a skeleton edge in H only if either y 2 = y 1 and ( x 1 ,x a skeleton edge in G or x 1 = x 2 and ( y 2 ,y 1 ) is a skeleton edge in G .
 H , then there is a path of skeleton edges going from x to y 1 in H not using e 1 . The path must use e 2 , otherwise we have a contradiction. The theorem will follow if we can show that the path must have exactly two edges. Assume otherwise. Assume, for simplicity, that the edge e 2 does not occur first in the path and let z be the node before x 2 in the path. Then we can build a new path by replacing the edges ( z,x 2 ) and e 2 with ( z,y 2 ) . This path does not use e it occurs in G , making e 1 a non-skeleton edge in G , which is a contradiction. If e 2 is the first edge in the path, we can select the next node after y 2 and repeat the argument.
We can now combine Theorem 5.3 and Theorem 5.4 into the G ENERATE C ANDIDATE algorithm given in Algorithm 3. We will first generate candidates by combining episodes from the previous rounds using Theorem 5.3. Secondly, we use Theorem 5.4 and for each episode from the previous rounds we add edges such that the last proper skeleton edge is no longer a skeleton edge in the candidate.
 C. Testing the Candidate Episode
Following the level-wise discovery, before computing the frequency of the episode, we need to test that all its subepisodes are discovered. Using transitively closed episodes has another important benefit.

Corollary 5.5 (of Theorem 3.3): Let G be a transitively closed episode. Let e be a proper skeleton edge of G . If H
Algorithm 3: G ENERATE C ANDIDATE . Generates candi-date episodes from the previously discovered episodes. input : A collection of previosly discovered episodes output : A collection of i -closed candidate episodes foreach G 1  X  X  do 2 e 1 = ( x 1 ,y 1 )  X  last ( G 1 ) ; 3 H X  H  X  X  4 foreach G 2 in H do 5 e 2  X  last ( G 2 ) ; 6 if G 1 and G 2 satisfy Thr. 5.3 and e 2 /  X  icl ( G 1 ) 7 foreach f = ( x 1 ,x 2 ) skeleton edge in G 1 such that 9 if e 2 /  X  icl ( G 1 ) then 11 if e 2 = last ( H ) and Prop. 5.2 holds then 12 output G 1 + e 2 ; 13 foreach f = ( y 2 ,y 1 ) skeleton edge in G 1 such that 15 if e 2 /  X  icl ( G 1 ) then 17 if e 2 = last ( H ) and Prop. 5.2 holds then 18 output G 1 + e 2 ; is an episode obtained by removing e from G , then there exists no episode H 1 , such that H  X  H 1  X  G .

Corollary 5.5 implies that using transitively closed episodes will guarantee the strongest conditions for an episode to pass to the frequency computation stage.
 If e is a skeleton edge of a transitively closed episode G , then G  X  e is transitively closed. Thus, for G to be frequent, G  X  e had to be discovered previously. This is the first test in T EST C ANDIDATE (given in Algorithm 4). In addition, following the level-wise approach for mining closed patterns [2], we test that G is not a subepisode of icl ( G  X  e ) , and if it is, then we can discard G .
The second test involves testing whether G  X  v , where v is a node in G , has also been discovered. Note that G  X  v has less nodes than G so, if G is frequent, we must have discovered G  X  v . Not all nodes need to be tested. If a node v has an adjacent proper skeleton edge, say e , then the episode G  X  e has a frequency lower than or equal to that of G  X  v . Since we have already tested G  X  e we do not need to test G  X  v . Consequently, we need to test only those nodes that have no proper skeleton edges. This leads us to the second test in T EST C ANDIDATE . Note that these nodes will either have no edges, or will have edges to the nodes having the same label. If both tests are passed we test the candidate episode for frequency.

Algorithm 4: T EST C ANDIDATE . An algorithm that checks if an episode is a proper candidate. input : An episode G . output : Boolean value, true if all subepisodes of G are foreach proper skeleton edge e in G do 2 if G  X  e is not discovered or e  X  E ( icl ( G  X  e )) foreach v in G not having a proper skeleton edge do 4 if G  X  v is not discovered then return false ; return true ; D. Proof of Correctness
In this section we will prove that all frequent i -closed episodes are discovered. We will prove this by induction over the number of edges. To that end, we say that a skeleton edge e in an episode G is derivable if there is a subepisode H such that e  X  E ( icl ( H ))  X  E ( H ) . Note that Lemma 2 in [2] implies that icl ( G ) = icl ( G  X  e ) . Hence, it is enough to show that either G has derivable edges or it is discovered. An episode G is discovered if an episode G 0 = G  X  e is discovered for each skeleton edge e . If G 0 does not contain derivable edges then, by the induction assumption, it is discovered. If it has derivable edges, they turn into non-skeleton edges by adding e . Start removing derivable edges, one by one, until you reach an episode H without derivable edges. It is easy to see that all removed edges are part of icl ( H ) . H is discovered due to the induction assumption and G 0 is discovered due to Line 13 in M INE DAG.
 E. Computing Closures
During the mining we need to compute the closure of an episode. We do this by discovering all possible valid instances of an episode in the sequence and using Def-inition 4.2. In order to discover the instances efficiently, we enumerate recursively all possible serial episodes H by removing sources (nodes without incoming edges) from the candidate episode G , such that G  X  H .

More specifically, assume that we have an episode G and that we have already removed K sources in the order ( n 1 ,...,n K ) . For each source v in G , we first test whether there are instances ( lab ( n 1 ) ,...,lab ( n K ) ,lab ( v )) in the sequence. If there are, we set n K +1 = v and test recursively G  X  v . Once G is empty, we have discovered a subsequence and its corresponding serial episode H such that H G . The caveat of this approach is that the number of such serial episodes can be exponential. However, our experiments demonstrate that this is not a problem in practice. The pseudo-code is given in Algorithms 5 and 6.

Algorithm 5: I-C LOSURE . An algorithm for computing the i -closure of an episode G . The parameter  X  is the size of the window. input : An episode G . output : i -closure of G . foreach v  X  sources ( G ) do 2 W  X  X  ( s i ) | s i = lab ( v ) } ; 3 if W 6 =  X  then 4 F IND S ERIALS ( G  X  v,W, X  ) ;
W  X  all sequences outputted by F IND S ERIALS ;
V ( H )  X  V ( G ) ;
E ( H )  X  T w  X  W E ( G w ) ; return H ;
Algorithm 6: F IND S ERIALS ( G,W, X  ) . A recursive sub-routine used by I-C LOSURE . Discovers all instances of episode G . input : An episode G . Partial candidate occurences W output : Instances of G in the sequences. if G has no nodes then 2 output any sequence in W ; foreach v  X  sources ( G ) do 4 R  X  X  X  ; 5 foreach w  X  W do 6 i  X  index of the first element in w ; 7 if there is s j s.t. lab ( v ) = s j and j  X  i &lt;  X  then 8 Add w concatenated with s j into R ; 9 if R 6 =  X  then
After the actual mining process we can further reduce the output by keeping only frequency-closed episodes. A naive approach would be to compare each pair of instance-closed episodes G and H and if fr ( G ) = fr ( H ) and G  X  H , remove G from output. This approach can be considerably sped up by realizing that we need only to test episodes with identical nodes and episodes of form G  X  v . The pseudo-code is given in Algorithm 7. The algorithm can be further sped up by exploiting the subset relationship between the episodes. Our experiments demonstrate that this comparison is feasible in practice.

Algorithm 7: F-C LOSURE . Postprocessing for comput-ing f -closed episodes from i -closures. input : i -closed episodes C . output : f -closed episodes. foreach G  X  X  do 2 foreach H  X  X  with V ( G ) = V ( H ) , H 6 = G do 3 if G  X  H and fr ( G ) = fr ( H ) then Mark G ; 4 if H  X  G and fr ( G ) = fr ( H ) then Mark H ; 5 foreach v  X  V ( G ) do 6 F  X  G  X  v ; 7 foreach H  X  X  , with V ( F ) = V ( H ) do 8 if H F and fr ( G ) = fr ( H ) then 9 Mark H ; return all unmarked episodes;
We tested our algorithm 1 on three text datasets, address , consisting of the inaugural addresses by the presidents of the United States 2 , merged to form a single long sequence, moby , the novel Moby Dick by Herman Melville 3 , and abstract , consisting of the first 739 NSF award abstracts from 1990 4 , also merged into one long sequence. All three sequences were preprocessed using the Porter Stemmer 5 and the stop words were removed.

We used a window of size 15 for all our experiments and varied the frequency threshold  X  . The main goal of our experiments was to demonstrate how we tackle the problem of pattern explosion. Figures 3(a), 3(b) and 3(c) show how the total number of frequent episodes compared with the identified i -closed and f -closed episodes we discovered in the three datasets. The results suggest that improvement is only visible at small thresholds and is less than a factor of 10 . The reason for this is that the major part of the output consists of episodes with a small number of nodes. Such episodes tend to be closed.

To get a more detailed picture we examined the ratio of the number of frequent episodes and the number of f -closed episodes (Figure 4(a)) and the ratio of the number of i -closed episodes and the number of f -closed episodes (Figure 4(b)) as a function of the number of nodes. We see that while there is no improvement with small episodes, using closed episodes is essential if we are interested in large episodes. In such a case we were able to reduce the output by several orders of magnitude. For example, in the address dataset, with a threshold of 30, there were 1226 frequent episodes of size 7, of which only 2 were f -closed. Clearly, the number of discovered i -closed episodes remains greater than the number of f -closed episodes, but does not explode, guaranteeing the feasibility of our algorithm. For example, in the abstract dataset, with a threshold of 200, there were 15976 frequent episodes of size 5, of which 912 were i -closed and 250 f -closed.
The runtimes of our experiments varied between a few seconds and 30 minutes for the largest experiments. How-ever, with low thresholds, our algorithm for finding closed episodes ran faster than the algorithm for finding all frequent episodes, and at the very lowest thresholds, our algorithm produced results, while the frequent-episodes algorithm ran out of memory. This demonstrates the infeasibility of approaching the problem by first generating all frequent episodes, and then pruning the non-closed ones. The i -closed episodes are the necessary intermediate step.

Searching for frequent patterns in data is a very common data mining problem. The first attempt at discovering se-quential patterns was made by Wang et al. [3]. There, the dataset consists of a number of sequences, and a pattern is considered interesting if it is long enough and can be found in a sufficient number of sequences. The method proposed in this paper, however, was not guaranteed to discover all interesting patterns, but a complete solution to a more general problem (dropping the pattern length constraint) was later provided by Agrawal and Srikant [4] using an A PRIORI -style algorithm [5].

It has been argued that not all discovered patterns are of interest to the user, and some research has gone into outputting only closed sequential patterns, where a sequence is considered closed if it is not properly contained in any other sequence which has the same frequency. Yan et al. [6], Tzvetkov et al. [7], and Wang and Han [8] proposed methods for mining such closed patterns, while Garriga [9] further reduced the output by post-processing it and representing the patterns using partial orders. Despite their name, the patterns discovered by Garriga are different from the tra-ditional episodes. A sequence covers an episode if every node of the DAG can be mapped to a symbol such that the order is respected, whereas a partial order discovered by Garriga is covered by a sequence if there is a subsequence corresponding to a path in the DAG from a source node to a sink node, that is, not all nodes need to be visited.
In another attempt to trim the output, Garofalakis et al. [10] proposed a family of algorithms called S PIRIT which allow the user to define regular expressions that specify the language that the discovered patterns must belong to.
Looking for frequent episodes in a single event sequence was first proposed by Mannila et al. [1]. The W INEPI algorithm finds all episodes that occur in a sufficient number of windows of fixed length. The frequency of an episode is defined as the fraction of all fixed-width sliding win-dows in which the episode occurs. The user is required to choose the width of the window and a frequency threshold. Specific algorithms are given for the case of parallel and serial episodes. However, no algorithm for detecting general episodes (DAGs) is provided.

The same paper proposes the M INEPI method, where the interestingness of an episode is measured by the number of minimal windows that contain it. As was shown by Tatti [11], M INEPI fails due to an error in its definition. Zhou et al. [12] proposed mining closed serial episodes based on the M INEPI method, without solving this error. Laxman et al. introduced a monotonic measure as the maximal number of non-overlapping occurrences of the episode [13].

Pei et al. [14] considered a restricted version of our problem setup. In their setup, items are allowed to occur only once in a window (string in their terminiology). This means that the discovered episodes can contain only one occurence of each item. This restriction allows them to easily construct closed episodes. Our setup is more general since we do not restrict the number of occurences of a symbol in the window and the miner introduced by Pei cannot be adapted to our problem setting since the restriction imposed by the authors plays a vital part in their algorithm.

Garriga [15] pointed out that W INEPI suffers from bias against longer episodes, and proposed solving this by in-creasing the window length proportionally to the episode length. However, as was pointed out by M  X  eger and Rigotti [16], the algorithm given in this paper contained an error.
An attempt to define frequency without using any win-dows has been made by Calders et al. [17] where the authors define an interestingness measure of an itemset in a stream to be the frequency starting from a point in time that maximizes it. However, this method is defined only for itemsets, or parallel episodes, and not for general episodes. Cule et al. [18] proposed a method that uses neither a window of fixed size, nor minimal occurrences, and an interestingness measure is defined as a combination of the cohesion and the frequency of an episode  X  again, only for parallel episodes. Tatti [11] and Gwadera et al. [19], [20] define an episode as interesting if its occurrences deviate from expectations.
Finally, an extensive overview of temporal data mining has been made by Laxman and Sastry [21].

In this paper, we tackled the problem of pattern explosion when mining frequent episodes in an event sequence. In such a setting, much of the output is redundant, as many episodes have the same frequency as some other, more specific, episodes. We therefore output only closed episodes, for which this is not the case. Further redundancy is found in the fact that some episodes can be represented in more than one way. We solve this problem by restricting ourselves to strict, transitively closed episodes.

Defining frequency-closed episodes created new prob-lems, as, unlike in some other settings, a non-closed frequent episode can have more than one closure. To solve this, we defined instance-closed episodes, and showed that the instance-closure of any given episode is unique. We further proved that every f -closed episode must also be i -closed. Based on this, we developed an algorithm that efficiently identifies i -closed episodes, as well as f -closed episodes, in a post-processing step. Experiments have confirmed that the reduction in output is considerable, and essential for large episodes, where we reduced the output by several orders of magnitude. Moreover, thanks to introducing i -closed episodes, we can now produce output for thresholds at which finding all frequent episodes is infeasible. Nikolaj Tatti is funded by a FWO postdoctoral mandate.
