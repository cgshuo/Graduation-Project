 The task of selecting and ordering information ap-pears in multiple conte xts in text generation and summarization. For instance, a typical multidocu-ment summarization system creates a summary by selecting a subset of input sentences and ordering them into a coherent text. Selection and ordering at the word level is commonly emplo yed in lexical re-alization. For instance, in the task of title generation, the headline is constructed by selecting and ordering words from the input text.

Decoding is an essential component of the selection-and-ordering process. Given selection and ordering preferences, the task is to nd a sequence of elements that maximally satises these preferences. One possible approach for nding such a solution is to decompose it into two tasks: rst, select a set of words based on indi vidual selection preferences, and then order the selected units into a well-formed sequence. Although the modularity of this approach is appealing, the decisions made in the selection step cannot be retracted. Therefore, we cannot guarantee that selected units can be ordered in a meaningful way, and we may end up with a suboptimal output.
In this paper , we investig ate decoding methods that simultaneously optimize selection and order -ing preferences. We formalize decoding as nd-ing a path in a directed weighted graph. 1 The vertices in the graph represent units with associ-ated selection scores, and the edges represent pair -wise ordering preferences. The desired solution is the highest-weighted acyclic path of a prespecied length. The requirement for acyclicity is essential because in a typical selection-and-ordering problem, a well-formed output does not include any repeated units. For instance, a summary of multiple docu-ments should not contain any repeated sentences.
Since the problem is NP-hard, nding an exact solution is challenging. We introduce a novel ran-domized decoding algorithm 2 based on the idea of color -coding (Alon et al., 1995). Although the algo-rithm is not guaranteed to nd the optimal solution on any single run, by increasing the number of runs the algorithm can guarantee an arbitrarily high prob-ability of success. The paper pro vides a theoretical analysis that establishes the connection between the required number of runs and the lik elihood of nd-ing the correct solution.

Ne xt, we sho w how to nd an exact solution using an inte ger linear programming (ILP) formulation. Although ILP is NP-hard, this method is guaranteed to compute the optimal solution. This allo ws us to experimentally investig ate the trade-of f between the accurac y and the efcienc y of decoding algorithms considered in the paper .

We evaluate the accurac y of the decoding algo-rithms on the task of title generation. The decod-ing algorithms introduced in the paper are compared against beam search, a heuristic search algorithm commonly used for selection-and-ordering and other natural language processing tasks. Our experiments sho w that the randomized decoder is an appealing al-ternati ve to both beam search and ILP when applied to selection-and-ordering problems. In this section, we formally dene the decoding task for selection-and-ordering problems. First, we intro-duce our graph representation and sho w an example of its construction for multidocument summariza-tion. (An additional example of graph construction for title generation is given in Section 6.) Then, we discuss the comple xity of this task and its connec-tion to classical NP-hard problems. 2.1 Graph Repr esentation We represent the set of selection units as the set of vertices V in a weighted directed graph G . The set of edges E represents pairwise ordering scores between all pairs of vertices in V . We also add a special source verte x s and sink verte x t . For each verte x v in V , we add an edge from s to v and an edge from v to t . We then dene the set of all ver-tices as V  X  = V [f s,t g , and the set of all edges as E  X  = E [f ( s,v ) 8 v 2 V g[f ( v,t ) 8 v 2 V g .
To simplify the representation, we remo ve all ver-tex weights in our graph structure and instead shift the weight for each verte x onto its incoming edges. For each pair of distinct vertices ( v,u ) 2 V , we set the weight of edge e v,u to be the sum of the log a-rithms of the selection score for u and the pairwise ordering score of ( v,u ) .

We also enhance our graph representation by grouping sets of vertices into equivalence classes . We introduce these classes to control for redundanc y as required in man y selection-and-ordering prob-lems. 3 For instance, in title generation, an equi va-lence class may consist of morphological variants of the same stem (i.e., examine and examination ). Be-cause a typical title is unlik ely to contain more than one word with the same stem, we can only select a single representati ve from each class.

Our task is now to nd the highest weighted acyclic path starting at s and ending at t with k ver-tices in between, such that no two vertices belong to the same equi valence class. 2.2 Example: Decoding for Multidocument In multidocument summarization, the vertices in the decoding graph represent sentences from input doc-uments. The vertices may be organized into equi va-lence classes that correspond to clusters of sentences con veying similar information. The edges in the graph represent the combination of the selection and the ordering scores. The selection scores encode the lik elihood of a sentence to be extracted, while pair -wise ordering scores capture coherence-based prece-dence lik elihood. The goal of the decoder is to nd the sequence of k non-redundant sentences that op-timize both the selection and the ordering scores. Finding an acyclic path with the highest weight will achie ve this goal. 2.3 Relation to Classical Pr oblems Our path-nding problem may seem to be simi-lar to the tractable shortest paths problem. Ho w-ever, the requirement that the path be long mak es it more similar to the the Traveling Salesman Problem (TSP). More precisely , our problem is an instance of the prize collecting traveling salesman problem , in which the salesman is required to visit k vertices at best cost (Balas, 1989; Awerb uch et al., 1995).
Since our problem is NP-hard, we might be pes-simistic about nding an exact solution. But our problem has an important feature: the length k of the path we want to nd is small relati ve to the num-ber of vertices n . This feature distinguishes our task from other decoding problems, such as decoding in machine translation (Germann et al., 2001), that are modeled using a standard TSP formulation. In gen-eral, the connection between n and k opens up a new range of solutions. For example, if we wanted to nd the best length-2 path, we could simply try all subsets of 2 vertices in the graph, in all 2 possible orders. This is a set of only O ( n 2 ) possibilities, so we can check all to identify the best in polynomial time.

This approach is very limited, howe ver: in gen-eral, its runtime of O ( n k ) for paths of length k mak es it prohibiti ve for all but the smallest values of k . We cannot really hope to avoid the exponential dependence on k , because doing so would give us a fast solution to an NP-hard problem, but there is hope of making the dependence  X less exponential.  X  This is captured by the denition of xed par ameter tractability (Do wne y and Fello ws, 1995). A prob-lem is x ed parameter tractable if we can mak e the exponential dependence on the parameter k indepen-dent of the polynomial dependence on the problem size n . This is the case for our problem: as we will describe belo w, an algorithm of Alon et al. can be used to achie ve a running time of roughly O (2 k n 2 ) . In other words, the path length k only exponentiates a small constant, instead of the problem size n , while the dependence on n is in fact quadratic. Decoding for selection-and-ordering problems is commonly implemented using beam search (Bank o et al., 2000; Corston-Oli ver et al., 2002; Jin and Hauptmann, 2001). Being heuristic in nature, this algorithm is not guaranteed to nd an optimal so-lution. Ho we ver, its simplicity and time efcienc y mak e it a decoding algorithm of choice for a wide range of NLP applications. In applications where beam decoding does not yield suf cient accurac y, researchers emplo y an alternati ve heuristic search, A* (Jelinek, 1969; Germann et al., 2001). While in some cases A* is quite effecti ve, in other cases its running time and memory requirements may equal that of an exhausti ve search. Time-and memory-bounded modications of A* (i.e., ID A-A*) do not suf fer from this limitation, but the y are not guaran-teed to nd the exact solution. Nor do the y pro-vide bounds on the lik elihood of nding the exact solution. Ne wly introduced methods based on lo-cal search can effecti vely examine lar ge areas of a search space (Eisner and Tromble, 2006), but the y still suf fer from the same limitations.

As an alternati ve to heuristic search algorithms, researchers also emplo y exact methods from com-binatorial optimization, in particular inte ger linear programming (Germann et al., 2001; Roth and Yih, 2004). While existing ILP solv ers nd the exact so-lution eventually , the running time may be too slo w for practical applications.

Our randomized decoder represents an impor -tant departure from pre vious approaches to decod-ing selection-and-ordering problems. The theoreti-cally established bounds on the performance of this algorithm enable us to explicitly control the trade-off between the quality and the efcienc y of the de-coding process. This property of our decoder sets it apart from existing heuristic algorithms that cannot guarantee an arbitrarily high probability of success. One might hope to solv e decoding with a dynamic program (lik e that for shortest paths) that gro ws an optimal path one verte x at a time. The problem is that this dynamic program may gro w to include a verte x already on the path, creating a cycle. One way to pre vent this is to remember the vertices used on each partial path, but this creates a dynamic program with too man y states to compute efciently .
Instead, we apply a color coding technique of Alon et al (1995). The basic step of the algo-rithm consists of randomly coloring the graph ver-tices with a set of colors of size r , and using dy-namic programming to nd the optimum length-k path without repeated color s . (Later , we describe how to determine the number of colors r .) Forbid-ding repeated colors excludes cycles as required, but remembering only colors on the path requires less state than remembering precisely which vertices are on the path. Since we color randomly , any single it-eration is not guaranteed to nd the optimal path; in a given coloring, two vertices along the optimal path may be assigned the same color , in which case the optimal path will never be selected. Therefore, the whole process is repeated multiple times, increasing the lik elihood of nding an optimal path.

Our algorithm is a variant of the original color -coding algorithm (Alon et al., 1995), which was de-veloped to detect the existence of paths of length k in an unweighted graph. We modify the original al-gorithm to nd the highest weighted path and also to handle equi valence classes of vertices. In addi-tion, we pro vide a method for determining the opti-mal number of colors to use for nding the highest weighted path of length k .

We rst describe the dynamic programming algo-rithm. Ne xt, we pro vide a probabilistic bound on the lik elihood of nding the optimal solution, and present a method for determining the optimal num-ber of colors for a given value of k .

Dynamic Pr ogramming Recall that we began with a weighted directed graph G to which we added articial start and end vertices s and t . We now posit a coloring of that graph that assigns a color c v to each verte x v aside from s and t . Our dynamic pro-gram returns the maximum score path of length k +2 (including the articial vertices s and t ) from s to t with no repeated colors.

Our dynamic program gro ws colorful paths  X  paths with at most one verte x of each color . For a given colorful path, we dene the spectrum of a path to be the set of colors (each used exactly once) of nodes on the interior of the path X we ex-clude the starting verte x (which will always be s ) and the ending verte x. To implement the dynamic program, we maintain a table q [ v,S ] inde xed by a path-ending verte x v and a spectrum S . For verte x v and spectrum S , entry q [ v,S ] contains the value of the maximum-score colorful path that starts at s , terminates at v , and has spectrum S in its interior .
We initialize the table with length-one paths: q [ v, ; ] represents the path from s to v , whose spec-trum is the empty set since there are no interior ver-tices. Its value is set to the score of edge ( s,v ) . We then iterate the dynamic program k times in order to build paths of length k + 1 starting at s . We ob-serv e that the optimum colorful path of length ` and spectrum S from s to v must consist of an optimum path from s to u (which will already have been found by the dynamic program) concatenated to the edge ( u,v ) . When we concatenate ( u,v ) , verte x u be-comes an interior verte x of the path, and so its color must not be in the pree xisting path' s spectrum, but joins the spectrum of the path we build. It follo ws that q [ v,S ] = max
After k iterations, for each verte x v we will have a list of optimal paths from s to v of length k + 1 with all possible spectra. The optimum length-k + 2 colorful path from s to t must follo w the optimum length-k + 1 path of some spectrum to some penul-timate verte x v and then proceed to verte x t ; we nd it by iterating over all such possible spectra and all vertices v to determine argmax v,S q [ v,S ] + w ( v,t ) .
Amplication The algorithm of Alon et al., and the variant we describe, are some what peculiar in that the probability of nding the optimal solu-tion in one coloring iteration is quite small. But this can easily be dealt with using a standard tech-nique called amplication (Motw ani and Ragha van, 1995). Suppose that the algorithm succeeds with small probability p , but that we would lik e it to suc-ceed with probability 1  X  where  X  is very small. We run the algorithm t = (1 /p ) ln 1 / X  times. The probability that the algorithm fails every single run is then (1 p ) t e  X  pt =  X  . But if the algorithm succeeds on even one run, then we will nd the op-timum answer (by taking the best of the answers we see).

No matter how man y times we run the algo-rithm, we cannot absolutely guarantee an optimal answer . Ho we ver, the chance of failure can easily be dri ven to negligible levels X achie ving, say , a one-in-a-billion chance of failure requires only 20 /p itera-tions by the pre vious analysis.

Determining the number of colors Suppose that we use r random colors and want to achie ve a given failure probability  X  . The probability that the opti-mal path has no repeated colors is: By the amplication analysis, the number of trials needed to dri ve the failure probability to the desired level will be inversely proportional to this quantity . At the same time, the dynamic programming table at each verte x will have size 2 r (inde xing on a bit vector of colors used per path), and the runtime of each trial will be proportional to this. Thus, the run-ning time for the necessary number of trials T r will be proportional to What r k should we choose to minimize this quantity? To answer , let us consider the ratio:
If this ratio is less than 1 , then using r + 1 col-ors will be faster than using r ; otherwise it will be slo wer . When r is very close to k , the abo ve equa-tion is tin y, indicating that one should increase r . When r k , the abo ve equation is huge, indicating one should decrease r . Some where in between, the ratio passes through 1, indicating the optimum point where neither increasing nor decreasing r will help. If we write  X  = k/r , and consider lar ge k , then T r +1 con verges to 2 e  X  (1  X  ) . Solving numerically to nd where this is equal to 1, we nd  X  . 76804 , which yields a running time proportional to approximately (4 . 5) k .

In practice, rather than using an (approximate) formula for the optimum r , one should simply plug all values of r in the range [ k, 2 k ] into the running-time formula in order to determine the best; doing so tak es negligible time. In this section, we sho w how to formulate the selection-and-ordering problem in the ILP frame-work. We represent each edge ( i,j ) from verte x i to verte x j with an indicator variable I i,j that is set to 1 if the edge is selected for the optimal path and 0 otherwise. In addition, the associated weight of the edge is represented by a constant w i,j .

The objecti ve is then to maximize the follo wing sum: This sum combines the weights of edges selected to be on the optimal path.

To ensure that the selected edges form a valid acyclic path starting at s and ending at t , we intro-duce the follo wing constraints:
Sour ce-Sink Constraints Exactly one edge orig-inating at source s is selected: Exactly one edge ending at sink t is selected:
Length Constraint Exactly k + 1 edges are se-lected: The k + 1 selected edges connect k + 2 vertices in-cluding s and t .

Balance Constraints Ev ery verte x v 2 V has in-degree equal to its out-de gree: Note that with this constraint, a verte x can have at most one outgoing and one incoming edge.

Redundancy Constraints To control for redun-danc y, we require that at most one representati ve from each equi valence class is selected. Let Z be a set of vertices that belong to the same equi valence class. For every equi valence class Z , we force the total out-de gree of all vertices in Z to be at most 1. Figure 1: A subgraph that contains a cycle, while satisfying constraints 2 through 5.

Acyclicity Constraints The constraints intro-duced abo ve do not fully prohibit the presence of cycles in the selected subgraph. Figure 1 sho ws an example of a selected subgraph that contains a cycle while satisfying all the abo ve constraints.
We force acyclicity with an additional set of vari-ables. The variables f i,j are intended to number the edges on the path from 1 to k + 1 , with the rst edge getting number f i,j = k + 1 , and the last getting number f i,j = 1 . All other edges will get f i,j = 0 . To enforce this, we start by ensuring that only the edges selected for the path ( I i,j = 1 ) get nonzero f -values: When I i,j = 0 , this constraint forces f i,j = 0 . When I i,j = 1 , this allo ws 0 f i,j k +1 . No w we introduce additional variables and constraints. We constrain demand variables d v by:
The right hand side sums the number of selected edges entering v , and will therefore be either 0 or 1. Ne xt we add variables a v and b v constrained by the equations: Note that a v sums over f values on all edges enter -ing v . Ho we ver, by the pre vious constraints those f -values can only be nonzero on the (at most one) selected edge entering v . So, a v is simply the f -value on the selected edge entering v , if one exists, and 0 otherwise. Similarly , b v is the f -value on the (at most one) selected edge lea ving v .

Finally , we add the constraints These last constraints let us argue, by induction, that a path of length exactly k + 1 must run from s to t , as follo ws. The pre vious constraints forced exactly one edge lea ving s , to some verte x v , to be selected. The constraint b s = k + 1 means that the f -value on this edge must be k + 1 . The balance constraint on v means some edge must be selected lea ving v . The constraint a v b v = d v means this edge must have f -value k . The argument continues the same way, building up a path. The balance constraints mean that the path must terminate at t , and the constraint that a t = 1 forces that termination to happen after exactly k + 1 edges. 4
For those familiar with max-o w, our program can be understood as follo ws. The variables I force a o w, of value 1, from s to t . The variables f rep-resent a o w with supply k + 1 at s and demand d v at v , being forced to obe y  X capacity constraints X  that let the o w tra vel only along edges with I = 1 . Task We applied our decoding algorithm to the task of title generation. This task has been extensi vely studied over the last six years (Bank o et al., 2000; Jin and Hauptmann, 2001). Title generation is a classic selection-and-ordering problem: during title realiza-tion, an algorithm has to tak e into account both the lik elihood of words appearing in the title and their ordering preferences. In the pre vious approaches, beam search has been used for decoding. Therefore, it is natural to explore more sophisticated decoding techniques lik e the ones described in this paper .
Our method for estimation of selection-and-ordering preferences is based on the technique de-scribed in (Bank o et al., 2000). We compute the lik elihood of a word in the document appearing in the title using a maximum entrop y classier . Ev ery stem is represented by commonly used positional and distrib utional features, such as location of the rst sentence that contains the stem and its TF*IDF . We estimate the ordering preferences using a bigram language model with Good-T uring smoothing.
In pre vious systems, the title length is either pro-vided to a decoder as a parameter , or heuristics are used to determine it. Since exploration of these heuristics is not the focus of our paper , we pro vide the decoder with the actual title length (as measured by the number of content words).

Graph Construction We construct a decoding graph in the follo wing fashion. Ev ery unique con-tent word comprises a verte x in the graph. All the morphological variants of a stem belong to the same equi valence class. An edge ( v,u ) in the graph en-codes the selection preference of u and the lik eli-hood of the transition from v to u .

Note that the graph does not contain any auxiliary words in its vertices. We handle the insertion of aux-iliary words by inserting additional edges. For every auxiliary word x , we add one edge representing the transition from v to u via x , and the selection pref-erence of u . The auxiliary word set consists of 24 prepositions and articles extracted from the corpus.
Cor pus Our corpus consists of 547 sections of a commonly used under graduate algorithms textbook. The average section contains 609.2 words. A title, on average, contains 3.7 words, among which 3.0 are content words; the shortest and longest titles have 1 and 13 words respecti vely . Our training set consists of the rst 382 sections, the remaining 165 sections are used for testing. The bigram language model is estimated from the body text of all sections in the corpus, consisting of 461,351 tok ens.

To assess the importance of the acyclicity con-straint, we compute the number of titles that have repeated content words. The empirical ndings sup-port our assumption: 97.9% of the titles do not con-tain repeated words.

Decoding Algorithms We consider three decod-ing algorithms: our color -coding algorithm, ILP , and beam search. 5 The beam search algorithm can only consider vertices which are not already in the path. 6 To solv e the ILP formulations, we emplo y a Mix ed Inte ger Programming solv er lp solve which implements the Branch-and-Bound algorithm. We implemented the rest of the decoders in Python with the Psyco speed-up module. We put substantial ef-fort to optimize the performance of all of the al-gorithms. The color -coding algorithm is imple-mented using parallelized computation of coloring iterations. Table 1 sho ws the performance of various decoding algorithms considered in the paper . We rst evalu-ate each algorithm by the running times it requires to nd all the optimal solutions on the test set. Since ILP is guaranteed to nd the optimal solution, we can use its output as a point of comparison. Table 1 lists both the average and the median running times. For some of the decoding algorithms, the dif ference between the two measurements is striking  X  6,536 seconds versus 57.3 seconds for ILP . This gap can be explained by outliers which greatly increase the av-erage running time. For instance, in the worst case, ILP tak es an astounding 136 hours to nd the opti-mal solution. Therefore, we base our comparison on the median running time.

The color -coding algorithm requires a median time of 9.7 seconds to nd an optimal solution com-pared to the 57.3 seconds tak en by ILP . Furthermore, as Figure 2 sho ws, the algorithm con verges quickly: just ele ven iterations are required to nd an optimal solution in 90% of the titles, and within 35 itera-tions all of the solutions are found. An alternati ve method for nding optimal solutions is to emplo y a beam search with a lar ge beam size. We found that for our test set, the smallest beam size that satises this condition is 1345, making it twenty-three times slo wer than the randomized decoder with respect to the median running time.

Does the decoding accurac y impact the quality of the generated titles? We can always trade speed for accurac y in heuristic search algorithms. As an ex-treme, consider a beam search with a beam of size 1: while it is very fast with a median running time the decoding algorithms. Figure 2: The proportion of exact solutions found for each iteration of the color coding algorithm. of less than one second, it is unable to nd any of the optimal solutions. The titles generated by this method have substantially lower scores than those produced by the optimal decoder , yielding a 0.2322 point dif ference in ROUGE scores. Ev en a lar ger beam size such as 80 (as used by Bank o et al. (2000)) does not match the title quality of the optimal de-coder . In this paper , we formalized the decoding task for selection-and-ordering as a problem of nding the highest-weighted acyclic path in a directed graph. The presented decoding algorithm emplo ys random-ized color -coding, and can closely approximate the ILP performance, without blo wing up the running time. The algorithm has been tested on title genera-tion, but the decoder is not specic to this task and can be applied to other generation and summariza-tion applications. The authors ackno wledge the support of the Na-tional Science Foundation (CAREER grant IIS-0448168 and grant IIS-0415865). We also would lik e to ackno wledge the MIT NLP group and the anon ymous revie wers for valuable comments.
