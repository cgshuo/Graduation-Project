 Named entities are observed in a large portion of web search queries (named entity queries), where each entity can be as-sociated with many different query terms that refer to vari-ous aspects of this entity. Organizing these query terms into topics helps understand major search intents about entities and the discovered topics are useful for applications such as query suggestion. Furthermore, we notice that named enti-ties can often be organized into categories and those from the same category share many generic topics. Therefore, working on a category of named entities instead of individ-ual ones helps avoid the problems caused by the sparsity and noise in the data. In this paper, Named Entity Topic Model (NETM) is proposed to discover generic topics for a category of named entities, where the quality of the generic topics is improved through the model design and the pa-rameter initialization. Experiments based on query log data show that NETM discovers high-quality topics and outper-forms the state-of-the-art techniques by 12.8% based on F1 measure.
 Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms Algorithms, Experimentation, Performance Keywords Verbose Query, Query Distribution, Query Reformulation
Named entities have been widely observed in web search queries. It is reported in [7] that about 71% of queries in the search traffic contain named entities. The named entity or simply entity discussed here is defined as any proper name, such as names of film actors/actresses, names of car models and names of cities. We study named entity queries ,which consist of two parts: the named entity and the topic term . For example, considering two named entity queries  X  X rit-Table 1: The topic terms observed for the named entity  X  X eattle X  restaurants, things to do in, events, apartments, attractions, airport shuttle, dining, transportation, vacation rentals, houses for rent in, brunch, subway things to do in airport shuttle restaurants apartments attractions subway brunch houses for rent in ney Spears biography X  and  X  X eattle things to do X ,  X  X ritney Spears X  and  X  X eattle X  are named entities and  X  X iography X  and  X  X hings to do X  are the corresponding topic terms. Note that a topic term could be either a word or a phrase.
Many different topic terms are usually observed for a named entity, where each of them refers to some topic about this named entity. Some examples of the topic terms observed for the named entity  X  X eattle X  are displayed in the upper part of Table 1 (ranked by their query frequencies). Orga-nizing these topic terms according to their underlying topics helps search engines understand major search intents about  X  X eattle X . An approach proposed in this paper 1 is able to group these terms into four major topics (as shown in the lower part of Table 1).

Motivated by the above example, we study how to auto-matically discover topics for named entities from their asso-ciated topic terms. The discovered topics and their associ-atedtopictermscanbeusedinapplicationssuchasquery suggestion. For instance, when the user searches for the query  X  X eattle tourism X , it is useful to suggest the alterna-tive queries from the same topic (Topic 1) such as  X  X eattle things to do X  and  X  X eattle attractions X . It is also useful to allow the user to explore other topics about  X  X eattle X  such as its transportation (Topic 2) and restaurants (Topic 3).
Furthermore, we notice that named entities can be or-ganized by categories. The named entities from the same category share many generic topics. For example,  X  X oston X ,  X  X hicago X ,  X  X an Francisco X  X nd X  X eattle X  X ll belong to the US City category and the topics discovered for  X  X eattle X  above are also applicable to the other three cities. Therefore, in-stead of detecting topics for individual named entity, we consider to discover generic topics for a category of named entities, which avoids the problems caused by the sparsity and noise in the topic terms of individual named entity, es-pecially for those unpopular ones.

Topic modeling for documents [8, 2] has been widely stud-ied in the literature. Similar ideas are applied here to named entity queries. Specifically, we transform the query click
The details will be described in the following parts. data into a collection of pseudo documents, where each pseudo document consists of topic terms related to a certain named entity. In this paper, we propose a novel topic modeling approach, i.e. Named Entity Topic Model (NETM), which detects the generic topics for a category of named entities. Two strategies are used by NETM to improve the quality of the generic topics. First, the entity-specific topic is in-troduced in the model to include the topic terms that are specific to a certain or a small number of named entities and cannot be classified into any generic topic. For exam-ple, the topic terms  X  X andalay Bay X  and  X  X aesars Palace X  are both famous hotels of X  X as Vegas X  X nd they are less likely to appear with other entities. An analysis based on sampled named entity queries shows that 35% queries contain the topic terms applied to only one named entity, thus, with-out the entity-specific topics, these terms would have to be assigned to some generic topics. Second, a parameter initial-ization strategy based on the seed term selection guides the model to focus on the generic topics. Experiments based on query log data show that NETM discovers high-quality top-ics and outperforms the state-of-the-art techniques by 12.8% based on F1 measure.
This paper studies how to organize topic terms accord-ing to their underlying topics, which is somehow similar to previous work on query suggestion that groups semantically related queries together. Different techniques have been pro-posed including using temporal behavior [11, 5], considering session context [6, 4], building query flow graphs[3] and com-bining click data and session data [10]. In general, these techniques work on individual queries and thus cannot work well for unpopular queries. In this paper, the generic topics are discovered for a category of named entities, which can be reasonably applied to those unpopular named entities.
Wang et al [12] used query reformulation data extracted from search session to mine broad latent query aspects. They detected common query aspects applicable to most queries. However, a large portion of important topics are category specific and thus can not be detected by their approach.
Pa  X sca and Durme [9] studied how to extract open-domain classes and class attributes from web documents and query logs. However, they did not consider how to organize these class attributes into different topics.

Balasubramanian and Cucerzan [1] studied to generate topic pages of living people. Their aspect model for each person is simply a collection of cooccurrence words with that person. These words are also not organized into topics.
Recently, Yin and Shah [13] considered to build taxonomy of search intents for named entity queries. Our work, inde-pendently developed during the same period as their work, shares several related insights. Despite the high level simi-larities, the approaches used are significantly different.
Topic model has been studied for a long time in informa-tion retrieval and natural language processing. The classic models include PLSA [8] and LDA [2]. Recently, this ap-proach has been applied to a lot of novel applications.
We have a category of entities 2 E = { e 1 ,e 2 ,...,e | E |
E | denotes the size of E . It is usually easy to get such
We use  X  X ntity X  and  X  X amed entity X  exchangeably. categories of entities. For example, Wikipedia provides a category for almost any reasonable category of entities.
A vocabulary V = { t 1 ,t 2 ,...,t | V | } consists of all possible topic terms about entities in E . A topic term is the part of a query excluding the entity. Note that a topic term can be either a single word or a phrase.

The query click data consists of a set of named entity queries, a set of URLs, and the number of clicks of each query on each URL. The named entity queries are denoted as Q = { q | q =( e,t ) ,e  X  E,t  X  V } ,where e denotes the entity and t denotes the topic term. The URLs are denoted as L = { l 1 ,l 2 ,...,l | L | } . The click relations are denoted as R = { ( q,l ): f ( q,l ) } ,where f ( q,l ) denotes the number of times l is clicked for query q .

Atopic T is defined as a multinomial distribution over the vocabulary V of topic terms, where t  X  V P ( t | T )=1. Here, P ( t | T ) is the parameters of the topic T .Givena category of entities E ,wewanttolearn k generic topics GT = { T 1 ,T 2 ,...,T k } shared by most entities of this cat-egory. In order to improve the quality of GT ,weintro-duce an entity-specific topic for each entity, which is ET = {
T e 1 ,T e 2 ,...,T e | E | } for all entities. For example,  X  X eviews X  is a generic topic learned for the category  X  X ar model X , where terms like  X  X eviews X ,  X  X oad test X  and  X  X atings X  receive high probabilities. For entity  X  X odge Charger X , its entity-specific topic assigns high probabilities to terms like  X  X olice car X ,  X  X aytona X  and  X  X uper Bee X .
Here, we describe how to transform query click data into a collection of pseudo documents. A pseudo document d is the set of topic terms that appear together with a particu-lar entity e in queries with clicks on a certain URL l .Itis defined for each pair of entity and URL ( e,l ), where d con-sists of topic terms t that have been used with e as queries ( e,t ). In addition, l must receive some clicks after posing quency of t in d , which is equal to f (( e,t ) ,l ), i.e. the click count on l for query ( e,t ). The topic terms associated with the same entity and URL pair are likely to talk about the same topic, since a URL (the corresponding web page) usu-ally provides information for one or a few related aspects of an entity. Thus, the above construction process satisfies the assumption of topic models [8, 2], where words co-occurring frequently in documents should belong to the same topic.
For example, the topic terms X 2009 accessories X ,  X  X oupe ac-cessaries X  and X  X ccessories X  appear with the entity-URL pair (Honda Accord, http://automobiles.honda.com/... ), thus a pseudo document is constructed for this entity-URL pair that contains all these three terms.

A collection of pseudo documents is defined as C = { d e,l E,l  X  L } . Our task is to learn generic topics GT from C . We will use  X  X ocument X  for  X  X seudo document X  unless indi-cated otherwise.
In Named Entity Topic Model, we assume each term of a document d e,l is selected either from k generic topics GT = {
T 1 ,T 2 ,...,T k } , or from the corresponding entity-specific topic T . The generic topics are used to model topic terms appli-cable to most entities. The entity-specific topic is used to model topic terms specific to a certain entity. The log like-lihood of generating the collection C = { d e,l | e  X  E,l is given as follows:
Eq. 1 shows that the log likelihood of generating the col-lection is calculated based on each document and topic term pair ( d e,l ,t ). Eq. 2 calculates the probability of generating of selecting a generic topic T i and P ( T e | d e,l ) is the probabil-ity of selecting the entity-specific topic. After selecting any topic, a topic term t can be generated with the probability P ( t | T i )and P ( t | T e ) correspondingly. P ( t | T eters of the generic topics and P ( t | T e ) is the parameters of the entity-specific topic. The generation process of NETM is further illustrated in Fig. 1. Note that among all entity-specific topics in ET ,only T e that corresponds to the entity e in the subscript of d e,l is used to generate ( d e,l ,t ). The pa-rameters are estimated through Expectation Maximization (EM). The details are omitted due to space limitation.
Here we discuss how to improve the quality of the generic topics through appropriate initialization of its parameters. First, we rank all topic terms t basedonthenumberofen-tities NE ( t )that t has been associated with. Second, we recover the underlying language model of a term t using Zhai and Lafferty X  X  method [14]. Here, the underlying lan-guage model LM t of t is a multinomial distribution, where high probabilities are assigned to not only t but also its se-mantically related terms. Third, given the top ranked seed terms and their associated underlying language models LM t we can easily pick the top k seed terms and use their LM t to initialize k generic topics. LM t also helps avoid picking very similar terms, since the similarities between two seed terms can be measured by the KL-divergence between their underlying language models.
We test our approaches on a diverse set of categories of entities, which are chosen to cover different types of entities frequently appearing in user queries. They are shown in Ta-ble 2 with number of entities in each category. We remove all entities with an disambiguation page on Wikipedia. The query click data is extracted from the query log of a com-mercial search engine from 2008/01/01 to 2008/12/31.
The baselines are PLSA [8] and LDA [2], the state-of-the-art topic modeling approaches. PLSA and LDA are applied to the collection of pseudo documents generated. For a fair comparison, different approaches generate the same number of generic topics, i.e. 20 topics. How to dynamically decide the number of topics is beyond the scope of this paper. We implement PLSA by ourselves according to [8]. An existing package LDA-C is used for LDA 3 . We display some examples of the generic topics learned by PLSA, LDA and NETM. Table 3 shows two generic topics learned for  X  X niversities X  and  X  X ars models X . The top terms are displayed for each topic 4 . In Table 3, PLSA and LDA mix the terms about different topics together. For example, for the topic  X  X ar parts X , terms about car parts, terms about car sale like  X  X sed X  and  X  X or sale X , terms about car review like X  X oad test X  X nd terms about car repair like X  X ransmission problems X  are also mixed together. In contrast, the quality of the topics generated by NETM is much better. For the topic  X  X niversities alumni X , NETM also discovers important terms such as  X  X lumni directory X  and  X  X lass rings X .
In this part, we quantitatively compare the quality of the generic topics generated by different approaches. For each category in Table 2, we sample 100 named entity queries from the query log data based on query frequencies to form the test set (in total 100  X  9=900 queries). For each topic modeling approach TM (i.e. PLSA, LDA and NETM), given atestquery( e,t ), where e is the entity and t is the topic term, TM will return a generic topic T i with the highest probability P ( t | T i ) among all generic topics. The returned topic T i is displayed to the annotator 5 . Then, the annotator (from Amazon Mechanical Turk 6 ) is asked to provide a rel-evance judgment (yes or no) according to whether the topic term in the query is well covered by the returned topic. For each pair of query and topic, three different annotators are asked to provide judgments. The standard F1 measure is used here to measure performance.
 First, we compare PLSA, LDA and NETM in Table 4.
 LDA fails to run on four categories (film actors, musicians, US cities and US retail companies), since the size of the collection is too large, which causes the memory problem for LDA-C.

Table 4 shows that NETM consistently outperforms PLSA and LDA on most categories. On average, NETM signif-icantly improves LDA by 13.8% on categories where LDA can run and NETM significantly improves PLSA by 12.8% on all categories. http://www.cs.princeton.edu/~blei/lda-c/
The name of the topic is constructed by combining the name of categories and the top one term of this topic.
Since annotators are usually not patient to read too much information, only top ranked terms of a topic are displayed. http://mturk.amazon.com Table 4: Comparisons of different approaches. The best performance (F1) of each category is bolded. Table 5: Comparisons with PLSA( LM t ). The best performance (F1) of each category is bolded.
Second, we make further analysis to understand where the advantages of NETM come. Aforementioned, two strate-gies are used by NETM to improve the quality of generic topics: the modeling of entity-specific topics and the ini-tialization method. In order to separate their effects, we consider a new approach PLSA( LM t ), that uses the same initialization method as NETM for PLSA, but not modeling the entity-specific topics. The comparisons of each approach are displayed in Table 5.

Table 5 shows that PLSA( LM t ) is better than PLSA, but it is still worse than NETM. Therefore, the advantages of NETM come from both strategies.

Some example is provided to show the effect of the initial-ization method used by NETM. For the  X  X ar model X  cate-gory, PLSA discovers two generic topics (pictures/wallpaper/ ebay...) and (photos/forums/video...)that should have been merged into the same topic. In contrast, NETM success-fully avoids this problem by discovering a generic topic (pic-tures/wallpaper/photos), since NETM recovers the language model of  X  X ictures X  for initialization, which not only assigns high probability to  X  X ictures X  but also to  X  X hotos X .
Another example is provided to show the effect of model-ing the entity-specific topics. NETM puts the term  X  X ater-gate X  into the entity-specific topic of  X  X ichard Nixon X  from the  X  X S president X  category. Without any special design for this type of terms, PLSA  X  X andomly X  organizes  X  X atergate X  into a topic (family tree/ house/descendants of/...) and the probability assigned is as high as 0.014, which deceases the quality of generic topics.
Named entity queries consist of a big portion of web query traffic. Discovering topics underlying named entity queries are useful for various search applications. A novel topic modeling approach NETM is proposed in this paper to dis-cover generic topics for a category of named entities. Ex-periments show that NETM detects high-quality topics and significantly outperforms the state-of-the-art techniques.
