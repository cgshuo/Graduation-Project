 We demonstrate that regularization can improve feedback in a language modeling framework.
 Categories and Subject Descriptors: H.3.3 Information Search and Retrieval: Relevance Feedback General Terms: Algorithms Keywords: regularization, relevance feedback, language modeling
In many information retrieval scenarios, in addition to a query, a user also supplies sample relevant and non-relevant documents. These judgments may be provided with the query or in response to some initial query probing the col-lection for documents. The second scenario, referred to as relevance feedback , is the focus of this paper.

We would like to improve the performance of a language modeling baseline for relevance feedback. In particular, we are interested in augmenting the generative and one-class approaches with a model of the discriminative information conveyed by positive and negative feedback information. In this poster, we propose a method for introducing relevance information by forcing scores of documents related to rele-vant documents to be high and those related to non-relevant documents to be low. We accomplish this by using a doc-ument re-ranking technique known as score regularization . Given an initial set of retrieval scores, score regularization refers to a process of re-scoring documents in order to im-prove the consistency of scores of topically related docu-ments [2]. We demonstrate that we can improve relevance feedback by conducting a post-retrieval re-ranking which in-corporates relevance information and document similarities.
The language modeling approach to information retrieval ranks documents by comparing each document X  X  smoothed language model,  X  d , to a language model estimated from the user X  X  short query,  X  Q . Without relevance judgments,  X  Work conducted at the Center for Intelligent Information Retrieval.
 the query model is usually a maximum likelihood estimate. When document relevance information is provided, we can estimate the true relevance model directly with binary weights [5, p. 69], P ( w |  X  R )=  X P ( w |  X  Q )+(1  X   X  ) R + is the set of documents judged relevant. Documents are then ranked according to cross entropy, where y is vector of document scores.

We note that, unlike the vector space model and the prob-abilistic retrieval model, there is no formal model of non-relevance in relevance feedback based on true relevance mod-els. True relevance models approach information retrieval from the perspective of density estimation. Relevant exam-ples provide statistics for the true relevance model. The non-relevance model, by default, is the language model esti-mated using collection statistics, P ( w |  X  C ). Since the major-ity of the collection is non-relevant, the information from ad-ditional non-relevant documents is insignificant. This might be seen as a minor theoretical detail given the empirical ev-idence that negative feedback does not result in significant improvements [1, 3]. However, we believe that the infor-mation in explicitly non-relevant documents can be useful in situations where no relevant documents are retrieved and the system must filter non-relevant information. For ex-ample, if the only known keywords for a topic retrieve a cohesive, non-relevant cluster, we would like to provide in-formation to remove the entire non-relevant cluster [7]. Fur-thermore, although the collection is a reasonable model of non-relevance, high-ranking non-relevant documents aid in refining the decision boundary between relevant and non-relevant documents.
Given an initial set of retrieval scores, y , score regulariza-tion refers to a process of re-scoring documents in order to improve the consistency of scores of topically related docu-ments [2]. The output of regularization is a vector of scores, f , which minimizes two cost functions. One cost function, S ( f ), measures the dissimilarity of scores of related doc-uments. Document relationships are encoded in a n  X  n matrix, W , of inter-document similarities. The other cost function, E ( f , y ), measures the dissimilarity of scores of the Figure 1: Relevance feedback results for trec12 and robust. Without feedback, trec12 QL performance is 0.2506 and 0.2800 if regularized. Robust QL per-formance is 0.2649 and 0.2955 if regularized. documents with the original retrieval scores. We linearly combine these into a composite function, The constraints are defined as, where, letting D ii = known as the combinatorial Laplacian.The Laplacian has been empirically shown to be an effective method for mea-suring the consistency of the scores of related documents. The closed form solution for computing f  X  is, where  X  = 1 1+  X  .

Whereas true relevance modeling is a non-parametric den-sity estimation method, regularization is a non-parametric function approximation method. One advantage of approach-ing this as function approximation is that we can explicitly model non-relevance differently than we do uncertainty. Put another way, in Equation 2, a score of 0 represents both non-relevant documents and unjudged documents. In reg-ularization, if we normalize scores to zero mean and unit variance, we can explicitly model relevant document scores (eg, y i &gt; 1), non-relevant document scores (eg, y i &lt;  X  and unjudged documents (eg, y i = 0). In practice, for each relevant document, we replace its score with a value sampled from the region of the Gaussian greater than the maximum score. We do the same replacement for each non-relevant document by using samples from the bottom region of the Gaussian. Given these adjusted scores, we compute regular-ized scores according to Equation 4.
We performed experiments on two data sets. The first data set, which we will call  X  X rec12 X , consists of the 150 TREC Ad Hoc topics 51-200. We used only the news col-lections on Tipster disks 1 and 2 [4]. The second data set, which we will call  X  X obust X , consists of the 250 TREC 2004 Robust topics [6]. We used only the news collections on TREC disks 4 and 5. For both data sets, we use only the title queries. We indexed collections using the Indri retrieval system, the Rainbow stop word list, and Krovetz stemming. We use TREC judgments (qrels) to simulate feedback.
We measure the performance of the system after receiving feedback on the top k documents. After the  X  X ser X  marks the top k documents as relevant or non-relevant, we re-retrieve documents using the true relevance, P ( w |  X  R ). This is our baseline. We then normalize and regularize the scores and compare them to our baseline. We note that judged doc-uments are included in the evaluation set so that we are testing the ability to re-retrieve relevant documents and pe-nalize non-relevant documents; this intended to avoid prob-lems with queries poorly represented in the corpus. We present the results of these experiments in Figure 1. The horizontal axis of this graph represents the number of documents judged in the initial retrieval. Given the results in [2], we should not be surprised that regularization consis-tently improves the performance of retrieval. However, the amount of improvement grows with the number of relevance judgments. We suspect that, as the number of judgments in-creases, the regularization component of the system becomes more important because the additional data introduces a more discriminative component to standard true relevance models, allowing us to take advantage of additional data.
This work was supported in part by the Center for In-telligent Information Retrieval and in part by the Defense Advanced Research Projects Agency under contract number HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are the au-thor and do not necessarily reflect those of the sponsor.
