 Words can be grouped together into equivalence classes to help reduce data sparsity and better gener-alize data. Word clusters are useful in many NLP ap-plications. Within machine translation word classes are used in word alignment (Brown et al., 1993; Och and Ney, 2000), translation models (Koehn and Hoang, 2007; Wuebker et al., 2013), reordering (Cherry, 2013), preordering (Stymne, 2012), target-side inflection (Chahuneau et al., 2013), SAMT (Zollmann and Vogel, 2011), and OSM (Durrani et al., 2014), among many others.

Word clusterings have also found utility in pars-ing (Koo et al., 2008; Candito and Seddah, 2010; Kong et al., 2014), chunking (Turian et al., 2010), NER (Miller et al., 2004; Liang, 2005; Ratinov and Roth, 2009; Ritter et al., 2011), structure transfer (T  X  ackstr  X  om et al., 2012), and discourse relation dis-covery (Rutherford and Xue, 2014).

Word clusters also speed up normalization in training neural network and MaxEnt language models, via class-based decomposition (Goodman, 2001a). This reduces the normalization time from O ( | V | ) (the vocabulary size) to  X  X  ( improvements to O (log( | V | )) are found using hier-archical softmax (Morin and Bengio, 2005; Mnih and Hinton, 2009) . Word clustering partitions a vocabulary V , grouping together words that function similarly. This helps generalize language and alleviate data sparsity. We discuss flat clustering in this paper. Flat, or strict partitioning clustering surjectively maps word types onto a smaller set of clusters.

The exchange algorithm (Kneser and Ney, 1993) is an efficient technique that exhibits a general time complexity of O ( | V | X | C | X  I ) , where | V | is the number of word types, | C | is the number of classes, and I is the number of training iterations, typically &lt; 20 . This omits the specific method of exchang-ing words, which adds further complexity. Words are exchanged from one class to another until con-vergence or I .
One of the oldest and still most popular exchange algorithm implementations is mkcls (Och, 1995) 1 , which adds various metaheuristics to escape local optima. Botros et al. (2015) introduce their imple-mentation of three exchange-based algorithms. Mar-tin et al. (1998) and M  X  uller and Sch  X  utze (2015) 2 use trigrams within the exchange algorithm. Clark
The previous algorithms use an unlexicalized (two-sided) language model: P ( w i | w i  X  1 ) = P ( w i | c i ) P ( c i | c i  X  1 ) , where the class c i dicted word w i is conditioned on the class c i  X  1 of the previous word w i  X  1 . Goodman (2001b) altered this model so that c i is conditioned directly upon w i  X  1 , hence: P ( w i | w i  X  1 ) = P ( w i | c i ) P ( c This new model fractionates the history more, but it allows for a large speedup in hypothesizing an ex-change since the history doesn X  X  change. The re-sulting partially lexicalized (one-sided) class model gives the accompanying predictive exchange al-gorithm (Goodman, 2001b; Uszkoreit and Brants, 2008) a time complexity of O (( B + | V | )  X | C | X  I ) where B is the number of unique bigrams in the to this algorithm to enable high-quality large-scale word clusters. We developed a bidirectional, interpolated, refining, and alternating (B IRA ) predictive exchange algo-rithm. The goal of B IRA is to produce better clusters by using multiple, changing models to escape local optima. This uses both forward and reversed bigram class models to improve cluster quality by evaluat-ing log-likelihood on two different models. Unlike using trigrams, bidirectional bigram models only linearly increase time and memory requirements, and in fact some data structures can be shared. The two directions are interpolated to allow softer inte-gration of these two models: The interpolation weight  X  for the forward direction alternates to 1  X   X  every a iterations ( i ): Figure 1 illustrates the benefit of this  X  -inversion to help escape local minima, with lower training set perplexity by inverting  X  every four iterations: The time complexity is O (2  X  ( B + | V | )  X | C | X  I ) . The original predictive exchange algorithm can be obtained by setting  X  = 1 and a = 0 . 5
Another innovation, both in terms of cluster qual-ity and speed, is cluster refinement . The vocabulary is initially clustered into | G | sets, where | G || C | , typically 2 X 10 . After a few iterations ( i ) of this, the full partitioning C f is explored. Clustering G converges very quickly, typically requiring no more The intuition behind this is to group words first into broad classes, like nouns, verbs, adjectives, etc. In contrast to divisive hierarchical clustering and coarse-to-fine methods (Petrov, 2009), after the ini-tial iterations, the algorithm is still able to exchange any word to any cluster X  X here is no hard constraint that the more refined partitions be subsets of the ini-tial coarser partitions. This gives more flexibility in optimizing on log-likelihood, especially given the noise that naturally arises from coarser clusterings. We explored cluster refinement over more stages than just two, successively increasing the number of clusters. We observed no improvement over the two-stage method described above.

Each B IRA component can be applied to any exchange-based clusterer. The contributions of each of these are shown in Figure 2, which reports the development set perplexities (PP) of all combina-tions of B IRA components over the original pre-dictive exchange algorithm. The data and con-figurations are discussed in more detail in Sec-tion 4. The greatest PP reduction is due to using lambda inversion ( +Rev ), followed by cluster re-finement ( +Refine ), then interpolating the bidirec-tional models ( +BiDi ), with robust improvements by using all three of these X  X n 18% reduction in perplexity over the predictive exchange algorithm. We have found that both lambda inversion and clus-ter refinement prevent early convergence at local op-tima, while bidirectional models give immediate and consistent training set PP improvements, but this is attenuated in a unidirectional evaluation.

We observed that most of the computation for the predictive exchange algorithm is spent on the log-arithm function, calculating  X   X   X   X  N ( w,c )  X  log N ( w,c ) . 7 Since the codomain of N ( w,c ) is N 0 , and due to the power law distribution of the al-gorithm X  X  access to these entropy terms, we can pre-compute N  X  log N up to, say 10e+7, with minimal able speedup of around 40% . Our experiments consist of both intrinsic and extrin-sic evaluations. The intrinsic evaluation measures the perplexity (PP) of two-sided class-based models for English and Russian, and the extrinsic evalua-tion measures B LEU scores of phrase-based MT of Russian  X  English and Japanese  X  English texts. 4.1 Class-based Language Model Evaluation In this task we used 400, 800, and 1200 classes for English, and 800 classes for Russian. The data comes from the 2011 X 2013 News Crawl monolin-the data was deduplicated, shuffled, tokenized, digit-conflated, and lowercased. In order to have a large test set, one line per 100 of the resulting (shuffled) imum count threshold was set to 3 occurrences in the training set. Table 1 shows information on the resulting corpus.

The clusterings are evaluated on the PP of an ex-ternal 5-gram unidirectional two-sided class-based language model (LM). The n -gram-order interpola-tion weights are tuned using a distinct development set of comparable size and quality as the test set.
Table 2 and Figure 3 show perplexity results us-ing a varying number of classes. Two-sided ex-change gives the lowest perplexity across the board, although this is within a two-sided LM evaluation. We also evaluated clusters derived from word2vec (Mikolov et al., 2013) using various configura-better perplexities than both the original predictive sian experiments yielded higher perplexities for all clusterings, but otherwise the same comparative re-sults.

In general Brown clusters give slightly worse results relative to exchange-based clusters, since Brown clustering requires an early, permanent placement of frequent words, with further re-strictions imposed on the | C | -most frequent ing is only efficient on a small number of clusters, since there is a | C | 2 term in its time complexity. The original predictive exchange algorithm has a more fractionated history than the two-sided exchange algorithm. Interestingly, increasing the number of clusters causes a convergence in the word clusterings themselves, while also causing a divergence in the time complexities of these two varieties of the exchange algorithm. The metaheuristic techniques employed by the two-sided clusterer mkcls can be applied to other exchange-based clusterers X  X ncluding ours X  X or further improvements.

Table 3 presents wall clock times using the full training set, varying the number of word classes | C | (for English). 14 The predictive exchange-based clusterers (B IRA and Phrasal) exhibit slow increases in time as the number of classes increases, while the others (Brown and mkcls ) are much more sensi-tive to | C | . Our B IRA -based clusterer is three times faster than Phrasal for all these sets.

We performed an additional experiment, adding implementation took 3.0 hours to cluster 2.5 bil-lion training tokens , with | C | = 800 using modest 4.2 Machine Translation Evaluation We also evaluated the B IRA predictive exchange al-gorithm extrinsically in machine translation. As dis-cussed in Section 1, word clusters are employed in a variety of ways within machine translation systems, the most common of which is in word alignment where mkcls is widely used. As training sets get larger every year, mkcls struggles to keep pace, and is a substantial time bottleneck in MT pipelines with large datasets.

We used data from the Workshop on Ma-chine Translation 2015 (WMT15) Russian  X  English dataset and the Workshop on Asian Translation 2014 (WAT14) Japanese  X  English dataset (Nakazawa et al., 2014). Both pairs used standard configurations, like truecasing, MeCab segmentation for Japanese, MGIZA alignment, grow-diag-final-and phrase ex-traction, phrase-based Moses, quantized KenLM 5-gram modified Kneser-Ney LMs, and M ERT tuning.
The B LEU score differences between using mkcls and our B IRA implementation are small but there are a few statistically significant changes, us-ing bootstrap resampling (Koehn, 2004). Table 4 presents the B LEU score changes across varying cluster sizes (*: p -value &lt; 0.05, **: p -value &lt; 0.01). M
ERT tuning is quite erratic, and some of the B LEU differences could be affected by noise in the tun-ing process in obtaining quality weight values. Us-ing our B IRA implementation reduces the translation model training time with 500 clusters from 20 hours using mkcls (of which 60% of the time is spent on clustering) to just 8 hours (of which 5% is spent on clustering). We have presented improvements to the predictive exchange algorithm that address longstanding draw-backs of the original algorithm compared to other clustering algorithms, enabling new directions in us-ing large scale, high cluster-size word classes in NLP.

Botros et al. (2015) found that the one-sided model of the predictive exchange algorithm pro-duces better results for training LSTM-based lan-guage models compared to two-sided models, while two-sided models generally give better perplexity in class-based LM experiments. Our paper shows that B
IRA -based predictive exchange clusters are com-petitive with two-sided clusters even in a two-sided evaluation. They also give better perplexity than the original predictive exchange algorithm and Brown clustering.

The software is freely available at https:// github.com/jonsafari/clustercat .
 We would like to thank Hermann Ney and Kazuki Irie, as well as the reviewers for their useful com-ments. This work was supported by the QT21 project (Horizon 2020 No. 645452).

