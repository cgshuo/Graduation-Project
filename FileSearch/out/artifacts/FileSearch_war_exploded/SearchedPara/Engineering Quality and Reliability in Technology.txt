 The objective of technology-assisted review ( X  X AR X ) is to find as much relevant information as possible with reason-able effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reli-ability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this defini-tion of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quan-tifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 top-ics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.
 Keywords: Technology-assisted review; predictive coding; electronic discovery; e-discovery; test collections; relevance feedback; continuous active learning; reliability; quality; sys-tematic review.  X 
The views expressed herein are solely those of the author and should not be attributed to her firm or its clients.
A vexing question that has plagued the use of technology-assisted review ( X  X AR X ) is  X  X hen to stop X ; that is, know-ing when as much relevant information as possible has been found, with reasonable effort. We present a provably reli-able method to achieve high recall using any search strategy that repeatedly retrieves documents and receives relevance feedback, continuing indefinitely until a decision is made to discontinue the review process. Amenable search strategies include traditional ranked retrieval, 1 interactive searching and judging [8], move-to-front pooling [8], and continuos ac-tive learning ( X  X AL X ) [5].

For the particular implementation of CAL supplied as the baseline model implementation ( X  X MI X ) [7] for the TREC 2015 Total Recall Track [13], we present two stopping pro-cedures that achieve superior empirical reliability for com-parable effort, and comparable empirical reliability for less effort, relative to our provably reliable method.

Our primary motivation is to provide quality assurance for TAR applications, including electronic discovery ( X  X Dis-covery X ) in legal matters [5], systematic review in evidence-based medicine [10], and the creation of test collections for information retrieval ( X  X R X ) evaluation [14]. Since these ap-plications generally require that a human review each rele-vant document, we assume for this study that the effort to provide relevance feedback for relevant documents is a sunk cost. On the other hand, the effort to assess and provide rel-evance feedback for non-relevant documents is wasted. We measure review effort in terms of the total number of doc-uments reviewed, whether relevant or not. An ideal search would find all of the relevant documents with effort equal to precisely that number. An acceptable search would find most of the relevant documents with minimal wasted effort.
A reliable search method would achieve an acceptable search most of the time. More formally, if S is a random variable representing a search, and acceptable ( s ) is an indi-cator function denoting whether a particular search s has an acceptable result, we define: To this end, we define recall ( s ) and effort ( s ) to be the recall and effort associated with s . For simplicity, our primary
To be amenable, a retrieval method must be able to rank the entire collection. Incomplete rankings or set-based re-sults may be extended by adding the remaining documents in any order. Collection Source Description # Docs # Topics # Rel (R) At Home TREC 2015 Total Recall Jeb Bush public email 290,000 10 227-17,135 At Home TREC 2015 Total Recall Hacker forums 465,147 10 179-9,517 At Home TREC 2015 Total Recall Local news 902,434 10 23-2,094 RCV1-v2 Reuters News subject categories 804,414 103 5-381,327 Filtering TREC 2012 Filtering NIST topics 804,414 50 12-610 Robust-04 TREC 2004 Robust Amalgam of TREC ad-hoc topics 528,256 249 4-161 test collection, for a total of eight test collections. results use a goal-post definition [18] of acceptability: Our primary results further assume that 95% reliability is sufficiently high.

The methods and results detailed in this work are:
The modern literature on the effectiveness and reliabil-ity of high-recall retrieval is largely confined to the problem of constructing test collections for IR evaluation, and eDis-covery in legal matters. A 1985 study by Blair and Maron [2] showed that teams of lawyers and paralegals, using it-erative Boolean searches, believed they had achieved 75% recall, when in fact they had achieved 20%. Blair [3] later described the difficulty of measuring high recall in general, and the use of targeted searching, systematically constructed Boolean queries, and stratified sampling to estimate recall for the Blair and Maron study.

The Text Retrieval Conference ( X  X REC X ) [21] first ad-dressed the problem of IR evaluation for  X  X arge X  datasets, which at the time of TREC X  X  inception in 1992, contained on the order of 500,000 documents. TREC follows the Cran-field paradigm [20], which evaluates the results of subject systems against a gold standard that identifies every relevant document. For large datasets, the effort to render a human assessment for each document is prohibitive, thus occasion-ing the use of automated or semi-automated methods to limit the human review effort required to label the dataset. TREC saw the introduction of the  X  X ooling method, X  which selects the top-ranked documents from a number of inde-pendent retrieval efforts for assessment, and deems all other documents to be non-relevant. A number of studies ( see [19]) indicate that this method fails to identify a substan-tial number of documents, but even so, the resulting gold standard yields a stable evaluation of the relative effective-ness of candidate systems, as measured by Kendall X  X   X  rank correlation. We are unaware of any studies that address the effectiveness of pool-based gold standards for evaluating high-recall retrieval, or for simulating interactive relevance feedback. Studies suggest that greedy or machine-learning methods to select the pool yield a more nearly complete gold standard [8, 14].

Interactive searching and judging ( X  X SJ X ), in which a searcher repeatedly formulates queries and examines the top results from a relevance-ranking search engine, has been shown to yield gold standards with comparable quality to the pooling method, with considerably less effort [8]. Con-tinuous active learning ( X  X AL X ) [5] is essentially the same as ISJ, but uses machine learning instead of, or in addition to, manually formulated queries to rank the documents for review. An approach similar to CAL was used in the TREC 2012 Filtering Track ( see [17]) to construct the gold standard that was used for evaluation, and also to simulate relevance feedback. A subsequent study based on pooling showed that the CAL-like approach had achieved high recall, and high ef-fectiveness, as measured by Kendall X  X   X  [17]. CAL achieved superior results at the TREC 2009 Legal Track [4], and re-mains state of the art for eDiscovery.

The TREC 2015 Total Recall Track [13] represents the first study of high-recall human-in-the-loop retrieval in which all aspects of human intervention are simulated, and hence controlled. Fully automated or semi-automated re-trieval systems were tested through their interaction with an evaluation server. At the outset, the evaluation server pro-vided a document collection and a topic description, after which the system under test submitted potentially relevant documents from the collection to the evaluation server. In response, the evaluation server provided an assessment (de-rived from a pre-computed gold standard) for the submitted documents, and the process continued until the documents were exhausted or the system chose to stop.

Participants in the Total Recall Track were supplied with a CAL baseline model implementation 2 ( X  X MI X ) that, when connected to the evaluation server, performed all aspects of the task X  X ther than deciding when to stop X  X ithout hu-man intervention. Participating systems were allowed to run indefinitely, and were evaluated (primarily) on the quality of the ranking determined by the order in which the system presented documents to the server. Instead of actually ter-minating when they thought an acceptable result had been achieved, participants were invited to  X  X all their shot X  by in-dicating, in real time, when they would have stopped, had they been required to balance benefit with cost. The current study considers the addition of a call-your-shot mechanism to BMI, and, more generally, to any ranking system.
The TREC 2015 Total Recall Track contributed five fully labeled archival datasets . The Jeb Bush , Hacker Forums , and Local News datasets were used for the At Home task, in which participants ran their systems on their own platforms, connecting via the internet to the evaluation server, which was run by the track coordinators. The Kaine and MIMIC II datasets were used for the Sandbox task, in which par-ticipants encapsulated their systems as a virtual machine, which was run by the track coordinators, along with the evaluation server, isolated from the internet.

The reliability of methods for constructing gold standards for IR evaluation has typically been evaluated by how well the resulting gold standard ranks the relative effectiveness of precision-oriented retrieval systems, where the objective is to find as much relevant information as possible at low rank. For this purpose, a calibrated estimate is not required; it is sufficient to determine whether one system achieves higher recall than another, and the actual numerical value is as-cribed little meaning. A number of studies ( see [23]) eschew recall altogether, assuming that the user X  X  information need will be satisfied by a tiny fraction of a vast sea of relevant documents. Zobel et al. [23] suggest that recall is a poor effectiveness measure, even for the  X  X igh-recall applications X  where the user seeks  X  X otal recall, X  and that only an exten-sive ad-hoc effort using multiple queries and tools will satisfy the user that their information need has been met.

The reliability and effectiveness of TAR (also known as  X  X redictive coding X ) is the subject of much interest in the le-gal community [9, 16]. A number of approaches to TAR, to deciding when to stop, and to quality assurance have been advanced, but no stopping procedure has previously been shown to be mathematically or empirically reliable. Perhaps the most commonly used approach to TAR involves the use of a supervised machine-learning algorithm trained using a set of documents from the collection (typically referred to as a  X  X eed set X ) to partition the collection into a  X  X eview set, X  which is subject to human review, and a  X  X ull set, X  which
See http://plg.uwaterloo.ca/~gvcormac/trecvm/. is not. This approach is referred to as either simple passive learning ( X  X PL X ) or simple active learning ( X  X AL X ), depend-ing on whether or not the learning algorithm is involved in selecting the training documents [5]. Recently, CAL has been advanced as a superior alternative [5, 7].

Regardless of the TAR method used, the question remains of when to stop. For SPL and SAL, two questions must be answered: when to stop training; and how many documents should be included in the review set. For CAL, the sole ques-tion is when to stop. One approach that has been advanced is to draw a random hold-out set (referred to as a  X  X ontrol set X ) that is used to measure the effectiveness of the classi-fier, in order to determine when to stop training, and then to measure recall, so as to determine how many documents should comprise the review set. The control set must be large enough to contain a sufficient number of relevant doc-uments to yield a precise estimate. Bagdouri et al. [1] note that the use of a control set constitutes sequential sampling, with the net effect that it yields a biased estimate of recall, and cannot be used for quality assurance. As an alternative, they propose  X  X ertified text classification, X  in which part of the review budget is set aside to conduct a frequentist accep-tance test that will accept or reject the classifier. Bagdouri et al. are concerned with the problem of testing whether the classifier has achieved a threshold level of F 1 ; they do not consider recall, or how to proceed in the event that the classifier is rejected by the test.

The limitations of binary relevance may be of particular importance in evaluating the effectiveness and reliability of TAR systems. Binary relevance does not account for the dif-ferential importance of relevant documents, and there will necessarily be documents near the threshold about which competent assessors will disagree ( see [19]). In evaluating the recall of a system against a gold standard, there will necessarily be uncertainty for some documents as to whether the system is correct, the gold standard is correct, or reason-able minds could disagree. If a system fails to meet a target recall threshold, is it because the system has missed impor-tant documents, because it has missed marginal documents about which reasonable minds could differ, or because it has missed documents that are incorrectly coded relevant in the gold standard? And, is the effort to remedy the shortfall proportionate to the importance of the missed documents?
Binary relevance and fixed recall targets are examples of traditional goal-post methods in quality engineering ([18]), where success or failure is a binary quantity, and reliabil-ity is the probability of success. In quality engineering, a quadratic loss function blends reliability and effectiveness into a single quality measure, with targets, but no arbitrary thresholds [18].
Our target method involves drawing a target set T of k random relevant documents from the collection; for simplic-ity we fix k = 10, but a different number could be chosen. In order to draw T , we retrieve and review documents se-lected at random until k relevant documents are found, or the collection is exhausted. The underlying search strategy retrieves documents for review without knowledge of T , until every document in T has been found. This method achieves 95% reliability, as shown below.

Consider a document collection C and a function rel ( d ) indicating binary relevance. The number of relevant docu-ments in the collection is R = |{ d  X  C | rel ( d ) }| . A search strategy is a ranking on C where 1  X  rank ( d )  X | C | denotes the position of d in the ranking. It is important to note that the following argument holds for any such ranking, provided it is independent of T .

The retrieved set of the target method is the shortest pre-fix P of the ranking that contains T : Now consider the ranking relrank ( d ) of only relevant docu-ments: relrank ( d ) = d 0  X  C | rel ( d )  X  rank ( d 0 )  X  rank ( d ) . The last retrieved document d last is necessarily in T : The recall of our method is: Taking T to be a random variable, the method is reliable if: Assuming large R, consider the problem of determining a cutoff c such that: For the condition in Equation 2 to hold, it must be the case that the [numerically] top-ranked cR documents are absent from T , which occurs with probability It follows that: For all R &gt; 10, Combining (1) and (3), we have:
Reliability is obtained at the cost of supplemental review effort inversely proportional to R , the number of relevant documents. The number of randomly selected documents that need to be reviewed to find k relevant ones is k | C | average for R | C | . For k = 10 and prevalence R | C | the target method entails a review overhead of about 1,000 additional documents. Lower prevalence entails substan-tially more overhead, while higher prevalence entails less.
Our knee method relies on the assumption that CAL, in accordance with the probability-ranking principle, succeeds in ranking more-likely relevant documents before less-likely relevant documents. As a consequence, the gain curve plot-ting recall versus rank is assumed to be generally convex, with high slope ( i.e. ,  X  X arginal precision X ) at the outset, and near-zero slope once nearly all relevant documents have been retrieved.

An ideal gain curve would have slope 1 . 0 until an inflection point corresponding to the rank at which all documents had been retrieved, and slope 0 . 0 thereafter. An actual gain curve typically diverges from the ideal due to limitations in probability ranking, random factors, and a noisy gold standard. Suppose that the retrieval method were able to achieve 70% recall and 70% precision at some rank r , as is typical for modern classifiers [11], or as might be achieved by exhaustive manual review [19]. The slope, up to that rank ( slope &lt;r ), would be 0 . 7, and the slope after that rank ( slope &gt;r ) would approach, but not equal 0. For small values of R | C | , we would expect slope  X  0 . 0, and for all R we
Based on our experience with non-public datasets, we ob-served that for R &amp; 100,  X  6 . 0 (with suitable smooth-ing) was a good indicator of high recall, achieving recall and reliability that compared favorably to that achieved by the target method. We formed the hypothesis that these thresh-olds were universal; that the same threshold would work for a wide variety datasets, including the ten that we subse-quently used for our empirical evaluation ( see Table 1).
If we were to stop at the minimum rank s , such that there exists an inflection point 1 &lt; i &lt; s such that  X   X  6, we would almost certainly stop prematurely due to chance. Moreover, this na  X   X ve approach would entail quadratic computational effort as a function of the size of the collection. To avoid both eventualities, we evaluate  X  only at values of s arising from the batches of documents selected by BMI. The number of batches is proportional to log | C | , as the values of s are separated by an exponentially increasing interval. Relatively few of the candidate values for s will be viable, even by chance. Any residual sequential-testing bias is offset by a conservative choice of threshold for  X  .

For each value of s , we evaluate  X  at only one i , chosen us-ing a geometric  X  X nee-detection X  algorithm [15], illustrated in Figure 1. We draw a line from the origin to the recall achieved at rank s , and compute the maximum perpendicu-lar distance from this line to the gain curve. Our candidate value of i is the projection to the x -axis of the intersection between the perpendicular and the gain curve. Our rationale in choosing this point was that it would correctly choose the inflection point for an ideal curve, and would avoid anoma-lies associated with points very close to the origin or to rank s , while capturing our intuitive notion of a genuine tipping point.

We calculated the slope ratio as: Smoothing was accomplished by adding 1 to the number of relevant documents beyond the knee. This choice avoided the singularity of no relevant documents beyond the knee, and generally penalized situations in which the chosen in-flection point was close to s . No smoothing was applied to the numerator, as we were not concerned with occasional underestimates.
The case of R . 100 is more problematic. Any correction for small R faces a dual problem: 1. the stopping procedure has no knowledge of the value 2. even if it were known that R was small, the sparsity of The knee method relies entirely on the slope-ratio test, ad-justed to compensate for low R . Initial tuning on the train-ing collections from the TREC 2015 Total Recall Track in-dicated that a fixed lower bound  X  on the rank at which to stop, might be effective. For our submission to the TREC 2015 Total Recall At Home task, we conducted a parameter sweep of six combinations of  X   X  X  100 , 1000 } and  X   X  X  3 , 6 , 10 } . Our results showed that combinations involv-ing  X  = 100 or  X  = 3 were unreliable, and we eliminated them from further consideration. Unsurprisingly, the com-bination of  X  = 1000 ,  X  = 10 proved most reliable, achiev-ing the recall target for 29 of 30 topics ( reliability = 0 . 97 [0 . 830  X  0 . 999 95% c.i.]).

We observed that recall and reliability appeared to be lower for smaller R , while effort (especially for  X  = 10) ap-peared to be disproportionately higher for large R . This ob-servation led us to seek more reliable methods for small R , and to choose  X  = 6 for large R . To aid in this endeavor, we used a non-public dataset consisting of about 300,000 docu-ments reviewed by attorneys and labeled according to 63 cri-teria, with R ranging from 5 to 164 , 000 (median 431). Based on tuning experiments using this dataset, we calibrated the slope-ratio cutoff as a function of relret , the number of rel-evant documents retrieved at any given rank: In other words, we set the threshold for the slope ratio to be 150 when no relevant documents have been retrieved, 6 whenever at least 150 relevant documents have been re-trieved, and use linear interpolation between these values.
We further observed that with this adjustment, the choice of  X  = 100 versus  X  = 1000 became less critical. The lower value occasionally achieved lower effort than the higher value, and occasionally failed when the higher value did not. We chose to retain the value of  X  = 1000 from our earlier experiments.
A variant of our knee method X  X he budget method  X  adjusts for small R by stopping only when a review budget comparable to that of the target method has been expended, and the slope-ratio test  X   X  6 is also satisfied. This ad-justment substantially delays termination for small R , thus ensuring reliability.

The approach is predicated on the hypothesis that the supplemental review effort entailed by the target method would be better spent reviewing more documents retrieved by CAL. The target method entails the supplemental review of about 10 | C | R documents in order to find 10 relevant ones. According to the probability-ranking principle, we would ex-pect CAL to find more relevant documents than random selection, for any level of effort, up to and beyond 10 | C |
While the supplemental documents retrieved by the target method provide a statistical estimate of R , the documents retrieved by CAL provide a lower bound for R , and therefore an upper bound for the expected effort entailed by the target method. At the outset, this upper bound is loose, but as the review progresses, it tightens. The budget method retrieves documents using CAL until review effort exceeds this upper bound and  X   X  6 . 0, or until 0 . 75 | C | documents are retrieved. For small R , the budget determines the stopping point. For large R , enough relevant documents will likely be discov-ered to bound the review budget to an insubstantial fraction of R , and the slope-ratio test will determine when to stop. In any event, the review stops at 0 . 75 | C | . This final cutoff is predicated on the probability-ranking principle: random Figure 2: TREC 2015 Total Recall At Home Collec-tion. selection of 75% of the collection would, with high probabil-ity, achieve 70% recall; the top-ranked 75% should achieve even higher recall.
Testing the reliability of our stopping methods occasioned the use of  X  X ully assessed X  test collections, with a large num-ber of topics and documents, where by  X  X ully assessed, X  we mean that the pooling method, ISJ, or a rule base was used, and the resulting documents were labeled by a human as-sessor. From the limited number of collections that met these criteria, we selected the TREC 2015 Total Recall Track collections, the Reuters RCV1-v2 news dataset, the TREC 2002 Filtering Track collections, and the TREC 2004 and 2005 Robust Track collections, as detailed in Table 1. We used our Total Recall At Home participation to conduct an initial parameter sweep with six combinations, as well as final testing; the other datasets were used solely for testing.
The first phase of our experiments took place within the context of the TREC 2015 Total Recall Track, which had three distinct phases: training, At Home, and Sandbox. We conducted our initial development and tuning during the training phase, and submitted the knee method for eval-uation in the At Home phase, but not the Sandbox phase. We captured the sequence of documents retrieved by BMI in both the At Home and Sandbox phases, and later used them Figure 3: TREC 2015 Total Recall Sandbox Collec-tions. to simulate the effect of the stopping methods whose results are presented here. After conducting further tuning on our non-public collection of 300,000 documents with 63 topics, we froze all parameters, and ran BMI on the other evalua-tion datasets, capturing the order in which the documents were retrieved. We then simulated our stopping methods by applying them to the ranking.

Summary results showing reliability, average recall, and average effort for all collections are shown in Table 2. The overall reliability of the target method, the knee method, and the budget method are substantially higher than the target of 0 . 95. Considering reliability, alone, there is little to choose among the methods; but the recall achieved by the knee and budget methods is substantially higher, while the effort expended by the knee method is, for some datasets, dramatically lower.

As illustrated in Figures 2 through 6, R (the number of relevant documents) appears to be the principal determi-nant of effort. For small R , effort for the target and budget methods approaches the size of the collection, while effort for the knee method, with one notable exception, generally diminishes with R , approaching the floor of  X  = 1000 that we chose for this study. On the other hand, for large R , the effort for all methods appears proportional to R .

The top panel of Figure 2 compares recall and effort for the knee and target methods, for each topic in the At Home col-lection, ordered by R . We see that 28 of the 30 recall points for the knee method (shown by the green curve) fall above 0 . 7, indicating reliability of 0 . 93, while all of the points for the target method (shown by the red curve) fall above 0 . 7, indicating reliability of 1 . 00 for this collection. We also see that the most of the recall points for the knee method fall above those for the target method, indicating higher median recall, and the (signed) area between the curves is positive, indicating higher mean recall. Per-topic effort is shown as a bar graph on a logarithmic scale spanning three orders of magnitude. For small R, the knee method entails about 100 times less effort than the target method, while for large R , the effort is comparable.

The bottom panel of Figure 2 follows the same format, comparing the budget method (shown in blue) to the target method (shown in red). While the budget method achieves higher recall than the target method for nearly all topics, that superiority is not reflected in higher reliability. Effort for the two methods is very similar. The same observa-tions apply to the results for the other collections: For low R , recall for the budget method exceeds that of the target method, while effort is indistinguishable; for large R , recall and effort are indistinguishable from the knee method. Both methods are reliable.

For brevity, we show graphical results comparing only the knee and target methods for the other collections. Tabular results for all methods are presented in Table 2.

Figure 3 shows results for the Sandbox task of the TREC 2015 Total Recall Track, which was notable in that partici-pants had no prior access to the datasets or the topics, and their retrieval systems had to run fully autonomously. The top panel shows our results for the Kaine collection, which consisted of about 400,000 documents from Tim Kaine X  X  eight-year tenure as Governor of Virginia. These documents had been previously reviewed and labeled by the archivist at the Library of Virginia according to four statutory cat-egories:  X  X ecord X  (versus  X  X on-record X ),  X  X pen record, X   X  X e-stricted record, X  and  X  X ertaining to the Virginia Tech shoot-ing. X  Two of the topics had moderately high R  X  10 4 , and two had very high R  X  10 5 . For all topics, the knee method achieved higher recall at the expense of somewhat higher ef-fort. The bottom panel shows our results for the MIMIC II collection, which consisted of about 30,000 medical records
Figure 5: TREC 2002 Filtering Track Collections. collected from a hospital intensive care unit. The documents consisted of nurses X  notes, radiology reports, and discharge summaries. The  X  X opics X  consisted of ICD-9 diagnostic codes extracted from non-textual database records. With one ex-ception ( R = 179), all topics had moderately high R . The knee method generally achieved higher recall than the target method, at the expense of somewhat higher effort for most topics.

Figure 4 shows the results for the RCV1-v2 dataset, using the subject categories and descriptions published with the dataset as topics [11]. Over a very wide range 10 1 . R . 10 5 , we observe a familiar pattern: The knee method has somewhat higher recall and lower variance, with dramati-cally lower effort, for small R .

Figure 5 shows results for two sets of topics created for the TREC 2002 Filtering Track. The top panel shows re-sults for topics that were created and assessed by NIST for the track. All topics had low R  X  610; the majority had very low R  X  100. For all topics, including those with the low-est R 100, the knee method achieved near-perfect recall. Recall for the target method showed much higher variance, suggesting that its reliability is actually lower. The knee method entails order(s) of magnitude less effort. The lower panel shows results for intersection topics, each of which was the conjunction of two RCV1-v2 subject categories. If rel 1 ( d ) and rel 2 ( d ) indicate relevance for two RCV1-v2 top-ics, rel 1 ( d )  X  rel 2 ( d ) indicates relevance for the intersection Figure 6: TREC 2004-2005 Robust Track Collec-tions. topic. The intersection topics were reported as a failed ex-periment [17], since no system achieved reasonable results on them. The results show that, while the effort to achieve high recall for these anomalous topics is inordinately large, our stopping methods are reliable.

Figure 6 shows results for the TREC 2004 and 2005 Ro-bust tracks. In 2004, the Robust Track aggregated 150 topics developed for the TREC 6, TREC 7, and TREC 8 Ad-Hoc tasks, 50 topics developed for the 2003 Robust Track, and 49 new topics, for a total of 249 topics. For 2005, 50 of these topics X  X hose deemed to be  X  X ifficult X  X  X ere reprised with a new dataset. The top panel reports our results for 2004; the bottom for 2005. The results further confirm that the target and knee methods both achieve high reliability, while the knee method entails dramatically less effort.
As evidenced by the results above, reliability does not capture certain important aspects of effectiveness or effi-ciency. Moreover, empirical measurements of reliability lack statistical power, while parametric estimates depend on as-sumptions regarding the distribution of recall values. Since the choices of acceptable recall and acceptable reliability are both somewhat arbitrary, bias due to incorrect distributional assumptions may be of little consequence. We suggest that reporting the mean  X  and standard deviation  X  of recall conveys more useful information, if not a provably accurate estimate of reliability. Such an estimate would have to be compared to one or more tacit thresholds to determine the reliability of the method; for example, assuming normality, any pair of  X  and  X  such that  X   X  1 . 64  X   X  0 . 70 would be 95% reliable. More generally, the value of Q =  X   X  1 . 64  X  is a quantitative measure of quality, which may be used to determine the threshold level of acceptable recall for which 95% reliability may be obtained. Alternatively, by substi-tuting the appropriate z -score in place of 1 . 64, a threshold of reliability different from 95% may be tested.

We suggest that reliability and recall should be supplanted by quality estimates based on loss functions, of which recall and reliability are special cases. We define Q = 1  X  loss , where loss is the mean value of a loss function over all topics. If A quadratic loss function such as: captures the desirability of consistently high recall, subsum-ing the roles of  X  and  X  in the previous discussion. Our aspirational goal is to achieve 100% recall. Any shortfall is penalized, and larger shortfalls are penalized more heavily.
Quadratic loss further generalizes to other aspects of qual-ity, such as graded relevance, facet relevance [6], and effi-ciency. For example, let a 1 , a 2 , ... , a n be categories of relevance, and rel a ( d ) be the indicator function for category a . Define: The choice of weights  X  i is not critical; the value  X  i = 1 all  X  i will often suffice, as it rewards consistent recall over all categories, with the effect that documents belonging to rarer categories are afforded more influence.

Review effort may also be modeled as a category of loss, thus quantifying the notion of  X  X easonable effort. X  For the problem as we have framed it, an ideal method would entail effort = R . From the presentation of results in the TREC 2015 Total Recall Track Overview [13], we observe that a reasonable effort might entail effort = aR + b , where a  X  1 represents effort proportional to sunk review cost, and b  X  0 represents fixed overhead. We suggest that a  X  2 and b  X  1000 represent reasonable effort to achieve recall  X  0 . 70 with 95% reliability. The use of a quadratic loss replaces the a and b thresholds by a soft target: loss e may be used to measure efficiency in its own right, or treated as a category loss in (5). p loss e p loss re p loss r p loss e p loss re p loss h p loss r p loss h
In Table 3, we report, for each collection, the root mean loss ( p loss ) over all topics for relevance loss as defined in (4); effort loss as defined in (6); as well as their unweighted average, loss re = 0 . 5  X  loss r + 0 . 5  X  loss e . The results show conclusively the superiority of the budget method in terms of loss r . They show the general superiority of the knee method in terms of loss e , while calling to our attention three collec-tions where the target method is more efficient. On inspec-tion, we see that two of the three collections have exclusively or nearly exclusively topics with high prevalence. We fur-ther see that that the the target method X  X  narrow margin of superiority in terms of loss r is offset by a wide margin of in-feriority in loss e , as reflected in loss re . For the intersection collection, no system achieved acceptable loss e .

The bottom line is that the quality loss results support our qualitative observation that the knee method affords the best balance between consistently high recall and consis-tently low effort; the budget method provides consistently higher recall at the expense of disproportionate effort for topics with few relevant documents; the target method, while provably reliable, yields empirical results that are gen-erally inferior to the knee and budget methods.

To illustrate the use of quality loss for graded relevance, we used a subset of 84 topics from Robust-04, for which rel-evance assessments were available for the categories  X  X ighly relevant X  and  X  X elevant. X  Table 4 shows p loss r and p loss for these categories, respectively. The knee and target meth-ods have lower loss h , than loss r , indicating they retrieve highly relevant documents more consistently than merely relevant documents. The budget method shows the opposite effect, but even so, is markedly superior to the target and knee methods. While we cannot draw any firm conclusions from this small experiment, the results do not support the proposition that TAR methods achieve high recall by  X  X ulk-ing up X  on marginal documents at the expense of important ones ( cf. [12]).
To our knowledge, the target method is the first provably reliable method for TAR. The commonly used frequentist acceptance test ( see [1, 9]) offers a p-value or confidence level which is a measure of the reliability of the test , not the reliability of the TAR method, not the probability that a given result is acceptable, and not the probability that a TAR method will pass the acceptance test. In eDiscovery, it is common to calculate a frequentist recall estimate, with a 5% margin of error and 95% confidence, and deem the result acceptable if the estimate exceeds 75%. Calculating such an estimate requires a sample of about 385 random relevant documents, entailing 38.5 times as much surplus effort as the target method.

Our proof of reliability does not require that the target sample T be chosen at the outset, as long as it is independent of the retrieval method. The target method could be used as an acceptance test, such that the consequence of failing the test would be to continue to retrieve documents without knowledge of T , until all the documents in T are retrieved.
Over test collections like the ones used in this study, there can be little doubt ( p  X  0 . 00) that the knee and budget methods are reliable, that the budget method is more re-liable than both the knee and target methods, and that the knee method is the most efficient. As with any empiri-cal work, the test collections constitute convenience samples and ongoing research is necessary to characterize the scope of TAR tasks to which our results may be generalized.
The target method is reliable regardless of the underlying review method; however, if the underlying method uses a human in the loop to formulate queries or to influence the selection of documents in any way, that human must be iso-lated from any knowledge of T . The simplest approach to accomplish this goal might be to complete all such human intervention before drawing T , and to rely on fully auto-mated document selection thereafter. An alternative would be to establish an  X  X nformation barrier X  between those who draw T and those who conduct the search.

This work establishes the reliability of the knee and bud-get methods as applied to BMI. It remains to be determined how well these approaches would work X  X ossibly with differ-ent tuning parameters X  X or other CAL methods, including hybrid systems in which a human is afforded influence in the selection of documents for review. It is not obvious how to adapt the knee or budget method to SPL or SAL, for which an essential question is when to stop training.

The target method, by design, targets less than 100% re-call. It could be modified to continue past the point at which the last document in T is retrieved, thereby expending ad-ditional effort to increase the probability of achieving 100% recall. One might, for example, extrapolate from the dis-tribution of rank ( d  X  T ). The knee method, on the other hand, does target 100% recall, and only incidentally opti-mizes reliability. It appears that loss functions better char-acterize the tension among consistency, effectiveness, and efficiency, as compared to goal-post methods. Regardless of which measure is chosen for evaluation, systems should be tuned to optimize their suitability for their intended pur-pose, not the measure itself ( cf . [22]).
Reservations about the effectiveness and reliability of TAR have impeded its adoption for eDiscovery and other high-recall retrieval tasks. A primary area of concern has cen-tered on the issue of  X  X hen to stop, X  or knowing with rea-sonable certainty X  X nd being able to show an adversary or the court X  X hat a particular TAR effort has identified an ac-ceptable amount of relevant information. Many approaches to validation in common use today are simply invalid, or require disproportionate effort compared to the information they yield, and are often misunderstood and misapplied [9, 16].

We offer a method to determine when to stop that is guar-anteed to be reliable, for the price of reviewing a number of random documents that is an order of magnitude less than acceptance tests that estimate recall, but neither determine when to stop nor guarantee reliability. We provide two other methods that entail no effort beyond that required by the underlying TAR method and, while not providing a guaran-tee of reliability, consistently demonstrate better reliability, and better recall, when evaluated on eight test collections, comprising 555 topics and 4.5M documents. Of particular interest is the knee method which, in contrast to the other methods, is demonstrated to be reliable and efficient when the collection contains few relevant documents.

While our primary results are demonstrated using mea-sures derived from traditional goal-post methods X  X inary relevance, a recall threshold, and a reliability floor X  X e de-scribe how loss functions may be formulated to capture the tension among consistency, degrees of relevance, facets of relevance, and efficiency. We apply these formulae to show insights into our results that might not have been readily apparent from the goal-post measures. [1] M. Bagdouri, W. Webber, D. D. Lewis, and D. W. [2] D. Blair and M. E. Maron. An evaluation of retrieval [3] D. C. Blair. Stairs redux: Thoughts on the stairs [4] G. Cormack and M. Mojdeh. Machine learning for [5] G. V. Cormack and M. R. Grossman. Evaluation of [6] G. V. Cormack and M. R. Grossman. Multi-faceted [7] G. V. Cormack and M. R. Grossman. Autonomy and [8] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. [9] M. R. Grossman and G. V. Cormack. Comments on [10] C. Lefebvre, E. Manheimer, and J. Glanville.
 [11] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: [12] D. Remus and F. S. Levy. Can robots be lawyers? [13] A. Roegiest, G. V. Cormack, M. R. Grossman, and [14] M. Sanderson and H. Joho. Forming test collections [15] V. Satop  X  a  X  a, J. Albrecht, D. Irwin, and B. Raghavan. [16] K. Schieneman and T. Gricks. The implications of [17] I. Soboroff and S. Robertson. Building a filtering test [18] G. Taguchi. Introduction to Quality Engineering: [19] E. M. Voorhees. Variations in relevance judgments and [20] E. M. Voorhees. The philosophy of information [21] E. M. Voorhees and D. K. Harman. The Text [22] E. Yilmaz and S. Robertson. On the choice of [23] J. Zobel, A. Moffat, and L. A. Park. Against recall: Is
