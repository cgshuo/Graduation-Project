 Almost every application of spreading activation is accompa-nied by its own set of often heuristic restrictions on the dy-namics. We show that in constraint-free scenarios spreading activation would actually yield query-independent results, so that the specific choice of restrictions is not only a pragmatic computational issue, but crucially determines the outcome. H.3.3 [ Information storage and retrieval ]: Information Search and Retrieval X  retrieval models, search process Algorithms, Theory, Performance Spreading activation, cosine similarity, information retrieval Spreading activation was proposed by Quillian and Collins [7, 3] as a technique to query networks of information. It facilitates the extraction of subgraphs, nodes, or edges rel-evant to a given query. Upon activation of a number of specific nodes, their activation is spread iteratively to ad-jacent nodes until some termination criterion is met. The result is determined from the subset of activated nodes, their activation level, and induced subgraph.

While spreading activation techniques have initially been applied to semantic networks, associative retrieval [9] has paved the way for applications in information retrieval [2, 10, 4]. The fundamental idea of associative retrieval is that  X 
Research supported in part by DFG under grant GRK 1042 (Research Training Group  X  X xplorative Analysis and Visu-alization of Large Information Spaces X ) and the European Commission in 7th Framework Programme (FP7-ICT-2007-C FET-Open, contract no. BISON-211898).
 related information is connected in a network and that rel-evant information can be retrieved by considering associa-tions of concepts that are either known to be relevant or specified by the user.

To our knowledge, all these methods share the usage of constraints to steer the spread of activation inside a network, such as distance constraints to terminate the spreading pro-cedure after a certain number of iterations, or fan-out con-straints to direct the spreading. In Section 3 we show that without such constraints, spreading activation converges to query-independent fixed states, rendering pure (i.e., con-straint free) spreading activation an inadequate technique for answering queries. While this drawback is generally over-come by applying heuristic constraints that enforce conver-gence to query-specific states, their precise effects are dif-ficult to analyze. In Section 4 we show that another ap-proach to produce query-dependent activation levels is the accumulation of intermediate states. This approach is more amenable to theoretical analysis and can be combined freely with other commonly used heuristics. Spreading activation is related to neural networks. What both methods have in common is that units can be acti-vated by incoming activation, and edges spread outgoing activation to adjacent units. Based on the eight modeling aspects of neural networks defined in [8], the functionality of spreading activation can be specified in terms of the three components described below. Activation is spread on a graph G = ( V, E ; w ) with weights w : E  X  R . For ease of exposition we assume that V = { 1 , . . . , n } and that G is undirected, but our results easily generalize to directed graphs. We extend w to V  X  V by letting w ( u, v ) = 0 if ( u, v ) /  X  E . The set of neighbors of v  X  V is denoted by N ( v ) = { u : { u, v }  X  E } . The activation state at time k is denoted by a ( k )  X  R V , where a ( k ) activation of v  X  V . The state at time k &gt; 0 is obtained from the state at time k  X  1 via the following three families of functions.
 Output functions out v : R  X  R An output function de-Input functions in v : R n  X  R An input function aggre-Activation functions act v : R  X  R An activation func-The initial state a (0) is usually defined by activating those nodes that represent the query. An activation spreads across incident edges to adjacent nodes and activates these nodes as well. The process is usually terminated after a fixed number of iterations, activation of a given number of nodes, or simi-lar criteria. The query response is determined from the sub-graph induced by activated nodes. Often the desired form of result in information retrieval is a ranking of the nodes ob-tained from their final output activation (henceforth simply referred to as the activation). The general framework is most commonly instantiated with a linear input function and the identity function for activa-tion and output. In this scenario, the spreading activation process is conveniently described in matrix notation. Given a graph G = ( V, E ; w ) and an activation vector a ( k  X  1) next activation step yields a ( k ) v = for all v  X  V . With the weight matrix W  X  R n  X  n defined single iteration can be stated compactly as a ( k ) = W a and therefore In addition to termination criteria, the following heuristic constraints are common [4]:
Distance constraints : activation decreases with dis-tance from initially activated nodes and finally stops at a certain distance, with the argument that the strength of the relation decreases with their semantic distance.

Fan-out constraints : nodes of high out-degree are not activated to avoid excessive spreading.

Path constraints : activation spreads across preferential paths that reflect specific inference rules.

Activation constraints : to avoid the activation of all nodes that receive activation at all, a threshold function can be applied. In this case a certain degree of input activation is required to activate the node.

Crestani [4] argues that constraints are necessary, because pure (constraint-free) spreading activation has three serious drawbacks: 1. Activation spreads over the entire network. 2. Semantics of relations, represented as edge labels can 3. The implementation of an inference procedure based We show that the standard approach described above ex-hibits a crucial fourth drawback, namely convergence to a single query-independent state. From Eq. (1) it is obvious that pure linear activation spread-ing corresponds to power iteration with matrix W . From the Perron-Frobenius Lemma it is well known (see, e.g., [5]) that power iteration converges to the unique (up to scaling) principal eigenvector of W , if W is irreducible and aperiodic, or, in graph terminology, G is connected and not bipartite. These conditions are most often fulfilled in information re-trieval scenarios.

The absolute value of the largest eigenvalue of W is called the spectral radius,  X  ( W ). In network analysis, its associ-ated eigenvector is commonly known as eigenvector central-ity [1]. Its entries are guaranteed to have the same sign and induce a reasonable ranking reflecting a global notion of each node X  X  importance in the graph structure.

Consequently, after a sufficient number of iterations, the activation vector will approach eigenvector centrality. While this may actually be a reasonable default answer, it is not at all related to the query. Clearly, query-independence is not desirable in information retrieval. While the constraints used in typical applications of spreading activation avoid this problem, their effects are difficult to assess theoretically. To avoid query independence we pursue a different approach here. The iterations are modified to take more than just the directly preceding state into account. Thereby the interme-diate states following the initial state (the query) are able to influence the final result. Three seemingly natural such attempts are discussed below.
 A straightforward approach whereby the entire iteration his-tory is taken into account is to define the final activation as the sum of all intermediate states. While the iteration con-verges, the sum of intermediate states will not unless we introduce a vanishing series of weights. Hence consider the accumulated activation where  X  ( k ) is a decay function ensuring convergence.
Note that choosing  X  ( k ) =  X  k for a constant  X  &gt; 0 yields a variant of another well-known centrality index: Katz X  sta-tus [6] is defined as c Katz ( W ) = is the vector of all ones and  X  &lt;  X  ( W )  X  1 guarantees con-vergence. The status attribution matrix equals be interpreted as a global importance ranking biased by a query-defined a (0) .

Usually, n is large and W is sparse, so that precomput-ing the fully populated limit matrix is prohibitive. The computation can nevertheless be accelerated by noting that a  X  = ( I  X   X W )  X  1 a (0) and applying methods for sparse ma-trix inversion.
 A seemingly different approach is the constant renewal of the initial activation as in a ( k ) = a (0) + W a ( k  X  1) substitution shows, however, that a ( k ) = therefore this is actually the special case  X  ( k ) = 1 of the previous approach, and convergence is guaranteed only for  X  ( W ) &lt; 1.
 A frequently applied smoothing method adds inertia to an iteration process by partially retaining the previous state ( I + W ) k a (0) . We are thus only modifying the influence weight matrix in a way that corresponds to adding self loops of unit weight to each node. Consequently, the final activa-tion is an eigenvector corresponding to the dominant eigen-value of I + W , and therefore continuous to be the princi-pal eigenvector of W . This is because adding I adds one to every eigenvalue without changing the associated eigen-vectors. Moreover, convergence of power iteration may be slowed down, as the ratio of the largest and second-largest absolute eigenvalue may be reduced. On the other hand, the potential problem of bipartiteness is avoided. Of the approaches above, accumulation turned out to be the only candidate worth investigating, since the final rank-ing of inertia is equal to that of pure spreading activation and the convergence of activation renewal is not guaran-teed. Choosing an appropriate decay function, however, is difficult. Even for  X  ( k ) =  X  k we need to know the spectral radius  X  ( W ) for a reasonable choice of  X  .

In contrast, accumulating normalized activation vectors is much easier, because a converging series is obtained for any  X   X  (0 , 1), and it is indeed appropriate to normalize the total activation after each iteration to avoid numerical problems during power iteration.

Consider therefore the normalized power iteration a ( k ) W a ( k  X  1) / k W a ( k  X  1) k , and assume that k X k is the l (our arguments apply to other norms as well). The follow-ing theorem implies that the normalized iteration not only simplifies the choice of a decay function for accumulation, but also removes the need for conditions on W .

Theorem 1. Given any matrix W  X  R n  X  n and  X   X  [0 , 1) , a vector a  X   X 
Proof. Since k a ( k ) k = 1, | a ( k ) i |  X  1  X  i  X  V . Therefore a Since lim As an illustrative application we consider relatedness queries in a database of term-document relationships. The example is also interesting because it renders iterated cosine similar-ity a special case of spreading activation. Consider a graph G = ( V, E ; w ) in which the nodes V = D ] T are partitioned into documents D and terms T . Edges E = {{ d, t } : d  X  D, t  X  T, w ( d, t ) &gt; 0 } with weights w : D  X  T  X  R  X  0 describing the relations between docu-ments and terms. A common choice would be tf-idf (term frequency, inverse document frequency) values. Without loss of generality, we assume that G is connected.

As noted in [11], spreading activation can be instantiated for this bipartite network in such a way that activation after a single step is equal to the standard cosine similarity mea-sure between a virtual query document and all documents in the network. This is achieved by defining a spreading function for each class of the bipartition as follows.
Let every iteration consist of two phases, one in which documents activate terms, and another in which terms acti-vate documents. The spreading function for terms in T is a normalized weighted sum of document activations, The activation of documents is updated analogously, but by using term activations from within the same iteration, Given a set of query terms d q (which may be seen as a vir-tual document representing the query terms), let d q be the corresponding normalized term-vector space representation. That is ( d q ) t = 1 / k d q k , if term t is part of the query and otherwise 0. Furthermore let d = ( w ( d, t 1 ) , . . . , w ( d, t be the vector representation of document d  X  D . Then the cosine similarity of d q to every document d in the network is calculated by activating the terms contained in d q , i.e. a t := ( d q ) t , and spreading their activation to the docu-ments. The above spreading functions are of course chosen such that a (0) d = cos( d , d q ). When the next step is executed, i.e. the activation is spread from documents to terms, their new activation represents a new virtual document and after spreading this back to documents, their activation represents their cosine similarity to this new virtual document, and so on. Let W  X  R D  X  T be the weight matrix of G , i.e. ( W ) d,t w ( d, t ) for all d  X  D and t  X  T . In addition, let W D be the l 2 -row-normalization of W , ( W D ) d,t = w ( d, t ) / k d k and let W T  X  R T  X  D be the l 2 -row-normalization of its trans-ing term t  X  T analog to the document vectors. Further-more, let a ( k ) D and a ( k ) T denote the activation of documents and terms after k iterations. In matrix-vector notation, the above spreading functions become a ( k ) D = W D  X  a ( k ) Figure 1: 2D-projections of state trajectories in the alternating cosine similarity application with accu-mulated spreading activation. power iteration representing Since G is connected, the graph underlying W T W D is con-nected and not bipartite, therefore the power iteration con-verges and the considerations of the previous sections apply in this scenario. To illustrate the analytical results we applied the above described method to the TIME Magazine dataset of the SMART test collection. 1 This dataset consists of 425 doc-uments and 83 queries together with relevance judgements for the documents.

The example is not intended to be a performance bench-mark, but rather an illustration of the influence and conver-gence behavior of accumulation decay factor  X  . Therefore, in contrast to other experimental evaluations, the precise characteristics of the dataset are not important here. In-stead, we compare the state trajectories of pure spreading activation and spreading activation with accumulation on randomly chosen queries.

Figure 1 shows the trajectories of the iteration processes on four sample queries. Intermediate states are projected onto two dimensions using multidimensional scaling of the dissimilarities dis( r 1 , r 2 ) = 1  X  cos( r 1 , r 2 ). These angular dissimilarities were chosen mostly to support geometric in-terpretation of the iterative progression. Clearly, for com-parison of two rankings, other metrics such as rank inversion distance would be more appropriate. This is because activa-tion vectors resulting in equal rankings would be projected onto the same coordinates, whereas cosine-based dissimilar-ities differentiate between different activation vectors imply-ing the same ranking.

Trajectories of the intermediate states of pure spreading activation are depicted in gray. Black arrows indicate state trajectories of spreading activation with accumulation. Con-secutive intermediate states a ( k ) d are connected by a line, starting at a (0) d , which is represented by a dot. This initial activation equals the cosine distance of the documents D and the query document d q . It is evident from the trajectories ftp://ftp.cs.cornell.edu/pub/smart that all results converge very quickly to the same default result, i.e. the principal eigenvector of the system. With increasing  X  the accumulation approach yields activations resembling the query-independent one of pure spreading ac-tivation. This suggests that the choice of  X  represents what can be interpreted as the trade-off between narrow and more generalist responses to relatedness queries. Commonly-used spreading activation strategies constitute linear systems with additional constraints. Without these constraints, the approaches are equivalently described by power iteration converging to a query-independent state. Constraints therefore crucially influence the actual output, and their effects should be investigated more thoroughly.
Based on weighted accumulation of normalized activa-tion states, we propose an alternative strategy to introduce query-dependence in a controlled way. Results can be geared toward the initial query or a global response of pure spread-ing activation. This method is independent of other con-straints and can be combined with them freely. [1] P. Bonacich. Factoring and weighting approaches to [2] P. Cohen and R. Kjeldsen. Information retrieval by [3] A. Collins and E. Loftus. A spreading-activation [4] F. Crestani. Application of spreading activation [5] G. H. Golub and C. F. van Loan. Matrix [6] L. Katz. A new status index derived from sociometric [7] M. Quillian. Semantic memory. In M. Minsky, editor, [8] D. Rumelhart and J. McClelland. Parallel Distributed [9] G. Salton. Automatic Information Organization and [10] G. Salton and C. Buckley. On the use of spreading [11] R. Wilkinson and P. Hingston. Using the cosine
