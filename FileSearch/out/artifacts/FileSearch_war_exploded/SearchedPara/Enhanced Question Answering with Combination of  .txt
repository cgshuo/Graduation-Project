 QA. Among these questions, some are focused on  X  X ecord information X  like World Record or Olympic Record. These questions are defined as factoid questions, which answer questions for which the correct response is a single word or short phrase from the answer sentence. However, most of user X  X  frequently asked questions are not only factoid questions but also definitional questions. There are many questions, named answers should be prepared for some questions, while traditional QA finds appropri-ate answer in real-time[2, 3, 4]. such as web documents, electronic newspaper, and encyclopedia[5, 6]. Especially, encyclopedia contains the facts which were already approved or occurred in the past. particular forms. 
In this paper, we propose enhanced QA model by combining various pre-acquire answers. We define pre-acquired answer types as answer types which can be extracted in advance as candidate answers of question types that are frequently asked. Our talk will be developed with two directions: both record information questions and descrip-these questions. 
Firstly, we focus on record information questions, such as  X  X hich is the highest sentences which reveal the record information as  X  record sentence  X . Record informa-tion questions have some characteristics. Its length is short, so it has few queries. And trieval step. 
As it was mentioned early, QA system to process record question can be classified record information generated by manual[8]. AnswerBus tm [7] does real-time search, so response time is quite long. It also perform low accuracy because it dose not consider response time. But it has difficulties which are long term of update and lack of flexi-bility. To solve problem in prior, we define template that can express particular con-text in record sentence. 
Second point of views is descriptive question. Descriptive question are questions such as  X  X hat is tsunami? X  or  X  X hy is blood red? X  which need answer that contain the definitional information about the search term, explain some special phenomenon.(i.e. chemical reaction) or describe some particular events. At the recent works[4, 5, 6, 8], definitional QA, namely questions of the form  X  What is X?  X , is a developing research The systems in TREC-12[8, 10] applied complicated technique which was integrated manually constructed definition patterns with statistical ranking component. Some experiments[5, 6, 11] tried to use external resources such as WordNet and online knowledge bases on web[4]. Domain-specific definitional QA systems in the same context of our works have been developed. [12] applied on biographical summa-ries for people with data-driven method and [13] proposed mining topic-specific defi-descriptive question, such as  X  X hy X ,  X  X ow X , and  X  X hat kind of X . Sentences including record information generally have specific words which indicate that the sentence is record sentence. We defined specific words as  X  center vocabulary X  to identify record information and determined Record Type(RT) by it . We defined sentences which contain the center vocabulary as  X  record sentence  X .
 Exam1 : Fish House of the Britain is the first aquarium in the world Exam1 is a record sentence and  X  the first  X  is center vocabulary. By means of encyclo-pedia sample contents, we defined 55 center vocabularies. Table 1 shows examples of record sentences and center vocabularies. 
Our QA system is a domain specific system for encyclopedia. One of the character-istics of encyclopedia is that it has many descriptive sentences. Because encyclopedia contains facts about many different subjects or about one particular subject explained for reference, there are many sentences which present definition such as  X  X is Y . X  On the other hand, some sentences describe the process of some special event(i.e. the 1st World War) so that it forms particular sentence structures(5W1H) like news article. 
We defined Descriptive Answer Type (DAT) as answer types for descriptive ques-tions with two points of view: what kind of descriptive questions are in the use X  X  fre-quently asked questions? and what kind of descriptive answers can be patternized in the our corpus? The result of analyzing the logs of our web site shows that there are many questions about not only  X  X hy X  and  X  X ow X  but also definition. Descriptive an-swer sentences in corpus show particular syntactic patterns such as appositive clauses, parallel clauses, and adverb clauses of cause and effect. In this paper, we defined 10 types of DAT to reflect these features of sentences in encyclopedia. 
Table 2 shows example sentences with pattern for each DAT. For instance,  X  X  tsu-nami is a large wave , often caused by an earthquake. X  is an example for  X  X efinition X  DAT with pattern of [X is Y]. It also can be an example for  X  X eason X  DAT because of matching pattern of [X is caused by Y]. defining templates or patterns which express context and 2) extracting Answer Unit(AU) and storing storage. 3.1 Record Answer Indexing 3.1.1 Defining RAIT sentence,  X  X his song was made on the most sensitive season. X  has  X  the most  X  as center and term. For example, Exam1 has structure of  X  B of A is the first D in C  X  and restric-tion which is A: AT_COUNTRY|GEN 1 , B: TAG_NN|SUBJ 2 , C: MORP_WORLD 3 and D: TAG_NN+CP 4 . 
We define Record Answer Indexing Template (RAIT) that can represent contextual matches with the context. RAIT consists of two pieces of information: context restriction and RIU . RIU (Record Indexing Unit) is what is worthy to be extracted for record QA. Context Information: We put restrictions on Eojeol 5 as following by all 5. It is pos-sible that two more restrictions simultaneously exist in one Eojeol. z Morpheme restriction: Morpheme in formation that appear to Eojeol z POS(Part of Speech) 6 restriction: POS information that appear to Eojeol RIU: RIU consists of 6 units 3.1.2 Extracting RIU This is the step that extracts RIU using RAIT. Fig.1 shows RIU indexing process. The structure has  X  A is the first C in B  X .The following is explanation of context restriction for each Eojeol. z B: It is the left first Eojeol from center vocabulary, and its morpheme has to z C: It is the right first Eojeol from center vocabulary and its morpheme has to From indexing information, we know that A, B, and C have Answer, Location, and satisfies the context restriction of template 101, so the system extracts [Loca-tion=world], [AK=computer], and [Answer=ENIAC] as RIU. 3.2 Descriptive Answer Indexing 3.2.1 Descriptive Pattern Extraction Y ,.. and Y n .  X  
To extract descriptive answers using a pattern matching technique, we must con-struct descriptive patterns from pre-tagged corpus using DAT workbench. The DAT workbench was built to help DAT tagging and descriptive pattern extraction by man-ual. Extraction of descriptive patterns processes in a number of steps. First, we manu-ally construct initial surface patterns using training corpus with 10 DAT tags. Second, patterns to adapt to variation of terms. 
For building initial patterns, we constructed pre-tagged corpus with 10 DAT tags, then performed alignment of the surface tag boundary. The tagged sentences are scriptive clue terms and structures, such as  X  X is caused by Y  X  for  X  X eason X ,  X  X was workbench. extend initial patterns automatically. Initial patterns are too rigid because we look up tance in a sentence, it can fail to be recognized as a pattern. To solve this problem, we added sentence structure patterns on each DAT patterns, such as appositive clause patterns for  X  X efinition X , parallel clause patterns for  X  X ind X , and so on. 
Finally, we generalized patterns to conduct flexible pattern matching. We need to sentences. Several similar patterns under the same DAT tag integrated regular-expression union which is to be formulated Finite State Automata(FSA). For example,  X  X efinition X  patterns are represented by [X&lt;NP&gt; be called/named/known as Y&lt;NP&gt;]. 3.2.2 Extracting DIU stage performed pattern matching, extracting DIU, and storing our storage. We built a pattern matching system based on automata. After pattern matching, we need to filter-when  X  X  X  and  X  X  X  under the same meaning on our ETRI-LCN for Noun 7 ontology. For example,  X  X [Customs duties] are Y[taxes that people pay for importing and ex-porting goods] X  are accepted because  X  X ustom duties X  meaning is under the  X  X axes X  as a child node. Fig. 2 illustrated result of extracting DIU. DIU consists of Title, DAT tag, Value, V_title, Pattern_ID, Determin_word, and Clue_word. Title and Value means X and Y respectively. V_title is distinguished from Title by whether X is an entry in the ency- X  X he 1 st World War X  entry,  X  X t X  means title,  X  X he 1 st World War X . that general QA retrieves answers in real-time when a user question entered. To find appropriate answers in our pre-built knowledge, we perform query processing to iden-tify what kinds of question user wants, especially looking for RIU or DIU. 4.1 Record Answer Retrieval In the same meaning of record sentence in corpus, question which ask record informa-tion also have particular context. We define Record Question Indexing Template (RQIT) that can express contextual information of record question, like RAIT. RQIT have same restriction with RAIT and Question RIU (QRIU) of RQIT consists of 5 process that give answer to record question. 4.2 Descriptive Answer Retrieval Descriptive answer retrieval performs finding DIU candidates which are appropriate to user questions through query processing. The important role of query processing is word in a question. We used LSP 8 patterns for question analysis. Another function of query processing is to extract Determin_word or Clue_Terms in question in terms of determining what user questioned. Fig. 4 illustrates the result of QDIU(Question DIU) and candidate answers. 
If we find DIU candidates which are mapped to QTitle of QDIU exactly, the can-didate has weight of 1 through 0.7. Otherwise, if DIU candidate with V_title mapped to QTitle, it has weight of 0.5 because candidate with V_title means that it X  X  not cer-tain information. However, our weighting strategy will be improved forward. sub categories in Korean. Our encyclopedia contains many record sentences and defi-nitional sentences which explain subject, describe its sub-components, and classify sub kinds. 5.1 Coverage of Pre-acquired Answer We extracted 74,203 record sentences from whole contents of encyclopedia using center vocabulary and constructed 4,118 RIUs. Table 3 shows the result of record sentences and RIUs by center vocabulary. We built 183 RAITs for extracting RIU and 63 RQITs for analyzing record question. 
From table 3, Record sentences with  X  the ~est  X  and  X  the first  X  is 37% and 37.6% re-spectively, and RIU also is 37.4% and 36.1%.  X  the ~est  X  is center vocabulary for sev-making RAIT. 
To extract descriptive patterns, we built 1,853 pre-tagged sentences within 2,000 sentences were assigned to  X  X rinciple X . Table 4 shows the result of extracted descrip-tive patterns using tagged corpus. 408 patterns are generated for  X  X efinition X  from 760 tagged sentences, while 938 patterns for  X  X unction X  from 352 examples. That means the sentences of describing something X  X  function formed very diverse expressions. We integrated patterns to 279 regular-expression unions which are to be formulated as automata. 
Table 5 shows the result of DIU indexing. We extracted 300,252 DIUs from the whole encyclopedia using our Descriptive Answer Indexing process. Most DIUs(about 50%, 164,327 DIUs) are  X  X efinition X  and 14%(45,802 DIUs) are  X  X ind X . We assumed that the entries belonging to the  X  X istory X  category had many sentences about  X  X eason X  because history usually describes some events. However, we obtained only 25,110 DIUs of  X  X eason X . The reason for the small fragment is because patterns of  X  X eason X  have lack of expressing syntactic structure of adverb clauses of cause and effect.  X  X rinciple X  also has same problem of lack of patterns. 5.2 Evaluation of Use of Acquired Answer in QA To evaluate effects of pre-acquired answers, we built test collection, named ETRI QA Test Set[2] judged by 4 assessors. ETRI QA Test Set 2.0 consists of 1,047 &lt;question, encyclopedia. For performance comparisons, we used Top 1 and Top 5 precision, recall and F-score. Top 5 precision is a measure to consider whether there is a correct answer in top 5 ranking or not. Top 1 measured only one best ranked answer. 
For our experimental evaluations we constructed an operational system in the Web, named  X  X nyQuestion 2.0. X  To demonstrate how effectively our model works, we compared to traditional QA engine for record answer and to a sentence retrieval sys-tem for descriptive answer. While we built a question answering system for short answers, we did not compare our descriptive model with short answer engine because descriptive questions need long answers like sentences. 5.2.1 Effect of Record Answer For testing our record QA system, we used 510 questions of ETRI QA test Set 2.0 and left 537 questions for other test. In the 510 questions, there are 61 record questions. 
We expected that QA precision about record questions would be lower than gen-eral questions. So we compared QA performance for record questions with general questions in traditional QA system. Table 6 shows the result with Top 5 measure. 
As the second test, we compared our proposal record system with traditional QA system. As shown in Table 7, the precision of proposal system improved by 34.8%. In plate based record QA is very effective. System # of Q Retrieved Corrected Precision Recall F-score Our Record QA 61 22 21 0.955 5.2.2 Effect of Descriptive Answer posed method are higher than that of traditional sentence retrieval system. As ex-pected, we obtained better result(0.625) than sentence retrieval system(0.508). We gain 85.5%(0.290 to 0.538) increase on Top1 than sentence retrieval and 23.0%(0.508 creased, in sense that question answering wants exactly only one relevant answer. 
Whereas the difference between the recall of sentence retrieval system(0.507) and descriptive QA result(0.520) on Top5 is small, the F-score(0.508) is quite lower than more number of candidates retrieved. While sentence retrieval system retrieved 151 candidates on Top5, our descriptive QA method retrieved 101 DIUs under the same condition that the number of corrected answers of sentence retrieval is 77 and ours is 79. That is the reason why our descriptive QA model obtained better precisions both on Top1 and Top5. the inverted retrieval which should answer to the quiz style questions, such as  X  X hat descriptive patterns. This problem has influence on retrieval results, too. 5.2.3 Effect of Combining Pre-acquired Answers As the last analysis, our proposed model combined pre-acquired answers, record an-The comparison with the left 373 questions in 510 questions is needless because these same mechanism. Our traditional QA[2] focus on factoid questions and answer with  X  X hort answer X  like previous works. That QA also used language analysis component which include AT tagging, chunking and parsing. The result on Top 5 showed in Table 9. As we expected, F-score(0.608) of our combining model is higher than traditional QA(0.389). We can see the same results in Precision and Recall, too. Although the F-score(0.608) of our combining model is a little lower than sentence retrieval re-sult(0.614), we nevertheless emphasis the fact that the precision(0.825) of ours is even higher than that(0.630). That means our proposed model showed enhanced perform-ance in QA. Our proposed QA 80 66 0.825 0.482 0.608 Traditional QA 53 37 0.698 0.270 0.389 
Sentence Retrieval 130 82 0.630 0.598 0.614 We have proposed enhanced QA model with combination of various pre-acquired answers for record and descriptive questions. We also presented the result of a system which we had built to answer these questions. To reflect characteristics of record and descriptive sentences in encyclopedia, we defined 55 RTs by center vocabulary and 10 DATs as answer types. We explained how our system constructed descriptive dexing process. We had shown that our proposed models outperformed the traditional QA system with some experiments. We evaluated the coverage of indexing unit can-pre-acquired answer in QA. For record questions, we compared with traditional QA system. The result showed that precision of proposal record QA model is higher than traditional model by about 35% on Top5. And in descriptive QA, we obtained F-score sentence retrieval system on both Top1 and Top5. From the overall comparison, we prove that our proposed model which combined pre-acquired answers has enhanced. 
In record QA, our model has problem that it can not extract RIU from complex sentences. The recall of our system becomes low due to limitation of target sentences. automatic pattern generation, we will try to apply machine learning technique like the boosting algorithm. We also try to improve performance of our system by analyzing results of individual DATs, to figure out which DATs shows poor accuracy. More urgently, we have to build an inverted retrieval method. Finally, we will compare with other systems which participated in TREC by translating definitional questions of TREC in Korean. 
