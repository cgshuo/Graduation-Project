 Estimating the cardinality of unions and intersections of sets is a problem of interest in OLAP. Large data applications often require the use of approximate methods based on small sketches of the data. We give new estimators for the cardi-nality of unions and intersection and show they approximate an optimal estimation procedure. These estimators enable the improved accuracy of the streaming MinCount sketch to be exploited in distributed settings. Both theoretical and empirical results demonstrate substantial improvements over existing methods.
  X  Mathematics of computing  X  Probabilistic algo-rithms;  X  Theory of computation  X  Sketching and sampling;  X  Computing methodologies  X  Distributed algorithms; cardinality estimation; data sketching; randomized algorithms
Consider a dataset D . The basic approximate distinct count problem is to construct a memory efficient summa-rization S of D and to estimate the cardinality of the set of unique items A using just the sketch S . Under this ba-sic formulation, a sketch answers a cardinality question that must be specified before computing the sketch. We consider the extended problem of estimating the cardinality of set sketches. This has two important consequences. First, com-binatorially many cardinality questions can be accurately answered by sketches. Second, the significant improvements in accuracy using the HIPS or optimal martingale estimator of [10] and [23] can be realized in distributed settings. More specifically, we wish to define union  X   X  and intersection  X   X  operations on sketches and derive cardinality estimates that are optimal or nearly optimal.

This problem of estimating the number of distinct ele-ments in a dataset in a memory efficient manner appears in a wide range of applications. For example, in data ana-lytics and OLAP, a web company may count distinct users accessing a service [21]. The summarizations are used in networking to detect denial of service attacks by counting network flows [15], and in databases to optimize query plans [22]. Other applications include graph analysis where they are used to estimate the diameter of a graph [9], [5].
The extended problem for unions has particular impor-tance in OLAP and distributed settings. The streaming cardinality estimator given by [23], [10] is optimal in the sense that no unbiased estimator has lower asymptotic vari-ance. This is much stronger than typical space-complexity results in the theory literature [2], [19] that just guarantee an optimal rate. It guarantees optimality of the constant in front of the rate. This optimality is evident in practice as the estimators require half the space of existing meth-ods to achieve the same error on MinCount sketches. The streaming estimator, however, cannot be applied directly in distributed settings. In a map-reduce setting, the map-pers computing efficient streamed sketch summaries need a method to combine them at a reducer. A union operation on the sketches would allow the efficiency gains to be trans-fered to distributed settings and turn the streamed sketch into a mergeable summary [1]. These efficiency gains can be tremendous as empirical results show efficient cardinal-ity estimation for unions can reduce variance and the space requirements by an order of magnitude when many sketches are merged.

In OLAP settings, one is often interested in cardinality estimates on multi-dimensional data, for example counting the number of users accessing a web service by geographic location, time window, and other factors [21]. This results in exponentially many cardinalities to estimate. Unions and intersections allow a limited number of sketches to answer a multitude of cardinality questions.

Cardinality estimates for intersections are not as well stud-ied as for unions. While most sketches for approximate dis-tinct counting have a natural but inefficient union operation, they do not have one for intersections. Intersection car-dinality estimates are often computed using the inclusion-exclusion principle or by using Jaccard similarity [13], [4] when an estimator is available. These methods still require first accurately estimating the cardinality of the union or the Jaccard similarity of a set with the union.
Our contributions are as follows. We introduce two tech-niques for computing unions and intersections on approx-imate distinct counting sketches: pseudo-likelihood based methods and re-weighting component estimators. These are applied to Streaming MinCount sketches in this paper but may be applied to other sketches as well. The resulting es-timators demonstrate state of the art performance on both real and synthetic datasets. The pseudo-likelihood based MinCount method is conjectured to be asymptotically ef-ficient as it is nearly the same as the asymptotically ef-ficient maximum likelihood estimator. This establishes a nearly ideal baseline for comparison. Re-weighted compo-nent estimators are much simpler to implement and gener-alize, and they are empirically shown to be as efficient as the pseudo-likelihood based estimators. We also derive vari-ance estimates which allow confidence intervals to be given for the cardinality estimates. In the special case where the streams are different permutations of the same set, we show that merging the streaming estimates yields a more accu-rate estimate than the estimator on a single stream. Thus, unlike existing methods, the union operation on streaming sketches exploits information about the order of elements in each stream. The variance after averaging over all possible orderings is shown to be 2 / 3 the variance of the streaming MinCount estimator. In addition to improved estimation, the resulting methods yield mergeable summaries [1] under both union and intersection operations unlike existing meth-ods.
The basic approximate distinct counting problem has been well studied in the literature beginning with the seminal pa-per by Flajolet and Martin [17]. Methods include Linear Probabilistic Counting (LPCA) [25], HyperLogLog [16], the Multiresolution Bitmap [15], the S-bitmap [7],  X  -stable dis-tribution methods [12], and MinCount [18]. Kane, et al [19] also gave a method that is optimal in terms of space-complexity.

As described in [23], all these methods generate a stochas-tic process called the area process that is governed solely by the number of distinct elements encountered. A sketch S can be mapped to a set R ( S )  X  (0 , 1) called the remaining area. Given a random hash, each element in a stream is hashed to some value U . If U is still in the remaining area, U  X  R ( S ), the sketch is updated by cutting out a portion of the remaining area containing U . Otherwise, it does not affect the sketch. Since a value U cannot be cut out twice, duplicates do not affect the sketch. Under the assumption that the hash function is a strong universal hash, the original distribution of the data is irrelevant. A sketch is a function of the independent Uniform (0 , 1) random variates that are the hashed distinct items. It is a random quantity whose distribution depends only on the number of distinct items. Cardinality estimation is thus a statistical parameter esti-mation problem where the sketch is the observed data.
Using an optimal estimation procedure can lead to sub-stantial efficiency gains. For example, the LogLog and Su-perLogLog methods [14] share the same sketch as Hyper-LogLog and differ only in the estimation procedure. How-ever, LogLog requires 1.56 times the memory of Hyper-LogLog to achieve the same error and 2.4 times the memory of Streaming HyperLogLog. Likewise, the original cardinal-ity estimation method for the  X  -stable sketch proposed in [12] uses over twice the memory as that used in [8]. The dis-advantage of using the optimal streaming methods described in [23] and [10] is that they require the data to arrive in a single stream. They are not immediately applicable in dis-tributed settings or in OLAP settings where the sketches are pre-aggregations that need to be further merged.
We first consider basic estimation procedures for unions and intersections on distinct counting sketches. Many sketches used in approximate distinct counting have a natural union operation. For example, the LPCA sketch is a bitmap and is equivalent to a Bloom filter with the number of hashes k = 1. Taking the union of LPCA sketches simply takes the bitwise OR of the sketches. The resulting sketch is identi-cal to the sketch computed on the union of the original sets. This property holds for other sketches such as HyperLogLog [16] and MinCount [18] as well. In other words, there is an operation  X   X  such that S ( A 1 )  X   X  S ( A 2 ) = S ( A is the function that generates a sketch from a streamed set. For notational convenience, we will denote S i = S ( A i ).
Unlike the union, typically no natural intersection oper-ation exists for approximate distinct counting sketches. A simple method to obtain cardinality estimates for the inter-section is to use union estimates and the inclusion-exclusion where there is an estimate  X  J ( S 1 ,S 2 ) for the Jaccard similar-ity, two existing intersection cardinality estimator are given by [4] and [13]. Throughout, we will use a hat to denote estimated quantities, with  X  N ( S ) denoting a cardinality esti-mate using sketch S . The naive and Jaccard based estimates of the cardinality of the union and intersection are given by These estimation strategies turn out to be suboptimal. For the intersection estimators, one reason is that the error is often roughly proportional to the size of the union or the larger set, while a good procedure should give error that is bounded by the size of the smaller set. This can also lead to pathologies where the naive cardinality estimate of the in-tersection is negative. If a simple correction to replace these negative estimates by zero is used, the resulting estimator is provably biased.
In this paper we focus on applying new estimation tech-niques to Streaming MinCount sketches. The techniques may be used for other sketches, and their application is dis-cussed in section 13. The choice of Streaming MinCount is driven by two reasons. The first is that it simplifies the calculations since collision probabilities are negligible. The second is that the uniqueness of hash values allows for a closed intersection operation and gives more flexibility to the sketch.

The basic MinCount sketch stores the k minimum hash values and is also known as the K-minimum values [2] or bottom-k [11] sketch. When the hash values are uniformly distributed on (0 , 1), the estimator for the MinCount sketch is ( k  X  1) / X  where  X  is the largest stored hash value. It is easy to see that this estimator approximates the cardinality since the k th smallest hash value out of n roughly uniformly spaced values is approximately k/n . It can be shown that the estimator is the unique minimum variance unbiased esti-mator [6] when only the final set of hash values is observed. The Streaming MinCount sketch augments the basic Min-Count sketch with a running estimate of the cardinality. This estimate is given by where  X  t is the threshold for sketch S after encountering t elements and Z t = 1 if the sketch changed at time t . The threshold for a sketch is the largest stored hash value. The streaming estimator operates by incrementing the running estimate by 1 in expected value for each newly encountered element. By exploiting the sequence of sketches rather than just the final sketch, the streaming update procedure reduces the variance of the estimator by half.
The usual union operation throws away valuable informa-tion for the MinCount sketch. To see this consider the case where the sets A 1 ,A 2 are disjoint and of equal cardinality. The best estimator would simply add  X  N ( S 1  X  S 2 ) =  X   X  N ( S 2 ) which has variance ( | A 1 | 2 + | A 2 | 2 ) /k = | A However, the MinCount sketch throws away half of the hash values to maintain the size limit of k hash values. The result-ing union estimate correspondingly has twice the variance, | A 1  X  A 2 | 2 /k . This is the strategy employed by [4].
We introduce a simple improvement by constructing the largest possible merged MinCount sketch rather than fixing the size at k . Let  X  ( S ) be the threshold, the largest stored hash, for sketch S . Let h ( S ) denote the set of hashes stored in sketch S and h ( S, X  0 ) be the set that is  X   X  0 . For conve-nience, denote  X  i =  X  ( S i ). Improved union and intersection operators  X  0 ,  X  0 on MinCount sketches are defined by h ( S 1  X  0 S 2 ) := h ( S 1 , X  min )  X  h ( S 2 , X  min ) h ( S 1  X  0 S 2 ) := h ( S 1 )  X  h ( S 2 ) In other words, the hash values larger than the minimum threshold are discarded, and the sketches are merged by tak-ing the union or intersection of the remaining hash values. The resulting union sketch is exactly the same as a Min-Count sketch of size | h ( S 1  X  S 2 ) | constructed from A The intersection sketch is similar but allows for the threshold to not be in the intersection. This yields a closed intersection operator. It generates a new sketch and not just an estimate of the cardinality. The union and intersection estimators are This improvement may also be applied to the Discrete MaxCount sketch introduced by [23]. However, there is no obvious generalization of this improvement to the Hyper-LogLog sketch. For the Streaming MinCount sketch, al-though it does not define a union or intersection cardinality estimator that exploits the gains from the streaming esti-mates  X  N ( S i ), it allows a closed intersection operation to be defined.
The improvements proposed in the previous section are suboptimal in terms of statistical efficiency. Statistical ef-ficiency is a far more stringent optimality criterion than optimal space-complexity when comparing estimators. A consistent estimator  X   X  n of a parameter  X  is asymptotically (statistically) efficient if its asymptotic variance is equal to the Cramer-Rao lower bound on the variance [24]. Whereas space-complexity ensures that the error rate is optimal, asymp-totic efficiency ensures both the rate and the constant factor governing the rate are optimal. This is an important dis-tinction, since the rate is typically meaningless for estima-tion. Under mild regularity conditions, parameter estima-tion problems on i.i.d. observations invariably have an opti-mal rate of  X (1 / procedure achieves that rate. Only the constant governing the rate yields meaningful comparisons for estimators. By contrast, space-complexity is meaningful when considering the problems of constructing and encoding a sketch but not for estimation [23].

Under mild regularity conditions, the maximum-likelihood estimator (MLE) is an asymptotically efficient estimator. Although we are not able to derive the exact MLE, in section 8.2 we derive both the conditional likelihood given the car-dinality of the input sets as well as a full pseudo-likelihood. We conjecture that maximizing the full pseudo-likelihood gives an estimator that is asymptotically equivalent to the MLE. Although a full proof is out of the scope of this paper, we give a heuristic proof sketch that may be able to establish the asymptotic efficiency.

In addition to the class of likelihood based estimators, we derive a class of re-weighted estimators based on linear com-binations of the streaming cardinality estimates. These esti-mators have the advantage of being easy to implement and easy to generalize to other sketches. Although we show how to optimally weight the linear combinations, the resulting estimators are not theoretically guaranteed to be asymptot-ically efficient, but empirical results suggest that they are efficient or close to efficient.
The remainder of the paper focuses on exploiting the ef-ficiency gains from streaming estimation to improve cardi-nality estimation for union and intersection.

To fix some notation, suppose each sketch S i contains k i hash values and estimates the cardinality | A i | = n total cardinality is n tot = | X  i A i | . The threshold  X  sketch S i is the maximum stored hash value. The propor-tion of the total that belongs to set A i is denoted by p | A i | /n tot . For simplicity, only pairwise unions and intersec-tions are analyzed in detail, but merging multiple sketches is discussed in section 11 In the pairwise case, define q | A 1  X  A 2 | /n tot ,q 1 = | A 1 \ A 2 | /n tot , and q 2 = | A
For the analysis of the methods, we consider the asymp-totic regime where for each i , k i /n tot  X  0 and q i  X  c as n tot  X   X  for some constants c i . The hash function is assumed to be a strong universal hash that generates i.i.d. Uniform (0 , 1) random variates.
As described in section 2, cardinality estimation is a sta-tistical parameter estimation problem. Under this formula-tion, statistical theory provides two important results. The useful pieces of information in the sketch are encoded by sufficient statistics, and the maximum likelihood estimator is an asymptotically efficient estimator.
The notions of sufficiency and ancillarity are important in statistical estimation. A statistic is sufficient if it contains all the information necessary to reconstruct the likelihood func-tion. Under the likelihood principle, the likelihood contains everything useful for parameter estimation. A statistic is an-cillary if it is irrelevant for estimating the parameter. More formally, it is a statistic whose distribution does not depend on the parameter of interest. This gives a basic strategy for finding good estimators. Find the smallest sufficient statis-tic containing as few ancillary statistics as possible. Propose an estimator that is a function of the sufficient statistic.
For the MinCount sketch, the exact values of the stored hashes relative to the threshold are irrelevant for estimation and, hence, ancillary statistics. The value of the threshold, the largest stored hash, is a sufficient statistic when only the final set of hashed values is observed. Furthermore, this threshold is a complete and minimal sufficient statistic, so by the Lehman-Scheffe theorem, the usual MinCount estimator is a minimum variance unbiased estimator as shown by [6].
Likewise, the exact values of the stored hashes are irrel-evant when estimating the cardinality of the union or in-tersection of two sets. Assuming the sizes of the sketches for A 1 ,A 2 are fixed at k , a set of sufficient statistics for the cardinality of the union and intersection is given by the thresholds of the individual sketches,  X  1 and  X  2 , the number of common stored hashes | h ( S 1  X  0 S 2 ) | , the total number of hashes less than or equal to the smaller of the two thresholds | h ( S 1  X  0 S 2 ) | , and the streaming estimates of the cardinality  X  N ( S i ) and variance  X  Var( S i ).
The first class of estimators we present are the likelihood based estimators. Given a sufficient statistic, the parame-ter of interest may be estimated using the asymptotically efficient maximum likelihood estimator (MLE). The asymp-totic variance of the MLE is given by the inverse Fisher information. Under mild regularity conditions, this is the expected Hessian of the negative log-likelihood evaluated at the true parameters. This yields a natural estimate for the variance of the MLE as well. Compute the Hessian of the negative log-likelihood at the estimated rather than the true parameters and take its inverse.
 A closed form for the full likelihood of the Streaming Min-Count sketch is not known. Instead, we first derive the like-lihood function for the basic MinCount sketch. By plugging in the streaming estimates into this likelihood, it can be used to construct a surrogate likelihood or a pseudo-likelihood [3] for the intersection or union. This surrogate is a form of profile-likelihood for the Streaming Mincount sketches that include the streaming estimates as part of the sketches. We then derive an approximation to the full likelihood. We first derive the generative process for a pair of basic MinCount sketches from sets A 1 ,A 2 in lemma 1. This al-lows derivation of the likelihood. A rough description of the process is as follows. First, generate a threshold  X  1 and pro-pose values for | h ( S 1  X  0 S 2 ) | and | h ( S 2 , X  1 ) \ h ( S considering the constraint that S 2 contains at most k 2 hash values. If their total is less than k 2 and the constraint on S is not violated, then  X  1 =  X  min , and to complete sketch S simply draw the requisite number of additional points above  X  to compute  X  2 . Otherwise,  X  2 &lt;  X  1 and the proposed val-ues are thinned using sub-sampling.
 Lemma 1. Given sets A 1 ,A 2 and a random universal hash, MinCount generates random sketches S 1 ,S 2 of sizes k respectively. The sufficient statistics for the parameters | A | A 2 | , and | A 1  X  A 2 | are  X  1 ,  X  2 , | S 1  X  0 S 2 | , and | S These statistics have the distribution given by the following process. If U + V &lt; k 2 then If U + V = k 2 and C = 1 , Otherwise,  X  2 &lt;  X  1 and
Proof. The order statistics of the hash values determine the MinCount sketch. The order statistics of n uniform ran-dom variates are jointly Dirichlet distributed with parameter 1  X  R n , and the marginal distributions are Beta distributed. The labeling of uniform random variates as belonging to A \ A 2 , A 2 \ A 1 , or A 1  X  A 2 follows a multinomial distribu-tion. The lemma follows from conditional distributions for the Dirichlet and multinomial distributions and their prop-erties.

Note that in the case where U + V  X  C &lt; k 2 or, equiva-lently  X  2  X   X  1 , every generated variable is also observed in the final sketch. There are no hidden variables that need to be integrated out to form the likelihood. By symme-try, exchanging the indices of the variables and parameters does not change the likelihood. This observation reduces the computation of the log-likelihood to the simple case with no hidden variables and gives the following theorem.

Theorem 2. Let variables x 1 ,x 2 ,x 1  X  2 represent estimates MinCount sketch may be written as follows. abilities are given by the generative process in lemma 1.
To obtain an estimator that exploits the streaming cardi-nality estimates, simply replace x 1 =  X  N ( S 1 ) and x 2 to obtain a marginal profile likelihood.  X   X  After taking a union or intersection, the resulting sketch contains the new cardinality estimate along with the hash values stored by the improved MinCount sketch. Note that although the union cardinality estimate uses the inclusion-exclusion principle, it is simply a reparameterization of the intersection cardinality, and maximum likelihood estimates are invariant under reparameterization.

Since some of the distributions have discrete parameter spaces, we relax the optimization of the log-likelihood by re-placing factorials with the corresponding continuous gamma function when doing maximum likelihood estimation.
The profile-likelihood given above has a deficiency. It is asymptotically inefficient as it not a sufficiently good ap-proximation to the true likelihood and does not account for the distribution p (  X  N ( S 1 ) ,  X  N ( S 2 ) | h ( S tunately, this conditional distribution of the streaming esti-mates given the final hash values is intractable to compute. Instead, we approximate it with a tractable distribution to form what we refer to as a full pseudo-likelihood.
 We use a bivariate normal as the tractable distribution. This choice is well-motivated as one would expect that the central limit theorem to yield this as the limit distribution. The mean of this distribution is given by the streaming cardinality estimates. An approximate covariance matrix  X ( S 1 ,S 2 ) for the streaming cardinality estimates is derived in section 10. To simplify calculations, one may also use other choices such as a diagonal matrix.

Mathematically, the likelihood is given by where  X  is a bivariate normal density and  X ( S 1 ,S approximate covariance matrix for the streaming cardinal-ity estimates. The resulting pseudo-likelihood cardinality estimators for the intersection and union are Note that this also allows the estimates of | A 1 | and | A be improved when there is substantial overlap between the sets. In particular, when A 1 = A 2 , the estimate of | A be an average of the estimates  X  N ( S 1 ) and  X  N ( S 2 ).
The variance estimates for the pseudo-likelihood cardinal-ity estimators are where D denotes the directional derivative and  X  N pseudo the maximizer of the pseudo-likelihood.
Since the pseudo-likelihood is a surrogate for the true like-lihood, asymptotic efficiency results for the maximum like-lihood estimator do not directly apply. However, we conjec-ture that the pseudo-likelihood estimator is asymptotically equivalent to the maximum likelihood estimator. This re-sult would not be surprising since one would expect that the streaming cardinality estimates converge to a bivariate normal distribution by the central limit theorem. Further-more, by asymptotic results for M-estimators [24], such as the MLE, if the quadratic approximation of the pseudo-log-likelihood and log-likelihood converge to the same limit, their asymptotic distributions are the same. Since we es-timate the conditional covariance for the streaming esti-mates given the basic sketch, we have approximated all the quadratic terms of the log-likelihood.
We provide a second class of union and intersection car-dinality estimators that are easier to implement than the pseudo-likelihood based estimators. They are also easier generalize to sketches other than MinCount and to set op-erations on multiple sets. These estimators are formed by taking the weighted average of several unbiased or consistent estimators of the cardinality. We derive how to optimally combine these component estimators to obtain an estima-tor that performs nearly identically to the pseudo-likelihood based estimator. For the estimators presented in this paper, we use component estimators where each estimator is a mul-tiplicative scaling of a single streaming cardinality estimate.
We first describe the optimal procedure for combining multiple component estimators. Then we propose a set of component estimators and derive the variances and covari-ances needed to combine them. The procedure to optimally combine estimators is given by the following lemma.
Lemma 3 (Optimal weighting). Given unbiased (or consistent) estimators  X  N 1 ,...,  X  N m with non-singular covari-ance matrix  X  , the optimal weights that sum to 1 and min-imize variance are given by where 1 m is the m -vector of all ones. The resulting estimator is unbiased (or consistent) and has variance (1 T m  X   X  1
Proof. Since all the estimators are unbiased (or consis-tent), the re-weighted estimator is also unbiased (or consis-tent). Given unnormalized weights v , the variance of the es-timator v T  X  N/v T 1 is Var( v T  X  N/v T 1) = v T  X  v/v T imizing this Rayleigh quotient give the optimal weights.
Typically, the covariance  X  is not known, so a plug-in estimate of  X  is used to generate the weights. An alternate weighting is to treat the estimators as being independent and simply use the diagonal of the covariance. In this case, the simple re-weighted estimator is where z is a normalization constant that ensures weights sum to 1. Although this does not provide guarantees on the improvement, we find that this performs nearly as well as the optimal weighting.

We now consider the two ingredients needed to define a re-weighted estimator: a set of consistent estimators that form the components of the combined estimator and the covariance of these estimators to determine the weights.
The idea for defining the component estimators is to have maximally uncorrelated estimators. We minimize correla-tion by using component estimators which make use of only one of the streaming cardinality estimate in each component. This gives that the resulting weighted estimator is expressed as a linear combination of streaming cardinality estimates. These component estimators are shown to be unbiased or consistent and, hence, suitable candidates for a re-weighted estimator. The approximate variance of the estimators is computed in order to allow for computation of the weights.
To derive the component estimators, note that both the cardinality of the union and the intersection may be decom-posed into a ratio times the cardinality of one of the sets in the operation. | A 1  X  A 2 | = | A 1  X  A 2 | Define  X  i = | A 1  X  A 2 | / | A i | and  X  i = | A 1  X  A 2 estimators for  X  i and  X  i are
These give a consistent, plug-in estimator for the cardi-nality of an intersection and union as shown in the following lemma, proven in the long version of the paper, and theorem.
Lemma 4. The estimator  X   X  i is an unbiased estimator of | A It is consistent under the conditions in section 7. Similarly,  X   X  is a consistent estimator of | A 1  X  A 2 | / | A i | under the con-ditions in section 7.

Theorem 5. The estimators are consistent estimators of the | A 1  X  A 2 | , | A 1 the conditions given in section 7.

Proof. This immediately follows from Slutzky X  X  theorem since  X   X  i ,  X   X  i ,  X  N ( S i ) are consistent.

Another possible choice for component estimators is to pretend that all items in A 2 appear after A 1 in the stream. A more accurate streaming cardinality estimate can be cal-culated by not updating the threshold. However, the result-ing re-weighted estimator is suboptimal when A 1 and A 2 are disjoint and equal in size. It is equivalent to averaging the suboptimal improved union estimate with optimal sum of streaming estimates.
The variance of each component estimator is required to apply the simple re-weighting scheme. This is approximated by decomposing the component estimator so that the multi-pliers and cardinality estimates can be treated as essentially independent. Write n ) + applies to  X  N ( S i ), Slutzky X  X  lemma and the central limit the-orem imply that the quantity converges in distribution to a normal distribution with variance less than or equal to  X  ing Cov (  X  N ( S i ) ,  X   X  i ) is excluded as the terms are negatively correlated. The same argument applies to  X   X  i .

In this case, the expectation and variance of  X  N ( S already given in [23] and the expectations are known or ap-proximated by lemma 4. The only remaining quantities to compute are Var(  X   X  i ) and Var(  X   X  i ). The variances of  X   X  may be approximated using the following formulas.

These approximate variances may be used in the simple re-weighting scheme given in equation 11.  X 
Var(  X  N i ( S 1  X  S 2 ))  X   X  N i ( S 1  X  S 2 ) 2 (1  X   X   X   X  Var(  X  N i ( S 1  X  S 2 ))) =  X  N i ( S 1  X  S 2 ) 2 (
The component estimators may be compared to the im-proved intersection and union estimators given in section 5. Since  X  i | h ( S i , X  min ) |  X  | h ( S 1  X  0 S 2 ) | and  X  | h ( S 1  X  0 S 2 ) | , the variance of the improved intersection es-timator is approximately | A 1  X  A 2 | 2 / X  i | h ( S i variance of the component intersection estimator, and simi-larly the component union estimator, can be approximated by For the intersection estimates, since | h ( S i , X  min ) | X  k component estimators are better than the than the basic im-proved estimator even though they exploit the accuracy of only one of the streaming cardinality estimates  X  N ( S i thermore, the component with the smaller | h ( S i , X  min the greater improvement over the basic improved estimator. A surprising consequence of this approximation is that this component should represent an improvement over the ba-sic improved estimator even when the streaming cardinality estimate is not even used. In that case, the 1 / 2 k i term is replaced by 1 /k i in the improvement term. We believe the reason is that the basic improved estimator loses information about the size of the smaller set which constrains the size of the intersection. For the union estimates, the improve-ment depends on the multiplier  X  i . If  X  i  X  3 / 2, then the component estimator beats the basic improved estimator.
Computing the covariance of the streaming estimates is a necessary step for obtaining the full quadratic approxi-mation to the log-likelihood used by the pseudo-likelihood estimator. It is also needed to compute the weights for the full re-weighted estimator. We present the main ideas and results for calculating the covariance and leave the detailed calculations to the long version of the paper.

In the streaming formulation of MinCount, the remaining area process R ( S t ) =  X  t implicitly depends on the order  X  in which elements arrive. This dependency is difficult to an-alyze. We consider an alternative formulation that expands and extends the proof ideas in [10] and is based on the ranks of the hash values. The Streaming MinCount estimator can be expressed as where U ( i ) is the i th smallest hash value and Z i = 1 if and only if the estimator was incremented by 1 /U ( i ) lently, Z i = 1 if and only if fewer than k of the i  X  1 smallest hash values appear before the i th smallest value. Mathe-matically, Z i = 1( |{ j &lt; i :  X  j &lt;  X  i }| &lt; k ) where  X  position of U ( i ) in the stream.

When considering multiple sketches, denote the a th stream-ing estimate by  X  N ( S a ) = k a + P n tot i = k are the order statistics for all the hash values for A 1  X  A This rank formulation is the key to the analysis in this sec-tion. It separates the analysis into three independent com-ponents: the order statistics for the hash values U random orderings  X  ( a ) , and an indicator Y ( a ) i U ( i ) belongs to stream a . By the independence of Z and U and bilinearity of the covariance operator, the covariance Cov (  X  N ( S 1 ) ,  X  N ( S 2 )) of two streaming cardinality estimates can then decomposed into the cross terms
Cov Z Of these terms, Cov Z (1) i ,Z (2) j depends on the order of ele-ments in both streams. In particular, it depends on whether the common elements A 1  X  A 2 appear in the same order or different orders in the streams. We consider two cases: the orders are the same and the orders are independent random permutations.
 The covariances of the cardinality estimates are given by
The covariance  X  independent is of particular interest when q = 1 so that the sets A 1 = A 2 . The best estimate when the hash function is fixed takes the average of Streaming Min-Count estimates over all possible orderings of the same set. This limit estimator reduces the variance to  X  independent . By contrast, when applied to a single stream the Stream-ing MinCount estimator has variance n 2 2 k . Thus, even in the case where every cardinality estimate is for exactly the same set, the estimates can be improved by combining them. This result is borne out in figure 2 as the basic MinCount sketch requires 3 times the space as the averaged Streaming Min-Count sketches when q 0 = 1 and the sets are all the same.
For the pseudo-likelihood described in section 8.2, the required normal approximation is for the conditional dis-covariance. Under a multivariate normal approximation, this conditional distribution is easy to approximate. Let  X  N mincount be the non-streaming MinCount estimator that is based only on the offset  X  . Assume that the joint distri-bution of  X  N ( S 1 ),  X  N ( S 2 ),  X  N mincount ( S 1 ), and is multivariate normal. The covariance may be denoted in block form by where s,m denote the streaming and non-streaming Min-Count estimators.

Since the streaming estimate is optimal, it cannot be im-proved upon by conditioning on the threshold. In partic-ular, under a re-weighting scheme a linear combination of the streaming and non-streaming estimators does not im-prove the variance. Hence, the non-streaming estimator is approximately equal to the streaming estimator plus uncor-related noise,  X  sm  X   X  ss . The last component is the co-variance of the non-streaming MinCount estimates which is where ` is the sketch with the higher threshold. Under a multivariate normality approximation where  X  n i denotes the current estimate for n i , the conditional variance and mean  X  ss,i |  X  = E (  X  N ( S i ) |  X  ) are given by
 X   X 
The covariance of the multipliers may be approximated by first conditioning on the value in the denominator. This gives rough estimates where  X  2 = q 2 /p 2 and  X  1 &lt;  X  2 . The multiplier covariances may be incorporated by the same argument as in section 9.2.
Oftentimes the query of interest merges many sketches together rather than pairs. For example, if the data is pro-cessed on d mappers in a map-reduce framework, d sketches must be merged to estimate the cardinality.
 Merging multiple estimators using re-weighting is simple. The component estimators are slightly modified, and the re-weighting is virtually unchanged since covariance calcu-lations are pairwise calculations. The multipliers  X  i , X  mate the size of a set relative to the intersection or union of all the sets. The only difference is that the minimum thresh-old  X  min = min i {  X  i } and the number of relevant hashes pair.

Generalizing pseudo-likelihood based methods is more dif-ficult. Although the same technique of ordering the thresh-olds to compute the likelihood may be used, the likelihood itself contains exponentially many parameters when there are more than two sketches. The intersection of any subset of the sets { A i } is a parameter. This makes the optimiza-tion problem difficult, and the finite sample performance of likelihood based methods is not well understood.

Another strategy is to exploit that pairwise intersections and unions are closed operations. The result of intersect-ing or taking the union of two sketches results in another sketch. These sketches may be further merged. For the profile-likelihood, which does not utilize any variance or co-variance information for the estimates  X  N ( S 1 ) and the merge operation is straightforward. For other meth-ods, a surrogate for the covariances of the merged estimates needed for computation of the cardinality estimate.
The new estimators were evaluated on both real and sim-ulated data. The results on real data exactly match the theory and empirically demonstrate there is no difference between applying the methods on real or simulated data. We examine the behavior of the estimators in a variety of cases. In all cases, the pseudo-likelihood and re-weighted estimators that exploit the streaming cardinality estimates had the best performance and performed similarly. To as-sess the performance of the estimators, we computed the Relative Efficiency (RE) of the estimators and the rela-tive size required. The RE is defined as RE (  X  N (1) ,  X  typically scales as 1 /m where m is the number of samples used in the estimator, the RE measures how much more data  X  N (2) needs to achieve the same accuracy as  X  N (1) estimator with RE of 1/2 requires twice as many samples to match the variance of the baseline estimator. The relative size is the space required to achieve the same error and is 1 /RE . Figure 1 show that, analogous to the improvement of streaming estimators in the single stream problem, the new methods require half the space of the improved union esti-mator. They achieve even greater reductions in space when compared to basic MinCount estimator. When intersection sizes are small, the new estimators require less than 1 / 15 of the space of inclusion-exclusion based estimators. Com-pared to the basic improved estimator, the merged streamed intersection sketches require half the space when and one set is nearly a subset of the other.

In simulations for pairwise unions and intersections, we let | A 1 | = 2 20 and considered cases with both balanced, | A 2 | A 1 | , or unbalanced, | A 2 | = | A 1 | / 4, cardinalities. The size of the intersection was allowed to vary. Unless otherwise noted, the order of common elements in the streams was the less F igure 1: These figures show the sketch size needed by different estimators to achieve a given error rel-ative to the profile-likelihood estimator. For inter-sections on sets with significantly different cardinal-ities, the new likelihood and re-weighted estimators require far less space. favorable order for our methods; the elements appear in the same order. We tried different sketch sizes in our simulations but only present the results where the size k = 1024 as there was no material difference when k = 256 or 4196.

For real data, we used the MSNBC dataset from the UCI machine learning repository [20] to validate our simulation results. This dataset consists of 1 million users X  browsing his-tory of 17 web page categories. We consider the problem of estimating how many users visited a pair of categories using only the 17 sketches counting unique visits to each category. For comparison, we simulated data using parameters that match the real set and intersection cardinalities. For each set of parameters 10,000 sketches were generated with inde-pendent random hashes. Figure 3 shows, as expected, that the estimated cardinalities using the real data have the same distribution as the simulated data. A p-value was computed for each set of parameters testing if the RRMSE for real and simulated data are different. Under the null hypothesis that there is no difference in distributions, the p-values are uniformly distributed. A Kolmogorov-Smirnov test yields a p-value &gt; 0 . 6, so there is statistically no evidence in the test that the simulated data behaves differently from the real data.

There were only small difference in performance between the simple re-weighted estimator in equation 11 and the op-timally weighted estimator in equation 10 except when the cardinalities were unbalanced and the smaller set was nearly a subset of the larger one. This difference only observed for intersections. For unions, the simple re-weighting performed marginally better than the optimal re-weighting, possibly due to additional error in estimating the covariance of the component estimators. We explain this asymmetry by not-ing that in the case where A 2  X  A 1 and the order elements Figure 2: The left figure shows the RE with respect to the simple re-weighted estimator when taking the union of 100 random subsets A i drawn with proba-bility p from A tot . The right figure shows the RE of the full re-weighted estimator to the simple re-weighted estimator. Using the full covariance struc-ture improves intersection estimates but error in the covariance estimate hurts unions very slightly. Figure 3: The left figure shows the RRMSE when es-timators are run on real data versus simulated data. There is an exact correspondence, The right figure shows the distribution of p-values testing if there is a difference between the real versus simulated data. The distribution fits the null uniform distribution. appear is the same, every item in A 2 that updates sketch S must also update S 2 . Thus, S 1 contains almost no informa-tion about the intersection that is not already in S 2 . The simple re-weighting adds noise by giving nonzero weight to component  X  N 1 ( S 1  X  0 S 2 ). The reverse for unions is not true; S 2 contains useful information not in S 1 for estimating | A We note that the covariance estimate for the component es-timators is imperfect as it does not simply select the smaller set when A 2 is a proper subset of A 1 . However, the full re-weighted estimator empirically has relative efficiency  X  95% of an optimal re-weighting, so any improvement would be small.

To simulate the distributed map-reduce setting, the sim-ple re-weighted estimator was compared to the basic and improved MinCount estimators when taking the union of 100 sketches. Each sketch was generated by sub-sampling with probability p from a set of 10 6 elements. The sampled items were then randomly permuted before computing the sketch. Figure 2 shows the simple re-weighted estimator has high relative efficiency compared to the naive union estima-tor, especially when the size of the pairwise intersections is small. Likewise, the improved estimator has high relative efficiency compared to the basic estimator in those cases. Here, the re-weighted estimator requires 1 / 3 the space of the improved estimator rather than 1 / 2 due to the accuracy gains from averaging over independent permutations. Although the calculations presented are specialized for the MinCount sketch, the techniques apply to other sketches as well such as the Flajolet-Martin (FM) sketch used in Hy-perLogLog or the Discrete Max-Count sketch in [23]. In particular, the simple re-weighted estimator can be easily derived for the FM sketch. The only ingredients needed are estimates of the Jaccard similarities J ( A 1  X  A 2 ,A J ( A i ,A 1  X  A 2 ) and their variances. The FM sketch, how-ever, does not lead to a closed intersection operation that allows for further merging.

Another possible generalization is to estimate set differ-ences. For the likelihood based methods, this is exactly the same as estimating the intersection since it is simply an-other re-parameterization. For the re-weighting methods, the multipliers change but the analysis remains the same.
For the methods described in this paper, cardinality esti-mation for the union or intersection only requires constant space. There are a finite number of sufficient statistics and that these can be tabulated with constant memory if the hash values in each sketch can be accessed in sorted order. If a merged sketch and not just a cardinality estimate is re-quired, a closed union operation may result in a larger sketch where the size is proportional to the number of sets in the union. The sketch can be truncated to have k hash values to reduce the space requirements while cardinality estimation is still performed with all the hash values.

One unexplored area is determining the optimal order of pairwise merges. If many sketches cannot be merged simul-taneously, the desired cardinalities may still be computed using a sequence pairwise unions and intersections that ex-ploit the closed union and set operations.
For all the improved methods in this paper, computing the sufficient statistic requires O ( km ) time where m is the number of sketches. If the hash values are stored in sorted order, this is reduced to O ( k ). For the naive estimators, the running time is O ( mk log k ) if a heap is used to select the k values. For the pairwise likelihood based methods, the pseudo-likelihood function is from an exponential family and hence, log-concave. Evaluating the gradient and Hes-sian take constant time, so the running time is proportional to the number of iterations needed to solve this concave optimization problem. For the re-weighted component esti-mators, if the estimated covariance matrix is diagonal, com-puting the re-weighted estimator takes O ( m ) time. If a full covariance matrix is used, then inverting the matrix can take O ( m 3 ) time.
This paper presents and analyzes two new classes of meth-ods for estimating cardinalities of intersections and unions from sketches. These methods are applied to Streaming MinCount sketches, and variance estimates are derived as well. All the methods theoretically and empirically outper-form existing methods for estimating unions and intersec-tions. The new methods also lead to mergeable summaries under intersection and union operations. This allows both intersections and unions on sketches to be chained together while existing methods only allow unions to be chained. Ex-tensions to sketches other than the MinCount sketch are also discussed.
The pseudo-likelihood based estimators are derived as ap-proximations to the asymptotically optimal maximum likeli-hood estimator. We conjecture that the full pseudo-likelihood estimator is asymptotically equivalent to the maximum like-lihood estimator. The re-weighted component estimators are derived as near optimally weighted linear combinations of the streaming estimates. Empirically, the near optimally re-weighted component estimator matches the performance of the full maximum pseudo-likelihood estimator.

The derived estimators are useful in a variety of different settings. The re-weighted estimators can be easily general-ized to handle multiple set unions rather than just pairwise unions. The simple re-weighted estimators can be easily generalized to other sketches. The profile-likelihood can be used for sequences of pairwise merges.

The theoretical analysis also allows us to separate and identify the information contained in different parts of the sketch. Since the full re-weighted and the conjectured op-timal pseudo-likelihood methods perform nearly identically and since the multipliers for component estimators can be treated as nearly independent from the streaming cardinal-ity estimates, we see that the stored hash values encode information about the proportional sizes of the sets relative to their union and intersection while the streaming cardi-nality estimate contain information about the absolute size. Section 10 shows that each streaming estimate contains in-formation about the order of elements in the stream since the streaming cardinality variance can be decomposed into an irreducible component that depends on the order statistics for the hash values and a reducible component that depends on the ordering of elements in the stream. In the case of identical sets, averaging streaming estimates over all pos-sible orderings variance reduces the variance to n 2 / 3 k . In section 9.2, the single non-streaming component estimator based on the smaller set is shown to dominate the improved MinCount estimator that throws away the larger threshold. All these relevant pieces of information useful for estimation are contained in the sufficient statistics given in section 8.1 which the pseudo-likelihood estimator makes full use of.
Together these contributions advance the methodology, theory, and understanding for the approximate distinct count-ing problem. [1] P. K. Agarwal, G. Cormode, Z. Huang, J. M. Phillips, [2] Z. Bar-Yossef, T. Jayram, R. Kumar, D. Sivakumar, [3] J. Besag. Statistical analysis of non-lattice data. The [4] K. Beyer, R. Gemulla, P. J. Haas, B. Reinwald, and [5] P. Boldi, M. Rosa, and S. Vigna. Hyperanf: [6] P. Chassaing and L. Gerin. Efficient estimation of the [7] A. Chen, J. Cao, L. Shepp, and T. Nguyen. Distinct [8] P. Clifford and I. Cosma. A statistical analysis of [9] E. Cohen. Size-estimation framework with applications [10] E. Cohen. All-distances sketches, revisited: HIP [11] E. Cohen and H. Kaplan. Summarizing data using [12] G. Cormode, M. Datar, P. Indyk, and [13] T. Dasu, T. Johnson, S. Muthukrishnan, and [14] M. Durand and P. Flajolet. Loglog counting of large [15] C. Estan, G. Varghese, and M. Fisk. Bitmap [16] P. Flajolet,  X  E. Fusy, O. Gandouet, and F. Meunier. [17] P. Flajolet and G. Nigel Martin. Probabilistic counting [18] F. Giroire. Order statistics and estimating [19] D. Kane, J. Nelson, and D. Woodruff. An optimal [20] M. Lichman. UCI machine learning repository, 2013. [21] A. Metwally, D. Agrawal, and A. E. Abbadi. Why go [22] P. G. Selinger, M. M. Astrahan, D. D. Chamberlin, [23] D. Ting. Streamed approximate counting of distinct [24] A. van der Vaart. Asymptotic Statistics . Cambridge [25] K. Whang, B. Vander-Zanden, and H. Taylor. A
