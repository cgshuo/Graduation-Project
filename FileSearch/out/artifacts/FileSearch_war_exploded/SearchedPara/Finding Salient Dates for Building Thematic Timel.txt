 Our aim here was to build thematic timelines for a general domain topic defined by a user query. This task, which involves the extraction of important events, is related to the tasks of Retrospective Event Detection (Yang et al., 1998), or New Event Detec-tion , as defined for example in Topic Detection and Tracking (TDT) campaigns (Allan, 2002).

The majority of systems designed to tackle this task make use of textual information in a bag-of-words manner. They use little temporal informa-tion, generally only using document metadata, such as the document creation time (DCT). The few sys-tems that do make use of temporal information (such as the now discontinued Google timeline), only ex-tract absolute, full dates (that feature a day, month and year). In our corpus, described in Section 3.1, we found that only 7% of extracted temporal expres-sions are absolute dates.

We distinguish our work from that of previous re-searchers in that we have focused primarily on ex-tracted temporal information as opposed to other textual content. We show that using linguistic tem-poral processing helps extract important events in texts. Our system extracts a maximum of temporal information and uses only this information to detect salient dates for the construction of event timelines. Other types of content are used for initial thematic document retrieval. Output is a list of dates, ranked from most important to least important with respect to the given topic. Each date is presented with a set of relevant sentences.

We can see this work as a new, easily evaluable task of  X  X ate extraction X , which is an important com-ponent of timeline summarization.

In what follows, we first review some of the re-lated work in Section 2. Section 3 presents the re-sources used and gives an overview of the system. The system used for temporal analysis is described in Section 4, and the strategy used for indexing and finding salient dates, as well as the results obtained, are given in Section 5 1 . The ISO-TimeML language (Pustejovsky et al., 2010) is a specification language for manual anno-tation of temporal information in texts, but, to the best of our knowledge, it has not yet actually been used in information retrieval systems. Neverthe-less, (Alonso et al., 2007; Alonso, 2008; Kanhabua, 2009) and (Mestl et al., 2009), among others, have highlighted that the analysis of temporal informa-tion is often an essential component in text under-standing and is useful in a wide range of informa-tion retrieval applications. (Harabagiu and Bejan, 2005; Saquete et al., 2009) highlight the importance of processing temporal expressions in Question An-swering systems. For example, in the TREC-10 QA evaluation campaign, more than 10% of questions required an element of temporal processing in order to be correctly processed (Li et al., 2005a). In multi-document summarization, temporal processing en-ables a system to detect redundant excerpts from various texts on the same topic and to present re-sults in a relevant chronological order (Barzilay and Elhadad, 2002). Temporal processing is also useful for aiding medical decision-making. (Kim and Choi, 2011) present work on the extraction of temporal in-formation in clinical narrative texts. Similarly, (Jung et al., 2011) present an end-to-end system that pro-cesses clinical records, detects events and constructs timelines of patients X  medical histories.

The various editions of the TDT task have given rise to the development of different systems that de-tect novelty in news streams (Allan, 2002; Kumaran and Allen, 2004; Fung et al., 2005). Most of these systems are based on statistical bag-of-words mod-els that use similarity measures to determine prox-imity between documents (Li et al., 2005b; Brants et al., 2003). (Smith, 2002) used spatio-temporal in-formation from texts to detect events from a digital library. His method used place/time collocations and ranked events according to statistical measures.
Some efforts have been made for automatically building textual and graphical timelines. For ex-ample, (Allan et al., 2001) present a system that uses measures of pertinence and novelty to con-struct timelines that consist of one sentence per date. (Chieu and Lee, 2004) propose a similar system that extracts events relevant to a query from a collection of documents. Important events are those reported in a large number of news articles and each event is constructed according to one single query and rep-resented by a set of sentences. (Swan and Allen, 2000) present an approach to generating graphical timelines that involves extracting clusters of noun phrases and named entities. More recently, (Yan et al., 2011b; Yan et al., 2011a) used a summarization-based approach to automatically generate timelines, taking into account the evolutionary characteristics of news. 3.1 AFP Corpus For this work, we used a corpus of newswire texts provided by the AFP French news agency. The En-glish AFP corpus is composed of 1.3 million texts that span the 2004-2011 period (511 documents/day in average and 426 millions words). Each document is an XML file containing a title, a date of creation (DCT), set of keywords, and textual content split into paragraphs. 3.2 AFP Chronologies AFP  X  X hronologies X  (textual event timelines) are a specific type of articles written by AFP journal-ists in order to contextualize current events. These chronologies may concern any topic discussed in the media, and consist in a list of dates (typically be-tween 10 and 20) associated with a text describing the related event(s). Figure 1 shows an example of such a chronology. Further examples are given in Figure 2. We selected 91 chronologies satisfying the following constraints:  X  All dates in the chronologies are between 2004  X  All dates in the chronology are anterior to the  X  The temporal granularity of the chronology is
For learning and evaluation purposes, all chronologies were converted to a single XML format. Each document was manually associated with a user search query made up of the keywords required to retrieve the chronology. 3.3 System Overview Figure 3 shows the general architecture of the sys-tem. First, pre-processing of the AFP corpus tags and normalizes temporal expressions in each of the articles (step  X  in the Figure). Next, the corpus is indexed by the Lucene search engine 2 (step  X  ).
Given a query, a number of documents are re-trieved by Lucene (  X  ). These documents can be fil-tered (  X  ), and dates are extracted from the remain-ing documents. These dates are then ranked in order to show the most important ones to the user (  X  ), to-gether with the sentences that contain them. In this section, we describe the linguistic and tempo-ral information extracted during the pre-processing phase and how the extraction is carried out. We rely on the powerful linguistic analyzer XIP (A  X   X t-Mokhtar et al., 2002), that we adapted for our pur-poses. 4.1 XIP The linguistic analyzer we use performs a deep syn-tactic analysis of running text. It takes as input XML files and analyzes the textual content enclosed in the various XML tags in different ways that are specified in an XML guide (a file providing instruc-tions to the parser, see (Roux, 2004) for details). XIP performs complete linguistic processing rang-ing from tokenization to deep grammatical depen-dency analysis. It also performs named entity recog-nition (NER) of the most usual named entity cat-egories and recognizes temporal expressions. Lin-guistic units manipulated by the parser are either terminal categories or chunks. Each of these units is associated with an attribute-value matrix that con-tains the unit X  X  relevant morphological, syntactic and semantic information. Linguistic constituents are linked by oriented and labelled n-ary relations de-noting syntactic or semantic properties of the input text. A Java API is provided with the parser so that all linguistic structures and relations can be easily manipulated by Java code.

In the following subsections, we give details of the linguistic information that is used for the detec-tion of salient dates. 4.2 Named Entity Recognition Named Entity (NE) Recognition is one of the out-puts provided by XIP. NEs are represented as unary relations in the parser output. We used the exist-ing NE recognition module of the English grammar which tags the following NE types: location names , person names and organization names . Ambigu-ous NE types (ambiguity between type location or organization for country names for instance) are also considered. 4.3 Temporal Analysis A previous module for temporal analysis was de-veloped and integrated into the English grammar (Hag ` ege and Tannier, 2008), and evaluated during TempEval campaign (Verhagen et al., 2007). This module was adapted for tagging salient dates. Our goal with temporal analysis is to be able to tag and normalize 3 a selected subset of temporal expressions (TEs) which we consider to be relevant for our task. This subset of expressions is described in the follow-ing sections. 4.3.1 Absolute Dates
Absolute dates are dates that can be normalized without external or contextual knowledge. This is the case, for instance, of  X  X n January 5th 2003 X  . In these expressions, all information needed for nor-malization is contained in the linguistic expression. However, absolute dates are relatively infrequent in our corpus (7%), so in order to broaden the cover-age for the detection of salient dates, we decided to consider relative dates, which are far more frequent. 4.3.2 DCT-relative Dates
DCT-relative temporal expressions are those which are relative to the creation date of the docu-ment. This class represents 40% of dates extracted from the AFP corpus. Unlike the absolute dates, the linguistic expression does not provide all the infor-mation needed for normalization. External informa-tion is required, in particular, the date which corre-sponds to the moment of utterance. In news articles, this is the DCT. Two sub-classes of relative TEs can be distinguished. The first sub-class only requires knowledge of the DCT value to perform the normal-ization. This is the case of expressions like next Fri-day , which correspond to the calendar date of the first Friday following the DCT. The second sub-class requires further contextual knowledge for normal-ization. For example, on Friday will correspond ei-ther to last Friday or to next Friday depending on the context where this expression appears ( e.g. He is expected to come on Friday corresponds to next Friday while He arrived on Friday corresponds to last Friday ). In such cases, the tense of the verb that governs the TE is essential for normalization. This information is provided by the linguistic analy-sis carried out by XIP. 4.3.3 Underspecified Dates
Considering the kind of corpus we deal with (news), we decided to consider TEs whose granu-larity is at least equal to a day. As a result, TEs were normalized to a numerical YYYYMMDD for-mat (where YYYY corresponds to the year, MM to the month and DD to the day). In case of TEs with a granularity superior to the day or month, DD and MM fields remain unspecified accordingly. How-ever, these underspecified dates are not used in our experiments. 4.4 Modality and Reported Speech An important issue that can affect the calculation of salient dates is the modality associated with time-stamped events in text. For instance, the status of a salient date candidate in a sentence like  X  X he meet-ing takes place on Friday X  has to be distinguished from the one in  X  X he meeting should take place on Friday X  or  X  X he meeting will take place on Friday, Mr. Hong said X  . The time-stamped event meeting takes place is factual in the first example and can be taken as granted. In the second and third exam-ples, however, the event does not necessarily occur. This is expressed by the modality introduced by the modal auxiliary should (second example), or by the use of the future tense or reported speech (third ex-ample). We annotate TEs with information regard-ing the factuality of the event they modify. More specifically, we consider the following features: Events that are mentioned in the future: If a time-stamped event is in the future tense, we add a specific attribute MODALITY with value FUTURE to the corresponding TE annotation.
 Events used with a modal verb: If a time-stamped event is introduced by a modal verb such as should or would , then attribute MODALITY to the corresponding TE annotation has the value MODAL . Reported speech verbs: Reported speech verbs (or verbs of speaking) introduce indirect or reported speech. We dealt with time-stamped events gov-erned by a reported speech verb, or otherwise ap-pearing in reported speech. Once again, XIP X  X  lin-guistic analysis provided the necessary information, including the marking of reported speech verbs and clause segmentation of complex sentences. If a rel-evant TE modifies a reported speech verb, the anno-tation of this TE contains a specific attribute, DE CLARATION = X  YES  X . If the relevant TE modifies a verb that appears in a clause introduced by a re-ported speech verb then the annotation contains the
Note that the different annotations can be com-bined ( e.g. modality and reported speech can occur for a same time-stamped event). For example, the TE Friday in  X  X he meeting should take place on Fri-day, Mr. Hong said X  is annotated with both modality and reported speech attributes. 4.5 Corpus-dependent Special Cases While we developed the linguistic and temporal an-notators, we took into account some specificities of our corpus. We decided that the TEs today and now were not relevant for the detection of salient dates. In the AFP news corpus, these expressions are mostly generic expressions synomymous with nowadays and do not really time-stamp an event with respect to the DCT. Another specificity of the corpus is the fact that if the DCT corresponds to a Monday, and if an event in a past tense is described with the associated TE on Monday or Monday , it means that this event occurs on the DCT day itself, and not on the Monday before. We adapted the TE normalizer to these special cases. 4.6 Implementation and Example As said previously, a NER module is integrated into the XIP parser, which we used  X  X s is X . The TE tag-ger and normalizer was adapted from (Hag ` ege and Tannier, 2008). We used the Java API provided with the parser to perform the annotation and normal-ization of TEs. The output for the linguistic and temporal annotation consists in XML files where only selected information is kept (structural infor-mation distinguishing headlines from news content, DCT), and enriched with the linguistic annotations described before (NEs and TEs with relevant at-tributes corresponding to the normalization and typ-ing). Information concerning modality, future tense and reported speech, appears as attributes on the TE tag. Figure 4 shows an example of an analyzed ex-cerpt of a news article.

In this news excerpt, only one TE ( Wednesday ) is normalized as both The year 2004 and in a decade are not considered to be relevant. The first one being a generic TE and the second one being of granular-ity superior to a year. The annotation of the relevant TE has the attribute indicating that it time-stamps an event realized by a reported speech verb. The nor-malized value of the TE corresponds to the 5th of January 2005, which is a Wednesday. NEs are also annotated.

In the entire AFP corpus, 11.5 millions temporal expressions were detected, among which 845,000 absolute dates (7%) and 4.6 millions normalized relative dates (40%). Although we have not yet evaluated our tagging of relative dates, the system on which our current date normalization is based achieved good results in the TempEval (Verhagen et al., 2007) campaign. In Section 5.1, we propose two baseline approaches in order to give a good idea of the difficulty of the task (Section 5.4 also discusses this point). In Sec-tion 5.2, we present our experiments using simple filtering and statistics on dates calculated by Lucene. Finally, Section 5.3 gives details of our experiments with a learning approach. In our experiments, we used three different values to rank dates:  X  occ ( d ) is the number of textual units (docu- X  Lucene provides ranked documents together  X  An adaptation of classical tf.idf for dates:
In all experiments (including baselines), timelines have been built by considering only dates between the first and the last dates of the corresponding man-ual chronology. Processing runs were evaluated on manually-written chronologies (see Section 3.2) ac-cording to Mean Average Precision (MAP), which is a widely accepted metric for ranked lists. MAP gives a higher weight to higher ranked elements than lower ranked elements. Significance of evaluation results are indicated by the p-value results of the Stu-dent X  X  t-test ( t (90) = 1 . 9867 ).
 5.1 Baseline Runs BL DCT . Indexing and search were done at docu-BL abs . Indexing and search were done at sentence BL mix . Same as BL abs , except that sentences con-Table 1 shows results for these baseline runs. Using only DCTs with Lucene scores or tf.idf(d) already yielded interesting results, with MAP around 0.55. 5.2 Salient Date Extraction with XIP Results In these experiments, we considered a Lucene index to be built as follows: each document was taken to be a sentence containing a normalized date . This sentence was indexed with the title and keywords of the AFP article containing it. Given a query, the top 10,000 documents were retrieved. Combinations be-tween the following filtering operations were pos-sible, by removing all dates associated with a re-ported speech verb ( R ), a modal verb ( M ) and/or a future verb ( F ). All these filtering operations were intended to remove references to events that were not certain, thereby minimizing noise in results.
These processing runs are named SD runs, with indices representing the filtering operations. For ex-ample, a run obtained by filtering modal and future verbs is called SD M,F . In all combinations, dates were ranked by the sum of Lucene scores for these sentences ( luc ) or by tf.idf 4 .

Table 2 presents the results for this series of ex-periments. MAP values are much higher than for baselines. Using tf.idf ( d ) is only very slightly bet-ter than luc . Filtering operations bring significant improvement but the benefits of these different tech-niques have to be further investigated. 5.3 Machine-Learning Runs We used our set of manually-written chronologies as a training corpus to perform machine learning experiments. We used IcsiBoost 5 , an implementa-tion of adaptative boosting (AdaBoost (Freund and Schapire, 1997)).

In our approach, we consider two classes: salient dates are dates that have an entry in the manual chronologies, while non-salient dates are all other dates. This choice does, however, represent an im-portant bias. The choices of journalists are indeed very subjective, and chronologies must not exceed a certain length, which means that relevant dates can be thrown away. These issues will be discussed in Section 5.4.

The classifier instances were not all sentences re-trieved by the search engine. Using all sentences would not yield a useful feature set. We rather ag-gregated all sentences corresponding to the same date before learning the classifier. Therefore, each instance corresponded to a single date, and features were figures concerning the set of sentences contain-ing this date.

Features used in this series of runs are as follows: 1. Features representing the fact that the more 2. Features representing the fact that an important 3. Other features: 1) Lucene X  X  best ranking of the
We did not aim to classify dates, but rather to rank them. Instead, we used the predicted probability P ( d ) returned by the classifier, and mixed it with the Lucene score of sentences, or with date tf.idf : where val ( d ) is either luc ( d ) or tf.idf ( d ) .
Because the task is very subjective and (above all) because of the low quantity of learning data, we prefered not to opt for a  X  X earning to rank X  approach.
We evaluated this approach with a classic 4-fold cross-validation. Our 91 chronologies were ran-domly divided into 4 sub-samples, each of them be-ing used once as test data. The final scores, pre-sented in Table 3, are the average of these 4 pro-cesses. As shown in this table, the learning approach improves MAP results by about 0.05 point. 5.4 Discussion and Final Experiment Chronologies hand-written by journalists are a very useful resources for evaluation of our system, as they are completely dissociated from our research and are an exact representation of the output we aim to ob-tain. However, assembling such a chronology is a very subjective task, and no clear method for evalu-ation agreement between two journalists seems im-mediately apparent. Only experts can build such chronologies, and calculating this agreement would require at least two experts from each domain, which are hard to come by. One may then consider our sys-tem as a useful tool for building a chronology more objectively.

To illustrate this point, we chose four specific top-ics 6 and showed one of our runs on each topic to an AFP expert for these subjects. We asked him to as-sess the first 30 dates of these runs.
Table 4 presents results for this evaluation, com-paring average precision values obtained 1) against the original, manual chronologies ( AP C ), and 2) against the expert assessment ( AP E ). These values show that, for 3 runs out of 4, many dates returned by the system are considered as valid by the expert, even if not presented in the original chronology.
Even if this experiment is not strong enough to lead to a formal conclusion ( post-hoc evaluation with only 4 topics and a single assessor), this tends to show that our system produces usable outputs and that our system can be of help to journalists by pro-viding them with chronologies that are as useful and objective as possible. This article presents a task of  X  X ate extraction X  and shows the importance of taking temporal informa-tion into consideration and how with relatively sim-ple temporal processing, we were able to indirectly point to important events using the temporal infor-mation associated with these events. Of course, as our final goal consists in the detection of important events, we need to take into account the textual con-tent. In future work, we envisage providing, together with the detection of salient dates, a semantic analy-sis that will help determine the importance of events. Another interesting direction in which we soon aim to work is to consider all textual excerpts that are as-sociated with salient dates, and use clustering tech-niques to determine if textual excerpts correspond to the same event or not. Finally, as our news corpus is available both for English and French (compara-ble corpus, not necessarily translations), we aim to investigate cross-lingual extraction of salient dates and salient events.
