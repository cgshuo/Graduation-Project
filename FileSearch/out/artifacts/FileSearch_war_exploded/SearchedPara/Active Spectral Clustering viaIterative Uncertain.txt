 Spectral clustering is a widely used method for organizing data that only relies on pairwise similarity measurements. This makes its application to non-vectorial data straight-forward in principle, as long as all pairwise similarities are available. However, in recent years, numerous examples have emerged in which the cost of assessing similarities is sub-stantial or prohibitive. We propose an active learning al-gorithm for spectral clustering that incrementally measures only those similarities that are most likely to remove uncer-tainty in an intermediate clustering solution. In many ap-plications, similarities are not only costly to compute, but also noisy. We extend our algorithm to maintain running estimates of the true similarities, as well as estimates of their accuracy. Using this information, the algorithm up-dates only those estimates which are relatively inaccurate and whose update would most likely remove clustering un-certainty. We compare our methods on several datasets, including a realistic example where similarities are expen-sive and noisy. The results show a significant improvement in performance compared to the alternatives.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval; I.5.3 [ Pattern Recognition ]: Clus-tering Algorithms Spectral Clustering, Active Learning
Clustering is a fundamental problem involving summariz-ing, indexing and classifying various types of data. As data sets become larger and more complex, algorithms that de-pend on pairwise similarities X  X ather than fixed length fea-ture vector representations X  X re growing increasingly pop-ular. An important example is spectral clustering, which partitions data through a spectral analysis of the Laplacian matrix induced by the similarity graph.

Although methods based on pairwise similarities are grow-ing in popularity, a practical difficulty is that pairwise sim-ilarities can be expensive to acquire, be it due to computa-tional requirements, need for human input, or lack of observ-ability. In protein clustering, for example, a computation-ally expensive alignment process may be necessary before two proteins can be compared. Other examples in computa-tional biology require a combinatorial search for each pair-wise similarity, even when the datapoint can be represented in a compact form (a protein sequence is usually less than 1000 letters long). A counterpart to these computational is-sues is that often the only practical way to obtain similarities is to query human annotators. Here, measurements are not only expensive, but frequently also noisy. In this paper we consider the task of organizing a stream of snapshots taken by a wearable camera at a rate of about one photo per 20 seconds [12]. Clustering these snapshots is beyond the ca-pabilities of existing computer vision algorithms, so human guidance is necessary to either cluster the images, or learn improved models to do the clustering for us. As the human subject (e.g., an Alzheimer X  X  patient), wears the camera dur-ing an open-ended observation period, it is not clear a priori what the clusters should be. A way to tackle this problem is to ask human annotators to rate how similar any two photos are and then to cluster using this data. (For example, photos may be deemed similar if they were taken in similar loca-tions.) This is one of many examples where crowdsourcing, albeit expensive, can be used to collect pairwise similarity measurements. As a final example, in some situations the objects that are being compared can disappear over time, making retrospective comparisons difficult. Consider viral strains, which, if not preserved in a laboratory, may disap-pear, leaving behind only indirect assessments with other viruses. While similarities among preserved viruses can be acquired at a relatively high cost in the lab (e.g., crossre-activity of the immune responses), similarities involving the extinct strain are not available at any cost. In addition to this time barrier, geographic, legal and policy barriers can also make certain pairwise similarities inaccessible. All these examples illustrate that similarities may be noisy and arbi-trarily expensive to compute, to the extreme where certain similarities are completely unavailable. Figure 1: Two incomplete similarity matrices. The left was constructed from median image similarities, measured using Amazon Mechanical Turk (see Sec-tion 6.3). We permuted the rows and columns so that any clusters would become visible as diagonal blocks. Then we set 82% of the similarities to 0 (black regions). The right matrix was constructed similarly, but starting from similarities sampled uni-formly in [0 , 1] . The left matrix still shows clustering structure, which is also visible in the sign vector of the second Laplacian eigenvector, plotted below (see Sections 3 and 4 for details). The right matrix had almost no structure to start with, so no structure is visible in the corresponding eigenvector.

The potentially significant cost of obtaining pairwise sim-ilarities motivates the search for tradeoffs between desired clustering quality and the required amount of data. In this paper, we study this question in the setting of the spec-tral clustering of n objects when we do not have access to all pairwise measurements initially, but we can iteratively query the similarities from an external black-box procedure (which may impose restrictions on which of the subset of all n ( n  X  1) / 2 similarities can be queried). In this active learn-ing formulation the goal is to find a good approximation to the true clustering based on as few similarity evaluations as possible, thus reducing the overall cost (i.e. compute cycles, human tasks performed, laboratory material and other data collection expenses).
 A recent contribution in this direction has been made by Shamir and Tishby [18]. Their approach was designed for querying arbitrary similarity matrices, including those where no clustering structure is apparent. This paper significantly improves on their method by exploiting the fact that most realistic pairwise similarity matrices, even if incomplete, do exhibit clustering structure. Consider Figure 1, where on the left we show an incomplete matrix that we might re-alistically encounter and on the right a matrix constructed from random similarities. Shamir and Tishby assume no structure in the matrix (such as in the matrix on the right) and query measurements that maximally change the overall clustering solution. In contrast, we tailor our active learning algorithm to work well with realistic matrices (such as the one on the left) where information about the true clustering emerges quickly and can be leveraged to guide an improved query selection strategy.

An aspect of spectral clustering that is commonly ignored is that similarity measurements are generally noisy. Active spectral clustering methods have so far not taken these un-certainties into account. We extend our algorithm in this direction to allow repeat measurements of noisy similarities, and use those to compute running estimates of the true sim-ilarities, as well as estimates of their accuracy. Using this information, we extend our algorithm to measure that simi-larity which is relatively inaccurate and whose update would most likely remove clustering uncertainty.

The paper is organized as follows. In Section 2 we review related research. Section 3 presents some background on spectral clustering. Next, we describe our active learning algorithm in Section 4. In Section 5 we extend the algo-rithm to take measurement noise into account. We present experiments on synthetic and real datasets in Section 6 and conclude with final remarks in Section 7.
Spectral clustering was popularized in the machine learn-ing community by Shi and Malik [20] and shortly afterwards revisited by Ng, Jordan and Weiss [14]. Since then, it has permeated the literature and become a firm part of the prac-titioner X  X  toolbox. Over the years, numerous connections to other research fields have been made, a comprehensive re-view of which can be found in [24]. However, very little work in the spectral clustering literature has explicitly in-vestigated situations when only a subset of all similarities is known, possibly contaminated by noise.

One simple approach to adapting spectral clustering to the case of missing similarities is to exploit the burgeoning literature on matrix completion methods. A particularly straightforward approach is to impute a constant value (e.g., zero) for the missing similarities. Such approaches are of-tenusedinpractice,andsomeofitspropertieshavebeen analyzed theoretically [18].

More sophisticated methods can be deployed under the assumption that entire rows or columns of the similarity matrix can be measured. The driving assumption behind these methods is that the true matrix is of low rank, so that a small subset of elements approximately captures the global matrix structure. Among these approaches, the Nys-tr  X  om method [6, 26] is perhaps the most well known. Other algorithms that use row/column sampling include for in-stance [4, 5, 9]. Applications of some of these methods to spectral clustering are given in [7, 17]. In less controlled situations, we do not have access to entire rows/columns of measurements, but only an arbitrary subset; in this set-ting it is still possible to exploit a low-rank assumption for matrix completion [1, 22]. There is debate about the value of these methods in the spectral clustering setting; in par-ticular, Shamir and Tishby [18] argued that the low rank assumptions can be unrealistic in many spectral clustering applications and demonstrated that the Nystr  X  om method of-ten performs poorly. Additionally, most of the row/column sampling methods require that the sampling distribution de-pends on the entire similarity matrix [4, 5, 6, 9]. Because this matrix is unknown, these methods are of limited appli-cability in our setting. Finally, we highlight that with the exception of Huang et al. [11], relatively little work has been done on analyzing the influence of incomplete or perturbed similarity matrices on the spectral clustering solution.
Until recently, the study of active learning for spectral clustering was restricted to settings where the entire simi-larity matrix is known (perhaps approximately) and an ex-ternal oracle can be repeatedly queried for additional link-age constraints between objects (of the form must-link or cannot-link ). When the similarity matrix only approximately captures the desired clustering, adding such constraints it-eratively can help resolve ambiguous boundary cases. Rele-vant examples include [2, 13, 25, 27]. We note in particular the work of Xu et al. [27], in which the constraints are ab-sorbed by modifying the similarity matrix in a way that is akin to measuring similarities of higher quality. The focus in active learning for spectral clustering has only recently shifted to scenarios in which explicit costs are imposed on the measurement of similarities. This focus is exemplified by Shamir and Tishby [18], who propose and analyze an active learning method based on matrix perturbation theory [21].
We begin by presenting our notation and summarizing the key ideas of spectral clustering. For further details and various interpretations of spectral clustering we refer the reader to von Luxburg [24]. Given n objects, denote by W the n  X  n symmetric matrix of pairwise similarities among these objects. Typically, 0  X  w ij  X  1 ,i,j =1 ,...,n and w ii =1 ,i =1 ,...,n .Let D = diag( W 1 ) be the diago-nal matrix of row sums of W .The unnormalized Laplacian matrix is then given by Spectral clustering partitions the n objects into two groups by thresholding the second eigenvector v 2 of L .Specifi-cally, if we let the partition be encoded by variables c i { X  1 , +1 } ,i =1 ,...,n , then Here, we use the notation v 2 ( i ) to indicate the i th compo-nent of the vector v 2 . Because the partitioning is trivial to compute from the second eigenvector, we will occasion-ally refer to the eigenvector itself as the spectral clustering solution, rather than the partitioning.

Spectral clustering only sees the data as filtered through the matrix W . Thus it is possible to adapt the spectral ap-proach to the clustering of non-vectorial data such as graphs, sequences and sets; it suffices that similarity scores can be computed for these objects. This is generally accomplished via a kernel function, and computationally efficient kernels are available for certain kinds of structured objects [10, 19]. Unfortunately, however, kernel formulations are often too rigid to be adapted to specific needs, and often lack inter-pretability. As we move to more complex datasets, the no-tion of similarity a practitioner is interested in may not be captured by a kernel. Indeed, as highlighted in the Intro-duction, in many practical examples the similarities cannot be evaluated by a computer at all, but must be provided by an experiment or human annotator.
In this section we propose an active learning strategy that attempts to alleviate the above issues. Our work is based on a matrix perturbation argument for an intermediate es-timate of the Laplacian matrix. Given incomplete measure-ments, we estimate the true Laplacian matrix as where  X  W is the matrix of all pairwise measurements with zero imputed for unknown entries, and  X  D = diag(  X  W 1 ). The motivation for imputation with zero can be seen by rewriting the spectral clustering problem. The second eigenvector of the Laplacian  X  L can be found as Thus, similarities act as weights on soft constraints between eigenvector components. By imputing zero for missing sim-ilarities we merely ignore those constraints which are not supported by a measurement.

For any set of similarities  X  W ,thesecondeigenvector X  v 2 gives the best guess for an embedding of the objects on the line. The embedding is such that two groups of simi-lar objects are embedded away from zero, on the negative or positive orthant, respectively. Any objects that are ap-proximately equally similar to all remaining objects are em-bedded near zero. This is intuitive, for these are the ob-jects that cannot clearly be assigned to either of the two groups. Indeed, since a mean can be found by minimizing a mean squared error, we see from Eq. (4) that an object i with approximately constant similarities  X  w ij to remain-ing objects j should be embedded near the average of their embedding locations  X  v 2 ( j ). On the other hand, if the data actually clusters well and is reasonably balanced, then we expect the second eigenvector v 2 of the true Laplacian L to have elements with magnitude on the order of 1 / v v 2 = 1. In this way, the elements of most realistic em-beddings v 2 should be expected to be bounded away from zero. Spectral clustering based on incomplete data  X  W par-titions the objects by looking at the signs of the embedding  X  v (Eq. (2), with threshold at zero). Consequently, objects which are embedded near zero are the objects about whose cluster label we should be most  X  X ncertain X  about.
In many practical cases, a relatively small amount of data suffices so that  X  v 2 already indicates a useful clustering. The left sign vector shown in Figure 1 demonstrates this on a real dataset. We use such partial information as a guide towards measurements that more quickly reveal the true nature of the clustering. In this approach, our earlier intuition about the magnitude of  X  v 2 components plays a crucial role. More specifically, our active learning strategy uses matrix pertur-bation theory to reveal that entry of  X  W for which a constant perturbation would change the minimum magnitude element of  X  v 2 the most. The rationale is that by focussing on small magnitude components, we more quickly move them away from the cluster boundary (i.e. 0), and thus reduce uncer-tainty in the partial clustering. In effect, we try to choose measurements that help us push the embedding clusters fur-ther apart. If the data actually clusters well, this should quickly guide us to the clean clustering we expect to find. Algorithm 1: IU-RED
S = { ( i, j ): i, j  X  X  1 ,...,n } ,i&lt;j }  X 
W = I for t =1 ,...,n ( n  X  1) / 2 return Second eigenvector of  X  L = diag(  X  W 1 )  X   X  W
Suppose we have the Laplacian eigenvector decomposition  X  L = n p =1  X   X  p  X  v p  X  v p and that  X   X  1  X   X   X  2 ...  X   X  clustering,  X   X  1 =0and v 1 = 1 / theory (e.g. Stewart and Sun [21], Chapter V, Section 2.3) gives the first order change of the second eigenvector as provided  X   X  2 has multiplicity 1. Note that  X   X  L/ X   X  w ij e )( e i  X  e j ) , where e i is the indicator vector of i .If k argmin k |  X  v 2 ( k ) | , the change to the smallest magnitude ele-algorithm, IU-RED, is given in Algorithm 1.

A recent algorithm due to Shamir and Tishby [18] has a similar structure. The main steps are shown in Algo-rithm 2, which we refer to as S&amp;T throughout. 1 The algo-rithm chooses measurements that maximize the global change to  X  v 2 by maximizing the norm on line 2. The reasoning is the following: As more measurements are acquired, the esti-mate  X  v 2 will necessarily converge to the true eigenvector v regardless of the query ordering, since only a finite number of measurements can be made. Since constant perturbations to elements of  X  W can have varying effects on  X  v 2 , we should choose to update that element where the effect is largest.
If the similarity matrices were random, we would not ex-pect to see partial clusterings emerge in  X  v 2 ,evenwithfairly large amounts of data. Figure 1 has highlighted this. In this unstructured setting, Shamir and Tishby X  X  method may well be the best we can do, for it targets the global change in the clustering solution. Our algorithm exploits that practical similarity matrices are highly structured even when severely subsampled and uses this partial information as a guide for query selection. Our experiments emphasize that the empir-ical improvements of our method are significant, even though the algorithmic differences may at first appear minor.
The full algorithm requires a  X  X udget X  parameter b which specifies the maximum number of measurements that can be requested from an oracle. For this paper, we set b = n ( n  X  1) / 2. Another version of their algorithm interleaves active selection with random selection, which we consider in our experiments.
 Algorithm 2: S&amp;T [18]
S = { ( i, j ): i, j  X  X  1 ,...,n } ,i&lt;j }  X 
W = I for t =1 ,...,n ( n  X  1) / 2 return Second eigenvector of  X  L = diag(  X  W 1 )  X   X  W In many settings, only noisy similarities can be measured. In the Section 6, for example, we consider a crowdsourc-ing application where similarities are manually assigned by human labelers. A significant factor there is that even co-operative workers may disagree on similarity scores. Noisy similarities can be a fundamental problem, yet their impact on spectral clustering has not been thoroughly understood. Huang et al. [11] are among the few to investigate the ef-fects of perturbations when all similarities are known. To our knowledge, active spectral clustering with costly and noisy measurements has not been considered.

The simplest way to deal with noise is to take the mean or median of multiple repeated measurements. However, given m repeated measurements of normally distributed similari-ties, both the mean and median have a standard deviation that is only a factor of O (1 / gle measurement. Thus, to halve the standard deviation we need about four times as many measurements. Thus, mea-suring every similarity multiple times is a fairly expensive way to reduce the effects of noise. It is especially wasteful since it is likely that only similarities of objects close to the cluster boundaries need to be known accurately to resolve the decision boundary. This section gives an active learning algorithm that asks for repeat measurements only when the measurement can significantly change the current solution and when our uncertainty in the true similarity is large.
We have extended IU-RED to maintain a  X  X unning me-dian X  estimate for each similarity. At any time, the true similarity is estimated as the median of any repeat mea-surements. Additionally, we maintain for each similarity es-timate  X  w ij an estimate of its standard deviation,  X   X  ij no measurements were made, we let the standard deviation be that of a uniform on [0, 1]; i.e.,  X   X  ij = 1 / 12  X  0 . 2887. After a single measurement we set  X   X  ij = s ,anestimateof the population standard deviation, and we let  X   X  ij = s/ for m repeat measurements. 2 IU-RED was then modified to choose that measurement (possibly a repeat) where the
Several variations of this theme could be considered. Given enough samples, frequentist confidence intervals of the me-dian could be estimated using the bootstrap. Alternatively, given prior information one could use Bayesian techniques to estimate posterior means and variances.  X 6  X 4  X 2 0 2 4 6  X 2 0 2 Error rate 0 0.1 0.2 0.3 0.4 0.5 0 0.2 0.4  X 6  X 4  X 2 0 2 4 6  X 2 0 2 Error rate 0 0.1 0.2 0.3 0.4 0.5 0 0.2 0.4 choosing that measurement where our current estimate is uncertain and where the uncertainty matters. The S&amp;T al-gorithm can be similarly modified and is considered in that form during our experiments.
We begin this section by evaluating the basic IU-RED al-gorithm of Section 4 on synthetic and real datasets. Subsec-tion 6.3 then considers the extension to noisy measurements.
Our methods include IU-RED and a version of IU-RED where active selection is interleaved with random selection. We compared against three alternatives: S&amp;T, S&amp;T with interleaved random selection, and random selection only. The last three methods have been previously evaluated in Shamir and Tishby [18] using a similar evaluation method-ology and on similar datasets as we consider here. In par-ticular, we consider binary classification, which can be ex-tended to more than two clusters by recursive splitting or by expanding the reasoning outlined here to more eigenvectors. Shamir and Tishby also evaluated an algorithm based on the Nystr  X  om method [7], but often found performance to be poor if the low rank assumption was not met. We evaluate all methods on the misclustering error, relative to a spec-tral clustering solution with complete dat and give average results over 20 runs.
We first present results on a simple clustering task in which the data is drawn from mixtures of Gaussians with two components. The data and results are shown in Fig-ure 2. For each dataset we sampled a total of 200 points from two Gaussians of increasing separation. We normal-ized the data to lie in the unit hypercube and used a stan-dard radial basis function kernel to compute similarities. As expected, random selection usually performs worst, except when the cluster separation is minimal. Compared to our two algorithms, S&amp;T does poorly even on easy problems. As reported in [18], interleaving with random selection sig-nificantly boosts performance. IU-RED outperforms both 0.2 0.4 Error rate 0.2 0.4 Error rate 0.2 0.4 Error rate versions of S&amp;T early on, but ties with them towards the end. Interestingly, while interleaving IU-RED appears to eventually stabilize the error rates, it slightly hurts perfor-mance early on.
We have also evaluated our algorithm on a variety of real datasets. Five of the sets in this subsection are from the UCI repository [8] (iris, wine, pendigits, waveform, segmen-tation). An additional dataset concerning the similarity of faces is available from the University of Washington [15]. We followed [11, 18] and normalized the data to lie in the unit hypercube and used the Gaussian kernel to compute similar-ities. 3 Figures 3(a) X (e) show the results on the UCI datasets. Figure 3(f) shows results on the face dataset. Note that the scaling of the x -axis changes between plots. The difference
Although the UCI datasets were previously analyzed in [18], the results are not directly comparable, since their evaluations used different kernel parameters. Also, since each of these datasets contains more than two classes, it is possible that we chose two different classes for evaluating the spectral clustering. between methods is amplified on these datasets. Except for the iris dataset in Figure 3(a), S&amp;T performs relatively poorly, and is eventually outperformed by random selection. On three of five UCI datasets IU-RED outperforms all other methods early on. Interleaving usually increases the error initially, but eventually leads to a more stable algorithm with marginally lower error. The exception is the waveform dataset in Figure 3(d) where interleaving helps significantly. Lastly, on the face dataset IU-RED also outperforms S&amp;T early on, with slight gains for interleaving.

We conducted a further experiment to assess whether the superior performance of IU-RED over S&amp;T can be seen after a single active learning step, or only emerges after many such steps. In this experiment, we sampled a subset of similari-ties of approximately constant size uniformly at random and measured the decrease in error rate over one active selection step. These results were averaged over 30000 restarts. The first five rows of Table 1 show results for UCI data, the sixth row for the face data, and the last row for similarity matrices with off-diagonal entries uniform in [0, 1]. On four out of five UCI datasets and on the face dataset, IU-RED decreases the error more than S&amp;T. The one dataset where we perform worse is the waveform dataset, which Figure 3(d) shows to be challenging for all methods. Overall, our method performs much better than S&amp;T on structured similarity matrices. On random matrices both algorithms perform poorly, but now S&amp;T performs better than IU-RED.
Our next experiment focuses on the realistic example out-lined in the Introduction where similarities are hard to com-pute and noisy. The data of interest is a photo stream, acquired by a wearable camera at a rate of about one photo per 20 seconds. Clustering the data by the location at which an image was taken may be useful in a variety of applica-tions, ranging from health (e.g., in diagnosis and life quality improvement for Alzheimer X  X  patients) to summarization of personal memories. Because the data is collected in an en-tirely unconstrained way, analyzing it is beyond the capa-bilities of current unsupervised algorithms. The top row of Figure 4, for example, shows five images taken in the same kitchen. Clustering clearly requires some human input; at the very least a preliminary annotation that could be used to train supervised computer vision algorithms. It is im-practical to ask annotators to simply label images by their location, since salient locations and their number only be-come evident as the stream progresses and may change from week to week, and from subject to subject. Also, locations may be interconnected, and multiple locations might be vis-ible from the same viewpoint. In our data, for example, an open kitchen connects to the living room so that large parts of the space can be perceived to belong to both rooms. It is much more natural (and cost effective) to collect similarities between images and to infer a clustering from that data.
We took this approach in order to cluster 100 images taken from the photo stream described above [12]. Of these, about 50weretakeninanopenkitchen,and50weretakeninthe adjacent living room. Some example images are shown in Figure 4. We asked workers on Amazon Mechanical Turk to rate, on a scale from 1 to 10, how likely it was that a given pair of images was taken in the same room, with 10 indicat-ing certainty. The user ratings were divided by 10 and then used as similarities. Humans are adept at matching rooms by a loose jumble of visible objects, making this task fairly realistic. Indeed, the two rows of Figure 4 show representa-tive examples from a partition that was found by spectral clustering using the complete median similarity matrix, with the median running over three repeat measurements. A sub-sampled version of the median similarity matrix was shown in Figure 1 on the left. To collect one similarity for each pair of photos costs a total of US$74, so the three repeats required for the median cost a total of US$222.

We first evaluated our algorithms on the median similarity matrix using the same methods as before. The results are shown in Figure 5(a). The legend is the same as in Figure 2. Note that the x -axis is scaled to extend beyond 1 . 0, to ac-count for the three repeat measurements necessary to com-pute one median similarity. A fraction of f indicates that a total of fn ( n  X  1) / 2 pairwise measurements were made. As before, our method outperforms a number of competitors early on. The results can be also interpreted in terms of the amount of money that must be expended to achieve a clus-tering result of fixed quality. Each image comparison cost us US$0.045 on Amazon Mechanical Turk. Table 2 shows the resulting approximate cost in US$ for each algorithm in order to achieve an error rate of 0 . 05. IU-RED is at least 4 times cheaper than S&amp;T, which costs more than 30% of the complete-labelling cost. This difference can easily render larger image clustering tasks than ours impractical.
We also evaluated how IU-RED and S&amp;T compare over onlyoneactivechoice. Theresultisshownintherowof Table 1 labeled  X  X hotos. X  As before, our active learning framework outperforms that of Shamir and Tishby.

Next,weconsidertheextensionofIU-REDtodealwith noisy similarities, as outlined in Section 5. Figure 6 illus-trates the type of noise encountered in this labelling task. We allow up to three repeat measurements of similarities that are known with high uncertainty and which can poten-tially change the current solution. The results are shown in Figure 5(b). For comparison, Figure 5(c) shows results when no repeat measurements are allowed and the standard devi-ation is not estimated. The latter is the extreme counterpart to measuring every similarity three times. All error rates are relative to a spectral clustering computed from the complete median similarity matrix, averaged over 20 runs. As before, we scaled the x -axes to show the effective fraction of pairwise measurements that was made. For the no-repeats framework in Figure 5(c) this fraction cannot be larger than 1.0. An-other important consequence of this measuring framework is that all algorithms should yield approximately the same Error rate Error rate Error rate Figure 5: Results on the  X  X hotos X  dataset. Fig-ure (a) shows results when true similarities are es-timated as the median of three measurements. Fig-ure (b) shows results when the algorithm is allowed choose which repeat measurements to make. Up to three repeats are allowed. Figure (c) shows results when similarities are estimated by a single noisy measurement. The legend is the same as in Figure 3. average error rate at a fraction of 1 . 0, since at that point every algorithm has observed one complete set of (noisy) similarities. The differences between algorithms are thus only appreciated in the first half of the Figure 5(c).
Both versions of IU-RED continue to beat the remain-ing algorithms in either of the two new settings. However, random interleaving now helps slightly, where it was detri-mental before. With the exception of S&amp;T, most methods improve early on compared to Figure 5(a). On the one hand this is intuitive, since as long as the similarities are not ex-tremely noisy, three measurements do not convey three times as much information as one. One the other hand, it suggests that moderately noisy measurements can still be quite in-formative. Even so, we emphasize again that all algorithms should perform identically at a fraction of 1.0 in Figure 5(c). Maintaining running estimates and their accuracies is there-fore preferable as it does not force performance to equalize once a fixed measurement quantum has been reached. In-deed, both IU-RED and IU-RED with random interleaving perform marginally better in Figure 5(b) than Figure 5(c) once a fraction of 1.0 measurements is reached.
 Table 1: Average decrease in error rate across one selection step. IU-RED generally decreases the er-ror more than S&amp;T.
 Table 2: Approximate labelling costs in US$ to achieve a 0.05 error rate on the photos dataset.
 Figure 6: Worker disagreement for two image com-parisons on Amazon Mechanical Turk. For the left two photos, workers agreed they were certainly taken in the same room; for the right two, one worker asserted they were definitely not.
In this paper we have presented and evaluated an active learning algorithm for spectral clustering. Our main insight is that similarity matrices are not random, but usually ex-hibit clear clustering structure. Even when observing only a small fraction of the data, this structure becomes evident. Furthermore, assuming that the data clusters well, the final v 2 will usually have elements well away from zero. Moti-vated by these observations, our algorithm uses the current estimate  X  v 2 to choose measurements that will be most use-ful in removing elements close to zero, i.e., to push the two clusters in  X  v 2 apart. We have applied this algorithm to a range of datasets and showed that it generally outperforms a related algorithm by Shamir and Tishby.

The effects of costly and noisy similarities have so far been ignored in the active learning setting. We propose an exten-sion of our algorithm that maintains running estimates of the true similarities as well as their accuracies. By taking these accuracies into account during query selection, we can po-tentially avoid unnecessary repeat measurements and speed up the learning process in noisy settings.

Rahimi and Recht [16] previously showed that a version of spectral clustering related to normalized cuts [20] clusters data by finding a hyperplane that cuts the data in a lifted space. The signed distances of points to the hyperplane are given by rescaled elements of an Laplacian eigenvector, and the partitioning can be done by taking the sign of the dis-tances. If we have that W 1 = c 1 ,forsome c&gt; 0, then their result implies that our version of spectral clustering also finds such a hyperplane, and that the signed distances are proportional to the second eigenvector v 2 .Ouractive learning approach can then be interpreted as choosing mea-surements that can maximally perturb the margin between a hyperplane and the lifted datapoints. Rahimi and Recht X  X  observation has recently been used to derive an active learn-ing rule for the spectral graph transducer [2]. Here W is completely known, but for each object an additional binary class label can be queried. The rule chooses to label that point next which is currently closest to a hyperplane. Similar heuristics have also been employed in a number of other clas-sifiers and clustering frameworks [3, 13, 23, 27]. In all these methods, however, the pairwise similarities are assumed to be known initially (either implicitly or explicitly) but ad-ditional labels or constraints can be queried. In contrast, our setting allows for incomplete similarities which we can (perhaps only noisily) measure at high cost. [1] D. Achlioptas and F. McSherry. Fast Computation of [2] Z. Bod  X  o, Z. Minier, and L. Csat  X o. Active Learning [3] C. Campbell, N. Cristianini, and A. Smola. Query [4] P. Drineas and R. Kannan. Pass-Efficient Algorithms [5] P. Drineas, R. Kannan, and M. W. Mahoney. Fast [6] P. Drineas and M. W. Mahoney. On the Nystr  X  om [7] C. Fowlkes, S. Belongie, F. Chung, and J. Malik. [8] A. Frank and A. Asuncion. UCI Machine Learning [9] A. Frieze, R. Kannan, and S. Vempala. Fast [10] T. G  X  artner. A Survey of Kernels for Structured Data. [11] L. Huang, D. Yan, M. I. Jordan, and N. Taft. Spectral [12] N. Jojic, A. Perina, and V. Murino. Structural [13] P. K. Mallapragada, R. Jin, and A. K. Jain. Active [14] A. Y. Ng, M. I. Jordan, and Y. Weiss. On Spectral [15] University of Washington Information Design [16] A. Rahimi and B. Recht. Clustering with Normalized [17] T. Sakai and A. Imiya. Fast Spectral Clustering with [18] O. Shamir and N. Tishby. Spectral Clustering on a [19] J. Shawe-Taylor and N. Cristianini. Kernel Methods [20] J. Shi and J. Malik. Normalized Cuts and Image [21] G. W. Stewart and J. Sun. Matrix Perturbation [22] G. Tak  X  acs, I. Pil  X  aszy, B. N  X emeth, and D. Tikk. [23] S. Tong and D. Koller. Support Vector Machine Active [24] U. von Luxburg. A Tutorial on Spectral Clustering. [25] X. Wang and I. Davidson. Active Spectral Clustering. [26] C. K. I. Williams and M. Seeger. Using the Nystr  X  om [27] Q. Xu, M. desJardins, and K. L. Wagstaff. Active
