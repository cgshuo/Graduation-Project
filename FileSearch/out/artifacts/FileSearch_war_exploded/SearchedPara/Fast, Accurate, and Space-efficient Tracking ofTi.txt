 How can we discover interesting patterns from time-evolving high speed data streams? How to analyze the data streams quickly and accurately, with little space overhead? High speed data stream has been receiving increasing attentions due to its wide applications such as sensors, network traffic, social networks, etc. One of the most fundamental tasks in the data stream is to find frequent items; especially, finding recently frequent items has become important in real world applications.

In this paper, we propose TwMinSwap ,afast,accu-rate, and space-efficient method for tracking recent frequent items. TwMinSwap is a deterministic version of our moti-vating algorithm TwSample which is a sampling based ran-domized algorithm with nice theoretical guarantees. TwMin-Swap improves TwSample in terms of speed, accuracy, and memory usage. Both require only O ( k ) memory spaces, and do not require any prior knowledge on the stream such as its length and the number of distinct items in the stream. Through extensive experiments, we demonstrate that TwMin-Swap outperforms all competitors in terms of accuracy and memory usage, with fast running time. Thanks to TwMin-Swap , we report interesting discoveries in real world data streams, including the difference of trends between the win-ner and the loser of U.S. presidential candidates, and doubly-active patterns of movies.
 H.2.8 [ Information Systems ]: Database Applications X  Data mining Design, Experimentation, Algorithms Data Stream; Time-weighted Counting; Sampling; Frequent Items; Hot Items; Top-k Items
How can we discover currently hot items in high speed data streams, like keyword streams from social networks or restaurant check-ins? How to track them in real time with high accuracy and small memory requirements? These ques-tions are directly related to finding recently frequent items in a data stream. Formally, a data stream is defined by a se-quence of items where each item arrives one by one. Usually, the length of the stream and the number of distinct items are considered as very large numbers, possibly infinite. Due to this massiveness, it is impossible to store all the infor-mation from the stream, and thus it becomes important to efficiently use memory spaces. As a result, most data stream mining algorithms perform approximation rather than exact computation, and the followings are generally required [4]. First, the stream should be scanned as few times as possi-ble: only one scan is enough for many recent algorithms. Second, the amount of used memory spaces should be lim-ited and independent of the number of distinct items and the stream length, e.g. O ( k ) space complexity for finding top-k frequent items. Third, processing an item at each time should be fast because the rate of item arrival can be bursty, e.g. Internet traffic may be exploded by network anomalies like DDoS (Distributed Denial of Service) attacks. Fourth, an up-to-date result should be available on demand. These requirements allow that data stream mining algorithms run in real time with small memory spaces.

A number of studies [5, 6, 12, 20] have shown efficacy of their methods in finding frequent items in a data stream, but still it is not clear whether they can find recent fre-quent items correctly. Although an item whose frequency decreases over time tends to have a small count by con-struction of algorithms, it is not done explicitly. Another problem is that among discovered frequent items, it is hard to know when they have become frequent. Depending on applications, the problem is crucial. For example, when we monitor keywords mentioned in SNS, it is important to know which one is the current trend and which one is the past trend. To overcome this weakness, finding recent frequent items from a data stream has been also studied [2, 9, 13]. However, they have limitations in accuracy, running time, and memory usage.

In this paper, we propose TwMinSwap ,afast,accu-rate, and space-efficient method for tracking recent frequent items. TwMinSwap is a deterministic version of our moti-vating algorithm TwSample which is a sampling based ran-domized algorithm with nice theoretical guarantees. TwMin-Swap improves TwSample in terms of speed, accuracy, and Table 1: Comparison of performance of our algo-rithms and competitors. For each row, we write the best in bold and the worst with the canceled line. Our proposed deterministic algorithm TwMinSwap outperforms the others in precision &amp; recall, error in estimation of time-weighted counts, and memory usage. In running time, it is the second best one. memory usage. Both algorithms only require O ( k )space complexity. Especially, our deterministic algorithm TwMin-Swap requires no other parameter than k and  X  ,andthis simplicity enables to not only save memory spaces but also reduce per-item processing time. Table 1 compares our pro-posed TwMinSwap with other competitors, and Figure 1 shows the plots of memory usage vs. error in estimated time-weighted counts for them. TwMinSwap outperforms the others in terms of precision &amp; recall, time-weighted count estimation, and memory usage; its speed is comparable to the fastest competitor TwHCount requiring large memory spaces (see also Figure 4).

Conducting extensive experiments to compare with exist-ing methods, we demonstrate that our algorithms find top-k time-weighted frequent items with small memory spaces whose error is small in terms of precision &amp; recall, and esti-mated time-weighted counts. Also applying the algorithms to real world data streams, we show that tracking recently frequent activities and keywords enables to discover sudden bursts of attentions to currently hot events and trends in real time.

Our contributions are summarized as follows. 1. Method. We introduce time-weighted counting and 2. Performance. We show that (1) TwMinSwap out-3. Discovery. We apply TwMinSwap to real world data
The rest of the paper is organized as follows. In Section 2, we give related works including algorithms with and with-Figure 1: Our proposed deterministic algorithm TwMinSwap outperforms the others in both mem-ory usage and error of estimated time-weighted counts of top-k items. Despite comparable esti-mation of TwHCount( 10 ) to that of TwMinSwap , TwHCount( 10 ) requires much more memory spaces. out time-weighting. In Section 3, we describe and analyze our proposed method. In Section 4, experimental compar-ison with other competitors on synthetic data streams is provided, and in Section 5, we show the discovery results of applying our method to real world data streams. Finally we conclude our work in Section 6.
 Table 2 lists the symbols frequently used in this paper.
In this section, we describe related works on finding fre-quent items, and recent frequent items.
There have been numerous studies to find frequent items from a data stream, including not only developing methods but also comparing them [6, 20]. Here, we classify them into three categories as follows.

Sampling based Approach. This approach involves a probabilistic process, and obtained items are random sam-ples over all the items occurring in a data stream. Vitter [25] proposed a uniform sampling method from a data stream by which, in essence, higher frequency items are sampled much more than lower ones. Improving the space requirement for the sampling, Gibbons and Matias [12] invented a method called concise sampling which efficiently represents sampled items. By slightly modifying the method, they also pro-posed counting sampling to estimate the frequency of each item more accurately. A similar approach was adopted in [21] to obtain high frequency items.

Counter based Approach. A very basic form of the counter based approach is Majority that finds the majority item in a stream if it exists [3, 11]. By generalizing Major-ity , Misra and Gries [24] developed a method to find items occurring at least N/k times and it was improved in per-item processing time by [10, 16]. LossyCounting [21] does the same job but it additionally guarantees that no item whose count is less than N (1 /k  X  )isreported. SpaceSaving [23] reduces the space requirement not only for a general data distribution but also for a Zipf distribution.

Sketch based Approach. The sketch based approach is usually based on using multiple hash functions to map incoming items to a hash table. This can be also under-stood as maintaining a list of independent counters where each counter is shared by a few items and the sharing is determined by the hash functions. Charikar et al. proposed CountSketch that computes items appearing at least N/ ( k + 1) times with probability 1  X   X  while requiring O ( k/ 2 log N/ X  ) memory spaces. The space requirement was improved by CountMin [7]. GroupTest [8] was developed for a hot item query, which groups items and assumes one frequent item in each group. Jin et al. improved GroupTest in space and their algorithm guarantees the minimum count of items outputted [15].
Despite many algorithms to find frequent items from a data stream, researchers have agreed that recent items are more important than old ones and answering a query of find-ing recently frequent items is often required. Below, we in-troduce two approaches for the purpose.

Sliding Window based Approach. This approach di-vides recent times into window blocks, performs counting for items in the windows, and aggregates them. Golab et al. [13] proposed a method based on basic window blocks for identifying frequent items in packet streams in which item frequency is known to follow a power-law distribution. They also extended the work to the more general case that the item frequency follows a multinomial distribution [14]. There have been works using windows of various sizes. Arasu and Manku [2] provided deterministic and randomized al-gorithms for -approximate quantiles over sliding windows. Dallachiesa and Palpanas [9] studied the problem in ad hoc time windows. They use overlapping window blocks whose sizes exponentially grow by which a query for frequent items during a certain recent time period can be answered more accurately.

Time-aware based Approach. This approach implic-itly considers the recentness. Liu et al. [19] proposed a prun-ing method, a key operation of counter based algorithms, considering time information so that an older item is more likely to be pruned than a more recent one when the memory becomes full. A similar approach has been examined in [5, 26, 27]. All of them adopt a time fading factor in develop-ing methods to find frequent items which decreases a weight of an item over time. As a result, recent items have more weights, leading to more accurate results. Our proposed algorithms in this paper also belong to this category. We show that time-weighted counting can be done via sampling which guarantees its accuracy in expectation, and propose our main algorithm TwMinSwap via derandomization. Algorithm 1: TwSample : Randomized Time-Weighted Counting
Input : A data stream S ,anumber k of counters, a  X   X  1.
K  X  X  X  . 3 foreach new item u from S do 4 Downsampling (  X  ). 5 if bernoulli (  X   X   X  1 )=1 then 6 if u  X  K then 9 K  X  K  X  X  u } . 12 while | K | &gt;k do 13  X   X   X  +  X  . 14 Downsampling (  X   X  ). 18 Subroutine Downsampling (  X  ) 19 foreach v  X  K with counter c v do 20 c v  X  binomial ( c v , X  ). 21 if c v =0 then 22 K  X  K \{ v } .
In this section, we propose our method for finding time-weighted top-k items from a data stream. The main idea is derandomization of a sampling based algorithm. As a result, we propose a deterministic algorithm TwMinSwap ,which requires only O ( k ) memory spaces where k is the number of time-weighted frequent items that we want to find. To de-velop TwMinSwap , we first propose a sampling based ran-domized algorithm TwSample whose performance is guar-anteed in expectation. However, the probabilistic nature of TwSample requires several independent sampling sessions to achieve high accuracy, leading to large memory spaces and slow running time. On the other hand, TwMinSwap achieves high accuracy with fast running time while using only a single session.

We start with the definition a time-weighted count of an item [5, 26, 27].

Definition 1 (Time-weighted Count). Let u be an item occurring in a data stream at times t 1 ,...,t c .The time-weighted count of the item u is defined by where  X  is a decaying parameter and t cur is the current time.
To develop a sampling based randomized algorithm, we first define a penalized time-weighted count as follows. Definition 2 (Penalized Time-weighted Count).
 Let u be an item occurring in a data stream and let T u be a set of the times at which u has occurred. The penalized time-weighted count of the item u is defined by where  X  is a decaying parameter, t cur is the current time, and  X  is a default penalty term for items just arriving.
Let the sequence of items till time t cur be u 1 ,...,u t cur Given 0 &lt; X   X  1and  X   X  1, our randomized algorithm samples each item u t with probability  X  t cur  X  t +  X   X  1 is incrementally done with increasing  X  over time to ensure that the number of distinct items in the samples is at most k . Indeed, our algorithm can be understood as an extension of the uniform sampling in a data stream [12] to a time-weighted sampling. Below, we call that u is monitored if u has been sampled at least once.

Precisely, our randomized algorithm TwSample requires three parameters: the maximum number k of monitored items, the time-weighting factor  X  , and the increase ratio  X  for the penalty term  X  . Also there are three pieces of in-formation incrementally updated: the penalty term  X  ,the set K of monitored items, and counters c v associated with each v  X  K . Initially,  X  =1and K =  X  . A new item u cur-rently arriving is determined whether sampled or not with probability  X   X   X  1 . The sampling is done as follows: if u is currently monitored, i.e. u  X  K , c u is incremented by 1; oth-erwise u is added to K with its associated counter c u =1. After the sampling, if | K | &gt;k , downsampling is applied to all the sampled items so far with increasing  X  . Precisely,  X  is incremented by  X  , and for each v  X  K , c v is updated by a random number drawn from binomial ( c v , X   X  ) 1 .If c becomes 0, v is evicted from K . This downsampling is re-peated until | K | X  k . Lastly, whenever one time step passes, all items in K are unconditionally downsampled as follows: for each v  X  K, c v = binomial ( c v , X  ). Algorithm 1 fully describes TwSample .

Effect of  X  . Essentially,  X  determines the sampling rate for evicting existing items so that the number of sampled items does not exceed k .As  X  gets smaller, the amount of decrements of each item count in Line 14 is reduced, lead-ing to longer time for the eviction process in Line 12 to 15. Accuracy may increase since we do not evict more than required. On the other hand, as  X  gets larger, the eviction process finishes quickly, although the accuracy may decrease since we may evict more than one item.

Lemma 1. At time t cur with the penalty term  X  ,each item u occurring at time t  X  t cur has been sampled with probability
Proof. Let u be a new item at t =1and t cur =1. Itis unconditionally sampled because  X  = 1, and all counters are empty. In other words, u is sampled with the probability  X 
Let t cur  X  1. Assume that the lemma holds for time t cur That is, for each sample v at time t  X  t cur ,ithasbeen sampled with probability  X  t cur  X  t +  X   X  1 . Let us consider the process for t next = t cur +1. In Line 4, all samples are down-sampled with probability  X  . This means that after Line 4, remaining samples are with probability  X  t cur +1  X  t +  X  matches the sampling probability at time t next = t cur +1.
Next, we verify from Line 5 to 11. Let u be a new item occurring at t = t next .Clearly, u is sampled with probabil-u is currently monitored or not. Let us verify the down-sampling. Let d be the number of iterations by the while statement in Line 12. Then, after the downsampling, each sample experiences re-sampling with probability  X  d X  ,and matches the update of  X  =  X  + d X  .

Note that the sampling probability does not increase over time since  X  increases or remains the same at every time step. Hence, an item that is not a sample at a certain time cannot be a sample in the future.
 The following corollary is directly derived from Lemma 1.
Corollary 1. At any time t cur in TwSample ,theex-pectation of a counter c u of a monitored item u  X  K is where T u is a set of times at which u has occurred. Since we know  X  at any time, we can compute the expected time-weighted count for a monitored item u . That is, the estimated time-weighted count of u  X  K becomes  X  1  X   X  c u
Lemma 2. At any time, the probability p u that an item u is monitored satisfies the following inequality:
Proof. Note that p u =Pr[ c u &gt; 0] since c u = | T u | is a random variable where c u ( i ) indicates whether the i -th occurrence of u is sampled or not. Applying the Chernoff bound, we obtain the following inequality: p u =Pr[ c u &gt; 0] = 1  X  Pr [ c u =0]  X  1  X  exp (  X  E [ c Since E [ c u ]= P ( u ) by Corollary 1, the proof is done. Lemma 2 states that as an item is more insignificant, the probability that it is not monitored increases exponentially.
Although TwSample is simple and provides the theoret-ical guarantees of its output, its performance may be de-graded due to its probabilistic nature. First, the running time may become slow because the number of iterations for the downsampling with increasing  X  is not fixed and the time for drawing random variables from binomial ( c,  X  )used in the downsampling depends on c . Second, discovered top-k items and the associated counters may be inaccurate due to unintendedly large  X  . This inaccuracy can be resolved by maintaining s number of independent sessions each of which monitors at most k items, but it leads to more mem-ory spaces and longer running time. In the next section, we propose a deterministic variation of TwSample ,whichis fast and requires a single session of monitored items of size at most k .
In this section, we propose TwMinSwap for efficient top-k time-weighted frequent items discovery. This algorithm Algorithm 2: TwMinSwap : Deterministic Time-Weighted Counting
Input : A data stream S , the number of counters k ,
K  X  X  X  . 3 foreach new item u from S do 5 foreach v  X  K do 8 if u  X  K then 10 else if | K | &lt;k then 11 K  X  K  X  X  u } . is a deterministic version of TwSample with truncating in-significantly old items.

The main idea is to record the expected number of sam-ples for each item directly instead of applying the random process. Concretely, for an item u occurring at t  X  t cur instead of incrementing c u by 1 with probability  X  t cur we increment c u by  X  t cur  X  t with probability 1. Then, the downsampling with rate  X  becomes that for each monitored item v  X  K , c v =  X c v . With this deterministic scenario, however, we encounter a problem when all counters become full. Precisely, because the expected count for an item oc-curring at least one time in the stream never becomes 0, it needs pruning for dropping an insignificant monitored item to start monitoring a new item. We propose a simple heuris-tic for the pruning which does not require additional mem-ory spaces, leading to smaller memory usage compared with the previous approaches [5, 26, 27]. We note that this pro-posed pruning method here corresponds to decreasing the sampling rate by increasing  X  in TwSample .
 Details of the heuristic for the pruning are as follows. Let K be a set of currently monitored items where | K | = k ,and u be an item just arriving from a data stream. Our pruning method first finds an item v  X   X  K having the minimum v  X  and start monitoring u with initial count 1; otherwise, u is ignored. This swapping of u and v  X  makes sense because c smaller than the effect 1 by the single occurrence of u at t cur . The overall procedure of TwMinSwap is described in Algorithm 2.
 Advantages of TwMinSwap are summarized as follows. First, its memory usage is small. For each item, only an item identifier and its time-weighted count are maintained. This simple structure enables to reduce per-item computation compared with similar counter based approaches [26], and greatly saves memory spaces compared with sketch based algorithms [5]. Second, TwMinSwap requires the minimal parameters: k and  X  . This especially gives the benefit of reducing efforts for parameter tuning in practice. Third, in contrast to TwSample , TwMinSwap guarantees to output k number of items so long as N  X  k .

Per-item Processing Time. The main time-consuming operations are: (1) computing an item with the minimum count, and (2) multiplying  X  to all counters each of which re-quires scanning K . The second operation can be eliminated by maintaining the most recent time for each item when it occurs [26]. In this way, we benefit in speed when a new item is already monitored. In contrast, the first operation taking O ( k ) time is unavoidable. Although an efficient data struc-ture was proposed for the case without time-weighting [10], it cannot be applied to our time-weighted case since our counters record real numbers with which difference between two numbers is unfixed. As a result, TwMinSwap requires O ( k ) computation for each iteration.
We analyze TwMinSwap especially for the condition that a monitored item is not evicted from K . More precisely, Lemmas 3 and 4 state that TwMinSwap will not evict items whose frequencies of occurrences are above certain thresh-olds for a general and a power-law item distribution cases, respectively.
 Lemma 3. Let 0 &lt; X   X  1 be a time-decaying parameter of TwMinSwap . Any item with count c  X  1 will not be evicted from K if it occurs at least once per every 1  X  log  X   X  times where  X  = min { 1+  X , c } .

Proof. Assume that v  X  K with count c v = c  X  1ata certain time occurs for every d times. Note that any item whose count is at least 1 is never evicted from K by con-struction. Let us define the following function. where g (1) = c X  d  X  1 . It is clear that g ( r )is c v after rd time steps. Since c v always decreases from ( r  X  1) d +1to rd  X  1, it suffices to show that g ( r )  X  1with d  X  1  X  log for every r  X  1 where  X  =min { 1+  X , c } .

For r = 1, assume that c  X  1+  X  ; then, Assume that c&gt; 1+  X  .
For r&gt; 1, assume that g ( r  X  1)  X  1; then,
Below, we show which item is expected not to be evicted from K before the next occurrence of the item for a power-law item distribution.

Lemma 4. Let n be the number of items and let us con-sider a power-law item distribution with the exponent 2 .Any monitored item i  X  (1  X  log  X  (1 +  X  )) / 1 . 7 with count c  X   X  1 . 7 i 2 is expected not to be evicted. Table 3: Top-5 items with and without time-weighting for the two types of item distributions. With the dynamic item distribution, frequent items are different from those for the static distribution. Especially, with time-weighting, frequent items are completely different between the static and the dy-namic distributions X  X tems recently occurring many times are placed in high ranks for the dynamic dis-tribution.

Proof. Foranitem i  X  [1 ,n ], the probability that it occurs in a stream is where Z is the normalization constant. Hence, the expected number of occurrences before i occurs becomes i 2 Z .Since an occurrence of i obeys geometric ( i  X  2 /Z ), the probability that i does not occur during i 2 Z timestepsislessthan1 /e .
Assume that c i  X   X  + 1. The following guarantees in expectation that i will not be evicted by Lemma 3: 1  X  log The feasible i should satisfy
Assume c i &gt; X  + 1. The following inequality guarantees in expectation that i will not be evicted by Lemma 3:
Finally, Z  X   X  j =1 j  X  2 =  X  2 / 6  X  1 . 7, which completes the proof.
In this section, we present experimental results to show the performance of our proposed TwMinSwap with syn-thetic data streams. Especially, we want to answer the fol-lowing questions:
Q1 How many top-k time-weighted frequent items can we
Q2 How accurately can we estimate time-weighted counts
Q3 How fast is TwMinSwap ?
We consider two types of data streams generated from power-law distributions to simulate bursty item occurrences where N is the stream length and n is the number of distinct items. (c) Dynamic Distribution Figure 2: Our proposed TwMinSwap shows the best performance in both precision and recall whose val-ues are close to 1 , regardless of the distribution types and their  X  values. For distributions with small  X  in which frequencies of items are relatively even, TwSample shows low precision and recall, but as  X  gets larger, the values rapidly increase. TwFreq also shows better performance for large  X  ,butthe improved precision and recall are limited below 0 . 8 . Regardless of the hash table size, TwHCount shows the second best performance; however the perfor-mance is below that of TwMinSwap .Asexpected, SpaceSaving is degraded for the dynamic item dis-tributions while performing well on average for the static item distributions. Table 3 shows the difference between the two types of dis-tributions.
 N =10 6 ,n =10 4 ,r =0 . 8and k = 50 are fixed. Here, as  X  gets larger, item frequencies get skewed more. Note that al-though the stream length N is fixed in our experiments, the per-item time and the space complexities of our algorithms are independent on N . Figure 3: The estimated time-weighted counts by TwMinSwap the better). While TwHCount( 10 ) shows the second best performance, spaces performs poorly especially for small  X  . The accuracy of
We consider the following competitors, and in our exper-iments, all methods are implemented in Java.
The memory requirements for all the algorithms are as follows. TwMinSwap , TwFreq ,and SpaceSaving require O ( k )memoryspaces; TwSample requires O ( sk ) where s is the number of parallel sessions each of which indepen-dently samples at most k distinct items; TwHCount re-quires O ( k + rm ) where r is the number of hash functions and m is a range size of the hash functions.

We denote TwSample with s number of the independent sessions by TwSample( s ) ,and TwHCount with the hash table size w %of n ,i.e. rm = wn 100 ,by TwHCount( w ) .For TwSample ,weuse s =10and  X  =0 . 0001; for TwHCount , we use the parameters in the original paper [5], and set hash table sizes rm to 1% and 10% of n .

The overall comparison is summarized in Table 1 and Fig-ure 1. TwMinSwap outperforms the others in terms of ac-curacy and memory usage, and its speed is comparable to that of the fastest competitor TwHCount .
Figure 2 presents accuracy of discovered items in terms of precision &amp; recall. Overall, our proposed TwMinSwap outperforms other algorithms regardless of the types of item distributions and the exponent values  X  . Its precision and recallarealwaysverycloseto1. TwSample( 10 ) improves precision and recall as  X  gets larger in general. The reason why the recall of TwSample( 10 ) is low for  X  =1 . 75 is that a large amount of probability density is assigned to only fewer items as  X  gets larger, leading to a small number  X  k&lt;k of discovered items. The exact numbers are 33 and 36 for the static and the dynamic distributions, respectively. For TwMinSwap and TwHCount ,always  X  k = 50, and for TwFreq ,always  X  k  X  49.

TwFreq shows a similar pattern to TwSample( 10 ) but its improvement is less significant than TwSample( 10 ) . Count results in high precision and recall regardless of hash table sizes but they are still below TwMinSwap . Space-Figure 4: TwMinSwap is faster than TwSample( 10 ) and TwFreq in most cases of  X  . Although TwH-Count is slightly faster than TwMinSwap , its mem-ory usage is much larger than TwMinSwap as shown in Figure 1.
 Saving performs quite well for the static item distributions: both precision and recall are about 0 . 9 on average. However, since SpaceSaving does not consider a time-weighting fac-tor, for the dynamic item distributions its performance is greatly degraded as  X  gets larger.

The overall result implies that our time-weighted counting plays an important role in finding recent frequent items from data streams.
Accurately estimating time-weighted counts for discov-ered items enables to quantitatively compare them. Fig-ure 3 shows estimated time-weighted counts of the discov-ered top-k items. Notably, TwMinSwap estimates the true time-weighted counts very accurately, which is shown by the blue line (almost overlapped with green).

Since TwFreq provides an upper and a lower bounds of the true time-weighted count, we choose the mean of the two bounds. In general, as ranks of items get lower, its accuracy is generally degraded. One reason of the poor estimation of TwFreq for low rank items is that TwFreq is originally de-veloped to find frequent items having time-weighted counts above a certain threshold rather than top-k ones though it maintains at most k items. TwSample( 10 ) shows the third best performance, and estimates time-weighted counts more accurately for relatively large  X  . The performance is slightly better than TwFreq on average, but TwSample( 10 ) is not degraded for relatively low rank items.

The performance of TwHCount highly depends on the hash table size rm . The estimation of TwHCount( 1 ) re-sultsinlargererrorwhile TwHCount( 10 ) is comparable to TwMinSwap . This is notable because the estimation of
TwMinSwap is as accurate as that of TwHCount( 10 ) which keeps an additional hash table of size 0 . 1 n for accu-racy. The comparison of the error on average is shown in Figure 1.
Figure 4 shows running times of the algorithms taken to process all the items in the data streams. The overall trend is that running time decreases over increasing  X  . This is because with large  X  , the probability of a new item being already monitored increases, leading to infrequent invoking eviction processes such as DownSampling in TwSample . TwSample( 10 ) shows the slow running times due to per-forming 10 independent sampling.

Although TwFreq has the same per-item processing time as TwMinSwap in the big-O notation, TwFreq involves more computations for the eviction process than TwMin-Swap . TwFreq updates more variables and scans moni-tored items twice while TwMinSwap scans them once. As a result, the running time of TwFreq changes more dra-matically than TwMinSwap .

Since TwHCount has O ( r ) per-item processing time, it is the fastest. In fact, this fast running time is achieved by maintaining the hash table of size rm for approximate time-weighted counting, which leads to more memory spaces than TwMinSwap . Although the running time of TwHCount does not depend on m , to guarantee small error of estimated time-weightedcountsofdiscovereditems, rm should be large as shown in Figure 3. In our experiments, rm =0 . 1 n is satisfactory while rm =0 . 01 n is not.

All algorithms show no meaningful difference in running time with respect to the two types of distributions. In this section, we present discoveries from applying TwMin-Swap to several real world data streams.
Setup. The MemeTracker dataset [17] provides quotes and phrases from blogs and news media. We consider a keyword stream consisting of words in the quotes and the phrases where 571 stopwords provided in [18] are excluded. The stream covers time period between Aug 2008 and Apr 2009; the length of the stream is 1,681,760,809; we consider one minute as one time step.

Results. We run TwMinSwap with k = 300 and  X  = 0 . 9, and examine top-300 keywords for every month.
Figure 5a shows the tracking results of keywords related to the U.S. presidential election in Nov 4 2008. The val-ues for each month are normalized time-weighted counts di-vided by the sum of those of k number of discovered items. Since multiple items can occur at one time step, this normal-ization is required to eliminate effects of undesirably large time-weighted counts due to relatively large stream lengths for certain time periods. Both keywords related to the can-didates obama and mccain were mentioned actively before, and received less attentions after the election. Notably, de-spite high frequencies of both keywords, the winner obama was more frequently mentioned in blogs and media than the loser mccain . Even after the election, obama occasionally becomes hot.

Figure 5b and Figure 5c show sudden arising and quick vanishing of keywords closely related to two incidents: the Mumbai terror attack in Nov 2008, and the Gaza War begin-ning on Dec 2008. Although each incident happened in the last part of a month, TwMinSwap correctly detects related keywords as hot items in the report for that month.
Setup. The Amazon movie review dataset [22] provides user reviews with product id information where the movie title of each product id can be checked by http://www. amazon.com/dp/PRODUCT_ID . We consider the stream of the product ids. The stream covers the period from Aug 20 1997 to Sep 25 2012; the length of the stream is 7,911,684; the number of distinct product ids is 253,059.

Results. We run TwMinSwap with k = 100 and  X  = 0 . 9. Figure 6 shows the tracking result of several movies among the top-100, which is summarized as two patterns. The major pattern is doubly-active attention when a movie is released at theaters and in DVD as for Minority Report, X-Men 2, The Day After Tomorrow, Ratatouille ,and Captain America: The First Avenger . The other pattern is periodical attention: e.g. A Christmas Carol appears in every winter.
Setup. The Yelp dataset [1] provides tip data for busi-nesses by users. Here, the tips are short comments about the businesses, and each business has several associated cat-egories. We consider the stream of the business ids. The stream covers the period from Apr 16 2009 to Feb 11 2014; the length of the stream is 113 , 993; the number of distinct businessesis15 , 585.

Results. We run TwMinSwap with k =50and  X  = 0 . 9, and track the top-30 hot businesses per month. For each month, we obtain a category distribution with respect to the top-30 businesses. Figure 7 shows the result for shopping-related categories X  Shopping and Shopping Cen-ters . Around the new years, shopping activity increased. This reflects that there are several special days such as Christ-mas, New Year, and Valentine Day in winter. Figure 7: Count distribution over time for shopping related categories X  Shopping and Shopping Centers . For each month, the value is calculated with respect to top-30 businesses. In winter, there were a number of visits to shopping businesses.
In this paper we propose TwMinSwap , a fast, accurate, and space-efficient method for tracking recent frequent items from high speed data streams. We also present interesting discoveries from real world streams. The main contributions are the following: This research was supported by KT Institute of Conver-gence Technology, and by the KAIST High Risk High Re-turn Project (HRHRP).
