 In this paper, we present a medical record search system which is useful for identifying cohorts required in clinical studies. In particular, we propose a query-adaptive weight-ing method that can dynamically aggregate and score ev-idence in multiple medical reports (from different hospital departments or from different tests within the same depart-ment) of a patient. Furthermore, we explore several infor-mative features for learning our retrieval model. H.3.3 [ Information Search and Retrieval ]: Retrieval models medical record search; EMR; information retrieval; cohort identification; language models
The rich health information contained in electronic med-ical records (EMR) is useful for improving quality of care. One important application is to search EMR to identify co-horts for clinical studies, which requires retrieval systems specifically designed with medical domain knowledge.
To promote research on medical information retrieval, par-ticularly for EMR retrieval, the Text REtrieval Conference (TREC) organized a Medical Records track in 2011 and 2012 [11, 10]. The task is an ad hoc search task for patient visits based on unstructured text in EMR. One particular problem in EMR search is how to aggregate and score ev-idence that distributes across multiple documents. This is because a patient can have multiple medical reports gener-ated from several hospital departments or even from differ-ent tests within a single department.

In this paper, we propose a novel weighting method that can adaptively weight evidence with respect to different queries. We evaluate our algorithm on TREC test collections. The cross-validation results show that our weighting method is better than a fixed-weighting method across several evalu-ation metrics. Though the improvement is not statistically significant, we believe that our method has the potential to be further improved when more test collections are available.
Our work makes the following contributions: 1) we pro-pose a novel adaptive weighting method for aggregating and scoring evidence in medical records, 2) we propose and ex-plore several features that are based on semantic similarity between medical concepts for predicting the weights of our adaptive weighting method.
We use the official test collection of the TREC 2011 &amp; 2012 Medical Records Track [11, 10] for our experiments. The test collection contains 100,866 de-identified medical reports, mainly containing clinical narratives, from the Uni-versity of Pittsburgh NLP Repository.

The retrieval task 1 is an ad hoc search task for patient visits. A patient visit to the hospital usually results in mul-tiple medical reports, meaning there is a 1-to-n relationship between visits and reports.
 Table 1: Example topics of medical records track.
NIST released 81 information needs (or  X  X opics X  in TREC terminology) which were designed to require information mainly from the free-text fields (i.e., topics are not answer-able solely by the diagnostic codes). Topics are meant to reflect the types of queries that might be used to identify cohorts for comparative effectiveness research [11]. Table 1 lists several TREC topics as examples. The topic specifies the patient X  X  condition, disease, treatment, etc. Relevance judgments for the topics were also developed by TREC as-sessors based on the pooled results from TREC participants. Evidence in a visit can have different forms of distribution. Generally, there are two extreme cases: 1) Strong evidence exists in only one report of the visit; 2) Evidence spreads almost evenly across the majority of reports associated with the visit. Report-based Retrieval For the first case, we can estimate the relevance of a visit based on its most relevant report. Thus, we use reports as the initial retrieval units (i.e., building an index for reports and applying the retrieval model to each report), and then transform a report ranking into a visit ranking based on the strongest report-level evidence, which is equivalent to using the following report score merging method for ranking visits: score R ( V,Q ) = MAX(score( R V 1 ,Q ) , score( R V 2 ,Q where R V j is a report associated with visit V based on the report-to-visit mapping, score( R V j ,Q ) is the relevance score of the report with respect to query Q .
 Visit-based Retrieval Eq.1 cannot handle case 2 well. For example, if visit V 1 strong evidence in multiple reports and visit V 2 has strong evidence in only one report, V 1 and V 2 will have the same relevance score by using Eq.1. To deal with this problem, we aggregate evidence by merging reports from a single visit field by field into a single visit document V , and then per-forming retrieval from an index of visits.

Like report-based retrieval, this visit-based retrieval has its own disadvantages since it cannot handle case 1 well. For example, if visits V 1 and V 2 both have strong evidence in only one of their reports but V 1 has three times more reports than V 2 , the strong evidence in V 1 will be weakened after merging, resulting in V 1 receiving a lower relevance score than V 2 .
The comparison of the report-based and visit-based re-trievals shows that these two strategies complement each other. Thus, we propose a new query-adaptive scoring func-tion as shown below: score( V,Q )=  X  Q  X  score R ( V,Q )+(1  X   X  Q )  X  score V ( where score R ( V,Q ) and score V ( V,Q ) are the relevance scores of document V from report-based and visit-based retrievals respectively, and  X  Q is the query-adaptive coefficient for scoring merging. If we can adjust  X  Q appropriately, Eq.2 should be able to deal with all the evidence distribution cases mentioned above.
In this paper, we propose to adaptively set  X  Q with re-spect to different queries by learning the weight  X  Q based on a set of features.

In particular, we can view  X  Q as a mixing probability: the probability that the evidence clusters in only one report rather than spreads across multiple reports. Then, assuming the log-odds of that probability can be expressed as a linear combination of feature values, we may write: where  X  0 is a model intercept (or bias term), x i is the value of feature number i ,  X  i is the weight coefficient of that feature, and Q is a slack variable.

This is essentially a logistic regression model 2 . Logistic regression is fit using iteratively reweighted least squares to find the values of the  X  coefficients that are the best fit to training data. Given feature values and their  X  coeffi-cients, we can then predict the mixing probability  X  Q for new queries.
We propose 14 features that are possibly related to the ev-idence distribution in visits, and can be used to predict the weight  X  Q in Eq.2. All these features are based on charac-teristics of the medical concepts contained in the query. We detect these medical concepts using MetaMap [1], a medi-cal NLP tool developed at the National Library of Medicine (NLM) to map biomedical text to concepts in the Unified Medical Language System (UMLS) Metathesaurus. The concepts are represented by the Concept Unique Identifier (CUI) in UMLS Metathesaurus. Thus, we use Q C to repre-sent a concept query that is converted from the original text query Q and contains only CUIs. Next, we describe these 14 features in detail: 1. Length of the query Intuitively, evidence is more likely to resides across reports for long queries. Thus, we use the length of query | Q | as the feature to estimate the evidence distribution. It is defined formally as | Q | = w  X  Q cnt( w,Q ), where c ( w,Q ) is the count of term w in Q . 2. Number of concepts in the query Similarly, if a query contains more medical concepts, it is more likely to find that the evidence distributes across mul-tiple reports. We define this feature formally as | Q C | w  X  Q C cnt( w,Q C ), where cnt( w,Q C ) is the count of term w in Q C . Q C a better feature than Q because if the query contains a medical concept whose name is very long then Q might not be a good indicator of the evidence distribution. 3. Broad/narrow query concepts A text query can contain several medical concepts, for each of which the MetaMap program will return 1 to 10 candi-dates. We hypothesize that a concept with more candidates is less specific, and thus more likely to be a broad concept and appears in multiple reports. Thus, the average number of returned MetaMap candidates for concepts in a query may be a good indicator of evidence distribution. We define inal concept query length (i.e., the length before expansion), | Meta( w ) | is the number of concept candidates returned by MetaMap for term w in concept query Q C . 4. Semantic similarity among query concepts Intuitively, if Q C contains concepts that are semantically close, the associated evidence in a visit may also co-occur in a single report. However, if the concepts are semantically distant, the corresponding evidence may tend to distribute across reports. Thus, we use the semantic distance among query concepts to estimate how the evidence distributes.
We use YTEX 3 to measure semantic similarity. Given a pair of UMLS concepts, YTEX can produce knowledge based and distributional based similarity measures. The for-mer uses knowledge sources such as dictionaries, taxonomies, and semantic networks, while the latter mainly uses the dis-tribution of concepts within some domain-specific corpus [3]. We use the 11 measures listed in Table 2 as our features. Due to the limited space, we will not describe these features; Garla and Brandt provide a detailed overview [3].
For each query and each specific measure, we take the mean of the semantic similarity scores for all UMLS concept pairs in the query. This averaged semantic similarity score will be the feature score.
We use the Indri 4 retrieval system for indexing and re-trieving. In particular, we use the Porter stemmer to stem words in both text documents and queries, and use a stan-dard medical stoplist [4] for stopping words in queries only.
Our retrieval model is a linear combination of the Markov random field model (MRF) [8] and a mixture of external collection-based relevance models (MRM) [2] for query ex-pansion. Our collections for expansion are the ClueWeb09 Category B (excluding the Wikipedia pages) corpus, the 2009 Genomics Track corpus, 2012 Medical Subject Head-ings (MeSH), and the medical records corpus itself. Both report and visit-based retrievals use this system.
Because the focus of this work is to evaluate the adaptive scoring function as shown in 2, we will set the parameters of the MRF and MRM models to some default values. We use the same set of parameter values for both the report and visit-based retrievals. We set the Dirichlet smoothing pa-rameter  X  to 2500. For MRF model, we follow Metzler and Croft [8] and set the feature weights (  X  T , X  O , X  U ) to (0.8, 0.1, 0.1). For MRM model, we take take the top-weighted 10 terms from the top-ranked 50 documents for each expan-sion collection. More detail about our model is presented in recent work [13].

To evaluate our learning algorithm as described in Sec-tion 3, we first obtain the optimal coefficient  X  Q -opt for each topic Q by sweeping [0, 1] at a step size of 0.1. Then we conduct leave-one-out cross-validation (LOOCV), in each it-eration of which the system predicts  X  Q for one new topic based on  X  Q -opt  X  X  for the other 80 topics. With limited topics available for learning a relatively complex prediction model, using LOOCV can maximize the size of training data we can use in each iteration of the cross-validation, and lead to a better estimate for each feature weight.

We train our systems on MAP. This is because: 1) train-ing on MAP is most commonly used in IR to improve re-trieval performance; 2) we find that training on MAP im-proves the retrieval performance on other evaluation metrics as well while training on other evaluation measures does not Table 3: Features in the pruned set using LOOCV, sorted by their statistical significance scores. improve the overall performance. Thus, MAP will be the primary evaluation measure in this work. In fact, MAP cor-relates well with other evaluation measures as we will show in the Section 5.

To access the statistical significance of differences in the performance of two systems, we perform one-tailed paired t-test for MAP (since we train systems on MAP). We report scores for MAP, R-precision (Rprec), bpref, and precision at rank 10 (P10).
To choose a good feature combination, we use a greedy feature elimination approach in which we start with a full set of features and iteratively eliminate exactly one feature at a time that has the greatest negative impact on the retrieval performance until when further removing any feature will degrade the performance.

After the above feature set pruning step, there are 8 fea-tures left as shown in Table 3. We further study the im-portance of each feature by analyzing the prediction model trained in a randomly selected iteration of LOOCV using these 8 features. Based on the statistical significance of each feature as shown in Table 3, we can infer that: 1) All the intrinsic IC based features except IC LCH are in the pruned feature set, indicating that these types of similar-ity measures are generally more effective for predicting  X  than other measures. In fact, the intrinsic IC similarity mea-sure incorporates taxonomical evidence explicitly modeled in ontologies (such as the number of leaves/hyponyms and sub-sumers), which are not captured by the path-finding based measure. Furthermore, the intrinsic IC similarity measure avoids dependence on the availability of domain corpora, thus is considered more scalable and easily applicable than the distributional-based measure [9]. 2) R C is a good feature though it only uses similarity infor-mation about each query concept and its neighbors (rather than other query concepts) in the semantic network. 3) Neither | Q | nor Q C is in the pruned set, indicating that non-semantic-similarity-related features are generally not useful for estimating the evidence distribution. 4) RADA is a feature that might worth further exploration because both the Path-finding based and the intrinsic IC based RADA features are in the pruned set. Fixed Weighting We first evaluate the performance of Eq. 2 when  X  is fixed (i.e., not adaptive). In each iteration of the LOOCV, we obtain the best value setting for  X  on the 80 training topics by sweeping [0, 1] at a step size of 0.1, and then apply the trained  X  value to the single testing topic. We show the results in the  X  X ixed-weighting X  row of Table 4. Note that Table 4: Performance comparison. A superscript on the MAP score of system X corresponds to the initial of system Y, and indicates statistical signifi-cance (p &lt; 0.05) in the MAP difference between X and Y. The last column is the mean square error of the predicted weights.  X  X ixed-weighting X  corre-sponds to one of the top-ranked TREC systems as mentioned in Sections 4 and 5.2. this system is a better version of system udelSUM [13] which is one of the top-ranked 2012 Medical Records track systems. Optimal Weighting We also obtain the optimal  X  Q -opt for each topic separately by sweeping  X  from 0 to 1 with a step size of 0.1. Then, we use the  X  Q -opt  X  X  to compute the best retrieval performance (i.e., an upper-bound) Eq. 2 can possibly achieve, as shown in the  X  X ptimal-weighting X  row of Table 4.
 Performance Comparison Table 4 shows performance comparison of our adaptive merg-ing method with fixed-weighting, optimal-weighting, and two other baselines (report-based retrieval and visit-based retrieval). Our adaptive merging method is better than the fixed weighting method on all the evaluation metrics. The improvement is not statistically significant ( p =0 . 191), pos-sibly because 81 topics may not be enough to train a good prediction model for our adaptive weighting method. In ad-dition, the data are slightly skewed as Figure 1 showing that  X  Q -opt = 1 or 0.9 on about one third of the topics.
Figure 1: Distribution of topics against  X  Q -opt .
Due to the sensitivity of patient data, methods emerg-ing from research on information retrieval for EMR retrieval have not been well explored by academic researchers. For-tunately, the Text REtrieval Conference (TREC) organized the Medical Records track in 2011 &amp; 2012 making a set of real medical records and human judgments of relevance to search queries available to the research community.
Some interesting work have been done using the TREC collection. Limsopatham et al. [5] proposed an effective term representation to handle negated phrases in clinical text. They also incorporated dependence information of the negated terms into the term representation and achieved significant improvement over a baseline system that had no negation handling mechanism.

More recently, Limsopatham et al. [7] proposed an ef-fective representation for EMR retrieval, in which medical records and queries are represented by medical concepts that directly relate to symptom, diagnostic, test, diagnosis, and treatment. We have built on their work, combining a con-cept representation with text-based retrieval to improve on both and provide a base in which additional medical knowl-edge can be incorporated easily.

Among more relevant works, Limsopatham et al. [6] ex-plored using the type of medical records for enhancing re-trieval performance. They demonstrated that incorporating department level evidence of the medical reports in their ex-tended voting model and federated search model could im-prove the retrieval effectiveness. Their work opens another interesting direction for exploring evidence distribution and score merging. Zhu and Carterette X  X  system [12] aggregated report-level evidence and visit-level evidence, and achieved significant improvement over a strong baseline.
In this paper, we present a medical record search system which is useful for identifying cohorts required in clinical studies. In particular, we propose a query-adaptive weight-ing method that can dynamically aggregate and score evi-dence within multiple medical reports. We show by cross-validation that our weighting method is better than a fixed-weighting method across several evaluation metrics. Though the improvement is not statistically significant, we believe that our method has the potential to be further improved by incorporating other useful features or by using advanced prediction models. Furthermore, we explore several informa-tive features for weight prediction. We believe these features might be useful for improving medical IR systems.
