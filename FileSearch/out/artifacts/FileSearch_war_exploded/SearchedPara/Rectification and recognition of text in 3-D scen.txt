 Abstract. Real-world text on street signs, nameplates, etc. often lies in an oblique plane and hence cannot be recognized by traditional OCR systems due to perspec-tive distortion. Furthermore, such text often comprises only one or two lines, preventing the use of existing per-spective rectification methods that were primarily de-signed for images of document pages. We propose an ap-proach that reliably rectifies and subsequently recognizes individual lines of text. Our system, which includes novel algorithms for extraction of text from real-world scenery, perspective rectification, and binarization, has been rig-orously tested on still imagery as well as on MPEG-2 video clips in real time.
 Keywords: Videotext recognition  X  Video OCR  X  Scene text  X  Multimedia content analysis  X  Perspective rectification 1 Introduction The volume of collected multimedia data is expanding at a tremendous rate. A capability to automatically iden-tify the contents of real-world video and still imagery would enable multimedia data to be indexed in a conve-nient and meaningful way for later reference and would enable actions such as automatic notification and dis-semination to be triggered in real time by the contents of streaming video. Existing methods of realizing this video and image content analysis capability rely on the automated recognition of objects and scenes directly in the imagery. These methods have had limited success because (1) scenes may be arbitrarily complex and may contain almost anything and (2) the appearance of indi-vidual objects may vary greatly with lighting, point of view, etc. The recognition of text is easier than the recog-nition of objects in an arbitrarily complex scene because text was designed to be readable and has a regular form that humans can easily interpret. Therefore, a method for the reliable and accurate detection and recognition of text contained in a scene or superimposed on an image will significantly advance the development of automated video and image content analysis.
 pears on objects such as street signs, name plates, and billboards that are part of a scene captured in video or still images. Figure 1a shows an image of a caf  X  e scene captured with a video camera in which the name of the caf  X  e is viewed from an oblique angle, a text configuration that is quite common when the main subject of a scene is not the text itself. Such incidental text could be quite important: for example, it may be the only clue to the lo-cation of the captured imagery. However, state-of-the-art optical character recognition (OCR) systems are unable to detect and/or recognize such text, due in part to the effects of perspective distortion and rotation. Therefore, such images must be rectified before the text can be suc-cessfully recognized. Furthermore, existing text image rectification methods were designed for images of text pages and as such may require that the text be justi-fied, that it include many lines, and that the line spac-ing be uniform. These assumptions are clearly not valid for many instances of nondocument indoor and outdoor scene text such as text on street signs, billboards, name tags, and license plates.
 to detecting the orientation of the plane on which a sin-gle line of text is printed by taking advantage of the estimated 3-D scene geometry. The region of the image that contains the line of text is then rectified, i.e., trans-formed to a normalized coordinate system before OCR is performed. To this end, we make a weak perspective assumption in the Y direction and justify its applicabil-ity to real-world scene imagery. Our algorithm has been tested on specially prepared posters as well as text from real scenes. Our results show a significant improvement in OCR results when the text is rectified by our method, with azimuth angles up to 70  X  , elevation angles up to 45  X  , and in-plane rotation angles up to 30  X  . 2 Background 2.1 Perspective distortion of scene text To address the problem of recognizing text that lies on a planar surface in 3-D space, we note that the orien-tation angle of such text relative to the camera can be modeled in terms of three angles, as shown in Fig. 1b. These angles are  X   X  , the rotation in the plane perpendicular to the cam- X   X  and  X  , the horizontal (azimuth) and vertical (eleva-The three angles represent the extent of rotation that the text plane must undergo relative to the camera in each of its three axes to yield a frontal, horizontal view of the plane in the camera X  X  field of view. When  X  and  X  are zero and  X  is nonzero, the apparent width of the text is reduced, resulting in a change in aspect ratio and a loss of horizontal resolution. Similarly, when  X  and  X  are zero and  X  is nonzero, the text appears to be squashed ver-tically. The severity of perspective distortion is propor-tional to D/Z , where D is the extent of the text parallel to the optical axis (its  X  X epth X ) and Z is the distance from the text to the camera. When the text is not cen-tered at the optical axis or both  X  and  X  are nonzero, the text appears to be rotated in the image plane (Fig. 2a). If the text were rotated to remove this apparent angle by a text recognition process that mistakenly assumed the text is frontoparallel, the characters would be sheared (Fig. 2b). When both  X  and  X  are nonzero and perspec-tive distortion is significant, the shearing angle varies from left to right within the text region. OCR engines perform poorly if the shearing causes characters to touch or to be severely kerned (overlapped vertically). relative to the image plane, several types of distortions can be introduced that make it difficult to read the text. In the most general case, the distortion is described as a projective transformation (or homography) between the plane containing the text and the image plane. We can correct this distortion by applying the appropriate  X  X or-rective X  projective transformation to the image. That is, we can rotate and stretch the original image to create a normalized image, which we call a rectified image, in which the projective distortion has been removed. text line, L j , occurring in an obliquely viewed image, O , is a projective transformation, T ij , of the text plane. This transformation is described by where H isa3  X  3 matrix that maps the homogeneous coordinates m = fied coordinates m = where k is an arbitrary scaling constant.
 into three components: a 2-D Euclidian transformation E , an affine transformation A , and a projective trans-formation P : where
E = correspond to a Euclidean 2-D transformation (transla-tions along two axes t x , t y , a rotation  X  , and an isotropic scale factor s ); two correspond to an affine transforma-tion (a shear [ a ] and a nonisotropic scaling of one axis relative to the other [ b ]); and the remaining 2 degrees of freedom l x , l y represent a perspective foreshortening along the two axes.
 rameters produce changes that are harder to handle than others. The two translations are not a problem because they simply produce an image shift that is naturally han-dled by OCR systems. Similarly, the two scale factors are not a problem because the OCR systems typically include mechanisms to work at multiple scales. The Eu-clidean rotation is important but is easily computed from a line of text. Therefore, three critical parameters pro-duce distortions that are difficult for OCR systems to handle: the two perspective foreshortening parameters and the shearing. 2.2 Degrees of freedom for rectification of scene text Estimates of the plane parameters are computed from the orientations of the lines of text in the image and the borders of planar patch, if they are visible. To remove a projective distortion, we need to compute the three critical degrees of freedom associated with the plane on which the text is written. In general, we can do this by identifying three geometric constraints associated with the plane. For example, we can compute necessary pa-rameters, given two orthogonal pairs of parallel lines, such as the borders of a rectangular sign or two parallel lines of text and a set of vertical strokes within the text. The three constraints that can be derived from these sets of lines are two vanishing points (one from each set of parallel lines) and an orthogonality constraint between the sets of lines.
 cult to detect. In such cases, one can estimate the param-eters by making assumptions about the camera-to-plane imaging geometry that are often true. For example, peo-ple normally take pictures so that the horizon is hori-zontal in the image. In other words, they seldom rotate the camera about its principal axis. In addition, they often keep the axis of the camera relatively horizontal: that is, they do not tilt the camera up or down very much. When these two assumptions apply and the text lies on a vertical plane such as a wall of a building or a billboard, the projective distortion is only along the x -axis of the image. In this case, vertical lines in the real scene remain vertical in the image. The perspective fore-shortening in that direction can be computed from one constraint, such as a pair of horizontal parallel lines. tive effects are significantly smaller than the effects caused by out-of-plane rotations. This is the case if the depth variation in the text is small compared with the distance from the camera to the plane. In this case, the perspective distortion is reduced to an affine shear and the projection is described as a weak perspective projec-tion.
 main uncorrected after different sets of linear features are found and different assumptions are made about the plane-to-camera geometry. 3 Related work Most previous text recognition efforts in video and still imagery [7 X 10, 13, 17, 19, 20, 22 X 24] have assumed that the text lies in a plane that is oriented roughly per-pendicular to the optical axis of the camera. This as-sumption is often valid for scanned document images and imagery containing overlaid text captions but is not generally true for scene text in video and still images. Therefore, most previous video text recognition systems cannot handle scene text with perspective distortion. On the other hand, previous work related to camera-based document imaging handles only rotation-induced distor-tion [6, 15, 16, 21]. They assume that the text lines are rotated but still parallel and will not work when per-spective skew is present. The only other works known to the authors that address perspective skew and its rec-tification for text are by Clark and coauthors and by Dance [1, 2, 4, 5, 14, 18].
 to lie within a quadrilateral whose edges needed to be found in the image; this quadrilateral is then trans-formed to a rectangle via horizontal and vertical vanish-ing points. However, the document boundary quadrilat-eral may not always be visible in its entirety. Secondly, for many common objects that bear text in real-world scenes, such as street signs and shop facades, there may not be enough justified text lines present for the reliable extraction of the document boundary quadrilateral. Sub-sequent publications by these authors have described a method for computing horizontal and vertical vanishing points via text layout instead of the document bound-ary rectangle. They continue to assume that the text is justified (full, left, centered, or right) and that there are enough text lines in the scene to reliably estimate the justification. In order to compute the vertical vanishing point, they assume uniform spacing between text lines. As stated before, these assumptions do not hold true for many nondocument real-world scene text.
 tion as a generalization of the skew estimation problem, for which a body of previous work exists in the docu-ment analysis world. Based on a twice-iterated projec-tion profile computation, this method estimates angles of lines that are parallel and perpendicular to text lines, thereby estimating the parameters of perspective distor-tion. Similarly, the work by Pilu [18] aims to extract visual cues from the image that represent vertical and horizontal features in the document plane, e.g., by us-ing the columns of a multicolumn document page. Both of these approaches cannot be reliably applied to cases where the text is too short and comprises too few lines to enable the estimation of text justification and hence the vertical vanishing points. Both of these prior algo-rithms operate under the constraints represented as the bold italic text in Table 1. 1 putes motion parameters of the camera and its relative position vis-` a-vis the scene by matching multiple frames of video. These parameters are then used to compen-sate for the perspective skew of planar objects such as signs and billboards bearing text. However, their method assumes that calibration parameters of the camera are known beforehand. Also, it is not applicable to still im-agery. 4 Approach Previous work related to perspective rectification of doc-ument text focuses on text comprising multiple and jus-tified lines of text and as such cannot handle many com-mon real-world instances of in-scene text such as street signs, billboards, license plates, name plates, and track-ing numbers. Therefore, in this work we focus on the rectification of individual lines of text. Given the re-lationships described in Table 1, our general strategy is to identify as many properties of a region of text as possible and then compute a corrective transforma-tion, making as few assumptions as possible. First, our feature-extraction analysis is carried out independently from each individual line of text. Basic features extracted by our algorithm include the top and baselines and the dominant vertical direction of the character strokes. De-pending upon our success in finding these features, we can either make assumptions to substitute for missing constraints (and then compute a transformation that corrects for a full perspective projection) or compute a transformation that does not completely remove all de-grees of freedom. Next, the rectification parameters for each text line are computed from its features. Each text line is then rectified independently and sent to an OCR engine. These salient steps in our approach are detailed below. 1. Scene text detection . Our text detection and location process, somewhat similarly to those described by Smith and Kanade [20] and by Wu et al. [22], detects vertically oriented edge transitions and connected components of similar intensity in a grayscale image, and links those that are compatible in size and relative position to form lines of text. 2 acters in a sequence of scene text are of approximately the same intensity and that there is sufficient resolution to distinguish individual characters (the same precon-dition for the OCR engine to succeed). The grayscale image is first thresholded at N different thresholds (we used N = 8). Connectivity analysis is performed on each of the N thresholded images. Based on the extent of ver-tical overlap between neighboring blobs, collinear, char-acterlike blobs (as determined by their sizes relative to their spacing) are linked together at each of the N levels to form candidate text lines. In any of the thresholded images, characters within the same word may appear merged if the background varies or the character inten-sity is on the borderline of a threshold. Therefore, before linking, each blob may be replaced by multiple smaller blobs, based on the position overlap of blobs at an ad-jacent threshold level. Candidate lines of text at larger rotation angles often appear as broken pieces of text, since the linking is based on vertical overlap. Therefore, extensions of candidate text lines are investigated for po-tential merges with pieces of text along the same lines. tect text lines of both polarities (dark characters on light background and vice versa). The distances between ad-jacent blobs are typically less than those between ver-tical edges. This allows us to impose tighter tolerances on the linking to prevent the false linking of nontext to the ends of text regions. Also, this text-detection method was designed to detect both in-scene text and superim-posed graphical text (such as captions, speaker name, etc., especially in broadcast news imagery) and is there-fore capable of handling plain backgrounds (such as the backgrounds for street signs) as well as cluttered back-grounds (such as the background imagery of advertise-ments and superimposed text). The detection parame-ters were tuned to result in few false positives, so that the end-user content-based indexing and query system is provided with only those regions that are highly likely to be true text lines. Potential text blobs of low height are often filtered out since in any case the OCR algorithms cannot reliably detect such small text (less than eight pixels tall). However, a detailed discussion of the text-detection algorithm is beyond the scope of this paper. of detected text. Figure 3 shows a test image of a poster containing text that was captured at an azimuth angle of 70  X  ; the rectangles that have been fitted to each de-tected text line are shown in overlay. Some of the rectan-gles do not look to the eye like true rectangles because of the perspective view of the image contents. Computing the best-fitting rectangle for each text line is an expedi-ent way to approximate the location and extent of the text, but the top and bottom of the text are not accu-rately computed when significant perspective distortion is present. 2. Extraction of horizontal features . Our process esti-mates a top line and baseline for each line of text as follows. To compute the top line, we first mark the top-most foreground pixel in each column of the text line, e.g., going from left to right along the line of text. Next, going from top to bottom of the line of text, we count the total number of these marked top points along each horizontal scan line. The maximum such count, which is essentially the peak of the horizontal projection of the top points, is recorded. The line of text is then rotated in increments of 0.5  X  and the peak of the horizontal projec-tion is recorded for each angle. Over this range of angles the locus of points lying on the largest such peak are se-lected as the top line. We repeat this procedure for the baseline by marking the bottommost foreground pixel instead of the topmost foreground pixel. When the text consists of predominantly lowercase characters, the  X  X op line X  actually corresponds to the  X  X idline X  of the text that touches the tops of lowercase characters, excluding their ascenders. The best estimate of the top line should correspond to the rotation angle that yields the steep-est slope on the top side of the horizontal projection; the best estimate of the baseline is similarly computed. Figure 4a shows an example of this procedure.
 horizontal vanishing point V H can be computed as the in-tersection of these two lines. These coordinates are later used for text rectification. Let the homogeneous coordi-nates of the horizontal vanishing lines be xy 1 3. Extraction of vertical features . In addition to comput-ing two horizontally oriented lines, we would like to find and measure the angles of two vertically oriented lines to use in the computation of the rectification parame-ters. Unfortunately, an individual line of text does not have much vertical extent, and it is difficult to determine which parts of the text could be used as vertical cues. However, the height of the text is not usually a signifi-cant fraction of the depth of the text in 3-D space, so that the perspective foreshortening in the y dimension is ob-served to be relatively small. This effectively amounts to a weak-perspective assumption in the y direction, placing the vertical vanishing point at infinity. Therefore, in the absence of any other reliable vertical cues, we compute the dominant vertical direction (as characterized by the shear angle) of the text by first rotating the text line so that the baseline is now horizontal and then computing a series of vertical projections over the vertical edge tran-sitions after shearing the text line in 2  X  increments. The best estimate of the dominant vertical direction should correspond to the angle at which the sum of squares of the vertical projections is a maximum (on the assump-tion that the projection of true vertical strokes is greatest when they are sheared to a vertical position). Figure 4b shows an example of a shear angle computation. rection, the true vertical lines are assumed to be parallel in the observed image. The shear angle computed leads us to the slope  X  of these parallel lines. The vertical vanishing point, V v , is thus located at infinity with the homogeneous coordinates  X  10 4. Rectification . Given that the two translation param-eters ( t x , t y ) and the two scale parameters ( s , b ) are not relevant for the recognition process, the transfor-mation matrix H from Eq. 1 has 4 degrees of freedom. These 4 degrees of freedom can be used to arbitrarily map the points with homogeneous coordinates 001 and 111 and 111 horizontal vanishing point xy 1 maps to 100  X  10 T from step 3 maps to 010 T . Using Eq. 1, we obtain eight proportionality equations from the above four pairs; these equations can be solved to obtain a rec-tification transformation H . A closed-form solution is shown in Eq. 3.
 formation matrix to each pixel in the observed line of text, using bicubic interpolation whenever necessary to avoid any undefined pixels in the rectified image. assumption in the vertical ( y ) direction, is not perfect because when perspective distortion is significant, the foreshortening in y causes the angles of vertical strokes to vary as a function of horizontal position. In addition, the estimate of the dominant shear angle assumes that a significant fraction of the characters contain vertical strokes. Figure 5 shows the refined bounding boxes based on the top and baselines and on the dominant vertical direction. Figure 6 shows the warped text lines (1) after the rotation according to initial bounding quadrilateral, (2) after rotation according to the refined base line, and (3) after the lines are fully rectified according to Eq. 3. 5. OCR . The image N i , which contains all of the recti-fied lines from image O i , is then sent through the OCR process via the Scansoft, Inc. DevKit2000 OCR pack-age. Figure 7 shows, for the 70  X  azimuth test image, the recognition results overlaid on the normalized image. 5 Results To measure the improvement in recognition performance as a result of our rectification process, we ran the pro-cess on a set of test images of a poster containing text viewed at various angles. The evaluation was performed semiautomatically by the process shown in Fig. 8. We generated a ground truth data set (including the bound-ing boxes as well as the identities of the characters) by running the text-detection and OCR process on a refer-ence image R and manually correcting any recognition errors. For our reference image we used a frontoparallel view of the poster. In each of the test images O i , the positions of the four corners of the poster were automat-ically detected and used to compute a transformation E i that maps a pixel position in O i into a corresponding position in R . By applying T  X  1 ij and then E i , we mapped the OCR results for line L j from normalized coordinate space into the coordinate space of R . We expected that the lines and characters of text in correctly rectified im-ages would coincide with those in a true frontoparallel image. An automated process compares the recognized results to truth data on a line-by-line and character-by-character basis. Figure 9 shows the reference image over-laid with the ground truth data. Figure 10 shows the OCR results of Fig. 7 after the inverse mapping back into the coordinate system of reference image R . azimuth angle varies in increments of 10  X  . Figure 12 shows the character-recognition results as a function of azimuth angle for the various versions of rectification.  X  X ercentage Correct X  means the number of correctly rec-ognized characters divided by the number of characters in the referenced ground truth data set. As expected, the performance drops as the azimuth angle increases. At the most oblique angles, when the character stroke width and/or the spacing between characters becomes one pixel or less in the original image, the resolution available for the interpolation process during rectifica-tion is not sufficient to adequately preserve the character features. The graph shows that each of the three pro-cessing steps contributes to the increase in performance. These improvements are greatest at the more oblique angles.
 digital video, we designed our method to be compu-tationally efficient; it runs at 10 frames per second in 640  X  480 MPEG-2 compressed video frames on a stan-dard 1.7-GHz Pentium IV processor. Figure 13 shows two text samples from real-world imagery that have been detected and rectified by our algorithm, including two blocks of falsely detected text. Figure 14 shows a set of images of a real signboard, captured with varying az-imuth angles and analogous to the test images presented in Fig. 11. These images are viewed at a significant ele-vation angle compared with those in Fig. 11. Figure 15 shows the effects of the application of our algorithm on OCR accuracy. When these effects are compared with the results in Fig. 12, it can be seen that the accuracy gain via rectification for 30  X  ,40  X  , and 50  X  azimuth is larger because of the additional distortion in these im-ages due to the higher elevation angle. For instance, the gain in accuracy due to rectification at 40  X  azimuth angle is approximately 60% for Test Set 2 (Fig. 15), compared with about 30% for Test Set 1 (Fig. 12).
 based on vertical edge transitions and the extent of ver-tical overlap of neighboring blobs. Therefore, as the text is subjected to in-plane rotation, our analysis can be ex-pected to degrade as the rotation angles approach 45  X  . To study the effect of in-plane rotation on the recognition accuracy, we digitally rotated two images from Fig. 11  X  those corresponding to azimuth angles of 40  X  and 60  X   X  in steps of 5  X  starting from 0  X  up to 30  X  . An example set is shown in Fig. 16 for azimuth angle of 40  X  . Figure 17 shows the change in recognition accuracy as a function of the rotation angle. The accuracy is seen to degrade with increasing rotation, although not always. The degrada-tion was sharper for higher azimuth. Figure 18 compares OCR results with and without the application of our rec-tification method for the image set with azimuth of 40  X  , clearly showing the benefit of rectification. 6 Summary Scene text is an important feature used by automated systems to analyze the content of real-world scenes in still and video imagery. Unlike caption text superim-posed on video, scene text may not be frontoparallel relative to the camera, and resultant perspective distor-tions may render the text unreadable by current OCR systems. It is therefore desirable that OCR systems au-tomatically identify and rectify perspective distortions of scene text.
 ination of distortion components of scene text captured in still or video imagery and described some constraints that can be applied to correct these distortions. Our rec-tification algorithm works on single images where text may consist of a single short line (such as text on road signs and billboards.) Our experiments compared OCR accuracy before and after the application of the algo-rithm. The system was tested with images captured at a range of azimuth and elevation angles. Results show that OCR performance improves significantly once the text is rectified by our method. Given more information, such as multiple text lines and/or multiple video frames, our method can be augmented to incorporate additional constraints, resulting in better rectification and fewer as-sumptions. ability to use the text layout when multiple lines of text are present, and its susceptibility to text detection and blob linking errors when the text is severely rotated in plane or out of plane. When the text is severely rotated and captured digitally at VGA or SVGA resolutions, the characters tend to merge together and cannot be read even when rectified perfectly. The complexity of the background plays an important role in robust de-tection of text lines and subsequent feature extraction. Our method is also applicable to complex backgrounds such as the imagery behind superimposed caption text in video. Most in-scene text, such as street signs, billboards, name plates, etc., is printed on plain backgrounds for better readability and therefore is better detected by our system.
 References
