 Crowd selection is essential to crowd sourcing applications, since choosing the right workers with particular expertise to carry out crowdsourced tasks is extremely important. The central problem is simple but tricky: given a crowdsourced task, who are the most knowledgable users to ask? In this demo, we show our framework that tackles the problem of crowdsourced task assignment on Twitter according to the social activities of its users. Since user profiles on Twitter do not reveal user interests and skills, we transfer the knowledge from categorized Yahoo ! Answers datasets for learning user expertise. Then, we select the right crowd for certain tasks based on user expertise. We study the effectiveness of our system using extensive user evaluation. We further engage the attendees to participate a game called X  X hom to Ask on Twitter X . This helps understand our ideas in an interactive manner. Our crowd selection can be accessed by the follow-ing url http://webproject2.cse.ust.hk:8034/tcrowd/ . H.2.8 [ Database Management ]: Database Applications Algorithms, Experiments Crowdsourcing, Microblogs
In recent years, the studies of crowdsourcing techniques [6] have attracted a lot of attention due to their effectiveness in real-life applications, such as image tagging and natural language processing. Earlier approaches usually randomly select workers for certain tasks on well designed platform-4 discusses the demonstration plan and we conclude the pa-perinSection5.
We first present a general process of our system. Then, we introduce the transfer learning based algorithm.
The general process of our system is illustrated in Fig-ure 1, which can be divided into two phases: training bayesian models and processing crowdsourced tasks.
 A bayesian model is trained offline. First, we train a Na  X   X ve Bayesmodelbasedoncategorizedtasksfrom Yahoo!Answer website [1], denoted by TM . Then, we train another Na  X   X ve Bayes model based on the categorized answers from Ya-hoo!Answer website [1], denoted by AM .Weutilize TM to categorize the crowdsourced tasks. However, we find that the trained AM model cannot be directly applied for user ex-pertise inference, since the domain of tasks in Yahoo!Answer is different from the domain of tweets in Twitter. Therefore, we transfer the trained AM model to build a new bayesian model AM X  for user expertise inference. After obtaining the user expertise, we store the data into the databases. We will have more technical details of building the new model in the next section.

The crowdsourced tasks are processed online. Consider a crowdsourced task processing in our system. First, the user u inputs a crowdsourced task t .The TM model categorizes the input task. We consider all the  X  X ollowings X  and  X  X ol-lowers X  of user u as the candidate crowd for the task. The system queries the expertise level of the candidate crowd from the databases. Then, the system ranks these workers in the candidate crowd based on the expertise level and rec-ommends it to the user u .Finally,theuser u tweets the task to the recommended workers. The system keeps collecting the answers from twitter workers.
We transfer the trained AM model to a new AM X  mod-el based on the technique in [4]. Our algorithm first esti-mates the initial parameters under the categorized answers D c from Yahoo!Answer , and then uses an EM algorithm to revise the model AM for the tweets of users D u ,whichare uncategorized.
 Figure 2: An Example of Estimated User Expertise
The tweets are categorized after the algorithm converges in log-likehood. An example of the categorized tweets for the users in our databases is illustrated in Figure 2. The level of user expertise on the task category c equals to the number of produced tweets which is associated with category c ,given by where d u is the tweet produced by user u and f ( d u )isthe estimated category by using model AM X  . I ( f ( d u )= c )isan indicate function. When the user inputs the crowdsourced task with category c , the system ranks the workers in the candidate crowd based on their level of expertise level on that category.
We now explain the crowdsourced task processing of our system. The demonstration video of the system can be found in http://www.youtube.com/watch?v=PeMaw-gifpU .

To enable the crowdsourced task processing, the system asks for the authority of users X  account so that it is able to On the other hand, to protect the privacy of the account, our system does not the system generates the answer sheet, which is sent to our databases. Finally, our databases aggregate the collected sheets under the posted task.

The system also maintains the historical record of all the tasks and received answers. The users are able to search the posted tasks and its related answers from the system, illustrated in Figure 5.
In this demonstration, we plan to engage the attendees to participate in Crowd Selection by starting an interesting game called  X  X hom to Ask on Twitter X . We utilize the Yahoo!Answer datasets as the initial data source.
We build two Na  X   X ve Bayes models TM and AM based on the categorized Yahoo!Answer dataset. We also encourage the audience to login into our system with their Twitter ac-count. Then, the system transfers the AM model to the AM X  model for user expertise inference. In the meant time, we ex-plain how the system works. Then, we invite the audience to issue the crowdsourced task (i.e. technical questions raised during the conference presentation) to our system. The sys-tem shows the right crowd for asking these questions.
The system aims to match a specific task with the workers who have expertise to address it. Currently, we set the can-didate crowd of a Twitter user to be his/her followers and followings. The reason is that the users far away have much less incentive to address the crowdsourced task. Later, we will extend the candidate crowd to be the users on Twitter by incorporating an effective incentive mechanism.
In this demo, we explore a new issue of  X  X hom to Ask on Twitter X . Different from the general crowdsourced task processing, we focus on a specific task with needed expertise. We build a framework of crowd selection on Twitter. The system returns the workers ranked by their expertise level on the crowdsourced task category. Based on the algorithm, we devise a demonstration program on Yahoo!Answer data and engage the attendees to participate our system. We devise an interactive game  X  X hom to Ask on Twitter  X  to demonstrate the effectiveness of our system.

ACKNOWLEDGEMENTS : We thank the help from the UROP project students Xinyu Wang, Weikeng Chen and
