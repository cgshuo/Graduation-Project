 Clustering time series is a problem that has applications in a wide variety of fields, and has recently attracted a large amount of research. In this paper we focus on clustering data derived from Autoregressive Moving Average (ARMA) models using k -means and k -medoids algorithms with the Euclidean distance between estimated model parameters. We justify our choice of clustering technique and distance metric by reproducing results obtained in related research. Our research aim is to assess the affects of discretising data into binary sequences of above and below the median, a process known as clipping, on the clustering of time series. It is known that the fitted AR parameters of clipped data tend asymptotically to the parameters for unclipped data. We exploit this result to demonstrate that for long series the clustering accuracy when using clipped data from the class of ARMA models is not significantly different to that achieved with unclipped data. Next we show that if the data contains outliers then using clipped data produces significantly better clusterings. We then demonstrate that using clipped series requires much less memory and operations such as distance calculations can be much faster. Finally, we demonstrate these advantages on three real world data sets.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining General Terms: algorithms, theory, experimentation. Keywords: time series, clustering, ARMA.
The mining of time series in general and the clustering of time series in particular has attracted the interest of researchers from a wide range of fields, particularly from statistics [28], signal processing [10] and data mining [26]. An excellent survey of current research in the field has re-cently been published [24]. This growth in interest in time series clustering has resulted in the development of a wide variety of techniques designed to detect common underlying structural similarities in time dependent data. These tech-niques have been applied to data arising from many areas, for example: web mining [8]; finance and economics [30]; medicine [13]; meteorology [5]; speech recognition [10]; gene expression analysis [4] and robotics [33]. Because many of these applications generate huge data sets, mining with the raw data may be prohibitively costly in terms of time and memory. For example, a current project to analyse the tem-poral evolution of glomerular activity patterns in the an-tennal lobe of honeybees involves mining gigabytes of neu-ral measurements [11]. Even simple mining algorithms can take days to run, much longer if the data cannot be stored in main memory. Much larger data sets, up to petabytes in size, can arise in meteorology projects such as the HiGEM project [17]. In cases like these some form of dimensionality reduction or discretisation is essential.

This paper examines the effect of discretising time series on clustering accuracy. We transform real valued time series into binary series through the process of clipping . Clipping, or hard limiting, a time series is transforming a real valued time series Y into a binary series C where 1 represents above the population mean and 0 below, i.e. if  X  is the population mean of series Y and t denotes time then
Clipped time series retain much of the underlying struc-ture that characterises the real valued series. We demon-strate that clustering with clipped data provides the follow-ing benefits:
Our aim is to demonstrate these benefits over classes of al-ternative underlying models and from real world data. In [3] we assessed the effect of clipping on data originating from polynomial models on clusters found with the k -means algo-rithm. We found that, for data originating from a class of a mixture of two linear models, clipping does not significantly reduce the quality of the clusters when there are no outliers and does significantly improve the clustering accuracy in the presence of outliers, even when the probability of an outlier is small. In this paper we cluster using k -means, k -medoids and hierarchical methods with data from classes of Autore-gressive Moving Average (ARMA) models and real world data.
 An ARMA( p,q ) model has the form where  X  ( t ) are normally distributed random variables with variance  X  2 . ARMA parameters can be estimated from data using the autocorrelations (see Section 3.2). It is shown in [21] that there is a relationship between the autocorre-lation function of clipped and unclipped series and hence ARMA models fitted from clipped data tend asymptotically to models from unclipped data. This result, discussed in more detail in Section 3, forms the theoretical basis for us-ing clipped data. The experimental evidence of the benefits of clipping with clustering is presented in Sections 4 to 6. We describe the space and time complexity improvements that clipping provides in Section 4. In Section 5, we clus-ter data from the simplest form of ARMA model, AR(1), with fixed parameters taken from related research [20, 36]. We show that our method of clustering unclipped data is at least as good as the alternative methods described in [20, 36]. This demonstrates that our choice of clustering algo-rithms and parameterisation is not the source of the vari-ability observed in accuracy between clipped and unclipped data. We then demonstrate the benefits of clipping on mod-els randomly sampled from the class of all stationary AR(1) models and show that, if the series are long enough, clip-ping does not decrease the accuracy of clustering and is bet-ter with outliers. In Section 6 we cluster data from fixed ARMA models used in [27, 28]. Results obtained are com-parable to those published. We show that clipping produces more accurate clusters when there are outliers in the data. We cluster on models randomly sampled from the class of models given by all possible ARMA(1,0) to ARMA(2,2). We show that the benefits of clipping still exist in harder, multi-class, multi-model situations. In Section 7 we demonstrate the advantages of using clipped series on three real world data sets: ECG data and population data from [20] and reality check data set available from [23]. Finally, we sum-marise our findings and outline the future direction of this research in Section 8.
The approaches to clustering time series can be categorised as model based or model free. Model based approaches (also called generative approaches) assume some form of the un-derlying generating process, estimate the model from each data then cluster based on similarity between model param-eters. The most commonly assumed model forms are: poly-nomial mixture models (e.g. [12, 4, 3]); ARMA (e.g. [32, 27, 35, 20, 36]); Markov Chain and Hidden Markov models (MC and HMM) (e.g. [33, 8, 29, 34, 37]).

Reviews of methods used to cluster time series can be found in [3, 24]. Model free approaches involve the specifica-tion of a specific distance measure and/or a transformation of the data. Measures based on common subsequences (used in [9]), shape parameters [25] and correlation (e.g. [30]) have been proposed. Transformations used in conjunction with time series data mining include fast fourier transforms; dy-namic time warping; wavelet transforms; and piecewise con-stant approximation. A large number of transformations are evaluated in [24].
 One method that has similarities to our approach is the Symbolic Aggregate Approximation (SAX) transformation described in [26]. SAX is a transformation from real valued series into symbolic series through a two stage process of dimension reduction and discretisation. Clipping could be considered a specific type of SAX transformation with two classes and no dimension reduction. SAX may be of more general use in time series mining than clipping. However, the properties of clustering with clipped series are worthy of study because, firstly, binary data can be more compactly represented and efficiently manipulated and secondly, it is possible to assess their theoretical behaviour.

Other approaches use a combination of transformation and model fitting. For example, Kalpakis et al [20] fit ARMA models to the series then transform the fitted parameters into the associated Cepstrum. Our approach, described in detail in Section 3, is to transform the data then fit an ARMA model.
To test the effects of clipping we need to first specify: 1. a class of model from which the data in each cluster 2. the fitting procedure to estimate the true model (Sec-3. the clustering method employed (Section 3.3); 4. the experimental procedure and method of evaluating
We assume that the series in each true underlying cluster is generated by an ARMA model. The model for cluster i is a random variable. A model without outliers is assumed to be of the form
X i ( t )=  X  1  X  X i ( t  X  1) + ... X  p  X  X i ( t  X  p )+ where  X  i are N (0 , X  )and  X  , p , q ,  X  j and  X  j are constant for a model. One of the objectives of this paper is to demon-strate the robustness in the presence of outliers of using a clipped time series rather than the the raw data for cluster-ing. Hence, we add a further term to Equation 2 to model the effect of outliers. A time series is assumed to be gener-ated by a sequence of observations from the model where r = s  X  a  X  b. s is a constant, a  X  X  0 , 1 } and b  X  X  X  are observations of independent random variables, A and B ,where A has density f ( a )= p a (1  X  p ) 1  X  a and B has density f ( b )= 1 2 .r is a random shock effect that can occur with probability p , and if it occurs it has the effect of either adding or subtracting a constant s to the data (with equal probability). s is set to 10 for the experiments described in this paper, a value large enough to be very unusual but not so large as to dominate the model. Some example ARMA time series, with and without outliers, are shown in Figure 1. Figure 1: Example ARMA time series of length 500. The series on the right are the same as those on the left except that outliers have been added with p =0 . 01 .
 A time series y is a sequence of observations from a model Y , y = &lt;y (1) ,...y ( t ) ,...y ( n ) &gt;. A binary data series is generated by clipping above and below the sample me-dian. If  X  y is the sample median of the data series y ( t 1 ,...,n , then the associated discretised time series, c ,isde-fined as
If several series are generated by the same model, we iden-tify the generating process by a subscript i and the series index with a superscript j .Thusseries y j i would be assumed to be the j th series from model Y i .

Adataset D is parameterised as follows: there are k mod-els of the form given in Equation 3, each of which generates l time series, thus each of the l  X  k time series is of length n and is sampled at thesamepoints t =1 , 2 ,...,n ;  X  defines the variability of static noise, s the level of random shocks and p the proba-bility of shocks. Given a data set D , the problem is then to split the data into distinct sets or clusters, D 1 ,D 2 ,...,D such that,
To form these clusters, we fit an ARMA model to each series then cluster on the basis of the similarity of the fit-ted parameters. The method of fitting and clustering is described in Section 3.2.
Any invertible ARMA model can be represented as an infinite AR model, We estimate the AR model for each series by using a stan-dard three stage procedure. Firstly the autocorrelations  X  , X  2 ,... up to a fixed maximum lag are calculated (set to 50 in experiments reported). Secondly the partial fitted models and hence the partial autocorrelations are found by solving the Durbin-Levinson recursion. These use the fact that the correlation matrix is Toeplitz to create a computationally ef-ficient iteration. If the i th autoregressive term of the model fit to j parameters is denoted  X  i,j and  X  1 , 1 =  X  1 then the Durbin-Levinson recursion is  X  Finally the model that minimizes the Akaike information criteria (AIC) is chosen [1]. The AIC for a model with k AR terms fitted to a series of length n is defined as
Further information about fitting models can be found in [18]. The choice of this methodology (DL/AIC) is made on pragmatic grounds. The Durbin Levinson approach is simple and efficient especially when used with AIC as a fit-ting criterion, since the AIC can be calculated directly from the partial autocorrelations  X  i,j . In studies of model se-lection such as [16] AIC performs at least as well as other possibilities e.g. BIC [31]. The drawbacks of the procedure are well known (see [6]) and in our view minor. DL is ef-fectively a least squares approach. The alternative is to use likelihood methods. These have useful asymptotic theory, but at the cost of increased complexity. Broerson [6] among others has pointed out that exact maximum likelihood so-lutions may not give a real advantage in some time series problems. Other possibilities are the Burg approach [7] and Godolphins method [14] or to estimate via the spectrum. For our purposes any advantages of these approaches are of less importance than the DL/AIC benefits of simplicity and efficiency.

In [2] we demonstrate, by repeating the experiments de-scribed in Section 4 of [21], that using the Durbin-Levison recursions do not adversely affect the efficiency of the esti-mates of the model parameters.

Kedem, [21] and Kedem and Slud [22] derived several use-ful links between the original and the clipped series. For example, if we assume that the unclipped series y is both Gaussian and stationary to second order, we can use the bivariate normal distribution to compute probabilities. We assume there are no outliers in the data ( p =0),andwede-note the autocorrelation of lag r by  X  Y ( r ) for an unclipped model Y and  X  C ( r ) the autocorrelation of lag r for a clipped model C . It is not difficult to show that
P [ C ( r + s )=1and C ( s )=1]= 1 It follows that the autocorrelations of the clipped series,  X  ( k ), are given by  X  C ( k )= 2
Since the binary series gives the runs in the time series, we have, from equation 5, a link between the probability of a run and the correlation structure of the original process. Using a Taylor expansion we have  X  C ( k )= 2 which implies that the two series will have similar correla-tions but that those from the clipped series will have smaller absolute values. In fact we can go further and can show that with the Gaussian assumptions above Now define since E [ e ( t ) Y ( t + k )] = 0 we have a process where E [ e ( t )] = 0. This implies that if the original series can be written in autoregressive form, say
Y ( t )+  X  1 Y ( t  X  1) +  X  2 Y ( t  X  2) + ... +  X  p Y ( t  X  p then the clipped series has the form Here of course the errors are correlated. We can deduce that, given the covariance matrix of the errors e ( t ), we can produce a linear transformation which will give us uncor-related errors. Hence we deduce a linear ARMA model for original series implies a linear ARMA model for the clipped series .
Given a set of parameters to an ARMA model fitted by the procedure described in Section 3.2 we cluster with k -means and k -medoids using the Euclidean distance between the fit-ted parameters. For any clustering experiment, we restart k -means and k -medoids and take the clustering that mini-mizes the within cluster distance. In Section 7.3 we also use hierarchical clustering methods. More complex clustering mechanisms are used for similar data in [20, 36, 28]. In [24] it is observed that on many problems Euclidean distance performs as well or better than more complex measures. In Sections 5 and 6 we justify our choice of distance measure and clustering algorithm by demonstrating that Euclidean distance with k -means performs comparably with other pub-lished techniques on data from ARMA models.
An experiment consists of the following steps: set the pa-rameters; define a model space M ; randomly sample the model space to determine the k cluster models; for each cluster model, generate l series of length n ;fitamodelto each series using the method described in 3.2; cluster the series u times, taking as the best the clustering with the lowest within cluster distance from the centroid; evaluate the accuracy of the best clustering. Clustering performance is measured by the classification accuracy, i.e. the ratio of the percentage of the data in the final clustering that is in the correct cluster. Note we are measuring accuracy on the training data rather than applying the data to a separate testing data set. We do this because wish to measure the effects of outliers in the training data rather than assess the algorithm X  X  ability to solve the clustering problem. We use this measure rather than some of the alternatives (see [15]) since we know the correct clustering.

For a given clustering we measure the accuracy by form-ing a k  X  k contingency matrix. Since the clustering label may not coincide with the actual labelling (e.g. all those series in cluster 1 may be labelled cluster 2 by the clustering algorithm) we evaluate the accuracy (number correctly clas-sified divided by the total number of series) for all possible k ! permutations of the columns of the contingency table. The achieved accuracy is the maximum accuracy over all permutations.
One of the benefits of using clipped series is that the data can be packed efficiently in integer arrays and manipulated using bit operators. A series of doubles of length n can be stored in an array of n/ 64 integers, a worthwhile reduction particularly in applications that involve very long or very many series. For very large data sets packing the series may make the difference between being able to store the series in main memory and having to access the data from disk. Hence packing clipped series could provide a significant time improvement for clustering algorithms that require the re-calculation of models directly from the data at each iteration of the clustering.

Fewer operations may also be required to perform calcu-lations on binary series. One benefit of using clipped data comes in the calculation of the autocorrelations. An auto-correlation requires the calculation of the sum of the product of lagged series with the original, i.e.
 The autocorrelation calculations for a clipped series repre-sented as arrays of binaries requires fewer operations than that of an unclipped series (see [21] for details). We can get further speed up by packing the clipped series into integers then using logical bit operators and shift operators to find the multiplication. To find the sum R i we shift a copy of clipped C left i places, logical AND the two series together to perform the multiplication, then sum the number of bits in the result to find R i . This is much faster than looping through n times. We can also speed up the operation to sum the bits. Any algorithm to count the bits is  X ( n ). We can however improve the constant terms in the time complexity function with the use of a lookup table for the number of bits in 8 bit integers. We use shift operators to evaluate the integer value of each 8 bit sequence then use a lookup table to find the number of bits. Figure 2 demonstrates that calculating the autocorrelations is then approximately three times faster with the clipped data even when the series are stored in main memory. Each point in Figure 2 gives the time taken (averaged over 30 series) to find the autocorrela-tions. The times include the procedure to clip the data. The median of each series is found using the quick select algo-rithm, hence clipping can be done in O ( n ) time on average. Figure 2: Average time taken to find autocorrela-tions
Packing the data also offers the potential for improving the performance of the clustering algorithm. For example, the mean calculation for k -means becomes the problem of counting bits in subseries, which can be done more efficiently than the equivalent calculation with the unclipped data us-ing masks and bit count lookup tables. As demonstrated in Section 7.3 distance calculations can be made more efficient by similar mechanisms.
The first set of experiments are on data from AR(1) mod-els used in [20, 36]. Kalpakis et al [20] and Xiong and Ye-ung [36] both include experiments with clustering data from two AR(1) models with parameter in the range  X  =0 . 3to  X  =0 . 6. Kalpakis et al fit a model to the data using Lin-ear Predictive Coding then transform the parameters into the associated Cepstrum. They cluster using the Euclidean distance between Cepstrum as a distance metric.

Xiong and Yeung used an EM clustering algorithm based on the maximum likelihood estimates of the parameters of mixture ARMA models. The majority of the experimen-tation involved data from the same AR(1) models used by Kalpakis et al .

To demonstrate that clustering data from AR(1) mod-els using k -means with Euclidean distance on the fitted AR parameters is approximately as accurate as the Cepstrum based technique used in [20] and the mixtures of AR mod-els (MAR) method described in [36] we reproduce the ex-periments presented in Table 2 in [36] using the Cepstrum method of [20] and the Euclidean distance between param-eters. Data sets of 15 series were generated from two AR(1) models. Data from one cluster is from a model with  X  = 0 . 3  X  0 . 01, data from the second cluster is from a model with  X  as given in the first column of Table 1. It is assumed (but not stated in [36]) that the data series were of length 256, as in [20]. We perform two experiments to demonstrate how the fitting procedure affects the accuracy of the clustering.
Table 1 summarises all the results. Clustering on the fit-ted parameters and their Cepstrum was performed using restart k -means. The results for the unclipped data with no outliers and no AIC were found through fitting an AR model with a maximum of 20 terms without any check to mea-sure the significance of the included variables. Clustering on Euclidean distance between the parameters is significantly worse than with Cepstrum at each treatment level. This is Table 1: Clustering accuracy of k -means on AR(1), averaged over 30 runs
AR Coeff CEP Euclidean Dist 0.60  X  0.01 (0.93/ 0.99 /1.00/ 0.02 ) (0.73/ 0.97 /1.00/ 0.05 ) 0.50  X  0.01 (0.73/ 0.94 /1.00/ 0.04 ) (0.50/ 0.78 /1.00/ 0.12 ) 0.40  X  0.01 (0.63/ 0.76 /0.97/ 0.07 ) (0.50/ 0.58 /0.70/ 0.07 ) 0.60  X  0.01 (0.93/ 1.00 /1.00/ 0.01 ) (0.90/ 0.99 /1.00/ 0.02 ) 0.50  X  0.01 (0.73/ 0.95 /1.00/ 0.05 ) (0.87/ 0.95 /1.00/ 0.04 ) 0.40  X  0.01 (0.60/ 0.79 /0.97/ 0.07 ) (0.53/ 0.78 /0.93/ 0.08) 0.60  X  0.01 (0.73/ 0.94 /1.00/ 0.05) (0.80/ 0.92 /1.00/ 0.05) 0.50  X  0.01 (0.53/ 0.84 /0.97/ 0.08) (0.53/ 0.81 /0.97/ 0.09) 0.40  X  0.01 (0.50/ 0.70 /0.90/ 0.08) (0.50/ 0.66 /0.87/ 0.09) 0.60  X  0.01 (0.57/ 0.83 /1.00/ 0.09 ) (0.63/ 0.85 /0.97/ 0.08 ) 0.50  X  0.01 (0.57/ 0.73 /0.87/ 0.07 ) (0.53/ 0.76 /0.93/ 0.08 ) 0.40  X  0.01 (0.50/ 0.63 /0.77/ 0.07 ) (0.50/ 0.61 /0.77/ 0.07 ) 0.60  X  0.01 (0.77/ 0.94 /1.00/ 0.05 ) (0.80/ 0.95 /1.00/ 0.05 ) 0.50  X  0.01 (0.67/ 0.84 /0.97/ 0.07 ) (0.57/ 0.81 /0.97/ 0.08 ) 0.40  X  0.01 (0.50/ 0.69 /0.87/ 0.09 ) (0.50/ 0.67 /0.87/ 0.08 ) because of the noise introduced by the insignificant param-eter estimates. The Cepstrum reduce the effect of this noise by smoothing and hence produce more accurate clusterings. We can achieve equivalent accuracy with Euclidean distance by introducing the standard model selection method of min-imizing AIC described in Section 3.2. AIC nearly always reduces the model to 1 or 2 parameters. The second set of results in Table 1 show that the results are comparable to those given in Table 2 of [36]. This suggests that for this type of data k -means is as good as the MAR algorithm presented in [36]. Secondly, there is no significant difference between the mean accuracy at any treatment level between the Cep-strum clustering and the AIC clustering. This demonstrates that the LPC Cepstrum based clustering described in [20] is not better than the Euclidean based clustering if a standard method is used to reduce the noise by removing redundant parameter estimates.

The third part of Table 1 shows that clustering with clipped data does seem to degrade clustering performance for this model with n = 256. The averages are lower for all mod-els. However, the maximum of each run is approximately the same. The poor average figure is caused by occasional very bad run (as shown by the wide differences in minimum values) and the short data length.

Obviously the LPC Cepstrum and MAR techniques may be superior for data from other classes of model. However, our interest focuses on the benefits of clipping the data with possible outliers. There is no evidence to suggest that more complex algorithms provide significantly more accurate clus-tering for data from AR(1) models. Hence we use the sim-plest clustering methods available (k-means and k-medoids) and suggest that it is reasonable to assume that the benefits of clipping observed would also be seen if the more complex methods described in [36, 20] were used instead.
To demonstrate the benefits of clipping we repeated the experiments with a small probability of an outlier, as defined in Section 3.1. The results in the fourth and fifth part of Table 1 clearly show that the extra noise in the data means that the clustering on the clipped data produces more accu-rate clusterings. The mean difference between the clipped average and the unclipped average is significantly greater than zero at every level.
 Table 2: Accuracy using k-means and k -medoids on clipped data with  X  2 =0 . 50
Table 2 gives the accuracy of the k -means and k -medoids for the situations considered in previous experiments. The results in Table 2 demonstrate that k -meansclustersasac-curately as k -medoids. There is no significant difference be-tween the means of each combination. We use k -means for the majority of experimentation because it is faster.
To reinforce the observation that clustering with clipped data is better when the data contains outliers, we experi-ment with the parameter  X  chosen randomly on an interval [  X  0 . 9 , 0 . 9] for each cluster. The wider the range the easier, on average, the clustering task. If the series are of a rea-sonable length the clustering algorithms should be able to find the correct clusterings in the majority of cases. Fig-ure 3 shows the mean and median accuracy difference aver-aged over 30 runs for random models. Two observations can be made about these graphs. Firstly, the mean accuracy is lower with the clipped data when there are no outliers. This is caused by the occasional very bad clustering, a point il-lustrated by the fact that the median values are the same. This effect is reduced as the size of the series increases: with n = 2000 and p = 0 the observed mean difference in accu-racy was -0.0013; and with n = 10000 we observed a positive mean difference (0.0007). In both cases there was no signifi-cant difference between the medians (using a Wilcoxon test for paired samples).

Secondly, for low probability of outliers, the average clus-tering accuracy using unclipped data is significantly worse. As the probability of a data being an outlier increases the accuracy on the unclipped data decreases more rapidly than the accuracy on the clipped data, demonstrated by the pos-itive slope of both graphs in Figure 3.

Thus for two classes of data from AR(1) models, k -means clustering based on the Euclidean distance between fitted parameters and n = 256, clipping the data does not signifi-cantly reduce the median accuracy of clustering when there are no outliers and significantly increases the accuracy even when the probability of an outlier is small. To demonstrate that this result is not an artifact of the clustering algorithm we repeat the experiments using the k -medoids algorithm. Table 3 shows the results for clipped and unclipped data. The pattern of performance is the same: when there are no outliers the mean for unclipped data is higher but the me-dians are approximately the same; when the probability of a data being an outlier is 0.01 the average (both mean and median) clustering is significantly better with the clipped data.
 Figure 3: Difference in clustering accuracy between the clipped and unclipped series for varying values of p Table 3: Accuracy using k-medoids to cluster clipped data from random AR(1). (mean/median/ standard deviation ) Outlier p=0.01 (0.80/ 0.83 / 0.21 ) (0.86/ 0.97 / 0.182 )
To demonstrate that the result is observable in harder clustering problems, the experiments were repeated with larger values of k . Figure 4 shows the median accuracy for clipped and unclipped data for p = 0 on the left hand graph and p =0 . 01 on the right. For each cluster we are generat-ing 15 series of length 256 (i.e. l =15and n = 256). Since we keep l and n constant, the accuracy of clustering de-creases for both clipped and unclipped as k increases. This achieves the desired result of presenting progressively harder clustering problems. Figure 4 demonstrates that clustering on the clipped data improves the accuracy in the presence of outliers. With no outliers, clustering with clipped data is less accurate, but this gap in accuracy reduces as n in-creases. From these experiments we can conclude that if Figure 4: Clustering accuracy for alternative values of k, with no outliers (left) and a small probability of outlier (right) a set of time series are derived from a mixture model of AR(1) processes, and if each series is of adequate length, then clustering (using k -means or k -medoids) with clipped data rather than unclipped data: significantly speeds up the clustering process; does not significantly decrease the accu-racy of clustering; and significantly increases the clustering accuracy when the data contains undetected outliers.
Maharaj has investigated clustering time series from ARMA models in [27, 28]. She fits AR series to data from a vari-ety of ARMA models, then clusters using a greedy algo-rithm with a distance measure derived from the p-value of a significance test. The first experiment involved five gen-erating models ( k = 5) each generating four series ( l =4) of length 200 ( n = 200). The models used were AR(1), MA(1), AR(2), MA(2), ARMA(1,1). The second experi-ment was designed to be easier. The parameters were the same except k was changed to four and the models used were AR(0), AR(1), MA(1), ARMA(1,1). The results for k -Table 4: Clustering data from fixed ARMA models with no outliers means clustering using clipped and unclipped data with no outliers is shown in Table 4. Maharaj assesses the quality of a clustering by a measure based on the number of clusters exactly correct. This measure is not directly comparable to the accuracy measure we use. A simple conversion can be performed to estimate the accuracy by our definition from the measure used in [27]. If m clusters are exactly correct then m  X  l data are correct. Of the remaining ( k  X  m )  X  l we assume each is equally likely to be in any of the remaining clusters, and we assume that there are k +1 clusters, then we can estimate the overall accuracy. The estimated accuracy of the results reported in [27] is 0.60 for Experiment 1 and 0.95 for Experiment 2. Unclipped clustering with k -means performs better, although of course we have the advantage of knowing the number of clusters, k , apriori . The clipped series do not cluster as well on the easier problem (Experi-ment 2). The relatively large number of clusters means the data is not long enough to mitigate against the amount of information that is lost through clipping. However, if we add some outliers to the data, the situation is reversed. Ta-ble 5 shows that the outliers effect the accuracy of clusters on the clipped series far less than that on the unclipped.
For the class of mixture ARMA generating models we wish to demonstrate that if there are no outliers in the data and the series are long enough then clustering with clipped data does not significantly decrease the accuracy and if there are a small number of outliers, then clipping the data gives a sig-nificant increase in accuracy. To test these hypotheses we generated mixture models for two classes randomly struc-tured from AR(1), MA(1) to ARMA(2,2) and with random parameters. We are primarily interested in clustering sta-tionary ARMA models. This is because for long series non stationary models quickly increase or decrease to a point Table 5: Clustering data from fixed ARMA models with probability of an outlier equal to 0.01 where numeric errors can occur. To make sure the ran-dom model generated is stationary we perform a simple test where a series is discarded if a value above or below a certain threshold is observed. 10 series of length 1000 were generated from each class. The mean classification accuracy for clipped and unclipped data with and without outliers is shown in Table 6, aver-aged over 200 experiments. For the data with no outliers, we are unable to reject the null hypothesis that the pop-ulation mean difference is zero using a paired two sample t-test on the mean difference, hence we conclude there is no evidence to suggest that clustering accuracy is worse with clipped data for data from this model. When performing the test with the data containing outliers we are able to reject the null hypothesis at the 1% level and conclude that there is evidence to suggest the average accuracy is higher with clipped data when the probability of a data being an outlier is 0.01.
 Table 6: Clustering accuracy on random ARMA models, k =2
On the majority of cases both methods find the correct clustering. This is because of the wide range of possible models sampled make this a fairly easy problem. The num-ber of times one algorithm outperforms another is given in Table 7. It shows that when there are no outliers, un-clippedoutperformedclippedon25modelsandclippedout-performed unclipped on 21. With outliers, clipped did bet-ter on 29 occasions, unclipped on only 12.
 Table7: Number of times one method outperformed the other on random ARMA models, k =2
Figure 5 illustrates how the accuracy on the unclipped data decreases as the frequency of outlier increases when the series are of length n = 200.

A more complex clustering problem involves random data from ARMA models with more clusters. To test whether the benefits of clipping are still present on harder problems Figure 5: Accuracy for increasing probability of an outlier we generated data as before but with k from 2 to 9. Figure 6 shows the accuracy for clipped and unclipped data with n = 400 and no outliers. Figure 7 shows the results from the same experiment with the probability of an outliers equal to 0.01.
Figure 6: Mean accuracy for data with no outliers Figure 7: Mean accuracy for data with probability of an outlier 0.01
Figures 6 and 7 demonstrate that: the clustering problem becomes harder with the number of clusters; with no out-liers clipping does not decrease the accuracy compared to unclipped data even for harder problems up to six clusters; and if outliers are present, clipping increases the accuracy.
Clipping is less accurate for 7, 8 and 9 clusters. It is consis-tent with previous observations to suppose that the clipped series would be as accurate for larger number of clusters if the series were longer and that the unclipped data would not reach the level of accuracy of clipped data if outliers were present, however long the series.
Kalpakis et al [20] and Xioung and Yeung [36] also test their algorithms on four real data sets, available from [19]. We use two of the data sets, an ElectroCardioGram (ECG) data set and a population data set. We also evaluate the technique of clipping on a data set from [23], the Reality Check data set.
The ECG data consists of 70 series of 1000 ECG measure-ments. 22 of these patients suffered from malignant ventric-ular arrhythmia and are denoted Group 1 or venarh. Group 2, or normal, is made up of measurements of 13 healthy peo-ple. Group 3 (suprav) contains data on 35 people with the condition superventricular arrhythmia. The ECG data is characterised by large peaks and troughs and a high degree of periodicity which would normally indicate that autore-gressive models are unsuitable [18]. However, both [20, 36] fitted ARMA models. Kalpakis et al normalise the data, smooth each series with a period 3 moving average, apply the difference operator three times and fit AR(2) models to the data (i.e. fit an ARIMA(2,3,0) to normalised, smoothed data). They then perform two clustering experiments. Ex-periment 1 involves clustering data groups 1 and 2 (venarh and normal). Experiment 2 cluster involves clustering data groups 2 and 3 (normal and suprav). The results for restart k -means shown in Table 8 are within the range of values for the techniques used in [20]. Clipping the data actually leads to a higher level of accuracy in Experiment 2 than the best reported in [20], even without outliers in the data. This improvement can be explained by the unsuitability of an ARMA model. The peaks in the data set have a simi-lar effect to the estimators as outliers and clipping removes their unwanted influence. This highlights another advantage of clustering with clipped data. A large difference in accu-racy between clipped and unclipped data could indicate the presence of outliers, but it could also serve as a means of de-tecting model misspecification. A model based on estimates of frequency and amplitude may be more suitable for ECG data.
The population data set consists of annual population data for 20 states of the US for the period 1900-1999. The states are grouped into two clusters, the first with an expo-nential increasing trend, the second with a stabilizing trend. Following Kalpakis, we fit ARMA(1,1,0) to each series and cluster on the parameters. We also added noise to the data (probability of an outlier p =0 . 02). The results are given in Table 9. The results for k -means with no noise are within the range of those reported by Kalpakis for autocorellation
Table 9: Results for clustering population data based techniques. Clipping results in a significant degrada-tion in performance. The most obvious explanation for this is that the series are too short (100 data). The benefits of clipping can still be observed when noise is added. The out-liers do not effect the accuracy of the clusters formed with clipped data, but they significantly degrade the accuracy of the unclipped data.
The Reality Check data is a dataset for which the intuitive clustering can be found with Euclidean distance metrics. It consists of data from Space Shuttle telemetry, Exchange Rates and artificial sequences and serves a useful purpose of providing a basis for alternative distance metrics. Previous clusterings have been based on clustering model parameters with k-means. We use the reality check data to demonstrate that, firstly, clipping does not decrease the quality of clus-ters formed when distance is based on Euclidean distance between data rather than parameters of fitted models and secondly, that clipping does not massively alter the struc-ture of the dendrogram that results from clustering using hierarchical clustering methods.

Figures 8 and 9 show the dendrograms with clipped and unclipped data using average linkage hierarchical clustering. The only difference in the dendrograms is the order groups (9,10) and (6,7,8) are linked into largest clusters. The fact that this is not a major difference is demonstrated by the observation that the dendrograms for unclipped data using nearest and furthest linkage show a similar amount of devia-tion from the average linkage graph to the difference between Figures 8 and 9. Figure 8: Average linkage dendrogram for unclipped data
Hierarchical algorithms require the calculation of all dis-tances prior to the clustering process. Figure 10 demon-strates that the distance calculation with clipped data is approximately 5 times faster than with unclipped data. This speed up, coupled with the reduced space requirements of Figure 9: Average linkage dendrogram for clipped data clipped data, could provide a justification for clipping in its own right. Figure 10: Average time taken to find distances be-tween series length 1000
This paper discusses the benefits of clipping data when clustering time series. For data from the class of ARMA models we show that, theoretically, the models fitted to clipped data asymptotically approach the model fitted from the unclipped data. We demonstrate the application of this result in clustering through a series of clustering experiments with k -means and k -medoids using Euclidean distance on the fitted parameters as a distance metric. We justify our choice of clustering algorithm and distance metric by re-peating published experiments and reproducing comparable results. We then demonstrate how the property of clipped series can be used in clustering by randomly sampling AR(1) and ARMA models. Over this class of model we show that: calculating the autocorrelations is faster with clipped data; if the data series are long enough clustering accuracy on clipped data is not significantly less than clustering accu-racy on unclipped data; and if the data contains outliers, the clustering accuracy on clipped data is significantly bet-ter.

Clustering clipped data may provide a sufficiently good clustering in its own right, particularly when data sets are massive and the clustering algorithm is slow. It can also serve as a means of outlier detection or identification of the use of an inappropriate model, as demonstrated in Sec-tion 7.1.

Our advice to researchers wishing to cluster time series would be to start with clipped data, then examine any re-sults from more sophisticated transformations in relation to the results obtained after clipping, particularly if the series are long and time and space are an important considera-tions.

The next step in this research will be to evaluate the ef-fects of clipping on data originating from Hidden Markov Models and on more real data sets, such as those derived from biological [11] and meteorological projects [17], using a wider range of clustering algorithms.
