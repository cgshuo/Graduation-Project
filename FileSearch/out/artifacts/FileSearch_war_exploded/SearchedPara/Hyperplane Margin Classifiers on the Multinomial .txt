 Linear classifiers are a mainstay of machine learning algo-rithms, forming the basis for techniques such as the per-ceptron, logistic regression, boosting, and support vecto r machines. A linear classifier, parameterized by a vector w  X  R n , classifies examples according to the decision rule following the common practice of identifying x with the feature vector  X  ( x ) . The differences between different lin-lecting the parameter vector w based on a training set. Geometrically, the decision surface of a linear classi-fier is formed by a hyperplane or linear subspace in dimensional Euclidean space, { x  X  R n :  X  x, w  X  = 0 } where  X  X  ,  X  X  denotes the Euclidean inner product. (In both sometimes added; we prefer to absorb the bias into the no-tation given by the inner product, by setting x x .) The linearity assumption made by such classifiers can be justified on purely computational grounds; linear clas-simple to analyze and compute.
 Modern learning theory emphasizes the tension between fitting the training data well and the more desirable goal of achieving good generalization. A common practice is to choose a model that fits the data closely, but from a re-stricted class of models. The model class needs to be suf-ficiently rich to allow the choice of a good hypothesis, yet not so expressive that the selected model is likely to overfit the data. Hyperplane classifiers are attractive for balanc-ing these two goals. Indeed, linear hyperplanes are a rather restricted set of models, but they enjoy many unique prop-erties. For example, given two points x, y  X  R n , the set of points equidistant from x and y is a hyperplane; this lies behind the intuition that a hyperplane is the correct ge -hyperplane is the best decision boundary to separate two Gaussian distributions of equal covariance. Another disti n-guishing property is that a hyperplane in R n is isometric to R n  X  1 , and can therefore be thought of as a reduced dimen-sion version of the original feature space. Finally, a linea r hyperplane is the union of straight lines, which are distanc e minimizing curves, or geodesics, in Euclidean geometry. However, a fundamental assumption is implicitly associ-ated with linear classifiers, since they are based crucially on the use of the Euclidean geometry of R n . If the data or features at hand lack a Euclidean structure, the arguments above for linear classifiers break down; arguably, there is lack of Euclidean geometry for the feature vectors in most applications. This paper studies analogues of linear hyper -planes as a means of obtaining simple, yet effective classi-fiers when the data can be represented in terms of a natural geometric structure that is only locally Euclidean. This is the case for categorical data that is represented in terms of multinomial models, for which the associated geometry is spherical.
 general Riemannian spaces, we focus our attention on the multinomial manifold, which permits a relatively simple analysis. The multinomial manifold and its hyperplanes are the topics of Sections 2-4. The construction and training of margin based models is discussed in Section 5, with an em-phasis on spherical logistic regression. A brief examinati on of linear hyperplanes in general Riemannian manifolds ap-pears in Section 6 followed by experimental results for text classification given in Section 7. Concluding remarks are made in Section 8. The multinomial manifold is the parameter space of the multinomial distribution equipped with the Fisher information metric g where u, v are vectors tangent to P n at x , represented in the standard basis of R n +1 . Note that unlike conventional no-tation in statistics we denote the multinomial parameters b y x . The reason for doing so is that we identify multinomial parameters with text documents, as described in further de-tail in Section 7.
 isometric to the positive n -sphere with the metric inherited from the embedding Euclidean space (Kass, 1989). The isometry  X  : P n  X  S n  X  x 1 , . . . , on the positive sphere and apply them to P n through  X   X  1 . Of particular interest is the fact that the geodesic distanc e between x, y  X  P n may be now computed as the Euclidean length of the great circle connecting  X  ( x ) and  X  ( y ) ically, Using the above isometry we focus our attention, in the next few sections, on hyperplanes and margins on the n -sphere S n and the positive n -sphere S n oped there will apply directly to the multinomial manifold when followed by  X   X  1 .
 It is worth mentioning that the singular boundary of and S n or even differentiable manifolds with boundary. However, this is a technical issue that can be overcome by taking the interior of P n and S n we can now take points arbitrarily close to it, in both the Euclidean and the geodesic metric.
 We do not explore here the many interesting and motivat-ing properties of the Fisher information metric. For detail s on this topic see Kass and Voss (1997) and Amari and Na-gaoka (2000). Spivak (1975) contains a comprehensive in-troduction to Riemannian geometry. This section generalizes the notion of linear hyperplanes and margins to the n -sphere S n = { x  X  R n +1 : P 1 } . A similar treatment on the positive n -sphere S n complicated, and is postponed to the next section. In the re-mainder of the paper we denote points on P n , S n or S n vectors in R n +1 using the standard basis of the embedding space. The notation  X  X  ,  X  X  and k X k will be used for the Eu-clidean inner product and norm.
 A hyperplane on S n is defined as H E with the normal vector u . We occasionally need to refer to the unit normal vector and denote it by  X  u . H dimensional submanifold of S n which is isometric to S n  X  1 (Bridson &amp; Haefliger, 1999). Using the common notion of the distance of a point from a set d ( x, S ) = inf we make the following definitions.
 Definition 1. Let X be a metric space. A decision bound-ary is a subset of X that separates X into two connected components. The margin of x with respect to a decision boundary H is d ( x, H ) = inf Note that this definition reduces to the common definition of margin for Euclidean geometry and affine hyperplanes. In contrast to Gous (1998), our submanifolds are intersec-tions of the sphere with linear subspaces, not affine sets. One motivation for the above definition of hyperplane as the correct generalization of a Euclidean hyperplane is tha t H spherical metric. Further motivation is given in Section 6. Before we can obtain a closed form expression for margins on S n we need the following definitions.
 Definition 2. Given a point x  X  R n +1 , we define its reflec-tion with respect to E Note that if x  X  S n then r k r u ( x ) k Definition 3. The projection of x  X  S n \ {  X  u } on H defined to be Note that p  X  x,  X  u  X   X  u,  X  u  X  =  X  x,  X  u  X   X   X  x,  X  u  X  k  X  u k 2 tion is justified by the following proposition.
 Proposition 1. Let x  X  S n \ ( H Proof. Since q  X  H and ( a ) follows. Assertion ( b ) follows from cos d ( x, p u ( x )) = 2 cos 2  X   X  1 , cos(2 d ( x, p u ( x ))) = 2 cos 2 ( d ( x, p u ( x )))  X  1 and hence d ( x, p d ( x, q ) , q  X  H u cannot be any smaller than d ( x, p since this would result in a path from x to r shorter than the geodesic d ( x, r Parts ( b ) and ( c ) of Proposition 1 provide a closed form expression for the S n margin analogous to the Euclidean Euclidean signed margin y  X   X  u, x  X  is A plot of the signed margin as a function of  X  x,  X  u  X  and a geometric interpretation of the spherical margin appear in Figure 2. A hyperplane on the positive n -sphere S n H nition leads to a margin concept d ( x, H the S n margin d ( x, H The infimum above is attained by the continuity of and compactness of E distance d ( x, H The following theorem will be useful in computing d ( x, H u + ) . For a proof see Bridson and Haefliger (1999) page 17.
 Theorem 1. ( The Spherical Law of Cosines ) Consider a spherical triangle with geodesic edges of lengths a, b, c , where  X  is the vertex angle opposite to edge c . Then We have the following corollaries of Proposition 1. Proposition 2. If x  X  S n Proof. This follows immediately from the fact that p S Proposition 3. For x  X  S n where  X  S n Proof. Assume that q 6 X   X  S n a minimal geodesic  X  . Since p intersects the boundary  X  S n H p ( x ) = arg min y  X   X  d ( y, x ) , the geodesic from x to and  X  intersect orthogonally (this is an elementary result in Riemannian geometry, e.g. Lee (1997) p. 113). Using the spherical law of cosines, applied to the spherical triangle s that Hence r is closer to x than q . This contradicts the definition of q ; thus q can not lie in the interior of S n + . Before we proceed to compute d ( x, H we define the following concepts.
 Definition 4. The boundary of S n and S n A  X  { 1 , . . . , n + 1 } is  X 
A S n + = S n +  X  { x  X  R n +1 :  X  i  X  A, x i = 0 } Note that if A  X  A 0 then  X  tion  X  X  ,  X  X  and norm, where the summation is restricted to indices not in A .
 Definition 5. Given x  X  S n we define x | We abuse the notation by identifying x | responding point on S n  X  X  A | under the isometry  X  S n  X  X  A | mentioned in Definition 4. Note that if x  X  S n then x | the S n Proposition 4. Let  X  u  X  R n +1 be a unit vector, x  X  S n and q = arg min (possibly empty) set A = { 1  X  i  X  n + 1 : q Proof. If p earlier propositions and the fact that when A =  X  , k x k k x k = 1 and v | A = v . We thus restrict our attention to the case of A 6 =  X  .
 For all I  X  { 1 , . . . , n + 1 } we have It follows that By Proposition 3 applied to S n  X  X  A | we have that since lies in the interior of S n  X  X  A | then so does p Using Proposition 1 applied to S n  X  X  A | we can compute d ( x, H u + ) as d ( x, p u | A ( x | A )) = arccos In practice the boundary set A of q is not known. In our experiments we set A = { i : ( p simulations in low dimensions, the true boundary never lies outside of this set. The logistic regression model p ( y | x ) = 1 with y  X  { X  1 , 1 } , assumes Euclidean geometry. It can be reexpressed as where d is the Euclidean distance of x from the hyperplane that corresponds to the normal vector  X  u , and where  X  = k u k is a parameter.
 The generalization to spherical geometry involves simply changing the margin to reflect the appropriate geometry: p ( y | x ;  X  u,  X  )  X  Denoting s ample ( x, y ) is We compute the derivatives of the log-likelihood in several steps, using the chain rule and the notation z =  X  x | We have  X  arccos k x k A and hence The log-likelihood derivative with respect to  X  u (3) times The log-likelihood derivative with respect to  X  is Optimizing the log-likelihood with respect to  X  u requires results in a non-normalized vector. Performing the above gradient descent step followed by normalization has the ef-fect of moving along the sphere in a curve whose tangent gent space T iments described in Section 7.
 Note that the spherical logistic regression model has n + 1 parameters in contrast to the n +2 parameters of Euclidean ition that a hyperplane separating an n -dimensional man-ifold should have n parameters. The extra parameter in the Euclidean logistic regression is an artifact of the embed-ding of the n -dimensional multinomial space, on which the data lies, into an ( n + 1) -dimensional Euclidean space. The derivations and formulations above assume spherical data. If the data lies on the multinomial manifold, the isom-etry  X  mentioned in Section 2 has to precede these calcu-lations. The net effect is that x model equation, and in the log-likelihood and its deriva-tives.
 Synthetic data experiments contrasting Euclidean logisti c regression and spherical logistic regression on S n scribed in this section, are shown in Figure 3. The leftmost column shows an example where both models give a simi-lar solution. In general, however, as is the case in the other two columns, the two models yield significantly different decision boundaries. The definition of hyperplanes in general Riemannian mani-folds has two essential components. In addition to discrim-inating between two classes, hyperplanes should be regular in some sense with respect to the geometry. In Euclidean geometry, the two properties of discrimination and regular -ity coincide, as every affine subspace of dimension n  X  1 separates R n into two regions. In general, however, these two properties do not necessarily coincide, and have to be considered separately.
 The separation property implies that if N is a hyperplane of M then M \ N has two connected components. Note that this property is topological and independent of the metric. The linearity property is generalized through the notion of autoparallelism explained below. The following definition s and propositions are taken from Spivak (1975), Volume 3. We assume that  X  is the connection inherited from the met-ric g .
 Definition 6. Let M be a Riemannian manifold with con-nection  X  . A submanifold N  X  M is auto-parallel if par-allel translation in M along a curve C  X  N takes vectors tangent to N to vectors tangent to N .
 Proposition 5. A submanifold N  X  M is auto-parallel if and only if Definition 7. A submanifold N of M is totally geodesic at p  X  N if every geodesic  X  in M with  X  (0) = p,  X  0 (0)  X  T
N remains in N on some interval (  X  , ) . The submani-fold N is said to be totally geodesic if it is totally geodesic at every point.
 As a consequence, we have that N is totally geodesic if and only if every geodesic in N is also a geodesic in M . Proposition 6. Let N be a submanifold of ( M,  X  ) . Then 1. If N is auto-parallel in M then N is totally geodesic. 2. If M is totally geodesic and  X  is symmetric then M is Since the metric connection is symmetric, the last proposi-tion gives a complete equivalence between auto-parallelis m and totally geodesic submanifolds.
 We can now define linear hyperplanes on Riemannian man-ifolds.
 Definition 8. A linear decision boundary N in M is an autoparallel submanifold of M such that M \ N has two connected components.
 Several observations are in order. First note that if M is an n -dimensional manifold, the separability condition re-quires N to be an ( n  X  1) -dimensional submanifold. It is easy to see that every affine subspace of R n is totally geodesic and hence autoparallel. Conversely, since the met -ric connection is symmetric, every auto-parallel submani-fold of Euclidean space that separates it is an affine sub-space. As a result, we have that our generalization does indeed reduce to affine subspaces under Euclidean geom-etry. Similarly, the above definition reduces to spherical hyperplanes H hyperbolic half plane H 2 where the linear decision bound-aries are half-circles whose centers lie on the x axis. Hyperplanes on S n have the following additional nice prop-erties. They are the set of equidistant points from x, y  X  S n rameterized by n parameters. These properties are partic-ular to the sphere and do not hold in general (Bridson &amp; Haefliger, 1999).
 A natural embedding of text documents in the multinomial simplex is the L sentation (Joachims, 2000) Using this embedding we compared the performance of spherical logistic regression with Euclidean logistic reg res-sion. Since Euclidean logistic regression often performs better with L these results as well.
 The embedding may be motivated by the following argu-ment. Assuming that the text documents are generated by multinomial distributions  X  7 X  x , the embedding  X   X  is theo-retically justified as the maximum likelihood estimator. It makes more sense to view tf feature vectors as points in the simplex and not in the much larger Euclidean space. The choice of the Fisher information metric is motivated by the axiomatic characterization of  X  Cencov (1982) and by the vast experimental evidence of its usefulness in statis-tics.
 Experiments were conducted on both the Web-KB and the Reuters-21578 datasets. In the Web-KB dataset, the clas-sification task that was tested was each of the classes fac-ulty, course, project and student vs. the rest. In the Reuter s vs. the rest. The test error rates as a function of randomly sampled training sets of different sizes are shown in Fig-ure 4. In both cases, the positive and negative example sets are equally distributed, and the results were averaged over a 20-fold cross validation with the error bars indicating on e standard deviation. As mentioned in Section 4, we assume that the boundary set of q = arg min equal to A = { i : ( p The experiments show that the new linearity and margin concepts lead to more powerful classifiers than their Eu-clidean counterparts, which are commonly used in the lit-erature regardless of the geometry of the data. We have presented a generalization of hyperplane margin classifiers to the multinomial manifold. In related work, Gous (1998) treats regression rather than classification, a nd works with affine spherical subfamilies; see also (Hall &amp; Hofmann, 2000). Under affine subfamilies, many of the ge-ometrical properties developed here for margin-based hy-perplane models under multinomial geometry do not apply. The point of view of treating text documents as points on the simplex and using the Fisher information for construct-ing new classification schemes is presented in (Lafferty &amp; Lebanon, 2003). For categorical data, such as text, that nat -urally lie on the multinomial manifold, the new concepts of spherical hyperplanes and spherical margins presented here are better motivated than their Euclidean counterpart s. Experimental results on the Web-KB and Reuters-21578 datasets show that the resulting geometrical approach of spherical logistic regression leads to improved performan ce over standard logistic regression, which assumes Euclidea n geometry.
 This work was supported in part by NSF grants CCR-0122581 and IIS-0312814, and by ARDA contract MDA904-00-C-2106.
 Amari, S., &amp; Nagaoka, H. (2000). Methods of information geometry . American Mathematical Society.
 Bridson, M., &amp; Haefliger, A. (1999). Metric spaces of non-positive curvature , vol. 319 of A Series in Comprehen-sive Studies in Mathematics . Springer.  X  Cencov, N. N. (1982). Statistical decision rules and opti-mal inference . American Mathematical Society.
 Gous, A. (1998). Exponential and spherical subfamily models . Doctoral dissertation, Stanford University. Hall, K., &amp; Hofmann, T. (2000). Learning curved multi-nomial subfamilies for natural language processing and information retrieval. International Conference on Ma-chine Learning .
 Joachims, T. (2000). The maximum margin approach to learning text classifiers methods, theory and algorithms . Doctoral dissertation, Dortmund University.
 Kass, R. E. (1989). The geometry of asymptotic inference. Statistical Science , 4 , 188 X 234.
 Kass, R. E., &amp; Voss, P. W. (1997). Geometrical foundations of asymptotic inference . John Wiley &amp; Sons, Inc. Lafferty, J., &amp; Lebanon, G. (2003). Information diffusion kernels. Advances in Neural Information Processing, 15 . Lee, J. L. (1997). Riemannian manifolds, an introduction to curvature . Springer.
 Spivak, M. (1975). A comprehensive introduction to differ-
