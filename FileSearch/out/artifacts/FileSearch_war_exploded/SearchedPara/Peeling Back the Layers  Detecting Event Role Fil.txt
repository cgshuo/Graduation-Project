 Event extraction is an information extraction (IE) task that involves identifying the role fillers for events in a particular domain. For example, the Message Understanding Conferences (MUCs) chal-lenged NLP researchers to create event extraction systems for domains such as terrorism (e.g., to iden-tify the perpetrators, victims, and targets of terrorism events) and management succession (e.g., to iden-tify the people and companies involved in corporate management changes).

Most event extraction systems use either a learning-based classifier to label words as role fillers, or lexico-syntactic patterns to extract role fillers from pattern contexts. Both approaches, how-ever, generally tackle event recognition and role filler extraction at the same time. In other words, most event extraction systems primarily recognize contexts that explicitly refer to a relevant event. For example, a system that extracts information about murders will recognize expressions associated with murder (e.g.,  X  X illed X ,  X  X ssassinated X , or  X  X hot to death X ) and extract role fillers from the surround-ing context. But many role fillers occur in contexts that do not explicitly mention the event, and those fillers are often overlooked. For example, the per-petrator of a murder may be mentioned in the con-text of an arrest, an eyewitness report, or specula-tion about possible suspects. Victims may be named in sentences that discuss the aftermath of the event, such as the identification of bodies, transportation of the injured to a hospital, or conclusions drawn from an investigation. We will refer to these types of sentences as  X  X econdary contexts X  because they are generally not part of the main event description. Dis-course analysis is one option to explicitly link these secondary contexts to the event, but discourse mod-elling is itself a difficult problem.

The goal of our research is to improve event ex-traction by learning to identify secondary role filler contexts in the absence of event keywords. We cre-ate a set of classifiers to recognize role-specific con-texts that suggest the presence of a likely role filler regardless of whether a relevant event is mentioned or not. For example, our model should recognize that a sentence describing an arrest probably in-cludes a reference to a perpetrator, even though the crime itself is reported elsewhere.

Extracting information from these secondary con-texts can be risky, however, unless we know that the larger context is discussing a relevant event. To address this, we adopt a two-pronged strategy for event extraction that handles event narrative docu-ments differently from other documents. We define an event narrative as an article whose main purpose is to report the details of an event. We apply the role-specific sentence classifiers only to event narratives to aggressively search for role fillers in these sto-ries. However, other types of documents can men-tion relevant events too. The MUC-4 corpus, for ex-ample, includes interviews, speeches, and terrorist propaganda that contain information about terrorist events. We will refer to these documents as fleet-ing reference texts because they mention a relevant event somewhere in the document, albeit briefly. To ensure that relevant information is extracted from all documents, we also apply a conservative extraction process to every document to extract facts from ex-plicit event sentences.
 Our complete event extraction model, called TIER, incorporates both document genre and role-specific context recognition into 3 layers of analy-sis: document analysis, sentence analysis, and noun phrase (NP) analysis. At the top level, we train a text genre classifier to identify event narrative doc-uments. At the middle level, we create two types of sentence classifiers. Event sentence classifiers identify sentences that are associated with relevant events, and role-specific context classifiers identify sentences that contain possible role fillers irrespec-tive of whether an event is mentioned. At the low-est level, we use role filler extractors to label indi-vidual noun phrases as role fillers. As documents pass through the pipeline, they are analyzed at dif-ferent levels of granularity. All documents pass through the event sentence classifier, and event sen-tences are given to the role filler extractors. Docu-ments identified as event narratives additionally pass through role-specific sentence classifiers, and the role-specific sentences are also given to the role filler extractors. This multi-layered approach creates an event extraction system that can discover role fillers in a variety of different contexts, while maintaining good precision.

In the following sections, we position our research with respect to related work, present the details of our multi-layered event extraction model, and show experimental results for five event roles using the MUC-4 data set. Some event extraction data sets only include doc-uments that describe relevant events (e.g., well-known data sets for the domains of corporate ac-quisitions (Freitag, 1998b; Freitag and McCallum, 2000; Finn and Kushmerick, 2004), job postings (Califf and Mooney, 2003; Freitag and McCallum, 2000), and seminar announcements (Freitag, 1998b; Ciravegna, 2001; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Gu and Cercone, 2006). But many IE data sets present a more realistic task where the IE system must determine whether a relevant event is present in the document, and if so, extract its role fillers. Most of the Message Understand-ing Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)).
 Our research focuses on this setting where the event extraction system is not assured of getting only rele-vant documents to process.

Most event extraction models can be character-ized as either pattern-based or classifier-based ap-proaches. Early event extraction systems used hand-crafted patterns (e.g., (Appelt et al., 1993; Lehn-ert et al., 1991)), but more recent systems gener-ate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Fre-itag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis.

Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to ex-plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incor-porate wider context. Ji and Grishman (2008) en-force event role consistency across different docu-ments. (Liao and Grishman, 2010) use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identify-ing relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cer-cone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redundant extractions and worked with seminar an-nouncements, where the system was only given rel-evant documents. (Patwardhan and Riloff, 2007) de-veloped a system that learns to recognize event sen-tences and uses patterns that have a semantic affinity for an event role to extract role fillers. GLACIER (Patwardhan and Riloff, 2009) jointly considers sen-tential evidence and phrasal evidence in a unified probabilistic framework. Our research follows in the same spirit as these approaches by performing multiple levels of text analysis. But our event ex-traction model includes two novel contributions: (1) we develop a set of role-specific sentence classifiers to learn to recognize secondary contexts associated with each type of event role , and (2) we exploit text genre to incorporate a third level of analysis that en-ables the system to aggressively hunt for role fillers in documents that are event narratives. In Section 5, we compare the performance of our model with both the GLACIER system and Patwardhan &amp; Riloff X  X  semantic affinity model. The main idea behind our approach is to analyze documents at multiple levels of granularity in order to identify role fillers that occur in different types of contexts. Our event extraction model progressively  X  X ooms in X  on relevant information by first identi-fying the document type, then identifying sentences that are likely to contain relevant information, and finally analyzing individual noun phrases to identify role fillers. The key advantage of this architecture is that it allows us to search for information using two different principles: (1) we look for contexts that di-rectly refer to the event, as per most traditional event extraction systems, and (2) we look for secondary contexts that are often associated with a specific type of role filler. Identifying these role-specific contexts can root out important facts would have been oth-erwise missed. Figure 1 shows the multi-layered pipeline of our event extraction system.

An important aspect of our model is that two dif-ferent strategies are employed to handle documents of different types. The event extraction task is to find any description of a relevant event, even if the all documents are given to the event sentence recog-nizers and their mission is to identify any sentence that mentions a relevant event. This path through the pipeline is conservative because information is ex-tracted only from event sentences, but all documents are processed, including stories that contain only a fleeting reference to a relevant event.
The second path through the pipeline performs additional processing for documents that belong to the event narrative text genre. For event narratives, we assume that most of the document discusses a relevant event so we can more aggressively hunt for event-related information in secondary contexts.
In this section, we explain how we create the two types of sentence classifiers and the role filler extrac-tors. We will return to the issue of document genre and the event narrative classifier in Section 4. 3.1 Sentence Classification We have argued that event role fillers commonly oc-cur in two types of contexts: event contexts and role-specific secondary contexts. For the purposes of this research, we use sentences as our definition of a  X  X ontext X , although there are obviously many other possible definitions. An event context is a sen-tence that describes the actual event. A secondary context is a sentence that provides information re-lated to an event but in the context of other activities that precede or follow the event.

For both types of classifiers, we use exactly the same feature set, but we train them in different ways. The MUC-4 corpus used in our experiments in-cludes a training set consisting of documents and an-swer keys. Each document that describes a relevant event has answer key templates with the role fillers ( answer key strings ) for each event. To train the event sentence recognizer, we consider a sentence to be a positive training instance if it contains one or more answer key strings from any of the event roles. This produced 3,092 positive training sentences. All remaining sentences that do not contain any answer key strings are used as negative instances. This pro-duced 19,313 negative training sentences, yielding a roughly 6:1 ratio of negative to positive instances.
There is no guarantee that a classifier trained in this way will identify event sentences, but our hy-pothesis was that training across all of the event roles together would produce a classifier that learns to recognize general event contexts. This approach was also used to train GLACIER X  X  sentential event recognizer (Patwardhan and Riloff, 2009), and they demonstrated that this approach worked reasonably well when compared to training with event sentences labelled by human judges.

The main contribution of our work is introducing additional role-specific sentence classifiers to seek out role fillers that appear in less obvious secondary contexts. We train a set of role-specific sentence classifiers, one for each type of event role. Every sentence that contains a role filler of the appropri-ate type is used as a positive training instance. Sen-tences that do not contain any answer key strings are sifier to focus on the contexts specific to its particu-lar event role. We expect the role-specific sentence classifiers to find some secondary contexts that the event sentence classifier will miss, although some sentences may be classified as both.

Using all possible negative instances would pro-duce an extremely skewed ratio of negative to pos-itive instances. To control the skew and keep the training set-up consistent with the event sentence classifier, we randomly choose from the negative in-stances to produce a 6:1 ratio of negative to positive instances.

Both types of classifiers use an SVM model cre-ated with SVMlin (Keerthi and DeCoste, 2005), and exactly the same features. The feature set consists of the unigrams and bigrams that appear in the train-plus a few additional features to represent the tense of the main verb phrase in the sentence and whether the document is long ( &gt; 35 words) or short ( &lt; words). All of the feature values are binary. 3.2 Role Filler Extractors Our extraction model also includes a set of role filler extractors, one per event role. Each extractor re-ceives a sentence as input and determines which noun phrases (NPs) in the sentence are fillers for the event role. To train an SVM classifier, noun phrases corresponding to answer key strings for the event role are positive instances. We randomly choose among all noun phrases that are not in the answer keys to create a 10:1 ratio of negative to positive in-stances.
The feature set for the role filler extractors is much richer than that of the sentence classifiers be-cause they must carefully consider the local context surrounding a noun phrase. We will refer to the noun phrase being labelled as the targeted NP . The role filler extractors use three types of features:
Lexical features: we represent four words to the left and four words to the right of the targeted NP, as well as the head noun and modifiers (adjectives and noun modifiers) of the targeted NP itself.

Lexico-syntactic patterns: we use the AutoSlog pattern generator (Riloff, 1993) to automatically create lexico-syntactic patterns around each noun phrase in the sentence. These patterns are similar to dependency relations in that they typically repre-sent the syntactic role of the NP with respect to other constituents (e.g., subject-of, object-of, and noun ar-guments).

Semantic features: we use the Stanford NER tag-ger (Finkel et al., 2005) to determine if the targeted NP is a named entity, and we use the Sundance parser (Riloff and Phillips, 2004) to assign seman-tic class labels to each NP X  X  head noun. One of our goals was to explore the use of document genre to permit more aggressive strategies for ex-tracting role fillers. In this section, we first present an analysis of the MUC-4 data set which reveals the distribution of event narratives in the corpus, and then explain how we train a classifier to automati-cally identify event narrative stories. 4.1 Manual Analysis We define an event narrative as an article whose main focus is on reporting the details of an event. For the purposes of this research, we are only con-cerned with events that are relevant to the event ex-traction task (i.e., terrorism). An irrelevant docu-ment is an article that does not mention any rele-vant events. In between these extremes is another category of documents that briefly mention a rele-vant event, but the event is not the focus of the ar-ticle. We will refer to these documents as fleeting reference documents. Many of the fleeting reference documents in the MUC-4 corpus are transcripts of interviews, speeches, or terrorist propaganda com-muniques that refer to a terrorist event and mention at least one role filler, but within a discussion about a different topic (e.g., the political ramifications of a terrorist incident).

To gain a better understanding of how we might create a system to automatically distinguish event narrative documents from fleeting reference docu-ments, we manually labelled the 116 relevant docu-ments in our tuning set. This was an informal study solely to help us understand the nature of these texts.
The first row of Table 1 shows the distribution of event narratives and fleeting references based on our  X  X old standard X  manual annotations. We see that more than half of the relevant documents (62/116) are not focused on reporting a terrorist event, even though they contain information about a terrorist event somewhere in the document. 4.2 Heuristics for Event Narrative Our goal is to train a document classifier to automat-ically identify event narratives. The MUC-4 answer keys reveal which documents are relevant and irrel-evant with respect to the terrorism domain, but they do not tell us which relevant documents are event narratives and which are fleeting reference stories. Based on our manual analysis of the tuning set, we developed several heuristics to help separate them.
We observed two types of clues: the location of the relevant information, and the density of rele-vant information. First, we noticed that event nar-ratives tend to mention relevant information within the first several sentences, whereas fleeting refer-ence texts usually mention relevant information only in the middle or end of the document. Therefore our first heuristic requires that an event narrative men-tion a role filler within the first 7 sentences.
Second, event narratives generally have a higher density of relevant information. We use several cri-teria to estimate information density because a sin-gle criterion was inadequate to cover different sce-narios. For example, some documents mention role fillers throughout the document. Other documents contain a high concentration of role fillers in some parts of the document but no role fillers in other parts. We developed three density heuristics to ac-count for different situations. All of these heuristics count distinct role fillers. The first density heuristic requires that more than 50% of the sentences contain shows histograms for different values of this ratio in the event narrative (a) vs. the fleeting reference doc-uments (b). The histograms clearly show that docu-ments with a high ( &gt; 50%) ratio are almost always event narratives.
A second density heuristic requires that the ratio of different types of roles filled to sentences be &gt; requires that the ratio of distinct role fillers to sen-these three criteria are satisfied, then the document is considered to have a high density of relevant in-
We use these heuristics to label a document as an event narrative if: (1) it has a high density of relevant information, and (2) it mentions a role filler within the first 7 sentences.

The second row of Table 1 shows the performance of these heuristics on the tuning set. The heuristics correctly identify 40 reference stories, to achieve an overall accuracy of 82%. These results are undoubtedly optimistic be-cause the heuristics were derived from analysis of the tuning set. But we felt confident enough to move forward with using these heuristics to generate train-ing data for an event narrative classifier. 4.3 Event Narrative Classifier The heuristics above use the answer keys to help de-termine whether a story belongs to the event narra-tive genre, but our goal is to create a classifier that can identify event narrative documents without the benefit of answer keys. So we used the heuristics to automatically create training data for a classifier by labelling each relevant document in the training set as an event narrative or a fleeting reference doc-ument. Of the 700 relevant documents, 292 were labeled as event narratives. We then trained a doc-ument classifier using the 292 event narrative docu-ments as positive instances and all irrelevent training documents as negative instances. The 308 relevant documents that were not identified as event narra-tives were discarded to minimize noise (i.e., we es-timate that our heuristics fail to identify 25% of the event narratives). We then trained an SVM classifier using bag-of-words (unigram) features.

Table 2 shows the performance of the event nar-rative classifier on the manually labeled tuning set. The classifier identified 69% of the event narratives with 63% precision. Overall accuracy was 81%.
At first glance, the performance of this classifier is mediocre. However, these results should be inter-preted loosely because there is not always a clear di-viding line between event narratives and other doc-uments. For example, some documents begin with a specific event description in the first few para-graphs but then digress to discuss other topics. For-tunately, it is not essential for TIER to have a per-fect event narrative classifier since all documents will be processed by the event sentence recognizer anyway. The recall of the event narrative classifier means that nearly 70% of the event narratives will get additional scrutiny, which should help to find ad-ditional role fillers. Its precision of 63% means that some documents that are not event narratives will also get additional scrutiny, but information will be extracted only if both the role-specific sentence rec-ognizer and NP extractors believe they have found something relevant. 4.4 Domain-relevant Document Classifier For comparison X  X  sake, we also created a docu-ment classifier to identify domain-relevant docu-ments. That is, we trained a classifier to determine whether a document is relevant to the domain of terrorism, irrespective of the style of the document. We trained an SVM classifier with the same bag-of-words feature set, using all relevant documents in the training set as positive instances and all irrelevant documents as negative instances. We use this clas-sifier for several experiments described in the next section. 5.1 Data Set and Metrics We evaluated our approach on a standard benchmark collection for event extraction systems, the MUC-4 data set (MUC-4 Proceedings, 1992). The MUC-4 corpus consists of 1700 documents with associated answer key templates. To be consistent with previ-ously reported results on this data set, we use the 1300 DEV documents for training, 200 documents (TST1+TST2) as a tuning set and 200 documents (TST3+TST4) as the test set. Roughly half of the documents are relevant (i.e., they mention at least 1 terrorist event) and the rest are irrelevant.
We evaluate our system on the five MUC-4  X  X tring-fill X  event roles: perpetrator individuals, perpetrator organizations, physical targets, victims and weapons . The complete IE task involves tem-plate generation, which is complex because many documents have multiple templates (i.e., they dis-cuss multiple events). Our work focuses on extract-ing individual facts and not on template generation per se (e.g., we do not perform coreference resolu-tion or event tracking). Consequently, our evalua-tion follows that of other recent work and evaluates the accuracy of the extractions themselves by match-ing the head nouns of extracted NPs with the head nouns of answer key strings (e.g.,  X  X rmed guerril-sults are reported as Precision/Recall/F(1)-score for each event role separately. We also show an overall 5.2 Baselines As baselines, we compare the performance of our IE system with three other event extraction sys-tems. The first baseline is AutoSlog-TS (Riloff, 1996), which uses domain-specific extraction pat-terns. AutoSlog-TS applies its patterns to every sen-tence in every document, so does not attempt to explicitly identify relevant sentences or documents. The next two baselines are more recent systems: the (Patwardhan and Riloff, 2007) semantic affin-ity model and the (Patwardhan and Riloff, 2009) GLACIER system. The semantic affinity approach explicitly identifies event sentences and uses pat-terns that have a semantic affinity for an event role to extract role fillers. GLACIER is a probabilistic model that incorporates both phrasal and sentential evidence jointly to label role fillers.

The first 3 rows in Table 3 show the results for each of these systems on the MUC-4 data set. They all used the same evaluation criteria as our results. 5.3 Experimental Results The lower portion of Table 3 shows the results of a variety of event extraction models that we cre-ated using different components of our system. The AllSent row shows the performance of our Role Filler Extractors when applied to every sentence in every document. This system produced high recall, but precision was consistently low.

The EventSent row shows the performance of our Role Filler Extractors applied only to the event sentences identified by our event sentence classi-fier. This boosts precision across all event roles, but with a sharp reduction in recall. We see a roughly 20 point swing from recall to precision. These re-sults are similar to GLACIER X  X  results on most event roles, which isn X  X  surprising because GLACIER also incorporates event sentence identification.
 The RoleSent row shows the results of our Role Filler Extractors applied only to the role-specific sentences identified by our classifiers. We see a 12-13 point swing from recall to precision compared to the AllSent row. This result is consistent with our hypothesis that many role fillers exist in role-specific contexts that are not event sentences. As ex-pected, extracting facts from role-specific contexts that do not necessarily refer to an event is less reli-able. The EventSent+RoleSent row shows the re-sults when information is extracted from both types of sentences. We see slightly higher recall, which confirms that one set of extractions is not a strict subset of the other, but precision is still relatively low.

The next set of experiments incorporates docu-ment classification as the third layer of text analy-sis. The DomDoc/EventSent+DomDoc/RoleSent row shows the results of applying both types of sentence classifiers only to documents identified as domain-relevant by the Domain-relevant Document ( DomDoc ) Classifier described in Section 4.4. Ex-tracting information only from domain-relevant doc-uments improves precision by +6 , but also sacrifices 8 points of recall.

The EventSent row reveals that information found in event sentences has the highest precision, even without relying on document classification. We concluded that evidence of an event sentence is probably sufficient to warrant role filler extraction irrespective of the style of the document. As we dis-cussed in Section 4, many documents contain only a fleeting reference to an event, so it is important to be able to extract information from those isolated event descriptions as well. Consequently, we cre-ated a system, EventSent+DomDoc/RoleSent , that extracts information from event sentences in all doc-uments, but extracts information from role-specific sentences only if they appear in a domain-relevant document. This architecture captured the best of both worlds: recall improved from 58 % to 65 % with only a one point drop in precision.

Finally, we evaluated the idea of using document genre as a filter instead of domain relevance. The last row, EventSent+ENarrDoc/RoleSent , shows the results of our final architecture which extracts information from event sentences in all documents, but extracts information from role-specific sentences only in Event Narrative documents. This architec-ture produced the best F1 score of 56 . This model in-creases precision by an additional 4 points and pro-duces the best balance of recall and precision.
Overall, TIER X  X  multi-layered extraction architec-ture produced higher F1 scores than previous sys-tems on four of the five event roles. The improved recall is due to the additional extractions from sec-ondary contexts. The improved precision comes from our two-pronged strategy of treating event nar-ratives differently from other documents. TIER ag-gressively searches for extractions in event narrative stories but is conservative and extracts information only from event sentences in all other documents. 5.4 Analysis We looked through some examples of TIER X  X  output to try to gain insight about its strengths and limita-tions. TIER X  X  role-specific sentence classifiers did correctly identify some sentences containing role fillers that were not classified as event sentences. Several examples are shown below, with the role fillers in italics:
The first two sentences identify victims, but the terrorist event itself was mentioned earlier in the document. The third sentence contains a perpetrator ( the woman ), victims ( students ), and weapons ( hand grenades ) in the context of a hostage situation after the main event (a bus attack), when the perpetrator escaped. The fourth sentence describes incidental damage to civilian homes following clashes between government forces and guerrillas.

However there is substantial room for improve-ment in each of TIER X  X  subcomponents, and many role fillers are still overlooked. One reason is that it can be difficult to recognize acts of terrorism. Many sentences refer to a potentially relevant subevent (e.g., injury or physical damage) but recognizing that the event is part of a terrorist incident depends on the larger discourse. For example, consider the examples below that TIER did not recognize as relevant sentences:
In the first sentence, the event is described as someone pointing rifles at people and the perpetra-tors are referred to simply as individuals. There are no strong keywords in this sentence that reveal this is a terrorist attack. In the second sentence, police are being accused of state-sponsored terrorism when they seize civilians. The verb  X  X eize X  is common in this corpus, but usually refers to the seizing of weapons or drug stashes, not people. The third sen-tence describes a looting subevent. Acts of looting and vandalism are not usually considered to be ter-rorism, but in this article it is in the context of accu-sations of terrorist acts by government officials. We have presented a new approach to event extrac-tion that uses three levels of analysis: document genre classification to identify event narrative sto-ries, two types of sentence classifiers, and noun phrase classifiers. A key contribution of our work is the creation of role-specific sentence classifiers that can detect role fillers in secondary contexts that do not directly refer to the event. Another important as-pect of our approach is a two-pronged strategy that handles event narratives differently from other doc-uments. TIER aggressively hunts for role fillers in event narratives, but is conservative about extract-ing information from other documents. This strategy produced improvements in both recall and precision over previous state-of-the-art systems.

This work just scratches the surface of using doc-ument genre identification to improve information extraction accuracy. In future work, we hope to identify additional types of document genre styles and incorporate genre directly into the extraction model. Coreference resolution and discourse anal-ysis will also be important to further improve event extraction performance. We gratefully acknowledge the support of the Na-tional Science Foundation under grant IIS-1018314 and the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime con-tract no. FA8750-09-C-0172. Any opinions, find-ings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the U.S. government.
