 1. Introduction
In recent years, recognition of human actions has been a major concern in computer vision due to its immense applications in the fi eld of autonomous video surveillance, video retrieval and human computer interaction. These applications require methods for recognizing human actions and gestures in various scenarios.
Given a number of pre-de fi ned actions, the action recognition of these pre-existing actions. Many action recognition approaches the initial works by Bobick and Davis (2001) , the extracted silhouettes are used to construct binary motion energy image (MEI) and motion history image (MHI) templates for representing using hidden Markov model. Extracting silhouettes in real-life space  X  time volume over silhouette images ( Gorelick et al., 2007 ;
Yilmaz and Shah, 2008 ). Optical fl ow is another major technique in person-centered images in order to model relative motions readily available motion vectors from the compressed video stream for recognizing actions. Ali and Shah (2010) derive 11 kinematic applied to determine the dominant kinematic modes. The actions
Weinland et al. (2011) have presented a detailed survey on human action recognition.

Most of the action recognition algorithms are bench marked using one of the following publicly available datasets: (i) KTH (iii) IXMAS ( Weinland et al., 2006 ). KTH and Weizmann datasets are captured from a single camera and IXMAS dataset is created using 5 calibrated and synchronized cameras to capture actions in multiple views. Due to the lack of depth information, the actions dynamics with less ambiguity. Hence, the action recognition algorithms developed are mostly view dependent. Since 3-D data acquisition requires special stereo camera setup or expensive image capturing device, rarely 3-D optical fl ow based techniques an expensive SwissRanger SR4000 camera to capture the RGB
Depth database for action recognition. They have used 3-D optical fl ow for recognizing 4 different actions. Stereo camera has been used for detecting human in Nakada et al. (2008) .

However, with the advancement of camera technology, we are now able to capture the depth image that gives us information in the 3rd dimension with which we can represent and recognize actions more accurately, at an affordable cost. In this paper, we propose a method for human action recognition using spatio-views are not required to capture the motion in all directions.
Thus, using depth information, we are able to recognize actions optical fl ow combined with the depth fl ow gives us the complete ette) is extracted easily using the depth image. The 2-D optical fl ow is calculated on the gray scale images masked by the along with the depth images provided by the depth camera to are performed over time, the 3-D optical fl ow between 2 con-so that it contains enough details about an action. We have used motion of an action at different scales. These average motion vectors are used as features for representing the actions. The proposed algorithm has been evaluated on our Video Analytics Lab (VAL) dataset and another publicly available MHAD dataset, which were captured using kinect sensor with RGB and depth informa-tion. The proposed approach can be easily adapted to various applications such as gesture recognition, emotion recognition and gait recognition.

These feature vectors are then used to classify the actions using a Projection Based Learning (PBL) algorithm of a Meta-cognitive Radial Basis Function Network (McRBFN). McRBFN emulates the Nelson and Narens model of human meta-cognition ( Nelson and
Narens, 1980 ), and has 2 components, namely, a cognitive compo-nent and a meta-cognitive component. A radial basis function network with Gaussian activation function at the hidden layer is the cognitive component of McRBFN and a self-regulatory learning mechanism is its meta-cognitive component. McRBFN begins with zero hidden neurons, adds and prunes neurons until an optimum network structure is obtained. The self-regulatory learning mechanism of McRBFN uses the best human learning strategy, namely, self-regulated learning ( Wenden, 1998 ; Rivers, 2001 ; and how-to-learn in a meta-cognitive framework. Based on its used in the learning process (sample learn strategy) or reserved for future use (sample reserve strategy). The sample deletion strategy, sample learn strategy and the sample reserve strategy address the what-to-learn , how-to-learn and when-to-learn com-ponents of meta-cognition. Thus, the meta-cognitive component continuously assesses the knowledge of the cognitive component, identi fi es when a new knowledge is required and controls the learning ability of the cognitive component. Therefore, the net-tation of the training data, and is not over-trained.
During the sample learn strategy, McRBFN either adds a neuron or updates the parameters of the existing neurons. While adding a neuron, the input/hidden layer parameters of the network are fi xed based on the sample overlapping conditions, and the optimal output weights are estimated using a projection based learning algorithm. The problem of estimating the optimal output weights is formulated as a linear programming problem which is then converted to a system of linear equations and solved by the projection based learning algorithm. While solving the system of linear equations, PBL estimates the output weights corresponding to the minimum energy point of the hinge-loss error function. On the other hand, when a sample is used to update the existing network parameters, a recursive least square algorithm is used ( Chong and Zak, 2001 ). The McRBFN using the PBL to address the how-to-learn component of meta-cognition will be hereafter referred as,  X  Projection Based Learning algorithm of a Meta-cognitive Radial Basis Function Network (PBL-McRBFN)  X  .
The performance of PBL-McRBFN in recognizing actions is evaluated by a 10-fold cross validation and subject independent recognition. First, a 10-fold cross validation study is conducted are used to study the action recognition performance of PBL-McRBFN in comparison with Support Vector Machines (SVM) and superior action recognition performance of PBL-McRBFN. The person-independent action recognition performance of the classi-fi ers is studied by training the classi fi ers using 7 subjects and testing its generalization ability using the actions performed by the remaining subject. The results of this study show that PBL-McRBFN is able to generalize actions, independent of the repre-ANOVA test, which indicates the superior performance of the PBL-McRBFN. The performances of the classi fi ers are also veri through the Berkeley Multi-modal Human Action Database (MHAD) ( O fl i et al., 2013 ).

The paper is organized as follows: Section 2 presents the overview of the proposed action recognition model using 3-D optical fl ow features. In Section 3 , the data with 3-D optical features is described and the performance of PBL-McRBFN is studied in comparison to other classi fi ers from the literature. Finally, Section 4 summarizes this study on subject-independent human action recognition using 3-D optical fl ow features. 2. System overview
The overview of the proposed approach with VAL database is images. The 3-D optical fl ow is obtained using the estimated 2-D arranged spatio-temporal windows for representing the actions. These extracted features are then used to train a meta-cognitive radial basis function network using a projection based learning algorithm. We explain each component of Fig. 1 in detail in the following sections.
 2.1. Calibration of kinect The depth camera in kinect has a smaller fi eld of view than the
RGB camera. Hence, the obtained depth image is slightly magni-fi ed and translated with respect to RGB image. In order to use both the RGB and depth image simultaneously, we have to map the pixels in both the images. For calibrating kinect, we have used a fi xed parallelogram. The coordinates of the corners of the paralle-logram are obtained for both depth and RGB image. A warp matrix obtained based on this measurement is utilized for mapping depth image to the corresponding RGB image. 2.2. Depth normalization
The depth information provided by kinect is represented by 11 bits, but processing all the images in 11 bits is computationally expensive. Hence, we have to scale them down to some lower bit numbers, which results in losing fi ner depth information in the region of interest. To overcome both the problems, we fi nd 2 fl exible threshold values in which all the actions can be described completely for all users. The depth information of the farther background or the very close region to camera does not have any contribution in recognizing the actions. Now we scale down the depth values in this region to an 8-bit number which thereby
Fig. 2 illustrates the above normalization process. 2.3. Silhouette extraction using depth image
Detection and elimination of the background using only 2-dimensional (RGB) image is very dif fi cult and inef fi ever, the background can be easily identi fi ed and removed with the help of the depth image. We make use of the fact that the subject is always at a particular distance from the background above which we classify all the pixels as background. Thus, we easily get the depth silhouette of the subject:
D  X  i ; j ; t  X  X  where i , j denote row and column positions of the pixel in the image, t is the time stamp of the temporal frames,  X  is the background threshold depth value, D  X  is the depth image, D is the depth silhouette of the subject:
G  X  i ; j ; t  X  X  image, D is the silhouette of the depth image.

Fig. 3 shows the extracted the binary ( B ) and depth ( D ) silhouettes for a frame.

Then, we use this binary mask ( G ) to extract the silhouette of corresponding RGB image:
I  X   X  i ; j ; t  X  X  where I is the RGB image and I  X  is the silhouette of RGB image.
Executing the aforementioned simple steps, we could easily remove the background and extract only the region of interest for further analysis. 2.4. 3-D optical fl ow estimation This section provides details of 3-D optical fl ow estimation. optical fl ow and corresponding depth images. 2.4.1. 2-D optical fl ow computation
Calculation of 2-D optical fl ow is a well known problem in computer vision. There are many algorithms to compute the optical fl ow, of which we use pyramidal Lucas  X  Kanade algorithm speed and robustness. We obtained the gray scale image from the
RGB image and then computed the 2-D optical fl ow by applying the pyramidal Lucas  X  Kanade algorithm on the silhouette of gray scale images for every action video. 2.4.2. Depth fl ow computation
The depth image provided by the depth camera enables us to calculate the motion vectors in the Z direction also. The depth motion vectors can be easily computed using the 2-D optical and the depth images. The Z motion vectors can then be obtained just by subtracting the depth value of the same point on the subject in 2 consecutive depth frames.
 Let us consider a point in one of the temporal depth frames.
To get the new location of the point in next depth frame, we add the 2-D optical fl ow of that point to its present location ( X Y coordinates) in the current frame. This will give us the new location of that point in the next depth image: x  X  x o  X  M y  X  4  X  y  X  y o  X  M x  X  5  X  and vertical directions, respectively, for the current frame ( frame).

The precision of optical fl ow vectors depends on various factors such as surface texture, occlusion, covering and uncovering of the depth image directly may not give correct depth motion depth frame around the estimated points is considered.
We make a basic practical assumption that the depth of a window to get the average depth value of that part of subject. We then subtract this average value with the depth value of the ( Z direction) motion of that pixel: M frame, D n avg is the average depth value in the immediate small neighborhood of the pixel in second depth image and D (n 1) current depth image under consideration. 2.5. Feature extraction
For the extraction of the features, we follow the approach proposed in Babu and Suresh (2011) . First, a minimum bounding rectangle box that captures the complete motion of an action is obtained. This bounding box is adaptive and depends on the current silhouette image, and is obtained by accumulating the tight bounding box for each action segment.

This bounding box is then hierarchically divided into 54 windows placed symmetrically with respect to the subject ' in hierarchical fashion. The bounding box is divided into 6 6 windows, 3 3 windows, 2 2 windows, 2 1 windows, 1 2 windows and fi nally 1 1 window of equal size. We have then computed the average motion of each window by averaging the non-zero motion vectors of all 3 dimensions in each window. Hence we get the average motion of each window inside the length 162 (54 3) for every frame, where fi rst 54 features representing the average x motion vectors, next 54 features representing the average y motion vectors and the fi nal 54 features representing the average z motion vectors at different contain enough information to represent an action. Hence, we This contains good amount of information about the dynamics of an action. Hence, we have summed up across 8 frames with an the dataset can be represented by f X  u 1 ; c 1  X  ; ... ;  X  u t 3 directions used to represent actions and c t  X   X  1 ; ... ; one of the A actions. In the next section, we present a brief description of the McRBFN classi fi er that is used to map the features to their corresponding actions.
 2.6. Meta-cognitive radial basis function network Let the dataset generated using the procedure described in
Section 2.5 be given by f X  u 1 ; c 1  X  ; ... ;  X  u t ; c t  X   X  R m  X  X  u t  X   X  1 ; ... ; C are its corresponding action class labels. The coded class labels for the action classes are given by o  X  (
The objective of the neural network learning algorithm is to estimate the functional relationship between the action features
In this paper, we use the PBL-McRBFN developed in Babu et al. (2012) . Analogous to the model of human meta-cognition pro-posed by Nelson and Narens (1980) , McRBFN has 2 components, namely, a cognitive component and a meta-cognitive component
For complete details, one may refer to Babu et al. (2012) . 2.6.1. Cognitive component A single hidden layer radial basis function network with a
Gaussian activation function at its hidden layer is the cognitive component of McRBFN. The neurons in the input and output layers of the RBF network are linear.

Without loss of generality, let us assume that the RBF network has K neurons after t 1 samples. The neurons in the hidden layer of McRBFN use the Gaussian activation function and the response h  X  exp where c j  X  R m is the center of the j -hidden neuron and Gaussian width of the j -th hidden neuron.

The neurons in the output layer of the radial basis function network obtain the weighted sum of the hidden layer responses.
Thus, the response of the output neurons is the output of the sample ( b o t l ) is given by b o  X   X  K can be obtained from this output as b c  X  max
Since the hinge loss error function has been shown to estimate the posterior probability more accurately than the mean-square
Suresh et al., 2008 ), PBL-McRBFN also uses the hinge loss error function. The hinge loss error of the t -th sample is given by e  X  8 &lt; :  X  11  X  The maximum absolute hinge error ( E t ) is given by
E  X  max
Projection based learning algorithm : The projection based learn-ing algorithm works on the principle of minimization of energy function and fi nds the network output parameters for which the energy function is minimum, i.e., the network achieves the minimum of the energy function.
 The considered energy function is the sum of squared errors at McRBFN output neurons
J  X   X 
For t training samples, the overall energy function is de
J W  X  X  X  1 2  X  t  X 
 X  the energy function reduces to
J W  X  X  X  1 2  X  t where h i k is the response of the k -th hidden neuron for i -th training sample.

The optimal output weights  X  W n  X  R K n ) are estimated such that the total energy reaches its minimum:
W n  X  arg min and Suresh, 2013 )
W n  X  A 1 B  X  17  X  where the projection matrix A  X  R K K is given by a and the output matrix B  X  R K n is b  X   X  t
Next, we describe the meta-cognitive component of McRBFN and that has been developed using this hinge loss error function. 2.6.2. Meta-cognitive component
The meta-cognitive component has a knowledge about the knowledge of the cognitive component and controls the learning of the cognitive component. It contains a dynamic model of the cognitive component and comprises a self-regulatory learning mechanism to decide what-to-learn , when-to-learn and how-to-learn . As mentioned earlier, the cognitive component of McRBFN begins with zero hidden neurons, and the meta-cognitive compo-nent adds and prunes or updates existing neurons to the cognitive component until an optimum network structure is obtained.
A projection based fast learning algorithm is used to fi x the parameters of the neurons. Based on the error and the distance of the sample from the existing neurons, the meta-cognitive component chooses one of the following strategies for each sample in the dataset:
Sample delete strategy : If the knowledge contained in a sample to address the what-to-learn component of meta-cognition:
If c t  X  X  b c t AND E t  X   X  d ; then ; delete the sample  X  20  X  where  X  d is the delete threshold fi xed at a desired accuracy.
Sample learn strategy : This strategy decides how-to-learn the training sample. Depending on the novelty of knowledge con-tained in the sample, either the neuron growth strategy or the parameter update strategy is chosen.

Neuron growth strategy : When a new training sample has novel knowledge and the estimated class label is different from the actual class label then a new hidden neuron is added to represent the knowledge contained in the sample. The neuron growth criterion is given by
If  X  b
Suresh, 2013 ) and is de fi ned as  X   X  1 where K c is the number of neurons associated with class c , h knowledge measurement threshold and  X  a is the self-adaptive meta-cognitive neuron addition threshold. These thresholds select samples with signi fi cant knowledge for building the network so that the other samples can be used to fi ne tune the network parameters. The neuron addition threshold is self-adapted accord-ing to  X   X   X  X  a  X  X  1  X   X  E t  X  23  X  close to 1.

A training sample that is used to add a neuron may overlap with neurons in other classes or will form a distinct cluster far away from the nearest neuron in the same class. These conditions might affect the classi fi cation performance of a classi cantly. Hence, McRBFN measures the distance from the current sample to the nearest neuron in the inter and intra class while assigning the new neuron parameters. Thus, the parameters of a new hidden neuron are initialized based on the overlapping and distinct cluster criterion.

The nearest hidden neuron in the intra class ( nrS ) and the nearest hidden neuron in the inter class ( nrI )arede fi ned as nrS  X  arg min The Euclidean distances between the new training sample to nrS and nrI are given as d Using the nearest neuron distances, we determine the center and width of the new neuron based on the overlapping/non-over-avoid misclassi fi cation. When there is no overlap of the sample with any neuron in any class, the center and width of the new neuron is initialized as Then, the output weights are estimated using the projection based learning algorithm described below: The size of matrix A is increased from K K to  X  K  X  1  X  X  K  X  1  X  : neurons response for the t -th training sample. a K  X  1  X  R 1 K assigned as a a The size of matrix B is increased from K n to  X  K  X  1  X  n : B where matrix B  X  R K n is updated as B  X  B  X  X  h t  X  T  X  o t  X  T  X  31  X  and b K  X  1  X  R 1 n is a row vector assigned as b t -th sample is added as a hidden neuron which is signi fi different from the existing hidden neurons. After neglecting h vector in Eqs. (27) and (31) the output weights are estimated fi nally as where W K is the output weight matrix for K hidden neurons, and w K  X  1 is the vector of output weights for new hidden neuron.
It must be noted that the fi rst sample is used as the fi the network.
 used for updating the output weights of the cognitive component ( W  X  X  w 1 ; w 2 ; ... ; w K T ) if the following criterion is satis c t  X  b c t AND E t  X   X  where  X  u is the self-adaptive meta-cognitive parameter update  X   X   X  X  u  X  X  1  X   X  E t  X  35  X  is used for updating the output weight parameters, the PBL algorithm updates the output weight as given below. The matrices
A  X  R K K and B  X  R K n are updated as A  X  A  X  X  h t  X  T h t  X  36  X 
B  X  B  X  X  h t  X  T  X  o t  X  T  X  37  X  and the output weights are updated as W  X  W K  X  A 1  X  h t  X  T  X  e t  X  T  X  38  X  current sample knowledge, these samples may be used in later stage.

We summarize the PBL-McRBFN below: the cognitive component  X  b o t  X  using Eqs. (9) and (8) . 2. Estimate the predicted class label of the cognitive component  X  b c
 X  , maximum hinge error ( E t ) and class-wise signi fi cance measures  X   X  c  X  for the new training sample  X  u t  X  using
Eqs. (10) , (12) and (22) . 3. The meta-cognitive component selects one of the following strategies based on the above computed measures: (b) Neuron growth strategy :If  X  b c t  X  c t OR E t  X   X  a (c) Parameters update strategy :If c t  X  X  b c t AND E t  X   X  (d) Sample reserve strategy : When the new sample does not 4. The cognitive component executes the above selected strategy. 5. Continue steps 1  X  4 until there are no more samples in the training dataset. 3. Results and discussions
In this section, we evaluate the performance of PBL-McRBFN in recognizing actions using 3-dimensional features. Two different studies are conducted: a 10-fold cross-validation study and a subject-independent action recognition study. In both these stu-dies, the performance of PBL-McRBFN is compared with that of a optimal number of support vectors of SVM is obtained by optimiz-ing c and  X  in LIBSVM and the number of hidden neurons in ELM is obtained by the constructive-destructive procedure described in
Suresh et al. (2003) . The following measures are used to compare the performances of these classi fi ers:
Average classi fi cation ef fi ciency  X   X  a  X  :  X   X  1 n  X  n the training/testing dataset.

Overall classi fi cation ef fi ciency  X   X  o  X  :  X   X 
Geometric mean ef fi ciency  X   X  g  X  :  X   X  independent action recognition study. In these studies, the per-formances of the classi fi ers are compared using the performance
ANOVA test ( Japkowicz and Shah, 2011 ) to compare the perfor-
ANOVA measure compares the mean of individual experimental one-way ANOVA test, then pair-wise post hoc should be conducted
In this paper, a parametric Dunnett test is used to conduct the pair-wise comparison using the PBL-McRBFN classi fi er as the control.

Finally, to highlight the essence of 3-D features, the perfor-features is compared with its performance using 2-D features. 3.1. Dataset
The proposed approach is evaluated using 2 datasets, namely, the Video Analytics Lab (VAL) database 1 and the Berkeley Multi-database, recorded using the kinect in static surrounding condi-tions, has been generated by us. Here, both the depth and RGB images are recorded at an average rate of 30 frames per second.
The depth images are available as 11 bit images, but stored as 16 bit images. The resolutions of both depth and RGB images are 640 480. The kinect is placed at a fi xed height from the bowling, jumping, boxing, stretching. Each action is performed by 8 subjects for approximately 3 times. The number of frames varies depending upon the speed of the person. Fig. 5 shows the snap-shot of some of the actions from our database.

The MHAD database that contains 11 actions performed by 12 female subjects is the other dataset used in the study. The 11 actions include: jumping, jumping jacks, bending, punching, waving 2 hands, waving one hand, clapping, throwing, sit down/ stand up, sit down and stand up. The database was captured by 5 different systems: optical motion capture system, 4 multi-view accelerometers and 4 microphones. In our experiments, we have used only the information obtained from a single kinect camera for recognizing actions. 3.2. Performance study: 10-fold cross-validation study all the subjects are randomly selected for developing the classi and the remaining 25% of the samples in each action are used for testing the classi fi er. This approach is referred to as, 10-fold cross validation study for the VAL database and the MHAD. 3.2.1. VAL database
We present the results of the 10-fold cross validation study for the PBL-McRBFN classi fi er outperforms ELM and SVM classi better than SVM classi fi er, and at least 6% better than ELM classi fi er in recognizing the human actions.

Figs. 6 and 7 give the neuron history and the sample deletion cognitive component adds neurons to PBL-McRBFN during the
McRBFN deletes 34 samples that are similar to the knowledge acquired by the network. It can also be seen that the sample deletion is more pronounced towards the end of the training.
Hence, it can be observed that PBL-McRBFN has approximated the knowledge dynamics in the training dataset ef fi ciently.
The F -score based on the one-way ANOVA test on the 3 classi-fi ers  X  550.1177. This is greater than the F -statistic at 95% con ( F
The observed t obtained from the Dunnett test by comparing against SVM and ELM are 15.1705 and 33.1307, respectively. However, the critical t value is 2 : 40  X  t 3 ; 18 ; 0 : 05 be inferred that the PBL-McRBFN classi fi er outperforms SVM and
ELM classi fi ers, signi fi cantly. 3.2.2. MHAD the MHAD. From the table, it can be observed that the PBL-McRBFN classi using 3-D features by at least 4% and 13%, respectively.
The F -score based on the one-way ANOVA test on the 3 classi-fi ers  X  26.8546, which is greater than the F -statistic at 95% con level ( F 2 ; 18 ; 0 : 05 is 4.560), i.e., 26 : 8546 4 4 :
The observed t obtained from the Dunnett test by comparing against SVM and ELM are 2.4263 and 6.8557, respectively. How-inferred that the PBL-McRBFN classi fi er outperforms SVM and ELM classi fi ers, signi fi cantly. 3.3. Performance study: subject-independent action recognition study
In the subject-independent action recognition study, the actions performed by all subjects except one are used to develop the classi fi ers and the generalization ability of the classi tested using the actions performed by the untrained subject. 3.3.1. VAL database
Table 3 presents the testing ef fi ciencies of the 3 classi namely, SVM, ELM and PBL-McRBFN for the subject-independent action recognition study. From the performance results, it can be observed that the overall ef fi ciency of PBL-McRBFN classi respectively. Further, the testing geometric mean accuracy of the and that of the ELM classi fi ers is 0 when subjects 3 and 5 are fewer samples in this class. However, the PBL-McRBFN classi able to recognize all the 8 actions, even when the subject is not represented in the training dataset and when the sample imbal-ance is high. Hence, it can be inferred that PBL-McRBFN can perform person independent action recognition using 3-D fea-tures, ef fi ciently.

The F -score based on the one-way ANOVA test on the 3 classi-fi ers  X   X  o  X  for the leave-one out cross validation test using 3-D features is 9.5073, which is greater than the F -statistic at 95% con fi dence level ( F 2 ; 14 ; 0 : 05 is 3.739), i.e., 9 equality of means hypothesis can be rejected at 95% con fi level. As the equality hypothesis is rejected, we conduct the on this test, the observed t obtained by comparing against SVM and ELM are 2.4874 and 4.3454, respectively, while the critical one out cross validation study that the PBL-McRBFN classi performs signi fi cantly better than the SVM and ELM classi 3.3.2. MHAD
Table 4 presents the testing ef fi ciencies of the 3 classi namely, SVM, ELM and PBL-McRBFN for the subject-independent action recognition study using the MHAD. From the performance results, it can be observed that the overall ef fi ciency of PBL-rejected, we conduct the Dunnett test using the PBL-McRBFN by comparing against SVM and ELM are 0.8736 and 3.2620, McRBFN is greater than that of SVM by 2.21%, the statistical difference in the performances of these classi fi ers is not very signi fi cant. However, it must be noted that in the SVM classi the training samples are used as support vectors, which might affect the generalization performance signi fi cantly. 3.4. Performance study: comparison using 2-D and 3-D features
Next, to show the advantage of using 3-D features, we conduct the 10-fold cross validation study and the subject-independent action recognition study on the best performing PBL-McRBFN using 3-D and 2-D features. 3.4.1. VAL database
The average of the overall, average and geometric mean ef fi ciencies of the studies using 2-D and 3-D features of the VAL that the performance of PBL-McRBFN is better while using 3-D features than that using 2-D feature set. The performance of the action recognition task using 3-D features of VAL database has improved at least by 4%, compared to that obtained by 2-D substantial improvement in performance while using 3-D features over 2-D features in the subject-independent action recognition study. The improvement in performance is at least 17%. From the performance results of both the studies, it can be inferred that sensitive to the appearance of the person involved, compared to using only 2-D features for action recognition. 3.4.2. MHAD
The overall ef fi ciency of the performance study of PBL-McRBFN using 2-D and 3-D features of the MHAD is presented in Table 6 .
From the table, it can be observed that the PBL-McRBFN performs ef fi fi cation using 2-D features. The improvement in performance using 3-D features over 2-D features is at least 12.12% and 3.94% in the subject independent action recognition and 10-fold cross validation, respectively.

Thus, the following observations can be made from the perfor-mance results presented in this section:
PBL-McRBFN outperforms SVM and ELM in recognizing actions using 3-D features.

The PBL-McRBFN shows better performance in recognizing actions using 3-D optical fl ow based features in subject-independent scenario.

The action recognition performance of PBL-McRBFN is much better while using 3-D features, as compared to that of using 2-D features. 4. Conclusion
This paper presents an approach for action recognition using 3-D features obtained from the kinect sensor. The 3-D optical is estimated from 2-D optical fl ow and the depth information.
Thus, the 3-D optical fl ow feature captures the dynamics of the actions in space  X  time. The 3-D features are then used to train support vector machine, extreme learning machine and a meta-learning algorithm. The performances of these classi fi ers are compared using a 10-fold cross-validation study and a subject-independent action recognition study. Performance study on these classi fi ers shows that the PBL-McRBFN classi fi er outperforms the SVM and ELM classi fi ers. A statistical analysis using a one-way ANOVA test con fi rms the results from the quantitative analysis. the best performing PBL-McRBFN classi fi ers with and without the the leave-one out cross validation study and the 10-fold cross validation study. The proposed approach is evaluated using pub-licly available VAL and MHAD databases. The results indicate that the depth fl ow features help to make the action recognition task independent of the person. The proposed approach can be adapted to various other applications including gesture recognition, emo-tion recognition and gait recognition.
 Acknowledgements
The authors wish to express grateful thanks to the referees for their useful comments and suggestions to improve the presenta-tion of this paper.
 References
