 1. Introduction
Relation extraction is to find various predefined semantic relations between pairs of entities in text. The research on relation extraction has been promoted by the Message Understanding Conferences (MUCs) ( MUC, 1987 X 1998 ) and the NIST Automatic Content Extraction (ACE) program ( ACE, 2002 X 2006 ).
According to the ACE Program, an entity is an object or set of objects in the world and a relation is an explic-itly or implicitly stated relationship among entities. For example, the sentence  X  X  X ill Gates is chairman and chief software architect of Microsoft Corporation. X  X  conveys the ACE-style relation  X  X  X MPLOYMENT.exec X  X  between the entities  X  X  X ill Gates X  X  (PERSON.Name) and  X  X  X icrosoft Corporation X  X  (ORGANIZATION.Com-mercial). Extraction of such kinds of semantic relations between entities can be very useful for applications such as question answering, e.g. to answer the query  X  X  X ho is the president of the United States? X  X , and infor-relationship with the country  X  X  X he United States X  X .

Much research work has been done for relation extraction. Prior feature-based methods for this task ( Kambhatla, 2004; Zhou, Su, Zhang, &amp; Zhang, 2005 ) employed a large amount of diverse linguistic fea-tures, varying from lexical knowledge, entity mention information to syntactic parse trees, dependency trees and semantic features. Since a parse tree contains rich syntactic structured information, in principle, the structured features extracted from a parse tree should contribute much to performance improvement in rela-tion extraction. However it is reported ( Kambhatla, 2004; Zhou et al., 2005 ) that syntactic structured fea-tures contributes little to performance improvement. This may be mainly due to the fact that the hierarchical structured information in a parse tree is hard to be explicitly represented by a vector of linear features. As an alternative, kernel methods ( Collins &amp; Duffy, 2001 ) provide an elegant solution to explore implicitly structured features by directly computing the similarity between two trees. However, the sole two reported dependency tree kernels in relation extraction on the ACE corpora ( Bunescu &amp; Mooney, 2005;
Culotta &amp; Sorensen, 2004 ) showed much lower performance than the feature-based methods. One may ask: Are the structured features in syntactic parse trees useful in relation extraction? Can tree kernel meth-ods effectively capture the structured features as well as other various flat features that have been proven useful in the feature-based methods?
In this paper, we demonstrate the effectiveness of the syntactic structured features in relation extraction and study how to capture such features via a convolution tree kernel ( Collins &amp; Duffy, 2001 ) together with support vector machines (SVM) ( Vapnik, 1998 ). We also study how to select the proper feature space (e.g. the set of sub-trees to represent relation instances) to optimize the system performance. The experimen-tal results show that the convolution tree kernel can achieve comparable performance with previous best-reported feature-based methods. It also shows that our kernel method significantly outperforms previ-ous two dependency tree kernels ( Bunescu &amp; Mooney, 2005; Culotta &amp; Sorensen, 2004 ) for relation extraction.

Moreover, this paper proposes a composite kernel for relation extraction by combining the convolution tree kernel with a simple linear kernel. Our study demonstrates that the composite kernel is very effective in relation extraction. It also shows that, without extensive feature engineering, the composite kernel can not only capture most of the flat features used in previous work but also exploit the useful syntactic structured features effectively. An advantage of our method is that the composite kernel can easily cover more types of knowledge by introducing more kernels. Evaluation shows that our method outperforms previous best-reported methods and significantly outperforms previous kernel methods due to its effective exploration of the syntactic structured features.

The rest of the paper is organized as follows. In Section 2 , we review previous work. Then, a brief intro-duction about kernel-based relation extraction is given in Section 3 . Section 4 discusses different structured feature spaces over parse trees and the convolution tree kernel for relation extraction while the composite ker-nel is described in Section 5 . Section 6 shows the experimental results while further comparison of our work with related work is presented in Section 7 . Finally, we conclude our work in Section 8 . 2. Related work
The task of relation extraction was first introduced as a part of the Template Element task in MUC6 and formulated as the Template Relation task in MUC7 ( MUC, 1987 X 1998 ). Since then, many methods on rela-tion extraction, such as generative models ( Miller, Fox, Ramshaw, &amp; Weischedel, 2000, 19871998 ), feature-2004; Zelenko, Aone, &amp; Richardella, 2003 ) have been proposed in the literature.

Miller et al. (2000) address the task of relation extraction from the statistical parsing viewpoint. They aug-ment syntactic full parse trees with semantic information corresponding to entities and relations, and build generative models to integrate various tasks such as POS tagging, named entity recognition, template element scale of annotated corpus.

For the feature-based methods, Kambhatla (2004) employs Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text in relation extraction. Zhou et al. (2005) explore various features in relation extraction using SVM. They conduct exhaustive experiments to investigate the incorporation and the individual contribution of diverse features. They report that chunk-ing information contributes to most of the performance improvement from the syntactic aspect. The fea-tures used in Kambhatla (2004) and Zhou et al. (2005) have to be selected and carefully calibrated manually. Kambhatla (2004) uses the path of non-terminals connecting two entities in a parse tree as the parse tree features while Zhou et al. (2005) introduce additional chunking features to enhance the parse tree features. However, the hierarchical structured information in the parse trees is not well preserved in their parse tree features.

As an alternative to the feature-based methods, kernel methods ( Haussler, 1999 ) have been proposed to implicitly explore features in a high-dimensional space by employing a kernel to calculate the similarity between two objects directly. In particular, the kernel methods could be very effective at reducing the burden of feature engineering for structured objects in NLP research. This is because a kernel can measure the similarity between two discrete structured objects directly using the original representation of the objects instead of explicitly enumerating their features. In relation extraction, typical work using kernel methods includes Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005) .
Zelenko et al. (2003) develop a parse tree kernel for relation extraction. Their tree kernel is recursively defined in a top-down manner, matching nodes from roots to leaf nodes. For each pair of matching nodes, a subsequence kernel on their child nodes is invoked, which matches either contiguous or sparse subsequences of nodes. Culotta and Sorensen (2004) generalize this kernel to estimate the similarity between dependency trees. One may note that their tree kernel requires the matchable nodes must be at the same depth counting from the root node. This is a strong constraint on the matching of syntax so it is not surprising that the model has good precision but very low recall on the ACE corpora. In addition, according to the top-down node-matching mechanism of the kernel, once a node is not matchable with any node in the same layer in another another tree.

Bunescu and Mooney (2005) propose a shortest path dependency kernel in relation extraction. They argue that the information to model a relationship between entities is typically captured by the shortest path between the two entities in the dependency graph. Their kernel is very straightforward. It just sums up the number of common word classes at each position in the two paths. We notice that one issue of this kernel is that they limit the two paths must have the same length, otherwise the kernel similarity score is zero. Therefore, although this kernel shows non-trivial performance improvement than that of Culotta and Sorensen (2004) , the constraint makes the two dependency kernels share the similar behavior: good precision but much lower recall on the ACE corpora.

Zhao and Grishman (2005) define a feature-based composite kernel to integrate diverse features for rela-tion extraction. Their kernel displays very good performance on the 2004 version of ACE corpora. Since this is a feature-based kernel, all the features used in the kernel have to be explicitly enumerated. Similar to the feature-based methods, they also represent the tree feature as a link path between two entities. There-fore, we wonder whether their performance improvement is mainly due to the explicit incorporation of diverse linguistic features instead of the kernel method itself. Here, we would classify their method into the feature-based methods since their kernels can be easily represented by the dot products between explicit feature vectors.

The above discussion suggests that the hierarchical structured features in a parse tree may not be fully utilized in previous work, no matter whether feature-based or kernel-based. We believe that the tree structure features could play a more important role than that reported in previous work. Since convolution kernels ( Haussler, 1999 ) 1 aim to capture structured information in terms of sub-structures, which providing
Duffy, 2001 ) to explore syntactic structured features in relation extraction. To our knowledge, convolu-tion kernels have not been explored in relation extraction. Furthermore, in order to integrate both flat and structured features for relation extraction, two composite kernels are proposed and studied in this paper. 3. Kernel-based classifiers for relation extraction In this paper, relation extraction is re-cast as a classification problem using a machine learning algorithm.
In training, a classifier machine learning algorithm uses the annotated relation instances to learn a classifier non-relation) and thus extract possible relations.

Most learning algorithms rely on feature-based representation of input instances. That is, an annotated instance is transformed into a collection of features f 1 vector. However, in many NLP problems, it is computationally infeasible to generate features involving struc-tured information or long-distance dependencies. For example, one cannot enumerate efficiently all the sub-tree features for a full parse tree.

As an alternative to the feature-based methods, kernel methods ( Haussler, 1999 ) can implicitly explore tion satisfying the properties of being symmetric 2 and positive-definite. can be proven that a kernel function can measure the similarity between two input instances by computing
Duffy, 2001 ) to model syntactic structured features for relation extraction and further propose two composite kernels to integrate both flat and structured features for relation extraction.

Many classifiers, such as SVM, KNN and voted perceptrons, can be used with kernels by replacing the dot product with a kernel function. In this paper, we select SVM as the classifier since SVM represents the state-of-the-art in the machine learning research community, and there are good implementations of the algorithm available. In our implementation, we use the binary-class SVMLight deleveloped by Joachims (1998) . SVM is a supervised machine learning technique motivated by the statistical learning theory ( Vap-nik, 1998 ). Based on the structural risk minimization of the statistical learning theory, SVM seeks an opti-mal separating hyper-plane to divide the training examples into two classes and make decisions based on classifier. Therefore, we must extend SVM to multi-class (e.g. K ) classification for the ACE relation extrac-class from all others, instead of the pairwise strategy, which builds K pairs of classes. The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. 4. Tree kernels for relation extraction
In this section, we discuss the convolution tree kernel associated with different implicit relation feature spaces. In Section 4.1 , we define seven different relation feature spaces over parse trees. In Section 4.2 ,we introduce a convolution tree kernel for relation extraction. 4.1. Feature spaces 4
A relation instance is encapsulated by a parse tree. Thus, it is critical to understand which portion of a parse tree is important in kernel calculation. For this purpose, seven different feature spaces are defined as follows: (1) Minimum complete tree (MCT) : The complete sub-tree rooted by the nearest common ancestor of the (2) Path-enclosed tree (PT) : The smallest common sub-tree including the two entities. In other words, the (3) Chunking tree (CT) : The base phrase list extracted from the PT by pruning out all the internal structures (4) Context-sensitive path tree (CPT) : The PT extending with the 1st left sibling of the node of entity 1 and (5) Context-sensitive chunking tree (CCT) : The CT extending with the 1st left sibling of the node of entity 1 (6) Flattened PT (FPT) : The PT with (1) the single in and out arcs of non-terminal nodes (except POS (7) Flattened CPT (FCPT) : The CPT with (1) the single in and out arcs of non-terminal nodes (except POS testified he was powerless to stop the merger of an estimated 2000 ethnic Tutsi X  X  in the district of Tawba . X  X , excerpted from the 2003 version of ACE corpora, where an ACE-defined relation  X  X  X T.LOCATED X  X  exists between the entities  X  X  Tutsi X  X   X  X  (PER) and  X  X  district  X  X  (GPE). We use Charniak X  X  parser ( Charniak, 2001 )to parse the example sentence. Due to space limitation, we do not show the whole parse tree of the entire sentence here. In Fig. 1 , (1) T 1 is MCT for the relation instance, where the sub-structure circled by a dashed line is PT, which is also (2) T 3 is CT. By comparing the performance of T 2 and T 3 (3) T 4 is CPT, where the two structures circled by dashed lines are included as the context to T (4) T 5 is CCT, where the additional context structures are also circled by dashed lines. This is to study (5) Two flattened trees as given in T 6 and T 7 are also explored. The two circled nodes in T 4.2. Convolution tree kernel
Given the various feature spaces discussed in the previous subsection, we now study how to measure the similarity between two trees using convolution tree kernel in these feature spaces.
 A convolution kernel ( Haussler, 1999 ) aims to capture structured information in terms of sub-structures.
As a specialized convolution kernel, the convolution tree kernel suggested by Collins and Duffy (2001) counts the number of common sub-trees (sub-structures) as the syntactic structured similarity between two parse of each sub-tree type (regardless of its ancestors): where # subtree i ( T ) is the occurrence number of the i th sub-tree type ( subtree kernel K ( T 1 , T 2 ) to calculate the dot product between the above high-dimensional vectors implicitly where N 1 and N 2 are the sets of nodes in trees T 1 and T subtree i occurs with root at node n and zero otherwise, and D ( n rooted at n 1 and n 2 , i.e.
 where D ( n 1 , n 2 ) can be computed by the following recursive rules:
Rule 1 : if the productions (CFG rules) at n 1 and n 2 are different, D ( n
Rule 2 : else if both n 1 and n 2 are pre-terminals (POS tags), D ( n Rule 3 : else, D  X  n 1 ; n 2  X  X  k
The recursive rule (3) holds because given two nodes with the same children, one can construct common sub-trees using these children and common sub-trees of further offspring. The time complexity for computing this kernel is O( j N 1 j  X  j N 2 j ).
 5. Composite kernels for relation extraction
Our proposed composite kernels integrate the convolution tree kernel described as above with a linear entity-based kernel in order to capture both structured and flat features for relation extraction.
The linear entity-based kernel is based on entity-related features as defined in ACE program. For example, the ACE 2003 data defines four kinds of entity features: entity headword, entity type and subtype (only for
GPE 5 ), and mention type while the ACE 2004 data makes some modifications and introduces a new feature  X  X  X DC mention type X  X . Our statistics on the ACE data reveals that the entity features impose a strong con-straint on relation types. Therefore, we design a linear kernel to capture explicitly such features: where R 1 and R 2 stands for two relation instances, E i means the i th entity of a relation instance, and K a simple kernel function over the features of entities: kernel since it simply calculates the dot product of the entity feature vectors.

Given the above linear entity-based kernel, two composite kernels are explored in this paper: (1) Linear combination (2) Polynomial expansion
The polynomial expansion aims to explore the entity bi-gram features, esp. the combined features from the kernels, they are normalized before combination. This can avoid one kernel value being overwhelmed by that of another one.

Since a kernel function set is closed under normalization, polynomial expansion and linear combination ( Scho  X  lkopf &amp; Smola, 2001 ), the two composite kernels are also proper kernels. 6. Experimentation
The main aim of our experiment is to verify the effectiveness of using richer syntactic structures and the convolution tree kernel in relation extraction. Moreover, we also evaluate the effectiveness of the two pro-posed composite kernels. 6.1. Experimental setting We use the English portion of both the ACE 2003 and 2004 corpora from LDC in our experiments. These
ACE corpora are gathered from various newspapers, newswire and broadcasts. In the ACE 2003 data (LDC2003T11: 422/97 training/testing documents and LDC2004T09: 252/0 training/testing documents), the training set consists of 674 (422 + 252) documents and 9683 relation instances while the test set consists of 97 documents and 1386 relation instances. The ACE 2003 data defines 5 entity types, 5 major relation types and 24 relation subtypes. Table 1 lists the types and subtypes of relations for the ACE 2003 data, along with their frequencies of occurrence in the training set. It shows that this ACE 2003 data suffers from a small amount of annotated data for a few subtypes such as the subtype  X  X  X ounder X  X  under the type  X  X  X OLE X  X . It also shows that the ACE 2003 task defines some difficult subtypes such as the subtypes  X  X  X ased-In X  X ,  X  X  X ocated X  X  and  X  X  X esidence X  X  under the type  X  X  X T X  X , which are difficult even for human experts to differentiate. The ACE 2004 data (LDC2005T09) contains 451 documents and 5702 relation instances. It redefines 7 entity types, 7 major relation types and 23 subtypes. Since Zhao and Grishman (2005) use a 5-fold cross-validation on a subset of the 2004 data (newswire and broadcast news domains, containing 348 documents and 4400 relation instances), for comparison, we use the same setting for the ACE 2004 data.

Both corpora are parsed using Charniak X  X  parser ( Charniak, 2001 ). We iterate over all pairs of entity men-tions occurring in the same sentence to generate potential relation instances. ment order of the two mentions involved. For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2-ROLE.Citizen-Of-m1. Note that, in the 2003 data, 6 of these 24 relation subtypes are symmetric:  X  X  X EAR.Relative-Location X  X ,  X  X  X OCIAL.Associate X  X ,  X  X  X OCIAL.-
Other-Relative X  X ,  X  X  X OCIAL.Other-Professional X  X ,  X  X  X OCIAL.Sibling X  X , and  X  X  X OCIAL.Spouse X  X . In this way, we model relation extraction as a multi-class classification task with 43 (24  X  2 6 + 1) classes, two for each relation subtype (except the above 6 symmetric subtypes) and a  X  X  X ONE X  X  class for the case where the two mentions are not related. For the ACE RDC 2004 task, 6 of these 23 relation subtypes are symmetric:  X  X  X HYS.Near X  X ,  X  X  X ER-SOC.Business X  X ,  X  X  X ER-SOC.Family X  X ,  X  X  X ER-SOC.Other X  X ,  X  X  X MP-ORG.Partner X  X , and  X  X  X MP-ORG.Other X  X . In this way, we model relation extraction as a multi-class classification task with 41 (23  X  2 6 + 1) classes, two for each relation subtype (except the above 6 symmetric subtypes) and a  X  X  X ONE X  X  class for the case where the two mentions are not related. As discussed in Section 3 , SVM is selected as our kernel classifier. The training parameters are chosen using cross-validation ( C = 2.4 (SVM); k = 0.4(tree kernel)). In our implementation, we use SVMLight ( Joachims, 1998 ) and Tree Kernel Tools ( Mos-chitti, 2004 ) while Precision (P), Recall (R) and F-measure (F) are adopted to measure the performance. 6.2. Experimental results
In this subsection, we report the experimental results of different kernel setups. 6.2.1. Tree kernel over different feature spaces andsoonasshownin Fig. 1 ( Fig. 1 is the original entity-inclusive tree)) with  X  X  X P X  X . Table 2 compares the per-
Overall, the seven different feature spaces are all somewhat effective for relation extraction. This suggests that syntactic structured information has good predication power in relation extraction and the syntactic structured information can be well captured by the tree kernel.

MCT performs much worse than the others. The reasons may be that MCT includes too much left and right contextual information, which may introduce many noisy features and cause over-fitting (high precision and very low recall as shown in Table 2 ). This suggests that only keeping the complete (not partial) pro-duction rules in MCT does harm performance.

PT achieves best performance. This means that only keeping the portion of a parse tree enclosed by the shortest path between entities can model relations better than all others. This may be due to that most sig-nificant information is with PT and including contextual information may introduce too much noise.
Although context may include some useful information, it is still an open question on how to utilize cor-rectly such useful information in the tree kernel for relation extraction.

The performance of using CT drops by 5.8 in F-measure compared with that of using PT. This suggests that the middle and high-level structures beyond chunking are also very useful for relation extraction. The two context-sensitive trees (CPT and CCT) show lower performance than the corresponding original
PT and CT. In some cases (e.g. in sentence  X  X  X he merge of company A and company B ...  X  X ,  X  X  X erge X  X  is the context word), the context information is helpful. However, the effective scope of context is hard to deter-mine given the complexity and variability of natural languages.

The two flattened trees (FPT and FCPT) perform a little bit worse than the original trees. This suggests that the internal structures represented by single non-terminal nodes are useful for relation extraction.
Evaluation on the ACE 2004 data also shows that PT achieves best performance (72.5/56.7/63.6 in P/R/F) when using the parse tree structured information only. 9 More evaluations with the entity type and order infor-mation incorporated into tree nodes ( X  X  X 1-PER X  X ,  X  X  X 2-PER X  X  and  X  X  X -GPE X  X  as shown in Fig. 1 ) also show that PT performs best with 76.1/62.6/68.7 in P/R/F on the 2003 data and 74.1/62.4/67.7 in P/R/F on the 2004 data.
We note that the performance improvement by incorporating the entity info into tree nodes is significant. This suggests that the simple entity information is very useful in relation extraction. 6.2.2. Composite kernels Table 3 compares the performance of different kernel setups on the ACE major types. It clearly shows that:
The composite kernels achieve significant performance improvement over the two individual kernels. This indicates that the flat and the structured features are complementary and the composite kernels can well integrate them.

The composite kernel via the polynomial expansion outperforms the one via the linear combination by 2 in F-measure (72.1 vs. 70.1 and 70.9 vs. 69.1). It suggests that the bi-gram features are very useful.
The entity features are quite useful, which can achieve F-measures of 54.4/41.0 (56.2/48.2 with polynomial expansion d = 2) alone and can boost the performance largely by 7 (70.1 X 63.2/69.1 X 61.9) in F-measure when combining with the tree kernel. Table 7 compares the performance of different d  X  X  when using the entity kernel only. It shows that d = 2 achieves best performance while d = 4 performs even worse than d = 1. Another interesting observation is that precision increases and recall drops when d increases. This means that the n -gram entity features may lead to over-fitting when d &gt;2.

It is interesting that the ACE 2004 data shows consistent better performance on all setups than the 2003 data although the ACE 2003 data is two times larger than the ACE 2004 data. This may be due to two reasons: (1) The ACE 2004 data defines two new entity types and redefines the relation types and subtypes in order to reduce the inconsistency between LDC annotators; (2) More importantly, the ACE 2004 data defines 43 entity subtypes while there are only 3 subtypes in the 2003 data. For example, the entity kernel using the polynomial kernel performs significantly better by 8.0 (56.2 X 48.2) in F-measure on the subtypes of the 2004 data than that on the 2003 data.

Our composite kernel can achieve 77.3/65.6/70.9 and 76.1/68.4/72.1 in P/R/F over the ACE 2003 and 2004 major types, respectively. 6.2.3. Performance comparison
Tables 4 and 5 compare our method with previous work on the ACE 2003 and 2004 data, respectively. They show that our method outperforms previous methods and significantly outperforms previous two dependency path ( Bunescu &amp; Mooney, 2005 ) lack internal hierarchical structured information, so their corresponding kernels can only carry out node-matching directly over the nodes with word tokens; (2) the parse tree kernel layer and ancestors for the matchable nodes and identical length of two shortest paths, as discussed in Section 2 ).

The above experiments verify the effectiveness of our kernel methods for relation extraction. They suggest extraction. 6.2.4. Error analysis
Table 6 reports the error distribution of the polynomial composite kernel over the major types on the ACE data. It shows that 83.5% (198 + 115/198 + 115 + 62)/85. 8% (416 + 171/416 + 171 + 96) of the errors result from relation detection and only 16.5%/14.2% of the errors result from relation characterization. This may be due to data imbalance and sparseness issues since we find that the negative samples are eight times more than the positive samples in the training set. Nevertheless, it clearly directs our future work. 7. Discussion
In this section, we compare our method with previous work from feature engineering viewpoint and report some other observations and issues in our experiments. 7.1. Comparison with feature-based methods
It would be interesting to review the difference between our convolution tree kernel-based method and fea-ture-based methods. The basic difference between them lies in the relation instance representation (parse tree vs . feature vector) and the similarity calculation mechanism (kernel function vs . dot product). A relation ture-based methods. Moreover, our method estimates the similarity between two relation instances by only counting the number of common sub-structures while the feature-based methods calculate the dot product between the feature vectors directly. The main difference is different feature spaces. Regarding the parse tree we consider the entire sub-tree types and their occurring frequencies. In this way, the parse tree-related fea-tures (the path features and the chunking features) used in the feature-based methods can be embedded (as a subset) in our feature space. Moreover, the in-between word features and the entity-related features used in the feature-based methods can be also captured by the tree kernel and the linear entity-based kernel, respec-tively. Therefore our method has the potential of effectively capturing not only most of previous flat features but also useful syntactic structured features. 7.2. Comparison with previous kernel-based methods
It is also worth comparing our method with previous kernel-based methods. Since our method only counts the occurrence of each sub-tree without considering the layer and the ancestors of the root node of the sub-tree, our method is not limited by the constraints (identical layer and ancestors for the matchable nodes, as discussed in Section 2 )in Culotta and Sorensen (2004) . Moreover, the difference between our method and
Bunescu and Mooney (2005) is that their kernel is defined on the shortest path between two entities instead of the entire sub-trees. However, the path does not maintain the hierarchical structured information. In addi-tion, their kernel requires that the two paths should have the same length, which is proven empirically too strict. Finally, compared to Zhao and Grishman X  X  kernel, our method directly uses the original representation of a parse tree while they flatten a parse tree into a link and a path instead. 7.3. Computational issue
The recursively defined convolution tree kernel-based method is much slower compared to feature-based methods. In this paper, the computational issue is solved in three ways. First, the inclusion of the linear entity-based kernel makes the composite kernel converge fast. Furthermore, we find that the small portion (PT) of a full parse tree can effectively represent a relation instance. This significantly improves the speed.
Finally, the parse tree kernel requires exact match between two sub-trees, which normally does not occur very to linear O( j N 1 j + j N 2 j ), rather than O( j N 1 j *
RAM, our system only takes about 110 min and 30 min to do training on the ACE 2003 ( 77k training instances) and 2004 ( 33k training instances) data, respectively. 7.4. Further improvement
One of the potential problems in the parse tree kernel is that it carries out exact matches between sub-trees, tags (for example, the variations of a verb (i.e. go, went, gone)). To some degree, it could possibly lead to over-fitting and compromise the performance. However, the above issues can be handled by allowing grammar-driven partial rule matching and other approximate matching mechanisms in the parse tree kernel calculation.

Finally, it is worth noting that, by introducing more individual kernels, our method can easily scale to cover more features from a multitude of sources (e.g. Wordnet, gazetteers, etc.) that can be brought to bear on the task of relation extraction. In addition, we can also easily implement the feature weighting scheme by adjust-ing Eq. (5) and the rule (2) in calculating D ( n 1 , n 2 8. Conclusion and future work
Kernel functions have nice properties. In this paper, we explore the syntactic structured features using con-volution kernels over parse trees in relation extraction. Evaluation shows that: (1) the relations between enti-ties can be well represented by carefully calibrating effective portions of parse trees; (2) the hierarchical structured features embedded in a parse tree are particularly effective in relation extraction; (3) the convolu-tion tree kernel can effectively capture the syntactic structured features in relation extraction. Moreover, we have designed a composite kernel in relation extraction to resolve the inherent computational problem in tree kernels. Benefiting from the nice properties of the kernel methods, the composite kernel could well explore and combine the flat features and the structured syntactic features, and therefore outperforms previous best-reported feature-based methods on the ACE corpora.

To our knowledge, this is the first research to demonstrate that, without extensive feature engineering, an individual tree kernel can achieve comparable performance with the feature-based methods. This shows that the syntactic features embedded in a parse tree are particularly useful in relation extraction and which can be selecting effective portions of parse trees in kernel calculations) is very important for relation extraction.
The immediate extension of our work is to improve the accuracy of relation detection. We may adopt a issues. We may integrate more features (such as head words or WordNet semantics) into nodes of parse trees.
We can also benefit from machine learning algorithms to study how to solve the data imbalance and sparse-ness issues from the learning algorithm viewpoint. In the future, we would like to test our algorithm on the other version of the ACE corpora and to develop fast algorithm ( Vishwanathan &amp; Smola, 2002 ) to speed up the training and testing process of convolution kernels. Moreover, we will also want to design a more flex-ible tree kernel for more accurate similarity measure between parse trees.
 Acknowledgements
We would like to thank Dr. Alessandro Moschitti for his help in using his Tree Kernel Toolkits and fine-tuning the system. This research (for ZHOU GuoDong only) is supported by Project 60673041 under the Na-tional Natural Science Foundation of China and Project 2006AA01Z147 under the  X  X 863 X  X  National High-Tech Research and Development of China.
 References
