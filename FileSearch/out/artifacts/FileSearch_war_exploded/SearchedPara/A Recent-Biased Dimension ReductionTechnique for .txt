 Analyzing time series is a challenging topic in the field of data modelling and mining. In many applications, such as stock market, one concerns more about the recent data than what happened long ago. Besides the global trend, the recent data are very important to judge the similarity between time series and more significant to predict and make decisions than the detail of old data. For example, for a stockbroker, the long-term (say, six years) trend of stock price and the detailed variances in the last month of a stock are important, but the variance in a certain month four years ago is of little significance. In such kind of scenarios, a mechanism which favors the recent is called for. Nevertheless, most of the techniques for time series give equal significance to all data in the series. In this paper, we design a recent-biased technique to tackle the above problem. With our method, recent data are given more significance and kept with finer resolution, while old data with coarser resolution. Weights for DWT coefficients are derived from a decaying function, and then the coefficients with the largest k weights are chosen as the representation of the time series. Our technique is different from SWAT [1] in that the largest k coefficients are kept with our method while only a single coefficient is maintained at each level with SWAT. Our technique is also different from the traditional method of keeping the largest k coefficients and the RAM-DS algorithm [8], because with our technique the largest k coefficients are obtained from the weights only and has nothing to do with specific time series. However, the subsets of largest coefficients are different for different time series with both the tradition method and RAM-DS. The same are obtained from all time series without weighting. In this section, related work on similarity measures, dimension reduction and recent-biased techniques for time series data will be introduced. Euclidian Dis-tance and other L p -norms are popular to measure the distance between time series. Another measure, DTW (Dynamic Time Warpping), is designed to han-dle time series with some time shifts.
 be reduced to improve the efficiency of computation. Popular techniques include PCA, DFT (Discrete Fourier Transform), DWT (Discrete Wavelet Transform) [6], Landmark [7], major minima and maxima [4], PIP [3], PAA [5], etc. With DWT, a time series can be represented by a rough sketch by keeping only the first coefficients. Some researchers propose to use the largest coefficients to preserve the optimal amount of energy, or to choose the same subset of the coefficients for all time series for the ease of computing similarity [6].
 [1] to process queries over data streams that are biased towards the more recent values. A time weighting function is defined in [8] so that the old data values are with lower weights, and then more resources can be utilized to explore more recent data with finer granularities. Cohen et al uses decay functions to maintain time-decaying stream aggregates [2]. Our idea comes from the observation that recent data are usually more important than ancient data.
 Considering the time series shown in Figure 1, which pair of S 1 S 2 and S 1 S 3 is more similar? In many applications, we care more about recent data than ancient data, then S 1 and S 3 is more similar than S 1 and S 2 , since the difference between S 1 and S 3 happened long ago. However, the L p -norm distance between S 1 and S 2 is 1 , while that between S 1 and S 3 is also 1. So the two pairs are of the same similarity according to L p -norm distance. If using ( while that between S 1 and S 3 is 0.25. Therefore, S 1 is more similar to S 3 than to S 2 when a biased distance measure is used.
 to recent data. A simple idea is to give larger weights to more recent data. Actually, decaying functions are used widely for processing time series data. For example, exponential decaying functions are used for time series to find recent frequent itemsets adaptively, and to explore temporal and support count granularities in data streams [8]. In addition to exponential functions, polynomial and ployexponential decaying functions are also considered to maintain time-decaying stream aggregates [2]. Similarly, we define a recent-biased function as follows to help keep more recent data at finer scales. Recent-biased function B = b ( t ) ,t  X  0 is a monotonously decreasing function with b (0) = 1 and b (+  X  )=0, where t is the time elapsed till now.
 t is the time elapsed, and d is the decay factor, 0 &lt;d&lt; 1 , X &gt; 0. Linear decay can be get with linear function b t = n  X  t n , where 0  X  t  X  n and n is the length of time series. Based on the above recent-biased function, the recent-biased L p -norm distance with bias on recent is defined in the following.
 Definition 1 (Recent-Biased Distance &amp; Energy). The recent-biased dis-tance between time series S and S is defined as where  X  stands for the operator of inner product,  X  denotes L p -norm, and B is a recent-biased vector. If L 2 -norm is used, the recent-biased energy of S is defined as 3.1 Recent-Biased Dimension Reduction In this paper, Haar wavelet transform is used because it is very simple and widely used and is of linear time complexity. For Discrete Wavelet Transform, there are two different ways for choosing coefficients, the first k or the largest k coefficients. If the first k coefficients are selected, the global trend and variation can be preserved. With the largest coefficients kept, the parts of large energy are preserved and the energy is better kept, which is better for compressing a single signal. Nevertheless, when dealing with multiple time series, more storage space is required to keep the positions of coefficients and the distance computations is more complex. To keep more detail for recent data and preserve the recent-biased energy, a recent-biased technique based on DWT is designed in the following. Instead of keeping the largest or the first coefficients, the largest recent-biased coefficients are kept, which are computed from the weights only.
 values are shown in Figure 1. We can make the average to be zero by normalizing the time series, so the average C 0 is zero and is not considered here. Suppose that the time series are normalized that the average is zero. The original time series is C C from recent data to old data. Assume that B = b ( t ) ,t  X  0 is the bias function, so the recent-biased energy of S is In order to preserve the energy as large as possible, the coefficients with largest weights will be kept, and other coefficients are set to zero. It is difficult to tell which C ij is of the greatest importance from Formula (3). To make it j 1 = j 2 . Then, the question becomes to choose C i with the largest coefficients C where n is the length of time series. Since b i is a recent-biased function, it is monotonously decreasing with the increase of i . Therefore, C 11 is of the largest less) than 1 k =0 b 2 k . Different bias functions will probably lead to different sets of the largest k coefficients. Given the specific bias function, the weight of DWT coefficients can be calculated immediately with Formula (4) and ready for use for all time series. With our method, only the weighted function is used to decide which coefficients to keep, so the same subset of coefficients are chosen for all time series, which is different from the traditional method of keeping different subsets of the largest coefficients for different time series.
 similarity between time series and the recent-biased Euclidean distance and en-ergy are calculated with the following formulae.
 where C ij is in the first k coefficients. If the bias function is set to b i =1 ,i  X  0, that is, there is no bias, then the weights for those coefficients in Figure 1 are 8, 4, 4, 2, 2, 2, 2 from top to bottom. So those coefficients at higher level will be chosen first, which is the same as traditional DWT with the first coefficients. 3.2 Complexity Analysis Assume the length of time series is n , and there are m time series. The time complexity for computing weights from bias function are O ( n ) and the time complexity for getting the largest k weights is O ( kn ). The time complexity of DWT for a time series is O ( n ). Therefore, the total time complexity for pro-cessing m time series is O ( n + kn + mn ), i.e., O (( m + k +1) n ). As to space complexity, the space requirement for keeping the positions of the largest k co-efficients is O ( k ), and there are k coefficients for each time series, so the total space complexity is O ( k + mk ), i.e., O (( m +1) k ). Effectiveness of our algorithm for capturing recent details is shown in Figure 3. The original time series (see Figure 3a) is  X  X eleccum X  from Matlab, and the first 4096 values are kept. Linear bias function b ( t )= n  X  t n is used, where n=4096. The reconstructed times series after keeping the recent-biased largest k coefficients are shown in Figure 3b-h. These figures show clearly that the more recent data are preserved with more details while the older data kept with a coarser scale. used to test the accuracy of our technique in experiments. The close prices of indices from Jun 1988 to Oct 2004 are chosen and each time series is composed of 4096 points. To evaluate the effectiveness of our technique, we design a criterion to measure the precision of approximation after dimension reduction. Assume that S and S are respectively the original and reconstructed time series. The er-E ( S ) is the recent-biased energy of S defined in Formula (2). The experimental result for accuracy is shown in Figure 4. The horizontal axis stands for k ,the number of coefficients kept, and the vertical axis stands for the error rate. The solid line denotes the error rate of recent-biased DWT, while the dotted denotes that of traditional DWT with the first coefficients. It is clear that the accuracy gets improved as more coefficients are kept. Exponential bias functions are used, and the decay factor is d =1  X  1 1+10  X  . From Figure 4a to 4d, the decay function becomes less biased on recent with the increase of  X  . When the bias is large, higher accuracy can be achieved with our method than with traditional DWT with the first coefficients. When the bias is tiny (see Figure 4d), our method becomes nearly the same as traditional DWT with the first coefficients. We have designed a recent-biased technique for time series, which gives greater weights to more recent data and also preserves more details of recent data. Our experiment shows that the recent-biased technique is very efficient and effective to handle time series. Our future work includes combining our recent-biased idea with DFT, PIP, PAA and other dimension reduction techniques for time series data, and extending dynamic time warpping to a recent-biased measure.
