
Much work on skewed, stochastic, high dimensional, and biased datasets usually implicitly solve each problem sep-arately. Recently however, we have been approached by Texas Commission Environmental Quality (TCEQ) to help them build highly accurate ozone level alarm forecasting models for the Houston area, where these technical difficul-ties come together in one single problem. Key characteris-tics of this problem that are challenging and interesting in -clude: 1) the dataset is rather skewed (around 72 features, and 2% or 5% positives depending on the criteria of  X  X zone days X ), 2) evolving over time from year to year, 3) limited in collected data size (7 years or around 2500 data entries), 4) contain a large number of irrelevant features, 5) is biased i n terms of  X  X ample selection bias X , and 6) the true model is stochastic. Besides solving a difficult application proble m, this dataset offers a unique opportunity to explore new and existing data mining techniques, and to provide experience and guidance for similar problems. The main technical fo-cus addresses on how to estimate reliable probability given both sample selecting bias and a large number of irrele-vant features, and how to choose the most reliable decision threshold to predict the unknown future with different dist ri-bution. On the application side, the prediction accuracy of our approach is 20% higher in recall (correctly detects 1 to 3 more ozone days, depending on the year) and 10% higher in precision (15 to 30 fewer false alarm days per year) than state-of-art methods used by air quality control scientist s, these results are significant for TCEQ.
Ground ozone level depends on a sophisticated chemical and physical process as a function of many known and un-known factors, and stochastic in nature (i.e, with the same set of currently observable variables, the ozone level can differ from time to time). It has been an active topic for air quality study, an interdisciplinary field among atmo-spheric research, geochemistry, and geophysics, for many years since an ozone level above some well known thresh-old is rather harmful for human health, and affects other important parts of our daily life, such as farming, tourism etc. Therefore an accurate ozone alert forecasting system i s necessary to issue warnings to the public before the ozone reaches a dangerous level. In air quality study, the estima-tion of ozone level uses known physical and chemistry re-action theories that attempt to explain the underlying  X  X ru e X  mechanisms. There are several such theories around. As a result of these research, simulation systems, physical fo r-mulas and parametric models are created to calculate ozone level.

However, due to the difficulty of the problem and still limited knowledge about the true physical and chemical mechanism, existing approaches can only use a rather small number of parameters ( &lt; 10 ), and are still rather inaccurate and can be costly to build. However, it is a common be-lief among environmental scientists that a significant larg e number of other features currently never explored yet are very likely useful in building highly accurate ozone pre-diction model. Yet, little is known on exactly what these features are and how they actually interact in the formation of ozone. Information available is rather speculative in th e sense that we roughly know a rather exhaustive set of fea-tures out there. Indeed, this candidate list contains over 6 0 features. As mentioned earlier, none of today X  X  environmen -tal science knows as of yet how to use them. This provides a wonderful opportunities for data mining. As discussed in Section 1.3, the combination of rare class, large number of possibly irrelevant features, small training set size, fea ture selection bias, among others, is an interesting and challen g-ing problem.
Ground-level ozone O not emitted directly to the air like other air pollutants, bu t is formed as a result of a series of complex chemical re-action of VOCs (Volatile Organic Compounds) and NOx (or Nitrogen Oxides) in the presence of heat and sunlight. VOCs are emitted from a variety of sources, including mo-tor vehicles, chemical plants, refineries, factories, and o ther industrial sources. Nitrogen oxides are emitted from mo-tor vehicles, power plants, and other combustion devices such as off-road engines. Studies have shown that short term up to a few hours exposure to elevated ambient ozone can cause a number of health problems, such as asthma, chest pain and coughing [1]. The effects of long-term expo-sure is less established, although a few studies have associ -ated long-term exposure to elevated ozone with decreases in lung function, exacerbation of existing asthma and causing new asthma [9]. Besides the effects on human health, ele-vated ozone level also has negative effects on vegetation an d ecosystems, leading to reductions in agricultural and com-mercial forest yields, and increases plant susceptibility to disease, pests, and other environmental stresses. As part o f the mandate of Clean Air Act, United States Environmental Protection Agency (EPA) established the National Ambient Air Quality Standards (NAAQS) to regulate pollutants. In July 1997, the EPA announced a new 8-hour 80 parts per billion (ppb) standard as an amendment to the previous 1-hour 120 ppb standard. The new standard is for the pro-tection against longer exposure to the ground-level ozone. The 8-hour ozone is the average of the ozone concentra-tion of the past 8 hours. As a good case study, the Hous-ton/Galveston/Brazoria (HGB) area of southeast Texas has been experiencing some of the highest ozone levels recently recorded in North America, outpacing Los Angeles as the city with the most violation days in 1999 [6]. Recently, in 2004, the HGB area has a total of 52 days with measured 8-hour ozone reaching the 80 ppb dangerous level. Cur-rently the HGB is designated as a non-attainment area by EPA under the 8-hour ozone standard. As the fourth largest city in the U.S., it is of great interest for Houston to achiev e an attainment status before the EPA deadline in 2010. In the mean time, to monitor and forecast high ozone days be-fore it actually happens remains the top priorities for Texa s Commission on Environmental Quality (TCEQ).
Ozone level forecasting has been an active area for envi-ronmental science and meteorology. There are mainly two family of methods, air dynamic and statistical models. The dynamic forecasting uses 3-D air quality models to simu-late the atmospheric processes that influence the formation , transport and dispersion of ozone. The statistical methods , on the other hand, find the empirical statistical correlatio n between ozone and atmospheric parameters such as wind, temperature, etc.

Two examples of dynamic models were used in the north of Spain [14]. The first model uses three modules for ozone forecasting. The mesoscale model (MASS) provides the initial condition to the none-local boundary layer model based on the transient turbulence scheme, while the third module is a photochemical box model (OZIPR) in Eule-rian and Lagrangian modes and receives necessary infor-mation from the two previous modules. Quite different from the first model, the second forecast model, called MM5/UAM-V, is a grid model that predicts hourly 3D ozone concentration field. Both methods give good per-formance only for specific episode, but there is substantial computational cost in constructing these models and they are not portable to different locations, such as Houston. On the other hand, statistical forecasting is currently the most widely used method, primarily due to its low cost and competitive accuracy compared with the dynamic forecast-ing. Previously, various regression-based methods includ -ing regression trees, parametric regression equation, and artificial neural network (ANN), and others have been ex-plored for specific datasets at different locations. How-ever, Schlink et al [15] conducted an inter-model compari-son on 15 statistical techniques which were applied to ten data sets representing different meteorological and emis-sion conditions throughout Europe. They found that none of the 15 techniques performs better than others in all as-pects. Other none-regression based statistical methods ex -plored previously include fuzzy logic [8, 12], and Bayesian network [11].
In environmental science, the true model for ozone days are believed to be stochastic in nature. In other words, give n all relevant features in the feature vector x ity of an ozone day y =  X  X zone day X  conditional on x none-trivial. Formally, P ( y =  X  X zone day X  | x can be described as a density over x is stochastic, predictive mistakes and errors are inevitab le.
The dataset, described in detail in Section 2.1, contains 2500+ examples with 72 continuous features. Depending on the criterion for ozone days, either 2% or 5% of them are truly positive for the Houston area in the past 7 years. For data mining, it is a rather skewed and relatively sparse dist ri-bution. Small number of examples and large number of fea-tures increase statistical bias and variance even if we know the true stochastic model P ( y | x training data to estimate parameters inside the model.
In the same time, only about 10 features among these 72 features have been verified by environmental scientists to b e useful and relevant, and there is neither empirical nor theo -retical information as of yet on the relevance of the other 60 features. However, air quality control scientists have bee n speculating for a long time that some of these features might be useful, but just haven X  X  been able to either develop the theory or use simulations to justify their relevance. Part o f our task is to test their possible relevancy using data minin g techniques. This ought to be pursued with caution though, since the presence of a large number of features that are pos-sibly irrelevant may seriously introduce overfitting probl em.
By definition, the collected feature set x and relevant fea-ture set x non-empty intercept x venience, define x x , or formally, x = ( x from both y and x and P ( x P ( y | x r ) . This is trivial since P ( y | x ) = P ( y | x derivation P ( y | x ) = P ( y | x is exhaustive, and this is clearly not the case for 7 years of ozone data. In effect, irrelevant features change the proba -bility distribution represented in the data. For a normal da y training example ( x , y =  X  X ormal day X  ) , irrelevant features tend to push the probability P ( y =  X  X zone day X  | x ) down towards 0. Considering those irrelevant features, there is likely just one example with all these similar feature value s (both x for ozone day example, irrelevant features are likely to pus h up the probability to 1. Intuitively, as more irrelevant fea -tures are introduced into the feature vector, the empirical probability conditional on irrelevant features tends to ge t closer to the two extreme cases, either 1 for ozone days or 0 for normal days.

On the other hand, the date of the ozone alarm cannot be ignored, since an inductive model trained from historica l data will be used to predict ozone alarm in the future, and the number of  X  X zone days X  varies from year to year in the Houston area. Considering the date, the problem can be for-mulated as either a  X  X ata stream X  or  X  X ample selection bias X  problem. Evolving data stream is best described by changes in joint probability distribution P ( x , y ) = P ( y | x ) P ( x ) For ozone alarm forecasting, the physical law P ( y | x does not change over time, and as a result, neither P ( y | x ) nor P ( y | x P ( y | x ) may change, however, this is due to limited number of labeled examples. Under evolving data stream frame-work, the only possible change is in feature vector proba-bility distribution P ( x ) . In this sense, it is equivalent to  X  X eature sample selection bias X  as described below.
The dataset can be equally formulated as a  X  X ample se-lection bias X  problem [16]. Assume that s = 1 denotes that an example ( x , y ) is sampled from the universe of exam-ples into the training set, and s = 1 denotes that ( x , y ) not selected. Sample selection bias is best described by a dependency of s = 1 on feature vector x and class label y or P ( s = 1 | x , y ) . The sample selection bias is called a  X  X eature bias X , if it is explicitly dependent on feature vec -tor x and conditionally independent from class label y or P ( s = 1 | x , y ) = P ( s = 1 | x ) . Ozone day forecasting is an example of feature bias, since the training data set obvi-ously is unlikely to contain too many  X  X ays X  that are very similar to the future. Where there is sample selection bias, there are two closely-related challenges, 1) how to train an accurate model given sample selection bias, and 2) how to effectively use a model to predict the future with a differen t and yet unknown distribution.
Our work in this paper has made two main categories of contributions, one is a solution for ozone alarm forecastin g that is more accurate than state of the art methods adopted by TCEQ, and the other is the experience to formulate the application as a data mining problem, analyze its unique combinations of technical challenges, as well as the proces s to search for the most suitable solutions. Besides the pro-cess to find the satisfactory solutions, the most important technical contributions are as follows:  X  More accurate ozone forecasting system than current  X  Empirically shown the relevance of about 60 features  X  How to estimate reliable probability from a data set  X  How to choose the best decision threshold to use a
We start by describing how the Houston area data is col-lected, the feature sets and ozone forecasting task. We collected various meteorology and ozone data for the Houston, Galveston, and Brazoria (HGB) area. Seventy-two data attributes are extracted from several databases within two major federal data warehouse and one local database for air quality control. These are EPA AQS (Air Quality System) and NCDC (National Climate Data Cen-ter) [13] from the federal government as well as CAMS (Continuous Ambient Monitoring Stations) operated by TCEQ. The EPA AQS database is the national repository for information about airborne pollutants in the United States , and it provides the sensory information that is used in the regulatory feedback. Ozone exceeding above the National Air Quality Standard are based on the AQS data. The NCDC is the world X  X  largest active archive of meteorolog-ical data. In addition, the CAMS database archives some detailed air parameters and local meteorological data not available in the federal database. Since each database con-tains many site stations, and there are several of these sta-tions within the HGB area, we have chosen the informa-tion collected either at or closest to IAH-Bush Internation al Airport for the study. This is because the weather sta-tion at IAH is the only one that records a wide range of hourly weather parameters in the HGB area. In addition, the archived datasets at IAH have the longest recording history , and usually the least missing value and erroneous readings. In summary, these 72 attributes contains various measures of air pollutant and meteorological information for the tar -get area in our study.

Various air pollutant information from EPA AQS records and measures several sources of VOCs that contribute to the chemical reaction producing ozone in the target area. In addition, the CAMS Air database archives some addi-tional air pollutants information not available from EPS AQS, and these include the amount of various air includ-ing carbon monoxide (CO), nitrogen dioxide (NO2), sulfur dioxide (SO2), nitric oxide (NOx), fine particular matters (PM10 and PM2.5), oxides of nitrogen, and hydrogen sul-fide. When the recorded value for the same measurement is different between EPA AQS and CAMS, their average is used in our studies.

Since the meteorology at both surface level and upper at-mospheric level provides the physical condition that influ-ences the formation, transport and dispersion of pollutant s that contribute to ozone, meteorological data from both lev -els were obtained for modeling. NCDC Surface Airways (SA) database contains surface data such as relative humid-ity, ceiling height, sky cover, and etc. Other surface level features such ground wind speed and direction, and tem-perature, etc, are obtained from CAMS. For upper-air data, NCDC Raodiosonde Data of North America dataset pro-vides radiosonde observation (RAOB) record dated back to 1946. There are six RAOB stations in Texas. However, we use the data from the RAOB station in Lake Charles, Louisiana because it is the closest to Houston. Fifteen vari -ables are extracted from RAOB, including temperature (T), geopotential height (HT), dew point, wind speed and di-rection at the 850, 700, and 500 hPa levels. The vari-ables are sampled twice a day and the average is used in the study. The wind speed and direction are converted to U (east-west) component and V (south-north) component. Additional variables that may affect the air quality includ e previous day pollutant level (for carry over effect) and day type (workday or non-workday, which affects NOX emis-sions). In summary, seven (1998-2004) years hourly ozone data, meteorological surface data and daily upper air mete-orological data are collected in this study.
Since 1999, TCEQ started to issue ozone warning for public awareness for 9 metropolitan areas within Texas, in-cluding the HGB area. TCEQ forecasts are primarily based on the Criteria method [2]. It is an expert-rule-based sys-tem developed over the years. Daily weather forecasts from National Weather Service (NWS) are fed into the Criteria model to predict if ozone levels will reach or exceed a target level for a particular area. The criteria for the HGB area is based on a rather small set of parameters, such as, previous day ozone, maximum temperature and wind. The rules set are different for each month in the forecast ozone seasons, March to November for HGB area. In [10], researchers at TCEQ proposed a parametric ozone prediction model, called  X  X ocal ozone peak model X , based on monitored wind speed, temperature, and solar radiation, in conjunction wi th monitored estimates of upwind background levels. In this paper, we use the ozone peak model as base line since it is the model currently promoted by TCEQ. This local ozone peak model uses the upwind ozone background level (de-fault as 50 [2]), the maximum temperature in degrees F, the base temperature where net ozone production begins (50F), the solar radiation total of the day, the wind speed near sun-rise and midday. The emission factor can take values from 0 to 1. The following equation summarizes the parametric equation and the parameters involved.
 O 3 = U pwind + in which,  X  O 3 -Local ozone peak prediction  X  U pwind -Upwind ozone background level  X  EmF actor -Precursor emissions related factor  X  T max -Maximum temperature in degrees F  X  T b -Base temperature where net ozone production be- X  SRd -Solar radiation total for the day  X  W Sa -Wind speed near sunrise (using 09-12 UTC  X  W Sp -Wind speed mid-day (using 15-21 UTC fore-
We address those challenges as raised in Section 1.3.
Instead of predicting on class labels, it is well-known that a more effective approach on skewed and stochastic distri-bution is to directly estimate the probability distributio n it-self and then choose the best decision threshold to optimize on some given criteria, such as a compromise between pre-cision and recall.

In general, they are two families of methods, either de-scriptive or generative, and sometimes called, nonparamet -ric or parametric. Descriptive methods, such as decision trees, make rather loose assumption about the form the true unknown probability distribution. Both the structure of th e hypothesis and parameters within the hypothesis are esti-mated from the labeled training data. On the other hand, generative methods, such as naive Bayes and logistic regres -sion, assume that the true conditional probability follows a  X  X articular form X . The formula is fixed and learning is to estimate the parameters used inside the formula with Maxi-mum Likelihood Estimation. Since little is known about the true probability distribution of ozone days as a function of the large number of features, it is hard to choose the right generative methods, therefore descriptive methods are pre -ferred.
As discussed in Section 1.3, the conditional probability represented in the training data can be P ( y | x the dataset is exhaustively sampled. However, due to sam-ple selection bias, it is in fact P ( y | x cussed earlier, the effect of irrelevant feature is to make t he probability towards 0 or 1 or either under-estimate or over estimate.

One way to solve this problem is to train multiple mod-els, each of which are from a random feature subset. In other words, the model is built from x s x bility less  X  X harp X  (or away from 0 or 1), and the effect of x the correct or wrong directions. However, different models trained from different feature subset is unlikely to change the probability in the same direction unless these features are correlated. Without any knowledge about what features are relevant and which ones are not, the safest approach is to construct multiple models from different feature subset an d average their predictions. This is because on average, the multiple model will not perform worse than any of the sin-gle models, an issue explained further below. Yet, a separat e way to look at this issue is that variance in bias and variance decomposition can be reduced significantly with model av-eraging. There are many ways to train models from differ-ent feature subset. One of the simplest and somehow less ad hoc approach is to train multiple decision trees in differen t ways. Since there are 72 features and only 2500 examples, a decision path can at most test 12 features ( 2 12 = 4096 Therefore, two different trees are very unlikely to conside r the same 12 features out of 72. Assume all trees are uncorre-lated in choosing features. Then, 30 trees guarantees that a ll 72 features will be considered because (1  X  12
Since the dependency on the irrelevant features x not be ignored, every single model  X  constructed from the labeled training data is likely an inaccurate estimator of P ( y | x r ) . Let us denote the estimated probability by P ( y | x ,  X  ) . We show that the expected error to estimate P ( y | x r ) by averaging several models is no more than the expected error of any single model being averaged. Formally,
M A ( x ) =
The MSE for a single model  X  difference between the true and estimated probabilities squared.

E rror
In the above equation, the sum over which joint dis-tribution, either the unbiased joint distribution P ( x , y ) possibly biased distribution of the next year P ( x , y, s = near year ) , is insignificant. The expected MSE, if the model is chosen at random from a model space  X  , is then the same as Eq 3 except for there is an additional term P (  X  ) in the expectation.

If we were to take T models at random from the model space and average their predictions then the expected per-formance is E Therefore, if were to perform many repeat experiments comparing conditional probability averaging against a sin -gle model, on average, conditional probability averaging would perform no worse than a single model.
When there is feature sample selection bias and the test-ing data is completely withheld from the training process, in other words, both its feature vector values and class la-from the procedure described below is expected to be very similar to its actual performance on the testing data. 1. Use 10-fold cross-validation on an algorithm to be 2. Concatenate the estimated probability values from 10 3. Plot either precision-recall or ROC plot by choos-4. By reading from the precision-threshold plot obtained 5. Train a new model  X  using all available examples. 6. Use  X  to classify the future days. To be specific, the
We next argue the above process will return reasonable results for unseen future where neither feature vectors nor their true labels are known in advance. First, the precision recall plot is constructed by cross-validation, and is stil l ex-pected to be a reasonable estimate on the future testing data even under feature bias. Since neither the feature vector nor the prior ozone day probability for the coming year is known exactly, a better estimate would be quite hard to ob-tain. The cross-validation process already implicitly tak es feature bias into account. Every test set in 10-fold cross-validation contains a rather small number of examples con-sidering that the problem contains 72 features and 2500+ total number of examples. In other words, every test set is feature-biased, but just could be biased differently from t he days in the coming year. Nonetheless, when this process is repeated 10 times in cross-validation, the precision-re call plot is an average performance evaluated over 10 different feature bias distributions. Unless the feature distributi on of the coming year is so different from the training data, such as due to catastrophic weather pattern, the precision-reca ll plot is expected to a reasonable prediction for the coming year.

Each point or a precision-recall point, on the plot is es-timated from cross-validation. When these points appear rather continuous in the precision-recall plot, and not man y empty spaces in between adjacent points, the chance for sur-prises when using model  X  with decision threshold v the future days is little. The simple reason is that a contin-uous plot has more  X  X acts X  instead of  X  X peculations X  than a discontinuous or broken plot. One common held miscon-ception about precision-recall curve as well as ROC plot is that they are infinitely continuous. This is not the case in reality. The simple understanding is that validation data i s limited in size and many learning techniques have limited number of  X  X nique X  predictions. The line segment connect-ing adjacent points in precision-recall plot (or ROC plot) are not supported by any data. The connection is simply made for a visual line effect. For a chosen v above, if its adjacent precision-recall points are rather f ar away from v formance difference on future days. For the future year X  X  data, both the recall and precision given decision threshol d v otherwise read from the plot. For reassurance, it is prefer-able to have an idea on the approximate value if the esti-mate is indeed off. Obviously, a continuous precision-reca ll plot can provide this information. On the contrary, for a broken or discontinuous plot, one could only speculate and assume that the  X  X isual lines X  are close to reality. For this reason, when choosing v cent points. When two algorithms construct similar shaped precision-recall plot, one should prefer the algorithm wit h a more  X  X ontinuous X  plot for the same reason.
The probabilistic decision tree models are suitable for the skewed ozone modeling and forecasting because they output  X  X emi-continuous X  probabilities and a decision-threshold can be chosen to find the most satisfactory com-promise between recall and precision. There are two types of probability estimation trees -a single tree estimator an d an ensemble of trees. Obviously, the single tree is better for comprehensibility. However, it only approximates the true unknown model from one particular model represen-tation and may not have high accuracy. The ensemble is preferred when accurate probability estimation is desired . c4.5 and c4.4 [7] are representatives of single tree, baggin g trees and random decision tree [4] are examples of tree en-sembles. It is important to point out that bagging trees in our study average over posterior probability, but seminarl work on bagging uses voting of class labels. c4.4 A straightforward method to estimate class mem-bership probability is to use the class frequency at the cor-responding leaf. In our case, this is achieved by dividing th e number of high ozone days by total number of days from the training data that are sorted into the classifying leaf node . It has been noted [7] that frequency-based estimates of class-membership probability are not always accurate and statis-tically reliable. One reason is that the tree-growing algo-rithm searches for pure leaves, and tends to produce unre-alistically high probability estimates, especially when t he leaves cover few training examples. These estimates can be smoothed to mitigate these problems. As one of the sim-plest forms of smoothing, Laplace correction incorporates an equal prior for each class. Various experiments have showed that smoothing techniques can generally improve performance [7, 5]. Importantly, Provost et al [7] pointed out that many heuristics for improving classification accu-racy and minimizing tree size actually are biased  X  X gainst X  estimating accurate probabilities. For example, pruning v ia error reduction is blamed as the culprit. In sumary, c4.4 is a variation of c4.5 by replacing frequency estimation with Laplace correction and turning off pruning and collapsing.
Random Decision Trees Random decision tree [4] is an ensemble of individual trees, each of which is constructed  X  X andomly X . To build each tree, a feature is randomly se-lected from remaining features on which the data is split on. Along a decision path, a discrete feature can be selected only once; however, continuous features can be used multi-ple times, but each time with a different, randomly chosen splitting value. Features from the training data are used to construct tree structures, and the data values themselves a re used to update the class probabilities recorded in the leaf nodes. For a testing example, each tree produces a class probability. Probabilities from all the trees in the ensemb le are averaged to generate the overall class probability esti -mate. Normally, thirty trees is sufficient, and up to fifty trees may be necessary when the distribution is skewed. On average, the depth of a random tree is about half of the num-ber of the features, whereby distinct features are most like ly to be selected to maximize diversity for the ensemble.
Probabilistic decision trees are inductive methods that are constructed from labeled training examples and are af-fected by both sample selection bias (feature bias in our case) and amount of training data. However, parametric model used by TCEQ is not trained, but simply calculates the ozone level using some features for the day and is there-fore not affected by either sample selection bias or amount of training data. Given these differences, both  X  X xhaus-tive X  cross-validation tests that ignore the time stamp of t he data (therefore alleviating problem of sample selection bi as and number of training examples), and incremental tests that build models from previous years and test on subse-quent years are necessary. In particular, cross-validatio n tests are important from algorithmic point of view, since it will demonstrate the levels of accuracy that can be achieved baggingc4.4 bagginc4.5 rdf rdh 0.4752715 0.4770881 0.4526162 0.494517 0.3156655 0.2704359 0.2577432 0.3005114 baggingc4.4 bagginc4.5 rdf rdh 0.2621395 0.2543787 0.2378948 0.2232675 0.1996664 0.1226400 0.1202133 0.2520145 when there are significant number of training examples and not really biased. On the other hand, incremental tests can help reveal the limit of each algorithm when the number of examples are smaller and, most important of all, simulate the actual deployment of each method in practice.
In 10-fold cross-validation experiments, the time stamp of each day is omitted, and the 7 years worth of ozone data is completely shuffled prior to generating training and test -ing pairs. Seven decision tree algorithms are applied on the data for both 1-hr and 8-hr peak detection. These in-clude baggingc4.5, baggingc4.4, random decision tree with half depth rdh, and full depth rdf, c4.4 as well as c4.5 with and without pruning. Each tree model computes a posterior probability P ( y =  X  X zone day X  | x ,  X  ) . This notation explic-itly specifies its dependency on the feature vector x for each day as well as some decision tree model  X  . It is important to understand that the dependency on  X  cannot be ignored since each model estimates probability based on its trained model M and it may not be the true probability P ( y =  X  X zone day X  | x ) . We choose a subjective decision thresh-old v issue an alert. Obviously, with different values of v ent recall and precision will result. Normally with decreas -ing v say that model  X   X  is consistently higher than  X  bers. It is important to observe how the recall and precision are correlated for each chosen algorithm. The resulting re-call and precision results are plotted in a  X  X ecall-precisi on X  chart with x -axis as the recall and y -axis as the precision. This is similar to ROC, but is more straightforward for me-teorologists.

Each probabilistic decision tree algorithm does not generate exactly  X  X ontinuous X  probabilities, but  X  X emi-continuous X  estimates. For example, the number of  X  X nique X  probabilities generated by c4.4 and c4.5 cannot be more than the number of leaf nodes. For bagging and random decision trees, the number of unique probabilities cannot be more than the multiple of leaf nodes of all trees in the ensemble. In reality, each tree is correlated in cer-tain degree, the actual number on a given test set is ex-pected to be smaller than this upper bound. In order to compute an exhaustive recall-precision plot, we choose the unique probabilities (as a result of a model tested on the test set) as the decision thresholds. The results by concate -nating 10 folds together are shown in Figures 1 and 2. For both eight hour and one hour forecast, we present the re-sults in full recall range, between 0.4 and 0.6 where both the recall and precision numbers are useful to cover most alerts with decent number of false alarms, as well as the unique decision thresholds for each decision tree. The scal e on precision or y -axis is adjusted accordingly to empha-size the difference in results among different models. In all four plots, the baseline  X  X arametric X  model is the  X  X ark -est X  curve. Clearly, the precision obtained by the ensemble approaches (baggingc4.5,baggingc4.4, rdh, and rdf) for th e same recall numbers are nearly all higher than the baseline parametric approach, and consistently higher in the useful critical recall range of [0 . 4 , 0 . 6] , for both one hour standard and eight hour standard. Importantly, the higher precision obtained by trees are particularly obvious for the newly en-acted eight hour long duration standard. As shown in Fig-ure 1, each of the four ensemble methods has achieved twice as much precision as the parametric model, and single tree method c4.4 has obtained higher precision for recall range [0.4,0.6]. The difference among ensemble methods appears to be  X  X wisted X  and insignificant in the full recall range. In the critical range, rdh and baggingc4.4 appear to have achieved slightly higher precision than the other methods. For the one hour exposure standard, as shown in Figure 2, in decreasing order of precision for recall range [0.4,0.6], t he top performers are approximately, bagging4.4, baggingc4. 5, rdh, rdf, followed by the parametric model. In addition, as a summary of each curve in the precision-recall curve, we measure the  X  X overage X  area or integration under each curve, similar to AUC in ROC, as a single number to com-pare different models. The normalized area for each model-ing technique over the full range is summarized in Table 1. Since a high coverage implies better performance, the or-der of performance among different models is in consensus with the visual observation.

As shown in both Figure 1 and 2, the decision thresholds for the three single tree methods, c4.5prune, c4.5unprune, and c4.4, all exhibit an s -shape, and are either quite high (close to 1.0) or quite low (close to 0.0). This is due to the fact that single trees tends to construct pure nodes that are mostly ozone days or normal days. It appears that the Laplace correction employed by c4.5 can make the curve  X  X latter X , but it is limited. The decision thresholds of ran-dom decision trees are consistently lower than bagging, and this is due to bagging X  X  use of information gain to choose feature which results in  X  X urer X  nodes than random trees.
Incremental study is necessary, since in reality, the dataset is not collected all at once, but on a day-by-day ba-sis. We have chosen three algorithms for this study, bag-gingc4.4, random decision tree with half depth, and c4.4, as they are the best performers for ensemble methods or single trees in the cross-validation tests.

Month by Month Exhaustive Test In a month-by-month test, we incrementally include the previous month X  X  data to train a new model in order to make predictions for the coming month. We start with the whole year X  X  data in 1998, the incremental tests start on January 1999, and terminates in December 2004. For each test (one hour or eight hour), there is a total of 72 of training and test pairs. To compare with cross-validation results, we concat e-nate the estimated probabilities from 72 tests into a single file, choose unique probability values as decision thresh-olds to plot recall-precision curves for recall range [0.4, 0.6] and its corresponding coverage measurement, as shown in Figure 3. Comparing with exhaustive cross-validation tests, Figure 1 and 2, the parametric model X  X  performance is very similar but not the same, since it is not tested on the first year X  X  data. The performance of baggingc4.5 and rdh is slightly worse than the exhaustive cross-validation tests since the training sets are significantly smaller, i.e ., incrementally from 12 months to 83 months, as compared ever, they are still significantly better than the parametri c baggingc4.4 rdh c4.4 parametric 0.0505797 0.0408344 0.02170039 0.04158795 with recall=[0.4,0.6] model as summarized by the coverage measures in Fig-ure 3. To be specific, for eight hour test, rdh and bag-gingc4.4 achieve precision level 10% to 20% higher than the parametric model for different recall values. For the one hour test, baggingc4.4 X  X  precision is consistently abo ut 5% higher, but rdh and parametric model are very similar in precision. Compared with the two ensemble approaches, c4.4 X  X  performance appears to be significantly worse than the exhaustive test, and consistently worse than the parame t-ric model used by TCEQ. The reason is that single decision tree methods are sensitive to the amount of training data. When the amount is less, single tree X  X  error due to vari-ance increment becomes quite large. However, ensemble methods like bagging and rdt can effectively reduce vari-ance even if the training set size is small. This has been recently demonstrated in [17].

Annual Test The annual incremental test simulates the realistic scenario that meteorologists would use the proba -bilistic model. The reason is that the number of ozone days per year for both 1-hr and 8-hr peak is rather skewed and mostly accumulated during the summer months. Incremen-tal learning  X  X iner X  than one year increment is unlikely to include meaningful number of examples that could other-wise improve the model trained previously. For annual in-cremental learning, we start by building models from 1998 X  X  data to predict on 1999, incrementally assimilate the previ -ous year X  X  data and reconstruct the model to predict on the coming year, till 2004, for a total of 6 pairs of tests.
Like parametric model, we need to  X  X ix a decision thresh-old X  to make prediction on a daily basis. The threshold for the parametric model used by meteorologist is 120 for 1-hr peak and 80 for 8-hr peak, and the resulting recall is between 40% and 60%. We are interested in estimating a decision threshold for each algorithm to obtain similar lev -els of recall, and then comparing the resulting precisions (the higher the better). For this reason, 10-fold cross vali -dation is applied on the  X  X nnual X  training set to determine the thresholds used for the next year. Three thresholds are selected for this purpose, and they are the decision thresh-olds for recall=0.4, recall=0.6, and the average of these tw o thresholds. For example, on December 31, 2000, the train-ing data contains 1998, 1999 and 2000. In order to de-cide the decision threshold for the year 2001, we use 10-fold cross validation on the training data of 1998, 1999 and 2000. We concatenate all 10 result files generated from cross-validation together, and respectively find out the de -cision threshold for recall to be 0.4, 0.6 and the average of these two thresholds. This procedure is repeated for each of the three chosen decision tree algorithms, and a different s et of decision thresholds is selected for each model. We then use the respective model with the chosen thresholds to pre-dict the year 2001, and report the corresponding precision and recall. Since the collected data is from 1998 to 2004, this procedure is repeated 6 times. The results are averaged for 6 years in Table 2. The  X  X hreshold X  for decision trees is marked under  X 0.40 X ,  X 0.60 X  and  X  X vg X . These are not the actual threshold chosen for each method. It just indicates the thresholds chosen from cross-validation to obtain reca ll to be 0.4, 0.6 and the average of these two thresholds. We bold-font a result if both of recall and precision are higher or rather close to the parametric model. The advantages of bagginc4.4 and rdh over the parametric model are obvious. For the eight hour alert, comparing rdh (recall=0.608, pre-cision=0.323) and parametric (recall=0.568 and precision = 0.227), assuming that each year has about 25 8-hr ozone alert days, the result means that rdt can correctly detect 1 more day but issue 15 days fewer false alarms. For meteo-rologist, this is significant.

Ozone Month Only Annual Test One important obser-vation is that high ozone days only happen during warm days and never occur during winter time. For this reason, it is a reasonable conjecture that those months without any ozone days are unlikely to help to construct a model to de-tect ozone days for summer months. We design  X  X n ozone month only annual test X . The difference from the previously described annual test is that we only use the previous years X   X  X zone month X  data to make predictions for current year X  X  ozone month data. We define ozone month as the month which includes at least a day whose ozone level is higher than 80 for 8-hr peak and 120 for 1-hr peak. The total num-ber of months of out 12  X  7= 84 months has 46 months with 8-hour peak and 33 months with 1-hour peak. When the percentage of ozone days increases or the percentage of pos-itives increases, it becomes a slight different problem. As a result, the decision tree algorithms construct  X  X onceptua lly X  different models as compared to previous experiments, and the recall/precision as a function of decision threshold ov er different models also changes. Using cross-validation tes t, we have found that with recall between 0.7 and 0.8, the pre-cision by decision trees are still higher than the parametri c model for the same year. For this reason, in the ozone month annual test, the decision threshold chosen for the next year are the ones that obtain recall=0.7, 0.8, and the average of the two thresholds. The result is shown in Table 3. It is im-portant to understand that the recall for parametric model (not the tree model) is the same as previous annual test, however the precision is slightly higher. This is because th e none-ozone months have been taken out. In Table 3, when the decision tree X  X  methods achieve both higher recall and higher precision, the results are highlighted in bold-font s. Obviously, for each choice of decision thresholds, both bag -gingc4.4 and rdt have achieved 10% to 20% higher recall with up to 10% higher precision.
There are some belief that ozone level is not completely i.i.d from day to day. In other words, the ozone level of previous days in some ways may decide the ozone level of the following days. However, there is no clear consensus as of yet on how this should be modeled effectively. We have experimented a naive approach to take this into account for inductive learning. For each decision tree, instead of usin g cross-validation to estimate the optimal decision thresho ld for the next year, we find this out somehow  X  X ynamically X  as follows: set the initial decision threshold as 0.5, then p re-dict on the first day of the year. If the prediction is correct, keep the threshold and predict on the next day, otherwise adjust the threshold to make the prediction correct. How-ever the mistakes are always counted. Our experiments for both annual and ozone month only annual tests have shown that both recall and precision appear to be less. Another possibility is to include the previous day X  X  ozone alert as a feature. However, this will make the trees incompatible with the parametric model. In addition, we also have exper-imented with AdaBoost using unpruned c4.5 as the weak learner. With the same 10-fold tests used for Figure 1 and 2, boostedc4.5 achieved recall=0.231 and precision=0.479 3 for eight hour, and recall=0.111 and precision=0.293 for one hour. If we had plotted these in the same figures, it could have fallen within curves of the single decision trees .
Our work provides a machine-learning based solution to forecast ozone days for the Houston area, that is more ac-curate (20% higher recall and 10% higher precision) than existing method adopted by Texas Commission on Environ-mental Quality, as well as experiences and methods to solve problems with similar characteristics.

On the practical side, ozone level forecast is one of the most important and difficult problems for air quality con-trol. It is well established that ozone level above certain threshold is dangerous for human health and inadversely af-fects other parts of our daily life. Traditional approaches to forecast ozone alert relies on  X  X ir dynamics X  that simulate s the physical and chemical process that generate ozone. It is well known that these methods consume high compu-tational power and the solution is not portable from one scenario to another. In the same time, the prediction ac-curacy is still far from being desirable. On the other hand, regression-based methods (regression trees, neural netwo rk and parametric regression) have shown limited success to forecast ozone level. To the best of our knowledge, this paper is the first attempt that use inductive learning tech-nique to issue ozone level alert. Our choice learners are probabilistic decision trees and the base line comparison i s a parametric model developed by expert in ozone level pre-diction. Using seven recent years of data, rather exhaustiv e cross-validation experiments as well as incremental exper -iments, we have demonstrated that inductive learning can significantly improve an expert-based parametric model. In particular, in the annual incremental test, baggingc4.4 an d random decision tree can achieve 10% to 20% higher recall and up to 10% precision than the parametric model. Though our choice of inductive learners are none-exhaustive, this paper has shown that inductive learning can be a method of choice for ozone level forecast, and ensemble-based proba-bility trees provide better forecasts (higher recall and pr eci-sion) than existing approaches.

For data mining research, besides the procedure to ana-lyze and formulate the problem and look for the most appro-priate modeling technique, we have shown that in general, model averaging of posterior probability estimators train ed from random subset of feature vectors can effectively ap-proximate the true probability when there are 1) a lot of irrelevant features and 2) feature sample selection bias. F or stochastic problems under sample selection bias, we have provided a cross-validation based procedure and guide on how to choose the most appropriate decision threshold as a compromise between precision and recall, and in the same time, avoid  X  X urprises X  when applied on biased testing data where neither the feature vector nor the prior class distrib u-tion is known.

