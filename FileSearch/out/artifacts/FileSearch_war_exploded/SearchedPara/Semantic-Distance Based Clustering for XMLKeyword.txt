 Keyword search in database is gaining a lot of attentions [1-11]. Many exist-ing methods about XML keyword search are based on LCA (lowest common ancestor) model. A typical one of them is the SLCA (smallest LCA) method [2, 3, 5], which returns a group of sma llest answer subtrees of an XML tree. A smallest answer subtree is defined as: a subtree which includes all the keywords and any subtree of it doesn X  X  contain all the keywords. The root of a smallest answer subtree is called a SLCA. However, SLCA method probably omits some meaningful results. Example 1.1 shown in Fig. 1 illustrates a tree structure of an XML document, in which every node is denoted as its label and the number below it is the Dewey number.

Suppose that keywords are  X  X ML X ,  X  X ichael X  and  X  X avid X  are submitted to search in the document, and the nodes containing keywords are marked in red. If SLCA method is applied, the result would be a subtree rooted at node Article (0.2.2.0). The searching semantics is most likely to be  X  X hich papers about XML have been written by Michael and David X , and obviously it finds the paper  X  X ML3 X  while omitting the paper  X  X ML2 X . Moreover, users may have other reasonable searching intentions, like  X  X hich papers are written by Michael and David X  and  X  X hich papers about XML are written by Michael OR David X . For these two requests, paper  X  X TML1 X  and  X  X ML1 X  are both contented results but also omitted. If we regard any node set which contains all the keywords as a candidate results, actually what SLCA method does is picking out some  X  X ptimal X  ones from all the candidate results, and it considers a candidate result to be  X  X ptimal X  when its LCA is relatively lowest, so those candidate results with higher LCAs are discarded, this is the essential reason for SLCA method losing some meaningful results. Besides, SLCA method still has some other drawbacks: (1) all the results are isolated, which means any node only can exist in one result, and apparently it is inappropriate in some situations; (2) each result is demanded to contain all the keywords, means that SLCA methods only support  X  X ND X  logic between all keywords.

This paper presents a novel approach based on clustering techniques for XML keyword search, which can solve all above problems. Main contributions include: We propose a novel semantic distance m odel for XML keyword search in section 2; three clustering algorithms are designed in section 3, which are graph-based (GC), core-driven (CC) and loosened core-driven (LCC) algorithms; a ranking mechanism is proposed to sort all the searching results in section 4. We define the keywords submitted by users as a set containing t keywords L = { k Definition 2.1 (Search Semantics and Results). An XML document tree is a tree coded by Dewey numbers, and denoted as: d =( V, E, X, label ( id ), nodes, in which each node corresponds to an element or an attribute or a value in the document and has a unique identifier as well as a Dewey number; (2) E  X  V  X  V , is the set of edges in the tree; (3) label(id) , is a function which can get the label of the node whose identifier is id ;(4) X  X  V ,isthesetofkeyword nodes in the tree, and a keyword node is a node whose label contains any key-word; (5) pl ( id 1 ,id 2 ), is a function to obtain the path length between two nodes, in which id 1 and id 2 must have an ancestor-descendant relation, and the func-tion returns the number of edges between them; (6) depth ( id ) is a function to get the depth of the node whose identifier is id (the depth of the root is 1); (7) dwcode ( id ), is a function to get the Dewey number of the node whose identifier is id ;(8) lca ( V ), is a function to get the LCA of all nodes in V ,and V  X  V . Definition 2.2 (Shortest Path). The shortest path between two nodes The shortest path between two nodes x i and x j is the sum of the path between x i and lca ( x i ,x j ) and the path between x j and lca ( x i ,x j ). Meanwhile, the function spl ( x i ,x j ) is used to obtain the length of the shortest path. Apparently, spl ( x i ,x j )= pl ( lca ( x i ,x j ) ,x i )+ pl ( lca ( x i ,x j ) ,x j ). Definition 2.3 (Semantic Distance). The semantic distance between any two keyword nodes x i and x j is defined as a function dis ( x i ,x j ): dis ( x i ,x j )=
In the rest, semantic distance is called distance for short. In the formula of the distance function, the numerator and the denominator are the length of shortest path and the depth of LCA respectively. Obviously, the semantic distance of two keyword nodes is smaller when their shortest path is shorter or the depth of their LCA is lower. Assume the height (maximum depth) of the tree is h ,then the range of dis ( x i ,x j )is[0 , 2 h ]. In Example 1.1, evaluating all the distances between any two keyword nodes can get a semantic distance matrix (Fig. 2).
From Fig. 2, it can be found that the few smallest distances (0.40 and 0.67) are between two authors of a paper, and any largest distances (8.00) is between titles or authors of different papers which have no citation relation. The values of the distances match the practical meanings. Strictly speaking, the searching intentions of users can never be confirmed accurately ; so different than existing researches, we suggest that all keyword nodes are useful more or less and should be included in results. Based on the semantic distance model, we divide the set of keyword nodes X into a group of smaller sets, and each of them is called a  X  X luster X . Definition 2.4 (Cluster). Acluster C i is a set of keyword nodes, which means C .
We set a distance threshold to confine the size of clusters that the distance of any two keyword nodes in a cluster must be less equal to  X  . It actually means, two keyword nodes are regarded semantically related if their semantic distance is less than to  X  . Besides, there isn X  X  any restrict ion for which keywords can exist in one cluster, which means both of the  X  X ND X  and  X  X R X  logic between keywords can be supported. Nevertheless, given a distance threshold, there will be massive clusters satisfying the request, and many of them have inclusion relations with each other, so a concept of  X  X ptimal cluster X  is proposed.
 Definition 2.5 (Optimal Cluster). Given a distance threshold  X  ,asetof keyword nodes C i  X  X is called an optimal cluster iff: (1)  X  x i ,x j  X  C i ,s.t. Example 2.1: Assume the value of distance threshold is 2.0, then there are only part of the distances can be retained in the table shown in Fig. 2 (marked in blue), four optimal clusters is easily to be obtained: C 1 = x 1 ,x 2 ,C 2 = x 3 ,x 4 ,C 3 = x ,x 6 ,x 7 ,x 8 andC 4 = x 9 ,x 10 ,x 11 , which actually correspond to the contents of four papers respectively in Fig. 1.

Finding the optimal clusters is definitely not the last step. Actually a cluster is only a set of keyword nodes, how to generate enough useful information from clusters is another point. A simple approach is: for each cluster C i , return the whole subtree rooted at lca ( C i ) to users. However many these subtrees will have inclusion relations (for example the subtree rooted at lca ( C 3 ) and the subtree rooted at lca ( C 4 ) in Example 2.1), especially when the depth of some lca ( C i ) is high, this kind of problems become se rious. Moreover, it would cause some results being too large to be returned and for users to obtain useful information. So, for each (optimal) cluster , we generate and return a  X  X inimum building tree X  of it.
 Definition 2.6 (Minimum Building Tree). A minimum building tree (MBT) of any cluster C i is a tree whose root is lca ( C i ) and leaves are all the nodes in descendants ( C i ). The function descendants ( C i ) returns all the nodes in C i which doesn X  X  have any descendant node in C i .

Two extra operations are provided to expand the MBTs: (1) the  X  X xpansion X  of any node, which would add the subtree rooted at the node into the MBT. For example for the MBT of C 2 in Example 2.1, users can expand the node Authors(0.1.1) to get another author of the paper John (0.1.1.1) ; (2) the  X  X le-vation X  of the root, which will find the parent node of the MBT root and add it into the MBT, for example for the MBT of C 1 in Example 2.1, users can elevate the root Authors(0.0.1) and get its parent node Article(0.0) .Thus,userscan expand each MBT optionally according to their needs until satisfactory infor-mation is found.
 Definition 2.7 (XML Keyword Search). XML keyword search is a process that: for a set of keywords L submitted by users and a distance threshold  X  set in advance, a set of MBTs are obtained by searching an XML document d; moreover, each MBT could be expanded according to the demands of users. Since our goal is to divide a set of keyword nodes into groups according to the distances between keyword nodes, if each keyword node is regarded as an object, it is very natural to achieve our goal through clustering techniques, so we develop three clustering algorithms for XML keyword search: GC, CC, and LCC.
 Lemma 3.1. For an XML document tree d , any number of nodes which have thesamedeptharepickedout,and sorted in preorder to form a list I . For any node x i in I , traverse leftwards (rightwards) from it, then the distance between x i and the node each time met is non-decreasing.
 Lemma 3.2. The list I is defined as the same as in Lemma 3.1, also a similar list I is defined. Assume that the depth of the nodes in I is higher (lower) than that of the nodes in I , x i is an arbitrary node in I ,and x i is the ancestor (descendant) node of x i in I ( x i doesn X  X  have to exist in I or even in the tree, we can add one in the corresponding position of I if it X  X  inexistent). When traverse leftwards (rightwards) from x i in I , the distance between x i and the node each time met is non-decreasing.
 along with spl(xi X , xj) (spl(xi X , xj) is non-decreasing). Otherwise, it is obvious that depth(lca(xi, xj)) is non-increasing, according to the formula of semantic distance, is non-decreasing.

After the  X  is set and the keywords are submitted, we traverse the XML tree in preorder, find all the keyword nodes and put them into a hierarchical data structure.
 Definition 3.1 (Hierarchical Data Structure). Assume the height of the XML document tree is h ; the hierarchical data structure H isadatastructure which contains h ordered lists of keyword nodes, and the depth of any node in the i th list is i .

When we traverse the XML tree in preo rder, each node met would be added into the end of corresponding list in H if its label contains some keyword. Con-sequently, after the traversal is finished, H will contain all keyword nodes, and because in any list of it all nodes are added in preorder, Lemma 3.1 and Lemma 3.2 hold true in H . 3.1 GC: Graph-Based Clustering Algorithm for XML Keyword In algorithm GC, firstly we traverse H and connect any two nodes between which the distance is less equal to the distance threshold  X  with a link. After that a weighted undirected graph whose vertices are all the keyword nodes is obtained. Afterwards, a graph-partition algorithm is used to find all the maximal complete subgraphs (cliques); apparently the vertex set of each clique is an optimal cluster . H is accessed from top to bottom, and for each list, nodes are traversed from left to right. Assume x i is the node currently being accessed, we check the distances between x i and some neighbors which are right of or below x i , and because of the accessing order, the nodes left of or above x i needn X  X  be considered. Assume x j is a node right of x i in the same list, according to Lemma 3.1, dis ( x i ,x j ) is non-decreasing when the position of x j moves rightwards, so if dis ( x i ,x j )is greater than  X  , all nodes right of x j in the same list needn X  X  be regarded.
For the nodes in lower lists, firstly it should be confirm that how many lists should also because that depth ( x i )  X  depth ( lca (( x i ,x j ))) , then we can easily get that so only lists below x i need to be taken into account. Obviously, even there exists a descendant of x i in the ( f loor ( depth ( x i )  X   X  ) + 1)th lower list, the distance overflows.

For each of these ( f loor ( depth ( x i )  X   X  ) lists, first we find the position of the descendant node of x i in it (the descendant node of x i doesn X  X  have to exist), then traverse leftwards (rightwards) from the position until the distance overflows, and according to Lemma 3.2, all other nodes in the list needn X  X  be considered.
Each time the distance between tw o nodes is found to be less equal to  X  ,these two nodes are linked (by adding cursors point to each other) to build an edge, and the value of distance is recorded as the edge X  X  weight. A weighted undirected graph is obtained, and then a simple graph-partition algorithm is used to get all the cliques. The pseudocode of GC is given in Fig. 3. The graph-partition algorithm is not given here for the sake of space of this paper. We can show that the total cost of finding descendant X  X  position is h 2  X  O (log n )+ h  X  O ( n ).
It is easy to find that the time complexity of GC not only depends on the total number of keyword nodes, but also strongly relies on the distance threshold. The disadvantage of GC is the uncontrollable efficiency, especially when the distance threshold is large. So, we propose two other algorithms: CC and LCC.
 3.2 CC: Core-Driven Clustering Algorithm for XML Keyword GC is node-driven, which gathers some keyword nodes close enough to each other together. While the idea of algorithm CC is  X  X ivide-and-conquer X : finds some keyword node sets which are called  X  X ores X  in the first place, all the nodes in a core can definitely be clustered together ; afterwards, each of which affirmatively contains at least one optimal cluster; finally optimal clusters are obtained from thesecoresets.
 Definition 3.2 (Core). Given a distance threshold  X  ,akeywordnodeset R  X  X is called a core iff the distance between any two nodes in R is less equals to  X  .
 An optimal cluster is a core, but a core is not necessarily an optimal cluster. Lemma 3.3. Assume x l and x r are any two nodes in the same list of H ,and x l is left of x r . Given a distance threshold  X  and dis ( x l ,x r )  X   X  , then the set of nodes from x l to x r (including them) is a core.
 Proof: Take any two nodes x a and x b from the ( r  X  l +1) nodes from x l to x r , and let x a be left of x b . According to Lemma 3.1, dis ( x a ,x b )  X  dis ( x a ,x r )  X  dis ( x l ,x r )  X   X  , so the distance between any two nodes isn X  X  greater than  X  .At the beginning of algorithm CC, we traverse H and divide all keyword nodes in H into a number of cores, each of which is called a  X  X ore origin X .
 Definition 3.3 (Core Origin). O i is a set of certain keyword nodes in H ;it is called a core origin iff: (1) O i is a core; (2) all the nodes in O i are in the same list I of H ; (3) there doesn X  X  exist a core O i which satisfies O i  X  O i and all the nodes in O i are in I .

A core origin is actually an optimal cluster in one list. As illustrated in Fig. 4 (a): assume I is a list in H , x l  X  1 ,x l ,x r and x r +1 are four nodes in I and their positions are just as the same as illustrated in the figure, also dis ( x l ,x r )  X   X  , between x a and any node left of x l  X  1 or right of x r +1 is greater than  X  ,conse-quently x l ,...,x r is a core origin. It denotes that the positions of a core origin X  X  nodes are continuous in the list.
 After getting all the cores origin from H , some cores around each core origin O i are considered for the purpose of finding optimal clusters which contain all the nodes in O i . As illustrated in Fig.4 (b), x l ,...,x r is a core origin, and according to previous discussions, all other nodes in the same list needn X  X  be regarded. Assume I is a lower list, x l ,...,x l  X  X sasetofnodesin I whose distances to x l are all less equal to  X  ,and x r ,...,x r  X  X sasetin I of nodes whose distances to x r are all less isn X  X  any node in I could be added into x r ,...,x r to form a core; if the intersection isn X  X  null, it must be x r ,...,x l (it is easy to prove that x l can X  X  be right of x r and x r can X  X  be left of x l ), therefore we can get a lemma as follows.
 Lemma 3.4. If x r ,...,x l is a core, then: (1) the union of x l ,...,x r and x ,...,x l is a core; (2) any node in I other than the nodes of x r ,...,x l cannot be added into x l ,...,x r to build a core.
 Proof: For the first thesis, x l ,...,x r and x r ,...,x l are both cores, so the only thing needs to be proved is the distance between any two nodes from different sets is less equal to  X  . Assume x a is an arbitrary node in x l ,...,x r , x a is an arbitrary node in x r ,...,xl ,and dis ( x a ,x a ) &gt; X  , then according to Lemma 3.2:  X  , the position of x a  X  X  ancestor in I is right of x a . However, dis ( x l ,x a )and dis ( x r ,x a )arebothlessequalto  X  , the position of x a  X  X  ancestor node in I conflicts, so dis ( x a ,x a )  X   X  . For the second thesis, apparently, the distance between x r and any node left of x r and the distance between x l and any node left of x l all exceed  X  .

In the same way, we can prove that, when I is an upper list Lemma 3.4 still holds. Otherwise, when x r ,...,x l is not a core, we can divide it into several cores, and for each core Lemma 3.4 holds. The similar cores as x r ,...,x l are called  X  X elated cores X  of x l ,...,x r . After all the related cores of x l ,...,x r being found, we choose one core from each list except I , along with the core origin x ,...,x r a core set is obtained, and obviously some optimal clusters containing all the nodes of x l ,...,x r can be found from it. The pseudocode of algorithm CC is given in Fig. 5, and for simplicity it only considers the case that x r ,...,x l is a core.

Line 1-11 of Algorithm 2 costs O ( n ); line 6-9 in function findRelatedCores costs O (log n ), and line 11 costs O ( n ), so the total cost of findRelatedCores is O (log n )+ O ( n ). For the function findOptimalClusters at line 14 of Algorithm 2, its purpose is to find all optimal clusters which contain all nodes in a cluster origin O . Detailed code is omitted here. It is similar to searching cliques in a graph, but the difference is that we consider cores instead of nodes. The core set S contains a number of cores and each of which comes from a distinct list; assume there are t cores in S , then they have 2 t possible combinations at most; for each of the combination a function similar to findRelatedCores needs to be invoked for at most t 2 times; because t is always a small number (less than h ), we can see that the complexity of function findOptimalClusters is same as findRelatedCores .

Also we propose two extreme cases here: (1) when  X  is really large, then m is a small number, and the complexity will be O ( n ); (2) when  X  is very small, m tends to n , however line 11 in findRelatedCores will cost O (1), so the complexity will be O ( n  X  log n ). Moreover, in the worst case the complexity is O ( n 2 ). 3.3 LCC: Loosened Core-Driven Clustering Algorithm for XML In most cases users don X  X  have accurate requests for the returned results; we can loosen the restrictions of results for the purpose of improving efficiency. At the beginning of algorithm LCC, H is also traversed to get all the cores origin. Afterwards for each list I in H , two additional lists are built: a head node list HNL and a tail node list TNL . HNL and TNL orderly store the first nodes and the last nodes of cores origin in I respect ively. Thereafter, f or each core origin O , we find some cores origins close enough, and then instead of looking for optimal clusters out of them, we easily add all nodes of them into O to form a result. The pseudocode of LCC is in Fig. 6. Line 1 of Algorithm 3 is to find all the cores origin, the processing is the same which in Algorithm 2. For line 4-8 in function findRelatedCoresOrigin , the time complexity of each step is O (log n ); and for line 10, the related cores origin only need to be recorded with identifiers rather than be traversed. Assume the number of cores origin is m , then apparently the time complexity of algorithm LCC is O ( n )+ O ( m  X  log n ); the worst case happens when m tends to n ,thenitcomesto O ( n  X  logn ); otherwise, when m is a small number, it is O ( n ). The whole process of ranking mechanism is: firstly sort all the clusters gener-ated by clustering algorithms using the scoring function, and then transform them into MBTs orderly, finally return top-k or all ordered MBTs to users. The first criterion of the scoring function is the number of keywords, obviously con-taining more keywords indicates a better cluster, and the best clusters are those which contain all the keywords. On the other hand we consider the occurrence frequencies of keywords having no influence on ranking. For those clusters con-tain the same number of keywords, the criterion of comparison is the average distances of clusters.
 Definition 4.1 (Average Distance). The average distance of a cluster C i is the average value of all the distances between any two nodes in C i . Assume C i contains m nodes, and function dis mean ( C i ) is used to get the average distance
Assume h is the tree height, then the range of dis mean ( C i )is[1 /h, 2 h ]. Ap-parently for a cluster C i , a smaller dis mean ( C i ) indicates that all nodes of C i are more compact in the tree. Otherwise, if C i only contains one node, the average distance cannot be defined on it, however we can see that the MBT of it also contains a single node and means almost nothing to users, so this kind of clusters are worst for users. We define function keywordNum ( C i )togetthenumberof keywords, and score ( C i ) as the scoring function of clusters, moreover our final scoring function is as follows: Example 4.1: For the four optimal clusters obtained in Example 2.1, their scores are: score ( C 1 )=13 . 50, score ( C 2 )=12 . 50, score ( C 3 )=18 . 75 and score ( C 4 )= 19 . 25 respectively. So, the order of clusters is: C 4 , C 3 , C 1 , C 2 . The environment of our experiments is a PC with a 2.8GHZ CPU and 2G RAM; and the software includes: Windows XP, JDK 1.6, and the parser  X  X erces X . The data sets used to compare three clustering algorithms are DBLP (size 127M, 6332225 nodes) and Treebank (size 82M, 3829511) [12]. We build a vocabulary for each data set, which stores some terms existing in the document; the occurrence frequency of each term is between 5,000 and 15,000. We randomly choose several ones from vocabularies as keywords to search in the documents. The process is repeated for forty times and afterwards the average values are evaluated as the final results.

From Fig. 7 it X  X  easy to find that: (1) given certain keywords and distance threshold, the time costs of three algorithms is ranked as  X  X C, CC, LCC X  (from big to small) except when the distance threshold is very small; (2) with the increasing of the distance threshold, time cost of GC always increases, while timecostsofCCandLCCbothfirstincre ase and then decrea se. In Fig. 8 (a), when the distance threshold equals to 0.0, the returned results are those nodes whose labels contain multiple keywords, because each of them is considered as some different nodes; when the distance threshold equals to 6.0, almost all the keyword nodes gather into a few large cluster s; also with the thres hold increasing, the amount of clusters except single-no de ones firstly increases and then reduces to 1. Similar situations happen in other subfigures in Fig 8. We have evaluated the average distances of returned clusters: the results of GC and CC are the same, and LCC X  X  only very litter larger than them (at 4 decimal places). So, the results of GC and CC are the same (all the optimal clusters), and the results of LCC are less and bigger but not quite worse than them. we can see that in any case DBLP has more average keyword nodes, however, Fig. 7 indicates that any of the three algorithms costs more time in Treebank than in DBLP with the same conditions, which means the topology of the XML document tree definitely affect the efficiency strongly. The efficiencies become lower when the height is larger and the topology is more complicated. In this paper, to obtain all fragments of the XML document meaningful to users, we propose a novel approach called XKLUSTER, which include a novel semantic distance model, three clustering algorithms (GC, CC, LCC); a ranking mechanism.
 Acknowledgments. This research is supported in part by the National Nat-ural Science Foundation of China under grant 60773076, the Key F undamental Research of Shanghai unde r Grant 08JC1402500, Xiao-34-1.

