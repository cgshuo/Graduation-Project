 In Internet ad campaign, ranking of an ad on search result pages depends on a cost-per-click (CPC) of ad words offered by an advertiser and a quality score estimated by a search engine. Bidding for ad words with a higher CPC is more competitive than bidding for the same ad words with a lower CPC in the ad ranking competition. However, offering a higher CPC will increase a burden on advertisers. In cont rast, offering a lower CPC may decrease the exposure rate of their ads. Thus, how to select an appropriate CPC for ad words is i ndispensable for advertisers. In this paper, we extract different sema ntic levels of features, such as named entities, topic terminologies, and individual words from a large-scale real-world ad word s corpus, and explore various learning based prediction algorith ms. The thorough experimental results show that the CPC predic tion models considering more ad words semantics achieve better prediction performance, and the prediction model using the support vector regression (SVR) and features from all semantic levels performs the best. H.3.5 [Information Storage and Retrieval]: Online Information Services  X  commercial services; H.2.8 [Database Management]: Database Applications  X  data mi ning; J.4 [Social and Behavioral Sciences]: Economics Algorithm, Performance, Measurement, Economics CPC prediction, ad ranking, s earch engine optimization. The commercial values of ads for advertisers depend on their click through rates (CTRs). In s ponsored search, an ad CTR is connected with a notion of ad ranking. That is, ad clicks usually occur at the first and the second ranks [4]. In other words, the ad CTRs decrease when ads are shown at lower ranks on search result pages for the sake of reduced visual attention. An ad rank is determined by a functi on [13] shown as follows: An ad with a higher RankScore will be shown at a higher rank. Two factors determine a RankScore . One is the QualityScore , which depends on various factors in a search query, ad words, ad texts, etc. Some methods [4][13] extract features such as terms in ad contents, ad types, etc. to estimate the QualityScore . The other is the CPC of ad words, which is bidden by an advertiser. The CPC shows how much the advertis er is willing to pay a publisher (e.g., a search engine) for each click on the ad. Concretely, when a user clicks on the sponsored link on search result pages, the user Many related literatures have been proposed to analyze the ad words auctions in the economics and the computer science community. Most of the work focused on the bidding strategy of ad words rather than the prediction. Bajari and Hortascu [5] studied the factors which may affect the bidding price and utilized econometric techniques to build bidding models. Animesh et al. [3] explored different profiles of bidding strategies and examined the relationship between advertiser s X  bidding strategies and their firm quality in the online advertising. Edelman and Ostrovsky [7] aimed at strategic behavior anal yses of the ad words auction on Yahoo and Google. Kitts and Leblanc [10] estimated a distribution of the final prices in the ad words auctions and then used the distribution to optimize bidding strategies. Cary et al. [6] used a greedy bidding strategy for the ad words auction to balance revenue, convergence a nd robustness properties of bidding strategies. Many studies with respect to the price prediction of item auctions such as PDA [8], digital camera [12], etc. were proposed in the online auction. Wellman et al . [14] introduced a game theory-based trading agent (TA) for the price prediction in a travel domain like airline, hotel, and ticket prices. They created several virtual participating TAs. Each TA took into account evidences from different sources for bidding. All TA simulated competitive bidding via a random walk process. The prices were generated for prediction if the competitive bidding was done. In Internet ad campaign, literatures on the CPC prediction of ad words were comparatively fewer. The study proposed by Kitts and Leblanc [10] was a work that is similar to the CPC prediction implicitly. They simulated the pay-per-click (PPC) auctions of ad words based on the TA technology. The TAs considered various factors such as the advertiser X  X  specifications, the clicks estimation, the markets estimation, etc. to simulate the competitive bidding process for the PPC prediction. The CPC and the PPC are nearly synonymous but the CPC is more often used when referring to the Internet advertising. The PPC is a pay by a search engine to another publishers (e.g., personal websites, blogs, etc.) when an ad automatically scattered by the search engine on the publisher websites is clicked. The sear ch engine will split the earnings from the CPC to the PPC for publishers. However, the study in the PPC prediction of ad words was performed with artificially generated data and was not veri fied on the real-world data. As mentioned before, the key challe nge for advertisers is to select an appropriate CPC of their ad words. This paper proposes a learning-based approach to deal with this issue. The contributions of this work are three-fold. Fi rst, the CPC prediction model can overcome the cold-start problem, where advertisers wish to refer CPCs of their ad words that no one has yet been recorded in a database for the decision support. To the best of our knowledge, our work is the first attempt to propose prediction models for selecting CPCs of new ad words. Second, we employ the natural language processing (NLP) techniques to mine ad words semantics and construct learni ng-based models for the CPC prediction. Our proposed predicti on models can precisely predict the CPC of ad words without referring to external auction factors, i.e., the starting price, the bid increment, the reserve option, etc. Finally, we explore and analyze the CPC prediction of ad words from an analytical and an empirical perspective. In other words, we verify the prediction models empirically by a large-scale real-world ad words corpus obtained from a major search engine. The results show that the prediction m odels achieve promising results. only a topic terminology ( X  X runk driving lawyer X ) is selected by the LF strategy, but three topi c terminologies ( X  X runk driving lawyer X ,  X  X runk driving X  and  X  X awyer X ) are selected by the ALL strategy. WordNet 3.0 is employ ed to determine the senses of topic terminologies. Here, we postu late that the head of a topic terminology is always at its fi nal position. Then, we consult WordNet with the head of each topic terminology after stemming, and assign it a synset. In this way, a set of topic terminologies with the same synset are put into the same sense cluster. Finally, there are 273,721 topic terminologi es and 8,153 sense clusters. The weights of topic terminology features are set to binary. In contrast, a topic terminology may be ambiguous, so that the weights of the sense cluster features are determined by a normalized frequency count of all the matching synsets. Word Level (W) . Every word in ad words forms a feature. The weights of word features are set to binary. Besides, the length of ad words is also considered as a feature. Several machine learning algorithms such as the linear regression (LR) [8], the polynomial regression with degrees 2 and 3 (abbreviated as PR(2) and PR(3), respectively) [8], the regression tree (RT) [8], the genetic programming (GP) [11], the neuro-fuzzy (NF) [12], the back-propagation neural network (BPN) [12] and the support vector regression (SVR) [2] have been adopted to predict the auction prices in th e online auctions. Various CPC prediction models are constructe d by these prediction algorithms and combinations of features extracted from different semantic levels as mentioned in Section 4.1. For RT, the parameter of minimum number of observations is set to 10. For GP, the population size and the run generations are set to 2000 and 25, respectively. The elitism selection scheme is used and the fitness is measured the mean absolute percentage error (MAPE). For NF, the fuzzy associative memory (FAM ) is adopted to determine its parameters, including the relative importance of each fuzzy rule and the membership function. For BPN, several options of the neural network configurations are tested, and the hidden layer, learning rate and momentum are set to 1, 0.8 and 0.2, respectively. For SVR, the RBF kernel is us ed and the grid algorithm is adopted to determine the best parameters: c , gamma and  X  . We randomly partition the ad words corpus into two equal sets for training and testing. Three evalua tion metrics, i.e., mean square error (MSE), mean absolute e rror (MAE), and mean absolute percentage error (MAPE), defined as follows are considered. sample i , and n is the number of samples. Various CPC prediction models constructed by different prediction algorithms, combinations of features extracted from different semantic levels, i.e., named entity (N), topic terminology (T) and word (W), and two selec tion strategies, i.e., the longest first (LF) and all the combinations (ALL) are explored. Using features on the word level (W) only is a baseline. Features extracted from different semantic levels are compared. Figures 1-3 show the performance of CPC pr ediction models generated by different prediction algorithms and feature level combinations on MSE, MAE and MAPE evaluation metrics, respectively. The tendency is similar on the three evaluation metrics. For the six feature combinations, using SVR performs the best among all the performs better than the other feature combinations significantly (p-value&lt;0.001 in t-test) for every prediction algorithm. Table 2 further lists and analyzes the experimental results of the best prediction algorithm (i.e., SVR). Using features on the word and the topic terminology leve ls (W+T) improves 0.99% and 1.53% of MAPE performance with the LF and the ALL selection strategies, respectively, compared to the baseline. This is because a topic terminology is a conceptual representation of words. For example,  X  X awyer X  and  X  X ttorney X  on the topic terminology level have an identical sense, but they are different on the word level. Using features on the word and the named entity levels (W+N) improves 31.10% of MAPE performan ce. That demonstrates both NE and NE type are good indicators for prediction. We find that ad words containing NE  X  X alifor nia X  generally have higher CPCs than those containing  X  X ississippi  X  on the same theme. That meets our expectation because California X  X  gross state product (GSP) is higher than Mississippi X  X  in recent years. Similarly, we find that ad words containing NE type  X  X ity X  have higher CPCs than those containing  X  X ontinent X . That demonstrates CPCs of more precise words are higher than those of general words. The ALL selection strategy is better than the LF strategy. It may be because the LF strategy is a special case of the ALL strategy, and the ALL strategy considers more representations of topic terminologies. Integrating featur es on the three levels (W+N+T) with the ALL selection strategy performs the best. The model achieving MSE 1.1674, MAE 0.5879 and MAPE 0.2067 is better than the other models significantly (p-value&lt;0.001 in t-test). Table 2. Impact of Features on Different Semantic Levels Feature Level MSE MAE MAPE Improvement (%) W 1.6584 0.7261 0.3135  X   X   X  W+T (LF) 1.5542 0.7085 0.3104 6.28 2.42 0.99 W+T (ALL) 1.5342 0.6841 0.3087 7.49 5.78 1.53 W+N 1.5189 0.6419 0.2160 8.41 11.60 31.10 W+N+T (LF) 1.3956 0.6220 0.2080 15.85 14.34 33.65 W+N+T (ALL) 1.1674 0.5879 0.2067 29.61 19.03 34.07 Table 3 shows the performance of the best prediction model under different ranges of CPCs. Total 97.67% of CPCs are in the range from 0.01 to 10 USD. The results show our proposed model performs well within this rage. Table 3. Prediction Performance in Various CPC Ranges 
CPC #Samples Distribution (%) MSE MAE MAPE 1.01~10 541,827 44.4463 0.7000 0.6146 0.1998 10.01~20 21,812 1.7892 12.536 2.9618 0.1977 20.01~30 4,546 0.3729 31.8205 4.8148 0.1937 30.01~40 1,415 0.1161 96.3122 9.0743 0.2631 In, this paper, we propose vari ous models learned from a large-scale real-world ad words corpus for predicting CPCs of new ad words. We explore the features from different semantic levels. Considering more ad words semantics, such as named entities and topic terminologies, achieves better prediction performance than words only. Integrating featur es on all semantic levels and adopting all the combinations (A LL) selection strategy achieves better performance in every prediction algorithm. Using SVR 
