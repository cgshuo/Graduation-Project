 Bayesian network learning algorithms have been widely used for causal discovery since the pioneer work [13, 18]. Among all existing algorithms, three-phase dependency analysis al-gorithm (TPDA) [5] is the most efficient one in the sense that it has polynomial-time complexity. However, there are still some limitations to be improved. First, TPDA depends on mutual information-based conditional independence (CI ) tests, and so is not easy to be applied to continuous data. In addition, TPDA uses two phases to get approximate skele-tons of Bayesian networks, which is not efficient in practice. In this paper, we propose a two-phase algorithm with par-tial correlation-based CI tests: the first phase of the algo-rithm constructs a Markov random field from data, which provides a close approximation to the structure of the true Bayesian network; at the second phase, the algorithm re-moves redundant edges according to CI tests to get the true Bayesian network. We show that two-phase algorithm with partial correlation-based CI tests can deal with continuou s data following arbitrary distributions rather than only Ga us-sian distribution.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining Algorithms, Performance Bayesian networks, causal modeling, graphical models
Causal relation discovery was formulated theoretically as learning the structures of Bayesian networks by the pio-neer SGS algorithm [17], which is a representative algorith m learning Bayesian networks by CI tests without knowing ver-tex ordering in advance. A significant limitation is that SGS algorithm requires an exponential number of CI tests. The following PC algorithm [16] enhances the SGS algorithm so that it is efficient when underlying Bayesian networks are sparse but still has exponential-time complexity at the wor st case. TPDA is the first one that requires a polynomial num-ber of CI tests and guarantees to return correct networks. The key concept TPDA relying on is monotone faithfulness and all analysis about monotone faithfulness is based on mu-tual information. The problem is that mutual information is not easy to be applied to continuous data if we do not have enough knowledge of distribution data following. In addition, calculating conditional mutual information giv en a set of variables can be very difficult when the size of the set is large. This difficulty makes searching the initial ap-proximate skeletons of Bayesian networks not efficient.
In this paper, we propose a two-phase algorithm as an improved version of TPDA. The two-phase algorithm has following advantages: (i) it has polynomial running time and a more efficient phase to estimate initial skeletons than TPDA; (ii) it can deal with continuous data with arbitrary distributions; (iii) the correctness of output networks is guar-anteed. Here is the general idea of two-phase algorithm. In phase one, the algorithm constructs a Markov random field from data. This Markov random field is used as the initial skeleton of the Bayesian network in phase two. In the next step, for every edge in the Markov random field, phase two decides whether it is a true edge in the underlying Bayesian network by CI tests. If an edge is a redundant edge, it will be deleted in phase two. After these two phases, we get the cor-rect Bayesian network. This two-phase algorithm can serve as a general framework in the sense that it can be applied to whatever data as long as there is a way to conduct CI tests.
The difficulty of learning Markov random fields from data is that we have to conduct CI tests given a large set of vari-ables, which is also the reason why TPDA uses two compli-cated phases to get the initial approximations of the skele-tons of Bayesian networks. Two-phase algorithm employs partial correlation-based CI tests to overcome this difficul ty. Partial correlation-based CI tests are widely used in causa -tion discovering algorithms [17, 16, 12, 11, 14]. However, all existing algorithms only use partial correlation-base d CI tests to deal with Gaussian variables. For non-Gaussian data, a series of causation discovering algorithms [15, 6, 7 ] are purposed and those algorithms are combined with PC al-gorithm to deal with data following arbitrary distribution s. A recent attempt to apply partial correlation-based CI test s to non-Gaussian data can be found in [19]. In this paper, we will show that: (i) merely partial correlation-based CI tests are able to deal with linear model with arbitrary dis-tributions; (ii) Gaussian distribution is only a special ca se of linear model; (iii) partial correlation-based CI tests sat isfy monotone faithfulness according to experiment results.
The remainder of the paper is structured as follows. In section 2, we introduce important concepts of Markov ran-dom fields and Bayesian networks. Based on these concepts, we analyze the relation between Markov random fields and Bayesian networks and give the framework of two-phase al-gorithm. In section 3, we explain the idea of using par-tial correlation to measure conditional independence, and give a heuristic two-phase algorithm. In section 4, we give complexity analysis and correctness proof of two-phase al-gorithm. Some empirical results are shown and discussed in section 5. Finally, we conclude our work in section 6. Definition 2.1 Markov random field [9] is an undirected graph G M ( V, E ), where vertex set V denotes a set of ran-dom variables, satisfying the Markov property that an edge { u, v } /  X  E if and only if u and v are conditional independent given all other variables (denoted by u  X  X  X  v | V \{ u, v } ). Definition 2.2 Bayesian network [18] is a directed acyclic graph G B ( V, E ), where vertex set V denotes a set of ran-dom variables, satisfying the Markov property that every variable is independent of any subset of its non-descendant s variables conditioned on its parents. If we view all edges in G
B as undirected, we get an undirected graph which is the skeleton of G B , denoted by skeleton ( G B ).
 Definition 2.3 If a Markov random field G M ( V,  X  E ) and a Bayesian network G B ( V, E ) are defined on the same variable set V , we say that G M ( V,  X  E ) and G B ( V, E ) are a Markov-Random-Field-Bayesian-Network pair , abbreviated to MRF-BN pair .
 (a) Markov random field Definition 2.4 G B ( V, E ) is a Bayesian network and P a distribution over V . If G B represents all and only indepen-dence relations true in P , we say that G B and P are faithful [18] to each other.
 Remark 2.1 Under the condition that Bayesian network G
B ( V, E ) is faithful to the underlying distribution P , there is an edge in G B connecting vertices u and v if and only if there is no C  X  V \{ u, v } making u  X  X  X  v | C .
 Definition 2.5 In a Bayesian network G B , a vertex v is a collider [18] on a simple path U if there are two distinct adjacent vertices of v on U , and both of them are parents of v . Otherwise, we call v noncollider . Specially, if v is the parent of both of its adjacent vertices on U , v is called diverging vertex .
 Definition 2.6 In a Bayesian network G B ( V, E ), two ver-tices u and v are d-separated [18] by C  X  V \{ u, v } if and only if every simple path from u to v is blocked by C . A simple path U is blocked by C if and only if at least one noncollider on U is in C or at least one collider and all of its descendants are not in C . Specially, u and v can never be d-separated if there is an edge connecting u and v . In this paper, we call the set C cut set .
 Remark 2.2 Given the condition that Bayesian network G
B ( V, E ) and the underlying distribution P are faithful to each other, vertices u and v satisfy u  X  X  X  v | C , where C  X  V \{ u, v } , if and only if u and v are d-separated by C . This fact links conditional independence with a graphical repre -sentation, which provides a way to convert causation discov -ery into a graph searching problem.
Two-Phase algorithm works in a similar way as TPDA. It first constructs an approximate skeleton of the underlying Bayesian network. And then it removes redundant edges by CI tests to get the true Bayesian network. Correctness of the algorithm requires that the approximate skeleton must contain all edges in the true Bayesian network while effi-ciency of the algorithm requires that the approximate skele -ton contains as few redundant edges as possible. Two-phase algorithm is designed based on following facts which we will prove in section 4: (I) the Markov random field contains all edges in the skeleton of the corresponding Bayesian network in its MRF-BN pair as figure 1 illustrates. (II) for any two vertices u and v in a Bayesian network G B , if there exits at least one set of vertices d-separating u and v , we are al-ways capable to construct a cut set d-separating u and v by choosing vertices only from simple paths connecting u and v in G B .

Figure 1 shows an example of MRF-BN pair. In figure 1 (a), there is a Markov random field containing all edges be-longing to its counterpart Beyesian network in its MRF-BN pair as figure 1 (b) shows. However, there is also a redun-dant edge { A, B } in figure 1 (a), which has to be removed before we get the correct Bayesian network.

The first phase of two-phase algorithm constructs a Markov random field from data according to the Markov property in definition 2.1. Namely, for each pair of variables u and v in data set V , two-phase algorithm does not add edge { u, v } into Markov random field G M ( V, E ) if and only if u  X  X  X  v | V \{ u, v } . The difficult part is how to conduct CI tests when the size of V is large. TPDA uses two compli-cated phases to estimate skeletons of Bayesian networks to avoid CI tests on large V . Two-phase algorithm employs partial correlation-based CI tests to overcome the difficult y, since partial correlation can be easily computed even when the size of V is large, which will be explained in section 3. The first fact guarantees that all edges in the skeleton of the Bayesian network we try to learn are included in this Markov random field. However, this Markov random field Algorithm 1: Framework of two-phase algorithm Data : Data set D
Result : The underlying Bayesian network G B ( V, E ) 1 begin 2 // Phase I starts from here ; 3 Initialize a Markov random field G M (  X  V ,  X  E ) by 4 foreach pair { u, v } in  X  V do 5 if not u  X  X  X  v |  X  V \{ u, v } then 6 add { u, v } into  X  E ; 7 end if 8 end foreach 9 // Phase II starts from here ; 10 Initialize V =  X  V and E =  X  E ; 11 foreach edge { u, v } X  E do 12 Construct a set C by collecting all vertices on 13 foreach Z  X  C do 14 if u  X  X  X  v | Z then 15 remove { u, v } from E ; 16 end if 17 end foreach 18 end foreach 19 Orient edges in E and output G B ( V, E ) 20 end may also contain some redundant edges which do not belong to the underlying Bayesian network. Thus, in phase two, the algorithm checks every edge in this Markov random field by CI tests to decide whether to remove it from the skeleton of the output Bayesian network. The second fact we listing above enables us to do CI tests in an efficient way. The idea is as follows. To make sure that an edge, say { u, v } , belongs to the skeleton of a Bayesian network, in principle we have to show that u and v are dependent conditioned on any set of the variables except u and v themselves. If this is the case, the number of situations we have to check grows exponentially with the number of variables in the Bayesian network, which is the reason why most of the Bayesian net-work learning algorithms are inefficient or even infeasible i n practice. The second fact implies that when we try to find a set of vertices conditioned on which u and v are independent (d-separated), instead of searching the whole vertex set, w e can restrict our search within the vertices on simple pathes connecting u and v . In another word, if we cannot find a subset of the set of all vertices on simple pathes connecting u and v to d-separate u and v , edge { u, v } should be added to the skeleton of the Bayesian network. We describe above idea formally by the framework of two-phase algorithm.
There are some algorithms available for orienting edges in skeletons of Bayesian networks. The one we used in this paper is that in the SGS algorithm [17], which orients edges by identifying all V-structures in a Bayesian network. With -out other information, edge orienting algorithm cannot dis -tinguish Bayesian networks in a Markov equivalent class, which contains Baysian networks with same skeletons and V-structures. Therefore, two-phase algorithm just output s an arbitrary Bayesian network from Markov equivalent class .
In order to complete the framework two-phase algorithm, we need to figure out an efficient way to conduct CI tests. We notice that in phase two, if the size of the initial cut set C (denoted by | C | ) is large, checking all 2 | C | subsets of C can be very time-consuming. In this section we first introduce a partial correlation based CI tests. And then, we explain how to use partial correlation based CI tests improving efficienc y of two-phase algorithm.
Bayesian network learning algorithms always make some assumptions about continuous data to make it feasible to measure conditional independence. One commonly used as-sumption is that data following Gaussian distribution. In two-phase algorithm, we assume that data are governed by linear simultaneous equation models (SEMs). There are four advantages to make such assumption. The first is that un-der this linear assumption we can measure independence by correlation. The reason is that when there is only linear dependence between any two variables, correlation is equiv -alent to dependency. The second one is that linear model is also a commonly used assumption in many applications. Especially, Gaussian distribution is included in linear mo del in the sense that for every Gaussian distribution, it is alwa ys possible to find a linear SEM generating data following ex-actly the given Gaussian distribution. The third advantage is that conditional correlation can be estimated by partial correlation under linear assumption and partial correlati on is easy to compute. The last point is that measuring condi-tional independence by partial correlation gives us an answ er rather than yes or no, but a real number, and we can use information of this quantity to conduct heuristic CI tests, which makes two-phase algorithm more efficient. The first advantage is straightforward. We explain the second and third points in this section and leave the last points to be explained in section 3.2.

Before we explain what is partial correlation and how to use it measuring conditional independence, we have to in-troduce what are linear SEMs. In a linear SEM, data are generated by a batch of linear equations. Suppose that a set of random variables X = { x 1 , x 2 , ..., x N } are generated by the following procedure: Where disturbances e i are continuous random variables in-dependent with each others and coefficients c ij are con-stants. We say that X is governed by a linear SEM. We can represent Bayesian networks by SEMs in the way that each vertex is the weighted summation of its parents plus a distur -bance term. The Bayesian network in figure 1 corresponds to SEMs as follows: x A = e A , x B = e B , x C = c CA x A c where x i corresponds to vertex i and c ij is the weight of the edge connecting i and j .

Given the definition, we can explain why Gaussian dis-tribution is a special case of linear SEMs. In other words, sian distribution N (  X  ,  X ), there always exists a set of dis-turbances and coefficients making X follow exactly the same distribution as  X  X . It is obvious that if all disturbances e are Gaussian variables, X should follow multivariate Gaus-sian distribution, since the summation of Gaussian variabl es is a Gaussian variable as well. Therefore, the only thing left is how to choose coefficients and mean and variance of each disturbance to make X has the same mean vector and variance-covariance matrix as  X  X . Since mean vector is easier to deal with than variance-covariance matrix, we get started by constructing variance-covariance matrix. W e apply Cholesky decomposition to  X . Because  X  is positive definite, it is guaranteed that there exists a lower triangul ar matrix L = ( l ij ) N  X  N with strictly positive diagonal entries satisfying  X  = LL  X  , where L  X  is the transpose of L . And then, we generate a random vector Z = ( z 1 , z 2 , ..., z of z i following standard Gaussian distribution. We have  X  N (0 ,  X ). By expressing z i in terms of  X  x j , j = 1 , ..., i and setting disturbance e i = l ii z i , we get a linear SEM which generates  X  X following N (0 ,  X ). Finally, we get the model X =  X  X +  X  which generates data following exactly N (  X  ,  X ) as we desires. Therefore, data following linear SEMs is a more general assumption than Gaussian distribution.
Now, let us look at how to measure conditional indepen-dence under the linear SEMs. As we mention at the be-ginning of this section, we can use conditional correlation instead of conditional dependence under the linear model. However, conditional correlation is still not easy to com-pute. The following theorem, proof of which is given in [2], provides a way to estimate conditional correlation by parti al correlation.
 Theorem 3.1 For any two sets of random variables Y and Z ,  X  Y Y Z = E ( X  Y Y | Z ) if and only if E ( Y | Z ) =  X  +  X  Z , where  X  and  X  are regression coefficients.

Where  X  Y Y Z is the residual matrix of Y after eliminat-ing the effect of Z and  X  Y Y | Z is the variance-covariance matrix of Y conditioned on Z . Theorem 3.1 basically says that if a set of random variables Y can be expressed as linear regression of another set of random variables Z , we can use partial correlation as an estimator of conditional correlation. Linear SEMs satisfy this condition. Namely, any set of variables can be expressed as linear regression of other variables. That is the reason why we can use par-tial correlation as an estimator of conditional correlatio n under the linear SEMs assumption. Partial correlation can be compute according to the definition. If a random vector partial correlation coefficient between y i and y j is defined as where  X  y i y j Z is the ( i, j ) th item of  X  Y Y Z . In two-phase algorithm,  X  uv V being zero is an indicator of u  X  X  X  v | V ,
Partial correlation contains information more than yes or no when we use it to measure conditional independence. As a real number, partial correlation coefficient tells us how much dependency between two variables left after eliminat-ing the effect of another set of random variables. In words of Bayesian network, partial correlation measures dependenc y left between two vertices after we trying to block the pathes connecting them in the Bayesian network by another set of vertices. After each CI test with different cut set, partia l correlation may increase or decrease compared with previou s one, which furnishes a clue of how to construct cut set in a heuristic way. The idea is as follows. Remark 2.2 provides a graphical interpretation of conditional independence, wh ich indicates that finding a set of vertices making another two sets of vertices conditional independent is equivalent to c on-structing a cut set to d-separate these two sets of vertices. We can achieve this goal by blocking paths connecting these two sets of vertices one by one, until all paths between them are blocked, and hopefully decrease of absolute value of par -tial correlation can serve as an indicator that we successfu lly block a path without unblocking another one. This idea is formulated rigorously by the following concept.
 Definition 3.1 Given that a Bayesian network G B ( V, E ) is faithful to the underlying distribution P , for a pair of ver-|  X  uv C | for any c  X  C , if and only if removing c from C un-blocks some paths between u and v originally blocked by C . If this is true for every pair of vertices in G B , we say that G B and P are monotone faithful [5] to each other.
 Remark 3.1 Monotone faithfulness enables us to analyze conditional independence quantitatively. Furthermore, i t es-tablishes a stronger connection between conditional indep en-dence and d-separation. Based on monotone faithfulness, we can convert CI tests into a graph searching problem. By blocking as many paths between two vertices as possible, we can finally find the set of variables conditioned on which these two vertices are independent, if there does exist such a set.

Based on the idea of monotone faithfulness, we give the algorithm of heuristic CI tests, called separate .
Algorithm 2: Separate( u, v, C ) 1 begin 2 if |  X  uv C | &lt; threshold then 3 return true; 4 end if 5 while | C | &gt; 1 do 6 foreach c  X  C do 7 if |  X  uv C \{ c } | &lt; threshold then 8 return true; 9 end if 10 end foreach 11 if min 12 then return false; 13 else c = argmin 14 end while 15 return false; 16 end
Given the condition that monotone faithfulness is satis-fied, the procedure separate returns true if there exists a subset of C d-separating u and v . Now, we give the heuris-tic two-phase algorithm. Algorithm 3: Heuristic two-phase algorithm Data : Data set D
Result : The underlying Bayesian network G B ( V, E ) 1 begin 2 // Phase I starts from here ; 3 Initialize a Markov random field G M (  X  V ,  X  E ) by 4 foreach pair { u, v } in  X  V do 6 add { u, v } into  X  E ; 7 end if 8 end foreach 9 // Phase II starts from here ; 10 Initialize V =  X  V and E =  X  E ; 11 foreach edge { u, v } X  E do 12 Construct a set C by collecting all vertices on 13 if Separate( u, v, C ) then 14 remove { u, v } from E ; 15 end if 16 end foreach 17 Orient edges in E and output G B ( V, E ) 18 end
Suppose there are N random variables in the given data set. To get the approximate skeleton, the first phase of two-phase algorithm requires O ( N 2 ) CI tests, which is better than O ( N 4 ) required by TPDA. For the second phase of the framework of two-phase algorithm, there are at most O ( N 2 ) edges in E . For each edge, there are at most O (2 N ) cut set to be tested. Therefore, the number of CI tests required by the second phase is O ( N 2  X  2 N ) and the total number of CI tests required by the framework of two-phase algorithm is O ( N 2  X  2 N ) as well.

The first phase of the heuristic two-phase algorithm re-quires exactly the same number of CI tests as the framework. In the second phase, there are at most O ( N 2 ) edges in E and for each edge we need to call separate once. In the procedure separate , there are at most N  X  2 vertices in the initial cut set C . At most O ( N 2 ) CI tests are required and each one takes O ( N 3 ) to calculate the inverse variance matrix on in-volved vertices Therefore, the time complexity of procedur e separate is O ( N 5 ) at the worst case and the time complexity of the second phase of the heuristic two-phase algorithm is O ( N 7 ). The total time complexity of heuristic two-phase algorithm is O ( N 7 ) at the worst case. Although O ( N 7 ) seems not efficient enough, compared with those algorithms with exponential time at the worst case, this polynomial up-per bound of running time is much better. In addition, the worst case only happens when the skeleton of the underlying Bayesian network is very closed to a complete graph, which is rare in practice. Most of the time, Bayesian networks are sparse and the running time of two-phase algorithm is reasonably short.
The correctness proof needs following assumptions: (i) monotone faithfulness is satisfied; (ii) measurement for co n-ditional independence is accurate; (iii) there are no hidde n vertices in the underlying Bayesian network.
 Proposition 4.1 A Markov random field contains all edges in the skeleton of the Bayesian network of its MRF-BN pair. Proof: For an edge in a Bayesian network, according to definition 2.2 and faithfulness assumption, there is no set o f variables in the Bayesian network conditioned on which the endpoints of this edge are independent. Specially, the end-points of this edge should be dependent conditioned on all other variables. According to definition 2.1, this edge shou ld be in the corresponding Markov random field.
 Proposition 4.2 For any two vertices u and v in a Bayesian network G B , if there exits a set of vertices d-separating u and v , we are always capable to construct a cut set d-separating u and v by choosing vertices only from simple paths con-necting u and v in G B .
 Proof: According to definition 2.6, to d-separate u and v , we need to block every simple path connecting u and v in G
B . Considering a particular simple path, there are two ways to block it according to definition 2.6, namely includ-ing one of noncolliders on it into the cut set or excluding one of collider and and all of its descendants from the cut set. None of simple paths requires vertices not belonging to it to block itself. Therefore, we only need to consider vertices o n simple paths connecting u and v to d-separate u and v .
Proposition 4.1 and Proposition 4.2 together guarantee that the framework of two-phase algorithm is correct. To prove the heuristic two-phase algorithm is correct, we need to show that procedure separate always successfully find a cut set d-separating two vertices if such a set exists. Lemma 4.1 At the end of phase one of two-phase algorithm, for any two vertices u and v , only two kinds of simple paths between u and v are unblocked given initial cut set C which contains all vertices on simple paths in G B connecting u and v : (i) the edge connecting u and v , (ii) a path contains only one collider which is a child of both u and v .
 Proof: According to proposition 4.1, all true edges are in-cluded in edge set E at the end of phase one, so the initial cut set C must contain every vertex lying on all paths connect-ing u and v in the underlying Bayesian network. For paths including only one noncollider, they are obviously blocked by the initial cut set. For paths containing more than one vertices between u and v , at least one of any two adjacent vertices is a noncollider according to definition 2.5, and th is noncollider is included in the initial cut set. Therefore, a ll true paths containing more than one vertices between u and v must be blocked by the initial cut set.
 Lemma 4.2 In the procedure separate , for any two vertices u and v , if removing a vertex c from cut set C blocks a sim-ple path connecting u and v , it cannot unblock another path between u and v simultaneously. Proof: If removing c blocks a path U and simultaneously unblocks another path V connecting u and v , c must be the only vertex in C which blocks V . Lemma 4.2 can be proved by showing that there must be a collider w on V , w and all its descendants not included in C , which means that V is also blocked by w . Lemma 4.2 is proved by mathematical induction.
 Basis: for the first time a simple path connecting u and v is blocked by removing c from cut set C , any other simple paths connecting u and v cannot be unblocked at same time. Proof: For the first time removing c from cut set C blocking an originally unblocked path U implies that c is a collider and the only vertex on U according to lemma 4.1 and none of descendants of c belongs to C . If removing c from the cut set unblocks another path V , c must block V when c be-longs to the cut set. According to definition 2.6 c must be a noncollider on V , which means that c has descendants on V . We can prove that minimum descendants of c on V must be colliders on V (here we use  X  X inimum descendants X , since there are two minimum descendants if c is a diverging vertex on V ). If minimum descendants of c on V were not colliders on V , there would exist a directed path from c to either u or v , which is a contradiction. Remember that c is a descen-dent of both u and v , so there should not be a directed path from c to either u or v . Otherwise there would be a cycle in Bayesian network. Therefore, minimum descendants of c on V are colliders. Without loss of generality, we call one of minimum descendants w . Since removing c from cut set C blocks path U , all descendants of c are not included in C according to definition 2.6, which implies that w and all descendants of w are not included in C . Thus, path V is also blocked by w besides c .
 Inductive step: if removing previous n vertices from cut set does not block and unblock paths between u and v simulta-neously, removing ( n + 1) th vertex cannot block a path and unblock another path at same time.
 Proof: Procedure separate removes a vertex from cut set only when partial correlation between u and v does not in-crease. Suppose that a path was unblocked by removing mth vertex ( m  X  n ). There should be no paths blocked simulta-neously according to induction assumption, so that partial correlation between u and v would increase according to monotone faithfulness, which is a contradiction. Therefor e, removing previous n vertices does not unblock any originall y blocked paths, which means that the property presented by lemma 4.1 still holds during the procedure separate . Thus, we can use the same logic as we prove basis to show that removing ( n + 1) th vertex cannot block a path and unblock another path at same time.
Figure 2 illustrates the idea presented in the proof of lemma 4.2. There are two simple paths between u and v . We denote path u  X  x  X  v by U and path u  X  x  X  y  X  w  X  v by V . Only U is unblocked given the initial cut set C = { x, y, v } . To block U , procedure separate needs to remove x and all its descendants, y and w , from cut set. The minimum de-scendent of x , in this case w , has to be a collider on V . Otherwise, v , x , y and w form a cycle. To block U , y and w have to be removed before x . After w is removed, V is blocked by both x and w , so removing x cannot unblock V . Proposition 4.3 If two vertices in a Bayesian network can be d-separated by some set of vertices, procedure separate always separates these two vertices (returns true) in the se c-ond phase of the heuristic two-phase algorithm.
 Proof: According to lemma 4.2, procedure separate never unblocks an originally blocked path, since it would cause the partial correlation increasing. Therefore, the only th ing we need to prove is that procedure separate can block all paths containing only one collider which is a child of both endpoints of the paths according to lemma 4.1. Consider a particular path U containing only one collider c which is a child of both endpoints of path U . Suppose that the mini-mum descendent of c in current cut set C is w . Using the same logic as we prove lemma 4.2, we can show that remov-ing w from C will never unblock an originally blocked path. By keeping on removing the minimum descendent of c from C , we can finally removing c from C without unblocking any paths, which means procedure separate blocks path U suc-cessfully. Following the same way, procedure separate can block all paths containing only one collider which is a child of both endpoints of the paths.
We empirically evaluate two-phase algorithm and com-pare it with PC algorithm, TPDA algorithm and LiNGAM algorithm. Two-phase algorithm, PC algorithm and TPDA are all dependency analysis algorithms. And we implement these three algorithms based on partial correlation-based CI tests to compare efficiency and accuracy on linear Gaus-sian data. LiNGAM algorithm is specially designed for non-Gaussian data. Results of LiNGAM reported are all based on small networks. In this paper, we compare two-phase al-gorithm against LiNGAM algorithm on linear non-Gaussian data generated according to real world networks.
The networks used in the evaluation are listed in table 1, which are obtained from real decision support systems. Data are generated by linear SEMs according to these net-works. Each variable is a weighted summation of its parents plus a disturbance. Weights are sampled from uniform dis-tribution between 0.1 and 0.9. Disturbance terms are drawn from standard Gaussian, uniform and log-normal distribu-tion, so that we can evaluate the performance of two-phase algorithm on both Gaussian and non-Gaussian setting.
We now describe how we quantify the performance of two-phase algorithm in terms of efficiency and accuracy. To eval-uate efficiency, we use running time as our metric. For accu-racy, we use precision, recall and F 1 -measure as evaluation metrics, all of which are defined based on the similarity be-Table 1: Bayesian Networks Used in Evaluation tween the output Bayesian network and the target one. We use adjacent matrix to represent a Bayesian network. Let  X  A denote the target adjacent matrix, and  X  A the output one, precision P and recall R are defined as follows: Given precision and recall, F 1 is defined as: Since data are generated randomly, for each parameter set-ting we run 100 times, and then take average as our final result in order to get reliable evaluation.
We first compare two-phase algorithm with PC algorithm and TPDA on linear Gaussian data. Results are illustrated by figure 3. We can see that two-phase algorithm and TPDA have shorter running time than PC algorithm, especially when underlying networks contain large number of variables . Running time of PC algorithm grows dramatically with the sizes of networks, but two-phase algorithm and TPDA not. For the Munin network, PC algorithm fails to get any re-sults within 24 hours. The reason is that monotone faithful-ness assumption enables two-phase algorithm and TPDA to search the structure of Bayesian network with polynomial time complexity. For accuracy of algorithms, these three algorithms have almost same performance and two-phase algorithm performs slight better, since all of them are base d on dependency analysis and should output same results if CI tests are perfectly accurate.

And then, we compare two-phase algorithm with LiNGAM algorithm to check whether partial correlation-based CI te sts can deal with non-Gaussian data. We draw disturbances of linear SEMs from uniform distribution and log-normal dis-tribution and results are shown by figure 4 and figure 5. We can see that two-phase algorithm performs better than LiNGAM algorithm in both accuracy and running time. The running time of LiNGAM grows fast as the sizes of net-works increase. In addition, two-phase algorithm has al-most same performance on data drawn from different distri-butions, which is a very nice property that enable us not to make assumption about data. However, LiNGAM algorithm does not perform stably on different distributions.
All results listed above provide an evidence that partial correlation-based CI tests can works well under monotone faithfulness assumption, since heuristic two-phase algor ithm has almost same or better accuracy than PC algorithm which has a systematic search. Another property empirically il-lustrated by results here is that partial correlation-base d CI tests can deal with linear non-Gaussian data as we discussed in section 3.1.
In this paper, we propose a two-phase algorithm to dis-cover causal relations on continuous data generated by line ar SEMs. Compared with existing algorithms, two-phase algo-rithm has following advantages: (i) it can deal with continu -ous data with arbitrary distributions uniformly rather tha n by combining two separate components to deal with Gaus-sian and non-Gaussian cases respectively; (ii) it has polyn o-mial running time, so that it can be applied to learning large networks in practice; (iii) it is distribution-insensitiv e as long as data generated by linear SEMs; (iv) it is demonstrated equaling or outperforming the existing methods in terms of accuracy on a series of real world Bayesian networks.
Furthermore, we show that partial correlation-based CI tests have two nice properties: (i) it can deal with both Gaussian and non-Gaussian cases; (ii) it works well under monotone faithfulness assumption. The first property makes it possible to apply those algorithms [17, 16, 12, 11, 14] with partial correlation-based CI tests to non-Gaussian da ta. With the second property, we can construct more efficient heuristic search algorithms like two-phase algorithm with correlation-based CI tests.
The work described in this paper was partially supported by a grant from the Research Grants Council of the Hong Kong Special Administration Region, China. [1] S. Andreassen, A. Rosenfalck, B. Falck, K. G. Olesen, [2] K. Baba, R. Shibata, and M. Sibuya. Partial [3] I. A. Beinlich, H. J. Suermondt, R. M. Chavez, and [4] J. Binder, D. Koller, S. Russell, and K. Kanazawa. [5] J. Cheng, R. Greiner, J. Kelly, D. A. Bell, and W. Liu. [6] P. Hoyer, A. Hyv  X  arinen, R. Scheines, P. Spirtes, [7] A. Hyv  X  arinen, S. Shimizu, and P. Hoyer. Causal [8] A. L. Jensen and F. V. Jensen. Midas: An influence [9] R. Kindermann and J. L. Snell. Markov Random [10] K. Kristensen and I. A. Rasmussen. The use of a [11] R. Opgen-Rhein and K. Strimmer. From correlation to [12] J. Pearl. Causality : Models, Reasoning, and [13] J. Pearl and T. Verma. A theory of inferred causation. [14] J.-P. Pellet and A. Elisseeff. A partial [15] S. Shimizu, P. O. Hoyer, A. Hyv  X  arinen, and [16] P. Spirtes and C. Glymour. An algorithm for fast [17] P. Spirtes, C. Glymour, and R. Scheines. From [18] P. Spirtes, C. Glymour, and R. Scheines. Causation, [19] Z. Wang and L. Chan. A heuristic partial
