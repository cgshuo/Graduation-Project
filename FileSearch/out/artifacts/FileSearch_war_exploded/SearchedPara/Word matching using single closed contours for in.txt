 ORIGINAL PAPER Tomasz Adamek  X  Noel E. O X  X onnor  X  Alan F. Smeaton Abstract Effective indexing is crucial for providing convenient access to scanned versions of large collec-tions of historically valuable handwritten manuscripts. Since traditional handwriting recognizers based on optical character recognition (OCR) do not perform well on historical documents, recently a holistic word recognition approach has gained in popularity as an attractive and more straightforward solution (Lavrenko et al. in proc. document Image Analysis for Libraries (DIAL X 04), pp. 278 X 287, 2004). Such techniques attempt to recognize words based on scalar and profile-based features extracted from whole word images. In this paper, we propose a new approach to holistic word recognitionfor historical handwrittenmanuscripts based on matching word contours instead of whole images or word profiles. The new method consists of robust extraction of closed word contours and the applica-tion of an elastic contour matching technique proposed originally for general shapes (Adamek and O X  X onnor in IEEE Trans Circuits Syst Video Technol 5 : 2004). We demonstrate that multiscale contour-based descriptors can effectively capture intrinsic word features avoiding any segmentation of words into smaller subunits. Our experiments show a recognition accuracy of 83 % , which considerably exceeds the performance of other systems reported in the literature.
 Keywords Historical manuscripts  X  Holistic word recognition  X  Contour matching  X  Annotation  X  Indexing 1 Introduction There is an increasing need to digitally preserve and provide access to historical document collections [3]. However, the application of existing technology to this challenging problem exposes numerous problems. Off-the-shelf optical character recognition (OCR) or commercial document processing systems are often not able to cope with problems peculiar to historical manu-scripts such as poor quality of the manuscript itself, noise, stained paper, contrast variations, faded ink, and differences in pressure of the writing instrument. There-fore, recently a significant research effort has been devoted specifically to the analysis of historical docu-ments [1,3 X 7].

The approach described in this paper is motivated by the work of Lavrenko, Rath and Manmatha [1,5,6] who promote the idea of holistic word recognition for handwritten historical documents. Their main focus is on achieving reasonable recognition accuracy, which en-ables retrieval of handwritten pages from a user-sup-plied ASCII query. They argue that, although currently it is not feasible to produce near-perfect recognition re-sults, satisfactory retrieval can still be performed using the noisy outputs of recognizers [8]. In their word spot-ting approach for indexing historical handwritten manu-scripts, pages are segmented into words which are then matched as images and grouped into clusters which con-tain all instances of the same word. A partial index is constructed for the collection by tagging a number of the resulting clusters.
Therehas beenalargenumber of approaches tohand-written text recognition proposed in the past. Although, typically they cannot be directly used for word spotting in historical documents some solutions and observations are still relevant to the work presented here.
There has been extensive research devoted to isolatedhandwrittencharacter recognition, whichinthe-ory could be applied to word recognition [9 X 11]. A comprehensive review of handwriting recognition ap-proaches considering mainly on-line methods can be found in [9]. Many approaches to alphanumerics [9,10] and Chinese character recognition are based on some form of elastic matching. Importantly, some studies [9] of the character recognition task reported that elastic matching can halve the error rate of linear matching.
The approaches intended solely for word recogni-tion can be classified as either analytical or holistic .The approaches in the first category proceed by first identify-ing the characters and then building word interpretation. Typical approaches adopt an over-segmentation meth-odology followed by character model based recognition using dynamic programming (DP) [12]. On the other hand, the holistic approaches attempt to recognize the word as a whole without preliminary letter identifica-tion. An excellent discussion of the holistic paradigm can be found in [13]. The review presented here is pri-marily based on this discussion.

Madhvanath and Govindaraja [13] identifies three levels of word features which can be used by holis-tic recognizers, namely: low-, intermediate-, and high-level. Examples of typical low-level features include stroke direction distribution [14], and various profile-based features [5]. An early approach in this category, presented in [14], used elastic matching with eight direc-tion codes to recognize ten cursively written key words. Typical representative of intermediate-level features are edges, end-points, concavities, diagonal, and horizon-tal strokes [15]. High-level features include ascenders, descenders, loops, i-dots, t-bars, and length [16]. For example, the approach in [17] relied on detection of fea-tures such as gaps between words, word length, ascend-ers, and descenders for verification of handwritten phrases (lexicon reduction). A DP algorithm was used to match the predicted features with the extracted image features.

It should be noted that the introduction of interme-diate-or high-level features is mainly dictated by the need for constructing synthetic word models from char-acter models (ASCII), since in many applications the words or phrases to be verified or recognized are not known a priori, i.e., they are not available in the training collection. On the other hand, the word spotting in his-torical documents, as presented in [3,5], can use a stricter holistic paradigm and directly match the unknown words with annotated words from the training collection. Consequently, word spotting does not necessarily require extraction of intermediate-or high-level features.

To summarize, many approaches to handwritten text recognition currently reported in the literature are not suitable for word spotting in historical documents due to their (1) limitation to single alphanumerics [9,10], (2) reliance on character recognition [12], (3) limita-tion to applications with small lexicons [14], or (4) weak discriminatory power suitable for lexicon reduction rather than for word recognition [17].

Given that traditional handwriting recognizers, analytical or holistic, do not perform well on historical documents, recently an increasing research effort has been devoted specifically to the analysis of historical documents [1,4]. To the best of the authors X  knowledge all approaches available currently in literature are based on the holistic paradigm. So far, the most advanced approaches in this area were proposed by Lavrenko, Rath and Manmatha [1,3,5,6].

Since the quality of historical documents is often significantly degraded it is crucial to select the right features for word matching. The feature set proposed by Rath and Manmatha [5] includes smoothed versions of the word images and various profile-based features. For example, the shape of a word is captured by up-per and lower word profiles extracted by traversing the upper (lower) boundary of a word X  X  bounding box and recording for each image column the distance to the nearest  X  X nk X  pixel in that column. In order to cap-ture the  X  X nner X  structure of a word these features were extended by the number of background to  X  X nk X  tran-sitions. It appears that the biggest limitation of the above set of features is their strong dependency on good normalization, mainly skew/slant angle normalization and baseline 1 detection.

In their early approach profile-based features were compared using dynamic time warping (DTW) [6]. They demonstratedontwodifferent datasets from theGeorge Washington collection that their approach outperforms competing matching techniques, including the shape context approach [11] which is currently the best classi-fier for handwritten digits. They showed in [1] that appli-cation of statistical language models can further improve word recognition accuracy as a postprocessing step. A simple Hidden Markov Model with one state for each word resulted in over 10 % improvement in recognition accuracy. Recently, they proposed a complete search engine for historical manuscript images [3]. The main innovation of this system is that it is based on ideas devel-oped in information retrieval, cross-lingual retrieval, and automatic image annotation and retrieval rather than on direct matching of word features. Although the system shows promising results on an impressive collec-tion of 987 page images of George Washington X  X  manu-scripts, the objective evaluation was carried out only for 26 queries due to the lack of relevance judgments for such large collection.

In this work, our primary goal is to investigate the possibility of matching words using their contours in-stead of whole images or word profiles. We believe that contour-based descriptors can better capture a word X  X  important details and eliminate the need for skew and slant angle normalization. The proposed approach utilizes a rich multiscale contour representation which eliminates several problems related to holistic represen-tation of words. Since multiscale representation implic-itly captures details at multiple levels there is no need to choose between low-, intermediate-, and high-level features or their integration. Unlike the case of inter-mediate-, and high-level features there is no need for making any decision before the actual matching can proceed. Furthermore, elastic contour matching based on a DP technique provides a flexible way to compen-sate for inter-character and intra-character spacing vari-ations. In this initial approach, we assume that bounding boxes for each word are already known [1,18]. The inner holes in letters and dots are ignored.

The remainder of this article is organized as follows: the next section describes extraction of a single closed contour for each word. Then, the contour matching algo-rithm proposed originally for general shapes [2] is briefly described in Sect. 3. Next, experimental results are pre-sented in Sect. 4. Our outlook on future research is discussed in Sect. 5 and conclusions are formulated in Sect. 6. 2 Contour extraction Extraction of a single closed contour for each word is a key component of the proposed approach. The extrac-tion procedure has to deal with the poor quality typical of manuscripts (noise, stained paper, etc.), contrast vari-ations (faded ink, differences in pressure on the writing instrument) and the most challenging problem  X  dis-connected letters. Figure 1 shows a sample of a typical manuscript under consideration. Contour extraction is performed in five steps: binarization, localization of the main body of lowercase letters, connected components labeling, connecting disconnected letters, and contour tracing.

All parameters controlling the contour extraction are set empirically using scanned pages of modern hand-written text. It should be noted that theoretically these parameters could be optimized based on the annotated training collection. 2.1 Binarization In the first step, the pixels from the input gray level image are classified as either  X  X nk X  or  X  X aper X  by com-paring their values with a threshold. Since the contrast of the input word image can vary considerably, mainly due to differences in pressure on the writing instru-ment, there is no single optimal threshold that provides acceptable binarization for different parts of the word. Therefore, the optimal threshold has to be estimated individually for each pixel based on its local neigh-borhood. We chose Niblack X  X  [19] algorithm where the threshold T for a given pixel is computed using the mean  X  and the standard deviation  X  of the gray values in a small window centered at the pixel. The threshold is cal-culated according to the formula proposed by Sauvola et al. [20]: T =  X  1  X  k 1  X  where k is a constant set to 0.02 and R denotes the dynamics of the standard deviation and is set to 128. The window size was fixed manually to cover approxi-matively the width of one stroke.

However, the above approach does not produce smooth word outlines and as such was modified. Prior to binarization, morphological filtering [21] is used to create eroded and opened versions of the input image  X  the structuring element employed is shown in Fig. 2. The dynamic threshold T is calculated based on the eroded image. The binary classification of a single pixel as  X  X nk X  or  X  X aper X  is made by comparing its value from the opened image with the dynamic threshold calculated for this pixel based on the eroded image. An example of binarization is shown in Fig. 3. Typically, weak strokes (in this example top of double  X  X  X  and connections between  X  X  X  and  X  X  X ) are detected and even dilated with-out widening the strong strokes. However, it should be stressed that the above approach is just an adaptation of one of many binarization methods available in the literature  X  see for example [22]. Detailed evaluation of the influence of the binarization stage on the overall performance of the system is outside the scope of this article. 2.2 Position estimation Often binarization alone is not sufficient to obtain a single connected  X  X nk X  region for each word. In such cases, separated parts of the word have to be connected. The proposed connecting procedure utilizes the position of the word baseline and the x -height. 2 Detection is per-formed by analysis of the number of  X  X nk X  pixels in each line of the word binary image (see Fig. 4).

Let us assume that the origin of the coordinate system lies at the left bottom corner of the word bounding box and that the baseline and the line defining the top of the main body of lowercase letters are vertical. Their vertical coordinates are denoted as y l and y u , respectively. Let x height denote the x -height. Note that y u Also, let P V ( y ) be a function representing number of  X  X nk X  pixels in line y and y max be a vertical coordinate of the line with the maximum number of  X  X nk X  pixels. The y u is located by finding the closest line to y max fulfill-ing the conditions: y &gt; y max and P V ( y )&lt; X  u P V Analogically y l is located by finding the closest line to y max fulfilling the conditions: y  X 
P V ( y max ) . Parameters  X  u and  X  l were set empirically to 0.5 and 0.3, respectively.
Horizontal boundaries of lowercase letters are detected by analyzing the number of  X  X nk X  pixels in each column of the binary word image. Let P H ( x ) be a function representing number of  X  X nk X  pixels in col-umn x counted between y l and y u . The first column for which P H ( x )/ x height &gt; X  x found during scanning the word image from left(right) to right(left) is chosen as the left(right) boundary of lowercase letters. The parame-ter  X  x was set empirically to 0.1. Additional rules can be applied to eliminate the residuals of the vertical margin line as illustrated in the example shown in Fig. 5c.
It should be noted that the position of the baseline and the length of x -height are not directly used by the matching algorithm, but serve only as guidelines for the extraction of word contours. Moreover, if all  X  X nk X  pixels are already connected in the binarization step, the proper contour can be extracted even if the esti-mated baseline and x -height are inaccurate. Although precise identification of baseline and x -height is not crucial to the presented approach, it should be also noted that, the above procedure would fail to accurately identify the baseline and x -height in the presence of sig-nificant skew. In such cases, an alternative method for baseline detection should be investigate like, for exam-ple, the method presented in [16]. 2.3 Connected component labeling The connected component labeling algorithm scans the word X  X  binary image and groups  X  X nk X  pixels into com-ponents based on pixel 8-neighborhood connectivity. Once all groups are determined, each pixel is labeled according to the component it is assigned to. Only components containing a sufficient number of pixels within the area of the main body of lowercase letters are retained for further processing, where the required num-ber of pixels is computed as [ 0.1 x 2 height ] .Asa result, small  X  X nk X  regions and letter residuals from the line above or below the current word are eliminated  X  examples are shown in Fig. 5 (fourth column).
It should be noted that the above procedure removes punctuation and diacritic marks. This may be problematic for languages which utilize many diacritic marks. However, the results presented here indicate that ignoring diacritic marks, especially in the presence of significant noise and stains, is not critical for Western script. 2.4 Connecting disconnected letters Once the relevant components are identified they are sorted based on the horizontal positions of their cen-ter of gravity. Then, successive components are con-nected by adding the best connecting link (synthetic  X  X nk X  line) into the binary image. Our observations indi-cate that disconnections within parts of a letter are rare due to the dynamic thresholding. Therefore, it is reason-able to assume that disconnections occur only between letters. The best candidate for the link between two disconnected components is chosen by the following procedure.

At first, candidates for the beginning and the end of the link are chosen from the region on the left and right, respectively. The candidates for the beginning(end) of the link are chosen as the first contour pixels from the right(left) in each line of the left(right) region. Then, candidate links are created by pairing selected points from the left and right region. This is followed by link validation. A link is considered as valid only if:  X  Both ends of the link are  X  X trictly X  inside of the main  X  Both ends of the link are considerably above the
The shortest valid link is chosen as the best  X  X nk X  line connecting two components. Links connecting  X  X nk X  regions are highlighted in red in Fig. 5 (fourth column).
It should be noted that the above procedure does not necessarily need to connect components in exactly the same manner as a human would. First of all, the connection is typically made within the bounding box of lowercase letters and its shape will contribute little discriminatory information compared to the descend-ers and ascenders, i.e., differences in connection will affect only lower scales of the multiscale representation. Secondly, a word connected in a nonintuitive way should still be recognized if there is at least one example in the annotated collection connected in the same way. Moreover, our observations indicate that, even if the disconnection occurs within parts of a letter the above procedure is able to connect components in a reasonable manner allowing correct recognition. 2.5 Contour tracing Once all components are connected with the links, the final binary mask for the word can be created  X  see examples from the fifth column in Fig. 5. This is fol-lowed by a contour tracing procedure which extracts a single ordered contour for each word  X  see examples from the last column in Fig. 5. 3 Contour matching In our approach, similarities between word contours are measured using the contour matching technique pro-posed originally for comparing general shapes [2,23].
In [2], we introduced a rich shape description method, termedthemultiscaleconvexityconcavity(MCC) repre-sentation. In this representation, information about the amount of convexity/concavity at different scale levels is stored for each contour point (Fig. 6). A DTW tech-nique [24] is used to find an optimal refined alignment along the contours upon which the dissimilarity measure is defined. The approach is robust to several transfor-mations including translation, scaling, rotation, modest occlusion, and symmetric transformation. The method performs particularly well in cases of elastic deforma-tions and where the similarity between curves is weak. 3.1 MCC representation The MCC representation stores information about the amount of convexity/concavity at different scale levels for each contour point. The representation takes form of a 2D matrix where the rows correspond to the different scale levels  X  and the columns correspond to contour points (parameter u ). Position ( X  , u ) contains informa-tion about the degree of convexity or concavity for the uth contour point at scale level  X   X  see example in Fig. 6c.
The simplified contours at different scale levels are obtained via a curve evolution process similar to that used in [25] to extract CSS images. Let us assume a size normalized closed contour C represented by N contour points andparameterizedbyarclength u : C ( u ) = x ( u ) , y ( u ) , where u  X  0, N . The coordinate functions of C are convolved with a Gaussian kernel  X   X  of width  X   X  X  1, 2  X  X  X   X  ( u ) = x ( t ) X   X  ( u  X  t ) d t ,(2)  X   X  ( u ) = and similarly for y ( u ) .

The resulting contour, C  X  , becomes smoother with increasing value of  X  , until finally the contour is con-vex. The convexity/concavity measure is defined as the displacement of the contour between two consecutive scale levels. Convex and concave parts are distinguished via a change in sign. 3.2 Matching The similarity/dissimilarity measure between two con-tours is based on local distances between corresponding points (computed using their multiscale feature vectors). The correspondence between points is estab-lished using well-known DTW [24] technique.

When establishing correspondences between two contours, first distances between their contour points are computed using their multiscale feature vectors and stored in an N  X  N distance table allowing their con-venient examination. Each entry in the table stores a distance between a pair of contour points correspond-ing to the row and the column of this entry. The distances between contour points of both contours are computed in a following way. Define the MCC representation as an  X  max  X  N matrix F =[ f  X  u ] where f  X  u denotes the con-vexity/concavity measure for contour point u at scale  X  . Let contours A and B be represented by F A and F B , respectively. Pairwise distance between contour points u
A and u B from A and B , respectively, is defined as weighted sum of absolute differences between their con-vexity/concavity measure from all scales: d ( u A , u B ) = where r  X  denotes dynamic range of concavity/convexity measures at scale  X  defined as r  X  = max
Weights p  X  control the relative proportions between distances computed using different scales. Optimization of these weights is discussed in Sect. 3.3
Finding the optimal match between two contour representations corresponds to finding the lowest cost diagonal path through the table containing pairwise dis-tances between contour point features from both words  X  see example in Fig. 7. The optimal path has to begin and end at the cell corresponding to the alignment of the two starting contour points under consideration. In our original approach all possible combinations of start-ing points are examined to ensure an optimal match. Deviations of the path from a straight diagonal path compensates for elastic deformations between contours, i.e., stretching and shrinking of parts. The detailed DP formulation of the algorithm can be found in [2].
The final dissimilarity between two contours is calculated by normalizing the cost of the optimal path ( D min ) found by the matching algorithm by the num-ber of contour points used in the MCC representations: D ( A , B ) = D no extra penalty for stretching and shrinking of contour parts during the matching process. Rather, a globally optimal match between two contours is found using a rich multiscale feature for each contour point and dis-similarity is based solely on distances between corre-sponding points. This is contrary to the majority of other contour matching approaches based on DP where differ-ent penalty factors for stretching and shrinking, usually chosen in an ad hoc fashion, drive the matching process. 3.3 MCC-DCT representation Although the original MCC representation obtained very encouraging retrieval results [2], the adaptation of the weights p  X  proven to be difficult due to high correla-tion between information from different scales. In [23], we proposed an alternative version of the MCC rep-resentation, termed MCC-DCT, where a 1D discrete cosine transform (DCT) is applied to each multiscale contour point feature vector de-correlating information from different scale levels and placing most of the energy in low frequency coefficients  X  see example in Fig. 6d. In addition, we proposed an iterative optimization frame-work for determining the relative proportions in which information from different DCT coefficients should be combined in the final similarity measure. We showed that such optimization can further improve matching performance for a particular application. The MCC-DCT representation and the optimized matching ob-tained improved retrieval performance in collections containing general shapes.
 For the purpose of this work, we used the MCC-DCT version of the contour representation. The match-ing procedure was optimized using the shape collec-tion from the MPEG-7 Core Experiment  X  X E-Shape-1 X  (part B) [26]. It should be noted that this dataset con-tains general shapes and does not contain any exam-ples of word contours. In theory, the matching could be optimized using the annotated collection of handwritten manuscripts, however, this possibility could not be ex-plored in this work due to the lack of a suitable training collection.

An example of matching word contours using the above method is shown in Fig. 7. A more detailed description of the MCC representation and the asso-ciated matching algorithm can be found in [2]. 4 Experiments 4.1 Overall results For our experiments we used a set of 20 pages from the George Washington collection at the Library of Congress. These contain 4,856 word occurrences of 1,187 unique words. The collection was accurately segmented into words using the algorithm described in [18]. Ground-truth annotations for the word images were cre-ated manually [1].

The contours for all words were extracted accord-ing to the procedure described in Sect. 2. Then, for each word contour the MCC-DCT descriptor with 100 equally spaced contour points, each with a feature vector consisting of ten DCT coefficients (  X  max = 10), was extracted and stored.

Each of the 4,856 word occurrences was used as a query and its contour representation was matched against word representations from all manuscript pages except the page with the query word. A 1-nearest neigh-bor method was used to classify the query word and the classification result was compared against manual anno-tation. It should be noted that words from the same manuscript page as the query are excluded from match-ing for comparability reasons with the results reported in [1].

The results are measured in terms of average word error rate (WER). To separate out-of-vocabulary (OOV) errors from mismatches we report two types of WER, one that includes OOV words and one that omits them from the evaluation. In other words, WER that in-cludes OOV errors is the average rate of all incorrectly classified query words  X  irrespectively of the fact that the error was attributed to the performance of the recognition algorithm or simply there was no such word in the searched collection (outside the manuscript page containing the query word). On the other hand, WER that excludes OOV errors is the average rate of all incorrectly classified words for all queries which had at least one instance of the word in the searched collec-tion (outside the manuscript page containing the query word). Therefore, WER that excludes OOV errors will characterize solely the performance of the used recog-nition method while WER that includes OOV errors characterize the overall performance of the recognition system which depends also on the size of the annotated collection.

The lowest WER reported until now in the litera-ture for the George Washington collection was 0.449 when OOV words were included and 0.349 when OOV words were excluded [1]. It should be noted that this result was obtained after utilizing additional resources to train a statistical language model. Using a 27-dimen-sional feature vector representation of each word, with-out any language model postprocessing yielded a WER of 0.603 when OOV words are included and 0.531 when OOV words are excluded. In comparison our method obtained an average WER of 0.306 with OOV words and 0.174 without OOV words. This represents approxi-mately 50 % reduction of the non-OOV error rate obtained by our system despite the fact that it is based on contour mapping only. 4.2 Towards fast recognition The contour matching technique described in Sect. 3 was proposedoriginallyfor comparinggeneral shapes [2,23]. Although the method can be used without any changes, in this section we demonstrate possibilities for further adaptation to the specific task of word match-ing in order to improve performance and reduce com-putational load. In addition, a pruning technique for discarding unlikely matches is proposed. All experi-ments presented in this section are performed using a standard PC with 1.6 GHz Pentium 4 processor. For clarity, only WER without OOV words is considered. Note that the goal of the experiments is to demonstrate the influence of various parameter settings on speed and recognition performance and not a proposal for the final tuning of the algorithm. We are aware the latter case would require use of two collections, one for tuning and one for testing. 4.2.1 Modifications to the matching algorithm When matching two shapes, their relative rotation and therefore the circular shift between their contour representations is generally unknown. Hence, all circular shifts have to be examined to ensure an opti-mal match, implying a total complexity of O ( N 3 ) , where N is the number of contour points used. In the case of word matching, we can locate a single starting point for each contour in such a way that its position along the contour is consistent among different instances of the same word. Close to optimal matching can then be performed by examining only one circular shift corre-sponding to the alignment of starting points and there-fore reducing the matching complexity to O ( N 2 ) .We propose to locate the starting point from the part of the contour corresponding to the end of the word, rather than the beginning, due to better shape consistency at this point. The point is located by scanning the main body of lowercase letters and searching for the first  X  X nk X  pixel. The scanning starts at the right bottom and is performed from right to left and from bottom to top as shown in Fig. 8. Utilizing the location of starting points, reduced the average matching time for two words from 6 ms to less than 289  X  s whilst maintaining a low average WER of 0.182 (without OOV words).

In the case of the matching algorithm used, finding the optimal match corresponds to finding the lowest cost diagonal path through the table containing pairwise distances between contour points from both words  X  as illustrated in Fig. 7. The path has to begin and end at the cell corresponding to the alignment of the two starting contour points. Deviations of the path from a straight diagonal path compensate for inter-character and intra-character spacing variations.

It is common practice in the case of DTW techniques [24] to restrict the path to lie in the area close to the ideal straight diagonal line in order to speed up the matching process. In our original approach, the path is allowed to deviate from the straight diagonal path by no more than a predefined deviation threshold  X  T d N , where T most cases when matching general shapes, this restric-tion imposed on the path X  X  geometry had no affect on the optimal path. In order to investigate the influence of this restriction on the word recognition rate we plot-ted WER against different values of the parameter T d together with corresponding times required for match-ing two word contours  X  see Fig. 9. From the plot we can observe that constraining the path to the diagonal line would result in WER above 0.230 which confirms the requirement of elastic matching. The experiment also verified the linear dependency between T d and computational load of the matching. For the remain-der of this article we adopt T d = 0.08 as a good trade-off between recognition efficiency and matching speed. Although, this setting increases the average matching time to 532  X  s whilst the average WER without OOV words is further reduced to 0.165. 4.2.2 Pruning unlikely matches Pruning techniques can be used to quickly discard unlikely matches by requiring word images to have sim-ilar statistics [6]. In [27], pruning of word pairs was performed based on the area and aspect ratio of their bounding boxes. This idea was further extended in [28] by the additional requirement of two words to have the same number of descenders (strokes below the base-line). In our approach, the pruning statistics are extracted directly from the word contours. Pruning is performed based on contour complexity and the number of descenders and ascenders. For clarity, each pruning rule is first discussed independently and only the best settings for each of the rules is used in the final combined pruning criterion.

The shape complexity x i of a word contour C i can be defined as the ratio between its perimeter length l i and the square root of its area a i : x i = l i / easily estimated directly from the word contour [29]. The following rule was used to discard unlikely matches [30]: Rule 1: Two word contours C i and C j are similar only if | x i  X  x j | / min { x i , x j } X   X  x .

Clearly, increasing  X  x in Rule 1 allows for more matches to be processed by the matching algorithm (reducing recognition speed) and potentially leads to lower values of average WER. Table 1 demonstrates empirically the effect of pruning using different values of threshold  X  x on WER and number of pruned pairs. The results show that setting  X  x = 0.3 would prune al-most half of the total number of word pairs, which would otherwise have to be processed by the matching algo-rithm, while maintaining a low average WER of 0.165.
Word X  X  descenders and ascenders are identified by traversing their contours and labeling contour points as belonging to a descender or an ascender based on the comparison between their vertical positions with the vertical limits of the body of lowercase letters  X  (see Fig. 10). Final numbers of ascenders and descenders are found by counting the number of continuous sequences of points marked as ascenders or descenders. It should be noted that it may not be important whether or not the  X  X eal X  ascenders and descenders are detected. Reliable pruning should be possible as long as the handwriting style is consistent, i.e., the identification (or missing) of ascenders and descenders is performed in a consistent way.

Let DESC i and ASC i be the estimated number of descenders and ascenders of word contour C i .The following two rules are used to discard unlikely matches:
Rule 2: Two word contours C i and C j are similar only if | DESC i  X  DESC j | X   X  desc .

Rule 3: Two word contours C i and C j are similar only if | ASC i  X  ASC j | X   X  asc .

The thresholds  X  desc and  X  asc control the tolerance for the absolute differences between numbers of descend-ers and ascenders when pruning word pairs and should have only integer values. Tables 2 and 3 demonstrate the effects of pruning on the average WER and the number of pruned pairs using rules 2 and 3, respectively. Table 2 shows that the number of descenders provides very reliable evidence for identifying unlikely matches. Specifically,  X  desc = 0 would lead to early discarding of 50% of word pairs without noticeable difference in aver-age WER. In contrast, pruning based on the number of ascenders (Table 3) is less reliable most likely due to the generally more complex shape of the upper parts of the words. Therefore, to ensure that only small proportion of valid matches will be discarded by the Rule 3 we have to allow certain tolerance, e.g.,  X  asc = 1 would result in WER of 0.164.

In the last experiment all three pruning rules were combined:
Rule 4: Two word contours C i and C j are similar only if Rules 1 X 3 are satisfied.

Incorporating the above rule and setting  X  x = 0.2,  X  matches while maintaining a low average WER of 0.183 (excluding OOV words).

In summary, the relatively minor adaptation of the matching algorithm and the incorporation of the prun-ing technique could reduce the average time required to recognize a single word to 0.4 s (for the particular size of the annotated collection) whilst maintaining a low average WER. This corresponds roughly to a 70-fold increase in speed compared to the original approach. 4.3 Comparison with CSS In the past, different shape matching techniques (typi-cally not requiring parameterization of word contours) have been tested for their potential capabilities to match words. For example, Rath and Manmatha [6] demon-strated that the shape context approach [11], which is currently the best classifier for handwritten digits, per-forms poorly in terms of recognition precision and speed for word matching.

The contour extraction procedure presented in Sect. 2 opened a new possibility of utilizing matching tech-niques requiring ordering of the contour points for the purpose of word matching. In this section, we evalu-ate the performance of the curvature scale space (CSS) approach[25], whichwas adoptedas contour-basedshape descriptor by the ISO/IEC MPEG-7 standard. The CSS descriptor is very compact, allows fast matching, and extensive tests using general shape collections [31] revealed that the method is quite robust with respect to noise, scale, and orientation changes of objects. In the experiment, the CSS extraction and matching was performed using the MPEG-7 eXperimentation Model (XM software v5.6) [32].

Tofacilitatecomparisonof thethreeapproaches, their results are collected in Table 4. Adopting the CSS tech-nique for recognition of words from the George Wash-ington collection resulted in relatively high average WER of 0.350 (excluding OOV words). Such poor per-formance can be explained by two major drawbacks of the CSS technique: the occurrence of ambiguity with re-gard to concave segments and its inability to represent convex segments. This result confirms the need for utili-zation of a rich shape descriptor, as discussed in Sect. 3. 5 Future work An obvious improvement to further reduce WER would be the optimization of the similarity measure using the framework proposed in [23] in conjunction with a suit-able training collection of handwritten words. Further-more, currently all contours from the database are re-scaled to a predefined size prior to the extraction of the MCC representations. In the case of word match-ing, the x-height should be sufficient for size alignment. Replacing the size invariance property with x -height invariance could further improve discrimination between words. Incorporation of additional features in order to capture the  X  X nner X  structure of a word and dots, e.g., size and centroid of the inner holes [10], should also be investigated. The number of contour matchings necessary to classify a single word could be further re-duced by developing an appropriate indexing strategy for a training dictionary. We would like to investigate the possibility of reducing WER caused by OOV words by augmenting the training dictionary by synthesizing new word contours based on the initial training set. In theory, this could significantly reduce OOV errors. Fi-nally, it should be stressed that word matching is just a first step in word recognition. Incorporating a statistical language model as a postprocess could further improve the recognition accuracy of the system as was shown in [1].

Also, it would be interesting to compare different binarization methods discussed recently in [22] with the variant of Niblack X  X  method used in this work.
In the future we plan to incorporate this algorithm into a complete indexing system for large collections of handwritten historical documents, such as those used for experimentation purposes here, but also illuminated Gaelic manuscripts. Furthermore, in the latter case, a specific challenge we intend to investigate is the differ-entiation of multiple scribes within a single document based on characterizing the shapes of words. 6 Conclusions In this article, a new method for word recognition for historical manuscripts has been proposed. Extensive experimentation conducted using the George Washing-ton collection shows that systems based on word contour matchingcansignificantlyoutperform existingtechniques. Specifically, it has been shown that on a set of 20 pages, the average recognition accuracy was 83 % . Adaptation of the contour matching to the specific task of word recognition in order to improve performance and reduce computational load together with a simple pruning tech-nique was also discussed. Moreover, preliminary investi-gation of potential simplifications allows us to speculate that additional development would further improve the performance.
 References
