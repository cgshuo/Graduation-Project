
Contextual factors can greatly influence the utility of rec-ommendations for users. In many recommendation and per-sonalization applications, particularly in domains where user context changes dynamically, it is difficult to represent and model contextual factors directly, but it is often possible to observe their impact on user preferences during the course of users X  interactions with the system. In this paper, we in-troduce an interactive recommender system that can detect and adapt to changes in context based on the user X  X  ongoing behavior. The system, then, dynamically tailors its recom-mendations to match the user X  X  most recent preferences. We formulate this problem as a multi-armed bandit problem and use Thompson sampling heuristic to learn a model for the user. Following the Thompson sampling approach, the user model is updated after each interaction as the system ob-serves the corresponding rewards for the recommendations provided during that interaction. To generate contextual recommendations, the user X  X  preference model is monitored for changes at each step of interaction with the user and is updated incrementally. We will introduce a mechanism for detecting significant changes in the user X  X  preferences and will describe how it can be used to improve the performance of the recommender system.
 Context-aware recommendation; Collaborative filtering; Multi-armed bandit problem; Thompson sampling
Context-aware recommender systems have been the focus of various research studies in the past few years. The main motivation for designing these systems is that the contex-tual factors can influence users X  preferences. Therefore, the utility or usefulness of each recommended item does not re-main static and changes over time based the current context of the user.

The system may not always have complete knowledge of the contextual factors and their values at all times. Based on the classification presented in [1], knowledge of a recom-mender system about context can be of three types: fully observable, partially observable, and unobservable. The con-textual information is fully observable if all the relevant contextual factors, their structure and values are explicitly known. On the other hand, if only part of this knowledge is available, then it is partially observable to the system. For example, if a restaurant recommender system knows that lo-cation, and time are the only important contextual factors and the values of these factors are explicitly given to the system, the contextual knowledge is fully observable in the system. But often part of this information might be missing or unknown at design time making context only partially ob-servable. In the case of unobservable knowledge, no explicit information is available about the contextual factors.
If the system has full knowledge of the contextual factors, a representational approach to modeling context [6] can be used where context is defined and represented as a specific set of attributes of the environment within which the user X  X  interaction with the system takes place. On the other hand, if context is unobservable or only partially observable, a rec-ommender designed based on the representational view will not be able to adapt to unforeseen contextual situations that arise dynamically. However, the activity that arises from the context can be observed in terms of changes in user prefer-ences and behavior. Therefore, in such situations, an in-teractional view to modeling context [6] may be the more appropriate.

Although representational context has been extensively investigated [11][3][9], there has been less focus on interac-tional systems in context-aware recommendation research. This is partly due to the fact that it is more difficult to cap-ture the contextual information when it is not known to the system at design time.

In this paper, we investigate this problem using an interac-tive recommender system framework that can dynamically adapt to changes in context. At each step of the interac-tion, the system presents a list of recommendations and the user provides feedbacks for the recommended items. The feedbacks can be explicit, such as item ratings, or it can be implicitly inferred from user X  X  actions such as a click-through, lingering on a Web page, or purchasing a product. The user X  X  feedback is an indication of the recommendation X  X  utility and is used by the recommender to update the user X  X  preference model after each interaction.
The main objective of the proposed recommendation ap-proach is to identify the recommendation list at each step such that the average obtained utility is maximized over the interaction session with the user. Choosing the best rec-ommendation list at each step requires a strategy in which the best trade-off between exploitation and exploration is achieved. During an exploitation phase, the recommender system generates the recommendation list maximizes the im-mediate utility for the current step. During exploration, the system may recommend items that do not optimize the im-mediate utility, but reduce the uncertainty in modeling a user X  X  preferences and eventually help maximize the utility over the whole session.

This problem can be formulated as a multi-armed ban-dit problem (MAB) which has been studied in various sys-tems and applications [5][10]. However, considering users X  changes in context when designing bandit strategies has re-ceived very little attention despite the fact that changes in users X  preferences can greatly impact the performance of in-teractive recommender systems. For example, consider an online radio application that receives users X  feedback for each song played in a playlist. Assume that a user starts the ses-sion while reading but later changes her context to exercis-ing. This change in context can greatly affect the type of feedback the user provides for the recommendations. If the recommender does not distinguish the feedbacks before and after the context transition, it may take many interactional steps to adapt the recommendations to the new contextual state of the user.

In this paper, we introduce an interactive context-aware recommendation algorithm that takes into account the user X  X  interactional context. Our algorithm tracks changes in a user X  X  feedback behavior and detects any sudden significant changes in the user X  X  preferences. It will then adapt the rec-ommendations to match the user X  X  most current preferences. In our experiments, we evaluate the precision of our method in detecting contextual changes and investigate the extent to which our context-aware recommendation approach can provide improvements over non-contextual baselines.
Several researchers have previously investigated the use of contextual information in various recommendation ap-plications [13, 1]. Many of these studies assume a repre-sentational view to modeling context [11, 3, 9]. In [11], a context-aware playlist recommendation system is proposed which provides users the opportunity to browse their mu-sic by mood. The system described in [3], focuses on social context and proposes Poolcasting as a method to customize musical sequences for groups of listeners. The music rec-ommender introduced in [9], recommends music suited for places of interest. The contextual recommender described in [8] assumes that context is explicitly given as a query to the system. Our approach is different from [11], [3], [9], and [8] as the contextual factors are not pre-specified in our system.
In [7], a context-aware music recommender is introduced which infers contextual information based on the most re-cent sequence of songs liked by the user and their associated top frequent tags mined from social tagging Web sites. The system introduced in this paper is different from [7] in vari-ous aspects: the recommender presented in this paper does not rely on the item meta-data such as tags (which may not always be available). Instead, contextual changes are inferred just based on users X  feedbacks on items. Also, the presented recommender in this paper optimizes the utility over the whole user session rather than individual interac-tions.

Bandit algorithms have been widely used for the applica-tion of producing personalized recommendations [5, 10]. The news recommender introduced in [10] adapts to the temporal changes of existing content. In the proposed system, both users and items are represented as sets of features. Features of a user can include demographic attributes as well as fea-tures extracted from the user X  X  historical activities such as clicking or viewing items while item features may contain descriptive information and categories. They formulate this problem as a bandit problem with linear payoff and intro-duced an upper confidence bound bandit strategy which is shared among all the users. Similarly, the system in [5] uses a bandit algorithm based on Thompson sampling for the task of news recommendation and display advertisement. Sim-ilar to [10], several features are used to represent the user and the items and the recommender learns a single bandit strategy for all users while representing each user as a fixed set of features. Our approach is different from [10] and [5] as we are learning a bandit strategy for each user. While these two methods focus on adapting to the changes in content (or items), we are interested in capturing and adapting to the changes in users X  interests. Representing each user as a fixed set of features, as in [10] and [5] would prevent us from detecting changes in the users X  preferences and captur-ing interactional context. However, our approach enables us to monitor changes in the users X  behaviors. [4] uses a multi-armed bandit approach for mobile contex-tual recommendation.The main difference of our approach with [4] is that we are not taking a representational view to-wards context, instead we are interested in capturing change of behavior which can be induced by contextual changes.
Interactive recommenders have been extensively used in a variety of application domains. These systems usually rely on an online learning algorithm that gradually learns users X  preferences. At each step of the interaction, the sys-tem generates a list of recommendations and observes the user X  X  feedback on the recommended items indicating the utility of the recommendations. The goal of such a system is to maximize the total utility obtained over the whole in-teraction session.
Choosing the best strategy to select the recommendation list at each step can be formulated as a multi-armed bandit problem (MAB) . Consider a gambler who can choose from a row of slot machines known as one-armed bandits. The gambler receives a reward from playing each of these arms. To maximize his or her benefit, the gambler should decide on which slot machines to play, the order of playing them and the number of times that each slot machine is played. A sim-ilar type of decision problem occurs in many real-world ap-plications including interactive recommender systems. For an interactive recommender, the set of arms corresponds to a collection of items that can be recommended at each step, and the rewards correspond to the user X  X  feedback, such as ratings or a clickthrough on a recommended item.

An important consideration in a bandit algorithm is the trade-off between exploration and exploitation . Exploitation is referred to the phase where the available information gath-ered during the previous steps is used to choose the most profitable item. During exploration, the most profitable al-ternative may not necessarily be picked but the algorithm may choose other items in order to acquire more information about the preferences of the user which would help to gain more rewards in future interactions.

Several techniques have been proposed for solving the multi-armed bandit problem. Optimal solutions for multi-armed bandit problems are generally difficult to obtain com-putationally. However, various heuristics and techniques have been proposed to compute sub-optimal solutions effec-tively. The -greedy approach is one of the most famous and widely used techniques for solving the bandit problems. At each round t , the algorithm selects the arm with the highest expected reward with probability 1  X  , and selects a ran-dom arm with probability . The Upper Confidence Bounds (UCB) class of algorithms, on the other hand, attempt to guarantee an upper bound on the total regret (difference of the total reward from that of an optimal strategy).
Thompson Sampling is another heuristic that plays each arm in proportion to its probability of being optimal. As we are using this heuristic in our system, we briefly describe it in more details. In the next section, we will discuss how this heuristic is used in our system to generate the recommenda-tion list at each interaction step.

The reward (or utility) distribution for each item can de-pend on various factors including characteristics of the item, general preferences of the user, and the current context of the user. Let p ( r | a,  X  ) represent the utility distribution for item a , and  X  indicate the unknown parameter characterizing the distribution. Also, let E r ( r | a,  X  ) represent the expected reward for the item a for the given  X  .

At the first step of an interaction, the set of observations, denoted by D , is empty and p (  X  | D ) is initialized with a prior distribution for  X  . As more rewards are observed at each step, Bayesian updating is performed to update the  X  distribution based on the observed rewards.

Algorithm 1 describes the Thompson sampling procedure [5, 15]. At each step,  X  is drawn as a sample from p (  X  | The expected reward for each of the items is then computed and the arm having maximum expected reward is played and the reward for that arm is observed. The selected arm and the observed reward is then added to the observations set.
 Algorithm 1 Thompson Sampling
D=  X  for t =1to T do end for
We adapt Thompson sampling heuristic as the bandit strategy for generating a recommendation list at each step of interaction with a user. In this setting,  X  which char-acterizes the utility distribution for each item, represents a user X  X  preference model. It is a k -dimensional random vec-tor drawn from an unknown multivariate distribution. The user model is updated after each interaction.

Linearly parameterized bandits[14] are a special type of bandits where the expected reward of each item is a linear function of  X  . It has been shown that Thompson sampling with linear rewards can achieve theoretical regret bounds that are close to the best lower bounds [2]. In our approach we also assume that the rewards follow a linear Gaussian distribution.

Given d observations, let r  X  R d be the observed rewards for the recommended items and let F represent a d  X  k con-stant matrix containing the features of the recommended items. For example, given two observations ( a 1 , r 1 )and( a r ), r =[ r 1 ,r 2 ] has two elements r 1 and r 2 and F =[ f where f a i represents the k -dimensional feature vector for item a i . The item features can be obtained in various ways. They can be extracted based on the available content data for the items or can be extracted from the users X  feedback us-ing different techniques such as matrix factorization. Given a training data set represented as a matrix of users X  prefer-ences on items, we apply Principal Component Analysis to represent each item in a k -dimensional space (where k the number of users in the training data).

Assuming a linear Gaussian distribution, we have the fol-lowing prior and likelihood distributions:
Also, we assume  X   X  and  X  r are diagonal matrices with diagonal values of  X   X  ,and  X  r respectively.

Given a linear Gaussian system, the posterior p (  X  | r ) can be computed as follows (see [12] for a proof):
At each step t ,  X  t is drawn as a sample from the posterior distribution. The expected reward for each item x is then estimated as r x = f x  X   X  ,where f x represents the extracted features for the item x . The items are then ranked based on the estimated expected reward values. The item with the highest reward is recommended to the user.
This section describes our approach for adapting the rec-ommendations to contextual changes. Our method has two main steps. The first step, is to detect context transitions and the second step is to adapt the recommendations based on the detected transitions.

Our system includes a change detection module that tracks users X  interactions and detects any sudden and significant changes in the users X  behaviors. At each step t of interaction with a given user u , the change detection module models the user X  X  interactions in interval I t =( t  X  N,t ]aswellasthe interactions in the interval I ( t  X  N ) =( t  X  2 N,t  X  N ], where N is a fixed pre-specified parameter representing the length of the interval. These two windows are modeled by computing the following posterior distributions:
The above two distributions can each be computed ac-cording to equation 3 where r ,and F are substituted with the rewards and the item features of the recommendations in the corresponding window.

The distance between W I t and W I ( t  X  N ) is used as a mea-sure of dissimilarity between these two windows. Various measures such as KL-divergence can be used to compute the distance between the two windows. In our experiments, we used Mahalanobis distance which is computed as follows:
If the user feedback behavior is significantly different be-tween the two windows, then the models for W I t and W I ( would be different. Therefore, the change detection mod-ule in our system is based on the heuristic that sudden changes in the users X  feedback behavior would result in sud-den changes of the distance measure computed in 8. Based on this heuristic, we compute the distance measure at each step t , and exploit a change point analysis approach to de-tect sudden changes in a user X  X  feedback behavior.
In our system, we utilize the technique described in [16] for change point analysis. This procedure involves iteratively applying a combination of cumulative sum charts (CUSUM) and bootstrapping to detect the changes. For each of the detected changes, a confidence level is provided which rep-resents the confidence of the predicted change point. Given a pre-specified threshold, only those change points that have confidence level above that threshold are accepted. Also, in our adaptation of this approach, we have incorporated a look ahead ( LA ) parameter to improve the accuracy. Our change detection module accepts a change point only if there are at least LA observations after the predicted change point.
In our system, we assume a user X  X  interactions after the change point reflect his or her most recent preferences and the interactions before the change point reflect the user X  X  preferences in a different context. The strategy to aggregate the preferences from different contextual states depends on the specific application in which the recommender is being used. In some applications, a user X  X  preferences in a different context may still contain some useful information about the general preferences of that user. In this case, a good strat-egy for the recommender would be to consider all the user X  X  preferences while giving more weight to the feedbacks in the current context. In other applications, a user X  X  preferences in different contexts are almost independent. For example, the user may follow different independent tasks at different contextual states. In this situation, a good strategy would be to provide recommendations just based on the prefer-ences corresponding to the current context of the user, or in other words, discarding the interactions before the change of context. The recommender presented in this paper, discards users X  interactions before the last detected change point and produces the recommendations only based on the interac-tions in the current context.

Depending on the type of utility (binary, rating), the change point detection may falsely detect change points at earlier interactions where it starts to learn the users X  prefer-ences. To avoid this situation, we have incorporated another parameter in our model to avoid splitting the user profile if there are fewer interactions in the user X  X  profile than the pre-specified splitting threshold .

In our future work, we plan to investigate adapting a ban-dit strategy that considers all of user X  X  preferences. This would include, defining a utility function that while giving more weight to the preferences in the current context, also considers the interactions before the change point.
Evaluation of contextual recommenders can be very chal-lenging if contextual factors are unobservable. In this case, as explained in section 2, context cannot be represented as a set of known attributes. Instead, change of context is inferred from the change in users X  activities and interac-tions with the system. There are various datasets based on representational view to context. However, there are very few publicly available datasets that explicitly contain users X  change of preferences that could be used as the ground truth for evaluation if contextual factors are not known. In our evaluations, we designed two different experiments for simu-lating users X  change of behaviors. In our first experiment, we used Yahoo user-artist ratings dataset where ratings are in the range of 0 to 100. To simulate sudden changes in a user X  X  rating behavior, we built test hybrid profiles by merging two random user profiles. Treating the two selected user profiles as the rating behavior of a single hybrid user in two different contextual states, we evaluated our method in correctly pre-dicting the change of context and producing context-aware recommendations.

In the second experiment, we used a Web navigational dataset containing users X  browsing sessions. Assuming that a user X  X  change of context is more likely to happen between sessions rather than within sessions, we evaluated our algo-rithm ability in correct detection of the transition between two sessions. We then compared our recommender with var-ious non-contextual baselines. The dataset used in our first experiment was the Yahoo! Music ratings of musical artists version 1.0. This dataset represents a snapshot of the Yahoo! Music community X  X  preferences for various musical artists. It contains over ten million ratings of musical artists given by Yahoo! Music users over the course of a one month period sometime prior to March 2004. It contains ratings in the range of 0 to 100 given by 1948882 users to 98213 artists.

This experiment evaluated our algorithm in adapting to changes in users X  rating behaviors that could be a result of contextual changes. For example, members of a house-hold may use a single account to rate movies on a movie streaming application. Each member can rate movies differ-ently and as a result, the household hybrid user may show significant differences in rating behavior. The purpose of this experiment was two-fold: first, we evaluated the accu-Method Parameters
Bandit recommender Number of features = 5,  X   X  =0,  X  =0.5,  X  r =0.5
Contextual Recommender (proposed method) = 0.5,  X  r = 0.5, Change detection window size = 5, Change
User-based k NN Number of neighbors = 10 racy of our change detection approach in real-time detec-tion of changes. Secondly, we examined the performance of our recommender compared with non-contextual meth-ods. The evaluation was performed in the framework of an interactive recommender that interacts with the user in a number of consecutive rounds. In each round, the recom-mender presents a recommendation and observes the user X  X  feedback (utility) for the recommended item. The recom-menders were evaluated based on the average utility gained over the session of interaction.

We followed a five-fold cross validation approach for eval-uation. In each round, one of the folds was used for testing, one was used for tuning the parameters and the remain-ing folds were used for training the model. To simulate the changes in a user X  X  rating behavior, we randomly selected two users u 1 and u 2 from our test data and combined them to create a hybrid user u h . We treated each of these two users as the rating behavior of a single hybrid user in two different contextual states.

To simulate an interactive recommendation process, at each step of interaction, the recommender used the ratings in the hybrid test user profile to recommend a new item that has not been presented to the user before. We randomly chose an item rating from the u 1 profile and used it as the initial profile of the hybrid user. For the first T = 30 rounds of interaction, the observed utility for each recommended item x , was assumed to be the rating assigned to x by the user u 1 . We assumed that the contextual change occurs after T steps of interactions. Therefore, for the next T =30 steps, the same process was repeated with the difference that the observed utility for a recommended item was set to the rating for that item in the profile of u 2 . For this evaluation, we filtered the test users so that each user had at least T ratings in the user X  X  profile.

If the user has not rated a recommended item, one option would to ignore the recommendation and discard it from the evaluation results. However, this would create a bias in the experiment. If the user rates none of the recommendations, then the recommender should be negatively scored (in com-parison to a recommender that recommends items that at least are rated by the user). Setting the utility to zero for the non-rated items would also be too strict, as the user may not necessarily dislike the items that she hasn X  X  rated yet. Another possibility would be to set the utility to aver-age rating of all the users for that item. This also creates a bias as item popularities are not uniform. For example, if an algorithm suggests very rare items with high average ratings, it will gain a high utility score in the evaluation. A similar bias occurs if the average rating of the user is used as the utility for non-rated items. In our evaluation, if a recommended item had not been rated in the user X  X  profile, the observed utility was set to the average rating of all items in the dataset.

The set of parameters used in this experiment for each of the algorithms are specified in table 1.

User-based k NN was used as one of our baselines for this experiment. Cosine similarity metric was used to compute the similarity of users. In each round of interaction, the k NN algorithm computed the item scores and recommended a new item that has the highest score.

The optimal recommender was chosen as another baseline to compare our method with an ideal algorithm. This rec-ommender has full knowledge of users X  preferences and the contextual changes. For the first T = 30 iterations, it fol-lowed a greedy strategy and recommended new items that had the highest rating in the u 1 profile. After change of con-text at T = 30, in each round it recommended a new item that had the highest rating in the u 2 profile. The standard bandit algorithm with Thompson sampling heuristic (as de-scribe in section 4.2) was used as another baseline in our experiment and was trained with the parameters specified in table 1. In the remaining of the paper, we refer to this baseline as the standard bandit recommender.

Table 2 illustrates an example interaction session gener-ated based on the standard bandit recommender for this experiment.

The recommendations that are not rated in the user X  X  pro-file are marked as  X - X . For each recommended artist, a few of most similar artists that have previously been presented to the user are also shown. The artist similarities are deter-mined based on the Last.FM data. For any given artist, the Last.FM API can be used to find the most similar artists as well as the degree of similarity between the artists. In table 2, the similarity levels are abbreviated as M(medium), H(high), VH(very high), and SH(super high). As can be Figure 1: Experiment I-Average utility at each iter-ation seen, most of the first few (5) recommendations are not rated by u 1 . After iteration 6, the recommender starts generating better recommendations as most of them have high ratings in the u 1 profile. After the simulated change of context oc-curs at iteration 31, the user ( u 2 ) has not rated most of the recommendations, showing the dramatic degradation of the recommender performance. Note that many of the recom-mendations are artists similar to those rated highly by u For example, Foo fighters recommended at iteration 31, is highly similar to Nirvana recommended at iteration 21 or Hoobastank, recommended at iteration 32, has super high similarity to 3 Doors Down, which has received utility of 100 at iteration 27. This example shows that a traditional bandit approach cannot adapt to the changes in the users X  preferences.

Figure 1 presents the average utility at each iteration com-puted over the set of test users. The optimal recommender achieves the highest utility of 96 at the first step and its utility decreases as more iterations pass. This can be ex-plained by the fact that in the first T iterations items are recommended in the decreasing order of their ratings in the u 1 profile, therefore the average utility gradually decreases. After iteration 30, the optimal baseline recommends items based on their ratings in the u 2 profile and so a jump in utility can be observed at iteration 31. Both the standard bandit recommender and the contextual recommender need 5-6 iterations to learn users X  preferences and show a dra-matic improvement in the utility. As indicated in table 1, the number of factors is set to 5 for these two methods. We repeat this experiment for different number of factors. The experiments revealed that by lowering the number of factors, it takes less iteration for the algorithm to reach its best per-formance. However, the highest achieved utility would be smaller as well.

Figure 2 illustrates the average distance computed as in equation 8 in each round of interaction. As can be seen in the figure, at iterations 33 to 35, distance has the maximum value. As the window size for change detection is set to 5, it takes at least 3 rounds until W I t mostly contains the interactions after the change of context and W I t  X  N only has the interactions before the change of context and therefore, the distance metric become maximized.

Figure 3 presents the average number of change points detected at each iteration. As evident in the figure, the av-erage frequency of change points reaches the maximum value of 0.5 at iteration 6 where W I t  X  N has one interaction and W I t window contains 5 (window size) interactions. Setting Figure 2: Expriment I-Average distance at each it-eration the splitting threshold to 10 ensures that the user profile is not split for the detected change points before round 10. Also, at iteration 38 there X  X  another local maximum point where average frequency of change points reaches 0.4. This peak corresponds to the change of context in the hybrid user profile.
The data set used in our second experiment is the  X  X TI data X , which is generated based on the server log data from the host Computer Science department. The site contains various online applications, including admissions applica-tion, online advising, online registration, and faculty-specific Intranet applications. The dataset contains 20950 sessions. We pre-processed the data to remove pages with frequency of one in the whole dataset and also the top 30 most fre-quent pages. After pre-processing the data, we identified 5319 users, and 2453 distinct pageviews. Following a 10-fold cross-validation approach, the data was randomly split into ten folds. In each run, one of the folds was used for testing, one was used for tuning the parameters and the remaining folds were used for training the model. For evalu-ation, we further filtered the test cases to include only those Figure 3: Experiment I-Average frequency of change points at each iteration users that have at least two sessions in their profiles each containing at least L pageviews where L is the rounded up average session size and equals to seven in our dataset. For each test user, we randomly chose two sessions s 1 and s 2 (with the minimum length of L ) and discarded the remain-ing sessions. We chose the first pageview in s 1 and used it as the initial profile (or entry page) of the user. To simulate an interactive recommendation process, for the first T =10 iterations, the recommender used the current pageviews in the user X  X  profile to recommend an item that she had not yet viewed. If s 1 contained the recommended page, then it was counted as a hit , and otherwise, it was counted as a miss . The recommended item and the corresponding binary feedback (miss or hit) were then added to the user X  X  profile. After T = 10 iterations, we assumed the user starts a new session, s 2 . For the new session, the same interactive rec-ommendation process was repeated for another T iterations with the difference that if the recommended page was in-cluded in s 2 , it was counted as a hit, and otherwise a miss. Note that the user profile in iterations between T and 2 T contains the recommended items from both sessions s 1 and s .

The recommendation approaches were evaluated based on average Click Through Rate (CTR) obtained in each iter-ation across all the test users. CTR at iteration t equals to the total number of hits at step t divided by the total number of recommendations at that step.

We used First Order Markov Model as one of our base-lines. This approach models each page as a state and the state transition probabilities are estimated from the naviga-tional training data. It generates recommendations based on the state transition probability of the last page in the active user session.

The standard bandit recommender with Thompson sam-pling heuristic was used as another baseline in our approach. The set of parameters used for each of evaluated algorithms are specified in table 3. Figure 4 indicates the average CTR over the set of all the test users at different interac-tion steps for each of the recommendation algorithms. The CTR value for both the standard bandit and the contextual recommenders almost increases at each step for the first 7 iterations. The CTR reaches its maximum at the seventh iteration and has value of 0.31, and 0.32 for the contextual recommender and the standard bandit recommender respec-tively. Similar to experiment I, the number of iterations it Figure 4: Experiment II-Click through rate at dif-ferent iterations takes for the algorithm to reach the maximum utility peak almost equals to the number of features. In our experiments, we tuned the parameters to maximize the average CTR. Figure 5: Experiment II-Average distance at each iteration
Note that the popularity of the pages is not uniform in our dataset. Some of the pages are more popular and have higher prior of being visited by the users. As the recom-mender recommends only new pages, the CTR gradually starts decreasing as the popular pages are recommended.
The contextual recommender and the standard bandit recommender both show very similar CTR values for the first 10 iterations with the contextual recommender having a slightly better performance after the third iteration. Also, after change of context at iteration 11, the CTR decreases with a higher rate (in comparison to the iteration 10). After change of context, the difference between the CTR achieved by these two algorithms starts increasing. At iteration 15, the CTR for contextual recommender is close to 0.15 while it is only 0.08 for the standard bandit recommender.
In comparison to both the bandit and contextual recom-menders, Markov model has much smaller CTR at all the iterations. For Markov Model recommender, CTR mostly increases in the first 6 iterations and reaches to its max-imum peak of 0.2 at the sixth iteration. The CTR then decreases till the last iteration. Method Parameters
Bandit recommender Number of features = 6,  X   X  =0,  X  =1,  X  r = 0.001
Contextual Recommender Number of features = 6,  X   X  =0,  X   X  =1,  X  r = 0.001, Change detection window size = 3,
Figure 5 indicates the average distance measure at each iteration, computed by the change detection module. Note that the average distance has its peak value right after change of context (at iteration 11). Figure 6 indicates the aver-age frequency of change points at each iteration. For this dataset, choosing a high confidence threshold value helps to avoid false detection of change points at the earlier itera-tions. The average detected change points have its maxi-mum at iteration 13 corresponding to the change of context. Figure 6: Experiment II-Average frequency of change points at each iteration
In this paper we have presented a novel approach for adap-tation to contextual changes in interactive recommender sys-tems. In an online recommendation scenario, at each step the system provides the user with a list of recommendations and receives feedback from the user on the recommended items. Various multi-armed bandit algorithms can be used as recommendation strategies to maximize the average util-ity over each user X  X  session. Our approach extends the ex-isting bandit algorithms by considering the sudden varia-tions in the user X  X  feedback behavior as a result of contextual changes. Our system contains a change detection component that determines any significant changes in the users X  prefer-ences. If a change is detected, the bandit algorithm based on Thompson sampling, prioritizes the observed feedback after the detected change point reflecting the preferences of the user in the current context. The proposed recommender, discards users X  interactions before the last detected change point and produces the recommendations just based on the interactions in the current context. In our future work, we plan to investigate various approaches to take into account the impact of users X  interactions in prior contexts on the cur-rent user behavior. One approach might involve defining a utility function that while giving more priority to the inter-actions in the current context, would also take into account user X  X  feedback before the change point. [1] G. Adomavicius, B. Mobasher, F. Ricci, and [2] S. Agrawal and N. Goyal. Thompson sampling for [3] C. Baccigalupo. Poolcasting: an intelligent technique [4] D. Bouneffouf, A. Bouzeghoub, and A. L. Gan  X  A  X  garski. [5] O. Chapelle and L. Li. An empirical evaluation of [6] P. Dourish. What do we talk about when we talk [7] N. Hariri, B. Mobasher, and R. D. Burke.
 [8] N. Hariri, B. Mobasher, and R. D. Burke.
 [9] M. Kaminskas, F. Ricci, and M. Schedl.
 [10] L. Li, W. Chu, J. Langford, and R. E. Schapire. A [11] O. Meyers. A mood-based music classification and [12] K. P. Murphy. Machine Learning: A Probabilistic [13] F. Ricci, L. Rokach, B. Shapira, and P. B. Kantor, [14] P. Rusmevichientong and J. N. Tsitsiklis. Linearly [15] S. L. Scott. A modern bayesian look at the [16] T. Wayne. Change-point analysis: a powerful new tool
