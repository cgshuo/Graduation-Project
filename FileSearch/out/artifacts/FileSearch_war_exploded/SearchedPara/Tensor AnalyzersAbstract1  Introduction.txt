 Yichuan Tang tang@cs.toronto.edu Ruslan Salakhutdinov rsalakhu@cs.toronto.edu Geoffrey Hinton hinton@cs.toronto.edu Exploratory Factor Analysis is widely used in statistics to identify underlying linear factors. Mixtures of Fac-tor Analyzers have been used successfully for unsuper-vised learning (Yang et al., 1999; Verbeek, 2006). Fac-tor Analyzers (FAs) model each observation vector as a weighted linear combination of the unobserved fac-tor values plus additive uncorrelated noise. For many types of data, this additive generative process is less suitable than a generative process that also contains multiplicative interactions between latent factors. An example of multiplicative interactions is the set of face images under varying illuminations. It is known that these images of a particular person approximately lie on a 3 dimensional linear subspace, making a FA with 3 factors a good model (Belhumeur &amp; Kriegman, 1996). The linear subspace (and thus the factor load-ings) will be person-specific due to the way facial struc-tures interact with the light to create a face image. As a result of this person-specific property, the factor loadings need to be a function of the person identity when modeling face images of multiple people. In-stead of modeling 100 individuals with 100 separate FAs, it is desirable to use person-identity variables to linearly combine a  X  X asis X  (a set of) factor loadings to compactly model all 100 faces, drastically reducing the number of parameters. This factorial representation, as shown in Fig. 1, naturally allows for generalization to new people required for one-shot face recognition. Fig. 1 provides an illustration and Sec. 5.3 provides experimental validations.
 To this end, we introduce Tensor Analyzers (TAs), which generalize FAs to the multilinear setting by in-troducing a factor loading tensor and multiple groups of latent factors. Utilizing the loading tensor, a group can change how another group X  X  factors interact with the observed variables. This allows latent factor groups in a TA to learn highly interpretable represen-tations. In the faces example, one group could repre-sent lighting direction while the other could represent the identity.
 In the special case of a TA with only one group of factors, its loading tensor reduces to a loading matrix, and the model is exactly the same as an ordinary FA. In a TA, when conditioned on all but one group of factors, the model effectively becomes a FA where the factor loadings are a function of the factor values in the groups we are conditioning on. The posterior dis-tribution of the factor values in a FA can be computed analytically, so by cycling through each group of fac-tors, efficient alternating Gibbs sampling is therefore possible in the TA. A TA is a proper density model so the extension to a mixture of TAs (MTA) is straight-forward. When performing inference or learning in a TA, it is easy to make use of a supervisory signal that specifies the fact that 2 or more different observations are generated from the same factor values in some of the factor groups. This allows for TAs to seamlessly transition from an unsupervised density model to a semi and fully supervised model. Bilinear models with priors on the latent variables have been previously studied in the machine learning, statistics, and computer vision literatures. In (Grimes &amp; Rao, 2005), sparsity is induced on the codes of a bilinear model to learn translational invariant rep-resentation from video. However, their model does not try to maximize log p ( x ), but instead finds the MAP estimate of the code activations, `a la sparse cod-ing. Culpepper et al. (2011) describe an outer-product factorization of the bilinear model that is trained as a density model using EM, but expensive Hamiltonian dynamics are required for sampling from the posterior. In addition, their model only admits an approximate M-step and does not make it easy to incorporate la-bel information. Wang et al. (2007) proposed a multi-factor GPLVM extension for modeling human motion (henceforth referred to as GPSC). In their model, the parameters are integrated out, and factors are kernal-ized. Optimization is needed to find the latent coordi-nates. Computationally, as in GPLVM, GPSC scales cubicly in the size of the training data.
 While TAs and the above models take as input i.i.d. data vectors, there exists a plethora of tensor decom-position (TD) methods when the data comes in the form of N-way tensors (Tucker, 1963; Carroll &amp; Chang, 1970; Lathauwer &amp; Vandewalle, 2004; Wang &amp; Ahuja, 2003; Sun et al., 2006). The SVD algorithm was used to learn a bilinear model to separate style and content (Tenenbaum &amp; Freeman, 2000), which we will refer to as the S&amp;C model. Tucker decomposi-tion was applied to a 5-mode array of face images in (Vasilescu &amp; Terzopoulos, 2002), finding multilin-ear bases called TensorFaces. Shashua &amp; Hazan (2005) enforced non-negative constraints to PARAFAC de-composition. Chu &amp; Ghahramani (2009) introduced a Bayesian probabilistic version of Tucker decomposi-tion, while Xu et al. (2012) provided a nonparametric Bayesian extension. The main disadvantages of the tensor decomposition methods are that data must be arranged in a tensor and that inference given a single new test case is ill-posed and can be ad hoc 1 . In contrast, TAs do not have any of the above deficien-cies. Our main contribution is in the introduction of standard Gaussian priors on each latent groups, which allows us to utilize the efficient inference procedure of Factor Analysis as part of TA X  X  inference procedure. We also provide the EM algorithm that allows TAs to learn directly from data vectors in an entirely unsuper-vised manner. It can also make use of supervision in the form of equality constraints that specify that one group of factors should have the same vector of values for a subset of the training cases (Sec. 4.4). As an extension to FA, TA inherits an efficient inference al-gorithm that is used in each step of alternating Gibbs sampling and a closed-form M-step during learning. Unlike bilinear models, it can handle multilinear cases with 3 or more groups of latent factors. It can also be easily extended to a mixture model, provided we are willing to compute approximate densities, as de-scribed in (Sec. 4.3). In addition, posterior inference for a single test case is simple and accurate, as demon-strated by our one-shot face recognition experiments of Sec. 5.3. Following (Kolda &amp; Bader, 2009), we refer to the num-ber of dimensions of the tensor as its order (also known as modes). We will use bold lowercase letters to denote vectors (tensors of order one), e.g. x ; bold uppercase letters for matrices (tensors of order two), e.g. W . We use the notation w ( i, :) to denote the i -th row of matrix W . Higher order tensors are denoted by Euler script letters, e.g. a third-order tensor with dimensions of I , Fibers : Fibers are higher-order generalization of row/column vectors. Elements of a tensor fiber is found by fixing all but one index. Specifically, t (: ,j,k ) is the mode-1 fiber of the tensor T . Row and column vectors are the mode-2 and mode-1 fiber of a 2nd-order tensor, respectively.
 Matricization : Matricization is the process of  X  X lat-tening X  a tensor into a matrix, by reordering the ele-ments of the tensor. It is denoted by T ( n ) , where the mode-n fibers of T are placed in the columns of the re-sulting matrix T ( n ) . For example, given T  X  R I  X  J  X  K T n-mode vector product : By multiplying a vec-along the mode-n , the n -mode (vector) product is de-noted by T  X   X  n y . The resulting tensor is of size D 3.1. Factor Analyzers Let x  X  R D denote the D -dimensional data, let { z  X  R d : d  X  D } denote d -dimensional latent factors. FA is defined by a prior and likelihood: p ( z ) = N ( z ; 0 , I ) , p ( x | z ) = N ( x ;  X z +  X  ,  X  ) , (1) where I is the d  X  d identity matrix;  X   X  R D  X  d is the factor loading matrix,  X  is the mean. A diagonal  X   X  R D  X  D represents the variance of the observation noise. By integrating out the latent variable z , a FA model becomes a Gaussian with constrained covariance: where  X  =  X  X  T +  X  . For inference, we are interested in the posterior, which is also a multivariate Gaussian: where V = I +  X  T  X   X  1  X  , and m = V  X  1  X  T  X   X  1 ( x  X   X  ). Maximum likelihood estimation of the parameters is straightforward using the EM algorithm (Rubin &amp; Thayer, 1982). During the E-step, Eq. 3 is used to compute the posterior sufficient statistics. During the M-step, the expected complete-data log-likelihood E the model parameters  X  = {  X  ,  X  ,  X  } . TA replaces FA X  X  factor loading  X   X  R D  X  d with a a TA has J groups of factors: { z 1 , z 2 ,..., z J } : j = interactions between a particular factor group and the data is modified by the factor values in other groups. Given { z 2 ,..., z J } ,  X  new , which is the factor loading matrix between z 1 and x , is given by: We will use the notation TA { D,d 1 ,d 2 ,...,d J } to de-note the aforementioned TA. By using a ( J + 1)-order tensor T , a TA can model multiplicative interactions among its latent factors { z 1 , z 2 ,..., z J } . In contrast, FAs do not model multiplicative interactions involving terms such as z i z j : i 6 = j .
 Each group of factors has a standard Normal prior: For clarity of presentation, we assume J = 3 for the following equations. The likelihood p ( x | z 1 , z 2 , z where x  X  R D , m , and  X  are same as in FA. W R  X   X   X  is the Kronecker product operator. Multiplicative interactions are due to the term: z 3  X  z 2  X  z 1 , which is a vector with dimensionality of d 1 d 2 d 3 . We note that where z 1 ( i ) is the i -th element of vector z 1 , and t is the mode-1 fiber of T . T (1) ( z 3  X  z 2  X  z 1 ) is also equivalent For clarity, we can concatenate the factors and load-R TA is: log p ( x , z 1 , z 2 , z 3 ) = where e = Wy + T (1) u . The last term of the above equation indicates that the TA models contain higher-order interactions (squared of the outer product of all factors). In comparison, FAs have only 2nd-order in-teractions among its latent factors. Fig. 2 displays a visual diagram of the TA X  X  generative process. Conditioned on any two of the three groups of factors, e.g. z 2 and z 3 , the log-likelihood of x and z 1 becomes: log p ( x , z 1 | z 2 , z 3 ) =  X  Here, e can be re-written as ( m + W 2 z 2 + W 3 z 3 ) + ( W 1 + T  X   X  3 z 3  X   X  2 z 2 ) z 1 . We can see that condi-tioned on z 2 and z 3 , we have a FA with parameters (c.f. Eq. 1): The marginal probability density function is a Gaus-sian: p ( x | z 2 , z 3 ) = N ( x |  X  ,  X  X  T +  X  ). 4.1. Inference Higher order interaction in the TA means that in-ference is more complicated, since the joint posterior p ( z 1 , z 2 ,..., z 3 | x ) has no closed-form solution. We re-sort to alternating Gibbs sampling by cycling through p ( z 1 | x , z 2 , z 3 ); p ( z 2 | x , z 1 , z 3 ); p ( z Conditioned on two groups of factors, the posterior of the third is simple as the model reduces to a FA: p ( z 1 | x , z 2 , z 3 ) = N ( z 1 | V  X  1  X  T  X   X  1 ( x  X   X  ) , V where V = I +  X  T  X   X  1  X  .  X  ,  X  are defined by Eq. 7. Although inference involves a matrix inverse, it only has cost of O ( d 3 ), where d &lt;&lt; D , is the dimension of a latent factor group. d can be small since the data is as-sumed to be explained by a low dimensional manifold. We provide detailed timing evaluations in Sec. 5.5. 4.2. Learning Maximum likelihood learning of a TA is similar to FA and is straightforward using a stochastic variant of the EM algorithm (Levine &amp; Casella, 2001). Dur-ing the E-step, MCMC samples are drawn from the posterior distribution using alternating Gibbs sam-pling. In the M-step, the samples are used to ap-proximate the sufficient statistics involving u and y , followed by closed-form updates of the model param-eters,  X  = { W , T (1) ,  X  } .
 Algorithm 1 EM Learning for TA The expected joint log-likelihood function is: Q = E h log Setting  X  X   X  X  = 0, we have update equations (see Supp. Materials for the derivation):
During learning, we have found that the Gibbs sam-pler mixes very fast and that a relatively small number (20 to 100) are needed to achieve good performance. It is important to stress that Gibbs sampling is efficient in TAs because the posterior for each group is exact when conditioned on all other groups. It also mixes quickly because even though the variables of the pos-terior are dependent, the posterior itself is likely to be unimodal: an image of the face is explained by 1 light-ing code and 1 subject code (See Sec. 5.3). We provide trace plots of the latent variables to demonstrate the fast converegnce property in the Supp. Materials. 4.3. Likelihood Computation For model comparison, we are interested in evaluat-ing the data log-likelihood log p ( x |  X  ). As noted in Sec. 4, a TA with J groups of factors reduces to a FA when conditioned on J  X  1 factor groups. Utilizing the fact that log p ( x |  X  ) can be easily computed (Eq. 2), a Monte Carlo estimation of data log-likelihood in TA can be performed by sampling from the prior of the J  X  1 groups of factors. For example, in a model TA { D,d 1 ,d 2 } , J = 2: log p ( x ) = log This simple estimator is asymptotically unbiased but has high variance unless the dimensionality of z 2 , or d is very small. Since z 1 can be analytically integrated out, the Monte Carlo technique can be accurate when only one factor group has large dimensionality. For large d j , however, simple Monte Carlo estimation is very inefficient, giving an estimator with large vari-ance. In this situation, Annealed Importance Sam-pling (Neal, 2001) is a much better alternative. We can treat the problem of estimating log p ( x ) as cal-culating the partition function of unnormalized poste-an unnormalized distribution. The basic Importance Sampling gives: AIS provides a better estimate by first sampling from a tractable base distribution q ( z ). Subsequent MCMC steps are taken in a set of intermediate distribution, annealing to the distribution of interest: p ( z | x ). An-nealing allows for a much better estimate of w ( i ) . For TAs, we assume the base distribution is the prior over distribution is defined as: p ( { z j } )  X  q ( { z j } ) 1  X   X  p  X  ( { z j }| x )  X  = p ( { z where  X  is a scalar which varies from 0 . 0 to 1 . 0, as we anneal from the prior to the posterior. Derivations and experiments with the AIS estimator is provided in Supp. Materials. 4.4. Equality Constraints An equality constraint indicates that a subset { x ( k ) } K k =1 of the training data have the same factor values for the j -th factor group z j . For example, if group j = 1 represents the identity of a person and group j = 2 represents lighting directions, an equal-ity constraint on group 1 indicates that all images of the subset are from the same person, while an equal-ity constraint on group 2 indicates that images of this subset are from the same lighting conditions.
 During learning, the availability of equality constraints will only change the inference step. Assuming we have constraints for the factor group j = 1, the posterior for z j (Eq. 8) will be modified as follows: N ( z 1 |  X  V  X  1 T The M-step is not affected by the presence of equality constraints, so TAs can learn when equality constraints are provided for arbitrary subsets of the data. 4.5. Mixture of Tensor Analyzers Extending TAs to Mixture of Tensor Analyzers (MTAs) is straightforward, as Sec. 4.3 showed how p ( x | c ) can be efficiently approximated. Each com-ponent c will have its own parameters  X  c = { W c , T (1) ,c ,  X  c } . Posterior distribution over the factors and components can be decomposed as: can be sampled using Eq. 8.
 MTAs should be used instead of TAs when modeling highly multimodal data with multiplicative interac-tions such as multiple types of objects under varying illumination.
 For our experiments with MTAs, p ( c | x ) is approxi-mated with Eq. 12 using 1000 samples per mixture component. For our natural images experiment, it means only 180 ms per mixture components is re-quired, see sec 5.2. 5.1. Synthetic Data As a proof of concept, we compared TA to FA on two synthetic datasets (Fig. 3 A &amp; B). Data A is highly structured and is generated using a TA with random parameters. Data B has high kurtosis, with density concentrated at the origin. For both datasets, we learned using a TA { D = 2 ,d 1 = 2 ,d 2 = 2 } and a FA with the same number of parameters as the TA. The TAs performed model recovery nicely. The left panel of Fig. 3(a) displays training points. The data log-likelihood of the true model is -2.58. The mid-dle panel plots the samples of a TA, which achieved the log-probability of -2.62 on the training data. The right panel plots samples drawn from a FA. Like-wise in Fig. 3(b), TA is a significantly better model than the FA:  X  2 . 04  X  0 . 05 to  X  2 . 48  X  0 . 09. We also tested mixtures of TAs vs mixtures of FA (MFA) on data generated by randomly initialized MFA models. The performance of MTA and MFA were very similar, demonstrating that (M)TAs can also efficiently emu-late (M)FAs when necessary. 5.2. Natural Images Learning a good density model of natural images is useful for image denoising and inpainting. Follow-ing (Zoran &amp; Weiss, 2011), we compared MTAs to MFAs and other models on modeling image patches. Two million 8  X  8 patches were extracted from the training set of the Berkeley Natural Images database (Martin et al., 2001) for training, while 50,000 patches from the test set were extracted for testing. The DC component of each image patches were set to 0 by subtracting the patch mean form ev-ery pixel. For MTAs and MFAs 2 , we used 200 compo-nents and selected the number of latent factors based on cross validation. For MTAs, 64 factors were used while each component of the MTA is a TA { 64, 64, 5 } . For the Factorized Bilinear model, we used code from the authors 3 . Indp. Pixel and GMM results are from (Zoran &amp; Weiss, 2011), while the Deep MFA result is from (Tang et al., 2012).
 After training, Fig. 4(b) shows that samples of a MTA matches closely to the training patches in Fig. 4(a). Each row of Fig. 4(c) shows filters (fibers of the ten-sor) of one of the MTA components. Components of MTA specialize to model patches of different spatial frequencies and orientations. Fig. 4(d) lists quanti-tative evaluations of the different models. Although MTAs achieve good results, they do not outperform MFAs or GMMs with 200 mixture components 4 .
 We also compared the performance of MFAs vs. MTAs when the number of mixture components is smaller. Fig. 5 shows the average log-likelihoods on the test set. Due to the fact that MTAs can model higher-order interaction, it is able to outperform MFAs with 50 or less mixture components. However, as the num-ber of components increases, the advantage of MTAs disappears. 5.3. Face Recognition with equality constraints In a one-shot learning setting, classifiers must be trained from only one example per class. For face recognition, only one example per test subject is used for training. We use the Yale B database and its Ex-tended version (Lee et al., 2005). The database con-tains 38 subjects under 45 different lighting conditions. We use 28 subjects for training and test on the 10 subjects from the original Yale B database. The im-ages are first downsampled to 24  X  24, and we used a TA { 576 , 80 , 4 } , which contains 2 groups of factors. The learned TA allows for strong generalization to new people under new lighting conditions. It achieves an average test log-probability of 836  X  7 on the images of the 10 held-out subjects. As a comparison, the best MFA model achieved only 791  X  10. The number of components and factors of the MFA are selected us-ing grid search. The gain of 45 nats demonstrates a significant win for the TA.
 Qualitatively, to see how well the TA is able to fac-tor out identity from lighting, we first sample from the posterior distribution conditioned on a single test image using step 3 of Alg. 1. We then fix a factor group X  X  sampled values and sample the factors of the other group using its Normal prior. Results are shown in Fig. 6. A row in panel (b) shows the same person under different sampled lighting conditions. A row in panel (c) shows sampled people, but under the same lighting condition. We emphasize that only a single test image from novel subjects is used for inference (panel (a)).
 The one-shot recognition task consists of using a single image of each of the 10 test subjects for training dur-ing the testing phase. The rest of the images of the test subjects are then classified into 1 of 10 classes. Labels indicating lighting directions of the test images can be optionally provided. TAs can operate with or without the labels, while tensor decomposition meth-ods require these labels. We first use the 28 training subjects to learn the parameters for the TA { 576,80,4 } in the training phase. Equality constraints specifying which images have the same identity or lighting type are used during training. During the testing phase, a single image for each of the 10 test subjects is used to compute the mean of the posterior p ( z identity , z light using 200 Gibbs steps. For each one of the 10 sampled z identity , TA { 576,80,4 } is transformed into a person-specific FA using Eqs. 6, 7. The resulting 10 FAs define the class-conditionals of a generative classifier, which we use to classify test images of the 10 test subjects. We compared TA to standard classifiers and tensor de-composition methods. Human error is the average of testing several human subjects on the same one-shot recognition task. NN is the nearest neighbour classi-fier, and XCORR represents the normalized Cross Cor-relation. Multiclass linear SVM from the LIBLINEAR package (Fan et al., 2008) is used, where the hyperpa-rameter C was chosen using validation. Factor analyz-ers method requires the transfer of the factor loadings from the training phase. We first learned 28 FAs, one for each training subject. During the testing phase, each one of the 10 training images of the test subject is matched with the FA which gives it the largest like-lihood. A new FA is created, centered at the training image. The rest of the parameters are transferred from the matched FA. In essence, the transferred loadings model the lighting variations of the training subjects (1 of 28), which is most similar to the test phase train-ing image. After the creation of these 10 new FAs, classification is same as in the TA method.
 We also compared TA to various Tensor De-composition methods, including CP: CANDE-COMP/PARAFAC (Carroll &amp; Chang, 1970); NTF: Nonnegative Tensor Factorization (Shashua &amp; Hazan, 2005); Tucker (Tucker, 1963); and Proba-bilistic Tucker decomposition (Chu &amp; Ghahramani, 2009). For CP, Tucker, and NTF, we used the N-way toolbox (Bro, 1998). For the S&amp;C bilinear method, we implemented the exact algorithm as stated in Sec. 3.2 of (Tenenbaum &amp; Freeman, 2000). The style and content codes are adapted for new test images during one-shot learning. For GPSC, the code provided by the authors was used. In all experiments, hyper-parameters were selected by cross-validation. Recognition errors are shown in Fig. 6(d). Observe that the TA not only outperforms these methods by a significant margin, but it even achieves better than human performance. 5.4. Learning with incomplete equality We now demonstrate the advantage of the TA in a semi-supervised setting on the UMIST face database (Graham &amp; Allinson, 1998). It contains 20 subjects with 20 to 40 training images per subject. The variation consists of in-depth head rotations. We compared the TA to S&amp;C model and NN on the one-shot face recognition task. Out of the 20 subjects, 15 were used for training and 5 for testing. The split was randomized over 10 different trials. The images are downsampled to the resolution of 24  X  24. We experi-mented with equality constraints, using 3, 4, or 5 im-ages per training subject. During the training phase, a TA { 576,3,5 } model was trained using 30 EM itera-tions. The algorithms for classification are exactly the same as in Sec. 5.3. Fig. 7 plots recognition errors as a function of the number of images per subject used during the training phase . For the case where 5 im-ages per training subjects are available, TA achieves significantly lower errors at 12.4% compared to 24.9% for S&amp;C. If we add an equal number of images with-out equality constraints during training, the error is further reduced to 10.9% . (An additional experiment on concept learning from colorful shapes is presented in the Supp. Materials). 5.5. Computation Time Our experiments used Matlab on a standard multicore workstation, we report the computation time required for inference and learning. MTA on face images in Sec. 5.3: inference per image per Gibbs update takes 14 ms (milliseconds), while learning took a total of 587 secs. MFA learning took around 108 secs. In practice, matrix inversion is not very expensive in our model. For the face recognition model TA { 576,80,4 } , while having the same number of parameters as a FA with 320 latent factors (which requires inverting 320x320 matrix), only require inversion of 80x80 and 4x4 ma-trices, which takes 0.4 ms. MTA on natural images: inference per image per Gibbs update takes 47 ms, while the entire learning algorithm took 33 hours. In comparison, inference per image in Mixture of Factor Analyzers (MFA) takes 18 ms, while learning took 9.5 hours. We have introduced a new density model which ex-tends Factor Analysis to modeling multilinear inter-actions. Using efficient alternating sampling and the EM algorithm, we have shown that (M)TAs can learn more complex densities and separate factors of varia-tion, leading to the learning of simple concepts. More-over, at the important task of one-shot face recogni-tion, TAs outperform a variety of other models.
