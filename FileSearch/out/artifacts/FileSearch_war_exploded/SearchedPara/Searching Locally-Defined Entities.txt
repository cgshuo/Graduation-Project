 When consuming content, users typically encounter entities that they are not familiar with. A common scenario is when users want to nd information about entities directly within the content they are consuming. For example, when read-ing the book \Adventures of Huckleberry Finn", a user may lose track of the character Mary Jane and want to nd some paragraph in the book that gives relevant information about her. The way this is achieved today is by invoking the ubiq-uitous Find function (\Ctrl-F"). However, this only returns exact-matching results without any relevance ranking, lead-ing to a suboptimal user experience.

How can we go beyond the Ctrl-F function? To tackle this problem, we present algorithms for semantic matching and relevance ranking that enable users to e ectively search and understand entities that have been de ned in the con-tent that they are consuming, which we call locally-de ned entities . We rst analyze the limitations of standard infor-mation retrieval models when applied to searching locally-de ned entities, and then we propose a novel semantic en-tity retrieval model that addresses these limitations. We also present a ranking model that leverages multiple nov-el signals to model the relevance of a passage. A thorough experimental evaluation of the approach in the real-word ap-plication of searching characters within e-books shows that it outperforms the baselines by 60%+ in terms of NDCG. H.3.3 [ Information Search and Retrieval ]: Information ltering, retrieval models Algorithms; Experimentation Locally-De ned Entities; Descriptiveness; Within-document Search
Th is work was done when the rst author was on a summer internship at Microsoft Research.

When consuming content, users typically encounter enti-ties that they are not familiar with. To understand these entities, they may switch to a search engine to get related information. However, in addition to being distractive, this has obvious limitations in common scenarios such as: (1) an entity is only de ned in the document that the user is reading, and (2) the users prefer to see information about an entity within the content that they are consuming.
As an illustration, consider the following example. A user is reading a book on an e-reader. He/she nds a mention to a minor character (e.g., \Mary Jane" in Huckleberry Finn) but does not remember who the character is, or he/she nd-s the mention to another relatively popular character (e.g., \Aunt Sally") that occurs many times in the book but can-not keep track what her role is. In the former, there may be little information on the web (or available knowledge bases) about this character, while in the latter, the users may pre-fer information about the character within the book. As a result, in both cases, the user would want to nd some para-graphs in the book that give relevant information about the character, since the book itself contains enough information to give a good understanding of them.

As another example, consider an enterprise scenario, where the user is reading a document and nds a mention to the name of a project. If this is a new project, there may be no information about it in the company intranet (let alone the Internet). However, the document itself may contain enough introductory information about the project. More-over, in some cases, the user may believe that the current content is probably amongst the most relevant documents for the result. As a result, the user would want to nd some paragraphs directly in the current document that give rele-vant information about the project.

In this paper, we tackle the problem of enabling users to search and understand entities that have been de ned in the content that they are consuming. We call such entities locally-de ned entities . In the locally-de ned entity search task, the input (query) is the name (surface form) of an enti-ty for which the user would like to nd information; and the output is a list of relevant passages extracted from the ac-tual book/document that the user is reading. The passages are ranked by how well they describe the input entity.
This work is motivated by one instance of locally-de ned entity search that has important applications: searching for characters in e-books . This application is of signi cant prac-tical interest as e-books become ever more prevalent. E-books are typically rather long and may mention hundreds of characters most of which (particularly ctional charac-ters) may not appear in any knowledge base. Enabling the reader to understand the characters in e-book thus results in a signi cantly better reading experience. Figure 1 shows a screenshot of the experience enabled by locally-de ned en-tity search. The reader is reading a book (\Adventures of Huckleberry Finn") in an e-reader and reaches a mention to a character called \Mary Jane". In order to get more in-formation, the user selects \Mary Jane" and the application shows, directly within the e-reader, passages that provide rich descriptive information about this character.
One might argue that searching within a document has always been available through the ubiquitous Find (\Ctrl-F") function [20, 13]. However, a brute force approach of string matching, say, the rst paragraph where the entity name appears may lead to a suboptimal user experience. For example, in the book \Adventures of Huckleberry Finn", the rst paragraph mentioning Mary Jane is\Mary Jane decides to leave? Huck parting with Mary Jane." An arguably much better passage to describe Mary Jane is\Mary Jane was red-headed, but that don't make no di erence, she was most awful beautiful..." But the latter passage corresponds to the fourth mention of Mary Jane in the book. In fact, our data analysis shows that, in the applications of searching characters in e-books, 75% of the relevant paragraphs are not in the rst 10% matched paragraphs of a book.
There are a few works [11, 12, 20] that have attempted to address this problem by ranking passages within-document. However, they only used standard information retrieval mod-els and the bag-of-word matching to score the relevance of passages, which have been shown marginal improvements over Ctrl-F. Indeed, in our work, we have analyzed the lim-itations of standard information retrievals models when ap-plied to locally-de ned entity search. To mention but a few, the bag-of-words assumption behind these retrievals mod-els may cause partial matching to be inappropriately dealt with (e.g., \Jane" is always scored lower than \Mary Jane", though they may point the same person); anaphoric expres-sions are ignored (e.g., \she" is used to refer to \Mary Jane" in a passage, which could be regarded as an additional oc-currence of \Mary Jane", but does not count for scoring); and the document length hypothesis [29] should be revisited since the \documents" are passages from the same content (e.g, book) that the user is reading.
 The contributions of this work include the following:
To the best of our knowledge, the problem of locally-de ned entity search has not been addressed by previous work. We now brie y discuss research e orts that are relat-ed to our work in the following directions: entity linking and search, search within a document, and passage retrieval.
Entity linking and search. There is a rich literature on named entity linking [6, 23], which links the mention of an entity in a text to a publicly available knowledge base such as Wikipedia. Since locally-de ned entities may not appear in any available knowledge base, entity linking could only mark them as NIL to indicate there is no matching entry. Entity search [26] extracts relevant entities from doc-uments and returns directly these entities (instead of pas-sages or paragraphs) in response to a general query (rather than a named entity). Existing work focuses mainly on how to generate an entity-centric \document" representation by keyword extraction, and how to score an entity based on this entity representation using standard IR techniques.
Search within a document. We cast our problem as searching within the actual content that the user is reading. Despite the tremendous attention attracted by document re-trieval (motivated by web search applications), there is lim-ited research on algorithms for retrieval within documents, with only a few exceptions [11, 12, 20]. A simple within-document search function, known as \Find" (i.e., Ctrl-F) is available in almost all document readers/processors and In-ternet browsers. This function locates the occurrences of a query based on exact string matching, and lets users browse all matches linearly. As such, it is insucient to rank para-graphs that are descriptive about an entity (see [20] for a us-er study). Harper et al. [11, 12] presented a system, Smart-Skim, which uses a standard language modeling approach and bag-of-word matching to score the relevance of passages within a document w.r.t. a user's query, and then visualize the distribution of relevant passages using TileBars [13]. A related application that searches relevant passages within a document is the Amazon Kindle X-Ray feature. Although there is no publication about the details behind this feature, it appears to work similarly to SmartSkim. In Section 7, we
W e de ne a relevance notion of \descriptiveness" in later sections, which is exchangeable with \relevance". http://research.microsoft.com/en-us/people/ yuanhual/lde.aspx co mpare against the \Ctrl-F" function and several standard IR algorithms including a language modeling approach as in SmartSkim, and show that our methods perform signi cant-ly better for searching locally-de ned entities.

Passage retrieval and summarization. Passage re-trieval [4] segments documents into small passages, which are then taken as units for retrieval. Many passage retrieval techniques have been presented in the context of improv-ing relevance estimation of documents, e.g., [4, 16, 14, 21], while the research on passage retrieval for question answer-ing aims at returning relevant passages directly, e.g., [5, 32]. Di erently from all the existing work, our task is to retrieve entity-centric descriptive passages in response to a named-entity query. As we shall see later, solving the locally-de ned entity search problem simply using a standard passage re-trieval algorithm performs poorly, due to their lack of se-mantic matching and other appropriate ranking heuristics.
Our work is also related to automatic text summarization, in particular query-based summarization [33, 31] that iden-ti es the most query-relevant fragments of a document to generate search result snippets. However, standard summa-rization techniques presumably cannot summarize locally-de ned entities well without a clear understanding of the special notion of relevance. Our proposed techniques are complementary to the general query-based summarization by potentially enhancing the search result snippets when queries are locally-de ned entities.
The locally-de ned entity search problem is an information retrieval problem with the following elements:
We will consider passages that consist of a single para-graph from the content. The use of paragraph-based pas-sages is partially inspired by an interesting observation in question answering [19] that users generally prefer an an-swer embedded in a paragraph from which the answer was extracted over the answer alone, the answer in a sentence, or the answer in a longer context (e.g., a document-size chunk). Furthermore, paragraph is a discourse segmented by the au-thors and tends to be a more natural way to present results to the users.

A necessary condition for a passage p to be descriptive for entity e is that it must mention the entity. This suggests the following retrieval and ranking models. Given content T and entity e , the goal of the retrieval model is to produce a candidate set of passages SC such that every passage p in S mentions e . Given the retrieval set S , the goal of the ranking model is to rank the passages in S according to their descriptiveness with respect to e .

Both retrieval and ranking models pose singular challenges in the locally-de ned entity search problem. In the next section, we describe our solution for the retrieval model, and in Section 5 we focus on the ranking model.
Previous work [8] has shown that the good performance of a retrieval model, such as BM25, is mainly determined by the way in which various retrieval heuristics are used. These include term frequency (TF), inverse document fre-quency (IDF), and document length normalization. TF re-wards a document containing more distinct query terms and high frequency of a query term, IDF rewards the matching of a more discriminative query term, and document length normalization penalizes long document because a long doc-ument tends to match more query terms.

Take the well-known retrieval model BM25 [29] as an ex-ample. BM25 has been widely-accepted as a state-of-the-art model in IR. Its formula, as presented in [8], scores a docu-ment D with respect to query Q as follows: where c ( q;D ) is the raw TF of query term q in D ; j D j document length; avdl is the average document length in the whole corpus; N is the corpus size; df ( q ) is the document frequency of q in the corpus, log N +1 d f ( q ) is IDF; and k are free parameters that require manually tuning.
We now discuss the limitations of these retrieval heuristics when applied to locally-de ned entity search, and present a retrieval model that addresses these limitations.
Standard retrieval models are based on the bag-of-words assumption that takes both a query and a document as an unordered collection of terms for TF computation. This works well in most IR applications but has limitations for locally-de ned entity search. We now discuss these limita-tions and then present Entity Frequency (EF), a retrieval heuristic that addresses these limitations.

The rst limitation is that the bag-of-words assumption leads to an underestimation of partial matches. As an illus-tration, suppose that the query is \Mary Jane" and we have the following candidate passages:
The TF heuristic in practically all existing retrieval mod-els is that a document is preferred if it contains more distinct query terms. As a result, if we use standard retrieval mod-els, the rst passage above would be scored higher than the the second because it matches two query terms (\Mary" and \Jane") while the second passage matches just one (\Mary"). Ho wever, if both passages are indeed referring to the same person, they should get the same score. We tackle this prob-lem with our proposed EF retrieval heuristic by considering semantic matching of the entire entity, rather than counting how many query words are matched.

Another limitation of standard TF is that it may pro-duce either false positives or false negatives due to lack of semantic information. False positives can occur when a pas-sage is rewarded by the standard retrieval model because it matches some \term" of a query, without considering if the matching term in the passage refers to a di erent entity (or even a di erent type of entity). For example, a passage mentioning \Mary Ann" or the place \Highland Mary" could be mistakenly retrieved for query \Mary Jane". We tackle this problem with the EF retrieval heuristic by incorporat-ing Natural Language Processing notions of semantic type alignment and coreference analysis.

False negatives can occur because standard TF does not consider anaphoric resolution. For example, in the second passage above, \she" is used to refer to \Mary Jane" but this cannot be captured unless we employ anaphora resolution. In the standard IR setting, where the search results consist of entire documents, anaphora resolution does not necessar-ily play an important role. In contrast, anaphoric mentions are more important in locally-de ned entity search. One reason is that the retrieved \documents" are actually short passages and it is good writing style to avoid redundant mentions within the same passage (anaphora being one way of avoiding redundancy). We tackle this problem by incor-porating anaphora resolution into the EF retrieval heuris-tic. Anaphora resolution has been extensively studied in the NLP literature ([24]), but there have been relatively few pri-or research e orts that have applied it to IR tasks [1, 7, 25]. Our experimental results of Section 7 show that anaphora resolution clearly helps for locally-de ned entity search.
We are now ready to de ne the Entity Frequency (EF) retrieval heuristic.

Definition 1 (Entity Frequency). Given an entity name e , and a passage p containing N named entities e 1 ;:::;e that match e based on the standard bag-of-words matching, the entity frequency of e in passage p is where CR ( e i ) represents the number of anaphoric mentions that refers to e i ; r 2 [0 ; 1] controls the relative importance of an anaphoric mention as compared to e i itself; and E ( e is a variable indicating how likely that e i refers to e .
E ( e i ;e ) is dependent on the type of entities being con-sidered. We now present the function used in our experi-ments (Section 7) which is geared towards matching of per-son names. While some aspects of the function are general, an evaluation beyond person names is left as future work. E ( e i ;e ) = where j e j stands for the number of words contained in e , and e: has( e i ) is an indicator if e i is a word based substring of e . For example, \Mary Jane".has(\Jane") is true while \Mary Jane".has(\Mar") is false. Coref( e i , e ) is a score obtained by performing coreference resolution. Since o -the-shelf tool-s for coreference resolution are computationally expensive and have low accuracy (about 70%) [18], we propose the heuristics that we describe next.
 We consider two heuristics: local and global coreference. To compute local coreference, we rst do a \local" lookup by determining what entity, with the surface name being a superstring of e i , is the nearest one before the passage with-in a xed window (e.g., ten passages), inspired by the term proximity heuristic in information retrieval [22, 21]. If it is the query entity e , then coref ( e i ;e ) = 1. Otherwise, we compute global coreference based on statistics gathered from the entire content. The intuition is that if there are overlap-ping mentions beyond the xed local window (of, say, ten passages), the most likely entity assignment is the one that appears the most in the content. Suppose Q ( e i ) includes all possible entity names that contains e i , formally, we have
T o illustrate the computation of EF , suppose the query is \Mary Jane" and there are ve candidate passages as listed in Table 1. It is easy to see that only the last passage is clearly related to the query. The rst three are unrelated to the query: the rst one mentions the place \Highland Mary"; the second and third mentions di erent persons. It is unclear whether the fourth paragraph is relevant, and it would be necessary to look at the rest of the content (i.e., neighboring passages) to make a determination.

We show in Table 1 the EF scores: the rst three get a score of zero (due to type mismatch and partial matching rules), which matches the intuition that they are irrelevant to the query. The last gets the highest score (due to exact match and anaphora resolution) which matches the intuition that it is relevant to the query.

To determine the EF score of the third passage, we must compute the coreference score between the term\Mary"with mentions to \Mary Jane" in the rest of the content. We rst determine local coreference by looking at a window of neigh-boring passages. Suppose that the nearest mention overlap-ping \Mary" in that window is \Mary Williams". Since this is di erent from \Mary Jane", the local coreference score is zero (denoted by \L: 0" in Table 1). To compute the global coreference we consider all the named entities in the en-tire content that overlap with \Mary". Suppose that this is f \Mary Ann", \Mary Williams", \Mary Jane" g , and their frequencies in the entire content are 50, 40, 10. Then, the global coreference score is 10 50+40+10 = 0 : 1 (denoted by \G: 0.1" in Table 1).
Document length normalization plays an important role to fairly retrieve documents of all lengths. As highlighted by Robertson and Walker [28], the relationship between doc-ument relevance and length can be explained either by: (1) the scope hypothesis, the likelihood of a document's rele-vance increases with length due to the increase in covered material; or (2) the verbosity hypothesis, where a longer document may cover a similar scope than shorter documents but simply uses more words and phrases.

The verbosity hypothesis prevails over the scope hypoth-esis in most current IR models since they tend to penalize document length [30]. However, this may not be a reason-able assumption in locally-de ned entity search since all pas-sages of the same content (e.g., book) are likely to be written by the same author, and thus the verbosity of di erent pas-sages may not vary signi cantly.

We hypothesize that the scope hypothesis dominates over the verbosity hypothesis in locally-de ned entity search. That is, document length should be rewarded. To validate this hypothesis, we followed a procedure inspired by Singhal et al's nding [30] that a good retrieval function should retrieve documents of all lengths with similar likelihood of relevance. We compared the likelihood of relevance/retrieval of several BM25 models in the dataset that we will present in Section 6, where the content consists of e-books and the queries are characters appearing in the books.

The considered BM25 models di er only in the setting of the parameter b : a smaller b means less penalization of document length. We followed the binning analysis strat-egy proposed in [30] and plotted those likelihoods against all document length on each book. We observed that the retrieval likelihood gets closer to the relevance likelihood as b decreases. As an example, Figure 2a presents the results on \Adventures of Huckleberry Finn" 3 , where the bin size is 100 (varying it gives similar results). This shows that BM25 works the best when document length is not penalized, i.e., b = 0. However, we can see that even when b = 0, there is still a large gap between the retrieval and the relevance likelihoods, con rming our hypothesis that document length should be rewarded instead of being penalized.

Now we explore how to reward document length. In keep-ing with the terminology in the literature, we will talk about document length, but notice that the \document" in locally-de ned entity search is actually a passage. Given an ideal function g ( j D j ) for document length rewarding and a func-tion BM 25 b =0 that blocks document length, we assume that the true relevance function R with appropriate document length normalization can be written as where R can be regarded as the golden retrieval function as represented by the relevance likelihood in Figure 2a. Then g ( j
D j ) can be approximated as R B M 25 against document length in Figure 2c. We can see that g ( j
D j ) is roughly a monotonically increasing function with j
D j , which is consistent with the scope hypothesis [30] that the likelihood of a document's relevance increases with doc-ument length. Another interesting observation is there is a
Si milar results of other books are not shown due to space limitation. \pivot" point of document length, formally de ned as j D 0 When a passage D is shorter than this pivot value, g ( j D will be 0, causing the relevance score of this document to be 0; this is likely because a passage that is too short is unlikely to be descriptive. When a passage D is longer than this piv-ot value, g ( j D j ) increases monotonically but the increasing speed decreases with document length j D j ; this is also rea-sonable, since a relatively longer passage may contain more information, but if a document is already very long, increas-ing it further may not add much information. Our analysis shows that a logarithm function of document length ts the curve in Figure 2c very well. Therefore, we approximate the document length rewarding function as follows: where j D 0 j is the pivot length of a passage while j D max the size of the longest passage. 4
One may argue that the traditional window-based passage retrieval does not have this document length issue. Howev-er, our document length nding shows that the documnet length is indeed a useful signal to predict whether or not a passage that contains the entity is a good descriptive pas-sage for the entity. We thus hypothesize that the tradi-tional window-based passage retrieval would not work well for searching descriptive passages for locally-de ned entities, though it works well for capturing the traditional relevance notion for general queries, which have also been veri ed by our experiments.
We are now ready to present the retrieval function for locally-de ned entity search, which we call LRM. LRM com-bines EF and document length rewarding using BM25F's TF saturation function.
 where k 1 is similar to k 1 in BM25. Note that the relative LRM scores are not very sensitive to k 1 , since there is only one single \term" in all queries; we only keep it to adjust the relative importance of EF and document length. In experi-ments, we set k 1 = 1 : 5 empirically.

The retrieval likelihood of LRM is also compared with the relevance likelihood in Figure 2b. Obviously, LRM obtains the closest retrieval likelihood to the relevance likelihood as compared to any BM25 likelihoods shown in Figure 2a (other books have similar results). This shows analytically that LRM would be more e ective than BM25 for our tasks. Notice that LRM does not have an IDF component. The IDF heuristic rewards the matching of a more discriminative query term if there are multiple query terms. That is, IDF is used to balance multiple query terms, and does not a ect a retrieval model for queries that have only one single term, which is precisely the case in LRM, which uses the entire entity query as opposed to a bag-of-words assumption.
We now turn our attention to the ranking of passages based on how descriptive they are about the entity. We introduce three classes of \descriptiveness" features.
W e set D 0 = 8 for all books in our experiments.
Entity-centric descriptiveness directly captures the good-ness of a passage to pro le an entity. Intuitively, to introduce or pro le an entity, an author would often describe some typ-ical aspects of the entity. For person entities, those aspects might cover biographical information, social status and re-lationships, personality, appearance, career experience, etc. This suggests that there might be some useful \patterns" to recognize an informative passage. Let us look at an entity-centric descriptive passage for \Jean Valjean":
There are quite a few noticeable keywords (boldfaced in the passage) that re ect an entity's social status and fam-ily relationships. We name them \informative keywords" and we argue that they are potential indicators for entity-centric descriptiveness. The question now is how to mine such informative keywords. To answer this question, we observe that the vocabulary used to introduce local enti-ties that do not exist elsewhere, but the content may be similar to the one used to introduce more popular entities for which there may be information available in knowledge bases. In the case of characters for e-books, we resorted to summaries and descriptions available in Wikipedia. We crawled descriptions/summaries of famous ctional entities with Wikipedia pages, and then mined informative keywords based on the ratio of the likelihood of a word occurring in this crawled corpus over the likelihood of this word occur-ring in all Wikipedia articles 5 . We used a small dictionary of the top 50 extracted keywords in our work, which works well empirically. For illustration, the top 15 keywords are: \ill, son, age, friend, old, young, war, mother, love, father, child, die, daughter, family, and wife".

Finally, we design two features based on the informative keywords: (1) whether a passage contains an informative keyword, and (2) the frequency of informative keywords oc-curring in a passage. It is possible that some non-popular entities may not have those keywords in their contexts. We thus propose two more categories of features.
Related Entities. Time, place, and people are three key elements to describe a story. Intuitively, the fact that a pas-
W e did not include books used in the evaluation dataset. sage describes the interaction of an entity with other entities at some time in some place may indicate this is an informa-tive passage about the entity. Following this intuition, we argue that the occurrence of related named entities might imply the descriptiveness of the passage.

We consider three types of related named entities in this work: person, time, and location/organization. We use the frequency of each type of named entities as features.
Besides, we also design another feature, which is if any entity appears for the rst time in the book. It is reason-able to believe that the rst occurrence of an entity often indicates some new and sometimes important information.
Related Actions. We also consider how the query en-tity is connected to other entities through actions/events. The intuitive idea is that unusual actions tend to be more informative. For example, actions such as \A kills B" or \A was born in place B" should be more informative than \A says something to B" or \A walked in place B".

From the aforementioned examples, we see that an im-portant/unusual action is often associated with some un-usual verbs, such as \died", \killed", or \born", while a mi-nor/common action is often associated with some common verbs, such as \walked", \said", or \saw". This suggests that the \unusualness" (more technically, the IDF) of verbs relat-ed to an entity may imply the importance of the action. We thus encode the descriptiveness of actions using the IDF of verbs related to an entity, formally, log N D F ( v ) v is a verb, N is total number of passages in the book, and DF is the number of passages containing the verb. We con-sider the related verbs of c as the verbs co-occurring in the same sentence with the entity. Three features are instanti-ated from this signal, including the average, the maximum, and the minimum IDF of all verbs related to the entity.
The positional descriptiveness includes the order of pas-sage in the corpus (book) and occurrence position of the entity in the passage.

We use the\passage order"to indicate the relative sequen-tial order of a passage in the total passages where an entity appears. The passage order is perhaps the most widely-used ranking feature in existing within-document retrieval such as the \Ctrl-F" function. Our data analysis in Figure 3 demonstrates the distribution of descriptive passages w.r.t the passage order, where the passage order is normalized into 10 bins. It shows that the passage order indeed matter-s, and that the passages in the rst bin are more likely to be descriptive. However, only about 25% or less descriptive F igure 3: Distribution of descriptive passages w.r.t. an en-tity's lifetime in the book passages are covered by the rst bin, and the remaining 75% are distributed in other bins. This suggests that passage or-der, although useful, may not be a dominant signal.
We design two features based on this signal: (1) the nor-malized passage order into an integer value in [0 ; 10], and (2) the sequential order of a passage in the whole book, which is normalized in the same way.

Besides passage order, we nd that the occurrence posi-tion of an entity in the passage is also helpful. It is often the case that the entity occurs earlier tends to be more im-portant in a passage containing multiple entities. So we also present another two features based on the entity position: (1) if an entity appears in the beginning of the passage, and (2) the rst occurrence position of the entity in the passage.
We now describe the datasets to learn the ranking function and evaluate our methods. The datasets correspond to a setting where a user is reading e-books and seeks to obtain information about a character in the book. We used e-books obtained from the Gutenberg collection 6 . The list of books is given in Table 3. For each book, we obtained fairly exhaustive lists of characters from Wikipedia and used the character names as queries 7 .

We adopt the widely-accepted pooling technique [15] to generate candidates to be labeled. Several standard IR mod-els were used as the \runs", including BM25 ( k 1 = 1 : 5, b = 1) [29], language models with Dirichlet prior smooth-ing ( = 100) [27, 34], and pseudo-relevance feedback based on relevance models [17]. A key di erence of our task from traditional IR tasks is that we only care about top-ranked passages. In fact, in the real world scenario of e-book search, only top few results will be returned to users due to device screen size limitation. Therefore, for each entity, the top ranked 50 passages from each run were pooled together for acquiring judgments, resulting in a total of 8088 passages for ht tp://www.gutenberg.org
Most characters, particularly the least popular ones, do not have their own pages in Wikipedia. The nine books have their Wikipedia pages, from which we get the character lists. the nine books. The number of entities and passages from each book can be found in Table 3.

For each book, each labeler was given a list of entities, each of which was associated with a set of candidate passages in random order, and was asked to assign each candidate passage one of the 4 labels: Perfect, Good, Fair, and Bad.
Perfect : The passage is helpful to understand what the character is about. The passage has provided useful informa-tion to write a self-explanatory summary about the charac-ter. A perfect passage should contain (but is not limited to) signi cant biographical information, social status, social and family relationships, personality, traits, appearance, career experience, etc. Examples 1 and 2 in Table 2 show a per-fect passage for \Mary Jane" in \Adventures of Huckleberry Finn" and \Jean Valjean" in \Les Miserable" respectively.
Good : The passage contains some information about the character, but it is not enough to construct a self-explanatory summary describing it. It may still contain detailed informa-tion such as whether the character is interacting with others, performing activities, etc. A\good"passage for\Mary Jane" is Example 3 in Table 2.

Fair : While the passage mentions the character, it does not provide any noticeable information about it. Basically, our understanding of the character would not change after reading the passage. Example 4 in Table 2 shows a \fair" passage of \Mary Jane".

Bad : The passage is not related to the entity. For exam-ple, Example 4 in Table 2 is not related to \Mary Jane" be-cause\Mary"in the passage refers to\Sarah Mary Williams".
We obtained editorial judgments from two labelers (one of them is a professional editor and the other is an author of this paper). The labelers chose books that they had read in the past and were intimately familiar with, and labeled all the characters and passages for such books. There was an overlap of three books between the labelers (the overlapping books are shown in Table 3 with a \*" symbol). The number and proportion of judgments for each book are reported in Table 3. We can see that the largest proportion of passages get a Fair score (60.2%), followed by Bad, Good, and Perfect.
The three books judged by both labelers contain 99 en-tities with 3422 passages. We show the labeling consisten-cy in Figure 4. We can see that consistency decreases as the judgment level goes from \bad" to \perfect". The \bad" judgments have an extremely high consistency, with few ex-ceptions where the other labeler might give \fair" (3.9%) or \good" (0.5%), but never \perfect". This indicates that our assessors did a good job of labeling. We see the highest disagreement rate between \good" and \fair" labels, followed by \perfect" and \good", while \perfect" and \fair" have the lowest. This is mainly because \perfect", \good", and \fair" labels appear to be relatively subjective labels, in particular the \good" label, and the assignment of these labels may be a ected by the labelers' familiarity with the entity/book and T able 3: Summary of the constructed data set (N: total number of passages; pl : the average passage length; #e: #entities; #p: #passages) F igure 4: Distribution of judgments given by one labeler, when the other one's label is \bad", \fair", \good" and \per-fect", as shown in x-axis. personal judgment criteria. We can see that overall agree-ment ratio is as high as 86% for a 4-level relevance judgment task; this shows the high quality of the judgments.
We use a 4 point score scale, where\perfect",\good",\fair" and\bad"labels correspond to rating score 4, 2, 1, 0, respec-tively. This strategy is chosen because\perfect"passages are the most desirable ones for locally-de ned entity search. For those passages that two assessors have di erent opinions, we select a judgment with higher agreement ratio according to Figure 4 as its nal judgment. For example, if a passage is labeled as \good" and \fair" by two assessors, then \fair" will be chosen as its nal label. Each passage is rst preprocessed by applying the Porter Stemmer and removing stopwords using a standard stop-word list. The Stanford CoreNLP [9, 18] is used to pro-vide tokenization, sentence splitting, NER, coreference, and anaphora resolution. We use widely-used IR metrics includ-ing NDCG and precision as our main evaluation measures and report the scores at top 1, 2, 3, and 5 passages. For NDCG, we use the relevance labels presented in the previ-ous section. For precision we use binary labels. A result is considered to be related to the query if it is given one of the labels \perfect", \good", and \fair"; and unrelated otherwise. We use a state-of-the-art learning to rank algorithm called LambdaMART [2] to train a descriptiveness function, name-ly LDM (learned descriptiveness model). LambdaMART is a boosted tree version of LambdaRank [3] using MART [10]. Mo del @1 @2 @3 @5 St andard Find 30 .0 32 .1 3 1.7 32 .1 En tity Find 32 .4 33 .9 3 3.2 35 .5 BM 25 33 .2 37 .0 3 7.4 39 .8 PLM 33 .9 37 .2 3 7.7 39 .4 Sem antic Match 35 .7 37 .8 3 8.0 38 .5
LRM 51 .8* 52 .8* 5 1.4* 55 .1* " over Semantic Match 45 .1% 39 .7% 3 5.3% 43 .1% " over BM25 56 % 42 .7% 3 7.4% 38 .4%
LDM 57 .6 y 60 .2 y 62 .7 y 65. 5 y " over Semantic Match 61 .3% 59 .3% 6 5% 70 .1% " over BM25 73 .5% 62 .7% 6 7.7% 64 .6% T able 4: Comparison of average NDCG scores (%) on all books; * and y indicate the improvements over the baselines and LRM, respectively, are statistically signi cant (p &lt; 0.05) using Student's t-test.
 We use as features the descriptiveness signals described in Section 5, together with the LRM retrieval model scores, to train LDM. We use a leave-one-book-out strategy to eval-uate our algorithms. That is, each time, we train a model using eight books and test it on the remaining book.
For our approach, we will report results of the machine-learned ranking model LDM . We will also report results of the LRM retrieval model separately that we presented in Section 4. We compare our approaches against ve baselines:
We report the average NDCG scores of LDM, LRM and the baselines in Table 4. We can see that LDM dramatically outperforms all of the ve baselines. The average NDCG scores over all books is 60+% higher than all baselines on almost all top k (=1,2,3,5) positions. LDM performs reason-ably better than LRM, showing that the proposed descrip-tiveness signals e ectively contribute to accurately modeling the notion of descriptiveness. The detailed NDCG scores for each book are reported in Table 6. The rst column shows the book IDs referring to Table 3. We can see that LDM consistently outperforms other baselines on all books.
The average precision scores for di erent methods are shown in Table 5. We can see that LDM signi cantly out-performs all of the ve baselines, and that Precision@5 of LDM reaches about 90%. It is interesting to see that the T able 5: Comparison of average precision of LDM and L-RM with baselines over all books; * and y indicate the im-provements over the baselines and LRM, respectively, are statistically signi cant (p &lt; 0.05) using Student's t-test.. p recision of LDM is just slightly better than the precision of LRM. This is to be expected because precision con ates the labels \perfect", \good" and \fair" as correct. Thus, a passage is considered to be correct if it mentions the entity query, which is precisely what LRM is designed for.
As we can see in Tables 4 and 5, LRM by itself outperform-s all baselines signi cantly, with the average Precision@5 s-cores reaching as high as 89.9%, and NDCG@5 being over 35% above all baselines. We now turn our attention to the analysis of the contribution of the individual components of LRM (namely entity frequency (EF) and rewarded doc-ument length) to the performance of LRM. To this end, we report the results of two modi ed versions of LRM in Figures 5a and 5b. In LRM-EF, we replace EF with the traditional TF, and in LRM-DL, we remove the document length re-warding function g ( j D j ). The precision results show that LRM-DL works signi cantly better than LRM-EF, and per-forms just slightly worse than LRM, showing that the entity frequency (EF) retrieval heuristic plays a dominant role over rewarded document length. On the other hand, the NDCG scores demonstrate both retrieval heuristics may have sim-ilar capability for encoding \descriptiveness", though LRM-EF works slightly better. However, combining both EF and document length rewarding leads to a signi cant boost of NDCG, suggesting that the two components complement each other. For sensitivity analysis, by extensively testing various values of parameter k 1 and r , we found LRM works stably when k 1 2 [1 ; 2], r 2 [0 : 3 ; 1].
We further analyze the importance of di erent heuristics and features in contributing to the LDM ranking model. The ten most in uential features and their corresponding impor-tance scores are reported in Figure 6, which are based on the relative importance measure proposed in [10] to examine fea-ture weights. It shows that the LRM score, number of key-words, normalized location of lifetime and number of person entities are the top four most important features. Nonethe-less, all the proposed descriptiveness signals contribute to the descriptiveness model, and each of them contributes at least two features in the top-10 most important features.
We conclude by showing anecdotal results of the applica-tion of the LDM model and two of the baselines (Semantic Match and BM25) to the query \Mary Jane" in the book \Adventures of Huckleberry Finn". We list the top-3 results returned by two baselines and LDM in Table 7. We can see that LDM shows passages that have rich information about Mary Jane, such as her age, physical appearance, etc. In contrast, the passages retrieved by the baselines are either Figure 5: Comparison of Precision and NDCG results for LRM, LRM-EF, LRM-DL and BM25 not related to Mary Jane (e.g., the rst passage returned by BM25) or provide little information about her. We introduced the problem of locally-de ned entity search. The problem has important practical applications (such as search within e-books) and poses signi cant information re-trieval challenges. In particular, we showed the limitations of standard information retrievals heuristics (such as TF and document length normalization) when applied to locally-de ned entity search, and we designed a novel retrieval mod-el that addresses this limitations. We also presented a rank-ing model that leverages multiple novel signals to model the descriptiveness of a passage. A thorough experimental eval-uation of the approach shows that it outperforms the base-lines by 60%+ in terms of NDCG.
 Our ultimate goal is that all applications (word processors, Internet browsers, etc.) replace their Ctrl-F functions with locally-de ned entity search functions. This entails many di-rections for future work, including studying the use of these techniques on other entity types beyond person names (lo-cations, organizations, projects, etc.) and the impact of this functionality on di erent types of applications. We gratefully acknowledge helpful discussions with Ashok Chandra and useful comments from referees.
