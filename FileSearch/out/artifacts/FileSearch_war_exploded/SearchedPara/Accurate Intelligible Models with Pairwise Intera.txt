 Standard generalized additive models (GAMs) usually model the dependent variable as a sum of univariate models. Al-though previous studies have shown that standard GAMs can be interpreted by users, their accuracy is significantly less than more complex models that permit interactions.
In this paper, we suggest adding selected terms of inter-acting pairs of features to standard GAMs. The resulting models, which we call GA 2 M-models, for Generalized Addi-tive Models plus Interactions , consist of univariate terms and a small number of pairwise interaction terms. Since these models only include one-and two-dimensional components, the components of GA 2 M-models can be visualized and in-terpreted by users. To explore the huge (quadratic) number of pairs of features, we develop a novel, computationally ef-ficient method called FAST for ranking all possible pairs of features as candidates for inclusion into the model.
In a large-scale empirical study, we show the effectiveness of FAST in ranking candidate pairs of features. In addition, we show the surprising result that GA 2 M-models have al-most the same performance as the best full-complexity mod-els on a number of real datasets. Thus this paper postulates that for many problems, GA 2 M-models can yield models that are both intelligible and accurate.
 I.2.6 [ Computing Methodologies ]: Learning X  Induction classification, regression, interaction detection
Many machine learning techniques such as boosted or bagged trees, SVMs with RBF kernels, or deep neural nets are powerful classification and regression models for high-dimensional prediction problems. However, due to their complexity, the resulting models are hard to interpret for the user. But in many applications, intelligibility is as im-portant as accuracy [19], and thus building models that users can understand is a crucial requirement.

Generalized additive models (GAMs) are the gold stan-dard for intelligibility when only univariate terms are con-sidered [13, 19]. Standard GAMs have the form where g is the link function. Standard GAMs are easy to interpret since users can visualize the relationship between the univariate terms of the GAM and the dependent vari-able through a plot f i ( x i ) vs. x i . However there is unfor-tunately a significant gap between the performance of the best standard GAMs and full complexity models [19]. In particular, Equation 1 does not model any interactions be-tween features, and it is this limitation that lies at the core of the lack of accuracy of standard GAMs as compared to full complexity models.

Example 1. Consider the function F ( x ) = log( x 2 1 x x x 3 . F has a pairwise interaction ( x 2 ,x 3 ) , but no in-teractions between ( x 1 ,x 2 ) or ( x 1 ,x 3 ) , since log( x 2 log( x 1 ) + log( x 3 ) , which is additive.

Our first contribution in this paper is to build models that are more powerful than GAMs, but are still intelligible. We observe that two-dimensional interactions can still be rendered as heatmaps of f ij ( x i ,x j ) on the two-dimensional x ,x j -plane, and thus a model that includes only one-and two-dimensional components is still intelligible. Therefore in this paper, we propose building models of the form we call the resulting model class Generalized Additive Models plus Interactions , or short GA 2 Ms.

The main challenge in building GA 2 Ms is the large num-ber of pairs of features to consider. We thus only want to include  X  X rue X  interactions that pass some statistical test. To this end, we focus on problems with up to thousands of features since for truly high dimensional problems (e.g., mil-lions of features), it is almost intractable to test all possible pairwise interactions (e.g., trillions of feature pairs).
Existing approaches for detecting statistical interactions can be divided into two classes. One class of methods di-rectly models and compares the interaction effects and ad-ditive effects [10, 11, 18, 25]. One drawback of these meth-ods is that spurious interactions may be reported over low-density regions [15]. The second class of methods measures the performance drop in the model if certain interaction is not included; they compare the performance between re-stricted and unrestricted models, where restricted models are not allowed to model an interaction in question [22]. Although this class of methods does not suffer from the problem of low-density regions, they are computationally extremely expensive even for pairwise interaction detection.
Our second contribution in this paper is to scale the con-struction of GA 2 Ms by proposing a novel, extremely efficient method called FAST to measure and rank the strength of the interaction of all pairs of variables. Our experiments show that FAST can efficiently rank all pairwise interactions close to a ground truth ranking.

Our third contribution is an extensive empirical evalua-tion of GA 2 M-models. Surprisingly, on many of the datasets included in our study, the performance of GA 2 M-models is close and sometimes better than the performance of full-complexity models. These results indicate that GA 2 M-models not only make a significant step in improving accuracy over standard GAMs, but in some cases they actually come all the way to the performance of full-complexity models. The per-formance may be due to the difficulty of estimating intrin-sically high dimensional functions from limited data, sug-gesting that the bias associated with the GA 2 M structure is outweighed by a drop in variance. We also demonstrate that the resulting models are intelligible through a case study.
In this paper we make the following contributions: We start with a problem definition and a survey of related work in Sections 2 and 3.
Let D = { ( x i ,y i ) } N 1 denote a dataset of size N , where x = ( x i 1 ,...,x in ) is a feature vector with n features and y is the response. Let x = ( x 1 ,...,x n ) denote the variables or features in the dataset. For u  X  X  1 ,...,n } , we denote by x the subset of variables whose indices are in u . Similarly x will indicate the variables with indices not in u . To simplify notation, we denote U 1 = {{ i }| 1  X  i  X  n } , U 2 = {{ i,j }| 1  X  i &lt; j  X  n } , and U = U 1  X  X  2 , i.e., U contains all indices for all features and pairs of features.

For any u  X  X  , let H u denote the Hilbert space of Lebesgue measurable functions f u ( x u ), such that E [ f u ] = 0 and E [ f &lt;  X  , equipped with the inner product  X  f u ,f 0 u  X  = E [ f Let H 1 = P u  X  X  1 H u denote the Hilbert space of func-tions that have additive form F ( x ) = P u  X  X  1 f u univariate compnents; we call those components shape func-tions [19]. Similarly let H = P u  X  X  H u denote the Hilbert space of functions of x = ( x 1 ,...,x n ) that have additive form F ( x ) = P u  X  X  f u ( x u ) on both one-and two-dimensional shape functions. Models described by sums of low-order components are called generalized additive models (GAMs) , and in the remainder of the paper, we use GAMs to denote models that only consist of univariate terms.

We want to find the best model F  X  H that minimizes the following objective function: where L (  X  ,  X  ) is a non-negative convex loss function. When L is the squared loss, our problem becomes a regression prob-lem, and if L is logistic loss function, we are dealing with a classification problem.
Terms in GAMs can be represented by a variety of func-tions, including splines [24], regression trees, or tree ensem-bles [9]. There are two popular methods of fitting GAMs: Backfitting [13] and gradient boosting [10]. When the shape function is spline, fitting GAMs reduces to fitting general-ized linear models with different bases, which can be solved by least squares or iteratively reweighted least squares [25].
Spline-based methods become inefficient when modeling higher order interactions because the number of parame-ters to estimate grows exponentially; tree-based methods are more suitable in this case. Standard additive model-ing only involves modeling individual features (also called feature shaping ). Previous research showed that gradient boosting with ensembles of shallow regression trees is the most accurate method among a number of alternatives [19].
In this section, we briefly review existing approaches to interaction detection.

ANOVA . An additive model is fit with all pairwise inter-action terms [13] and the significance of interaction terms is measured through an analysis of variance (ANOVA) test [25]. The corresponding p -value for each pair can then be com-puted; however, this requires the computation of the full model, which is prohibitively expensive.

Partial Dependence Function . Friedman and Popescu proposed the following statistic to measure the strength of pairwise interactions, function (PDF) [10, 11] and F is a complex multi-dimensional function learned on the dataset. Computing  X  F u ( x u ) on the whole dataset is expensive, thus one often specifies a subset of size m on which to compute  X  F u ( x u ). The complexity is then O ( m 2 ). However, since partial dependence functions are computed based on uniform sampling, they may detect spurious interactions over low-density regions [15].
GUIDE . GUIDE tests pairwise interactions based on the  X  2 test [18]. An additive model F is fit in H 1 and residuals are obtained. To detect interactions for ( x i ,x j divides the ( x i ,x j )-space into four quadrants by splitting the range of each variable into two halves at the sample median. Then GUIDE constructs a 2  X  4 contingency table using the residual signs as rows and the quadrants as columns. The cell values in the table are the number of  X + X  X  and  X - X  X  in each quadrant. These counts permit the computation of a p -value to measure the interaction strength of a pair. While this might be more robust to outliers, in practice it is less powerful than the method we propose.

Grove . Sorokina et al. proposed a grove-based method to detect statistical interactions [22]. To measure the strength of a pair ( x i ,x j ), they build both the restricted model R and unrestricted model F ( x ), where R ij ( x ) is prevented from modeling an interaction ( x i ,x j ): To correctly estimate interaction strength, such method re-quires a model to be highly predictive when certain interac-tion is not allowed to appear, and therefore many learning algorithms are not applicable (e.g., bagged decision trees). To this end, they choose to use Additive Groves [21].
They measure the performance as standardized root mean squared error (RMSE) and quantify the interaction strength I ij by the difference between R ij ( x ) and F ( x ), where Std ( F  X  ( x )) is calculated as standard deviation of the response values in the training set. The ranking of all pairs can be generated based on the strength I ij .

To handle correlations among features, they use a vari-ant of backward elimination [12] to do feature selection. Although Grove is accurate in practice, building restricted and unrestricted models are computationally expensive and therefore this method is almost infeasible for large high di-mensional datasets.
For simplicity and without loss of generality, we focus in this exposition on regression problems. Since there are O ( n 2 ) pairwise interactions, it is very hard to detect pair-wise interactions when n is large. Therefore we propose a framework using greedy forward stagewise selection strategy to build the most accurate model in H .

Algorithm 1 summarizes our approach called GA 2 M. We maintain two sets S and Z , where S contains the selected pairs so far and Z is the set of the remaining pairs (Line 1-2). We start with the best additive model F so far in Hilbert space H 1 + P u  X  X  H u (Line 4) and detect interactions on the residual R (Line 5). Then for each pair in Z , we build an interaction model on the residual R (Line 6-7). We select the best interaction pair and include it in S (Line 9-10). We then repeat this process until there is no gain in accuracy.
Note that Algorithm 1 will find an overcomplete set S by the greedy nature of the forward selection strategy. When features are correlated, it is also possible that the algorithm includes false pairs. For example, consider the function in Example 1. If x 1 is highly correlated with x 3 , then ( x may look like an interaction pair, and it may be included in S before we select ( x 2 ,x 3 ). But since we will refit the model every time we include a new pair, it is expected that F will Algorithm 1 GA 2 M Framework 1: S  X   X  2: Z  X  X  2 3: while not converge do 5: R  X  y  X  F ( x ) 6: for all u  X  X  do 7: F u  X  E [ R | x u ] 8: u  X   X  arg min u  X  X  1 2 E [( R  X  F u ( x u )) 2 ] 9: S  X  X  X  X  u  X  } 10: Z  X  X  X  X  u  X  } Figure 1: Illustration for searching cuts on input space of x i and x j . On the left we show a heat map on the target for different values of x i and x j . c i and c are cuts for x i and x j , respectively. On the right we show an extremely simple predictor of modeling pairwise interaction. perfectly model ( x 2 ,x 3 ) and therefore ( x 1 ,x 2 ) will become a less important term in F .

For large high-dimensional datasets, however, Algorithm 1 is very expensive for two reasons. First, fitting interaction models for O ( n 2 ) pairs in Z can be very expensive if the model is non-trivial. Second, every time we add a pair, we need to refit the whole model, which is also very expensive for large datasets. As we will see in Section 4.1 and Sec-tion 4.2, we will relax some of the constraints in Algorithm 1 to achieve better scalability while still staying accurate.
Consider the conceptual additive model in Equation 2, given a pair of variables ( x i ,x j ) we wish to measure how much benefit we can get if we model f ij ( x i ,x j ) instead of f ( x i ) + f j ( x j ). Since we start with shaping individual fea-tures and always detect interactions on the residual, f i f ( x j ) are presumably modeled and therefore we only need to look at the residual sum of squares ( RSS ) for the inter-action model f ij . The intuition is that when ( x strong interaction, modeling f ij can significantly reduce the RSS . However, we do not wish to fully build f ij since this is a very expensive operation; instead we are looking for a cheap substitute.
Our idea is to build an extremely simple model for f using cuts on the input space of x i and x j , as illustrated in Figure 1. The simplest model we can build is to place one cut on each variable, i.e., we place one c i and one cut Figure 2: Illustration for computing sum of targets for each quadrant. Given that the value of red quad-rant is known, we can easily recover values in other quadrant using marginal cumulative histograms. c j on x i and x j , respectively. Those cuts are parallel to the axes. The interaction predictor T ij is constructed by taking the mean of all points in each quadrant. We search for all possible ( c i ,c j ) and pick the best T ij with the lowest RSS , which is assigned as weight for ( x i ,x j ) to measure the strength of interaction.
Na  X   X ve implementation of FAST is straightforward, but careless implementation has very high complexity since we need to repeatedly build a lot of T ij for different cuts. The key insight for faster version of FAST is that we do not need to scan through the dataset each time to compute T ij and compute its RSS . We show that by using very sim-ple bookkeeping data structures, we can greatly reduce the complexity.
 values for variable x i , where d i = | dom( x i ) | . Define H as the sum of targets when x i = v , and define H w the sum of weights (or counts) when x i = v . Intuitively, these are the standard histograms when constructing re-gression trees. Similarly, we define CH t i ( v ) and CH as the cumulative histogram for sum of targets and sum of weights, respectively, i.e., CH t i ( v ) = P u  X  v CH w i ( v ) = P u  X  v H w i ( u ). Accordingly, define CH t P P H ij ( u,v ) and H w ij ( u,v ) as the sum of targets and the sum of weights, respectively, when ( x i ,x j ) = ( u,v ).
Consider again the input space for ( x i ,x j ), we need a quick way to compute the sum of targets and sum of weights for each quadrant. Figure 2 shows an example for computing sum of targets on each quadrant. Given the above notations, we already know the marginal cumulative histograms for x i and x j , but unfortunately using these marginal values only can not recover values on four quadrants. Thus, we have to compute value for one quadrant.

We show that it is very easy and efficient to compute all possible values for the red quadrant given any cuts ( c i using dynamic programming. Once that quadrant is known, we can easily recover values in other quadrant using marginal cumulative histograms. We store those values into lookup tables. Let L t ( c i ,c j ) = [ a,b,c,d ] be the lookup table for sum Algorithm 2 ConstructLookupTable 1: sum  X  0 2: for q = 1 to d j do 3: sum  X  sum + H t ij ( v 1 i ,v q j ) 4: a [1][ q ]  X  sum 5: L ( v 1 i ,v q j )  X  ComputeV alues ( CH t i ,CH t j ,a [1][ q ]) 6: for p = 2 to d i do 7: sum  X  0 8: for q = 1 to d j do 9: sum  X  sum + H t ij ( v p i ,v q j ) 10: a [ p ][ q ]  X  sum + a [ p  X  1][ q ] 11: L ( v p i ,v q j )  X  ComputeV alues ( CH t i ,CH t j ,a [ p ][ q ]) of targets on cuts ( c i ,c j ), and denote L w ( c i ,c j as the lookup table for sum of weights on cuts ( c i ,c j Algorithm 2 describes how to compute the lookup table L . We focus on computing quadrant a and other quad-rants can be easily computed, which is handled by subrou-tine ComputeV alues . Given H t ij , we first compute a s for the first row of L t (Line 3-5). Let a [ p ][ q ] denote the value for cuts ( p,q ). Note a [ p ][ q ] = a [ p  X  1][ q ] + P Thus we can efficiently compute the rest of the lookup table row by row (Line 6-11).

Once we have L t and L w , given any cuts ( c i ,c j ), we can easily construct T ij . For example, we can set the leftmost that with those bookkeeping data structures, we can reduce the complexity of building predictors to O (1).
In this section, we show that calculating RSS for T can be very efficient. Consider the definition of RSS . Let T .r denote the prediction value on region r , where r  X  { a,b,c,d } .
 RSS = In practical implementation, we only need to care about P in relative ordering of RSS , and it is easy to see the com-plexity of computing RSS for T ij is O (1).
For each pair ( x i ,x j ), computing the histograms and cu-mulative histograms needs to scan through the data and therefore its complexity is O ( N ). Constructing the lookup tables takes O ( d i d j + N ) time. Thus, the time complexity of FAST is O ( d i d j + N ) for one pair ( x i ,x j ). Besides, Since we need to store d i -by-d j matrices for each pair, the space complexity is O ( d i d j ).

For continuous features, d i d j can be quite large. However, we can discretize the features into b equi-frequency bins. Such feature discretizing usually does not hurt the perfor-mance of regression tree [17]. As we will see in Section 5, FAST is not sensitive to a wide range of b s. Therefore, the complexity can be reduced to O ( b 2 + N ) per pair when we discretize features into b bins. For small b s ( b  X  256), we can quickly process each pair.
With FAST, we can quickly rank of all pairs in Z , the re-maining pair set, and add the best interaction to the model. However, refitting the whole model after each pair is added can be very expensive for large high-dimensional datasets. Therefore, we propose a two-stage construction approach. 1. In Stage 1, build the best additive model F in H 1 using 2. In Stage 2, fix the one-dimensional functions, and build
To scale up to large datasets and many features, we dis-cretize the features into 256 equi-frequency bins for contin-uous features. 1 We find such feature discretization rarely hurts the performance but substantially reduces the run-ning time and memory footprint since we can use one byte to store a feature value. Besides, discretizing the features re-moves the sorting requirement for continuous features when searching for the best cuts in the space.

Previous research showed that feature shaping using gra-dient boosting [10] with shallow regression tree ensembles can achieve the best accuracy [19]. We follow similar ap-proach (i.e., gradient boosting with shallow tree-like ensem-bles) in this work. However, a regression tree is not the ideal learning method for each component for two reasons. First, while regression trees are good as a generic shape functions for any x u , shaping a single feature is equivalent to cutting on a line, but line cutting can be made more efficient than regression tree. Second, using regression tree to shape pair-wise functions can be problematic. Recall that in Stage 1, we obtain the best additive model after gradient boosting converges. This means adding more cuts to any one feature does not reduce the error, and equivalently, any cut on a sin-gle feature is random. Therefore, when we begin to shape pairwise interactions, the root test in a regression tree that is constructed greedily top-down is random.

Similar to [19], to effectively shape pairwise interactions, we build shallow tree-like models on the residuals as illus-trated in Figure 3. We enumerate all possible cuts c x . Given this cut, we greedily search the best cut c 1 j in the region above c i and similarly greedily search the best cut c in the region below c i . Note we can reuse the lookup table L t and L w we developed for FAST for fast search of those three cuts. Figure 3 shows an example of computing the leaf values given c i , c 1 j and c 2 j . Similarly, we can quickly compute the RSS given any combination of 3 cuts once the leaf values are available, just as we did in Section 4.1.4, and therefore it is very fast to search for the best combination of cuts in this space. Similarly, we search for the best combination of 3 cuts with 1 cut on x j and 2 cuts on x i and pick the better model with lower RSS . It is easy to see the complexity is O ( N + b 2 ), where b is the number of bins for each feature and b = 256 in our case.
Note that this is not the number of bins used in FAST, the interaction detection process. Here we use 256 bins for feature/pair shaping. Figure 3: Illustration for computing shape function for pairwise interaction.

For large datasets, even refitting the model on selected pairs can be very expensive. Therefore, we propose to use the ranking of FAST right after Stage 1, to select the top-K pairs to S , and fit a model using the pairs in S on the residual R , where K is chosen according to computing power.
Models that combine both accuracy and intelligibility are important. Usually S will still be an overcomplete set. For intelligibility, once we have learned the best model in H , we would like to rank all terms (one-and two-dimensional components) so that we can focus on the most important features, or pairwise interactions. Therefore, we need to assign weights for each term. We use p E [ f 2 u ], the standard deviation of f u (since E [ f u ] = 0), as the weight for term u . Note this is a natural generalization of the weights in the linear models; this is easy to see since f i ( x p that E [ x 2 i ] = 1.
In this section we report experimental results on both syn-thetic and real datasets. The results in Section 5.1 show GA 2 M learns models that are nearly as accurate as full-complexity random forest models while using terms that de-pend only on single features and pairwise interactions and thus are intelligible. The results in Section 5.2 demonstrate that FAST finds the most important interactions of O ( n 2 feature pairs to include in the model. Section 5.3 compares the computational cost of FAST and GA 2 M to competing methods. Section 5.4 briefly discusses several important de-sign choices made for FAST and GA 2 M. Finally, Section 5.5 concludes with a case study.
We run experiments on ten real datasets to show the accu-racy that GA 2 M can achieve with models that depend only on 1-d features and pairwise feature interactions.
Table 1 summarizes the 10 datasets. Five are regression problems:  X  X elta X  is the task of controlling the ailerons of an F16 aircraft [1].  X  X ompAct X  is from the Delve repository and describes the state of multiuser computers [2].  X  X ole X  describes a telecommunication problem [23].  X  X alHousing X  describes how housing prices depend on census variables [16].  X  X SLR10k X  is a learning-to-rank dataset but we treat rele-vance as regression targets [3]. The other five datasets are bi-nary classification problems: The  X  X pambase X ,  X  X agic X  and  X  X etter X  datasets are from the UCI repository [4].  X  X isette X  is from the NIPS feature selection challenge [5].  X  X hysics X  is from the KDD Cup 2004 [6].

The features in all datasets are discretized into 256 equi-frequency bins. For each model we include at most 1000 feature pairs; we include all feature pairs in the six problems with least dimension, and the top 1000 feature pairs found by FAST on the  X  X ole X ,  X  X SLR10k X ,  X  X pambase X ,  X  X isette X , and  X  X hysics X  datasets. Although it is possible that higher accuracy might be obtained by including more or fewer fea-ture pairs, search for the optimal number of pairs is expen-sive and GA 2 M is reasonably robust to excess feature pairs. However, it is too expensive to include all feature pairs on problems with many features. We use 8 bins for FAST in all experiments.
We compare GA 2 M to linear/logistic regression, feature shaping (GAMs) without interactions, and full-complexity random forests. For regression problems we report root mean squared error (RMSE) and for classification problems we report 0 / 1 loss. To compare results across different datasets, we normalize results by the error of GAMs on each dataset. For all experiments, we train on 80% of the data and hold aside 20% of the data as test sets.

In addition to FAST, we also consider three baseline meth-ods on five high dimensional datasets, i.e., GA 2 M Rand, GA 2 M Coef and GA 2 M Order. GA 2 M Rand means we add same number of random pairs to GAM. GA 2 M Order and GA 2 M Coef use the weights of 1-d features in GAM to pro-pose pairs; GA 2 M Order generates pairs by the order of 1-d features and GA 2 M Coef generates pairs by the product of weights of 1-d features.
 The regression and classification results are presented in Table 2 and Table 3. As expected, the improvement over linear models from shaping individual features (GAMs) is substantial: on average feature shaping reduces RMSE 34% on the regression problems, and reduces 0 / 1 loss 44% on the classification problems. What is surprising, however, is that by adding shaped pairwise interactions to the models, GA 2 M FAST substantially closes the accuracy gap between unintelligible full-complexity models such as random forests and GAMs. On some datasets, GA 2 M FAST even outper-forms the best random forest model. Also, none of the base-line methods perform comparably GA 2 M FAST.
In this section we evaluate how accurately FAST detects feature interactions on synthetic problems.
To evaluate sensitivity of FAST we use the synthetic func-tion generator in [10] to generate random functions. Because these are synthetic function, we know the ground truth in-teracting pairs and use average precision (area under the precision-recall curve evaluated at true points) as the eval-
Figure 4: Sensitivity of FAST to the number of bins.
Figure 5: Precision/Cost on synthetic function. uation metric. We vary b = 2 , 4 ,..., 256 and the dataset size N = 10 2 , 10 3 ,..., 10 6 . For each fixed N , we generate datasets with n features and k higher order interactions x where | u | = b 1 . 5 + r c and r is drawn from an exponential distribution with mean  X  = 1. We experiment with two cases: 10 features with 25 higher order interactions and 100 features with 1000 higher order interactions.

Figure 4 shows the mean average precision and variance for 100 trials at each setting. As expected, average pre-cision increases as dataset size increases, and decreases as the number of features increases from 10 (left graph) to 100 (right graph). When there are only 10 features and as many as 10 6 samples, FAST ranks all true interactions above all non-interacting pairs (average precision = 1) in most cases, but as the sample size decreases or the problem difficulty increases average precision drops below 1. In the graph on the right with 100 features there are 4950 feature pairs, and FAST needs large sample sizes (10 6 or greater) to achieve av-erage precision above 0.7, and as expected performs poorly when there are fewer samples than pairs of features.
On these test problems the optimal number of bins ap-pears to be about b = 8, with average precision falling slightly for number of bins larger and smaller than 8. This is a classic bias-variance tradeoff: smaller b reduces the chances of overfitting but at the risk of failing to model some kinds of interactions, while large b allows more complex interac-tions to be modeled but at the risk of allowing some false interactions to be confused with weak true interactions.
The previous section showed that FAST accurately de-tects feature interactions when the number of samples is much larger than the number of feature pairs, but that ac-curacy drops as the number of feature pairs grows compa-rable to and then larger than the number of samples. In this section we compare the accuracy of FAST to the in-teraction detection methods discussed in Section 3.2. For ANOVA, we use R package mgcv to compute p -values un-der a Wald test [25]. For PDF, we use RuleFit package and we choose m = 100 , 200 , 400 , 800, where m is the sample size that trades off efficiency and accuracy [7]. Grove is available in TreeExtra package [8].

Here we conduct experiments on synthetic data generated by the following function [14, 22].
 and the other variables are uniformly distributed in [0 , 1]. We generate 10 , 000 points for these experiments. Figure 5(a) shows the average precision of the methods. On this prob-lem, the Grove and ANOVA methods are accurate and rank all 11 true pairs in the top of the list. FAST is almost as good and correctly ranks the top ten pairs. The other meth-ods are significantly less accurate than Grove, ANOVA, and FAST.

To understand why FAST does not pick up the 11 th pair, we plot heat maps of the residuals of selected pairs in Fig-ure 6. ( x 1 ,x 2 ) and ( x 2 ,x 7 ) are two of the correctly ranked true pairs, ( x 1 ,x 7 ) is a false pair ranked below the true pairs FAST detects correctly but above the true pair it misses, and ( x 8 ,x 10 ) is the true pair FAST misses and ranks below this false pair. The heat maps show strong interactions are easy to distinguish, but some false interactions such as ( x 1 can have signal as strong as that of weak true interactions such as ( x 8 ,x 10 ). In fact, Sorokina et al. found that x is a weak feature, and do not consider pairs that use x 8 interactions on 5 , 000 samples [22], so we are near the thresh-old of detectability of ( x 8 ,x 10 ) going from 5 , 000 to 10 , 000 samples.
If features are correlated, spurious interactions may be detected because it is difficult to tell the difference between a true interaction between x 1 and x 2 and the spurious in-teraction between x 1 and x 3 when x 3 is strongly correlated with x 2 ; any interaction detection method such as FAST that examines pairs in isolation will have this problem. With GA 2 M, however, it is fine to include some false positive pairs because GA 2 M is able to post-filter false positive pairs by looking at the term weights of shaped interactions in the final model.
 To demonstrate this, we use the synthetic function in Equation 10, but make x 6 correlated to x 1 . We generate 2 datasets, one with  X  ( x 1 ,x 6 ) = 0 . 5 and the other with  X  ( x 1 ,x 6 ) = 0 . 95, where  X  is the correlation coefficient. We run FAST on residuals after feature shaping. We give the top 20 pairs found by FAST to GA 2 M, which then uses gra-dient boosting to shape those pairwise interactions. Figure 7 illustrates how the weights of selected pairwise interactions evolve after each step of gradient boosting. Although the pair ( x 2 ,x 6 ) can be incorrectly introduced by FAST because of the high correlation between x 1 and x 6 , the weight on this false pair decreases quickly as boosting proceeds, indicating that this pair is spurious. This not only allows the model trained on the pairs to remain accurate in the face of spu-rious pairs, but also reduces the weight (and ranking) given to this shaped term so that intelligibility is not be hurt by the spurious term. Figure 6: True/Spurious heat maps. Features are discretized into 32 bins for visualization. Figure 7: Weights for pairwise interaction terms in the model.
Figure 5(b) illustrates the running time of different meth-ods on 10 , 000 samples from Equation 10. Model building time is included. FAST takes about 10 seconds to rank all possible pairs while the two other accurate methods, ANOVA and Grove, are 3-4 orders of magnitude slower. Grove, which is probably the most accurate interaction de-tection method currently available, takes almost a week to run once on this data. This shows the advantage of FAST; it is very fast with high accuracy. On this problem FAST takes less than 1 second to rank all pairs and the majority of time is devoted to building the additive model.

Figure 8 shows the running time of FAST per pair on real datasets. It is clear that on real datasets, FAST is both accurate and efficient.
An alternate to interaction detection that we considered was to build ensembles of trees on residuals after shaping the individual features and then look at tree statistics to find combinations of features that co-occur in paths more often than their independent rate warrants. By using 1-step look-ahead at the root we also hoped to partially mitigate the myopia of greedy feature installation to make interactions more likely to be detected. Unfortunately, features with high  X  X o-occurence counts X  did not correlate well with true interactions on synthetic test problems, and the best tree-based methods we could devise did not detect interactions as well as FAST, and were considerably more expensive.
Figure 8: Computational cost on real datasets.
Learning-to-rank is an important research topic in the data mining, machine learning and information retrieval com-munities. In this section, we train intelligible models with shaped one-dimensional features and pairwise interactions on the  X  X SLR10k X  dataset. A complete description of fea-tures can be found in [3]. We show the top 10 most im-portant individual features and their shape functions in first two rows of Figure 9. The number above each plot is the weight for the corresponding term in the model. Interest-ingly, we found BM25 [20], usually considered as a powerful feature for ranking, ranked 70 th (BM25 url) in the list af-ter shaping. Other features such as IDF (inverse document frequency) enjoy much higher weight in the learned model.
The last two rows of Figure 9 show the 10 most important pairwise interactions and their term strengths. Each of them shows a clear interaction that could not be modeled by addi-tive terms. The non-linear shaping of the individual features in the top plots and the pairwise interactions in the bottom plots are intelligible to experts and feature engineers, but would be well hidden in full-complexity models.
We present a framework called GA 2 M for building intel-ligible models with pairwise interactions. Adding pairwise interactions to traditional GAMs retains intelligibility, while substantially increasing model accuracy. To scale up pair-wise interaction detection, we propose a novel method called FAST that efficiently measures the strength of all potential pairwise interactions.

Acknowledgements. We thank the anonymous review-ers for their valuable comments, and we thank Nick Craswell of Microsoft Bing for insightful discussions. This research has been supported by the NSF under Grants IIS-0911036 and IIS-1012593. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF. [1] http://www.liaad.up.pt/~ltorgo/Regression/ [2] http: [3] http: [4] http://archive.ics.uci.edu/ml/ . [5] http://www.nipsfsc.ecs.soton.ac.uk/ . [6] http://osmot.cs.cornell.edu/kddcup/ . [7] http: [8] http://additivegroves.net . [9] E. Bauer and R. Kohavi. An empirical comparison of [10] J. Friedman. Greedy function approximation: a [11] J. Friedman and B. Popescu. Predictive learning via [12] I. Guyon and A. Elisseeff. An introduction to variable [13] T. Hastie and R. Tibshirani. Generalized additive [14] G. Hooker. Discovering additive structure in black box [15] G. Hooker. Generalized functional anova diagnostics [16] R. Kelley Pace and R. Barry. Sparse spatial [17] P. Li, C. Burges, and Q. Wu. Mcrank: Learning to [18] W. Loh. Regression trees with unbiased variable [19] Y. Lou, R. Caruana, and J. Gehrke. Intelligible [20] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [21] D. Sorokina, R. Caruana, and M. Riedewald. Additive [22] D. Sorokina, R. Caruana, M. Riedewald, and D. Fink. [23] S. M. Weiss and N. Indurkhya. Rule-based machine [24] S. Wood. Thin plate regression splines. Journal of the [25] S. Wood. Generalized additive models: an introduction
