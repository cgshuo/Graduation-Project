 Search trails mined from browser or toolbar logs comprise queries and the post-query pages that user s visit. Implic it endorsements from many trails can be useful for search result ranking, where the presence of a page on a trail increases its query relevance. Follow-ing a search trail requires user ef fort, yet little is known about the benefit that users obtain from this activity versus, say, sticking with the clicked search result or jumping directly to the destina-tion page at the end of the trail. In this paper, we present a log-based study estimating the user value of trail following. We com-pare the relevance, topic covera ge, topic diversity, novelty, and utility of full trails over that provided by sub-trails, trail origins (landing pages), and trail destina tions (pages where trails end). Our findings demonstrate significan t value to users in following trails, especially for certain quer y types. The findings have impli-cations for the design of search systems, including trail recom-mendation systems that display trails on search result pages. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search process, selection process Experimentation, Human Factors, Measurement Search trails, trail fo llowing, log analysis Web search engines afford keyword access to Web content. In response to search queries, these e ngines return lists of Web pages ranked based on their predicted re levance. For decades, the infor-mation retrieval (IR) research community has worked extensively on algorithmic techniques to effectively rank documents (c.f. [22]). However, research in areas such as information foraging [18], berrypicking [2], and orienteering [17], suggests that indi-vidual items may be insufficient for vague or complex information needs. In such circumstances, search results may only serve as the starting points for exploration [24]. Search trails are a series of Web pages starting with a search query and terminating with an event such as session inactivity [33]. Al-though the traversal of trails following a query is common, little is known about how much value users derive from following the trail versus sticking with the origin (the clicked search result) or jumping to the destination page at the end of the trail [32]. In this paper we present a log-based stud y estimating the value to users of traversing multi-page search trails. Our primary aim is to esti-mate the benefit that trail following brings to users under different metrics versus viewing only the origin and/or destination pages. Significant differences in the performance of trails over origins and destinations would suggest that users benefit from the journey as well as the origin and the destination. Knowing if and when this is the case could help us build more effective search systems centered around trails, e.g., full trails could be shown to users directly on the results page. We estimate the value of trails, sub-trails, origins, and destinations (collectively called trail sources ) based on the relevance, completene ss, diversity, novelty, and utili-ty of the information they contai n. We conduct this study using a log-based methodology since logs contain evidence of real user behaviors at scale and provide c overage of many types of infor-mation needs. Information need c overage is important since dif-ferences in source performance ma y not hold for all search tasks. The remainder of this paper is structured as follows. Section 2 presents related work on trails. Section 3 describes the primary data source used in our study, as well as the extraction and labe-ling of search trails, and trail statistics. Section 4 describes the experiment performed to estimate th e value of trails or sub-trails, including a comparison with trail origins and trail destinations. Section 5 describes the findings of our study for all queries and different query types. Findings are discussed along with their implications in Section 6. We conclude in Section 7. Vannevar Bush first introduced the co ncept of trails when he envi-sioned the memex , a theoretical proto-hypertext system to extend human memory [4]. Bush foresaw  X  X  new profession of trail blaz-ers, those who find delight in the ta sk of establishing useful trails through the enormous mass of the common record. X  Associative trails explicitly created by trailblazing users form links between stored materials that can help ot hers navigate. Interaction logging via browsers and toolbars has made us all (implicit) trail blazers. A search trail consists of an origin page, intermediate pages, and a destination page. Origin pages are the search results that start a trail, and may be referred to as landing pages in other literature. The use of query and origin pages from search engine click logs has been shown to be useful for improving search result relevance [1][14]. Teevan et al. [24] studied users jumping directly to desti-nation pages and introduced the concept of teleportation when they observed users issuing sophistic ated queries in an attempt to navigate to a page they knew existed deep in a Web site. White et al. [32] incorporated destination pages corresponding to Web search queries into search interface prototypes and presented them to user study participants. Most users found destination pages useful when shown on the search results page after the query was submitted. Bilenko and White [3] studied full trails, including the origin, intermediate, and destinat ion pages. They found that treat-ing the pages in these trails as endorsements improved ranking in search engines. Individual pages in full trails have been shown to improve search results, destina tion pages have been shown to benefit users, and origin pages ha ve been studied extensively in search relevance. We are the first to study the value of trails to users and directly compare trails to origins and destinations. Trails have been studied in domains outside of IR. Wexelblat and Maes [29] introduced annotations in Web browsers called  X  X oot-prints, X  which are trails through a Website assembled by the Web-site designer. Their evaluation found that users required signifi-cantly less steps to find informati on using their system. Freyne et al. [12] add a second dimension to footprints by displaying icons with links to offer visual cues to the user. These cues are gathered from past users and include popularity, recency, and user-generated annotations. More recent work by Wang and Zhai [28] continues the footprint metaphor in a topic map. This topic map allows the user to navigate hori zontally to related queries, and vertically to queries of different specificity. Simulated users with a predefined strategy benefited from such maps. Pirolli and Card [18] developed a sophisticated model of user behavior called in-formation foraging derived from how animals forage for food in the wild. They use a foraging metaphor to discuss how informa-tion foragers could use cues left by previous visitors to find  X  X atches X  of information in a collection and consume patch in-formation to satisfy information needs. Fu and Pirolli [13] devel-oped and validated computational cognitive models of Web navi-gation behavior based on information foraging theory. ScentTrails [16] combines browsing and searching into a single interface by highlighting potentia lly valuable hyperlinks. Olston and Chi perform user studies with different interfaces incorporat-ing  X  X cents X  of trails in the search results. Users could find infor-mation faster and more successfully using ScentTrails than by either searching or browsing alone. O X  X ay and Jeffries [17] pro-pose the orienteering analogy fo r understanding users X  informa-tion-seeking strategies. Their qualitative study relates to ours in describing the benefits of build ing a system that considers the entirety of users X  paths. Similarly, Bates X  X  berrypicking [2] dis-cusses users moving between information sources due to dynamic information needs. Search trails ar e extensions of these ideas into Web search, showing the routes with information to harvest, and orienting them towards the windi ng paths others have taken. As with orienteering and berrypicking, the origin and destination are important but the route taken in-between is also important; in this study we estimate how much benefi t users gain from this journey. Trigg [26] introduced the concept of guided tours , whereby au-thors could construct sequences of pages that may be useful to others. Reich et al. [19] discuss t ours and trails as tools for helping hypertext users by showing where others have gone. Tours and trails in hypertext differ; trails are marked by users at each step while tours are typically author ed beforehand and may have a hierarchical structure. Reich et al. also propose following users with similar interests as they move around the collection. Beyond hypertext, Chalmers et al. [6] present a system where people who are  X  X ecommenders X  manually cons truct Web navigation paths. These recommenders share their pa ths with others. Wheeldon and Levene [30] propose an algorithm for generating trails to assist in Web navigation. Trails are presented in a tree interface attached to the browser. User study particip ants expressed satisfaction with the trails, noting that seeing the relationship between links helped, and found trails to be usef ul as a navigational aid. The study described in this paper differs from previous work in that we are focused on estimating the value that trail following brings to users , rather than describing existing trail traversal be-havior, modeling user behavior, or using trails or computational models to recommend future actions. If findings show that users benefit from trail following, likely post-query trails could be con-sidered in search system design and even as units of retrieval [23]. In this section we describe the logs, trail mining from the logs, automatic classification of trail pages, and summary trail statistics. The primary source of data for this study was the anonymized logs of URLs visited by users who opted in to provide data through a widely-distributed browser toolbar. These log entries include a unique identifier for the user, a timestamp for each page view, a unique browser window identifier (to resolve ambiguities in determining which browser a page was viewed), and the URL of the Web page visited. Intranet and secure (https) URL visits were excluded at the source to maintain user privacy. In order to remove variability caused by ge ographic and linguistic variation in search behavior, we only include entries generated in the Eng-lish speaking United States locale. The results described in this paper are based on a sample of URL visits during a three-month period from March 2009 through May 2009, representing millions of URL visits from 100,000 unique users. The user sample was selected at random from a larger set of twelve million users after we had pre-filtered the data to remove several thousand extreme-ly-active outlier users, all of wh om issued over one thousand que-ries per day on average across th e three-month period. These high-volume users were likely automated traffic. For each user, we required an adequate number of We b page visits to create their long-term search history that was used to evaluate source novelty (described in more detail later). Therefore, in addition to removing outliers, we also only selected users who issued at least 30 queries per month from March 2009 to May 2009 inclusive. We mined tens of millions of search trails from the May 2009 logs, referred to hereafter as  X   X  . As defined by White and Drucker [33], search trails consist of a temporally-ordered sequence of URLs beginning with a search e ngine query and terminating with either: (i) another query, (ii) a period of user inactivity of 30 or more minutes, or (iii) the termination of the browser instance or tab. The 30-minute inactivity timeout is commonly used to de-marcate sessions in Web log analyses (e.g., [9]). We chose to use search trails rather than session trails (which comprise multiple queries) to lessen the likelihood of query skew, where user intent shifts over the course of the session, making it challenging to associate visited pages to the original query. Figure 1 illustrates a search trail, expressed as a Web behavior graph [5]. The trail starts with a search engine query ( 1 X  ) (which also includes the search-engine result page (SERP)) and comprises a set of pages visited until the trail terminates with a new query or an inactivity timeout. The nodes of the graph represent Web pages that the user has vi-sited: rectangles represent page views and rounded rectangles represent search engine result pages. Vertical lines represent back-tracking to an earlier state (e.g., returning to a page of results in a search engine after following an unproductive link). A  X  X ack X  arrow, such as that below 4 X  , indicates that the user has requested to visit a page seen earlier in the search trail. Time runs left to right and then from top to bottom. In addition to the complete trail, also marked on Figure 1 are the origin (the search result, 2 X  ), the destination (the trail X  X  terminal page, 5 X  ), and the pages between origin and destination (in this case 2 X  X ,3 X ,4 X ,3 X  X  ). Three of the five evaluation metrics used in our study X  X overage, diversity, and novelty X  X se info rmation about pa ge topicality. Millions of unique URLs were presen t in the set of all trails mined from the toolbar logs. This made the evaluation of coverage, no-velty, and diversity challenging as it was impracti cal to download all pages and comparisons based on URLs would be severely limited. To address this challenge, we classified the Web pages sourced from each context into the topical hierarchy from a popu-lar Web directory, the Open Directory Project (ODP) (dmoz.org). Given the large number of pages involved, we used automatic classification. Our classifier assi gned labels to pages based on the ODP using a similar approach to Shen et al. [21]. Classification began with URLs present in the ODP and incrementally pruned non-present URLs one path level at a time until a match was found or miss declared. Similar to [21], we excluded the  X  X egion-al X  and  X  X orld X  top-level ODP categories since they are typically uninformative for building interest models. There were 15 million search trails followed by the 100,000 tool-bar users in our sample during May 2009. The median (Med ) number of trails followed per user was 91 (mean (M ) was 160, standard deviation (SD ) was 228). The median number of steps in the trails was two (M =5.3, SD =12.2), (i.e., the search engine re-sult page and a single result click), but around one third of the trails were abandoned following the query, and around one third of trails contained three or more pages. The median time spent on trails was 81 seconds (M =308s, SD =615s), and around 20% of trails contained backtracking to a site already visited in the trail. Interestingly, around 19.3% of trails with three steps or more (i.e., had pages between the origin and de stination) had at least one site with a different ODP label to the origin and destination pages. Analysis of the queries on the remaining 80.7% of trails revealed that their original queries were generally navigational (e.g., [delta airlines] ) or directed informational (e.g., [what is daylight savings time?] ). For other types of informational query, such as undi-rected, advice, locate or list [20], intermediate pages may be valu-able to users. The extent of this value is estimated in our study. We devised an experiment to determine the value of search trails compared to search results and destinations. In this section we outline the research questions that drove our study, describe the experimental variants, summarize the trail data preparation, and present the metrics used to compare sources. Our study answers a number of res earch questions. Specifically, of the four sources (origin, destination, sub-trail, and full-trail), which: (i) provide more relevant information? ( 1 X  X  ); (ii) provide more topic coverage? ( 2 X  X  ); (iii) provide more topic diversity? ( 3 X  X  ); (iv) provide more novel information? ( 4 X  X  ), and; (v) pro-vide more useful information? ( 5 X  X  ). Answers to these questions help us understand the value of trail (or sub-trail) traversal com-pared to viewing only the origin and/or destination pages. To determine the value of trail traversal we experiment with a number of trail sources. They are as follows: Origin : The first page in the trail after the SERP, visited by click-ing on a search result hyperlink. This is regarded as a baseline in this study since current search e ngines show this source alone in search results. 2 X  is the origin in Figure 1. Destination : The last page in the trail, visited prior to trail termi-nation through a follow-up query or inactivity timeout. Destina-tions are defined similarly to the popular destinations from White et al. [32]. We include them here for comparison with that earlier work. 5 X  is the destination in Figure 1. Sub-trail : All pages in the trail except for destination, including all post-SERP pages. 2 X  X ,3 X ,4 X ,3 X ,2 X  X  is the sub-trail in Figure 1. Full-trail : The complete trail, including all post-SERP pages. We mine these sources from each trail in  X   X  and compute the val-ue of each source in terms of re levance, coverage, diversity, no-velty, and utility across all queries and divided by query type. We elected not to study intermediate pages directly (i.e., pages in the trail that lie between the origin and destination) since a trail must contain an origin page in our cu rrent definition. The value of the intermediate pages over the origin can be estimated by comparing the performance differences between origins and sub-trails . To help ensure experimental integrity, we did not use all search trails in  X   X  . Instead, we filtered  X   X  based on the following criteria:  X  Queries originating the trails were normalized to facilitate com-parability between trails, and between the trails and other re-sources (as described in the ne xt section). Normalization in-volved the removal of punctuatio n, lowercasing, trimming extraneous whitespace, and ordering terms alphabetically.  X  Trails were required to contain at least three pages: an origin page, a destination page, and at least one intermediate page. It was important to have these sources in all trails used since we wanted to compare their value.  X  To ensure that origin pages were reached through a SERP click, we required that the first non-SERP page in the trail be con-nected to the SERP with a hyperlink click (i.e., the referrer of the origin page must be a SERP ). Trail pages thereafter were not required to be joined via a hyperlink click.  X  The coverage of our ODP classifier with URL back-off was approximately 65%. A missing label may have skewed the dis-tribution of labels for or against a particular source. We there-fore required that all selected trails be fully labeled.  X  To prevent sample bias from highly-active users, we selected at most 10 search trails that met the above criteria from each user. The application of these criteria reduced  X   X  to one quarter of its original size, but yielded a high-quality data set for our study. We used five metrics to compare the different trail sources: relev-ance, coverage, diversity, novelty and utility. These metrics were chosen to capture many important elements of information seek-ing, as highlighted by the wealth of relevant research in the IR community (e.g., [7][8]). The use of multiple metrics allowed us to compare the value of the different sources in different ways. For example, a trail destination page may be less relevant than sub-trail, but may provide additi onal information not in the sub-trail. We now describe each metric and its implementation. The first metric used to compare the sources was relevance to the query that initiated the trail. In addition to the trail data used dur-ing the course of this study, we also obtained human relevance judgments for over twenty thousand queries that were randomly sampled by frequency from the query logs of the Bing search engine; they were normalized per the description in Section 4.3, and were present in  X   X  . Trained judges assigned relevance labels on a six-point scale X  Bad , Poor , Fair , Good , Excellent and Per-fect  X  X o top-ranked pooled Web sear ch results for each query from the Google, Yahoo!, and Bing search engines as part of a separate search engine assessm ent activity. This provided hun-dreds of relevance judgments for each query. These judgments allowed us to estimate the releva nce of information encountered at different parts of the trails. For each trail in  X   X  , we computed the average relevance judgment score fo r each source. Each page in the trail was used at most once in relevance score calculations, even if it appeared multiple times in the trail. This discounted revisitation, since diminishing retu rns from each repeat visit to a page in the same trail were likely. In this analysis we only used trails for which we had a relevance judgment for the origin page, the destination page, and at least one intermediate page. Trails for 8,712 queries, comprising a query set  X  and initiating around two million trails, afforded a detailed comparison of source relevance. Another aspect that we studied was topic coverage, meant to re-flect the value of each trail source in providing access to the cen-tral themes of the query topic. To estimate the coverage of each trail source, we first constructed a set of query interest models representing the dominant intents associated with each query in  X  . These models served as the grou nd truth for our estimates of cov-erage (in this subsection) and di versity (in the ne xt subsection). Each constructed query interest m odel is assumed to contain most of the significant themes for th e query. A query X  X  interest model comprises the ODP category labels assigned to the URLs in the union of the top-200 search results for that query from Google, Yahoo! and Bing. ODP category labels are grouped and their fre-quency values are normalized such that across all labels they sum to one. For example, the highest-weighted labels in the query interest model for [solar system discoveries] , and their associated normalized frequencies (  X   X  ), are shown in Figure 2. Label  X   X   X  X  X  X / X  X  X  X  X / X  X  X  X  X  X  X  X  X  X / X  X  X  X  X  X  X / X  X  X  0.64  X  X  X  X  X _ X  X  X _ X  X  X  X / X  X  X  X  X / X  X  X  X  X  X  X  X  X  X / X  X  X  X  X  X  X / X  X  X  0.18  X  X  X  X  X  X  X  X  X  X  X  X  X / X  X  X  X  X  X  X  X  X / X  X  X  0.16 Figure 2. Top ODP categories for [solar system discoveries] . To improve the reliability of our coverage estimates, we selected a set of query interest models,  X   X  , that were required to be based on at least 100 fully-labeled search results (i.e., we re not missing a label and did not have a label from an ignored ODP category) and were based only on labels with a fre quency count of at least five (to reduce label noise).  X   X  was modified to include only trails originating from queries with interest models in  X   X  . For each trail  X  in  X   X  , we created a source interest model comprising ODP cate-gory labels and associated frequencies for origin , destination , sub-trail , or full-trail . We then compute the coverage of each source  X  in  X  (denoted  X   X  ) using: Where l is ODP category label and  X   X  represents the normalized frequency weight of that label in the corresponding interest model for the current query, denoted as  X   X  . Another aspect studied was topic diversity, which estimates the fraction of unique query-relevant concepts surfaced by a given trail source. Exposure to different perspectives and ideas may help users with complex or exploratory search tasks. Indeed, existing search engines already consider diversity in the search results they present to satisfy more user s with the first few results. To estimate the diversity of information provided by each trail source we use an approach similar to our coverage estimation. We generate trail interest models for each trail source and compare those with the relevant query interest model to estimate diversity. The main difference between how the estimates of coverage and diversity lies in whether normali zed label frequency is considered. When estimating covera ge we want to establish the fraction of  X  appearing in  X   X  (i.e., label frequency is us ed). In contrast, when we estimate diversity, we only coun t the number of unique category labels from  X   X  that appear in  X   X  (i.e., frequency is ignored). For each trail  X  in  X   X  originating with one of the queries in  X  created a source interest mode l comprising ODP category labels and associated frequencies for origin , destination , sub-trail , and full-trail. We computed diversity for each  X   X  using: Where l is ODP label and  X |  X  | is the number of unique  X  Another aspect that we studie d was the amount of new query-relevant information from each trail source. Novel information may help users learn about a new subject area or broaden their understanding of an area with wh ich they are already familiar. Trails with novelty contain info rmation that users have not en-countered for a query. Unlike cove rage and diversity, the novelty provided by a trail source may depend on both the query and the user. For example, what is new topic-related information for one individual may not be new informa tion for another. Therefore, to estimate the novelty of the information provided by each trail source, we first had to construct a model of each user X  X  general interest in the query topic based on historic data. To do this, we leveraged users X  search trails for the two-month period from March to April 2009 inclusive (referred to hereafter as  X  constructed historic interest models  X  , for all user-query pairs. Each interest model  X   X  , whose query was present in  X  prised a distribution of ODP category labels (and associated nor-malized frequencies) similar to t hose used in earlier coverage and diversity estimates. Only labels appearing in the query interest model  X   X  are included in  X   X  . The historic interest model is there-fore a subset of  X   X  focused on a given user X  X  history with that query. White et al. [31] used a similar approach to depict long-term user interests. We estimate the novelty of each trail source relative to the historic interest model for the user and the query. For each trail  X  in  X   X  , we built source interest models to estimate the source novelty based on whethe r it contained topic-related information not in  X   X  . The novelty of each  X   X  is estimated using: Where l represents an ODP category label present in  X  and  X  not in  X   X  , and  X |  X  | represents the number of unique  X  The final aspect that we studied wa s the utility of each of the trail sources, estimated for the purposes of this study using page dwell time (i.e., the amount of time spent on a particular page by a user). Dwelling on a page for a significant amount of time implies that a user may be deriving utility from it. Indeed, prior research has shown that during search activity, a dwell time of 30 seconds or more on a Web page can be indicative of page utility [11]. We apply this threshold in our analysis and across all trails in  X  estimate the fraction of page views from the origin , destination , sub-trail , and full-trail that exceed this dwell time threshold. In all metrics used in this study, a higher value is regarded as a more positive outcome. The metrics are computed for each trail, then micro-averaged within each query, and then macro-averaged across all queries to obtain a single value for each source-metric pair. This procedure ensures that all queries are treated equally in the analysis and popular queries are not allowed to dominate the aggregated metric values for each source. Although we might expect sub-trails and full-trails to have higher metric scores than origins or destinations (simply because they have more pages), it is the extent that the metrics X  values increase from these sources that lets us estimate the addition al value of trails and sub-trails. This is reasonable since we plan to show full-trails and sub-trails directly to users on the search engine result page. In this section so far we have described the research questions, the four trail sources evaluated, tra il data preparation procedures, and the metrics used to evaluate the sources. The methodology em-ployed during our experiments co mprised the following steps: 1. Construct the set of query interest models  X   X  of queries for which we have human relevance judgments (  X  ). 2. Construct historic interest models (  X  ) for each user-query pair in  X   X  , filtered to only include queries appearing in  X  The data sets created during the first two steps are used to eva-luate each of the four trail sources. 3. For each search trail  X  in  X   X  : a. Assign ODP labels to pages all pages in  X  . b. Build source interest models for the origin , destination , sub-c. Compute relevance, coverage, di versity, novelty and utility 4. Compute the average values for each metric per query, and then average across all queries (to treat all queries equally), breaking out the findings by query type as appropriate. In the next section we report on the findings from our study. We first present findings over all queries; then divided by query type, varying query popularity and query re-finding behavior, both of which have been shown to infl uence search interaction in pre-vious work [8][10]. Since our data were shown to be normally distributed, we use parametric statistical testing, with  X .05 X  . We computed the five metrics across all trails in  X  port on source performance. Relevance: We begin our analysis by reporting on the relevance of the information encountered at the origin , destination , sub-trail and full-trail , determined using human relevance judgments. As noted in the previous section, the judgments were captured for query-URL pairs on a six-point scale, ranging from 0 ( Bad ) to 5 ( Perfect ). Sources that provide more relevant information would be expected to have a higher aver age relevance score. In the  X  X ll X  column of Table 1 (shaded) we report on the mean average relev-ance score obtained from each of the sources across all trails in  X  . Also reported are the percentage differences between the re-levance score obtained for each of the non-origin sources and origins (  X  X  ) to estimate the additional value obtained from full or partial trail traversal, or from teleporting directly to destinations. We do not show standard deviat ions to avoid crowding findings. The findings show that the releva nce scores for all sources were generally positive (around three or Good ). An independent-measures analysis of variance (ANOVA) computed between the relevance scores obtained from all four sources revealed no signif-icant differences in the relevance of the origin page versus infor-mation encountered on the trail (  X  (3,8708) = 1.5,  X  X  0.21). However, as is apparent in the tabl e, trends in the findings suggest that the relevance scores for non-origin sources were slightly low-er than those of the origin pages (e.g., 3.3 versus 2.9-3.0). This may be related to a combinatio n of the distance between non-origin sources and the original queries, and the effect of dynam-ism in information needs as users traverse search trails [33]. Since non-origin sources are further from the query than origin pages, they may be less query releva nt as user needs evolve. Coverage: We also studied the extent that each trail source cov-ered the query interest models representing the dominant themes for each query. The coverage estimate of each source for each trail was computed using Equation 1. The average coverage scores for each metric are reported in the  X  X ll X  column of Table 1, along with the percentage difference between each of the sources and origin . The findings show that on av erage, around 40% of the total mass of the query interest models can be covered by origins and destinations , and around 50% are covered by sub-trails and full-trails (coverage gains of 20-30% from traversing trails). Analysis of the findings using a one-way independent measures ANOVA revealed statistically significant differences between the sources (  X  (3,8708) = 5.5,  X  X  .001). Post-hoc testing, performed using Tukey tests, revealed that on average across all queries, sub-trails and full-trails covered more of the query interest models in  X  than the origins or destinations alone (all  X  &lt; 0.01). Relevance Coverage Diversity Novelty Utility Diversity: To estimate the extent that each trail source covers different aspects of the query interest model, we calculated their diversity using Equation 2. Increas ed diversity may be useful to users engaged in search tasks with multiple sub-tasks, such as planning a vacation. The average coverage scores for each source across all trails in  X   X  are reported in the  X  X ll X  column of Table 1. The findings show that approxima tely one-third of the central themes for a query can be captured by each trail source, with more topic diversity coming from the trail-based sources (diversity gains of 30-40% from traversing trails). Statistical analysis of the findings reveals significant differen ces between the levels of topic diversity provided by each source (  X  (3,8708) = 7.0,  X  X  0.001). Post-hoc testing revealed that sub-trails and full-trails provide more diversity than origins (all  X  &lt; 0.01). The increase in diversi-ty for destinations over origins was not statistically significant. Novelty: Novelty calculations estim ate the amount of new query-relevant information provided to user s by each of the trail sources. Unlike the other metrics in this study, novelty is specific to both user and query; one user X  X  experience with a query may differ from another X  X . As described pr eviously, novelty is computed based on the number of new query -relevant ODP category labels added to a user X  X  query interest models compared with historic data. In Table 1 ( X  X ll X  column) we report on the average novelty score and the percentage diff erences between all non-origin sources and the origin only. The findings show modest increases in the amount of new informati on obtained from all sources, but seemingly larger gains from the non-origin trail sources (0.13-0.16 versus 0.03). Statistical an alysis of our findings revealed differences among the sources (  X  (3,8708) = 3.0,  X  X  .01). Post-hoc testing revealed significant differences between sub-trails / trails provide more novel information than origins or destinations . In turn, destinations provide slightly more novel information than origins , but differences were not significant (  X  X  0.12). Utility: We also studied the utility of each trail source. To esti-mate utility for a given Web page from the logs, we used a 30-second page dwell time threshold se lected based on previous work [11]. For each of the sources across all trails in  X  the fraction of trails for which eac h source contained a useful page (i.e., a page with a dwell time equaled or exceeded the 30-second threshold). These values are shown in the  X  X ll X  column of Table 1. Also shown are the percentage differences between non-origin sources and origins . The findings show that just under half of origins and destinations are useful, around 60% of sub-trails have useful pages, and almost two-thirds of full-trails contain useful pages. Statistical analysis of th e findings revealed significant dif-ferences between the sources in terms of their estimated utility (  X  (3,8708) = 3.3,  X  X  .01). Post-hoc testing revealed that all sources differed from trail origins ( destinations :  X  = .03; sub-trails :  X   X  .01; full-trails :  X   X  .01). It seems that users find non-origin pages more useful than origin pages. This may be because origin pages are search results and may only be the starting points for a search task or sub-task [24]. One important factor that may cause variation in the effectiveness of search trails is the nature of the search query. Downey et al. [10] showed that user behavior following a query varied signifi-cantly with query popularity. Teevan et al. showed that the fre-quency with which a query is reissued by a given user over a pe-riod of time (so-called  X  X e-finding X  behavior) affects that user X  X  search interactions for that partic ular query [25]. To test whether such factors influenced the source value we varied query populari-ty and history as part of our expe rimental design. In the remainder of this section we report on the findings of this analysis. To study the effect of query popularity on source value, we created a tripartite division of queries in  X   X  , grouping them into low , medium , and high , based on user frequency in  X  larity queries were issued by at most one user in  X   X  , medium pop-ularity queries were issued by between 1 and 100 users in  X  high popularity queries were issued by over 100 users in  X  Table 1 presents findings on the effect of query popularity on source performance for each of the five metrics we study. On all metrics, we observe a trend that as query popularity increases, each of the metric values also increases. The relative ordering and percentage gains from the trail s ources remain consistent across all five metrics. However, within each metric, differences in the values obtained for the three query popularity groupings are not significant using a two-way in dependent measures ANOVA with source and query popularity group as the factors (source (rows): all  X   X  .02; popularity (columns): all  X  X  .13).  X  -statistics for all performed ANOVA are not reported to avoid crowding the paper. Small increases in coverage as query popularity increases may be attributable to the dominance of the intent associated with the query. More popular queries are more likely to have a single do-minant intent, giving the category label for that intent a high weight (  X   X  from Equation 1). Since coverage derives from  X  are likely to observe increases in coverage as a dominant intent with a high  X   X  . Improvements in search engine performance as query frequency increases (already noted in [10]) may account for some of the slight increases in relevance and utility with populari-ty (Table 1). We also studied the effect of query history on the value of each of the four trail sources. We divided queries into three groups X  none , some , and lots  X  X ased on the number of times they were issued by a particular user in  X   X  . Queries in none appeared in  X  appear in  X   X  , queries in some appeared in  X   X  and were issued by a particular user 30 times or less in  X   X  (i.e., on average less than once every two days), and queries in lots appeared in  X  those issued by a particular user more than 30 times in  X  average more than once per two days). Table 1 presents findings on the eff ect of query history on source performance for each of the five metrics. From the findings, it seems that as query history increases, there is a mixed effect on the five metrics. However, within each metric all differences be-tween sources and between query history groupings are signifi-cant, as shown by a two-way in dependent measures ANOVA with source and query history grouping as the factors (source (rows): all  X   X  .001; history (columns): all  X   X  .001). We found that relevance and utility rise across all sources given increased re-finding behavior. This is perhaps because users are more familiar with the query topic and are more able to identify relevant infor-mation. Similar findings have been reported in previous work on topic familiarity (e.g., [15]). In contrast, coverage, diversity, and novelty decrease, perhaps as a resu lt of a reduced variance in the pages visited. Such c onsistency in interaction behavior for queries with high re-finding rates has been reported previously [27]. We have demonstrated that follo wing search trails provides users with significant additional benefit in terms of coverage, diversity, novelty, and utility over origin s and destinations. Although more work is required to supplement the methodology used in our study and further understand the impact of experimental decisions such as only studying search trails that could be fully-labeled using ODP lookup, our log analysis helps establish the value of trails to users and inform search system design. We showed that full-trails and sub-trails provided significantly more coverage, diversity, novelty, and utility, versus trail origins and destinations. The one metric for which we did not obtain sig-nificant differences between origin and non-origin sources was relevance. Trends in the findings s uggest that trails were less rele-vant than origins. This may be related to the definition of relev-ance in this study. Our relevance judgments are assigned to pairs of queries and search results. Ho wever, during the session, user intent may shift and the relevance to the initial query is dynamic [2]. Pages encountered on the trails may be relevant but not ap-pear so due to these shifts. More work is required on how relev-ance changes during browsing and to understand the relevance benefit from trails. Enha ncements include studying the cumulative relevance of trail information, co nsidering relevance changes, and devising proxies for relevance sim ilar to that used for utility. Destinations were more useful an d led to a slight novelty increase over origins. This confirms some of the findings of White et al. [32], who showed in a user study that destinations were a useful addition to the results interface. In retrospect, this agrees with expectations that users will give more attention, and hence dwell on the destination page. In inform ation foraging theory [18] where this corresponds to a food patch, us ers satisfy some or all of their need and do not pursue the information scent further. While destinations were useful in one metric, adding intermediate pages contributed to gains in seve ral metrics, notably in novelty, diversity, and utility, where the differences between origins and sub-trails are substantial. The success of sub-trails suggests that users may not need to traverse full-trails to derive significant val-ue from post-query navigation. As expected, full-trails provide even more benefit than sub-trails ; full-trails are sub-trails plus destinations . Although the findings of our study appear to support trail recommendation, they also suggest that the nature of the query is important. For some queries, the trails might be useful in supporting exploration, but for ot her queries, especially for fo-cused tasks, presenting trail info rmation might be a hindrance. Questions remain about how to sele ct trails and how to integrate trails into the SERP. Popular sear ch trails are typically short and obvious, so we need to consider diverse and unexpected trails, perhaps leveraging popular sub-trails as well as full trails in trail selection algorithms. Trail select ion methods could discount trails with numerous cases of rapid b acktracking or maximize relev-ance, coverage, diversity, novelty , and utility with the shortest path. Alternatively, we can personalize trail recommendation by weighting trails based on the extent of the current user X  X  re-finding behavior or perform a priori trail analysis to recommend trails when the destination is unclear (i.e., users end up on many pages), and present trail destinations when the destination is clear (i.e., many users end up at the sa me page). Trails can be presented as an alternative to result lists, as instant answers above result lists, in pop-ups shown after hove ring over a result, below each result along with the snippet and URL, or even on the click trail a user is following. Follow-up user studies and large-scale flights will further analyze trail appropriateness for different queries and compare trail selection algorithms and trail presentation methods. In this paper we have presented a study estimating the value of search trails to users. Our l og-based methodology has allowed us to systematically compare the estimated value of trails to other trail components: trail origins (cli cked search results), the trail destinations (terminal trail page s), and sub-trails comprising the origin plus intermediate pages. We studied the relevance, cover-age, diversity, novelty, and utility of each of the four sources using metrics devised for this purpo se, human relevance judgments, historic log data, and URL classi fication where appropriate. When we varied the query by overall popularity, the values of each me-tric increased with query frequency. The evaluation showed that full-trails and sub-trails provide users with significantly more topic coverage, topic di versity, and novelty than trail origins, and slightly more useful but slightly less relevant information than the origins. Our findings show that there is value in the trail (the scen-ic route), as well as the origin and the destinatio n. These findings vary slightly by query popularit y over all users and significantly by the level of re-finding perform ed by a user for a given query. The next steps are to investigat e best-trail selection for query-origin pairs and add trails to search engine result pages. [1] Agichtein, E., Brill, E. &amp; Dumais, S. (2006). Improving web [2] Bates, M.J. (1989). The design of browsing and berrypicking [3] Bilenko, M. &amp; White, R.W. (2008). Mining the search trails [4] Bush, V. (1945) As we may think. Atlantic Monthly , 3(2): [5] Card, S.K. et al. (2001). Informat ion scent as a driver of web [6] Chalmers, M., Rodden, K. &amp; Brodbeck, D. (1998). The order [7] Clarke, C.L.A. et al. (2008). N ovelty and diversity in infor-[8] Cole, M. et al. (2009) Usefulness as the criterion for evalua-[9] Downey, D., Dumais, S. &amp; Horvitz, E. (2007). Models of [10] Downey, D. et al. (2008). U nderstanding the relationship [11] Fox, S. et al. (2005). Evaluating implicit measures to im-[12] Freyne, J. et al. (2007). Coll ecting community wisdom: inte-[13] Fu, W.-T. &amp; Pirolli, P. (2007). SNIF-ACT: A cognitive model [14] Joachims, T. (2002). Optimizi ng search engines using click-[15] Kelly, D. &amp; Cool, C. (2002). The effects of topic familiarity [16] Olston, C. &amp; Chi, E.H. (2003) . ScentTrails: integrating [17] O X  X ay, V. &amp; Jeffries, R. (1993). Orienteering in an informa-[18] Pirolli, P. &amp; Card, S.K. (1999). Information foraging. Psy-[19] Reich, S. et al. (1999). Where have you been from here? [20] Rose, D.E. &amp; Levinson, D. (2004 ). Understanding user goals [21] Shen, X., Dumais, S. &amp; Horvitz, E. (2005). Analysis of topic [22] Singhal, A. (2001). Modern information retrieval: a brief [23] Singla, A., White, R.W. &amp; Huang, J. (2010). Studying trail-[24] Teevan, J. et al. (2004). The perfect search engine is not [25] Teevan, J. et al. (2007) Inform ation re-retrieval: repeat que-[26] Trigg, R.H. (1988). Guided tours and tabletops: tools for [27] Tyler, S.K. &amp; Teevan, J. (2010). Large scale query log analy-[28] Wang, X. &amp; Zhai, C. (2009). Beyond hyperlinks: organizing [29] Wexelblat, A. &amp; Maes, P. ( 1999). Footprints: history-rich [30] Wheeldon, R. &amp; Levene, M. (2003). The best trail algorithm [31] White, R.W., Bailey, P. &amp; Chen, L. (2009). Predicting user [32] White, R.W., Bilenko, M. &amp; Cucerzan, S. (2007). Studying [33] White, R.W. &amp; Drucker, S.M. (2007). Investigating beha-
