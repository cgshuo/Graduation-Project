 Sequential recurrent neural networks (RNNs) are remarkably effective models of natural language. In the last few years, language model results that substantially improve over long-established state-of-the-art baselines have been obtained using RNNs (Zaremba et al., 2015; Mikolov et al., 2010) as well as in various conditional language modeling tasks such as machine translation (Bahdanau et al., 2015), image caption generation (Xu et al., 2015), and dia-logue generation (Wen et al., 2015). Despite these impressive results, sequential models are a priori inappropriate models of natural language, since re-lationships among words are largely organized in terms of latent nested structures rather than sequen-tial surface order (Chomsky, 1957).

In this paper, we introduce recurrent neural net-work grammars (RNNGs;  X  2), a new generative probabilistic model of sentences that explicitly mod-els nested, hierarchical relationships among words and phrases. RNNGs operate via a recursive syntac-tic process reminiscent of probabilistic context-free grammar generation, but decisions are parameter-ized using RNNs that condition on the entire syntac-tic derivation history, greatly relaxing context-free independence assumptions.

The foundation of this work is a top-down vari-ant of transition-based parsing (  X  3). We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neu-ral models of syntactic generation exist (Hender-son, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and left-corner parsers which operate in a largely bottom-up fashion. While this construction is appealing be-cause inference is relatively straightforward, it lim-its the use of top-down grammar information, which maintain the algorithmic convenience of transition-based parsing but incorporate top-down (i.e., root-to-terminal) syntactic information (  X  4).

The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. Sim-ilar to previously published discriminative bottom-up transition-based parsers (Henderson, 2004; Sagae and Lavie, 2005; Zhang and Clark, 2011, inter alia ), greedy prediction with our model yields a linear-time deterministic parser (provided an upper bound on the number of actions taken between process-ing subsequent terminal symbols is imposed); how-ever, our algorithm generates arbitrary tree struc-tures directly, without the binarization required by shift-reduce parsers. The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences, and this is used to solve a second practical challenge with RNNGs: approx-imating the marginal likelihood and MAP tree of a sentence under the generative model. We present a simple importance sampling algorithm which uses samples from the discriminative parser to solve in-ference problems in the generative model (  X  5).
Experiments show that RNNGs are effective for both language modeling and parsing (  X  6). Our gen-erative model obtains (i) the best-known parsing re-sults using a single supervised generative model and (ii) better perplexities in language modeling than state-of-the-art sequential LSTM language models. Surprisingly X  X lthough in line with previous pars-ing results showing the effectiveness of genera-tive models (Henderson, 2004; Johnson, 2001) X  parsing with the generative model obtains signifi-cantly better results than parsing with the discrim-inative model. Formally, an RNNG is a triple ( N,  X  ,  X ) consisting of a finite set of nonterminal symbols ( N ), a finite set of terminal symbols (  X  ) such that N  X   X  =  X  , and a collection of neural network parameters  X  . It does not explicitly define rules since these are im-plicitly characterized by  X  . The algorithm that the grammar uses to generate trees and strings in the lan-guage is characterized in terms of a transition-based algorithm, which is outlined in the next section. In the section after that, the semantics of the param-eters that are used to turn this into a stochastic al-gorithm that generates pairs of trees and strings are discussed. RNNGs are based on a top-down generation algo-rithm that relies on a stack data structure of par-tially completed syntactic constituents. To empha-size the similarity of our algorithm to more familiar bottom-up shift-reduce recognition algorithms, we first present the parsing (rather than generation) ver-sion of our algorithm (  X  3.1) and then present modi-fications to turn it into a generator (  X  3.2). 3.1 Parser Transitions The parsing algorithm transforms a sequence of words x into a parse tree y using two data structures (a stack and an input buffer). As with the bottom-up algorithm of Sagae and Lavie (2005), our algo-rithm begins with the stack ( S ) empty and the com-plete sequence of words in the input buffer ( B ). The buffer contains unprocessed terminal symbols, and the stack contains terminal symbols,  X  X pen X  nonter-minal symbols, and completed constituents. At each timestep, one of the following three classes of op-erations (Fig. 1) is selected by a classifier, based on the current contents on the stack and buffer:  X 
NT (X) introduces an  X  X pen nonterminal X  X onto the top of the stack. Open nonterminals are written as a nonterminal symbol preceded by an open parenthesis, e.g.,  X (VP X , and they represent a nonterminal whose child nodes have not yet been fully constructed. Open nonterminals are  X  X losed X  to form complete constituents by subse-quent REDUCE operations.  X 
SHIFT removes the terminal symbol x from the front of the input buffer, and pushes it onto the top of the stack.  X 
REDUCE repeatedly pops completed subtrees or terminal symbols from the stack until an open nonterminal is encountered, and then this open
NT is popped and used as the label of a new con-stituent that has the popped subtrees as its chil-dren. This new completed constituent is pushed onto the stack as a single composite item. A single
REDUCE operation can thus create constituents with an unbounded number of children.
 The parsing algorithm terminates when there is a single completed constituent on the stack and the buffer is empty. Fig. 2 shows an example parse using our transition set. Note that in this paper we do not model preterminal symbols (i.e., part-of-speech tags) and our examples therefore do not in-
Our transition set is closely related to the op-erations used in Earley X  X  algorithm which likewise introduces nonterminals symbols with its PREDICT operation and later COMPLETE s them after consum-ing terminal symbols one at a time using SCAN (Earley, 1970). It is likewise closely related to the  X  X inearized X  parse trees proposed by Vinyals et al. (2015) and to the top-down, left-to-right decompo-sitions of trees used in previous generative parsing and language modeling work (Roark, 2001, 2004; Charniak, 2010).

A further connection is to LL (  X  ) parsing which uses an unbounded lookahead (compactly repre-sented by a DFA) to distinguish between parse alter-natives in a top-down parser (Parr and Fisher, 2011); however, our parser uses an RNN encoding of the lookahead rather than a DFA.
 Constraints on parser transitions. To guarantee that only well-formed phrase-structure trees are pro-duced by the parser, we impose the following con-straints on the transitions that can be applied at each step which are a function of the parser state ( B,S,n ) where n is the number of open nonterminals on the stack:  X  The NT (X) operation can only be applied if B is not empty and n &lt; 100 . 3  X  The SHIFT operation can only be applied if B is not empty and n  X  1 .  X  The REDUCE operation can only be applied if the top of the stack is not an open nonterminal sym-bol.  X  The REDUCE operation can only be applied if n  X  2 or if the buffer is empty.
 To designate the set of valid parser transitions, we write A D ( B,S,n ) . 3.2 Generator Transitions The parsing algorithm that maps from sequences of words to parse trees can be adapted with mi-nor changes to produce an algorithm that stochas-tically generates trees and terminal symbols. Two changes are required: (i) there is no input buffer of unprocessed words, rather there is an output buffer ( T ), and (ii) instead of a SHIFT operation there are GEN ( x ) operations which generate terminal symbol x  X   X  and add it to the top of the stack and the out-put buffer. At each timestep an action is stochasti-cally selected according to a conditional distribution that depends on the current contents of B and T . The algorithm terminates when a single completed constituent remains on the stack. Fig. 4 shows an example generation sequence.
 Constraints on generator transitions. The gen-eration algorithm also requires slightly modified constraints. These are:  X  The GEN ( x ) operation can only be applied if n  X  1 .  X  The REDUCE operation can only be applied if the top of the stack is not an open nonterminal symbol and n  X  1 .
 To designate the set of valid generator transitions, we write A G ( T,S,n ) .

This transition set generates trees using nearly the same structure building actions and stack configura-tions as the  X  X op-down PDA X  construction proposed by Abney et al. (1999), albeit without the restriction that the trees be in Chomsky normal form. 3.3 Transition Sequences from Trees Any parse tree can be converted to a sequence of transitions via a depth-first, left-to-right traversal of a parse tree. Since there is a unique depth-first, left-ro-right traversal of a tree, there is exactly one tran-sition sequence of each tree. For a tree y and a sequence of symbols x , we write a ( x , y ) to indi-cate the corresponding sequence of generation tran-sitions, and b ( x , y ) to indicate the parser transitions. 3.4 Runtime Analysis A detailed analysis of the algorithmic properties of our top-down parser is beyond the scope of this pa-per; however, we briefly state several facts. As-suming the availability of constant time push and pop operations, the runtime is linear in the number of the nodes in the parse tree that is generated by the parser/generator (intuitively, this is true since al-though an individual REDUCE operation may require Stack t Buffer t Open NTs t Action Stack t +1 Buffer t +1 S B n NT (X) S | (X B n + 1 S x | B n SHIFT S | x B n S | (X |  X  1 | ... |  X  ` B n REDUCE S | (X  X  1 ...  X  ` ) B n  X  1
The | hungry | cat | meows | . NT (S)
Stack t Terms t Open NTs t Action Stack t +1 Terms t +1 Open NTs S T n NT (X) S | (X T n + 1
S T n GEN ( x ) S | x T | x n S | (X |  X  1 | ... |  X  ` T n REDUCE S | (X  X  1 ...  X  ` ) T n  X  1 applying a number of pops that is linear in the num-ber of input symbols, the total number of pop opera-tions across an entire parse/generation run will also be linear). Since there is no way to bound the num-ber of output nodes in a parse tree as a function of the number of input words, stating the runtime com-plexity of the parsing algorithm as a function of the input size requires further assumptions. Assuming our fixed constraint on maximum depth, it is linear. 3.5 Comparison to Other Models Our generation algorithm algorithm differs from previous stack-based parsing/generation algorithms in two ways. First, it constructs rooted tree struc-tures top down (rather than bottom up), and sec-ond, the transition operators are capable of directly generating arbitrary tree structures rather than, e.g., assuming binarized trees, as is the case in much prior work that has used transition-based algorithms to produce phrase-structure trees (Sagae and Lavie, 2005; Zhang and Clark, 2011; Zhu et al., 2013). RNNGs use the generator transition set just pre-sented to define a joint distribution on syntax trees ( y ) and words ( x ). This distribution is defined as a sequence model over generator transitions that is pa-rameterized using a continuous space embedding of the algorithm state at each time step ( u t ); i.e., p ( x , y ) = and where action-specific embeddings r a and bias vector b are parameters in  X  .

The representation of the algorithm state at time t , u t , is computed by combining the representation of the generator X  X  three data structures: the output buffer ( T t ), represented by an embedding o t , the stack ( S t ), represented by an embedding s t , and the history of actions ( a &lt;t ) taken by the generator, rep-resented by an embedding h t , where W and c are parameters. Refer to Figure 5 for an illustration of the architecture.

The output buffer, stack, and history are se-quences that grow unboundedly, and to obtain rep-resentations of them we use recurrent neural net-works to  X  X ncode X  their contents (Cho et al., 2014). Since the output buffer and history of actions are only appended to and only contain symbols from a finite alphabet, it is straightforward to apply a stan-dard RNN encoding architecture. The stack ( S ) is more complicated for two reasons. First, the ele-ments of the stack are more complicated objects than symbols from a discrete alphabet: open nontermi-nals, terminals, and full trees, are all present on the stack. Second, it is manipulated using both push and pop operations. To efficiently obtain representations of S under push and pop operations, we use stack LSTMs (Dyer et al., 2015). 4.1 Syntactic Composition Function When a REDUCE operation is executed, the parser pops a sequence of completed subtrees and/or to-kens (together with their vector embeddings) from the stack and makes them children of the most recent open nonterminal on the stack,  X  X ompleting X  the constituent. To compute an embedding of this new subtree, we use a composition function based on bidirectional LSTMs, which is illustrated in Fig. 6.
The first vector read by the LSTM in both the for-ward and reverse directions is an embedding of the label on the constituent being constructed (in the fig-ure, NP). This is followed by the embeddings of the child subtrees (or tokens) in forward or reverse or-der. Intuitively, this order serves to  X  X otify X  each LSTM what sort of head it should be looking for as it processes the child node embeddings. The final state of the forward and reverse LSTMs are concatenated, passed through an affine transformation and a tanh cause each of the child node embeddings ( u , v , w in Fig. 6) is computed similarly (if it corresponds to an internal node), this composition function is a kind of recursive neural network. 4.2 Word Generation To reduce the size of A G ( S,T,n ) , word genera-tion is broken into two parts. First, the decision to generate is made (by predicting GEN as an action), and then choosing the word, conditional on the cur-rent parser state. To further reduce the computa-tional complexity of modeling the generation of a word, we use a class-factored softmax (Baltescu and Blunsom, 2015; Goodman, 2001). By using classes for a vocabulary of size |  X  | , this prediction step runs in time O ( the full-vocabulary softmax. To obtain clusters, we use the greedy agglomerative clustering algorithm of Brown et al. (1992). 4.3 Training The parameters in the model are learned to maxi-mize the likelihood of a corpus of trees. 4.4 Discriminative Parsing Model A discriminative parsing model can be obtained by replacing the embedding of T t at each time step with an embedding of the input buffer B t . To train this model, the conditional likelihood of each sequence Our generative model p ( x , y ) defines a joint dis-tribution on trees ( y ) and sequences of words ( x ). To evaluate this as a language model, it is neces-sary to compute the marginal probability p ( x ) = P a parser, we need to be able to find the MAP parse tree, i.e., the tree y  X  X  ( x ) that maximizes p ( x , y ) . However, because of the unbounded dependencies across the sequence of parsing actions in our model, exactly solving either of these inference problems is intractable. To obtain estimates of these, we use a variant of importance sampling (Doucet and Jo-hansen, 2011).

Our importance sampling algorithm uses a condi-tional proposal distribution q ( y | x ) with the fol-lowing properties: (i) p ( x , y ) &gt; 0 =  X  q ( y | x ) &gt; 0 ; (ii) samples y  X  q ( y | x ) can be ob-tained efficiently; and (iii) the conditional probabil-ities q ( y | x ) of these samples are known. While many such distributions are available, the discrim-inatively trained variant of our parser (  X  4.4) ful-fills these requirements: sequences of actions can be sampled using a simple ancestral sampling ap-proach, and, since parse trees and action sequences exist in a one-to-one relationship, the product of the action probabilities is the conditional probability of the parse tree under q . We therefore use our discrim-inative parser as our proposal distribution.
Importance sampling uses importance weights , which we define as w ( x , y ) = p ( x , y ) /q ( y | x ) , to compute this estimate. Under this definition, we can derive the estimator as follows: p ( x ) = We now replace this expectation with its Monte Carlo estimate as follows, using N samples from q : To obtain an estimate of the MAP tree  X  y , we choose the sampled tree with the highest probability under the joint model p ( x , y ) . We present results of our two models both on parsing (discriminative and generative) and as a language model (generative only) in English and Chinese. Data. For English,  X  2 X 21 of the Penn Treebank are used as training corpus for both, with  X  24 held out as validation, and  X  23 used for evaluation. Sin-gleton words in the training corpus with unknown word classes using the the Berkeley parser X  X  map-served, and numbers (beyond singletons) are not normalized. For Chinese, we use the Penn Chinese the Chinese experiments, we use a single unknown Model and training parameters. For the dis-criminative model, we used hidden dimensions of 128 and 2-layer LSTMs (larger numbers of dimen-sions reduced validation set performance). For the generative model, we used 256 dimensions and 2-layer LSTMs. For both models, we tuned the dropout rate to maximize validation set likelihood, obtaining optimal rates of 0.2 (discriminative) and 0.3 (generative). For the sequential LSTM baseline for the language model, we also found an optimal dropout rate of 0.3. For training we used stochas-tic gradient descent with a learning rate of 0.1. All parameters were initialized according to recommen-dations given by Glorot and Bengio (2010).
 English parsing results. Table 2 (last two rows) gives the performance of our parser on Section 23, as well as the performance of several representa-tive models. For the discriminative model, we used a greedy decoding rule as opposed to beam search in some shift-reduce baselines. For the generative model, we obtained 100 independent samples from a flattened distribution of the discriminative parser (by exponentiating each probability by  X  = 0 . 8 and renormalizing) and reranked them according to the Chinese parsing results. Chinese parsing results were obtained with the same methodology as in En-glish and show the same pattern (Table 6).
 Language model results. We report held-out per-word perplexities of three language models, both se-quential and syntactic. Log probabilities are normal-ized by the number of words (excluding the stop Model type F 1 Henderson (2004) D 89.4 Socher et al. (2013a) D 90.4 Zhu et al. (2013) D 90.4 Vinyals et al. (2015)  X  WSJ only D 90.5 Petrov and Klein (2007) G 90.1 Bod (2003) G 90.7 Shindo et al. (2012)  X  single G 91.1 Shindo et al. (2012)  X  ensemble G 92.4 Zhu et al. (2013) S 91.3 McClosky et al. (2006) S 92.1 Vinyals et al. (2015)  X  single S 92.5 Vinyals et al. (2015)  X  ensemble S 92.8 Discriminative, q ( y | x ) D 89.8 Generative,  X  p ( y | x ) G 92.4 symbol), inverted, and exponentiated to yield the perplexity. Results are summarized in Table 4. It is clear from our experiments that the proposed generative model is quite effective both as a parser and as a language model. This is the result of (i) relaxing conventional independence assumptions (e.g., context-freeness) and (ii) inferring continu-ous representations of symbols alongside non-linear models of their syntactic relationships. The most significant question that remains is why the dis-criminative model X  X hich has more information available to it than the generative model X  X erforms Model test ppl (PTB) test ppl (CTB) IKN 5-gram 169.3 255.2 LSTM LM 113.4 207.3
RNNG 102.4 171.9 worse than the generative model. This pattern has been observed before in neural parsing by Hender-son (2004), who hypothesized that larger, unstruc-tured conditioning contexts are harder to learn from, and provide opportunities to overfit. Our discrimi-native model conditions on the entire history, stack, and buffer, while our generative model only ac-cesses the history and stack. The fully discrimina-tive model of Vinyals et al. (2015) was able to obtain results similar to those of our generative model (al-beit using much larger training sets obtained through semisupervision) but similar results to those of our discriminative parser using the same data. In light of their results, we believe Henderson X  X  hypothesis is correct, and that generative models should be con-sidered as a more statistically efficient method for learning neural networks from small data. Our language model combines work from two mod-eling traditions: (i) recurrent neural network lan-guage models and (ii) syntactic language model-ing. Recurrent neural network language models use RNNs to compute representations of an un-bounded history of words in a left-to-right language model (Zaremba et al., 2015; Mikolov et al., 2010; Elman, 1990). Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991). There is an extensive literature here, but one strand of work has emphasized a bottom-up generation of the tree, using variants of shift-reduce parser ac-tions to define the probability space (Chelba and Jelinek, 2000; Emami and Jelinek, 2005). The neural-network X  X ased model of Henderson (2004) is particularly similar to ours in using an unbounded history in a neural network architecture to param-eterize generative parsing based on a left-corner model. Dependency-only language models have also been explored (Titov and Henderson, 2007; Buys and Blunsom, 2015a,b). Modeling generation top-down as a rooted branching process that recur-sively rewrites nonterminals has been explored by Charniak (2000) and Roark (2001). Of particular note is the work of Charniak (2010), which uses ran-dom forests and hand-engineered features over the entire syntactic derivation history to make decisions over the next action to take.

The neural networks we use to model sentences are structured according to the syntax of the sen-tence being generated. Syntactically structured neu-ral architectures have been explored in a num-ber of applications, including discriminative pars-ing (Socher et al., 2013a; Kiperwasser and Gold-berg, 2016), sentiment analysis (Tai et al., 2015; Socher et al., 2013b), and sentence representa-tion (Socher et al., 2011; Bowman et al., 2006). However, these models have been, without excep-tion, discriminative; this is the first work to use syn-tactically structured neural models to generate lan-guage. Earlier work has demonstrated that sequen-tial RNNs have the capacity to recognize context-free (and beyond) languages (Sun et al., 1998; Siegelmann and Sontag, 1995). In contrast, our work may be understood as a way of incorporating a context-free inductive bias into the model structure. RNNGs can be combined with a particle filter infer-ence scheme (rather than the importance sampling method based on a discriminative parser,  X  5) to pro-duce a left-to-right marginalization algorithm that runs in expected linear time. Thus, they could be used in applications that require language models.
A second possibility is to replace the sequential generation architectures found in many neural net-work transduction problems that produce sentences conditioned on some input. Previous work in ma-chine translation has showed that conditional syn-tactic models can function quite well without the computationally expensive marginalization process at decoding time (Galley et al., 2006; Gimpel and Smith, 2014).

A third consideration regarding how RNNGs, hu-man sentence processing takes place in a left-to-right, incremental order. While an RNNG is not a processing model (it is a grammar), the fact that it is left-to-right opens up several possibilities for devel-oping new sentence processing models based on an explicit grammars, similar to the processing model of Charniak (2010).

Finally, although we considered only the super-vised learning scenario, RNNGs are joint models that could be trained without trees, for example, us-ing expectation maximization. We introduced recurrent neural network grammars, a probabilistic model of phrase-structure trees that can be trained generatively and used as a language model or a parser, and a corresponding discrimina-tive model that can be used as a parser. Apart from out-of-vocabulary preprocessing, the approach re-quires no feature design or transformations to tree-bank data. The generative model outperforms ev-ery previously published parser built on a single su-pervised generative model in English, and a bit be-hind the best-reported generative model in Chinese. As language models, RNNGs outperform the best single-sentence language models.
 We thank Brendan O X  X onnor, Swabha Swayamdipta, and Brian Roark for feedback on drafts of this paper, and Jan Buys, Phil Blunsom, and Yue Zhang for help with data preparation. This work was sponsored in part by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under Contract No. HR0011-15-C-0114; it was also sup-ported in part by Contract No. W911NF-15-1-0543 with the DARPA and the Army Research Office (ARO). Approved for public release, distribution unlimited. The views expressed are those of the au-thors and do not reflect the official policy or position of the Department of Defense or the U.S. Govern-ment. Miguel Ballesteros was supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA).
