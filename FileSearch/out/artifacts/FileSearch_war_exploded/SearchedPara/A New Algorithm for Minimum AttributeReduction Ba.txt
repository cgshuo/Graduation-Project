 Attribute reduction of a decision table based on the theory of rough sets has proved to be a very useful approach for knowledge discovery [1][2]. Finding just a reduct is usually not a difficult task and there have been many heuristic algo-rithms available in the literature for this purpose [3]-[6]. Computing a minimum attribute reduct that contains the least number of attributes is however more difficult and has been proved to be an NP-hard problem by Wong and Ziarko [7]. It turns out that the above mentioned algorithms are generally not effective since there is no guarantee for them to get a minimum reduct.

Formally, the minimum attribute reduction problem is a nonlinearly con-strained combinatorial optimization problem. Hence, global optimization meth-ods could be used to solve it. As a matter of fact, several algorithms along this direction have been investigated. For instance, Wroblewski [8] and Li [9] dis-cussed in different ways the application o f genetic algorithms(GAs) to the min-imum attribute reduction problem and some interesting results were reported. More recently, the authors [10] and Dai [11] independently applied the binary swarm optimization algorithm due to Kennedy and Eberhart to deal with the problem and the results seemed to be en couraging. However, these algorithms are not quite effective in the sense that the probability for them to find a min-imum reduct appears to be low. For some data sets, the algorithms may even perform much worse than Hu and Cercone X  X  heuristic reduction algorithm [3]. One reason is that the fitness functions used in these algorithms are not suitable enough for ensuring the optimality of the final results computed. This is due to kind of inappropriate use of the penalty function method in defining the fitness functions. Actually, a suitable fitness f unction is crucial to a successful applica-tion of evolutionary computation methods like genetic algorithms and particle swarm optimization algorithms. In this paper, the problem of minimum attribute reduction is transformed into an unconstrained binary optimization problem. A suitable fitness function is defined and the equivalence of optimality between the original problem and the transformed one is proved. An improved binary parti-cle swarm optimization algorithm combined with some vaccination mechanism is then presented to solve the transform ed problem. Experimental results on a number of data sets obtained from the UCI machine learning repository show that the proposed algorithm has a higher possibility of finding a minimum reduc-tion and remarkably outperforms some existing algorithms specifically designed for minimum attribute reduction in both quality of solution and computational complexity.

The rest of the paper is organized as fo llows. Section 2 pres ents some back-ground information on the attribute reduction problem. Section 3 describes how to equivalently transform the minimum attribute reduction problem into an un-constrained binary optimization problem by defining a suitable fitness function. In section 4, we give an improved binary swarm optimization algorithm for solv-ing the transformed problem. In section 5, we present some experimental results and finally, in section 6, we conclude. A decision table can be represented as a quadruple L = { U, A, V, f } [1], where A is a union of condition attributes set C and decision attributes set D , V is the domains of attributes belonging to A ,and f : U  X  A  X  X  X  V is an information function assigning attribute values to objects belonging to U . Assume that C contains m condition attributes a 1 ,  X  X  X  ,a m and without loss of generality that D contains only one decision attribute which takes k ( &gt; 1) distinct values. For a subset P  X  A , IND ( P ) represents the indiscernible relation induced by the attributes belonging to P and there should be no confusion if we use U to represent either a set of a ttributes or the relation IND ( P ). A subset X  X  U represents a concept and the partition induced by IND ( P ) is called a knowledge base and denoted by U/IND ( P ). In particular, U/IND ( D )= { Y 1 ,  X  X  X  ,Y k } is the knowledge base of decision classes.
 Let X  X  U and R  X  C .The R  X  X ower approximation of X is defined as R X = { x  X  U :[ x ] R  X  X } ,where[ x ] R refers to an equivalence class of IND ( R ) determined by element x .The R  X  X pproximation quality with respect to decisions ourself to the classic reduction as defined in the following.
 Definition 1. Let R  X  C .If R is a minimal set satisfying  X  R =  X  C ,then R is said to be a relative reduct of C or simply a reduct. The intersection of all reducts is called the attribute core of C and denoted as Core ( C ) . A minimum reduct is a reduct that contains the least number of attributes. Usually, there can be more than one minimum reduct. By definition, finding a minimum attribute reduction can be formulated as a nonlinearly constrained combinatorial optimization problem as follows:
Let { 0 , 1 } m be the m -dimensional Boolean space and  X  be a mapping from { 0 , 1 } m to the power set 2 C such that: Then, the minimum reduction problem (1) can be reformulated as the following constrained binary optimization problem: where 0  X  S ( x )= m i =1 x i  X  m .

Given a vector x  X  X  0 , 1 } m , if it is a feasible solution to Problem (2), then its corresponding subset of attributes  X  ( x ) is a reduct. Furthermore, if it is an optimal solution to Problem (2), then  X  ( x ) is a minimum reduct.

The first PSO algorithm was introduced in 1995 by Kennedy and Eberhart [12] for continuous optimization problems and since then many improved ver-sions of it have been presented [13][14]. It is a population-based optimization algorithm inspired by the social behavior of birds and, like other algorithms of its kind, it is initialized with a population of possible solutions (called particles) randomly located in a d -dimensional solution space. A fitness function deter-mines the quality of a particle X  X  position. A particle at time step t has a position vector and a velocity vector. The algorithm iterates updating the trajectories of the swarm through the solution space on the basis of information about each particle X  X  previous best performance and the best previous performance of its neighbors until a stopping criterion is met. In 1997, Kennedy and Eberhart [15] developed a binary version of PSO for solving combinatorial optimization prob-lems.

Usually, PSO algorithms can be directly applied to solve an unconstrained optimization problem since the fitness function can be defined in a straight-forward way. However, when dealing with a constrained optimization problem, things become a bit complicated. The mo st commonly used approach is to trans-form the constrained problem into an unconstrained one via the penalty function method. This amounts to defining a fitness function by enforcing the constraints into the objective function. However, if the penalty is not properly imposed on the fitness function, the transformation will not assure the equivalence of op-timality between the two problems. Thus, it is important to define a suitable fitness function for ensuring a better performance of a PSO algorithm. We shall discuss in this section how to equivalently transform Problem (2) into an unconstrained binary optimization problem that could be directly solved by a binary particle swarm optimization method.

Let us consider the following unconstrained binary optimization problem: where the fitness function is given by
We have the following main results con cerning the equivalence between Prob-lem (2) and Problem (3).
 Theorem 1. If x  X  is an optimal solution to Problem (2), then x  X  is also an optimal solution to Problem (3).
 Proof. Let P =  X  ( x  X  ). By hypothesis, we have  X  P =  X  C and hence F ( x  X  )=  X 
C +2 m then by definition, F ( x )= m  X  s ( x )+  X  R &lt;m +  X  C  X  F ( x  X  ). If  X  R =  X  C and R is a reduct, then by the optimality of x  X  and the definition of F ,we If  X  R =  X  C and R is not a reduct, then it means that R contains a reduct complete.
 Theorem 2. Suppose that x  X  is an optimal solution to Problem (3). Let P =  X  ( x  X  ) .Then, x  X  is also an optimal solution to Problem (2), or P is a minimum reduct.
 Proof. First, we show that  X  P =  X  C . We use proof by contradiction. Assume that  X  P &lt; X  C . Then, by definition, F ( x  X  )= m  X  s ( x  X  )+  X  P &lt;m +  X  C .Let y =(1 , 1 ,  X  X  X  , 1) T  X  X  0 , 1 } m .Wehave  X  ( y )= C and S ( y )= m . By definition, F ( y )=  X  C +2 m  X  m&gt;F ( x  X  ), contradicting the optimality of x  X  .Thus,  X  P =  X  . Next, assume that there exists Q , a subset of P , such that  X  Q =  X  C .Let  X 
C +2 m thus shown that x  X  is a feasible solution of Problem (2).

Now, for any feasible solution  X  x of Problem (2), we have  X  R =  X  C with R =  X  ( X  x ). By definition, F ( X  x )=  X  C +2 m  X  S ( X  x ). Since x  X  is a maximum of F by This implies that x  X  is also an optimal solution to Problem (2). The proof is thus done. We present in this section an improved b inary PSO algorithm with some vacci-nation mechanism for solving Problem (3). We choose to use Skowron X  X  discerni-bility matrix of a decision table [16] as a criterion for preparing vaccines. The basic steps of our algorithm is as follows:
Step 1. Initialization. { 0 , 1 } m istheparticlespace;thesizeoftheswarmisN; the maximum number of iterations is set to T; the velocity along each dimension is bounded by v max .

Initialize the swarm as P (0) = { x 1 (0) ,  X  X  X  ,x N (0) } and the corresponding ve-locity vectors as v i (0) ,i =1 ,  X  X  X  ,N . Initialize the i th particle X  X  previous best performance position pb i (0) as x i (0) and then identify the best previous perfor-mance position of the swarm as gb (0). Set t =0.

Step 2. Calculate the discernibility matrix M and Core ( C ) [17]. If Core ( C )= { a denote by frq ( a ) the frequency of its occurrence in matrix M and set where fmax and fmin are respectively the maximum and minimum of all these frequencies.
 Compute the vaccination pattern HC = { h ( a j ): j  X  X  1 ,  X  X  X  ,m }\ IC } .
Step 3. Update the positions and velocity of particles according to the fol-lowing equations: where pb i ( t ) is the previous best performance position of particle i and gb ( t )is the best previous performance position of the whole swarm; w ( t )= T  X  t T is the interval [0,1].
Step 4. Vaccination. Randomly select a subs et of particles and inject to them the vaccine pattern HC according to the following rule:
If i th particle is chosen for vaccination, then its position after vaccination, denoted by y i ( t ), is computed as follows: where rand is a uniformly randomized number in the interval [0,1].

An immune selection is then performed in such a way that if the vaccination increases the fitness value of a particle, then the particle is replaced by the vaccinated one.

Step 5. If some stopping cr iterion is met or t&gt; T, then stop and output the subset of attributes  X  ( gb ( t )). Otherwise, set t = t + 1, repeat Step 3. To evaluate its performance, the propo sed algorithm IPSO was implemented on a 2.8GHz machine running Windows XP with 512 MB of main memory and then tested on 5 real data sets obtained from the UCI machine learning repository. These data sets were chosen because Hu X  X  reduction algorithm [3] fails to get a minimum reduct for each of them. For comparison, three minimum reduction algorithms, denoted by GA1 [9], PSO1 [10] and PSO2 [11] respectively, were also tested. Due to page limitation, we report here only the results corresponding to a specific setting of parameters. These parameters were as follows: T = 500, N = 20, the learning coefficients c 1 = c 2 = 2 for all PSO based algorithms, the crossover probability p c = 0.7 and mutation probability p m =0 . 01 in algorithm GA1. In order to test how fast an algorithm can find a solution, a minimum reduct for each of these data sets was calculated beforehand via an exhaustive search and was then used to define the optimality stopping criterion. If an al-gorithm terminates with a solution satisfying the stopping criterion within the allowed iterations, then the solution corresponds to a minimum reduct and we say that this run of the algorithm is successful. Each algorithm was independently run 20 times in the experiments and three values were reported, including the number of attributes contained in the best solution found during the 20 runs, the ratio of successful runs and the mean computational time. The results are respectively listed in Tables 1, 2 and 3. For each test data set, the information on the number of attributes of a minimum reduct is also included in Table 1 under the column label Known Best .

Table 1 shows the best results picked up from among the 20 output solutions of the 20 runs of each algorithm. We see that our proposed algorithm could find a minimum reduct for all test data sets, while none of the other algorithms could achieve this goal within the allowed iterations. Actually, the other algorithms could obtain a minimum reduct only for the first two data sets.

Table 2 shows how frequently an algorithm could find a minimum reduct during the 20 runs. It can be seen that for each test data set, the proposed algorithm had a high ratio of successful runs or a high probability of getting a minimum reduct, whereas the other algorithms rarely had successful runs.
Table 3 shows the experimental results on the average computational time of each algorithm. Obviously, the proposed algorithm performs better than the others. We have studied in this paper the problem of how to effectively compute a mini-mum reduct of a decision table based on PSO algorithms. By defining a suitable fitness function for the evaluation of a particle X  X  quality, we developed an im-proved binary PSO algorithm that outper forms some recent global optimization techniques based algorithms for minimum attribute reduction. This suggests that binary PSO based algorithms could be promising and even competent in solving the minimum attribute reduction problem.
 Acknowledgement. This work was partly funded by National Science Founda-tion of China(No.60602052) and by Fujian Science Foundation(No.2006J0029).
