 We present Coclus , a novel diagonal co-clustering algorithm which is able to effectively co-cluster binary or contingency matrices by directly maximizing an adapted version of the modularity measure traditionally used for networks. While some effective co-clustering algorithms already exist that use network-related measures (normalized cut, modularity), they do so by using spectral relaxations of the discrete op-timization problems. In contrast, Coclus allows to get even better co-clusters by directly maximizing modularity using an iterative alternating optimization procedure. Extensive comparative experiments performed on various document-term datasets demonstrate that our algorithm is very effec-tive, stable and outperforms other co-clustering algorithms. H.3.3 [ Information Search and Retrieval ]: Clustering Co-Clustering, Modularity
In the era of big data, there is more than ever a need for techniques that simultaneously group words and documents into meaningful clusters, thus making large data sets easier to handle and interpret. Co-clustering techniques just serve this purpose.

Given a data matrix X of size n  X  d where I is the set of n rows, and J the set of d columns, a co-cluster k is a submatrix I k  X  J k ( I k  X  I,J k  X  J ) where rows and columns follow some consistent patterns.

For a survey of the different structures of co-clusters and the different algorithms employed the reader is referred to [6, 8, 2, 3].

In this paper, we are especially concerned with algorithms that seek an optimal block diagonal co-clustering, mean-ing that data points and features have the same number of clusters and that, after proper permutation of the rows and columns, the algorithm produces as result a block di-agonal matrix (see figure 1). In the context of document-term matrices, this co-clustering model has the advantage of directly producing interpretable descriptions of the re-sulting document clusters. For example, Li [5] has proposed a block diagonal clustering algorithm that, given a binary document-term matrix, produces a block diagonal matrix of ones. This algorithm consists in alternating the clustering Figure 1: Left : Original data -Middle : data re-organized according to row partition -Right : data reorganized according to row and column partitions. of rows and columns minimizing the squared error between the original X data and its approximation AB T where A and B are binary matrices. Another notable co-clustering algorithm is the bipartite spectral graph partitioning algo-rithm described in [1]. This algorithm finds the optimal min-imum cut partitions in a bipartite document-term graph by computing the second left and right singular vector of the normalized document-term matrix, thus using a real relax-ation of the discrete optimization problem. More recently, another attempt to use network-related criteria in the field of block diagonal structure is the spectral co-clustering max-imizing a generalization of the modularity [4]. Compared to other binary clustering methods based on nonnegative ma-trix factorization or tri-factorization , this algorithm appears to perform better in the field of document clustering. The two last-mentioned co-clustering algorithms [1, 4] both try to optimize criteria initially used in the studies of networks (normalized cut, modularity). However they do so by using a spectral relaxation of the discrete optimization problem. In contrast, our algorithm, Coclus , is based on a direct max-imization of one of these criteria, namely bipartite modu-larity. We show that this approach outperforms previous block diagonal co-clustering algorithms, as demonstrated by comparative experiments conducted on numerous datasets.
Notation. We will consider the partition of the sets I of n objects and the set J of d attributes into g non overlapping clusters, where g may be greater or equal to 2. Let us define an n  X  g index matrix Z and an d  X  g index matrix W with one column for each cluster; Z = ( Z 1 | Z 2 | ... | Z W = ( W 1 | W 2 | ... | W g ). Each column Z k or W k is defined as follows: z ik = 1 if object i belongs to cluster Z k and z otherwise, and in the same manner w jk = 1 if attribute j belongs to cluster W k and w jk = 0 otherwise.
In this section we first review the standard graph modu-larity measure and then present the adaptation needed for using modularity in the co-clustering context.
Modularity is a quality criterion for graph clustering, which has received considerable attention in several disciplines since the seminal work presented in [9]. Maximizing the modu-larity measure can be expressed in the form of an integer linear programming. Given the graph G = ( V,E ), let A be a binary, symmetric adjacency matrix with ( i,i 0 ) as entry; and a ii 0 = 1 if there is an edge between the nodes i and i there is no edge between nodes i and i 0 , a ii 0 is equal to zero. Finding a partition of the set of nodes V into homogeneous subsets leads to the resolution of the following integer linear program: max C Q ( A,C ) where Q ( A,C ) is the modularity Taking c ii 0 = P g k =1 z ik z i 0 k , the expression of Q becomes where 2 | E | = P i,i 0 a ii 0 = a .. is the total number of edges and a (1) becomes Q ( A,C ) = 1 2 | E | Trace [( A  X   X  ) C ] . The researched binary matrix C is defined by ZZ t which models a partition in a relational space and therefore must check the properties of an equivalence relation.
In a bipartite context, the basic idea is to model the si-multaneous row and column partitions using the relation C defined on I  X  J and called block seriation in [7]. Noting that C = ZW t and the general term can be expressed as follows: c ij = 1 if object i is in the same block as attribute j and c ij = 0 otherwise. Then c ij = P g k =1 z ik w jk . Now, given a rectangular matrix A defined on I  X  J , modularity can be reformulated as follows in the co-clustering context: where a .. = P i,j a ij = | E | is the total weight of edges and a i. = P j a ij (the degree of i ) and a .j = P i a ij (the degree of j ). This modularity measure can also take the following form:
Q ( A,C ) = 1 Matrix ZW t represents a block seriation relation respect-ing the binary , assignment constraints and triad impossible properties (see [7] for further details). As the objective func-tion (3) is linear with respect to C and as the constraints that C must respect are linear equations, the problem can theoretically be solved using an integer linear programming solver. However, this problem is NP hard, and as a result, in practice, we use heuristics for dealing with large data sets.
Hereafter, we propose to tackle the co-clustering problem by maximizing the modularity criterion.

Proposition 1: Let A be a ( n  X  d ) binary or contin-gency data and C be a ( n  X  d ) defining a block seriation, the modularity measure Q ( A,C ) can be rewritten as 1. Q ( A,C ) = 1 a
A W := { a ik = P d j =1 w jk a ij ; i = 1 ,...,n ; k = 1 ,...,g } 2. Q ( A,C ) = 1 a
A Z := { a jk = P n i =1 z ik a ij ; k = 1 ,...,g ; j = 1 ,...,d } Proof: Q ( A,C ) = 1 In the same manner, we can show that Using proposition 1, the goal is to maximize modularity by alternatively maximizing Q ( A W ,Z ) and Q ( A Z ,W ). The op-timal classification binary matrices Z  X  and W  X  are respec-tively defined by Z  X  = arg max Z Trace ( A W  X   X  W ) t W  X  = arg max clustering is described in more details in algorithm 1. Here-after, we illustrate an execution by means of a small example (a binary matrix A of size (5  X  4) with g = 2 co-clusters). Algorithm 1 Coclus
Input: binary or contingency data A , number of co-clusters g
Output: partition matrices Z and W 1. Initialization of W repeat until noChange of Q ( A,ZW t )
Coclus is computationally efficient for sparse data and its complexity can be shown to be O ( nz.it. (2 .g )) where nz is the number of non-zero values in the input data and it the number of iterations which is small. Empirically, about 15 iterations seemed sufficient for the datasets we used in our experiments.
To test the clustering performance of Coclus against other algorithms, we ran it on 10 real datasets with different sizes and balances 1 coefficient (table 1). In Figure 2, we report the distributions of document sizes 2 defined by a i. = P We observe that, unlike other datasets, SPORTS and RE-VIEWS have very different document sizes. We will see how
The balance coefficient is defined as the ratio of the number of documents in the smallest class to the number of docu-ments in the largest class. Number of terms in a document.
 we can exploit this remark.
 Figure 2: Boxplot of document sizes of each dataset
The competitive algorithms retained for comparison with ours are: the spectral graph partitioning proposed in [1] de-noted as Spec in the sequel and the spectral co-clustering maximizing a generalized bipartite modularity, normalized by the row and column cluster size [4], denoted as SpecCo . We chose these algorithms because they seek a block diago-nal co-clustering of terms and documents, just as ours. For Spec we used the Scikit-learn implementation of Dhillon X  X  algorithm 3 , and we reimplemented the SpecCo algorithm.
To evaluate Coclus , we consider three situations: original data, binarized data and finally after normalized TF-IDF transformation. The used TF-IDF weighting scheme is the of term i in document j , tf ij is the frequency of term i in document j , n is the total number of documents and d j is the number of documents containing term j . The L 2 normaliza-tion was applied on the obtained TF-IDF matrix. When the data sets are binary or original we perform a normalization only when the datasets present high variance in terms of document sizes. In this case, we propose to divide each row by its norm before applying the co-clustering algorithms in order to eliminate the biases induced by the length of a doc-ument. For instance, SPORTS and REVIEWS require this normalization. For the other datasets, this normalization does not increase significantly the performance of compared algorithms.

To validate the obtained results, we used the clustering accuracy and normalized mutual information. Clustering accuracy, denoted Acc , measures the extent to which each cluster contains data points from the corresponding class and is defined as follows: Acc = 1 n max [ P C where C k is the k th cluster in the final results, and L true m th class. T ( C k , L m ) is the number of entities which be-long to class m and are assigned to cluster k . The greater the clustering accuracy, the better the clustering performance. We also used the normalized mutual information (NMI), http://scikit-learn.org/stable/ TF-IDF data.
 which is estimated by: NMI = where N k denotes the number of data contained in the clus-ter C k (1  X  k  X  g ),  X  N ` is the number of data belonging to the class L ` (1  X  `  X  g ), and N k,` denotes the number of data that are in the intersection between cluster C k and class L ` . The larger the NMI , the better the quality of clustering. The results presented in figure 3 and table 2, were obtained by running each algorithm 100 times with random initialization. NMI values reported in figure 3 show that our algorithm is very stable and has a good behavior whatever the nature of the data (binary, contingency or TF-IDF). The results show that Coclus is better than SpecCo which is almost always better than Spec . To confirm this, we perform t-tests comparing the two best algorithms Coclus and SpecCo . In Table 2, we observe that Coclus outperfoms significantly SpecCo in almost all situations.
We have presented a novel diagonal co-clustering algo-rithm which directly maximizes a modularity measure adapted to a bipartite context. In particular, we have shown that this modularity can be maximized using an iterative alternat-ing optimization, which led to a new, effective co-clustering algorithm. Experimental results obtained on several real datasets indeed demonstrated the effectiveness of our algo-rithm. The experiments also showed its robustness and flex-ibility since it can achieve very good results both on binary and contingency document-term tables. Paths for future research might include extending the approach to fuzzy co-clustering and assessing the number of co-clusters. [1] I. Dhillon. Co-clustering documents and words using [2] G. Govaert and M. Nadif. Block clustering with [3] G. Govaert and M. Nadif. Co-Clustering: Models, [4] L. Labiod and M. Nadif. Co-clustering for binary and [5] T. Li. A general model for clustering binary data. In [6] S. C. Madeira and A. L. Oliveira. Biclustering [7] F. Marcotorchino. Seriation problems: An overview. [8] I. V. Mechelen, H. Bock, and P. De Boeck. Two-mode [9] M. E. J. Newman and M. Girvan. Finding and
