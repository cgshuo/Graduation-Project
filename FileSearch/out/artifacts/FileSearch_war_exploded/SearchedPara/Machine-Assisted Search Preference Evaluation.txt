 Information Retrieval systems are traditionally evaluated using the relevance of web pages to individual queries. Other work on IR evaluation has focused on exploring the use of preference judg-ments over two search result lists. Unlike traditional query-docu-ment evaluation, collecting preference judgments over two search result-lists takes the context of documents , and hence tak es the in-teraction between search results, into consideration. Moreover, preference judgments have been shown to produce more accurate results compared to absolute judgment. On the other hand result list preference judgments have very high annotation cost. In this work, we investigate how machine learned models can assist human judges in order to collect reliable result list preference judgments at large scale with lower judgment-cost. We build novel models that can predict user preference automatically. We investigate the effect of different features on the prediction quality. We focus on predict-ing preferences with high confidence and show that these models can be effectively used to assist human judges resulting in signifi-cant reduction in annotation cost.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  selection process, search process.
 Preference judgments , relevance prediction , user preferences , ma-chine assisted evaluation IR systems have been traditionally evaluated using test collections and standard evaluation measures where judges are asked to assign an absolute relevance assessment to search results. More recently, pairwise preference judgments for IR evaluation have been receiv-ing increasing attention. In pairwise preference judgment, we use preference judgments over two search result lists, produced by two IR systems. This enables comparing search results in context [2] . Using this method, it becomes possible to consider the interaction between search results as part of the evaluation criteria. The bene-fits of user preference based evaluation have been demonstrated by Sanderson et al. [30] who also found high correlations between user preferences and diversity measures such as intent-aware precision or  X  -NDCG [13] . However, pairwise preference judgments, while potentially more accurate, can be costly and time consuming. For example, in the user study, that we describe later to collect result preference labels, we found out a list preference judgment takes more than twice the time needed to assign an absolute relevance assessment to individ-ual documents in the same list. Also, the number of different con-figurations of pairs of result lists is significantly larger than the number of documents. This makes list preference judgments much less reusable and increases the amount of annotation work needed. Finally, when comparing incremental versions of the same search system, most of the queries will likely have no difference in quality. This means that valuable annotation time and cost will be wasted judging result lists that do not contribute to the outcome of the com-parison. Note that difference in quality cannot be simply measured by result overlap between the two lists X  as we will show later. Since result lists with zero overlap can still have the same quality. In order to alleviate this problem, we investigate a new machine-assisted framework where we can leverage the power of prediction models to automatically predict preference judgments on search re-sult lists. We use a wide range of features to predict preference judg-ments including relevance-based measures, click-based features, ti-tle and snippet features and diversity based features. We study the effectiveness of every group of features and combine them to build a single system for preference prediction. We also show that we can achieve very good performance even without using any human la-bels (relevance-based measures).
 Note that our objective is not to replace human annotators nor to build a perfect predictor of search preferences. If a perfect predictor of search preference exists, then one could, at least in theory, use it to score all possible results and return the best result. Rather, our main objective is to show that we can use search preference predic-tions to prioritize queries and assist human annotators performing the evaluation in a more efficient manner. For that, we do not need a perfect predictor, rather we need a predictor that can accurately predict the outcome for a portion of the samples with very high confidence. Automatic prediction can significantly reduce the cost and the time needed for collecting preference judgments, without sacrificing any evaluation accuracy. It can be used in hybrid scenarios, with human judges , to compare rankers . We show that automatic predictions can be used to exclude queries where no difference in quality exists be-tween the two rankers and send the rest to human judges. Alterna-tively, we also show that we can rely on high-confidence predicted results from automatic prediction and combine them with human judgment but only for low-confidence predicted instances . More specifically, we make the following contributions with this re-search:  X  Develop methods to automatically distinguish between pairs of  X  Develop methods to accurately predict which search result list  X  Analyze the models to understand which features are most in- X  Use the automatic prediction models to assist human judges and The remainder of this paper is structured as follows. Section 2 de-scribes related work. A formal problem definition is given in Sec-tion 3. Section 4 describes the dataset and how preference judg-ments were collected. In Section 5, we describe the proposed ap-proach. Section 6 reports the experiments and shows how the pro-posed models can be used to assist human judges. There are two areas of work related to the research presented in this paper: (i) Query document relevance and (ii) preference judgments for IR evaluation. We cover each of these areas in turn. State of the art measurement of query result-set relevance for web search uses relevance metrics such as MAP, discounted cumulative gain (DCG) [24] and normalized DCG (NDCG) . These metrics re-quire manual judgments of the relevance of documents in the search result list to individual queries. Previous work has also estimated query document relevance using models derived from user click behavior [3] [11] . Other research work has used the click patterns to compare different ranking functions, i.e. to derive a metric [11] [16] [25] .
 Joachims [25] studied the relation between implicit measures and search engine evaluation. In this work, a technique based entirely on clickthrough data was introduced to evaluate ranking functions. More recently Joachims et al. [26] conducted eye tracking studies to correlate between explicit relevance ratings and implicit measures. Agichtein et al. [3] presented models of search user be-havior to predict web search result preferences. They present ed a model for clickthrough data interpretation that improves relevance prediction accuracy. They also extended their model to include query-level features and browsing features. They showed that in-corporating these features resulted in an improvement in relevance prediction accuracy. Even though query document relevance is a very important signal for evaluating Web search rankers, query document judgments do not consider the interaction between results as part of the judging criteria. Our work is different from this line of work in that our ob-jective is to predict user preference at the result list level, not at the document level. Our work shows that using information about the result list highlights valuable insights into how users compare search result lists. Pairwise preference judgments for IR evaluation have bee n receiv-ing increasing attention from the research community. This work can be grouped into two groups, based on the unit of evaluation (document based vs. list based). We survey both groups in this sec-tion.
 One of the earliest mentions of preference based IR evaluation is in the work of Joachims [25] who hypothesized that a click can be regarded as a preference judgment implying that the user prefers the clicked document over all documents ranked above it. This re-sulted in further work on evaluating IR systems using interleaving where the user is shown a list of documents produced by alternating between the output of two ranking functions while removing dupli-cates. The clicks of users seeing the combined list of results are interpreted as a relative preference comparing the quality of the two ranking functions [28] [29] . Carterette et al. [10] hypothesized that preference judgments of the form  X  X ocument A is more relevant than document B  X  are more accurate compared to absolute judg-ments. They show significant improvement in inter assessor agree-ment when collecting preference instead of absolute judgments. They also proposed methods for evaluating search engines using preference judgments.
 The second line of research is where a list of results is evaluated rather than a single document. Although not preference based, Bai-ley et al. [5] introduced a method for evaluating the relevance of all visible components of a Web search results page, in the context of that results page. This is done in an absolute judgment setting not a preference judgment one though. They show that this method al-lows to investigate aspects of component relevance that are difficult to judge in isolation. Such contextual aspects include component-level information redundancy and cross-component coherence. Thomas and Hawking [31] described a preference evaluation meth-odology where two sets of search results were presented side-by -side and users were asked to choose the one that they preferred. The method was used to compare the first page of Google results (re-sults 1 through 10) to the third page (results 21 through 30). They show a clear preference from judges to prefer top results . Sanderson et al. [30] studied the correlation between preference based judgments and standard test collections measure like NDCG, MRR, Precision@10, etc. They collected user preference judg-ments using crowdsourcing for 30 TREC search topics and 19 TREC runs. They provided evidence that effectiveness measured on a test collection correlates with user preference judgments. This line of work is perhaps the most similar to the work in this paper. Previous work has focused on proposing preference based judgments as a measure for evaluating search systems and on stud-ying the correlation between preference judgments and traditional relevance measures. On the other hand, this work focuses on auto-matically predicting preference judgments using a wide range of signals as will be described later. We start by defining some terms used through-out the paper: Definition: Preference judgment is a judgment collection frame-work where judges are asked to indicate their preference for one of the paired systems.
 Definition: Side-by -Side is a preference methodology where two sets of search results are presented side-by -side to users who are then asked which of the two they prefer.
 Definition: A Result List is an ordered set of N results returned by a search system in response to a particular query.
 Definition: An IR System or Ranking function is a system that takes a query and a document collection (for this work the document col-lection consists of all the documents in the index of a commercial search engine) and returns an ordered set of search results. Assume we have two lists of search-results produced by two differ-ent ranking functions for the same query. In response to a query q , every ranking function returns an ordered list of N results. We con-sider the top 10 results only if possible (some ranking functions may return less than 10 results for some queries). So given a list  X  ={ X  1 ,..., X   X  } , where  X   X  10 and  X   X  is the result returned at position  X  , produced from ranking function  X  1 and a list  X  { X  1 ,..., X   X  } , where  X   X  10 and  X  X  X  is the result returned at posi-tion  X  , produced from ranking function  X  2 , our objective s are: (i) predict whether  X  1 and  X  2 are of similar or different quality, and (ii) if  X  1 and  X  2 are different in terms of quality, then predict which one is better. Experiment is conducted on preference judgments for pairs of search-result lists shown to side-by -side annotators similar to [31] . It requires a query set, multiple search systems (ranking functions) and a method to collect judgments from annotators as follows. Our query set consists of 5000 queries randomly sampled from the logs of a commercial search engine. The queries vary in terms of popularity (60% head queries vs. 40% tail queries), length (59% short (i.e. 4 words or less) vs. 41% long (i.e. more than 4 words) and intent (8% navigational vs. 92% informational). The queries also covered a large span of topics (Travel, Technology, Entertain-ment, etc.). We use the Open Directory Project (ODP, dmoz.org) category labels to assign topical labels to queries using a method similar to [6] . Tail queries were identified by finding queries that appeared once in the sample and did not appear in a 3 months period preceding the period from which the queries were sampled using the logs of a commercial search engine . We also need different ranking functions that can generate different result lists for the same query. For this, we use two pairs of search engines (ranking functions). We split our query-test collecting into two equally sized splits at random. For the first subset of queries, we draw results from two live search engines (Bing and Google) using publically available APIs. For the second subset, we use the ranking results produced by one commercial search engine along with results generated from a 3 months older version of the same engine that was regarded as having slightly lower performance by the ranker developers . This set is judged preferably to construct a dataset with preference judgments collected from different search systems (presumably with larger differences) and preference judg-ments collected from different versions of the same search system (presumably with smaller differences). This will ensure that the ma-chine learned predictors are exposed to a variety of result-set pairs. Later in our experimental section, we study the effect of training the classifiers on one dataset and testing it on another. For each query, we extract the top 10 results from the two search systems. In a few rare cases, where the search system returned less than 10 results, all results were extracted. Along with the URLs of the returned documents, we also ke ep the titles and the snippets. To collect user preference judgments, we use search results from two different rankers as described in Section 4.2 . The two result lists were shown to annotators in a side-by -side setting similar to [31] . For every query, the results of two rankers were shown side-by-side to the annotator. Each result list had the first 10 search re-sults returned by the ranker except for a few cases where the ranker returned less than 10 results. The URLs, titles and the snippets of every search results were shown to the annotator. The ordering in which the result sets were assigned to sides (left or right) was ran-domized . Figure 1 shows an example of a partial snapshot that was shown to the annotators. Annotators were asked to examine the two sides and then submit their preference judgment. Preference judgments were collecte d on a 7 point scale ranging from a strong preference to the left side to the mid-point reflecting no preference to the last point reflecting strong preference to the right side. Annotators had access to both search result pages (SERPs) and all documents. They were in-structed to examine the two SERPs and all the documents before submitting their judgments. In total, the data set contained 5,000 unique queries. Every query was labeled by 3 annotators. We use 150 annotators from a pool of trained judges recruited from a crowdsourcing service, which provided access to crowd workers under contract. Judges resided in the United States and were fluent in English. As is necessary with a study on a remote crowdsourcing platform, we took several precautions to maintain data integrity, in-cluding the use of hidden quality control questions to filter out poor-quality judges.
 The overall agreement with the consensus label using the 7 point scale was 73%. Note that we do not enforce all queries to be anno-tated by the same group of annotators. This violates the assump-tions underlying the Kappa statistics which require the computation of the chance agreement of each annotator. Due to the violation of this assumption, we do not use Kappa to measure inter-annotator agreement in this study. In a separate study , trained annotators were asked to label all query document pairs with standard relevance judgments on a five-point scale with the values (Perfect, Excellent, Good, Fair, Bad). The query document pairs were shown out of context (i.e. the full result list was not shown). In order to predict which SERP is better, we use a wide range of features to describe the two result lists and the difference between them. Different types of queries may have different criterion for user preference. For example, in navigational queries, getting the URL the user desires to navigate to as the first result is most im-portant. On the other hand in informational queries multiple rele-vant results may be appreciated more. Hence the first category of features is query level features that try to characterize the query along multiple dimensions. The relevance of individual results i s likely to play a great role in deciding which side the user prefers. For that we use multiple relevance features to characterize the rel-evance of the results. Since relevance features require human judg-ments, we also use click-based features to act as a cheap way to obtain surrogate for relevance. The perception of the quality and the relevance of the results is affected by the quality and the rele-vance of the result snippets, which we model using the snippet fea-tures . Finally, for particular queries result diversity, and not only relevance, is very important and hence we investigate the im-portance of having SERP diversity features . For all features described in this section, except for query features, we compute the feature values for both sides as well as the delta between them. Different search queries have different characteristics and those characteristics may affect the user perception of preference. For ex-ample, users may value diversity more than other dimensions for a particular group of queries. For another group of queries, relevance may be more important. To capture these variations in preference dimensions, we use a set of features to describe the query of inter-est.
 Query Length : These are features of the query string itself includ-ing the number of characters and the number of tokens in the query.
 Query Type: We categorize queries as either navigational or infor-mational (following the taxonomy of Broder [9] ). The immediate intent of a navigational query is to reach a particular site while the immediate intent of an informational query is to acquire some in-formation. Hence users submitting a navigational query are most likely interested in finding the particular site on top of the search results where users submitting informational queries may be inter-ested in multiple relevant results with useful information. Query Topic: We also categorize queries into different topical cat-egories. Topical categories are important in order to allow the clas-sifier to handle signals differently for different topics. For example, in a news query, freshness is likely to be more important than in queries with different topics. To assign topical categories to queries, we use the Open Directory Project (ODP), also referred to as dmoz.org. ODP uses a hierarchical scheme for organizing URLs into categories and subcategories. To assign ODP categories to que-ries, we use the method in [6] . ODP categories are assigned to que-ries based the ODP categories of URLs that has been clicked or returned for that query. We allow queries to belong to multiple top-ical categories by defining a topic distribution over all classes rather than assigning every query to a single category. This is particularly useful when result documents have multiple aspects.
 Query History: We also include historic features of the query such as the query frequency in the logs of a commercial search engine and the average result clickthrough rate (CTR) for the query. A re-sult clickthrough rate is the number of times it has received a click divided by the number of times it has been submitted. We also cal-culate clickthrough rates for clicks with dwell time larger than 30 seconds (Long CTR). Previous work has shown that dwell time ex-ceeding 30 seconds is highly correlated with satisfaction [18] . These signals can be used as a proxy for both query popularity and query difficulty. Historic feature values were derived from a 6 months  X  worth of logs from a commercial search engine in 2012. These features act as a proxy for query popularity and difficulty which we hypothesis that they may have an impact on the user X  X  perception of preference. Previous research has reported contradicting results on the correla-tion between query document relevance and user preference. Some studies concluded that there is no significant relation between user preference and document relevance [21] . Other studies found that such relation does exist only when measured differences were large [4] . We also find studies that report a clear preference between doc-ument relevance and user preference [30] [31] .
 We hypothesis that the preference toward a certain set of results is related to the relevance of this set which can be characterized by the relevance of individual results. We also hypothesis that not only the aggregate relevance features (e.g. NDCG) are importance but also features characterizing how good is the most relevant result and how bad is the least relevant result in the set. Hence i n this study, we used standard relevance judgments on a five-point scale with the values (Perfect, Excellent, Good, Fair, Bad) for each query-document pair to derive the following features: Relevance@N: The relevance of every URL at positions 1 through 10 as well as the difference in relevance between the two sides. Precision@N: Precision is the fraction of documents in the result list that are relevant to the query. Relevant documents are docu-ments with a relevance rating of Perfect, Excellent, or Good. We computed precisions at positions 1 through 10 for both sides as well as the delta between them.
 NDCG@N: DCG (Discounted Cumulative Gain) [24] is a standard position-weighted mean of the documents relevance . NDCG (Nor-malized DCG) is a normalized form of DCG that is computed by dividing DCG by the ideal DCG (IDCG). The ideal DCG is o b-tained by sorting all documents of a result list by relevance and computing the DCG . We compute NDCG at positions 1 through 10 for both sides as well as the delta between them.
 Best@N: We also compute best relevance label assigned to the top N documents at different positions (i.e. label of the best document in top N results) and use this value for both sides as features. Worst@N: Similarly , we also compute the relevance label of the worst document for every list at different positions and use this value for both sides as features.
 Perfect/Excellent/Bad Ratio: The existence of very good or very bad results on the search result page can affect the user perception of the page quality and hence affect the user X  X  preference judgment. To capture this behavior, we use the percentage of results with Per-fect, Excellent, or Bad labels as features. Previous work has shown that click information can be leveraged to derive a relevance signal [25] . Moreover, the click dwell time (i.e. time spent on the landing page before returning to the search engine) has also been shown to be correlated with satisfaction. More specifically, previous work has suggested that clicks with dwell time exceeding 30 seconds are highly correlated with satis-faction [18] . In this section, we describe how we leverage click data to build features. We obtain click-based relevance judgments using a log-based methodology similar to that used in [20] [32] . This method infers relevance judgments for query-document pairs from search-re sult clicks. We consider three types of clicks in labeling user feedback in the logs: long clicks , short clicks , and no clicks . We define a long click as either a click with dwell time of 30 sec-onds or more, or the last result click in the session. Clicks with dwell time shorter than 30 seconds are considered short clicks. We assign one of the three rating labels to each query-document pair. In each impression, if a document received at least one long click, it is labeled as 2; if a document received only short clicks, it is labeled as 1; if a document was not clicked at all, it is labeled as 0. This gives us a three-level judgment for each query document pair and each impression. To assign a single value for query docu-ment pairs with multiple impressions we compute the median of all impressions. We recompute all the relevance features from Section 5.2 using the three point scale automatically inferred from the click data. A label of 2 denoted a perfect result, while a label of 0 denoted a bad result. Additionally, we also add the clickthrought rate (CTR), number of times a result has been clicked divided by the total num-ber of its impressions, and the long clickthrough rate (long CTR) , considering only clicks with dwell time of 30 seconds or more, for every query-document pair as features. Click data was derived from a 6 months  X  worth of logs from a commercial search engine. Search Engines display several pieces of information for every search result. This includes the URL of the Web page, its title, and a snippet. The snippet is a brief summary that describes the content of the Web page. Typically, every result has a URL, a title and a snippet. The URL, title and snippet could vary considerably across results though. For example, some results may not have any snip-pet. Others have much longer/shorter snippets compared to other results. We also notice that some terms in the URL, title and snippet are bolded while others are not. The number of bolded terms varies across results. The number of terms that match the query terms var-ies considerably too.
 Previous research has shown that snippet and title features can significantly influence the perceived relevance of results and hence the user behavior and clickthrough rate [12] [16] . We hypothesize that features of the titles and snippets as well as features assessing the relation between them and the query are important for assessing the overall quality of a given search result page. The features char-acterizing this relationship are described below.
 Text Length: The length of the titles and the snippets in terms of the number of characters and the number of words are also used as features. Result level and list level (minimum, maximum and aver-age) are used like other features.
 Term Match: We also use features that characterize the relation between the query and the URL, title, and snippet of results. We start by performing standard normalization where we replace all letters with their corresponding lower case representation. We also replace all runs of whitespace characters with a single space and remove any leading or trailing spaces. Text normalization is applied to queries, URLs, titles, and snippets. In addition to the standard normalization, we also break queries that do not respect word boundaries into words. Word breaking is a well-studied topic that has proved to be useful for many natural language processing ap-plications. This becomes a frequent problem with queries when us-ers do not observe the correct word boundaries (for example:  X  southjeseycraigslist  X  for  X  south jersey craiglist  X ) or when users are searching for a part of a URL (for example  X  quincycollege  X  for  X  quincy college  X ). We use a freely available word breaker Web ser-vice available [33] and apply it to queries and URLs.
 Following the text normalization and word breaking steps, we re-move all stop words from text and compute the number of query terms that exist in the URL, title or snippet of each result. Term match is done in two ways: exact match and approximate match. Exact match is when two terms are identical. The objective of ap-proximate match is to capture spelling variants and misspelling. We do that by allowing two terms to match if the Levenshtein edit dis-tance between them is less than 2 Phrase Match : In addition to matching terms, we go beyond the bag of words approach and we match phrases . For example, in the query  X  new york weather  X , snippets with the phrase  X  new york  X  is rewarded while another one with the phrase  X  new mexico  X  should not be rewarded just because it has the term  X  new  X . We start by segmenting each query into phrases. Query segmenta-tion is the process of taking a user X  X  search query and dividing the tokens into individual phrases or semantic units [7] . We segment queries, URLS, titles, and snippets into phrases by computing the point wise mutual information score for each pair of consecutive words. A segment break is introduced whenever the point wise mu-tual information between two consecutive words drops below a cer-tain threshold  X . The threshold we use,  X =0. 895 , is selected to maximize the break accuracy [7] on the Bergsma-Wang-Corpus [9]. Like term matching, we compute the phrase match between the query and the URL, title, and snippet for each result. We also use exact matching and approximate matching as described in the term matching features. Features to describe every result, as well as the minimum, maximum and average over the entire list are used. Highlighted Terms: Highlighted terms in titles and snippets have been shown to draw user X  X  attention to specific results [23] . We use the number of highlighted terms in every result as well as the min-imum, maximum and average number of highlighted terms for the entire result list as features. These values are computed for the URL, the title and the snippet. Terms highlighted by search engines don X  X  necessarily appear in the query. Readability Level: Previous research has investigated the use of readability level to improve general and personalized result ranking [14] . We hypothesize that title and caption readability levels can be used as a signal to predict search result quality. We assess the read-ability of text using a vocabulary based method. We use the Dale-Chall readability measure which computes the fraction of unknown words in test relative to the Dale word list [12] . This feature is com-puted for the titles and snippets of all results. The average, mini-mum and maximum values are used to assess the readability level of the entire result list. Evaluation methods that use a query document pair as a unit of evaluation mostly focus on query document relevance because in the absence of the other document s, the relevance between the query and the document is indeed the most important aspect. On the other hand, when evaluating a result list that consists of multiple documents, other important aspects should be taken into consider-ation like diversity. Diversity has been recognized as in important aspect of both result ranking and search evaluation and had its own task in the TREC Web Track [15] . For example in informational or exploratory searches, results sets that are both relevant and diverse are likely to be preferred over relevant sets with little or no diver-sity. To assess the diversity of a result list, we use the following features: Domain Diversity : We build a distribution of the domain names of all results in the list. To assess diversity, we measure the domain distribution Richness and Entropy. Richness simply quantifies how many different types (i.e. domains) exist in the dataset. For exam-ple, if the 10 results belong to 10 different domains, the richness will be 10, while if two results to the same domain and the other eight belong to different domains, the richness will be 9. The second measure is the Shannon entropy of the domain distri-bution. Entropy was originally proposed to quantify uncertainty or information content. The idea is that the more different types there are, the more equal their probability and the more difficult it is to predict an unseen one. Entropy is often computed as:  X   X   X   X  ln X   X   X   X =1 where  X   X  is the proportion of results belonging to the i th domain and  X  is the number of unique domains in the dataset. ODP Diversity : Similar to the domain Diversity, we compute the Richness and the Entropy with respect to the distributions of the results  X  ODP categories. We perform automatic classification of URLs into ODP categories. URLs in the directory were directly classified. Missing URLs were incrementally pruned one level at a time until a match was found or a miss declared. We use the first three levels of the ODP hierarchy to represent every URL (e.g., Rec-reation:Travel :Transportation ).
 Text Diversity: The domain and ODP diversity features assign the page topicality and uses that to assess diversity. Another angle to consider for diversity estimation is to look at the similarity between the text of the title and snippet of every result. To estimate the text diversity we use both the Jaccard coefficient and the cosine simi-larity. To compute the Jaccard coefficient, we represent every result as a set of terms that occurred in its title and snippet after removing stop words. We compute the Jaccard coefficient between the set of terms in every pair of results. This value is computed for all pairs and then the average is computed. We also compute the cosine sim-ilarity between text representations of results. Using the terms in the title and snippet, we represent every result as an N-dimensional vector where  X  is the number of unique terms across all results. The similarity between every two vectors is estimated by comp u-ting the cosine of the angle between the two vectors. The average similarity over all pairs is then computed and used as a proxy for the result list diversity. One important challenge that we are trying to address is predicting whether there is a quality difference between two lists of results or not. List preference judgments are typically used to compare tw o ranking functions. They are particularly useful when the difference between the two ranking functions is not large. When this is the case, it is likely that the judges will find no quality difference be-tween the results produced by the two ranking functions for a large portion of the queries in the test. For example, in the data described in Section 4, judges indicated that they have no preference toward any side in almost 50% of the queries.
 Queries, where no-preference is reported between the two ranking functions, contribute nothing to the outcome of the comparison even though they consume a significant amount of the annotation time and cost. In this section, we report the results of an experiment where we try to distinguish between queries where no-preference is likely to be reported and queries where a preference exist. This can help significantly reduce the annotation cost as we describe in Section 6.5. Note that we address this prediction task separately since we use that in the query prioritization application that will be described later. Also, recall that prediction by itself is not the ob-jective. Rather it is mainly intended for use as a way to reduce an-notation cost as we will show later. We train a binary classifier (no-preference vs. preference) using the data from Section 4 and all the features reported in Section 5. All experiments used 10 fold cross validation across all queries and the two datasets (i.e. data from the two pairs of ranking functions). We also show the results of training the classifier on one dataset and testing it on another one later. We tried different learning functions (logistic regression, Support Vector Machines, Random Forests and Gradient boosted decision trees). The learning function had little effect on the performance and hence we report herein only results using the Gradient boosted decision trees [19] . Classifiers are built to compare the effectiveness of the proposed system, and different baselines as follows: Rank Correlation : We use a rank correlation coefficient [34] that is a variant of Kendall X  X   X  coefficient. It has a value ranging be-tween 0 and 1 depending on whether the two results lists are iden-tical or completely different. It also assigns higher weight to differ-ences at higher positions. We compute the rank correlation at posi-tions 1 through 10 and use these as features to train a classifier. NDCG : We compute the NDCG at positions 1 through 10 for both result lists as well as the delta between them and use these as fea-tures to train a gradient boosted decision tree classifier. Relevance Only : In this baseline, we use all the relevance features described in Section 5.2 .
 All but Relevance : Here, we use all the features from Section 5 except the relevance features in Section 5.2 . It is important to note that all the features can be computed automatically without any hu-man judgments with the exception of features in Section 5.2. All Features : Finally, we train a classifier on all our features. The results of this experiment are shown in Table 1. The table shows the accuracy, F1-measure and the Area under the Curve (AUC) for all classifiers. The table shows that the Rank Correlation classifier has the worst performance. This is expected because this classifier is assessing the overlap between the two result lists rather than the similarity in terms of quality. For example, it would predict that two lists have different quality only if the number of shared results is limited. Our dataset has many instances where the two result lists are different in terms of results they contain but similar in terms of quality.
 Surprisingly, using NDCG only did not perform well with an accu-racy of 62.2%. This suggests that NDCG is indeed correlated with user preference but there are other factors, different from NDCG, which also affect user preference. When we add the remaining rel-evance features, the performance significantly improves to 74. 45 % accurac y.
 Adding other features like the click-based features, the title and snippet features, and the diversity features (the All Features classi-fier) improves the performance even further taking the accuracy up to 78.15%. This shows the value of other factors, in addition to rel-evance, in preference judgments. The differences between the full classifier and all baselines are statically significant at the 0.01 level using a paired t-test.
 All the features we described in Section 5 can be readily computed from the query, result list, and click logs except for the relevance features described in Section 5.2 which requires query-document judgments. To assess the usefulness of the relevance features, we exclude all relevance features and only used features that can be computed without human intervention ( All but Relevance classi-fier). The accuracy for this classifier was 75.38% (3 points less than the All Features classifier). This shows that while the relevance fea-tures are very useful, we can still achieve good performance with-out them. In the second experiment, we train a classifier to predict which side is better (Left vs. Right). We train a binary classifier on two classes (left better vs. right better). Like the previous experiment, we use the data from Section 4 and all the features reported in Section 5. All experiments used 10 fold cross validation we report the results of the Gradient boosted decision trees learning algorithm [19] which outperformed other learning algorithms like logistic regres-sion, Support Vector Machines, and Random Forests. We use all the classifiers from the previous experiment except for the rank corre-lation baseline which may be suitable for distinguishing between similar and different results lists, but cannot be used for telling which result list is better. The classifiers we use are NDCG, Rele-vance Only, All but Relevance and All Features .
 The accuracy, F1-me asure and the Area under the Curve (AUC) for all classifiers are shown in Table 2. The table shows that NDCG features have the worst performance. Adding the other relevance features improves the accuracy by 2 more points. Adding all other features (query-based, click-based, title and caption and diversity) features further improves the performance leading to higher accu-racy, F1-measure and AUC. The differences between the full clas-sifier and all baselines are statically significant at the 0.01 level us-ing a paired t-test.
 We also tried to exclude all the relevance features and keep only the features that do not require any human interventions. This classifier ( All but Relevance ) achieved an accuracy of 68.42. To assess the importance of different features and feature groups, we train classifiers using each feature group separately and report the accuracy in Table 3. Note that we do consider using the query features only since these are intended to segment the query space with other features but cannot be used solely as they are unaware of the actual results. We notice that relevance-based features per-form best for both classification tasks. It is closely followed by click-based features which confirms our hypothesis that we can use click-based features instead of human-labeled relevance to assess query-document relevance with a small performance loss. The title and snippet features also perform well in identifying cases where the two sides are about the same. They surpass click-based features yet the difference was not statically significant (at the 0.05 level using a paired t-test). This shows that the titles and the snip-pets play an important role in deciding whether the two sides are about the same or not. Diversity features had the least accuracy, but still was very useful predictor in both classifiers. The low accuracy is probably due to the low coverage as diversity is an important attribute for some queries, but has no effect on other queries. We also compute the error reduction for each feature at each node split using the squared loss function. We then aggregate the error reduction at all splits for every feature and use that as a measure of feature gain [19] . Note that other feature selection methods (e.g.  X  2 ) yielded similar results and hence we report the feature gain accord-ing to the error reduction criteria only. We rank the features accord-ing to their feature gain for each classifier. We report the top-10 
Table 1. Results for the preference vs. no -preference clas-
Table 5 . Performance breakdown for different groups of categories of features in Table 4. We report feature types (e.g. Snip-pet Phrase Match), instead of actual features (e.g. Snippet Phrase Match for URL 1). The table shows that the delta features are much more useful than the absolute features, as expected . It also shows that the title and the snippet features are very important for the pref-erence vs. no-preference classifier which agrees with our feature group analysis. We also notice that diversity plays an important role for the left vs. right classifier, where they rank right after the NDCG features. Finally, we notice that the bad/excellent ratio features are ranked highly implying that the existence of a very good or a very bad results is important in the perception of preference for users. High Confidence Prediction : Recall that our objective is to clas-sify a portion of the instances with very high confidence and use these to assist the human evaluation process in order to reduce an-notation time and cost. For this objective, it is very important to study how the proposed method performs with respect to instances predicted with high confidence. In this experiment, we measure the performance of the proposed methods when the system is allowed to abstain from classifying the instances for which it has low confidence. We regard the absolute difference between the classifi-cation probability and 0.5 as a confidence measure and evaluate the top instances with the highest confidence level at different threshold values. Figure 2 shows the accuracy for 10-fold cross validation for the left vs. right classifier at different thresholds. Figure 3 shows the same data for the no-preference vs. preference classifier. Figure 3 shows that the accuracy can reach 85% if we abstain from classi-fying the 50% of the data in which we have low confidence. We see a better trade-off between confidence and accuracy in Figure 2 where we see that the accuracy can improve by 10 points (to 82%) if we abstain from classifying the 50% of the data in which we have low confidence. If we only consider the 20% of the data in which we have the highest confidence, we can have an accuracy of almost 90 % for both classifiers.
 Performance Breakdown : To understand when the system works well and when it fails to produce the correct prediction, we show a breakdown of performance by different query dimensions in Table 5. More specifically, we break queries into the following categories: Navigational vs. Informational, Head vs. Tail and Short vs. Long. We notice that the system performs much better for navigational, head, and short queries compared to informational, tail, and long queries respectively. The latter classes of queries poses more chal-lenges in terms of feature generation because they are typically harder to collect relevance labels for, and have less click infor-mation (especially tail queries). Model Generalizability : Next, we assess how well the proposed method can be transferred from one pair of ranking functions to another pair. In other words, we are interested in answering the fol-lowing question: can we train the model using a particular setup of queries, result rankings, and documents and then transfer it to a dif-ferent setup (different queries, different methods to produce the rankings, different documents)? This question is particularly important because the objective of the proposed model, as we will show later, is to assist human judges by automatically assigning the label to instances for which the model has high confidence. For that to happen, the model must perform well on new sets of queries and new ranking functions. This allows us to use the trained models for new data that has never been seen before without the need to retrain the model on the new data. To answer this question, we rerun our experiment with a different train/test split. As described in Section 4, our data consists of two different datasets that come from different pairs of ranking func-tions and different sets of queries. In this new experiment, we train the model on one dataset and test on the other one. Results in Tables 6 and 7 show that we can achieve very similar performance to the 10-fold cross validation when training and testing on different da-tasets. This shows that the models can be reliably transferred to new unseen sets of queries, documents and ranking functions. It is im-portant to mention that running a cross-validation on this same test-set results in similar performance. This is an important finding since it shows that we do not need to retrain the classifier for different experiments since the classifiers generalize well to new unseen data from new ranking functions.

T able 6 . Results for the preference vs. no -preference clas-
T able 7 . Results for the left vs. right classifier when using 10 -fold cross validation on the two datasets vs. training on Figure 2 . Effect of varying confidence threshold on accu-Figure 3 . Effect of varying confidence threshold on accu-Accuracy Accuracy We now turn our attention to the main objective of this work which is how the proposed models can be used in facilitating the evalua-tion of search systems. The first step toward that goal is to make sure that the models learned from data generated from a set of que-ries, documents and rankers can be reliably used on different sets of queries, documents and rankers. We show that this is indeed the case in the experiments reported in the previous subsection (Tables 6 and 7). Following our experiments where we show that we can predict whether there is a difference in result quality or not (prefer-ence vs. no-preference) and which side is better (left vs. right), we can propose different applications for the proposed method. The overall objective of these applications is to provide a machine-as-sisted evaluation framework where we can leverage the power of the prediction models to reduce the time and cost of annotations without losing any evaluation accuracy.
 Query Prioritization : The objective of our first application is to prioritize queries in terms of which ones should be sent to the judges and which ones should not. As we have seen earlier, there is a large percentage of queries where the output of the two rankers is about the same. These queries do not contribute to the evaluation outcome and it would be best if we can ignore them and focus the attention of the judges on queries where the results of the two rank-ers are likely to be different. We use the classifier described in Sec-tion 6.1 to predict whether two pairs of result lists have similar or different quality. We rank the instances by the prediction confi-dence and gradually increase the amount of data automatically la-beled and observe the effect on the estimate of which ranker is bet-ter. To compare two rankers A and B, we follow previous work, e.g. [27] , and define a WinLoss measure as the number of times ranker A is preferred over ranker B minus the number of times B is pre-ferred over A, divided by the total number of instances.
 Figure 4 shows the WinLoss statistics for different values of no-preference data predicted. A value of 0% denotes the case where no prediction is employed at all, whereas a score of 100% denotes the case where all instances predicted by the classifier as having similar quality are not sent to the judges. Note that almost 50% of the in-stances are predicted as having no difference. As we mentioned ear-lier, instances were sorted by the confidence prediction and hence the figure shows the WinLoss value when we use the  X % of the labels with the highest confidence. We notice from the figures that we can reliably identify and automatically label cases where no dif-ference is likely to be observed and this has very little effect on the evaluation outcome.
 Automatic Labeling : Another application is to not only predict in-stances where no difference in quality exists, but also to predict which side is better. Whenever the classifier makes such a predic-tion with high confidence, we can abstain from sending this in-stance to the judges and use the predicted value instead. To test this setting, we start by sorting all instances by their confidence values and we hold different portions of the data with the highest confi-dence for automatic labeling and we send the rest to the judges. Th e WinLoss statistics for this experiment are reported in Figure 5. In this experiment, we assume that we have the true label of all in-stances where the two result sets are similar to focus on studying the effect of the Left vs. Right classifier by itself. We then repeat the experiment but with the two classifiers employed to fully pre-dict left vs. no-preference vs. right labels. The Winloss statistics for this experiment are shown in Figure 6. Figures 5 and 6 show that the effect of automatic prediction is small when we only use the instances where the system is the most con-fident. As we include more and more instances, the gap between the true WinLoss value and the estimated one increases. To better un-derstand this effect and to ignore the direction of the change (if the errors favor ranker A, the change will be positive, otherwise it will be negative), we plot the percentage change in the evaluation out-come (WinLoss) against the percentage of data automatically la-beled in Figure 7. The figure shows that we can reduce the annota-tion cost by 30% and the overall evaluation outcome will be less Figure 6 . Winloss ratio for different values of predicted data Figure 7 . Percentage change in true winloss ratio for differ-ent values of predicted data ( left vs. no -preference vs. right ) than 5% different from the true outcome. If we automatically label up to 50% of the data, this difference grows to 15%. Search result lists user preference judgment is a powerful method that has been successfully used to compare different ranking func-tions. Even though, preference judgment at the result list level can produce more accurate results, it also requires a higher annotation cost. In this work, we show that we can accurately predict the user preference judgments using a wide range of signals including query-document relevance, click-based features, title and snippet features, and diversity features. We also show that by classifying instances where we have high confidence only, we can achieve very high levels of accuracy. Additionally, we demonstrate that click-based signals can be reliably used as a surrogate for query-docu-ment relevance which allows the method to make predictions using features that do not require any human involvement. The proposed method can be reliably transferred to unseen sets of queries, docu-ments and rankers. This allows us to use the proposed methods to construct a machine assisted evaluation framework where the pre-diction models can be used to assist human judges. We have shown that the proposed method can be used to reduce the time and cost needed to collect preference judgments by automatically identify-ing the winning side or by automatically discarding queries where the performance of the two ranking functions is similar. We have also seen that the classifiers generalize to new unseen data from new rankers hence they can be trained once and used for many new experiments. As future work, we plan to study the feasibility of. formulating the machine-assisted evaluation problem in an active learning setup where result list pairs where the classifier is least certain about are annotated and gradually added to the training data. We believe that this may lead to even further reductions in annota-tion cost.
 [1] Al-Maskari, A., Sanderson, S. and Clough, P. (2007). The re-[2] Al-Maskari, A., Sanderson, M., Clough, P., &amp; Airio, E. (2008). [3] Agichtein, E., Brill, E., and Dumais, S.T. (2006). Improving [4] Allan, J., Carterette, B., &amp; Lewis, J. (2005 ). When will infor-[5] Bailey, P., Craswell, N., White, R.W., Chen, L., Satyana-[6] Bennett, P., Svore, K., and Dumais, S. (2010) Classification-[7] Bergsma, S., and Wang Q. I. (2007). Learning Noun Phrase [8] Borlund, P. The concept of relevance in IR. (2003). JASIST , [9] Broder, A. (2002). A taxonomy of web search. SIGIR Forum [10] Carterette, B., Bennett, P., Chickering, D.M., and Dumais, S. [11] Carterette, B. and Jones, R. (2007). Evaluating search engines [12] Chall, J. and Dale, E . (1995). Readability Revisited: The New [13] Clarke, C., Kolla, M., Cormack, G., Vechtomova, O., Ash-[14] Collins-Thompson, K., Bennett, P., White, R., De la Chica, S. [15] Craswell, N., Soboroff, I. and Clarke, C. (2009). Overview of [16] Demeester, T ., Nguyen, D ., Trieschnigg, D ., Develder, C. and [17] Dupret, G., Murdock, V. and Piwowarski, B. (2007). Web [18] Fox, S., Karnawat, K., Mydland, M., Dumais, S.T., and White, [19] Friedman, J., Hastie, T. and Tibshirani , R. (2000). Additive [20] Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J.-Y. (2009). [21] Hersh, W., Turpin, A., Price, S., Chan, B., Kramer, D., Sa-[22] Huffman, S. and M. Hochster, M. (2007). How well does re-[23] Iofciu, T., Craswell, N., Shokouhi, M. (2009). Evaluating the [24] Jarvelin, K. and Kekalainen, K. (2002). Cumulated gain-based [25] Joachims, T. (2002). Evaluating search engines using click-[26] Joachims, T., Granka, L., Pan, B., Hembrooke, H. and Gay , [27] Kim, J., Kazai, G. and Zitouni, I. (2013). Relevance dimen-[28] Radlinski, F., Kurup, M. and Joachims, T. (2008). How Does [29] Radlinski, F. and Craswell, N. (2010). Comparing the sensi-[30] Sanderson, M., Paramita, M., Clough, P. and Kanoulas, E. [31] Thomas, P. and Hawking, D. (2006). Evaluation by comparing [32] Sontag, D., Collins-Thompson, K., Bennett, P., White, R.W., [33] Wang, K., Thrasher, C., Hsu, B. (2011). Web Scale NLP: A [34] Yilmaz, E., Aslam, J. A. and Robertson, S. (2008). A new 
