 Compounding represents one of the most productive word formation types in many languages. In partic-ular, Germanic languages (e.g., German or Dutch) show high productivity in closed compounding, i.e., in creating one-word compounds such as the Ger-man Armutsbek  X  ampfungsprogramm  X  X overty elimi-nation program X . Previous studies on German cor-pora reveal that almost half of the corpus types are compounds, whereas individual compounds are very infrequent (Baroni et al., 2002). Therefore, an automatic compound analysis is indispensable and represents an essential component in many natural language processing (NLP) tasks such as machine translation (MT) or information retrieval (IR).
Besides determining the concatenated constituent forms, i.e., the correct split points (e.g., Armuts | bek  X  ampfungs | programm ), a compound split-ter needs to normalize each part (e.g., Armut + Bek  X  ampfung + Programm ), because down-stream applications such as MT systems expect lemma-tized words as input. However, normalization of constituent forms is non-trivial and usually re-quires language-specific knowledge (e.g., linking el-ements). State-of-the-art lemmatizers, designed for regular word inflection, would fail, because con-stituent forms often contain linking elements lead-ing to a non-paradigmatic word form of the corre-sponding lexeme (e.g., Armuts  X  X overty + s  X  never occurs as an isolated token in German corpora, since the s -suffix, often used for genitive or pluralization, is not used with Armut ). Moreover, morphologi-cal operations during compounding vary a lot across languages and lexemes: we find cases that start from the lemma and have additions (e.g., linking elements), truncations (e.g., reductions to a verbal stem), word-internal operations (e.g., Umlautung) and combinations thereof (e.g., the first constituent of the German Weihnachts | baum  X  X hristmas tree X , Weihnachten , undergoes both the en -truncation and the s -suffixation).

In this paper, we present a language-independent, unsupervised compound splitter that normalizes constituent forms by tolerantly retrieving candi-date lemmas using an N gram index and weighting string differences with inflectional information de-rived from lemmatized corpora.

Most previous work on compound splitting in-cludes language-specific knowledge such as large lexicons and morphological analyzers (Fritzinger and Fraser, 2010) or hand-crafted lists of link-ing elements and rules for modeling morphologi-cal transitions (Koehn and Knight, 2003; Stymne, 2008; Weller and Heid, 2012), which makes the approaches language-dependent. Macherey et al., (2011) were the first to overcome this limitation by learning morphological compounding operations automatically by retrieving compounds and their constituents from parallel corpora including English as support language.

We would like to take this one step further by avoiding the usage of parallel data, which are known to be sparse and frequently domain-specific, while Bretschneider and Zillner (2015) showed that com-pounding morphology varies between different do-mains. Instead, we exploit lemmatized corpora and use word inflection as an approximation to com-pounding morphology. This way, we are able to pro-cess compounds of any type of domain.

Our contributions are as follows. Firstly, we develop a language-independent and unsupervised compound splitter that does not rely on parallel data. As we will show, our system significantly outper-forms language-dependent, knowledge-rich state-of-the-art methods in predicting the best split point. Secondly, in a controlled experiment, we show that compound splitting based on inflectional morphol-ogy performs similarly to splitting based on an ex-tensive hand-crafted set of rules for compounding morphology. Thirdly, we perform a comprehensive, intrinsic evaluation of compound splitting, which is often missing in previous work that focuses on task-based evaluation (e.g., MT), and thus evaluates per-formance only indirectly. We compare splitting per-formance for several languages for two disciplines: (1) prediction of the correct split points and (2) nor-malization of the constituent forms. To the best of our knowledge, we are the first to evaluate these dis-ciplines separately.

The paper is structured as follows. Section 2 out-lines previous work on compound splitting. Sec-tion 3 discusses some theoretical assumptions on which we base our splitting method. Section 4 shows two efficient and flexible data structures used for our statistical compound splitter, which is de-scribed in Section 5. Section 6 presents some split-ting experiments performed on German, Dutch and Afrikaans. Finally, Section 7 concludes and points to future work. In the following discussion, we focus on splitting approaches that address morphological transforma-tions, as these are most relevant for our work. Previ-ous work on compound splitting can be roughly di-vided into two groups: (1) statistical approaches that are mainly based on large corpora and (2) linguisti-cally based splitters, usually relying on knowledge-rich morphological analyzers or rules.

Statistical approaches generate all possible splits and rank them according to corpus statistics. Al-though independent of lexical resources, most meth-ods contain morphological knowledge in terms of linking elements. The most influential statistical splitter is developed by Koehn and Knight (2003) who addressed German compound splitting by scor-ing splits according to the geometric mean of the potential constituents X  frequencies. For normaliza-tion, they selected the two fillers +  X  s and +  X  es. Stymne (2008) performed several experiments to measure the impact of varying parameters of Koehn and Knight X  X  (2003) algorithm for factored statisti-cal MT. Instead of using two single fillers, she im-plemented the collection of the 20 most frequent morphological transformations for German com-pounding as presented by Langer (1998). She ob-served that splitting parameters should not neces-sarily be the same for translating in different di-rections. Bretschneider and Zillner (2015) com-pared the splitting performance between Koehn and Knight X  X  (2003) two fillers and Langer X  X  (1998) col-lection, illustrating the necessity of an exhaustive set of linking elements. Moreover, they showed that Langer X  X  (1998) data is still not sufficient for domain-specific targets. Macherey et al., (2011) were the first to overcome the need for manual mor-phological input and the limitation to a fixed set of linking elements by learning morphological oper-ations automatically from parallel corpora includ-ing a support language which creates open com-pounds and has only little inflection, such as En-glish. We take this one step further by avoiding the dependence on such parallel corpora, known to be sparse, and by approximating compounding mor-phology with word inflection learned from monolin-gual preprocessed data.

Linguistically based splitters are usually rely-ing on a lexical database or a set of linguistic rules. While these splitters outperform statisti-cal approaches (Escart  X   X n, 2014), they are designed for a specific language and thus less applicable to other languages. Nie X en and Ney (2000) used the morpho-syntactic analyzer GERTWOL (Mariikka Haapalainen and Ari Majorin, 1995) for splitting compounds. Schmid (2004) developed the mor-phological analyzer SMOR , that enumerates linguis-tically motivated compound splits. Fritzinger and Fraser (2010) combined SMOR with Koehn and Knight X  X  (2003) statistical approach and outper-formed both individual methods. Weller and Heid (2012) extended the splitter of Koehn and Knight (2003) with a list of PoS-tagged lemmas and a hand-crafted set of morphological transition rules. While our approach similarly exploits lemma and PoS in-formation, we avoid the manual input of transition rules. The splitting architecture, data structure, features and evaluation we propose in this paper are based on a number of assumptions and considerations that we would like to discuss first. 3.1 Morphological transformation Closed (or concatenative) compounding is the main spelling form in many languages around the world, e.g., Germanic languages such as German , Dutch , Swedish , Afrikaans or Danish , Uralic languages such as Estonian or Finnish , Hellenic languages such as Modern Greek , Slavic languages such as Russian and many more. Most closed-compounding languages use morphological transformations. Ger-man or Dutch often insert a linking element between the constituents while Greek reduces the first con-stituent to its morphological stem and adds a com-pound marker. In contrast to conjugations of ir-regular verbs (such as to be ), there is only a mi-nor string difference (e.g., in terms of edit distance (
ED )) between constituent form and corresponding lemma (usually they differ in at most two charac-ters). This minimal difference makes it possible to interpret constituent normalization as a kind of tol-erant string retrieval (which is presented for the case of spelling correction within IR by Manning et al., (2008)). This is why we are using an N gram index for retrieving the candidate lemmas with the highest string similarity to the constituent form. 3.2 Inflectional morphology Relying exclusively on the highest string similarity for the normalization, would lead to candidate lem-mas that result from linguistically unmotivated op-erations, e.g., the German H  X  uhner  X  X hickens X  would be normalized to the most string-similar lemma H  X  une  X  X iant X  ( ED =2) and not to the correct but less string-similar lemma Huhn  X  X hicken X  ( ED =3). Thus, a linguistic restrictor is indispensable for finding the underlying lemma for a given constituent.

In many languages, inflectional morphology shares operations with compounding morphology, e.g., the German H  X  uhner in H  X  uhner | suppe  X  X hicken soup X  is equivalent to the plural form of Huhn . But even for non-paradigmatic constituent forms (e.g., Armut s  X  X overty X ), we can find cases of inflection that use the transformation at hand (e.g., the geni-tive form of window: Fenster s ).

We thus decided to approximate compounding morphology by using inflectional morphology as de-rived from lemmatized corpus tokens. We realize that the inflectional approximation does not work for all closed-compounding languages but it does for a large subset that is known to have a large variety of linking elements and is therefore most in need of unsupervised morphology induction, the Germanic languages. Moreover, our flexible system can be easily supported with morphological information, which is suitable for languages like Greek, that use a special compound marker. 3.3 Compound headedness Most closed-compounding languages usually follow the r ight h and h ead r ule (RHHR), i.e., the head of a compound is the right-most constituent and encodes the principal semantics and the PoS of the com-pound. As done by previous splitting approaches (Stymne, 2008; Weller and Heid, 2012), we assume the RHHR and allow only splits for which the right-hand side constituents has the same PoS as the com-pound. 3.4 Splitting depth The granularity of the morphological analysis needed differs with the type of application. For MT, a compound should not be split deeper than into parts for which a translation is known, whereas for linguistic research, a deeper morphological analysis is desirable.

For example, while an MT system needs a binary split for the German Fernsehzeitschrift  X  X elevision journal X , for a linguistic analysis, a split into four parts as given in Figure 1 is also valid and introduces etymological clues (e.g., how far is Zeit  X  X ime X  re-lated to Zeitschrift  X  X ournal X ). Our flexible approach 3.5 Constituent length balance While compounds can be build up from almost any semantic concept pair, we observed a bias towards constituent pairs having a similar word length.
For the German, Dutch and Afrikaans compound splitting gold standards (described in Section 6.2) comprising m split compounds, we randomly re-combine all modifiers with all heads to a set of m re-combinations. For both original compounds and re-combined compounds, we measure the character dif-ference in length between modifier and head form. Compound set German Dutch Afrikaans Recombination 3.43 2.74 2.92
As shown in Table 1, all original compounds have a smaller difference than the recombinations. There-fore, we decided to promote compound splits with more balanced constituent lengths. 4.1 Ngram index As described in Section 3.1, we tolerantly retrieve candidate lemmas using an N gram index, in order to limit the search space and allow for a quick can-didate lookup during splitting.

N gram Lemma length (LL) Lemmas  X hund 4 hund#13162  X hund 11 hundef  X  uhrer#251,  X h * hn 4 hahn#2078,
As search key, we use N grams of variable length ( N  X  15 ). Word-initial N grams are indicated by  X  and word-final N grams end on $. By using N grams, we are able to capture any kind of transformation a lemma can undergo when involved in compounding.
Sometimes, a transformation includes a charac-ter replacement within the word (e.g., Umlautung). This leads to a very small set of N grams a con-stituent has in common with its underlying lemma. For example H  X  uhner and Huhn only have the bi-grams  X h and hn in common, which is also true for many irrelevant words such as  X Haarsc hnitt  X  X air-cut X . In order to reduce noise and increase efficiency, we include the wildcard  X  for a single character in N grams 2 . This way, H  X  uhner and Huhn have the common 5gram  X H * hn . As a further cue, we con-sider the lemma length (assuming that there is only a minor difference to the constituent X  X  length).
For a given lemmatized and PoS-tagged corpus, we index all content words (i.e., nouns, adjectives and verbs) by generating all N grams and mapping them to a list of frequency-ranked lemma-freq pairs. Language MOP Corpus frequency Examples
German Afrikaans $/se$ 34K &lt; proses, prosesse &gt;  X  X rocess X  4.2 Morphological operation patterns Macherey et al., (2011) describe a representation of compounding morphology using a single character replacement at either the beginning, the middle or the end of a word. For our experiments, we adopt this format. Since it is possible that a morpholog-ical operation takes place at several positions of a word, we combine all atomic replacements into a pattern describing a series of operations. This trans-formation from a word  X  to a word  X  is referred to as m orphological o peration p attern (MOP). For compiling an MOP, we use the Levenshtein edit dis-tance algorithm including the four operations IN -SERT (adding a character), DELETE (removing a character), REPLACE (exchanging a character  X  i by  X  ) and COPY (retaining a character). In a back-trace step, we determine the first set of operations that lead to a minimum edit distance. Except for COPY , we interpret all operations as replacements (insertion and deletion are replacements of or by an empty element respectively). We merge all adja-cent replacements by concatenating the source and target characters. Word-initial source and target se-quences start with  X  and word-final sequences end on $. Sequences of adjacent COPY operations are repre-sented by  X  :  X  and separate the merged replacements. For example, in H  X  uhner | suppe , the modifier lemma Huhn is transformed to H  X  uhner by replacing u by  X  u (i.e., Umlautung) and adding the suffix er . The corresponding MOP is  X  u/  X  u:$/er$  X . The second column in Table 3 shows some additional German, Dutch and Afrikaans examples of MOPs.

As discussed in Section 3.2, we try to approxi-mate compounding MOPs using inflectional MOPs. In a lemmatized corpus, for each lemmatized word token, we determine the MOP that represents the transformation from lemma to word form. We col-lect all inflectional MOPs with their token-based corpus frequency. The third column in Table 3 shows the corpus frequencies for the corresponding MOPs. Our compound splitter can process compounds composed of any content word type (i.e., nouns, verbs and adjectives) and of any number of con-stituents, and provides both the split points (e.g., H  X  uhner | suppe ) and the normalized constituents (e.g., Huhn + Suppe ). The splitter is designed recur-sively, which allows us to represent the compound split both hierarchical (i.e., as a tree structure) and as a linear sequence. Figure 2 shows the architecture of our splitting algorithm. The recursive main method starts with the target word as a single constituent and recursively splits the constituents produced by the binary splitter (Section 5.1) until an atomic result is returned. The binary splitter has two subtasks: (1) for each potential constituent form, a set of candi-date lemmas is retrieved (Section 5.2) and (2) all candidate lemma combinations are ranked and the best split is returned (Section 5.3). 5.1 Binary splitter We first generate all possible binary splits with a minimum constituent length of 2 (e.g., for  X  Olpreis add a non-split option. For each potential constituent among the generated splits, we retrieve the M most probable lemmas as described in Section 5.2. We consider all M 2 lemma combinations of all possible splits and rank them as described in Section 5.3. The highest-ranked split is returned. 5.2 Candidate lemma retrieval In this step, we retrieve the M most probable can-didate lemmas for a given constituent. For this task, we make use of the N gram index, described in Sec-tion 4.1. Instead of applying MOPs directly which would be the classical and more efficient way, we decided to look up candidates using the N gram in-dex first, thereby following the assumption that there is only a minor string difference between lemma and constituent form (cf. Section 3.1). In a second step, the inflectional MOPs are used to rank the candi-date lemmas. While following this order, we put less weight on our approximation and thereby avoid false lemmas due to irrelevant inflectional MOPs. The pseudocode for the candidate lemma retrieval is given in Algorithm 1.
 Algorithm 1 Candidate lemma retrieval 1: Constituent c 2: LLs  X  lemma lengths,  X   X  around len( c ) 3: CLs  X  [ ] . the candidate lemmas 4: for L  X  len ( c ) to 1 do 5: LGs  X  generate all L grams of c 6: for Lg in LGs do 7: CLs  X  CLs + top  X  ( IDX [Lg][LLs]) 8: if len( CLs ) &gt; 1 then 9: break . otherwise, L is decremented 10: score ( CLs ) . according to a lemma model 11: rank ( CLs ) . according to the scores 12: return top M ( CLs )
For a given constituent c , we search for lemmas with a minimum lemma length (LL) of 2 which ranges between  X   X  around the length of c (lines 1-2). All retrieved candidate lemmas are stored in the list CLs (line 3). Starting with the L = len( c ), we inspect all L grams of c (lines 4-5). For a given L gram, we retrieve the top  X  most frequent lemmas that have a length  X   X  around the length of c (lines 6-7). If there are no lemmas retrieved, we decre-ment L (lines 8-9) 4 . All retrieved candidate lemmas are scored (line 10) according to our lemma model, for which we present two lemma features.

The first feature is based on the lemma promi-nence (LP) as given in (1), i.e., we multiply the cor-pus frequency ( cf ) of a lemma l i (as given in the N gram index) with the token number of l i in CLs (i.e., with the prominence of l i among all inspected L grams).

The second feature estimates the suitability of the MOP (MS) transforming the candidate lemma l to the constituent form at hand, c , (represented as  X  MOP [ l i ,c ]  X ), as given in (2). As the first com-ponent, we use the corpus frequency extracted with the inflectional MOPs as described in Section 4.2. We rescale the MOP frequency with the resulting edit distance between the candidate lemma l i and the constituent form at hand, c , (represented as ED ( l i ,c ) ) 5 . As motivated in Section 3.1, we ex-pect MOPs having a small edit distance to be more prominent in compounding. Such MOPs are not necessarily most frequent in inflection, e.g., the fre-quent irregular Afrikaans verb wees ( to be ) leads to MOPs like MOP[ wees , is ] =  X wee/ X i , which has an ED of 3.

All candidate lemmas are finally scored as prod-uct of lemma prominence and MOP suitability, as given in (3). We rank all candidate lemmas and return the top M candidates (lines 11-12). 5.3 Best split determination In the final step, we determine the best split among all split combinations (i.e., pairs of retrieved can-didate lemmas for modifier ( l m ) and head ( l h ), and corresponding split point) and the non-split option. For this task, we use a combination model, which considers the interaction between l m and l h . In-spired by Koehn and Knight (2003), as a first fea-ture, we take the geometric mean of the products of lemma score multiplied by the length of the cor-responding constituent form, as given in (4). The length factor promotes splits with more balanced lengths (as motivated in Section 3.5), which miti-gates the impact of short and high-frequent words on the overall score. For binary splits, we use the constituent set con = { l m ,l h } and for the non-split option, we use con = { l h } .
The second feature is based on the assumption that the PoS of a compound word  X  usually equals the PoS of its head l h , as discussed in Section 3.3. Since our splitter works out of context, we try to subsume all possible PoS tags by representing them as a distribution over the PoS probabilities monolingual PoS-tagged corpus. The value of the head-PoS-equality (hEQ) feature is defined as the the cosine similarity between the PoS probability distributions of compound word  X  and head l h , hEQ ( X  ,l h ) . If the PoS tag of the compound is un-known, we take 1.0 as default value.
Finally, all candidate lemma combinations (in-cluding the non-split option) are ranked according to the splitting score given in (5). The highest-scored split is returned as output of the binary splitter, being subject to the recursive process. Figure 3 shows an example of the recursive splitter output for the Ger-man compound Studienbescheinigungsablaufdatum  X  X nrollment certification expiration date X  with the re-lated MOPs.
 In our experiments, we focus on German, Dutch and Afrikaans, but expect to see similar performance for other Germanic languages. 6.1 Data We use the German and Dutch version of Wikipedia, we use Treetagger (Schmid, 1995).
Corpus # tokens # types language words words lemmas MOPs German 665M 9.0M 8.8M 1201 Afrikaans 57M 748K 696K 459
We tokenize the Taalkommissie corpus using the approach of Augustinus and Dirix (2013). We PoS-tag the corpus using the tool described in Eiselen and Puttkammer (2014) and use the lemmatizer of Peter Dirix, the second author of the previous paper.
Table 4 shows some statistics of the three prepro-cessed corpora. Since the Afrikaans corpus is one order of magnitude smaller than the German corpus, we expect a lower performance for the Afrikaans splitter. 6.2 Gold standard For evaluating our splitting method on German , we use the binary split compound set developed prises 51,230 binary split samples. For Dutch and Afrikaans , we use the split point gold standards de-veloped by Verhoeven et al., (2014), which comprise 21,941 samples for Dutch and 17,369 for Afrikaans. 6.3 Evaluation measures We evaluate the splitting quality with respect to two disciplines: (1) determination of the correct split points and (2) normalization of the resulting mod-accuracy measure as described in Koehn and Knight (2003). The split point accuracy ( SPAcc ) refers to the correctness of the split points (on word level) and the normalization accuracy ( NormAcc ) mea-sures the amount of both correct split points and modifier lemmas. All systems presented in this pa-per provide a ranked list of splits. This allows for a more fine-grained ranking evaluation of the binary splitting decisions with respect to the first n posi-tions. Accuracy@ n refers to the amount of correct splits among the top n splits. We stop at n = 3 , be-cause we do not expect to see a crucial difference in the performance gap for higher values of n . 6.4 Parameter setting and models in There are three parameters presented in the candi-date lemma retrieval. For efficiency reasons, we set the number of lemmas retrieved per L gram and lemma length (  X  ) to 20 and the final number of re-trieved candidate lemmas ( M ) to 3. For the maxi-mum difference in length between lemma and con-stituent form, we observed that  X  = 2 covers all compounding operations for Germanic languages.
For German, we compare our system based on in-flectional MOPs (LP.MS (LP.  X  ), which lacks a linguistic restrictor after can-didate lemma lookup from the N gram index, against an upper bound (LP.MS and frequencies derived from the normalizations in the gold standard and against a version that uses a hand-crafted set of MOPs and frequencies derived from Langer X  X  (1998) set of fillers (LP.MS addition, we compare our system against previous work: the splitting methods of Fritzinger and Fraser and Afrikaans, we compare with the SPAcc numbers of Verhoeven et al., (2014). 6.5 Results and discussion Table 5 shows the German results for the binary compound splitting. We present the split point accu-racy (SPAcc) and the normalization accuracy (Nor-mAcc) for the splits ranked @1-3. We first compare LP.MS both systems with respect to SPAcc and reaches 99.4% for SPAcc@3. While for NormAcc@1 our splitter X  X  performance is less than 2 percentage point lower than the system of FF2010, which heavily re-lies on language-dependent and knowledge-rich re-sources, we significantly outperform both systems in comparison for NormAcc@2 and NormAcc@3.
 This proves that one can attain state-of-the-art per-formance on compound splitting by using language-independent and unsupervised methods, and in par-ticular by means of inflectional information.
In an error analysis, it turned out that FF2010 (i.e., SMOR ) cannot process 2% of the gold samples. However on a common processable test set, we still find our system to outperform FF2010 significantly, which indicates that the difference in performance is not just a matter of coverage. WH2012 leaves several compounds unsplit, for which our splitter provides the correct analysis. This is partly due to the hand-crafted transition rules of WH2012, which cannot capture all morphological operations, such as in Hilfs | bereitschaft  X  X ooperativeness X , for which the MOP e$/s$ (i.e., the combination of e -truncation and s -suffixation) is not even covered by Langer X  X  (1998) published collection.

To evaluate whether our assumption about the us-ability of inflectional MOPs holds, we run some con-trolled experiments with two variants of our system, shown in the last two lines of Table 5. The exhaus-tive set of inflectional MOPs (LP.MS petitive performance in SPAcc with the hand-crafted set of Langer (1998) (LP.MS bound (LP.MS by only 1 percentage point in NormAcc.

Our separate evaluation of SPAcc and NormAcc reveals a lower performance for the normalization across all systems, as this is a much harder disci-pline. In addition, we can conclude that normal-ization requires more linguistic knowledge: while the LP-baseline (LP.  X  ) underperforms heavily, both FF2010 and LP.MS Langer , systems with a lot of lex-ical and morphological information, outperform all systems in comparison at NormAcc@1.

For illustrating the multilingual applicability of our splitter, we perform an experiment on Dutch and N -ary splits. While Verhoeven et al., (2014) use a supervised approach for predicting the correct split points, our unsupervised splitter outperforms their Dutch results significantly 14 . Although Dutch and Afrikaans are similar languages, the SPAcc achieved by our system for Afrikaans is 3.6% worse than the method presented by Verhoeven et al., (2014). This result is partly due to the crucial corpus size differ-ences presented in Table 4.
 We presented a language-independent, unsupervised compound splitter based on inflectional morphology that significantly outperforms state-of-the-art meth-ods in finding the correct split points, relying only on monolingual PoS-tagged and lemmatized corpora. We provided a comprehensive, intrinsic evaluation of several systems in comparison for several lan-guages on two separate disciplines: split point deter-mination and constituent normalization. As a result, we draw the conclusions that inflectional morphol-ogy is a practical approximation for compounding in Germanic languages and overcomes the neces-sity of manual input, because both hand-crafted sets of compounding operations and operations derived from the gold standard lead to small differences in performance only. In future work, we plan to adapt our methods for learning compounding morphology for languages such as Greek, that have a special compound marker.
 We thank the anonymous reviewers for their help-ful feedback. We also thank our colleague Stefan M  X  uller for the discussions and feedback, and Marion Di Marco and Fabienne Cap for providing their split-ting methods. This research was funded by the Ger-man Research Foundation (Collaborative Research Centre 732, Project D11).
