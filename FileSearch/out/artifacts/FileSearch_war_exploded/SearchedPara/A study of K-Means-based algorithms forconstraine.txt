
University of S X o Paulo, S X o Carlos, Brazil University of Texas, Austin, TX, USA 1. Introduction
Constrained clustering arises from the need for incorporating known information about the desired data partitions into the process of clustering data [5] . There are several types of constraints, e.g., about pairs of objects [24], clusters [3], and partitions [12]. Among these, the most usual ones are the con-straints about pairs of objects  X  specifically the Must-Link (ML) and Cannot-Link (CL) constraints [6, 8,15,18,20,24,25]. Considering a set X = { x i } N i =1 of N objects, each one represented by a vector x ter, whereas a CL constraint c = ( i, j ) indicates that x i and x j should lie in different clusters. From a set of ML constraints, new constraints can be deduced by using the transitivity property. Although CL constraints do not present this property, the combination of ML and CL constraints can yield to new
Some algorithms for clustering with constraints [20,24,25] do not allow any violation of the constraints all the constraints. While this may be interesting in some circumstances, it is important to have in mind that, in many practical applications, the constraints are usually provided by users who are unaware of the spatial disposition of the data. Therefore, the need to satisfy all the constraints can make the clustering process intractable, and an empty partition is often returned by such an algorithm. To illustrate this, consider Fig. 1, which depicts the petal and sepal areas for the well-known Iris dataset. The three classes are represented by different markers, and the centroid of each class is represented by a dot. The markers enclosed by rectangles correspond to some objects whose nearest centroids are not those of their respective classes. Thus, assuming that every class corresponds to a different cluster, a constrained clustering algorithm based on k -means will not be able to satisfy some pair-wise constraints derived from the classes. To overcome this limitation, more flexible algorithms have been developed [6,8,18, 28]. These algorithms seek to minimize the number of violated constraints. This way a partition that agrees as much as possible with the user X  X  constraints can be found. For this reason, such constraints are sometimes called soft constraints .

Despite the increasing number of studies on clustering with constraints, there is a lack of studies providing empirical comparisons among algorithms. Moreover, frequently the adopted experimental methodology differs from one paper to another, e.g., due to the use of different performance measures and number of constraints, which makes it virtually impossible the (indirect) comparison between algo-rithms described in different papers. Also, the identification of classes of problems for which a particular algorithm could be preferred is very difficult.

This paper presents an extensive comparative analysis of three well-known k -means-based algorithms for constrained clustering, namely: the Constrained Vector Quantization Error (CVQE) algorithm [8], its variant called LCVQE [18], and the MPCK-Means algorithm [6]. We performed computational com-plexity analyses not reported in the original references. From the experimental point of view, three criteria were used in our study  X  Adjusted Rand Index, Normalized Mutual Information, and number of violated constraints. Experiments were performed on 20 datasets, and for each of them 800 sets of constraints were generated. The statistical significance of the obtained results was also addressed. It is worth noting that the authors of CVQE, LCVQE, and MPCK-Means employed a relatively limited number of datasets (five on average) and/or sets of constraints (typically three). The accuracies of the obtained partitions were only assessed in [6,18]. We also analyzed the robustness of the algorithms with respect to noisy constraints. This issue was briefly analyzed in [18] by considering the case when noisy constraints are rare. Also, the effect of noisy constraints on probabilistic models was studied in [17]. In our experiments, we assess the robustness of each algorithm by considering different proportions of noisy constraints. To sum up, we provide a richer evaluation on all these aspects than previous stud-ies. Finally, the compared algorithms are based on the well-known k -means algorithm [21,26], which is widely used in practice. Thus, by performing experiments with k -means based algorithms, interest-ing conclusions can be drawn for a large audience  X  e.g., k -means users and data mining practitioners potentially interested in this kind of algorithm.

The remainder of this paper is organized as follows. In the next section the algorithms under study are briefly described. Section 3 addresses the methodology adopted to perform comparisons among algorithms and presents our experimental results. Finally, Section 4 concludes our work.
 Notation . A hard partition of the data is a collection P = { C i } k i =1 of k disjoint clusters such that
C i = X and C i  X  C j =  X  ,  X  i = j ,and | C i | =0 ,  X  i ,where | C i | denotes the number of objects in cluster C i . Each cluster C i is represented by a prototype  X  i . The distance between an object x i and a prototype  X  j is calculated by using the squared Euclidean distance, i.e., x i  X   X  j 2 =( x i  X   X  M of ML constraints and a set C of CL constraints. Using o M ( l ) and o that return the first and second objects of the l th ML constraint, it is possible to define the functions g
M ( l ) and g M ( l ) that return, respectively, the indices of the clusters that the first and second objects constraints being violated is defined as V M = { i |M i  X  X  ,g M ( i ) = g M ( i ) } and, similarly, the set indicator function, that is equal to one when the condition is satisfied and 0 otherwise. 2. Algorithms 2.1. CVQE
The Constrained Vector Quantization Error (CVQE) algorithm [8] employs the objective function of k -means augmented by two terms that consider the costs of violating the constraints. The costs of violating ML and CL constraints are computed from distances between prototypes. For a ML constraint, the cost is the distance between the prototypes of the clusters that contain the objects that should be in the same cluster. For a CL constraint, the cost is the distance between the prototype of the cluster in which the objects rely on and the nearest neighbor prototype (second-closest cluster). More formally, the objective function is defined as: is the usual vector quantization error. The second and third terms are the sum of the costs imposed by the violated ML and CL constraints, respectively.

The CVQE algorithm assigns objects to clusters as follows: (i) objects that are not involved in any constraint are assigned to the closest cluster; (ii) pairs of objects involved in ML and CL constraints are assigned to clusters that minimize the objective function  X  Eq. (1). To do so, all possible assignment combinations are verified. The prototypes (  X  j ,j =1 ...k ) are updated according to: This update procedure can be interpreted as follows [8]: if a ML constraint is violated, the prototype of the cluster that has the first object of the constraint is moved towards the prototype of the cluster containing the second object of the constraint (second numerator term of Eq. (3)). In case of violation of a CL constraint, the prototype of the cluster containing the two objects in question is moved towards the nearest neighbor prototype of the second object of the constraint (third numerator term of Eq. (3)). Algorithm 1 summarizes the CVQE X  X  steps.

As discussed in [18], CVQE has some drawbacks. First, the algorithm is sensitive to the order of the objects in each constraint. This can be readily seen from the prototype update rule for the case of a ML violation, because only the prototype of the cluster in which the first object relies on is affected. Second, checking all possible assignment combinations leads to O ( k 2 ) calculations, thus being compu-tationally demanding for applications in which the number of clusters is high. Third, in case of violation of constraints, only the distances between prototypes are considered in the penalization, i.e., the object positions in relation to these prototypes are ignored. Aimed at circumventing these limitations, a variant of this algorithm, called Linear CVQE, has been presented in [18]. 2.2. LCVQE
The LCVQE algorithm [18,19] uses a modified version of the objective function in Eq. (1). The cost of violating a ML constraint is now based on the distances between objects and prototypes. These distances are computed by considering the object of one cluster and the prototype of the other cluster. For a CL constraint, the object that is the farthest to the prototype is determined. Then, the distance between this Algorithm 1 Constrained Vector Quantization Error [8]. object and its nearest neighbor prototype (second-closest cluster) is used as the violation cost. More precisely, the objective function J LCV QE is defined as: where the first term is the vector quantization error, the second and third terms are the costs related to the violated ML constraints, and the last term is the cost imposed by the violated CL constraints.
The auxiliary functions, R j ( l ) and V ( l ) , are defined in Eqs (6) and (7), respectively. Intuitively, the l th CL constraint farthest from the prototype of its cluster.

The assignment of objects to clusters also differs from CVQE. First, every object is assigned to the closest cluster. For each ML constraint being violated, only three assignment possibilities are examined: (i) maintain the violation; (ii) assign the two objects to the cluster whose prototype is the nearest to second object ( o M ( l ) ). For each CL constraint being violated, only two cases are checked: (i) maintain updated as: The update rule can be interpreted as follows. Let l be the index of a ML constraint that is being violated, and the prototype  X  n is moved towards the object o M ( l ) (second and third numerator terms of Eq. (8)). moved towards o C ( l ) (fourth numerator term of Eq. (8)). The main steps of the LCVQE algorithm are described in Algorithm 2. 2.3. MPCK-Means
The Metric Pairwise Constrained K-Means (MPCK-Means) algorithm [6] seeks to learn a distance metric that best fits the constraints. Specifically, a positive semi-definite matrix A j is learned for each cluster C j by parameterizing the Euclidean distance as follows: x i  X   X  j A ( x i  X   X  j ) T A j ( x i  X   X  j ) . There are other algorithms capable of learning distance metrics by using constraints [2,4,16,27]. These algorithms, however, do not perform clustering. Instead, a single metric is learned for all clusters, thus being limited to clusters with similar shapes [6]. The advantage of this type of method is the ability to model non-spherical clusters for which standard Euclidean distance is biased. Specifically, this method is able to identify clusters of elliptical shapes. The MPCK-Means X  objective function is: where  X  log(det( A j )) arises due to the normalizing constant of a more generalized k -means model, 1 w l and  X  w l are user-defined weights to penalize the violation of the l th ML constraint and the l th CL Algorithm 2 Linear Constrained Vector Quantization Error [18,19]. constraint, respectively. As in [6], we set w l =  X  w l =1 . The norm x maximum distance between two objects  X  considering the distance metric of cluster C g C ( l ) . The auxiliary functions f ML and f CL compute the penalties for violating the l th ML and CL constraints, respectively. The former, f ML , penalizes the violation of a ML constraint proportionally to the distance between objects. As a violation of this type involves two clusters, the computation of the distance between the two objects in question considers two distance metrics, one from each cluster. The latter, f CL , penalizes the violation of a CL constraint by making it inversely proportional to the distance between the two objects in question.

The MPCK-Means algorithm uses a heuristic method for initializing the prototypes by considering the provided constraints. Initially, it deduces all the possible ML and CL constraints from the original set of constraints. To simplify the notation, the set M will here refer to the set of ML constraints provided and deduced. Similarly, the set C will represent CL constraints. Then, the connected components of a graph, obtained by considering that the objects are vertices and that each ML constraint is an edge, are found. Such connected components form the set of neighborhoods,  X  .If |  X  | k , |  X  | prototypes are initialized as the centroids of th e objects that for m each neighborhood,  X  i ,andthe k  X  X   X  | remaining prototypes are initialized as the overall data mean, added with random noise. If |  X  | &gt;k , k neighbors are chosen by using a weighted variant of the farthest-first algorithm [13], whose weights correspond to the number of objects in each neighborhood. Thus, this initialization procedure is biased towards distant neighborhoods representing a large number of objects [6].

In order to assign objects to clusters, MPCK-Means uses a strategy that is sensitive to the data presen-tation order. In brief, objects are assigned to the clusters that minimize the cost in Eq. (9). Consider that object x i has been assigned to a given cluster. This choice is not revised in the respective iteration, i.e., it cannot be changed, and it is only considered when checking for possible violations involving other objects, as well as for assessing the impact of such assignments on the objective function. To alleviate this problem, the objects are processed in a random order at each iteration.

The prototypes are updated according to the k -means rule, and each matrix A j is updated according to Eq. (13), for which additional care must be taken [6]. In particular, initially it is necessary to check if the sum of the covariance matrices on the right hand side of Eq. (12) is singular. If so, a fraction of the inversion is not positive semi-definite, it is necessary to project it into the set of positive semi-definite matrices to ensure that it can parameterize a distance metric [6]. This is accomplished by the procedure described in [27], which consists of decomposing the matrix A j = X T  X X ,where X is the matrix of eigenvectors of A j and  X  is the diagonal matrix with the eigenvalues of A j . After the decomposition, the eigenvalues in  X  that are smaller than zero are replaced by 0, and then A j is reconstructed. Finally, the main steps of MPCK-Means are summarized in Algorithm 3.
 2.4. Computational complexity
We studied the computational costs (per iteration) of CVQE, LCVQE, and MPCK-Means. Here we provide a summary of our analyses. Recall that k,N,M are the numbers of clusters, objects, and at-tributes, respectively. Also, M and C are the sets of ML and CL constraints, respectively. The computa-linear complexity with the number of objects and attributes. However, differently from k -means, CVQE has quadratic complexity with k . The computational complexity of LCVQE is O ( k  X  M  X  ( N + |M| + |C| )) , thus being more computationally efficient than CVQE with regard to k . Finally, MPCK-Means X  com-plexity is O ( k  X  M  X  ( |M| + |C| )+ M 2  X  ( |M| + |C| )+ k  X  M 3 + k  X  N  X  M 2 + N 2  X  M 2 ) ,wherethe cubic complexity with the number of attributes comes from the computation of determinants and eigen-decomposition of each matrix A j , and the quadratic complexity with N is due to the updates of the pair of farthest objects for each metric.
 Algorithm 3 Metric Pairwise Constrained K-Means [6]. 3. Empirical evaluation 3.1. Methodology
In order to compare the algorithms CVQE, LCVQE and MPCK-Means, 2 experiments were performed on two sets of datasets  X  as reported in Table 1. The first set is composed of 10 datasets commonly used as benchmarks in the literature. Most of them are available at the UCI Repository [1]. In addition, we have used the 9Gauss dataset [7], which is formed by nine balanced clusters arranged according to Gaussian distributions that have some degree of overlapping, as well as the Protein dataset [6]. Following [6], for the Letters dataset only classes I, J, and L were considered and, for Pendigits, only classes 3, 8, and 9 were considered. According to [6] these classes represent difficult classification problems. The second set, denoted by Bioinformatics for simplicity, is composed of 10 datasets that are benchmarks for the Cancer Gene Expression Data domain [9]. We selected only datasets containing more than 100 objects, so that a considerable number of constraints could be generated. For the experiments with gene expression datasets, objects have been normalized by using z-scores [29]. The main characteristics of the employed datasets are given in Table 1.
 The quality of the obtained partitions is assessed by means of two external validity criteria, namely: Adjusted Rand Index (ARI) [14] and Normalized Mutual Information (NMI) [22]. Both criteria are well-known and commonly used in machine learning studies [23]. All algorithms were executed using the a priori known number of clusters. For the UCI datasets, it is assumed that classes correspond to clusters.
Sets of constraints with eight different sizes were generated so that |M| + |C|  X  R = { 25 , 50 , randomly chosen from the range [1 ,N ] . The labels of the objects x i and x j are verified and, if they are the same, a ML constraint is added, otherwise, a CL constraint is added. If the constraint has already been added (possibly with the reversed order of objects), it is discarded. For each number of constraints, T = 100 repetitions were performed, thus totaling 800 sets of constraints for each dataset.
All the studied algorithms are based on k -means, which is known to be sensitive to different initial-izations. Therefore, each algorithm was run five times. For both CVQE and LCVQE, the initializations differ by the objects randomly selected to be the initial prototypes, whereas for MPCK-Means the initial-izations differ on the point coordinates obtained through the addition of random noise. The best partition obtained from different initializations is determined by considering the value of the objective function of each algorithm, thus simulating a practical application of the algorithms being compared. We have used the standard k -means algorithm as a baseline for assessing the impact of incorporating constraints into the clustering process.
 Original and deduced constraints were considered when assessing the performance of CVQE and LCVQE. If only the provided constraints are considered, then their respective results are referenced by the algorithm names. If the original set of provided constraints is augmented by deduced constraints, then the correspondent results are referenced by CVQE-A and LCVQE-A. By comparing these different approaches, it is possible to assess to what extent the deduced constraints can improve the quality of the obtained partitions. MPCK-Means uses the deduced constraints as part of its initialization. As a consequence, both provided and deduced constraints are used when running this algorithm. We also investigate if learning a particular distance metric per cluster provides better results than learning a single metric for all clusters. The algorithm that learns multiple metrics is denoted by MPCK-M. Analogously, MPCK-S denotes the algorithm that learns a single metric for all clusters.

In order to provide some reassurance about the validity and non-randomness of the obtained results, we present the results of statistical tests by following the approach proposed by Dem X ar [10]. In brief, this approach is aimed at comparing multiple algorithms on multiple datasets, and it is based on the use of the well-known Friedman test with a corresponding post-hoc test. The Friedman test is a non-parametric counterpart of the well-known ANOVA. If the null hypothesis, which states that the algo-rithms under study have similar performances, is rejected, then we proceed with the Nemenyi post-hoc test for pair-wise comparisons between algorithms. Statistical tests were performed individually for ev-ery investigated criterion, namely: ARI, NMI, and number of violated constraints (VIO). The tests were performed separately for each number of constraints in R = { 25 , 50 , 75 , 100 , 125 , 150 , 175 , 200 } by averaging the values obtained over T = 100 repetitions.
 3.2. Results on UCI Datasets
The results obtained for the ARI and NMI criteria are summarized by means of the percentage of wins/ties/lo sses for each pair of algorit hms in Tables 2 and 3, respectivel y (considering every analyzed case 3 ). For example, the value  X 45.6/10.5/43.9 X  in the 2 nd row and 3 rd column of Table 2 means that CVQE presented ARI values greater than those obtained by LCVQE in 45.6% cases, equal values in 10.5% cases, and smaller values in 43.9% cases. From these tables, one can see that the use of de-duced constraints by CVQE and LCVQE resulted in a similar number of gains and losses compared to using only the provided, original constraints. For example, considering the ARI, CVQE-A showed better results than CVQE in 39.4% cases and worse results in 37.3% cases. This and other similar results show that increasing the number of constraints (especially if these are deduced from known constraints) does not necessarily yield to better data partitions. Another interesting observation is that MPCK-S per-formed equal or better than CVQE-A and LCVQE-A in more than 55% of cases, while MPCK-M has not presented better results than LCVQE-A. Comparing CVQE to LCVQE, it can be noted that CVQE obtained better results in a slightly larger number of cases. By considering the two versions of MPCK-Means, learning a single metric for all clusters (MPCK-S) showed equal or better results than learning a particular metric per cluster (MPCK-M) in 66% of the cases. As expected, all algorithms provided better results than k -means.

Table 4 summarizes the percentage of wins/ties/losses for the number of violated constraints (VIO) in the partitions found by each algorithm. The number of constraints violated by MPCK-Means is less or equal than the number of constraints violated by CVQE-A and LCVQE-A in more than 96% of cases, thus suggesting that MPCK-Means can indeed learn a metric suitable for satisfying the constraints, but this property is not enough for generalization purposes with respect to the objects not directly affected by the constraints. It can also be observed from Table 4 that LCVQE violated a number of constraints smaller or equal to those violated by CVQE in 90% of cases, suggesting that the LCVQE X  X  procedure for updating prototypes is better than the one adopted in CVQE. As expected, all algorithms performed bet-ter than k -means for the VIO criterion. Table 5 summarizes the average numbers of violated constraints for each algorithm (standard deviations between parentheses). The averages were computed from both the provided sets of constraints (randomly generated according to the procedure previously described) and deduced sets of constraints. One can see that MPCK-Means has presented the best results. More-over, as the number of constraints increases, the use of a single metric for all clusters presented better results than the use of a particular metric per cluster. Also, LCVQE provided better results than CVQE for both ML and CL constraints. These trends are illustrated in Fig. 2.

Table 6 summarizes the results of significance tests. Only the pairs of algorithms for which statisti-cally significant differences (  X  = 5%) were found are listed. The first column indicates the investigated criteria (ARI, NMI, and VIO), and the second column lists the pairs of algorithms  X  where the first listed algorithm obtained better results than the second one. The third column presents the respective sizes of the constraint sets. For instance, MPCK-M obtained better results than CVQE-A with respect to  X  X IO X  tistically significant differences were not observed. For VIO, CVQE has not shown significantly better results than k -means for any number of constraints. It is interesting to note that MPCK-Means obtained significantly better results than CVQE-A in most of the scenarios. However, the same does not hold with respect to LCVQE, which is more computationally efficient than MPCK-Means. Thus, by taking into account all the assessed criteria, LCVQE has shown the best trade-off solutions. 3.3. Results on bioinformatics datasets
As in the previous section, we assessed the studied algorithms according to three criteria  X  ARI, NMI, and VIO. Tables 7 X 9 present the relative percentages of wins/ties/losses for each pair of algorithms. For both ARI and NMI, LCVQE obtained equal or better results than CVQE in more than 74% of the cases. LCVQE also violated less constraints than CVQE in 55% of the cases. In this sense, note from Table 10 that the performance differences are particularly favorable to LCVQE for CL constraints. The use of a particular metric per cluster (MPCK-M) has provided worse results than the use of a single metric for all clusters in about 65% of the cases (with respect to ARI and NMI). However, the use of multiple metrics allowed violating less or the same quantity of constraints in 82% of the cases. In essence, these results are in accordance with those reported for UCI datasets, thus suggesting that, for the employed Bioinformatics data, violating less constraints do not necessarily lead to more accurate clusterings. Surprisingly, even the standard k -means resulted in better partitions than MPCK-Means in more than 63% of the cases (considering ARI and NMI). Another interesting observation is that the use of a specific metric per cluster (MPCK-M) has resulted in particularly worse results when CL constraints are taken into account  X  see Table 10, which is also visualized in Fig. 3 to illustrate the behavior of each algorithm with respect to the number of violated constraints.

Table 11 presents a summary of the outcomes of the performed statistical tests. For compactness, we only present the pairs of algorithms for which significant differences were observed for both accuracy measures (ARI and NMI). One can see that LCVQE and LCVQE-A presented significantly better results than MPCK-Means, especially when the number of provided constraints is small. Taking the number of violated constraints (VIO) into account, only CVQE has not presented significantly better results than k -means, and LCVQE violated less constraints than CVQE-A in most of the cases.

Although CVQE and LCVQE have, in general, shown better results than MPCK-Means in our experi-ments, it is important to have in mind that, for particular datasets, learning distance metrics can indeed be advantageous. In order to illustrate this aspect, consider Fig. 4, which shows the average values for ARI obtained by each algorithm, for different sets of constraints, in the Singh-2002 dataset. It can be seen that MPCK-Means scores well above CVQE and LCVQE, especially when more than 75 constraints were provided. This result indicates that the Euclidean distance is not a good metric for this dataset. As a consequence, CVQE and LCVQE cannot satisfy the constraints while improving the data partitions. This fact becomes even clearer when VIO is analyzed, e.g., for 200 constraints the average number of violations by CVQE, LCVQE, and MPCK-Means is 86, 57, and 0, respectively. Another interesting ob-servation is that the results of MPCK-S were equal or better than those obtained by MPCK-M for any number of constraints, again suggesting that the additional computational cost of learning a distance metric for each cluster is unnecessary for these datasets. 3.4. Noisy constraints
We also assessed the performance of the algorithms in scenarios where some of the constraints are noisy. A related study was performed in [18], where only very small fractions of noisy constraints were considered for a few datasets. Our experimental setting is more representative: we performed experi-ments on twenty datasets (Table 1) for different fractions of noisy constraints. More specifically, we considered scenarios where 200 constraints were provided. From these constraints, different proportions {5%, 10%, 15%, 20%, 25%, 30%} of noisy constrai nts were generated by randomly selecting a con-straint and changing its type, i.e., turning a ML constraint into a CL one, and vice-versa. This procedure is inspired in the noisy edge noise model [11]. We run experiments by taking into account the same (100) trials described in Section 3.1, i.e., for each trial the 200 initial (not noisy) constraints are exactly as described in the previous experiments.

For the sake of illustration, we show the NMI values obtained for the Iris dataset  X  Fig. 5. The trends depicted in this graph can be considered typical for our experiments with other datasets. One can ob-serve from this figure that CVQE shows the best behavior. This is expected from the trend observed in our experiments reported in Sections 3.2 and 3.3, which do not involve noisy constraints, namely: it tends to violate more constraints than LCVQE. Also, algorithms using the augmented set of constraints (CVQE-A, LCVQE-A, and MPCK-Means) presented a more sensible decrease in performance. Since the augmented set may contain constraints that were derived from noisy ones, this is also expected. An interesting observation is that CVQE-A has shown more robustness to noisy constraints than LCVQE. Finally, Fig. 5 also shows that MPCK-Means is less robust with respect to noisy constraints. This can be explained by the fact that it seeks to adapt a distance metric, which in this case can be seen as a projection [6] into a wrong direction, thus resulting in low NMI values.

For a more general perspective, there are two important aspects to be evaluated in our experimental setting. The first one is to identify algorithms that are more robust to noise. We evaluate this aspect by means of the relative difference between the NMI 4 values obtained with and without noisy constraints. More precisely, we use the measure in Eq. (13), which captures the average difference, among the T trials performed, in terms of NMI values obtained by a particular algorithm ( i ), for a given amount of noise ( n ), in dataset d .WeuseNMI noisy and NMI to denote NMI values obtained when comparing the reference partition with the partition obtained with and without noisy constraints, respectively. The average rankings of the NMI differences are summarized in Table 12, where, for example, the value of  X 2.00 X  in the 4 th row and 4 th column indicates that, after sorting the differences obtained by each algorithm (with 15% of the constraints being noisy), CVQE-A is the second best algorithm (on average). Note that the NMI values are always computed with respect to a reference partition, given by the classes. Thus, it is expected that the greater the impact of noisy constraints the higher the NMI difference. From these results, one can observe that CVQE is the most robust.

The second aspect to be analyzed refers to the algorithm X  X  capability of correctly violating noisy constraints while respecting the remaining ones. To that end, two fractions are added up, namely: (i) the fraction of constraints that were correctly violated (i.e., are indeed noisy) and (ii) the fraction of constraints that were correctly satisfied (i.e., those that are not noisy). The resulting measure is analogous to the well-known accuracy. More precisely, consider that algorithm i was run in dataset d , which has a particular proportion of noisy constraints ( n ). The accuracy is then measured by means of Eq. (15), where F , V ,and N denote the full set of constraints ( F = M X  X  ), full set of violated constraints ( the t th trial and c  X  denotes a generic constraint that can be either ML or CL.
 Table 13 presents the average rankings obtained from the accuracies. One can observe that, by consid-ering 5 X 15% of noisy constraints, MPCK-S is the best algorithm (on average), whereas CVQE is the best one for 20 X 30% of noisy constraints. The accuracies obtained by MPCK-S are explained from the observation that the constraint sets are imbalanced. Recall from Sections 3.2 and 3.3 that MPCK-Means usually violates a very small (close to 0) number of constraints. Thus, for small proportions of noisy constraints, it is advant ageous (from the accuracy viewpoint) to satis fy most of them (no matter whether they are noisy or not). When such an imbalance is lessened (i.e., the proportion of noisy constraints is increased), its accuracies decrease. As expected, the accuracy of LCVQE also deteriorates with the increasing number of noisy constraints, but for a small amount of noise (5%), it is still competitive with CVQE.

In summary, LCVQE seems to be the most appropriate algorithm for scenarios where the expected amount of noise in the constraints is low. For noisier sets of constraints, the more computationally demanding CVQE tends to provide better results. 4. Conclusions
In this study, three well-known algorithms for k -means-based clustering with constraints  X  Con-strained Vector Quantization Error (CVQE) [8], its variant named LCVQE [18], and the Metric Pair-wise Constrained K-Means (MPCK-Means) [6]  X  were systematically compared. These algorithms are designed to minimize the number of violated constraints in the process of clustering. In addition, MPCK-Means also seeks to learn a distance metric that maximizes the agreement with the constraints. Besides evaluating the algorithms with respect to the number of violated constraints, two criteria that capture the accuracy of the obtained data partitions have been adopted  X  Adjusted Rand Index (ARI) and Normal-ized Mutual Information (NMI). Experiments were performed on 20 datasets of different characteristics. For each dataset, 800 sets of constraints were generated. In order to provide some reassurance about the non-randomness of the obtained results, outcomes of statistical tests of significance were presented. Computational complexity analyses not available in the original references have also been presented. Thus, we provided a far richer evaluation on all these aspects compared to previous studies.
In order to analyze the obtained results, we grouped the datasets into two subsets. The first subset is formed by UCI datasets, whereas the second set has Bioinformatics data (more precisely, benchmarks for Cancer Gene Expression Data). For both UCI and Bioinformatics datasets, LCVQE has shown to be competitive with CVQE in terms of accuracy, while violating less constraints and being more com-putationally efficient. From this viewpoint, our work confirms the claims made from a more limited set of experiments in [18]. Also, in most of the cases both CVQE and LCVQE presented better accuracy than MPCK-Means, which is capable of learning distance metrics. In this sense, it was also observed that learning a particular distance metric for each cluster does not necessarily lead to better results than learning a single metric for all clusters. These results suggest that the Euclidean distance is appropriate to compute dissimilarities between objects for most of the investigated datasets  X  we have normalized the bioinformatics datasets by means of z-scores, thus making the Euclidean distance consistent with the Pearson correlation, which is widely used for such datasets. Although CVQE and LCVQE have, in general, shown better results than MPCK-Means in our experiments, it is important to keep in mind that, for particular datasets, learning distance metrics can indeed be advantageous. As in [6], our more exten-sive study corroborates the conclusion that MPCK-Means presents better results when more constraints are available. We shall also observe that essentially the same conclusions can be drawn if one considers the UCI and Bioinformatics datasets altogether, namely: in most of the cases LCVQE provides the best clusterings and MPCK-Means violates less constraints. Overall, by taking into account all the assessed criteria, LCVQE has shown the best trade-off solutions.
 Some useful observations can also be made from our experiments with noisy constraints. For instance, CVQE has shown to be robust even in very noisy scenarios (e.g., 30% of noisy constraints). For more well-behaved scenarios (e.g., 5% of noisy constraints), its more computationally efficient counterpart  X  LCVQE  X  has presented a reasonable tradeoff between efficiency and accuracy.

Finally, a variety of (more specific) new experimental findings emerged from our study. For instance, our results suggest that deduced constraints usually do not help finding better data partitions. In addition, we have observed that ML constraints are harder to be satisfied, especially by the algorithms that have found better clusterings. We shall stress that our study was focused on clustering problems, for which the underlying assumption is that both ML and CL constraints should help finding better data partitions. However, from a classification point of view, ML constraints may be misleading. For instance, if a particular class is formed by more than one cluster, then satisfying ML constraints may imply in having less accurate classifiers. Therefore, from the different viewpoint of building classifiers from constrained clustering algorithms (e.g., in semi-supervised learning settings), CL constraints tend to be more useful. Also, for these (and related) application scenarios, one should not assume that the number of clusters is equal to the number of classes. Instead, the number of clusters should be optimized from data, in such a way that ML constraints could also be used to induce better models. From these observations, we believe that there is still room for investigating how algorithms for constrained clustering can help semi-supervised learning.
 Acknowledgments
This work has been supported b y NSF Grants (IIS-0713142 and IIS-1016614) and by the Brazil-ian Research Agencies FAPESP (2009/17795-0 and 2009/17856-0) and CNPq (200372/2011-4 and 202141/2010-1).
 References
