 1. Introduction
Digital images are almost parts of our daily lives and the truth of images becomes a serious ethical issue as they can be altered everywhere at any moment using advanced digitization and digital processing techniques. Human has puzzled that which images are believable. Here, we call those images created using digital processing techniques digital fake images. Digital fake images can be categorized into two types. One is based on computer graphics (CGs), with images created from end to end using computer graphic software like Maya and 3D Studio Max. From the website ( http://www.alias.com/eng/etc/fakeorfoto/quiz.html ), we can find some tests about natural images and CGs, which give us a strong feeling that CGs are hard to be differentiated from natural images.
The other is forged images, which are created by splicing different objects of images into a new image, which looks natural. Digital forgery is also called digital splicing and it is generally referred to when we talk about fake images. In Brugioni (1999) , a tutorial of fake images is introduced. A general procedure about image fakery is described in Fiete . Image fakery was developed early last century. Although early fake images are created using darkroom techniques, it is obvious that almost all fake images are created using computer softwares now. Generally, the content of fake images is inconsistent with the real worlds, usually leaving no visual clue of artificial manipulations. We are no longer able to guarantee the trueness of digital images, which has been consid-ered as a valid recording of events. As a serious ethical issue, it is exigent to differentiate natural images and fake images.
Currently, fake images look more and more realistic with the advance of image fakery techniques. What one can do is to find the inconsistency in an image which indicates its fakeness. Generally, any inconsistency can be the evidence that an image has been altered, with respect to illumination conditions, edge sharpness, resolution, tone, relative scale, noise characteristics, etc. In order to make the fake images more realistic or the detection difficult, the simplest method perhaps is to process the image in such a way that its quality is reduced. The most common methods are blurring the edges, adding random noise, reducing the size of the image, or compressing the image, all of which will cover up telltale signs of manipulation. Many fake images are of such poor quality that even the most accurate measurements/detections cannot determine if inconsistencies exist. In the past few years, some works have been done to recognize natural images and fake images.

In Ng and Chang (2004) , a model for image splicing or photo-montage is proposed. The spliced images are a kind of fake images. Aiming at detecting the spliced images, a model based on bipolar signal perturbation is introduced . On the other hand, a blind detection of photomontage is developed using higher order statistics in Ng et al. (2004) . Both works detect photomontage using a statistical model of bicoherence features, which were designed to detect human speech signals originally. As is well-known that the statistical distribution of audio signals is very different from that of digital images. The bicoherence-based higher order statistical model is targeted to reflect the intrinsic characteristics of i mages, but according to reported detection results there still have rooms for improvement. On the other hand, Farid et al. have introduced some techniques for exposing digital forgeries ( Popescu and Farid, 2004 ), including techniques employing the inconsistences in digital camera imaging techniques ( Popescu and Farid, 2005 ), digital image sampling techniques ( Popescu and Farid, 2005 ), the direction of point light ( Johnson and
Farid, 2005 ), principal component analysis ( Farid, 2004 ) and higher order wavelet statistics ( Farid and Lyu, 2003 ). Lyu and Farid (2005) , proposed a classification scheme between photorealistic images and photographic images using SVMs based on statistical characteristics in DWT domain, where photorealistic images refer the images created by computer graphic software completely. The DWT-based features are demonstrated to be discriminative between photographic and photorealistic images.

Hsu and Chang (2006) , proposed a method, which first indicated the suspicious spliced area in an image to estimate the camera response function, and the cross-fitting errors is used for classifica-tion. Wang et al. introduced camera characteristic inconsistency for image forgery detection ( Fang et al., 2009 ). Since image forgery leaves traces in image quality, the measurement of image quality was also applied for image forgery detection by Zhang et al. (2008) .
On the other hand, both image forgery and information hiding cause inconsistency of images, steganalysis techniques, which are used to detect hidden messages, are also widely used in image forgery detection by Zhang et al. (2009) , Bayram et al. (2002) , Shi et al. (2007) , Fu et al. (2006) and Chen et al. (2007) , respectively.
Higher order statistics have been used for exposing forged images ( Ng et al., 2004; Farid and Lyu, 2003 ) and computer graphics ( Lyu and Farid, 2005 ), respectively, which are proved to be effective. So, the well-constructed statistical features can reveal the intrinsic characteristics of natural images and fake images. In this paper, we present a detection scheme to reveal digital fakery based on multiresolution decomposition and higher order local autocorrelations. Then, we employ support vector machines (SVMs) to classify natural images and fake images.
The rest of this paper is organized as follows. In Section 2, some preliminaries about higher order autocorrelations are intro-duced. In Section 3, multiresolution decomposition and higher order autocorrelations are employed to construct the features for revealing digital fakery. In Section 4, SVMs are used to classify natural images and fake images based on the extracted features.
Then, in Section 5, simulations and discussions are included to demonstrate the effectiveness of the proposed detection scheme.
Finally, conclusions are given in Section 6. 2. Preliminaries
Autocorrelation (AC) is a powerful shift-invariant signal statis-tics, which has been extended to higher order cases in Mclaughlin and Raviv (1968) .
 Definition 1. Let f : O D R d -R be a real valued function.
The n -th order autocorrelation function with n displacements  X f t 1 , t 2 , ... , t n g is defined as follows:
R
It is obvious that R ( n ) f is shift invariant, i.e., the higher order autocorrelations (HACs) of f ( x ) are the same as those of f ( x + t ), where t is a shift parameter. Suppose the function F is the discretized function of f , the discretized version of Eq. (1) can be given as which are valuable in developing our detection scheme.
Lemma 1. The second and higher even order autocorrelation of two functions f 1 and f 2 are equal if and only if f 1 ( x )  X  f f 2 is a shifted version of f 1 . In other words, f 1 can be recovered from the HAC R ( n ) f with a shift parameter t .

Lemma 2. Let f 1 and f 2 be two real valued functions defined over the same domain , and their higher order autocorrelation functions are column vectors R  X  n  X  f the two column vectors is given by
Here, F 1 and F 2 are the discretized functions of f 1 and f respectively.

Higher order autocorrelations, as effective feature extraction methods, have been applied in many pattern recognition and computer vision domain, such as face recognition, etc ( Goudail et al., 1996, 1993; Kreutz et al., 1996; Popovici and Thiran, 2001, 2002, 2004 ). An important limitation in applications for higher order autocorrelations is the computation complexity. Each com-tion, suppose the element t i in t has m i distinct values, it has
Q i  X  1 m i features for the n -th order autocorrelations. Thus, the computation of features will be very expensive, e.g., there are about 10 16 features for an image with size 64 64, if n  X  5. In order to decrease the computation complexity, some restrictions are proposed in Kurita et al. (1992) , where the order n is restricted up to 3, i.e., n  X  1,2,3, and the range of the displacements  X f 1 , t 2 , t 3 g is restricted within a local 3 3 window. Thus, higher order autocorrelations become higher order local auto-correlations (HLACs). Then based on the shift-invariance property, the number of patterns of displacements can be reduced to 25, and these patterns form the feature column vector. Fig. 1 depicts an illustration of the selected patterns, where there are 1, 4 and 20 features for the order 1, 2 and 3, respectively, in the case of the restrictions described above. 3. Image statistical features
Typically, the creators of digital fake images ignore the known physical properties of camera images. The most significant cam-era effects are edge sharpness, influenced by lens diffraction, focus, and motion blur; perspective geometry and noise proper-ties, mainly due to the detector and compression. Computer graphics is usually created without any camera effects, and digital forged images are also created without the camera effects appeared at the edges between the forged area and the natural area, since camera effects degrade the image quality and make the images less appealing to the audience. However, this results to have images that are physically impossible to be captured with a camera in the real world. This motivates us to reveal the intrinsic characters of natural images and fake images from two aspects. 3.1. Multiresolution decomposition
In order to reveal digital fakery, we make use of multiresolu-tion decomposition, which, as a powerful image analysis tool, decomposes images using basis functions. Multiresolution image decomposition has been used in many application domains, e.g., image compression, image coding, image restoration, texture analysis and data analysis. Through multiresolution decomposi-tion, the signal characteristics in detail can be localized in different position, orientation and scale. We think that the detail coefficients in multiresolution decomposition contain many intrinsic characteristics of natural images and fakes images, such as edge, texture, noising, filtering, etc. This is also demonstrated in Lyu and Farid (2005) .

Two dimensional discrete wavelet transformation (2D-DWT) is a powerful multiresolution analysis tool in many applications, which decomposes an image into three components. Given an image I , DWT splits the image into three scales and orientations, i.e., the horizontal subbands H i ( x , y ), the vertical subbands V and the diagonal subbands D i ( x , y ), where i  X  1,2, y decomposition scale, which describes the basis feature distributions. Fig. 2 shows the three levels of 2D DWTs using symlets wavelet with kernel filter length 10 for an ordinary real image, a fake image and a CG generated image. 3.2. Higher order autocorrelations
From an image analysis point of view, the most intrinsic characteristics for revealing the differences between natural images and fake images are perhaps local one, such as edge, noise, etc. Hence, we consider that well-constructed local higher order statistics can depict the unique features of natural images and fake images, since they can describe the basic feature correlations. Without loss of generality, consider the horizontal band H i ( x ), we employ the discretized higher order autocorrela-tion statistics to describe the statistical relation of the neighbor-hood characteristics, which is defined by R where s  X  1,2, y , l denotes the decomposition level. At each level, through autocorrelations, a feature column vector R  X  n  X  obtained. Thus, we can obtain l feature column vectors in the horizontal subbands. The process is repeated for the subbands V ( x ) and D i ( x ) as follows: R R Correspondingly, these features form the feature column vectors R In the cases of color images, the whole process can be applied in the luminance components. Based on Lemma 1, each coefficient subband corresponds to a unique higher order local autocorrela-tion statistics, and vice versa. Here, an assumption is made that there are no shifts for the coefficient subbands. Thus, we propose the use of HLACs to depict the intrinsic statistical features of digital images. Fig. 3 illustrates the proposed feature extraction process for our fake image detection scheme. 4. Detection
In our scheme, natural images and fake images are classified using the SVMs with Gaussian RBF kernel such that
K  X  X i , X j  X  X  exp  X  g J X i X j J 2  X  , where g is a kernel parameter, X
X denote two input feature column vectors. First of all, we need to specify the feature column vector for the applied SVMs. Here, we apply the same restrictions as that in Kurita et al. (1992) and the vector dimension is 25. Thus, we can obtain 3 l 25-dimensional feature column vectors. Then, we specify the final feature column vector using the average of the 3 l feature vectors as follow: which is a 25-dimensional feature vector. In the training stage, the output of SVM is set to 1 for natural images, and 1 for fake images. Thus, the computational complexity is decreased rationally.

In practice, the computational complexity of SVMs comes from the computation of the kernel functions. Considering the Gaus-sian RBF kernel, it is an inner product-based kernel such that
K  X  R i , R j  X  X  exp  X  g J R i R j J 2  X  X  exp  X  g / R i R
We can see that it can be obtained by only computing the inner products of two feature column vectors which can be obtained directly using Eq. (4) without computing the feature vector. Thus, the huge computation of HLAC statistics is saved. So, we can recombine the feature column vectors into a new longer feature column vector as follows:
R
R
R where  X  j  X  denotes the concatenation of two vectors, the super-script T denotes the transpose of a matrix, and R ( n ) is the recombined feature vector. The new feature column vector con-tains more statistical information of digital images. The inner product of two new feature column vectors can be obtained as the following: where R i ( n ) and R j ( n ) denote the feature vectors of two images obtained from Eq. (13). Considering a comparison for the pro-posed classification scheme, we construct the feature vector R using two different ways. One is the method described above, i.e., the order n is restricted up to 3, the range of the displacements t  X f t 1 , t 2 , t 3 g are restricted within a local 3 3 window, and the length of the feature column vector is 25 3 l  X  75 l . The other is to construct the feature vector through restricting the order n up to 3 and restrict the range of the displacements t  X f t 1 within a local 5 5 window. 5. Simulations and discussions natural images, 800 fake images and 800 computer graphics images, respectively, are used to test the proposed scheme. examples of the natural images, forged images and computer graphics images are shown in Figs. 4 X 6 , respectively. The popular SVM software LIBSVM ( Chang and Lin, ) was adopted in our simulations to integrate the inner product by Lemma 2. Half of the images in each database are used to train the SVM and the remaining halves are used for testing. The decomposition para-meter l is set to 3. In Section 4, we have presented three methods to construct the feature column vector for SVMs. In order to compare the classification ability of different feature combina-tions and construct a better feature vector, we use SVM-1 to denote the applied SVM using the feature vector constructed using Eq. (8), SVM-2 to denote the applied SVM using the feature vector constructed using Eq. (13) whose displacement t is defined within a 3 3 window, and SVM-3 to denote the applied SVM using the feature vector constructed using Eq. (13) whose displacement t is defined within a 5 5 window. For these three methods, the order n is restricted up to 3.

Table 1 shows the classification accuracies for the three applied SVMs, where SVM-1 correctly classified 43.5% natural images with a 2.31% false negative rate, where the false negative rate is the percentage of fake images that are incorrectly deter-mined as natural images, SVM-2 correctly classified 57.4% natural images with a 1.94% false negative rate, and SVM-3 correctly classified 62.5% natural images with a 1.56% false negative rate. It is obvious that the performance of SVM-3 is the best while that of SVM-1 is the worst among the three classifiers. Furthermore, through reconstructing the feature vector using Eq. (13) instead of Eq. (8), the performance of classifiers are improved significantly. Note that the testing accuracies of the three classifiers are fairly close to the training accuracies. This indicates that the classifiers are general and therefore only SVM-1 and SVM-2 are used in later experiments.

In Fig. 7 , the ROC curves between the false positive rate and the true positive rate for the three classifiers are shown. Again, the false positive rate is the percentage of natural images that are True positive rate incorrectly detected as fake images, and the true positive rate is the percentage of fake images that are correctly classified as fake images. It can be concluded that the performances of SVM-3 are better than the other two classifiers, and the performances of
SVM-1 are the worst. Note that the ROC curve of SVM-2 is fairly close to that of SVM-3, indicating that the performance improve-ment is limited and insignificant through widening the window of the displacement t .

There are two steps for the general classification. The first is feature computation, and the second is training and testing using the features. While in the proposed scheme, the two steps are joint together. We used SVM-1 to compare the computation time for the proposed scheme and the general classification. We find that the proposed scheme is much faster and saved more 60% time than the general classification.

To further validate our experimental results, we randomly labeled the test images with 1 and 1, where half of the images in each database are randomly assigned to fake images (output  X  1) and the others are assigned to natural images (output  X  1). Then, the classifiers were trained and tested. Table 2 records the classification results, where SVM-1 correctly classified 18.85% natural images with a 12.79% false negative rate, and SVM-2 correctly classified 20.43% natural images with a 9.19% false negative rate, which were much worse than the cases when the correct outputs are assigned as shown in Table 1 .Notethat the testing accuracies of the classifiers were much lower than the training ones. The results indicates that the SVM classifiers are based on rational statistical features for natural and fake images.
Through analyzing the natural images that are misjudged, we find that some of these images have many irregular textures or smooth area, some of which are also hard to identify for human eyes. Some others are abnormal in tone, coherence, etc., for example, some overexposed camera images are also misclassified.
We also tested whether the classifiers can differentiate forged images and computer graphics. Half of the forged image database and the computer graphic database were used to train the SVM and the remaining images were used for testing, where the output of the classifiers were set to 1 for the forged images and 1 for the computer graphics ones. Table 3 shows the classification accuracies for the classifiers, where SVM-1 correctly classified 40.38% natural images with a 2.12% false negative rate, and SVM-2 correctly classified 55.63% natural images with a 2% false negative rate. It is obvious that the performance of SVM-2 is better than that of SVM-1. Note that the testing accuracies of the two classifiers are fairly close to the training accuracies. Fig. 8 shows the ROC curves between the false positive rate and the true positive rate for the three classifiers. It can be concluded that the performances of SVM-2 are better than those of SVM-1. Thus, we consider that the proposed classification schemes can also classify forged images and computer graphics images effectively. 2009; Fu et al., 2006; Chen et al., 2007 ) are chosen to compare with the proposed scheme. All the methods used the same
Columbia Image Splicing Detection Evaluation Dataset. And the experiment results are shown in Table 4 . We can see that accuracy of all the methods are over 80%, the classification accuracy in our scheme reaches 88.32% for SVM-2. In term of the scheme improvements, there are over four types of features in Zhang et al. (2009) , which is a combination of other existing image forgery detection schemes. While in the proposed scheme, we developed a new type of features for fake image detection. In term of feature dimension, there are only 75 features for SVM-1, which is the shortest than the others, and the accuracy still reaches 85.65%, and the performance is encouraging. 6. Conclusions based on multiresolution decomposition and higher order local autocorrelation statistics which can describe the intrinsic char-acteristics between natural images, forged images and computer graphics images. We make use of SVMs to classify the images, which shows that different statistical feature construction contain different image statistical information. By right of the inner product lemma of higher order autocorrelation, the feature extrac-tion and SVM are joint together, and the computation complexity is decreased significantly. Note that with the limitation of digital processing techniques, the intrinsic statistical features of digital images still cannot be removed or altered, such as camera effect, motion blur etc., though natural images and fake images are not perceptually indistinguishable, they can still be differentiated. Acknowledgments
This work was supported by National Natural Science Founda-tion of China (NSFC no. 60803136), Guangzhou Science and
Technology Program (no. 2009J1-C541-2). Credits for the use of the Columbia Image Splicing Detection Evaluation Dataset are given to the DVMM Laboratory of Columbia University, CalPhotos
Digital Library and the photographers listed in http://www.ee. columbia.edu/ln/dvmm/downloads/AuthSplicedDataSet/photogra phers.htm . The authors thank the editor and the reviewer for their help and suggestions, which improved this paper.
 References
