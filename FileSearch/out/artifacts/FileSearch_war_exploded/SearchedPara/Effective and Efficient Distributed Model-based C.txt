
In many companies data is distributed among several sites, i.e. each site generates its own data and manages its own data repository. Analyzing and mining these dis-tributed sources requires distributed data mining techniques to find global patterns representing the complete informa-tion. The transmission of the entire local data set is of-ten unacceptable because of performance considerations, privacy and security aspects, and bandwidth constraints. Traditional data mining algorithms, demanding access to complete data, are not appropriate for distributed applica-tions. Thus, there is a need for distributed data mining algo-rithms in order to analyze and discover new knowledge in distributed environments. One of the most important data mining tasks is clustering which aims at detecting groups of similar data objects. In this paper, we propose a dis-tributed model-based clustering algorithm that uses EM for detecting local models in terms of mixtures of Gaussian dis-tributions. We propose an efficient and effective algorithm for deriving and merging these local Gaussian distributions to generate a meaningful global model. In a broad experi-mental evaluation we show that our framework is scalable in a highly distributed environment.
Many companies gather terabytes of data containing sig-nificant, hidden information that needs to be uncovered us-ing data mining techniques. Nowadays, more and more companies typically are decentrally organized and spatially distributed. Each site generates its own data and manages its own data repository. The challenge, however, is to access current knowledge from anywhere at any time with a delay of no more than a few minutes. Analyzing and mining these distributed sources requires distributed data mining tech-niques because the wanted patterns should be mined on a global view of these distributed sources. However, the tradi-tional approach for a data mining environment is a central-ized warehouse-based architecture where the relevant data is regularly uploaded into the warehouse for centralized data mining. Obviously, this centralized approach causes high response times and may be limited by network and security constraints. It does usually not exploit the full capacity of all distributed resources since the data mining step resides on the central sever while the computational power of the clients is idle. In addition, in terms of privacy issues, it may be inappropriate to send the entire data over a public network to a global server. Thus, the traditional, central-ized data mining model is not suitable for most distributed environments. Thus, a scalable and privacy preserving so-lution for distributed data mining applications requires the distributed processing of data [10]. A good distributed data mining framework performs data mining operations based on the type and the availability of the distributed resources. As suggested in [10], a distributed data mining solution con-sists of the following steps. First, a data mining algorithm is locally applied to each of the k sites separately and in-dependently. The results are k local sets of patterns called local models . Second, the local models are transferred to a central server. The central server combines the local mod-els to generate a global model . Third, the global model may optionally be sent back to local sites.

The data mining technique we focus in this paper is clus-tering which aims at partitioning the data objects into dis-tinct groups (clusters) while maximizing the intra-cluster similarity and minimizing the inter-cluster similarity. Many clustering algorithms for the centralized approach have been proposed so far using different clustering notions, e.g. distribution-(or model-)based, center-based, or density-based (cf. [7] for an overview). In general, all those meth-ods are applicable for a distributed solution as far as they produce a local model in Step 1 of the distributed data min-ing process that is as compact as possible but provides as much information as needed fo r building a global model in Step 2. Unfortunately, many traditional clustering algo-rithms produce a clustering that cannot be easily described by a simple prototype. For example, density-based clus-tering [3] detects clusters of arbitrary shape. However, de-scribing a cluster having a complex shape might become quite expensive possibly causing large transfer rates. Thus, a local model should describe each cluster by a  X  X uitable X  prototype. Obviously, such prototyping should also meet privacy constraints. We argue, that the EM clustering algo-rithm provides exactly such prototypes. EM describes the data set by a set of Gaussian distributions consisting of the cluster center (mean) and covariance matrix. The latter de-scribes the density of points around the center of the cluster. If certain constraints are met, privacy is preserved because the exact values of the data objects cannot be retrieved from the distribution.

We propose a novel distributed clustering algorithm called DMBC (Distributed Model-Based Clustering). The local models are acquired using EM clustering. Since the necessary number of clusters on each site might be strongly varying, DMBC automatically determines a suitable num-ber of local clusters based on three privacy and performance constraints. The constraints control the maximum transfer volume that is allowed from an individual site and assure that each local data object is described as good as possible and prohibit the transfer of clusters that could leas to a vi-olation of privacy aspects. To combine the local clusters at the central server, the aggregation step of DMBC can em-ploy two variants of parametrization to either derive a global clustering offering k clusters or an arbitrary set of clusters that are considerably different from each other. In both cases, DMBC derives a meaningful global mixture model of Gaussian in efficient time. Our broad experimental eval-uation shows that DMBC is a scalable solution for cluster-ing in a distributed env ironment that achieves comparative results compared to a centralized EM-based approach.
The rest of the paper is organized as follows. Section 2 discusses related work on distributed clustering and intro-duces the main concepts of EM clustering. In Section 3 we describe our novel DMBC algorithm in more detail. Section 4 provides an extensive experi mental evaluation of the per-formance and the accuracy of DMBC. Section 5 concludes the paper.
In the following, we will review recent work on parallel and distributed clustering. Parallel clustering is related to the problem of distributed clustering because the data ob-jects are also distributed over several clients where a local clustering is performed. The local clusterings are merged to produce the final model. However, parallel clustering meth-ods can control the assignment of data objects to each site. Thus, the merge step is usually less complex and implying different problems than the merge step of distributed data mining approaches. However, several recent approaches for distributed data mining are adoptions of parallel clustering algorithms.

Parallel versions of k -means, k -Harmonic-Means, EM [2, 5], and DBSCAN [12] are all not applicable within dis-tributed environments because all methods rely on a cen-tralized view of the data during or before the clustering is computed.

In [11] a parallel algorithm is proposed for clustering web documents distributed randomly over several sites. Any clustering algorithm can be used to generate local clus-ters. The entire local clusters are sent back to the server rather than compact prototypes. Clusters are merged if they share a given number of documents which is determined by deriving maximum-sized itemsets from the documents. Obviously, since all local documents are transferred to the server, this approach does not consider any privacy issues.
In [8] a distributed version of DBSCAN [3] is pre-sented. The local clusters are represented by special objects that have the best representative power. This representa-tive power is based on two quality measures that take the density-based clustering concepts into account. For each representative, a covering radius and a covering number is aggregated for the global merge step. The performance of the proposed method is heavily dependent on the number of representatives. If it is chosen too small, the accuracy sig-nificantly decreases. Otherwise, the runtime increases due to high transfer cost. In addition, since real data objects are sent to the global server, this approach does also not con-sider any privacy issues.

In [9] a single-link hierarchical clustering algorithm for vertically distributed data is proposed. However, our new approach DMBC is focused on horizontally distributed data.
Clustering algorithms can also be used to obtain a com-pact representation of a data set. An efficient and effective way to represent a local subset of a distributed data set is to use a mixture of different distribution functions. The most prominent algorithm that tries to describe the data by mul-tiple distribution functions is the EM algorithm [1]. In the following, we describe a variant of this algorithm which is used by our DMBC algorithm.

Let D beasetof d -dimensional points, i.e. D X  R d .The general idea of the EM algorithm is to describe the data by a mixture model M of k Gaussian distributions, where k is the only input parameter. Instead of assigning each object to a cluster as it is the case for k -means-based clustering algorithms, EM assigns each object to a cluster according to a weight representing the probability of membership.
Each cluster C  X  M is a tuple C =(  X  C ,  X  C ) ,where  X 
C is the mean value of all points in C and  X  C is the d  X  covariance matrix of all points in C . To compute the prob-ability distributions, we need the following concepts.
The probability density of a point x  X  X  within a Gaus-sian density distribution C =(  X  C ,  X  C ) is computed in the following way:
The combined density for k clusters can then be com-puted by: where w C i is the fraction of points that belongs to cluster C
Then, the probability that a point x  X  X  belongs to a cluster C can be computed by the rule of Bayes:
The log-likelihood of a mixture model M = ( C 1 ,...,C k ) of k Gaussian distributions which describes how good the model approximates the actual data set can be computed by:
The higher the value of E ( M ) , the more likely it is that thedataset D corresponds to the mixture M . Thus, the aim of the EM algorithm is to optimize the parameters of M in a way that E ( M ) is maximized. For that purpose, the algorithm proceeds in four steps: 1. Initialization Since the clusters, i.e. Gaussian distri-butions C 1 ,...,C k , are unknown at the beginning, a set of k initial clusters are generated randomly. For that purpose, each point x  X  X  is randomly assigned to one cluster C i . An initial model is produced by computing  X  C and  X  C for each cluster C  X  M . 2. Expectation Based on the current model, the parame-ters  X  C and  X  C can be computed for each cluster C  X  M and the log-likelihood E ( M ) of this mixture model M is obtained. 3. Maximization In this step, E ( M ) is improved via a recomputation of the parameters for each of the k clusters. Given a mixture model M , the parameters  X  C ,  X  C ,and w of each cluster C  X  M are recomputed. The resulting mix-ture M has an equal or higher log-likelihood than M ,i.e. E ( M )  X  E ( M ) . For improving the mixture, the parame-ters are recomputed as follows: 4. Iteration Step 2 and 3 are iterated until the log-likelihood of the improved mixture model M differs from the log-likelihood of the previous mixture M by a smaller value than a user specified threshold  X  , i.e. until | E ( M )  X  E ( M ) | &lt; X  .

The result of the EM algorithm is a set of kd -dimensional Gaussian distributions, each represented by the mean value  X  and the covariance matrix  X  and a weight w . The assignment of a point x  X  X  to a cluster C is given by the probability P ( C | x ) . We thus can compute how likely a point is assigned to each of the k clusters.

The log-likelihood of the result of the EM algorithm is usually dependent on the initial mixture model, i.e. on the model assumed in step 1, and on the number of clusters k . In [4] a method for producing a good initial mixture is presented which is based on multiple sampling. It is em-pirically shown that using this method, the EM algorithm achieves accurate clustering results.
In the following, we will refer to the clustering gener-ated by the centralized approach as centralized clustering and call the clusters that are part of the global clustering centralized clusters .
As discussed above, the centralized solution has several drawbacks which led to the distributed approach where the data is clustered locally at each site. Afterwards only the in-formation about the local clusters are transfered to a central server. The server can now reconstruct the global clustering which should be as similar to the centralized clustering as possible, by combining the local clusters. For this recombi-nation, we can distinguish the following cases: Case 1: All objects of a global cluster are found on a single local site. In this case, the global cluster can be spotted easily at the local site and should be added to the global clustering on the central site. Case 2: The objects of a global cluster are spread over sev-eral sites. In this case, we have to distinguish: Case 2(a): All objects are rather well described by some local cluster. In this subcase, a good clustering algorithm should discover which local clusters belong to the same global cluster and merge them at the central site. Case 2(b): Some of the objects of a global cluster are locally considered as noise or as members of local clus-ters that are built from object predominantly belonging to other global clusters. These objects contribute to the wrong global cluster or noise.
 Case 2(c): A cluster is distributed over several sites and none of them contains enough data objects for deriving a local cluster. In this case, the objects of the global clus-ter are considered as noise or parts of other clusters at each client site. Only by combining the local objects the global cluster would become visible.
 Because of the last two subcases, it is quite difficult, if not impossible, to develop an efficient distributed clustering al-gorithm that transmits local clusters and exactly rebuilds the centralized clustering for all cases. Since some of the centralized clusters may only be discovered if the noise ob-jects of several sites are combined, the clustering algorithm would have to transmit all objects that are not well described by any local cluster as well as the clusters. However, trans-mitting all these objects has two major drawbacks. First, privacy preservation gets almost impossible by transmit-ting single data objects. Second, with large amounts of noise, it becomes necessary to transmit large amounts of data as well and thus, the advant ages of distributed cluster-ing might get lost. However, since there is no other solution for this dilemma, a good distributed clustering algorithm should at least offer the possibility to adjust to the users preferences on privacy, performance and the degree of how good the derived distributed clustering corresponds to the centralized clustering. In the following, we will refer to the degree of how good a distributed clustering corresponds to the centralized clustering as the agreement of both clus-terings. Note, that a good distributed clustering algorithm cannot guarantee an agreement of 100% in all scenarios as discussed above.

In the following, we describe a method, called Dis-tributed Model-based Clustering (DMBC) that is based on the EM clustering of local sites. Instead of transmitting the complete local data set, we only transmit a number of lo-cal Gaussians and their weights to the central site. Since a Gaussian distribution is represented only by a mean vector and a covariance matrix, the amount of transferred informa-tion is much smaller. Therefore, the needed bandwidth is much smaller. Furthermore, the Gaussians derived by EM are always built according to all underlying data objects and drawing detailed conclusions about individual data objects is not possible in almost all settings. Since we will addition-ally control the remaining cases, the privacy of local data objects is preserved. Thus, our method avoids the problem of building a global clustering and still derives a mixture of Gaussian distributions achieving a high agreement with the centralized clustering.
 The DMBC algorithm proceeds in 4 steps: Step 1: The local data on each client site is clustered using EM. Thus, the data at each client site is now represented by a small but descriptive set of Gaussian distributions and a distribution of weights over these Gaussians. The number of clusters for the EM algorithm is optimized automatically with respect to the parametrization controlling the privacy level and the transfer volume.
 Step 2: The local Gaussians and weights are transmitted to the central site.
 Step 3: Similar local Gaussians are joined to find a com-pact global distribution. Thus, each cluster in a global EM clustering is only represented by a single Gaussian. Step 4: The calculated global clustering can optionally be transmitted back to the client sites.
To cluster the data at each client site, we employ the EM algorithm as described in section 2.2. The most important aspect of this step is the question how to choose the param-eter k , i.e. the number of Gaussians that is used to describe the local data distribution. Since the data distribution can strongly vary between each site, simply selecting a global value for k as the expected number of global clusters might be rather inappropriate. Therefore, our algorithm automat-ically determines a particular value k i for each site S i us note that k i not only influences the transfer volume, but also the privacy and exactness of the clustering as well. The larger k i , the higher is the probability that a Gaussian is strongly influenced by only a few data sets. In this case, the privacy can be seriously jeopardized because it might be easy to approximate the instances that are represented by this Gaussian. On the other hand, a large number of k i usually increases the tendency that all local data objects are well represented by a local Gaussian. To conclude, a very high value for k i will increase the agreement between our derived clustering and the global clustering, but it will also increase the transferred data volume.

To find a clustering containing an appropriate number of clusters, we first of all introduce the parameter k max de-scribing the maximum number of Gaussians for each local site. k max limits the maximum transfer from a local site to the central site in step 2 and thus can be derived from the available bandwidth. To measure the degree that all data objects are well represented by th e given clustering, we in-troduce the function cover ( C i,j ) .
 Definition 1 (Cover) Let M = C 1 ,...,C k be a mixture of Gaussians describing the density distribution within D Furthermore, let t  X  [0 ,..., 1] be a probability threshold. Then, the cover of the model M , denoted by Cov ( M ) ,is defined as follows: Cov ( M )= |{ x | x  X  X  X  X  X  C i  X  M : P ( C i | x )  X  t }|
Intuitively, the cover is the number of data objects that provide at least a probability of t for some Gaussian in the clustering. Let us note that Cov ( M ) is related but different to the log-likelihood E ( M ) that is optimized by the EM algorithm.

The pseudo code of the algorithm localEM to derive a local clustering is depicted in Figure 1. The algorithm chooses the smallest clustering M that achieves a maximum cover by successively increasing k and testing the cover of the resulting clustering.

At last, we have to control the level of privacy, we need to ensure. Thus, we have to measure how far it is possible to draw conclusions about individual data sets from the found clustering C . Therefore, we define the so-called privacy score (PScore): Definition 2 (Privacy Score) Let C i  X  M be a cluster that is described by a d -dimensional Gaussian determined by the variance vector  X  i and a covariance matrix  X  i . Then, the privacy-score , denoted by PScore ( C i ) , is defined as fol-lows:
Theideaofthe PScore ( C i ) is quite simple. Only if the variance in each dimension is very small, it is possible to draw conclusions about the underlying feature vectors. Let us note that the local cluster description of a cluster deter-mined by the EM algorithm is always built using the com-plete data set. As a result, it is impossible to derive detailed information about single feature vectors even for clusters having small weights if the variance values are large enough for at least a single dimension. Thus, we define a privacy threshold  X  p that is the lower limit for the PScore ( C i acluster C i that is allowed to be transferred. If a cluster C i has a smaller privacy-score, i.e. PScore ( C i ) &lt; X  p do not transmit the cluster because it would be possible to conclude that there is at least one feature vector stored on the local site that strongly resembles the transferred mean value.
 To conclude, at each site we determine the smallest EM clustering providing a maximum cover and afterwards transfer all clusters that do not violate the predefined level of privacy.
The purpose of this step is to combine the locally derived clusters to a distributed clustering describing the complete data distribution in a best possible way. The difficulty in this step is to find out which of the clusters are likely to describe the same global cluster. To find out which of the given lo-cal clusters should be joined, first of all we need a measure that describes the likelihood of two local Gaussians C 1 and C 2 to model the same global cluster. Simply, using the dis-tance between mean vectors is not applicable here because the significance of this distance strongly decreases with in-creasing variance values. Therefore, we define a new mea-sure that considers the dependency between variance and mean value, called mutual support.
 Definition 3 (Mutual Support) Let C 1 ,C 2 be two Gaus-sian determined by a mean vector  X  i and a covariance ma-trix  X  i . Then the mutual support of C 1 ,C 2 is given by: Let us note that x is a d -dimensional feature vector and thus MS ( x ) is defined using the integral over all d dimensions. The mutual support has several characteristics that makes it well suited for measuring the similarity between two Gaus-sians. The larger the variance values become, the less steep are the probability density functions of the Gaussians and the less important is the distance between the mean val-ues. Comparing a low variance distribution with a high variance Gaussian, will display a small mutual support. In the comparably small range where a low-variance distribu-tion displays strong density the high-variance distribution provides only moderate density and in the large area where the high-variance distribution displays still moderate den-sity, the density of the low-variance distribution decreases the product very strongly. Thus, the mutual support of two Gaussians specified by very similar mean values but quite different covariance matrices is also rather small.
After finding a method to compare two local Gaussians, we now start to determine which of the local clusters should be merged. To determine a distributed clustering from a set of local clusters C with a number of k global clusters, we can now proceed as described in Figure 2.

Another alternative for deriving a joined distributed clus-tering is to specify a threshold parameter  X  andjoinallclus-ters displaying a mutual support of at least  X  . In this case, all pairs of clusters ( C i ,C j ) are marked if MS ( C i ,C j Again we first of all, find the marked pairs of clusters that are connected by common clusters and afterwards merge all clusters in this connected set. Therefore, both approaches are independent of the order the clusters are merged. After determining which clusters have to be joined, we still need to derive a common Gaussian from a connected set of local clusters. Therefore, we derive a new mean  X  C for a set of Gaussian clusters C = { C 1 ,...,C m } that are specified by  X  and  X  i in the following way.
Here,  X  ( C k )= Cov ( M l ) where cluster C k has its origin on site l ,i.e.  X  ( C k ) denotes the cover of site l . The entries of the covariance matrix for the i th line and the j th column are calculated as following:
Let us note that we again need to employ a multiple in-tegral to calculate the new covariance matrix because we do not have the actual data distribution at each site. Therefore, we assume that the local density given by each local clus-tering is a well enough description of this distribution. To consider the number of data objects that are stored at each site, we additionally weight the influence of each distribu-tion with the cover we transmitted from this site.
The weight of C can be determined as
If we apply DMBC as proposed in the previous subsec-tion on higher dimensional data sets ( d&gt; 2 ), we face the problem that, in order to compute both the mutual support as well as the covariance matrix of a merged cluster, we have to evaluate multiple integrals.

Thus, in order to be scalable for higher dimensional data sets, we propose a variant of DMBC that uses variances in-stead of covariances for cluster representation. In particu-lar, we assume the attributes to be indepent of each other and represent a cluster C by its mean vector  X  C and its d -dimensional variance vector  X  C .The i -th value of  X  C ,de-noted by  X  i C , indicates the variance of the Gaussian along attribute i .

As a consequence, the resulting Gaussians form ellipsoid-shaped clusters that are constrainted to be axis-parallel. We will see later in the experimental evaluation, that this simplification does not cause a significant loss of quality. However, the benefits of this modification are the following. First, we are able to solve the integral of the mutual support analytically. Second, to compute the vari-ance vector of a merged cluster, we need to solve only one integral rather than multiple integrals. Third, the transfer cost for each local cluster are reduced from O ( d 2 ) for the covariance matrix to O ( d ) for the variance vector.
In fact, the mutual support of a pair of clusters C = {
C 1 ,C 2 } can be computed as
The following lemma enables us to solve this integral over d -dimensions analytically.
 local clusters. Then MS ( C 1 ,C 2 )= Proof. Let N  X  i  X  and  X  by: and apply the logarithm to the equation, it follows that Thus, we obtain
MS ( C 1 ,C 2 )=
The j -th component of the variance vector of the global cluster C which evolved from the merge of m clusters C i is given as:  X 
Note that for component  X  j C we compute only a 1-dimensional integral over dimension j .
We implemented our versions of DMBC in Java and run several tests on a workstation featuring two 1.8 GHz Opteron processors and 8 GByte main memory.

The test bed consists of one artificial 2-dimensional data set (denoted as DS1) and two real word data sets (denoted as DS2 and DS3). The latter two are derived from 68,040 im-ages of the corel image feature collection of the UCI KDD archive 1 . DS2 contains 9-dimensional color moments of images in HSV color space (mean, standard deviation and skewness). DS3 comprises a description of the corel images based on co-occurrence textures with 16 dimensions.
The experimal results of DMBC on the synthetic DS1 (Figure 3) demonstrates that our algorithm is capable to handle cases 1, 2(a)-(c) described in Section 3.1: DMBC finds the global cluster  X  X  X  the objects of which are exis-tent on only one single client, i.e. client 2 (Case 1). DMBC finds the global clusters  X  X  X ,  X  X  X , and  X  X  X  the objects of which are rather well described by local clusters on all sites (Case 2(a)). DMBC finds the global cluster  X  X  X  the objects of which are distrubuted over all sites such that none of the sites exhibit a local cluster (Case 2(c)). Several objects of clusters  X  X  X  and  X  X  X  are prototypes for Case 2(b) because they are members of local clusters that are built from objects predominantly belonging to another global cluster.
In order to demonstrate the robustness of our clustering algorithm w.r.t. the number of clusters on each client site, we performed distributed clu stering with a predetermined number of clusters. We measured the agreement between the distributed clustering and the results of the centralized EM algorithm using the Rand Index [6], also known as Rand Statistics.
 On the 2-dimensional synthetic data set (DS1) shown in Figure 3 DMBC achieved a Rand index of approximately 99.9%, indicating a high agreement of our method with the centralized clustering. For data DS2 and DS3 we used the variant of DMBC based on variances (cf. Section 3.4). As shown in Figure 4, DMBC achieves high Rand Index val-ues, i.e. our distributed approach produces a high level of agreement with the results of centralized clustering algo-rithms on all numbers of clusters. This also indicates that the variant proposed in Section 3.4 using variances instead of covariances does produce accurate results, too.
We evaluated the scalability of the proposed algorithm w.r.t. the number of clusters on each client site. The results are depicted in Figure 4(a). As it can be observed, in all settings, the Rand Index is near the optimal value. Thus, the agreement between the centralized clustering and the global distributed clustering is very high.

In addition, we investigated the scalability of the pro-posed algorithm w.r.t. the number of client sites. Figure 4(b) presents the agreement using the Rand Index between results calculated by our approach and the centralized clus-tering algorithm. The number of client sites involved in the distributed clustering was varying from 2 to 10. The high value of the Rand Index in all experiment evaluations shows that our algorithm is scalable w.r.t. the number of client sites and delivers results that do not differ from that of the global acting algorithm.

We also investigated the transfer cost w.r.t. the number of clusters on each client site and w.r.t. the number of client sites. The results are depicted in Figure 4(c) and 4(d). As transfer cost we measured the ratio of the number of bytes that are transfered using DMBC and of the number of bytes that are transfered using the centralized approach. As it can be seen, the transfer cost is in general very low. Even for a very large number of clusters, DMBC needs less than 1% of the bytes transfered by the centrailized approach. In addition, we can observe, that the transfer cost increases only linearly w.r.t. the number of local clusters and w.r.t. the number of client sites. Compared to other existing dis-tributed clustering approaches, e.g. the density-based dis-tributed approach in [8], where the local transfer cost is at least 15% of the local data in order to achieve a high agree-ment, our DMBC reduces the transfer cost dramatically.
Last, we investigated the robustness of DMBC w.r.t. the probability threshold t which affects the cover of the local models. As the results (not shown due to space limitations) suggest, DMBC is rather robust w.r.t. a broad range of val-ues for t . In fact, we observed a Rand Index over 98% when varying the values of t from0.05upto0.45.

To sum up, our experiments demonstrated the robust-ness, the efficiency, and the applicability of both of our pro-posed variants for distributed model-based clustering. In this paper, we proposed a distributed version for EM clustering called distributed model-based clustering (DMBC). Our method applies the EM algorithm at the lo-cal sites generating a model containing a set of Gaussian distributions. Each Gaussian is represented by its mean and its covariance matrix or  X  for higher dimensions the vari-ance vector. We also proposed a merge step of the local Gaussians that can handle covariances as well as variances. Compared to recent approaches for pure distributed cluster-ing, DMBC enables respecting an arbitrary level of privacy and dramatically reduces the transfer costs. Our experimen-tal evaluation demonstrates the robustness, the efficiency, and the applicability of both of our proposed variants for distributed clustering.

For future work we will examine the use of other distri-bution functions instead of Gaussian for clustering.
