 Proof. We assume in the statement of the theorem that for some constant M&gt; 1. This assumption is jus-tified if the accumulated cost obtained by the online algorithm is larger than the cost obtained by the opti-mal batch algorithm for most of the training examples. We argue that if this assumption is violated then the online algorithm is revealed to be a very good per-former, and it therefore makes little sense to judge its performance by the deviation from another algorithm (the  X  X ptimal X  batch algorithm) whose performance is worse for a significant fraction of the training sample. rewrite the regret ( 6 ) with a single loss function Under the conditions of the theorem, it is shown in (corolllary 2a ( Xiao , 2010 )) that where  X  T =(  X  D 2 + G 2  X  ) sublinearity of the inner product, we get From the definition of W l  X  1 t = l  X  1 k =1  X  W k t and W We use the bound on ||  X  W k t  X   X  W k  X  || from (theorem 1b ( Xiao , 2010 )) and assumption ( 8 ) to obtain where Q = 2 M  X  (  X  D 2 + G 2  X  ). Inserting this last in-from which ( 7 ) immediately follows.
 The synthetic data is created in the following manner: we define a binary tree with k leaves. Each leaf in the tree represents a single binary classification task. Each node in the tree corresponds to a single binary feature f  X  X  X  1 , 1 } . For a single task we divide the features into two groups: task-specific and task-irrelevant. Task-irrelevant features have equal proba-bility of having the value 1 or  X  1 for all examples in the task. Task-specific features are assigned the value 1 for all positive examples of the task. All negative examples of the task must have at least one feature from the task-specific feature set with a value of  X  1. The task-specific feature set is the set of features corre-sponding to all nodes on the path from the leaf to the root, while all other nodes define the task-irrelevant feature set. For each group of tasks, their shared fea-tures are those corresponding to common ancestors of the corresponding leaves. An illustration of the binary tree and a an example of a sample set of a specific task is given in Fig. 4 .
 Structure discovery: Feature weights learnt at different learning steps of the cascade for an experiment with 100 synthetic tasks, 199 features and 20 positive and negative samples per task, are shown in Fig 5 .As can be seen, the first learning stages, l = 1 and l = 2 capture shared information, while the last stages, e.g. stage l = 4 capture task specific features. The hierarchical shared structure of features is discovered in the sense that higher levels in the cascade share the same set of features and as the cascade progresses the chosen features are shared by less tasks. The pattern of learnt feature weights fits the synthetic data pattern of feature generation.
 Parameter Robustness Fig. 6 shows the robust-ness of the hierarchical approach with respect to the regularization parameter  X  . For all three regularizing approaches we varied the value of  X  in the range [0.01-2] with 0.01 jumps. For the hierarchal approach we set  X  We also found the method to be quite robust to the parameter L determining the number of levels in the cascade. Varying the value of L between 3 to 7 on the synthetic data with 100 tasks gave close results in the range 94.5% to 95.5%.
 Single Level Comparison Fig. 7 we show a com-parison of the online cascade to a group of single level
L=2 L=3 L=4 L=5 L=6 L=7 L=8 92.42 95.47 95.73 94.74 95.21 94.48 93.52 regularization schemes, using the same set of  X  val-ues we used in the cascade. Clearly no single-level regularization achieves as good performance as the hi-erarchical method.
 Adding Tasks We examined the effect of the num-ber of tasks in the multitask setting. Ideally adding more tasks should never reduce performance, while in most cases leading to improved performance. We tested two scenarios -adding tasks which are simi-larly related to the existing group of tasks Fig. 8 a, and adding tasks which are loosely related to all other tasks but strongly related among themselves Fig. 8 b. With additional tasks of the same degree of related-ness, we increase the amount of information available for sharing. As expected, we see in Fig. 8 athatper-formance improves with increasing number of tasks, both for the hierarchical algorithm and for  X  X 12Reg X .  X  X 1Reg X  is not intended for information sharing, and therefore it is not affected by increasing the number of tasks. When adding loosely related tasks, we see in Fig. 8 b that the performance of the hierarchical algo-rithm increases as we add more tasks; for the  X  X 12Reg X  method, on the other hand, we see a significant drop in performance. This is because in this case the over-all relatedness among all tasks decreases as additional tasks are added; the  X  X 12Reg X  method still tries to share information among all tasks, and its performance therefore decreases. Data Rotation Next, we wish to isolate the two fac-tors of sparsity and shared information. The synthetic data was constructed so that there is an increased level of shared information between classes as a function of the distance between their respective leaves in the defining tree of features. The shared features are also sparse. In order to maintain the shared information and eliminate sparseness, we rotate the vector of fea-tures; when the rotation is applied to more features, the amount of sparseness decreases respectively. Table 5 shows the comparative results both for the case when all features are rotated and when only half are rotated (in which case the features being rotated are chosen randomly). As expected, the regularization-free method - X  X oReg X  -is not effected by any of this. The performance of  X  X 1Reg X , which assumes spar-sity, drops as expected, reaching baseline with full ro-tation, presumably because during cross-validation a very low value for the regularization parameter is cho-sen. The two methods which exploit shared informa-tion, our hierarchical algorithm and the  X  X 12Reg X  base-line method, perform better than baseline even with no sparseness (full rotation), showing the advantage of being able to share information.
 L1Reg 92.54  X  0.17 86.70  X  0.59 73.01  X  0.09 L12Reg 91.49  X  0.2 85.56  X  0.62 78.49  X  0.16 NoReg 72.88  X  0.19 72.81  X  0.12 73.03  X  0.10 C.1. ILSVRC2010 Baseline Comparison In Fig 9 we show the error rates for the Top-1 sce-nario, considering a single classification. We show re-sults when training using all the data Fig 9 -left, and when using only 100 examples per each task Fig 9 -right. The results are shown as a function of the num-ber of repetitions of the algorithm over all the data. At convergence we see an improvement of 1 . 67% in ac-curacy when using the cascade with 7 levels, 28 . 96% compared to 27 . 29%. ( Zhao et al. , 2011 ) obtained an improvement of 1 . 8% in accuracy when compar-ing their approach to their own baseline, 22 . 1% vs. 20 . 3%. We obtained a similar rate of improvement us-ing much less information (not knowing the hierarchy) for a higher range of accuracies.
 We note that our baseline approaches converge after 20 repetitions when using all the data, (for clarity we show only up to 15 repetitions in the left plot of Fig 9 ). This effectively means the same runtime, as the cas-cade runtime is linear in the number of levels where each level has the same complexity of the baseline ap-proaches. On the other hand the online cascade al-gorithm 2 can be trivially parallelized where as the repetitions over the data for a single baseline cannot. Thus, in a parallel setting the gain in runtime would be linear in the number of levels of the cascade. A trivial parallelization can be implemented by running each level of the online cascade on a time stamp shifted by l thus the first level of the cascade will see at time t the t sample while level l will see sample t  X  l . Batch Method The batch knowledge-transfer method is described below in Algorithm 3 . The pro-jection matrix P l is defined by the first z columns of the orthonormal matrix U l ,where svd ( W l )= U l  X  V l . Online Method The online knowledge-transfer al-gorithm is described below in Algorithm 4 ; it succeeds t old iterations of Algorithm 2 . The input to the al-gorithm is the same set of parameters used for Algo-rithm 2 and its intermediate calculations -the cas-cade { W l old } L l =1 and the set of final average subgradi-ents {  X  U l old } L l =1 . These are used to approximate future subgradients of the already learnt tasks, since Algo-rithm 4 receives no additional data-points for these tasks. The parameters of Algorithm 2 are used be-cause cross-validation for parameter estimation is not possible with small sample.
 Below we denote the vector corresponding to the mean valueofeachfeatureas mean ( W l old ). We denote the concatenation of columns by  X  . In order to account for the difference between the old time step t old to the new time step t we consider h ( W ) to be the squared l 2 norm applied to each column separately. We calculate the inner product of the resulting vector with  X   X  t in step 1(a).(iv); the concatenation of k times t old with t .
 D.1. Experiments In this section we evaluate our algorithms for knowl-edge transfer in small sample scenarios. We start by comparing the different methods on controlled syn-thetic data in Section D.1.1 . We then test the perfor-mance of our method using several real data datasets employing different image representation schemes in two different settings: medium size , with several tens of classes and a dimensionality of 1000 features as im-Algorithm 3 Knowledge-Transfer with shared fea-tures projections Input : L number of levels { Output : W 1. W 0 =0 2. for l =1to L 3. w = w L age representation in Section D.1.2 ;and large size , with hundreds of classes and an image representation of 21000 features in Section D.1.3 . We also compared the performance in a  X 1-vs-rest X  setting and in a setting with a common negative class (as in clutter). The methods used for comparison are the following: Batch methods  X  KT-Batch-H : corresponds to Algorithm 3 where  X  KT-Batch-NoReg : here knowledge transfer corre-Online methods  X  KT-On-H : corresponds to Algorithm 4 where we  X  KT-On-L12 : corresponds to Algorithm 4 with Algorithm 4 Online Knowledge-Transfer learning cascade Input : {  X 
U l old } L l =1 the average subgradient of the last iteration {
W l old } L l =1 the set of parameters learnt by Algo-t old number of temporal iterations of Algorithm 2 Initialization:  X  1. for t = 1,2,3,... do Baseline methods, without Knowledge Transfer:  X  NoKT-Batch : corresponds to the multi-task batch  X  NoKT-On-NoReg : corresponds to the multi-task D.1.1. Synthetic Data To test Algorithms 3 and 4 we trained 99 tasks us-ing the multi-task batch and online algorithms with only 99 tasks, keeping the remaining task aside as the unknown novel task. Each known task was trained with 50 examples, with 10 repetitions over the data for the online Algorithm 2 . After this multi-task pre-processing had finished, we trained the left out task us-ing Algorithms 3 and 4 with either 1-10, 20 or 50 exam-ples (with 100 repetitions in the online Algorithm 4 ). This was done 100 times, leaving out in turn each of the original tasks. In the batch knowledge-transfer we chose the rank of each projection matrix to keep 99.9% of the variance in the data. In the hierarchi-cal knowledge-transfer this resulted in approximately 10 dimensions at the top level of the cascade, and 90 dimensions at the lowest level of the cascade. As can be seen in Fig. 10 a, our Knowledge-Transfer methods based on shared multi-task models achieve the best performance as compared to the alterna-tive methods. The online knowledge-transfer method achieves the best results on this dataset. Note that with very small samples, the method which attempts to share information with all tasks equally -KT-On-L12 -achieves the best performance. As the sample increases to 50, Algorithm 4 is able to perform bet-ter by exploiting the different levels of sharing given by the hierarchical approach KT-On-H . For both on-line Knowledge-Transfer options we see a significant improvement in performance as compared to the on-line with no knowledge transfer approach, NoKT-On-NoReg .
 Looking at the batch method we see that Knowledge-Transfer based on sharing information between the original models, KT-Batch-H , outperforms signifi-cantly the knowledge-transfer based on no sharing of information in the original model training, KT-Batch-NoReg .The KT-Batch-NoReg actually per-forms no better than the no knowledge-transfer ap-proach NoKT-Batch .
 It is also interesting to note that the difference in av-erage performance between the novel task to the pre-trained tasks is less then 0.5% for 50 training exam-ples when using the hierarchical knowledge-transfer. This indicates that in this experiment our hierarchi-cal knowledge-transfer method reaches the potential of sharing as in the multi-task method, which outper-forms all other methods on this synthetic data. D.1.2. Medium size We tested our method with real data, starting with a moderate problem size and only 31 classes. For these experiments we used a subset of the ILSVRC(2010) challenge ( Berg et al. , 2010 ), which is an image dataset organized according to the WordNet hierarchy. From this huge dataset we chose 31 classes (synsets) 4 for the set of pre-trained known classes with many training ex-amples. This group of classes was chosen heuristically to contain varying levels of relatedness among classes, grouping together various terrestrial, aerial and sea ve-hicles, buildings, sea animals etc. For the novel classes with small sample we considered 30 randomly chosen classes from the remaining 969 classes in the dataset. The set of features used to describe images in this data set is based on the Sift features quantized into a code-book of 1000 words, which was tf-idf normalized. We considered binary learning tasks where each chosen class, either pre-trained or novel, is contrasted with a set of images (negative examples) chosen from a group of different classes. The negative set was constructed in two ways: In the 1-vs-rest condition the negative set of classes, the Rest , was defined as the group of 31 classes from which knowledge is transferred. In the second condition the negative set included 31 differ-ent classes sampled randomly from the original dataset excluding the small sample classes and the set of pre-trained classes. In this second condition all classes, both pre-trained and novel, had the exact same set of negative examples. This condition resembles previous experiments with knowledge-transfer ( Zweig &amp; Wein-shall , 2007 ), where all tasks share the same negative set.
 In both conditions the pre-trained models were trained using 480 positive examples and 480 negative exam-ples. For the positive set of the small sample classes, we considered a sample size in the range 1-20. For the negative set we used all examples from each negative group (480 examples per class in the 1-vs-rest condi-tion and 480 in total in the second condition). Exam-ples were weighted according to sample size. For the pre-trained models we used a validation set of 60 ex-amples per class; we used 100 examples per each class as its test set.
 We considered each of the novel 30 classes separately. The experiment was repeated 8 times with different random splits of the data, for both the pre-trained and novel tasks. In the batch knowledge transfer methods we set the projection rank to maintain 99.9% of the variance in the original models learnt.
 Results for the condition with shared negative set are showninFig. 11 a. Results for the 1-vs-rest condi-tion are shown in Fig. 11 b. We see that knowledge-transfer methods achieve improved performance as compared to the alternative. In both conidtions the best performer is the hierarchical batch approach for Knowledge-transfer, KT-Batch-H . The poor perfor-mance of the KT-Online-L12 can be explained by the fact that the regularization coefficient  X  chosen by cross-validation during the multi-task pre-learning phase of the pre-trained models was chosen to be very low, indicating that a single level of sharing is not suf-ficient for this data.
 D.1.3. Large size As the ultimate knowledge transfer challenge, we tested our method with real data and large prob-lem size. Thus we used all 1000 classes from the ILSVRC(2010) challenge ( Berg et al. , 2010 ). Each im-age was represented by a vector of 21000 dimensions, following the representation scheme used by ( Zhao et al. , 2011 ). 900 classes were chosen randomly as pre-trained classes, while the remaining 100 classes were used as the novel classes with small sample. We con-sidered 1-vs-rest tasks as explained above. Pre-trained tasks were trained using 500 examples from the posi-tive class and 500 examples chosen randomly from the remaining 899 classes. We used the online Algorithm 2 with 2 repetitions over the training data to train pre-trained tasks.
 During test, the set of negative examples in the 1-vs-rest condition was chosen randomly from all of the dataset, total of 999 classes. We used the labeled test set provided by ( Berg et al. , 2010 ). As small sam-ple we considered 1-20 examples per class. Due to the original large image representation, in the batch knowledge-transfer methods we fixed the projection rank to maintain only 80% of the variance in the orig-inal models learnt.
 We note that once the pre-trained models are com-puted, each step of training with the projected batch approach is faster than each step of the online ap-proach as the online Algorithm 4 needs at each step to consider all the pre-trained parameters in order to compute the regularization value, while the batch Al-gorithm 3 considers these parameters only once during the projection phase. Using a big image representation as we do the online methods becomes computationally expensive if repeating the experiment for each of the novel small samples classes separately.
 Results are shown in Fig. 10 b. Clearly all methods in-ducing information sharing outperformed significantly the batch and online learning with no sharing. The NoKT-on-NoReg method performed poorly similarly to NoKT-batch and was omitted for brevity. KT-on-L12 also preformed poorly due to the very low regu-
 Alon Zweig alon.zweig@mail.huji.ac.il Daphna Weinshall daphna@cs.huji.ac.il Information sharing can be a very powerful tool in vari-ous domains. Consider visual object recognition where different categories typically share much in common: cars and trucks can be found on the road and both classes have wheels, cows and horses have four legs and can be found in the field together, etc. Accord-ingly, different information sharing approaches have been developed ( Torralba et al. , 2007 ; Obozinski et al. , 2007 ; Quattoni et al. , 2008 ; Amit et al. , 2007 ; Duchi &amp; Singer , 2009 ; Kim &amp; Xing , 2010 ; Shalev-Shwartz et al. , 2011 ; Kang et al. , 2011 ).
 In this paper we focus on the multi-task and multi-class settings, where the learning is performed jointly for many tasks or classes. Multi-task ( Caruana , 1997 ) is a setting where there are several individual tasks which are trained jointly, e.g., character recognition for different writers. Multi-class is the case where there is only a single classification task involving several possi-ble labels (object classes), where the task is to assign each example a single label.
 Intuitively the more data and tasks or classes there are, the more one can benefit from sharing information between them. Recent approaches ( Obozinski et al. , 2007 ; Quattoni et al. , 2008 ; Amit et al. , 2007 ; Duchi &amp; Singer , 2009 ; Shalev-Shwartz et al. , 2011 ) to informa-tion sharing consider all tasks as a single group without discriminating between them. However, applying such approaches to datasets with many diverse tasks focus the learning on shared information among all tasks, which might miss out on some relevant information shared between a smaller group of tasks.
 To address this challenge, our work takes a hierarchi-cal approach to sharing by gradually enforcing differ-ent levels of sharing, thus scaling up to scenarios with many tasks. The basic intuition is that it is desirable to be able to share a lot of information with a few related objects, while sharing less information with a larger set of less related objects. For example, we may encourage modest information sharing between a wide range of recognition tasks such as all road vehicles, and separately seek more active sharing among related ob-jectssuchasalltypesoftrucks.
 Previous work investigating the sharing of information at different levels either assume that the structure of sharing is known, or solve the hard problem of cluster-ing the tasks into sharing groups ( Torralba et al. , 2007 ; Kang et al. , 2011 ; Jawanpuria &amp; Nath , 2012 ; Kim &amp; Xing , 2010 ; Zhao et al. , 2011 ). The clustering ap-proach solves a hard problem and can be used most ef-fectively with smaller data-sets, while clearly when the hierarchy is known methods which take advantage of it should be preferred. But under those condition where these methods are not effective, our implicit approach enables different levels of sharing without knowing the hierarchy or finding it explicitly.
 More specifically, we propose a top-down iterative fea-ture selection approach: It starts with a high level where sharing features among all classes is induced. It then gradually decreases the incentive to share in successive levels, until there is no sharing at all and all tasks are considered separately in the last level. As a result, by decreasing the level of incentive to share, we achieve sharing between different subsets of tasks. The final classifier is a linear combination of diverse classifiers, where diversity is achieved by varying the regularization term.
 The diversity of regularization we exploit is based on two commonly used regularization functions: the l 1 norm ( Tibshirani , 1996 ) which induces feature spar-sity, and the l 1 / l 2 norm analyzed in ( Obozinski et al. , 2007 ) which induces feature sparsity while favoring feature sharing between all tasks. Recently the sparse group lasso ( Friedman et al. , 2010 ) algorithm has been introduced, a linear combination of the lasso and group-lasso ( Yuan &amp; Lin , 2006 ) algorithms, which can yield sparse solutions in a selected group of variables, or in other words, it can discover smaller groups than the original group constraint ( l 1 / l 2 ).
 The importance of hierarchy of classes (or taxonomy) has been acknowledged in several recent recognition approaches. A supervised known hierarchy has been used to guide classifier combination ( Zweig &amp; Wein-shall , 2007 ), learn distance matrices ( Hwang et al. , 2011 ; Verma et al. , 2012 ), induce orthogonal trans-fer down the hierarchy ( Zhou et al. , 2011 ) or detect novel classes ( Weinshall et al. , 2008 ). Recent work on multi-class classification ( Bengio et al. , 2010 ; Gao &amp; Koller , 2011 ; Yang &amp; Tsang , 2011 ) has also tried to infer explicitly some hierarchical discriminative struc-ture over the input space, which is more efficient and accurate than the traditional multi-class flat structure. The goal in these methods is typically to use the dis-covered hierarchical structure to gain efficient access to data. This goal is in a sense orthogonal (and com-plementary) to our aim at exploiting the implicit hi-erarchical structure for information sharing for both multi-task and multi-class problems.
 The main contribution of this paper is to develop an implicit hierarchical regularization approach for infor-mation sharing in multi-task and multi-class learning (see Section 2 ). Another important contribution is the extension to the online setting where we are able to consider a lot more learning tasks simultaneously, thus benefiting from the many different levels of sharing in the data (see Section 3 for algorithm description and regret analysis). In Section 4 we describe extensive experiments on both synthetic and seven popular real datasets. In Section 5 we briefly present the extension of our approach to a knowledge-transfer setting. The results show that our algorithm performs better than baseline methods chosen for comparison, and state of the art methods described in ( Kang et al. , 2011 ; Kim &amp;Xing , 2010 ). It scales well to large data scenarios, achieving significantly better results even when com-pared to the case where an explicit hierarchy is known in advance ( Zhao et al. , 2011 ). We now describe our learning approach, which learns while sharing examples between tasks. We focus only on classification tasks, though our approach can be easily generalized to regression tasks.
 Notations Let k denote the number of tasks or classes. In the multi-task setting we assume that each task is binary, where x  X  X  n is a datapoint and y  X  X  X  1 , 1 } its label. Each task comes with its own sample set S i = { ( x s , y s ) } m i s =1 ,where m i is the sam-ple size and i  X  X  1 ... k } . In the multi-class setting we assume a single sample set S = { ( x s , y s ) } m s =1 ,where y s  X  X  1 ..k } . Henceforth, when we refer to k classes or tasks, we shall use the term tasks to refer to both without loss of generality.
 Let n denote the number of features, matrix W  X  R n  X  k the matrix of feature weights being learnt jointly for the k tasks, and w i the i  X  X hcolumnof W .Let b  X  X  k denote the vector of threshold parameters, where b i is the threshold parameter corresponding to task i . || W || 1 denotes the l 1 norm of W and || W || 1 , 2 denotes its l 1 / l 2 norm-|| W || 1 , 2 = n j =1 || w j w j is the j  X  X h row of matrix W and || w j || 2 its l 2 norm. The classifiers we use for the i  X  X h task are linear clas-sifiers of the form f i ( x )= w i  X  x + b i . Binary task classification is obtained by taking the sign of f i ( x ), while multi-class classification retrieves the class with maximal value of f i ( x ).
 To simplify the presentation we henceforth omit the explicit reference to the bias term b ; in this notation b is concatenated to matrix W as the last row, and each datapoint x has 1 added as its last element. Whenever the regularization of W is discussed, it is assumed that the last row of W is not affected. The classifiers now take the form f i ( x )= w i  X  x .
 To measure loss we use the binary and multi-class hinge loss functions ( Crammer &amp; Singer , 2002 ). The multitask loss is defined as follows:
Without joint regularization this is just the sum of the loss of k individual tasks. The multi-class loss is defined as:
L ( S , W )= For brevity we will refer to both functions as L ( W ). Note that the choice of the hinge loss is not es-sential, and any other smooth convex loss function (see ( Wright et al. , 2009 )) can be used. 2.1. Hierarchical regularization We construct a hierarchy of regularization functions in order to generate a diverse set of classifiers that can be combined to achieve better classification. The con-struction of the regularization cascade is guided by the desire to achieve different levels of information sharing among tasks. Specifically, at the highest level in the hierarchy we encourage classifiers to share information among all tasks by using regularization based on the l / l 2 norm. At the bottom of the hierarchy we induce sparse regularization of the classifiers with no sharing by using the l 1 norm. Intermediate levels capture de-creasing levels of sharing (going from top to bottom), by using for regularization a linear combination of the l and l 1 / l 2 norms. We denote the regularization term of level l by  X  l : where  X  l is the mixing coefficient and  X  l is the regu-larization coefficient. The regularization coefficient of the last row of W l corresponding to bias b is 0. For each individual task (column of W l )welearn L classifiers, where each classifier is regularized differ-ently. Choosing the L mixing terms  X  l  X  [0 .. 1] di-versely results in inducing L different levels of sharing, with maximal sharing at  X  l = 0 and no incentive to share at  X  l =1. 1 2.2. Cascade algorithm Learning all diversely regularized classifiers jointly in-volves a large number of parameters which increases multiplicatively with the number of levels L .Alarge number of parameters could harm the generalization properties of any algorithm which attempts to solve the optimization problem directly. We therefore devel-oped an iterative approach presented in Algorithm 1 , where each level is optimized separately using the op-timal value from higher levels in the hierarchy. Specifically, we denote by L the preset number of lev-els in our algorithm. In each level only a single set of parameters W l is being learnt, with regularization uniquely defined by  X  l . We start by inducing maximal sharing with  X  1 = 0. As the algorithm proceeds  X  l monotonically increases, inducing decreased amount of sharing between tasks as compared to previous steps. In the last level we set  X  L = 1, to induce sparse regu-larization with no incentive to share.
 Thus starting from l =1upto l = L , the algorithm for sparse group learning cascade solves The learnt parameters are aggregated through the learning cascade, where each step l of the algorithm receives as input the learnt parameters up to that point-W l  X  1 . Thus the combination of input parame-ters learnt earlier together with a decrease in incentive to share is intended to guide the learning to focus on more task/class specific information as compared to previous steps.
 Note also that this sort of parameter passing between levels works only in conjunction with the regulariza-tion; without regularization, the solution of each step is not affected by the solution from previous steps. In our experiments we set  X  l = l  X  1 L  X  1 for all l  X  X  1 ..L while the set of parameters {  X  l } L l =1 is chosen using cross-validation.
 Algorithm 1 Regularization cascade Output : W 1. W 1 =argmin 2. for l =2to L 3. W = W L 2.3. Batch optimization At each step of the cascade we have a single uncon-strained convex optimization problem, where we mini-mize over a smooth convex loss function summed with a non-smooth regularization term ( 4 ). This type of optimization problems has been studied extensively in the optimization community in recent years ( Wright et al. , 2009 ; Beck &amp; Teboulle , 2009 ). We used two pop-ular optimization methods ( Daubechies et al. , 2004 ; Beck &amp; Teboulle , 2009 ), which converge to the single global optimum with rate of convergence O( 1 T 2 )for ( Beck &amp; Teboulle , 2009 ). Both are iterative procedures which solve at iteration t the following sub-problem: min where  X  ( W t ) = ((1  X   X  ) || W t || 1 , 2 +  X  || W t || a constant factor corresponding to the step size of it-eration t . This sub-problem has a closed form solution presented in ( Sprechmann et al. , 2011 ), which yields an efficient implementation for solving a single itera-tion of Algorithm 1 . The complexity of the algorithm is L times the complexity of solving ( 4 ). When the number of training examples is very large, it quickly becomes computationally prohibitive to solve ( 4 ), the main step of Algorithm 1 . We therefore devel-oped an online algorithm which solves this optimiza-tion problem by considering one example at a time -the set of parameters W l is updated each time a new mini-sample appears containing a single example from each task.
 In order to solve ( 4 ) we adopt the efficient dual-averaging method proposed by ( Xiao , 2010 ), which is a first-order method for solving stochastic and online regularized learning problems. Specifically we build on the work of ( Yang et al. , 2010 ), who presented a closed form-solution for the case of sparse group lasso needed for our specific implementation of the dual-averaging approach. The update performed at each time step by the dual averaging method can be written as: where U denotes the subgradient of L t ( W + W l  X  1 ) with respect to W ,  X  U the average subgradient up to time t , h ( W )= 1 2 || W || 2 2 an auxiliary strongly con-vex function, and  X  a constant which determines the convergence properties of the algorithm.
 Algorithm 2 describes our online algorithm. It follows from the analysis in ( Xiao , 2010 ) that the run-time and memory complexity of the online algorithm based on update ( 5 ) is linear in the dimensionality of the parameter-set, which in our setting is nk . Note that in each time step a new example from each task is processed through the entire cascade before moving on to the next example.
 Algorithm 2 Online regularization cascade Initialization:  X  W l 0 =0 ,  X  U l 0 =0  X  l  X  X  1 ..L } , W 0  X  t 1. for t = 1,2,3,... do 3.1. Regret analysis In online algorithms, regret measures the differ-ence between the accumulated loss over the sequence of examples produced by the online learning algo-rithm, as compared to the loss with a single set of parameters used for all examples and optimally chosen in hindsight. For T iterations of the al-gorithm, we can write the regret as R T ( W  X  )= t =1 ( L t ( W t )+  X  ( W t )  X  L t ( W  X  )  X   X  ( W  X  )), where W  X  =argmin At each time step t , Algorithm 2 chooses for each level l&gt; 1 of the cascade the set of parameters  X  W l t ,tobe added to the set of parameters W l  X  1 t calculated in the previous level of the cascade. Thus the loss in level l both the example at time t and the estimate W l  X  1 t obtained in previous learning stages of the cascade. We define the following regret function that compares the performance of the algorithm at level l to the best choice of parameters for all levels up to level l : where  X  W l  X  =argmin  X  = 0 and W We note that the key difference between the usual def-inition of regret above and the definition in ( 6 )isthat in the usual regret definition we consider the same loss function for the learnt and optimal set of parameters. In ( 6 ), on the other hand, we consider two different different set of parameters derived from previous levels in the cascade.
 We now state the main result, which provides an upper bound on the regret ( 6 ) and thus proves convergence. Theorem 1. Suppose there exist G and D such that  X  t ,l || U l t || &lt; G and h ( W ) &lt; D 2 , and suppose that R where A =(  X  D 2 + G 2  X  ) , B = 4 3 ( l  X  1) G 2 M  X  A , C =  X  ( M  X  1)(  X  D 2 + G 2  X  ) for some constant M&gt; 1 ,and  X  denotes the convexity parameter of  X  .
 Proof. See Appendix A in suppl. material. Comparison Methods. We compare our algo-rithms to three baseline methods, representing three common optimization approaches:  X  X oReg X  -where learning is done simply by minimizing the loss function without regularization.  X  X 12 X  -a common approach to multi-task learning where in addition to minimizing the loss function we also regularize for group sparse-ness (enforcing feature sharing) using the l 1 / l 2 norm.  X  X 1 X -a very common regularization approach where the loss function is regularized in order to induce spar-sity by using the l 1 norm. All methods are optimized using the same algorithms described above, where for the non-hierarchical methods we set L = 1, for  X  X oReg X  we set  X  = 0, for  X  X 12 X  we set  X  = 0, and for  X  X 1 X  we set  X  = 1. The parameter  X  for  X  X 12 X  and  X  X 1 X  is also chosen using cross validation.
 We also use for comparison three recent approaches which exploit relatedness at multiple levels. (i) The single stage approach of ( Kang et al. , 2011 )whichsi-multaneously finds the grouping of tasks and learns the tasks. (ii) The tree-guided algorithm of ( Kim &amp; Xing , 2010 ) which can be viewed as a two stage ap-proach, where the tasks are learnt after a hierarchical grouping of tasks is either discovered or provided. We applied the tree-guided algorithm in three conditions: when the true hierarchy is known, denoted  X  X GGL-Opt X ; when it is discovered by agglomerative cluster-ing (as suggested in ( Kim &amp; Xing , 2010 )), denoted  X  X GGL-Cluster X ; or when randomly chosen (random permutation of the leafs of a binary tree), denoted  X  X GGL-Rand X . (iii) The method described in ( Zhao et al. , 2011 ) which is based on the work of ( Kim &amp; Xing , 2010 ) and extends it to a large scale setting where a hierarchy of classes is assumed to be known. 4.1. Synthetic data In order to understand when our proposed method is likely to achieve improved performance, we tested it in a controlled manner on a synthetic dataset we had cre-ated. This synthetic dataset defines a group of tasks related in a hierarchical manner: the features corre-spond to nodes in a tree-like structure, and the num-ber of tasks sharing each feature decreases with the distance of the feature node from the tree root. We tested to see if our approach is able to discover (im-plicitly) and exploit the hidden structure thus defined. The inputs to the algorithm are the sets of examples and labels from all k tasks { S i } k i =1 , without any knowl-edge of the underlying structure.
 Classification performance: We start by showing in Fig. 1 -left that our hierarchical algorithm achieves the highest accuracy results, both for the batch and online settings. For the smallest sample size the  X  X 12 X  base-line achieves similar performance, while for the largest sample size the  X  X 1 X  baseline closes the gap indicating that given enough examples, sharing of information be-tween classes becomes less important. We also see that the online Algorithm 2 converges to the performance of the batch algorithm after seeing enough examples. The advantage of the hierarchical method is not due simply to the fact that it employs a combination of classifiers, but rather that it clearly benefits from shar-ing information. Specifically, when the cascade was ap-plied to each task separately, it achieved only 93.21% accuracy as compared to 95.4% accuracy when applied to all the 100 tasks jointly.
 To evaluate our implicit approach we compared it to the Tree-Guided Group Lasso ( Kim &amp; Xing , 2010 ) (TGGL) where the hierarchy is assumed to be known -either provided by a supervisor (the true hierar-chy), clustered at pre-processing, or randomly chosen. Fig. 1 -right shows results for 100 tasks and 20 posi-tive examples each. We challenged the discovery of task relatedness structure by adding to the original feature representation a varying number (500-4000) of irrelevant features, where each irrelevant feature takes the value  X -1 X  or  X 1 X  randomly. Our method performs much better than TGGL with random hierarchy or clustering-based hierarchy. Interestingly, this advan-tage is maintained even when TGGL gets to use the true hierarchy, with up to 1500 irrelevant features, pos-sibly due to other beneficial features of the cascade. 4.2. Real data Small Scale ( Kang et al. , 2011 ) describe a multi-task experimental setting using two digit recog-nition datasets MNIST ( LeCun et al. , 1998 )and USPS ( Hull , 1994 ), which are small datasets with only 10 classes/digits. For comparison with ( Kang et al. , 2011 ), we ran our method on these datasets using the same representations, and fixing L = 3 for both datasets. Table 1 shows all results, demonstrating clear advantage to our method. The results of our ba-sic baseline methods  X  X oReg X  and  X  X 1 X  achieve similar or worse results, 2 comparable to the single task base-line approach presented in ( Kang et al. , 2011 ). Thus, the advantage of the cascade  X  X  X  does not stem from the different optimization procedures, but rather re-flects the different approach to sharing.
 Medium Scale We tested our batch approach with four medium sized data sets: Cifar100 ( Krizhevsky &amp; Hinton , 2009 ), Caltech101, Caltech256 ( Griffin et al. , 2007 ) and MIT-Indoor Scene dataset ( Quattoni &amp; Tor-ralba , 2009 ) with 100, 102, 257 and 67 categories in each dataset respectively. We tested both the multi-class and multi-task settings. For the multi-task set-ting we consider the 1-vs-all tasks. For Cifar-100 we fixed L = 5 for the number of levels in the cascade, and for the larger datasets of Caltech101/256 and Indoor-Scene we used L =4.
 We also investigated a variety of features: for the Cifar-100 we used the global Gist representation em-beddedinanapproximationoftheRBFfeaturespace using random projections as suggested by ( Rahimi &amp; Recht , 2007 ), resulting in a 768 feature vector. For the Caltech101/256 we used the output of the first stage of Gehler et al X  X  kernel combination approach ( Gehler &amp;Nowozin , 2009 ) (which achieves state of the art re-sults on Caltech101/256) as the set of features. For the MIT-Indoor Scene dataset we used Object Bank features ( Li et al. , 2010 ), which achieves state-of-the-art results on this and other datasets.
 We tested the Cifar-100 dataset in the multi-task and multi-class settings. We used 410 images from each class as the training set, 30 images as a validation set, and 60 images as the test set. For the multi-task setting we considered the 100 1-vs-rest classification tasks. For each binary task, we used all images in the train set of each class as the positive set, and 5 exam-ples from each class in the  X  X est X  set of classes as the negative set. The experiment was repeated 10 times using different random splits of the data. Results are shown in Table 2 , showing similar performance for the cascade and the competing Tree-Guided Group Lasso method.
 In our experiments with the MIT-Indoor Scene dataset we used 20, 50 and 80 images per scene category as a training set, and 80, 50 and 20 images per cate-gory as test set respectively. We repeated the experi-ment 10 times for random splits of the data, including the single predefined data split provided by ( Quat-toni&amp;Torralba , 2009 ) as a benchmark. Fig. 2 -left shows the classification results of the cascade, which significantly outperformed the baseline methods and the previously reported state of the art result of ( Li et al. , 2010 ) on the original data split of ( Quattoni &amp; Torralba , 2009 ), using the exact same feature repre-sentation. We also significantly outperformed  X  X GGL-Cluster X  and  X  X GGL-Rand X .
 With Caltech101 and Caltech256 we used the data provided by ( Gehler &amp; Nowozin , 2009 ) for compar-isons in both their original multi-class scenario and a new multi-task scenario. We tested our approach us-ing the exact same experimental setting of ( Gehler &amp; Nowozin , 2009 ) given the scripts and data provided by the authors. In the original multi-class setting ad-dressed in ( Gehler &amp; Nowozin , 2009 ) our results com-pare to their state-of-the-art results both for 30 train-ing images (78.1%) in the Caltech101 and for 50 images (50%) in the Caltech256.
 In the multi-task scenario we trained a single 1-vs-rest classifier for each class. In addition to our regular baseline comparisons we implemented a variant of the  X  -LPB method, which was used in ( Gehler &amp; Nowozin , 2009 ) as the basis to their multi-class approach. Fig. 2 -right, shows results for the multi-task setting on the Caltech256. First we note that our algorithm outperforms all other methods including  X  -LPB. We also note that given this dataset all regularization ap-proaches exceed the NoReg baseline, indicating that this data is sparse and benefits from information shar-ing. (Results for the Caltech101 are similar and have therefore been omitted.) Large Scale We demonstrate the ability of our on-line method to scale up to large datasets with many labels and many examples per each label by testing it on the ILSVRC(2010) challenge ( Berg et al. , 2010 ). This is a large scale visual object recognition challenge, with 1000 categories and 668-3047 examples per cate-gory. With so many categories the usual l 1 / l 2 regular-ization is expected to be too crude, identifying only a few shared features among such a big group of diverse classes. On the other hand, we expect our hierarchical method to capture varying levels of useful information to share.
 We compared our method to the hierarchical scheme of ( Zhao et al. , 2011 ) (using their exact same fea-ture representation). Rather than compute the hier-archy from the data, their method takes advantage of a known semantic word-net hierarchy, which is used to define a hierarchical group-lasso regularization and calculate a similarity matrix augmented into the loss function. The comparison was done on the single split of the data provided by the challenge ( Berg et al. , 2010 ).
 Table 3 shows our performance as compared to that of ( Zhao et al. , 2011 ). We show accuracy rates when considering the 1-5 top classified labels. In all settings we achieve significantly better performance using the exact same image representation and much less labeled information. 4.3. Discussion For small and medium scale datasets we see that our cascade approach and the batch Algorithm 1 outper-form the baseline methods significantly. For the large scale dataset the online Algorithm 2 significantly out-performs all other baseline methods. It is interesting to note that even when the alternative baseline meth-ods perform poorly, implying that the regularization functions are not beneficial on their own, combining them as we do in our hierarchical approach improves performance. This can be explained by the fact that a linear combination of classifiers is known to improve performance if the classifiers are accurate and diverse. When comparing to the recent related work of ( Kim &amp;Xing , 2010 ) denoted TGGL, we see that our im-plicit approach performs significantly better with the synthetic and MIT-Indoor scene datasets, while on the Cifar dataset we obtain similar results. With the syn-thetic dataset we saw that as the clustering of the task hierarchy becomes more challenging, TGGL with clus-tering degrades quickly while the performance of our method degrades more gracefully. We expect this to be the case in many real world problems where the un-derlining hierarchy is not known in advance. Further-more, we note that with the small scale digit datasets our approach outperformed significantly the reported results of ( Kang et al. , 2011 ). Finally, we note that our approach can be used in a pure online setting while these two alternative methods cannot.
 We also compared with a third method ( Zhao et al. , 2011 ) using the ILSVRC(2010) challenge ( Berg et al. , 2010 ); this is a challenging large scale visual catego-rization task, whose size -both the number of cate-gories and the number of examples per category, pro-vides the challenges particularly suitable for our ap-proach. The online algorithm makes it possible to scale up to such a big dataset, while the hierarchical shar-ing is important with possibly many relevant levels of sharing between the tasks. Particularly encouraging is the improvement in performance when compared to the aforementioned work where an explicit hierarchy was provided to the algorithm. We now briefly outline two natural extensions of our multi-task learning algorithms to achieve knowledge-transfer to novel related tasks which arrive with too few training examples. 3 The transfer of information is based on the cascade of matrices { W l } L l =1 learnt during pre-training. The extension of the batch al-gorithm is based on dimensionality reduction of pre-trained models. The extension of the online method maintains the same regularization structure and pa-rameters of the online multi-task setting.
 Batch Method The method is based on project-ing the data into the subspaces defined by the learnt models { W l } L l =1 . Consequently the new task is rep-resented in L sub-spaces that capture the structure of the shared information between the previous k tasks. Specifically, we define each projection matrix P l by the first z columns of the orthonormal matrix U l ,where svd ( W l )= U l  X  V l .Ateachlevel l we project the new data-points onto P l . Thus the modeling of the new task involves z  X  L parameters, wherein the l  X  X h level data is projected onto the unique subspace char-acteristic of level l . In order to pass the parameters to the next level we project back to the original feature space.
 Online Method We assume that a related set of tasks has been learnt using the online Algorithm 2 .In order to initialize the online learning of a new single task or group of tasks, we concatenate the parameters of the new tasks to the previously learnt parameters, thus influencing the structure of the newly learnt task parameters.
 The batch method is particularly useful when the data lies in a high dimensional feature space, and the number of examples from the novel task is too small to learn effectively in such a space. The online approach is particularly useful for bootstrapping the online learning of novel classes, achieving higher per-formance at the early stages of the learning as com-pared to a non transfer approach. Results are shown in Fig. 3 . We presented a cascade of regularized optimization problems designed to induce implicit hierarchical shar-ing of information in multi-task, multi-class and knowledge-transfer settings. We described efficient batch and online learning algorithms implementing the cascade. For the online algorithm we provided regret analysis from which it follows that the average regret of our learning method converges. The method was tested on synthetic data and seven real datasets, show-ing significant advantage over baseline methods, and similar or improved performance as compared to alter-native state of the art methods.
 We would like to thank Bin Zhao and Prof. Eric Xing for providing the code for representing the data in our comparison to their work ( Zhao et al. , 2011 ). We would also like to thank Uri Shalit for helpful discus-sions and literature pointers.
 Research funded in part by the Intel Collaborative Re-search Institute for Computational Intelligence (ICRI-CI).

