 Syntax-based machine translation is well known for its ability to handle non-local reordering. Syntax-based models either use linguistic annota-tion on the source language side (Huang, 2006; Liu et al., 2006), target language side (Galley et al., 2004; Galley et al., 2006) or are syntactic in a structural sense only (Chiang, 2005). Recent shared tasks have shown that systems integrat-ing information on the target language side, also called string-to-tree systems, achieve the best per-formance on several language pairs (Bojar et al., 2014). At the same time, soft syntactic features significantly improve the translation quality of hi-erarchical systems (Hiero) as shown in (Marton et al., 2012; Chiang, 2010; Liu et al., 2011; Cui et al., 2010). Improving the performance of string-to-tree systems through the integration of soft syn-tactic constraints on the source language side is therefore an interesting task.

So far, all approaches on this topic include soft syntactic constraints into the rules of string-to-tree (Zhang et al., 2011; Huck et al., 2014) or string-to-dependency (Huang et al., 2013) systems and define heuristics to determine to what extent these constituents match the syntactic structure of the source sentence. We propose a novel way to in-tegrate soft syntactic constraints into a string-to-tree system. We define a discriminative rule se-lection model for string-to-tree machine transla-tion. We consider rule selection as a multi-class classification problem where the task is to select the correct target side of a rule given its source side as well as contextual information about the source sentence and the considered rule. So far, such models have been applied to systems without syntactic annotation on the target language side. He et al. (2008), He et al. (2010) and Cui et al. (2010) apply such rule selection models to hier-archical machine translation, Liu et al. (2008) to tree-to-string systems and Zhai et al. (2013) to systems based on predicate argument structures. When target side syntactic annotations are taken into account, the task of rule selection has to be reformulated (see Section 2) while the same type of model can be used in approaches without target annotations. This work is the first attempt to define a rule selection model for a string-to-tree system. We make our implementation publicly available as
We show in Section 2 that string-to-tree rule se-lection is different from the hierarchical case ad-dressed by previous work and define our rule se-lection model. In Section 3 we present the train-ing procedure before providing a proof-of-concept evaluation in Section 4. 2.1 String-to-tree machine translation We present string-to-tree machine translation as implemented in Moses (which is the framework that we use). String-to-tree rules have the form X/A  X   X   X , X ,  X  X  . On the source language side, Figure 1: Word-aligned sentence pairs with target-side parse.

Figure 2: Partial translation during decoding. all non-terminals have the unique label X while on the target language side non-terminals are an-notated with syntactic labels n t  X  N t . The left-hand side X/A consists of source and target non-terminals. In the right hand side (rhs),  X  is a string of source terminal symbols and the non-terminal X . The string  X  consists of target terminals and non-terminals n t  X  N t . The alignment  X  is a one-to-one correspondence between source and target non-terminal symbols. String-to-tree rules are ex-tracted from pairs of strings and trees as exempli-fied in Figure 1. Rules r 1 and r 2 are example rules extracted from this data. During decoding, CYK+ chart parsing (Chappe-lier et al., 1998) with cube pruning and language model scoring is performed on an input sentence such as F below. Each time a rule is applied to the input sentence, candidate target trees are built. Figure 2 shows the partial translations built after the segments Diverses and importantes have been decoded. Given these partial translations, rule r 1 can be applied in a further decoding step.
 2.2 String-to-tree rule selection Rule selection is the problem of selecting the rule with the correct target side among rules with the same source side. For hierarchical machine trans-lation (Hiero), the rule selection problem consists of choosing, among r 3 and r 4 , the rule that cor-rectly applies to F ( r 3 in our example). Rule selection models disambiguate between these rules using context information about the source sentence and the shape of the rules.

In string-to-tree machine translation, the rule selection problem is different. Because the decod-ing process is guided by target side syntactic an-notation, partial trees built during decoding must be considered when new rules are applied. For instance, when a rule is selected to translate sen-tence F given the partial translations in Figure 2, then the non-terminals in the target side of this rule must match the constituents selected so far. Consequently, rules r 1 and r 2 (Section 2.1) are rules for r 1 would be r 5 and r 6 below.
For consistency with decoding, we redefine the rule selection problem for the string-to-tree case. In this setup, it is the task of disambiguating rules with the same source side and aligned target non-terminals . As a consequence, our rule selection model (presented next) is not only normalized over the source rhs of the rules but also takes target non-terminals into account. The default rule scor-ing procedure for string-to-tree rules implemented in Moses uses the same normalization as we do. However, Williams and Koehn (2012) propose to normalize string-to-tree rules over the source rhs only.
Figure 3: French sentence with input parse tree. 2.3 Rule selection model We denote string-to-tree rules with X/A  X   X   X , X ,  X  X  , as in Section 2.1. By  X  Nt t , we de-note target non-terminals with their alignment to source non-terminals. 3 C ( f, X  ) is context infor-mation in the source sentence f and the source side  X  . R (  X , X  ) represents features on string-to-tree rules. The rule selection model estimates P (  X  | C ( f, X  ) ,R (  X , X  ) , X ,  X  Nt t ) and is normal-ized over the set G 0 of candidate target sides  X  0 for a given  X  and  X  Nt t . The function GTO :  X   X  G 0 generates, given the source side  X  and target non-terminals  X  Nt t , the set G 0 of all corresponding target sides  X  0 . The estimated distribution can be written as:
In the same fashion as (Cui et al., 2010) do for the hierarchical case, we define a global rule selec-tion model instead of a model that is local to the source side of each rule.
 To illustrate the feature templates C ( f, X  ) and R (  X , X  ) of our rule selection model, we suppose that rule r 1 has been extracted from the French sentence in Figure 3. The syntactic features are: The rule internal features are: Our rule selection model is integrated in the Moses string-to-tree system as an additional fea-ture of the log-linear model. We create training examples using the rule extrac-We begin by generating a rule-table using this pro-cedure. Then, each time a rule r : X/A  X   X   X , X ,  X  X  can be extracted from the training data, we generate a new training example. The target side  X  of the extracted rule is a positive instance and gets a loss of 0. To generate negative sam-ples, we collect all rules r 2 ,...,r n that have the same source language side as r as well as the same aligned target non-terminals  X  Nt t . Each of these rules is a negative example and gets a cost of 1. As an example, suppose that rule r 1 introduced in Section 2.1 has been extracted from the training example in Figure 1. The target side  X  JJ 1 JJ 2 characteristics X  is a correct class and gets a cost of 0. The target side of all other rules having the same source side and aligned target non-terminals , such as rule r 5 and r 6 , are incorrect classes.
For model training, we use the cost-sensitive one-against-all-reduction (Beygelzimer et al., fitting to training data by employing early stop-ping once classifier accuracy decreases on a held-4.1 Experimental Setup Our baseline system is a syntax-based system with linguistic annotation on the target language side ( string-to-tree ). We use the version implemented in the Moses open source toolkit (Hoang et al., 2009; Williams and Koehn, 2012) with standard parameters. Rule extraction is performed as in (Galley et al., 2004) with rule composition (Gal-ley et al., 2006; DeNeefe et al., 2007). Non-lexical unary rules are removed (Chung et al., 2011) and scope-3 pruning (Hopkins and Langmead, 2010) is performed. Rule scoring is done using relative frequencies normalized over the source rhs and aligned non-terminals in the target rhs. The con-trastive system is the same string-to-tree system but augmented with our rule selection model as a feature of the log-linear model. Table 2: String-to-tree system evaluation results.
We evaluate the baseline and our global model on three domains: (1) news , (2) medical , and (3) science . The training data for news is taken from Europarl-v4. Development and test sets are from the news translation task of WMT 2009 (Callison-Burch et al., 2009). For medical we use the biomedical data from EMEA (Tiedemann, 2009). Since this is a parallel corpus only, we first removed duplicate sentences and then constructed development and test sets by randomly selecting sentence pairs. As training data for science we use the scientific abstracts data provided by Carpuat et al. (2013). Table 1 gives an overview of the corpora sizes.

Berkeley parser (Petrov et al., 2006) is used to parse the English side of each parallel corpus (for string-to-tree rule extraction) as well as for pars-ing the French source side (for feature extraction). We trained a 5-gram language model on the En-glish side of each training corpus using the SRI Language Modeling Toolkit (Stolcke, 2002). We train the model in the standard way and gener-ate word alignments using GIZA++. After train-ing, we reduced the number of translation rules by only keeping the 30-best rules with the same source side according to the direct rule transla-tion rule probability. Our rule selection model was trained with VW. All systems were tuned using batch MIRA (Cherry and Foster, 2012). We mea-sured the overall translation quality with 4-gram BLEU (Papineni et al., 2002), which was com-puted on tokenized and lowercased data for all sys-tems. Statistical significance is computed with the pairwise bootstrap resampling technique of Koehn (2004). 4.2 Results Table 2 displays the BLEU scores for our experi-ments. On science and news , small improve-ments are achieved while for medical a small decrease is observed. None of these differences is statistically significant.

An analysis of the system outputs for each do-main showed that the small improvements are due to the fact that in string-to-tree systems there is not enough ambiguity between competing rules dur-ing decoding. To support this conjecture, we first analyzed rule diversity by looking at the negative samples collected during training example acqui-sition. In a second step, we compared the results of the string-to-tree systems in Table 2 with a sys-tem where the translation rules are much more am-biguous. To this aim, we applied our approach to a hierarchical system in the same line as (Cui et al., 2010). Finally, we further tested the ability of our system to disambiguate between competing rules by training a model on the concatenation of all do-mains. 4.3 Analysis of Rule Diversity The amount of competing rules during decoding can be estimated by looking at the negative sam-ples collected for each training example. This analysis showed that the diversity of rules contain-ing non-terminal symbols is limited. We present rules q 1 to q 3 (taken from science ) to illustrate the poor diversity observed in our training exam-ples. Rules q 1 to q 3 are the only rules with source side ` a X 1 X 2  X  eventail X 3 . This number is very low given that the source side contains three non-terminal symbols out of which two are adjacent. More-over, the difference between these rules is limited to the lexical translation of  X  eventail . This lack of diversity is due to the constraint that compet-ing string-to-tree rules must have the same aligned non-terminal symbols, which is taken into account when collecting negative samples. In other words, the ambiguity between translation rules in a string-to-tree system is heavily restricted by the target side syntax.

The observed lack of diversity could be min-imized by allowing rules with the same source rhs to have different aligned target non-terminals. In this perspective, rule scoring should be done by normalizing over the source rhs only as in Williams and Koehn (2012). The rule selection model in Section 2.3 should then be redefined and normalized over all rules with the same source rhs. Another way to improve rule diversity would be to remove target non-terminals and use preference Table 3: Hierarchical system evaluation results. The results in bold are statistically significant im-provements over the Baseline (at confidence p &lt; 0 . 05 ). grammars as in Huck et al. (2014). 4.4 Comparison with Hierarchical Rule We applied our approach in a hierarchical phrase-based setting (Hiero). To this end, we trained 3 Hi-ero baseline systems and 3 Hiero systems aug-mented with our rule selection model on the data given in Section 4.1. The results of these ex-periments are shown in Table 3. Our augmented system largely outperforms the baselines. Inter-estingly, hierarchical rule selection significantly helps on the medical and scientific domain but still yields results that are significantly lower than those of the string-to-tree systems. This indicates that systems with target side syntax better disam-biguate than hierarchical models with improved rule selection. Overall, we find the results of both types of systems promising and we will consider how to introduce more diversity into the rules of string-to-tree systems. 4.5 Concatenation of Training Data In order to further evaluate the ability of our model to disambiguate string-to-tree rules, we trained a system using the concatenated training data of all 3 domains as presented in Section 4.1. This global model was then used to tune and decode using the development and test data of each domain. The results in Table 4 show that even on concatenated data our rule selection model does not improve over the baseline.
 Table 4: String-to-tree system evaluation results with concatenated training data. We presented the first attempt to define a rule se-lection model with syntactic features for string-to-tree machine translation. We have shown that in order to be applied to the string-to-tree case, the rule selection problem must be redefined. An ex-tensive evaluation on French-English translation tasks for different domains has shown that rule se-lection cannot significantly improve string-to-tree systems. An analysis of rule diversity and an em-pirical comparison with hierarchical rule selection indicate that the low improvements are due to the fact that the ambiguity between string-to-tree rules is too small to be improved with a rule selection model. In future work, we will use different tech-niques to improve the diversity of the string-to-tree rules considered during decoding in our system. We thank all members of the DAMT team of the 2012 JHU Summer Workshop. We are especially grateful to Hal Daum  X  e III and Ales Tamchyna for their ongoing support in the implementation of our system. We also thank Andreas Maletti for his shared expertise on tree grammars. This project has received funding from the European Unions Horizon 2020 research and innovation programme under grant agreement No 644402 (HimL) and the DFG grant Models of Morphosyntax for Statistical Machine Translation (Phase 2) , which we grate-fully acknowledge.

