 Mining data streams is important in both science and com-merce. Two major challenges are (1) the data may grow without limit so that it is difficult to retain a long history; and (2) the underlying concept of the data may change over time. Different from common practice that keeps recent raw data, this paper uses a measure of conceptual equivalence to organize the data history into a history of concepts. Along the journey of concept change, it identifies new concepts as well as re-appearing ones, and learns transition pattern s among concepts to help prediction. Different from conven-tional methodology that passively waits until the concept changes, this paper incorporates proactive and reactive pre-dictions. In a proactive mode, it anticipates what the new concept will be if a future concept change takes place, and prepares prediction strategies in advance. If the anticipa -tion turns out to be correct, a proper prediction model can be launched instantly upon the concept change. If not, it promptly resorts to a reactive mode: adapting a prediction model to the new data. A system RePro is proposed to im-plement these new ideas. Experiments compare the system with representative existing prediction methods on variou s benchmark data sets that represent diversified scenarios of concept change. Empirical evidence demonstrates that the proposed methodology is an effective and efficient solution to prediction for data streams.
 H.2.8 [ Database Management ]: Database Management X  data mining ; I.2.6 [ Artificial Intelligence ]: Learning Algorithms Data stream, proactive learning, conceptual equivalence Prediction for data streams is important. Streaming data such as transition flows in stock market are very common in the modern society. This particular paper is set in the con-text of classification learning. A data stream is a sequence of instances. The task is to predict every instance X  X  class. Prediction for data streams is however not a trivial task. Streaming data have special characteristics that bring new challenges. First, the data may grow without limit and it is difficult to retain their complete history. Since one often needs to learn from the history, what information should be kept is critical. Second, the underlying concept of the data may change over time. This requires updating the prediction model accordingly and promptly.

Substantial progress has been made on the problem of prediction for data streams [2, 3, 4, 5, 10, 12, 13]. How-ever, a number of problems remain open. First, previous approaches have not well organized nor made good use of the history of data streams. Two typical methodologies ex-ist. One is to keep a recent history of raw instances since the whole history is too much to retain. As to be explained later, it may be applicable to concept drift but loses valu-able information in the context of concept shift . Another methodology, upon detecting a concept change, discards the history corresponding to the outdated concept and accumu-lates instances to learn the new concept from scratch. As to be demonstrated later, it ignores useful experience and i s suboptimal if a previous concept is re-established.
A second open problem is that existing approaches are mainly interested in predicting the class of each specific in -stance. No significant effort has been devoted to foreseeing a bigger picture, that is, what the new concept will be if a concept change takes place. This prediction, if possible, i s at a more general level and is proactive . It will help prepare for the future and can instantly launch a new prediction strat-egy upon detecting a concept change. For example, people have learned over time the concept change among seasons, such as winter comes after autumn. As a result, people pre-pare for the winter during the autumn, which is of great help in many aspects of life. However, the problem of predicting the oncoming concept can be far more complicated than the example. The transition among concepts can be probabilis-tic rather than deterministic. Brand new concepts can show up, of which the history has no information.

This paper sets out to tackle those open problems. The goals involve (1) organizing the history of raw data into a history of compact concepts by identifying new concepts as well as re-appearing historical ones; (2) learning pattern s of concept transition from the concept history; and (3) carryi ng out effective and efficient prediction at two levels, a general level of predicting each oncoming concept and a specific level of predicting each instance X  X  class.
A data stream is a sequence of instances . Each instance is a vector of attribute values with a class label. If its class is known, an instance is a labelled instance. Otherwise it is unlabelled . Predicting for an instance is to decide the class of an unlabelled instance by evidence from labelled instances .
A concept is represented by the learning results of a clas-sification algorithm, such as a classification rule set or a Bayesian probability table.

Concept change has three modes. Concept drift is gradual [9, 11, 13] while concept shift is more abrupt [6, 9]. Sampling change [8, 13] is a change of data distribu-tion that leads to revising the current model even when the concept remains the same. One may assume for concept drift that the current concept is the most similar to the oncoming concept because of its continuity [12]. Concept shift how-ever does not hold this assumption. For instance, suppose a democrat government succeeds a republican government. To predict what national policies will take place, it is bett er to check what happened under previous democrat govern-ments in history, rather than check what happened under the last republican one although it was the most recent.
This history is built along the journey of concept change by identifying new concepts as well as re-appearing ones. First, it retains essential information of a long history wh ere raw data are prohibitive to keep. Second, it stores previ-ous concepts so that they can be reused if needed, which is more efficient than learning from scratch. Third, the transi-tions among concepts can be learned therefrom, which helps proactively predict what the oncoming concept is.
A classification algorithm is used to abstract a concept from the raw data. A user may choose any classification al-gorithm of his/her interest, or choose one that appears to be good at learning the current data. In this particular paper, C4.5rules [7] is employed since it is commonly used and can achieve a reasonable classification accuracy in general.
A trigger detection algorithm finds instances, across which the underlying concept has changed and the predic-tion model should be modified. It is especially important when concept shifts. Keogh and Tsymbal et al. [4, 11] have reviewed numerous trigger detection algorithms. A sliding-window methodology is used here with two parameters win-dow size and error threshold . The beginning of the window is always a misclassified instance. When the window is full and its error rate exceeds the error threshold, the beginnin g instance is taken as a trigger; otherwise, the beginning of t he window is slid to the next misclassified instance (if there is any) and previous instances are dropped from the window.
A conceptual equivalence measure checks whether a concept is historical or brand new. In Table 1, it calculates the equivalence degree between a newly-learned concept V and each historical concept T by comparing their verdicts on each available instance. If the degree is above a user-define d threshold, V is a reappearing T . Otherwise, V is a new concept. It is more proper than the conceptual equivalence measure used in the previous work [14] that relies on the classification accuracies of V and T on D . For example, the accuracy and the degree of conceptual equivalence are not necessarily positively correlated. Although V and T classify D with low accuracies, their equivalence degree can be very high if they agree on classifying many instances even when their classifications are both wrong.

Input : concept V newly learned from current data D , and historical concept T .

Output : ce  X  [-1,1], the bigger the value, the higher the degree of conceptual equivalence between V and T .
Begin ce = 0;
FOREACH instance I  X  D return ( ce = ce / | D | );
A stable learning size specifies the number of instances above which the learned concept can be deemed stable. In order to be sensitive to concept change, the trigger-window size is normally small and is sometimes insufficient to in-duce the adequate concept. If one has to learn a concept out of these few instances, the concept is not stable (such as of high classification variance) and should be re-learned once a stable-learning-size of labelled instances are accu mu-lated. In practice, the system keeps two concept histories: H retaining stable concepts and triggers; and H retaining possibly unstable concepts and triggers. Assume the stable learning size is s and the instance index for the last stable trigger is j . Each concept C t learned before j + s is stored in H . When classes of instances across [ j , j + s ) are known, a single concept C k is learned therefrom. If C t s X  average clas-sification accuracy is no better than C k  X  X , C k is deemed as a stable concept and is stored into H ; and H is updated by H . Otherwise, C t s are deemed as a series of stable concepts and H is updated by H . Although H may be temporarily used to predict each instance X  X  class, H is always used to learn transition matrices and to predict the oncoming concept.
Integrating all the above key components, the process of building a concept history is illustrated in Table 2. For demonstration X  X  sake, assume that the window size is 10, the stable learning size is 30 and the trigger error threshol d is 55%. A  X  represents an instance where a stable trigger is detected. A  X  represents an instance where a temporary trigger is detected. A stance. A  X  represents a wrongly classified instance. Only instances coming after the last stable trigger and up to now are retained for the purpose of calculating conceptual equi v-alence. Other historical instances are not essential to kee p.
As in Stage 3 of Table 2, upon trigger detection one should change the model to classify oncoming instances. Avail-able information is (1) a history of concepts up to this trig-ger; and (2) a window size of labelled instances, which con-tributed to detecting this new trigger. Depending on when to start preparing this new model, prediction can be proac-tive or reactive. Each approach has its own pros and cons. A system ReP ro is proposed to combine their strengths. Table 2: The process of building concept history window is full and the error rate is 80% which is above the error threshold. Hence, t is detected as a new trigger. stable. Use conceptual equivalence (as in Table 1) to check stable and contemporary concept histories. 
Stage 3 A new prediction model is chosen for instances coming after t+9. Assume a new trigger is detected at t+18. A concep t C learned over [t, t+18) can be unstable since it is learned fr om to o few instances. Store it only into the temporary concept history. 
Stage 4 When time arrives at t+29, 30 instances are known. If no learn a concept C k+1 from [t, t+29]. If C update temporary history by stable history. Otherwise update stable history by temporary history. 
When a stable-learning-size of instances have known classes, the first concept can be learned and stored in the stable history. 
A proactive mode predicts the forthcoming concept given the current one by evidence from the concept history. It is ahead of a trigger detection and is independent of the trigge r window. Once a new trigger is detected, the predicted con-cept immediately takes over the classification task. The ad-vantages are the quick response and stable prediction model .
The concept history is treated as a Markov Chain where each distinct concept is a state. A Markov chain is a model commonly used in information retrieval. It involves a se-quence of states where the probability of the current state depends on the past state(s). If the probability only depend s on the immediate past state, it is a first-order Markov chain. Otherwise it is a higher-order Markov chain.

A transition matrix can be built and updated from the chain over time, recording the frequency or probability of each alternative concept following a candidate concept. For example, a sequence of arriving weather concepts is spring, summer, autumn, winter, spring, summer, hurri-cane, autumn, winter, spring, flood, summer, autumn, win-ter, spring, summer, autumn, winter, spring, summer, hur-ricane, autumn, ... Table 3 shows an accumulated first-order transition matrix. Whenever a new concept comes, a new row and a new column are added in the table.
 Transition patterns among concepts can be learned from the transition matrix, such as the most likely concept after autumn is winter. However, problems can happen when the anticipation is less confident, such as no state has a domi-nant probability given the current state. In this case, one may turn to higher-order transition matrices. The higher order the matrix to maintain, the more expensive the sys-tem becomes. Or, one can use a reactive approach to break ties, which is the topic of the following section.
A reactive mode waits until the concept has changed to construct a prediction model depending on the trigger in-stances. It can be either contemporary or historical .
Upon detecting a trigger, the contemporary-reactive mode does not consult the concept history. Instead, it trains a prediction model simply from the trigger instances to classify oncoming instances. Although straightforward , this approach risks high classification variance when the sl id-ing window is small. For example, C4.5rules can almost al-ways form a set of rules with a high classification accuracy across a window of instances. The rules, however, are very likely to misclassify oncoming instances because of its poo r generalization strength. As a result, another new trigger i s soon detected even when the data are from the same con-cept. Nonetheless, it can be a measure of last resort if the concept is brand new and has no history to learn from.
Upon detecting a trigger, the historical-reactive mode retrieves a concept from the history that is most appropri-ate for the trigger instances. It tests each distinct histor ical concept X  X  classification accuracy across the trigger windo w. The one with the highest accuracy is chosen as the new pre-diction model. One merit of consulting the concept history is that each concept accepted by the history is a stable clas-sifier. Hence it can avoid the contemporary mode X  X  problem. However, there are also potential disadvantages. One prob-lem happens when the new concept is very different from existing ones. Consequently, the history offers a low classi fi-cation accuracy on the window. Another potential concern is its efficiency. If there are many distinct concepts in his-tory, it may take a while to test every single one.
A system RePro (re active plus pro active), as in Table 4, incorporates proactive and reactive predictions, using on e X  X  strength to offset the other X  X  weakness. As a result, if the proactive component of RePro foresees multiple prob-able concepts given the current concept, one can use the historical-reactive component as a tie breaker. The other way around, if there are many historical concepts, the proac -tive component can offer a short list to the historical-react ive component and speed up the process. If both the proactive and historical-reactive modelling incur low accuracy in th e new trigger window, it indicates that the new concept is very different from historical ones. Hence, a contemporary-reactive classifier can help to cope with the emergency. Nonetheless, as explained before, in order to avoid the high classification variance problem, when a stable-learning-s ize of labelled instances are accumulated, one should update th e contemporary-reactive concepts by a stable concept.
Input : A list of distinct historical concepts C DIS , a concept sequence C SEQ , a transition matrix C T RA , a window of trigger instances I W IN , a probability threshold threshold prob , a classification accuracy thresh-old threshold accu .
 Output : Concept c next for oncoming instances.

Begin c last = the last stable concept in C SEQ ; // Proactive c next (s) = concept(s) whose probability given c last is big-ger than threshold prob , according to C T RA ;
IF multiple c next s exist // Historical-reactive IF no c next exists // Contemporary-reactive
IF accuracy of c next on I W IN is less than threshold accu return ( c next );
Many approaches have been published that predict in concept-changing scenarios. Three typical mechanisms are discussed here. More comprehensive reviews can be found in various informative survey papers [4, 11].

The FLORA system [13] relates to RePro in that it stores old concepts and reuses them if appropriate. However, it is oriented to data of small sizes instead of streaming data [3] . It represents concepts by conjunctions of attribute values and measures the conceptual equivalence by a syntactical comparison. This is less applicable in streaming data where learned rules can be easily of a large number and of protean appearances. Besides, FLORA stores the concepts only for a reactive purpose. It does not explore their associations and hence can not be proactive.

The concept-adapting very fast decision tree (CVFDT) [3] is one of the best-known systems that achieves efficient clas-sification in streaming data. It represents a popular method -ology that continuously adapts the prediction model to com-ing instances. CVFDT relates to RePro in that it detects triggers. However, upon finding a concept change, CVFDT builds a new prediction model from scratch. It resembles the contemporary-reactive modelling. In cases where histo ry repeats itself, CVFDT can not take advantage of previous experience and hence may be less efficient. Between the gap where the old model is outdated and the new model has not matured, the prediction accuracy may suffer.

The weighted classifier ensemble (WCE) [12] represents a family of algorithms that ensemble learners for prediction . To classify a coming instance, WCE divides historical raw data into sequential chunks of fixed size, builds a classifier from each chunk, and composes an ensemble where each clas-sifier is weighted proportional to its classification accura cy on the chunk most recent to the instance to be classified. WCE relates to RePro in that it uses a history of concepts (classifiers). However, the quality of this history is contr olled by an arbitrary chunk size k . There is no trigger detection nor conceptual equivalence. As a result, the following two scenarios can witness sub-optimal prediction accuracies.
Scenario 1 : in the most recent data chunk C 1 that is the gauge to weigh classifiers in the ensemble, the majority of instances are not from the new concept. As illustrated below, an instance of concept  X  is to be classified. Because the majority of instances in C 1 are of concept  X , classifiers suitable for  X  (such as C 1 and C 2 ) will be ironically given a higher priority than those suitable for  X  (such as C 4 ).
Scenario 2 : the previous concept dominates the retained raw data. As illustrated below, when classifying an instanc e of concept  X , most classifiers are still indulged in the previ -ous concept  X . As a result, even when the gauge chunk C 1 favors instances of  X , its correct weighing can be overshad-owed by the large number of improper classifiers (a single high-weight C 1 against many low-weight C 2 , C 3 and C 4
Experiments are conducted to verify whether RePro out-in various types of concept-changing scenarios.
Benchmark data are employed to cover concept drift, con-cept shift and sampling change. When applicable, instances are randomly generated with time as seed. Hence, the reap-pearance of concepts unnecessarily accompany the reappear -ance of instances, which better simulates the real world.
Hyperplane can simulate concept drift by its continuous moving [3, 5, 10, 11, 12]. A hyperplane in a d -dimensional space [0 , 1] d is denoted by P d i =1 w i x i = w 0 , where each vector of variables &lt; x 1 , ..., x d &gt; is a randomly generated instance and is uniformly distributed. Instances satisfyi ng P i =1 w i x i  X  w 0 are labelled as positive, and otherwise nega-tive. The value of each coefficient w i is continuously changed so that the hyperplane is gradually drifting in the space. roughly half of the instances are positive, and the other hal f are negative. Particularly in this experiment, w i  X  X  changing range is [-3,+3] and its changing rate is 0.005 per instance.
Stagger can simulate concept shift [5, 9, 11, 13]. There are three alternative underlying concepts, A : if color = red  X  size = small , class= positive ; otherwise, class= negative ; B : if color = green  X  shape = circle , class= positive ; oth-erwise, class= negative ; and C : if size = medium  X  large , class= positive ; otherwise, class= negative . Particularly in this experiment, 500 instances are randomly generated ac-streaming data X  X  large amount. cording to each concept. Besides, one can control the tran-sition among concepts from stochastic to deterministic by tuning the z parameter of Zipfian distribution, which has been detailed in Appendix A of [15]. In the remainder of the paper, unless otherwise mentioned, z is set as 1, sim-ulating the most common case in the real world where the transitions among concepts are probabilistic (neither det er-ministic nor totally random).
 Network intrusion can simulate sampling change [1]. It was used for the 3rd International KDD Tools Compe-tition. It includes a wide variety of intrusions simulated in a military network environment. The task is to build a prediction model capable of distinguishing between nor-mal connections (Class 1) and network intrusions (Classes 2,3,4,5). Different periods witness bursts of different intr u-sion classes. Assuming all data are simultaneously availab le, learners like C4.5rules can achieve a high classification ac -curacy on the whole data set. Hence one may think that there is only a single concept underlying the data. However, in the context of streaming data, a learner X  X  observation is limited since instances come one by one.
The original implementation of WCE specifies a param-eter, chunk size s . It does not update the ensemble until another s instances have come and been labelled. Read-ers may be curious about its performance under different frequencies of updating the ensemble. Hence, a more dy-namic version, dynamic WCE (DWCE), is also implemented here. DWCE specifies an additional parameter, buffer size f ( f  X  s ). Once f new instances are labelled, DWCE repar-titions the data into chunks of size s , and retrains and re-ensembles classifiers. According to empirical evidence as detailed in Appendix B of [15], when it is sufficient to avoid high classification variance, the smaller the chunk size, th e lower error WCE gets. With the same chunk size, DWCE with a smaller buffer size can outperform WCE in accu-racy. WCE and DWCE with parameter settings that have produced the lowest error rates are taken to compare with RePro.

As for CVFDT, the software published by its authors is used (http://www.cs.washington.edu/dm/vfml). There are abundant parameters. Various parameter settings are teste d and the best results are taken to compare with RePro.
The prediction error and time of each method on each data set are illustrated in Figure 1. The incremental results alo ng the time line are depicted in Figures 2, 3 and 4. Generally, RePro achieves lower error rates than all other methods in all data sets except for DWCE in Hyperplane, in which case RePro is far more efficient than DWCE. As a matter of fact, DWCE X  X  time overhead is prohibitively high, making it almost unfeasible for predicting in streaming data.
Specifically in Stagger, the concept shifts among A , B and C every 500 instances. RePro X  X  prediction error rate in-creases upon each concept change and decreases later when a proper prediction model is constructed. With time going by and the concept history growing longer, RePro learns better the transitions among concepts, adjusts faster to the con-cept change and achieves the lowest error rate. The second best method is CVFDT. Compared with RePro, CVFDT always learns a concept from scratch upon trigger detection no matter whether this concept is a reappearance of an old one. This rebuilding needs time and incurs high classifica-tion variance before the classifier becomes stable. Hence it has a slower adaption to the concept change and incurs more prediction errors than RePro. Although DWCE is better as expected than WCE in terms of prediction accuracy, both are trigger-insensitive and produce highest error rates in the context of concept shift.
In Hyperplane, the concept drifts up and down. This is the niche for WCE and DWCE because the most recent con-cept is always the most similar to the new one. Nonetheless, RePro offers a surprisingly competitive accuracy. An insigh t into RePro reveals that the conceptual equivalence measure distinguishes all concepts into two equivalence groups: on e with w i  X  [  X  3 , 0] and the other with w i  X  (0 , +3]. Otherwise the hyperplanes are too close to be worth differentiating. As a result, a concept sequence of A , B , A , B ,  X  X  X  is learned. To classify an instance of a latter B , RePro employs a former B . This is no less effective than classifying an instance by its recent instances when the concept drifts. As for CVFDT, it incurs the highest error rate. The reason is that the concept of P d i =1 w i x i  X  w 0 is not an easy one to learn and CVFDT does not learn from the history. CVFDT needs to wait long before it builds a new stable classifier from scratch, dur-ing which period many prediction mistakes have been made. This is compounded by the dilemma that the concept may soon change again after this (relatively long) waiting peri od and the newly-stabled classifier has to be discarded then.
In Network Intrusion, transitions among different intru-sion types are more random than deterministic. Hence, the reactive component of RePro often takes over the prediction task. Because an intrusion type sometimes re-appears itsel f in the data stream, RePro is able to reuse historical concept s and achieve the lowest prediction error. CVFDT acquires a competitive performance because a intrusion type is much easier to learn than a Hyperplane even if the learning starts from scratch. Again WCE and DWCE witness suboptimal results because there does not necessarily exist continual ity among intrusion types. Hence the recent concepts are not always the most appropriate to use. Figure 4: Incremental results on Network Intrusion
In summary, RePro excels in scenarios of concept shift and sampling change. It also offers a competitive accuracy when the concept drifts. In all cases, RePro is very efficient.
Human beings live in a changing world whose history may repeat itself. Hence, both foreseeing into the future and retrospecting into the history are important. This pa-per has proposed a novel mechanism to organize, into a concept history , data that stream along the time line. As a result, the problem of the intractable amount of stream-ing data is solved since concepts are much more compact than raw data while still retain the essential information. Besides, patterns among concept transitions can be learned from this history, which helps foresee the future. This pape r has also proposed a novel system RePro to predict for the concept-changing data streams. Not only can RePro con-duct a reactive prediction: detecting the concept change and accordingly modifying the prediction model for oncom-ing instances; but also RePro can be proactive : predicting the coming concept given the current concept. By making good use of the concept history, and incorporating reactive and proactive modelling, RePro is able to achieve both effec-tive and efficient predictions in various scenarios of concep t change, including concept shift, concept drift and samplin g change. Although different types of changes have differ-ent characteristics and require different optimal solution s, a challenge in reality is that one seldom knows beforehand what type of change is happening. Hence a mechanism like RePro that performs well in general is very useful. [1] C. C. Aggarwal, J. Han, J. Wang, and P. S. Yu. A [2] V. Ganti, J. Gehrke, and R. Ramakrishnan. Demon: [3] G. Hulten, L. Spencer, and P. Domingos. Mining [4] E. Keogh and S. Kasetty. On the need for time series [5] J. Z. Kolter and M. A. Maloof. Dynamic weighted [6] C. Lanquillon and I. Renz. Adaptive information [7] J. R. Quinlan. C4.5: Programs for Machine Learning . [8] M. Salganicoff. Tolerating concept and sampling shift [9] K. O. Stanley. Learning concept drift with a [10] W. N. Street and Y. Kim. A streaming ensemble [11] A. Tsymbal. The problem of concept drift: definitions [12] H. Wang, W. Fan, P. S. Yu, and J. Han. Mining [13] G. Widmer and M. Kubat. Learning in the presence of [14] Y. Yang, X. Wu, and X. Zhu. Dealing with [15] Y. Yang, X. Wu, and X. Zhu. Proactive-reactive
