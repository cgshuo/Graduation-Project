 REGULAR PAPER Niina Haiminen  X  Aristides Gionis  X  Kari Laasonen Abstract We study the problem of segmenting a sequence into k pieces so that the resulting segmentation satisfies monotonicity or unimodality constraints. Uni-modal functions can be used to model phenomena in which a measured variable first increases to a certain level and then decreases. We combine a well-known unimodal regression algorithm with a simple dynamic-programming approach to obtain an optimal quadratic-time algorithm for the problem of unimodal k -segmentation. In addition, we describe a more efficient greedy-merging heuristic that is experimentally shown to give solutions very close to the optimal. As a con-crete application of our algorithms, we describe methods for testing if a sequence behaves unimodally or not. The methods include segmentation error comparisons, permutation testing, and a BIC-based scoring scheme. Our experimental evalua-tion shows that our algorithms and the proposed unimodality tests give very intu-itive results, for both real-valued and binary data.
 Keywords Unimodal  X  Segmentation  X  Regression  X  Algorithms  X  BIC  X  Binary data 1 Introduction The problem of regression, which deals with fitting curves or functions to a set of points, is among the most well-studied problems in statistics and data mining. Regression functions represent data models that can be used for knowledge ex-traction, understanding of the data, and prediction [ 6]. In many cases, the data are assumed to come from an a priori known class of distributions and the task is to find the parameters of the distribution that best fit the data. In this paper, tonic or unimodal . The problem of computing monotonic and unimodal regres-ence [ 11, 12], because it arises in a wide range of applications, such as statistical modeling [ 14], operations research [ 8], medicine and drug design [ 7], and image processing [ 13].
 variable shows a single-mode behavior: its expected value first rises to a certain level and then drops. Examples of data that exhibit unimodal behavior include ( i ) the size of a population of a species over time, ( ii ) daily volumes of network traffic, ( iii ) stock-market quotes in quarterly or annual periods, when small fluc-tuations are ignored, etc.
 tion for a real-valued sequence can be computed in linear time by variants of the shown how to cleverly organize the PAV operations to achieve a linear-time algo-rithm for the seemingly more complex problem of unimodal regression. However, one of the drawbacks of the above methods is that they do not restrict the size of the resulting regression and they can potentially report models with very high complexity.
 regression functions by studying the following problem: Given a univariate se-quence and an integer k , partition the sequence into k segments and represent each segment by a constant value, so that the segment values satisfy unimodal-ity (or monotonicity) constraints and the total representation error is minimized. We call this problem unimodal (or monotonic) k -segmentation. The problem is polynomial, but naive dynamic programming algorithms have running times of the order of high-degree polynomials.
 solved in O ( n 2 k ) time, which is the same as the time required to solve the un-restricted k -segmentation problem, i.e., segmenting without the unimodality con-straints. Our algorithm combines the PAV algorithm with dynamic-programming techniques. The algorithm can be extended to handle higher-mode segmentations, e.g., bimodal, and the optimality proof holds with minor changes to it. In addition to the optimal algorithm, we describe a fast greedy heuristic, which runs in time O ( n log n ) and in practice gives solutions very close to optimal.
 modality tests for sequences. We explore three alternatives: Comparing segmen-tation errors, permutation testing, and a scoring scheme based on the Bayesian in-formation criterion (BIC). We found that all tests are able to distinguish between unimodal and nonunimodal sequences, both for real-valued and binary data. In our experimental evaluation, we also demonstrate that the proposed algorithms provide intuitive segmentations.
 define the problem of unimodal k -segmentation. In Sect. 3, we describe our algo-rithms, while the optimality proofs are given in Sect. 4. Section 5 introduces our methods for unimodality detection. In Section 6, we discuss our experiments, and Sect. 7 contains a short conclusion. 2 Preliminaries A real-valued sequence X consists of n values (also referred to as points), i.e., X = x 1 ,..., x n . The problems of regression and segmentation seek for rep-resenting the sequence X in a way that satisfies certain constraints. For the monotonic -and unimodal -regression problems, monotonicity constraints are im-posed on the regression values of the sequence. For the segmentation problem, the representation of the sequence is constrained to a small number of piecewise constant segments. 2.1 Monotonic regression The goal of increasing monotonic regression is to map each point x i of the se-quence X to a point  X  x i ,sothat and the regression error is minimized. Decreasing monotonic regression is defined in a similar fashion. Equation ( 1) defines the regression error with respect to the L 2 norm. In general, any L p norm, 1  X  p  X  X  X  , can be used; the norms L 1 (sum of absolute values) and L  X  (max) are also commonly used in statistics. 2.2 Unimodal regression Similarly to monotonic regression, a unimodal regression maps each value x i of values increase up to some point  X  x t and then decrease for the rest of the points. In other words, the goal is to minimize the regression error E R defined by Eq. ( 1), subject to the unimodality constraints Note that only one point of the sequence (or multiple points with the same value) the case, the regression error could be reduced by making a point mapped to the top, with a value higher than  X  x t , a new top of the regression.
 creasing. In the rest of the paper, without loss of generality, we refer by  X  X ni-modal X  to regressions whose values are first increasing and then decreasing. Since monotonicity is a special case of unimodality, all our results hold also for mono-tonic segmentations.
 represented by the same value  X  x in the regression. That value  X  x , common for all the points in r j , is denoted by  X  r j . The subsequence r j can be represented by the indices of its first and last points, denoted here by f j and l j , respectively. Thus f  X  r are strictly increasing up to  X  x t and then strictly decreasing. The pairs of indices ( f m is not specified as input to the regression problem; any solution that satisfies the regression constraints is a feasible one, and the optimal solution is defined over all feasible solutions. The solutions improve as m increases, up to some sequence-dependent value, after which the error does not decrease anymore when increasing the number of regression segments. 2.3 k -segmentation The task of k -segmentation is to represent the sequence with k piecewise constant segments with as small an error as possible. A segmentation of the original se-rally we have f j  X  l j , f 1 = 1, l k = n ,and f j = l j  X  1 + 1, for j = 2 ,..., k .A point x i in the original sequence is represented by the value of the segment s j to k , the goal is to find the segmentation that minimizes the total error, defined as 2.4 Unimodal segmentation We now define the problem of unimodal segmentation, which is the focus of this paper. Unimodal segmentation combines the two previous problems in a natural way: we seek to represent a sequence by a small number of segments and subject to the unimodality constraints.
 Given a sequence X = x 1 ,..., x n , and an integer k  X  n ,findthe k -segmentation S of X that minimizes the error of the segmentation E S as defined by Eq. ( 2), and satisfies the unimodality constraints  X  s 1  X   X  X  X   X   X  s p  X   X  X  X   X   X  s k . in higher dimensions with appropriately defined error metrics, but the algorithms not aware of any algorithms for monotonic or unimodal regression for higher-dimensional data. 3 Algorithms In this section, we describe our main algorithm for the problem of unimodal seg-mentation. The algorithm combines in a natural way two previously known al-gorithms: ( i ) the PAV algorithm for unimodal regression, and ( ii ) the dynamic-programming algorithm for k -segmentation. In fact, the algorithm is quite simple: it first applies PAV on the original sequence, and then uses dynamic programming on the resulting unimodal sequence to obtain a k -segmentation. Our algorithm runs in time O ( n 2 k ) for a sequence of n points. In the next section, we prove that this simple algorithm produces the optimal unimodal k -segmentation.
 more detail the PAV and the dynamic-programming algorithms. In addition to the optimal algorithm, we describe in Sect. 3.4 a more efficient greedy-merging heuristic that is experimentally shown to give solutions very close to optimal. In Sect. 3.5 , we also briefly discuss a naive algorithm that can easily be shown to produce the optimal result, but with a prohibitively expensive running time. 3.1 Regression algorithms Computing a monotonic regression of a sequence can be done in linear time by the classic algorithm of  X  X ool-adjacent violators X  (PAV) by Ayer et al. [ 1]. The PAV algorithm is surprisingly simple: It starts by considering each point of the sequence as a separate regression value, and as long as two adjacent values vio-late the monotonicity constraint they are merged and replaced by their weighted average. The process continues until no violators remain. It can be shown that the process computes the optimal regression regardless of the order in which the pairs are merged [ 14 ].
 find the optimal monotonic regression left and right of the top point, and select the best solution from among all the candidates. However, Stout [ 15] was able to devise a linear-time algorithm for the problem of unimodal regression. He real-ized that in the above quadratic-time algorithm some information is recomputed due to independent calls to monotonic regression, and he showed how to cleverly organize these calls to achieve linear-time computation of the regression. 3.2 Segmentation algorithm solved by dynamic programming in time O ( n 2 k ) . The solution is based on com-denotes the error of segmenting the sequence x 1 ,..., x i using p segments. The computation is based on the equation where E [ j , i ] is the error of representing the subsequence x j ,..., x i with one segment. 3.3 The O PT algorithm In this section, we discuss our main algorithm, which we call O PT .Givenase-quence X ,O PT first finds the unimodal regression U of X , resulting in m regres-sion segments r j . The values of the regression segments are then segmented to reduce the number of segments from m to k . Pseudocode is given in Fig. 1. unimodal segments in U are taken to be the output of the algorithm. One can see that the m segments of U form the best k -segmentation. The reason is that, since using more segments can only help, the error of the optimal k -segmentation cannot be greater than the error of the m -segmentation U .However, U is the optimal over all possible segmentations, and, in particular, its error cannot be greater than of the optimal k -segmentation, therefore the two have to be equal. If it is required that the final segmentation has exactly k segments, then we can insert  X  X rtificial X  segment boundaries and increase the number of segments from m to k without changing the error of the solution.
 is smaller than the number of the regression segments, i.e., m &gt; k . In this case, O
PT views the m segments of U as weighted points, and it applies the dynamic programming segmentation algorithm on those points. In the next section, we will show that the resulting segmentation is unimodal, and, in fact, it is the optimal unimodal k -segmentation. In other words, the optimal k -segmentation will never need to combine regression segments into larger segments.
 gression algorithm runs in O ( n ) time, and the dynamic programming algorithm for producing k segments on a sequence of m points runs in time O ( m 2 k ) .Since m = O ( n ) , the overall complexity of O PT is O ( n 2 k ) . In all of our experiments, m was much smaller than n , so in practice the actual running time of O ( n + m 2 k ) also apply the approximate segmentation technique of Guha et al. [ 5] and ob-tain a ( 1 + ) -approximation to the optimal unimodal k -segmentation in time O ( 1 k 2 n log n ) . 3.4 The G REEDY algorithm For large sequences, the quadratic running time of O PT can be a bottleneck. In this section, we describe a more efficient algorithm, called G REEDY . gression and segmentation. The difference is that in the segmentation step, instead of applying the expensive dynamic-programming algorithm, a greedy merging process is performed: Starting with m regression segments, we iteratively merge the two consecutive segments that yield the least error, until reaching k segments. E ( potential merging can be computed in constant time by keeping two precomputed arrays of the sequence: The sum of the values and the sum of squares of the values two consecutive segments in a priority queue. Initially, each point is a segment on its own, and the queue contains the errors associated with merging any two consecutive points. As segments are merged, the entries of the segments adjacent to the newly merged segments are also updated. This structure yields the overall running time O ( n log n ) for G REEDY .
 duce optimal solutions. Consider a simple example of m = 4 mono-tone regression points with values 1 , 2 + , 3  X  , 4 , for vanishingly small , and segmentations with k = 2. The algorithm G REEDY will gener-ate segmentation 1 , 2 + , 3  X  4 or 1 2 + , 3  X  , 4 with error E optimal solution is the segmentation 1 , 2 + 3  X  , 4 with error  X  1. In fact, we have not been able to find a counterexample with an error ratio worse than 2, even when looking at much more complicated situations. Furthermore, in all of our experiments, the ratio of the errors was smaller than 1.2. An interesting open problem is to examine if there exists some guarantee for the quality of the results produced by G REEDY . 3.5 A dynamic-programming alternative For comparison with our main algorithm, we briefly sketch an alternative  X  X rute force X  algorithm. We first explain how this algorithm works for the monotonic segmentation. The idea is to extend the dynamic programming solution of Eq. ( 3), by considering that each segment takes a value from a finite set L ={ l 1 ,..., l N } , sequence x 1 ,..., x i , with exactly p segments, and where no segment exceeds the h th value of the set L , we have the equation where E [ j , i , l h ] is the error of the sequence x j ,..., x i using at maximum the L are only the averages of subsequences of the original sequence, and thus | L |= n ( n  X  1 )/ 2 = O ( n 2 ) . Computing the above equation by dynamic programming gives the optimal monotonic k -segmentation, but it requires time O ( n 4 k ) ,which is prohibitively expensive in practice.
 k -segmentations, one increasing from left to right and one decreasing from right to left. In fact, the computation can be done with no additional overhead by utilizing information stored in the dynamic programming tables (we omit the details). 4 Analysis of the algorithms In this section, we prove some properties of our algorithms, namely that they al-ways produce unimodal segmentations, and that the O PT algorithm is indeed op-timal. Our first lemma is quite intuitive and it shows that the algorithms O PT and G REEDY both produce unimodal segmentations.
 Lemma 1 Let X be a sequence and U a unimodal regression of X . Any possi-ble way of combining consecutive segments of U into larger segments yields a unimodal segmentation for X .
 Proof. We prove the Lemma by induction, by showing that combining any two segments of a unimodal k -segmentation gives a unimodal ( k  X  1 ) -segmentation. Consider merging segments s j and s j + 1 . By our assumption, the subsequences s ,..., s merging, with respect to the top of the regression r t : 3. The top is contained in s j or s j + 1 . All the points in Thus, merging any two consecutive segments gives a unimodal ( k  X  1 ) -segmentation.
 solution.
 Lemma 2 Let X be a sequence and R be an optimal increasing monotonic re-gression of X . For any regression segment r j of R, and any split of r j into a prefix segment r p j and a suffix segment r s j , we have  X  r p j  X   X  r s j . Proof. The Lemma follows from the definition of increasing monotonic regression. that is smaller than the average a s j of the points in the suffix r s j . arate r s j into a new regression segment and assign it level a s j .
 If a s j  X  X  r j + 1 , we can separate r s j into a new segment with level  X  r j + 1 . (  X  r tradicts the optimality of R .
 Theorem 1 Given a sequence X and an integer k, the algorithm described in Sect. 3.3 yields the optimal unimodal k-segmentation.
 Proof. Let R be the unimodal regression computed by PAV and S be the optimal unimodal k -segmentation. We prove the optimality of the algorithm by showing that the segment boundaries in S never split any regression segment r j  X  R . exists a segment boundary b h , such that for some regression segment r j its prefix r j and suffix r  X  s one).
 remain unchanged). In the second case, the error is reduced when the points in r p j Lemma 2,and  X  s h + 1  X  X  s h . This case is demonstrated in Fig. 2, with b h marking the new segment boundary yielding a smaller error than the boundary b h . ment r t does not cause any difficulties, as it will always consist of a single point, and can, therefore, not be divided between segments. If r t contained more than one point, the error of the regression could be reduced by making the highest point into a new regression segment (this would not violate the unimodality constraint). segment boundary that occurs within a regression segment. This means that S was not an optimal segmentation. Thus, the segment boundaries can never split any regression segment. 5 Unimodality detection In the previous sections, we showed how to construct the best unimodal model of a given sequence. Next, we approach the task of deciding how good this model is, i.e., deciding whether the best unimodal segmentation of a sequence is a signifi-cantly better way to describe the sequence than the best unrestricted segmentation. Unrestricted here means again the segmentation with no unimodality constraints. Deciding if a sequence is better described by a unimodal or an unrestricted model is, in effect, equivalent to deciding if the sequence is unimodal or not. tion: Comparing segmentation errors, a method based on permutation tests, and a scoring scheme based on the Bayesian information criterion (BIC). All meth-ods are based on obtaining the best unimodal and unrestricted segmentations, and comparing them to each other. In the first two approaches, we only compare the errors induced by the unimodal and unrestricted segmentations. In the BIC ap-proach, we compare the errors as well as the costs of the two models. Experimen-tal results from applying the three techniques in real sequences are discussed in the next section. 5.1 Comparing segmentation errors One simple way of studying the relationship between the best unimodal and the best unrestricted model is to compare the segmentation errors induced by the two models. For this, we need to fix some suitably large number of segments k .The intuition behind this approach is that if a sequence behaves unimodally, then its best unrestricted segmentation resembles its best unimodal segmentation. In that case, the corresponding segmentation errors are also close to each other. If the ratio of the two segmentation errors is close to 1, we decide that the sequence is unimodal, otherwise we declare it to be nonunimodal. 5.2 Permutation tests Another method to identify the existence of unimodal structure is by performing permutation tests. First, we obtain a unimodal segmentation, again for some large enough k , and then we randomly permute the resulting segments. If the average segmentation error of the random permutations is significantly higher than that of the original sequence, it means that the unimodal structure of the data was lost in the permutation process. For nonunimodal data, permuting the segments should not have a significant effect on the unimodal segmentation error, as the unimodal segmentation was not a particularly good model of the unpermuted sequence. If the ratio of the original error versus the average error or permuted sequences is close to 0, it means that the sequence is better described as unimodal. 5.3 BIC-based scoring We can also use a measure inspired by the Bayesian information criterion (BIC) [6] to assess the goodness of a segmentation. This score has the form B = X  2 l + R , where l is the log-likelihood of the fit. The penalty term R favors simplicity X  X he more complex models will have a larger value of R . The conventional form for the BIC sets R = m log n ,where m is the number of parameters and n is the length of the sequence. The penalty is thus proportional to the number of parameters in the model. When we are dealing with regressions, it is not immediately clear what the value of m should be. Even if we could come up with a relationship between m and the number of segments k , it would not account for the differences in the regression models; any k -segment regression would have the same penalty term, whether unimodal or not.
 quired to encode the segmentation parameters. If there are k segments, the k  X  1 segment boundaries take ( k  X  1 ) log n bits to encode. But since we are interested only in comparing two different models, we will ignore this term, as it is included in all the different BIC scores.
 levels  X  s j . As mentioned in Sect. 3.5 , there are P =| L |= n ( n  X  1 )/ 2 possible segment values. In an unrestricted regression, we choose k values from this set, so the encoding cost will be R free = k log P . In a unimodal regression, the segment levels are first ascending, then descending. This means that we do not need to encode the ordering of the segments, only which group of segment values was used. Making the simplifying assumption that there are k / 2 ascending and k / 2 descending segments, the encoding cost is values x j and x j + 1 are equal, no regression method is going to place a segment fragments , contiguous strings of identical values. If there are s fragments, we sub-stitute P = s ( s  X  1 )/ 2for P in Eq. ( 4). This change has a more pronounced effect in binary sequences, where the number of fragments may be much smaller than the number of bits. 6 Experiments In this section, we present results from applying our unimodality detection tech-niques. We experimented both with real timeseries data and randomly generated algorithm G REEDY . 6.1 Experiments on real data The aim of the experiments was to test methods for measuring unimodality of the data. The three methods described in the previous section were applied on eight sequences of 1600 and 2000 points extracted from exchange-rate data in [ 9]. The idea was to compare the results of our methods between sequences that seemed unimodal by a visual inspection and sequences that did not seem to exhibit uni-modal behavior. The dataset that we experimented on consisted of four sequences that seemed roughly unimodal, and four sequences that did not seem to display unimodal behavior (see Fig. 3). 6.1.1 Comparing segmentation errors As we discussed in the previous section, the first approach is based on comparing the errors of unimodal and unrestricted segmentations. For the sequences shown in Fig. 3, the errors for unrestricted and unimodal segmentations can be found in Fig. 4. There seems to be a clear difference between the unimodal and nonuni-modal sequences. For k = 1, both algorithms give the same result (with the av-erage of the sequence as the only level). When k increases, the unrestricted error starts to deviate from the unimodal error. When k reaches the number of the re-gression segments in the sequence, unimodal error does not decrease anymore, while the unrestricted error decreases up to k = n . With nonunimodal sequences, the unimodal error stays at a level clearly higher than the unrestricted error. For, say, k = 20, the difference between the two types of sequences is already clear (as well as for a wide range of values of k ).
 unimodality of each sequence, we select large enough value of k in order to iden-shown in Fig. 6. For the ratios calculated in Fig. 6,weused k = 20 segments, but as it can be deduced from Fig. 4, similar behavior is exhibited for a wide range of the parameter k . From Fig. 6, it can be seen that, for the data used here, a simple threshold of, e.g, 0 . 3, can be used to classify the sequences: Values higher than the threshold imply unimodality of the sequence. 6.1.2 Permutation tests For the second method of deciding unimodality, we first applied O PT on each data sequence. Then, we permuted the discovered segments to obtain sequences con-sisting of a concatenation of the k segments in a random order, and we applied O
PT on the permuted sequences. We chose k to be 20, and performed 100 iter-ations for each sequence. A histogram of the errors is presented in Fig. 5.Itis clearly visible that the errors for the unimodal sequences deviate more from the permutation errors than those for the nonunimodal sequences. To obtain a statistic similar to the one in the previous experiment, we measured the ratio of the seg-mentation error of the original sequence to the average error for the permutations. The results are shown in Fig. 6. It seems that this ratio also separates the uni-modal and nonunimodal sequences, and we can set a threshold at, e.g., 0.2. Values smaller than the threshold imply unimodality of the sequence.
 standard t -test to see if the original error seems to stem from the same distribution as the permutation errors. The results are also shown in Fig. 6. The larger numbers for the unimodal sequences indicate that they differ significantly from the error distribution of the random permutations. Again, a threshold value can be used to distinguish between the two types of sequences. 6.1.3 BIC-based scoring In the last set of experiments, we obtained the B -optimal segmentations for the sequences. The BIC-like B scores were calculated by the formula B =  X  2 l + 2log P k / 2 ,where P = n ( n  X  1 )/ 2and l is the log-likelihood of the data given by the model. The scores are plotted as a function of k in Fig. 7, along with the values of k that resulted in best scores. The results are similar to those ob-tained by comparing the segmentation errors only. As k increases, the unimodal and unrestricted B scores start deviating more from each other. For the unimodal nonunimodal sequences, the scores are clearly far apart for a large range of dif-ferent values of k . This behavior further validates the intuition that a unimodal model is a good way to model the seemingly unimodal sequences, while for the nonunimodal sequences we should prefer models with no unimodality constraints. stricted score are shown in Fig. 6. The ratio of the scores shows quite clearly that modeling the unimodal sequences with unimodality constraints does not affect the modeling cost significantly. However, for nonunimodal sequences, the choice be-tween the unimodal and unrestricted models has a large impact on the modeling cost. Thus the ratio of the scores seems to separate the unimodal and nonunimodal k , but we can search through all values of k (up to the size of the regression, at most) to find the best B scores, and then compare those. The only parameter left to decide is a threshold value, for choosing which ratios are high enough to have originated from a unimodal sequence. 6.2 Experiments on binary sequences We experimented with randomly generated binary sequences, in effect, random strings of binary digits. We applied the model selection techniques based on BIC-like scoring, as described in Sect. 5.3 . We obtained both the optimal unimodal and unrestricted k -segmentations, and the segmentation with the lowest score B was selected. For brevity, in the following, we say that a string is unimodal if it is best represented by its B -optimal unimodal segmentation, as opposed to its B -optimal unrestricted segmentation.
 these observations and taking the logarithm we get the log-likelihood of the string: The BIC-like score is then specified as B = X  2 l + R ,where R is the encoding cost of the model.
 length 50 bits. Each string was first classified manually. The B scoring, with vary-ing penalty terms R was computed. Strings where the B scoring agreed with the manual classification are shown as  X  X nimod X  or  X  X ther. X  The category  X  X nimod (false) X  indicates strings that were erroneously classified as unimodal, and strings marked as  X  X ther (false) X  should have been unimodal but were not classified as such.
 score; there are eight errors for 43 strings. The other plots show the results for the modified B score based on encoding cost, which performs better. There are three misclassifications in the rightmost plot, and none in the middle one. All mis-takes occur in strings which are not clearly unimodal but not clearly nonunimodal, either.
 line. For example, bimodal strings cannot be represented well by any unimodal re-gression, so their unimodal score is larger than the unrestricted score. Conversely, strings that are clearly unimodal are placed above the diagonal. Since such strings are usually at a constant distance from the diagonal, we are not able to determine if a string is  X  X ore unimodal X  than another. 6.3 Performance of G REEDY We conducted experiments to demonstrate the performance of the algorithm G
REEDY compared to O PT , and to validate two methods for measuring the uni-modality of a sequence. First, we ran both G REEDY and O PT on two different datasets, and measured the average ratio of the errors they produced. Dataset 1 consisted of 11 unimodally-behaving subsequences of 750 points extracted from water-level measurements [ 9], and Dataset 2 from 20 generated random walk se-quences of 2000 points. We observed that the algorithms give results very close to each other, both for the unimodal and nonunimodal datasets. The error ratios small k , and starts to decrease and approaches 1 again. This shows that the greedy algorithm gives very good results in practice. 7Conclusion We have presented two algorithms for the problem of segmenting a sequence into k pieces with monotonicity or unimodality constraints. The first algorithm is an optimal algorithm based on a well-known regression algorithm and on dynamic programming. The second algorithm, G REEDY , is not optimal, but is more ef-ficient and we experimentally verified that it gives results close to optimal. Ad-ditionally, we described three tests for distinguishing if a sequence is unimodal or not: Comparing segmentation errors, performing permutation tests, and BIC-based scoring.
 vided evidence that the suggested algorithms and the unimodality tests perform very well in practice. An interesting open problem is to examine if there exists some guarantee for the quality of the results produced by G REEDY .
 References Author Biographies
