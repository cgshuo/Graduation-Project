 Regression data are often modelled as noisy observations of an underlying process. The simplest assumption is that all noise is independent and identically distributed (i.i.d.) zero-mean Gaussian, work of Gaussian processes [4] is well-suited to these condi tions, for which all computations remain the real world be argued to have originated in the addition of many i.i.d. sources. The random com-ponent in the signal may be caused by human or measurement err or, or it may be the manifestation encountering small quantities of highly implausible data, we require robustness , i.e. a model whose predictions are not greatly affected by outliers.
 cannot explain large non-Gaussian deviations, which eithe r skew the mean interpolant away from favour smoothness and ignore such erroneous data. Figure 1c shows how this can be achieved using a two-component noise model Figure 1: Black dots show noisy samples from the sinc function. In panels (a) and (b), the be-heavy-tailed likelihood (1) in panel (c) is more resilient. Unfortunately, even this model fails for the cluster of outliers in panel (d). Here, grey lines show te n repeated runs of the EP inference algorithm, while the black line and shaded region are their a veraged mean and confidence intervals respectively X  X rossly at odds with those of the latent gener ative model. in which observations y large variance outlier distribution (  X  2 for all but the smallest problems due to the exponential expl osion of terms in products of (1). In this paper, we address the more fundamental GP assumption of i.i.d. noise. Our research is mo-tivated by observing how the predictive distribution suffe rs for heavy-tailed models when outliers eters were taken from the optimal solution to (c), but even wi thout the challenge of hyperparameter optimization there is now considerable uncertainty in the p osterior since the competing interpreta-parably reduced. One simple remedy is to make the tails of the likelihood heavier. However, since the noise model is global, this has ramifications across the e ntire data space, potentially causing single setting will be universally satisfactory.
 The model introduced in this paper, which we call the twinned Gaussian process (TGP), generalizes the noise model (1) by using a GP gating function to choose bet ween the  X  X eal X  and  X  X utlier dis-to hug the data points tightly; more dubious observations ca n be treated appropriately by broaden-ing the noise distribution in their vicinity. Our model is al so a specialization of the GP mixtures proposed by Tresp [1] and Rasmussen and Ghahramani [2]; inde ed, the latter automatically infers the correct number of components to use. One may therefore wo nder what can possibly be gained by restricting ourselves to a comparatively simple archite cture. The answer is in the computational overhead required for the different approaches, since thes e more general models require inference by Monte Carlo methods. We argue that the two-component mixt ure is often a sensible distribution for modelling real data, with a natural interpretation and t he heavy tails required for robustness; its weaknesses are exposed primarily when the noise distrib ution is not homoscedastic. The TGP largely solves this problem, and allows inference by an effic ient expectation propagation (EP) [5] procedure (rather than resorting to more heavy duty Monte Ca rlo methods). Hence, provided a two-component mixture is likely to reflect adequately the noise o n our data, the TGP will give similar results to the generalized mixtures mentioned above, but at a fraction of the cost.
 Goldberg et al. [3] suggest an approach to input-dependent n oise in the spirit of the TGP, in which the log variance on observations is itself modelled as a GP (t he logarithm since noise variance is and fitting the noise process. A further stage of Gibbs sampli ng is required at each test point to Metropolis-Hastings algorithm is suggested for updating h yperparameters. Given a domain X and covariance function K ( , )  X  X  X X X  R , a Gaussian process (GP) over the space of real-valued functions of X specifies the joint distribution at any finite set X  X  X  : where the f = { f matrix , the evaluation of the covariance function at all pairs ( x the posterior distribution over the f , given the observed X and y , which with the assumption of i.i.d. Gaussian corrupted observations is also normally di stributed. Predictions at X marginalizing over f in the (Gaussian) joint p ( f , f Robust GP regression is achieved by using a leptokurtic likelihood distribution, i.e. one whose tails have more mass than the Gaussian. Common choices are the Lapl ace (or double exponential) distri-likelihood over an outlying observation does not exert the s trong pull on the posterior witnessed with a light-tailed noise model. Kuss [7] describes how infe rence can be performed for all these likelihoods, and establishes that in many cases their perfo rmance is broadly comparable. Since it bears closest resemblance to the twinned GP, we are particul arly interested in the mixture; however, bution, which guarantees a unimodal posterior and allows mo re reliable EP convergence. In any inappropriate that our model is most beneficial.
 The graphical model for the TGP is shown in figure 2b. We augmen t the standard process over f the domain between the real and outlier components of the noi se model In the TGP likelihood, we therefore mix two forms of Gaussian corruption, one strongly peaked at with respect to EP updates. The two priors may have quite diff erent covariance structure, reflect-ing our different beliefs about correlations in the signal a nd in the noise domain. In addition, we accommodate prior beliefs about the prevalence of outliers with a non-zero mean process on u , Our model can be understood as lying between two extremes: ob serve that we recover the heavy-tailed (mixture of Gaussians) GP by forcing absolute correl ation in u and adjusting the mean of the u -process to m standard mixture model where independently we must decide t o which component an input belongs. distribution over f whose unnormalized form factorizes into a product of terms, such as a dense Gaussian prior t the approximate posterior as a product of scaled site functions  X  t these sites are usually chosen from an exponential family wi th natural parameters  X  , since in this case their product retains the same functional form as its co mponents. The Gaussian (  X  ,  X  ) has a natural parameterization ( b ,  X  ) = (  X   X  1  X  ,  X  1 exact: Panel (b) shows a graphical model for the twinned Gaussian process (TGP), in which an auxiliary set of hidden variables u describes the noisiness of the data. where Z is the marginal likelihood and z at the global minimum of some divergence measure d ( p k q ) , but the necessary optimization is usu-on a pointwise basis: at each iteration, we select a new site n , and from the product of the cav-ity distribution formed by the current marginal with the omissi on of that site, and the true likeli-hood term t min  X  ment matching between the two distributions, with scale z ments. After each site update, the moments at the remaining s ites are liable to change, and several iterations may be required before convergence.
 The priors over u and f are independent, but we expect correlations in the posterio r after condi-tioning on observations. To understand this, consider a sin gle observation ( x general terms, either u Now, recall that the prior over u and f is and the likelihood factorizes into a product of terms (2); ou r site approximations  X  t Gaussian in ( f seek to match. These are most easily obtained by differentia tion of the zeroth moments Z of each component. We find Z writing the inner Gaussian as N z n The integral for the outlier component is similar; Z efficiency, we make rank-two updates of the full approximate covariance on ( f , u ) during the EP loop, and refresh the posterior at the end of each cycle to avo id loss of precision. Figure 3: Using the twinned Gaussian process provides a natu ral resilience against clustered noisy two, four and five repeated observations at f = 5 . (Outliers in real data are not necessarily so tightly packed, but the symmetry of this approximation allo ws us to treat them as a single unit: by  X  X osterior X , for example, we mean the a posteriori belief in all the observations X  (identical) latent f .) The context is provided by the prior, which gives 95% confid ence to data around f = 0  X  2 . The However, a repeated observation (box two on the left) causes the EP solution to collapse onto the posterior mass). The twinned GP better preserves the margin al distribution of f by maintaining a joint distribution over both f and u : in the second and third columns respectively are contours o f the true log joint (we use a broad zero-mean prior on u ) and that inferred by EP, together with the marginal posterior over f . Only with a fifth observation X  X inal box X  X s the context of f essentially overruled by the TGP approximation. The thick bar in the cent ral column marks the cross-section corresponding to the unnormalized posterior from column on e. 3.1 Predictions If the outlier component describes nuisance noise that shou ld be eliminated, we require at test in-puts x (approximate) posterior The noise process may itself be of interest, in which case we n eed to marginalize over both u f p ( y  X  | x  X  , X, y ) = This distribution is no longer Gaussian, but its moments may be recovered easily by the same method used to obtain moments of the tilted distribution.
 EP provides in addition to the approximate moments of the pos terior distribution an estimate of the marginal likelihood and its derivatives with respect to ker nel hyperparameters. Again, we refer the interested reader to the algorithm presented in [8], adding here only that our implementation uses log noise values on (  X  2 3.2 Complexity quires the inverse of a 2 N  X  2 N positive semi-definite matrix, most efficiently achieved th rough Cholesky factorization (this Cholesky factor can be retain ed for use in calculating the approximate log marginal likelihood). The total number of loops require d for convergence of EP is typically in-dependent of N , and can be upper bounded by a small constant, say 10, making t he entire inference bust regression by EP, which admittedly masks the larger coe fficient that appears in approximating cision matrix in a standard GP can be obtained with a single di vision, whereas our model requires the inversion of a 2  X  2 matrix. We identify two general noise characteristics for which our model may be suitable. The first is when the outlying observations can appear in clusters: we sa w in figure 1d how these occurrences the optima hampers procedures for evidence maximization. I n figure 4 we illustrate how the TGP succeeds where the mixture and Laplace models fail; note how the mean process on u falls sharply in the contaminated regions. This is a stable solution, and h yperparameters can be fit reliably. A data set which exhibits the superior predictive modelling of the TGP in a domain where robust methods can also expect to perform well is provided by Kuss [7 ] in a variation on a set of Friedman [9]. The samples are drawn from a function of ten-dimensiona l vectors x which depend only on the first five components: -10 0 10 We generated ten sets of 90 training examples and 10000 test e xamples by sampling x uniformly in procedure of [7]: ten training points were added at random wi th outputs sampled from N (15 , 9) (a accurately. In a second experiment, the training set was aug mented with two Gaussian clusters each at 10  X  3 . Output values were then drawn from N (0 , 1) for all ten points, to give highly correlated values distant from the underlying function (Friedman (2)) . Now the TGP excels where the other methods offer no improvement on the standard GP; it also yiel ds very confident predictions (cf. Friedman (1)), because once the outliers have been accounte d for there are fewer corrupted regions; furthermore, estimates of where the data are corrupted can b e recovered by considering the process on u . In both experiments, the training data were renormalized t o zero mean and unit variance, and throughout, we used the anisotropic squared exponential fo r the f process (implementing so-called relevance determination), and an isotropic version for u . The approximate marginal likelihood was maximized on three to five randomly initialized models; we ch ose for testing the most favoured. the input (i.e. heteroscedastic). The twinned GP can simula te this changing variance by modulating the u process, allocating varying weight to the two components. B y way of example, the behaviour for the one-dimensional motorcycle set [10] is shown in fig. 5 c. However, since the input-dependent noise is not modelled directly, there are two notable danger s associated with this approach: first, the predictive variance saturates when all weight has been a pportioned to one or other component; second, the  X  X utlier X  component can dominate the variance e stimates of the mixture. This is partic-ularly problematic when variance on the data ranges over sev eral orders of magnitude, such that the  X  X utlier X  width must be comparably broader than that of the  X  real X  component. In such cases, only with extreme values of u can the smallest errors be predicted, but in consequence the process tends to sweep precipitately through the region of sensitivity wh ere variance predictions can be made ac-curately. To circumvent these problems we might employ the w arped GP [11] to rescale the process on u in a supervised manner, but we do not explore these ideas furt her here.
Figure 5: Results for the Friedman data, and the predictions of the TGP on the motorcycle set. With prior knowledge of the nature of corruptions affecting the signal, we can seek to model the noise distribution more accurately, for example by introdu cing a compound likelihood for the outlier component p broadening the likelihood function appropriately. For exa mple, with  X  = 2 , we may write In the former case, the preceding analysis applies with smal l changes: each component of the outlier distribution contributes moments independently. The seco nd model introduces significant compu-uisite moments needed in the EP loop are now intractable, alt hough an inner EP loop can be used to approximate them, since the product of  X  s behaves in essence like the standard model for GP classification. We omit details, and defer experiments with such a model to future work. We have presented a method for robust GP regression that impr oves upon classical approaches by allowing the noise variance to vary in the input space. We fou nd improved convergence on problems which upset the standard mixture model, and have shown how pr edictive certainty can be improved by adopting the TGP even for problems which do not. The model a lso allows an arbitrary process regions which may otherwise be considered erroneous. A gene ralization of our ideas appears as the mixture of GPs [1], and the infinite mixture [2], but both i nvolve a slow inference procedure. When faster solutions are required for robust inference, an d a two-component mixture is an adequate model for the task, we believe the TGP is a very attractive opt ion.
 References
