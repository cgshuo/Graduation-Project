 This paper proposes GBM ( gravitation-based model ), a physical model for information retrieval inspired by Newton X  X  theory of gravitation. A mapping is built in this model from concepts of information retrieval (documents, queries, relevance, etc) to those of physics (mass, distance, radius, attractive force, etc). This model actually provides a new perspective on IR problems. A family of effective term weighting functions can be derived from it, including the well-known BM25 formula. This model has some advantages over most existing ones: First, because it is directly based on basic physical laws, the derived formulas and algorithms can have their explicit physical interpretation. Second, the ranking formulas derived from this model satisfy more intuitive heuristics than most of existing ones, thus have the potential to behave empirically better and to be used safely on various settings. Finally, a new approach for structured document retrieval derived from this model is more reasonable and behaves better than existing ones. H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms, Experimentation, Theory Information retrieval models, Gravitation-based model, theory of gravitation, mass estimation, structured document retrieval Information retrieval (IR) models, which define the representation of documents, queries, and the relevance relationship between past several decades, many categories of IR models (and their variants) have been proposed and studied [2], including Boolean models, vector space models [3][4], probabilistic and logic models [10][14][6][1], and language models [12][13][7][24], etc. The key behind all the models is the primary perspective on information retrieval. The Boolean model views IR problems from the perspective of set theory and Boolean algebra, while the perspective used in the vector space model is vector and linear algebra. Most of other categories of models take the probabilistic perspective, which is the most dominating perspective on information retrieval today. It may be extremely hard to answer questions like  X  X hat is the essence of information retrieval X , and  X  X hat is the right perspective of it X . However, it is clear that, till now, we know more about information retrieval each time when a new perspective is adopted. It would also be helpful to view information retrieval from more new perspectives. Although many of the models (and the formulas and algorithms derived from them) have been successfully applied to various tasks, there are still some problems faced by them: First, the retrieval formulas (formal or ad-hoc) conducted by most IR models fail to satisfy even some basic intuitive heuristic constraints [5]; Second, the retrieval formulas derived or motivated from many IR models commonly lack intuitive interpretations, especially physical interpretations. At the same time, we are living in a physical world which is dominated by fundamental physical laws. Can we get help from  X  X he God X  in acquiring deeper understanding of information retrieval? In this paper, we try to view information retrieval from the perspective of physics, a quite different perspective from existing ones. We propose a new framework which models documents, queries, and their relationships using basic concepts in physics. In particular, documents and queries are modeled as objects with specific structures; and the relationship between a query and a document is modeled as the attractive force between them. A basic rule used here is Sir Isaac Newton X  X  theory of gravitation (see Section2.1 for a brief introduction of it), a fundamental law of the universe. The primary goal of the model is to help learning more about information retrieval from a new perspective. It is encouraging that we can really benefit from the nature. With the new perspective and model, we get the following preliminary achievements, 1. We have derived a family of effective ranking formulas 2. The BM25 term weighting function [9][11] can be easily There is a small issue for the TDC constraint, which will be discussed in Section 3.2.4.3. 3. A more reasonable approach for structured document In this paper, we will examine the gravitation-based model theoretically and empirically. We first give some background knowledge and related work in section 2. In section 3, the GBM model will be introduced and analyzed theoretically. The performance of the model is tested by some experiments in section 4. We conclude the paper in Section 5. In this section, we will first give a brief introduction of Newton X  X  theory of gravitation, upon which our model is built. And then some related work is discussed. Gravitation is one of the four fundamental forces of nature. It governs the motion of stars and plays a crucial role in most processes on the earth. Newton proposed in 1687 his famous law of gravitation which demonstrated that any two objects in the universe have attractive force between them. For two particles with masses m 1 , m 2 respectively and with distance d between them, the gravitational force between them can be expressed as follows, where G is a constant called the universal gravitation constant. And the direction of the force is along a line between the two force between two spheres can be viewed as all their masses are concentrated in their centers. All IR models have their primary perspectives on information retrieval problems. Some ranking formulas for retrieval are commonly derived or motivated from the models. The term weighting function , which defines the score of a document given one query term, is the most important part of a ranking formula. The followings are a brief overview of some categories of IR models and the most effective term weighting functions for them. In the vector space model [3][4], each document is represented as a vector of terms, so does a query. And the relevance is measured by the similarity (e.g. cosine of angle) between the query vector and the document vector. Many term weighting functions have been proposed upon this model (and its variants), among which the pivoted normalization weighting formula [4] seems to be an outstanding one, where s (between 0.0 to 1.0) is a parameter. Please see Table 1 (section 3.1) for the notations used in the above formula. The probabilistic model [10][14][6][1] formulates the IR problem in a probabilistic framework, which gives the relevance of a document and a user query by estimating the probability that the document is exactly what the user needs. Variants of probabilistic models include Bayesian networks, inference network models, belief network models, etc. Please refer to [8] for an overview of them. Okapi X  X  BM25 formula [9][11] is shown as one of the most effective and robust ranking formulas in this category (and even in commonly used simplification [9][11][16] is, has the potential  X  X egative IDF X  issue, as has been discussed in [5]. Like in [5], we will use )) ( / ) 1 ln(( t df N + as the expression of w ( t ) in the following part of the paper. The above formula is first discovered by Robertson et al [9][10][11], inspired by the shape of a complex formula derived from a probabilistic model under the 2-Poisson assumption. Amati and Rijsbergen propose in [1] a probabilistic framework for generating nonparametric term weighting functions. They claim that the BM25 function with some special parameters ( k b =0.75; or k 1 =2, b =0.75) can be approximated numerically by one of their generated functions I(n)L2 (with k 1 =1 and 2 respectively). By following these brilliant works, we try to derive BM25 from our model and give it a physical explanation. The language model [12][13][7][24] also adopts a probabilistic framework. However, different from traditional probabilistic models, it interprets the relevance between a document and a query as the probability of generating the query from the document X  X  model. Smoothing, which adjusts term probabilities to overcome data sparseness, is critical to the performance of language models. Among various smoothing methods, the Dirichlet prior smoothing seems to be discussed frequently, where ) | /(| | |  X   X  + = D D , and ) | ( C t P And  X  is a parameter whose value is commonly set to be multiples of the average document length. structured document retrieval naturally and effectively. So another kind of work related to ours is structured document retrieval. A document is said to be structured when it contains multiple fields. Document X  X  field structure is commonly used to improve retrieval performance in practice. The most commonly used approach for structured document retrieval may be score/rank (linear) combination [15][18][19][20], which treats each field as a separate document and computes scores/ranks for them. In computing scores for each field, any ranking function for unstructured document retrieval can be adopted. Robertson et al [16] introduced, as an extension of the BM25 formula, a simple and efficient method which combines term frequencies instead of field scores. Ogilvie et al [17] proposed, within the language model framework, another approach which develop a separate language model for each field and then fix them linearly. These approaches essentially combine term frequencies instead of scores. We will discuss some issues with the above (two kinds of) methods and their relations with ours in Section 3.3. The gravitation-based model (GBM) tries to understand the IR problem within the physical framework. To achieve this, we first build a mapping from the concepts in information retrieval to those in physics (Section 3.1). And then a basic model (with two versions: discrete and continuous) is introduced and analyzed (Section 3.2). Then, in Section 3.3, we will show how GBM supports structured document retrieval naturally and effectively. Finally, two possible future extensions for GBM are briefly discussed. As in some other IR papers, we use D , Q , C to denote a document, a query, and the whole collection respectively. Some other commonly used notations are listed in Table 1. In the GBM model, a term (word) in a document is viewed as a physical object composed of some amount of particles. A particle has two attributes with it: type, and mass. The type of a particle is determined by the term it composes. For different occurrences of the same term, their corresponding particles have the same type. Two particles of the same type have some kind of gravitational force between them and so they are mutually types. For two particles P 1 , P 2 with mass m 1 and m the gravitational force between them is (by formula 2.1), where d is the distance between the two particles, and G is a constant. A term is an object composed of particles with a specified structure (or shape). Two shapes (see Figure-1) will be discussed in this paper: the sphere, and the ideal cylinder. An ideal cylinder viewed as a line segment. There are three attributes related to a term: type, mass, and diameter. For a sphere-shape term, its diameter is defined naturally as the sphere X  X  diameter. While for a term with an ideal cylinder shape, its diameter is defined as its height (see Figure 1(b)). The mass and diameter of a term t in document D are term a radius (denoted by r ( t , D )) as the half value of the diameter (see Table-2 for a list of notations). The two kinds of term shapes will be used respectively by the discrete and continuous versions of GBM (Section 3.2). (a) Sphere-shape terms (b) Ideal-cylinder-shape terms A document is modeled as a higher level object (than a term) composed of a list of terms objects. There are two categories of term objects in each document: One category, called explicit (or document. The other category is called implicit (or hidden) terms which are not occurred in the document. Implicit terms are used to represent the hidden meaning of the document. All the hidden terms in document D are denoted as H ( D ). We use |D| and | H ( D )| to represent the number of explicit and implicit terms of D respectively. As a result, the total number of terms of the whole document can be expressed as | D |+| H ( D )|, or | ) ( | D H D U It is reasonable and natural to assume that the mass of a document is the total masses of all its seen and hidden terms, The fundamental logic behind is that the idea of a document is expressed by and only by all its seen and hidden terms. As a document is modeled as a list of terms, we can also define a diameter for it. The diameter of a document is (naturally) defined as the sum of the diameters of all its terms. That is, 
Figure 1. Term and document objects in the GBM model Similar to the modeling of a document, a query is modeled as an object composed of its terms (no hidden terms are assumed for simplicity). Assume that each query has unit mass 2 . The relevance of a document given a query is modeled as the force between the objects corresponding to them. According to physics principles, the force between two objects is determined by their shapes, their masses, and the distance between them. Here we describe two basic versions of gravitation-based model in which only TF, IDF, and document length is considered. In the discrete version, terms are modeled as spheres; while in the continuous version, each term is represented as an ideal cylinder. natural order in the document, its structure is assumed to be changed under the attraction of a query in this model, as is illustrated in Figure 2. In the figure, spheres labeled with  X  X   X  X   X , etc are the occurrences of query terms in the document, while query Q). Under the gravitational attraction of query Q , the terms which feel larger gravitational forces will get nearer to the query. Given a query, each document has an optimized term placement where the aggregated force between the query and the document is maximal. In this model, the relevance of a document given a query is defined by the attractive force between them when the document is in its optimal-term-placement state. Now we have transformed the relevance computation in IR into a term placement optimization problem. It is clear from Figure 2 that the maximal (optimized) gravitational force between a query term t and document D (using formula 3.1) is, Where l is the distance between the query and the document (see Figure 2). According to the principle of the composition of forces, the force between D and Q can be expressed as follows, Therefore we get the expression of relevance between D and Q . See our technique report [21] for a detail discussion about it. Here we introduce the continuous version of our model, by representing terms as ideal cylinders of particles (see Section 3.1.2 and Figure 1). The relevance of a document given a query is still defined by the attractive force with optimal placement (the attractive force between a query term t and document D can be expressed as follows (by using formula 3.1), The above formulas (3.4 and 3.6) can not be used directly for We will (in Section 3.2.4) deduce the ultimate formula for retrieval after making estimations (in Section 3.2.3) for the mass and diameter of a term in a document. To simplify the model and make the deduction process concise, we give some simplicity assumptions on which the estimation is based. Please note that the model itself does not rely on these assumptions. As documents vary in their masses and lengths, the same term may have different mass values in different documents. However, we can define a document-independent mass for each (type of) term, ) ( t m , which denotes the average mass of term t in the whole collection. We can also consider ) ( t m as the global importance of term t in the considered collection. We then simply make the following simplicity assumptions, Assumption 1 : For any two terms, their mass ratio in any document is equal to the ratio of their average masses in the whole collection. This can be formally expressed as Assumption 2 : The average mass of a term depends on and only on the inverse document frequency. Using the IDF expression in Formula 2.2, we have Assumption 3 : The average global importance of all terms in a document is constant (i.e. independent of the document itself). This assumption implies that most documents are less likely to contain only weighty or light terms, but have more chance to contain terms with various masses. Formally, Now we try to estimate the mass of a term in a document. By combining formula 3.2, 3.7, and 3.9, we get, The following assumptions are for estimating the diameter of a term in a document. Assumption 4 : The number of hidden terms in a document relies independent of the document itself. That is, As | ) ( | | | | ) ( | D H D D H D + = U , we have the following formula, where ) 1 /( 1 Also assuming all terms in the same document have equal diameters for simplicity and according to formula 3.3, we have, Now we analyze the proposed model using the mass and diameter estimation results in the previous sub-section. For the continuous model, by applying formula 3.10 and 3.13 to formula 3.6, we have, where
D | | ) 1 ( ) (  X  +  X  =  X   X   X  . And the physical meaning of ) ( D the diameter per term when document D have average document length. Similarly, formula 3.4 for the discrete model can be transformed as (by applying formula 3.10 and 3.13), Formula 3.14 and 3.15 are the final term ranking functions derived from our GBM model. expression ) ( D  X  and ) ( D  X  are constant accordingly. In this condition, it is easy to prove that formula 3.14 is equivalent to formula 2.3, the (simplified) BM25 term weighting function. Note that ) ( / D l  X  and  X  in formula 3.14 are corresponding to parameter k 1 and b in formula 2.3 respectively. And ignored (because it is a constant). Formula 3.15 also has a close relationship with BM25. Please refer to our technique report [21] for details. So far the gravitation field function is inverse-square (1/x However, other gravitation fields can be considered. By similar process with formula 3.14, we can get a family of formulas (see Table 3) for the continuous basic GBM model, corresponding to different force field functions respectively. Among them, the Okapi formula (corresponding to the case of pow=2) is the simplest and most easy-to-compute 3 one! Table 3. Selected term weighting functions derived from GBM 
Gravitatio nal-Field-Function ( 1  X  pow )  X  In [5], Fang et al proposed some heuristic constraints related to TF, IDF, and document length that all reasonable ranking formulas should follow, namely TFC1, TFC2, TDC, LNC1, LNC2, and TF-LNC. As there is a small issue for constraint TDC 4 , so we replace it with M-TDC here. It can be checked that each formula in Table-3 satisfies all the above heuristic constraints. The details of analysis are omitted here due to space limitations. Experimental results (see Section 4) indicate that the formulas derived from the basic model have high performance. In the basic GBM model, a document is treated as a bag of words, and the relevance of a documents respected to a query is examined by only considering TF, IDF, and document length. However, the power of our model is more than this. In this subsection, we show that GBM can support structured document retrieval easily, formally and in a natural way. Only the four fundamental operations of arithmetic (addition, subtraction, multiplication, and division) are needed to compute it. The TDC constraint in [5] says  X  X et q be a query and w be two query terms. Assume | d 1 |=| d 2 |, c ( w c ( w 1 , d 2 )+ c ( w 2 , d 2 ). If idf ( w 1 )  X  idf ( w c ( w 1 , d 1 )=10, c ( w 2 , d 1 )=0, c ( w 1 , d 2 )=5, c ( w idf ( w 1 ) is only slightly larger than idf ( w 2 ). By applying TDC, we intuitive (supported by TFC1 and TFC2) that f ( d
So there should be a small bug in TDC. This constraint would become reasonable if condition  X  c ( w 1 , d 1 )+ c ( w + c ( w 2 , d 2 ) X  was replaced by  X  c ( w as M-TDC (modified TDC) here. In modeling structured documents, the fundamental principles are the same as those in the model for unstructured documents. They are 1) Terms will compete for places and the ones have larger gravitational forces will get nearer to the query terms, and 2) The relevance of a document given a query is defined by the maximal gravitational force between them among all possible term placements. different kinds of fields may have different masses. The mass of a term in a field F can now be computed by a formula similar with Formula 3.10, but with document D in it replaced by field F . In Figure 4 we explain how our model deals with document structures. In the figure,  X  X  1 ,f 1  X  etc represents term t the document, and two kinds of filling styles for rectangles represent two fields respectively. And because masses of terms in field 1 are larger than those of field 2, field 1 X  X  terms are nearer to the query. The optimized term placement for document D is shown in the figure. Please note that terms in different fields may have different diameters as well. Note that this is NOT the only structured document retrieval approach that can be derived from our model. But it is in the most simplest and easy-to-understand ones. As introduced in Section 2.3, most methods for structured document retrieval adopt some kinds of combination over the fields: score/rank combination, or term frequency combination. Robertson et al [16] argued that score combination  X  X an be quite dangerous X  and they prefer TF combination. We agree with their discussions about score combination. For a multi-term query, with score combination, a document matching a single query term over many fields could get unreasonably higher score than another document which matches all the query terms in a few fields. Score combination is not reasonable at this aspect. However, the results of TF combination may also be unreasonable in some conditions. Please see the following example (assume the TF combination method in [16] is used): Assume four documents d 1 , d 2 , d 3 , d 4 with equal length, and each combination method) are assumed to be 5 and 1 respectively. Also the four documents are as follows, Now consider the relevance of the documents given a query containing only one term t . By TF combination, the score of d small than that of d 2 (since 1*5+0 &lt; 0*5+6). This may be reasonable. But if applying the same TF combination method to d This is not coherent to common sense, because it is clearly that the score of d 3 (which contains term t in its high-weight field, and enough times of the same term in its low-weight field) should be larger than that of d 4 (which does not contain t in its high-weighted field at all). From the above discussion, we see that both score combination heuristic constraints. Although they may have been su ccessfully applied to many datasets and tasks, it is really dangerous to adopt them in all conditions. We DO need some heuristic constraints for the retrieval of structured documents, just like those proposed by [5] for unstructured documents. In contrast to the two existing methods, the approach derived from our model is immune from the above two issues. We are not saying here that our method will satisfy all the potential heuristic constraints in the future. But, as our model is originated from basic physical concepts and laws, it can be hoped to have more chances to obey human intuitions. The retrieval performance of our approach will be tested by experiments in section 4. In this subsection, we illustrate briefly two potential extensions to the GBM model: term proximity, and the combination between relevance scores and static document ranks. Till now, term proximity information is omitted in our model. However, it is intuitive that adjacent query terms in a document should have larger gravitational forces (with the query) than distant ones. So our model is promising to provide a clear explanation of term proximity. In previous analysis and discusses, document mass m ( D ) (see formula 3.14 and 3.15) is simply set to be constant. However, the mass of a document is a measure of its quality, which depends on how informative and important the document is. So document mass may have some strong relationships with static document ranks (e.g. PageRank [22] in Web search). We plan to exploit this in the future. In this section, we verify the analysis of previous section and test the performance of our model by some experiments performed on standard datasets and queries. We performed experiments on two corpora and seven query sets (see Table 3 and 4) used from TREC [23] 2000 to 2004. All the experiments are performed on an information retrieval platform built by our organization. In parsing documents, image alt information is extracted as part of body-text, while meta keywords and descriptions are discarded. A na X ve word breaker (which treats characters other than letters and digitals as punctuations) is adopted to separate document text as terms. All terms are indexed, i.e. no stop-words are removed. In the processing of document and query terms, the Porter stemmer is used for stemming. To evaluate the query results, we use mean average precision (MAP), since it applicable to all the tasks to be performed here. In tuning parameter to optimize an evaluation measure, we use a grid search method similar with that described by Robertson et al in [16],  X  X valuate the performance of a system on a successively smaller grid over the set of parameters being optimized, until an adequate minimum-step value is reached X . In this subsection, we compare the formulas derived from our basic model with other most commonly used formulas. Although quite a lot of term weighting functions have been proposed, those as most effective ones in their categories respectively. We would not like to compare our formulas with the earliest VSM or probabilistic formula or one of the language model formulas without smoothing to get a  X  X eems-like X  significant performance improvement. The formulas participated in comparison are VSM-Piv (the pivoted normalization VSM formula, see Formula 2.2), LM-Dir (the language model formula with Dirichlet prior smoothing, see Formula 2.4), GBM-Std (formula 3.14 derived from our model, which is equivalent to the BM25 formula), GBM-Std-Disc (formula 3.15 derived from our discrete model), GBM-Inv (the GBM formula derived by gravitational field function x / 1 , see Section 3.2.4.2), and GBM-Exp (the GBM and we use mean average precision as the performance measure. From Table 6, we can see that the performances of the ranking formulas listed are comparable, and GBM-Std (i.e. Okapi) performs slightly better than VSM-Piv and LM-Dir. These results give more confidence that the formulas derived from our basic model are comparable to or better than state-of-the-art high performance ones. In addition, as our formulas satisfy all the heuristic constraints proposed (while most of others do not), they are hoped to be robust on various kinds of datasets and tasks. Please note that the average precision for some tasks is lower than the top results in TREC submissions. This is because only body text is used here. We now test our model X  X  performance over structured document retrieval. A trivial way to process a structured document is to merge all fields into an unstructured one. This is used as our baseline for comparison. In additional to our approach, we also implemented two other approaches analyzed in Section 3.3.2: linear score combination (referred to as ScoreComb), and linear TF combination over Okapi (FreqComb). Figure 5 shows the performance comparison over some tasks when combining body and title fields. In the experiments, the scores for the baseline, ScoreComb, and FreqComb are all generated by the BM25 formula (or, equivalently, GBM-Std) having parameter b =0.75 (which is the optimal or suboptimal the parameter  X  is fixed as 0.75 accordingly. Given the fixed value of b , the baseline score is acquired (as the optimal score) by tuning parameter k 1 , while the scores for ScoreComb, FreqComb, and our approach in the figure are got by tuning k 1 (or ) ( D  X  for our model) and the weights of fields. From the figure, we can see that, our model is highly effective in dealing with structured documents. In this paper, a gravitation-based IR model (GBM) was proposed, by adopting a physical perspective on information retrieval. In the basic version of GBM, each document is viewed as a bag of words and only TF, IDF, and document length is considered in modeling. The basic GBM model can not only give a physical interpretation of existing term weighting functions (e.g. BM25), but also derive new effective ranking formulas. The experimental results over some standard datasets and tasks show that the formulas derived from the basic GBM are among the most highly performance ranking functions proposed so far. In addition, as the derived term weighting functions satisfies all the heuristic constraints proposed in [5], they are more reasonable and hoped to be robust on various conditions. If document structure is considered, a novel approach for structured document retrieval can be naturally conducted from our model. The advantage of the approach is analyzed and tested by heuristics and experiments. Most dominating IR models (including probabilistic models, language models, inference network models, etc) adopt a probabilistic perspective on information retrieval. Great successes have been and are being achieved with this perspective. However, we believe that viewing the problem from a different viewpoint is the same important as going deeper from traditional perspectives. This paper may be a first step to take a physical viewpoint. And we found that this new perspective gives us some insights for information retrieval. As a totally new formal model, there are a lot of thing to do with it. First, as the derivation of the BM25 term weighting function from our model hints some internal (unknown) relationship between our model and other models, it may be interesting and necessary to study the relationship and possible combination between our model and existing models. Second, as pointed out in Section 3.4, this model has potential to include term proximity and static document ranking (which are extreme important in Web search) in its framework. Some work is needed to achieve this in the future. Finally, it is unclear whether and how the model can support some IR techniques, e.g. relevance feedback. We would like to thank Guomao Xin, Ni Lao, and the anonymous reviewers for their helpful suggestions. [1] G. Amati and C.J.V. Rijsbergen. Probabilistic models of [2] K. Sparck Jones and P. Willett, editors. Readings in [3] G. Salton, A.Wong, and C.S. Yang. A vector space model for [4] A. Singhal, C. Buckley, and M. Mitra. Pivoted document [5] H. Fang, T. Tao, and C. Zhai. A formal study of information [6] N. Fuhr. Probabilistic models in information retrieval. The [7] C. Zhai and J. Lafferty. A study of smoothing methods for [8] R. Baeza-Yates, and B. Ribeiro-Neto. Modern Information [9] S.E. Robertson and S. Walker. Some simple effective [11] S. E. Robertson, S. Walker, and M. Beaulieu. Okapi at [12] J. Ponte and W.B. Croft. A language modeling approach to [13] F. Song and B. Croft. A general language model for [14] S.K.M. Wong and Y.Y. Yao. On modeling information [15] W.B. Croft. Combining approaches to information retrieval. [16] S. Robetson, H. Zaragoza, and M. Yaylor. Simple BM25 [18] R. Wilkinson. Effective retrieval of structured documents. In [19] M. Lalmas. Uniform representation of content and structure [20] S.H. Myaeng, D.H.Jang, M.S. Kim, and Z.C.Zhoo. A [21] S. Shi, J.R. Wen, Q. Yu, R. Song, and W.Y. Ma. Gravitation-[22] L. Page, S. Brin, R. Motwani, T. Winograd. The PageRank [23] TREC main page: http://trec.nist.gov/ [24] B. Croft and J. Lafferty, editors. Language Modeling for 
