 This paper describes a new test collection for passage re-trieval from multilingual, informal text. The task being modeled is that of a monolingual English-speaking user who wishes to search discussion forum text in a foreign language. The system retrieves relevant short passages of text and presents them to the user, translated into English. The test collection contains more than 2 billion words of discussion thread text, 250 queries representing complex informational search needs, and manual relevance judgments of forum post passages, pooled from real systems. This information re-trieval test collection is the first to combine multilingual search, passage retrieval, and informal online genre text.
The DARPA Broad Operational Language Translation (BOLT) program was  X  X imed at enabling communications with non-English-speaking populations and identifying in-formation in foreign-language resources. X  X 2] As part of the program, performers were tasked to complete a multilingual passage retrieval task: in response to a textual query in En-glish, retrieve relevant passages from English, Arabic and Chinese discussion forums, translated into English.
To evaluate this task, we constructed a test collection of 2 million discussion forum threads totalling nearly 70 million posts and 250 search topics with free-text information need statements. The information needs represent users looking for information about, relationships between, effects of, and opinions about current events and socially prominent sub-jects. Systems participating in the BOLT program retrieved short passages in response to each query and translated them into English. These passages were manually judged for rele-vance by bilingual speakers with access to the original doc-uments in their original language. The first 100 topics have an average of 185 judged passages each, and the latter 150 topics have an average of 486 judged passages. The com-bination of multilinguality, passage retrieval, and informal online discussion text makes this test collection unique.
Passage retrieval has been extensively studied in informa-tion retrieval [4], information extraction [9], and question answering [7]. As shown in the TREC HARD track [1], passage retrieval tasks can be complicated to specify. They also present a challenge to reproducibility: unless systems retrieve the exact same passages as were judged, interpreting passage-level relevance judgments is not straightforward.
The BOLT IR task is to retrieve short relevant passages of text from informal text in response to a natural language En-glish sentence representing a complex information need. We imagine an English-speaking intelligence analyst searching across multiple languages. The analyst is looking to survey diverse views and different relationships among people, both document authors and people mentioned in the documents. Because the documents in the collection are in multiple lan-guages, passages in languages other than English must be translated to English for the end user.

We have found it helpful to describe this task as an imag-inary interface for the analyst that presents retrieved snip-pets in context; the user can select a snippet to drill down to the full document, and switch back and forth between the translated text and the original (presumably with a na-tive speaker or linguist assisting). Because the user views the retrieved passages in context, it is not critical that pas-sages exactly contain just the relevant information, nor do they need to disambiguate or coreference things mentioned in the passage. Because the native language is one step lower in the drill-down, translation quality is inherent in the task and affects the interaction. This perspective was reflected in the BOLT IR evaluation which did not require passages to contain only relevant content, and which measured re-trieval effectiveness separately from translation quality. Al-though we do not report details of the evaluation in this paper, these considerations are important for understand-ing the relevance assessments.
The document collection is a set of two million online dis-cussion forum threads collected by the Linguistic Data Con-sortium (LDC) containing 2.3 billion words of text. The threads are available in the original HTML and also in a cleaned XML format which was used in all stages of building the collection. The threads come from a number of different forums on different subjects. The threads come from three different identified language sources: English, Arabic, and Mandarin Chinese. The Arabic subcollection is intended to word. target the Egyptian Arabic dialect, but the posts often con-tain Modern Standard Arabic and/or other Arabic dialects.
From a total forum crawl of roughly 3 billion words, LDC selected a subset of roughly 700 million words from each language to form the final collection. Table 1 gives statistics on the size of the collection.
The user X  X  information needs that give rise to the queries are called  X  X opics X . The topics were developed by LDC an-notators, which included native speakers of Mandarin and Egyptian as well as English speakers. The topics were devel-oped using a collection exploration process: the annotator thought of a topic and did preliminary searches to deter-mine whether the topic had any relevant information in the collection. The annotator then formulated a query for their topic, and surveyed the relevant threads that were returned by a basic text search tool. Based on this survey, they made a qualitative judgment about the prevalence of relevant in-formation in the corpus, and developed topics where their judgment was that the topic was not likely to be highly pro-ductive for the corpus. As part of the query, the annotator could indicate that they only wanted information in Arabic, Chinese, or English, or would accept results from any lan-guage. This factor was a constraint on relevant information.
After arriving at a right-sized topic, the annotator wrote a textual description and rules of interpretation for assess-ing relevance. The topics were classified according to two informal taxonomies: the subject of the topic ( X  X sks-about X , which could be a person, location, organization, movement, event, or abstract entity, . . . ), and the kind of informa-tion desired ( X  X sks-for X : which could be statements or opin-ions, relationships, effects, information about, participants or members of, . . . ). The annotator also noted from which languages they expected the majority of relevant informa-tion to come, based on their searching. In contrast to the language-target restriction in the query, this expectation was not a restriction on relevance; relevant information could come from any language, as long as the query allowed it. Lastly, the annotator included one or more examples of rel-evant passages from their search. Figure 1 shows an exam-ple topic. Systems only had access to the topic number, the query , and the language-target information, but the com-plete topic statement was developed ahead of time and used for reference during assessment.

There were three BOLT IR evaluations over the course of three phases of the program. The first phase was highly exploratory, whereas the latter two, which we call P2 and P3, resulted in comparable collections following roughly the same rules for topic development and relevance assessment. In phase 3, topics tended to have more relevant passages due to the pooling procedure used (see below), but they actually have a greater variance in relevance because effort was put towards creating topics in P3 with small ( &lt; 100 passages) relevant sets.
In response to the topic, evaluation systems automatically retrieved a ranked list of up to 1000 citations from the doc-ument collection. A citation is defined to be an English passage of no more than 250 characters, with a pointer back to the original source text. In Figure 1, the thread , post , offset , and length is the pointer for first example citation. System outputs followed this format, with threads and posts counting from 1 and offsets starting at 0 within the cleaned XML formatted collection. Citations were required to be English, and so Arabic or Chinese passages needed to be translated before they could be returned to the user; sys-tems conceivably could do this at any time prior to return-ing results, including automatically translating the entire corpus at the start. Citations could not cross post or thread boundaries.

We combined the top 100 citations from each evaluation system to form a pool.[8] Pooling usually drops duplicated documents, but in this case, we planned for the assessors to judge the citation text as well as the pointer, so many close duplicates needed to be kept in the pool. To make it eas-ier for the assessors to be consistent across near-duplicate cases, we combined all citations with greater than 95% to-ken bigram overlap into equivalence classes. Neither near-duplication nor relevance are actually transitive, so assess-ments made to equivalence classes were hand-checked to make sure we didn X  X  propagate judgments too far.

The pooling method can only be as effective as the diver-sity of the constituent system outputs, and indeed in phase 2 we were concerned that because the evaluation only has a limited number of participants, the estimates of recall we computed were biased high. For phase 3, we solicited a range of system outputs from each evaluation participant, includ-ing baselines which would not be measured but which served to enrich the pool. This resulted in nearly twice the num-ber of relevant citations compared with phase 2. Because this occurred despite the aforementioned effort to limit the number of relevant citations in phase 3, we are hopeful that the phase 3 relevance judgments represent a more reusable collection with better recall estimates. Section 7 discusses recall further. &lt;topic number="BIR_300158"&gt; &lt;/topic&gt; clauses.
Rather than making a simple relevance judgment for each citation, the assessor followed a decision-points annotation model [5] derived from the application scenario described in section 2, in order to tease apart relevance from translation quality. [3] First, the assessor decided whether the citation was clear enough to make a relevance judgment, or if they felt they probably understood the citation but wanted to re-fer to the original language text, or if the citation was com-pletely incomprehensible. Incomprehensible citations were not assessed further.

For comprehensible citations, the assessor judged whether the citation satisfied the topic X  X  rules of interpretation for relevance, and further whether the citation provided useful information to the user. Citations were judged to be not useful if for example they merely restated the query. If the assessor needed to refer to the original language citation, ei-ther because they wished to see greater context or wanted to confirm their understanding of the translation, they decided whether the original Mandarin or Arabic text was relevant and useful as above, and then returned to the translated ci-tation to make a judgment of relevance for the citation itself. In phase 3, the assessor further noted whether they felt they were being generous in their judgment.

This multi-stage process allowed the assessor to separately consider translation acceptability from relevance. Resources did not permit us to do deep assessment of translations, as might be expected of an MT corpus, but by folding transla-tion acceptability into the relevance assessment process, it is possible to subset the data to use only the most acceptable translations or to allow relevance within the original citation context.

In the BOLT evaluation, systems received credit for cita-tions that were understandable, relevant and useful, even if the source language needed to be checked to make that judg-ment. Having these fine-grained assessments with a rule for giving credit means that later users of this data can choose to give credit for stricter or looser scenarios.

We presented a topic X  X  citation pool to the assessor in random order to avoid system bias effects. A single citation was judged for all citations in a near-duplicate equivalent class, and equivalence classes were checked for consistency in a second pass.

The assessor judging the citations for a particular topic was not necessarily the topic X  X  creator. In particular, cita-tions that came from Mandarin or Arabic were assessed by a native speaker. A subset of the topics were selected to be fully assessed by a second assessor.

In phase 3, the assessor additionally noted whether the source text for Arabic citations included Egyptian Arabic or not. This annotation was made following the realization that Egyptian writers frequently code-switch between dialectal and standard Arabic, and that the line between a passage being in Modern Standard Arabic or Egyptian dialect was hard to draw. Although the assessor did not mark the code-switch boundaries, we hope that this data might be useful for training systems to differentiate the two dialects of Arabic.
The BOLT IR test collections are straightforward to use to measure the effectiveness of post and thread retrieval. Mea-suring passage retrieval is more complex, because retrieved passages may not directly correspond to the passages that were judged by our assessors. Also note that the assessor did not judge the source language text when the translated citation was sufficient. In general, the reusability of passage-level assessments is not well understood and deserves more study.

A more significant issue is assessment coverage and recall estimation. Because our pools were limited to DARPA con-tractors being evaluated on the IR task, it X  X  likely that there are many relevant posts that were not pooled and hence not judged. Experimenters should be cautious in handling un-judged retrieved posts, especially for systems that do find judged relevant posts around those same ranks. Retrieval measures that are robust to missing judgments, such as those proposed by Sakai [6] or Yilmaz et al [10], may be useful here.
While this paper is not an evaluation report, we would like to give an indication of reasonable baseline performance which may be expected on this collection. We do this here by anonymizing the BOLT submissions and reporting precision and recall of retrieved forum posts.

The official measures in BOLT included variants of preci-sion and recall based on character counts for citations. Since BOLT program, phase 2 (left) and phase 3 (right). as we note in the previous section, reusing this collection for passage retrieval evaluation requires some significant ef-fort, we convert the original BOLT submissions to simpler post retrieval runs by ranking posts according to the highest ranked passage retrieved from that post. A post is consid-ered to be relevant if any relevant passage is known from that post. Because all these systems were pooled, we are not affected by unjudged posts.

Please note that these systems were not optimized for this measure, and while these measures are reasonable to com-pute for this collection, they are not the best comparisons of these specific runs, which were developed to solve the passage retrieval task rather than the post retrieval task. The purpose of these scores is solely to provide indicators of reasonable state-of-the-art performance on a post retrieval task using this data. Figure 2 plots interpolated precision at standard recall points, as reported by the trec_eval tool Mean average precision for the P2 runs is 0.45 and 0.46, and for the P3 runs ranges from 0.25 to 0.42. The BOLT IR collections, built as part of DARPA X  X  Broad Operational Language Translation program, are new test collections that can be used to measure multilingual retrieval from informal discussion forum text. Furthermore, as the annotations are at the passage level, the collection provides a basis for considering how to create reusable collections for passage retrieval evaluation. The corpora described in this paper have been distributed to performers in the DARPA BOLT program, and are expected to be published in LDC X  X  catalog in 2016.

Although three collections were built over the course of the program, we only consider the latter two, from phases 2 and 3, to be reusable as IR test collections. The collection built for phase 1 represented a learning curve in designing the passage retrieval task and assessment process, and while that data is likely quite useful to researchers it should be regarded with caution as an IR test collection.
 Finally, this paper has been submitted as part of a new SIGIR call for short papers on datasets and test collections, and as such we hope it can be regarded as a proposed tem-https://github.com/usnistgov/trec eval/ plate for such papers. In particular, we recommend that papers describing IR test collections provide specific guid-ance on usage and report baseline effectiveness scores. [1] J. Allan. HARD track overview in TREC 2005: High [2] BOLT program home page. http://www.darpa.mil/ [3] K. Griffitt and S. Strassel. The query of everything: [4] M. Kaszkiel and J. Zobel. Passage retrieval revisited. [5] J. Medero, K. Maeda, S. Strassel, and C. Walker. An [6] T. Sakai and N. Kando. On information retrieval [7] S. Tellex, B. Katz, J. Lin, A. Fernandes, and [8] E. M. Voorhees and D. K. Harman, editors. TREC: [9] W. Xu, R. Grishman, and L. Zhao. Passage retrieval [10] E. Yilmaz, E. Kanoulas, and J. A. Aslam. A simple
