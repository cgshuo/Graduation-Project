 Discovery, design and development of new drugs is an expensive and challenging process. Any new drug should not only produce the desired response to the dis-ease but should do so with minimal side effects. One of the key steps in the drug design process is the identification of the chemical compounds ( hit compounds or just hits ) that display the desired and reproducible behavior against the spe-cific biomolecular target [33]. This represents a significant hurdle in the early stages of drug discovery. Therefore, computational techniques that build models to correctly assign chemical compounds to various classes or retrieve compounds of desired class from a database have become popular in the pharmaceutical industry.
 representations of molecular graphs that can build good classification models or retrieve actives from a database in an effective way. Towards this goal, a number of different approaches have been developed that represent each compound by a set of descriptors that are based on frequency, physiochemical properties as well as topological and geometric substructures (fragments) [3,4,18,25,38,42,49]. based on fingerprints, which represent each molecular graph by a fixed length bit-vector derived by enumerating all bounded length paths in the graph (e.g., Daylight [3]), fingerprints that consists of fragments of increasing size around atoms (Extended connectivity based descriptors [24,38]), and on sets of fragments that have been identified a priori by domain experts (e.g., Maccs keys [4, 14]). However, in recent years, research in the data mining community has generated new classes of descriptors based on frequently occurring substructures [18] and selected cycles &amp; trees [25] that have been shown to achieve promising results. important in providing effective descriptor-based representations in the context of SVM-based chemical compound classification and ranked-retrieval. We also study the effectiveness of various descriptor-based similarity measures for both deriving kernel functions for SVM-based classification and for ranked-retrieval. The five previously developed descriptors that we study are fingerprints [10], extended connectivity fingerprints [5, 38], Maccs keys [4], Cycles &amp; trees [25] and frequent subgraph-based descriptors [18]. Each of these descriptors repre-sent certain inherent choices that are made in designing any substructure based descriptor space. In order to better understand the strengths and weaknesses of the design choices and their impact on classification and retrieval performance, we also introduce a new set of fragment-based descriptors. These descriptors are derived from the set of all connected fragments present in the molecular graphs (graph fragments or GF) and three of its subsets.
 and also conduct an experimental study on these descriptors using 28 differ-ent classification and retrieval problems derived from 18 datasets. Our study compares the performance achieved by the various descriptors and provides key insights on how the topology, discovery method, exactness and completeness of representation affects the performance. Our experiments also show that for both the classification and the retrieval tasks the GF descriptors are equivalent to ex-tended connectivity fingerprints and consistently and statistically outperformed all the other methods studied in this paper. Moreover, a kernel function intro-duced in this paper that takes into account the length (size) of the fragments present in the set of descriptors lead to better overall results, especially when used with the GF-based descriptors.
 ground on the molecular graph representation of chemical compounds. Section 3 describes the previously developed descriptors. Section 4 provides a detailed description of the characteristics of various descriptor spaces. Section 5 describes the various descriptor spaces introduced in this paper. Section 6 provides a de-scription of the various kernel functions used. Section 7 contains experimental evaluation techniques and results. Section 8 contains discussion of the various descriptors in the light of the results and provides concluding remarks on this work. In this paper we represent each compound by its corresponding molecular graph [29]. The vertices of these graphs correspond to the various atoms (e.g., carbon, nitrogen, oxygen, etc.), and the edges correspond to the bonds between the atoms (e.g., single, double, etc.). Each of the vertices and edges has a label associated with it. The labels on the vertices correspond to the type of atoms and the labels on the edges correspond to the type of bonds. Specifically, we use atomic numbers or a unique identifiers for each atomic number as the atom typing for vertices. For the edge labels, we use separate integers or identifiers for single, double and triple bonds. We also apply two commonly used structure normalization transformations [33]. First, we label all bonds in aromatic rings as aromatic (i.e., a different edge-label), and second, we remove the hydrogen atoms that are connected to carbon atoms (i.e., hydrogen-suppressed chemical graphs). To generate fingerprints and Maccs keys we use the Smiles [3] representation as an input. Fingerprints are used to encode structural characteristics of a chemical com-pound into a fixed bit vector and are used extensively for various tasks in chem-ical informatics. These fingerprints are typically generated by enumerating all cycles and linear paths up to a given number of bonds and hashing each of these cycles and paths into a fixed bit-string [3,10]. The specific bit-string that is gen-erated depends on the number of bonds, the number of bits that are set, the hashing function, and the length of the bit-string. The key property of these fingerprint descriptors is that they encode a very large number of sub-structures into a compact representation. Many variants of these fingerprints exist, some use predefined structural fragments in conjunction with the fingerprints (Unity fingerprints [6]), others count the number of times a bit position is set (holo-gram [7]), etc. In [30], it is shown that the performance of most of these finger-prints is comparable. We will refer to these descriptors as fp-n where n is the number of bits that are used. Molecular descriptors and fingerprints based on the extended connectivity con-cept have been described by several authors [24,38]. Recently, these fingerprints have been popularized by their implementation within Pipeline Pilot [5]. These fingerprints are generated by first assigning some initial label to each atom and then applying a Morgan type algorithm [35] to generate the fingerprints. Mor-gan X  X  algorithm consists of l iterations. In each iteration, a new label is generated and assigned to each atom by combining the current labels of the neighboring atoms (i.e, connected via a bond). The union of the labels assigned to all the atoms over all the l iterations are used as the descriptors to represent each com-pound.
 topology around each atom in the form of shells whose radius ranges from 1 to l . Thus, these descriptors can capture rather complex topologies. The value for l is a user supplied parameter and typically ranges from two to five. integers (e.g 32 bits), and the new label is generated by applying an arithmetic or logical operation on the labels of the neighboring atoms. As a result, the same label can potentially be assigned to multiple atoms, even when the topology of their surrounding shells are different. However, detailed studies have shown that such  X  X ollisions X  are usually rare [5,38]. We will refer to this descriptor as ECFP . Molecular Design Limited (MDL) created the key based fingerprints (Maccs Keys) [4] based on pattern matching of a chemical compound structure to a pre-defined set of structural fragments that have been identified by domain ex-perts [19]. Each such structural fragment becomes a key and occupies a fixed position in the descriptor space. This approach relies on pre-defined rules to en-capsulate the essential molecular descriptors a-priori and does not learn them from the chemical dataset. This descriptor space is notably different from fin-gerprint based descriptor space. Unlike fingerprints, no folding (hashing) is per-formed on the sub-structures. We will use the 166 structural keys by Molecular Design Limited (MDL) and will refer to this descriptor space as MK . Horovath et al [25] developed a method that is based on representing every com-pound as a set of cycles and certain kinds of trees. In particular, the idea is to identify all the biconnected components (blocks) of a chemical graph. Once these blocks are identified, the first set of features is generated by enumerating up to a certain number of simple cycles (bounded cyclicity) for the blocks. Once the cycles are identified, all the blocks of the chemical graph are deleted. The result-ing graph is a collection of leftover trees forming a forest. Each such tree is used as a descriptor. The final descriptor space is the union of the cycles and leftover trees. The tree patterns used in this representation are of a specific topology and size that depends on the position of blocks in the chemical graph. We will refer to this descriptor space as CT . A number of methods have been proposed in recent years to find frequently occurring sub-structures in a chemical graph database [26, 32, 36, 51]. Frequent sub-structures of a chemical graph database D are defined as all sub-structures that are present in at least  X  (  X   X | D | ) of compounds of the database, where  X  is the absolute minimum frequency requirement (also called absolute minimum support constraint). These frequent sub-structures can be used as descriptors for the compounds in that database. One of the important properties of the sub-structures generated, like Maccs Keys, is that they can have arbitrary topology. Moreover, every sub-structure generated is connected and frequent (as deter-mined by the minimum support constraint  X  ).
 on the value of  X  . Therefore, unlike the Maccs keys, the descriptor space can change for a particular problem instance if the value of  X  is changed. Moreover, unlike fingerprints, all frequent subgraphs irrespective of their size (number of bonds) form the descriptor space. A potential disadvantage of this method is that it is unclear how to select a suitable value of  X  for a given problem. A very high value will fail to discover important sub-structures whereas a very low value will result in combinatorial explosion of frequent subgraphs. We will refer to this descriptor space as FS . A careful analysis of the five descriptor spaces described in Section 3 illustrate four dimensions along which these schemes compare with each other and repre-sent some of the choices that have been explored in designing fragment-based (or fragment-derived) descriptors for chemical compounds. Table 1 summarizes the characteristics of these descriptor spaces along the four dimensions. The first dimension is associated with whether the fragments are determined directly from the dataset at hand or they have been pre-identified by domain experts. Maccs keys is an example of a descriptor space whose fragments have been determined a priori whereas in all other schemes used in this study, the fragments are deter-mined directly from the dataset. The advantage of an a priori approach is that sub-structures of arbitrary topology can form a part of the descriptor space. Moreover, the sub-structures selected encode domain knowledge in a compact descriptor space. But it also has a disadvantage of potentially not being able to adapt to the characteristics for a particular dataset and classification problem. tual fragments. On one end of the spectrum, schemes like fingerprints use rather simple topologies consisting of paths and cycles, whereas on the other end, fre-quent sub-structure-based descriptors allow topologies with arbitrary complex-ity. Topologically complex fragments along with simple ones might enrich the descriptor space.
 precisely represented in the descriptor space. For example, most schemes gener-ate descriptors that are precise in the sense that there is a one-to-one mapping between the fragments and the dimensions of the descriptor space. In contrast, due to the hashing approach that they use or the fixed-length of their representa-tion, descriptors such as fingerprints and extended connectivity fingerprints lead to imprecise representations (i.e., many fragments can map to the same dimen-sion of the descriptor space). Depending on the number of these many-to-one mappings, these descriptors can lead to representations with varying degree of information loss.
 space to cover all (or nearly all) of the dataset. Descriptor spaces created from fingerprints, extended connectivity fingerprints, and cycles &amp; trees are guaran-teed to contain fragments or hashed fragments from each one of the compounds. On the other hand, descriptor spaces corresponding to Maccs keys and frequent sub-structures may lead to a descriptor-based representation of the dataset in which some of the compounds have no (or a very small number) of descriptors. A descriptor space that covers all the compounds of a dataset has the advantage of encoding some amount of information for every compound.
 dynamically from the dataset, use fragments with simple and complex topologies, lead to precise representations, and have a high degree of coverage may be ex-pected to perform better in the context of chemical compound classification and retrieval as they allow for a better representation of the underlying compounds. The descriptors that come closest to satisfying all the desirable properties are ECFP, CT and FS. ECFP virtually satisfies all of the properties except pre-cise representation since there is the possibility of collisions [5,38]. On the other hand, CT does not attempt to enumerate trees (only cycles are enumerated). Furthermore, the tree topologies depend on the location of blocks in the molec-ular graph. Lastly, FS suffers from potential incomplete coverage depending on the support threshold. To better study the impact of the above design choices, we introduce a new de-scriptor space that we believe better captures the desired characteristics along the above four dimensions. Like FS, this descriptor space is determined dynamically from the dataset, the topology of the fragments that it consists of are arbitrary connected fragments and leads to a precise representation. However, unlike FS, which may suffer from partial coverage, the new descriptor is ensured to have 100% coverage by eliminating the minimum support criterion and generating all fragments. In order to control the exponential number of fragments generated we replace the minimum support criterion in FS with an upper bound. Thus, this descriptor space consists of all connected fragments up to a given length l (i.e., number of bonds) that exist in the dataset at hand. We will refer to this descriptor space as Graph Fragments (GF) . The algorithm to efficiently generate this descriptor space is described in Appendix A.
 graph fragments. The first, termed as Tree Fragments (TF), is the collection of all fragments that have at least one node of degree greater than two and contains no cycles. This set forms all the tree fragments. The second set, called Path Fragments (PF), is just the set of linear paths where the degree of every node in every fragment is less than or equal to two. The third set of fragments, called Acyclic Fragments (AF) are derived such that AF = TF  X  PF. Table 1 also provides a description of their properties in terms of design choices. It should be pointed out that TF descriptors may lead to incomplete coverage when a compound is itself a linear path of atoms.
 fingerprints [10] and the path-based generalized fingerprints in [42]. But [10] and [42] also use cycles along with the linear paths. Also note that acyclic fragments (AF) are also referred to as free trees. Another important observation is that any frequent sub-structure based descriptor space is a superset of Graph-Fragments when the minimum support threshold (  X  ) is one. Given the descriptor space, each chemical compound can be represented by a vector X whose i th dimension will have a non-zero value if the compound con-tains that descriptor and will have a value of zero otherwise. The value for each descriptor that is present can be either one, leading to a vector representation that captures presence or absence of the various descriptors (referred to as binary vectors) or the number of times (number of embeddings) that each descriptor occurs in the compound, leading to a representation that also captures the fre-quency information (referred to as frequency vectors).
 sification algorithms that we develop in this paper use support vector machines (SVM) [44] as the underlying learning methodology, as they have been shown to be highly effective, especially in high dimensional spaces. One of the key param-eters that affects the performance of SVM is the choice of the kernel function ( K ), that measures the similarity between pairs of compounds. Any function can be used as a kernel as long as, for any number n and any possible set of distinct compounds { X 1 , . . . , X n } , the n  X  n Gram matrix defined by K i,j = K ( X i , X j ) is symmetric positive semidefinite. These functions are said to satisfy Mercer X  X  conditions and are called Mercer kernels, or simply valid kernels.
 function. This kernel was selected because it has been shown to be an effective way to measure the similarity between chemical compound pairs and outper-form Tanimoto coefficient [42] (which is the most widely used kernel function in cheminformatics) in empirical evaluations. Given the vector representation of two compounds X and Y , the Min-Max kernel function is given by where the terms x i and y i are the values along the i th dimension of the X and Y vectors, respectively. Note that in the case of binary vectors, these will be either zero or one, whereas in the case of frequency vectors these will be equal to the number of times the i th descriptor exists in the two compounds. Moreover, note that the Min-Max kernel is a valid kernel as it has been shown to satisfy Mercer X  X  conditions [42] and reduces to Tanimoto kernel in the case of binary vectors. that contain fragments of different lengths is that they contain no mechanism to ensure that descriptors of various lengths contribute in a non-trivial way to the computed kernel function values. This is especially true for the GF descriptor space and its subsets in which each compound tends to have a much larger number of longer length fragments (e.g. length six and seven) than shorter length (e.g. length two and three). To overcome this problem we modified the above kernel function to give equal weight to the fragments of each length. Particularly, for the Min-Max kernel function, this is obtained as follows. Let X l and Y l be the feature vectors of X and Y with respect to only the features of length l , and let L be the length of the largest feature. Then, the length-differentiated Min-Max kernel function K  X  MM ( X, Y ) is given by We will refer to this as the length-differentiated kernel function , and we will refer to the one that do not differentiate between different length fragments as pooled kernel function .
 tion, one that is binary and pooled, frequency and pooled, binary and length-differentiated and frequency and length-differentiated. We also studied these four flavors of RBF kernel, but the results were worse than Min-Max [46] so we are not including them here. We will follow the convention of using the symbols K , K f , K  X  b , and K  X  f to refer to binary and pooled, frequency and pooled, bi-nary and length-differentiated and frequency and length-differentiated Min-Max kernel functions, respectively.
 The performance of the different descriptors and kernel functions was assessed on 28 different classification problems from 18 different datasets. The size, distri-bution and compound characteristics of the 28 classification problems are shown in Table 2. Each of the 28 classification problems is unique in that it has different distribution of positive class (ranging from 1% in H2 to 50% in C1), different number of compounds (ranging from the smallest with 559 compounds to largest with 78,995 compounds) and compounds of different average sizes (ranging from the 14 atoms per compound to 37 atoms per compound on an average in C1 and H3 respectively).
 ation Challenge [11,40]. It contains data published by the US National Institute for Environmental Health Sciences and consists of bio-assays of different chemical compounds on rodents to study the carcinogenicity properties of the compounds. Each compound is evaluated on male rats, female rats, male mice, and female mice, and is assigned class labels indicating the toxicity or non-toxicity of the compound for that animal. We derive four problem sets out of this dataset, one corresponding to each of the rodents mentioned above. These will be referred to as P1 , P2 , P3 , and P4 .
 mutagenicity data set was extracted from the carcinogenic potency database (CPDB) [20] and provides mutagenicity classes (mutagens and nonmutagens) as determined by the Salmonella/microsome assay (Ames test [13]). The problem for this dataset is to distinguish between these two classes. We will refer this dataset as C1 .
 AIDS Antiviral Screen program [1, 31]. Each compound in the data set is eval-uated for evidence of anti-HIV activity. Compounds that provided at least 50 percent protection were listed as confirmed moderately active (CM). Compounds that reproducibly provided 100 percent protection were listed as confirmed active (CA). Compounds neither active nor moderately active were listed as confirmed inactive (CI). We formulated three problems out of this dataset. The first prob-lem is designed to distinguish between CM+CA and CI; the second between CA and CI, and the third between CA and CM. We will refer to these problems as H1 , H2 , and H3 , respectively.
 Discoverys anthrax project at the University of Oxford [37]. The goal of this project was to discover small molecules that would bind with the heptameric protective antigen component of the anthrax toxin, and prevent it from spreading its toxic effects. The screen identified a set of 12,376 compounds that could potentially bind to the anthrax toxin and a set of 22,460 compounds that were unlikely to bind to the toxin. The task for this data set was to identify if a given a chemical compound will bind the anthrax toxin (active) or not (inactive). This dataset problem is referred as A1 .
 and each drug compound in this dataset is marked either as Oral (O), Topical (T), Absorbent (A) or Injectable (I) depending on the mode of administration of that drug. This dataset was compiled mostly from the FDA X  X  Orange book [12] and the MDL database [8]. A detailed description of this dataset can be found in [45]. Four tasks are defined from this dataset: to distinguish between Oral and Absorbent D1 , between Oral and Topical D2 , between Oral and Injectable D3 and between Oral and everything else (Topical + Absorbent + Injectable) as D3 .
 [17]. Monoamine Oxidase are enzymes that catalyze the oxidation of neurotrans-mitters and neuromodulator called monoamines. This dataset consists of com-pounds that are Mono amine Oxidase inhibitors. The compounds of this dataset have been categorized into four different classes (0, 1, 2 and 3) based on the levels of activity, with the lowest labeled as 0 (inactive) and the highest labeled as 3 (highest potency), all based on the IC50 values of each compound in a MAO assay. We derive three problems from this dataset: M1 with positive class com-pounds as labels 1, 2 and 3 and negative class as compounds with label 0, M2 with positive class as labels 2 and 3 and negative class compounds as labels 0 and 1, and finally the last problem M3 with positive class compounds as label 3 and rest of the compounds in negative class.
 to cancer cell lines [9]. Twelve datasets are selected from the bioassay records for twelve different types of cancer cell lines. Each of the NCI anti-cancer screens forms a classification problem. Since there is more than one screen available for any particular type of cancer (for example colon cancer, breast cancer etc .), we decided to use the screen that had the most number of compounds tested on it. Each of these bioassay records have information on the assay type, compound identifier, activity score, outcome etc. as submitted by the depositor of the bioas-say screen. The class labels on these datasets is decided by the  X  X utcome X  field of the bioassay which is either active or inactive. We used the original class labels associated with each compound for this study. Table 3 proves details of the 12 different bioassays used for this study.
 were unable to generate the fingerprints due to the use of third party software for fingerprint generation and impossible kekule forms and serious valence errors in raw data. Furthermore, many compounds in these datasets were non drug-like, in that, they contained elements such as arsenic, lead etc. All such compounds were removed from their respective datasets. The dataset cleaning made the sets of compounds used for different descriptors exactly the same and allowed objective comparison of the descriptor spaces. Another important observation is that the active compounds in almost all the datasets used in this study do not fall into a particular target activity class. In most assays, the target is either unknown (for example NCI cancer and HIV assays, anthrax and toxicity datasets etc.) or the classification and retrieval problems are defined to be target non-specific (for example Drug dataset). The only exception to this is the MAO dataset that consists of Monoamine Oxidase inhibitors. The classification results were obtained by performing a 5-way cross validation on the dataset, ensuring that the class distribution in each fold is identical to the original dataset. In each one of the cross validation experiments, the test-set was never considered and the algorithm used only the training-set to generate the descriptor space representation and to build the classification model. The exact same training and test sets were used in descriptor generation and cross validation experiments for all the different schemes. The SVM classifier exper-iments were run on Dual Core AMD Opterons with 4 GB of memory. For the SVM classifier we used the SVMLight library [27] with all the default parameter settings except the kernel. We also compare the effectiveness of the different descriptor spaces for the task that is commonly referred to as a ranked-retrieval or database screening [48]. The goal of this task is, given a compound that has been experimentally determined to be active, to find other compounds from a database that are active as well. Since the activity of a chemical compound depends on its molecular structure, and compounds with similar molecular structure tend to have similar chemical function, this task essentially maps to ranking the compounds in the database based on how similar they are to the query compound. In our experiments, for each dataset we used each of its active compounds as a query and evaluated the extent to which the various descriptor spaces along with the kernel functions studied in this paper lead to similarity measures that can successfully retrieve the other active compounds. Notice that all the kernel functions described in Section 6 are valid similarity measures. All descriptors were generated on a Pentium 2.6 GHz machine with 1 GB mem-ory. For fingerprints, we used Chemaxon X  X  fingerprint program called Screen [10]. We experimented using 256-, 512-, 1024-, 2048-, 4196-and 8192-bit length fin-gerprints. We used default settings of the two parameters: number of bonds or maximum length of the pattern generated (up to seven) and number of bits set by a pattern (three). We found that 8192-bits produced better results (even though their performance advantage was not statistically significant compared to 2048-and 4196-bit fingerprints). For this reason, we use 8192-bit fingerprints (fp-n where n = 8192) in all the comparisons against other descriptors. The type of descriptor space generated by this algorithm is calibrated by two variables: (i) the initial atom label used to describe each atom and (ii) the max-imum shell radius (i.e, the farthest atom considered in terms of bond distance). For our study we used the atomic number as the initial label for each atom and a maximum shell radius of three. Thus the fragments that form the ECFP de-scriptor space are a union of all the fragments formed by taking all atoms one, two and three bond distance away from every atom.
 Computing Group [2]. For Cyclic patterns and Trees, we use 1000 as the upper bound on the number of cycles to be enumerated as described in [25] in our own implementation of the algorithm. To generate frequent sub-structures, we use the FSG algorithm described in [32], although any other frequent subgraph discovery algorithm could be used. Table 4 contains the values of  X  used for positive and negative classes in each dataset. Most of the support values are the same or lower than in [18] for the common datasets and are derived in the same fashion as described in [18]. The lowest support value was selected that could allow FSG to use a reasonable amount of time and memory. In the context of fp-n the only kernel applicable is the binary and pooled ( K b ) kernel. This is because these hashed fingerprints are inherently binary and do not provide frequency information. In the context of ECFP and MK, only two kernels ( K b and K f ) are applied as the length information for ECFP and Maccs keys were not available. For the rest of the descriptor spaces (GF, AF, TF, PF, CT and FSG), we applied all the four kernels described in Section 6. The classification performance was assessed by computing the ROC50 values [21], which is the area under the ROC curve up to the first 50 false positives. This is a much more appropriate performance assessment measure than traditional ROC value for datasets with very small positive classes. This is because for such problem settings, a user will most likely stop examining the highest scor-ing predictions as soon as he/she starts encountering a certain number of false positives [21].
 pounds in the context of ranked-retrieval task by looking at the fraction of pos-itive compounds that were recovered in the top k retrieved compounds. Specif-ically, we report the fraction of positives recovered in the top k retrieved com-pounds in a ranked-retrieval task in which every positive compound is used as query. We call this metric normalized hit rate (NHR) and it is computed as fol-lows. Suppose N is the number of compounds in a dataset, N + is the number of positive (active) compounds in that dataset and hits k is the number of positives found in the top k retrieved compounds over all queries. Then, the normalized hit rate is given by we compute a summary statistic that we refer to as the Average Relative Quality to the Best (ARQB) as follows: Let r i,j be the ROC50 (NHR) value achieved by the scheme j on the dataset i , and let r  X  i be the maximum (i.e. the best) ROC50 (NHR) value achieved for this dataset over all the schemes. Then the ARQB for scheme j is equal to (1 /T ) ( P i ( r i,j /r  X  i )), where T is the number of datasets. An ARQB value of one indicates that the scheme achieved the best results for all the datasets compared to the other schemes, and a low ARQB value indicates a poorly performing scheme.
 significance of any two descriptors based on the performance measures described above. A p -value of 0 . 01 is used as threshold for all comparisons. Table 5 shows the number of graph fragments (GF) of various lengths that were generated for each dataset as well as the time required to generate the fragments of length seven. These results show that the number of fragments does increase considerably with l , which essentially puts a practical upper bound on the length of the fragments that can be used for classification. In fact, for l = 8 (not shown here), the number of fragments were about three to five times more than that for l = 7, which made it impractical to build SVM-based classifier for many of the datasets. However, on the positive side, the amount of time required to generate these fragments is reasonable, and is significantly lower than that required for learning the SVM models. To evaluate the impact of the fragment length on the performance achieved by the GF descriptors for classification and retrieval, we performed a study in which we varied the maximum fragment length l from two to seven bonds. The results of this study are shown in Table 6 and 7. These results were obtained using the K f kernel, which as will be shown later, is one of the best performing kernels for GF descriptors.
 tends to improve as l increases, and the scheme that use up to length seven fragments achieve the best overall performance (in terms of ARQB). Moreover, all of these differences are statistically significant. On the other hand the retrieval performance in terms of ARQB, as shown in Table 7, saturates as l increases from six to seven. Also, results with l equal to five, six and seven are not statistically different from each other for ranked-retrieval. This indicates that, for ranked-retrieval task, larger fragments do not improve performance in the context of GF fragments.
 Table 8 and 9 shows the classification and ranked-retrieval performance of the different kernel functions described in Section 6 for the GF descriptors. These results were obtained for GF descriptors containing fragments of length up to seven.
 differentiated frequency vectors). Thus, giving equal weights to the fragments of various lengths leads to better results. Note that for classification results in Table 8, based on the Wilcoxon statistical test of p = 0 . 01, the differences between K  X  b and K  X  f are not significant, but K  X  f is statistically better than K b . Also, K  X  f is statistically better than K f at p = 0 . 05. Moreover, the table shows that including frequency information leads to better results. For ranked-retrieval results in Table 9, K  X  f is statistically better than K b at p = 0 . 05. But all the other pairwise comparisons of the different kernels show statistically equivalent performance to each other.
 To compare the classification performance of the various descriptor spaces we performed a series of experiments in which all the kernels described in Section 6 that can be used in conjunction with the nine descriptor spaces (fp-n , MK, CT, FS, ECFP, GF, AF, TF, and PF) are employed to classify the various datasets. In order to objectively compare these nine schemes, in Table 10 we only compare the ROC50 results achieved by the two kernels (binary and pooled K b and frequency and pooled K f ) that are applicable to most of the descriptor spaces. In addition, Table 11 shows whether or not these schemes in combination with the kernels used achieve ROC50 results that are statistically different from each other. The results for GF, AF, TF, and PF were obtained for fragments up to length seven.
 AF and ECFP descriptors lead to ROC50 results that are statistically better than that achieved by all other previously developed schemes. From a statistical point, GF, AF and ECFP descriptors is equivalent to each other for the same kernel function. In addition, the performance achieved by both TF and PF is also good and in general better than that achieved by the earlier approaches.
 descriptors achieve the best overall results, whereas MK tend to perform the worst. However, from a statistical significance standpoint CT, MK, and FS are equivalent.
 results than fp-n (the difference is significant at p = 0 . 05). Since the fp-n descrip-tors were also generated by enumerating paths of length up to seven (and also cycles), the performance difference suggests that the folding that takes place due to the fingerprint X  X  hashing approach negatively impacts the classification perfor-mance. This result is in agreement with [42] albeit we perform this comparison on a much higher number of datasets.
 ent descriptors it can be observed that K f generally performs better than K b in terms of ARQB. The differences between the two kernels is generally statis-tically significant at p = 0 . 05 for most descriptors, although it is not significant p = 0 . 01. Thus, adding frequency information helps in improving classification performance. For the task of ranked-retrieval we experimented using all possible combinations of the nine descriptor spaces and four kernel functions. Again, due to the reason mentioned in the section Section 7.5.1, in Table 12 we only show the NHR results for ( K b and K f ) for each descriptor space. Table 13 shows the extent to which the relative performance of various schemes are statistically significant. trends with respect to the relative performance of the various descriptor spaces. In the case of ranked-retrieval, the ECFP descriptor with K f is the best scheme in terms of ARQB outperforming most of the other schemes. Also, GF-based descriptors (GF, AF and TF) and ECFP are statistically equivalent and out-perform other descriptors. Moreover, using frequency information in the kernel function ( K f ) leads to better results than just the binary presence/absence. An interesting observation is that although CT, FS and MK form the set of schemes that perform the worst among all the nine descriptors, fp-n is only slightly better than CT, MK or FS in terms of ARQB and statistically all the four are equiv-alent. This is not the case in classification where fp-n does significantly better than CT, MK and FS. Also the average performance of the AF, TF, and PF descriptors (as measured by AQRB) is higher than fp-n , CT, MK and FS as well. In recent years many new descriptors and graph kernels have been introduced in the datamining literature and their classification performance has been suc-cessfully assessed. The performance assessment measure used in those studies is primarily area under the ROC curve. In Table 14 we compare the ROC results of GF, AF, TF, and PF with the results of recently introduced Cycles &amp; Trees (CT) [25] , random-walk based graph kernels (RWK) [28], weighted decomposi-tion kernels (WDK) [34] and Frequent subgraph based descriptors [18]. We use the length-differentiated Min-Max kernel ( K  X  f ) for GF-based descriptors and its subsets. The results could only be compared for the common datasets with those used in these studies. We use the default misclassification cost factor (1.0) and do not optimize for regularization parameter in GF-based descriptors and its subsets. We compare our results with the best results (Gaussian version of inter-section kernel described in [25]) of Cycles &amp; Trees using misclassification cost of 1.0. In the case of WDK, the authors only report numbers with misclassification cost set to match the positive to negative compound ratio. They also optimize the regularization parameter. Hence we had to use best results from those num-bers. For RWK, the code was provided to us by Mr. Kashima. We report RWK numbers for PTC dataset only as it was not possible to generate results for RWK for large aids dataset owing to the high computational complexity of the scheme. It can be observed from Table 14 that the GF descriptor outperforms CT, RWK, WDK and FSG for the majority of the datasets. Moreover, the best performing method consistently fall into one of the GF, AF, TF, or PF descriptors (except CA vs CM) despite the fact that no optimization performed on the SVM param-eters. The average improvement of GF over the ROC values of WDK, CT, RWK, and FSG for the common datasets is 1.5%, 1.66%, 6% and 6.4% respectively. The work in this paper was primarily motivated by our desire to understand which aspects of the molecular graph are important in providing effective descriptor-based representations for the classification and retrieval tasks given the four design choices described in Section 4 (dataset specificity, fragment complexity, preciseness, and coverage) and the fact that no scheme leads to a descriptor space that is strictly superior (in terms of what it captures) to the rest of the schemes. Most of the descriptor spaces make some compromises along at least one of these dimensions. We believe that the experimental results presented in Section 7.5 provide some answers on the relative importance and impact of these design choices.
 resentation is a key property and helps PF outperform fp-n even though the former utilizes only path-based fragments, whereas fp-n also uses fragments cor-responding to cycles. Similarly, the results comparing GF against FS suggest that the 100% coverage of GF is a critical property as it helps outperform the FS approach. To ascertain this fact we decided to eliminate infrequent fragments of GF by applying support threshold similar to FS scheme on all datasets (data not shown). We found that the performance of the resulting classifier degrades as compared to GF. Thus, arbitrary support thresholds used to limit the number of fragments generated in graph mining deteriorates classification performance. Generating all fragments with support greater than or equal to one but having an upper limit on the size of fragments (GF) is a much better approach to clas-sify chemical compounds. Also, the results comparing the schemes that utilize dataset specific fragment discovery approaches against the MK scheme show that relying on pre-identified fragments will lead to lower performance. Finally, the results comparing GF against AF, TF and PF show that everything else being the same, more complex fragments do lead to better results; however, these gains are not substantial.
 satisfy all four of the desirable design choices, achieve the best results for both the classification and retrieval tasks. Moreover, these two descriptor-based repre-sentations are generally better than the state-of-the-art graph kernel approaches (RWK and WDK in Section 7.6) that operate directly on the compound X  X  molec-ular graph. Furthermore, the advantage of the descriptor-based representation over the graph kernel approach is that the process of determining the simi-larity between two compounds is decoupled from the actual descriptor space generation. To a large extent, this make the process of designing better descrip-tor spaces and similarity (kernel) functions independent of each other and thus much easier. However, a potential drawback is that it requires careful selection of the descriptor-based representation as well as the similarity function for the particular dataset and descriptor space, respectively.
 higher in the classification or retrieval tasks, we found that there are consider-able differences as to the true positives or hits that were identified by the GF and ECFP descriptor spaces. For example, Figure 1 a shows how the true positives overlap 1 between the two schemes on three different subsets: (i) top 50 predic-tions, (ii) top 100 predictions, and (iii) all compounds until encountering the first 50 false positives. In the first subset, the overlap is only 62%, whereas for the third subset (which is usually contains more compounds than the other two), the overlap increases to 80%. A similar analysis is shown in Figure 1 b for the retrieval task, and this time the overlap percentages are somewhat smaller, ranging from 39% to 61%. These overlap results show that there are considerable differences between the predictions and rankings produced by the two descriptor spaces. Thus, even though the overall performance of the GF and ECFP descriptors (as measured by ARQB) is quite similar, they tend to produce qualitative different results. This suggests that applying Fusion based techniques (that combine rank-ings obtained from different descriptor spaces) [48] to the rankings produced by GF and ECFP might lead to improvement in the performance over just the GF or ECFP ranked-retrieval results.
 This work was supported by NSF EIA-9986042, IIS-0431135, NIH RLM008713A, ACI 0133464, the Army High Performance Computing Research Center contract number DAAD19-01-2-0014, and by the Digital Technology Center at the Uni-versity of Minnesota. Any frequent subgraph mining algorithm with a support threshold of one can be used to derive a set of bounded size GF descriptors. We tried to generate bounded size GF using existing frequent subgraph mining algorithms like FSG [32], Gaston [36] and gSpan [51]. But we faced considerable runtime and memory problems in the case of FSG and Gaston for large datasets with support threshold of one. Also, the current implementation of gSpan that we downloaded does not support an upper bound on the length of fragments. Moreover, frequent subgraph mining algorithms are not designed for this particular task at hand. Specifically, these algorithms spend a significant amount of time in support computation or calculation of embedding lists to speed up support computation [50]. But since all the possible bounded size subgraphs are generated in GF, every subgraph discovered is frequent and hence there is no need for support computations. For these reasons, in order to generate all connected graph fragments, we developed an algorithm that was inspired by the recursive technique for generating all the spanning trees of a graph G [47].
 of G that contain e and S  X  e ( G ) be the set of all spanning trees of G that do not is equal to the set of all spanning trees of G , denoted by S ( G ). Now, if S ( G/e ) denotes an edge contraction operation (i.e., the vertices incident on e are collapsed together) then S e ( G ) can be obtained from S ( G/e ) by adding e . If G \ e denotes an edge deletion operation, then S  X  e ( G ) is nothing more than S ( G \ e ). From the above observations we can come up with the following recurrence relation for generating S ( G ) where e is an arbitrary edge of G , and eS ( G/e ) denotes the set of all spanning trees obtained by adding e to each spanning tree in S ( G/e ).
 nected graph fragments of a certain length l by modifying it in three different ways. These modifications are needed to ensure that (i) arbitrary graph fragments and cyclic fragments are generated (ii) the graph fragments that are returned are connected, and (iii) only all the fragments of length l are returned. The first ob-jective can be achieved by simply changing the edge contraction operation to an edge deletion operation. The second can be achieved by imposing the constraint that the edge e must be incident on a vertex of G that was obtained via an edge deletion operation, if such a vertex exist. If G does not have any such vertex (i.e., it corresponds to the original graph), then e is selected in an arbitrary fashion. The length requirement can be ensured by terminating the recurrence relation when exactly l edges have been selected. In light of these modifications, the new recurrence relation that generates all the connected graph fragments of length l , denoted by F ( G, l ) is given by where e is satisfies the above constraints.
 labelling [32] for every fragment generated from a molecular graph. The canonical labelling of every fragment can also be used to count the number of embeddings of a fragment in a molecular graph. Note that the recurrence relation above generates each fragment only once. Thus two isomorphic fragments in the same molecular graph differ by at least one edge. Also note that since the primary goal of this paper is compare different descriptor spaces, we did not compare the
