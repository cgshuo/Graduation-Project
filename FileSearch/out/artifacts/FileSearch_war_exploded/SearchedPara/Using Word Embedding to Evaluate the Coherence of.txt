 Scholars often seek to understand topics discussed on Twit-ter using topic modelling approaches. Several coherence metrics have been proposed for evaluating the coherence of the topics generated by these approaches, including the pre-calculated Pointwise Mutual Information (PMI) of word pairs and the Latent Semantic Analysis (LSA) word repre-sentation vectors. As Twitter data contains abbreviations and a number of peculiarities (e.g. hashtags), it can be chal-lenging to train effective PMI data or LSA word represen-tation. Recently, Word Embedding (WE) has emerged as a particularly effective approach for capturing the similarity among words. Hence, in this paper, we propose new Word Embedding-based topic coherence metrics. To determine the usefulness of these new metrics, we compare them with the previous PMI/LSA-based metrics. We also conduct a large-scale crowdsourced user study to determine whether the new Word Embedding-based metrics better align with human preferences. Using two Twitter datasets, our results show that the WE-based metrics can capture the coherence of topics in tweets more robustly and efficiently than the PMI/LSA-based ones.
Topic modelling approaches can be used by scholars to capture the topics discussed in various corpora, including news articles, books [5] and tweets [4, 15]. Since typically for such scenarios no ground-truth exists to determine how well the topic modelling approach works, a number of topic coherence metrics have been proposed to assess the perfor-mance of the topic modelling approaches in extracting com-prehensible and coherent topics from corpora. These metrics often capture the semantic similarity of words in a topic us-ing external sources such as Wikipedia or WordNet.
To evaluate the coherence of a topic, a coherence metric averages the semantic similarity of words in topics. Recently, two effective coherence metrics, namely the Pointwise Mu-tual Information (PMI)-based [11] of word pairs and the La-tent Semantic Analysis (LSA) word representation-based [3] metrics, have been adapted to tweet corpora [3]. Through a large crowdsourcing study, Fang et al. [3] found that a PMI-based metric using a Twitter background dataset aligned best with human preferences of topic coherence. However, some challenges remain, particularly because of the unique-ness of Twitter data, where unlike many other corpora, tweets contain abbreviations, several peculiarities (e.g. hash-tags) and a vast vocabulary. For example, the PMI metric leverages the co-occurrence data of approx. 354 million word pairs [3], which is voluminous to store. Recently, Word Em-bedding (WE) has emerged as a more effective word rep-resentation than, among others, LSA [8, 9, 10]. Using WE word representation models, scholars have improved the per-formance of classification [6], machine translation [16], and other tasks. However, it remains to be seen whether Word Embedding can be effectively used to evaluate the coherence of topics in comparison with existing metrics.

In this paper, we propose a new Word Embedding-based metric, which we instantiate using 8 different Word Em-bedding models (trained using different datasets and dif-ferent parameters). We also use as baselines two types of existing effective metrics based on PMI and LSA. We con-duct a large-scale pairwise user study, comparing human judgements with the 8 WE-based and the 4 PMI/LSA-based baseline metrics. We generate the topic pairs using three topic modelling approaches (i.e. LDA, Twitter LDA and Pachinko Allocation Model ). First, we rank the topic mod-elling approaches using each of the deployed coherence met-rics. Second, we assess the extent to which the topical pref-erences emanating from the 12 metrics align with human assessments. Using two Twitter datasets, our results show that the new Word Embedding-based metrics outperform the PMI/LSA-based ones in capturing the coherence of top-ics in terms of robustness and efficientness.
In this paper, we use three topic modelling approaches to generate topics for Twitter data. They have been chosen because of their reasonable computational cost and scalabil-ity on high volumes of tweets. First, we use LDA [2], where each of K topics is represented by a term distribution  X  , while each document has a topic distribution  X  . Second, we experiment with an extension of LDA, the Pachinko Alloca-tion Model (PAM) [7]. The topic layer in PAM is divided into a super-topic layer (distribution over sub-topics) and a sub-topic layer (distribution over terms). Third, we use a topic modelling approach tailored to tweet corpora, namely Twitter LDA (TLDA) [15], where a Bernoulli distribution is estimated and used to control the selection between  X  X eal X  terms and background terms. PAM and TLDA are known to generate more coherent topics than LDA on news corpora and tweets, respectively [7, 15].

There are two main existing types of effective topic co-herence metrics. One metric, Pointwise Mutual Information (PMI), developed by Newman et al. [11], captures the se-mantic similarity of pairs of words in a topic, by examin-ing how the word pairs co-occur in external sources such as Wikipedia. PMI has been tested on news articles and books. Fang et al. [3] showed that the PMI metric deployed using Twitter background datasets was closest to human judge-ments. Moreover, they also adapted a second type of metric, the LSA word representation metric, to evaluate the coher-ence of topics. The LSA-based coherence metric proved to be less aligned with human assessments than the PMI-based one on tweet corpora.

Turning to Word Embedding approaches, recently schol-ars have applied Feed-Forward Neural Network (FFNN) for Word Embeddings [1]. In this approach, similar to LSA, a word is represented as a continuous vector in a Word Embedding model. Based on FFNN, Mikolov et al. [8, 9] proposed a skip-gram model to generate Word Embeddings from large datasets more efficiently and effectively. Godin et al. [6] improved the performance of document classifica-tion via Word Embeddings. Indeed, Neural Networks were shown to generate more effective word representations than LSA [10]. Therefore, in this paper, we propose new Word Embedding-based metrics to capture the coherence of top-ics. We adopt the skip-gram approach to obtain our Word Embedding models. In the next section, we explain how we deploy the PMI, LSA, and WE-based metrics.
We use three types of coherence metrics for Twitter data based on PMI and LSA, and Word Embeddings (WE), which are instantiated into 12 metrics. We first define the existing PMI &amp; LSA-based metrics before introducing the new Word Embedding-based metric to evaluate the coherence of topics. PMI &amp; LSA metrics. A topic t can be represented by the top n = 10 words ( { w 1 ,w 2 ,...,w 10 } ) (selected by their prob-topic can be calculated by averaging the semantic similarity of pairs of words associated with that topic (Equation (1)). Both Newman et al. [11] and Fang et al. [3] showed that the PMI of pairs of words can capture the coherence of topics identified from both standard and tweet corpora. In particu-lar, for the PMI metric, Equation (2) is used to measure the similarity of word w i and w j based on co-occurrence statis-tics obtained from a background corpus (e.g. Wikipedia or a large sample of tweets -detailed in Section 5) along with Equation (1). Note that the PMIs of word pairs need to be pre-calculated from these external datasets.

LSA can also be used to capture the semantic similarity of word pairs [13]. In applying LSA, each word is repre-sented by a dense vector in the reduced LSA space, V obtained by applying Singular Value Decomposition on a background corpus. Therefore, the LSA metric determines the similarity of two words by measuring the distance be-tween the vectors of the words using a cosine function, by replacing Equation (2) in Equation (1) with: WE metric. Recently, as highlighted above, Word Em-beddings have been shown to produce more effective word representations than LSA. Hence, we propose the use of WE vectors V w i , obtained from a pre-trained Word Embedding model on a large text dataset. If two words are semantically similar, the cosine similarity  X  as per Equation (3)  X  of their word vectors is higher. We describe how we train the Word Embedding models in Section 5.

We use the methodology explained in Section 4 to examine whether the WE-based metric can capture the coherence of topics from tweets, and how well WE, PMI, and LSA metrics compare with human judgements.
We follow Fang et al. [3], and adopt a pairwise user study to gather human preferences to evaluate the effectiveness of the aforementioned metrics. Since it is difficult for humans to generate graded coherence scores of topics, we select a pairwise study, where a human is asked to choose which of two topics is more coherent. For our study, we first gener-ate topic pairs using the three topic modelling approaches: LDA, PAM, and TLDA. We then determine whether the metrics can accurately identify the more coherent topics compared with human coherence assessments.

Ground Truth Generation. We use pairwise compar-isons for the three topic modelling approaches, specifically: LDA vs. TLDA, LDA vs. PAM and TLDA vs. PAM. Each comparison unit consists of a certain number of topic pairs, where each pair contains a topic from topic models T 1 and T , respectively. Note that T 1 and T 2 are generated using any two topic modelling approaches among the three. In the pairwise user study, a human is asked to choose the more coherent topic among two topics presented in a given topic pair. For ease of assessment, we present the human with two similar topics in a topic pair. We first randomly select a number of topics from the topic model T 1 . For each se-lected topic, we use Equation (4) to select its closest topic in T 2 , where V t is a vector representation using the term distribution of topic t . The selected topic pair is denoted as Pair( T 1  X  T 2 ). Similarly, we also generate the same number of Pair( T 2  X  T 1 ) for the comparison unit ( T 1 ,T 2 ). Hence, for each comparison unit, we obtain a set of topic pairs. For example, we generate Pairs(LDA  X  TLDA &amp; TLDA  X  LDA) for the comparison unit (LDA, TLDA).
A given coherence metric generates a coherence score for each topic in a topic pair. Thus for each comparison unit, we have a group of data pairs. We then apply the Wilcoxon signed-rank test to compute the statistical significance level of the difference between the two sets of data sampled in order to determine the better topic model between the two approaches utilised (e.g. TLDA &gt; LDA). Therefore the out-comes of three comparison units gives the performance rank-ing order of the three topic modelling approaches. For in-stance, we obtain the ranking order LDA(1 st ) &gt; TLDA(2 &gt; PAM(3 rd ) from LDA &gt; TLDA, LDA &gt; PAM &amp; TLDA &gt; PAM. Similar to Fang et al., we do not observe a Condorcet para-dox (e.g. TLDA &gt; LDA, LDA &gt; PAM &amp; PAM &gt; TLDA) in our experiments. Turning to our user study, a topic receives a vote if it is preferred by a human. Using the vote fraction of topics, we can also obtain an ordering of the three topic modelling approaches, i.e. the human ground-truth ranking.
Comparison of Coherence Metrics. A good metric should rank the three topic modelling approaches in a high agreement with humans. First, the three topic modelling approaches are ranked using each of the deployed coherence metrics. Second, the rankings are compared to the rankings from the generated ground-truth to identify which of the metrics agree most with the human assessments.
Datasets. In this paper, we use the same two Twit-ter datasets from [3]. The first dataset 1 is comprised of the tweets of 2,852 newspaper journalists in the US state of New York posted from 20 May 2015 to 19 August 2015, denoted here as NYJ. The second dataset consists of tweets related to the first TV political leaders debate during the UK Gen-eral Election held in April 2015, denoted as TVD 2 . Details of these two datasets are shown in Table 1.

Metrics Setup. We deploy 12 coherence metrics in total, which are implemented by two external sources: Wikipedia and a separate background Twitter dataset. The background Twitter dataset, which is also identical to the one used in [3], represents 1%-5% random tweets crawled from 01 Jan 2015 to 30 June 2015. Following [3], we remove stopwords, terms occurring in less than 20 tweets, and the retweets. The re-maining tweets (30,151,847) are used to pre-calculate the PMI data, LSA word representation, and the WE models. The setup of the 12 metrics is described below: Existing PMI/LSA metrics (4) . We use the LSA word repre-sentation (1M tokens) and the PMI data (179M word pairs) from the SEMILAR 3 platform to implement the Wikipedia PMI/LSA-based metrics (W-PMI/W-LSA). For the Twit-ter PMI/LSA-based metrics (T-PMI/T-LSA), the Twitter background data contains 609k tokens and 354M word pairs. WE metrics using GloV e (4) . These metrics are instanti-ated using Word Embedding models from Wikipedia 4 and Twitter, pre-trained using the GloV e [12] tool. The metrics respectively, where d = 100 means that the size of word vec-tors in the WE model is 100.
 WE metrics using word2vec (4) . These metrics use Word Embedding models newly trained using the separate Twitter background dataset, but making use of the word2vec 5 tool. We denote the coherence metrics using our newly trained the size of the context window size in the trained models.
We noticed that the unstemmed WE word representation performs poorly in our experiments. Hence, we stem the words in our 4 newly trained ( word2vec ) WE models. Note that the WE models of GloV e are not stemmed. We chose to use WE models with different pre-set parameters (e.g. con-text window and vector size) as we wish to examine whether these parameters affect the coherence evaluation task.
Topic Pairs Setup. Mallet 6 and Twitter LDA 7 are used to implement the three topic modelling approaches for the two Twitter datasets. The LDA parameters  X  and  X  are set to 50 /K and 0 . 01 according to [14], and for TLDA  X  = 20 ac-
This dataset was collected by tracking the jour-nalists X  Twitter handles using Twitter Stream-ing API. 2 Collected by searching for debate-re-lated hashtags using the Twitter Streaming API semanticsimilarity.org 4 It also contains English Gi-gaword V5 . 5 deeplearning4j.org 6 mallet.cs.umass.edu github.com/minghui/Twitter-LDA cording to [15]. Since the NYJ dataset contains many topics given the length of time and the fact that journalists discuss many issues, we use a high number of topics, K = 100. On the other hand, because the TVD dataset covers only one de-bate and the accompanying 2 hours X  tweets, we use a smaller number of topic, K = 30. Each topic modelling approach is repeated 5 times. Therefore, for each topic modelling approach, we obtain 500 (150) topics in the NYJ (TVD) dataset, respectively. We use the methodology described in Section 4 to generate 100 topic pairs for each comparison unit. In total, we obtain 600 topic pairs from the two used Twitter datasets. In Section 6, we explain how we perform the pairwise user study using these 600 topic pairs.
We now describe how we use CrowdFlower 8 workers to perform the topic preference task while ensuring job quality.
Job Description. We show a CrowdFlower worker the top 10 words (ranked by their probabilities in a topic) of 2 topics in a topic pair, and the 3 most retweeted tweets in these 2 topics. We ask the workers to select the more coher-ent topic among the two presented using these 10 words. We describe a more coherent topic as one that is less mixed and that can be easily interpreted. The workers are instructed to take into account: 1) the number of semantically sim-ilar words (e.g. Knicks &amp; basketball) among the 10 shown words, 2) whether the presented words suggest a mixed topic (i.e. more than one discussion) and 3) whether the displayed words gives more information about a discussion. To reach a decision, a worker can also use three associated tweets for the two topics. We give two rules for using these tweets: 1) whether the 10 shown words are reflected by their tweets and 2) whether these tweets are related with the two topics. We collect 5 judgements from 5 different workers for each topic pair. For each judgement, we paid a worker $0.05.
Quality Control. To ensure quality control, only those workers who passed a test were allowed to enter the topic preference task. For the test, we choose a number of topic pairs. The topic preference of the selected topic pairs were verified in advance, and they were used to set the test ques-tions for the quality control. The worker must have main-tained more than 70% accuracy on the test questions through the whole task, otherwise their judgements were nullified. Overall, we used 168 trusted workers for this user study.
We first demonstrate whether the 12 used coherence met-rics can differentiate the three topic modelling approaches in comparison with human assessments. We also show whether they can distinguish the more coherent topic from a topic pair in a manner similar to that of humans.

The column  X  X anking Order Matching X  in Table 2 shows the extent to which the ranking order of the 12 metrics ex-actly or partially matches that of human judgements for our two Twitter datasets. The human ground-truth ranking or-der is LDA 1 st &gt; TLDA 2 nd &gt; PAM 3 rd for the NYJ dataset, If there are no significant differences between two topic mod-elling approaches, they share the same rank in the table. If a metric receives a ranking such as LDA 1 st &gt; TLDA 2 nd/ 3 rd &gt; PAM 2 nd/ 3 rd in the NYJ dataset, we say that the ranking order partially matches the human ground-truth one.
We find, first, that the PMI-based metrics differentiate the three topic modelling approaches very well, with the ex-crowdflower.com Table 2: The matching of the ranking order of the three topic modelling approaches from each metric and from humans, and the agreement of the topic preferences between each metric and humans.
 ception of the W-PMI metric in the TVD dataset, whose ranking order only partially matches the ground-truth or-der. Second, all of our WE-based metrics using our trained WE models partially match the ground-truth ranking order. This finding indicates that the WE-based metrics have the ability to differentiate the three topic modelling approaches comparably to humans. However, the WE-based metrics us-ing the GloV e pre-trained models do not differentiate well between the modelling approaches. One possible reason is that the words in those WE models are not stemmed. The coherence metric focuses on capturing the semantic simi-larity between words rather than their syntax. The other possible reason is that words on Twitter are different from the words in Wikipedia, as tweets contain abbreviations, misspellings and hashtags. Moreover, if the time period of the Twitter background dataset does not match that of the testing datasets form which we extract the topics, then the trained WE models may not adequately capture the seman-tic similarity of words, as is likely the case for GloV e .
The column  X  X reference Agreement X  in Table 2 lists the agreement rate on choosing the preferred topics from 300 topic pairs in the two Twitter datasets between each metric and humans. As there are three options ( X  X opic 1 X ,  X  X opic 2 X  and  X  X o preference X ) in the topic preference task, the baseline agreement rate is 33 . 3%. We obverse that the WE-based metrics using our trained WE models have consis-tently high agreement rates across both datasets. In addi-tion, while most of the metrics do not perform well on the TVD dataset, the WE-based metrics using our trained WE models have the hignest agreement with humans. We also find that a slightly higher dimension and context window are likely to get a better agreement rate, such as T-WE w =3 It is interesting to note that unlike the newly trained WE models, the WE-based metric using WE models from GloV e do not have a high agreement with humans.

In summary, the WE-based metrics can effectively capture the coherence of topics from tweets, with a high agreement with humans. The WE-based metrics perform more robustly across the two Twitter datasets. Besides, the WE based metrics have an additional important benefit. In contrast to the PMI-based coherence that leverages the PMI data of a few hundreds millions of word pairs, the WE-based metrics use only a few hundred thousand (the size of vocabulary) words X  vectors. Moreover, since skip gram is highly paral-lelizable, it is much faster to train a WE model than an LSA one [6]. Hence, we conclude that the WE-based met-rics are more robust, efficient, and practical to use than the PMI/LSA-based ones.
We proposed a new Word Embedding -based topic coher-ence metric, and instantiated it using 8 different WE models. To identify the usefulness of these WE-based metrics, we conducted a large-scale pairwise user study to gauge human preferences. We examined which of the 8 WE-based metrics and the 4 existing PMI/LSA-based metrics align best with human assessments. We found that the WE-based metrics can effectively capture the coherence of topics from tweets. In addition, they performed more robustly and more effi-ciently than the PMI/LSA-based metrics. In future work, we will explore how the Word Embedding training parame-ters affect the coherence evaluation task.
