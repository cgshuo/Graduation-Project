 Hierarchical classification is a challenging problem yet bears a broad application in real-world tasks. Item categorization in the ecommerce domain is such an example. In a large-scale industrial setting such as eBay, a vast amount of items need to be categorized into a large number of leaf categories, on top of which a complex topic hierarchy is defined. Other than the scale challenges, item data is extremely sparse and skewed distributed over categories, and exhibits heteroge-neous characteristics across categories. A common strat-egy for hierarchical classification is the  X  X ates-and-experts X  methods, where a high-level classification is made first (the gates), followed by a low-level distinction (the experts). In this paper, we propose to leverage domain-specific feature generation and modeling techniques to greatly enhance the classification accuracy of the experts. In particular, we in-novatively derive features to encode various rich domain knowledge and linguistic hints, and then adapt a SVM-based model to distinguish several very confusing category groups appeared as the performance bottleneck of a currently de-ployed live system at eBay. We use illustrative examples and empirical results to demonstrate the effectiveness of our ap-proach, particularly the merit of smartly designed domain-specific features.
 H.4 [ Information Systems Applications ]: Miscellaneous Algorithms Product, Accessory, Classification, Catalog
Online commerce has gained popularity since the past decade. eBay, one of the largest online C2C marketplaces, features a very large and long-tail inventory with millions of items entered into the marketplace everyday. To manage items effectively and help buyers find them easily, eBay orga-nizes items into fine-grained categories structured as a topic hierarchy. Item categorization is fundamental to many as-pects during an item life cycle, including assigning relevant specifics, inspecting listing violation, charging insertion and final value fees, surfacing the item to users through either searching or browsing, and finally converting item to pur-chase. The task of item categorization can be formulated as a text classification problem intuitively. However, it is more challenging compared with traditional text classifica-tion tasks due to the peculiarities arise from a real large-scale e-Commerce environment, as described in [7]. We especially desire a model capable of: (1) processing huge volume of training and testing data, and large number of classes; (2) handling effectively data sparseness and unbalanced distri-bution, and (3) making real-time prediction of item cate-gories.

In real-world text categorization applications, categories are typically organized into concept hierarchies or taxonomies. Extensive researches have shown the value of hierarchical information [5, 8, 3, 1, 10, 2]. Comparing with the conven-tional flatten classification approach which treats leaf cate-gories separately regardless of the hierarchical structure on top of them and distinguishes a category from all others in one run, hierarchical classification [5, 8, 3] uses a  X  X ates-and-experts X  strategy with the top-level distinctions serving as  X  X ate X  to the lower level  X  X xperts X . The core idea is to decompose a large-scale classification problem into a simple model to make coarse-grained classification, followed by a sophisticated model to make fine-grained distinction. The coarse-grained classification deals with a very large num-ber of examples while maintaining satisfactory classification accuracy; and the fine-grained distinction is learned only within a subtree under each top level category thus can afford more complex feature generation and classification algorithms. Shen et al. [7] further generated a problem-specific latent hierarchy rather than employing the existing human-defined topic structure for the hierarchical classifi-cation. They proposed a graph-based clustering method to group highly-confusing categories and applied more sophisti-cated learning algorithm to distinguish the categories within each group.

Based on the hierarchical classification framework [7], this paper further explores how to design fine-grained models to distinguish those highly-confusing categories at the second level of classification. It is observed that there are 15.7% groups of confusing categories belonging to the Electronic domain on eBay site, hence we will use this domain for il-lustration onwards. The electronics domain consists of four meta categories including Cameras &amp; Photo, Cell Phones &amp; PDAs, Computers &amp; Networking and Consumer Electron-ics. The major challenge of categorization in this domain is to differentiate among the categories of product , product accessory , accessory bundles and whole sale , for examples:
In order to effectively differentiate among most confus-ing categories, it is necessary to incorporate domain specific knowledge into the second-level classification model. In this paper, we innovatively encode various domain knowledge into smartly designed features, including morphological fea-tures, quantity features and pattern features, under a SVM framework. Our goal is to effectively classify product , prod-uct accessory , accessory bundles and whole sale categories. Experimental results on real-world large-scale e-commence data show that the problem-specific model significantly out-performs the existing model on the site which only uses un-igram words of item titles as features.

The model will be applied to determine the most proper category automatically or to recommend categories for sell-ers X  listings. Also, the work can be utilized to detect outlier items within each category to assist in fraud detection. It is observed that vicious sellers generally like to put product accessory items into product category, which will compro-mise user experiences, for instance, the buyers see many cheap  X  iphone case  X  or  X  screen protector  X  in the first page when they search  X  iphone 4  X  or click into the  X  Cell Phones &amp; PDAs  X  Cell Phones &amp; Smartphones  X  category.
The remainder of this paper is structured as follows. We first introduce the SVM-based item categorization briefly. Next, we describe the features designed for the item catego-rization in Electronics domain. Finally we show and discuss experimental results with real-world ecommerce data. We conclude the paper with future works.
Support Vector Machines (SVM) [9] has strong theoretical motivation in statistical learning theory. Extensive empir-ical comparisons on text classification [11, 6] have shown that among the methods available today, SVM achieves su-perb generalization performance and therefore is considered as the state-of-the-art in text classification.

SVM constructs a binary classifier that predicts whether an instance ~x is positive or negative, where the instance is represented as a feature vector. In the case of linearly sep-arable instances, the decision f ( x ) = sgn ( ~w  X  ~x + b ) is made based on a separating hyperplane ~w  X  ~x + b = 0 ( ~w  X  X  n R ). SVM is to find the optimal hyperplane that separates the positive and negative training instances with a maxi-mal margin, by solving a dual quadratic programming prob-lem. In the case of linearly non-separable, SVM projects the instances from the original space R n to a higher dimen-sional space R N based on a kernel function K ( ~x 1  X ( ~x 1 )  X   X ( ~x 2 ). By this means, a linear separation will be made in the new space. We employed a highly efficient im-plementation of SVM with linear kernel, LibLinear [4] for the item categorization. This package also supports multi-class classification using one-versus-the-rest strategy.
When a seller lists an item on eBay, he or she is asked to write a title to briefly describe the item. Once the title is entered, the site suggests proper categories for the seller to choose. So the item categorization is only based on the information from item title. Title of an item on eBay site usually contains 10 words.

In data preprocessing step, item titles are first tokenized, and then punctuations are eliminated. A small stopword list is used to omit the most common words. Furthermore, we observe that (1) numbers are indicative to distinguish be-tween single sale and whole sale categories, and (2) prepo-sitions, such as with and for , are useful to judge product and accessory categories. Therefore, contrary to conven-tional text classification tasks, we keep numbers and prepo-sitions in titles. The existing model uses the unigrams of title words as features. For certain domains, however, we observe that unigrams of words are not capable of differen-tiating between product and accessory categories. For ex-ample, the items  X  Apple iphone 3G Black (16GB)+External Case Cover  X  and  X  FOR Apple iphone 3G leather case cover black  X  belong to different categories while they share a large portion of words. Therefore, it is necessary to explore more domain-specific predictive features. In addition to unigram words, we propose three feature sets including morphologi-cal features, quantity features and pattern features for the item categorization in Electronics domain.

Morphological Features These features are to capture the special words which mix alphabets and digits, such as w755 , i325 , XV6175 , SPH-M520 and 3G . These words are represented as the features AlphaDigit , Alpha-AlphaDigit and DigitAlpha , which usually indicate model names of electronic products.

Quantity Features We collect the evidence from item title to indicate quantity of an item. The most frequent words which occur adjacent to a digit expression in the  X  Wholesale Lots  X ,  X  Accessory Bundles  X  and  X  Multi-Packs  X  categories are extracted. The words, along with the adjacent digit slot, will be regarded as a multiple-quantity pattern, &lt; digit &gt; in1 . In addition, the phrases, such as  X  lots of  X  and  X  wholesale  X  in titles are very indicative of  X  Wholesale Lots  X  categories. The pattern  X  &lt; phrase &gt; + &lt; phrase &gt;  X  are use-ful for  X  Accessory Bundles  X  categories, such as  X  Car+Home charger+Case cover pouch for at&amp;t ZTE F160  X . The evi-dence of titles starting with digit expression are also helpful to detect the multiple quantity items.

Pattern Features There are two steps to generate pat-tern features: key phrase extraction and key phrase rela-tion identification. For each group of categories, we first extract bi-grams of words from training data, rank them by frequency and select the ones which cover at least 10% training instances in one category. Some examples in dif-ferent groups are shown in Table 1. We found that these words are usually the main nouns or noun phrases of some compound phrases and are essential for the semantics of the phrases. Next, we identify how the key phrases are con-nected in titles, including the relation between key phrases, and the relation between key phrase and preposition, such as  X  for  X  and  X  with  X . For examples, the pattern  X  case cover for  X  frequently occurs in the category  X  Cell Phones &amp; PDAs  X  Cell Phone &amp; PDA Accessories  X  Cases, Covers &amp; Skins  X , while the pattern  X  apple iphone with  X  indicates the category  X  Cell Phones &amp; PDAs  X  Cell Phones &amp; Smartphones  X .
The eBay category structure is a six-level topic hierarchy, where there are 39 top-level nodes called meta categories, and more than 20,000 bottom-level nodes called leaf cate-gories. The hierarchy was designed and maintained by hu-man experts. The item categorization is to classify items into leaf categories. A labeled training dataset is collected from all of the sold items on eBay site over 20 days. The trained model is then evaluated on the sold items in the 3 days after the training period. There are totally 11,855,161 items for training and 2,399,460 for testing.
 In the hierarchical framework, as described by [7], the KNN-based model (the first-level classifier) classifies items into latent groups, then a SVM-based model (the second-level classifier) further differentiates the categories within a group. The groups are generated based on the confusing score between leaf categories, as proposed in [7]. In our empirical evaluation, we focus on the second-level classifica-tion for the category groups in the Electronics domain. The system is evaluated on the largest 5 groups in this domain containing 85,794 items in the test set, as shown in Table 2.
We use the categories assigned by sellers as the proxy of ground-truth labels, under the assumption that items suc-cessfully sold have been placed into appropriate categories by sellers. Although the assumption does not always hold, it avoids the otherwise very expensive human labeling, and serves as a good approximation. We observed an error rate of 15% on sold items by seller labeling against editorial val-idation. This error rate in label approximation can give a loose upper bound of actual prediction error, based on ob-served experimental results.

We compare the domain-specific SVM-based classification model ( SVM DS ) with the existing SVM model which em-ploys only bag of title words as features ( SVM BOW ). The model is evaluated on the top 5 groups individually with the precision measure.
 Table 3: Overall Performance on the Top 5 groups Group #Cat. #Test data SVM BOW SVM DS G1 11 49354 94.5% 96.2% G2 7 15492 92.0% 93.6% G3 2 7617 94.2% 95.7% G4 2 7137 84.2% 85.5% G5 2 6194 92.9% 95.8%
Table 3 shows the number of categories, the number of items for testing and the overall precisions of SVM BOW and SVM DS for each group. At all of the groups, SVM DS consistently outperforms SVM BOW . It is important to note that 1-2% of precision lift is considered to be significant, since the base number of items to be categorized is in the order of hundreds of millions and one percent error rate may impact millions of users X  experiences. The results clearly show that various rich domain knowledge and linguistic pat-terns are predictive signals for distinguishing subtle cate-gories.
We have demonstrated the value of carefully designed domain-specific features, in the context of hierarchical clas-sification. We argue that with the  X  X ates-and-experts X  strat-egy for hierarchical classification, many predictive signals are quite localized, thus motivating a closer attention for these domain-specific features. In traditional text classification literature, local features are generally not recommended, that is also why a simple frequency-based feature selection works near optimal in many tasks. However, in a real-world environment where data heterogeneity and sparseness be-come predominant, a serious study of local pattern will pay off. We have demonstrated our approach with intriguing ex-amples, and proposed several innovative ways to derive fea-tures based on rich domain knowledge and linguistic hints.
The authors greatly appreciate catalog product develop-ment team (Rajyashree Mukherjee, Suresh Raman and Niraj Kothari) and catalog product manager (Chelly Yan) for the valuable discussion and effective collaboration in the suc-cessful product deployment. [1] L. Cai and T. Hofmann. Hierarchical document [2] O. Dekel, J. Keshet, and Y. Singer. Large margin [3] S. T. Dumais and H. Chen. Hierarchical classification [4] R. E. Fan, K. W. Chang, C. J. Hsieh, X. R. Wang, [5] D. Koller and M. Sahami. Hierarchically classifying [6] F. Sebastiani. Machine learning in automated text [7] D. Shen, M. Somaiya, J. D. Ruvini, and [8] A. S.Weigend, E. D.Wiener, and J. O. Pedersen.
 [9] V. N. Vapnik. Statistical learning theory , 1998. [10] K. Weinberger and O. Chapelle. Large margin [11] Y. Yang and X. Liu. A re-examination of text
