 Discussion boards and online forums are important platform s for people to share information. Users post questions or proble ms onto discussion boards and rely on others to provide possible sol utions and such question-related content sometimes even dominate s the whole discussion board. However, to retrieve this kind of in forma-tion automatically and effectively is still a non-trivial t ask. In ad-dition, the existence of other types of information (e.g., a nnounce-ments, plans, elaborations, etc.) makes it difficult to assu me that every thread in a discussion board is about a question.
We consider the problems of identifying question-related t hreads and their potential answers as classification tasks. Experi mental results across multiple datasets demonstrate that our meth od can significantly improve the performance in both question dete ction and answer finding subtasks. We also do a careful comparison o f how different types of features contribute to the final resul t and show that non-content features play a key role in improving o verall performance. Finally, we show that a ranking scheme based on our classification approach can yield much better performan ce than prior published methods.
 H.3 [ Information Storage and Retrieval ]: H3.3 Information Search and Retrieval; H.4 [ Information Systems Applications ]: H4.3 Communications Applications X  Bulletin boards Algorithm, Experimentation question answering, discussion boards, online forums, cla ssifica-tion
Discussion boards, also known as online forums, are popular web applications widely used in different areas including c ustomer support, community development, interactive reporting an d online education. Online users share ideas, discuss issues and for m com-munities within discussion boards, generating a large amou nt of content on a variety of topics. As a result, interest in knowl edge dis-covery and information extraction from such sources has inc reased in the research community.

While the motivation for users to participate in discussion boards varies, in many cases, people would like to use discussion bo ards as problem-solving platforms. Users post questions, usually related to some specific problem, and rely on others to provide potentia l an-swers. Numerous commercial organizations such as Dell and I BM directly use discussion boards as problem-solving solutio ns for an-swering questions and discussing needs posed by customers. Cong et al. [8] found that 90% of 40 discussion boards they investi gated contain question-answering knowledge. Using speech acts a nalysis on several sampled discussion boards, Kim et al. [22, 21] sho wed that question answering content is usually the largest type of con-tent on discussion boards in terms of the number of user-gene rated posts. Therefore, mining such content becomes desirable an d valu-able.

Mining question answering content from discussion boards h as several potential applications. First, search engines can enhance search quality for question or problem related queries by pr oviding answers mined from discussion boards. Second, online Quest ion Answering (QA) services such as Yahoo! Answers 1 , Answers.com and AllExperts 3 would benefit from using content extracted from discussion boards as potential solutions or suggestions wh en users ask questions similar to what people have discussed on forum s. This would eliminate the time users wait for answers and enri ch the knowledge base of those QA services as well since discuss ion boards have a longer history than that of QA services and also own a much larger amount of user generated content. Third, users who often provide questions in forums may have certain expert kn owl-edge in particular areas. Researchers are trying to find expe rts in so-cial media by utilizing question answering content; author ities are discovered in discussion boards by understanding question answer-ing content and user interactions [4, 33, 20]. In addition, q uestion answering content extracted from discussion boards can be f urther used to augment the knowledge base of automatic chat-bots [1 1, 15].

Although general content mining of discussion boards has gained significant attention in recent years, the retrieval of ques-tion and potential answers from forums automatically and ef fec-tively is still a non-trivial task. Users typically start a t hread by creating an initial post with arbitrary content and others r eply to it http://answers.yahoo.com/ http://www.answers.com/ http://www.allexperts.com/ in accordance with the type of the first post. For example, if t he first post is about a question, following posts may contain si milar experiences and potential solutions. If the first post is an a nnounce-ment, following posts may contain clarifications, elaborat ions and acknowledgments. Hence, due to the existence of different t ypes of information, we cannot assume that every thread on a discu s-sion board is about a question, which makes discussion board s fun-damentally different from QA services like Yahoo! Answers t hat are designed specifically for question answering. Addition ally, the asynchronous nature of discussion boards makes it possible or even common for multiple users to pursue different questions in p arallel within one thread.

In this paper, we explore the problem of extracting question an-swering content from discussion boards and divide it into tw o sub-tasks: identifying question-related first posts and finding potential answers in subsequent responses within the corresponding t hreads. We address both subtasks as classification problems and focu s on the following research questions: 1. Can we detect question-related threads in an efficient and ef-2. Can we effectively discover potential answers without ac tu-3. Can this task be treated as a traditional information retr ieval We choose several content-based and non-content based feat ures and carefully compare them individually and also in combina tions. We do not use any service-or dataset-specific heuristics or f eatures (like the rank of users) in our classification model; therefo re our approach should be usable in any discussion board. In order t o test whether our method can improve performance in both subtasks , we mainly compare our approach with one recent similar work [ 8] (to our knowledge, the first to attack the same problem) and sh ow significant improvements in experimental results.

The rest of this paper is organized as follows: We discuss rel ated work below. Section 3 defines our tasks in more detail. Sectio n 4 presents our features and gives a simple overview of other a p-proaches from previous work. Experimental results are repo rted in section 5. Section 6 concludes the paper.
Although discussion boards are a popular destination for us ers looking for help, relatively little research directly addr esses the problem of mining question answering content from discussi on boards.

Cong et al. [8] was the first to address a problem similar to wha t we discuss in this paper. They developed a classification-ba sed method for question detection by using sequential pattern f eatures automatically extracted from both questions and non-quest ions in forums. They preprocessed each sentence from the first posts by applying a Part-Of-Speech (POS) tagger while keeping keywo rds including 5W1H (What, When, Why, Where, Which and How) words and modal words. The sequential pattern features are b ased on the results of the POS tagger. Though achieving reasonabl e per-formance, this approach suffers from the typically time-co nsuming POS analysis process. More importantly, the definition of  X  X  ues-tions X  in their work is slightly different from our work. The y fo-cused on question sentences or question paragraphs while we treat the first post as a whole if it is about a question. For the subta sk of finding answers, they proposed an unsupervised graph-bas ed ap-proach for ranking candidate answers leveraging the releva nce be-tween replied posts, the similarity between the replied pos t and the first post, and author information as well. We will show that o ur method outperforms their approach both in effectiveness an d effi-ciency.

A second related work is that of Ding et al. [9] who proposed a general framework based on Conditional Random Fields (CRF s) to detect the contexts and answers of questions from forum th reads. They did not address the question detection subtask in the wo rk and their approach is a complicated method that may not be applie d to larger datasets. Some features they used within the framewo rk are the same as what we will use in this paper. However, they did not provide a careful comparison of those features and show h ow different features contribute to the results.

In addition to these two directly related papers, there is so me re-search on knowledge acquisition from discussion boards. Zh ou and Hovy [34] presented a summarization system utilizing the in put-reply pairs extracted from online chat archives. Their syst em is not specifically designed for question answering content. F eng et al. [11] proposed a system to automatically answer studen ts X  queries by matching the reply posts from an annotated corpus of archived threaded discussions with students X  queries, whi ch is a different problem from our work. Huang et al. [15] presented an approach for extracting high-quality &lt;thread-title, repl y&gt; pairs as chat knowledge from online discussion boards so as to efficie ntly support the construction of a chat-bot for a certain domain. They also did not focus on question related threads in discussion boards.
Other previous work was trying to understand and mine discus -sion boards for more general purposes. Antonelli and Sapino [2] proposed a system to classify discussion threads based on ru les de-rived by using both speech acts and graph analysis. Although their system can identify questions and answers as well as other ty pes of threads, their dataset was small and they only provided pr ecision measures in their experimental results. Kim et al. [22, 21] a nd Feng et al. [12] used speech acts analysis to mine and assess discu ssion boards for understanding students X  activities and convers ation fo-cuses. They used only a small dataset and did not address ques tion answering content in their work. Lin and Cho [23] introduced sev-eral techniques to preprocess questions extracted from dis cussion board including  X  X arbage text X  removal, question segmenta tion and merging questions. They did not discuss how to identify ques tion content and their answers. Shrestha et al. [27] detected int errog-ative questions using a classification method and built a cla ssifier to find answers using lexical features based on similarity me asure-ment and email-specific features.

Compared to the problem we address, extensive research has been done on QA services like Yahoo! Answers or other Frequen t Asked Questions (FAQ) services. Jeon et al. [17, 16], Duan et al. [10], and Cao et al. [5] tackled the problem of finding ques -tions in the QA services that are semantically similar to a us er X  X  question. Song et al. [28] proposed a metric  X  X uestion utili ty X  for studying usefulness of questions and showed how question ut ility can be integrated into question search as static ranking. Je on et al. [18] presented a framework for using non-textual featur es like click counts to predict the quality of answers, incorporate d with language modeling-based retrieval model. Surdeanu et al. [ 29], Xue et al. [32], Berger et al. [3], Jijkoun et al. [19], and Rie zler et al. [26] described various retrieval models or systems to extract answers from QA or FAQ services. Liu et al. [24] proposed auto -matic summarization techniques to summarize answers for re -use purposes. Gyongyi et al. [13] performed an analysis of 10 mon ths of Yahoo! Answers data that provided insights into user beha vior and impact as well as into various aspects of the service and i ts pos-sible evolution. Some of the above work is complementary to o ur approach, and therefore could be employed to enhance our met hods but in general all work above does not need to detect question s.
Traditional Question Answering tasks in TREC style have bee n well studied; see for example Vorhees [31]. That work mainly fo-cused on constructing short answers for a relatively limite d types of questions, such as factoid questions, from a large corpus . This makes it possible to identify the answer type. In contrast, t ypical questions extracted in discussion boards are more complex a nd usu-ally consist of multiple sentences or even several paragrap hs, and it is also difficult to represent and identify answer types fo r those questions.
In this section, we discuss the problem in detail and then pre sent a definition of the problem.
If the first post of one thread is about a specific problem that needs to be solved, we would consider that post as a whole to be a question post. We do not focus on identifying  X  X uestion sent ences X  or  X  X uestion paragraphs X  but instead to find whether the first post is a  X  X uestion post X . Since users often express their proble ms in an informal way and questions are stated in various formats, it is difficult to recognize questions at the sentence or even para graph level.
 For example, the following paragraph is a question post from UbuntuForums.org, the official discussion board of Ubuntu L inux. Although the last sentence is a question sentence, it gives u s little information about what the real problem is. The true problem is the scenario the author described with several sentences as a whole. This post has another paragraph providing machine configura tions which we do not include here. Therefore, it is reasonable to t reat the whole post as a question post.

If there are multiple questions discussed in the first post, t he in-teraction in following replied posts might become complex ( e.g., users may answer all those questions while others may only re -sponse to some of them). To simplify the task, we treat it as a single question post.
If one of the replied posts contains answers to the questions pro-posed in the first post, we regard that reply as an answer post. As we discussed above, we do not consider the number of answers should match the number of questions. Additionally, we only con-sider those replies that directly answer the questions from the first post. We ignore other questions (usually elaborated from th e orig-inal ones) within replied posts and their corresponding ans wers. Although such answers may provide more information to the or ig-inal questions and therefore could be potential better answ ers, in reality, users need to understand all replied posts above to get an overall idea and answers would become less meaningful if we o nly extract that single reply as the answer to the first post.
We also consider replied posts not containing the actual con tent of answers but providing links to other answers as answer pos ts. If multiple posts provide links to other potential answers, we treat the first one as the answer post.
A discussion board is a collection of threads. Each thread co n-sists of the first post and following replied posts. Our task i s: 1. To detect whether the first post is a  X  X uestion post X  contai n-2. If the first post is a  X  X uestion post X , try to identify the be st Therefore, the result from our system is question-answer po st pairs. Ideally, users do not need other information (e.g., the post s between them) to understand these pairs.
We consider both subtasks described in Section 3 as classific a-tion problems. In this section, we introduce the features we use and a brief review of previous approaches.
For this subtask, we describe and use several features other re-searchers have used previously (e.g., question mark, 5W1H w ords) as well as features that are borrowed from other fields (e.g., N-gram). In summary, we use the number of question marks, the number of each 5W1H words, total number of posts within one thread and authorship (the number of posts one user starts and the numbe r of posts one user replies) as features.
In this subtask, we focus on how to detect answer posts withou t analyzing the content of each post using natural language pr ocess-ing techniques. We are also interested in how non-content fe atures can contribute to classification results. In summary, we use the position of the answer post, the author -ship, N-gram, the count of each stop word and the score of Quer y Likelihood Model as features.
We principally compare our method with the approaches intro -duced by Cong et al. [8], a recent work addressing a similar pr ob-lem. We briefly review their method below.

To detect the questions, they used the supervised learning a p-proach Sequential Pattern Mining. First, each sentence is p re-processed by a POS tagger only leaving 5W1H words and modal words. Then the sequential patterns are generated by a modifi ed version of the PrefixSpan algorithm [25] to incorporate both mini-mum support and minimum confidence, which are assigned empir -ically. They treat all generated patterns as features. They consid-ered  X  X inding answers X  as a retrieval problem. The retrieval model they introduced is a graph-based model incorporated with in ter-posts relevance, authorship and the similarity between rep lied posts and the first post. They showed two variations of the graph-ba sed model that one is combined with the Query Likelihood languag e model and another is combined with the KL-divergence langua ge model.

We implement all these methods and compare them in our ex-periments. Note that they did not explicitly define  X  X uestio n X  and  X  X nswer X . Therefore, our task may be slightly different fro m theirs.
We selected two discussion boards as our data source. We crawled 721,442 threads from Photography On The Net 4 , a digital http://photography-on-the.net/ camera forum (DC dataset), and 555,954 threads from UbuntuF o-rums 5 , an Ubuntu Linux community forum (Ubuntu dataset).
For the question detection subtask, we randomly sampled 572 threads from the Ubuntu dataset and 500 threads from the DC dataset. We manually labeled all first posts in these threads into question posts and non-question posts using our criteria in troduced in Section 3. For answer detection subtask, we selected 500 a ddi-tional question-related threads from both data sources. Th erefore, we have 2,580 posts in total (including the first posts) from t he Ubuntu dataset and 3,962 posts in total (including the first p osts) from the DC dataset. We manually labeled all posts into answe rs and non-answers. We note that in accordance with our problem definition, only one answer post per thread is labeled as such (the remainder are labeled as non-answers).

We preprocessed all posts by modifying possible abbreviati ons into their full form (e.g.,  X  X e X  X e X  into  X  X e are X ,  X  X t X  X  X  int o  X  X t is X ) and stemming all words. For Sequential Pattern Mining, the S tan-ford Log-linear Part-Of-Speech Tagger [30] was used and min i-mum support and minimum confidence were set to 1.5% and 80% respectively. For N-gram, we generated 3,114 N-grams (1-5 g rams) from the Ubuntu dataset and 1,604 N-grams from DC dataset for question detection while 2,600 N-grams from Ubuntu dataset and 1,503 N-grams from DC dataset for answer detection. For stop -words, we used 571 normal stop words. 6 We use LIBSVM 2.88 [7] as our classifier and all classification results are obtained through 10-fold cross validation. In order to avoid classification b ias and get better results, we balanced our data into around 50% positiv e sam-ples versus 50% negative samples in all experiments. For exa mple, we have 500 positive instances and 2080 negative instances f or an-swer detection on Ubuntu dataset. Therefore, we replicated the positive training instances four times to give 2,000 exampl es (but left the test set unchanged). Since in any real settings, the data is inherently skewed, a better learning approach such as cost-sensitive learning may be more realistic. Table 1 shows all the feature s we used and their abbreviations.
We first evaluate the performance of features introduced in S ec-tion 4.1 individually. Table 2 gives the results of precisio n, re-call, F-measure and accuracy (sorted by accuracy) of the Ubu ntu dataset and Table 3 shows the results from the DC dataset. It i s eas-ily to notice that Length , 5W1H and Question Mark , three simple heuristics, generally cannot give good performance while Sequen-tial Pattern Mining always outperforms these simple methods on http://ubuntuforums.org/ http://www.lextek.com/manuals/onix/stopwords2.html both datasets, which validates the experiments performed b y Cong et al. [8]. Additionally, the results show that Authorship is a much better heuristic and can achieve reasonable performance co mpared with Sequential Pattern Mining although it seems that performance may be dataset dependent. On both dataset, N-grams achieves the best performance in all metrics in terms of a single feature. This suggests that users do use certain language patterns to expr ess prob-lems and questions in discussion boards. Table 4 shows 10 sam ple N-grams extracted from DC dataset that were used for questio n de-tection. Note that the results are stemmed words.
 Since N-grams and Sequential Pattern Mining (which requires a POS tagger) are relatively complicated methods (vs. simple heuris-tics such as finding question marks and 5W1H words), the com-putational effort may be impractical for large datasets. In order to avoid high computation methods, we do further experiments o n the combinations of those simple methods and see whether the per for-mance can improve and therefore make simple combinations vi able alternatives.

Tables 5 and 6 show the combinations of simple features com-pared to N-grams and Sequential Pattern Mining . We observe that the performance can be improved by combining features. Spec if-ically, Authorship + Question Mark + 5W1H Words + Length achieved similar or even better results than Sequential Pattern Mining on both datasets. Notice that the computation of these feature s is much simpler than Sequential Pattern Mining . In addition, Ques-tion Mark + 5W1H Words + Length , which only requires local infor-mation, also achieved reasonable performance compared to t hose features individually since Authorship needs global information. From these results, we found that although these features in dividu-ally cannot give much evidence reflecting whether a post conc erns a question, the combination of them is able to characterize t he first post and interestingly, none of these simple features attem pts to un-derstand the real semantics of the question posts.

For this subtask, we first did the experiments using individ-ual features, as we did in Question Detection. In order to com -pare with the methods introduced by Cong et al. [8], we used the ranking score from their retrieval models as a feature to train our classifier. Since Graph-based model + Query Likelihood Model and Graph-based model + KL-divergence Model perform similarly on both datasets (shown later in Section 5.4), we only use Graph-based model + Query Likelihood Model in this subtask as an exam-ple.

Tables 7 and 8 show the experimental results. In general, Lan-guage Model and Graph+Query Likelihood Model did not perform well using the ranking score as features. A possible reason i s that these methods are mainly based on relevance retrieval m odels, which aim to find the information most relevant to the query (i n our case, the question posts). Since all posts within a question thread may be more or less relevant to the question, it is difficult to rank them and distinguish the best answers from others based on co ntent relevance or similarity measurement. In addition, relevan ce-based models may unable to handle big lexical gaps between questio ns and answers. We show one example from UbuntuForums below.
The first post: The answer post:
Notice that this answer post contains a web link while all  X  X e y-words X  (e.g., ubuntu 8.10, asus AS V3-P5V900, crash and etc. ) in the first post do not appear in the answer post. If we calculate Query Likelihood Model score for the answer post, nearly all words in the question post can only receive  X  X ackground X  smoothing scor e and hence the model would rank this post  X  X rrelevant X . Essentia lly the same situation happens when using similarity measurement ( e.g., cosine similarity).

N-gram did not outperform other features in this subtask, which suffers from various expressions in answer posts. Interest ingly, the Stopword approach has performance similar to N-gram in both datasets. N-gram usually requires more computational effort than Stopword since Stopword has a fixed number of features for all datasets while N-gram needs to be generated separately and usually contains thousands of features. Therefore, in our later exp eriments, we use Stopword instead of N-gram . We also note that Authorship and Position , two simple heuristics, perform reasonably well and achieve comparatively high F1-Score on both datasets.

Inspired by question detection subtask, we conducted exper i-ments using combinations of features on the two datasets. Ta bles 9 and 10 provide the corresponding results. In this subtask, we not only combine simple heuristics but also combine non-con tent features and content-based features. The first interesting finding is that Position + Authorship outperforms all other feature combina-tions and greatly improves the performance. This would expl ain that senior members usually answer questions in certain pos itions (e.g., near to the top post). This combination is easy to comp ute and there are no other parameters to tune.

In order to better understand how these two features contrib ute to the final results, we plot them in Figure 1 and Figure 2 for bo th datasets. The X-axis shows the ratio of the number of startin g posts versus follow-up posts for users who answered questio ns in our datasets. The Y-axis shows the ratio of the position of an swer posts from the top of the thread versus to the bottom. Both figu res demonstrate the obvious signal that most answer posts are cl ose to the top when the author of these posts are senior users who usu ally write replies rather than starting posts.

We also notice that the combination of content-based featur es (e.g., Language Model , Stop words ) and non-content features (e.g., Position , Authorship ) may also get better results compared to Table 7 and Table 8. The Position + Stopword combination performed rea-sonably well on both datasets, only requires local informat ion, and is simpler than any kind of relevance-based features. In gen eral, we can see that performance benefits from a combination of featu res, especially those simple features. Additionally, the combi nation of non-content and content features also improves performanc e sig-nificantly.
We also propose a simple ranking scheme based on the classifi-cation method. The ranking score is simply computed by linea rly combining position and authorship information: where V 1 , V 2 and V 3 are scores from classifiers of combination of position and authorship, position only and authorship only respec-tively.  X  and  X  are empirical parameters and we set 0.6 to both of them.
 Table 11 shows the results compared to basic Query Likelihoo d Language Model, Graph-based+KL-divergence model propose d by [8] in terms of Precision@1 and Mean Reciprocal Rank (MRR) where MRR is the mean of the reciprocal ranks of the answers ov er a set of questions. Our ranking scheme outperforms other pre vious relevance-based approaches. In this paper we defined the problem of selecting Question and Answer post pairs from discussion boards and addressed it as a classification problem. The contributions of this paper inc lude: 1. We show that the use of N-grams and the combination of 2. We show that the number of posts a user starts and the num-3. We show that relevance-based retrieval methods would not 4. Using classification results, we are able to design a simpl e Future work might consider the following problems. 1. This work only addresses answer posts that directly an-2. This work does not consider the number of questions in the
This work explicitly defines the problem of selecting questi on answering post pairs from discussion boards and shows bette r per-formance compared to previous approaches. We believe that t his is a first step toward a better understanding of the interacti on of question answering in such media.
 This work was supported in part by a grant from the National Science Foundation under award IIS-0545875. We appreciate the thoughtful discussions with XiaoGuang Qi, Na Dai and Jian Wa ng. [1] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and [2] F. Antonelli and M. Sapino. A rule based approach to [3] A. Berger, R. Caruana, D. Cohn, D. Freitag, and V. Mittal. [4] M. Bouguessa, B. Dumoulin, and S. Wang. Identifying [5] Y. Cao, H. Duan, C.-Y. Lin, Y. Yu, and H.-W. Hon.
 [6] V. R. Carvalho and W. W. Cohen. Improving email speech [7] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support [8] G. Cong, L. Wang, C.-Y. Lin, Y.-I. Song, and Y. Sun. Findin g [9] S. Ding, G. Cong, C. Lin, and X. Zhu. Using conditional [10] H. Duan, Y. Cao, C.-Y. Lin, and Y. Yu. Searching question s [11] D. Feng, E. Shaw, J. Kim, and E. Hovy. An intelligent [12] D. Feng, E. Shaw, J. Kim, and E. Hovy. Learning to detect [13] Z. Gy X ngyi, G. Koutrika, J. Pedersen, and H. Garcia-Mol ina. [14] M. Hu, E.-P. Lim, A. Sun, H. W. Lauw, and B.-Q. Vuong. On [15] J. Huang, M. Zhou, and D. Yang. Extracting chatbot [16] J. Jeon, W. B. Croft, and J. H. Lee. Finding semantically [17] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar quest ions [18] J. Jeon, W. B. Croft, J. H. Lee, and S. Park. A framework to [19] V. Jijkoun and M. de Rijke. Retrieving answers from [20] P. Jurczyk and E. Agichtein. Discovering authorities i n [21] J. Kim, G. Chern, D. Feng, E. Shaw, and E. Hovy. Mining [22] J. Kim, E. Shaw, D. Feng, C. Beal, and E. Hovy. Modeling [23] C.-J. Lin and C.-H. Cho. Question pre-processing in a QA [24] Y. Liu, S. Li, Y. Cao, C.-Y. Lin, D. Han, and Y. Yu. [25] J. Pei, J. Han, B. Mortazavi-Asl, H. Pinto, Q. Chen, [26] S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal , and [27] L. Shrestha and K. McKeown. Detection of question-answ er [28] Y.-I. Song, C.-Y. Lin, Y. Cao, and H.-C. Rim. Question [29] M. Surdeanu, M. Ciaramita, and H. Zaragoza. Learning to [30] K. Toutanova and C. D. Manning. Enriching the knowledge [31] E. M. Voorhees. The TREC question answering track. Nat. [32] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for [33] J. Zhang, M. S. Ackerman, and L. Adamic. Expertise [34] L. Zhou and E. Hovy. Digesting virtual  X  X eek X  culture: t he
