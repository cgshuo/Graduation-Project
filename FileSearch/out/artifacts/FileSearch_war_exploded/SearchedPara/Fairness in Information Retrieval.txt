 The offline evaluation of Information Retrieval (IR) systems is performed through the use of test collections. A test col-lection, in its essence, is composed of: a collection of doc-uments, a set of topics and, a set of relevance assessments for each topic, derived from the collection of documents. Ideally, for each topic, all the documents of the test col-lection should be judged, but due to the dimensions of the collections of documents, and their exponential growth over the years, this practice soon became impractical. There-fore, early in IR history, this problem has been addressed through the use of the pooling method [5]. The pooling method consists of optimizing the relevance assessment pro-cess by pooling the documents retrieved by different search engines following a particular pooling strategy. The most common one consists on pooling the top d documents of each run. The pool is constructed from systems taking part in a challenge for which the collection was made, at a specific point in time, after which the collection is generally frozen in terms of relevance judgments. This method leads to a bias called pool bias , which is the effect that documents that were not selected in the pool created from the original runs will never be considered relevant . Thereby, this bias affects the evaluation of a system that has not been part of the pool, with any IR evaluation measures, making the comparison with pooled systems unfair .

IR measures have evolved over the years and become more and more complex and difficult to interpret. Witnessing a need in industry for measures that  X  X ake sense X , I focus on the problematics of the two fundamental IR evaluation mea-sures, Precision at cut-off P @ n and Recall at cut-off R @ n . There are two reasons to consider such  X  X imple X  metrics: first, they are cornerstones for many other developed metrics and, second, they are easy to understand by all users. To the eyes of a practitioner, these two evaluation measures are interest-ing because they lead to more intuitive interpretations like, how much time people are reading useless documents (low precision), or how many relevant documents they are miss-ing (low recall). But this last interpretation, due to the fact that recall is inversely proportional to the number of rele-vant documents per topic, is very difficult to be addressed if to be judged is just a portion of the collection of docu-ments, as it is done when using the pooling method. To tackle this problem, another kind of evaluation has been de-veloped, based on measuring how much an IR system makes documents accessible [1]. Accessibility measures can be seen as a complementary evaluation to recall because they pro-vide information on whether some relevant documents are not retrieved due to an unfairness in accessibility.
The main goal of this Ph.D. is to increase the stability and reusability of existing test collections, when to be evaluated are systems in terms of precision, recall, and accessibility. The outcome will be: the development of a novel estimator to tackle the pool bias issue for P @ n [4], and R @ n , a com-prehensive analysis of the effect of the estimator on varying pooling strategies [3], and finally, to support the evaluation of recall, an analytic approach to the evaluation of accessi-bility measures [2].
  X  Information systems  X  Retrieval effectiveness; Test collections; Pool Bias, Pooling Method, P@n, R@n, Accessibility This research was supported by the Austrian Science Fund (FWF) project number P25905-N23 (ADmIRE).
 [1] L. Azzopardi and V. Vinay. Accessibility in information [2] A. Lipani, M. Lupu, A. Aizawa, and A. Hanbury. An [3] A. Lipani, M. Lupu, and A. Hanbury. The curious [4] A. Lipani, M. Lupu, and A. Hanbury. Splitting water: [5] K. Sp  X  arck Jones and C. J. van Rijsbergen. Report on
