 With the explosive growth of online videos, automatic real-time categorization of Web videos plays a key role for organizing, brows-ing and retrieving the huge amount of videos on the Web. Previous work shows that, in addition to text features, content features of videos are also useful for Web video classification. Unfortunately, extracting content features is computationally prohibitive for real-time video classification. In this paper we propose a novel video classification framework that is able to exploit both content and text features for video classification while avoiding the expensive computation of extracting content features at classification time. The main idea of our approach is to utilize the content features ex-tracted from training data to enrich the text based semantic kernels, yielding content-enriched semantic kernels. The content-enriched semantic kernels enable to utilize both content and text features for classifying new videos without extracting their content features. The experimental results show that our approach significantly out-performs the state-of-the-art video classification methods. H.3 [ Information Systems ]: Information Storage and Retrieval Algorithms, Experimentation, Performance Web, Video, Content, text, Classification
Recent years have witnessed an explosive growth of online video sharing and many Web sites provide video sharing services such as YouTube, GoogleVideo, YahooVideo, MySpace, and ClipShack. These video sharing Web sites often organize online videos into categories so that users can browse videos by categories and search within a certain category.

The category labels in the video sharing Web sites are usually provided by users when they upload videos to the Web sites. How-ever, the manual annotation of category label for the Web videos suffers from some problems. For example, it can be burdensome for users to choose an appropriate category for a video; and users generally have different understandings on video categories, and thus the user labeled categories are often inconsistent. Hence, it will be highly desirable to be able to automatically suggest cate-gories based on the text description and content of video even if the suggestion may not be perfect. Additionally, some Web sites, e.g., Google Video, collect not only the Web videos uploaded by users in video sharing Web sites, but also the videos embedded in news websites, blogs, etc. For the latter, the videos usually do not have category information. Hence, automatic video categorization is essential to determine categories of such videos so that the videos without user labeled categories can also be organized in the same way as the videos having category labels.

The semantics of videos are described by both content features and textual descriptions. On one hand, most Web videos are asso-ciated with text descriptions: almost every video sharing Web site (e.g., YouTube) requires users to attach tags and descriptions; most videos from other circumstances, e.g., blogsphere and news Web sites, are also surrounded by text. On the other hand, Web videos are visual content-rich, e.g., color, texture, shape, etc. The previ-ous work on automatic video classification [4, 26, 16] exploits both text features and content features for video categorization, and the results reported in the existing work show that text features con-sistently outperform content features for video classification; how-ever, the performance of using both text features and content fea-tures is better than the performance of using either of them alone.
Although content features can contribute to video classification, extracting content features is computationally expensive. The time cost of extracting content features is generally in the same order of magnitude with the video length. For example, it is reported [9] that the speed of extracting some visual features, e.g., SIFT, is less than 10 frames per second. The expensive computational cost renders the previous work utilizing content features impractical in two typical application scenarios: 1) to automatically categorize huge volumes of Web videos, and 2) to provide real-time category suggestions for a new uploaded video in a video sharing Web site.
The existing work on video classification [4, 26, 16] simply treats the text description of a video as a bag of words to build classifier as in text classification for general documents. However, the text as-sociated with Web video has special characteristics compared with normal text documents. First, the text descriptions of videos are usually quite short (tens of words). Additionally the descriptions of different videos usually share very few identical words. That is, the feature space is extreme sparse. Second, the text descriptions (often generated from users) of Web videos often contain special words, such as names of persons or organizations, acronyms (e.g.  X  X NA X  for  X  X otal Nonstop Action") and Web language.

The extreme sparsity limits the performance of most existing text classification methods. One would be tempted to employ the clas-sification methods based on semantic kernels using WordNet or Word co-occurrence for Web video classification to alleviate the sparsity problem. However, the large number of special words are not covered by Wordnet and this will limit the performance of clas-sification method [3] using WordNet based semantic kernels; the method [22] based on co-occurrences of words in a collection itself is limited by the feature sparsity problem.

To this end, in this paper we propose a new video classification framework that exploits features from both text and visual content in a novel way. Specifically, we integrate visual content features ex-tracted from the training data into the computation of the semantic similarity between words to enrich the text-based semantic similar-ity. Based on this novel similarity measure, we construct content-enriched semantic kernel , and employ it to build a text-based clas-sifier. At classification time, we do not need to extract the content features of videos to be classified, which is computationally expen-sive, while we are still able to implicitly utilize the content features to promote classification performance.

Compared with existing work on video classification, the pro-posed framework has a salient feature: it incorporates content fea-tures into building text-based classifier without sacrificing efficiency at the classification stage. To further improve the classification per-formance, we also employ Multi-Kernel SVM techniques to com-bine multiple kernels including th e proposed content-enriched se-mantic kernel and text-based semantic kernels. In a summary, this paper makes the following contributions: 1. We present a novel framework that is able to exploit both 2. We implement this framework by introducing Content-Enriched 3. We conduct an extensive performance study on a large real-
The rest of this paper is organized as follows. Section 2 dis-cusses related work. In Section 3, we present the proposed video classification framework and Content-Enriched Similarity (CES). The proposed approach is evaluated in Section 4. We conclude this paper in Section 5.
With the increasing availability of online videos, automatic video categorization has attracted increasing attention recently [4, 26, 15, 16, 19, 23]. We summarize the existing work into three video clas-sification frameworks. Text-based Framework: The framework is given in Figure 1(a). Existing approaches in this framework cast video classification as a text classification task. Each video is represented with  X  X  bag of word X  features and text classification models are used to build clas-sifiers. Approaches in this framework avoid the expensive compu-tation on extracting content features and can be efficient in classi-fication time. However, the video classification methods based on text feature alone have two disadvantages. First, they cannot lever-age the rich information contained in video content. Second, the sparsity of text features for online videos limits the performance of these approaches as discussed in Introduction.
 Content-based Framework: Figure 1(b) shows the framework. The approaches in this framework use visual content features, e.g., color, texture and edge, to construct the classifiers [15, 26, 19, 23]. Different classification models are employed, such as rule based model[15], decision tree [23] and SVM [19]. Several categoriza-tion methods are used in [4, 26], such as Naive Bayes, Maximum Entropy, SVM, etc., and it is shown that SVM obtains the best per-formance. The content features are less effective in general than text features, although they perform better on some video cate-gories. In addition, extracting content features is very time consum-ing, which limits their application on a very large video database. Fusion Framework using Both Text and Content Features: The framework is shown in Figure 1(c), and is adopted in the re-cent work [4, 26]. These approaches build separate classifiers us-ing text features or different content features. At the classification time, these approaches extract the content features for a new video, and the results from separate classifiers are fused to determine the category of the video. As for the fusion methods, [4] uses a vot-ing scheme, the classification is based on a linear combination of results of separate classifiers in [26], and [16] uses the judgment for each class from each classifier as features, and then builds a meta-classifier to make the final decision. These approaches [4, 26] perform better than the approaches using text features or con-tent features alone. However, this framework also suffers from the expensive computational cost of extracting content features at the classification time. Moreover, they build separate classifiers inde-pendently and then employ fusion methods to combine the results. This misses the correlation of different features for classification.
Since text-based classifiers play significant roles in video classi-fication [26, 4], we also introduce related work on text classifica-tion. Many techniques have been proposed in text categorization field [8, 21, 27]. According to the surveys in [21, 27], SVM of-ten outperforms other methods. There is a lot of work on semantic kernels using SVM for categorization to explore the semantic sim-ilarity between words. Bloehdorn et al. [2] summarized previous work on semantic kernels and proposed a general framework for semantic kernels. The framework [2] divides the methods of se-mantic kernels into two categories. First, syntactic structures of sentences are extracted and Tree Kernels [18] have been proposed to exploit the parse trees of sentences. Second, ontology and term co-occurrence have been employed to compute the semantic simi-larity between words and this semantic similarity is used in kernel functions. WordNet [10] is used in [3] and term co-occurrence [6] is used in [22] to derive a Latent Semantic Kernel. The idea behind them is to incorporate a similarity matrix that records the similarity between tokens (words or latent semantic words), and then use this matrix for semantic expansion of original feature space.
To our knowledge, none of previous work on video classifica-tion employs semantic kernels for video classification. Even if the semantic kernels are used, it can only partly address the feature sparsity problem for Web video classification: The text descrip-tions of online videos contain a large number of words that are not in WordNet and this limits the classification performance of seman-tic kernels based on WordNet. Additionally, the video descriptions are usually short and generated by users. Users may not often use two words with similar senses in a short description. For example, consider two synonyms bunny and rabbit . Most users will not use both words to describe animal rabbit in a short video description. Therefore, this limits the effectiveness of Semantic Kernels using co-occurrence Similarity for Web video classification.
 We briefly introduce SVM, kernel function and Multi-Kernel SVM[7, 17] to provide background of the proposed approach. SVM trains a hyper-plane to separate data of two categories while mini-mizing the empirical risk. Various non-linear kernels are proposed for classification problems where the data in different categories is not linearly separable. The idea is to apply a non-linear mapping of data to a feature space, in which the linear SVM method can be used. This mapping is defined as  X  : X  X  F ,where X is the original space, and F is the feature space. Because it is difficult to build a feature space directly, instead kernel functions are used to implicitly define the feature space.
 D EFINITION 1(K ERNEL F UNCTION ). A kernel is a function K , such that for all x , z  X  X,K ( x , z )=  X  ( x )  X  ( z mapping from X to an (inner product) feature space F .

The works of semantic kernel aim at defining the different func-tion  X  ( . ) satisfying the above definition using knowledge from Word-Net or cooccurence statistics.

Two word-similarity matrices are popularly used in defining se-mantic kernels. First, several semantic similarity measure [25, 14] between words are defined based on WordNet . Second, semantic similarity between words is also computed by Word Co-occurrence. Intuitively, two words are similar when they frequently appear to-gether in some documents. The relationship between words oc-curring in the same document is called co-occurrence. Term CO-Occurrence (COO) is a popular similarity measure of words. If we represent each word as a vector of term frequency ( tf ) in each document, then COO of two words can be computed by the cosine similarity of their vectors.
In this section, we first present the proposed framework for video classification, then introduce the core component of the proposed framework, namely content-enriched semantic kernels, and finally present the algorithm for Web video classification.
Our proposed framework for online video classification is shown in Figure 2. The framework consists of the following steps: given a training video data, we extract both text and visual features from it. After that, we obtain Content-Enriched Similarity (CES) between words (to be explained in the next subsection), and extend the se-mantic kernel technique to the CES to build a video classifier. At the classification stage, this classifier classifies a new video using its text features (but not its content features).

The idea is that we incorporate visual content information ex-tracted from training data into the semantic similarity between words. That is, classifier is built using the kernels based on the semantic similarity that considers both text and content information. The de-sign mechanism enables us to utilize visual clues of Web videos to obtain more reasonable semantic similarity among words.

Although the proposed framework makes use of both text fea-tures and content features as the fusion framework in Figure 1(c), it is very different from the fusion framework in the following two aspects: First, in the fusion methods [4, 16] content features are em-ployed as features to build classifier, and thus the content features of video to be classified need to be extracted at classification time. In contrast, in the proposed framework content features of training data are used to calculate Content-Enriched Similarity (CES) be-tween words and the similarity is integrated into semantic kernels to build classifier. We avoid extracting content features from the video to be classified at the classification time. Second, the text classifiers built using fusion methods[4, 16] for video categoriza-tion do not consider the semantic information of words.

Compared with existing video classification frameworks, the pro-posed framework has two salient advantages. First, our framework is comparable to the text-based framework in Figure 1(a) in terms of classification efficiency. Both frameworks do not need to ex-tract content features at the classification stage. This is essential for a framework to be applied to real-time online video classification and classifying online videos of a large scale. Second, our frame-work is able to achieve better classification accuracy than existing approaches, because our approach can not only take advantage of both text and content features, but also effectively address the prob-lem of sparse features in Web video categorization.
In the existing classification work, only the text information is taken into consideration for kernel construction, which is insuffi-cient for Web video classification as we discussed previously.
The motivation of proposing Content-Enriched Similarity is based on two key observations. First, the effects of text feature and con-tent feature are typically complementary: for some categories (e.g., news video, music video), text-based classifiers have better accu-racy than content-based classifiers; while for other categories, like films, content-based classifiers work better [4, 26]. The combina-tion of two types of features is more effective for video classifica-tion as shown in [4, 26], although text-based approaches generally perform better than content-base d approaches. S econd, u tilizing content feature in video classification stage is computationally ex-pensive, as content feature extraction is time consuming.
Ideally we can have a way to leverage content features without extracting content features at the classification time. This might sound impossible at first glance. Our idea is to use content fea-tures extracted from training data to enhance the text features to obtain content-enriched semantic kernels for classification. This idea is inspired by the way that semantic similarity is used to en-hance the text features in semantic kernel. However, we will see that we compute the content-enriched semantic kernels in a very different fashion from existing work on semantic kernels.
We proceed to introduce the key idea of semantic kernels, and present the idea of integrating content features into kernels for clas-sification. The key component of semantic kernels is a word sim-ilarity matrix. We denote the matrix by P , where each matrix en-try P ij represents the semantic similarity between words i and j . The semantic similarity between two words can be computed using WordNet or term co-occurrence as presented in Section 2.2. Fol-lowing the work on text classification using semantic kernel, e.g., [3], we can compute the semantic kernels using similarity matrix P as follows: where X is the document-word matrix and X ij gives the term fre-quency ( tf )ofword j in document i .

Although WordNet or co-occurrence based similarity measures can capture some semantic similarities between words, they are still not sufficient for online video classification and they ignore the content information of videos.

Our idea is to exploit the content features to enhance the similar-ity matrix P , so that we are able to utilize the content features. If we have the Content-Enriched Similarity matrix, we can define the content-enriched semantic kernel similarly as it is in Equation 1. We next present how to compute Content-Enriched Similarity be-tween words and how to derive matrix P using Content-Enriched Similarity.

We observe that similar words often appear in the text descrip-tions of similar videos although they may not appear in text de-scription of single video. Here, we use the visual content infor-mation of videos to decide whether two videos are similar. If two videos are similar in terms of their content information, the words in their text description would be somehow similar. It is expected that we can capture the similarity between words even if they do not appear in the text description of a single video, e.g., bunny and rabbit , since they may appear in the text descriptions of different videos that all contain visual features of animal rabbit . We notice that the similarity between bunny and rabbit can also be captured by WordNet (but not term co-occurrence). However the coverage of WordNet is only about 50% in our video collection downloaded from YouTube. As another example, Content-Enriched Similar-ity can also capture the word similarity between or favorite and favourite . Note that, for the example of bunny and rabbit , content features like shape and color can associate these two words. For the example of favorite and favourite , there are no specified features; however, as these two words are often used alternatively, the visual characters of their associated videos are similar statistically. These relations cannot be extracted by WordNet and term co-occurrence (more examples will be given in Section 4.2).
 We proceed to present the method of computing Content-Enriched Similarity. We first extract content features of training data, and then cluster videos based on visual content features to obtain k clusters, C 1 ,... C k ,where k is a parameter to be determined on training data. Generally, we expect that two words are similar if they appear in the same cluster, within which the videos are simi-lar in terms of content. This statistical co-occurrence information implies the semantic tightness between two words, i.e., the higher frequency of co-occurrence represents closer similarity. The pa-rameter k , the number of clusters, will affect the similarity mea-sure between words. With a small k we will get large clusters, and thus two dissimilar videos may be included in a cluster. This will degrade the similarity implicated by co-occurrence. On the con-trary, if too many clusters are generated with a large k , some words may be only related with weak connections indicated by infrequent co-occurrences. The weak connections are not always informative: they may introduce noise in the Content-Enriched Similarity, how-ever we will miss similarity relationships if we prune all weak con-nections. Hence, we need to use an appropriate value for parameter k , which can be set empirically using a development set, if any, or using cross-validation on training set.

After we have k clusters, we next compute the word similarity in the space of k clusters. For each word w , it is associated with a vec-tor in document (video) space, i.e., w = &lt;tf w, 1 ,tf w, 2 &gt; ,where tf w,i is the frequency of term w in the i th text descrip-tion, and | D | is the number of text descriptions (videos) in the col-lection. In order to compute the frequencies of word w in each cluster, we project the vector w from document space to cluster space. To do this, we need to define a video-cluster relation ma-trix VS , where each entry VS ij =1 , if video i is in cluster C otherwise VS ij =0 . Then we can project w to w c as follows. w of word w in cluster i .

Given two words x and y , together with their term frequency vectors in document space, x = &lt;tf x, 1 ,tf x, 2 , ..., tf y vectors in cluster space according to Equation 2. Then we can com-pute their semantic similarity using cosine similarity as follows. where VS is the video-cluster relation matrix presented earlier.
In practice, we can use matrix calculation to compute the content-enriched semantic similarity for all word pairs. Given the document-word matrix X and video-cluster relation matrix VS ,wefirstcom-pute the product of the two matrices Y = X T  X  VS .Matrix Y is a word-cluster matrix and we need normalize the matrix to compute cosine similarity. For each row vector y of Y , we normalize it by Enriched Similarity matrix CES can be calculated as follows: It is natural to derive explanation for Equation 4. Y=X T  X  gives the distribution of words in the cluster space, and according to Equation 4, word pairs occurring frequently in the same clusters will have larger similarity score. The visual content information of videos is incorporated in the matrix VS without introducing expen-sive computation in matrix processing.
The Content-Enriched Similarity matrix CES is used to replace the matrix P in Equation 1 to compute the Content-Enriched Simi-larity kernel K . In the previous section, we proposed a novel Content-Enriched Similarity measurement to evaluate the similarity between words. To theoretically justify our approach can effectively capture the similarity information, we start from Pointwise Mutual Informa-tion (PMI) in information theory [5], which is a measure of as-sociation/similarity between two objects (words) and defined as on solid statistical theory.

Let x , y be two words in vocabulary set V , d a video in video dataset D .Then P ( x, y ) can be derived by: where  X  denotes the values are proportional, P ( d i = d j probability that video d i and d j are equal. Then, we can compute P ( x ) using this P ( x, y ) , i.e., Substituting Formulas 5 and 6 into PMI , we can get
Now, the remaining task is to estimate the value of P ( d and P ( x  X  d i ,y  X  d j | d i = d j ) .

In the traditional co-occurrence similarity calculation, only the words appearing in the same video text description are considered to be similar, i.e., i must be equal to j . In our proposed CES mecha-nism, P ( d i = d j ) can be seen as the probability that videos d d are similar, e.g., d i and d j are in the same cluster. A straightfor-ward method can be used to estimate P ( d i = d j ) which is denoted by e ij as:
To compute the value of P ( x  X  d i ,y  X  d j | d i = d j ) , we can adopt the model based on term frequency ( tf ). There exist many reasonable estimation methods, e.g., the popularly used two models as follows: or Integrating Formula 9 into SI ( x, y ) ,wehave:  X  i,j e ij tf ( x, d i ) tf ( y,d j )
The above theory is a natural extension of state-of-the-art ap-proaches by introducing a new parameter of e ij , i.e., P ( d There may exist the case that i = j but P ( d i = d j ) =0 ;inother words, the similar videos can contribute to word similarity compu-tation.

Recall the aforementioned co-occu rrence similarity computation which uses inner product as semantic similarity measurement. The inner product of co-occurrence measurement is based on the as-sumption that e ij is not equal to 0 if and only if i = j . Our mecha-nism relaxes this restriction that we apply the inner production in an un-orthogonal space, where e ij is not equal to 0 if videos i and j are similar, e.g., videos are in same cluster.

Let x and y represent the tf vectors of words x and y respec-tively. We can redefine the dot function to compute the similarity: where &lt; x , y &gt; is i,j tf ( x, d i ) tf ( y,d j ) e This equation can be easily proved in a 2-norm Euclid space. Ob-viously, Sim ( x, y ) =0 does not require words x and y to appear in the same video. However, it relies on e ij , which represents the similarity between the videos i and j .

Revisit Formula 11, let x and y in SI ( x, y ) be also represented by two tf vectors, we have
After normalizing x and y to unit norm [12], i.e., | x | = we can get
The above formula shows that our similarity measure is consis-tent with PMI model, i.e, if there is a tighter  X  X emantic" association between words x and y , the similarity value will be higher. In other words, the Content-Enriched Similarity is capable of capturing the semantic relation between words, and hence can be applied to build semantic kernel for video classifier. Moreover, our discussion is not limited to 2-norm Euclid space, and one can derive a formula in an-other space, e.g., 1-norm space, by making a different estimation of p ( x, y ) and p ( x ) , using Equation 10 instead of 9.
In this subsection, we summarize the procedure of building clas-sifier using the content-enriched mechanism and applying classifier at the classification stage.
 The process for building classifiers is presented in Algorithm 1. To build classifiers using the Content-enriched semantic kernels, three steps are processed for the training dataset.
 Feature extraction (line 1): We extract the text feature from the text description, and visual content feature from video such as color and texture.
 CES computation (lines 3-10): We first employ k -means cluster-ing method to cluster the training data into k clusters using content features extracted from training data. And then, we construct a video-cluster relation matrix VS to compute the Content-enriched Similarity between the words in the word set by Formula 4. Building Classifiers (lines 11-12): For each category in the train-ing dataset, we build SVM classifier employing the Content-enriched kernel. As Web video sets are multi-class sets, we use One-Against-All [11, 17] method, one of the most widely used methods for multi-class classification, to train classifiers. Thus, each category will have a classifier. Algorithm 1 : Algorithm for building CES classifiers Input : Labeled training video data, and parameter k Result : A set of classifiers
Extract text and content features;
Cluster the dataset into k groups; foreach Vi deo v i do foreach word-pair in Vocabulary Set do
Construct a CES kernel by Formula 1; foreach category do return Classifiers .
At the classification stage, we use only text features of new video, but not its content features, as the input to the classifiers. Each classifier will return a score to indicate the possibility of the video belonging to the category; the class label of the classifier which returns the highest score is assigned to the video.
We proceed to extend the proposed content-enriched framework using Multi-Kernel SVM techniques [13, 20]. When various ker-nel functions exist for a classification job, Multi-Kernel learning optimization can take the advantage of individual kernel function and converge towards a more reasonable solution. For example, [13] proposed the Multi-Kernel t echnique that defines a new opti-mization function and solves it as Semi-definite Programming and Quadratically Constrained Quadratic Programming.
 Multi-Kernel SVM techniques can be applied to the proposed CES scheme that only uses text features at the classification stage. Therefore, we can generate different types of kernels for multiple kernel optimization, e.g., the content-enriched semantic kernels and text-based semantic kernels using WordNet or term co-occurrence.
In order to assign appropriate weights to different types of ker-nels, we adopt a standard Multi-Kernel scheme [20]. Given several kernels created using different word pair-wise similarity matrices as we discussed above, we first train a linear combination between them, and then use this fusion result as the final kernel. The Multi-Kernel optimization can effectively explore the advantages of var-ious similarity measures, and will not have obvious effect on the classification efficiency as only the text features are considered at the classification stage of our framework.
In this section, we evaluate our proposed method for online video classification by comparing with state-of-the-art classification tech-niques. All experiments were conducted on a PC running Linux with 2.4 GHz CPU and 3G memory.
In order to evaluate the performance of our approach, we collect the real-life video data from YouTube ( http://www.youtube. com ). During the process of crawling the video data, we collect the videos uploaded to YouTube every 5 minutes by YouTube API on Sep 23 &amp; 24, 2009, which are denoted as  X  X T923 X  and  X  X T924 X  respectively. This makes our collected two data sets be represen-tative of the distribution of YouTube videos. Finally, 5149 videos from  X  X T923 X  and 4447 videos from  X  X T924 X  are collected. For our studies, the datasets of  X  X T923 X  and  X  X T924 X  are taken as our training set and testing set, respectively. The Youtube videos are or-ganized with 15 categories, and the videos belonging to  X  X omedy",  X  X usic" and  X  X ntertainment" attract more interests relatively.
Both the text descriptions and visual content features of videos are extracted for Web video classification task. In our study, the text features extracted from videos include  X  X ideo title X  and the descrip-tions provided by users who uploaded the videos. We do stemming on these text features by using a standard WordNet stemmer and re-move stop words. Employing a similar filtering mechanism in [3], we eliminate words with frequency less than 5 to remove the noisy text features in our experiments. For the visual content features of our videos, we extract the followings features: color (24D), texture (8D) and edge (180D) features.
We evaluate both effectiveness and efficiency in our performance study. For efficiency, we concern more on the time cost at the clas-sification stage, as the classifiers are generated offline. The effec-tiveness performance in terms of accuracy of our video classifica-tion problem is measured as an F-score for classification results. It is defined as (2 pr ) / ( p + r ) ,where p is the precision (the number of correct results divided by the number of all returned results) and r represents the recall value (the number of correct results divided by the number of results that should have been returned). F-score can be calculated for each category and then averaged, or can be cal-culated over all decisions and then averaged. The former is called Macro-F, and the latter is called micro-F.
In the proposed content-enriched Classifier, we first generate the clusters based on the visual content feature of video as introduced in Section 3. The number of clusters k is the key parameter to com-pute the matrix, which may affect the similarity between words and hence the classification performance. We use k-means clustering algorithm and compute the enriched similarity matrixes using the training data with 5149 videos. We have observed the performance of classification is poor when the number of cluster is too large or small. If k is small, i.e., the cluster has more videos averagely, two dissimilar videos may be included in a cluster, which degrades the similarity implicated by word co-occurrence in an identical clus-ter. Additionally, more word relationships are generated, which makes the kernel space denser and difficult to be separated by SVM classifiers. On the contrary, if the number of clusters is large, the clustering based mechanism cannot extract enough similarity rela-tionships. Based on the experimental results, we set the number of clusters for generating enriched semantic kernels as 100, which will be used as default parameter in the rest of our experiments. The detailed tuning results are omitted due to the space constraint.
The content-enriched word similarity is the core component in our CES framework. The CES based on content feature can capture some interesting relationships between words through clustering of visual information, which cannot be discovered by other text-based methods. Some examples are illust rated in Table 1. We also show the results of WordNet and co-occurrence (COO) based methods which are generally used for text classifier.
 Table 1: Example of Word Similarity by Different Approaches
We observe that the word pairs in Table 1 are quite  X  X ontent-enriched" in several meaningful ways. First, we can extract WordNet-like type-of relations, e.g., (goose, bird) and (wombat, animal) ,be-cause they often point to similar objects, which can be analyzed easily using visual features instead of text features. Second, we can extract Visual Similarities between words, which can be rep-resented by own-properties-of relations, e.g. (jasmine, fragrance) and (rose, fragrance) . These relations may not be simply extracted by only using text features. For example, visual similarity between videos captures the relationship for flower jasmine and rose ,and this relationship can be utilized to further predict their relation with fragrance . Finally, it is interesting that we find our methods can even discover some typos and synonyms in different languages. For example, (francesco, francisco) is mined by CES, which seems like a common typo; (favorite, favourite) is also detected: one of them is American English, and the other is British English. It is difficult for text-based approaches to discover such relationships, because it seems unlikely that a user uses both words to describe the video.
Note that, there exist some noises in the extracted word relation-ships, and some discovered relationships may not precisely repre-sent the similarity between words. However, it is difficult, if not impossible, to evaluate the performance quantitatively, due to the absences of golden-standard word similarity/relationship and eval-uation methods, as the text descriptions for Web video are generally free text and with diverse content. The results demonstrated in Ta-ble 1 show that these relationships discovered by CES are meaning-ful and agree with our common sense, and the classification results reflect the superiority of our proposed methods.
In this subsection, we report our experimental results on the comparisons among four frameworks including our proposal and three existing frameworks introduced in Section 2.1. We compare our proposed CES method with other competitors, i.e., Text [3] that represents the classifier based on the text information of Web videos, Content that represents the classifier based on the visual content of videos, and Fusion that explores multi-modal infor-mation of videos by fusing the results from Text and Content classifiers [4, 26]. Note that, the Text classifier can expand word similarity to form SVM kernels based on the WordNet and word co-occurrence as introduced in Section 2.2, which performs better than linear SVM based on text feature. The WordNet based kernel yields similar performance with the co-occurrence based kernel in our experiment, and we integrate word co-occurrence based kernel in the Text classifier using the approach [3] in our study.
Table 2 shows the performance on classification effectiveness of different frameworks. We find that the performance considering only content features is almost 50% lower than that of utilizing text features in general. There exists semantic gap between visual content feature and video information, thus direct application of visual feature cannot achieve satisfactory results.
 Table 2: Performance Comparison with Different Frameworks
Fusion framework using fused classifiers outperforms the frame-works using text or content features, and it achieves 5% increase compared with Text in terms of Macro-F score. As the Content classifier is much weaker than the Text classifier, the fusion may also import noise, and hence degenerates the impact of fusion.
The proposed CES classifier outperforms the existing methods apparently by 20% than the Text approach. In CES approach, we incorporate visual information into the semantic similarity measure between words rather than into the classifier directly. This design mechanism is expected to make use of visual clues of Web videos to obtain more reasonable semantic relations among words, which can better bridge the semantic gap between visual feature and video content. CES exploits the video similarity based on visual content and integrates both text and visual content feature for video classi-fication. The effects of text and content features are typically com-plementary, and the combination can improve overall effectiveness. Additionally, the clustering of videos can help extract relations be-tween words from different videos, and thus effectively addresses the problem of over-sparse kernel matrix in text categorization.
Table 3 shows the detailed F-score performance on all the cat-egories of video dataset. Since the Content classifier performs much worse than other methods, we exclude it for comparison. We can see that CES performs the best in 11 out of 15 categories. The detailed performance comparison on F-score demonstrates the ad-vantage of our methods.
Compared with classification accuracy, time cost is usually con-sidered to be less important [21]. However, in the scenario of online video classification, it is essential for us to take the efficiency into consideration. This is because only with high efficiency can a real-time categorization approach provide satisfactory user experience. In this section, we only evaluate the time cost of the classification stage, as the building classifiers is done offline.

Text-based classifiers and CES classifier yield comparable effi-ciency and average classification time cost for an incoming video is less than 0.01 second. There exist some marginal computational cost differences due to the kernel density in SVM classifier. The approaches based on content features, i.e., Content and Fusion , perform significantly worse due to the expensive content feature extraction. The time cost of content feature extraction from video is typically of the same order of magnitude as video length, which makes it unacceptable to utilize the content-based video classifi-cation methods for Web video classification. Take Youtube as an example, more than 20 hours videos are uploaded per minute on average.

In a summary, compared with the state of the art approaches for video classification, our CES-based video classification method performs superiorly in terms of both effectiveness and efficiency . It utilizes the content features at the training stage, but it does not need to extract content features when classifying an new video in the Web. The proposed content-enriched kernel provides a promis-ing solution for online video classification.
In the last set of experiments, we evaluate the classification per-formance on multi-kernel enhancement. Since the content based methods, e.g., Content classifier and Fusion , perform much worse in terms of time efficiency, and thus are not applicable in online Web video classification scenario, we here only present the results on the combination of two approaches, i.e., Text and CES . Table 4: Performance Comparison with Multi-Kernel Solution The results on classification effectiveness are shown in Table 4. As been discovered earlier, the Multi-Kernel method that takes all of the similarity measures into consideration performs best for video classification. We can see that Multi-Kernel SVM can obtain the best performance, which is 33% better than Text-based classifier. This improvement not only demonstrates the great success of Multi-Kernel SVM in fusing different semantic kernels, but also indicates the different similarity measures may convey and capture differ-ent complementary features and they together can provide high-performance video classification. Note that, the Multi-kernel op-timization will not introduce extra time cost at the classification stage, as only one joint kernel will be generated after Multi-kernel learning in the training stage.
In this paper, we presented a novel framework that efficiently exploits both visual content and text features to facilitate online video categorization. Within this framework, we proposed effec-tive content-enriched semantic kernel which can extract word rela-tionship by clustering the videos with visual content features. Ex-tensive experimental results based on a large dataset demonstrate the superior performance of the proposed method in terms of both effectiveness and efficiency for video classification.
 This work opens to several interesting directions for future work. Notably, it is interesting to investigate the performance issues of the current framework by using other content features of video, such as motion and audio features, to generate CES. Moreover, it will be interesting to adapt and apply CES for query expansion/suggestion in video retrieval. This research was supported by the National Natural Science Foundation of China under Grant No. 60933004 and 60811120098.
