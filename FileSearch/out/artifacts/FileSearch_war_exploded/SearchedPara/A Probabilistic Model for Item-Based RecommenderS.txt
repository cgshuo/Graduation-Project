 Recommender systems estimate the conditional probability P ( x j | x i )ofitem x j being bought, given that a customer has already purchased item x i . While there are different ways of approximating this conditional probability, the expression is generally taken to refer to the frequency of co-occurrence of items in the same basket, or other user-specific item lists, rather than being seen as the co-occurrence of x j with x as a proportion of all other items bought alongside x i .This paper proposes a probabilistic calculus for the calculation of conditionals based on item rather than basket counts. The proposed method has the consequence that items bough to-gether as part of small baskets are more predictive of each other than if they co-occur in large baskets. Empirical re-sults suggests that this may result in better take-up of per-sonalised recommendations.
 Categories and Subject Descriptors: H.3.3 [Information Systems]: Information Search and Retrieval General Terms: Algorithms, Experimentation, Performance. Keywords: shopping basket recommendation, naive bayes clas-sifier, performance evaluation, popularity-based ordering.
The estimation of class-conditional probabilities lies at the heart of personalised recommender systems. It is normally the case that co-occurrence of item pairs is empirically estimated by the mean proportion of baskets in which the two items appear together. We shall refer to this as basket-based estimation. However, this loses track of the context of remaining items purchased, in particular whether the observed pairing is one of a few or very many item pairs in the relevant baskets in the user-item data matrix. This pa-per proposes an alternative modeling framework where each item pairing is counted as a proportion of the total basket size, that is to say the frequency of paired occurrences for item x j with item x is normalised by the total number of items purchased alongside x This we term item-based estimation. It require s a re-formulation Copyright 2007 ACM 978-1-59593-730-8/07/0010 ... $ 5.00. of the probabilistic calculus for the calculation of P ( x is discussed in the next section. Section 3 illustrates the proposed methodology by reference to the simplest estimator of the condi-tional, the Na X ve Bayes (NB) recommender system, and is followed by a brief evaluation of the expected uptake of recommendations estimated from historical data in a real-world application. For con-venience, the NB classifiers using basket-based and item-based es-timates will be referred to NB(baskets) and NB(items), respectively, throughout the rest of the paper.
A probability estimate represents the expected frequency of oc-currence of an event of interest taken over a defined space of possibilities. In collaborative filtering there are two different in-terpretations of the base population from which inferences about item purchases are made, which may be the number of purchasing episodes (i.e. baskets) or the totality of individual items purchased [5,7,3,6]. 1. Basket-based probability estimator . Inthisestimate, which 2. Item-based probability estimator . The proposal is to count
Note that the basket-based estimate does not discriminate be-tween items that have co-occurred with different numbers of other items. In contrast, the item-based approach captures the item co-occurrence information in the feature, or item, space.
Furthermore, for a probabilistic calculus to be complete and con-sistent, the following three conditions must be met: The first two identities follow trivially from the stated definitions. The third identity holds as
To illustrate the difference between the two probability estima-tions, consider the example given in Table 1 in which there are eight baskets (i.e. | baskets | =8 ) and 23 basket items in total across all the baskets (i.e. | item pairs | = i N F ( x i )=48 ). With the basket-based probability estimation: Similarly, with the item-based estimation, It is easy to show that both probability estimations satisfy Bayes theorem in this example, i.e. P ( d | a )= P ( a | d ) P ( d ) /P ( a ) .
The recommendation list is usually generated by ranking all the non-purchased items by the corre sponding posteri or proba bilities P ( C i = x i | F ) , and then recommending the topN most probable ones 1 . In a NB recommender system, the calculation of P ( C x | F ) can be simplified as follow: The above equation implies that NB(baskets) and NB(items) will generate the same recommendation lists if they rank N ( x N
D ( x i ) for NB(baskets) and N F ( x i ) for NB(items), in the same order. Otherwise, the two lists will be different.

In a preliminary experiment, we evaluate the impact of N ( x on the ranking position of item x i in the recommendation list. The result is shown in Figure1. As can be seen, among 210 active cat-egories, there are only around 23 categories with the same ranking position in both cases. Figure 1: Histogram of the difference between the ranks of the prior probability generated by NB(baskets) and NB(items) i.e. Rank ( P D ( x i ))  X  Rank ( P F ( x i )) . Among 210 active categories in total, there are only around 23 categories with the same ranking position in both cases.
In order to compare NB(baskets) and NB(items), LeShop pro-vided us with access to link-anonymized shopping basket data from actual historical records comprising individual transactions, as pre-viously described in [6]. We use data covering six consecutive months for training, and ran tests on the following seventh month.
We notice that the authors in [7] calculate the purchase likelihood in a way similar to P ( C i | F ) liminary experiment, however, we did not observe any ad-vantage of such an approach in terms of recommendation performance. Figure 2: Performance comparison with increasing coverage of the active categories. testset = LeShop.
 After aggregating the training baskets 2 , there are around 210 active categories and 40,000 baskets. The test data are not aggregated. We also used data from an anonymous Belgium retailer [2] in order to verify our results. However, as the results of the Belgium basket dataset show similar patterns to that of the LeShop, we report its results only in Section 4.2.

In keeping with our previous live intervention study of recom-mender systems [6], binary hit rate (HR Binary) is used for mea-suring the system performance and it is calculated as the proportion of the baskets having at least one correctly predicted item. Fur-thermore, we evaluate the performance following the same strategy proposed in [6], which removes duplicated items from the test bas-kets, re-orders those items in descending global-popularity order and then defines the last three (i.e. the least popular items) as the target to be predicted, given the remainder as evidence .This evaluation setting reflects a real scenario of marketing promotion: increasing visibility of less popular (or new) products. As shown in our previous study, among other alternatives (e.g. temporal and leave-one-out ordering), the popularity-based approach is the only evaluation strategy that generates a consistent ranking of several recommendation algorithms in terms of their live performance in a check-out recommender system using the marginal distribution of item purchase for the prior of a NB recommender algorithm. The consistency sought is to correctly rank the expected take-up of recommendations between four groups of consumers in a 2 contingency table comprising active vs. non-active and case vs. control baskets. The former characterizes consumers according to their frequency of shopping episodes during the preceding 6 month block period, and the latter separates the consumers receiv-ing personalised recommendations from those receiving a generic recommendation using the global prior only. More details about the data and the popularity-based evaluation strategy can be found on [6].
Due to the feasibility concern, it is common practise for many
A well-known characteristic of shopping habits is seasonal behavior. Its effect is partly avoided in this study by aggre-gating shopping baskets for each customer over the training period real-life recommender systems to model the correlations among a subset of all the product categories, also called active categories . In this section, we compare the performance difference among sev-eral algorithms. A performance curve is plotted, for each methods, 1 . 0) of all the n % categories (i.e. the categories that account for most of the total spend over the training data). Note that, a low (or high) coverage of the active categories is indicative of a small (or large) value of the average basket size, which is shown in Figure 2(b).

Here, we also implement the item-based collaborative filtering algorithm (item CF) [3], for comparison. In item CF, the pairwise item similarity is calculated as: where  X   X  [0 , 1] , N D ( x i ) is the number of shoppers that have purchased the items in the training data and R ( x i ,x j in the normalized n  X  m user-item matrix. With item CF, we report only its best performance.

Figure 2 shows results of each method evaluated on the LeShop data. As can be seen, all the methods do best at the minimum value of the average basket size, while the performance drops with increasing category coverage (i.e. the data become sparser). It is interesting to note that, both NB-based methods perform similar when the average basket size is small. This is because the value of P
F ( F j | C i ) is closer to P D ( F j | C i ) (cf. Eq(2) and Eq(4)) when the basket size is small. In an extreme example where all the baskets contain no more than two items, P F ( F j | C i ) are identical.
With increasing basket sizes (Figure 2(b)), NB(items) consis-tently outperforms NB(baskets). We attribute this to the benefit of item-based probability estimate (Eq(4)) as it assigns lower weight to the co-occurring items in larger baskets. This is similar to the con-cept adopted in Eq(5) that gives lower weight to the shoppers that have purchased more items [3]. Consequently, we see that, with increasing basket sizes, both NB(items) and item CF outperform NB(baskets), which does not discriminate between items take have co-occurred with different number of other items. Our future work will explore the reasons why the item CF approach shows inferior performance in comparison with NB(items) for larger baskets.
In ongoing work we find that the performance of NB(baskets) is adversely affected by the increasing number of evidence items. It is well known that feature selection using information gain can benefit the NB classifier in text classification [4]. In practice, however, we choose to use the lastN items to deal with the  X  X oisy X  features (or items) due to the concern of the difference between the recom-mender system and the text classifier [7]. Given a list of basket items ordered by their global prior (i.e. popularity), we use only the last few to predict the purchase probability of the new items. Intuitively, these less popular (or frequent) items are more useful in capturing item similarity than those universally liked ones [1].
Figure 3 shows the performance curves of all the methods with 10 ,all ) . Here, NB(baskets) shows its best performance at a value of five ( lastN =5). Both the NB(items) and item CF models are relatively noise-resistant and do best when all the items are used for prediction. This is a practical advantage over NB(baskets) as it saves the time required to regulate the model for the optimal value of lastN over different time periods. Figure 3: Performance comparison with increasing lastN items. coverage = 80% and testset = LeShop.
In keeping with our previous work regarding the performance evaluation of a recommender system, we also report the results obtained by using all-but-one (or leave-one-out) protocol [1] and compare them with another baseline method that simply recom-mends the most popular items not present in the basket (referred as popular in Figure4). Note that, with the Belgium data, we only test it with the NB-based methods as the user information required by the item CF is not available.

As shown in Figure4(b), the popular recommendation has the best performance with the leave-one-out evaluation ( p&lt; 0 . 05 , McNemar X  X  test). This is because the leave-one-out approach favors the performance on common classes as it weights all the items equally.

The popularity-based evaluation, on the contrary, favors the per-formance over rare classes (i.e. less popular items) as it assigns zero weight to the  X  X its X  over the popular items in the test bas-kets. It is worth noting that, our previous live intervention study in LeShop has revealed that the performance comparison over the rare classes are more indicative of the models X  live performance as the customers are unlikely to accept the recommendations of those common classes (popular product items). As shown in Figure4(a), NB(items) with the popularity ordering has the best performance in all the basket datasets ( p&lt; 0 . 05 , McNemar X  X  test). Figure 4: Performance on the Belgium data and three months of the LeShop data. coverage = 80% .
A probabilistic framework using individual item purchases was proposed and its performance for personalised recommender sys-tems was benchmarked against the standard basket-based estimates of conditional probabilities. The two alternative models of con-ditional probabilities were evaluated over two real-world basket datasets. The NB(items) model is found to be uniformly better than the NB(baskets) in terms of both binary hit rates and noise resis-tance. We attribute this to the item-based approach discriminating between items co-occurring with different numbers of other items. As the novelty (or focus) of this paper is not in the use of condi-tional probability but how it is estimated, one follow-up study is to evaluate the impact of the two estimates for other personalized recommender systems such as the association rule based system. Another future work is to generalize the current idea to estimate the high-order conditional p robability, e.g. the conditional probability based on the co-occurrence of three or four items at the same time. It is intended also to deploy the NB(items) model in a live recom-mender system and so track its actual performance against other candidate methods.
 The authors wish to thank Dominique Locher and his team at LeShop ( www.LeShop.ch -The No.1 e-grocer of Switzerland since 1998) for providing us with the data for this work, without which the work presented in this paper would not have been possible. [1] J. Breese, D. Heckerman, and C. Kadie. Empirical analysis of [2] T. Brijs, G. Swinnen, K. Vanhoof, and G. Wets. Using [3] M. Deshpande and G. Karypis. Item-based top-n [4] A. McCallum and K. Nigam. A comparison of event models [5] V.Robles,n.P.Larra E.Menasalvas,M.S.P  X  erez, and [6] C. M. Sordo-Garcia, M. B. Dias, M. Li, W. El-Deredy, and [7] T. Zhang and V. S. Iyengar. Recommender systems using
