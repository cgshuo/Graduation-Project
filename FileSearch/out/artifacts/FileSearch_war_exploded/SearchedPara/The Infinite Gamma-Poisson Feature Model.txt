 Indian buffet process [7] can be very useful for solving this problem. the distribution over partitions of objects induced by the D irichlet process [1]. some image data. Let X = { X 4 we specify X denote the number of times feature k occurs in the data point X a corresponds to a data point and each column to a feature. Give n that z with a feature-specific parameter  X  where m that favors sparsity (in a sense that will be explained short ly): The hyperparameter  X  itself is given a vague gamma prior G (  X  ;  X  we can easily integrate out the parameters {  X  K of all elements of Z . Since the columns are independent this expectation is K P N E ( z nk ) is given by where N B ( z positive integers sparsity of this matrix.
 generative process: where D (  X  revealed through the Ewens X  X  distribution [5].
 data. trices as follows. We assume that for any possible z large integer. We define h = ( z a left-ordered standard form. Let K K to the same left-ordered form. Thus, the probability of [ Z ] is We assume that the first K features are unrepresented i.e. m limit of the Dirichlet-multinomial pair [6, 9]. The limit ta kes the following form: where m = P K + quential stochastic process that reproduces this distribu tion. 3.1 The stochastic process stochastic process are discussed below.
 g number of feature occurrences for the first data point. Given g ( z drawing from Ewens X  X  distribution [5] over integer partiti ons which is given by where v (1) while the latter is a distribution over partitions of object s. Let K k , with k  X  K given by m integer g from the Ewens X  X  formula. This process produces the followi ng distribution: with the Indian buffet process. 3.2 Conditional distributions of the conditionals for the finite model or by using the stocha stic process. Suppose that for the current value of Z , there exist K k  X  K + . Let m  X  n,k = new features 2 and accounts for all k such that m ber from N B ( g partition of the integer g be directly expressed from Equation (7) and the prior of  X  , is given by conditional of  X  given data and Z . patches that are detected in the image so as X P starting from the joint distribution of all variables which is given by joint = p (  X  ) P ( Z |  X  ) p ( {  X  model. The parameter vector  X  for the feature (object class) k . Each  X  set of {  X  eters {  X  counting the non-zero columns of Z .
 The parameter vector  X  occurrence of feature k where j = 1 , . . . , z M The assignment variable s ni = { s kj patch i .  X  only on the n th row of Z .
 The parameters ( m n ,  X  of patch locations. Thus y ni follows a image-specific Gaussian mixture with M assume that the component kj has mean m nkj and covariance  X  and  X  n ( for the pair ( m nkj ,  X  mial: P ( s local image patch is generated according to and depicted in the graphical model shown in Figure 1.
 bution all the parameters {  X  samples from the posterior P ( S, Z,  X  | X ) . 4.1 MCMC inference according to p (  X  ) P ( Z |  X  ) P ( W | S, Z ) Q N change an element of Z by one so as | z new 1 proposal distribution for changing z Suppose we wish to sample a new value for z S components M On the other hand the dimensionality of the assignments S affected when z the marginalized posterior conditional P ( z where P ( z sampling using as an importance distribution the posterior conditional P ( S Sampling from P ( S so that the sum is approximated by P ( W | S  X  discrete appearances (right). The MCMC algorithm was initi alized with K z K images. K using the posterior mean values for a pair ( m nkj ,  X  occurrences of the same feature.
 to the car-category). These z them being a non-negative integer valued matrix.

