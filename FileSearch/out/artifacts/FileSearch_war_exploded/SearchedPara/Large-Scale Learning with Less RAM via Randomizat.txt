 Daniel Golovin dgg@google.com D. Sculley dsculley@google.com H. Brendan McMahan mcmahan@google.com Michael Young mwyoung@google.com Google, Inc., Pittsburgh, PA, and Seattle, WA As the growth of machine learning data sets contin-ues to accelerate, available machine memory (RAM) is an increasingly important constraint. This is true for training massive-scale distributed learning systems, such as those used for predicting ad click through rates (CTR) for sponsored search (Richardson et al., 2007; Craswell et al., 2008; Bilenko &amp; Richardson, 2011; Streeter &amp; McMahan, 2010) or for filtering email spam at scale (Goodman et al., 2007). Minimizing RAM use is also important on a single machine if we wish to uti-lize the limited memory of a fast GPU processor, or to simply use fast L1-cache more effectively. After train-ing, memory cost remains a key consideration at pre-diction time as real-world models are often replicated to multiple machines to minimize prediction latency. Efficient learning at peta-scale is commonly achieved by online gradient descent (OGD) (Zinkevich, 2003) or stochastic gradient descent (SGD), (e.g., Bottou &amp; Bousquet, 2008), in which many tiny steps are accu-mulated in a weight vector  X   X  R d . For large-scale learning, storing  X  can consume considerable RAM, especially when datasets far exceed memory capacity and examples are streamed from network or disk. Our goal is to reduce the memory needed to store  X  . Standard implementations store coefficients in single precision floating-point representation, using 32 bits per value. This provides fine-grained precision needed to accumulate these tiny steps with minimal roundoff error, but has a dynamic range that far exceeds the needs of practical machine learning (see Figure 1). We use coefficient representations that have more lim-ited precision and dynamic range, allowing values to be stored cheaply. This coarse grid does not provide enough resolution to accumulate gradient steps with-out error, as the grid spacing may be larger than the updates. But we can obtain a provable safety guaran-tee through a suitable OGD algorithm that uses ran-domized rounding to project its coefficients onto the grid each round. The precision of the grid used on each round may be fixed in advance or changed adaptively as learning progresses. At prediction time, more ag-gressive rounding is possible because errors no longer accumulate.
 Online learning on large feature spaces where some features occur very frequently and others are rare often benefits from per-coordinate learning rates, but this requires an additional 32-bit count to be stored for each coordinate. In the spirit of randomized rounding, we limit the memory footprint of this strategy by using an 8-bit randomized counter for each coordinate based on a variant of Morris X  X  algorithm (1978). We show the resulting regret bounds are only slightly worse than the exact counting variant (Theorem 3.3), and empirical results show negligible added loss.
 Contributions This paper gives the following theo-retical and empirical results: 1. Using a pre-determined fixed-point representation 2. The cost of a per-coordinate learning rate sched-3. Using an adaptive per-coordinate coarse represen-4. Variable-width encoding at prediction time allows Approaches 1 and 2 are particularly attractive, as they require only small code changes and use negligible ad-ditional CPU time. Approaches 3 and 4 require more sophisticated data structures. In addition to the sources already referenced, related work has been done in several areas.
 Smaller Models A classic approach to reducing memory usage is to encourage sparsity, for example via the Lasso (Tibshirani, 1996) variant of least-squares regression, and the more general application of L 1 reg-ularizers (Duchi et al., 2008; Langford et al., 2009; Xiao, 2009; McMahan, 2011). A more recent trend has been to reduce memory cost via the use of feature hashing (Weinberger et al., 2009). Both families of ap-proaches are effective. The coarse encoding schemes reported here may be used in conjunction with these methods to give further reductions in memory usage. Randomized Rounding Randomized rounding schemes have been widely used in numerical com-puting and algorithm design (Raghavan &amp; Tompson, 1987). Recently, the related technique of random-ized counting has enabled compact language models (Van Durme &amp; Lall, 2009). To our knowledge, this paper gives the first algorithms and analysis for online learning with randomized rounding and counting. Per-Coordinate Learning Rates Duchi et al. (2010) and McMahan &amp; Streeter (2010) demon-strated that per-coordinate adaptive regularization ( i.e. , adaptive learning rates) can greatly boost pre-diction accuracy. The intuition is to let the learning rate for common features decrease quickly, while keep-ing the learning rate high for rare features. This adap-tivity increases RAM cost by requiring an additional statistic to be stored for each coordinate, most often as an additional 32-bit integer. Our approach reduces this cost by using an 8-bit randomized counter instead, using a variant of Morris X  X  algorithm (Morris, 1978). For concreteness, we focus on logistic regression with binary feature vectors x  X  { 0 , 1 } d and labels y  X  { 0 , 1 } . The model has coefficients  X   X  R d , and gives predictions p  X  ( x )  X   X  (  X   X  x ), where  X  ( z )  X  1 / (1+ e is the logistic function. Logistic regression finds the model that minimizes the logistic X  X oss L . Given a la-beled example ( x,y ) the logistic X  X oss is
L ( x,y ;  X  )  X  X  X  y log ( p  X  ( x ))  X  (1  X  y ) log (1  X  p where we take 0 log 0 = 0. Here, we take log to be the natural logarithm. We define k x k p as the ` p norm of a vector x ; when the subscript p is omitted, the ` 2 norm is implied. We use the compressed summa-tion notation g 1: t  X  P t s =1 g s for scalars, and similarly f The basic algorithm we propose and analyze is a vari-ant of online gradient descent (OGD) that stores coef-ficients  X  in a limited precision format using a discrete set ( Z ) d . For each OGD update, we compute each new coefficient value in 64-bit floating point represen-tation and then use randomized rounding to project the updated value back to the coarser representation. A useful representation for the discrete set ( Z ) the Qn.m fixed-point representation. This uses n bits for the integral part of the value, and m bits for the fractional part. Adding in a sign bit results in a total of K = n + m + 1 bits per value. The value m may be fixed in advance, or set adaptively as described below. We use the method RandomRound from Algorithm 1 to project values onto this encoding.
 The added CPU cost of fixed-point encoding and ran-domized rounding is low. Typically K is chosen to correspond to a machine integer (say K = 8 or 16), Algorithm 1 OGD-Rand-1d so converting back to a floating point representa-tions requires a single integer-float multiplication (by = 2  X  m ). Randomized rounding requires a call to a pseudo-random number generator, which may be done in 18-20 flops. Overall, the added CPU overhead is negligible, especially as many large-scale learning methods are I/O bound reading from disk or network rather than CPU bound. 3.1. Regret Bounds for Randomized Rounding We now prove theoretical guarantees (in the form of upper bounds on regret) for a variant of OGD that uses randomized rounding on an adaptive grid as well as per-coordinate learning rates. (These bounds can also be applied to a fixed grid). We use the standard definition given a sequence of convex loss functions f t . Here the  X   X  our algorithm plays are random variables, and since we allow the adversary to adapt based on the pre-viously observed  X   X  t , the f t and post-hoc optimal  X  are also random variables. We prove bounds on ex-pected regret, where the expectation is with respect to the randomization used by our algorithms (high-probability bounds are also possible). We consider regret with respect to the best model in the non-discretized comparison class F = [  X  R,R ] d .
 We follow the usual reduction from convex to lin-ear functions introduced by Zinkevich (2003); see also Shalev-Shwartz (2012, Sec. 2.4). Further, since we consider the hyper-rectangle feasible set F = [  X  R,R ] d the linear problem decomposes into n independent one-dimensional problems. 1 In this setting, we con-sider OGD with randomized rounding to an adaptive grid of resolution t on round t , and an adaptive learn-ing rate  X  t . We then run one copy of this algorithm for each coordinate of the original convex problem, implying that we can choose the  X  t and t schedules appropriately for each coordinate. For simplicity, we assume the t resolutions are chosen so that  X  R and + R are always gridpoints. Algorithm 1 gives the one-dimensional version, which is run independently on each coordinate (with a different learning rate and dis-cretization schedule) in Algorithm 2. The core result is a regret bound for Algorithm 1 (omitted proofs can be found in the Appendix): Theorem 3.1. Consider running Algorithm 1 with adaptive non-increasing learning-rate schedule  X  t , and discretization schedule t such that t  X   X  X  t for a con-stant  X  &gt; 0 . Then, against any sequence of gradi-ents g 1 ,...,g T (possibly selected by an adaptive ad-versary) with | g t |  X  G , against any comparator point  X   X   X  [  X  R,R ] , we have
E [Regret(  X   X  )]  X  By choosing  X  sufficiently small, we obtain an expected regret bound that is indistinguishable from the non-rounded version (which is obtained by taking  X  = 0). In practice, we find simply choosing  X  = 1 yields ex-cellent results. With some care in the choice of norms used, it is straightforward to extend the above result to d dimensions. Applying the above algorithm on a per-coordinate basis yields the following guarantee: Corollary 3.2. Consider running Algorithm 2 on the feasible set F = [  X  R,R ] d , which in turn runs Algorithm 1 on each coordinate. We use per-coordinate learning rates  X  t,i =  X /  X  2 R/ p G 2 +  X  2 , where  X  t,i  X  t is the number of non-zero g s,i seen on coordinate i on rounds s = 1 ,...,t . Then, against convex loss functions f t , with g t a sub-gradient of f t at  X   X  t , such that  X  t, k g t k  X   X  G , we have
E [Regret]  X  The proof follows by summing the bound from The-orem 3.1 over each coordinate, considering only the rounds when g t,i 6 = 0, and then using the inequality P on each coordinate.
 The core intuition behind this algorithm is that for fea-tures where we have little data (that is,  X  i is small, for Algorithm 2 OGD-Rand example rare words in a bag-of-words representation, identified by a binary feature), using a fine-precision coefficient is unnecessary, as we can X  X  estimate the cor-rect coefficient with much confidence. This is in fact the same reason using a larger learning rate is ap-propriate, so it is no coincidence the theory suggests choosing t and  X  t to be of the same magnitude. Fixed Discretization Rather than implementing an adaptive discretization schedule, it is more straight-forward and more efficient to choose a fixed grid res-olution, for example a 16-bit Qn.m representation is sufficient for many applications. 2 In this case, one can apply the above theory, but simply stop decreasing the learning rate once it reaches say (= 2  X  m ). Then, the  X  1: T term in the regret bound yields a linear term like O ( T ); this is unavoidable when using a fixed reso-lution . One could let the learning rate continue to decrease like 1 / in fact, lower-bounding the learning-rate is known to allow online gradient descent to provide regret bounds against a moving comparator (Zinkevich, 2003).
 Data Structures There are several viable ap-proaches to storing models with variable X  X ized coef-ficients. One can store all keys at a fixed (low) preci-sion, then maintain a sequence of maps ( e.g. , as hash-tables), each containing a mapping from keys to coeffi-cients of increasing precision. Alternately, a simple lin-ear probing hash X  X able for variable length keys is effi-cient for a wide variety of distributions on key lengths, as demonstrated by Thorup (2009). With this data structure, keys and coefficient values can be treated as strings over 4-bit or 8-bit bytes, for example. Bland-ford &amp; Blelloch (2008) provide yet another data struc-ture: a compact dictionary for variable length keys. Finally, for a fixed model, one can write out the string s of all coefficients (without end of string delimiters), store a second binary string of length s with ones at the coefficient boundaries, and use any of a number of rank/select data structures to index into it, e.g. , the one of Patrascu (2008). 3.2. Approximate Feature Counts Online convex optimization methods typically use a learning rate that decreases over time, e.g. , setting  X  t proportional to 1 / require storing a unique count  X  i for each coordinate, where  X  i is the number of times coordinate i has ap-peared with a non-zero gradient so far. Significant space is saved by using a 8-bit randomized counting scheme rather than a 32-bit (or 64-bit) integer to store the d total counts. We use a variant of Morris X  prob-abilistic counting algorithm (1978) analyzed by Flajo-let (1985). Specifically, we initialize a counter C = 1, and on each increment operation, we increment C with probability p ( C ) = b  X  C , where base b is a parameter. unbiased estimator of the true count. We then use learning rates  X  t,i =  X / p  X   X  t,i + 1, which ensures that even when  X   X  t,i = 0 we don X  X  divide by zero. We compute high-probability bounds on this counter in Lemma A.1. Using these bounds for  X  t,i in conjunc-tion with Theorem 3.1, we obtain the following result (proof deferred to the appendix).
 Theorem 3.3. Consider running the algorithm of Corollary 3.2 under the assumptions specified there, but using approximate counts  X   X  i in place of the exact counts  X  i . The approximate counts are computed using the randomized counter described above with any base b &gt; 1 . Thus,  X   X  t,i is the estimated number of times g s,i 6 = 0 on rounds s = 1 ,...,t , and the per X  X oordinate learning rates are  X  t,i =  X / p  X   X  t,i + 1 . With an appro-priate choice of  X  we have E [Regret( g )] = o R p G 2 +  X  2 T 0 . 5+  X  for all  X  &gt; 0 , where the o -notation hides a small constant factor and the dependence on the base b . 3 Many real-world problems require large-scale predic-tion . Achieving scale may require that a trained model be replicated to multiple machines (Bucilu X a et al., 2006). Saving RAM via rounding is especially at-tractive here, because unlike in training accumulated roundoff error is no longer an issue. This allows even more aggressive rounding to be used safely.
 Consider a rounding a trained model  X  to some  X   X  . We can bound both the additive and relative effect on logistic X  X oss L (  X  ) in terms of the quantity |  X   X  x  X   X  Lemma 4.1 (Additive Error) . Fix  X ,  X   X  and ( x,y ) . Let  X  = |  X   X  x  X   X   X   X  x | . Then the logistic X  X oss satisfies x,y, X  and i , which implies the result.
 Lemma 4.2 (Relative Error) . Fix  X ,  X   X  and ( x,y )  X  { 0 , 1 } d  X { 0 , 1 } . Let  X  = |  X   X  x  X   X   X   X  x | . Then Proofs for results in this section can be found in the extended version of this paper. Now, suppose we are using fixed precision numbers to store our model coeffi-cients such as the Qn.m encoding described earlier, with a precision of . This induces a grid of feasible model coefficient vectors. If we randomly round each coeffi-cient  X  i (where |  X  i |  X  2 n ) independently up or down to the nearest feasible value  X   X  i , such that E [  X   X  i then for any x  X  { 0 , 1 } d our predicted log-odds ratio,  X   X   X  x is distributed as a sum of independent random variables {  X   X  i | x i = 1 } .
 Let k = k x k 0 . In this situation, note that |  X   X  x  X  x |  X  k x k 1 = k , since |  X  i  X   X   X  i |  X  for all i . Thus Lemma 4.1 implies Similarly, Lemma 4.2 immediately provides an upper bound of e k  X  1 on relative logistic error; this bound is relatively tight for small k , and holds with proba-bility one, but it does not exploit the fact that the randomness is unbiased and that errors should cancel out when k is large. The following theorem gives a bound on expected relative error that is much tighter for large k : Theorem 4.3. Let  X   X  be a model obtained from  X  using unbiased randomized rounding to a precision grid as described above. Then, the expected logistic X  loss relative error of  X   X  on any input x is at most 2  X  Additional Compression Figure 1 reveals that co-efficient values are not uniformly distributed. Stor-ing these values in a fixed-point representation means that individual values will occur many times. Basic information theory shows that the more common val-ues may be encoded with fewer bits. The theoret-ical bound for a whole model with d coefficients is bility of occurrence of v in  X  across all dimensions d . Variable length encoding schemes may approach this limit and achieve further RAM savings. We evaluated on both public and private large data sets. We used the public RCV1 text classification data set, specifically from Chang &amp; Lin (2011). In keeping with common practice on this data set, the smaller  X  X rain X  split of 20,242 examples was used for parameter tuning and the larger  X  X est X  split of 677,399 examples was used for the full online learning exper-iments. We also report results from a private CTR data set of roughly 30M examples and 20M features, sampled from real ad click data from a major search engine. Even larger experiments were run on data sets of billions of examples and billions of dimensions, with similar results as those reported here.
 The evaluation metrics for predictions are error rate for the RCV1 data, and AucLoss (or 1-AUC) relative to a control model for the CTR data. Lower values are better. Metrics are computed using progressive validation (Blum et al., 1999) as is standard for online learning: on each round a prediction is made for a given example and record for evaluation, and only after that is the model allowed to train on the example. We also report the number of bits per coordinate used. Rounding During Training Our main results are given in Figure 2. The comparison baseline is online logistic regression using a single global learning rate and 32-bit floats to store coefficients. We also test the effect of per-coordinate learning rates with both 32-bit integers for exact counts and with 8-bit random-ized counts. We test the range of tradeoffs available for fixed-precision rounding with randomized counts, varying the number of precision m in q2.m encoding to plot the tradeoff curve (cyan). We also test the range of tradeoffs available for adaptive-precision rounding with randomized counts, varying the precision scalar  X  to plot the tradeoff curve (dark red). For all ran-domized counts a base of 1.1 was used. Other than these differences, the algorithms tested are identical. Using a single global learning rate, a fixed q2.13 en-coding saves 50% of the RAM at no added loss com-pared to the baseline. The addition of per-coordinate learning rates gives significant improvement in predic-tive performance, but at the price of added memory consumption, increasing from 32 bits per coordinate to 64 bits per coordinate in the baselines. Using random-ized counts reduces this down to 40 bits per coordi-nate. However, both the fixed-precision and the adap-tive precision methods give far better results, achiev-ing the same excellent predictive performance as the 64-bit method with 24 bits per coefficient or less. This saves 62.5% of the RAM cost compared to the 64-bit method, and is still smaller than using 32-bit floats with a global learning rate.
 The benefit of adaptive precision is only apparent on the larger CTR data set, which has a  X  X ong tail X  distri-bution of support across features. However, it is useful to note that the simpler fixed-precision method also gives great benefit. For example, using q2.13 encod-ing for coefficient values and 8-bit randomized counters allows full-byte alignment in naive data structures. Rounding at Prediction Time We tested the ef-fect of performing coarser randomized rounding of a fully-trained model on the CTR data, and compared to the loss incurred using a 32-bit floating point represen-tation. These results, given in Table 1, clearly support the theoretical analysis that suggests more aggressive rounding is possible at prediction time. Surprisingly coarse levels of precision give excellent results, with little or no loss in predictive performance. The mem-ory savings achievable in this scheme are considerable, down to less than two bits per value for q2.7 with the-oretically optimal encoding of the discrete values. Randomized storage of coefficient values provides an efficient method for achieving significant RAM savings both during training and at prediction time.
 While in this work we focus on OGD, similar ran-domized rounding schemes may be applied to other learning algorithms. The extension to algorithms that efficiently handle L 1 regularization, like RDA (Xiao, 2009) and FTRL-Proximal (McMahan, 2011), is rela-tively straightforward. 4 Large scale kernel machines, matrix decompositions, topic models, and other large-scale learning methods may all be modifiable to take advantage of RAM savings through low precision ran-domized rounding methods.
 A.1. Proof of Theorem 3.1 Our analysis extends the technique of Zinkevich (2003). Let  X   X  be any feasible point (with possibly in-finite precision coefficients). By the definition of  X  t +1 k  X  Rearranging the above yields g  X  (  X   X  t  X   X   X  )  X  = where the  X  t = 1 2  X  terms will capture the extra regret due to the random-ized rounding. Summing over t , and following Zinke-vich X  X  analysis, we obtain a bound of It remains to bound  X  1: T . Letting d t =  X  t +1  X   X   X  t +1 and a t = d t / X  t , we have We bound each of the terms in this last expression in expectation. First, note | d t |  X  t  X   X  X  t by defi-nition of the resolution of the rounding grid, and so | a |  X   X  . Further E [ d t ] = 0 since the rounding is unbiased. Letting W = | a 1: T | , by Jensen X  X  inequal-ity we have E [ W ] 2  X  E [ W 2 ]. Thus, E [ | a 1: T | ]  X  p
E [( a 1: T ) 2 ] = p Var( a 1: T ) , where the last equality follows from the fact E [ a 1: T ] = 0. The a t are not inde-pendent given an adaptive adversary. 5 Nevertheless, consider any a s and a t with s &lt; t . Since both have expectation zero, Cov( a s ,a t ) = E [ a s a t ]. By construc-tion, E [ a t | g t , X  t , hist t ] = 0, where hist t history of the game up until round t , which includes a s in particular. Thus
Cov( a s ,a t ) = E [ a s a t ] = E E [ a s a t | g t , X  t For all t , | a t |  X   X  so Var( a t )  X   X  2 , and Var( a P t Var( a t )  X   X  Next, consider E [  X   X  2 t +1  X   X  2 t +1 |  X  t +1 ]. Since E [  X  t +1 ] =  X  t +1 , for any shift s  X  R , we have E ( s ) so taking s =  X  t +1 , 1  X  Combining this result with E [ | a 1: T | ]  X   X  which completes the proof.
 A.2. Approximate Counting We first provide high X  X robability bounds for the ap-proximate counter.
 Lemma A.1. Fix T and t  X  T . Let C t +1 be the value of the counter after t increment operations using the approximate counting algorithm described in Sec-estimated count  X   X  ( C t +1 ) satisfies and Both T and c are essentially parameters of the bound; in the Eq. (2), any choices of T and c that keep T constant produce the same bound. In the first bound, the result is sharpest when T = t , but it will be con-venient to set T equal to the total number of rounds so that we can easily take a union bound (in the proof of Theorem 3.3). Proof of Lemma A.1. Fix a sequence of T increments, and let C i denote the value of the approximate counter at the start of increment number i , so C 1 = 1. Let X j = |{ i : C i = j }| , a random variable for the number of increments for which the counter stayed at j . We start with the bound of Eq. (1). When C = j , the update probability is p j = p ( j ) = b  X  j , so for any ` we have X j  X  ` j with probability at most (1  X  p j ) ` j all x . To make this at most T  X  c it suffices to take ` j = c (log T ) /p j = cb j log T . Taking a (rather loose) union bound over j = 1 , 2 ,...,T , we have For Eq. (1), it suffices to show that if this does not oc-cur, then  X   X  ( C t )  X  t/ ( bc log( T ))  X  1. Note P C t With our supposition that X j  X  cb j log T for all j , this cally increasing and b &gt; 1, simple algebra then shows  X   X  ( C t +1 )  X   X   X  ( C t )  X  t/ ( bc log( T ))  X  1. Next consider the bound of Eq. (2). Let j 0 be the minimum value such that p ( j 0 )  X  1 /et , and fix k  X  0. Then C t +1  X  j 0 + k implies the counter was incre-mented k times with an increment probability at most p ( j 0 ). Thus, Note that j 0  X d log b ( et ) e . Taking k = p 2 c log b is sufficient to ensure this probability is at most T  X  c since k  X  k  X  1 and k 2  X  k  X  2 c log b T . Observing that  X   X  d log b ( et ) e + p 2 c log b ( T ) + 1  X  et b  X  1 b completes the proof.
 Proof of Theorem 3.3. We prove the bound for the one-dimensional case; the general bound then fol-lows by summing over dimensions. Since we con-sider a single dimension, we assume | g t | &gt; 0 on all rounds. This is without loss of generality, because we can implicitly skip all rounds with zero gradients, which means we don X  X  need to make the distinction be-tween t and  X  t,i . We abuse notation slightly by defining  X   X   X   X   X  ( C t +1 )  X  t =  X  t for the approximate count on round t . We begin from the bound of Theorem 3.1, with learning rates  X  t =  X / Lemma A.1 with c = 2 . 5 then implies
Pr [  X   X  t + 1 &lt; k 1 t ]  X  where k 1 = 1 / ( bc log T ) and k 2 = eb union bound on t = 1 ,...,T on the first bound implies with probability 1  X  1  X  where we have used the inequality P T t =1 1  X  Similarly, the second inequality implies with probabil-Taking a union bound, Eqs. (3) and (4) hold with prob-ability at least 1  X  2 / probability at most 2 / for any  X , X  0  X  [  X  R,R ] (using the convexity of f t and the bound on the gradients G ), on any run of the algo-rithm, regret is bounded by 2 RGT . Thus, these failed cases contribute at most 4 RG gret bound.
 Now suppose Eqs. (3) and (4) hold. Choosing  X  = stants, and note for any  X  &gt; 0, both 1  X  o ( T  X  ). Thus, when Eqs. (3) and (4) hold, E [Regret]  X  Adding 4 RG statements fail still leaves the same bound.
 It follows from the proof that we have the more precise but cumbersome upper bound on E [Regret]: 2 R 2
