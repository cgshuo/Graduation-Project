 If Kolmogorov complexity [25] measures information in one object and Information Distance [4, 23, 24, 42] measures information shared by two ob jects, how do we measure in-formation shared by many objects? This paper provides an initial pragmatic study of this fundamental data mining question. Firstly, E m ( x 1 ,x 2 ,...,x n ) is defined to be the minimum amount of thermodynamic energy needed to con-vert from any x i to any x j . With this definition several the-oretical problems have been solved. Second, our newly pro-posed theory is applied to select a comprehensive review and a specialized review from many reviews: (1) Core feature words, expanded words and dependent words are extracted respectively. (2) Comprehensive and specialized reviews are selected according to the information among them. This method of selecting a single review can be extended to select multiple reviews as well. Finally, experiments show that this comprehensive and specialized review mining method based on our new theory can do the job efficiently.
 H.2.8 [ DATABASE MANAGEMENT ]: Database Ap-plications X  Data mining ; I.2.7 [ ARTIFICIAL INTELLI-GENCE ]: Natural Language Processing X  Text analysis Theory Data Mining, Text Mining, Kolmogorov Complexity,Information Distance
A great deal of data mining research can be regarded as gathering information from information carrying objects. However, without a general agreement of what is informa-tion in one object, what is information shared by two ob-jects, and what is information shared among many objects, we end up with dozens of arbitrary measures and algorithms that perhaps optimal under one measure, but not under an-other. The authors of [39] have articulated this problem. The field will certainly continue to grow and flourish with-out settling such a problem, as it has been, as a field of engineering.

This work represents another step of our continued ef-forts to solve this problem. Over the past decade, we have answered the question of  X  X hat is the shared information between two objects X . We introduced the metric of infor-mation distance [4, 23, 24] that is provably better than all other  X  X easonable X  metrics in all application domains. These include all metrics listed in [39] that satisfy distance metric requirements and when they are normalized to the range of 0 and 1 to be compared with the normalized information distance. This theory has been widely accepted and further studied by the theoretical community [41, 40, 36, 27, 26, 9]. Our theory has also led to several successful applications in the data mining community, for example, [18, 42]. In [18], Keogh, Lonardi, and Ratanamahatana compared a variant of our approach in [23] to 51 measures from 7 data mining related conferences including SIGKDD, SIGMOD, ICDM, ICDE, SSDB, VLDB, PKDD, PAKDD, and have concluded that our information distance based method was superior to all these parameter-laden methods on their benchmark data. In the meantime, the theory has found dozens of applications in many fields from weather forecasting to software engineer-ing, and to bioinformatics [1, 3, 8, 11, 12, 10, 14, 19, 21, 20, 22, 38, 30, 31, 32, 35, 2, 34, 28, 29]. A complete list of references is in the third edition of [25].

However, in many data mining applications, we are more interested in mining shared information from many, not just two, information carrying entities. For example, what is the public opinion on the United States presidential election, from the blogs? What do the customers say about a prod-uct, from the reviews? Which article, among many, covers the news most comprehensively? Or specialized in one par-ticular news item?
Kolmogorov complexity and our prior theory of informa-tion distance are not sufficient for these tasks. Kolmogorov complexity deals with one object and information distance deals with two objects. There is a conspicuous gap: a theory dealing with many objects.
Kolmogorov complexity was introduced almost half a cen-tury ago by R. Solomonoff, A.N. Kolmogorov and G. Chaitin, see [25]. It is now widely accepted as an information theory for individual objects parallel to that of Shannon X  X  informa-tion theory which is defined on an ensemble of objects. Fix a universal Turing machine U . The Kolmogorov complexity of a binary string x condition to another binary string y , K
U ( x | y ), is the length of the shortest (prefix-free) program for U that outputs x with input y . Itcanbeshownthatfor different universal Turing machine U , for all x, y where the constant C depends only on U .Thuswesim-ply write K U ( x | y )as K ( x | y ). We write K ( x | ), where is the empty string, as K ( x ). For a comprehensive study of Kolmogorov complexity and its applications, see [25].
In the classical Newton X  X  world,  X  X istance X  is measured uniquely. This has not been the case for distance in cyber space. A good information distance metric should not only be application independent but also provably better than other  X  X easonable X  definitions.

Traditional distances such as the Euclidean distance or the Hamming distance fail for even trivial examples. Tan et al [39] have demonstrated that none of the 21 metrics used in data mining community is universal, practically. In fact, for any computable distance, we can always find counterex-amples.

What would be a good departure point for defining an  X  X nformation distance X  between two objects? To answer this question, in the early 1990 X  X , we [4] have studied the energy cost of conversion between two strings x and y . John von Neumann hypothesized that performing 1 bit of information processing costs 1 KT of energy, where K is the Boltzmann X  X  constant and T is the room temperature. Observing that re-versible computations can be done for free, in early 1960 X  X  Rolf Landauer revised von Neumann X  X  proposal to hold only for irreversible computations. We proposed in [4] to use the minimum energy needed to convert between x and y to de-fine their distance, as it is an objective measure. Thus, if one wishes to erase string x , then one can reversibly convert it to x  X  , x  X  X  shortest effective description, then erase x the process of erasing | x  X  | bits is irreversible computation. Carrying on from this line of thinking, we have defined in [4] that the energy to convert between x and y to be the small-est number of bits needed to convert from x to y and vice versa. That is, with respect to a universal Turing machine U , the cost of conversion between x and y is: vation, and some other concerns, we have defined the sum distance in [4]: However, the following theore m proved in [4] was a surprise. Theorem 1. E ( x, y )=max { K ( x | y ) ,K ( y | x ) } . Thus, the max distance was defined in [4]: Both distances are shown to satisfy the basic distance re-quirements such as positivity, symmetricity, triangle inequal-ity, in [4]. It was further shown that D max and D sum mi-norize (up to constant factors) all other distances that are computable and satisfies some reasonable density condition that within distance k to any string x , there are at most 2 strings. Formally, a distance D is admissible if D max ( x, y ) satisfies the above requirement because of Kraft X  X  Inequality (with the prefix-free version of Kolmogorov com-plexity). It was proved in [4] that for any admissible com-putable distance D , there is a constant c , for all x, y , Putting it bluntly, if any such distance D discovers some similarity between x and y , so will D max .

Then, after normalization [23] and [24], this theory has been initially applied to alignment free whole genome phylogeny [23], chain letter history [5], language his-tory [3, 24], plagiarism detection [8], and more recently to music classification and clustering [11, 10], parameter-free data mining paradigm [18], protein sequence classification [20], protein structure compa rison [16], heart rhythm data analysis [37, 35], question and answering system [42], bioin-formatics [28, 29], and many more. However, in many of these applications, even when many objects are involved, only pair-wise information distance is computed. This has limited the applicability of this theory.
Can we generalize the theory of information distance to more than two objects? We make a new proposal in this section and perform experiments in the next section.
Similar to Formula 1, given strings x 1 ,...,x n ,wecande-fine the minimum amount of thermodynamic energy needed to convert from any x i to any x j as:
E m ( x 1 ,...,x n )=min {| p | : U ( x i ,p,j )= x j for all i, j Clearly, However, similar to Theorem 1, the following theorem demon-strates a rather surprising property.
 Theorem 2. Modulo to an O (log n ) additive factor,
Proof. Suppose all binary strings are given in a list s 1 s , ... . Define a set V as follows: a vector v =( i 1 ,i 2 is in V if and only if K ( s i 1 s i 2 ...s i n | s i j ) for every j =1 , 2 ,...,n .
Regard V as the vertices of a graph G = V, E .Two vertices u =( u 1 ,...,u n )and v =( v 1 ,...,v n )aresuchthat ( u, v )  X  E if and only if there is 1  X  j  X  n such that u For any given u  X  V and 1  X  j  X  n , by the definition of v = u j .Denote D =max i K ( x 1 x 2 ...x n | x i ). The degree of the graph G is therefore bounded by It is known that a graph with degree d has a d -coloring. Therefore, G has a coloring V = V 1  X  V 2  X  ...  X  V K such that K  X  n  X  2 D . Clearly, ( x 1 ,x 2 ,...,x n )  X  V .Inordertocom-pute x i from x j for any pair of i and j , a universal turing ma-chine only needs to know which V k contains ( x 1 ,x 2 ,...,x Such a program needs only log 2 ( n  X  2 D )= D +log 2 bits.
 Notice that Theorem 2 is a strong claim. Comparing to Theorem 1 where the saving is only linear, the saving here is quadratic. It is possible to prove that E m satisfies the usual metricity properties such as being symmetric and sat-isfying the triangle inequality. It is also possible to prove the E m also has the  X  X niversality X  property that for any other non-trivial distance E m for many objects satisfying theaboveproperties,wealwayshaveforall x 1 ,...,x n ,we have E m ( x 1 ,...,x n )  X  E m ( x 1 ,...,x n )+ O (1). Thus this provides a theory guiding us to compute how much infor-mation a given set of n objects share.
 The following theorem is a corollary of Theorem 2:
Theorem 3. Modulo to an O (log n ) additive factor, min i K ( x 1 ...x n
Given n objects, the left-hand side of the equation may be interpreted as the most comprehensive object that contains the most information about all of the others. The right-hand side of the equation may be interpreted as the most specialized object that is similar to all of the others.
Let us consider news items on the internet. If we wish to choose a news article that covers the most news, we can use the left hand side of Theorem 3. If we wish to look at a typical coverage of a single topic, we can use the right-hand side of Theorem 3.

In the next section, we will use this new theory to guide our practical work. The easiest-to-obtain dataset for exper-imenting our theory turns out to be product reviews. These data can be easily annotated, too. As it turns out, our work also provides an interesting fresh perspective to the work of summarization.
With the rapid development of Web2.0 and e-commerce that emphasizes the participation of users, more and more Websites, such as Amazon (http://www.amazon.com) and Epinions (http://www.epinions.com), encourage people to express opinions on products by posting reviews [43]. These reviews are very useful for readers and will possibly influence their purchasing decisions. However, it would cost too much time for readers to read all of the hundreds of reviews of the same product. Thus, automatic review mining and sum-marization is a very practical concern. The most research on this topic focus on feature-opinion pairs extraction and sentiment orientation decision [17, 33, 15, 7].

However, a human reader is usually not completely satis-fied by a machine generated dull report and may still prefer to read a vivid and complete review article written by a good human writer. This raises the need of selecting the best review from a set of reviews. If only one review from a set was to be read, the most sensible choice would be the most comprehensive review that covers the most information about the other reviews. This is the first goal of our review selection method.

After the most comprehensive review is read, a user may become more interested in one particular feature of the prod-uct and would like to read another concise and representative review focusing on that feature only. Therefore it becomes useful to select the best specialized review that focuses on a given feature and represents the other reviewers X  opinions on that feature only. This becomes the second goal of our review selection method.

Our review selection method is based on the information distance discussed in the previous section. From the discus-sion after Theorem 3, the left-hand side and right-hand side of Equation (6) defines a way to select the most compre-hensive and the best specialized reviews, respectively. How-ever, our problem is that neither the Kolmogorov complexity K (  X  ,  X  )nor D max (  X  ,  X  ) is computable. Therefore, we have to find a way to  X  X pproximate X  these two measures.

The most useful information in a review article is the En-glish words that are related to the product features. If we can extract all of these related words from the review arti-cles, the size of the word set can be regarded as a very rough estimation of information content (or Kolmogorov complex-ity) of the review articles. Although this is a very inaccurate approximation, in Section 5 we will see that this already gives very good practical results.

Our method is outlined in the following. First, for each type of product (such as digital camera), a small set of core feature words (such as price, image) is generated through statistics. Then this core set of words are used to generate the expanded words with an algorithm. Thirdly, an English parser is used to find the dependent words associated to the occurrences of the core feature words and expanded words in a review. For each review-feature pair, the union of the core feature words and expanded words in the review, and their dependent words found by the parser define the related word set of the review on the feature. Lastly, each distinct word in a word set is assigned with one unit of information content; and the left-hand side and right-hand side of Formula (6) are used to select the comprehensive review and specialized review, respectively.
Here  X  X eatures X  broadly mean product features (or at-tributes) and functions that have been commented on in re-views[17]. For example, pixel, memory, shutter, battery,. . . , are features of a digital camera. Given a feature, the core feature words are the very few most common English words that are used to refer to that feature. For example, both  X  X mage X  and  X  X icture X  are used to refer to the same feature of a digital camera.

Feature words are the most direct and frequent words de-scribing a feature, therefore , if there is a feature word in a sentence, it is most likely talking about this feature.
In [17], the authors indicated that when customers com-ment on product features, the words they use converge. Ac-cording to the statistical results of our training corpus, we can get the same conclusion. If we remove the feature words with frequency lower than 1% of the total frequency of all feature words, the remaining words, which are just core fea-ture words, can still cover more than 90% occurrences. Then some of those with the same meaning (such as  X  X mage X  and  X  X icture X ) are grouped into on e feature. In our experiments, on average, each product X  X  each feature has 1.4 core feature words.
Apart from core feature words, many others less-frequently used words that are connected to the feature also contribute to the information content of the feature. For example,  X  X rice X  is an important feature of a product, but the word  X  X rice X  is usually dropped from a sentence. Instead, words such as  X $ X ,  X  X ollars X ,  X  X SD X , and  X  X AD X  are used. It would be impossible to manually enumerate all these different ex-pressions of the same thing. Therefore, these expanded words should be generated automatically.
 We use information distance to expand words. In [12], the Google code of length G ( x ) represents the shortest expected prefix-code word length of the associated Google event x . Then the Google distribution can be used as a compressor for the Google semantics associated with the search terms. Normalized Google distance(NGD) is defined as follows:
NGD ( x, y )= G ( x, y ) where f ( x ) denotes the number of pages containing x ,and f ( x, y ) denotes the number of pages containing both x and y , as reported by Google.

In our work, the distance d between words is defined ac-cording to NGD based on the words X  frequencies and co-occurrence frequencies in the training corpus. Let  X  be a feature and A be the set of its core feature words. The dis-tance between a word w and the feature  X  is then defined to be
Then a distance threshold is used to determine which words should be included in the set of expanded words for a given feature.
If a core feature word or an expanded word is found in a sentence, the words which have grammatical dependent relationship [13] with it are called the dependent words. For example, in sentence  X 4x digital zoom is great X , the words  X 4x X ,  X  X igital X  and  X  X reat X  are all dependent words of the core feature word  X  X oom X . All these words also contribute to the reviews and are important to determine the reviewer X  X  attitude towards a feature.

The Stanford Parser [13] is used to parse each review. For review i and feature j , the core feature words and expanded words that occur in the review are first computed. Then the parsing result is examined to find all the dependent words for the core feature words and expanded words.

The word set S ij is defined to be the union of all the core feature words and expanded words that occur in the review, plus all of their expanded words. Thus, a review is represented by a vector of word sets for all the product X  X  features: S i =( S i 1 ,S i 2 ,...,S in ). Notice that S for two different features j and j may share some common words, but with perhaps totally different meanings. For this reason, it is very important to use an English parser and keep the related word sets for different features separately.
Let S and T be two sets of words, and each word carries one unit of information. Then the Kolmogorov complexity can be intuitively estimated by Here | X | is the size of the set X . Such intuition can be extended to vectors of sets. For two vectors of sets S i ( S i 1 ,S i 2 ,...,S in ), i =1 , 2, define and Then and K ( S 1 S 2 ...S n | S i ) can all be naturally defined as be-fore. Thus, we are able to use Equation (6) for our review selection.

If there are m reviews x 1 ,x 2 ,...,x m ,and n features u straightforwardly from the left-hand side of Equation (6), the most comprehensive review i is such that that is,
The best specialized review needs some minor changes to the right-hand side of Equation (6). Without modification, the best specialized review i for a feature j would be such that
However, for specialized review we want that (a) the re-view focuses on the given feature only, and (b) an review ar-ticle that does not discuss the feature should not be counted in the selection. Therefore, the above formula is modified to be here where S kj is in j th entry.

More specifically, S ij is changed to S i to penalize the con-tent of review i not related to feature j ;andthereviews with an empty word set on feature j are excluded from the selection.

Our method of selecting a single review can be extended to select multiple reviews as well. For example, one can adapt the maximal marginal relevance method introduced in [6] by using our distance, and select these reviews one by one incrementally.
We conducted our experiments using customer reviews on two electronics products: digital cameras (DC) and televi-sions (TV). Totally 138,985 reviews containing 28 million words are used as the corpus for extracting features and computing information distances between feature words and other words. With these distances, expanded words are ex-tracted by method introduced in 4.2.2.

Our experiments focus on single comprehensive and spe-cialized review selection.
To test the performance of comprehensive review selec-tion, six popular DC models from DC and eight TV models with most reviews are selected, resulting 14 test sets with 1381 reviews, approximately 100 reviews each 1 .

Two independent teams from Canada and China, each with two people, annotated the reviews. Each review is an-notated by each team, independently, with one of the fol-lowing four labels according to its comprehensiveness: 1. Label  X  X  X : the most comprehensive reviews. Each test 2. Label  X  X  X : the reviews are very close to the reviews 3. Label  X  X  X : the reviews are fairly comprehensive, but 4. Label  X  X  X : the reviews are incomprehensive, irrelevant,
Then two annotations for each review, from each team, is combined into the final annotation as follows: in each orig-inal annotation, an  X  X  X  is counted as 3 points,  X  X  X  2 points,  X  X  X  1 point and  X  X  X  0 point. A review with a sum of 6 or 5 points gets a final label  X  X  X , a review with 4 or 3 points gets  X  X  X , 2 or 1  X  X  X  and 0  X  X  X .
All these reviews, together with their annota-tionsaswellasourexperimentaldataareavail-able at our website http : //learn.tsinghua.edu.cn : 8080 / 2005310464 /ComprehensiveAndSpecialized.htm .
In our system, reviews are ra nkedaccordingtoequation (7), and those ranked first in their test sets by our system are called  X  X op 1 reviews X , so there are 14  X  X op 1 reviews X . Table 1 evaluates how all top 1 reviews coincide with the annotations. The last four columns show the number of top 1 reviews are labeled with  X  X  X , X  X  X , X  X  X  and  X  X  X  in human an-notation, respectively. The last row is the numbers of  X  X  X ,  X  X  X ,  X  X  X  and  X  X  X  labels, respectively, in annotation. Obvi-ously, most top 1 reviews are labeled with  X  X  X  X  or  X  X  X  X , and no  X  X  X  X .

Then top N reviews are checked to see whether the re-views ranked by information distance coincide with their comprehensiveness. Table 2 shows the results:
Firstly, it can be seen from first five columns that, in 70 reviews ranked top 5 in 14 test sets, there are 14  X  X  X  X , 34  X  X  X  X , and 18  X  X  X  X , only 4  X  X  X  X . The last row shows the total number of reviews that are labeled by  X  X  X , X  X  X , X  X  X  and  X  X  X , respectively. More than half  X  X  X  reviews are in top 2 of its test set, and almost all  X  X  X  X  are in top 10.

Then two manual annotations are used as baselines to be compared with the result of our system, called  X  X aseline 1 X  and  X  X aseline 2 X  in Table 2. As human annotations only have four labels:  X  X  X , X  X  X , X  X  X  and  X  X  X , a simple method is used to change them into numeric rankings: firstly, reviews labeled  X  X  X  are ranked highest, followed by  X  X  X , and then  X  X  X .  X  X  X  X  are all in the last. Second, reviews with the same label are ranked randomly. For most test sets, each of them has only one review labeled  X  X  X , therefore, highest ranking reviews are much less affected by random. These two baselines are also measured by final annotation. The results are shown in the last eight columns of Table 2. From this table we can see human annotations differ slightly from each other. Our system result is close to manual ones.
 Figure 1 shows the proportions of  X  X  X , X  X  X , X  X  X  and  X  X  X  in top N results of our system and in the annotation. It can be seen from the figure that in the annotation, reviews labeled  X  X  X  take a fairly small part (only 1.23%), and  X  X  X  X  take almost three quarters of the test set, while in Top 1 and Top 2 results,  X  X  X  X  are around 40%.
A set containing 339 DC reviews are used as the special-ized review selection task test set, and twelve features are selected. Specialized reviews selected by our system are com-pared with the human annotation, selected according to the most frequent opinion on a specific feature. Nine of twelve reviews selected by our system agree with human-annotated popular opinion on particular feature.

Table 3 is the result of specialized review selection. The first column contains DC features, and the second column is the most frequently used sentence or phrase to describe corresponding features, selected by human. The last column shows the relevant sentences or phrases, selected by human, of the specialized reviews selected by our system. According Figure 1: Proportions of  X  X  X  to  X  X  X  in top N results of our system and the annotation. to the table, except features  X  X xposure X ,  X  X lash X  and  X  X em-ory X , reviews selected by our system agree with the humanly selected opinion very well.
We have initially developed the theory of information dis-tance among many objects and solved several theoretical problems. We have provided a framework so that such a theory can be applied to review mining. We have actually built a comprehensive and specialized review mining system based on our new theory and demonstrated the performance of our system.

In the future work, we will further improve our approach by using sentiment classification method. For example, while talking about a type of digital camera,  X  X reat battery life X  has the same meaning with  X  X ong battery life X , therefore, if sentiment information is considered, reviews selected by our improved approach may be more typical and the accuracy can be promoted.
 The work of CL was done at the University of Waterloo as an exchange student, financially supported by China State-funded Study Abroad Program. ML is supported by NSERC RGPIN46506 and a Canada Research Chair program; BM is supported by NSERC and a Canada Research Chair pro-gram.
 Table 3: Specialized Review Selection. Third col-umn: the reviews are automatically selected, but the sentence / phrase for a specific feature is manu-ally picked.

Feature Popular Opinion of the Review Set Sentence or Phrase exposure missing manually adjustable exposure image good pictures ...great pictures ... memory not enough/need an extra memory card screen large screen ...large bright screen ... shutter fast/quick shutter ...fast shutter lag ... [1] C. An  X  e and M. Sanderson. Missing the forest for the [2] T. Arbuchle, A. Balaban, D. Peters, and M. Lawford. [3] D. Benedetto, E. Caglioti, and V. Loreto. Language [4] C. Bennett, P. Gacs, M. Li, P. Vit  X  anyi, and W. Zurek. [5] C. Bennett, M. Li, and B. Ma. Chain letters and [6] J. Carbonell and J. Goldstein. The use of mmr, [7] P. Chaovalit and L. Zhou. Movie review mining: a [8] X. Chen, B. Francia, M. Li, B. Mckinnon, and [9] A. Chernov, A. Muchnik, A. Romashchenko, A. Shen, [10] R. Cilibrasi and P. Vit  X  anyi. Clustering by [11] R. Cilibrasi, P. Vit  X  anyi, and R. de Wolf. Algorithmic [12] R. L. Cilibrasi and P. M. Vit  X  anyi. The google [13] M. C. de Marneffe, B. MacCartney, and C. D. [14] K. Emanuel, S. Ravela, E. Vivant, and C. Risi. A [15] M. Gamon, A. Aue, S. C. Oliver, and E. Ringger. [16] M. Hayashida and T. Akutsu. Image [17] M. Hu and B. Liu. Mining and summarizing customer [18] E. Keogh, S. Lonardi, and C. Ratanamahatana. [19] S. Kirk and S. Jenkins. Information theory-based [20] A. Kocsor, A. Kertesz-Farkas, L. Kajan, and [21] A. Kraskov, H. Stogbauer, R. Andrzejak, and [22] N. Krasnogor and D. Pelta. Measuring the similarity of [23] M.Li,J.Badger,X.Chen,S.Kwong,P.Kearney,and [24] M. Li, X. Chen, X. Li, B. Ma, and P. Vit  X  anyi. The [25] M. Li and P. Vit  X  anyi. An Introduction to Kolmogorov [26] A. Muchnik. Conditional comlexity and codes. [27] A. Muchnik and N. Vereshchagin. Logical operations [28] M. Nykter, N. Price, M. Aldana, S. Ramsey, [29] M.Nykter,N.Price,A.Larjo,T.Aho,S.Kauffman, [30] H. Otu and K. Sayood. A new sequence distance [31] H. Pao and J. Case. Computing entropy for ortholog [32] D. Parry. Use of Kolmogorov distance identification of [33] A. M. Popescu and O. Etzioni. Extracting product [34] S. Rahmati and J. Glasgow. Noise tolerance of [35] C.Santos,J.Bernardes,P.Vit  X  anyi, and L. Antunes. [36] A. Shen and N. Vereshchagin. Logical operations and [37] A. Siebes and Z. Struzik. Complex data: Mining using [38] W. Taha, S. Crosby, and K. Swadi. A new approach to [39] P. Tan, V. Kumar, and J. Srivastava. Selecting the [40] N. Vereshchagin and M. V X  X ugin. Independent [41] M. V X  X ugin. Information distance and conditional [42] X. Zhang, Y. Hao, X. Zhu, and M. Li. Information [43] L. Zhuang, F. Jing, and X. Zhu. Movie review mining
