 Rare events detection is a challenging problem frequently encountered in data mining research. It can be characterized as an imbalanced classification problem. Many are usually of severe class imbalance, i.e. the occurrence frequency or probability of usually show a strong bias against rare events. This makes rare events very difficult to predict, although they are highly important. 
Detecting rare events on online data has two major requirements: (i) the algorithm of these requirements according to our knowledge. sensitive analysis is widely accepted as a reasonable approach to evaluate imbalanced classifiers in many real-world applications [3]. The proposed approach can accurately estimate statistical distributions and calculate the expected benefit of each class label. A size-varying moving window and a forgetting factor to smoothly remove old approach has consistently outperformed other well-known cost-insensitive online algorithms such as CVFDT [4], online Na X ve Bayes [5], Ensemble classifier [6], and Winnow [7]. 
The rest of this paper is organized as follows: In Section 2, the work related to our research is summarized. In Section 3, th e problem of rare event detection is foundation of our approach. In Section 5, extensive tests are conducted on real-world datasets. Section 6 concludes the paper. methods do not place more emphasis on common classes, therefore show no bias against rare classes. Different from above methods, cost-sensitive analysis [3] relaxes the assumption that all classification errors have the same cost, and gives more weight to rare classes. Therefore, it is able to ev aluate the classifier performance effectively and set a proper classification objective. The absolute/relative lack of data is another proposed to rebalance the class distribution to solve this problem. There are a few well-known online algorithms currently available: CVFDT [4], classifier without referring to old data. 3.1 Basic Problem Formulation Given a dataset ) , ),...( , )...( , ( point. ) ,... , ( label assigned to ' f between observation vector relationship f , and to classify the unseen data with the model. In this paper, we focus detection, the data volume can be potentially infinite. 3.2 Evaluation Criteria A proper evaluation criterion needs to be defined to guide the training process. Here we start with four preliminary definitions: 
Assuming the cost of misclassifying positive observation is ) , ( N P C and the cost of classification is defined as [3], minimize defined as, 
The total benefit defined above is actually identical to the cost metric discussed in [11]. This metric is introduced since the co mmonly used cost-sensitive analysis only to be zero. Our approach is desi gned to maximize the total benefit minimizing the cost justified in the experiments. Cost/benefit analysis is incorporated into the decision procedure to determine a proper decision rule. We conduct a theoretical analysis to show that the point estimate of the occurring probability of rare events is insufficient for accurate classification. Instead, STOCS employs a confidence interval for density estimation. A size-varying moving only requires a single pass over data. 4.1 Fundamentals of STOCS Approach Given an observation ) ,... , ( probability of the target event 1 c can be calculated using Equation (3) [5]. 
Assuming that each attribute domain is divided into several intervals and interval ing distributions ) ( be replaced by ) ,......, | ( probability . 
As discussed in [3], the cost/benefit analysis should be incorporated into the classifier to handle rare cla sses. We assume that the benefits of true positive and true are denoted as ) , ( P N C and ) , ( N P C . Given an observation region ) ,... , ( = and sample proportion ) ,......, , | ( training data. Now the question is that if we know the exact value of p , how should we determine the class label for classification can be calculated as, Similarly, the expected benefit of classifying t X as 0 c is: c has a greater probability to achieve a larger benefit, and vice versa. Therefore, we will have a very accurate estimation of p , by choosing a very small  X  , e.g. 0.01. The expected benefit and where Calculation of expected benefit is demonstrat ed in Fig.1. The expected benefit can be calculated by integration over the z-interval ] , [ equals the positive area p A minus the negative area n A in Fig.1. 5.1 Algorithms Selected for Comparison The performance of STOCS is compared w ith several well-known online algorithms, and Winnow [7]. Two real-world datasets, Australia electricity price dataset [12] and KDD Cup 99 dataset [13] are used as the benchmarking datasets. 5.2 Experiments on Real-World Datasets Australian National Electricity Market (NEM). In the electricity market, the abnormal (they can be hundreds of times greater than normal prices), hence are of high interest [2]. The price spike forecasting problem is a typical rare events detection problem on minutes. Hence a fast algorithm is required to do training and classification. Secondly, price data is suitable to be a benchmark dataset for our algorithm. The price data used in our experiment are downloaded from the website of NEM [12]. 
In the experiments, parameters of STOCS are set as: 5000 , 01 . 0 , 20 = = = In price spike forecasting, true negative is usually considered having no benefit while superior to other methods in dealing with rare events detection. 
The second real-world dataset used in our experiment is from the well-known data mining competition, KDD Cup 99 [13]. This dataset contains data of network experiment, the class label  X  X nmpgetattack X  is regarded as the rare event other class labels are cons idered as common event shown in Fig. 3. are negative because only the costs are calculated in the algorithm. 
According to Fig. 3, STOCS again achiev es consistently better performance than other algorithms on KDD Cup 99 dataset. The results clearly indicate that STOCS is classification benefits. 
To demonstrate the time effectiveness of STOCS, the cumulative processing time of STOCS on two benchmark datasets are shown in Figs 4-5. As discussed in Section window size has been determined and STOCS st arts to detect rare events. As seen in STOCS has a linear time complexity and therefore is a highly efficient algorithm for handling high-speed online data. normal distribution, according to the Ce ntral Limit Theory (CLT). With these probabilities, the observations can be classified by evaluating the expecting benefits. therefore demonstrates a significant improvement on predicting rare events over introduced to incrementally revise the classifier with new data. 
